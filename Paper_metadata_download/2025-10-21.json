[
  {
    "paper": {
      "id": "2510.17681",
      "authors": [
        {
          "_id": "68f6ed0724c448936311184f",
          "name": "Yuandong Pu",
          "hidden": false
        },
        {
          "_id": "68f6ed0724c4489363111850",
          "name": "Le Zhuo",
          "hidden": false
        },
        {
          "_id": "68f6ed0724c4489363111851",
          "name": "Songhao Han",
          "hidden": false
        },
        {
          "_id": "68f6ed0724c4489363111852",
          "name": "Jinbo Xing",
          "hidden": false
        },
        {
          "_id": "68f6ed0724c4489363111853",
          "name": "Kaiwen Zhu",
          "hidden": false
        },
        {
          "_id": "68f6ed0724c4489363111854",
          "name": "Shuo Cao",
          "hidden": false
        },
        {
          "_id": "68f6ed0724c4489363111855",
          "name": "Bin Fu",
          "hidden": false
        },
        {
          "_id": "68f6ed0724c4489363111856",
          "name": "Si Liu",
          "hidden": false
        },
        {
          "_id": "68f6ed0724c4489363111857",
          "name": "Hongsheng Li",
          "hidden": false
        },
        {
          "_id": "68f6ed0724c4489363111858",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "68f6ed0724c4489363111859",
          "name": "Wenlong Zhang",
          "hidden": false
        },
        {
          "_id": "68f6ed0724c448936311185a",
          "name": "Xi Chen",
          "hidden": false
        },
        {
          "_id": "68f6ed0724c448936311185b",
          "name": "Yihao Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-20T15:53:57.000Z",
      "submittedOnDailyAt": "2025-10-21T00:48:55.230Z",
      "title": "PICABench: How Far Are We from Physically Realistic Image Editing?",
      "submittedOnDailyBy": {
        "_id": "625d5b9f0bec31f086e04cd9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1650285458447-noauth.jpeg",
        "isPro": false,
        "fullname": "YuandongPu",
        "user": "Andrew613",
        "type": "user"
      },
      "summary": "Image editing has achieved remarkable progress recently. Modern editing\nmodels could already follow complex instructions to manipulate the original\ncontent. However, beyond completing the editing instructions, the accompanying\nphysical effects are the key to the generation realism. For example, removing\nan object should also remove its shadow, reflections, and interactions with\nnearby objects. Unfortunately, existing models and benchmarks mainly focus on\ninstruction completion but overlook these physical effects. So, at this moment,\nhow far are we from physically realistic image editing? To answer this, we\nintroduce PICABench, which systematically evaluates physical realism across\neight sub-dimension (spanning optics, mechanics, and state transitions) for\nmost of the common editing operations (add, remove, attribute change, etc). We\nfurther propose the PICAEval, a reliable evaluation protocol that uses\nVLM-as-a-judge with per-case, region-level human annotations and questions.\nBeyond benchmarking, we also explore effective solutions by learning physics\nfrom videos and construct a training dataset PICA-100K. After evaluating most\nof the mainstream models, we observe that physical realism remains a\nchallenging problem with large rooms to explore. We hope that our benchmark and\nproposed solutions can serve as a foundation for future work moving from naive\ncontent editing toward physically consistent realism.",
      "upvotes": 40,
      "discussionId": "68f6ed0724c448936311185c",
      "projectPage": "https://picabench.github.io/",
      "githubRepo": "https://github.com/Andrew0613/PICABench",
      "ai_summary": "PICABench and PICAEval evaluate physical realism in image editing by assessing eight sub-dimensions and using VLM-as-a-judge with human annotations, highlighting the need for physics-based solutions.",
      "ai_keywords": [
        "PICABench",
        "PICAEval",
        "VLM-as-a-judge",
        "physical realism",
        "image editing",
        "physics-based solutions",
        "PICA-100K"
      ],
      "githubStars": 3
    },
    "publishedAt": "2025-10-20T11:53:57.000Z",
    "title": "PICABench: How Far Are We from Physically Realistic Image Editing?",
    "summary": "Image editing has achieved remarkable progress recently. Modern editing\nmodels could already follow complex instructions to manipulate the original\ncontent. However, beyond completing the editing instructions, the accompanying\nphysical effects are the key to the generation realism. For example, removing\nan object should also remove its shadow, reflections, and interactions with\nnearby objects. Unfortunately, existing models and benchmarks mainly focus on\ninstruction completion but overlook these physical effects. So, at this moment,\nhow far are we from physically realistic image editing? To answer this, we\nintroduce PICABench, which systematically evaluates physical realism across\neight sub-dimension (spanning optics, mechanics, and state transitions) for\nmost of the common editing operations (add, remove, attribute change, etc). We\nfurther propose the PICAEval, a reliable evaluation protocol that uses\nVLM-as-a-judge with per-case, region-level human annotations and questions.\nBeyond benchmarking, we also explore effective solutions by learning physics\nfrom videos and construct a training dataset PICA-100K. After evaluating most\nof the mainstream models, we observe that physical realism remains a\nchallenging problem with large rooms to explore. We hope that our benchmark and\nproposed solutions can serve as a foundation for future work moving from naive\ncontent editing toward physically consistent realism.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.17681.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "625d5b9f0bec31f086e04cd9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1650285458447-noauth.jpeg",
      "fullname": "YuandongPu",
      "name": "Andrew613",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.16872",
      "authors": [
        {
          "_id": "68f6ed0424c4489363111848",
          "name": "Shaolei Zhang",
          "hidden": false
        },
        {
          "_id": "68f6ed0424c4489363111849",
          "name": "Ju Fan",
          "hidden": false
        },
        {
          "_id": "68f6ed0424c448936311184a",
          "name": "Meihao Fan",
          "hidden": false
        },
        {
          "_id": "68f6ed0424c448936311184b",
          "name": "Guoliang Li",
          "hidden": false
        },
        {
          "_id": "68f6ed0424c448936311184c",
          "name": "Xiaoyong Du",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-19T15:13:42.000Z",
      "submittedOnDailyAt": "2025-10-21T00:48:17.032Z",
      "title": "DeepAnalyze: Agentic Large Language Models for Autonomous Data Science",
      "submittedOnDailyBy": {
        "_id": "64803e5dc57f629056c601f1",
        "avatarUrl": "/avatars/a9e9c97c70714e3a29bef2cf929ee6b3.svg",
        "isPro": false,
        "fullname": "Shaolei Zhang",
        "user": "zhangshaolei",
        "type": "user"
      },
      "summary": "Autonomous data science, from raw data sources to analyst-grade deep research\nreports, has been a long-standing challenge, and is now becoming feasible with\nthe emergence of powerful large language models (LLMs). Recent workflow-based\ndata agents have shown promising results on specific data tasks but remain\nfundamentally limited in achieving fully autonomous data science due to their\nreliance on predefined workflows. In this paper, we introduce DeepAnalyze-8B,\nthe first agentic LLM designed for autonomous data science, capable of\nautomatically completing the end-toend pipeline from data sources to\nanalyst-grade deep research reports. To tackle high-complexity data science\ntasks, we propose a curriculum-based agentic training paradigm that emulates\nthe learning trajectory of human data scientists, enabling LLMs to\nprogressively acquire and integrate multiple capabilities in real-world\nenvironments. We also introduce a data-grounded trajectory synthesis framework\nthat constructs high-quality training data. Through agentic training,\nDeepAnalyze learns to perform a broad spectrum of data tasks, ranging from data\nquestion answering and specialized analytical tasks to open-ended data\nresearch. Experiments demonstrate that, with only 8B parameters, DeepAnalyze\noutperforms previous workflow-based agents built on most advanced proprietary\nLLMs. The model, code, and training data of DeepAnalyze are open-sourced,\npaving the way toward autonomous data science.",
      "upvotes": 40,
      "discussionId": "68f6ed0424c448936311184d",
      "projectPage": "https://ruc-deepanalyze.github.io/",
      "githubRepo": "https://github.com/ruc-datalab/DeepAnalyze",
      "ai_summary": "DeepAnalyze-8B, an agentic LLM, autonomously completes the data science pipeline from raw data to research reports using curriculum-based training and data-grounded trajectory synthesis.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "workflow-based data agents",
        "agentic LLM",
        "curriculum-based agentic training",
        "data-grounded trajectory synthesis",
        "data question answering",
        "specialized analytical tasks",
        "open-ended data research"
      ],
      "githubStars": 28,
      "organization": {
        "_id": "621a22353bae762bb9faaffb",
        "name": "RUC-DataLab",
        "fullname": "RUC-DataLab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64803e5dc57f629056c601f1/tsYgFKBKYc4VNfO8g5zmP.png"
      }
    },
    "publishedAt": "2025-10-19T11:13:42.000Z",
    "title": "DeepAnalyze: Agentic Large Language Models for Autonomous Data Science",
    "summary": "Autonomous data science, from raw data sources to analyst-grade deep research\nreports, has been a long-standing challenge, and is now becoming feasible with\nthe emergence of powerful large language models (LLMs). Recent workflow-based\ndata agents have shown promising results on specific data tasks but remain\nfundamentally limited in achieving fully autonomous data science due to their\nreliance on predefined workflows. In this paper, we introduce DeepAnalyze-8B,\nthe first agentic LLM designed for autonomous data science, capable of\nautomatically completing the end-toend pipeline from data sources to\nanalyst-grade deep research reports. To tackle high-complexity data science\ntasks, we propose a curriculum-based agentic training paradigm that emulates\nthe learning trajectory of human data scientists, enabling LLMs to\nprogressively acquire and integrate multiple capabilities in real-world\nenvironments. We also introduce a data-grounded trajectory synthesis framework\nthat constructs high-quality training data. Through agentic training,\nDeepAnalyze learns to perform a broad spectrum of data tasks, ranging from data\nquestion answering and specialized analytical tasks to open-ended data\nresearch. Experiments demonstrate that, with only 8B parameters, DeepAnalyze\noutperforms previous workflow-based agents built on most advanced proprietary\nLLMs. The model, code, and training data of DeepAnalyze are open-sourced,\npaving the way toward autonomous data science.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.16872.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64803e5dc57f629056c601f1",
      "avatarUrl": "/avatars/a9e9c97c70714e3a29bef2cf929ee6b3.svg",
      "fullname": "Shaolei Zhang",
      "name": "zhangshaolei",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "organization": {
      "_id": "621a22353bae762bb9faaffb",
      "name": "RUC-DataLab",
      "fullname": "RUC-DataLab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64803e5dc57f629056c601f1/tsYgFKBKYc4VNfO8g5zmP.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.17800",
      "authors": [
        {
          "_id": "68f6f94824c4489363111924",
          "name": "Jiale Cheng",
          "hidden": false
        },
        {
          "_id": "68f6f94824c4489363111925",
          "name": "Yusen Liu",
          "hidden": false
        },
        {
          "_id": "68f6f94824c4489363111926",
          "name": "Xinyu Zhang",
          "hidden": false
        },
        {
          "_id": "68f6f94824c4489363111927",
          "name": "Yulin Fei",
          "hidden": false
        },
        {
          "_id": "68f6f94824c4489363111928",
          "name": "Wenyi Hong",
          "hidden": false
        },
        {
          "_id": "68f6f94824c4489363111929",
          "name": "Ruiliang Lyu",
          "hidden": false
        },
        {
          "_id": "68f6f94824c448936311192a",
          "name": "Weihan Wang",
          "hidden": false
        },
        {
          "_id": "68f6f94824c448936311192b",
          "name": "Zhe Su",
          "hidden": false
        },
        {
          "_id": "68f6f94824c448936311192c",
          "name": "Xiaotao Gu",
          "hidden": false
        },
        {
          "_id": "68f6f94824c448936311192d",
          "name": "Xiao Liu",
          "hidden": false
        },
        {
          "_id": "68f6f94824c448936311192e",
          "name": "Yushi Bai",
          "hidden": false
        },
        {
          "_id": "68f6f94824c448936311192f",
          "name": "Jie Tang",
          "hidden": false
        },
        {
          "_id": "68f6f94824c4489363111930",
          "name": "Hongning Wang",
          "hidden": false
        },
        {
          "_id": "68f6f94824c4489363111931",
          "name": "Minlie Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-20T17:58:56.000Z",
      "submittedOnDailyAt": "2025-10-21T02:00:03.910Z",
      "title": "Glyph: Scaling Context Windows via Visual-Text Compression",
      "submittedOnDailyBy": {
        "_id": "627626d42d26ac639e56f565",
        "avatarUrl": "/avatars/805c5f909f52656345b8bde486c9fa8f.svg",
        "isPro": false,
        "fullname": "Jiale Cheng",
        "user": "CCCCCC",
        "type": "user"
      },
      "summary": "Large language models (LLMs) increasingly rely on long-context modeling for\ntasks such as document understanding, code analysis, and multi-step reasoning.\nHowever, scaling context windows to the million-token level brings prohibitive\ncomputational and memory costs, limiting the practicality of long-context LLMs.\nIn this work, we take a different perspective-visual context scaling-to tackle\nthis challenge. Instead of extending token-based sequences, we propose Glyph, a\nframework that renders long texts into images and processes them with\nvision-language models (VLMs). This approach substantially compresses textual\ninput while preserving semantic information, and we further design an\nLLM-driven genetic search to identify optimal visual rendering configurations\nfor balancing accuracy and compression. Through extensive experiments, we\ndemonstrate that our method achieves 3-4x token compression while maintaining\naccuracy comparable to leading LLMs such as Qwen3-8B on various long-context\nbenchmarks. This compression also leads to around 4x faster prefilling and\ndecoding, and approximately 2x faster SFT training. Furthermore, under extreme\ncompression, a 128K-context VLM could scale to handle 1M-token-level text\ntasks. In addition, the rendered text data benefits real-world multimodal\ntasks, such as document understanding. Our code and model are released at\nhttps://github.com/thu-coai/Glyph.",
      "upvotes": 30,
      "discussionId": "68f6f94824c4489363111932",
      "ai_summary": "Glyph compresses long textual inputs into images using vision-language models, achieving significant token compression and improved performance in long-context tasks.",
      "ai_keywords": [
        "long-context modeling",
        "large language models",
        "token-based sequences",
        "Glyph",
        "vision-language models",
        "genetic search",
        "token compression",
        "prefilling",
        "decoding",
        "SFT training",
        "multimodal tasks",
        "document understanding"
      ]
    },
    "publishedAt": "2025-10-20T13:58:56.000Z",
    "title": "Glyph: Scaling Context Windows via Visual-Text Compression",
    "summary": "Large language models (LLMs) increasingly rely on long-context modeling for\ntasks such as document understanding, code analysis, and multi-step reasoning.\nHowever, scaling context windows to the million-token level brings prohibitive\ncomputational and memory costs, limiting the practicality of long-context LLMs.\nIn this work, we take a different perspective-visual context scaling-to tackle\nthis challenge. Instead of extending token-based sequences, we propose Glyph, a\nframework that renders long texts into images and processes them with\nvision-language models (VLMs). This approach substantially compresses textual\ninput while preserving semantic information, and we further design an\nLLM-driven genetic search to identify optimal visual rendering configurations\nfor balancing accuracy and compression. Through extensive experiments, we\ndemonstrate that our method achieves 3-4x token compression while maintaining\naccuracy comparable to leading LLMs such as Qwen3-8B on various long-context\nbenchmarks. This compression also leads to around 4x faster prefilling and\ndecoding, and approximately 2x faster SFT training. Furthermore, under extreme\ncompression, a 128K-context VLM could scale to handle 1M-token-level text\ntasks. In addition, the rendered text data benefits real-world multimodal\ntasks, such as document understanding. Our code and model are released at\nhttps://github.com/thu-coai/Glyph.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.17800.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "627626d42d26ac639e56f565",
      "avatarUrl": "/avatars/805c5f909f52656345b8bde486c9fa8f.svg",
      "fullname": "Jiale Cheng",
      "name": "CCCCCC",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.17354",
      "authors": [
        {
          "_id": "68f6ebff24c448936311182d",
          "name": "Chenghao Zhang",
          "hidden": false
        },
        {
          "_id": "68f6ebff24c448936311182e",
          "name": "Guanting Dong",
          "hidden": false
        },
        {
          "_id": "68f6ebff24c448936311182f",
          "name": "Xinyu Yang",
          "hidden": false
        },
        {
          "_id": "68f6ebff24c4489363111830",
          "name": "Zhicheng Dou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-20T09:56:43.000Z",
      "submittedOnDailyAt": "2025-10-21T00:50:13.474Z",
      "title": "Towards Mixed-Modal Retrieval for Universal Retrieval-Augmented\n  Generation",
      "submittedOnDailyBy": {
        "_id": "6710ac3fb4ee4920580a5f0e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6710ac3fb4ee4920580a5f0e/OhQQFlZmkmLQpMYqKCGP6.jpeg",
        "isPro": false,
        "fullname": "Chenghao Zhang",
        "user": "SnowNation",
        "type": "user"
      },
      "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for\nenhancing large language models (LLMs) by retrieving relevant documents from an\nexternal corpus. However, existing RAG systems primarily focus on unimodal text\ndocuments, and often fall short in real-world scenarios where both queries and\ndocuments may contain mixed modalities (such as text and images). In this\npaper, we address the challenge of Universal Retrieval-Augmented Generation\n(URAG), which involves retrieving and reasoning over mixed-modal information to\nimprove vision-language generation. To this end, we propose Nyx, a unified\nmixed-modal to mixed-modal retriever tailored for URAG scenarios. To mitigate\nthe scarcity of realistic mixed-modal data, we introduce a four-stage automated\npipeline for generation and filtering, leveraging web documents to construct\nNyxQA, a dataset comprising diverse mixed-modal question-answer pairs that\nbetter reflect real-world information needs. Building on this high-quality\ndataset, we adopt a two-stage training framework for Nyx: we first perform\npre-training on NyxQA along with a variety of open-source retrieval datasets,\nfollowed by supervised fine-tuning using feedback from downstream\nvision-language models (VLMs) to align retrieval outputs with generative\npreferences. Experimental results demonstrate that Nyx not only performs\ncompetitively on standard text-only RAG benchmarks, but also excels in the more\ngeneral and realistic URAG setting, significantly improving generation quality\nin vision-language tasks.",
      "upvotes": 25,
      "discussionId": "68f6ec0024c4489363111831",
      "githubRepo": "https://github.com/SnowNation101/Nyx",
      "ai_summary": "Nyx, a unified mixed-modal retriever, enhances vision-language generation by retrieving and reasoning over mixed-modal data, outperforming existing RAG systems in real-world scenarios.",
      "ai_keywords": [
        "Retrieval-Augmented Generation",
        "RAG",
        "large language models",
        "LLMs",
        "mixed modalities",
        "Universal Retrieval-Augmented Generation",
        "URAG",
        "Nyx",
        "NyxQA",
        "two-stage training framework",
        "pre-training",
        "supervised fine-tuning",
        "vision-language models",
        "VLMs",
        "generation quality",
        "vision-language tasks"
      ],
      "githubStars": 6,
      "organization": {
        "_id": "622177ac43826d6f261f8208",
        "name": "RUC",
        "fullname": "Renmin University of China",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/670IAX9A2-BflqA5MiSBW.jpeg"
      }
    },
    "publishedAt": "2025-10-20T05:56:43.000Z",
    "title": "Towards Mixed-Modal Retrieval for Universal Retrieval-Augmented\n  Generation",
    "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for\nenhancing large language models (LLMs) by retrieving relevant documents from an\nexternal corpus. However, existing RAG systems primarily focus on unimodal text\ndocuments, and often fall short in real-world scenarios where both queries and\ndocuments may contain mixed modalities (such as text and images). In this\npaper, we address the challenge of Universal Retrieval-Augmented Generation\n(URAG), which involves retrieving and reasoning over mixed-modal information to\nimprove vision-language generation. To this end, we propose Nyx, a unified\nmixed-modal to mixed-modal retriever tailored for URAG scenarios. To mitigate\nthe scarcity of realistic mixed-modal data, we introduce a four-stage automated\npipeline for generation and filtering, leveraging web documents to construct\nNyxQA, a dataset comprising diverse mixed-modal question-answer pairs that\nbetter reflect real-world information needs. Building on this high-quality\ndataset, we adopt a two-stage training framework for Nyx: we first perform\npre-training on NyxQA along with a variety of open-source retrieval datasets,\nfollowed by supervised fine-tuning using feedback from downstream\nvision-language models (VLMs) to align retrieval outputs with generative\npreferences. Experimental results demonstrate that Nyx not only performs\ncompetitively on standard text-only RAG benchmarks, but also excels in the more\ngeneral and realistic URAG setting, significantly improving generation quality\nin vision-language tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.17354.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6710ac3fb4ee4920580a5f0e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6710ac3fb4ee4920580a5f0e/OhQQFlZmkmLQpMYqKCGP6.jpeg",
      "fullname": "Chenghao Zhang",
      "name": "SnowNation",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "organization": {
      "_id": "622177ac43826d6f261f8208",
      "name": "RUC",
      "fullname": "Renmin University of China",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/670IAX9A2-BflqA5MiSBW.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.15346",
      "authors": [
        {
          "_id": "68f5cb858589920bf4d321c6",
          "user": {
            "_id": "67f778ddbb19958f5d96c2a8",
            "avatarUrl": "/avatars/49a3f119b456ff94f28f09b2fe78bb18.svg",
            "isPro": false,
            "fullname": "Heecheol Yun",
            "user": "yoon6503",
            "type": "user"
          },
          "name": "Heecheol Yun",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-20T12:09:53.779Z",
          "hidden": false
        },
        {
          "_id": "68f5cb858589920bf4d321c7",
          "name": "Kwangmin Ki",
          "hidden": false
        },
        {
          "_id": "68f5cb858589920bf4d321c8",
          "name": "Junghyun Lee",
          "hidden": false
        },
        {
          "_id": "68f5cb858589920bf4d321c9",
          "name": "Eunho Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-17T06:18:29.000Z",
      "submittedOnDailyAt": "2025-10-21T03:22:40.496Z",
      "title": "When to Ensemble: Identifying Token-Level Points for Stable and Fast LLM\n  Ensembling",
      "submittedOnDailyBy": {
        "_id": "67f778ddbb19958f5d96c2a8",
        "avatarUrl": "/avatars/49a3f119b456ff94f28f09b2fe78bb18.svg",
        "isPro": false,
        "fullname": "Heecheol Yun",
        "user": "yoon6503",
        "type": "user"
      },
      "summary": "Ensembling Large Language Models (LLMs) has gained attention as a promising\napproach to surpass the performance of individual models by leveraging their\ncomplementary strengths. In particular, aggregating models' next-token\nprobability distributions to select the next token has been shown to be\neffective in various tasks. However, while successful for short-form answers,\nits application to long-form generation remains underexplored. In this paper,\nwe show that using existing ensemble methods in long-form generation requires a\ncareful choice of ensembling positions, since the standard practice of\nensembling at every token often degrades performance. We identify two key\nfactors for determining these positions: tokenization mismatch across models\nand consensus in their next-token probability distributions. Based on this, we\npropose SAFE, (Stable And Fast LLM Ensembling), a framework that selectively\nensembles by jointly considering these factors. To further improve stability,\nwe introduce a probability sharpening strategy that consolidates probabilities\nspread across multiple sub-word tokens representing the same word into a single\nrepresentative token. Our experiments on diverse benchmarks, including MATH500\nand BBH, demonstrate that SAFE outperforms existing methods in both accuracy\nand efficiency, with gains achieved even when ensembling fewer than 1% of\ntokens.",
      "upvotes": 21,
      "discussionId": "68f5cb858589920bf4d321ca",
      "ai_summary": "SAFE, a selective ensembling framework for large language models, improves long-form generation by considering tokenization mismatch and consensus in probability distributions, leading to better accuracy and efficiency.",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "ensembling",
        "next-token probability distributions",
        "long-form generation",
        "tokenization mismatch",
        "consensus",
        "probability sharpening",
        "MATH500",
        "BBH"
      ],
      "organization": {
        "_id": "6475760c33192631bad2bb38",
        "name": "kaist-ai",
        "fullname": "KAIST AI",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6469949654873f0043b09c22/aaZFiyXe1qR-Dmy_xq67m.png"
      }
    },
    "publishedAt": "2025-10-17T02:18:29.000Z",
    "title": "When to Ensemble: Identifying Token-Level Points for Stable and Fast LLM\n  Ensembling",
    "summary": "Ensembling Large Language Models (LLMs) has gained attention as a promising\napproach to surpass the performance of individual models by leveraging their\ncomplementary strengths. In particular, aggregating models' next-token\nprobability distributions to select the next token has been shown to be\neffective in various tasks. However, while successful for short-form answers,\nits application to long-form generation remains underexplored. In this paper,\nwe show that using existing ensemble methods in long-form generation requires a\ncareful choice of ensembling positions, since the standard practice of\nensembling at every token often degrades performance. We identify two key\nfactors for determining these positions: tokenization mismatch across models\nand consensus in their next-token probability distributions. Based on this, we\npropose SAFE, (Stable And Fast LLM Ensembling), a framework that selectively\nensembles by jointly considering these factors. To further improve stability,\nwe introduce a probability sharpening strategy that consolidates probabilities\nspread across multiple sub-word tokens representing the same word into a single\nrepresentative token. Our experiments on diverse benchmarks, including MATH500\nand BBH, demonstrate that SAFE outperforms existing methods in both accuracy\nand efficiency, with gains achieved even when ensembling fewer than 1% of\ntokens.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.15346.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "67f778ddbb19958f5d96c2a8",
      "avatarUrl": "/avatars/49a3f119b456ff94f28f09b2fe78bb18.svg",
      "fullname": "Heecheol Yun",
      "name": "yoon6503",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "6475760c33192631bad2bb38",
      "name": "kaist-ai",
      "fullname": "KAIST AI",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6469949654873f0043b09c22/aaZFiyXe1qR-Dmy_xq67m.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.17509",
      "authors": [
        {
          "_id": "68f704d324c4489363111991",
          "name": "Shiyu Ni",
          "hidden": false
        },
        {
          "_id": "68f704d324c4489363111992",
          "name": "Keping Bi",
          "hidden": false
        },
        {
          "_id": "68f704d324c4489363111993",
          "name": "Jiafeng Guo",
          "hidden": false
        },
        {
          "_id": "68f704d324c4489363111994",
          "name": "Minghao Tang",
          "hidden": false
        },
        {
          "_id": "68f704d324c4489363111995",
          "name": "Jingtong Wu",
          "hidden": false
        },
        {
          "_id": "68f704d324c4489363111996",
          "name": "Zengxin Han",
          "hidden": false
        },
        {
          "_id": "68f704d324c4489363111997",
          "name": "Xueqi Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-20T13:05:22.000Z",
      "submittedOnDailyAt": "2025-10-21T02:31:10.938Z",
      "title": "Annotation-Efficient Universal Honesty Alignment",
      "submittedOnDailyBy": {
        "_id": "616bfc2b40e2f69baa1c7add",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/616bfc2b40e2f69baa1c7add/Os7_qgMei-2lRVelrOG7B.jpeg",
        "isPro": false,
        "fullname": "Run-Ze Fan",
        "user": "Vfrz",
        "type": "user"
      },
      "summary": "Honesty alignment-the ability of large language models (LLMs) to recognize\ntheir knowledge boundaries and express calibrated confidence-is essential for\ntrustworthy deployment. Existing methods either rely on training-free\nconfidence estimation (e.g., token probabilities, self-consistency) or\ntraining-based calibration with correctness annotations. While effective,\nachieving universal honesty alignment with training-based calibration requires\ncostly, large-scale labeling. To support annotation-efficient training, we\nintroduce Elicitation-Then-Calibration (EliCal), a two-stage framework that\nfirst elicits internal confidence using inexpensive self-consistency\nsupervision, then calibrates this confidence with a small set of correctness\nannotations. To support a large-scale study, we release HonestyBench, a\nbenchmark covering ten free-form QA datasets with 560k training and 70k\nevaluation instances annotated with correctness and self-consistency signals.\nExperiments show that EliCal achieves near-optimal alignment with only 1k\ncorrectness annotations (0.18% of full supervision) and better alignment\nperformance on unseen MMLU tasks than the calibration-only baseline, offering a\nscalable solution toward universal honesty alignment in LLMs.",
      "upvotes": 15,
      "discussionId": "68f704d324c4489363111998",
      "ai_summary": "EliCal, a two-stage framework combining self-consistency supervision and minimal correctness annotations, achieves near-optimal honesty alignment in large language models with limited annotation effort.",
      "ai_keywords": [
        "large language models",
        "honesty alignment",
        "confidence estimation",
        "self-consistency",
        "calibration",
        "correctness annotations",
        "Elicitation-Then-Calibration",
        "EliCal",
        "HonestyBench",
        "free-form QA datasets",
        "MMLU tasks"
      ]
    },
    "publishedAt": "2025-10-20T09:05:22.000Z",
    "title": "Annotation-Efficient Universal Honesty Alignment",
    "summary": "Honesty alignment-the ability of large language models (LLMs) to recognize\ntheir knowledge boundaries and express calibrated confidence-is essential for\ntrustworthy deployment. Existing methods either rely on training-free\nconfidence estimation (e.g., token probabilities, self-consistency) or\ntraining-based calibration with correctness annotations. While effective,\nachieving universal honesty alignment with training-based calibration requires\ncostly, large-scale labeling. To support annotation-efficient training, we\nintroduce Elicitation-Then-Calibration (EliCal), a two-stage framework that\nfirst elicits internal confidence using inexpensive self-consistency\nsupervision, then calibrates this confidence with a small set of correctness\nannotations. To support a large-scale study, we release HonestyBench, a\nbenchmark covering ten free-form QA datasets with 560k training and 70k\nevaluation instances annotated with correctness and self-consistency signals.\nExperiments show that EliCal achieves near-optimal alignment with only 1k\ncorrectness annotations (0.18% of full supervision) and better alignment\nperformance on unseen MMLU tasks than the calibration-only baseline, offering a\nscalable solution toward universal honesty alignment in LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.17509.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "616bfc2b40e2f69baa1c7add",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/616bfc2b40e2f69baa1c7add/Os7_qgMei-2lRVelrOG7B.jpeg",
      "fullname": "Run-Ze Fan",
      "name": "Vfrz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.16888",
      "authors": [
        {
          "_id": "68f7036a24c4489363111985",
          "name": "Zongjian Li",
          "hidden": false
        },
        {
          "_id": "68f7036a24c4489363111986",
          "name": "Zheyuan Liu",
          "hidden": false
        },
        {
          "_id": "68f7036a24c4489363111987",
          "name": "Qihui Zhang",
          "hidden": false
        },
        {
          "_id": "68f7036a24c4489363111988",
          "name": "Bin Lin",
          "hidden": false
        },
        {
          "_id": "68f7036a24c4489363111989",
          "name": "Shenghai Yuan",
          "hidden": false
        },
        {
          "_id": "68f7036a24c448936311198a",
          "name": "Zhiyuan Yan",
          "hidden": false
        },
        {
          "_id": "68f7036a24c448936311198b",
          "name": "Yang Ye",
          "hidden": false
        },
        {
          "_id": "68f7036a24c448936311198c",
          "name": "Wangbo Yu",
          "hidden": false
        },
        {
          "_id": "68f7036a24c448936311198d",
          "name": "Yuwei Niu",
          "hidden": false
        },
        {
          "_id": "68f7036a24c448936311198e",
          "name": "Li Yuan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-19T15:38:06.000Z",
      "submittedOnDailyAt": "2025-10-21T02:23:34.106Z",
      "title": "Uniworld-V2: Reinforce Image Editing with Diffusion Negative-aware\n  Finetuning and MLLM Implicit Feedback",
      "submittedOnDailyBy": {
        "_id": "646df3c04ad7f907279f14c3",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646df3c04ad7f907279f14c3/WZjSDtAmezmjbczLtCP2B.jpeg",
        "isPro": false,
        "fullname": "Zongjian Li",
        "user": "chestnutlzj",
        "type": "user"
      },
      "summary": "Instruction-based image editing has achieved remarkable progress; however,\nmodels solely trained via supervised fine-tuning often overfit to annotated\npatterns, hindering their ability to explore and generalize beyond training\ndistributions. To this end, we introduce Edit-R1, a novel post-training\nframework for instruction-based image editing based on policy optimization.\nSpecifically, we utilize Diffusion Negative-aware Finetuning (DiffusionNFT), a\nlikelihood-free policy optimization method consistent with the flow matching\nforward process, thereby enabling the use of higher-order samplers and more\nefficient training. Another key challenge here is the absence of a universal\nreward model, resulting from the diverse nature of editing instructions and\ntasks. To bridge this gap, we employ a Multimodal Large Language Model (MLLM)\nas a unified, training-free reward model, leveraging its output logits to\nprovide fine-grained feedback. Furthermore, we carefully design a low-variance\ngroup filtering mechanism to reduce MLLM scoring noise and stabilize\noptimization. UniWorld-V2, trained with this framework, achieves\nstate-of-the-art results on the ImgEdit and GEdit-Bench benchmarks,\nscoring 4.49 and 7.83, respectively. Crucially, our framework is\nmodel-agnostic, delivering substantial performance gains when applied to\ndiverse base models like Qwen-Image-Edit and FLUX-Kontext, demonstrating its\nwide applicability. Code and models are publicly available at\nhttps://github.com/PKU-YuanGroup/UniWorld-V2.",
      "upvotes": 10,
      "discussionId": "68f7036a24c448936311198f",
      "githubRepo": "https://github.com/PKU-YuanGroup/UniWorld-V2",
      "ai_summary": "Edit-R1, a post-training framework using Diffusion Negative-aware Finetuning and a Multimodal Large Language Model, achieves state-of-the-art results in instruction-based image editing by addressing overfitting and lack of a universal reward model.",
      "ai_keywords": [
        "Diffusion Negative-aware Finetuning",
        "DiffusionNFT",
        "flow matching",
        "higher-order samplers",
        "Multimodal Large Language Model",
        "MLLM",
        "low-variance group filtering",
        "UniWorld-V2",
        "ImgEdit",
        "GEdit-Bench"
      ],
      "githubStars": 9,
      "organization": {
        "_id": "61dcd8e344f59573371b5cb6",
        "name": "PekingUniversity",
        "fullname": "Peking University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
      }
    },
    "publishedAt": "2025-10-19T11:38:06.000Z",
    "title": "Uniworld-V2: Reinforce Image Editing with Diffusion Negative-aware\n  Finetuning and MLLM Implicit Feedback",
    "summary": "Instruction-based image editing has achieved remarkable progress; however,\nmodels solely trained via supervised fine-tuning often overfit to annotated\npatterns, hindering their ability to explore and generalize beyond training\ndistributions. To this end, we introduce Edit-R1, a novel post-training\nframework for instruction-based image editing based on policy optimization.\nSpecifically, we utilize Diffusion Negative-aware Finetuning (DiffusionNFT), a\nlikelihood-free policy optimization method consistent with the flow matching\nforward process, thereby enabling the use of higher-order samplers and more\nefficient training. Another key challenge here is the absence of a universal\nreward model, resulting from the diverse nature of editing instructions and\ntasks. To bridge this gap, we employ a Multimodal Large Language Model (MLLM)\nas a unified, training-free reward model, leveraging its output logits to\nprovide fine-grained feedback. Furthermore, we carefully design a low-variance\ngroup filtering mechanism to reduce MLLM scoring noise and stabilize\noptimization. UniWorld-V2, trained with this framework, achieves\nstate-of-the-art results on the ImgEdit and GEdit-Bench benchmarks,\nscoring 4.49 and 7.83, respectively. Crucially, our framework is\nmodel-agnostic, delivering substantial performance gains when applied to\ndiverse base models like Qwen-Image-Edit and FLUX-Kontext, demonstrating its\nwide applicability. Code and models are publicly available at\nhttps://github.com/PKU-YuanGroup/UniWorld-V2.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.16888.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "646df3c04ad7f907279f14c3",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646df3c04ad7f907279f14c3/WZjSDtAmezmjbczLtCP2B.jpeg",
      "fullname": "Zongjian Li",
      "name": "chestnutlzj",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "organization": {
      "_id": "61dcd8e344f59573371b5cb6",
      "name": "PekingUniversity",
      "fullname": "Peking University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.16333",
      "authors": [
        {
          "_id": "68f6fb4824c4489363111942",
          "name": "Junha Song",
          "hidden": false
        },
        {
          "_id": "68f6fb4824c4489363111943",
          "name": "Sangdoo Yun",
          "hidden": false
        },
        {
          "_id": "68f6fb4824c4489363111944",
          "name": "Dongyoon Han",
          "hidden": false
        },
        {
          "_id": "68f6fb4824c4489363111945",
          "name": "Jaegul Choo",
          "hidden": false
        },
        {
          "_id": "68f6fb4824c4489363111946",
          "name": "Byeongho Heo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-18T03:37:17.000Z",
      "submittedOnDailyAt": "2025-10-21T01:47:47.175Z",
      "title": "RL makes MLLMs see better than SFT",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "A dominant assumption in Multimodal Language Model (MLLM) research is that\nits performance is largely inherited from the LLM backbone, given its immense\nparameter scale and remarkable capabilities. This has created a void in the\nunderstanding of the vision encoder, which determines how MLLMs perceive\nimages. The recent shift in MLLM training paradigms, from Supervised Finetuning\n(SFT) to Reinforcement Learning (RL), magnifies this oversight-namely, the\nsignificant lack of analysis on how such training reshapes the vision encoder\nas well as the MLLM. To address this, we first investigate the impact of\ntraining strategies on MLLMs, where RL shows a clear advantage over SFT in\nstrongly vision-related VQA benchmarks. Motivated by this, we conduct a\ncritical yet under-explored analysis of the vision encoder of MLLMs through\ndiverse and in-depth experiments, ranging from ImageNet classification and\nsegmentation to gradient visualization. Our results demonstrate that MLLM's\npost-training strategy (i.e., SFT or RL) not only leads to distinct outcomes on\nMLLM downstream tasks, but also fundamentally reshapes MLLM's underlying visual\nrepresentations. Specifically, the key finding of our study is that RL produces\nstronger and precisely localized visual representations compared to SFT,\nboosting the ability of the vision encoder for MLLM. We then reframe our\nfindings into a simple recipe for building strong vision encoders for MLLMs,\nPreference-Instructed Vision OpTimization (PIVOT). When integrated into MLLMs,\na PIVOT-trained vision encoder outperforms even larger and more heavily-trained\ncounterparts, despite requiring less than 1% of the computational cost of\nstandard vision pretraining. This result opens an effective and efficient path\nfor advancing the vision backbones of MLLMs. Project page available at\nhttps://june-page.github.io/pivot/",
      "upvotes": 10,
      "discussionId": "68f6fb4824c4489363111947",
      "projectPage": "https://june-page.github.io/pivot/",
      "ai_summary": "Reinforcement Learning enhances vision encoders in Multimodal Language Models, leading to better visual representations and performance compared to Supervised Fine-tuning.",
      "ai_keywords": [
        "Multimodal Language Model",
        "LLM backbone",
        "vision encoder",
        "Supervised Finetuning",
        "Reinforcement Learning",
        "VQA benchmarks",
        "ImageNet classification",
        "segmentation",
        "gradient visualization",
        "visual representations",
        "Preference-Instructed Vision OpTimization",
        "PIVOT"
      ],
      "organization": {
        "_id": "64ffe603efd273eec7768bde",
        "name": "naver-ai",
        "fullname": "NAVER AI Lab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64ff1755b75685dd7a46e146/Zj2bxgq31oSqwVrw16IE_.png"
      }
    },
    "publishedAt": "2025-10-17T23:37:17.000Z",
    "title": "RL makes MLLMs see better than SFT",
    "summary": "A dominant assumption in Multimodal Language Model (MLLM) research is that\nits performance is largely inherited from the LLM backbone, given its immense\nparameter scale and remarkable capabilities. This has created a void in the\nunderstanding of the vision encoder, which determines how MLLMs perceive\nimages. The recent shift in MLLM training paradigms, from Supervised Finetuning\n(SFT) to Reinforcement Learning (RL), magnifies this oversight-namely, the\nsignificant lack of analysis on how such training reshapes the vision encoder\nas well as the MLLM. To address this, we first investigate the impact of\ntraining strategies on MLLMs, where RL shows a clear advantage over SFT in\nstrongly vision-related VQA benchmarks. Motivated by this, we conduct a\ncritical yet under-explored analysis of the vision encoder of MLLMs through\ndiverse and in-depth experiments, ranging from ImageNet classification and\nsegmentation to gradient visualization. Our results demonstrate that MLLM's\npost-training strategy (i.e., SFT or RL) not only leads to distinct outcomes on\nMLLM downstream tasks, but also fundamentally reshapes MLLM's underlying visual\nrepresentations. Specifically, the key finding of our study is that RL produces\nstronger and precisely localized visual representations compared to SFT,\nboosting the ability of the vision encoder for MLLM. We then reframe our\nfindings into a simple recipe for building strong vision encoders for MLLMs,\nPreference-Instructed Vision OpTimization (PIVOT). When integrated into MLLMs,\na PIVOT-trained vision encoder outperforms even larger and more heavily-trained\ncounterparts, despite requiring less than 1% of the computational cost of\nstandard vision pretraining. This result opens an effective and efficient path\nfor advancing the vision backbones of MLLMs. Project page available at\nhttps://june-page.github.io/pivot/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.16333.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 133
    },
    "organization": {
      "_id": "64ffe603efd273eec7768bde",
      "name": "naver-ai",
      "fullname": "NAVER AI Lab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64ff1755b75685dd7a46e146/Zj2bxgq31oSqwVrw16IE_.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.17795",
      "authors": [
        {
          "_id": "68f6f0d824c4489363111892",
          "name": "Yujie Luo",
          "hidden": false
        },
        {
          "_id": "68f6f0d824c4489363111893",
          "name": "Zhuoyun Yu",
          "hidden": false
        },
        {
          "_id": "68f6f0d824c4489363111894",
          "name": "Xuehai Wang",
          "hidden": false
        },
        {
          "_id": "68f6f0d824c4489363111895",
          "name": "Yuqi Zhu",
          "hidden": false
        },
        {
          "_id": "68f6f0d824c4489363111896",
          "name": "Ningyu Zhang",
          "hidden": false
        },
        {
          "_id": "68f6f0d824c4489363111897",
          "name": "Lanning Wei",
          "hidden": false
        },
        {
          "_id": "68f6f0d824c4489363111898",
          "name": "Lun Du",
          "hidden": false
        },
        {
          "_id": "68f6f0d824c4489363111899",
          "name": "Da Zheng",
          "hidden": false
        },
        {
          "_id": "68f6f0d824c448936311189a",
          "name": "Huajun Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-20T17:53:23.000Z",
      "submittedOnDailyAt": "2025-10-21T01:03:47.002Z",
      "title": "Executable Knowledge Graphs for Replicating AI Research",
      "submittedOnDailyBy": {
        "_id": "620b3bbb0668e435407c8d0a",
        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
        "isPro": true,
        "fullname": "Ningyu Zhang",
        "user": "Ningyu",
        "type": "user"
      },
      "summary": "Replicating AI research is a crucial yet challenging task for large language\nmodel (LLM) agents. Existing approaches often struggle to generate executable\ncode, primarily due to insufficient background knowledge and the limitations of\nretrieval-augmented generation (RAG) methods, which fail to capture latent\ntechnical details hidden in referenced papers. Furthermore, previous approaches\ntend to overlook valuable implementation-level code signals and lack structured\nknowledge representations that support multi-granular retrieval and reuse. To\novercome these challenges, we propose Executable Knowledge Graphs (xKG), a\nmodular and pluggable knowledge base that automatically integrates technical\ninsights, code snippets, and domain-specific knowledge extracted from\nscientific literature. When integrated into three agent frameworks with two\ndifferent LLMs, xKG shows substantial performance gains (10.9% with o3-mini) on\nPaperBench, demonstrating its effectiveness as a general and extensible\nsolution for automated AI research replication. Code will released at\nhttps://github.com/zjunlp/xKG.",
      "upvotes": 9,
      "discussionId": "68f6f0d824c448936311189b",
      "ai_summary": "Executable Knowledge Graphs (xKG) enhance AI research replication by integrating technical insights and code snippets from scientific literature, improving performance in automated replication tasks.",
      "ai_keywords": [
        "large language model (LLM)",
        "retrieval-augmented generation (RAG)",
        "Executable Knowledge Graphs (xKG)",
        "technical insights",
        "code snippets",
        "domain-specific knowledge",
        "PaperBench",
        "o3-mini"
      ],
      "organization": {
        "_id": "67c1d682826160b28f778510",
        "name": "antgroup",
        "fullname": "Ant Group",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/7VcPHdLSGlged3ixK1dys.jpeg"
      }
    },
    "publishedAt": "2025-10-20T13:53:23.000Z",
    "title": "Executable Knowledge Graphs for Replicating AI Research",
    "summary": "Replicating AI research is a crucial yet challenging task for large language\nmodel (LLM) agents. Existing approaches often struggle to generate executable\ncode, primarily due to insufficient background knowledge and the limitations of\nretrieval-augmented generation (RAG) methods, which fail to capture latent\ntechnical details hidden in referenced papers. Furthermore, previous approaches\ntend to overlook valuable implementation-level code signals and lack structured\nknowledge representations that support multi-granular retrieval and reuse. To\novercome these challenges, we propose Executable Knowledge Graphs (xKG), a\nmodular and pluggable knowledge base that automatically integrates technical\ninsights, code snippets, and domain-specific knowledge extracted from\nscientific literature. When integrated into three agent frameworks with two\ndifferent LLMs, xKG shows substantial performance gains (10.9% with o3-mini) on\nPaperBench, demonstrating its effectiveness as a general and extensible\nsolution for automated AI research replication. Code will released at\nhttps://github.com/zjunlp/xKG.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.17795.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620b3bbb0668e435407c8d0a",
      "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
      "fullname": "Ningyu Zhang",
      "name": "Ningyu",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 30
    },
    "organization": {
      "_id": "67c1d682826160b28f778510",
      "name": "antgroup",
      "fullname": "Ant Group",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/7VcPHdLSGlged3ixK1dys.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.17803",
      "authors": [
        {
          "_id": "68f6e6e924c44893631117db",
          "name": "Zixin Yin",
          "hidden": false
        },
        {
          "_id": "68f6e6e924c44893631117dc",
          "name": "Ling-Hao Chen",
          "hidden": false
        },
        {
          "_id": "68f6e6e924c44893631117dd",
          "name": "Lionel Ni",
          "hidden": false
        },
        {
          "_id": "68f6e6e924c44893631117de",
          "name": "Xili Dai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-20T17:59:52.000Z",
      "submittedOnDailyAt": "2025-10-21T00:43:49.416Z",
      "title": "ConsistEdit: Highly Consistent and Precise Training-free Visual Editing",
      "submittedOnDailyBy": {
        "_id": "6503ccaf13d750b4604649e4",
        "avatarUrl": "/avatars/96bebff9284d61f37c83e7da6a7e9bac.svg",
        "isPro": false,
        "fullname": "Zixin Yin",
        "user": "zachary-yin",
        "type": "user"
      },
      "summary": "Recent advances in training-free attention control methods have enabled\nflexible and efficient text-guided editing capabilities for existing generation\nmodels. However, current approaches struggle to simultaneously deliver strong\nediting strength while preserving consistency with the source. This limitation\nbecomes particularly critical in multi-round and video editing, where visual\nerrors can accumulate over time. Moreover, most existing methods enforce global\nconsistency, which limits their ability to modify individual attributes such as\ntexture while preserving others, thereby hindering fine-grained editing.\nRecently, the architectural shift from U-Net to MM-DiT has brought significant\nimprovements in generative performance and introduced a novel mechanism for\nintegrating text and vision modalities. These advancements pave the way for\novercoming challenges that previous methods failed to resolve. Through an\nin-depth analysis of MM-DiT, we identify three key insights into its attention\nmechanisms. Building on these, we propose ConsistEdit, a novel attention\ncontrol method specifically tailored for MM-DiT. ConsistEdit incorporates\nvision-only attention control, mask-guided pre-attention fusion, and\ndifferentiated manipulation of the query, key, and value tokens to produce\nconsistent, prompt-aligned edits. Extensive experiments demonstrate that\nConsistEdit achieves state-of-the-art performance across a wide range of image\nand video editing tasks, including both structure-consistent and\nstructure-inconsistent scenarios. Unlike prior methods, it is the first\napproach to perform editing across all inference steps and attention layers\nwithout handcraft, significantly enhancing reliability and consistency, which\nenables robust multi-round and multi-region editing. Furthermore, it supports\nprogressive adjustment of structural consistency, enabling finer control.",
      "upvotes": 8,
      "discussionId": "68f6e6ea24c44893631117df",
      "projectPage": "https://zxyin.github.io/ConsistEdit",
      "githubRepo": "https://github.com/zxYin/ConsistEdit_Code",
      "ai_summary": "ConsistEdit, a novel attention control method for MM-DiT, enhances image and video editing by ensuring consistency and fine-grained control across all inference steps and attention layers.",
      "ai_keywords": [
        "attention control",
        "MM-DiT",
        "vision-only attention control",
        "mask-guided pre-attention fusion",
        "query",
        "key",
        "value tokens",
        "structure-consistent",
        "structure-inconsistent",
        "multi-round editing",
        "multi-region editing",
        "progressive adjustment"
      ],
      "githubStars": 14
    },
    "publishedAt": "2025-10-20T13:59:52.000Z",
    "title": "ConsistEdit: Highly Consistent and Precise Training-free Visual Editing",
    "summary": "Recent advances in training-free attention control methods have enabled\nflexible and efficient text-guided editing capabilities for existing generation\nmodels. However, current approaches struggle to simultaneously deliver strong\nediting strength while preserving consistency with the source. This limitation\nbecomes particularly critical in multi-round and video editing, where visual\nerrors can accumulate over time. Moreover, most existing methods enforce global\nconsistency, which limits their ability to modify individual attributes such as\ntexture while preserving others, thereby hindering fine-grained editing.\nRecently, the architectural shift from U-Net to MM-DiT has brought significant\nimprovements in generative performance and introduced a novel mechanism for\nintegrating text and vision modalities. These advancements pave the way for\novercoming challenges that previous methods failed to resolve. Through an\nin-depth analysis of MM-DiT, we identify three key insights into its attention\nmechanisms. Building on these, we propose ConsistEdit, a novel attention\ncontrol method specifically tailored for MM-DiT. ConsistEdit incorporates\nvision-only attention control, mask-guided pre-attention fusion, and\ndifferentiated manipulation of the query, key, and value tokens to produce\nconsistent, prompt-aligned edits. Extensive experiments demonstrate that\nConsistEdit achieves state-of-the-art performance across a wide range of image\nand video editing tasks, including both structure-consistent and\nstructure-inconsistent scenarios. Unlike prior methods, it is the first\napproach to perform editing across all inference steps and attention layers\nwithout handcraft, significantly enhancing reliability and consistency, which\nenables robust multi-round and multi-region editing. Furthermore, it supports\nprogressive adjustment of structural consistency, enabling finer control.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.17803.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6503ccaf13d750b4604649e4",
      "avatarUrl": "/avatars/96bebff9284d61f37c83e7da6a7e9bac.svg",
      "fullname": "Zixin Yin",
      "name": "zachary-yin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.17269",
      "authors": [
        {
          "_id": "68f6f4e524c44893631118d3",
          "name": "Luis Wiedmann",
          "hidden": false
        },
        {
          "_id": "68f6f4e524c44893631118d4",
          "name": "Orr Zohar",
          "hidden": false
        },
        {
          "_id": "68f6f4e524c44893631118d5",
          "name": "Amir Mahla",
          "hidden": false
        },
        {
          "_id": "68f6f4e524c44893631118d6",
          "name": "Xiaohan Wang",
          "hidden": false
        },
        {
          "_id": "68f6f4e524c44893631118d7",
          "name": "Rui Li",
          "hidden": false
        },
        {
          "_id": "68f6f4e524c44893631118d8",
          "name": "Thibaud Frere",
          "hidden": false
        },
        {
          "_id": "68f6f4e524c44893631118d9",
          "name": "Leandro von Werra",
          "hidden": false
        },
        {
          "_id": "68f6f4e524c44893631118da",
          "name": "Aritra Roy Gosthipaty",
          "hidden": false
        },
        {
          "_id": "68f6f4e524c44893631118db",
          "name": "Andrs Marafioti",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-20T07:54:46.000Z",
      "submittedOnDailyAt": "2025-10-21T01:20:29.268Z",
      "title": "FineVision: Open Data Is All You Need",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "The advancement of vision-language models (VLMs) is hampered by a fragmented\nlandscape of inconsistent and contaminated public datasets. We introduce\nFineVision, a meticulously collected, curated, and unified corpus of 24 million\nsamples - the largest open resource of its kind. We unify more than 200 sources\ninto 185 subsets via a semi-automated, human-in-the-loop pipeline: automation\nperforms bulk ingestion and schema mapping, while reviewers audit mappings and\nspot-check outputs to verify faithful consumption of annotations, appropriate\nformatting and diversity, and safety; issues trigger targeted fixes and\nre-runs. The workflow further applies rigorous de-duplication within and across\nsources and decontamination against 66 public benchmarks. FineVision also\nencompasses agentic/GUI tasks with a unified action space; reviewers validate\nschemas and inspect a sample of trajectories to confirm executable fidelity.\nModels trained on FineVision consistently outperform those trained on existing\nopen mixtures across a broad evaluation suite, underscoring the benefits of\nscale, data hygiene, and balanced automation with human oversight. We release\nthe corpus and curation tools to accelerate data-centric VLM research.",
      "upvotes": 8,
      "discussionId": "68f6f4e524c44893631118dc",
      "projectPage": "https://huggingface.co/spaces/HuggingFaceM4/FineVision",
      "ai_summary": "FineVision, a large-scale and curated dataset, enhances vision-language models through rigorous data collection, de-duplication, and human oversight, leading to improved performance.",
      "ai_keywords": [
        "vision-language models",
        "FineVision",
        "semi-automated pipeline",
        "human-in-the-loop",
        "schema mapping",
        "de-duplication",
        "decontamination",
        "agentic/GUI tasks",
        "unified action space",
        "data-centric research"
      ],
      "organization": {
        "_id": "5e67bd5b1009063689407478",
        "name": "huggingface",
        "fullname": "Hugging Face",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583856921041-5dd96eb166059660ed1ee413.png"
      }
    },
    "publishedAt": "2025-10-20T03:54:46.000Z",
    "title": "FineVision: Open Data Is All You Need",
    "summary": "The advancement of vision-language models (VLMs) is hampered by a fragmented\nlandscape of inconsistent and contaminated public datasets. We introduce\nFineVision, a meticulously collected, curated, and unified corpus of 24 million\nsamples - the largest open resource of its kind. We unify more than 200 sources\ninto 185 subsets via a semi-automated, human-in-the-loop pipeline: automation\nperforms bulk ingestion and schema mapping, while reviewers audit mappings and\nspot-check outputs to verify faithful consumption of annotations, appropriate\nformatting and diversity, and safety; issues trigger targeted fixes and\nre-runs. The workflow further applies rigorous de-duplication within and across\nsources and decontamination against 66 public benchmarks. FineVision also\nencompasses agentic/GUI tasks with a unified action space; reviewers validate\nschemas and inspect a sample of trajectories to confirm executable fidelity.\nModels trained on FineVision consistently outperform those trained on existing\nopen mixtures across a broad evaluation suite, underscoring the benefits of\nscale, data hygiene, and balanced automation with human oversight. We release\nthe corpus and curation tools to accelerate data-centric VLM research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.17269.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 133
    },
    "organization": {
      "_id": "5e67bd5b1009063689407478",
      "name": "huggingface",
      "fullname": "Hugging Face",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583856921041-5dd96eb166059660ed1ee413.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.17498",
      "authors": [
        {
          "_id": "68f6f00024c448936311187a",
          "name": "Zihan Liu",
          "hidden": false
        },
        {
          "_id": "68f6f00024c448936311187b",
          "name": "Shun Zheng",
          "hidden": false
        },
        {
          "_id": "68f6f00024c448936311187c",
          "name": "Xumeng Wen",
          "hidden": false
        },
        {
          "_id": "68f6f00024c448936311187d",
          "name": "Yang Wang",
          "hidden": false
        },
        {
          "_id": "68f6f00024c448936311187e",
          "name": "Jiang Bian",
          "hidden": false
        },
        {
          "_id": "68f6f00024c448936311187f",
          "name": "Mao Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-20T12:51:42.000Z",
      "submittedOnDailyAt": "2025-10-21T01:20:30.132Z",
      "title": "Deep Self-Evolving Reasoning",
      "submittedOnDailyBy": {
        "_id": "64a7a2bad001860e0c34f7f2",
        "avatarUrl": "/avatars/927c71448c3f8a65532b387196a82f61.svg",
        "isPro": false,
        "fullname": "Shun Zheng",
        "user": "shun-zheng",
        "type": "user"
      },
      "summary": "Long-form chain-of-thought reasoning has become a cornerstone of advanced\nreasoning in large language models. While recent verification-refinement\nframeworks have enabled proprietary models to solve Olympiad-level problems,\ntheir effectiveness hinges on strong, reliable verification and correction\ncapabilities, which remain fragile in open-weight, smaller-scale models. This\nwork demonstrates that even with weak verification and refinement capabilities\non hard tasks, the reasoning limits of such models can be substantially\nextended through a probabilistic paradigm we call Deep Self-Evolving Reasoning\n(DSER). We conceptualize iterative reasoning as a Markov chain, where each step\nrepresents a stochastic transition in the solution space. The key insight is\nthat convergence to a correct solution is guaranteed as long as the probability\nof improvement marginally exceeds that of degradation. By running multiple\nlong-horizon, self-evolving processes in parallel, DSER amplifies these small\npositive tendencies, enabling the model to asymptotically approach correct\nanswers. Empirically, we apply DSER to the DeepSeek-R1-0528-Qwen3-8B model. On\nthe challenging AIME 2024-2025 benchmark, DSER solves 5 out of 9 previously\nunsolvable problems and boosts overall performance, enabling this compact model\nto surpass the single-turn accuracy of its 600B-parameter teacher through\nmajority voting. Beyond its immediate utility for test-time scaling, the DSER\nframework serves to diagnose the fundamental limitations of current open-weight\nreasoners. By clearly delineating their shortcomings in self-verification,\nrefinement, and stability, our findings establish a clear research agenda for\ndeveloping next-generation models with powerful, intrinsic self-evolving\ncapabilities.",
      "upvotes": 6,
      "discussionId": "68f6f00124c4489363111880",
      "ai_summary": "Deep Self-Evolving Reasoning (DSER) extends the reasoning capabilities of smaller models by iteratively improving solutions through a probabilistic Markov chain, enabling them to solve previously unsolvable problems and surpass larger models in accuracy.",
      "ai_keywords": [
        "Deep Self-Evolving Reasoning",
        "DSER",
        "Markov chain",
        "iterative reasoning",
        "probabilistic paradigm",
        "self-evolving processes",
        "AIME 2024-2025 benchmark",
        "majority voting",
        "self-verification",
        "refinement",
        "stability"
      ],
      "organization": {
        "_id": "5e6485f787403103f9f1055e",
        "name": "microsoft",
        "fullname": "Microsoft",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"
      }
    },
    "publishedAt": "2025-10-20T08:51:42.000Z",
    "title": "Deep Self-Evolving Reasoning",
    "summary": "Long-form chain-of-thought reasoning has become a cornerstone of advanced\nreasoning in large language models. While recent verification-refinement\nframeworks have enabled proprietary models to solve Olympiad-level problems,\ntheir effectiveness hinges on strong, reliable verification and correction\ncapabilities, which remain fragile in open-weight, smaller-scale models. This\nwork demonstrates that even with weak verification and refinement capabilities\non hard tasks, the reasoning limits of such models can be substantially\nextended through a probabilistic paradigm we call Deep Self-Evolving Reasoning\n(DSER). We conceptualize iterative reasoning as a Markov chain, where each step\nrepresents a stochastic transition in the solution space. The key insight is\nthat convergence to a correct solution is guaranteed as long as the probability\nof improvement marginally exceeds that of degradation. By running multiple\nlong-horizon, self-evolving processes in parallel, DSER amplifies these small\npositive tendencies, enabling the model to asymptotically approach correct\nanswers. Empirically, we apply DSER to the DeepSeek-R1-0528-Qwen3-8B model. On\nthe challenging AIME 2024-2025 benchmark, DSER solves 5 out of 9 previously\nunsolvable problems and boosts overall performance, enabling this compact model\nto surpass the single-turn accuracy of its 600B-parameter teacher through\nmajority voting. Beyond its immediate utility for test-time scaling, the DSER\nframework serves to diagnose the fundamental limitations of current open-weight\nreasoners. By clearly delineating their shortcomings in self-verification,\nrefinement, and stability, our findings establish a clear research agenda for\ndeveloping next-generation models with powerful, intrinsic self-evolving\ncapabilities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.17498.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a7a2bad001860e0c34f7f2",
      "avatarUrl": "/avatars/927c71448c3f8a65532b387196a82f61.svg",
      "fullname": "Shun Zheng",
      "name": "shun-zheng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "organization": {
      "_id": "5e6485f787403103f9f1055e",
      "name": "microsoft",
      "fullname": "Microsoft",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.16720",
      "authors": [
        {
          "_id": "68f6ebbc24c4489363111823",
          "name": "Jitao Sang",
          "hidden": false
        },
        {
          "_id": "68f6ebbc24c4489363111824",
          "name": "Jinlin Xiao",
          "hidden": false
        },
        {
          "_id": "68f6ebbc24c4489363111825",
          "name": "Jiarun Han",
          "hidden": false
        },
        {
          "_id": "68f6ebbc24c4489363111826",
          "name": "Jilin Chen",
          "hidden": false
        },
        {
          "_id": "68f6ebbc24c4489363111827",
          "name": "Xiaoyi Chen",
          "hidden": false
        },
        {
          "_id": "68f6ebbc24c4489363111828",
          "name": "Shuyu Wei",
          "hidden": false
        },
        {
          "_id": "68f6ebbc24c4489363111829",
          "name": "Yongjie Sun",
          "hidden": false
        },
        {
          "_id": "68f6ebbc24c448936311182a",
          "name": "Yuhang Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-19T05:23:43.000Z",
      "submittedOnDailyAt": "2025-10-21T01:25:59.196Z",
      "title": "Beyond Pipelines: A Survey of the Paradigm Shift toward Model-Native\n  Agentic AI",
      "submittedOnDailyBy": {
        "_id": "6494457c6339264dd78bcb95",
        "avatarUrl": "/avatars/d87842251f1a43f50cc827f0e2a995ee.svg",
        "isPro": false,
        "fullname": "sdzy",
        "user": "sdzy",
        "type": "user"
      },
      "summary": "The rapid evolution of agentic AI marks a new phase in artificial\nintelligence, where Large Language Models (LLMs) no longer merely respond but\nact, reason, and adapt. This survey traces the paradigm shift in building\nagentic AI: from Pipeline-based systems, where planning, tool use, and memory\nare orchestrated by external logic, to the emerging Model-native paradigm,\nwhere these capabilities are internalized within the model's parameters. We\nfirst position Reinforcement Learning (RL) as the algorithmic engine enabling\nthis paradigm shift. By reframing learning from imitating static data to\noutcome-driven exploration, RL underpins a unified solution of LLM + RL + Task\nacross language, vision and embodied domains. Building on this, the survey\nsystematically reviews how each capability -- Planning, Tool use, and Memory --\nhas evolved from externally scripted modules to end-to-end learned behaviors.\nFurthermore, it examines how this paradigm shift has reshaped major agent\napplications, specifically the Deep Research agent emphasizing long-horizon\nreasoning and the GUI agent emphasizing embodied interaction. We conclude by\ndiscussing the continued internalization of agentic capabilities like\nMulti-agent collaboration and Reflection, alongside the evolving roles of the\nsystem and model layers in future agentic AI. Together, these developments\noutline a coherent trajectory toward model-native agentic AI as an integrated\nlearning and interaction framework, marking the transition from constructing\nsystems that apply intelligence to developing models that grow intelligence\nthrough experience.",
      "upvotes": 6,
      "discussionId": "68f6ebbc24c448936311182b",
      "projectPage": "https://github.com/ADaM-BJTU/model-native-agentic-ai",
      "githubRepo": "https://github.com/ADaM-BJTU/model-native-agentic-ai",
      "ai_summary": "The survey outlines the shift from pipeline-based to model-native agentic AI, emphasizing the role of reinforcement learning in integrating planning, tool use, and memory within large language models across various domains.",
      "ai_keywords": [
        "Reinforcement Learning",
        "Model-native paradigm",
        "Large Language Models",
        "Planning",
        "Tool use",
        "Memory",
        "Deep Research agent",
        "GUI agent",
        "Multi-agent collaboration",
        "Reflection"
      ],
      "githubStars": 21,
      "organization": {
        "_id": "647c8655396de7684de79a2e",
        "name": "BJTUniversity",
        "fullname": "Beijing JiaoTong University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6469b882af67f5c95414775c/NT0fZaZxB_1Cf85zeFYb3.png"
      }
    },
    "publishedAt": "2025-10-19T01:23:43.000Z",
    "title": "Beyond Pipelines: A Survey of the Paradigm Shift toward Model-Native\n  Agentic AI",
    "summary": "The rapid evolution of agentic AI marks a new phase in artificial\nintelligence, where Large Language Models (LLMs) no longer merely respond but\nact, reason, and adapt. This survey traces the paradigm shift in building\nagentic AI: from Pipeline-based systems, where planning, tool use, and memory\nare orchestrated by external logic, to the emerging Model-native paradigm,\nwhere these capabilities are internalized within the model's parameters. We\nfirst position Reinforcement Learning (RL) as the algorithmic engine enabling\nthis paradigm shift. By reframing learning from imitating static data to\noutcome-driven exploration, RL underpins a unified solution of LLM + RL + Task\nacross language, vision and embodied domains. Building on this, the survey\nsystematically reviews how each capability -- Planning, Tool use, and Memory --\nhas evolved from externally scripted modules to end-to-end learned behaviors.\nFurthermore, it examines how this paradigm shift has reshaped major agent\napplications, specifically the Deep Research agent emphasizing long-horizon\nreasoning and the GUI agent emphasizing embodied interaction. We conclude by\ndiscussing the continued internalization of agentic capabilities like\nMulti-agent collaboration and Reflection, alongside the evolving roles of the\nsystem and model layers in future agentic AI. Together, these developments\noutline a coherent trajectory toward model-native agentic AI as an integrated\nlearning and interaction framework, marking the transition from constructing\nsystems that apply intelligence to developing models that grow intelligence\nthrough experience.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.16720.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6494457c6339264dd78bcb95",
      "avatarUrl": "/avatars/d87842251f1a43f50cc827f0e2a995ee.svg",
      "fullname": "sdzy",
      "name": "sdzy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "organization": {
      "_id": "647c8655396de7684de79a2e",
      "name": "BJTUniversity",
      "fullname": "Beijing JiaoTong University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6469b882af67f5c95414775c/NT0fZaZxB_1Cf85zeFYb3.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.16751",
      "authors": [
        {
          "_id": "68f6ea4a24c4489363111810",
          "name": "Erik Riise",
          "hidden": false
        },
        {
          "_id": "68f6ea4a24c4489363111811",
          "name": "Mehmet Onurcan Kaya",
          "hidden": false
        },
        {
          "_id": "68f6ea4a24c4489363111812",
          "name": "Dim P. Papadopoulos",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-19T08:28:06.000Z",
      "submittedOnDailyAt": "2025-10-21T00:35:50.068Z",
      "title": "Visual Autoregressive Models Beat Diffusion Models on Inference Time\n  Scaling",
      "submittedOnDailyBy": {
        "_id": "63be9021da08ed0544f36c38",
        "avatarUrl": "/avatars/42511d8b1d3a3ef99bc154c98b72dfba.svg",
        "isPro": false,
        "fullname": "onurcan",
        "user": "monurcan",
        "type": "user"
      },
      "summary": "While inference-time scaling through search has revolutionized Large Language\nModels, translating these gains to image generation has proven difficult.\nRecent attempts to apply search strategies to continuous diffusion models show\nlimited benefits, with simple random sampling often performing best. We\ndemonstrate that the discrete, sequential nature of visual autoregressive\nmodels enables effective search for image generation. We show that beam search\nsubstantially improves text-to-image generation, enabling a 2B parameter\nautoregressive model to outperform a 12B parameter diffusion model across\nbenchmarks. Systematic ablations show that this advantage comes from the\ndiscrete token space, which allows early pruning and computational reuse, and\nour verifier analysis highlights trade-offs between speed and reasoning\ncapability. These findings suggest that model architecture, not just scale, is\ncritical for inference-time optimization in visual generation.",
      "upvotes": 4,
      "discussionId": "68f6ea4b24c4489363111813",
      "ai_summary": "Beam search in discrete visual autoregressive models enhances text-to-image generation more effectively than search in continuous diffusion models, highlighting architecture's importance over scale.",
      "ai_keywords": [
        "visual autoregressive models",
        "beam search",
        "diffusion models",
        "discrete token space",
        "early pruning",
        "computational reuse",
        "verifier analysis"
      ]
    },
    "publishedAt": "2025-10-19T04:28:06.000Z",
    "title": "Visual Autoregressive Models Beat Diffusion Models on Inference Time\n  Scaling",
    "summary": "While inference-time scaling through search has revolutionized Large Language\nModels, translating these gains to image generation has proven difficult.\nRecent attempts to apply search strategies to continuous diffusion models show\nlimited benefits, with simple random sampling often performing best. We\ndemonstrate that the discrete, sequential nature of visual autoregressive\nmodels enables effective search for image generation. We show that beam search\nsubstantially improves text-to-image generation, enabling a 2B parameter\nautoregressive model to outperform a 12B parameter diffusion model across\nbenchmarks. Systematic ablations show that this advantage comes from the\ndiscrete token space, which allows early pruning and computational reuse, and\nour verifier analysis highlights trade-offs between speed and reasoning\ncapability. These findings suggest that model architecture, not just scale, is\ncritical for inference-time optimization in visual generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.16751.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63be9021da08ed0544f36c38",
      "avatarUrl": "/avatars/42511d8b1d3a3ef99bc154c98b72dfba.svg",
      "fullname": "onurcan",
      "name": "monurcan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.15021",
      "authors": [
        {
          "_id": "68f58fc18589920bf4d32065",
          "name": "Jiaxin Ge",
          "hidden": false
        },
        {
          "_id": "68f58fc18589920bf4d32066",
          "name": "Grace Luo",
          "hidden": false
        },
        {
          "_id": "68f58fc18589920bf4d32067",
          "name": "Heekyung Lee",
          "hidden": false
        },
        {
          "_id": "68f58fc18589920bf4d32068",
          "name": "Nishant Malpani",
          "hidden": false
        },
        {
          "_id": "68f58fc18589920bf4d32069",
          "name": "Long Lian",
          "hidden": false
        },
        {
          "_id": "68f58fc18589920bf4d3206a",
          "name": "XuDong Wang",
          "hidden": false
        },
        {
          "_id": "68f58fc18589920bf4d3206b",
          "name": "Aleksander Holynski",
          "hidden": false
        },
        {
          "_id": "68f58fc18589920bf4d3206c",
          "name": "Trevor Darrell",
          "hidden": false
        },
        {
          "_id": "68f58fc18589920bf4d3206d",
          "name": "Sewon Min",
          "hidden": false
        },
        {
          "_id": "68f58fc18589920bf4d3206e",
          "name": "David M. Chan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-16T17:59:30.000Z",
      "submittedOnDailyAt": "2025-10-21T03:34:49.939Z",
      "title": "Constantly Improving Image Models Need Constantly Improving Benchmarks",
      "submittedOnDailyBy": {
        "_id": "64bdc1e8d05a97d722b2c1f6",
        "avatarUrl": "/avatars/b04cbe17e7cd0a1f5bf0e8e008640a1d.svg",
        "isPro": false,
        "fullname": "Jiaxin Ge",
        "user": "para-lost",
        "type": "user"
      },
      "summary": "Recent advances in image generation, often driven by proprietary systems like\nGPT-4o Image Gen, regularly introduce new capabilities that reshape how users\ninteract with these models. Existing benchmarks often lag behind and fail to\ncapture these emerging use cases, leaving a gap between community perceptions\nof progress and formal evaluation. To address this, we present ECHO, a\nframework for constructing benchmarks directly from real-world evidence of\nmodel use: social media posts that showcase novel prompts and qualitative user\njudgments. Applying this framework to GPT-4o Image Gen, we construct a dataset\nof over 31,000 prompts curated from such posts. Our analysis shows that ECHO\n(1) discovers creative and complex tasks absent from existing benchmarks, such\nas re-rendering product labels across languages or generating receipts with\nspecified totals, (2) more clearly distinguishes state-of-the-art models from\nalternatives, and (3) surfaces community feedback that we use to inform the\ndesign of metrics for model quality (e.g., measuring observed shifts in color,\nidentity, and structure). Our website is at https://echo-bench.github.io.",
      "upvotes": 4,
      "discussionId": "68f58fc28589920bf4d3206f",
      "projectPage": "https://echo-bench.github.io/",
      "githubRepo": "https://github.com/para-lost/ECHO",
      "ai_summary": "ECHO is a framework that constructs benchmarks for image generation models using real-world social media data, uncovering complex tasks and improving model evaluation.",
      "ai_keywords": [
        "image generation",
        "GPT-4o Image Gen",
        "benchmarks",
        "social media posts",
        "qualitative user judgments",
        "dataset",
        "re-rendering",
        "product labels",
        "generating receipts",
        "model quality",
        "metrics",
        "color shifts",
        "identity shifts",
        "structure shifts"
      ],
      "githubStars": 6,
      "organization": {
        "_id": "66b1baeff10262fc4fa61961",
        "name": "UCBerkeley",
        "fullname": "University of California, Berkeley",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f425c3a096536aeab42dea/bxNKEkprdm5JI1wkjmNAL.png"
      }
    },
    "publishedAt": "2025-10-16T13:59:30.000Z",
    "title": "Constantly Improving Image Models Need Constantly Improving Benchmarks",
    "summary": "Recent advances in image generation, often driven by proprietary systems like\nGPT-4o Image Gen, regularly introduce new capabilities that reshape how users\ninteract with these models. Existing benchmarks often lag behind and fail to\ncapture these emerging use cases, leaving a gap between community perceptions\nof progress and formal evaluation. To address this, we present ECHO, a\nframework for constructing benchmarks directly from real-world evidence of\nmodel use: social media posts that showcase novel prompts and qualitative user\njudgments. Applying this framework to GPT-4o Image Gen, we construct a dataset\nof over 31,000 prompts curated from such posts. Our analysis shows that ECHO\n(1) discovers creative and complex tasks absent from existing benchmarks, such\nas re-rendering product labels across languages or generating receipts with\nspecified totals, (2) more clearly distinguishes state-of-the-art models from\nalternatives, and (3) surfaces community feedback that we use to inform the\ndesign of metrics for model quality (e.g., measuring observed shifts in color,\nidentity, and structure). Our website is at https://echo-bench.github.io.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.15021.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64bdc1e8d05a97d722b2c1f6",
      "avatarUrl": "/avatars/b04cbe17e7cd0a1f5bf0e8e008640a1d.svg",
      "fullname": "Jiaxin Ge",
      "name": "para-lost",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "66b1baeff10262fc4fa61961",
      "name": "UCBerkeley",
      "fullname": "University of California, Berkeley",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f425c3a096536aeab42dea/bxNKEkprdm5JI1wkjmNAL.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.17797",
      "authors": [
        {
          "_id": "68f6f37a24c44893631118b7",
          "name": "Akshara Prabhakar",
          "hidden": false
        },
        {
          "_id": "68f6f37a24c44893631118b8",
          "name": "Roshan Ram",
          "hidden": false
        },
        {
          "_id": "68f6f37a24c44893631118b9",
          "name": "Zixiang Chen",
          "hidden": false
        },
        {
          "_id": "68f6f37a24c44893631118ba",
          "name": "Silvio Savarese",
          "hidden": false
        },
        {
          "_id": "68f6f37a24c44893631118bb",
          "name": "Frank Wang",
          "hidden": false
        },
        {
          "_id": "68f6f37a24c44893631118bc",
          "name": "Caiming Xiong",
          "hidden": false
        },
        {
          "_id": "68f6f37a24c44893631118bd",
          "name": "Huan Wang",
          "hidden": false
        },
        {
          "_id": "68f6f37a24c44893631118be",
          "name": "Weiran Yao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-20T17:55:11.000Z",
      "submittedOnDailyAt": "2025-10-21T01:14:26.499Z",
      "title": "Enterprise Deep Research: Steerable Multi-Agent Deep Research for\n  Enterprise Analytics",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "As information grows exponentially, enterprises face increasing pressure to\ntransform unstructured data into coherent, actionable insights. While\nautonomous agents show promise, they often struggle with domain-specific\nnuances, intent alignment, and enterprise integration. We present Enterprise\nDeep Research (EDR), a multi-agent system that integrates (1) a Master Planning\nAgent for adaptive query decomposition, (2) four specialized search agents\n(General, Academic, GitHub, LinkedIn), (3) an extensible MCP-based tool\necosystem supporting NL2SQL, file analysis, and enterprise workflows, (4) a\nVisualization Agent for data-driven insights, and (5) a reflection mechanism\nthat detects knowledge gaps and updates research direction with optional\nhuman-in-the-loop steering guidance. These components enable automated report\ngeneration, real-time streaming, and seamless enterprise deployment, as\nvalidated on internal datasets. On open-ended benchmarks including DeepResearch\nBench and DeepConsult, EDR outperforms state-of-the-art agentic systems without\nany human steering. We release the EDR framework and benchmark trajectories to\nadvance research on multi-agent reasoning applications.\n  Code at https://github.com/SalesforceAIResearch/enterprise-deep-research and\nDataset at https://huggingface.co/datasets/Salesforce/EDR-200",
      "upvotes": 3,
      "discussionId": "68f6f37b24c44893631118bf",
      "githubRepo": "https://github.com/SalesforceAIResearch/enterprise-deep-research",
      "ai_summary": "Enterprise Deep Research (EDR) is a multi-agent system that automates report generation and real-time data analysis by integrating specialized agents and tools, outperforming existing agentic systems on open benchmarks.",
      "ai_keywords": [
        "multi-agent system",
        "Master Planning Agent",
        "adaptive query decomposition",
        "specialized search agents",
        "General",
        "Academic",
        "GitHub",
        "LinkedIn",
        "MCP-based tool ecosystem",
        "NL2SQL",
        "file analysis",
        "enterprise workflows",
        "Visualization Agent",
        "data-driven insights",
        "reflection mechanism",
        "knowledge gaps",
        "human-in-the-loop steering guidance",
        "automated report generation",
        "real-time streaming",
        "seamless enterprise deployment",
        "DeepResearch Bench",
        "DeepConsult"
      ],
      "githubStars": 4,
      "organization": {
        "_id": "5f6d64475e78cc6b0ed31e4c",
        "name": "Salesforce",
        "fullname": "Salesforce",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1602756670970-noauth.jpeg"
      }
    },
    "publishedAt": "2025-10-20T13:55:11.000Z",
    "title": "Enterprise Deep Research: Steerable Multi-Agent Deep Research for\n  Enterprise Analytics",
    "summary": "As information grows exponentially, enterprises face increasing pressure to\ntransform unstructured data into coherent, actionable insights. While\nautonomous agents show promise, they often struggle with domain-specific\nnuances, intent alignment, and enterprise integration. We present Enterprise\nDeep Research (EDR), a multi-agent system that integrates (1) a Master Planning\nAgent for adaptive query decomposition, (2) four specialized search agents\n(General, Academic, GitHub, LinkedIn), (3) an extensible MCP-based tool\necosystem supporting NL2SQL, file analysis, and enterprise workflows, (4) a\nVisualization Agent for data-driven insights, and (5) a reflection mechanism\nthat detects knowledge gaps and updates research direction with optional\nhuman-in-the-loop steering guidance. These components enable automated report\ngeneration, real-time streaming, and seamless enterprise deployment, as\nvalidated on internal datasets. On open-ended benchmarks including DeepResearch\nBench and DeepConsult, EDR outperforms state-of-the-art agentic systems without\nany human steering. We release the EDR framework and benchmark trajectories to\nadvance research on multi-agent reasoning applications.\n  Code at https://github.com/SalesforceAIResearch/enterprise-deep-research and\nDataset at https://huggingface.co/datasets/Salesforce/EDR-200",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.17797.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 133
    },
    "organization": {
      "_id": "5f6d64475e78cc6b0ed31e4c",
      "name": "Salesforce",
      "fullname": "Salesforce",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1602756670970-noauth.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.17790",
      "authors": [
        {
          "_id": "68f6ff6224c4489363111976",
          "name": "Yuhao Yang",
          "hidden": false
        },
        {
          "_id": "68f6ff6224c4489363111977",
          "name": "Zhen Yang",
          "hidden": false
        },
        {
          "_id": "68f6ff6224c4489363111978",
          "name": "Zi-Yi Dou",
          "hidden": false
        },
        {
          "_id": "68f6ff6224c4489363111979",
          "name": "Anh Nguyen",
          "hidden": false
        },
        {
          "_id": "68f6ff6224c448936311197a",
          "name": "Keen You",
          "hidden": false
        },
        {
          "_id": "68f6ff6224c448936311197b",
          "name": "Omar Attia",
          "hidden": false
        },
        {
          "_id": "68f6ff6224c448936311197c",
          "name": "Andrew Szot",
          "hidden": false
        },
        {
          "_id": "68f6ff6224c448936311197d",
          "name": "Michael Feng",
          "hidden": false
        },
        {
          "_id": "68f6ff6224c448936311197e",
          "name": "Ram Ramrakhya",
          "hidden": false
        },
        {
          "_id": "68f6ff6224c448936311197f",
          "name": "Alexander Toshev",
          "hidden": false
        },
        {
          "_id": "68f6ff6224c4489363111980",
          "name": "Chao Huang",
          "hidden": false
        },
        {
          "_id": "68f6ff6224c4489363111981",
          "name": "Yinfei Yang",
          "hidden": false
        },
        {
          "_id": "68f6ff6224c4489363111982",
          "name": "Zhe Gan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-20T17:48:26.000Z",
      "submittedOnDailyAt": "2025-10-21T02:05:12.480Z",
      "title": "UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Multimodal agents for computer use rely exclusively on primitive actions\n(click, type, scroll) that require accurate visual grounding and lengthy\nexecution chains, leading to cascading failures and performance bottlenecks.\nWhile other agents leverage rich programmatic interfaces (APIs, MCP servers,\ntools), computer-use agents (CUAs) remain isolated from these capabilities. We\npresent UltraCUA, a foundation model that bridges this gap through hybrid\naction -- seamlessly integrating GUI primitives with high-level programmatic\ntool calls. To achieve this, our approach comprises four key components: (1) an\nautomated pipeline that scales programmatic tools from software documentation,\nopen-source repositories, and code generation; (2) a synthetic data engine\nproducing over 17,000 verifiable tasks spanning real-world computer-use\nscenarios; (3) a large-scale high-quality hybrid action trajectory collection\nwith both low-level GUI actions and high-level programmatic tool calls; and (4)\na two-stage training pipeline combining supervised fine-tuning with online\nreinforcement learning, enabling strategic alternation between low-level and\nhigh-level actions. Experiments with our 7B and 32B models demonstrate\nsubstantial improvements over state-of-the-art agents. On OSWorld, UltraCUA\nmodels achieve an average 22% relative improvement over base models, while\nbeing 11% faster in terms of steps. Out-of-domain evaluation on\nWindowsAgentArena shows our model reaches 21.7% success rate, outperforming\nbaselines trained on Windows data. The hybrid action mechanism proves critical,\nreducing error propagation while maintaining execution efficiency.",
      "upvotes": 3,
      "discussionId": "68f6ff6224c4489363111983",
      "ai_summary": "UltraCUA integrates GUI actions with programmatic tools to improve computer-use agent performance and efficiency.",
      "ai_keywords": [
        "GUI primitives",
        "programmatic interfaces",
        "APIs",
        "MCP servers",
        "tools",
        "foundation model",
        "hybrid action",
        "automated pipeline",
        "synthetic data engine",
        "high-quality hybrid action trajectory",
        "supervised fine-tuning",
        "online reinforcement learning",
        "OSWorld",
        "WindowsAgentArena"
      ],
      "organization": {
        "_id": "628cbd99ef14f971b69948ab",
        "name": "apple",
        "fullname": "Apple",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
      }
    },
    "publishedAt": "2025-10-20T13:48:26.000Z",
    "title": "UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action",
    "summary": "Multimodal agents for computer use rely exclusively on primitive actions\n(click, type, scroll) that require accurate visual grounding and lengthy\nexecution chains, leading to cascading failures and performance bottlenecks.\nWhile other agents leverage rich programmatic interfaces (APIs, MCP servers,\ntools), computer-use agents (CUAs) remain isolated from these capabilities. We\npresent UltraCUA, a foundation model that bridges this gap through hybrid\naction -- seamlessly integrating GUI primitives with high-level programmatic\ntool calls. To achieve this, our approach comprises four key components: (1) an\nautomated pipeline that scales programmatic tools from software documentation,\nopen-source repositories, and code generation; (2) a synthetic data engine\nproducing over 17,000 verifiable tasks spanning real-world computer-use\nscenarios; (3) a large-scale high-quality hybrid action trajectory collection\nwith both low-level GUI actions and high-level programmatic tool calls; and (4)\na two-stage training pipeline combining supervised fine-tuning with online\nreinforcement learning, enabling strategic alternation between low-level and\nhigh-level actions. Experiments with our 7B and 32B models demonstrate\nsubstantial improvements over state-of-the-art agents. On OSWorld, UltraCUA\nmodels achieve an average 22% relative improvement over base models, while\nbeing 11% faster in terms of steps. Out-of-domain evaluation on\nWindowsAgentArena shows our model reaches 21.7% success rate, outperforming\nbaselines trained on Windows data. The hybrid action mechanism proves critical,\nreducing error propagation while maintaining execution efficiency.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.17790.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 133
    },
    "organization": {
      "_id": "628cbd99ef14f971b69948ab",
      "name": "apple",
      "fullname": "Apple",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.16259",
      "authors": [
        {
          "_id": "68f6f3bf24c44893631118c1",
          "name": "Zhehao Zhang",
          "hidden": false
        },
        {
          "_id": "68f6f3bf24c44893631118c2",
          "name": "Weijie Xu",
          "hidden": false
        },
        {
          "_id": "68f6f3bf24c44893631118c3",
          "name": "Shixian Cui",
          "hidden": false
        },
        {
          "_id": "68f6f3bf24c44893631118c4",
          "name": "Chandan K. Reddy",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63e3f57754f51ea342ce26be/W4rEeLd9b1Wam1JngHj1-.png",
        "https://cdn-uploads.huggingface.co/production/uploads/63e3f57754f51ea342ce26be/DjQP4ztyFID4WsfHl7QNn.png"
      ],
      "publishedAt": "2025-10-17T23:16:34.000Z",
      "submittedOnDailyAt": "2025-10-21T01:26:24.565Z",
      "title": "Distractor Injection Attacks on Large Reasoning Models: Characterization\n  and Defense",
      "submittedOnDailyBy": {
        "_id": "63e3f57754f51ea342ce26be",
        "avatarUrl": "/avatars/df9d52c376bed6868d341bb006bec212.svg",
        "isPro": false,
        "fullname": "Weijie Xu",
        "user": "xwjzds",
        "type": "user"
      },
      "summary": "Recent advances in large reasoning models (LRMs) have enabled remarkable\nperformance on complex tasks such as mathematics and coding by generating long\nChain-of-Thought (CoT) traces. In this paper, we identify and systematically\nanalyze a critical vulnerability we term reasoning distraction, where LRMs are\ndiverted from their primary objective by irrelevant yet complex tasks\nmaliciously embedded in the prompt. Through a comprehensive study across\ndiverse models and benchmarks, we show that even state-of-the-art LRMs are\nhighly susceptible, with injected distractors reducing task accuracy by up to\n60%. We further reveal that certain alignment techniques can amplify this\nweakness and that models may exhibit covert compliance, following hidden\nadversarial instructions in reasoning while concealing them in the final\noutput. To mitigate these risks, we propose a training-based defense that\ncombines Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on\nsynthetic adversarial data, improving robustness by over 50 points on\nchallenging distractor attacks. Our findings establish reasoning distraction as\na distinct and urgent threat to LRM reliability and provide a practical step\ntoward safer and more trustworthy reasoning systems.",
      "upvotes": 3,
      "discussionId": "68f6f3bf24c44893631118c5",
      "ai_summary": "Large reasoning models are vulnerable to reasoning distraction, where irrelevant tasks embedded in prompts reduce accuracy, and a combined SFT and RL defense improves robustness.",
      "ai_keywords": [
        "large reasoning models",
        "Chain-of-Thought",
        "reasoning distraction",
        "Supervised Fine-Tuning",
        "Reinforcement Learning",
        "adversarial data"
      ],
      "organization": {
        "_id": "615250af54cfe5db853ec856",
        "name": "AmazonScience",
        "fullname": "Amazon Science",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1632784986195-61393f6f905b1938233881e2.png"
      }
    },
    "publishedAt": "2025-10-17T19:16:34.000Z",
    "title": "Distractor Injection Attacks on Large Reasoning Models: Characterization\n  and Defense",
    "summary": "Recent advances in large reasoning models (LRMs) have enabled remarkable\nperformance on complex tasks such as mathematics and coding by generating long\nChain-of-Thought (CoT) traces. In this paper, we identify and systematically\nanalyze a critical vulnerability we term reasoning distraction, where LRMs are\ndiverted from their primary objective by irrelevant yet complex tasks\nmaliciously embedded in the prompt. Through a comprehensive study across\ndiverse models and benchmarks, we show that even state-of-the-art LRMs are\nhighly susceptible, with injected distractors reducing task accuracy by up to\n60%. We further reveal that certain alignment techniques can amplify this\nweakness and that models may exhibit covert compliance, following hidden\nadversarial instructions in reasoning while concealing them in the final\noutput. To mitigate these risks, we propose a training-based defense that\ncombines Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on\nsynthetic adversarial data, improving robustness by over 50 points on\nchallenging distractor attacks. Our findings establish reasoning distraction as\na distinct and urgent threat to LRM reliability and provide a practical step\ntoward safer and more trustworthy reasoning systems.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63e3f57754f51ea342ce26be/W4rEeLd9b1Wam1JngHj1-.png",
      "https://cdn-uploads.huggingface.co/production/uploads/63e3f57754f51ea342ce26be/DjQP4ztyFID4WsfHl7QNn.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.16259.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63e3f57754f51ea342ce26be",
      "avatarUrl": "/avatars/df9d52c376bed6868d341bb006bec212.svg",
      "fullname": "Weijie Xu",
      "name": "xwjzds",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 16
    },
    "organization": {
      "_id": "615250af54cfe5db853ec856",
      "name": "AmazonScience",
      "fullname": "Amazon Science",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1632784986195-61393f6f905b1938233881e2.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.16258",
      "authors": [
        {
          "_id": "68f6f7b124c44893631118fc",
          "name": "Claire McLean",
          "hidden": false
        },
        {
          "_id": "68f6f7b124c44893631118fd",
          "name": "Makenzie Meendering",
          "hidden": false
        },
        {
          "_id": "68f6f7b124c44893631118fe",
          "name": "Tristan Swartz",
          "hidden": false
        },
        {
          "_id": "68f6f7b124c44893631118ff",
          "name": "Orri Gabbay",
          "hidden": false
        },
        {
          "_id": "68f6f7b124c4489363111900",
          "name": "Alexandra Olsen",
          "hidden": false
        },
        {
          "_id": "68f6f7b124c4489363111901",
          "name": "Rachel Jacobs",
          "hidden": false
        },
        {
          "_id": "68f6f7b124c4489363111902",
          "name": "Nicholas Rosen",
          "hidden": false
        },
        {
          "_id": "68f6f7b124c4489363111903",
          "name": "Philippe de Bree",
          "hidden": false
        },
        {
          "_id": "68f6f7b124c4489363111904",
          "name": "Tony Garcia",
          "hidden": false
        },
        {
          "_id": "68f6f7b124c4489363111905",
          "name": "Gadsden Merrill",
          "hidden": false
        },
        {
          "_id": "68f6f7b124c4489363111906",
          "name": "Jake Sandakly",
          "hidden": false
        },
        {
          "_id": "68f6f7b124c4489363111907",
          "name": "Julia Buffalini",
          "hidden": false
        },
        {
          "_id": "68f6f7b124c4489363111908",
          "name": "Neham Jain",
          "hidden": false
        },
        {
          "_id": "68f6f7b124c4489363111909",
          "name": "Steven Krenn",
          "hidden": false
        },
        {
          "_id": "68f6f7b124c448936311190a",
          "name": "Moneish Kumar",
          "hidden": false
        },
        {
          "_id": "68f6f7b124c448936311190b",
          "name": "Dejan Markovic",
          "hidden": false
        },
        {
          "_id": "68f6f7b124c448936311190c",
          "name": "Evonne Ng",
          "hidden": false
        },
        {
          "_id": "68f6f7b124c448936311190d",
          "name": "Fabian Prada",
          "hidden": false
        },
        {
          "_id": "68f6f7b124c448936311190e",
          "name": "Andrew Saba",
          "hidden": false
        },
        {
          "_id": "68f6f7b124c448936311190f",
          "name": "Siwei Zhang",
          "hidden": false
        },
        {
          "_id": "68f6f7b124c4489363111910",
          "name": "Vasu Agrawal",
          "hidden": false
        },
        {
          "_id": "68f6f7b124c4489363111911",
          "name": "Tim Godisart",
          "hidden": false
        },
        {
          "_id": "68f6f7b124c4489363111912",
          "name": "Alexander Richard",
          "hidden": false
        },
        {
          "_id": "68f6f7b124c4489363111913",
          "name": "Michael Zollhoefer",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-17T23:06:36.000Z",
      "submittedOnDailyAt": "2025-10-21T01:32:26.971Z",
      "title": "Embody 3D: A Large-scale Multimodal Motion and Behavior Dataset",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "The Codec Avatars Lab at Meta introduces Embody 3D, a multimodal dataset of\n500 individual hours of 3D motion data from 439 participants collected in a\nmulti-camera collection stage, amounting to over 54 million frames of tracked\n3D motion. The dataset features a wide range of single-person motion data,\nincluding prompted motions, hand gestures, and locomotion; as well as\nmulti-person behavioral and conversational data like discussions, conversations\nin different emotional states, collaborative activities, and co-living\nscenarios in an apartment-like space. We provide tracked human motion including\nhand tracking and body shape, text annotations, and a separate audio track for\neach participant.",
      "upvotes": 3,
      "discussionId": "68f6f7b124c4489363111914",
      "projectPage": "https://www.meta.com/emerging-tech/codec-avatars/embody-3d/",
      "ai_summary": "Embody 3D is a multimodal dataset featuring extensive 3D motion data with hand tracking, body shape, text annotations, and audio tracks from multiple participants in various scenarios.",
      "ai_keywords": [
        "3D motion data",
        "multi-camera collection",
        "hand tracking",
        "body shape",
        "text annotations",
        "audio track",
        "multimodal dataset"
      ],
      "organization": {
        "_id": "66b54027408752ae16404b05",
        "name": "metaresearch",
        "fullname": "Meta Research",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66b25f3f58babfaeb76112dc/2GmiaF075AZ7BcE538oPk.png"
      }
    },
    "publishedAt": "2025-10-17T19:06:36.000Z",
    "title": "Embody 3D: A Large-scale Multimodal Motion and Behavior Dataset",
    "summary": "The Codec Avatars Lab at Meta introduces Embody 3D, a multimodal dataset of\n500 individual hours of 3D motion data from 439 participants collected in a\nmulti-camera collection stage, amounting to over 54 million frames of tracked\n3D motion. The dataset features a wide range of single-person motion data,\nincluding prompted motions, hand gestures, and locomotion; as well as\nmulti-person behavioral and conversational data like discussions, conversations\nin different emotional states, collaborative activities, and co-living\nscenarios in an apartment-like space. We provide tracked human motion including\nhand tracking and body shape, text annotations, and a separate audio track for\neach participant.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.16258.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 133
    },
    "organization": {
      "_id": "66b54027408752ae16404b05",
      "name": "metaresearch",
      "fullname": "Meta Research",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66b25f3f58babfaeb76112dc/2GmiaF075AZ7BcE538oPk.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.15821",
      "authors": [
        {
          "_id": "68f5d4928589920bf4d321f2",
          "name": "Abdul Fatir Ansari",
          "hidden": false
        },
        {
          "_id": "68f5d4928589920bf4d321f3",
          "name": "Oleksandr Shchur",
          "hidden": false
        },
        {
          "_id": "68f5d4928589920bf4d321f4",
          "name": "Jaris Kken",
          "hidden": false
        },
        {
          "_id": "68f5d4928589920bf4d321f5",
          "name": "Andreas Auer",
          "hidden": false
        },
        {
          "_id": "68f5d4928589920bf4d321f6",
          "name": "Boran Han",
          "hidden": false
        },
        {
          "_id": "68f5d4928589920bf4d321f7",
          "name": "Pedro Mercado",
          "hidden": false
        },
        {
          "_id": "68f5d4928589920bf4d321f8",
          "name": "Syama Sundar Rangapuram",
          "hidden": false
        },
        {
          "_id": "68f5d4928589920bf4d321f9",
          "name": "Huibin Shen",
          "hidden": false
        },
        {
          "_id": "68f5d4928589920bf4d321fa",
          "name": "Lorenzo Stella",
          "hidden": false
        },
        {
          "_id": "68f5d4928589920bf4d321fb",
          "name": "Xiyuan Zhang",
          "hidden": false
        },
        {
          "_id": "68f5d4928589920bf4d321fc",
          "name": "Mononito Goswami",
          "hidden": false
        },
        {
          "_id": "68f5d4928589920bf4d321fd",
          "name": "Shubham Kapoor",
          "hidden": false
        },
        {
          "_id": "68f5d4928589920bf4d321fe",
          "name": "Danielle C. Maddix",
          "hidden": false
        },
        {
          "_id": "68f5d4928589920bf4d321ff",
          "name": "Pablo Guerron",
          "hidden": false
        },
        {
          "_id": "68f5d4928589920bf4d32200",
          "name": "Tony Hu",
          "hidden": false
        },
        {
          "_id": "68f5d4928589920bf4d32201",
          "name": "Junming Yin",
          "hidden": false
        },
        {
          "_id": "68f5d4928589920bf4d32202",
          "name": "Nick Erickson",
          "hidden": false
        },
        {
          "_id": "68f5d4928589920bf4d32203",
          "name": "Prateek Mutalik Desai",
          "hidden": false
        },
        {
          "_id": "68f5d4928589920bf4d32204",
          "name": "Hao Wang",
          "hidden": false
        },
        {
          "_id": "68f5d4928589920bf4d32205",
          "name": "Huzefa Rangwala",
          "hidden": false
        },
        {
          "_id": "68f5d4928589920bf4d32206",
          "name": "George Karypis",
          "hidden": false
        },
        {
          "_id": "68f5d4928589920bf4d32207",
          "name": "Yuyang Wang",
          "hidden": false
        },
        {
          "_id": "68f5d4928589920bf4d32208",
          "name": "Michael Bohlke-Schneider",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64aa5930b6512b83282588c4/7LNVonvpFfFAJ0DMSq4JY.png"
      ],
      "publishedAt": "2025-10-17T17:00:53.000Z",
      "submittedOnDailyAt": "2025-10-21T04:46:59.786Z",
      "title": "Chronos-2: From Univariate to Universal Forecasting",
      "submittedOnDailyBy": {
        "_id": "64aa5930b6512b83282588c4",
        "avatarUrl": "/avatars/57e60d9d36b5ee8911d41415850891d5.svg",
        "isPro": false,
        "fullname": "Abdul Fatir Ansari",
        "user": "abdulfatir",
        "type": "user"
      },
      "summary": "Pretrained time series models have enabled inference-only forecasting systems\nthat produce accurate predictions without task-specific training. However,\nexisting approaches largely focus on univariate forecasting, limiting their\napplicability in real-world scenarios where multivariate data and covariates\nplay a crucial role. We present Chronos-2, a pretrained model capable of\nhandling univariate, multivariate, and covariate-informed forecasting tasks in\na zero-shot manner. Chronos-2 employs a group attention mechanism that\nfacilitates in-context learning (ICL) through efficient information sharing\nacross multiple time series within a group, which may represent sets of related\nseries, variates of a multivariate series, or targets and covariates in a\nforecasting task. These general capabilities are achieved through training on\nsynthetic datasets that impose diverse multivariate structures on univariate\nseries. Chronos-2 delivers state-of-the-art performance across three\ncomprehensive benchmarks: fev-bench, GIFT-Eval, and Chronos Benchmark II. On\nfev-bench, which emphasizes multivariate and covariate-informed forecasting,\nChronos-2's universal ICL capabilities lead to substantial improvements over\nexisting models. On tasks involving covariates, it consistently outperforms\nbaselines by a wide margin. Case studies in the energy and retail domains\nfurther highlight its practical advantages. The in-context learning\ncapabilities of Chronos-2 establish it as a general-purpose forecasting model\nthat can be used \"as is\" in real-world forecasting pipelines.",
      "upvotes": 3,
      "discussionId": "68f5d4928589920bf4d32209",
      "githubRepo": "https://github.com/amazon-science/chronos-forecasting",
      "ai_summary": "Chronos-2, a pretrained model with a group attention mechanism, achieves state-of-the-art performance in zero-shot univariate, multivariate, and covariate-informed forecasting tasks.",
      "ai_keywords": [
        "pretrained model",
        "group attention mechanism",
        "in-context learning",
        "ICL",
        "synthetic datasets",
        "multivariate structures",
        "fev-bench",
        "GIFT-Eval",
        "Chronos Benchmark II",
        "energy domain",
        "retail domain"
      ],
      "githubStars": 3762,
      "organization": {
        "_id": "5ffdfbadbba2ae614d771970",
        "name": "amazon",
        "fullname": "Amazon",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66f19ed428ae41c20c470792/8y7msN6A6W82LdQhQd85a.png"
      }
    },
    "publishedAt": "2025-10-17T13:00:53.000Z",
    "title": "Chronos-2: From Univariate to Universal Forecasting",
    "summary": "Pretrained time series models have enabled inference-only forecasting systems\nthat produce accurate predictions without task-specific training. However,\nexisting approaches largely focus on univariate forecasting, limiting their\napplicability in real-world scenarios where multivariate data and covariates\nplay a crucial role. We present Chronos-2, a pretrained model capable of\nhandling univariate, multivariate, and covariate-informed forecasting tasks in\na zero-shot manner. Chronos-2 employs a group attention mechanism that\nfacilitates in-context learning (ICL) through efficient information sharing\nacross multiple time series within a group, which may represent sets of related\nseries, variates of a multivariate series, or targets and covariates in a\nforecasting task. These general capabilities are achieved through training on\nsynthetic datasets that impose diverse multivariate structures on univariate\nseries. Chronos-2 delivers state-of-the-art performance across three\ncomprehensive benchmarks: fev-bench, GIFT-Eval, and Chronos Benchmark II. On\nfev-bench, which emphasizes multivariate and covariate-informed forecasting,\nChronos-2's universal ICL capabilities lead to substantial improvements over\nexisting models. On tasks involving covariates, it consistently outperforms\nbaselines by a wide margin. Case studies in the energy and retail domains\nfurther highlight its practical advantages. The in-context learning\ncapabilities of Chronos-2 establish it as a general-purpose forecasting model\nthat can be used \"as is\" in real-world forecasting pipelines.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64aa5930b6512b83282588c4/7LNVonvpFfFAJ0DMSq4JY.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.15821.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64aa5930b6512b83282588c4",
      "avatarUrl": "/avatars/57e60d9d36b5ee8911d41415850891d5.svg",
      "fullname": "Abdul Fatir Ansari",
      "name": "abdulfatir",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "organization": {
      "_id": "5ffdfbadbba2ae614d771970",
      "name": "amazon",
      "fullname": "Amazon",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66f19ed428ae41c20c470792/8y7msN6A6W82LdQhQd85a.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.14605",
      "authors": [
        {
          "_id": "68f33e1b8589920bf4d31af3",
          "user": {
            "_id": "66e03d7123e5f162e7e5c5ac",
            "avatarUrl": "/avatars/28089b6ffeacafff2c798b7579689c1e.svg",
            "isPro": false,
            "fullname": "hongyuyang",
            "user": "hongyuyang23casia",
            "type": "user"
          },
          "name": "Yuyang Hong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-20T12:12:55.883Z",
          "hidden": false
        },
        {
          "_id": "68f33e1b8589920bf4d31af4",
          "name": "Jiaqi Gu",
          "hidden": false
        },
        {
          "_id": "68f33e1b8589920bf4d31af5",
          "name": "Qi Yang",
          "hidden": false
        },
        {
          "_id": "68f33e1b8589920bf4d31af6",
          "name": "Lubin Fan",
          "hidden": false
        },
        {
          "_id": "68f33e1b8589920bf4d31af7",
          "name": "Yue Wu",
          "hidden": false
        },
        {
          "_id": "68f33e1b8589920bf4d31af8",
          "name": "Ying Wang",
          "hidden": false
        },
        {
          "_id": "68f33e1b8589920bf4d31af9",
          "name": "Kun Ding",
          "hidden": false
        },
        {
          "_id": "68f33e1b8589920bf4d31afa",
          "name": "Shiming Xiang",
          "hidden": false
        },
        {
          "_id": "68f33e1b8589920bf4d31afb",
          "name": "Jieping Ye",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-16T12:10:00.000Z",
      "submittedOnDailyAt": "2025-10-21T00:58:55.843Z",
      "title": "Knowledge-based Visual Question Answer with Multimodal Processing,\n  Retrieval and Filtering",
      "submittedOnDailyBy": {
        "_id": "66e03d7123e5f162e7e5c5ac",
        "avatarUrl": "/avatars/28089b6ffeacafff2c798b7579689c1e.svg",
        "isPro": false,
        "fullname": "hongyuyang",
        "user": "hongyuyang23casia",
        "type": "user"
      },
      "summary": "Knowledge-based visual question answering (KB-VQA) requires visual language\nmodels (VLMs) to integrate visual understanding with external knowledge\nretrieval. Although retrieval-augmented generation (RAG) achieves significant\nadvances in this task by combining knowledge-base querying, it still struggles\nwith the quality of multimodal queries and the relevance of retrieved results.\nTo overcome these challenges, we propose a novel three-stage method, termed\nWiki-PRF, including Processing, Retrieval and Filtering stages. The processing\nstage dynamically invokes visual tools to extract precise multimodal\ninformation for retrieval. The retrieval stage integrates visual and text\nfeatures to achieve multimodal knowledge retrieval. The filtering stage\nperforms relevance filtering and concentration on retrieval results. To this\nend, we introduce a visual language model trained with answer accuracy and\nformat consistency as reward signals via a reinforcement learning manner. This\nenhances the model's reasoning, tool invocation for accurate queries, and\nfiltering of irrelevant content. Experiments on benchmark datasets (E-VQA and\nInfoSeek) show significant improvements~(36.0 and 42.8) in answer quality,\nachieving state-of-the-art performance. Code is available at\nhttps://github.com/cqu-student/Wiki-PRF",
      "upvotes": 3,
      "discussionId": "68f33e1b8589920bf4d31afc",
      "ai_summary": "A novel three-stage method, Wiki-PRF, enhances knowledge-based visual question answering by improving multimodal query quality and relevance through visual language models and reinforcement learning.",
      "ai_keywords": [
        "knowledge-based visual question answering",
        "KB-VQA",
        "visual language models",
        "VLMs",
        "retrieval-augmented generation",
        "RAG",
        "multimodal queries",
        "multimodal knowledge retrieval",
        "relevance filtering",
        "reinforcement learning",
        "answer accuracy",
        "format consistency",
        "E-VQA",
        "InfoSeek"
      ],
      "organization": {
        "_id": "640a887796aae649741a586f",
        "name": "CASIA",
        "fullname": "Chinese Academic of Science Institute of Automation",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1678411888885-6388984e8a5dbe2f3dc5afee.jpeg"
      }
    },
    "publishedAt": "2025-10-16T08:10:00.000Z",
    "title": "Knowledge-based Visual Question Answer with Multimodal Processing,\n  Retrieval and Filtering",
    "summary": "Knowledge-based visual question answering (KB-VQA) requires visual language\nmodels (VLMs) to integrate visual understanding with external knowledge\nretrieval. Although retrieval-augmented generation (RAG) achieves significant\nadvances in this task by combining knowledge-base querying, it still struggles\nwith the quality of multimodal queries and the relevance of retrieved results.\nTo overcome these challenges, we propose a novel three-stage method, termed\nWiki-PRF, including Processing, Retrieval and Filtering stages. The processing\nstage dynamically invokes visual tools to extract precise multimodal\ninformation for retrieval. The retrieval stage integrates visual and text\nfeatures to achieve multimodal knowledge retrieval. The filtering stage\nperforms relevance filtering and concentration on retrieval results. To this\nend, we introduce a visual language model trained with answer accuracy and\nformat consistency as reward signals via a reinforcement learning manner. This\nenhances the model's reasoning, tool invocation for accurate queries, and\nfiltering of irrelevant content. Experiments on benchmark datasets (E-VQA and\nInfoSeek) show significant improvements~(36.0 and 42.8) in answer quality,\nachieving state-of-the-art performance. Code is available at\nhttps://github.com/cqu-student/Wiki-PRF",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14605.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66e03d7123e5f162e7e5c5ac",
      "avatarUrl": "/avatars/28089b6ffeacafff2c798b7579689c1e.svg",
      "fullname": "hongyuyang",
      "name": "hongyuyang23casia",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "organization": {
      "_id": "640a887796aae649741a586f",
      "name": "CASIA",
      "fullname": "Chinese Academic of Science Institute of Automation",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1678411888885-6388984e8a5dbe2f3dc5afee.jpeg"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.15527",
      "authors": [
        {
          "_id": "68f6ea0d24c448936311180d",
          "name": "Aditya Vir",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-17T10:59:24.000Z",
      "submittedOnDailyAt": "2025-10-21T00:34:37.118Z",
      "title": "Balanced Multi-Task Attention for Satellite Image Classification: A\n  Systematic Approach to Achieving 97.23% Accuracy on EuroSAT Without\n  Pre-Training",
      "submittedOnDailyBy": {
        "_id": "63be9021da08ed0544f36c38",
        "avatarUrl": "/avatars/42511d8b1d3a3ef99bc154c98b72dfba.svg",
        "isPro": false,
        "fullname": "onurcan",
        "user": "monurcan",
        "type": "user"
      },
      "summary": "This work presents a systematic investigation of custom convolutional neural\nnetwork architectures for satellite land use classification, achieving 97.23%\ntest accuracy on the EuroSAT dataset without reliance on pre-trained models.\nThrough three progressive architectural iterations (baseline: 94.30%,\nCBAM-enhanced: 95.98%, and balanced multi-task attention: 97.23%) we identify\nand address specific failure modes in satellite imagery classification. Our\nprincipal contribution is a novel balanced multi-task attention mechanism that\ncombines Coordinate Attention for spatial feature extraction with\nSqueeze-Excitation blocks for spectral feature extraction, unified through a\nlearnable fusion parameter. Experimental results demonstrate that this\nlearnable parameter autonomously converges to alpha approximately 0.57,\nindicating near-equal importance of spatial and spectral modalities for\nsatellite imagery. We employ progressive DropBlock regularization (5-20% by\nnetwork depth) and class-balanced loss weighting to address overfitting and\nconfusion pattern imbalance. The final 12-layer architecture achieves Cohen's\nKappa of 0.9692 with all classes exceeding 94.46% accuracy, demonstrating\nconfidence calibration with a 24.25% gap between correct and incorrect\npredictions. Our approach achieves performance within 1.34% of fine-tuned\nResNet-50 (98.57%) while requiring no external data, validating the efficacy of\nsystematic architectural design for domain-specific applications. Complete\ncode, trained models, and evaluation scripts are publicly available.",
      "upvotes": 2,
      "discussionId": "68f6ea0e24c448936311180e",
      "ai_summary": "A novel balanced multi-task attention mechanism in custom convolutional neural networks improves satellite land use classification accuracy to 97.23% on the EuroSAT dataset without pre-trained models.",
      "ai_keywords": [
        "convolutional neural networks",
        "satellite land use classification",
        "EuroSAT dataset",
        "CBAM-enhanced",
        "balanced multi-task attention",
        "Coordinate Attention",
        "Squeeze-Excitation blocks",
        "learnable fusion parameter",
        "progressive DropBlock regularization",
        "class-balanced loss weighting",
        "Cohen's Kappa",
        "ResNet-50"
      ]
    },
    "publishedAt": "2025-10-17T06:59:24.000Z",
    "title": "Balanced Multi-Task Attention for Satellite Image Classification: A\n  Systematic Approach to Achieving 97.23% Accuracy on EuroSAT Without\n  Pre-Training",
    "summary": "This work presents a systematic investigation of custom convolutional neural\nnetwork architectures for satellite land use classification, achieving 97.23%\ntest accuracy on the EuroSAT dataset without reliance on pre-trained models.\nThrough three progressive architectural iterations (baseline: 94.30%,\nCBAM-enhanced: 95.98%, and balanced multi-task attention: 97.23%) we identify\nand address specific failure modes in satellite imagery classification. Our\nprincipal contribution is a novel balanced multi-task attention mechanism that\ncombines Coordinate Attention for spatial feature extraction with\nSqueeze-Excitation blocks for spectral feature extraction, unified through a\nlearnable fusion parameter. Experimental results demonstrate that this\nlearnable parameter autonomously converges to alpha approximately 0.57,\nindicating near-equal importance of spatial and spectral modalities for\nsatellite imagery. We employ progressive DropBlock regularization (5-20% by\nnetwork depth) and class-balanced loss weighting to address overfitting and\nconfusion pattern imbalance. The final 12-layer architecture achieves Cohen's\nKappa of 0.9692 with all classes exceeding 94.46% accuracy, demonstrating\nconfidence calibration with a 24.25% gap between correct and incorrect\npredictions. Our approach achieves performance within 1.34% of fine-tuned\nResNet-50 (98.57%) while requiring no external data, validating the efficacy of\nsystematic architectural design for domain-specific applications. Complete\ncode, trained models, and evaluation scripts are publicly available.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.15527.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63be9021da08ed0544f36c38",
      "avatarUrl": "/avatars/42511d8b1d3a3ef99bc154c98b72dfba.svg",
      "fullname": "onurcan",
      "name": "monurcan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.17793",
      "authors": [
        {
          "_id": "68f6e6f524c44893631117e1",
          "name": "Austin Xu",
          "hidden": false
        },
        {
          "_id": "68f6e6f524c44893631117e2",
          "name": "Xuan-Phi Nguyen",
          "hidden": false
        },
        {
          "_id": "68f6e6f524c44893631117e3",
          "name": "Yilun Zhou",
          "hidden": false
        },
        {
          "_id": "68f6e6f524c44893631117e4",
          "name": "Chien-Sheng Wu",
          "hidden": false
        },
        {
          "_id": "68f6e6f524c44893631117e5",
          "name": "Caiming Xiong",
          "hidden": false
        },
        {
          "_id": "68f6e6f524c44893631117e6",
          "name": "Shafiq Joty",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-20T17:52:06.000Z",
      "submittedOnDailyAt": "2025-10-21T04:00:23.082Z",
      "title": "Foundational Automatic Evaluators: Scaling Multi-Task Generative\n  Evaluator Training for Reasoning-Centric Domains",
      "submittedOnDailyBy": {
        "_id": "6668e86dc4ef4175fb18d250",
        "avatarUrl": "/avatars/d34925685799a1218017f85ce4d44e6e.svg",
        "isPro": false,
        "fullname": "Austin Xu",
        "user": "austinxu87",
        "type": "user"
      },
      "summary": "Finetuning specialized generative evaluators has emerged as a popular\nparadigm to meet the increasing demand for scalable evaluation during both\ntraining and test-time. However, recent work has largely focused on applying\nnew methodology, such as reinforcement learning (RL), to training evaluators,\nshying away from large-scale, data-driven development. In this work, we focus\non data scaling, curating a set of 2.5M samples spanning five unique evaluation\ntasks (pairwise, step-level, reference-free and reference-based verification,\nand single rating) and multiple domains focused on reasoning evaluation. With\nour data, we train Foundational Automatic Reasoning Evaluators (FARE), a family\nof 8B and 20B (with 3.6B active) parameter evaluators, with a simple iterative\nrejection-sampling supervised finetuning (SFT) approach. FARE-8B challenges\nlarger specialized RL-trained evaluators and FARE-20B sets the new standard for\nopen-source evaluators, surpassing specialized 70B+ evaluators. Beyond static\nbenchmarks, we evaluate FARE in real-world tasks: As inference-time rerankers,\nFARE-20B achieves near-oracle performance on MATH. As verifiers in RL training,\nFARE improves the downstream RL-trained model performance by up to 14.1% vs.\nstring-matching verifiers. When initialized from FARE, a continually-finetuned\nFARE-Code outperforms gpt-oss-20B by 65% on evaluating test-case quality.",
      "upvotes": 1,
      "discussionId": "68f6e6f624c44893631117e7",
      "ai_summary": "FARE, a family of large-scale parameter evaluators, surpasses specialized RL-trained evaluators in both static benchmarks and real-world tasks through data-driven development and iterative rejection-sampling supervised finetuning.",
      "ai_keywords": [
        "generative evaluators",
        "reinforcement learning",
        "Foundational Automatic Reasoning Evaluators",
        "parameter evaluators",
        "iterative rejection-sampling supervised finetuning",
        "MATH",
        "continually-finetuned",
        "gpt-oss-20B"
      ],
      "organization": {
        "_id": "5f6d64475e78cc6b0ed31e4c",
        "name": "Salesforce",
        "fullname": "Salesforce",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1602756670970-noauth.jpeg"
      }
    },
    "publishedAt": "2025-10-20T13:52:06.000Z",
    "title": "Foundational Automatic Evaluators: Scaling Multi-Task Generative\n  Evaluator Training for Reasoning-Centric Domains",
    "summary": "Finetuning specialized generative evaluators has emerged as a popular\nparadigm to meet the increasing demand for scalable evaluation during both\ntraining and test-time. However, recent work has largely focused on applying\nnew methodology, such as reinforcement learning (RL), to training evaluators,\nshying away from large-scale, data-driven development. In this work, we focus\non data scaling, curating a set of 2.5M samples spanning five unique evaluation\ntasks (pairwise, step-level, reference-free and reference-based verification,\nand single rating) and multiple domains focused on reasoning evaluation. With\nour data, we train Foundational Automatic Reasoning Evaluators (FARE), a family\nof 8B and 20B (with 3.6B active) parameter evaluators, with a simple iterative\nrejection-sampling supervised finetuning (SFT) approach. FARE-8B challenges\nlarger specialized RL-trained evaluators and FARE-20B sets the new standard for\nopen-source evaluators, surpassing specialized 70B+ evaluators. Beyond static\nbenchmarks, we evaluate FARE in real-world tasks: As inference-time rerankers,\nFARE-20B achieves near-oracle performance on MATH. As verifiers in RL training,\nFARE improves the downstream RL-trained model performance by up to 14.1% vs.\nstring-matching verifiers. When initialized from FARE, a continually-finetuned\nFARE-Code outperforms gpt-oss-20B by 65% on evaluating test-case quality.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.17793.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6668e86dc4ef4175fb18d250",
      "avatarUrl": "/avatars/d34925685799a1218017f85ce4d44e6e.svg",
      "fullname": "Austin Xu",
      "name": "austinxu87",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "organization": {
      "_id": "5f6d64475e78cc6b0ed31e4c",
      "name": "Salesforce",
      "fullname": "Salesforce",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1602756670970-noauth.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.16641",
      "authors": [
        {
          "_id": "68f7390224c4489363111b4b",
          "name": "Young-Jun Lee",
          "hidden": false
        },
        {
          "_id": "68f7390224c4489363111b4c",
          "name": "Byung-Kwan Lee",
          "hidden": false
        },
        {
          "_id": "68f7390224c4489363111b4d",
          "name": "Jianshu Zhang",
          "hidden": false
        },
        {
          "_id": "68f7390224c4489363111b4e",
          "name": "Yechan Hwang",
          "hidden": false
        },
        {
          "_id": "68f7390224c4489363111b4f",
          "name": "Byungsoo Ko",
          "hidden": false
        },
        {
          "_id": "68f7390224c4489363111b50",
          "name": "Han-Gyu Kim",
          "hidden": false
        },
        {
          "_id": "68f7390224c4489363111b51",
          "name": "Dongyu Yao",
          "hidden": false
        },
        {
          "_id": "68f7390224c4489363111b52",
          "name": "Xuankun Rong",
          "hidden": false
        },
        {
          "_id": "68f7390224c4489363111b53",
          "name": "Eojin Joo",
          "hidden": false
        },
        {
          "_id": "68f7390224c4489363111b54",
          "name": "Seung-Ho Han",
          "hidden": false
        },
        {
          "_id": "68f7390224c4489363111b55",
          "name": "Bowon Ko",
          "hidden": false
        },
        {
          "_id": "68f7390224c4489363111b56",
          "name": "Ho-Jin Choi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-18T21:00:12.000Z",
      "submittedOnDailyAt": "2025-10-21T06:17:11.707Z",
      "title": "MultiVerse: A Multi-Turn Conversation Benchmark for Evaluating Large\n  Vision and Language Models",
      "submittedOnDailyBy": {
        "_id": "6434b6619bd5a84b5dcfa4de",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6434b6619bd5a84b5dcfa4de/h8Q6kPNjFNc03wmdboHzq.jpeg",
        "isPro": true,
        "fullname": "Young-Jun Lee",
        "user": "passing2961",
        "type": "user"
      },
      "summary": "Vision-and-Language Models (VLMs) have shown impressive capabilities on\nsingle-turn benchmarks, yet real-world applications often demand more intricate\nmulti-turn dialogues. Existing multi-turn datasets (e.g, MMDU, ConvBench) only\npartially capture the breadth and depth of conversational scenarios encountered\nby users. In this work, we introduce MultiVerse, a novel multi-turn\nconversation benchmark featuring 647 dialogues - each averaging four turns -\nderived from a diverse set of 12 popular VLM evaluation benchmarks. With 484\ntasks and 484 interaction goals, MultiVerse covers a wide range of topics, from\nfactual knowledge and perception to advanced reasoning tasks such as\nmathematics and coding. To facilitate robust assessment, we propose a\nchecklist-based evaluation method that leverages GPT-4o as the automated\nevaluator, measuring performance across 37 key aspects, including perceptual\naccuracy, linguistic clarity, and factual correctness. We evaluate 18 VLMs on\nMultiVerse, revealing that even the strongest models (e.g., GPT-4o) achieve\nonly a 50% success rate in complex multi-turn conversations, highlighting the\ndataset's challenging nature. Notably, we find that providing full dialogue\ncontext significantly enhances performance for smaller or weaker models,\nemphasizing the importance of in-context learning. We believe MultiVerse is a\nlandscape of evaluating multi-turn interaction abilities for VLMs.",
      "upvotes": 1,
      "discussionId": "68f7390324c4489363111b57",
      "githubRepo": "https://github.com/passing2961/MultiVerse",
      "ai_summary": "MultiVerse, a new multi-turn conversation benchmark, evaluates VLMs across diverse tasks and interaction goals, revealing challenges and the importance of in-context learning.",
      "ai_keywords": [
        "Vision-and-Language Models",
        "multi-turn dialogues",
        "MMDU",
        "ConvBench",
        "MultiVerse",
        "GPT-4o",
        "checklist-based evaluation",
        "perceptual accuracy",
        "linguistic clarity",
        "factual correctness",
        "in-context learning"
      ],
      "organization": {
        "_id": "635b304962fb2bc1b52c6291",
        "name": "KAIST",
        "fullname": "KAIST"
      }
    },
    "publishedAt": "2025-10-18T17:00:12.000Z",
    "title": "MultiVerse: A Multi-Turn Conversation Benchmark for Evaluating Large\n  Vision and Language Models",
    "summary": "Vision-and-Language Models (VLMs) have shown impressive capabilities on\nsingle-turn benchmarks, yet real-world applications often demand more intricate\nmulti-turn dialogues. Existing multi-turn datasets (e.g, MMDU, ConvBench) only\npartially capture the breadth and depth of conversational scenarios encountered\nby users. In this work, we introduce MultiVerse, a novel multi-turn\nconversation benchmark featuring 647 dialogues - each averaging four turns -\nderived from a diverse set of 12 popular VLM evaluation benchmarks. With 484\ntasks and 484 interaction goals, MultiVerse covers a wide range of topics, from\nfactual knowledge and perception to advanced reasoning tasks such as\nmathematics and coding. To facilitate robust assessment, we propose a\nchecklist-based evaluation method that leverages GPT-4o as the automated\nevaluator, measuring performance across 37 key aspects, including perceptual\naccuracy, linguistic clarity, and factual correctness. We evaluate 18 VLMs on\nMultiVerse, revealing that even the strongest models (e.g., GPT-4o) achieve\nonly a 50% success rate in complex multi-turn conversations, highlighting the\ndataset's challenging nature. Notably, we find that providing full dialogue\ncontext significantly enhances performance for smaller or weaker models,\nemphasizing the importance of in-context learning. We believe MultiVerse is a\nlandscape of evaluating multi-turn interaction abilities for VLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.16641.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6434b6619bd5a84b5dcfa4de",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6434b6619bd5a84b5dcfa4de/h8Q6kPNjFNc03wmdboHzq.jpeg",
      "fullname": "Young-Jun Lee",
      "name": "passing2961",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "organization": {
      "_id": "635b304962fb2bc1b52c6291",
      "name": "KAIST",
      "fullname": "KAIST"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.16156",
      "authors": [
        {
          "_id": "68f6f42324c44893631118c7",
          "name": "Yueqian Lin",
          "hidden": false
        },
        {
          "_id": "68f6f42324c44893631118c8",
          "name": "Zhengmian Hu",
          "hidden": false
        },
        {
          "_id": "68f6f42324c44893631118c9",
          "name": "Jayakumar Subramanian",
          "hidden": false
        },
        {
          "_id": "68f6f42324c44893631118ca",
          "name": "Qinsi Wang",
          "hidden": false
        },
        {
          "_id": "68f6f42324c44893631118cb",
          "name": "Nikos Vlassis",
          "hidden": false
        },
        {
          "_id": "68f6f42324c44893631118cc",
          "name": "Hai \"Helen\" Li",
          "hidden": false
        },
        {
          "_id": "68f6f42324c44893631118cd",
          "name": "Yiran Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-17T19:00:08.000Z",
      "submittedOnDailyAt": "2025-10-21T01:20:53.305Z",
      "title": "AsyncVoice Agent: Real-Time Explanation for LLM Planning and Reasoning",
      "submittedOnDailyBy": {
        "_id": "64b5198c25882acb62fb77ef",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b5198c25882acb62fb77ef/HX9pfMEPQlfjvSAgSLplY.png",
        "isPro": false,
        "fullname": "Yueqian Lin",
        "user": "linyueqian",
        "type": "user"
      },
      "summary": "Effective human-AI collaboration on complex reasoning tasks requires that\nusers understand and interact with the model's process, not just receive an\noutput. However, the monolithic text from methods like Chain-of-Thought (CoT)\nprevents this, as current interfaces lack real-time verbalization and robust\nuser barge-in. We present AsyncVoice Agent, a system whose asynchronous\narchitecture decouples a streaming LLM backend from a conversational voice\nfrontend. This design allows narration and inference to run in parallel,\nempowering users to interrupt, query, and steer the model's reasoning process\nat any time. Objective benchmarks show this approach reduces interaction\nlatency by more than 600x compared to monolithic baselines while ensuring high\nfidelity and competitive task accuracy. By enabling a two-way dialogue with a\nmodel's thought process, AsyncVoice Agent offers a new paradigm for building\nmore effective, steerable, and trustworthy human-AI systems for high-stakes\ntasks.",
      "upvotes": 1,
      "discussionId": "68f6f42324c44893631118ce",
      "ai_summary": "AsyncVoice Agent, with its asynchronous architecture, enhances human-AI collaboration by enabling real-time interaction and interruption of the model's reasoning process, significantly reducing latency while maintaining accuracy.",
      "ai_keywords": [
        "Chain-of-Thought",
        "CoT",
        "streaming LLM backend",
        "conversational voice frontend",
        "narration",
        "inference",
        "interaction latency",
        "high-fidelity",
        "task accuracy",
        "two-way dialogue"
      ]
    },
    "publishedAt": "2025-10-17T15:00:08.000Z",
    "title": "AsyncVoice Agent: Real-Time Explanation for LLM Planning and Reasoning",
    "summary": "Effective human-AI collaboration on complex reasoning tasks requires that\nusers understand and interact with the model's process, not just receive an\noutput. However, the monolithic text from methods like Chain-of-Thought (CoT)\nprevents this, as current interfaces lack real-time verbalization and robust\nuser barge-in. We present AsyncVoice Agent, a system whose asynchronous\narchitecture decouples a streaming LLM backend from a conversational voice\nfrontend. This design allows narration and inference to run in parallel,\nempowering users to interrupt, query, and steer the model's reasoning process\nat any time. Objective benchmarks show this approach reduces interaction\nlatency by more than 600x compared to monolithic baselines while ensuring high\nfidelity and competitive task accuracy. By enabling a two-way dialogue with a\nmodel's thought process, AsyncVoice Agent offers a new paradigm for building\nmore effective, steerable, and trustworthy human-AI systems for high-stakes\ntasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.16156.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b5198c25882acb62fb77ef",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b5198c25882acb62fb77ef/HX9pfMEPQlfjvSAgSLplY.png",
      "fullname": "Yueqian Lin",
      "name": "linyueqian",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.16136",
      "authors": [
        {
          "_id": "68f7229e24c44893631119eb",
          "name": "Sayan Deb Sarkar",
          "hidden": false
        },
        {
          "_id": "68f7229e24c44893631119ec",
          "name": "Sinisa Stekovic",
          "hidden": false
        },
        {
          "_id": "68f7229e24c44893631119ed",
          "name": "Vincent Lepetit",
          "hidden": false
        },
        {
          "_id": "68f7229e24c44893631119ee",
          "name": "Iro Armeni",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-17T18:22:04.000Z",
      "submittedOnDailyAt": "2025-10-21T05:02:57.476Z",
      "title": "GuideFlow3D: Optimization-Guided Rectified Flow For Appearance Transfer",
      "submittedOnDailyBy": {
        "_id": "650ec19e6620b0c57e2a551b",
        "avatarUrl": "/avatars/c26c03fa920d857120f03c9ccb9f1d7a.svg",
        "isPro": false,
        "fullname": "Sayan Deb Sarkar",
        "user": "sayandsarkar",
        "type": "user"
      },
      "summary": "Transferring appearance to 3D assets using different representations of the\nappearance object - such as images or text - has garnered interest due to its\nwide range of applications in industries like gaming, augmented reality, and\ndigital content creation. However, state-of-the-art methods still fail when the\ngeometry between the input and appearance objects is significantly different. A\nstraightforward approach is to directly apply a 3D generative model, but we\nshow that this ultimately fails to produce appealing results. Instead, we\npropose a principled approach inspired by universal guidance. Given a\npretrained rectified flow model conditioned on image or text, our training-free\nmethod interacts with the sampling process by periodically adding guidance.\nThis guidance can be modeled as a differentiable loss function, and we\nexperiment with two different types of guidance including part-aware losses for\nappearance and self-similarity. Our experiments show that our approach\nsuccessfully transfers texture and geometric details to the input 3D asset,\noutperforming baselines both qualitatively and quantitatively. We also show\nthat traditional metrics are not suitable for evaluating the task due to their\ninability of focusing on local details and comparing dissimilar inputs, in\nabsence of ground truth data. We thus evaluate appearance transfer quality with\na GPT-based system objectively ranking outputs, ensuring robust and human-like\nassessment, as further confirmed by our user study. Beyond showcased scenarios,\nour method is general and could be extended to different types of diffusion\nmodels and guidance functions.",
      "upvotes": 0,
      "discussionId": "68f7229e24c44893631119ef",
      "projectPage": "https://sayands.github.io/guideflow3d/",
      "ai_summary": "A method using pretrained rectified flow models with periodic guidance successfully transfers appearance and geometric details to 3D assets, outperforming baselines and evaluated using a GPT-based system.",
      "ai_keywords": [
        "rectified flow model",
        "guidance",
        "differentiable loss function",
        "part-aware losses",
        "self-similarity",
        "GPT-based system",
        "diffusion models"
      ],
      "organization": {
        "_id": "6745347f0efcb641a76ed31f",
        "name": "gradient-spaces",
        "fullname": "Gradient Spaces Research Group",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/650ec19e6620b0c57e2a551b/D6wE8-X3R25IUKnQ5Pf89.jpeg"
      }
    },
    "publishedAt": "2025-10-17T14:22:04.000Z",
    "title": "GuideFlow3D: Optimization-Guided Rectified Flow For Appearance Transfer",
    "summary": "Transferring appearance to 3D assets using different representations of the\nappearance object - such as images or text - has garnered interest due to its\nwide range of applications in industries like gaming, augmented reality, and\ndigital content creation. However, state-of-the-art methods still fail when the\ngeometry between the input and appearance objects is significantly different. A\nstraightforward approach is to directly apply a 3D generative model, but we\nshow that this ultimately fails to produce appealing results. Instead, we\npropose a principled approach inspired by universal guidance. Given a\npretrained rectified flow model conditioned on image or text, our training-free\nmethod interacts with the sampling process by periodically adding guidance.\nThis guidance can be modeled as a differentiable loss function, and we\nexperiment with two different types of guidance including part-aware losses for\nappearance and self-similarity. Our experiments show that our approach\nsuccessfully transfers texture and geometric details to the input 3D asset,\noutperforming baselines both qualitatively and quantitatively. We also show\nthat traditional metrics are not suitable for evaluating the task due to their\ninability of focusing on local details and comparing dissimilar inputs, in\nabsence of ground truth data. We thus evaluate appearance transfer quality with\na GPT-based system objectively ranking outputs, ensuring robust and human-like\nassessment, as further confirmed by our user study. Beyond showcased scenarios,\nour method is general and could be extended to different types of diffusion\nmodels and guidance functions.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.16136.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "650ec19e6620b0c57e2a551b",
      "avatarUrl": "/avatars/c26c03fa920d857120f03c9ccb9f1d7a.svg",
      "fullname": "Sayan Deb Sarkar",
      "name": "sayandsarkar",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "organization": {
      "_id": "6745347f0efcb641a76ed31f",
      "name": "gradient-spaces",
      "fullname": "Gradient Spaces Research Group",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/650ec19e6620b0c57e2a551b/D6wE8-X3R25IUKnQ5Pf89.jpeg"
    },
    "isAuthorParticipating": false
  }
]