[
  {
    "paper": {
      "id": "2508.07999",
      "authors": [
        {
          "_id": "689aa9c6fab6fdd2e52ac459",
          "name": "Ryan Wong",
          "hidden": false
        },
        {
          "_id": "689aa9c6fab6fdd2e52ac45a",
          "name": "Jiawei Wang",
          "hidden": false
        },
        {
          "_id": "689aa9c6fab6fdd2e52ac45b",
          "name": "Junjie Zhao",
          "hidden": false
        },
        {
          "_id": "689aa9c6fab6fdd2e52ac45c",
          "name": "Li Chen",
          "hidden": false
        },
        {
          "_id": "689aa9c6fab6fdd2e52ac45d",
          "name": "Yan Gao",
          "hidden": false
        },
        {
          "_id": "689aa9c6fab6fdd2e52ac45e",
          "name": "Long Zhang",
          "hidden": false
        },
        {
          "_id": "689aa9c6fab6fdd2e52ac45f",
          "name": "Xuan Zhou",
          "hidden": false
        },
        {
          "_id": "689aa9c6fab6fdd2e52ac460",
          "name": "Zuo Wang",
          "hidden": false
        },
        {
          "_id": "689aa9c6fab6fdd2e52ac461",
          "name": "Kai Xiang",
          "hidden": false
        },
        {
          "_id": "689aa9c6fab6fdd2e52ac462",
          "name": "Ge Zhang",
          "hidden": false
        },
        {
          "_id": "689aa9c6fab6fdd2e52ac463",
          "name": "Wenhao Huang",
          "hidden": false
        },
        {
          "_id": "689aa9c6fab6fdd2e52ac464",
          "name": "Yang Wang",
          "hidden": false
        },
        {
          "_id": "689aa9c6fab6fdd2e52ac465",
          "name": "Ke Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-11T14:03:09.000Z",
      "submittedOnDailyAt": "2025-08-12T01:13:47.675Z",
      "title": "WideSearch: Benchmarking Agentic Broad Info-Seeking",
      "submittedOnDailyBy": {
        "_id": "64060b49a577649430bf6974",
        "avatarUrl": "/avatars/74d0d6ed656b593e4c101b09edf18c7a.svg",
        "isPro": false,
        "fullname": "Jiawei Wang",
        "user": "Jarvis1111",
        "type": "user"
      },
      "summary": "From professional research to everyday planning, many tasks are bottlenecked\nby wide-scale information seeking, which is more repetitive than cognitively\ncomplex. With the rapid development of Large Language Models (LLMs), automated\nsearch agents powered by LLMs offer a promising solution to liberate humans\nfrom this tedious work. However, the capability of these agents to perform such\n\"wide-context\" collection reliably and completely remains largely unevaluated\ndue to a lack of suitable benchmarks. To bridge this gap, we introduce\nWideSearch, a new benchmark engineered to evaluate agent reliability on these\nlarge-scale collection tasks. The benchmark features 200 manually curated\nquestions (100 in English, 100 in Chinese) from over 15 diverse domains,\ngrounded in real user queries. Each task requires agents to collect large-scale\natomic information, which could be verified one by one objectively, and arrange\nit into a well-organized output. A rigorous five-stage quality control pipeline\nensures the difficulty, completeness, and verifiability of the dataset. We\nbenchmark over 10 state-of-the-art agentic search systems, including\nsingle-agent, multi-agent frameworks, and end-to-end commercial systems. Most\nsystems achieve overall success rates near 0\\%, with the best performer\nreaching just 5\\%. However, given sufficient time, cross-validation by multiple\nhuman testers can achieve a near 100\\% success rate. These results demonstrate\nthat present search agents have critical deficiencies in large-scale\ninformation seeking, underscoring urgent areas for future research and\ndevelopment in agentic search. Our dataset, evaluation pipeline, and benchmark\nresults have been publicly released at https://widesearch-seed.github.io/",
      "upvotes": 63,
      "discussionId": "689aa9c6fab6fdd2e52ac466",
      "projectPage": "https://widesearch-seed.github.io/",
      "githubRepo": "https://github.com/ByteDance-Seed/WideSearch",
      "ai_summary": "WideSearch is a new benchmark evaluating the reliability of automated search agents in large-scale information collection tasks, revealing significant deficiencies in current systems.",
      "ai_keywords": [
        "Large Language Models",
        "automated search agents",
        "WideSearch",
        "benchmark",
        "quality control pipeline",
        "agentic search systems",
        "single-agent",
        "multi-agent frameworks",
        "end-to-end commercial systems"
      ],
      "githubStars": 22
    },
    "publishedAt": "2025-08-11T10:03:09.000Z",
    "title": "WideSearch: Benchmarking Agentic Broad Info-Seeking",
    "summary": "From professional research to everyday planning, many tasks are bottlenecked\nby wide-scale information seeking, which is more repetitive than cognitively\ncomplex. With the rapid development of Large Language Models (LLMs), automated\nsearch agents powered by LLMs offer a promising solution to liberate humans\nfrom this tedious work. However, the capability of these agents to perform such\n\"wide-context\" collection reliably and completely remains largely unevaluated\ndue to a lack of suitable benchmarks. To bridge this gap, we introduce\nWideSearch, a new benchmark engineered to evaluate agent reliability on these\nlarge-scale collection tasks. The benchmark features 200 manually curated\nquestions (100 in English, 100 in Chinese) from over 15 diverse domains,\ngrounded in real user queries. Each task requires agents to collect large-scale\natomic information, which could be verified one by one objectively, and arrange\nit into a well-organized output. A rigorous five-stage quality control pipeline\nensures the difficulty, completeness, and verifiability of the dataset. We\nbenchmark over 10 state-of-the-art agentic search systems, including\nsingle-agent, multi-agent frameworks, and end-to-end commercial systems. Most\nsystems achieve overall success rates near 0\\%, with the best performer\nreaching just 5\\%. However, given sufficient time, cross-validation by multiple\nhuman testers can achieve a near 100\\% success rate. These results demonstrate\nthat present search agents have critical deficiencies in large-scale\ninformation seeking, underscoring urgent areas for future research and\ndevelopment in agentic search. Our dataset, evaluation pipeline, and benchmark\nresults have been publicly released at https://widesearch-seed.github.io/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.07999.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64060b49a577649430bf6974",
      "avatarUrl": "/avatars/74d0d6ed656b593e4c101b09edf18c7a.svg",
      "fullname": "Jiawei Wang",
      "name": "Jarvis1111",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.07050",
      "authors": [
        {
          "_id": "689ab8b6fab6fdd2e52ac4a2",
          "name": "Wenhan Liu",
          "hidden": false
        },
        {
          "_id": "689ab8b6fab6fdd2e52ac4a3",
          "name": "Xinyu Ma",
          "hidden": false
        },
        {
          "_id": "689ab8b6fab6fdd2e52ac4a4",
          "name": "Weiwei Sun",
          "hidden": false
        },
        {
          "_id": "689ab8b6fab6fdd2e52ac4a5",
          "name": "Yutao Zhu",
          "hidden": false
        },
        {
          "_id": "689ab8b6fab6fdd2e52ac4a6",
          "name": "Yuchen Li",
          "hidden": false
        },
        {
          "_id": "689ab8b6fab6fdd2e52ac4a7",
          "name": "Dawei Yin",
          "hidden": false
        },
        {
          "_id": "689ab8b6fab6fdd2e52ac4a8",
          "name": "Zhicheng Dou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-09T17:26:18.000Z",
      "submittedOnDailyAt": "2025-08-12T02:16:23.014Z",
      "title": "ReasonRank: Empowering Passage Ranking with Strong Reasoning Ability",
      "submittedOnDailyBy": {
        "_id": "62cd2f13979d883655cd5377",
        "avatarUrl": "/avatars/400c252d20d68aca56e0d0280498ce17.svg",
        "isPro": false,
        "fullname": "Xinyu Ma",
        "user": "xyma",
        "type": "user"
      },
      "summary": "Large Language Model (LLM) based listwise ranking has shown superior\nperformance in many passage ranking tasks. With the development of Large\nReasoning Models, many studies have demonstrated that step-by-step reasoning\nduring test-time helps improve listwise ranking performance. However, due to\nthe scarcity of reasoning-intensive training data, existing rerankers perform\npoorly in many complex ranking scenarios and the ranking ability of\nreasoning-intensive rerankers remains largely underdeveloped. In this paper, we\nfirst propose an automated reasoning-intensive training data synthesis\nframework, which sources training queries and passages from diverse domains and\napplies DeepSeek-R1 to generate high-quality training labels. A\nself-consistency data filtering mechanism is designed to ensure the data\nquality. To empower the listwise reranker with strong reasoning ability, we\nfurther propose a two-stage post-training approach, which includes a cold-start\nsupervised fine-tuning (SFT) stage for reasoning pattern learning and a\nreinforcement learning (RL) stage for further ranking ability enhancement.\nDuring the RL stage, based on the nature of listwise ranking, we design a\nmulti-view ranking reward, which is more effective than a ranking metric-based\nreward. Extensive experiments demonstrate that our trained reasoning-intensive\nreranker ReasonRank outperforms existing baselines significantly and\nalso achieves much lower latency than pointwise reranker Rank1. Through\nfurther experiments, our ReasonRank has achieved state-of-the-art (SOTA)\nperformance 40.6 on the BRIGHT\nleaderboard\\footnote{https://brightbenchmark.github.io/.} Our codes are\navailable at https://github.com/8421BCD/ReasonRank.",
      "upvotes": 47,
      "discussionId": "689ab8b6fab6fdd2e52ac4a9",
      "projectPage": "https://github.com/8421BCD/ReasonRank",
      "githubRepo": "https://github.com/8421BCD/ReasonRank",
      "ai_summary": "A reasoning-intensive reranker, ReasonRank, achieves state-of-the-art performance in passage ranking tasks by using synthesized training data and a two-stage post-training approach with reinforcement learning.",
      "ai_keywords": [
        "Large Language Model",
        "listwise ranking",
        "Large Reasoning Models",
        "step-by-step reasoning",
        "DeepSeek-R1",
        "self-consistency data filtering",
        "cold-start supervised fine-tuning",
        "reinforcement learning",
        "multi-view ranking reward",
        "BRIGHT leaderboard"
      ],
      "githubStars": 33
    },
    "publishedAt": "2025-08-09T13:26:18.000Z",
    "title": "ReasonRank: Empowering Passage Ranking with Strong Reasoning Ability",
    "summary": "Large Language Model (LLM) based listwise ranking has shown superior\nperformance in many passage ranking tasks. With the development of Large\nReasoning Models, many studies have demonstrated that step-by-step reasoning\nduring test-time helps improve listwise ranking performance. However, due to\nthe scarcity of reasoning-intensive training data, existing rerankers perform\npoorly in many complex ranking scenarios and the ranking ability of\nreasoning-intensive rerankers remains largely underdeveloped. In this paper, we\nfirst propose an automated reasoning-intensive training data synthesis\nframework, which sources training queries and passages from diverse domains and\napplies DeepSeek-R1 to generate high-quality training labels. A\nself-consistency data filtering mechanism is designed to ensure the data\nquality. To empower the listwise reranker with strong reasoning ability, we\nfurther propose a two-stage post-training approach, which includes a cold-start\nsupervised fine-tuning (SFT) stage for reasoning pattern learning and a\nreinforcement learning (RL) stage for further ranking ability enhancement.\nDuring the RL stage, based on the nature of listwise ranking, we design a\nmulti-view ranking reward, which is more effective than a ranking metric-based\nreward. Extensive experiments demonstrate that our trained reasoning-intensive\nreranker ReasonRank outperforms existing baselines significantly and\nalso achieves much lower latency than pointwise reranker Rank1. Through\nfurther experiments, our ReasonRank has achieved state-of-the-art (SOTA)\nperformance 40.6 on the BRIGHT\nleaderboard\\footnote{https://brightbenchmark.github.io/.} Our codes are\navailable at https://github.com/8421BCD/ReasonRank.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.07050.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "62cd2f13979d883655cd5377",
      "avatarUrl": "/avatars/400c252d20d68aca56e0d0280498ce17.svg",
      "fullname": "Xinyu Ma",
      "name": "xyma",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.07981",
      "authors": [
        {
          "_id": "689ac656fab6fdd2e52ac504",
          "name": "Fangyuan Mao",
          "hidden": false
        },
        {
          "_id": "689ac656fab6fdd2e52ac505",
          "name": "Aiming Hao",
          "hidden": false
        },
        {
          "_id": "689ac656fab6fdd2e52ac506",
          "name": "Jintao Chen",
          "hidden": false
        },
        {
          "_id": "689ac656fab6fdd2e52ac507",
          "name": "Dongxia Liu",
          "hidden": false
        },
        {
          "_id": "689ac656fab6fdd2e52ac508",
          "name": "Xiaokun Feng",
          "hidden": false
        },
        {
          "_id": "689ac656fab6fdd2e52ac509",
          "name": "Jiashu Zhu",
          "hidden": false
        },
        {
          "_id": "689ac656fab6fdd2e52ac50a",
          "name": "Meiqi Wu",
          "hidden": false
        },
        {
          "_id": "689ac656fab6fdd2e52ac50b",
          "name": "Chubin Chen",
          "hidden": false
        },
        {
          "_id": "689ac656fab6fdd2e52ac50c",
          "name": "Jiahong Wu",
          "hidden": false
        },
        {
          "_id": "689ac656fab6fdd2e52ac50d",
          "name": "Xiangxiang Chu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-11T13:41:24.000Z",
      "submittedOnDailyAt": "2025-08-12T04:36:46.480Z",
      "title": "Omni-Effects: Unified and Spatially-Controllable Visual Effects\n  Generation",
      "submittedOnDailyBy": {
        "_id": "66d255e3947594430c723ff6",
        "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg",
        "isPro": false,
        "fullname": "xiaochonglinghu",
        "user": "xiaochonglinghu",
        "type": "user"
      },
      "summary": "Visual effects (VFX) are essential visual enhancements fundamental to modern\ncinematic production. Although video generation models offer cost-efficient\nsolutions for VFX production, current methods are constrained by per-effect\nLoRA training, which limits generation to single effects. This fundamental\nlimitation impedes applications that require spatially controllable composite\neffects, i.e., the concurrent generation of multiple effects at designated\nlocations. However, integrating diverse effects into a unified framework faces\nmajor challenges: interference from effect variations and spatial\nuncontrollability during multi-VFX joint training. To tackle these challenges,\nwe propose Omni-Effects, a first unified framework capable of generating\nprompt-guided effects and spatially controllable composite effects. The core of\nour framework comprises two key innovations: (1) LoRA-based Mixture of Experts\n(LoRA-MoE), which employs a group of expert LoRAs, integrating diverse effects\nwithin a unified model while effectively mitigating cross-task interference.\n(2) Spatial-Aware Prompt (SAP) incorporates spatial mask information into the\ntext token, enabling precise spatial control. Furthermore, we introduce an\nIndependent-Information Flow (IIF) module integrated within the SAP, isolating\nthe control signals corresponding to individual effects to prevent any unwanted\nblending. To facilitate this research, we construct a comprehensive VFX dataset\nOmni-VFX via a novel data collection pipeline combining image editing and\nFirst-Last Frame-to-Video (FLF2V) synthesis, and introduce a dedicated VFX\nevaluation framework for validating model performance. Extensive experiments\ndemonstrate that Omni-Effects achieves precise spatial control and diverse\neffect generation, enabling users to specify both the category and location of\ndesired effects.",
      "upvotes": 37,
      "discussionId": "689ac657fab6fdd2e52ac50e",
      "projectPage": "https://amap-ml.github.io/Omni-Effects.github.io/",
      "githubRepo": "https://github.com/AMAP-ML/Omni-Effects",
      "ai_summary": "Omni-Effects is a unified framework that enables the generation of prompt-guided and spatially controllable composite visual effects using LoRA-based Mixture of Experts and Spatial-Aware Prompt with Independent-Information Flow.",
      "ai_keywords": [
        "LoRA",
        "Mixture of Experts",
        "LoRA-MoE",
        "Spatial-Aware Prompt",
        "SAP",
        "Independent-Information Flow",
        "IIF",
        "Omni-VFX",
        "First-Last Frame-to-Video",
        "FLF2V"
      ],
      "githubStars": 14
    },
    "publishedAt": "2025-08-11T09:41:24.000Z",
    "title": "Omni-Effects: Unified and Spatially-Controllable Visual Effects\n  Generation",
    "summary": "Visual effects (VFX) are essential visual enhancements fundamental to modern\ncinematic production. Although video generation models offer cost-efficient\nsolutions for VFX production, current methods are constrained by per-effect\nLoRA training, which limits generation to single effects. This fundamental\nlimitation impedes applications that require spatially controllable composite\neffects, i.e., the concurrent generation of multiple effects at designated\nlocations. However, integrating diverse effects into a unified framework faces\nmajor challenges: interference from effect variations and spatial\nuncontrollability during multi-VFX joint training. To tackle these challenges,\nwe propose Omni-Effects, a first unified framework capable of generating\nprompt-guided effects and spatially controllable composite effects. The core of\nour framework comprises two key innovations: (1) LoRA-based Mixture of Experts\n(LoRA-MoE), which employs a group of expert LoRAs, integrating diverse effects\nwithin a unified model while effectively mitigating cross-task interference.\n(2) Spatial-Aware Prompt (SAP) incorporates spatial mask information into the\ntext token, enabling precise spatial control. Furthermore, we introduce an\nIndependent-Information Flow (IIF) module integrated within the SAP, isolating\nthe control signals corresponding to individual effects to prevent any unwanted\nblending. To facilitate this research, we construct a comprehensive VFX dataset\nOmni-VFX via a novel data collection pipeline combining image editing and\nFirst-Last Frame-to-Video (FLF2V) synthesis, and introduce a dedicated VFX\nevaluation framework for validating model performance. Extensive experiments\ndemonstrate that Omni-Effects achieves precise spatial control and diverse\neffect generation, enabling users to specify both the category and location of\ndesired effects.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.07981.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66d255e3947594430c723ff6",
      "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg",
      "fullname": "xiaochonglinghu",
      "name": "xiaochonglinghu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.07629",
      "authors": [
        {
          "_id": "689ab4aefab6fdd2e52ac483",
          "name": "Zhenpeng Su",
          "hidden": false
        },
        {
          "_id": "689ab4aefab6fdd2e52ac484",
          "name": "Leiyu Pan",
          "hidden": false
        },
        {
          "_id": "689ab4aefab6fdd2e52ac485",
          "name": "Xue Bai",
          "hidden": false
        },
        {
          "_id": "689ab4aefab6fdd2e52ac486",
          "name": "Dening Liu",
          "hidden": false
        },
        {
          "_id": "689ab4aefab6fdd2e52ac487",
          "name": "Guanting Dong",
          "hidden": false
        },
        {
          "_id": "689ab4aefab6fdd2e52ac488",
          "name": "Jiaming Huang",
          "hidden": false
        },
        {
          "_id": "689ab4aefab6fdd2e52ac489",
          "name": "Wenping Hu",
          "hidden": false
        },
        {
          "_id": "689ab4aefab6fdd2e52ac48a",
          "name": "Guorui Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-11T05:17:51.000Z",
      "submittedOnDailyAt": "2025-08-12T01:59:54.855Z",
      "title": "Klear-Reasoner: Advancing Reasoning Capability via Gradient-Preserving\n  Clipping Policy Optimization",
      "submittedOnDailyBy": {
        "_id": "61c2cf8d1172fa7969904d99",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61c2cf8d1172fa7969904d99/R10G5h3d9Q_YQ__Hc-H4k.jpeg",
        "isPro": false,
        "fullname": "suu",
        "user": "Suu",
        "type": "user"
      },
      "summary": "We present Klear-Reasoner, a model with long reasoning capabilities that\ndemonstrates careful deliberation during problem solving, achieving outstanding\nperformance across multiple benchmarks. Although there are already many\nexcellent works related to inference models in the current community, there are\nstill many problems with reproducing high-performance inference models due to\nincomplete disclosure of training details. This report provides an in-depth\nanalysis of the reasoning model, covering the entire post-training workflow\nfrom data preparation and long Chain-of-Thought supervised fine-tuning (long\nCoT SFT) to reinforcement learning (RL), along with detailed ablation studies\nfor each experimental component. For SFT data, our experiments show that a\nsmall number of high-quality data sources are more effective than a large\nnumber of diverse data sources, and that difficult samples can achieve better\nresults without accuracy filtering. In addition, we investigate two key issues\nwith current clipping mechanisms in RL: Clipping suppresses critical\nexploration signals and ignores suboptimal trajectories. To address these\nchallenges, we propose Gradient-Preserving clipping Policy Optimization (GPPO)\nthat gently backpropagates gradients from clipped tokens. GPPO not only\nenhances the model's exploration capacity but also improves its efficiency in\nlearning from negative samples. Klear-Reasoner exhibits exceptional reasoning\nabilities in mathematics and programming, scoring 90.5\\% on AIME 2024, 83.2\\%\non AIME 2025, 66.0\\% on LiveCodeBench V5 and 58.1\\% on LiveCodeBench V6.",
      "upvotes": 19,
      "discussionId": "689ab4affab6fdd2e52ac48b",
      "projectPage": "https://github.com/suu990901/KlearReasoner",
      "githubRepo": "https://github.com/suu990901/KlearReasoner",
      "ai_summary": "Klear-Reasoner, a model with long reasoning capabilities, achieves high performance across benchmarks through detailed post-training workflows, including long Chain-of-Thought supervised fine-tuning and reinforcement learning with Gradient-Preserving clipping Policy Optimization.",
      "ai_keywords": [
        "long reasoning",
        "Chain-of-Thought supervised fine-tuning",
        "long CoT SFT",
        "reinforcement learning",
        "RL",
        "Gradient-Preserving clipping Policy Optimization",
        "GPPO",
        "exploration signals",
        "suboptimal trajectories",
        "backpropagation",
        "gradients",
        "clipped tokens",
        "AIME",
        "LiveCodeBench"
      ],
      "githubStars": 8
    },
    "publishedAt": "2025-08-11T01:17:51.000Z",
    "title": "Klear-Reasoner: Advancing Reasoning Capability via Gradient-Preserving\n  Clipping Policy Optimization",
    "summary": "We present Klear-Reasoner, a model with long reasoning capabilities that\ndemonstrates careful deliberation during problem solving, achieving outstanding\nperformance across multiple benchmarks. Although there are already many\nexcellent works related to inference models in the current community, there are\nstill many problems with reproducing high-performance inference models due to\nincomplete disclosure of training details. This report provides an in-depth\nanalysis of the reasoning model, covering the entire post-training workflow\nfrom data preparation and long Chain-of-Thought supervised fine-tuning (long\nCoT SFT) to reinforcement learning (RL), along with detailed ablation studies\nfor each experimental component. For SFT data, our experiments show that a\nsmall number of high-quality data sources are more effective than a large\nnumber of diverse data sources, and that difficult samples can achieve better\nresults without accuracy filtering. In addition, we investigate two key issues\nwith current clipping mechanisms in RL: Clipping suppresses critical\nexploration signals and ignores suboptimal trajectories. To address these\nchallenges, we propose Gradient-Preserving clipping Policy Optimization (GPPO)\nthat gently backpropagates gradients from clipped tokens. GPPO not only\nenhances the model's exploration capacity but also improves its efficiency in\nlearning from negative samples. Klear-Reasoner exhibits exceptional reasoning\nabilities in mathematics and programming, scoring 90.5\\% on AIME 2024, 83.2\\%\non AIME 2025, 66.0\\% on LiveCodeBench V5 and 58.1\\% on LiveCodeBench V6.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.07629.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "61c2cf8d1172fa7969904d99",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61c2cf8d1172fa7969904d99/R10G5h3d9Q_YQ__Hc-H4k.jpeg",
      "fullname": "suu",
      "name": "Suu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.22034",
      "authors": [
        {
          "_id": "689ac5a8fab6fdd2e52ac4f1",
          "name": "Cheng Qian",
          "hidden": false
        },
        {
          "_id": "689ac5a8fab6fdd2e52ac4f2",
          "name": "Zuxin Liu",
          "hidden": false
        },
        {
          "_id": "689ac5a8fab6fdd2e52ac4f3",
          "name": "Akshara Prabhakar",
          "hidden": false
        },
        {
          "_id": "689ac5a8fab6fdd2e52ac4f4",
          "name": "Zhiwei Liu",
          "hidden": false
        },
        {
          "_id": "689ac5a8fab6fdd2e52ac4f5",
          "name": "Jianguo Zhang",
          "hidden": false
        },
        {
          "_id": "689ac5a8fab6fdd2e52ac4f6",
          "name": "Haolin Chen",
          "hidden": false
        },
        {
          "_id": "689ac5a8fab6fdd2e52ac4f7",
          "name": "Heng Ji",
          "hidden": false
        },
        {
          "_id": "689ac5a8fab6fdd2e52ac4f8",
          "name": "Weiran Yao",
          "hidden": false
        },
        {
          "_id": "689ac5a8fab6fdd2e52ac4f9",
          "name": "Shelby Heinecke",
          "hidden": false
        },
        {
          "_id": "689ac5a8fab6fdd2e52ac4fa",
          "name": "Silvio Savarese",
          "hidden": false
        },
        {
          "_id": "689ac5a8fab6fdd2e52ac4fb",
          "name": "Caiming Xiong",
          "hidden": false
        },
        {
          "_id": "689ac5a8fab6fdd2e52ac4fc",
          "name": "Huan Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-29T17:34:12.000Z",
      "submittedOnDailyAt": "2025-08-12T03:10:58.124Z",
      "title": "UserBench: An Interactive Gym Environment for User-Centric Agents",
      "submittedOnDailyBy": {
        "_id": "665e121c6007027038fd4005",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/sIVBJAGM-Kneq9KMf8aXb.png",
        "isPro": false,
        "fullname": "Cheng Qian",
        "user": "chengq9",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs)-based agents have made impressive progress in\nreasoning and tool use, enabling them to solve complex tasks. However, their\nability to proactively collaborate with users, especially when goals are vague,\nevolving, or indirectly expressed, remains underexplored. To address this gap,\nwe introduce UserBench, a user-centric benchmark designed to evaluate agents in\nmulti-turn, preference-driven interactions. UserBench features simulated users\nwho start with underspecified goals and reveal preferences incrementally,\nrequiring agents to proactively clarify intent and make grounded decisions with\ntools. Our evaluation of leading open- and closed-source LLMs reveals a\nsignificant disconnect between task completion and user alignment. For\ninstance, models provide answers that fully align with all user intents only\n20% of the time on average, and even the most advanced models uncover fewer\nthan 30% of all user preferences through active interaction. These results\nhighlight the challenges of building agents that are not just capable task\nexecutors, but true collaborative partners. UserBench offers an interactive\nenvironment to measure and advance this critical capability.",
      "upvotes": 18,
      "discussionId": "689ac5a8fab6fdd2e52ac4fd",
      "githubRepo": "https://github.com/SalesforceAIResearch/UserBench",
      "ai_summary": "UserBench evaluates LLM-based agents in multi-turn interactions with simulated users, revealing gaps in task completion and user alignment.",
      "ai_keywords": [
        "Large Language Models",
        "LLMs",
        "UserBench",
        "multi-turn interactions",
        "preference-driven interactions",
        "simulated users",
        "underspecified goals",
        "user alignment",
        "task executors",
        "collaborative partners"
      ],
      "githubStars": 22
    },
    "publishedAt": "2025-07-29T13:34:12.000Z",
    "title": "UserBench: An Interactive Gym Environment for User-Centric Agents",
    "summary": "Large Language Models (LLMs)-based agents have made impressive progress in\nreasoning and tool use, enabling them to solve complex tasks. However, their\nability to proactively collaborate with users, especially when goals are vague,\nevolving, or indirectly expressed, remains underexplored. To address this gap,\nwe introduce UserBench, a user-centric benchmark designed to evaluate agents in\nmulti-turn, preference-driven interactions. UserBench features simulated users\nwho start with underspecified goals and reveal preferences incrementally,\nrequiring agents to proactively clarify intent and make grounded decisions with\ntools. Our evaluation of leading open- and closed-source LLMs reveals a\nsignificant disconnect between task completion and user alignment. For\ninstance, models provide answers that fully align with all user intents only\n20% of the time on average, and even the most advanced models uncover fewer\nthan 30% of all user preferences through active interaction. These results\nhighlight the challenges of building agents that are not just capable task\nexecutors, but true collaborative partners. UserBench offers an interactive\nenvironment to measure and advance this critical capability.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.22034.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "665e121c6007027038fd4005",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/sIVBJAGM-Kneq9KMf8aXb.png",
      "fullname": "Cheng Qian",
      "name": "chengq9",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.06600",
      "authors": [
        {
          "_id": "689aa99dfab6fdd2e52ac443",
          "name": "Zijian Chen",
          "hidden": false
        },
        {
          "_id": "689aa99dfab6fdd2e52ac444",
          "name": "Xueguang Ma",
          "hidden": false
        },
        {
          "_id": "689aa99dfab6fdd2e52ac445",
          "name": "Shengyao Zhuang",
          "hidden": false
        },
        {
          "_id": "689aa99dfab6fdd2e52ac446",
          "name": "Ping Nie",
          "hidden": false
        },
        {
          "_id": "689aa99dfab6fdd2e52ac447",
          "name": "Kai Zou",
          "hidden": false
        },
        {
          "_id": "689aa99dfab6fdd2e52ac448",
          "name": "Andrew Liu",
          "hidden": false
        },
        {
          "_id": "689aa99dfab6fdd2e52ac449",
          "name": "Joshua Green",
          "hidden": false
        },
        {
          "_id": "689aa99dfab6fdd2e52ac44a",
          "name": "Kshama Patel",
          "hidden": false
        },
        {
          "_id": "689aa99dfab6fdd2e52ac44b",
          "name": "Ruoxi Meng",
          "hidden": false
        },
        {
          "_id": "689aa99dfab6fdd2e52ac44c",
          "name": "Mingyi Su",
          "hidden": false
        },
        {
          "_id": "689aa99dfab6fdd2e52ac44d",
          "name": "Sahel Sharifymoghaddam",
          "hidden": false
        },
        {
          "_id": "689aa99dfab6fdd2e52ac44e",
          "name": "Yanxi Li",
          "hidden": false
        },
        {
          "_id": "689aa99dfab6fdd2e52ac44f",
          "name": "Haoran Hong",
          "hidden": false
        },
        {
          "_id": "689aa99dfab6fdd2e52ac450",
          "name": "Xinyu Shi",
          "hidden": false
        },
        {
          "_id": "689aa99dfab6fdd2e52ac451",
          "name": "Xuye Liu",
          "hidden": false
        },
        {
          "_id": "689aa99dfab6fdd2e52ac452",
          "name": "Nandan Thakur",
          "hidden": false
        },
        {
          "_id": "689aa99dfab6fdd2e52ac453",
          "name": "Crystina Zhang",
          "hidden": false
        },
        {
          "_id": "689aa99dfab6fdd2e52ac454",
          "name": "Luyu Gao",
          "hidden": false
        },
        {
          "_id": "689aa99dfab6fdd2e52ac455",
          "name": "Wenhu Chen",
          "hidden": false
        },
        {
          "_id": "689aa99dfab6fdd2e52ac456",
          "name": "Jimmy Lin",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/5ec82854968f6028e0559f70/1jy5u5L06u17REGKLZqjH.png"
      ],
      "publishedAt": "2025-08-08T17:55:11.000Z",
      "submittedOnDailyAt": "2025-08-12T01:14:41.177Z",
      "title": "BrowseComp-Plus: A More Fair and Transparent Evaluation Benchmark of\n  Deep-Research Agent",
      "submittedOnDailyBy": {
        "_id": "5ec82854968f6028e0559f70",
        "avatarUrl": "/avatars/45b58d912f7d00cb351947cd79d5eeb4.svg",
        "isPro": true,
        "fullname": "Xueguang Ma",
        "user": "MrLight",
        "type": "user"
      },
      "summary": "Deep-Research agents, which integrate large language models (LLMs) with\nsearch tools, have shown success in improving the effectiveness of handling\ncomplex queries that require iterative search planning and reasoning over\nsearch results. Evaluations on current benchmarks like BrowseComp relies on\nblack-box live web search APIs, have notable limitations in (1) fairness:\ndynamic and opaque web APIs hinder fair comparisons and reproducibility of deep\nresearch methods; (2) transparency: lack of control over the document corpus\nmakes it difficult to isolate retriever contributions. In other words, the\ncurrent evaluations may compare a complete deep research system at a given\ntime, but they do not foster well-controlled experiments to provide insights\ninto the capability of underlying deep research LLMs. To address these\nchallenges, we introduce BrowseComp-Plus, a benchmark derived from BrowseComp,\nemploying a fixed, carefully curated corpus. Each query in BrowseComp-Plus\nincludes human-verified supporting documents and mined challenging negatives,\nenabling controlled experimentation. The benchmark is shown to be effective in\ndistinguishing the performance of deep research systems. For instance, the\nopen-source model Search-R1, when paired with the BM25 retriever, achieves\n3.86% accuracy, whereas the GPT-5 achieves 55.9%. Integrating the GPT-5 with\nthe Qwen3-Embedding-8B retriever further enhances its accuracy to 70.1% with\nfewer search calls. This benchmark allows comprehensive evaluation and\ndisentangled analysis of deep research agents and retrieval methods, fostering\ninsights into retrieval effectiveness, citation accuracy, and context\nengineering in Deep-Research system.",
      "upvotes": 12,
      "discussionId": "689aa99dfab6fdd2e52ac457",
      "projectPage": "https://texttron.github.io/BrowseComp-Plus/",
      "githubRepo": "https://github.com/texttron/BrowseComp-Plus",
      "ai_summary": "BrowseComp-Plus, a curated benchmark, enables controlled evaluation of deep research agents and retrieval methods, providing insights into their performance and effectiveness.",
      "ai_keywords": [
        "deep-Research agents",
        "large language models (LLMs)",
        "search tools",
        "iterative search planning",
        "BrowseComp",
        "black-box live web search APIs",
        "fairness",
        "transparency",
        "document corpus",
        "retriever contributions",
        "BrowseComp-Plus",
        "human-verified supporting documents",
        "challenging negatives",
        "Search-R1",
        "GPT-5",
        "Qwen3-Embedding-8B",
        "retrieval effectiveness",
        "citation accuracy",
        "context engineering"
      ],
      "githubStars": 21
    },
    "publishedAt": "2025-08-08T13:55:11.000Z",
    "title": "BrowseComp-Plus: A More Fair and Transparent Evaluation Benchmark of\n  Deep-Research Agent",
    "summary": "Deep-Research agents, which integrate large language models (LLMs) with\nsearch tools, have shown success in improving the effectiveness of handling\ncomplex queries that require iterative search planning and reasoning over\nsearch results. Evaluations on current benchmarks like BrowseComp relies on\nblack-box live web search APIs, have notable limitations in (1) fairness:\ndynamic and opaque web APIs hinder fair comparisons and reproducibility of deep\nresearch methods; (2) transparency: lack of control over the document corpus\nmakes it difficult to isolate retriever contributions. In other words, the\ncurrent evaluations may compare a complete deep research system at a given\ntime, but they do not foster well-controlled experiments to provide insights\ninto the capability of underlying deep research LLMs. To address these\nchallenges, we introduce BrowseComp-Plus, a benchmark derived from BrowseComp,\nemploying a fixed, carefully curated corpus. Each query in BrowseComp-Plus\nincludes human-verified supporting documents and mined challenging negatives,\nenabling controlled experimentation. The benchmark is shown to be effective in\ndistinguishing the performance of deep research systems. For instance, the\nopen-source model Search-R1, when paired with the BM25 retriever, achieves\n3.86% accuracy, whereas the GPT-5 achieves 55.9%. Integrating the GPT-5 with\nthe Qwen3-Embedding-8B retriever further enhances its accuracy to 70.1% with\nfewer search calls. This benchmark allows comprehensive evaluation and\ndisentangled analysis of deep research agents and retrieval methods, fostering\ninsights into retrieval effectiveness, citation accuracy, and context\nengineering in Deep-Research system.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/5ec82854968f6028e0559f70/1jy5u5L06u17REGKLZqjH.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.06600.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5ec82854968f6028e0559f70",
      "avatarUrl": "/avatars/45b58d912f7d00cb351947cd79d5eeb4.svg",
      "fullname": "Xueguang Ma",
      "name": "MrLight",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 28
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.05614",
      "authors": [
        {
          "_id": "6895644d48b0ae5ca2710d64",
          "user": {
            "_id": "677d3523a6918748bf81d0e9",
            "avatarUrl": "/avatars/c9961ac54d089efb36db20c421c2bea2.svg",
            "isPro": false,
            "fullname": "wangzixuan",
            "user": "wangzx1210",
            "type": "user"
          },
          "name": "Zixuan Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-08T16:19:35.707Z",
          "hidden": false
        },
        {
          "_id": "6895644d48b0ae5ca2710d65",
          "name": "Dingming Li",
          "hidden": false
        },
        {
          "_id": "6895644d48b0ae5ca2710d66",
          "name": "Hongxing Li",
          "hidden": false
        },
        {
          "_id": "6895644d48b0ae5ca2710d67",
          "name": "Shuo Chen",
          "hidden": false
        },
        {
          "_id": "6895644d48b0ae5ca2710d68",
          "user": {
            "_id": "64098738342c26884c792c93",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64098738342c26884c792c93/SxBUd-wLrl-PjQsrVYJte.jpeg",
            "isPro": false,
            "fullname": "Yuchen Yan",
            "user": "yanyc",
            "type": "user"
          },
          "name": "Yuchen Yan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-11T06:50:03.727Z",
          "hidden": false
        },
        {
          "_id": "6895644d48b0ae5ca2710d69",
          "name": "Wenqi Zhang",
          "hidden": false
        },
        {
          "_id": "6895644d48b0ae5ca2710d6a",
          "user": {
            "_id": "5e1058e9fcf41d740b69966d",
            "avatarUrl": "/avatars/ce74839ba871f2b54313a670a233ba82.svg",
            "isPro": false,
            "fullname": "Yongliang Shen",
            "user": "tricktreat",
            "type": "user"
          },
          "name": "Yongliang Shen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-11T06:50:07.402Z",
          "hidden": false
        },
        {
          "_id": "6895644d48b0ae5ca2710d6b",
          "name": "Weiming Lu",
          "hidden": false
        },
        {
          "_id": "6895644d48b0ae5ca2710d6c",
          "name": "Jun Xiao",
          "hidden": false
        },
        {
          "_id": "6895644d48b0ae5ca2710d6d",
          "name": "Yueting Zhuang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-07T17:54:15.000Z",
      "submittedOnDailyAt": "2025-08-12T00:34:06.839Z",
      "title": "OmniEAR: Benchmarking Agent Reasoning in Embodied Tasks",
      "submittedOnDailyBy": {
        "_id": "5e1058e9fcf41d740b69966d",
        "avatarUrl": "/avatars/ce74839ba871f2b54313a670a233ba82.svg",
        "isPro": false,
        "fullname": "Yongliang Shen",
        "user": "tricktreat",
        "type": "user"
      },
      "summary": "Large language models excel at abstract reasoning but their capacity for\nembodied agent reasoning remains largely unexplored. We present OmniEAR, a\ncomprehensive framework for evaluating how language models reason about\nphysical interactions, tool usage, and multi-agent coordination in embodied\ntasks. Unlike existing benchmarks that provide predefined tool sets or explicit\ncollaboration directives, OmniEAR requires agents to dynamically acquire\ncapabilities and autonomously determine coordination strategies based on task\ndemands. Through text-based environment representation, we model continuous\nphysical properties and complex spatial relationships across 1,500 scenarios\nspanning household and industrial domains. Our systematic evaluation reveals\nsevere performance degradation when models must reason from constraints: while\nachieving 85-96% success with explicit instructions, performance drops to\n56-85% for tool reasoning and 63-85% for implicit collaboration, with compound\ntasks showing over 50% failure rates. Surprisingly, complete environmental\ninformation degrades coordination performance, indicating models cannot filter\ntask-relevant constraints. Fine-tuning improves single-agent tasks dramatically\n(0.6% to 76.3%) but yields minimal multi-agent gains (1.5% to 5.5%), exposing\nfundamental architectural limitations. These findings demonstrate that embodied\nreasoning poses fundamentally different challenges than current models can\naddress, establishing OmniEAR as a rigorous benchmark for evaluating and\nadvancing embodied AI systems. Our code and data are included in the\nsupplementary materials and will be open-sourced upon acceptance.",
      "upvotes": 12,
      "discussionId": "6895644d48b0ae5ca2710d6e",
      "projectPage": "https://zju-real.github.io/OmniEmbodied/",
      "githubRepo": "https://github.com/ZJU-REAL/OmniEmbodied",
      "ai_summary": "OmniEAR evaluates language models' embodied reasoning capabilities in physical interactions, tool usage, and multi-agent coordination, revealing performance degradation under constraints and highlighting architectural limitations.",
      "ai_keywords": [
        "embodied agent reasoning",
        "OmniEAR",
        "text-based environment representation",
        "continuous physical properties",
        "complex spatial relationships",
        "tool reasoning",
        "implicit collaboration",
        "fine-tuning",
        "embodied AI systems"
      ],
      "githubStars": 25
    },
    "publishedAt": "2025-08-07T13:54:15.000Z",
    "title": "OmniEAR: Benchmarking Agent Reasoning in Embodied Tasks",
    "summary": "Large language models excel at abstract reasoning but their capacity for\nembodied agent reasoning remains largely unexplored. We present OmniEAR, a\ncomprehensive framework for evaluating how language models reason about\nphysical interactions, tool usage, and multi-agent coordination in embodied\ntasks. Unlike existing benchmarks that provide predefined tool sets or explicit\ncollaboration directives, OmniEAR requires agents to dynamically acquire\ncapabilities and autonomously determine coordination strategies based on task\ndemands. Through text-based environment representation, we model continuous\nphysical properties and complex spatial relationships across 1,500 scenarios\nspanning household and industrial domains. Our systematic evaluation reveals\nsevere performance degradation when models must reason from constraints: while\nachieving 85-96% success with explicit instructions, performance drops to\n56-85% for tool reasoning and 63-85% for implicit collaboration, with compound\ntasks showing over 50% failure rates. Surprisingly, complete environmental\ninformation degrades coordination performance, indicating models cannot filter\ntask-relevant constraints. Fine-tuning improves single-agent tasks dramatically\n(0.6% to 76.3%) but yields minimal multi-agent gains (1.5% to 5.5%), exposing\nfundamental architectural limitations. These findings demonstrate that embodied\nreasoning poses fundamentally different challenges than current models can\naddress, establishing OmniEAR as a rigorous benchmark for evaluating and\nadvancing embodied AI systems. Our code and data are included in the\nsupplementary materials and will be open-sourced upon acceptance.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.05614.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5e1058e9fcf41d740b69966d",
      "avatarUrl": "/avatars/ce74839ba871f2b54313a670a233ba82.svg",
      "fullname": "Yongliang Shen",
      "name": "tricktreat",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 26
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2508.07917",
      "authors": [
        {
          "_id": "689aa8affab6fdd2e52ac42b",
          "name": "Jason Lee",
          "hidden": false
        },
        {
          "_id": "689aa8affab6fdd2e52ac42c",
          "name": "Jiafei Duan",
          "hidden": false
        },
        {
          "_id": "689aa8affab6fdd2e52ac42d",
          "name": "Haoquan Fang",
          "hidden": false
        },
        {
          "_id": "689aa8affab6fdd2e52ac42e",
          "name": "Yuquan Deng",
          "hidden": false
        },
        {
          "_id": "689aa8affab6fdd2e52ac42f",
          "name": "Shuo Liu",
          "hidden": false
        },
        {
          "_id": "689aa8affab6fdd2e52ac430",
          "name": "Boyang Li",
          "hidden": false
        },
        {
          "_id": "689aa8affab6fdd2e52ac431",
          "name": "Bohan Fang",
          "hidden": false
        },
        {
          "_id": "689aa8affab6fdd2e52ac432",
          "name": "Jieyu Zhang",
          "hidden": false
        },
        {
          "_id": "689aa8affab6fdd2e52ac433",
          "name": "Yi Ru Wang",
          "hidden": false
        },
        {
          "_id": "689aa8affab6fdd2e52ac434",
          "name": "Sangho Lee",
          "hidden": false
        },
        {
          "_id": "689aa8affab6fdd2e52ac435",
          "name": "Winson Han",
          "hidden": false
        },
        {
          "_id": "689aa8affab6fdd2e52ac436",
          "name": "Wilbert Pumacay",
          "hidden": false
        },
        {
          "_id": "689aa8affab6fdd2e52ac437",
          "name": "Angelica Wu",
          "hidden": false
        },
        {
          "_id": "689aa8affab6fdd2e52ac438",
          "name": "Rose Hendrix",
          "hidden": false
        },
        {
          "_id": "689aa8affab6fdd2e52ac439",
          "name": "Karen Farley",
          "hidden": false
        },
        {
          "_id": "689aa8affab6fdd2e52ac43a",
          "name": "Eli VanderBilt",
          "hidden": false
        },
        {
          "_id": "689aa8affab6fdd2e52ac43b",
          "name": "Ali Farhadi",
          "hidden": false
        },
        {
          "_id": "689aa8affab6fdd2e52ac43c",
          "name": "Dieter Fox",
          "hidden": false
        },
        {
          "_id": "689aa8affab6fdd2e52ac43d",
          "name": "Ranjay Krishna",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/632b42626110e37dba3d5bcb/IjiimEs8QhghdkWHmhrsu.png"
      ],
      "publishedAt": "2025-08-11T12:32:45.000Z",
      "submittedOnDailyAt": "2025-08-12T01:08:01.584Z",
      "title": "MolmoAct: Action Reasoning Models that can Reason in Space",
      "submittedOnDailyBy": {
        "_id": "632b42626110e37dba3d5bcb",
        "avatarUrl": "/avatars/ca70a15def71ee84f4f149db5e954843.svg",
        "isPro": false,
        "fullname": "Duan",
        "user": "Jiafei1224",
        "type": "user"
      },
      "summary": "Reasoning is central to purposeful action, yet most robotic foundation models\nmap perception and instructions directly to control, which limits adaptability,\ngeneralization, and semantic grounding. We introduce Action Reasoning Models\n(ARMs), a class of vision-language-action models that integrate perception,\nplanning, and control through a structured three-stage pipeline. Our model,\nMolmoAct, encodes observations and instructions into depth-aware perception\ntokens, generates mid-level spatial plans as editable trajectory traces, and\npredicts precise low-level actions, enabling explainable and steerable\nbehavior. MolmoAct-7B-D achieves strong performance across simulation and\nreal-world settings: 70.5% zero-shot accuracy on SimplerEnv Visual Matching\ntasks, surpassing closed-source Pi-0 and GR00T N1; 86.6% average success on\nLIBERO, including an additional 6.3% gain over ThinkAct on long-horizon tasks;\nand in real-world fine-tuning, an additional 10% (single-arm) and an additional\n22.7% (bimanual) task progression over Pi-0-FAST. It also outperforms baselines\nby an additional 23.3% on out-of-distribution generalization and achieves top\nhuman-preference scores for open-ended instruction following and trajectory\nsteering. Furthermore, we release, for the first time, the MolmoAct Dataset --\na mid-training robot dataset comprising over 10,000 high quality robot\ntrajectories across diverse scenarios and tasks. Training with this dataset\nyields an average 5.5% improvement in general performance over the base model.\nWe release all model weights, training code, our collected dataset, and our\naction reasoning dataset, establishing MolmoAct as both a state-of-the-art\nrobotics foundation model and an open blueprint for building ARMs that\ntransform perception into purposeful action through structured reasoning.\nBlogpost: https://allenai.org/blog/molmoact",
      "upvotes": 11,
      "discussionId": "689aa8b0fab6fdd2e52ac43e",
      "ai_summary": "Action Reasoning Models (ARMs) integrate perception, planning, and control to enable adaptable and explainable robotic behavior, achieving superior performance across various tasks and settings.",
      "ai_keywords": [
        "Action Reasoning Models",
        "ARMs",
        "MolmoAct",
        "depth-aware perception tokens",
        "mid-level spatial plans",
        "trajectory traces",
        "low-level actions",
        "SimplerEnv Visual Matching",
        "LIBERO",
        "ThinkAct",
        "MolmoAct Dataset",
        "open-ended instruction following",
        "trajectory steering"
      ]
    },
    "publishedAt": "2025-08-11T08:32:45.000Z",
    "title": "MolmoAct: Action Reasoning Models that can Reason in Space",
    "summary": "Reasoning is central to purposeful action, yet most robotic foundation models\nmap perception and instructions directly to control, which limits adaptability,\ngeneralization, and semantic grounding. We introduce Action Reasoning Models\n(ARMs), a class of vision-language-action models that integrate perception,\nplanning, and control through a structured three-stage pipeline. Our model,\nMolmoAct, encodes observations and instructions into depth-aware perception\ntokens, generates mid-level spatial plans as editable trajectory traces, and\npredicts precise low-level actions, enabling explainable and steerable\nbehavior. MolmoAct-7B-D achieves strong performance across simulation and\nreal-world settings: 70.5% zero-shot accuracy on SimplerEnv Visual Matching\ntasks, surpassing closed-source Pi-0 and GR00T N1; 86.6% average success on\nLIBERO, including an additional 6.3% gain over ThinkAct on long-horizon tasks;\nand in real-world fine-tuning, an additional 10% (single-arm) and an additional\n22.7% (bimanual) task progression over Pi-0-FAST. It also outperforms baselines\nby an additional 23.3% on out-of-distribution generalization and achieves top\nhuman-preference scores for open-ended instruction following and trajectory\nsteering. Furthermore, we release, for the first time, the MolmoAct Dataset --\na mid-training robot dataset comprising over 10,000 high quality robot\ntrajectories across diverse scenarios and tasks. Training with this dataset\nyields an average 5.5% improvement in general performance over the base model.\nWe release all model weights, training code, our collected dataset, and our\naction reasoning dataset, establishing MolmoAct as both a state-of-the-art\nrobotics foundation model and an open blueprint for building ARMs that\ntransform perception into purposeful action through structured reasoning.\nBlogpost: https://allenai.org/blog/molmoact",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/632b42626110e37dba3d5bcb/IjiimEs8QhghdkWHmhrsu.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.07917.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "632b42626110e37dba3d5bcb",
      "avatarUrl": "/avatars/ca70a15def71ee84f4f149db5e954843.svg",
      "fullname": "Duan",
      "name": "Jiafei1224",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.06026",
      "authors": [
        {
          "_id": "689abd09fab6fdd2e52ac4c4",
          "name": "Yidong Wang",
          "hidden": false
        },
        {
          "_id": "689abd09fab6fdd2e52ac4c5",
          "name": "Xin Wang",
          "hidden": false
        },
        {
          "_id": "689abd09fab6fdd2e52ac4c6",
          "name": "Cunxiang Wang",
          "hidden": false
        },
        {
          "_id": "689abd09fab6fdd2e52ac4c7",
          "name": "Junfeng Fang",
          "hidden": false
        },
        {
          "_id": "689abd09fab6fdd2e52ac4c8",
          "name": "Qiufeng Wang",
          "hidden": false
        },
        {
          "_id": "689abd09fab6fdd2e52ac4c9",
          "name": "Jianing Chu",
          "hidden": false
        },
        {
          "_id": "689abd09fab6fdd2e52ac4ca",
          "name": "Xuran Meng",
          "hidden": false
        },
        {
          "_id": "689abd09fab6fdd2e52ac4cb",
          "name": "Shuxun Yang",
          "hidden": false
        },
        {
          "_id": "689abd09fab6fdd2e52ac4cc",
          "name": "Libo Qin",
          "hidden": false
        },
        {
          "_id": "689abd09fab6fdd2e52ac4cd",
          "name": "Yue Zhang",
          "hidden": false
        },
        {
          "_id": "689abd09fab6fdd2e52ac4ce",
          "name": "Wei Ye",
          "hidden": false
        },
        {
          "_id": "689abd09fab6fdd2e52ac4cf",
          "name": "Shikun Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-08T05:25:54.000Z",
      "submittedOnDailyAt": "2025-08-12T02:44:36.603Z",
      "title": "Temporal Self-Rewarding Language Models: Decoupling Chosen-Rejected via\n  Past-Future",
      "submittedOnDailyBy": {
        "_id": "6226f23d7551f131b4eba61d",
        "avatarUrl": "/avatars/4b135465799760468999336cef39d768.svg",
        "isPro": false,
        "fullname": "Yidong Wang",
        "user": "qianlanwyd",
        "type": "user"
      },
      "summary": "Self-Rewarding Language Models propose an architecture in which the Large\nLanguage Models(LLMs) both generates responses and evaluates its own outputs\nvia LLM-as-a-Judge prompting, dynamically improving its generative capabilities\nthrough iterative Direct Preference Optimization (DPO). However, our analysis\nreveals a critical limitation in existing Self-Rewarding paradigms: the\nsynchronized improvement of chosen and rejected responses progressively narrows\nthe representational difference between contrasting samples, undermining\neffective preference learning. We propose Temporal Self-Rewarding\nLanguage Models that strategically coordinate past, present, and future model\ngenerations to sustain learning signals. Our dual-phase framework introduces:\n(1) Anchored Rejection - fixing rejected responses using the past\ninitial model's outputs and (2) Future-Guided Chosen - dynamically\ncurating chosen samples using next-generation model predictions. Extensive\nexperiments across three model families (Llama, Qwen, Mistral) and different\nmodel sizes (Llama3B/8B/70B) demonstrate significant improvements when trained\nwith our method compared to Self-Rewarding using same computation resources.\nFor example, Llama3.1-8B reaches a 29.44 win rate on AlpacaEval 2.0 with our\nmethod, outperforming the Self-Rewarding baseline (19.69) by 9.75. Notably, our\nmethod also demonstrates superior out-of-distribution generalization across\nmathematical reasoning (GSM8K), knowledge-based QA (ARC, TruthfulQA), and code\ngeneration (HumanEval) tasks, even though we do not specifically collect such\ntraining data.",
      "upvotes": 9,
      "discussionId": "689abd09fab6fdd2e52ac4d0",
      "ai_summary": "Temporal Self-Rewarding Language Models improve generative capabilities by strategically using past and future model outputs, enhancing preference learning and out-of-distribution generalization.",
      "ai_keywords": [
        "Self-Rewarding Language Models",
        "Large Language Models",
        "LLM-as-a-Judge",
        "Direct Preference Optimization",
        "Anchored Rejection",
        "Future-Guided Chosen",
        "Temporal Self-Rewarding Language Models",
        "AlpacaEval",
        "GSM8K",
        "ARC",
        "TruthfulQA",
        "HumanEval"
      ]
    },
    "publishedAt": "2025-08-08T01:25:54.000Z",
    "title": "Temporal Self-Rewarding Language Models: Decoupling Chosen-Rejected via\n  Past-Future",
    "summary": "Self-Rewarding Language Models propose an architecture in which the Large\nLanguage Models(LLMs) both generates responses and evaluates its own outputs\nvia LLM-as-a-Judge prompting, dynamically improving its generative capabilities\nthrough iterative Direct Preference Optimization (DPO). However, our analysis\nreveals a critical limitation in existing Self-Rewarding paradigms: the\nsynchronized improvement of chosen and rejected responses progressively narrows\nthe representational difference between contrasting samples, undermining\neffective preference learning. We propose Temporal Self-Rewarding\nLanguage Models that strategically coordinate past, present, and future model\ngenerations to sustain learning signals. Our dual-phase framework introduces:\n(1) Anchored Rejection - fixing rejected responses using the past\ninitial model's outputs and (2) Future-Guided Chosen - dynamically\ncurating chosen samples using next-generation model predictions. Extensive\nexperiments across three model families (Llama, Qwen, Mistral) and different\nmodel sizes (Llama3B/8B/70B) demonstrate significant improvements when trained\nwith our method compared to Self-Rewarding using same computation resources.\nFor example, Llama3.1-8B reaches a 29.44 win rate on AlpacaEval 2.0 with our\nmethod, outperforming the Self-Rewarding baseline (19.69) by 9.75. Notably, our\nmethod also demonstrates superior out-of-distribution generalization across\nmathematical reasoning (GSM8K), knowledge-based QA (ARC, TruthfulQA), and code\ngeneration (HumanEval) tasks, even though we do not specifically collect such\ntraining data.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.06026.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6226f23d7551f131b4eba61d",
      "avatarUrl": "/avatars/4b135465799760468999336cef39d768.svg",
      "fullname": "Yidong Wang",
      "name": "qianlanwyd",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.08189",
      "authors": [
        {
          "_id": "689ad956fab6fdd2e52ac54c",
          "name": "Weijia Wu",
          "hidden": false
        },
        {
          "_id": "689ad956fab6fdd2e52ac54d",
          "name": "Chen Gao",
          "hidden": false
        },
        {
          "_id": "689ad956fab6fdd2e52ac54e",
          "name": "Joya Chen",
          "hidden": false
        },
        {
          "_id": "689ad956fab6fdd2e52ac54f",
          "name": "Kevin Qinghong Lin",
          "hidden": false
        },
        {
          "_id": "689ad956fab6fdd2e52ac550",
          "name": "Qingwei Meng",
          "hidden": false
        },
        {
          "_id": "689ad956fab6fdd2e52ac551",
          "name": "Yiming Zhang",
          "hidden": false
        },
        {
          "_id": "689ad956fab6fdd2e52ac552",
          "name": "Yuke Qiu",
          "hidden": false
        },
        {
          "_id": "689ad956fab6fdd2e52ac553",
          "name": "Hong Zhou",
          "hidden": false
        },
        {
          "_id": "689ad956fab6fdd2e52ac554",
          "name": "Mike Zheng Shou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-11T17:08:55.000Z",
      "submittedOnDailyAt": "2025-08-12T04:35:26.917Z",
      "title": "Reinforcement Learning in Vision: A Survey",
      "submittedOnDailyBy": {
        "_id": "6345a93afe134dfd7a0cfabd",
        "avatarUrl": "/avatars/65130ce06b1c72ab1066678419731d88.svg",
        "isPro": false,
        "fullname": "wu weijia",
        "user": "weijiawu",
        "type": "user"
      },
      "summary": "Recent advances at the intersection of reinforcement learning (RL) and visual\nintelligence have enabled agents that not only perceive complex visual scenes\nbut also reason, generate, and act within them. This survey offers a critical\nand up-to-date synthesis of the field. We first formalize visual RL problems\nand trace the evolution of policy-optimization strategies from RLHF to\nverifiable reward paradigms, and from Proximal Policy Optimization to Group\nRelative Policy Optimization. We then organize more than 200 representative\nworks into four thematic pillars: multi-modal large language models, visual\ngeneration, unified model frameworks, and vision-language-action models. For\neach pillar we examine algorithmic design, reward engineering, benchmark\nprogress, and we distill trends such as curriculum-driven training,\npreference-aligned diffusion, and unified reward modeling. Finally, we review\nevaluation protocols spanning set-level fidelity, sample-level preference, and\nstate-level stability, and we identify open challenges that include sample\nefficiency, generalization, and safe deployment. Our goal is to provide\nresearchers and practitioners with a coherent map of the rapidly expanding\nlandscape of visual RL and to highlight promising directions for future\ninquiry. Resources are available at:\nhttps://github.com/weijiawu/Awesome-Visual-Reinforcement-Learning.",
      "upvotes": 6,
      "discussionId": "689ad956fab6fdd2e52ac555",
      "ai_summary": "This survey synthesizes recent advancements in visual reinforcement learning, covering policy optimization strategies, thematic pillars, and evaluation protocols, while highlighting open challenges.",
      "ai_keywords": [
        "reinforcement learning",
        "visual intelligence",
        "visual RL",
        "policy-optimization strategies",
        "RLHF",
        "verifiable reward paradigms",
        "Proximal Policy Optimization",
        "Group Relative Policy Optimization",
        "multi-modal large language models",
        "visual generation",
        "unified model frameworks",
        "vision-language-action models",
        "curriculum-driven training",
        "preference-aligned diffusion",
        "unified reward modeling",
        "set-level fidelity",
        "sample-level preference",
        "state-level stability",
        "sample efficiency",
        "generalization",
        "safe deployment"
      ]
    },
    "publishedAt": "2025-08-11T13:08:55.000Z",
    "title": "Reinforcement Learning in Vision: A Survey",
    "summary": "Recent advances at the intersection of reinforcement learning (RL) and visual\nintelligence have enabled agents that not only perceive complex visual scenes\nbut also reason, generate, and act within them. This survey offers a critical\nand up-to-date synthesis of the field. We first formalize visual RL problems\nand trace the evolution of policy-optimization strategies from RLHF to\nverifiable reward paradigms, and from Proximal Policy Optimization to Group\nRelative Policy Optimization. We then organize more than 200 representative\nworks into four thematic pillars: multi-modal large language models, visual\ngeneration, unified model frameworks, and vision-language-action models. For\neach pillar we examine algorithmic design, reward engineering, benchmark\nprogress, and we distill trends such as curriculum-driven training,\npreference-aligned diffusion, and unified reward modeling. Finally, we review\nevaluation protocols spanning set-level fidelity, sample-level preference, and\nstate-level stability, and we identify open challenges that include sample\nefficiency, generalization, and safe deployment. Our goal is to provide\nresearchers and practitioners with a coherent map of the rapidly expanding\nlandscape of visual RL and to highlight promising directions for future\ninquiry. Resources are available at:\nhttps://github.com/weijiawu/Awesome-Visual-Reinforcement-Learning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.08189.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6345a93afe134dfd7a0cfabd",
      "avatarUrl": "/avatars/65130ce06b1c72ab1066678419731d88.svg",
      "fullname": "wu weijia",
      "name": "weijiawu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.08134",
      "authors": [
        {
          "_id": "689aabdffab6fdd2e52ac471",
          "name": "Zeqian Long",
          "hidden": false
        },
        {
          "_id": "689aabdffab6fdd2e52ac472",
          "name": "Mingzhe Zheng",
          "hidden": false
        },
        {
          "_id": "689aabdffab6fdd2e52ac473",
          "name": "Kunyu Feng",
          "hidden": false
        },
        {
          "_id": "689aabdffab6fdd2e52ac474",
          "name": "Xinhua Zhang",
          "hidden": false
        },
        {
          "_id": "689aabdffab6fdd2e52ac475",
          "name": "Hongyu Liu",
          "hidden": false
        },
        {
          "_id": "689aabdffab6fdd2e52ac476",
          "name": "Harry Yang",
          "hidden": false
        },
        {
          "_id": "689aabdffab6fdd2e52ac477",
          "name": "Linfeng Zhang",
          "hidden": false
        },
        {
          "_id": "689aabdffab6fdd2e52ac478",
          "name": "Qifeng Chen",
          "hidden": false
        },
        {
          "_id": "689aabdffab6fdd2e52ac479",
          "name": "Yue Ma",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62f0c4abe2999b231e5a893c/HMqtPF7sxMLPS7fh0CDGV.jpeg"
      ],
      "publishedAt": "2025-08-11T16:10:00.000Z",
      "submittedOnDailyAt": "2025-08-12T01:23:21.523Z",
      "title": "Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided\n  Region Control",
      "submittedOnDailyBy": {
        "_id": "62f0c4abe2999b231e5a893c",
        "avatarUrl": "/avatars/90da268b877a7ffe6665075c84018a83.svg",
        "isPro": false,
        "fullname": "Mingzhe Zheng",
        "user": "Dunge0nMaster",
        "type": "user"
      },
      "summary": "While recent flow-based image editing models demonstrate general-purpose\ncapabilities across diverse tasks, they often struggle to specialize in\nchallenging scenarios -- particularly those involving large-scale shape\ntransformations. When performing such structural edits, these methods either\nfail to achieve the intended shape change or inadvertently alter non-target\nregions, resulting in degraded background quality. We propose\nFollow-Your-Shape, a training-free and mask-free framework that supports\nprecise and controllable editing of object shapes while strictly preserving\nnon-target content. Motivated by the divergence between inversion and editing\ntrajectories, we compute a Trajectory Divergence Map (TDM) by comparing\ntoken-wise velocity differences between the inversion and denoising paths. The\nTDM enables precise localization of editable regions and guides a Scheduled KV\nInjection mechanism that ensures stable and faithful editing. To facilitate a\nrigorous evaluation, we introduce ReShapeBench, a new benchmark comprising 120\nnew images and enriched prompt pairs specifically curated for shape-aware\nediting. Experiments demonstrate that our method achieves superior editability\nand visual fidelity, particularly in tasks requiring large-scale shape\nreplacement.",
      "upvotes": 5,
      "discussionId": "689aabe0fab6fdd2e52ac47a",
      "ai_summary": "Follow-Your-Shape framework uses a Trajectory Divergence Map and Scheduled KV Injection to enable precise and controllable shape editing in images while preserving non-target content.",
      "ai_keywords": [
        "flow-based image editing",
        "Trajectory Divergence Map",
        "token-wise velocity differences",
        "Scheduled KV Injection",
        "ReShapeBench",
        "shape-aware editing"
      ]
    },
    "publishedAt": "2025-08-11T12:10:00.000Z",
    "title": "Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided\n  Region Control",
    "summary": "While recent flow-based image editing models demonstrate general-purpose\ncapabilities across diverse tasks, they often struggle to specialize in\nchallenging scenarios -- particularly those involving large-scale shape\ntransformations. When performing such structural edits, these methods either\nfail to achieve the intended shape change or inadvertently alter non-target\nregions, resulting in degraded background quality. We propose\nFollow-Your-Shape, a training-free and mask-free framework that supports\nprecise and controllable editing of object shapes while strictly preserving\nnon-target content. Motivated by the divergence between inversion and editing\ntrajectories, we compute a Trajectory Divergence Map (TDM) by comparing\ntoken-wise velocity differences between the inversion and denoising paths. The\nTDM enables precise localization of editable regions and guides a Scheduled KV\nInjection mechanism that ensures stable and faithful editing. To facilitate a\nrigorous evaluation, we introduce ReShapeBench, a new benchmark comprising 120\nnew images and enriched prompt pairs specifically curated for shape-aware\nediting. Experiments demonstrate that our method achieves superior editability\nand visual fidelity, particularly in tasks requiring large-scale shape\nreplacement.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62f0c4abe2999b231e5a893c/HMqtPF7sxMLPS7fh0CDGV.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.08134.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62f0c4abe2999b231e5a893c",
      "avatarUrl": "/avatars/90da268b877a7ffe6665075c84018a83.svg",
      "fullname": "Mingzhe Zheng",
      "name": "Dunge0nMaster",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.07101",
      "authors": [
        {
          "_id": "689ab5bffab6fdd2e52ac492",
          "name": "Lijie Yang",
          "hidden": false
        },
        {
          "_id": "689ab5bffab6fdd2e52ac493",
          "name": "Zhihao Zhang",
          "hidden": false
        },
        {
          "_id": "689ab5bffab6fdd2e52ac494",
          "name": "Arti Jain",
          "hidden": false
        },
        {
          "_id": "689ab5bffab6fdd2e52ac495",
          "name": "Shijie Cao",
          "hidden": false
        },
        {
          "_id": "689ab5bffab6fdd2e52ac496",
          "name": "Baihong Yuan",
          "hidden": false
        },
        {
          "_id": "689ab5bffab6fdd2e52ac497",
          "name": "Yiwei Chen",
          "hidden": false
        },
        {
          "_id": "689ab5bffab6fdd2e52ac498",
          "name": "Zhihao Jia",
          "hidden": false
        },
        {
          "_id": "689ab5bffab6fdd2e52ac499",
          "name": "Ravi Netravali",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-09T21:10:33.000Z",
      "submittedOnDailyAt": "2025-08-12T02:37:36.025Z",
      "title": "Less Is More: Training-Free Sparse Attention with Global Locality for\n  Efficient Reasoning",
      "submittedOnDailyBy": {
        "_id": "65d8523167819fdde61c9e9d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/7CrWbLndFzd3YZDke1OgK.jpeg",
        "isPro": false,
        "fullname": "Lijie Yang",
        "user": "drkylj",
        "type": "user"
      },
      "summary": "Large reasoning models achieve strong performance through test-time scaling\nbut incur substantial computational overhead, particularly from excessive token\ngeneration when processing short input prompts. While sparse attention\nmechanisms can reduce latency and memory usage, existing approaches suffer from\nsignificant accuracy degradation due to accumulated errors during\nlong-generation reasoning. These methods generally require either high token\nretention rates or expensive retraining. We introduce LessIsMore, a\ntraining-free sparse attention mechanism for reasoning tasks, which leverages\nglobal attention patterns rather than relying on traditional head-specific\nlocal optimizations. LessIsMore aggregates token selections from local\nattention heads with recent contextual information, enabling unified cross-head\ntoken ranking for future decoding layers. This unified selection improves\ngeneralization and efficiency by avoiding the need to maintain separate token\nsubsets per head. Evaluation across diverse reasoning tasks and benchmarks\nshows that LessIsMore preserves -- and in some cases improves -- accuracy while\nachieving a 1.1times average decoding speed-up compared to full attention.\nMoreover, LessIsMore attends to 2times fewer tokens without accuracy loss,\nachieving a 1.13times end-to-end speed-up compared to existing sparse\nattention methods.",
      "upvotes": 4,
      "discussionId": "689ab5bffab6fdd2e52ac49a",
      "ai_summary": "LessIsMore is a training-free sparse attention mechanism that improves efficiency and generalization in reasoning tasks by aggregating token selections from local attention heads.",
      "ai_keywords": [
        "sparse attention mechanisms",
        "global attention patterns",
        "local attention heads",
        "token ranking",
        "decoding speed-up",
        "end-to-end speed-up"
      ]
    },
    "publishedAt": "2025-08-09T17:10:33.000Z",
    "title": "Less Is More: Training-Free Sparse Attention with Global Locality for\n  Efficient Reasoning",
    "summary": "Large reasoning models achieve strong performance through test-time scaling\nbut incur substantial computational overhead, particularly from excessive token\ngeneration when processing short input prompts. While sparse attention\nmechanisms can reduce latency and memory usage, existing approaches suffer from\nsignificant accuracy degradation due to accumulated errors during\nlong-generation reasoning. These methods generally require either high token\nretention rates or expensive retraining. We introduce LessIsMore, a\ntraining-free sparse attention mechanism for reasoning tasks, which leverages\nglobal attention patterns rather than relying on traditional head-specific\nlocal optimizations. LessIsMore aggregates token selections from local\nattention heads with recent contextual information, enabling unified cross-head\ntoken ranking for future decoding layers. This unified selection improves\ngeneralization and efficiency by avoiding the need to maintain separate token\nsubsets per head. Evaluation across diverse reasoning tasks and benchmarks\nshows that LessIsMore preserves -- and in some cases improves -- accuracy while\nachieving a 1.1times average decoding speed-up compared to full attention.\nMoreover, LessIsMore attends to 2times fewer tokens without accuracy loss,\nachieving a 1.13times end-to-end speed-up compared to existing sparse\nattention methods.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.07101.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "65d8523167819fdde61c9e9d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/7CrWbLndFzd3YZDke1OgK.jpeg",
      "fullname": "Lijie Yang",
      "name": "drkylj",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.08221",
      "authors": [
        {
          "_id": "689aefacfab6fdd2e52ac5ef",
          "name": "Zihe Liu",
          "hidden": false
        },
        {
          "_id": "689aefacfab6fdd2e52ac5f0",
          "name": "Jiashun Liu",
          "hidden": false
        },
        {
          "_id": "689aefacfab6fdd2e52ac5f1",
          "name": "Yancheng He",
          "hidden": false
        },
        {
          "_id": "689aefacfab6fdd2e52ac5f2",
          "name": "Weixun Wang",
          "hidden": false
        },
        {
          "_id": "689aefacfab6fdd2e52ac5f3",
          "name": "Jiaheng Liu",
          "hidden": false
        },
        {
          "_id": "689aefacfab6fdd2e52ac5f4",
          "name": "Ling Pan",
          "hidden": false
        },
        {
          "_id": "689aefacfab6fdd2e52ac5f5",
          "name": "Xinyu Hu",
          "hidden": false
        },
        {
          "_id": "689aefacfab6fdd2e52ac5f6",
          "name": "Shaopan Xiong",
          "hidden": false
        },
        {
          "_id": "689aefacfab6fdd2e52ac5f7",
          "name": "Ju Huang",
          "hidden": false
        },
        {
          "_id": "689aefacfab6fdd2e52ac5f8",
          "name": "Jian Hu",
          "hidden": false
        },
        {
          "_id": "689aefacfab6fdd2e52ac5f9",
          "name": "Shengyi Huang",
          "hidden": false
        },
        {
          "_id": "689aefacfab6fdd2e52ac5fa",
          "name": "Siran Yang",
          "hidden": false
        },
        {
          "_id": "689aefacfab6fdd2e52ac5fb",
          "name": "Jiamang Wang",
          "hidden": false
        },
        {
          "_id": "689aefacfab6fdd2e52ac5fc",
          "name": "Wenbo Su",
          "hidden": false
        },
        {
          "_id": "689aefacfab6fdd2e52ac5fd",
          "name": "Bo Zheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-11T17:39:45.000Z",
      "submittedOnDailyAt": "2025-08-12T06:11:01.790Z",
      "title": "Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning",
      "submittedOnDailyBy": {
        "_id": "65377c30e48353201e6fdda0",
        "avatarUrl": "/avatars/a8f803b6f2e598eaee9c52c0d2ddfc16.svg",
        "isPro": false,
        "fullname": "Jiaheng Liu",
        "user": "CheeryLJH",
        "type": "user"
      },
      "summary": "Reinforcement learning for LLM reasoning has rapidly emerged as a prominent\nresearch area, marked by a significant surge in related studies on both\nalgorithmic innovations and practical applications. Despite this progress,\nseveral critical challenges remain, including the absence of standardized\nguidelines for employing RL techniques and a fragmented understanding of their\nunderlying mechanisms. Additionally, inconsistent experimental settings,\nvariations in training data, and differences in model initialization have led\nto conflicting conclusions, obscuring the key characteristics of these\ntechniques and creating confusion among practitioners when selecting\nappropriate techniques. This paper systematically reviews widely adopted RL\ntechniques through rigorous reproductions and isolated evaluations within a\nunified open-source framework. We analyze the internal mechanisms, applicable\nscenarios, and core principles of each technique through fine-grained\nexperiments, including datasets of varying difficulty, model sizes, and\narchitectures. Based on these insights, we present clear guidelines for\nselecting RL techniques tailored to specific setups, and provide a reliable\nroadmap for practitioners navigating the RL for the LLM domain. Finally, we\nreveal that a minimalist combination of two techniques can unlock the learning\ncapability of critic-free policies using vanilla PPO loss. The results\ndemonstrate that our simple combination consistently improves performance,\nsurpassing strategies like GRPO and DAPO.",
      "upvotes": 3,
      "discussionId": "689aefacfab6fdd2e52ac5fe",
      "ai_summary": "A systematic review of reinforcement learning techniques for large language model reasoning reveals clear guidelines and demonstrates that a minimalist combination of techniques can improve performance over existing strategies.",
      "ai_keywords": [
        "reinforcement learning",
        "LLM reasoning",
        "RL techniques",
        "standardized guidelines",
        "experimental settings",
        "training data",
        "model initialization",
        "internal mechanisms",
        "applicable scenarios",
        "core principles",
        "datasets",
        "model sizes",
        "architectures",
        "critic-free policies",
        "vanilla PPO loss",
        "GRPO",
        "DAPO"
      ]
    },
    "publishedAt": "2025-08-11T13:39:45.000Z",
    "title": "Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning",
    "summary": "Reinforcement learning for LLM reasoning has rapidly emerged as a prominent\nresearch area, marked by a significant surge in related studies on both\nalgorithmic innovations and practical applications. Despite this progress,\nseveral critical challenges remain, including the absence of standardized\nguidelines for employing RL techniques and a fragmented understanding of their\nunderlying mechanisms. Additionally, inconsistent experimental settings,\nvariations in training data, and differences in model initialization have led\nto conflicting conclusions, obscuring the key characteristics of these\ntechniques and creating confusion among practitioners when selecting\nappropriate techniques. This paper systematically reviews widely adopted RL\ntechniques through rigorous reproductions and isolated evaluations within a\nunified open-source framework. We analyze the internal mechanisms, applicable\nscenarios, and core principles of each technique through fine-grained\nexperiments, including datasets of varying difficulty, model sizes, and\narchitectures. Based on these insights, we present clear guidelines for\nselecting RL techniques tailored to specific setups, and provide a reliable\nroadmap for practitioners navigating the RL for the LLM domain. Finally, we\nreveal that a minimalist combination of two techniques can unlock the learning\ncapability of critic-free policies using vanilla PPO loss. The results\ndemonstrate that our simple combination consistently improves performance,\nsurpassing strategies like GRPO and DAPO.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.08221.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65377c30e48353201e6fdda0",
      "avatarUrl": "/avatars/a8f803b6f2e598eaee9c52c0d2ddfc16.svg",
      "fullname": "Jiaheng Liu",
      "name": "CheeryLJH",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.06426",
      "authors": [
        {
          "_id": "689ae1cffab6fdd2e52ac58a",
          "name": "Youguang Xing",
          "hidden": false
        },
        {
          "_id": "689ae1cffab6fdd2e52ac58b",
          "name": "Xu Luo",
          "hidden": false
        },
        {
          "_id": "689ae1cffab6fdd2e52ac58c",
          "name": "Junlin Xie",
          "hidden": false
        },
        {
          "_id": "689ae1cffab6fdd2e52ac58d",
          "name": "Lianli Gao",
          "hidden": false
        },
        {
          "_id": "689ae1cffab6fdd2e52ac58e",
          "name": "Hengtao Shen",
          "hidden": false
        },
        {
          "_id": "689ae1cffab6fdd2e52ac58f",
          "name": "Jingkuan Song",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64d19b5431c655ff8aa35081/5TBtciRXQf9CfUMCC2Pr-.mp4"
      ],
      "publishedAt": "2025-08-08T16:14:01.000Z",
      "submittedOnDailyAt": "2025-08-12T05:15:20.195Z",
      "title": "Shortcut Learning in Generalist Robot Policies: The Role of Dataset\n  Diversity and Fragmentation",
      "submittedOnDailyBy": {
        "_id": "64d19b5431c655ff8aa35081",
        "avatarUrl": "/avatars/5acdbe28ff841b0df32c3948799ac8f2.svg",
        "isPro": false,
        "fullname": "Xu Luo",
        "user": "FrankLuox",
        "type": "user"
      },
      "summary": "Generalist robot policies trained on large-scale datasets such as Open\nX-Embodiment (OXE) demonstrate strong performance across a wide range of tasks.\nHowever, they often struggle to generalize beyond the distribution of their\ntraining data. In this paper, we investigate the underlying cause of this\nlimited generalization capability. We identify shortcut learning -- the\nreliance on task-irrelevant features -- as a key impediment to generalization.\nThrough comprehensive theoretical and empirical analysis, we uncover two\nprimary contributors to shortcut learning: (1) limited diversity within\nindividual sub-datasets, and (2) significant distributional disparities across\nsub-datasets, leading to dataset fragmentation. These issues arise from the\ninherent structure of large-scale datasets like OXE, which are typically\ncomposed of multiple sub-datasets collected independently across varied\nenvironments and embodiments. Our findings provide critical insights into\ndataset collection strategies that can reduce shortcut learning and enhance the\ngeneralization ability of generalist robot policies. Moreover, in scenarios\nwhere acquiring new large-scale data is impractical, we demonstrate that\ncarefully selected robotic data augmentation strategies can effectively reduce\nshortcut learning in existing offline datasets, thereby improving\ngeneralization capabilities of generalist robot policies, e.g., pi_0, in\nboth simulation and real-world environments. More information at\nhttps://lucky-light-sun.github.io/proj/shortcut-learning-in-grps/.",
      "upvotes": 2,
      "discussionId": "689ae1cffab6fdd2e52ac590",
      "projectPage": "https://lucky-light-sun.github.io/proj/shortcut-learning-in-grps/",
      "ai_summary": "Shortcut learning in generalist robot policies trained on large-scale datasets limits generalization, and this can be mitigated through improved dataset collection and data augmentation strategies.",
      "ai_keywords": [
        "shortcut learning",
        "generalist robot policies",
        "Open X-Embodiment",
        "dataset fragmentation",
        "robotic data augmentation"
      ]
    },
    "publishedAt": "2025-08-08T12:14:01.000Z",
    "title": "Shortcut Learning in Generalist Robot Policies: The Role of Dataset\n  Diversity and Fragmentation",
    "summary": "Generalist robot policies trained on large-scale datasets such as Open\nX-Embodiment (OXE) demonstrate strong performance across a wide range of tasks.\nHowever, they often struggle to generalize beyond the distribution of their\ntraining data. In this paper, we investigate the underlying cause of this\nlimited generalization capability. We identify shortcut learning -- the\nreliance on task-irrelevant features -- as a key impediment to generalization.\nThrough comprehensive theoretical and empirical analysis, we uncover two\nprimary contributors to shortcut learning: (1) limited diversity within\nindividual sub-datasets, and (2) significant distributional disparities across\nsub-datasets, leading to dataset fragmentation. These issues arise from the\ninherent structure of large-scale datasets like OXE, which are typically\ncomposed of multiple sub-datasets collected independently across varied\nenvironments and embodiments. Our findings provide critical insights into\ndataset collection strategies that can reduce shortcut learning and enhance the\ngeneralization ability of generalist robot policies. Moreover, in scenarios\nwhere acquiring new large-scale data is impractical, we demonstrate that\ncarefully selected robotic data augmentation strategies can effectively reduce\nshortcut learning in existing offline datasets, thereby improving\ngeneralization capabilities of generalist robot policies, e.g., pi_0, in\nboth simulation and real-world environments. More information at\nhttps://lucky-light-sun.github.io/proj/shortcut-learning-in-grps/.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64d19b5431c655ff8aa35081/5TBtciRXQf9CfUMCC2Pr-.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.06426.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64d19b5431c655ff8aa35081",
      "avatarUrl": "/avatars/5acdbe28ff841b0df32c3948799ac8f2.svg",
      "fullname": "Xu Luo",
      "name": "FrankLuox",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.07662",
      "authors": [
        {
          "_id": "689ad76cfab6fdd2e52ac544",
          "name": "Ihor Stepanov",
          "hidden": false
        },
        {
          "_id": "689ad76cfab6fdd2e52ac545",
          "name": "Mykhailo Shtopko",
          "hidden": false
        },
        {
          "_id": "689ad76cfab6fdd2e52ac546",
          "name": "Dmytro Vodianytskyi",
          "hidden": false
        },
        {
          "_id": "689ad76cfab6fdd2e52ac547",
          "name": "Oleksandr Lukashov",
          "hidden": false
        },
        {
          "_id": "689ad76cfab6fdd2e52ac548",
          "name": "Alexander Yavorskyi",
          "hidden": false
        },
        {
          "_id": "689ad76cfab6fdd2e52ac549",
          "name": "Mykyta Yaroshenko",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/5e6a3d4ea9afd5125d9ec064/bGVX6dwt_d9_-25Cor_CL.jpeg"
      ],
      "publishedAt": "2025-08-11T06:22:25.000Z",
      "submittedOnDailyAt": "2025-08-12T04:35:37.522Z",
      "title": "GLiClass: Generalist Lightweight Model for Sequence Classification Tasks",
      "submittedOnDailyBy": {
        "_id": "5e6a3d4ea9afd5125d9ec064",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1584020801691-noauth.jpeg",
        "isPro": true,
        "fullname": "Stefan Schweter",
        "user": "stefan-it",
        "type": "user"
      },
      "summary": "Classification is one of the most widespread tasks in AI applications,\nserving often as the first step in filtering, sorting, and categorizing data.\nSince modern AI systems must handle large volumes of input data and early\npipeline stages can propagate errors downstream, achieving high efficiency and\naccuracy is critical. Moreover, classification requirements can change\ndynamically based on user needs, necessitating models with strong zero-shot\ncapabilities. While generative LLMs have become mainstream for zero-shot\nclassification due to their versatility, they suffer from inconsistent\ninstruction following and computational inefficiency. Cross-encoders, commonly\nused as rerankers in RAG pipelines, face a different bottleneck: they must\nprocess text-label pairs sequentially, significantly reducing efficiency with\nlarge label sets. Embedding-based approaches offer good efficiency but struggle\nwith complex scenarios involving logical and semantic constraints. We propose\nGLiClass, a novel method that adapts the GLiNER architecture for sequence\nclassification tasks. Our approach achieves strong accuracy and efficiency\ncomparable to embedding-based methods, while maintaining the flexibility needed\nfor zero-shot and few-shot learning scenarios. Additionally, we adapted\nproximal policy optimization (PPO) for multi-label text classification,\nenabling training classifiers in data-sparse conditions or from human feedback.",
      "upvotes": 1,
      "discussionId": "689ad76cfab6fdd2e52ac54a",
      "ai_summary": "GLiClass, an adaptation of GLiNER, achieves efficient and accurate sequence classification with zero-shot and few-shot capabilities, and PPO is adapted for multi-label text classification in data-sparse conditions.",
      "ai_keywords": [
        "GLiClass",
        "GLiNER",
        "sequence classification",
        "zero-shot learning",
        "few-shot learning",
        "proximal policy optimization",
        "PPO",
        "multi-label text classification"
      ]
    },
    "publishedAt": "2025-08-11T02:22:25.000Z",
    "title": "GLiClass: Generalist Lightweight Model for Sequence Classification Tasks",
    "summary": "Classification is one of the most widespread tasks in AI applications,\nserving often as the first step in filtering, sorting, and categorizing data.\nSince modern AI systems must handle large volumes of input data and early\npipeline stages can propagate errors downstream, achieving high efficiency and\naccuracy is critical. Moreover, classification requirements can change\ndynamically based on user needs, necessitating models with strong zero-shot\ncapabilities. While generative LLMs have become mainstream for zero-shot\nclassification due to their versatility, they suffer from inconsistent\ninstruction following and computational inefficiency. Cross-encoders, commonly\nused as rerankers in RAG pipelines, face a different bottleneck: they must\nprocess text-label pairs sequentially, significantly reducing efficiency with\nlarge label sets. Embedding-based approaches offer good efficiency but struggle\nwith complex scenarios involving logical and semantic constraints. We propose\nGLiClass, a novel method that adapts the GLiNER architecture for sequence\nclassification tasks. Our approach achieves strong accuracy and efficiency\ncomparable to embedding-based methods, while maintaining the flexibility needed\nfor zero-shot and few-shot learning scenarios. Additionally, we adapted\nproximal policy optimization (PPO) for multi-label text classification,\nenabling training classifiers in data-sparse conditions or from human feedback.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/5e6a3d4ea9afd5125d9ec064/bGVX6dwt_d9_-25Cor_CL.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.07662.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5e6a3d4ea9afd5125d9ec064",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1584020801691-noauth.jpeg",
      "fullname": "Stefan Schweter",
      "name": "stefan-it",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3198
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.07493",
      "authors": [
        {
          "_id": "689ad246fab6fdd2e52ac52f",
          "name": "Jian Chen",
          "hidden": false
        },
        {
          "_id": "689ad246fab6fdd2e52ac530",
          "name": "Ming Li",
          "hidden": false
        },
        {
          "_id": "689ad246fab6fdd2e52ac531",
          "name": "Jihyung Kil",
          "hidden": false
        },
        {
          "_id": "689ad246fab6fdd2e52ac532",
          "name": "Chenguang Wang",
          "hidden": false
        },
        {
          "_id": "689ad246fab6fdd2e52ac533",
          "name": "Tong Yu",
          "hidden": false
        },
        {
          "_id": "689ad246fab6fdd2e52ac534",
          "name": "Ryan Rossi",
          "hidden": false
        },
        {
          "_id": "689ad246fab6fdd2e52ac535",
          "name": "Tianyi Zhou",
          "hidden": false
        },
        {
          "_id": "689ad246fab6fdd2e52ac536",
          "name": "Changyou Chen",
          "hidden": false
        },
        {
          "_id": "689ad246fab6fdd2e52ac537",
          "name": "Ruiyi Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64593836c16ecb4815e082ce/C2mHzrOVk-3qKt6prS_QN.png"
      ],
      "publishedAt": "2025-08-10T21:44:43.000Z",
      "submittedOnDailyAt": "2025-08-12T04:05:51.971Z",
      "title": "VisR-Bench: An Empirical Study on Visual Retrieval-Augmented Generation\n  for Multilingual Long Document Understanding",
      "submittedOnDailyBy": {
        "_id": "64593836c16ecb4815e082ce",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64593836c16ecb4815e082ce/T9WM0Ly6ADi3a9JUSIsSV.png",
        "isPro": false,
        "fullname": "Jian Chen",
        "user": "puar-playground",
        "type": "user"
      },
      "summary": "Most organizational data in this world are stored as documents, and visual\nretrieval plays a crucial role in unlocking the collective intelligence from\nall these documents. However, existing benchmarks focus on English-only\ndocument retrieval or only consider multilingual question-answering on a\nsingle-page image. To bridge this gap, we introduce VisR-Bench, a multilingual\nbenchmark designed for question-driven multimodal retrieval in long documents.\nOur benchmark comprises over 35K high-quality QA pairs across 1.2K documents,\nenabling fine-grained evaluation of multimodal retrieval. VisR-Bench spans\nsixteen languages with three question types (figures, text, and tables),\noffering diverse linguistic and question coverage. Unlike prior datasets, we\ninclude queries without explicit answers, preventing models from relying on\nsuperficial keyword matching. We evaluate various retrieval models, including\ntext-based methods, multimodal encoders, and MLLMs, providing insights into\ntheir strengths and limitations. Our results show that while MLLMs\nsignificantly outperform text-based and multimodal encoder models, they still\nstruggle with structured tables and low-resource languages, highlighting key\nchallenges in multilingual visual retrieval.",
      "upvotes": 1,
      "discussionId": "689ad247fab6fdd2e52ac538",
      "githubRepo": "https://github.com/puar-playground/VisR-Bench",
      "ai_summary": "VisR-Bench is a multilingual benchmark for question-driven multimodal retrieval in long documents, evaluating various models across different languages and question types.",
      "ai_keywords": [
        "multimodal benchmark",
        "question-driven retrieval",
        "long documents",
        "QA pairs",
        "multimodal encoders",
        "MLLMs",
        "structured tables",
        "low-resource languages"
      ],
      "githubStars": 3
    },
    "publishedAt": "2025-08-10T17:44:43.000Z",
    "title": "VisR-Bench: An Empirical Study on Visual Retrieval-Augmented Generation\n  for Multilingual Long Document Understanding",
    "summary": "Most organizational data in this world are stored as documents, and visual\nretrieval plays a crucial role in unlocking the collective intelligence from\nall these documents. However, existing benchmarks focus on English-only\ndocument retrieval or only consider multilingual question-answering on a\nsingle-page image. To bridge this gap, we introduce VisR-Bench, a multilingual\nbenchmark designed for question-driven multimodal retrieval in long documents.\nOur benchmark comprises over 35K high-quality QA pairs across 1.2K documents,\nenabling fine-grained evaluation of multimodal retrieval. VisR-Bench spans\nsixteen languages with three question types (figures, text, and tables),\noffering diverse linguistic and question coverage. Unlike prior datasets, we\ninclude queries without explicit answers, preventing models from relying on\nsuperficial keyword matching. We evaluate various retrieval models, including\ntext-based methods, multimodal encoders, and MLLMs, providing insights into\ntheir strengths and limitations. Our results show that while MLLMs\nsignificantly outperform text-based and multimodal encoder models, they still\nstruggle with structured tables and low-resource languages, highlighting key\nchallenges in multilingual visual retrieval.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64593836c16ecb4815e082ce/C2mHzrOVk-3qKt6prS_QN.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.07493.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64593836c16ecb4815e082ce",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64593836c16ecb4815e082ce/T9WM0Ly6ADi3a9JUSIsSV.png",
      "fullname": "Jian Chen",
      "name": "puar-playground",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.05954",
      "authors": [
        {
          "_id": "689a1735fab6fdd2e52ac3a0",
          "name": "Han Lin",
          "hidden": false
        },
        {
          "_id": "689a1735fab6fdd2e52ac3a1",
          "name": "Jaemin Cho",
          "hidden": false
        },
        {
          "_id": "689a1735fab6fdd2e52ac3a2",
          "name": "Amir Zadeh",
          "hidden": false
        },
        {
          "_id": "689a1735fab6fdd2e52ac3a3",
          "name": "Chuan Li",
          "hidden": false
        },
        {
          "_id": "689a1735fab6fdd2e52ac3a4",
          "name": "Mohit Bansal",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/5ffe32d8942cf3533d364449/k74Ei9XNjuQbQf6LZYKge.png"
      ],
      "publishedAt": "2025-08-08T02:38:47.000Z",
      "submittedOnDailyAt": "2025-08-12T05:51:34.690Z",
      "title": "Bifrost-1: Bridging Multimodal LLMs and Diffusion Models with\n  Patch-level CLIP Latents",
      "submittedOnDailyBy": {
        "_id": "5ffe32d8942cf3533d364449",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654821969191-5ffe32d8942cf3533d364449.jpeg",
        "isPro": false,
        "fullname": "Jaemin Cho",
        "user": "j-min",
        "type": "user"
      },
      "summary": "There is growing interest in integrating high-fidelity visual synthesis\ncapabilities into large language models (LLMs) without compromising their\nstrong reasoning capabilities. Existing methods that directly train LLMs or\nbridge LLMs and diffusion models usually suffer from costly training since the\nbackbone LLMs have not seen image representations during pretraining. We\npresent Bifrost-1, a unified framework that bridges pretrained multimodal LLMs\n(MLLMs) and diffusion models using patch-level CLIP image embeddings as latent\nvariables, which are natively aligned with the MLLM's CLIP visual encoder.\nThese patch-level image embeddings are integrated into the diffusion model with\na lightweight adaptation of its ControlNet. To retain the original multimodal\nreasoning capabilities of MLLMs, we equip the MLLM with a visual generation\nbranch initialized from the original MLLM parameters when predicting the\npatch-level image embeddings. By seamlessly integrating pretrained MLLMs and\ndiffusion models with patch-level CLIP latents, our framework enables\nhigh-fidelity controllable image generation with significant training\nefficiency. Our experiments demonstrate that Bifrost-1 achieves comparable or\nbetter performance than previous methods in terms of visual fidelity and\nmultimodal understanding, with substantially lower compute during training. We\nalso provide comprehensive ablation studies showing the effectiveness of our\ndesign choices.",
      "upvotes": 1,
      "discussionId": "689a1735fab6fdd2e52ac3a5",
      "projectPage": "https://bifrost-1.github.io/",
      "githubRepo": "https://github.com/HL-hanlin/Bifrost-1",
      "ai_summary": "Bifrost-1 integrates pretrained multimodal LLMs and diffusion models using patch-level CLIP embeddings to enable efficient high-fidelity image generation with strong multimodal reasoning.",
      "ai_keywords": [
        "LLMs",
        "diffusion models",
        "patch-level CLIP image embeddings",
        "latent variables",
        "CLIP visual encoder",
        "ControlNet",
        "visual generation branch",
        "multimodal reasoning",
        "visual fidelity",
        "multimodal understanding"
      ]
    },
    "publishedAt": "2025-08-07T22:38:47.000Z",
    "title": "Bifrost-1: Bridging Multimodal LLMs and Diffusion Models with\n  Patch-level CLIP Latents",
    "summary": "There is growing interest in integrating high-fidelity visual synthesis\ncapabilities into large language models (LLMs) without compromising their\nstrong reasoning capabilities. Existing methods that directly train LLMs or\nbridge LLMs and diffusion models usually suffer from costly training since the\nbackbone LLMs have not seen image representations during pretraining. We\npresent Bifrost-1, a unified framework that bridges pretrained multimodal LLMs\n(MLLMs) and diffusion models using patch-level CLIP image embeddings as latent\nvariables, which are natively aligned with the MLLM's CLIP visual encoder.\nThese patch-level image embeddings are integrated into the diffusion model with\na lightweight adaptation of its ControlNet. To retain the original multimodal\nreasoning capabilities of MLLMs, we equip the MLLM with a visual generation\nbranch initialized from the original MLLM parameters when predicting the\npatch-level image embeddings. By seamlessly integrating pretrained MLLMs and\ndiffusion models with patch-level CLIP latents, our framework enables\nhigh-fidelity controllable image generation with significant training\nefficiency. Our experiments demonstrate that Bifrost-1 achieves comparable or\nbetter performance than previous methods in terms of visual fidelity and\nmultimodal understanding, with substantially lower compute during training. We\nalso provide comprehensive ablation studies showing the effectiveness of our\ndesign choices.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/5ffe32d8942cf3533d364449/k74Ei9XNjuQbQf6LZYKge.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.05954.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5ffe32d8942cf3533d364449",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654821969191-5ffe32d8942cf3533d364449.jpeg",
      "fullname": "Jaemin Cho",
      "name": "j-min",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.03365",
      "authors": [
        {
          "_id": "6892ae738da45ffb0a2b23ca",
          "name": "Bodam Kim",
          "hidden": false
        },
        {
          "_id": "6892ae738da45ffb0a2b23cb",
          "name": "Hiskias Dingeto",
          "hidden": false
        },
        {
          "_id": "6892ae738da45ffb0a2b23cc",
          "name": "Taeyoun Kwon",
          "hidden": false
        },
        {
          "_id": "6892ae738da45ffb0a2b23cd",
          "name": "Dasol Choi",
          "hidden": false
        },
        {
          "_id": "6892ae738da45ffb0a2b23ce",
          "user": {
            "_id": "6540fbf9cb7fffd683942b43",
            "avatarUrl": "/avatars/d4a64fbde511d0949e1c339179586850.svg",
            "isPro": false,
            "fullname": "DongGeon Lee",
            "user": "oneonlee",
            "type": "user"
          },
          "name": "DongGeon Lee",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-06T19:17:41.907Z",
          "hidden": false
        },
        {
          "_id": "6892ae738da45ffb0a2b23cf",
          "name": "Haon Park",
          "hidden": false
        },
        {
          "_id": "6892ae738da45ffb0a2b23d0",
          "name": "JaeHoon Lee",
          "hidden": false
        },
        {
          "_id": "6892ae738da45ffb0a2b23d1",
          "name": "Jongho Shin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-05T12:14:01.000Z",
      "submittedOnDailyAt": "2025-08-12T04:15:19.448Z",
      "title": "When Good Sounds Go Adversarial: Jailbreaking Audio-Language Models with\n  Benign Inputs",
      "submittedOnDailyBy": {
        "_id": "6540fbf9cb7fffd683942b43",
        "avatarUrl": "/avatars/d4a64fbde511d0949e1c339179586850.svg",
        "isPro": false,
        "fullname": "DongGeon Lee",
        "user": "oneonlee",
        "type": "user"
      },
      "summary": "As large language models become increasingly integrated into daily life,\naudio has emerged as a key interface for human-AI interaction. However, this\nconvenience also introduces new vulnerabilities, making audio a potential\nattack surface for adversaries. Our research introduces WhisperInject, a\ntwo-stage adversarial audio attack framework that can manipulate\nstate-of-the-art audio language models to generate harmful content. Our method\nuses imperceptible perturbations in audio inputs that remain benign to human\nlisteners. The first stage uses a novel reward-based optimization method,\nReinforcement Learning with Projected Gradient Descent (RL-PGD), to guide the\ntarget model to circumvent its own safety protocols and generate harmful native\nresponses. This native harmful response then serves as the target for Stage 2,\nPayload Injection, where we use Projected Gradient Descent (PGD) to optimize\nsubtle perturbations that are embedded into benign audio carriers, such as\nweather queries or greeting messages. Validated under the rigorous\nStrongREJECT, LlamaGuard, as well as Human Evaluation safety evaluation\nframework, our experiments demonstrate a success rate exceeding 86% across\nQwen2.5-Omni-3B, Qwen2.5-Omni-7B, and Phi-4-Multimodal. Our work demonstrates a\nnew class of practical, audio-native threats, moving beyond theoretical\nexploits to reveal a feasible and covert method for manipulating AI behavior.",
      "upvotes": 1,
      "discussionId": "6892ae738da45ffb0a2b23d2",
      "ai_summary": "WhisperInject uses RL-PGD and PGD to create imperceptible audio perturbations that manipulate large language models into generating harmful content.",
      "ai_keywords": [
        "WhisperInject",
        "Reinforcement Learning with Projected Gradient Descent (RL-PGD)",
        "Projected Gradient Descent (PGD)",
        "audio language models",
        "safety protocols",
        "StrongREJECT",
        "LlamaGuard",
        "Human Evaluation",
        "Qwen2.5-Omni-3B",
        "Qwen2.5-Omni-7B",
        "Phi-4-Multimodal"
      ]
    },
    "publishedAt": "2025-08-05T08:14:01.000Z",
    "title": "When Good Sounds Go Adversarial: Jailbreaking Audio-Language Models with\n  Benign Inputs",
    "summary": "As large language models become increasingly integrated into daily life,\naudio has emerged as a key interface for human-AI interaction. However, this\nconvenience also introduces new vulnerabilities, making audio a potential\nattack surface for adversaries. Our research introduces WhisperInject, a\ntwo-stage adversarial audio attack framework that can manipulate\nstate-of-the-art audio language models to generate harmful content. Our method\nuses imperceptible perturbations in audio inputs that remain benign to human\nlisteners. The first stage uses a novel reward-based optimization method,\nReinforcement Learning with Projected Gradient Descent (RL-PGD), to guide the\ntarget model to circumvent its own safety protocols and generate harmful native\nresponses. This native harmful response then serves as the target for Stage 2,\nPayload Injection, where we use Projected Gradient Descent (PGD) to optimize\nsubtle perturbations that are embedded into benign audio carriers, such as\nweather queries or greeting messages. Validated under the rigorous\nStrongREJECT, LlamaGuard, as well as Human Evaluation safety evaluation\nframework, our experiments demonstrate a success rate exceeding 86% across\nQwen2.5-Omni-3B, Qwen2.5-Omni-7B, and Phi-4-Multimodal. Our work demonstrates a\nnew class of practical, audio-native threats, moving beyond theoretical\nexploits to reveal a feasible and covert method for manipulating AI behavior.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.03365.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6540fbf9cb7fffd683942b43",
      "avatarUrl": "/avatars/d4a64fbde511d0949e1c339179586850.svg",
      "fullname": "DongGeon Lee",
      "name": "oneonlee",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2508.06601",
      "authors": [
        {
          "_id": "689aa13ffab6fdd2e52ac416",
          "name": "Kyle O'Brien",
          "hidden": false
        },
        {
          "_id": "689aa13ffab6fdd2e52ac417",
          "name": "Stephen Casper",
          "hidden": false
        },
        {
          "_id": "689aa13ffab6fdd2e52ac418",
          "name": "Quentin Anthony",
          "hidden": false
        },
        {
          "_id": "689aa13ffab6fdd2e52ac419",
          "name": "Tomek Korbak",
          "hidden": false
        },
        {
          "_id": "689aa13ffab6fdd2e52ac41a",
          "name": "Robert Kirk",
          "hidden": false
        },
        {
          "_id": "689aa13ffab6fdd2e52ac41b",
          "name": "Xander Davies",
          "hidden": false
        },
        {
          "_id": "689aa13ffab6fdd2e52ac41c",
          "name": "Ishan Mishra",
          "hidden": false
        },
        {
          "_id": "689aa13ffab6fdd2e52ac41d",
          "name": "Geoffrey Irving",
          "hidden": false
        },
        {
          "_id": "689aa13ffab6fdd2e52ac41e",
          "name": "Yarin Gal",
          "hidden": false
        },
        {
          "_id": "689aa13ffab6fdd2e52ac41f",
          "name": "Stella Biderman",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-08T17:59:47.000Z",
      "submittedOnDailyAt": "2025-08-12T00:35:06.219Z",
      "title": "Deep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant\n  Safeguards into Open-Weight LLMs",
      "submittedOnDailyBy": {
        "_id": "60347d3660e3dd96631c9093",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60347d3660e3dd96631c9093/B3fuZer5N04tZIAYrLnz4.jpeg",
        "isPro": false,
        "fullname": "Stella Biderman",
        "user": "stellaathena",
        "type": "user"
      },
      "summary": "Open-weight AI systems offer unique benefits, including enhanced\ntransparency, open research, and decentralized access. However, they are\nvulnerable to tampering attacks which can efficiently elicit harmful behaviors\nby modifying weights or activations. Currently, there is not yet a robust\nscience of open-weight model risk management. Existing safety fine-tuning\nmethods and other post-training techniques have struggled to make LLMs\nresistant to more than a few dozen steps of adversarial fine-tuning. In this\npaper, we investigate whether filtering text about dual-use topics from\ntraining data can prevent unwanted capabilities and serve as a more\ntamper-resistant safeguard. We introduce a multi-stage pipeline for scalable\ndata filtering and show that it offers a tractable and effective method for\nminimizing biothreat proxy knowledge in LLMs. We pretrain multiple\n6.9B-parameter models from scratch and find that they exhibit substantial\nresistance to adversarial fine-tuning attacks on up to 10,000 steps and 300M\ntokens of biothreat-related text -- outperforming existing post-training\nbaselines by over an order of magnitude -- with no observed degradation to\nunrelated capabilities. However, while filtered models lack internalized\ndangerous knowledge, we find that they can still leverage such information when\nit is provided in context (e.g., via search tool augmentation), demonstrating a\nneed for a defense-in-depth approach. Overall, these findings help to establish\npretraining data curation as a promising layer of defense for open-weight AI\nsystems.",
      "upvotes": 0,
      "discussionId": "689aa140fab6fdd2e52ac420",
      "ai_summary": "Data filtering during pretraining enhances LLM resistance to adversarial fine-tuning attacks without degrading unrelated capabilities, offering a promising defense mechanism for open-weight AI systems.",
      "ai_keywords": [
        "open-weight AI systems",
        "tampering attacks",
        "adversarial fine-tuning",
        "data filtering",
        "multi-stage pipeline",
        "biothreat proxy knowledge",
        "pretraining",
        "defense-in-depth"
      ]
    },
    "publishedAt": "2025-08-08T13:59:47.000Z",
    "title": "Deep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant\n  Safeguards into Open-Weight LLMs",
    "summary": "Open-weight AI systems offer unique benefits, including enhanced\ntransparency, open research, and decentralized access. However, they are\nvulnerable to tampering attacks which can efficiently elicit harmful behaviors\nby modifying weights or activations. Currently, there is not yet a robust\nscience of open-weight model risk management. Existing safety fine-tuning\nmethods and other post-training techniques have struggled to make LLMs\nresistant to more than a few dozen steps of adversarial fine-tuning. In this\npaper, we investigate whether filtering text about dual-use topics from\ntraining data can prevent unwanted capabilities and serve as a more\ntamper-resistant safeguard. We introduce a multi-stage pipeline for scalable\ndata filtering and show that it offers a tractable and effective method for\nminimizing biothreat proxy knowledge in LLMs. We pretrain multiple\n6.9B-parameter models from scratch and find that they exhibit substantial\nresistance to adversarial fine-tuning attacks on up to 10,000 steps and 300M\ntokens of biothreat-related text -- outperforming existing post-training\nbaselines by over an order of magnitude -- with no observed degradation to\nunrelated capabilities. However, while filtered models lack internalized\ndangerous knowledge, we find that they can still leverage such information when\nit is provided in context (e.g., via search tool augmentation), demonstrating a\nneed for a defense-in-depth approach. Overall, these findings help to establish\npretraining data curation as a promising layer of defense for open-weight AI\nsystems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.06601.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60347d3660e3dd96631c9093",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60347d3660e3dd96631c9093/B3fuZer5N04tZIAYrLnz4.jpeg",
      "fullname": "Stella Biderman",
      "name": "stellaathena",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3451
    },
    "isAuthorParticipating": false
  }
]