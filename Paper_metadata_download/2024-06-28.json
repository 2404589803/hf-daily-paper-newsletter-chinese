[
    {
        "paper": {
            "id": "2406.19389",
            "authors": [
                {
                    "_id": "667e26bdb0f62e5e076bc54f",
                    "name": "Tao Zhang",
                    "hidden": false
                },
                {
                    "_id": "667e26bdb0f62e5e076bc550",
                    "user": {
                        "avatarUrl": "/avatars/1f5e9b9dfcc16df8e88e3dcecfcb4e10.svg",
                        "isPro": false,
                        "fullname": "Xiangtai Li",
                        "user": "LXT",
                        "type": "user"
                    },
                    "name": "Xiangtai Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-28T09:20:50.107Z",
                    "hidden": false
                },
                {
                    "_id": "667e26bdb0f62e5e076bc551",
                    "name": "Hao Fei",
                    "hidden": false
                },
                {
                    "_id": "667e26bdb0f62e5e076bc552",
                    "user": {
                        "avatarUrl": "/avatars/d09a9ee329bb8c3a9e2929d67d24e97d.svg",
                        "isPro": false,
                        "fullname": "Haobo Yuan",
                        "user": "HarborYuan",
                        "type": "user"
                    },
                    "name": "Haobo Yuan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-28T09:21:29.050Z",
                    "hidden": false
                },
                {
                    "_id": "667e26bdb0f62e5e076bc553",
                    "user": {
                        "avatarUrl": "/avatars/4b7b3f41c2e2cfa21dd43bbac6e081ae.svg",
                        "isPro": false,
                        "fullname": "Shengqiong Wu",
                        "user": "ChocoWu",
                        "type": "user"
                    },
                    "name": "Shengqiong Wu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-28T09:25:21.171Z",
                    "hidden": false
                },
                {
                    "_id": "667e26bdb0f62e5e076bc554",
                    "name": "Shunping Ji",
                    "hidden": false
                },
                {
                    "_id": "667e26bdb0f62e5e076bc555",
                    "name": "Chen Change Loy",
                    "hidden": false
                },
                {
                    "_id": "667e26bdb0f62e5e076bc556",
                    "name": "Shuicheng Yan",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-27T17:59:01.000Z",
            "title": "OMG-LLaVA: Bridging Image-level, Object-level, Pixel-level Reasoning and\n  Understanding",
            "summary": "Current universal segmentation methods demonstrate strong capabilities in\npixel-level image and video understanding. However, they lack reasoning\nabilities and cannot be controlled via text instructions. In contrast, large\nvision-language multimodal models exhibit powerful vision-based conversation\nand reasoning capabilities but lack pixel-level understanding and have\ndifficulty accepting visual prompts for flexible user interaction. This paper\nproposes OMG-LLaVA, a new and elegant framework combining powerful pixel-level\nvision understanding with reasoning abilities. It can accept various visual and\ntext prompts for flexible user interaction. Specifically, we use a universal\nsegmentation method as the visual encoder, integrating image information,\nperception priors, and visual prompts into visual tokens provided to the LLM.\nThe LLM is responsible for understanding the user's text instructions and\nproviding text responses and pixel-level segmentation results based on the\nvisual information. We propose perception prior embedding to better integrate\nperception priors with image features. OMG-LLaVA achieves image-level,\nobject-level, and pixel-level reasoning and understanding in a single model,\nmatching or surpassing the performance of specialized methods on multiple\nbenchmarks. Rather than using LLM to connect each specialist, our work aims at\nend-to-end training on one encoder, one decoder, and one LLM. The code and\nmodel have been released for further research.",
            "upvotes": 30
        },
        "publishedAt": "2024-06-28T01:34:31.710Z",
        "title": "OMG-LLaVA: Bridging Image-level, Object-level, Pixel-level Reasoning and Understanding",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.19389.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "/avatars/1f5e9b9dfcc16df8e88e3dcecfcb4e10.svg",
            "fullname": "Xiangtai Li",
            "name": "LXT",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.18629",
            "authors": [
                {
                    "_id": "667e1ec7b3072cb50c79ef59",
                    "user": {
                        "avatarUrl": "/avatars/f275237f36a112624d59a7e3f73237d3.svg",
                        "isPro": false,
                        "fullname": "Xin Lai",
                        "user": "xinlai",
                        "type": "user"
                    },
                    "name": "Xin Lai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-28T09:53:21.974Z",
                    "hidden": false
                },
                {
                    "_id": "667e1ec7b3072cb50c79ef5a",
                    "user": {
                        "avatarUrl": "/avatars/2d9eff3a2dff6d02e45ae8964fb91f27.svg",
                        "isPro": false,
                        "fullname": "zt tian",
                        "user": "tianzhuotao",
                        "type": "user"
                    },
                    "name": "Zhuotao Tian",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-28T09:53:16.366Z",
                    "hidden": false
                },
                {
                    "_id": "667e1ec7b3072cb50c79ef5b",
                    "user": {
                        "avatarUrl": "/avatars/8c51c1ff337ac372c7caafa4fa5019dc.svg",
                        "isPro": false,
                        "fullname": "Yukang Chen",
                        "user": "Yukang2",
                        "type": "user"
                    },
                    "name": "Yukang Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-28T09:53:04.262Z",
                    "hidden": false
                },
                {
                    "_id": "667e1ec7b3072cb50c79ef5c",
                    "user": {
                        "avatarUrl": "/avatars/d71324fa89458cab73723802a4bbca64.svg",
                        "isPro": false,
                        "fullname": "Senqiao Yang",
                        "user": "Senqiao",
                        "type": "user"
                    },
                    "name": "Senqiao Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-28T09:52:57.935Z",
                    "hidden": false
                },
                {
                    "_id": "667e1ec7b3072cb50c79ef5d",
                    "user": {
                        "avatarUrl": "/avatars/22d66b872173ce0935c2f4e6ba1e001d.svg",
                        "isPro": false,
                        "fullname": "Xiangru Peng",
                        "user": "pengxr",
                        "type": "user"
                    },
                    "name": "Xiangru Peng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-28T09:52:52.509Z",
                    "hidden": false
                },
                {
                    "_id": "667e1ec7b3072cb50c79ef5e",
                    "user": {
                        "avatarUrl": "/avatars/caba60f52a490fc3aece6d4ae036d2c3.svg",
                        "isPro": false,
                        "fullname": "Jim",
                        "user": "jiajiaya",
                        "type": "user"
                    },
                    "name": "Jiaya Jia",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-28T09:52:46.492Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-26T17:43:06.000Z",
            "title": "Step-DPO: Step-wise Preference Optimization for Long-chain Reasoning of\n  LLMs",
            "summary": "Mathematical reasoning presents a significant challenge for Large Language\nModels (LLMs) due to the extensive and precise chain of reasoning required for\naccuracy. Ensuring the correctness of each reasoning step is critical. To\naddress this, we aim to enhance the robustness and factuality of LLMs by\nlearning from human feedback. However, Direct Preference Optimization (DPO) has\nshown limited benefits for long-chain mathematical reasoning, as models\nemploying DPO struggle to identify detailed errors in incorrect answers. This\nlimitation stems from a lack of fine-grained process supervision. We propose a\nsimple, effective, and data-efficient method called Step-DPO, which treats\nindividual reasoning steps as units for preference optimization rather than\nevaluating answers holistically. Additionally, we have developed a data\nconstruction pipeline for Step-DPO, enabling the creation of a high-quality\ndataset containing 10K step-wise preference pairs. We also observe that in DPO,\nself-generated data is more effective than data generated by humans or GPT-4,\ndue to the latter's out-of-distribution nature. Our findings demonstrate that\nas few as 10K preference data pairs and fewer than 500 Step-DPO training steps\ncan yield a nearly 3% gain in accuracy on MATH for models with over 70B\nparameters. Notably, Step-DPO, when applied to Qwen2-72B-Instruct, achieves\nscores of 70.8% and 94.0% on the test sets of MATH and GSM8K, respectively,\nsurpassing a series of closed-source models, including GPT-4-1106,\nClaude-3-Opus, and Gemini-1.5-Pro. Our code, data, and models are available at\nhttps://github.com/dvlab-research/Step-DPO.",
            "upvotes": 22
        },
        "publishedAt": "2024-06-28T00:58:45.956Z",
        "title": "Step-DPO: Step-wise Preference Optimization for Long-chain Reasoning of LLMs",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.18629.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "/avatars/f275237f36a112624d59a7e3f73237d3.svg",
            "fullname": "Xin Lai",
            "name": "xinlai",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.19226",
            "authors": [
                {
                    "_id": "667e2af648fdddb8dee340ff",
                    "user": {
                        "avatarUrl": "/avatars/8b9372a233d4c00b555625fa7b5203e2.svg",
                        "isPro": false,
                        "fullname": "Zheyuan Zhang",
                        "user": "Zheyuan22",
                        "type": "user"
                    },
                    "name": "Zheyuan Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-28T09:12:54.968Z",
                    "hidden": false
                },
                {
                    "_id": "667e2af648fdddb8dee34100",
                    "name": "Daniel Zhang-Li",
                    "hidden": false
                },
                {
                    "_id": "667e2af648fdddb8dee34101",
                    "user": {
                        "avatarUrl": "/avatars/fba1fb87bcac340a8eee3b9f4fc35bc5.svg",
                        "isPro": false,
                        "fullname": "Jifan Yu",
                        "user": "JovanYu",
                        "type": "user"
                    },
                    "name": "Jifan Yu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-28T12:17:42.774Z",
                    "hidden": false
                },
                {
                    "_id": "667e2af648fdddb8dee34102",
                    "name": "Linlu Gong",
                    "hidden": false
                },
                {
                    "_id": "667e2af648fdddb8dee34103",
                    "name": "Jinchang Zhou",
                    "hidden": false
                },
                {
                    "_id": "667e2af648fdddb8dee34104",
                    "user": {
                        "avatarUrl": "/avatars/79dca137583875191ea883b193b630b7.svg",
                        "isPro": false,
                        "fullname": "Zhiyuan Liu",
                        "user": "zibuyu9",
                        "type": "user"
                    },
                    "name": "Zhiyuan Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-28T12:17:24.883Z",
                    "hidden": false
                },
                {
                    "_id": "667e2af648fdddb8dee34105",
                    "name": "Lei Hou",
                    "hidden": false
                },
                {
                    "_id": "667e2af648fdddb8dee34106",
                    "user": {
                        "avatarUrl": "/avatars/63e46f15bb76bd9d4508fd0f54f39829.svg",
                        "isPro": false,
                        "fullname": "Juanzi Li",
                        "user": "juanli",
                        "type": "user"
                    },
                    "name": "Juanzi Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-28T10:07:14.950Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-27T14:51:07.000Z",
            "title": "Simulating Classroom Education with LLM-Empowered Agents",
            "summary": "Large language models (LLMs) have been employed in various intelligent\neducational tasks to assist teaching. While preliminary explorations have\nfocused on independent LLM-empowered agents for specific educational tasks, the\npotential for LLMs within a multi-agent collaborative framework to simulate a\nclassroom with real user participation remains unexplored. In this work, we\npropose SimClass, a multi-agent classroom simulation framework involving user\nparticipation. We recognize representative class roles and introduce a novel\nclass control mechanism for automatic classroom teaching, and conduct user\nexperiments in two real-world courses. Utilizing the Flanders Interactive\nAnalysis System and Community of Inquiry theoretical frame works from\neducational analysis, we demonstrate that LLMs can simulate traditional\nclassroom interaction patterns effectively while enhancing user's experience.\nWe also observe emergent group behaviors among agents in SimClass, where agents\ncollaborate to create enlivening interactions in classrooms to improve user\nlearning process. We hope this work pioneers the application of LLM-empowered\nmulti-agent systems in virtual classroom teaching.",
            "upvotes": 17
        },
        "publishedAt": "2024-06-28T01:46:28.486Z",
        "title": "Simulating Classroom Education with LLM-Empowered Agents",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.19226.png",
        "numComments": 5,
        "submittedBy": {
            "avatarUrl": "/avatars/5a72dc56ffb69b6b21910f9a63b68ea4.svg",
            "fullname": "Zijun",
            "name": "TranSirius",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.19215",
            "authors": [
                {
                    "_id": "667e1f61e677941031fd7de6",
                    "user": {
                        "avatarUrl": "/avatars/5a72dc56ffb69b6b21910f9a63b68ea4.svg",
                        "isPro": false,
                        "fullname": "Zijun",
                        "user": "TranSirius",
                        "type": "user"
                    },
                    "name": "Zijun Yao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-28T08:02:55.154Z",
                    "hidden": false
                },
                {
                    "_id": "667e1f61e677941031fd7de7",
                    "name": "Weijian Qi",
                    "hidden": false
                },
                {
                    "_id": "667e1f61e677941031fd7de8",
                    "user": {
                        "avatarUrl": "/avatars/cf64c88e71c730fc0a54df59e17bffbb.svg",
                        "isPro": false,
                        "fullname": "Liangming Pan",
                        "user": "Liangming",
                        "type": "user"
                    },
                    "name": "Liangming Pan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-28T11:43:43.362Z",
                    "hidden": false
                },
                {
                    "_id": "667e1f61e677941031fd7de9",
                    "user": {
                        "avatarUrl": "/avatars/3dbc37af162b94d68cb83665ac4528c3.svg",
                        "isPro": false,
                        "fullname": "ShulinCao",
                        "user": "caoshulin",
                        "type": "user"
                    },
                    "name": "Shulin Cao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-28T11:43:36.918Z",
                    "hidden": false
                },
                {
                    "_id": "667e1f61e677941031fd7dea",
                    "name": "Linmei Hu",
                    "hidden": false
                },
                {
                    "_id": "667e1f61e677941031fd7deb",
                    "name": "Weichuan Liu",
                    "hidden": false
                },
                {
                    "_id": "667e1f61e677941031fd7dec",
                    "name": "Lei Hou",
                    "hidden": false
                },
                {
                    "_id": "667e1f61e677941031fd7ded",
                    "user": {
                        "avatarUrl": "/avatars/63e46f15bb76bd9d4508fd0f54f39829.svg",
                        "isPro": false,
                        "fullname": "Juanzi Li",
                        "user": "juanli",
                        "type": "user"
                    },
                    "name": "Juanzi Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-28T11:43:22.144Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-27T14:38:33.000Z",
            "title": "SeaKR: Self-aware Knowledge Retrieval for Adaptive Retrieval Augmented\n  Generation",
            "summary": "This paper introduces Self-aware Knowledge Retrieval (SeaKR), a novel\nadaptive RAG model that extracts self-aware uncertainty of LLMs from their\ninternal states. SeaKR activates retrieval when the LLMs present high\nself-aware uncertainty for generation. To effectively integrate retrieved\nknowledge snippets, SeaKR re-ranks them based on LLM's self-aware uncertainty\nto preserve the snippet that reduces their uncertainty to the utmost. To\nfacilitate solving complex tasks that require multiple retrievals, SeaKR\nutilizes their self-aware uncertainty to choose among different reasoning\nstrategies. Our experiments on both complex and simple Question Answering\ndatasets show that SeaKR outperforms existing adaptive RAG methods. We release\nour code at https://github.com/THU-KEG/SeaKR.",
            "upvotes": 14
        },
        "publishedAt": "2024-06-28T01:01:12.070Z",
        "title": "SeaKR: Self-aware Knowledge Retrieval for Adaptive Retrieval Augmented Generation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.19215.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/5a72dc56ffb69b6b21910f9a63b68ea4.svg",
            "fullname": "Zijun",
            "name": "TranSirius",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.19227",
            "authors": [
                {
                    "_id": "667e1e2ad29e971c593931a0",
                    "name": "Yantao Liu",
                    "hidden": false
                },
                {
                    "_id": "667e1e2ad29e971c593931a1",
                    "name": "Zhao Zhang",
                    "hidden": false
                },
                {
                    "_id": "667e1e2ad29e971c593931a2",
                    "user": {
                        "avatarUrl": "/avatars/5a72dc56ffb69b6b21910f9a63b68ea4.svg",
                        "isPro": false,
                        "fullname": "Zijun",
                        "user": "TranSirius",
                        "type": "user"
                    },
                    "name": "Zijun Yao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-28T08:02:57.469Z",
                    "hidden": false
                },
                {
                    "_id": "667e1e2ad29e971c593931a3",
                    "user": {
                        "avatarUrl": "/avatars/3dbc37af162b94d68cb83665ac4528c3.svg",
                        "isPro": false,
                        "fullname": "ShulinCao",
                        "user": "caoshulin",
                        "type": "user"
                    },
                    "name": "Shulin Cao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-28T12:11:21.575Z",
                    "hidden": false
                },
                {
                    "_id": "667e1e2ad29e971c593931a4",
                    "name": "Lei Hou",
                    "hidden": false
                },
                {
                    "_id": "667e1e2ad29e971c593931a5",
                    "user": {
                        "avatarUrl": "/avatars/63e46f15bb76bd9d4508fd0f54f39829.svg",
                        "isPro": false,
                        "fullname": "Juanzi Li",
                        "user": "juanli",
                        "type": "user"
                    },
                    "name": "Juanzi Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-28T12:11:10.760Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-27T14:51:17.000Z",
            "title": "Aligning Teacher with Student Preferences for Tailored Training Data\n  Generation",
            "summary": "Large Language Models (LLMs) have shown significant promise as copilots in\nvarious tasks. Local deployment of LLMs on edge devices is necessary when\nhandling privacy-sensitive data or latency-sensitive tasks. The computational\nconstraints of such devices make direct deployment of powerful large-scale LLMs\nimpractical, necessitating the Knowledge Distillation from large-scale models\nto lightweight models. Lots of work has been done to elicit diversity and\nquality training examples from LLMs, but little attention has been paid to\naligning teacher instructional content based on student preferences, akin to\n\"responsive teaching\" in pedagogy. Thus, we propose ARTE, dubbed Aligning\nTeacheR with StudenT PreferencEs, a framework that aligns the teacher model\nwith student preferences to generate tailored training examples for Knowledge\nDistillation. Specifically, we elicit draft questions and rationales from the\nteacher model, then collect student preferences on these questions and\nrationales using students' performance with in-context learning as a proxy, and\nfinally align the teacher model with student preferences. In the end, we repeat\nthe first step with the aligned teacher model to elicit tailored training\nexamples for the student model on the target task. Extensive experiments on\nacademic benchmarks demonstrate the superiority of ARTE over existing\ninstruction-tuning datasets distilled from powerful LLMs. Moreover, we\nthoroughly investigate the generalization of ARTE, including the generalization\nof fine-tuned student models in reasoning ability and the generalization of\naligned teacher models to generate tailored training data across tasks and\nstudents. In summary, our contributions lie in proposing a novel framework for\ntailored training example generation, demonstrating its efficacy in\nexperiments, and investigating the generalization of both student & aligned\nteacher models in ARTE.",
            "upvotes": 14
        },
        "publishedAt": "2024-06-28T00:55:07.426Z",
        "title": "Aligning Teacher with Student Preferences for Tailored Training Data Generation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.19227.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "/avatars/5a72dc56ffb69b6b21910f9a63b68ea4.svg",
            "fullname": "Zijun",
            "name": "TranSirius",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.14909",
            "authors": [
                {
                    "_id": "667e3ffc1127552103ab356e",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6445fd9ba56444c355dcbcba/NCyRCD-MK-yA0_qY6I2y0.png",
                        "isPro": false,
                        "fullname": "Tianyu Fu",
                        "user": "fuvty",
                        "type": "user"
                    },
                    "name": "Tianyu Fu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-28T09:12:56.556Z",
                    "hidden": false
                },
                {
                    "_id": "667e3ffc1127552103ab356f",
                    "name": "Haofeng Huang",
                    "hidden": false
                },
                {
                    "_id": "667e3ffc1127552103ab3570",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678782881444-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Xuefei Ning",
                        "user": "Foxfi",
                        "type": "user"
                    },
                    "name": "Xuefei Ning",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-28T08:02:43.467Z",
                    "hidden": false
                },
                {
                    "_id": "667e3ffc1127552103ab3571",
                    "user": {
                        "avatarUrl": "/avatars/26bf5e3f19057835ee95d72c24904d77.svg",
                        "isPro": false,
                        "fullname": "Genghan Zhang",
                        "user": "Genghan",
                        "type": "user"
                    },
                    "name": "Genghan Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-28T12:32:53.942Z",
                    "hidden": false
                },
                {
                    "_id": "667e3ffc1127552103ab3572",
                    "user": {
                        "avatarUrl": "/avatars/2aff6e564565bd243fdb5dfb2f1ef545.svg",
                        "isPro": false,
                        "fullname": "Boju Chen",
                        "user": "Herdsman",
                        "type": "user"
                    },
                    "name": "Boju Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-28T12:33:02.302Z",
                    "hidden": false
                },
                {
                    "_id": "667e3ffc1127552103ab3573",
                    "user": {
                        "avatarUrl": "/avatars/d57cb6de6da34ef2b54ed434ae752d4d.svg",
                        "isPro": false,
                        "fullname": "Tianqi Wu",
                        "user": "wutianqidx",
                        "type": "user"
                    },
                    "name": "Tianqi Wu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-28T12:33:08.192Z",
                    "hidden": false
                },
                {
                    "_id": "667e3ffc1127552103ab3574",
                    "user": {
                        "avatarUrl": "/avatars/6e45e9dbaa5b5410b94dd1e17eabd2f3.svg",
                        "isPro": false,
                        "fullname": "Hongyi Wang",
                        "user": "hwang595",
                        "type": "user"
                    },
                    "name": "Hongyi Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-28T12:33:49.980Z",
                    "hidden": false
                },
                {
                    "_id": "667e3ffc1127552103ab3575",
                    "user": {
                        "avatarUrl": "/avatars/646e455e6bd39bbbf7abc8e3f3c8f225.svg",
                        "isPro": false,
                        "fullname": "Huang Zixiao",
                        "user": "Hzxx",
                        "type": "user"
                    },
                    "name": "Zixiao Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-28T12:34:01.780Z",
                    "hidden": false
                },
                {
                    "_id": "667e3ffc1127552103ab3576",
                    "user": {
                        "avatarUrl": "/avatars/b35ec1320adc058bff871f14971485ba.svg",
                        "isPro": false,
                        "fullname": "Shiyao Li",
                        "user": "LSY-noya",
                        "type": "user"
                    },
                    "name": "Shiyao Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-28T12:44:32.414Z",
                    "hidden": false
                },
                {
                    "_id": "667e3ffc1127552103ab3577",
                    "name": "Shengen Yan",
                    "hidden": false
                },
                {
                    "_id": "667e3ffc1127552103ab3578",
                    "name": "Guohao Dai",
                    "hidden": false
                },
                {
                    "_id": "667e3ffc1127552103ab3579",
                    "name": "Huazhong Yang",
                    "hidden": false
                },
                {
                    "_id": "667e3ffc1127552103ab357a",
                    "name": "Yu Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-21T06:58:37.000Z",
            "title": "MoA: Mixture of Sparse Attention for Automatic Large Language Model\n  Compression",
            "summary": "Sparse attention can effectively mitigate the significant memory and\nthroughput demands of Large Language Models (LLMs) in long contexts. Existing\nmethods typically employ a uniform sparse attention mask, applying the same\nsparse pattern across different attention heads and input lengths. However,\nthis uniform approach fails to capture the diverse attention patterns inherent\nin LLMs, ignoring their distinct accuracy-latency trade-offs. To address this\nchallenge, we propose the Mixture of Attention (MoA), which automatically\ntailors distinct sparse attention configurations to different heads and layers.\nMoA constructs and navigates a search space of various attention patterns and\ntheir scaling rules relative to input sequence lengths. It profiles the model,\nevaluates potential configurations, and pinpoints the optimal sparse attention\ncompression plan. MoA adapts to varying input sizes, revealing that some\nattention heads expand their focus to accommodate longer sequences, while other\nheads consistently concentrate on fixed-length local contexts. Experiments show\nthat MoA increases the effective context length by 3.9times with the same\naverage attention span, boosting retrieval accuracy by 1.5-7.1times over the\nuniform-attention baseline across Vicuna-7B, Vicuna-13B, and Llama3-8B models.\nMoreover, MoA narrows the capability gaps between sparse and dense models,\nreducing the maximum relative performance drop from 9%-36% to within 5%\nacross two long-context understanding benchmarks. MoA achieves a\n1.2-1.4times GPU memory reduction and boosts decode throughput by 5.5-6.7\ntimes for 7B and 13B dense models on a single GPU, with minimal impact on\nperformance.",
            "upvotes": 8
        },
        "publishedAt": "2024-06-28T03:16:19.563Z",
        "title": "MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.14909.png",
        "numComments": 4,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678782881444-noauth.jpeg",
            "fullname": "Xuefei Ning",
            "name": "Foxfi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.14629",
            "authors": [
                {
                    "_id": "667e2a17039ad1fcb8d2e79f",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678782881444-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Xuefei Ning",
                        "user": "Foxfi",
                        "type": "user"
                    },
                    "name": "Xuefei Ning",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-28T08:02:47.626Z",
                    "hidden": false
                },
                {
                    "_id": "667e2a17039ad1fcb8d2e7a0",
                    "user": {
                        "avatarUrl": "/avatars/67c42a6e26bbeddc4bf706eb8f1a75b1.svg",
                        "isPro": false,
                        "fullname": "Zifu Wang",
                        "user": "wangzifu",
                        "type": "user"
                    },
                    "name": "Zifu Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-28T14:58:10.478Z",
                    "hidden": false
                },
                {
                    "_id": "667e2a17039ad1fcb8d2e7a1",
                    "user": {
                        "avatarUrl": "/avatars/b35ec1320adc058bff871f14971485ba.svg",
                        "isPro": false,
                        "fullname": "Shiyao Li",
                        "user": "LSY-noya",
                        "type": "user"
                    },
                    "name": "Shiyao Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-28T13:09:58.037Z",
                    "hidden": false
                },
                {
                    "_id": "667e2a17039ad1fcb8d2e7a2",
                    "user": {
                        "avatarUrl": "/avatars/59d1975634e84095b69423c02441d453.svg",
                        "isPro": false,
                        "fullname": "Zinan Lin",
                        "user": "fjxmlzn",
                        "type": "user"
                    },
                    "name": "Zinan Lin",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2024-06-28T03:12:24.852Z",
                    "hidden": false
                },
                {
                    "_id": "667e2a17039ad1fcb8d2e7a3",
                    "user": {
                        "avatarUrl": "/avatars/2d0c70d5a6d38a080eaa939acc9f0f09.svg",
                        "isPro": false,
                        "fullname": "Peiran Yao",
                        "user": "peiran-yao",
                        "type": "user"
                    },
                    "name": "Peiran Yao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-28T13:10:03.887Z",
                    "hidden": false
                },
                {
                    "_id": "667e2a17039ad1fcb8d2e7a4",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6445fd9ba56444c355dcbcba/NCyRCD-MK-yA0_qY6I2y0.png",
                        "isPro": false,
                        "fullname": "Tianyu Fu",
                        "user": "fuvty",
                        "type": "user"
                    },
                    "name": "Tianyu Fu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-28T09:12:52.141Z",
                    "hidden": false
                },
                {
                    "_id": "667e2a17039ad1fcb8d2e7a5",
                    "name": "Matthew B. Blaschko",
                    "hidden": false
                },
                {
                    "_id": "667e2a17039ad1fcb8d2e7a6",
                    "name": "Guohao Dai",
                    "hidden": false
                },
                {
                    "_id": "667e2a17039ad1fcb8d2e7a7",
                    "name": "Huazhong Yang",
                    "hidden": false
                },
                {
                    "_id": "667e2a17039ad1fcb8d2e7a8",
                    "name": "Yu Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-20T18:00:17.000Z",
            "title": "Can LLMs Learn by Teaching? A Preliminary Study",
            "summary": "Teaching to improve student models (e.g., knowledge distillation) is an\nextensively studied methodology in LLMs. However, for humans, teaching not only\nimproves students but also improves teachers. We ask: Can LLMs also learn by\nteaching (LbT)? If yes, we can potentially unlock the possibility of\ncontinuously advancing the models without solely relying on human-produced data\nor stronger models. In this paper, we provide a preliminary exploration of this\nambitious agenda. We show that LbT ideas can be incorporated into existing LLM\ntraining/prompting pipelines and provide noticeable improvements. Specifically,\nwe design three methods, each mimicking one of the three levels of LbT in\nhumans: observing students' feedback, learning from the feedback, and learning\niteratively, with the goals of improving answer accuracy without training and\nimproving models' inherent capability with fine-tuning. The findings are\nencouraging. For example, similar to LbT in human, we see that: (1) LbT can\ninduce weak-to-strong generalization: strong models can improve themselves by\nteaching other weak models; (2) Diversity in students might help: teaching\nmultiple students could be better than teaching one student or the teacher\nitself. We hope that this early promise can inspire future research on LbT and\nmore broadly adopting the advanced techniques in education to improve LLMs. The\ncode is available at https://github.com/imagination-research/lbt.",
            "upvotes": 8
        },
        "publishedAt": "2024-06-28T02:50:57.605Z",
        "title": "Can LLMs Learn by Teaching? A Preliminary Study",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.14629.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.18790",
            "authors": [
                {
                    "_id": "667ec70aa7d8b1deba80afa8",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1668468846504-63407fadb78ed99eab00203d.jpeg",
                        "isPro": false,
                        "fullname": "Will Berman",
                        "user": "williamberman",
                        "type": "user"
                    },
                    "name": "William Berman",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-28T14:57:47.457Z",
                    "hidden": false
                },
                {
                    "_id": "667ec70aa7d8b1deba80afa9",
                    "name": "Alexander Peysakhovich",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-26T23:21:42.000Z",
            "title": "MUMU: Bootstrapping Multimodal Image Generation from Text-to-Image Data",
            "summary": "We train a model to generate images from multimodal prompts of interleaved\ntext and images such as \"a <picture of a man> man and his <picture of a dog>\ndog in an <picture of a cartoon> animated style.\" We bootstrap a multimodal\ndataset by extracting semantically meaningful image crops corresponding to\nwords in the image captions of synthetically generated and publicly available\ntext-image data. Our model, MUMU, is composed of a vision-language model\nencoder with a diffusion decoder and is trained on a single 8xH100 GPU node.\nDespite being only trained on crops from the same image, MUMU learns to compose\ninputs from different images into a coherent output. For example, an input of a\nrealistic person and a cartoon will output the same person in the cartoon\nstyle, and an input of a standing subject and a scooter will output the subject\nriding the scooter. As a result, our model generalizes to tasks such as style\ntransfer and character consistency. Our results show the promise of using\nmultimodal models as general purpose controllers for image generation.",
            "upvotes": 7
        },
        "publishedAt": "2024-06-28T12:53:06.029Z",
        "title": "MUMU: Bootstrapping Multimodal Image Generation from Text-to-Image Data",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/624bebf604abc7ebb01789af/nSFVtZNT454aVZkhm5efB.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.18790.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1649143001781-624bebf604abc7ebb01789af.jpeg",
            "fullname": "Apolinário from multimodal AI art",
            "name": "multimodalart",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.08316",
            "authors": [
                {
                    "_id": "666a633a3536e9a956be85aa",
                    "user": {
                        "avatarUrl": "/avatars/06f05622e232304d3f0b8c291f3263be.svg",
                        "isPro": true,
                        "fullname": "Wen-Ding Li",
                        "user": "xu3kev",
                        "type": "user"
                    },
                    "name": "Wen-Ding Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-28T08:03:13.875Z",
                    "hidden": false
                },
                {
                    "_id": "666a633a3536e9a956be85ab",
                    "name": "Kevin Ellis",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-12T15:16:40.000Z",
            "title": "Is Programming by Example solved by LLMs?",
            "summary": "Programming-by-Examples (PBE) aims to generate an algorithm from input-output\nexamples. Such systems are practically and theoretically important: from an\nend-user perspective, they are deployed to millions of people, and from an AI\nperspective, PBE corresponds to a very general form of few-shot inductive\ninference. Given the success of Large Language Models (LLMs) in code-generation\ntasks, we investigate here the extent to which LLMs can be said to have\n`solved' PBE. We experiment on classic domains such as lists and strings, and\nan uncommon graphics programming domain not well represented in typical\npretraining data. We find that pretrained models are not effective at PBE, but\nthat they can be fine-tuned for much higher performance, provided the test\nproblems are in-distribution. We analyze empirically what causes these models\nto succeed and fail, and take steps toward understanding how to achieve better\nout-of-distribution generalization. Collectively these results suggest that\nLLMs make strong progress toward solving the typical suite of PBE tasks,\npotentially increasing the flexibility and applicability of PBE systems, while\nalso identifying ways in which LLMs still fall short.",
            "upvotes": 6
        },
        "publishedAt": "2024-06-28T02:55:19.518Z",
        "title": "Is Programming by Example solved by LLMs?",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.08316.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.19395",
            "authors": [
                {
                    "_id": "667e2f9ccef5b7fa35d65325",
                    "user": {
                        "avatarUrl": "/avatars/fc154a27c8e2e833f830208e05d66f5b.svg",
                        "isPro": false,
                        "fullname": "Mohammad Salama",
                        "user": "MoSalama98",
                        "type": "user"
                    },
                    "name": "Mohammad Salama",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-28T08:02:45.409Z",
                    "hidden": false
                },
                {
                    "_id": "667e2f9ccef5b7fa35d65326",
                    "user": {
                        "avatarUrl": "/avatars/113f02c1b1f8d33d3487daa867afcd3f.svg",
                        "isPro": false,
                        "fullname": "Jonathan Kahana",
                        "user": "jonkahana",
                        "type": "user"
                    },
                    "name": "Jonathan Kahana",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-28T13:19:03.199Z",
                    "hidden": false
                },
                {
                    "_id": "667e2f9ccef5b7fa35d65327",
                    "user": {
                        "avatarUrl": "/avatars/4d3685ff4ba623c7a5b4c0c3c6f08622.svg",
                        "isPro": false,
                        "fullname": "Eliahu Horwitz",
                        "user": "Eliahu",
                        "type": "user"
                    },
                    "name": "Eliahu Horwitz",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-28T13:19:09.094Z",
                    "hidden": false
                },
                {
                    "_id": "667e2f9ccef5b7fa35d65328",
                    "user": {
                        "avatarUrl": "/avatars/19d6ab141ec2cd25c1c3b45fd8f69910.svg",
                        "isPro": false,
                        "fullname": "Yedid Hoshen",
                        "user": "yedid",
                        "type": "user"
                    },
                    "name": "Yedid Hoshen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-28T13:19:14.192Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-27T17:59:53.000Z",
            "title": "Dataset Size Recovery from LoRA Weights",
            "summary": "Model inversion and membership inference attacks aim to reconstruct and\nverify the data which a model was trained on. However, they are not guaranteed\nto find all training samples as they do not know the size of the training set.\nIn this paper, we introduce a new task: dataset size recovery, that aims to\ndetermine the number of samples used to train a model, directly from its\nweights. We then propose DSiRe, a method for recovering the number of images\nused to fine-tune a model, in the common case where fine-tuning uses LoRA. We\ndiscover that both the norm and the spectrum of the LoRA matrices are closely\nlinked to the fine-tuning dataset size; we leverage this finding to propose a\nsimple yet effective prediction algorithm. To evaluate dataset size recovery of\nLoRA weights, we develop and release a new benchmark, LoRA-WiSE, consisting of\nover 25000 weight snapshots from more than 2000 diverse LoRA fine-tuned models.\nOur best classifier can predict the number of fine-tuning images with a mean\nabsolute error of 0.36 images, establishing the feasibility of this attack.",
            "upvotes": 5
        },
        "publishedAt": "2024-06-28T09:41:13.833Z",
        "title": "Dataset Size Recovery from LoRA Weights",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.19395.png",
        "numComments": 4,
        "submittedBy": {
            "avatarUrl": "/avatars/fc154a27c8e2e833f830208e05d66f5b.svg",
            "fullname": "Mohammad Salama",
            "name": "MoSalama98",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.19314",
            "authors": [
                {
                    "_id": "667e723a685216449bd881ad",
                    "user": {
                        "avatarUrl": "/avatars/f5822f3bb4691b3635e2dc4e939da89d.svg",
                        "isPro": false,
                        "fullname": "Colin White",
                        "user": "colin-abacus",
                        "type": "user"
                    },
                    "name": "Colin White",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2024-06-28T16:47:45.586Z",
                    "hidden": false
                },
                {
                    "_id": "667e723a685216449bd881ae",
                    "user": {
                        "avatarUrl": "/avatars/4225df1688219945b19d262d5b44377d.svg",
                        "isPro": false,
                        "fullname": "Samuel",
                        "user": "spamueldooley",
                        "type": "user"
                    },
                    "name": "Samuel Dooley",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-28T14:58:13.864Z",
                    "hidden": false
                },
                {
                    "_id": "667e723a685216449bd881af",
                    "user": {
                        "avatarUrl": "/avatars/c5687ab72bb8c3e865573a3047d48800.svg",
                        "isPro": false,
                        "fullname": "Manley Roberts",
                        "user": "manleyroberts",
                        "type": "user"
                    },
                    "name": "Manley Roberts",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-28T09:09:05.398Z",
                    "hidden": false
                },
                {
                    "_id": "667e723a685216449bd881b0",
                    "user": {
                        "avatarUrl": "/avatars/55d71dc415b8c1f6029ecb233abb866f.svg",
                        "isPro": false,
                        "fullname": "Arka Pal",
                        "user": "ArkaAbacus",
                        "type": "user"
                    },
                    "name": "Arka Pal",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-28T09:09:15.951Z",
                    "hidden": false
                },
                {
                    "_id": "667e723a685216449bd881b1",
                    "name": "Ben Feuer",
                    "hidden": false
                },
                {
                    "_id": "667e723a685216449bd881b2",
                    "name": "Siddhartha Jain",
                    "hidden": false
                },
                {
                    "_id": "667e723a685216449bd881b3",
                    "name": "Ravid Shwartz-Ziv",
                    "hidden": false
                },
                {
                    "_id": "667e723a685216449bd881b4",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e2b1ec282ee5f9624cfbcb/4SVTp93cvRevacoJgiXzS.jpeg",
                        "isPro": false,
                        "fullname": "Neel Jain",
                        "user": "nsjain",
                        "type": "user"
                    },
                    "name": "Neel Jain",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-28T09:09:40.850Z",
                    "hidden": false
                },
                {
                    "_id": "667e723a685216449bd881b5",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/603406de3d343696d96ef2a6/FS3xGIC9G_JpyQ806VAyc.jpeg",
                        "isPro": false,
                        "fullname": "Khalid Saifullah",
                        "user": "khalidsaifullaah",
                        "type": "user"
                    },
                    "name": "Khalid Saifullah",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-28T09:11:55.071Z",
                    "hidden": false
                },
                {
                    "_id": "667e723a685216449bd881b6",
                    "user": {
                        "avatarUrl": "/avatars/98a21dc6874414305e6479d6d2ea5bef.svg",
                        "isPro": false,
                        "fullname": "Siddartha Naidu",
                        "user": "siddartha-abacus",
                        "type": "user"
                    },
                    "name": "Siddartha Naidu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-28T09:11:09.508Z",
                    "hidden": false
                },
                {
                    "_id": "667e723a685216449bd881b7",
                    "user": {
                        "avatarUrl": "/avatars/52c30caa0ee11347f82420a14ec19996.svg",
                        "isPro": false,
                        "fullname": "Chinmay Hegde",
                        "user": "chegde",
                        "type": "user"
                    },
                    "name": "Chinmay Hegde",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-28T09:11:02.411Z",
                    "hidden": false
                },
                {
                    "_id": "667e723a685216449bd881b8",
                    "user": {
                        "avatarUrl": "/avatars/9156dc406ed3f9ee62b73657ac20f5ed.svg",
                        "isPro": false,
                        "fullname": "Yann LeCun",
                        "user": "ylecun",
                        "type": "user"
                    },
                    "name": "Yann LeCun",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-28T09:08:51.180Z",
                    "hidden": false
                },
                {
                    "_id": "667e723a685216449bd881b9",
                    "user": {
                        "avatarUrl": "/avatars/84dfdca8e1cd6fbf50d6fb2a6f1b488d.svg",
                        "isPro": false,
                        "fullname": "Tom Goldstein",
                        "user": "tomgoldstein",
                        "type": "user"
                    },
                    "name": "Tom Goldstein",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-28T09:10:38.927Z",
                    "hidden": false
                },
                {
                    "_id": "667e723a685216449bd881ba",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644bf65522d211df6444a7f4/k_ddZdQDg2fzhwjI1EXyx.jpeg",
                        "isPro": false,
                        "fullname": "Willie Neiswanger",
                        "user": "willieneis",
                        "type": "user"
                    },
                    "name": "Willie Neiswanger",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-28T09:10:31.811Z",
                    "hidden": false
                },
                {
                    "_id": "667e723a685216449bd881bb",
                    "name": "Micah Goldblum",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-27T16:47:42.000Z",
            "title": "LiveBench: A Challenging, Contamination-Free LLM Benchmark",
            "summary": "Test set contamination, wherein test data from a benchmark ends up in a newer\nmodel's training set, is a well-documented obstacle for fair LLM evaluation and\ncan quickly render benchmarks obsolete. To mitigate this, many recent\nbenchmarks crowdsource new prompts and evaluations from human or LLM judges;\nhowever, these can introduce significant biases, and break down when scoring\nhard questions. In this work, we introduce a new benchmark for LLMs designed to\nbe immune to both test set contamination and the pitfalls of LLM judging and\nhuman crowdsourcing. We release LiveBench, the first benchmark that (1)\ncontains frequently-updated questions from recent information sources, (2)\nscores answers automatically according to objective ground-truth values, and\n(3) contains a wide variety of challenging tasks, spanning math, coding,\nreasoning, language, instruction following, and data analysis. To achieve this,\nLiveBench contains questions that are based on recently-released math\ncompetitions, arXiv papers, news articles, and datasets, and it contains\nharder, contamination-free versions of tasks from previous benchmarks such as\nBig-Bench Hard, AMPS, and IFEval. We evaluate many prominent closed-source\nmodels, as well as dozens of open-source models ranging from 0.5B to 110B in\nsize. LiveBench is difficult, with top models achieving below 65% accuracy. We\nrelease all questions, code, and model answers. Questions will be added and\nupdated on a monthly basis, and we will release new tasks and harder versions\nof tasks over time so that LiveBench can distinguish between the capabilities\nof LLMs as they improve in the future. We welcome community engagement and\ncollaboration for expanding the benchmark tasks and models.",
            "upvotes": 5
        },
        "publishedAt": "2024-06-28T06:51:46.460Z",
        "title": "LiveBench: A Challenging, Contamination-Free LLM Benchmark",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.19314.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1627505688463-60107b385ac3e86b3ea4fc34.jpeg",
            "fullname": "Daniel van Strien",
            "name": "davanstrien",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.19263",
            "authors": [
                {
                    "_id": "667e21ecddca7a5233b55dcf",
                    "name": "Yue Fan",
                    "hidden": false
                },
                {
                    "_id": "667e21ecddca7a5233b55dd0",
                    "name": "Lei Ding",
                    "hidden": false
                },
                {
                    "_id": "667e21ecddca7a5233b55dd1",
                    "name": "Ching-Chen Kuo",
                    "hidden": false
                },
                {
                    "_id": "667e21ecddca7a5233b55dd2",
                    "name": "Shan Jiang",
                    "hidden": false
                },
                {
                    "_id": "667e21ecddca7a5233b55dd3",
                    "name": "Yang Zhao",
                    "hidden": false
                },
                {
                    "_id": "667e21ecddca7a5233b55dd4",
                    "name": "Xinze Guan",
                    "hidden": false
                },
                {
                    "_id": "667e21ecddca7a5233b55dd5",
                    "name": "Jie Yang",
                    "hidden": false
                },
                {
                    "_id": "667e21ecddca7a5233b55dd6",
                    "name": "Yi Zhang",
                    "hidden": false
                },
                {
                    "_id": "667e21ecddca7a5233b55dd7",
                    "user": {
                        "avatarUrl": "/avatars/05abee0b6317f100923936ca2099e9eb.svg",
                        "isPro": false,
                        "fullname": "Xin Eric Wang",
                        "user": "xw-eric",
                        "type": "user"
                    },
                    "name": "Xin Eric Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-28T08:02:53.091Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-27T15:34:16.000Z",
            "title": "Read Anywhere Pointed: Layout-aware GUI Screen Reading with Tree-of-Lens\n  Grounding",
            "summary": "Graphical User Interfaces (GUIs) are central to our interaction with digital\ndevices. Recently, growing efforts have been made to build models for various\nGUI understanding tasks. However, these efforts largely overlook an important\nGUI-referring task: screen reading based on user-indicated points, which we\nname the Screen Point-and-Read (SPR) task. This task is predominantly handled\nby rigid accessible screen reading tools, in great need of new models driven by\nadvancements in Multimodal Large Language Models (MLLMs). In this paper, we\npropose a Tree-of-Lens (ToL) agent, utilizing a novel ToL grounding mechanism,\nto address the SPR task. Based on the input point coordinate and the\ncorresponding GUI screenshot, our ToL agent constructs a Hierarchical Layout\nTree. Based on the tree, our ToL agent not only comprehends the content of the\nindicated area but also articulates the layout and spatial relationships\nbetween elements. Such layout information is crucial for accurately\ninterpreting information on the screen, distinguishing our ToL agent from other\nscreen reading tools. We also thoroughly evaluate the ToL agent against other\nbaselines on a newly proposed SPR benchmark, which includes GUIs from mobile,\nweb, and operating systems. Last but not least, we test the ToL agent on mobile\nGUI navigation tasks, demonstrating its utility in identifying incorrect\nactions along the path of agent execution trajectories. Code and data:\nscreen-point-and-read.github.io",
            "upvotes": 5
        },
        "publishedAt": "2024-06-28T01:13:02.526Z",
        "title": "Read Anywhere Pointed: Layout-aware GUI Screen Reading with Tree-of-Lens Grounding",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64679a226192d39142245e5e/TpwXPF2bb2hSkM6eYBIXk.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.19263.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/05abee0b6317f100923936ca2099e9eb.svg",
            "fullname": "Xin Eric Wang",
            "name": "xw-eric",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.18120",
            "authors": [
                {
                    "_id": "667e709e64b6cc4e11f589c5",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
                        "isPro": false,
                        "fullname": "Ahmed Heakl",
                        "user": "ahmedheakl",
                        "type": "user"
                    },
                    "name": "Ahmed Heakl",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-28T09:12:58.124Z",
                    "hidden": false
                },
                {
                    "_id": "667e709e64b6cc4e11f589c6",
                    "user": {
                        "avatarUrl": "/avatars/fe0c0f44e923d3c2a23f9b1e64aecadd.svg",
                        "isPro": false,
                        "fullname": "Youssef Zaghloul",
                        "user": "zaghloul2012",
                        "type": "user"
                    },
                    "name": "Youssef Zaghloul",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-28T13:00:12.828Z",
                    "hidden": false
                },
                {
                    "_id": "667e709e64b6cc4e11f589c7",
                    "name": "Mennatullah Ali",
                    "hidden": false
                },
                {
                    "_id": "667e709e64b6cc4e11f589c8",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677865500544-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Rania hossam elbadry ",
                        "user": "Raniahossam33",
                        "type": "user"
                    },
                    "name": "Rania Hossam",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-28T13:00:30.099Z",
                    "hidden": false
                },
                {
                    "_id": "667e709e64b6cc4e11f589c9",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1672657431429-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Walid Gomaa",
                        "user": "wg-imhotep",
                        "type": "user"
                    },
                    "name": "Walid Gomaa",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-28T13:00:37.094Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-26T07:19:51.000Z",
            "title": "ArzEn-LLM: Code-Switched Egyptian Arabic-English Translation and Speech\n  Recognition Using LLMs",
            "summary": "Motivated by the widespread increase in the phenomenon of code-switching\nbetween Egyptian Arabic and English in recent times, this paper explores the\nintricacies of machine translation (MT) and automatic speech recognition (ASR)\nsystems, focusing on translating code-switched Egyptian Arabic-English to\neither English or Egyptian Arabic. Our goal is to present the methodologies\nemployed in developing these systems, utilizing large language models such as\nLLama and Gemma. In the field of ASR, we explore the utilization of the Whisper\nmodel for code-switched Egyptian Arabic recognition, detailing our experimental\nprocedures including data preprocessing and training techniques. Through the\nimplementation of a consecutive speech-to-text translation system that\nintegrates ASR with MT, we aim to overcome challenges posed by limited\nresources and the unique characteristics of the Egyptian Arabic dialect.\nEvaluation against established metrics showcases promising results, with our\nmethodologies yielding a significant improvement of 56% in English\ntranslation over the state-of-the-art and 9.3% in Arabic translation. Since\ncode-switching is deeply inherent in spoken languages, it is crucial that ASR\nsystems can effectively handle this phenomenon. This capability is crucial for\nenabling seamless interaction in various domains, including business\nnegotiations, cultural exchanges, and academic discourse. Our models and code\nare available as open-source resources. Code:\nhttp://github.com/ahmedheakl/arazn-llm}, Models:\nhttp://huggingface.co/collections/ahmedheakl/arazn-llm-662ceaf12777656607b9524e.",
            "upvotes": 3
        },
        "publishedAt": "2024-06-28T06:46:54.798Z",
        "title": "ArzEn-LLM: Code-Switched Egyptian Arabic-English Translation and Speech Recognition Using LLMs",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/QQzIgmnksfmXp1tZFcoN8.png",
            "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/Vcjbxh5WvE4cP1toXeEvu.png",
            "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/w0lbtx7GKfLYAIyo4nvzS.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.18120.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
            "fullname": "Ahmed Heakl",
            "name": "ahmedheakl",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.18676",
            "authors": [
                {
                    "_id": "667ee6f166aad3ce8aa0184f",
                    "name": "Guanting Dong",
                    "hidden": false
                },
                {
                    "_id": "667ee6f166aad3ce8aa01850",
                    "name": "Yutao Zhu",
                    "hidden": false
                },
                {
                    "_id": "667ee6f166aad3ce8aa01851",
                    "name": "Chenghao Zhang",
                    "hidden": false
                },
                {
                    "_id": "667ee6f166aad3ce8aa01852",
                    "name": "Zechen Wang",
                    "hidden": false
                },
                {
                    "_id": "667ee6f166aad3ce8aa01853",
                    "name": "Zhicheng Dou",
                    "hidden": false
                },
                {
                    "_id": "667ee6f166aad3ce8aa01854",
                    "name": "Ji-Rong Wen",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-26T18:26:53.000Z",
            "title": "Understand What LLM Needs: Dual Preference Alignment for\n  Retrieval-Augmented Generation",
            "summary": "Retrieval-augmented generation (RAG) has demonstrated effectiveness in\nmitigating the hallucination problem of large language models (LLMs). However,\nthe difficulty of aligning the retriever with the diverse LLMs' knowledge\npreferences inevitably poses an inevitable challenge in developing a reliable\nRAG system. To address this issue, we propose DPA-RAG, a universal framework\ndesigned to align diverse knowledge preferences within RAG systems.\nSpecifically, we initially introduce a preference knowledge construction\npipline and incorporate five novel query augmentation strategies to alleviate\npreference data scarcity. Based on preference data, DPA-RAG accomplishes both\nexternal and internal preference alignment: 1) It jointly integrate pair-wise,\npoint-wise, and contrastive preference alignment abilities into the reranker,\nachieving external preference alignment among RAG components. 2) It further\nintroduces a pre-aligned stage before vanilla Supervised Fine-tuning (SFT),\nenabling LLMs to implicitly capture knowledge aligned with their reasoning\npreferences, achieving LLMs' internal alignment. Experimental results across\nfour knowledge-intensive QA datasets demonstrate that DPA-RAG outperforms all\nbaselines and seamlessly integrates both black-box and open-sourced LLM\nreaders. Further qualitative analysis and discussions also provide empirical\nguidance for achieving reliable RAG systems. Our code is publicly available at\nhttps://github.com/dongguanting/DPA-RAG.",
            "upvotes": 2
        },
        "publishedAt": "2024-06-28T15:15:06.382Z",
        "title": "Understand What LLM Needs: Dual Preference Alignment for Retrieval-Augmented Generation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.18676.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "/avatars/23f7a65c6bf56b5ab0f455fea7bb70bf.svg",
            "fullname": "KABI",
            "name": "dongguanting",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.17513",
            "authors": [
                {
                    "_id": "667e925e8a819eb0d305bce8",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66261ff68fee5ba32455ca80/4BWSZjF7nqsFi3i5l40cB.jpeg",
                        "isPro": false,
                        "fullname": "Matteo Bortoletto",
                        "user": "mb22222",
                        "type": "user"
                    },
                    "name": "Matteo Bortoletto",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-28T12:11:38.262Z",
                    "hidden": false
                },
                {
                    "_id": "667e925e8a819eb0d305bce9",
                    "user": {
                        "avatarUrl": "/avatars/df105c2adf10620a19c3ff85d06aa64e.svg",
                        "isPro": false,
                        "fullname": "Constantin Ruhdorfer",
                        "user": "ConstantinRuhdorfer",
                        "type": "user"
                    },
                    "name": "Constantin Ruhdorfer",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-28T13:21:19.749Z",
                    "hidden": false
                },
                {
                    "_id": "667e925e8a819eb0d305bcea",
                    "name": "Lei Shi",
                    "hidden": false
                },
                {
                    "_id": "667e925e8a819eb0d305bceb",
                    "name": "Andreas Bulling",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-25T12:51:06.000Z",
            "title": "Benchmarking Mental State Representations in Language Models",
            "summary": "While numerous works have assessed the generative performance of language\nmodels (LMs) on tasks requiring Theory of Mind reasoning, research into the\nmodels' internal representation of mental states remains limited. Recent work\nhas used probing to demonstrate that LMs can represent beliefs of themselves\nand others. However, these claims are accompanied by limited evaluation, making\nit difficult to assess how mental state representations are affected by model\ndesign and training choices. We report an extensive benchmark with various LM\ntypes with different model sizes, fine-tuning approaches, and prompt designs to\nstudy the robustness of mental state representations and memorisation issues\nwithin the probes. Our results show that the quality of models' internal\nrepresentations of the beliefs of others increases with model size and, more\ncrucially, with fine-tuning. We are the first to study how prompt variations\nimpact probing performance on theory of mind tasks. We demonstrate that models'\nrepresentations are sensitive to prompt variations, even when such variations\nshould be beneficial. Finally, we complement previous activation editing\nexperiments on Theory of Mind tasks and show that it is possible to improve\nmodels' reasoning performance by steering their activations without the need to\ntrain any probe.",
            "upvotes": 1
        },
        "publishedAt": "2024-06-28T11:10:50.504Z",
        "title": "Benchmarking Mental State Representations in Language Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.17513.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66261ff68fee5ba32455ca80/4BWSZjF7nqsFi3i5l40cB.jpeg",
            "fullname": "Matteo Bortoletto",
            "name": "mb22222",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.18125",
            "authors": [
                {
                    "_id": "667e73ec3b2efcc4414c055b",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
                        "isPro": false,
                        "fullname": "Ahmed Heakl",
                        "user": "ahmedheakl",
                        "type": "user"
                    },
                    "name": "Ahmed Heakl",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-28T09:12:59.542Z",
                    "hidden": false
                },
                {
                    "_id": "667e73ec3b2efcc4414c055c",
                    "name": "Youssef Mohamed",
                    "hidden": false
                },
                {
                    "_id": "667e73ec3b2efcc4414c055d",
                    "name": "Noran Mohamed",
                    "hidden": false
                },
                {
                    "_id": "667e73ec3b2efcc4414c055e",
                    "name": "Ali Sharkaway",
                    "hidden": false
                },
                {
                    "_id": "667e73ec3b2efcc4414c055f",
                    "name": "Ahmed Zaky",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-26T07:25:18.000Z",
            "title": "ResumeAtlas: Revisiting Resume Classification with Large-Scale Datasets\n  and Large Language Models",
            "summary": "The increasing reliance on online recruitment platforms coupled with the\nadoption of AI technologies has highlighted the critical need for efficient\nresume classification methods. However, challenges such as small datasets, lack\nof standardized resume templates, and privacy concerns hinder the accuracy and\neffectiveness of existing classification models. In this work, we address these\nchallenges by presenting a comprehensive approach to resume classification. We\ncurated a large-scale dataset of 13,389 resumes from diverse sources and\nemployed Large Language Models (LLMs) such as BERT and Gemma1.1 2B for\nclassification. Our results demonstrate significant improvements over\ntraditional machine learning approaches, with our best model achieving a top-1\naccuracy of 92\\% and a top-5 accuracy of 97.5\\%. These findings underscore the\nimportance of dataset quality and advanced model architectures in enhancing the\naccuracy and robustness of resume classification systems, thus advancing the\nfield of online recruitment practices.",
            "upvotes": 1
        },
        "publishedAt": "2024-06-28T06:58:26.786Z",
        "title": "ResumeAtlas: Revisiting Resume Classification with Large-Scale Datasets and Large Language Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.18125.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
            "fullname": "Ahmed Heakl",
            "name": "ahmedheakl",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.19223",
            "authors": [
                {
                    "_id": "667e76ecf0754a8a1d55471d",
                    "name": "Björn Deiseroth",
                    "hidden": false
                },
                {
                    "_id": "667e76ecf0754a8a1d55471e",
                    "name": "Manuel Brack",
                    "hidden": false
                },
                {
                    "_id": "667e76ecf0754a8a1d55471f",
                    "name": "Patrick Schramowski",
                    "hidden": false
                },
                {
                    "_id": "667e76ecf0754a8a1d554720",
                    "name": "Kristian Kersting",
                    "hidden": false
                },
                {
                    "_id": "667e76ecf0754a8a1d554721",
                    "name": "Samuel Weinbach",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-27T14:49:08.000Z",
            "title": "T-FREE: Tokenizer-Free Generative LLMs via Sparse Representations for\n  Memory-Efficient Embeddings",
            "summary": "Tokenizers are crucial for encoding information in Large Language Models, but\ntheir development has recently stagnated, and they contain inherent weaknesses.\nMajor limitations include computational overhead, ineffective vocabulary use,\nand unnecessarily large embedding and head layers. Additionally, their\nperformance is biased towards a reference corpus, leading to reduced\neffectiveness for underrepresented languages.\n  To remedy these issues, we propose T-FREE, which directly embeds words\nthrough sparse activation patterns over character triplets, and does not\nrequire a reference corpus. T-FREE inherently exploits morphological\nsimilarities and allows for strong compression of embedding layers. In our\nexhaustive experimental evaluation, we achieve competitive downstream\nperformance with a parameter reduction of more than 85% on these layers.\nFurther, T-FREE shows significant improvements in cross-lingual transfer\nlearning.",
            "upvotes": 0
        },
        "publishedAt": "2024-06-28T15:35:41.873Z",
        "title": "T-FREE: Tokenizer-Free Generative LLMs via Sparse Representations for Memory-Efficient Embeddings",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.19223.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62fa1d95e8c9c532aa75331c/WFfk_n8gOj845pSkfdazA.jpeg",
            "fullname": "Manuel Brack",
            "name": "mbrack",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    }
]