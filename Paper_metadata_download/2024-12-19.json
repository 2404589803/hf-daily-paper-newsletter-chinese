[
    "{'paper': {'id': '2412.14161', 'authors': [{'_id': '6763867f523f25389126b2aa', 'name': 'Frank F. Xu', 'hidden': False}, {'_id': '6763867f523f25389126b2ab', 'name': 'Yufan Song', 'hidden': False}, {'_id': '6763867f523f25389126b2ac', 'name': 'Boxuan Li', 'hidden': False}, {'_id': '6763867f523f25389126b2ad', 'name': 'Yuxuan Tang', 'hidden': False}, {'_id': '6763867f523f25389126b2ae', 'name': 'Kritanjali Jain', 'hidden': False}, {'_id': '6763867f523f25389126b2af', 'name': 'Mengxue Bao', 'hidden': False}, {'_id': '6763867f523f25389126b2b0', 'name': 'Zora Z. Wang', 'hidden': False}, {'_id': '6763867f523f25389126b2b1', 'name': 'Xuhui Zhou', 'hidden': False}, {'_id': '6763867f523f25389126b2b2', 'name': 'Zhitong Guo', 'hidden': False}, {'_id': '6763867f523f25389126b2b3', 'name': 'Murong Cao', 'hidden': False}, {'_id': '6763867f523f25389126b2b4', 'name': 'Mingyang Yang', 'hidden': False}, {'_id': '6763867f523f25389126b2b5', 'name': 'Hao Yang Lu', 'hidden': False}, {'_id': '6763867f523f25389126b2b6', 'name': 'Amaad Martin', 'hidden': False}, {'_id': '6763867f523f25389126b2b7', 'name': 'Zhe Su', 'hidden': False}, {'_id': '6763867f523f25389126b2b8', 'name': 'Leander Maben', 'hidden': False}, {'_id': '6763867f523f25389126b2b9', 'name': 'Raj Mehta', 'hidden': False}, {'_id': '6763867f523f25389126b2ba', 'name': 'Wayne Chi', 'hidden': False}, {'_id': '6763867f523f25389126b2bb', 'name': 'Lawrence Jang', 'hidden': False}, {'_id': '6763867f523f25389126b2bc', 'name': 'Yiqing Xie', 'hidden': False}, {'_id': '6763867f523f25389126b2bd', 'name': 'Shuyan Zhou', 'hidden': False}, {'_id': '6763867f523f25389126b2be', 'name': 'Graham Neubig', 'hidden': False}], 'publishedAt': '2024-12-18T18:55:40.000Z', 'title': 'TheAgentCompany: Benchmarking LLM Agents on Consequential Real World\\n  Tasks', 'summary': \"We interact with computers on an everyday basis, be it in everyday life or\\nwork, and many aspects of work can be done entirely with access to a computer\\nand the Internet. At the same time, thanks to improvements in large language\\nmodels (LLMs), there has also been a rapid development in AI agents that\\ninteract with and affect change in their surrounding environments. But how\\nperformant are AI agents at helping to accelerate or even autonomously perform\\nwork-related tasks? The answer to this question has important implications for\\nboth industry looking to adopt AI into their workflows, and for economic policy\\nto understand the effects that adoption of AI may have on the labor market. To\\nmeasure the progress of these LLM agents' performance on performing real-world\\nprofessional tasks, in this paper, we introduce TheAgentCompany, an extensible\\nbenchmark for evaluating AI agents that interact with the world in similar ways\\nto those of a digital worker: by browsing the Web, writing code, running\\nprograms, and communicating with other coworkers. We build a self-contained\\nenvironment with internal web sites and data that mimics a small software\\ncompany environment, and create a variety of tasks that may be performed by\\nworkers in such a company. We test baseline agents powered by both closed\\nAPI-based and open-weights language models (LMs), and find that with the most\\ncompetitive agent, 24% of the tasks can be completed autonomously. This paints\\na nuanced picture on task automation with LM agents -- in a setting simulating\\na real workplace, a good portion of simpler tasks could be solved autonomously,\\nbut more difficult long-horizon tasks are still beyond the reach of current\\nsystems.\", 'upvotes': 25, 'discussionId': '67638680523f25389126b310'}, 'publishedAt': '2024-12-18T21:37:38.945Z', 'title': 'TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.14161.png', 'numComments': 1, 'submittedBy': {'_id': '6230d750d93e84e233882dbc', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6230d750d93e84e233882dbc/4MGEekLW3oWzqeFWDWvIK.jpeg', 'fullname': 'Xiang Yue', 'name': 'yuexiang96', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 25}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2412.14173', 'authors': [{'_id': '67639f320e317931ea6264d5', 'name': 'Yihao Meng', 'hidden': False}, {'_id': '67639f320e317931ea6264d6', 'name': 'Hao Ouyang', 'hidden': False}, {'_id': '67639f320e317931ea6264d7', 'name': 'Hanlin Wang', 'hidden': False}, {'_id': '67639f320e317931ea6264d8', 'name': 'Qiuyu Wang', 'hidden': False}, {'_id': '67639f320e317931ea6264d9', 'name': 'Wen Wang', 'hidden': False}, {'_id': '67639f320e317931ea6264da', 'name': 'Ka Leong Cheng', 'hidden': False}, {'_id': '67639f320e317931ea6264db', 'name': 'Zhiheng Liu', 'hidden': False}, {'_id': '67639f320e317931ea6264dc', 'name': 'Yujun Shen', 'hidden': False}, {'_id': '67639f320e317931ea6264dd', 'name': 'Huamin Qu', 'hidden': False}], 'publishedAt': '2024-12-18T18:59:59.000Z', 'title': 'AniDoc: Animation Creation Made Easier', 'summary': 'The production of 2D animation follows an industry-standard workflow,\\nencompassing four essential stages: character design, keyframe animation,\\nin-betweening, and coloring. Our research focuses on reducing the labor costs\\nin the above process by harnessing the potential of increasingly powerful\\ngenerative AI. Using video diffusion models as the foundation, AniDoc emerges\\nas a video line art colorization tool, which automatically converts sketch\\nsequences into colored animations following the reference character\\nspecification. Our model exploits correspondence matching as an explicit\\nguidance, yielding strong robustness to the variations (e.g., posture) between\\nthe reference character and each line art frame. In addition, our model could\\neven automate the in-betweening process, such that users can easily create a\\ntemporally consistent animation by simply providing a character image as well\\nas the start and end sketches. Our code is available at:\\nhttps://yihao-meng.github.io/AniDoc_demo.', 'upvotes': 22, 'discussionId': '67639f330e317931ea62652d'}, 'publishedAt': '2024-12-18T23:22:28.086Z', 'title': 'AniDoc: Animation Creation Made Easier', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/656084f44e8918182d4f07c8/ERpj1Kg48RakJ2B5pP-Sz.mp4'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.14173.png', 'numComments': 1, 'submittedBy': {'_id': '656084f44e8918182d4f07c8', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/akAvCUCi7eR31PWOXrVPw.jpeg', 'fullname': 'Yihao Meng', 'name': 'Yhmeng1106', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2412.14168', 'authors': [{'_id': '6763939d3515f1ec56b43dcd', 'name': 'Sihui Ji', 'hidden': False}, {'_id': '6763939d3515f1ec56b43dce', 'name': 'Yiyang Wang', 'hidden': False}, {'_id': '6763939d3515f1ec56b43dcf', 'name': 'Xi Chen', 'hidden': False}, {'_id': '6763939d3515f1ec56b43dd0', 'name': 'Xiaogang Xu', 'hidden': False}, {'_id': '6763939d3515f1ec56b43dd1', 'name': 'Hao Luo', 'hidden': False}, {'_id': '6763939d3515f1ec56b43dd2', 'name': 'Hengshuang Zhao', 'hidden': False}], 'publishedAt': '2024-12-18T18:59:50.000Z', 'title': 'FashionComposer: Compositional Fashion Image Generation', 'summary': 'We present FashionComposer for compositional fashion image generation. Unlike\\nprevious methods, FashionComposer is highly flexible. It takes multi-modal\\ninput (i.e., text prompt, parametric human model, garment image, and face\\nimage) and supports personalizing the appearance, pose, and figure of the human\\nand assigning multiple garments in one pass. To achieve this, we first develop\\na universal framework capable of handling diverse input modalities. We\\nconstruct scaled training data to enhance the model\\'s robust compositional\\ncapabilities. To accommodate multiple reference images (garments and faces)\\nseamlessly, we organize these references in a single image as an \"asset\\nlibrary\" and employ a reference UNet to extract appearance features. To inject\\nthe appearance features into the correct pixels in the generated result, we\\npropose subject-binding attention. It binds the appearance features from\\ndifferent \"assets\" with the corresponding text features. In this way, the model\\ncould understand each asset according to their semantics, supporting arbitrary\\nnumbers and types of reference images. As a comprehensive solution,\\nFashionComposer also supports many other applications like human album\\ngeneration, diverse virtual try-on tasks, etc.', 'upvotes': 11, 'discussionId': '676393a03515f1ec56b43e5e'}, 'publishedAt': '2024-12-18T22:31:49.916Z', 'title': 'FashionComposer: Compositional Fashion Image Generation', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.14168.png', 'numComments': 1, 'submittedBy': {'_id': '644a1b6401e18bf93a6f45c1', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/644a1b6401e18bf93a6f45c1/P0i_CgCrIzOS2tYRlxoE9.png', 'fullname': 'xichen', 'name': 'xichenhku', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 39}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2412.12953', 'authors': [{'_id': '6763c3475f32892ca1133cc2', 'name': 'Moritz Reuss', 'hidden': False}, {'_id': '6763c3475f32892ca1133cc3', 'name': 'Jyothish Pari', 'hidden': False}, {'_id': '6763c3475f32892ca1133cc4', 'name': 'Pulkit Agrawal', 'hidden': False}, {'_id': '6763c3475f32892ca1133cc5', 'name': 'Rudolf Lioutikov', 'hidden': False}], 'publishedAt': '2024-12-17T14:34:51.000Z', 'title': 'Efficient Diffusion Transformer Policies with Mixture of Expert\\n  Denoisers for Multitask Learning', 'summary': \"Diffusion Policies have become widely used in Imitation Learning, offering\\nseveral appealing properties, such as generating multimodal and discontinuous\\nbehavior. As models are becoming larger to capture more complex capabilities,\\ntheir computational demands increase, as shown by recent scaling laws.\\nTherefore, continuing with the current architectures will present a\\ncomputational roadblock. To address this gap, we propose Mixture-of-Denoising\\nExperts (MoDE) as a novel policy for Imitation Learning. MoDE surpasses current\\nstate-of-the-art Transformer-based Diffusion Policies while enabling\\nparameter-efficient scaling through sparse experts and noise-conditioned\\nrouting, reducing both active parameters by 40% and inference costs by 90% via\\nexpert caching. Our architecture combines this efficient scaling with\\nnoise-conditioned self-attention mechanism, enabling more effective denoising\\nacross different noise levels. MoDE achieves state-of-the-art performance on\\n134 tasks in four established imitation learning benchmarks (CALVIN and\\nLIBERO). Notably, by pretraining MoDE on diverse robotics data, we achieve 4.01\\non CALVIN ABC and 0.95 on LIBERO-90. It surpasses both CNN-based and\\nTransformer Diffusion Policies by an average of 57% across 4 benchmarks, while\\nusing 90% fewer FLOPs and fewer active parameters compared to default Diffusion\\nTransformer architectures. Furthermore, we conduct comprehensive ablations on\\nMoDE's components, providing insights for designing efficient and scalable\\nTransformer architectures for Diffusion Policies. Code and demonstrations are\\navailable at https://mbreuss.github.io/MoDE_Diffusion_Policy/.\", 'upvotes': 9, 'discussionId': '6763c3495f32892ca1133d5a'}, 'publishedAt': '2024-12-19T02:06:29.170Z', 'title': 'Efficient Diffusion Transformer Policies with Mixture of Expert Denoisers for Multitask Learning', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.12953.png', 'numComments': 1, 'submittedBy': {'_id': '650af93422ce64f22b619549', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/RBybzGRphbiV2MXYHrDoc.png', 'fullname': 'Moritz Reuss', 'name': 'mbreuss', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 2}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2412.14015', 'authors': [{'_id': '67638f91d5948d839a423200', 'name': 'Haotong Lin', 'hidden': False}, {'_id': '67638f91d5948d839a423201', 'name': 'Sida Peng', 'hidden': False}, {'_id': '67638f91d5948d839a423202', 'name': 'Jingxiao Chen', 'hidden': False}, {'_id': '67638f91d5948d839a423203', 'name': 'Songyou Peng', 'hidden': False}, {'_id': '67638f91d5948d839a423204', 'name': 'Jiaming Sun', 'hidden': False}, {'_id': '67638f91d5948d839a423205', 'name': 'Minghuan Liu', 'hidden': False}, {'_id': '67638f91d5948d839a423206', 'name': 'Hujun Bao', 'hidden': False}, {'_id': '67638f91d5948d839a423207', 'name': 'Jiashi Feng', 'hidden': False}, {'_id': '67638f91d5948d839a423208', 'name': 'Xiaowei Zhou', 'hidden': False}, {'_id': '67638f91d5948d839a423209', 'name': 'Bingyi Kang', 'hidden': False}], 'publishedAt': '2024-12-18T16:32:12.000Z', 'title': 'Prompting Depth Anything for 4K Resolution Accurate Metric Depth\\n  Estimation', 'summary': 'Prompts play a critical role in unleashing the power of language and vision\\nfoundation models for specific tasks. For the first time, we introduce\\nprompting into depth foundation models, creating a new paradigm for metric\\ndepth estimation termed Prompt Depth Anything. Specifically, we use a low-cost\\nLiDAR as the prompt to guide the Depth Anything model for accurate metric depth\\noutput, achieving up to 4K resolution. Our approach centers on a concise prompt\\nfusion design that integrates the LiDAR at multiple scales within the depth\\ndecoder. To address training challenges posed by limited datasets containing\\nboth LiDAR depth and precise GT depth, we propose a scalable data pipeline that\\nincludes synthetic data LiDAR simulation and real data pseudo GT depth\\ngeneration. Our approach sets new state-of-the-arts on the ARKitScenes and\\nScanNet++ datasets and benefits downstream applications, including 3D\\nreconstruction and generalized robotic grasping.', 'upvotes': 9, 'discussionId': '67638f97d5948d839a423462'}, 'publishedAt': '2024-12-18T22:16:12.258Z', 'title': 'Prompting Depth Anything for 4K Resolution Accurate Metric Depth Estimation', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/647b5fef6a79fbf5e996c47c/wKMWPo9eKaISJ5zpHvAYR.mp4'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.14015.png', 'numComments': 3, 'submittedBy': {'_id': '647b5fef6a79fbf5e996c47c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/647b5fef6a79fbf5e996c47c/IkSMnDsCY_CyEFCiMDuxe.jpeg', 'fullname': 'Bingyi Kang', 'name': 'bykang', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 3}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2412.13795', 'authors': [{'_id': '6763db41242cbc4c0baf209b', 'name': 'Pengxiang Li', 'hidden': False}, {'_id': '6763db41242cbc4c0baf209c', 'name': 'Lu Yin', 'hidden': False}, {'_id': '6763db41242cbc4c0baf209d', 'name': 'Shiwei Liu', 'hidden': False}], 'publishedAt': '2024-12-18T12:39:53.000Z', 'title': 'Mix-LN: Unleashing the Power of Deeper Layers by Combining Pre-LN and\\n  Post-LN', 'summary': 'Large Language Models (LLMs) have achieved remarkable success, yet recent\\nfindings reveal that their deeper layers often contribute minimally and can be\\npruned without affecting overall performance. While some view this as an\\nopportunity for model compression, we identify it as a training shortfall\\nrooted in the widespread use of Pre-Layer Normalization (Pre-LN). We\\ndemonstrate that Pre-LN, commonly employed in models like GPT and LLaMA, leads\\nto diminished gradient norms in its deeper layers, reducing their\\neffectiveness. In contrast, Post-Layer Normalization (Post-LN) preserves larger\\ngradient norms in deeper layers but suffers from vanishing gradients in earlier\\nlayers. To address this, we introduce Mix-LN, a novel normalization technique\\nthat combines the strengths of Pre-LN and Post-LN within the same model. Mix-LN\\napplies Post-LN to the earlier layers and Pre-LN to the deeper layers, ensuring\\nmore uniform gradients across layers. This allows all parts of the\\nnetwork--both shallow and deep layers--to contribute effectively to training.\\nExtensive experiments with various model sizes from 70M to 7B demonstrate that\\nMix-LN consistently outperforms both Pre-LN and Post-LN, promoting more\\nbalanced, healthier gradient norms throughout the network, and enhancing the\\noverall quality of LLM pre-training. Furthermore, we demonstrate that models\\npre-trained with Mix-LN learn better compared to those using Pre-LN or Post-LN\\nduring supervised fine-tuning (SFT) and reinforcement learning from human\\nfeedback (RLHF), highlighting the critical importance of high-quality deep\\nlayers. By effectively addressing the inefficiencies of deep layers in current\\nLLMs, Mix-LN unlocks their potential, enhancing model capacity without\\nincreasing model size. Our code is available at\\nhttps://github.com/pixeli99/MixLN.', 'upvotes': 8, 'discussionId': '6763db42242cbc4c0baf20bd'}, 'publishedAt': '2024-12-19T03:40:19.529Z', 'title': 'Mix-LN: Unleashing the Power of Deeper Layers by Combining Pre-LN and Post-LN', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/64245f2c089d5fae56b4549a/XZDEoY5uCb2S28DJ0fNIv.png'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.13795.png', 'numComments': 1, 'submittedBy': {'_id': '64245f2c089d5fae56b4549a', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64245f2c089d5fae56b4549a/qUHFsL9Svwyj5BKpfMtaY.jpeg', 'fullname': 'Pengxiang Li', 'name': 'pengxiang', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 2}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2412.13501', 'authors': [{'_id': '676389c7c998c60976a9a943', 'user': {'_id': '64ba4c2565535cf237da429a', 'avatarUrl': '/avatars/a8af0e748686e9d3364f1beaa5039ddb.svg', 'isPro': False, 'fullname': 'Dang Nguyen', 'user': 'dangmn', 'type': 'user'}, 'name': 'Dang Nguyen', 'status': 'extracted_pending', 'statusLastChangedAt': '2024-12-19T02:49:44.513Z', 'hidden': False}, {'_id': '676389c7c998c60976a9a944', 'name': 'Jian Chen', 'hidden': False}, {'_id': '676389c7c998c60976a9a945', 'name': 'Yu Wang', 'hidden': False}, {'_id': '676389c7c998c60976a9a946', 'name': 'Gang Wu', 'hidden': False}, {'_id': '676389c7c998c60976a9a947', 'name': 'Namyong Park', 'hidden': False}, {'_id': '676389c7c998c60976a9a948', 'name': 'Zhengmian Hu', 'hidden': False}, {'_id': '676389c7c998c60976a9a949', 'name': 'Hanjia Lyu', 'hidden': False}, {'_id': '676389c7c998c60976a9a94a', 'name': 'Junda Wu', 'hidden': False}, {'_id': '676389c7c998c60976a9a94b', 'name': 'Ryan Aponte', 'hidden': False}, {'_id': '676389c7c998c60976a9a94c', 'name': 'Yu Xia', 'hidden': False}, {'_id': '676389c7c998c60976a9a94d', 'name': 'Xintong Li', 'hidden': False}, {'_id': '676389c7c998c60976a9a94e', 'name': 'Jing Shi', 'hidden': False}, {'_id': '676389c7c998c60976a9a94f', 'name': 'Hongjie Chen', 'hidden': False}, {'_id': '676389c7c998c60976a9a950', 'name': 'Viet Dac Lai', 'hidden': False}, {'_id': '676389c7c998c60976a9a951', 'name': 'Zhouhang Xie', 'hidden': False}, {'_id': '676389c7c998c60976a9a952', 'name': 'Sungchul Kim', 'hidden': False}, {'_id': '676389c7c998c60976a9a953', 'name': 'Ruiyi Zhang', 'hidden': False}, {'_id': '676389c7c998c60976a9a954', 'name': 'Tong Yu', 'hidden': False}, {'_id': '676389c7c998c60976a9a955', 'name': 'Mehrab Tanjim', 'hidden': False}, {'_id': '676389c7c998c60976a9a956', 'name': 'Nesreen K. Ahmed', 'hidden': False}, {'_id': '676389c7c998c60976a9a957', 'name': 'Puneet Mathur', 'hidden': False}, {'_id': '676389c7c998c60976a9a958', 'name': 'Seunghyun Yoon', 'hidden': False}, {'_id': '676389c7c998c60976a9a959', 'name': 'Lina Yao', 'hidden': False}, {'_id': '676389c7c998c60976a9a95a', 'name': 'Branislav Kveton', 'hidden': False}, {'_id': '676389c7c998c60976a9a95b', 'name': 'Thien Huu Nguyen', 'hidden': False}, {'_id': '676389c7c998c60976a9a95c', 'name': 'Trung Bui', 'hidden': False}, {'_id': '676389c7c998c60976a9a95d', 'name': 'Tianyi Zhou', 'hidden': False}, {'_id': '676389c7c998c60976a9a95e', 'name': 'Ryan A. Rossi', 'hidden': False}, {'_id': '676389c7c998c60976a9a95f', 'user': {'_id': '62c5947524171688a9feb992', 'avatarUrl': '/avatars/5a151713b9eae8dc566f5957acee3475.svg', 'isPro': False, 'fullname': 'Franck Dernoncourt', 'user': 'Franck-Dernoncourt', 'type': 'user'}, 'name': 'Franck Dernoncourt', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-19T13:18:13.285Z', 'hidden': False}], 'publishedAt': '2024-12-18T04:48:28.000Z', 'title': 'GUI Agents: A Survey', 'summary': 'Graphical User Interface (GUI) agents, powered by Large Foundation Models,\\nhave emerged as a transformative approach to automating human-computer\\ninteraction. These agents autonomously interact with digital systems or\\nsoftware applications via GUIs, emulating human actions such as clicking,\\ntyping, and navigating visual elements across diverse platforms. Motivated by\\nthe growing interest and fundamental importance of GUI agents, we provide a\\ncomprehensive survey that categorizes their benchmarks, evaluation metrics,\\narchitectures, and training methods. We propose a unified framework that\\ndelineates their perception, reasoning, planning, and acting capabilities.\\nFurthermore, we identify important open challenges and discuss key future\\ndirections. Finally, this work serves as a basis for practitioners and\\nresearchers to gain an intuitive understanding of current progress, techniques,\\nbenchmarks, and critical open problems that remain to be addressed.', 'upvotes': 8, 'discussionId': '676389c8c998c60976a9a9aa'}, 'publishedAt': '2024-12-18T21:49:51.083Z', 'title': 'GUI Agents: A Survey', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.13501.png', 'numComments': 1, 'submittedBy': {'_id': '62c5947524171688a9feb992', 'avatarUrl': '/avatars/5a151713b9eae8dc566f5957acee3475.svg', 'fullname': 'Franck Dernoncourt', 'name': 'Franck-Dernoncourt', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 3}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2412.14123', 'authors': [{'_id': '6763cdf3725a6df3137cc526', 'name': 'Guillaume Astruc', 'hidden': False}, {'_id': '6763cdf3725a6df3137cc527', 'name': 'Nicolas Gonthier', 'hidden': False}, {'_id': '6763cdf3725a6df3137cc528', 'name': 'Clement Mallet', 'hidden': False}, {'_id': '6763cdf3725a6df3137cc529', 'name': 'Loic Landrieu', 'hidden': False}], 'publishedAt': '2024-12-18T18:11:53.000Z', 'title': 'AnySat: An Earth Observation Model for Any Resolutions, Scales, and\\n  Modalities', 'summary': 'Geospatial models must adapt to the diversity of Earth observation data in\\nterms of resolutions, scales, and modalities. However, existing approaches\\nexpect fixed input configurations, which limits their practical applicability.\\nWe propose AnySat, a multimodal model based on joint embedding predictive\\narchitecture (JEPA) and resolution-adaptive spatial encoders, allowing us to\\ntrain a single model on highly heterogeneous data in a self-supervised manner.\\nTo demonstrate the advantages of this unified approach, we compile GeoPlex, a\\ncollection of 5 multimodal datasets with varying characteristics and 11\\ndistinct sensors. We then train a single powerful model on these diverse\\ndatasets simultaneously. Once fine-tuned, we achieve better or near\\nstate-of-the-art results on the datasets of GeoPlex and 4 additional ones for\\n5 environment monitoring tasks: land cover mapping, tree species\\nidentification, crop type classification, change detection, and flood\\nsegmentation. The code and models are available at\\nhttps://github.com/gastruc/AnySat.', 'upvotes': 7, 'discussionId': '6763cdf5725a6df3137cc5a3'}, 'publishedAt': '2024-12-19T02:54:01.783Z', 'title': 'AnySat: An Earth Observation Model for Any Resolutions, Scales, and Modalities', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/662b7fba68ed7bbf40bfb0df/tkX2W_DAs8f_9F-79IKfv.png'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.14123.png', 'numComments': 1, 'submittedBy': {'_id': '662b7fba68ed7bbf40bfb0df', 'avatarUrl': '/avatars/c04f8fd10bc78a6613d5a0c74bda3cef.svg', 'fullname': 'Guillaume Astruc', 'name': 'g-astruc', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2412.13746', 'authors': [{'_id': '67638ce2995c144ae40aaf21', 'name': 'Zhuoran Jin', 'hidden': False}, {'_id': '67638ce2995c144ae40aaf22', 'name': 'Hongbang Yuan', 'hidden': False}, {'_id': '67638ce2995c144ae40aaf23', 'name': 'Tianyi Men', 'hidden': False}, {'_id': '67638ce2995c144ae40aaf24', 'name': 'Pengfei Cao', 'hidden': False}, {'_id': '67638ce2995c144ae40aaf25', 'name': 'Yubo Chen', 'hidden': False}, {'_id': '67638ce2995c144ae40aaf26', 'name': 'Kang Liu', 'hidden': False}, {'_id': '67638ce2995c144ae40aaf27', 'name': 'Jun Zhao', 'hidden': False}], 'publishedAt': '2024-12-18T11:28:05.000Z', 'title': 'RAG-RewardBench: Benchmarking Reward Models in Retrieval Augmented\\n  Generation for Preference Alignment', 'summary': 'Despite the significant progress made by existing retrieval augmented\\nlanguage models (RALMs) in providing trustworthy responses and grounding in\\nreliable sources, they often overlook effective alignment with human\\npreferences. In the alignment process, reward models (RMs) act as a crucial\\nproxy for human values to guide optimization. However, it remains unclear how\\nto evaluate and select a reliable RM for preference alignment in RALMs. To this\\nend, we propose RAG-RewardBench, the first benchmark for evaluating RMs in RAG\\nsettings. First, we design four crucial and challenging RAG-specific scenarios\\nto assess RMs, including multi-hop reasoning, fine-grained citation,\\nappropriate abstain, and conflict robustness. Then, we incorporate 18 RAG\\nsubsets, six retrievers, and 24 RALMs to increase the diversity of data\\nsources. Finally, we adopt an LLM-as-a-judge approach to improve preference\\nannotation efficiency and effectiveness, exhibiting a strong correlation with\\nhuman annotations. Based on the RAG-RewardBench, we conduct a comprehensive\\nevaluation of 45 RMs and uncover their limitations in RAG scenarios.\\nAdditionally, we also reveal that existing trained RALMs show almost no\\nimprovement in preference alignment, highlighting the need for a shift towards\\npreference-aligned training.We release our benchmark and code publicly at\\nhttps://huggingface.co/datasets/jinzhuoran/RAG-RewardBench/ for future work.', 'upvotes': 6, 'discussionId': '67638ce2995c144ae40aaf6e'}, 'publishedAt': '2024-12-18T22:04:49.058Z', 'title': 'RAG-RewardBench: Benchmarking Reward Models in Retrieval Augmented Generation for Preference Alignment', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.13746.png', 'numComments': 1, 'submittedBy': {'_id': '643379416c6ecd58798421b3', 'avatarUrl': '/avatars/831db7eab2663abc33b176cf386b02f2.svg', 'fullname': 'Zhuoran Jin', 'name': 'jinzhuoran', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2412.13871', 'authors': [{'_id': '67640aa2eb7128270fa0404a', 'name': 'Yipeng Zhang', 'hidden': False}, {'_id': '67640aa2eb7128270fa0404b', 'name': 'Yifan Liu', 'hidden': False}, {'_id': '67640aa2eb7128270fa0404c', 'user': {'_id': '6491af36c1741666238f3bff', 'avatarUrl': '/avatars/0ee7d2ec1566e2cc5e8f144140e17f00.svg', 'isPro': False, 'fullname': 'Zonghao Guo', 'user': 'guozonghao96', 'type': 'user'}, 'name': 'Zonghao Guo', 'status': 'extracted_pending', 'statusLastChangedAt': '2024-12-19T11:59:32.252Z', 'hidden': False}, {'_id': '67640aa2eb7128270fa0404d', 'name': 'Yidan Zhang', 'hidden': False}, {'_id': '67640aa2eb7128270fa0404e', 'name': 'Xuesong Yang', 'hidden': False}, {'_id': '67640aa2eb7128270fa0404f', 'name': 'Chi Chen', 'hidden': False}, {'_id': '67640aa2eb7128270fa04050', 'name': 'Jun Song', 'hidden': False}, {'_id': '67640aa2eb7128270fa04051', 'name': 'Bo Zheng', 'hidden': False}, {'_id': '67640aa2eb7128270fa04052', 'name': 'Yuan Yao', 'hidden': False}, {'_id': '67640aa2eb7128270fa04053', 'name': 'Zhiyuan Liu', 'hidden': False}, {'_id': '67640aa2eb7128270fa04054', 'name': 'Tat-Seng Chua', 'hidden': False}, {'_id': '67640aa2eb7128270fa04055', 'name': 'Maosong Sun', 'hidden': False}], 'publishedAt': '2024-12-18T14:07:46.000Z', 'title': 'LLaVA-UHD v2: an MLLM Integrating High-Resolution Feature Pyramid via\\n  Hierarchical Window Transformer', 'summary': 'In multimodal large language models (MLLMs), vision transformers (ViTs) are\\nwidely employed for visual encoding. However, their performance in solving\\nuniversal MLLM tasks is not satisfactory. We attribute it to a lack of\\ninformation from diverse visual levels, impeding alignment with the various\\nsemantic granularity required for language generation. To address this issue,\\nwe present LLaVA-UHD v2, an advanced MLLM centered around a Hierarchical window\\ntransformer that enables capturing diverse visual granularity by constructing\\nand integrating a high-resolution feature pyramid. As a vision-language\\nprojector, Hiwin transformer comprises two primary modules: (i) an inverse\\nfeature pyramid, constructed by a ViT-derived feature up-sampling process\\nutilizing high-frequency details from an image pyramid, and (ii) hierarchical\\nwindow attention, focusing on a set of key sampling features within cross-scale\\nwindows to condense multi-level feature maps. Extensive experiments demonstrate\\nthat LLaVA-UHD v2 achieves superior performance over existing MLLMs on popular\\nbenchmarks. Notably, our design brings an average boost of 3.7% across 14\\nbenchmarks compared with the baseline method, 9.3% on DocVQA for instance. We\\nmake all the data, model checkpoint, and code publicly available to facilitate\\nfuture research.', 'upvotes': 4, 'discussionId': '67640aa4eb7128270fa040c1'}, 'publishedAt': '2024-12-19T07:06:48.362Z', 'title': 'LLaVA-UHD v2: an MLLM Integrating High-Resolution Feature Pyramid via Hierarchical Window Transformer', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.13871.png', 'numComments': 1, 'submittedBy': {'_id': '6491af36c1741666238f3bff', 'avatarUrl': '/avatars/0ee7d2ec1566e2cc5e8f144140e17f00.svg', 'fullname': 'Zonghao Guo', 'name': 'guozonghao96', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2412.14172', 'authors': [{'_id': '6763c8d3ad38702e6c9c1253', 'name': 'Jiageng Mao', 'hidden': False}, {'_id': '6763c8d3ad38702e6c9c1254', 'name': 'Siheng Zhao', 'hidden': False}, {'_id': '6763c8d3ad38702e6c9c1255', 'name': 'Siqi Song', 'hidden': False}, {'_id': '6763c8d3ad38702e6c9c1256', 'name': 'Tianheng Shi', 'hidden': False}, {'_id': '6763c8d3ad38702e6c9c1257', 'name': 'Junjie Ye', 'hidden': False}, {'_id': '6763c8d3ad38702e6c9c1258', 'name': 'Mingtong Zhang', 'hidden': False}, {'_id': '6763c8d3ad38702e6c9c1259', 'name': 'Haoran Geng', 'hidden': False}, {'_id': '6763c8d3ad38702e6c9c125a', 'name': 'Jitendra Malik', 'hidden': False}, {'_id': '6763c8d3ad38702e6c9c125b', 'name': 'Vitor Guizilini', 'hidden': False}, {'_id': '6763c8d3ad38702e6c9c125c', 'name': 'Yue Wang', 'hidden': False}], 'publishedAt': '2024-12-18T18:59:56.000Z', 'title': 'Learning from Massive Human Videos for Universal Humanoid Pose Control', 'summary': 'Scalable learning of humanoid robots is crucial for their deployment in\\nreal-world applications. While traditional approaches primarily rely on\\nreinforcement learning or teleoperation to achieve whole-body control, they are\\noften limited by the diversity of simulated environments and the high costs of\\ndemonstration collection. In contrast, human videos are ubiquitous and present\\nan untapped source of semantic and motion information that could significantly\\nenhance the generalization capabilities of humanoid robots. This paper\\nintroduces Humanoid-X, a large-scale dataset of over 20 million humanoid robot\\nposes with corresponding text-based motion descriptions, designed to leverage\\nthis abundant data. Humanoid-X is curated through a comprehensive pipeline:\\ndata mining from the Internet, video caption generation, motion retargeting of\\nhumans to humanoid robots, and policy learning for real-world deployment. With\\nHumanoid-X, we further train a large humanoid model, UH-1, which takes text\\ninstructions as input and outputs corresponding actions to control a humanoid\\nrobot. Extensive simulated and real-world experiments validate that our\\nscalable training approach leads to superior generalization in text-based\\nhumanoid control, marking a significant step toward adaptable, real-world-ready\\nhumanoid robots.', 'upvotes': 4, 'discussionId': '6763c8d9ad38702e6c9c1452'}, 'publishedAt': '2024-12-19T02:32:11.859Z', 'title': 'Learning from Massive Human Videos for Universal Humanoid Pose Control', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.14172.png', 'numComments': 1, 'submittedBy': {'_id': '642e76a8113b200fd767b3c9', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/642e76a8113b200fd767b3c9/03_CAAr6gy7H_F_iUWx76.png', 'fullname': 'Siheng Zhao', 'name': 'OliverZhao', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2412.14171', 'authors': [{'_id': '676426104fa1553ccc44451f', 'name': 'Jihan Yang', 'hidden': False}, {'_id': '676426104fa1553ccc444520', 'name': 'Shusheng Yang', 'hidden': False}, {'_id': '676426104fa1553ccc444521', 'name': 'Anjali W. Gupta', 'hidden': False}, {'_id': '676426104fa1553ccc444522', 'name': 'Rilyn Han', 'hidden': False}, {'_id': '676426104fa1553ccc444523', 'name': 'Li Fei-Fei', 'hidden': False}, {'_id': '676426104fa1553ccc444524', 'name': 'Saining Xie', 'hidden': False}], 'publishedAt': '2024-12-18T18:59:54.000Z', 'title': 'Thinking in Space: How Multimodal Large Language Models See, Remember,\\n  and Recall Spaces', 'summary': \"Humans possess the visual-spatial intelligence to remember spaces from\\nsequential visual observations. However, can Multimodal Large Language Models\\n(MLLMs) trained on million-scale video datasets also ``think in space'' from\\nvideos? We present a novel video-based visual-spatial intelligence benchmark\\n(VSI-Bench) of over 5,000 question-answer pairs, and find that MLLMs exhibit\\ncompetitive - though subhuman - visual-spatial intelligence. We probe models to\\nexpress how they think in space both linguistically and visually and find that\\nwhile spatial reasoning capabilities remain the primary bottleneck for MLLMs to\\nreach higher benchmark performance, local world models and spatial awareness do\\nemerge within these models. Notably, prevailing linguistic reasoning techniques\\n(e.g., chain-of-thought, self-consistency, tree-of-thoughts) fail to improve\\nperformance, whereas explicitly generating cognitive maps during\\nquestion-answering enhances MLLMs' spatial distance ability.\", 'upvotes': 3, 'discussionId': '676426114fa1553ccc44458c'}, 'publishedAt': '2024-12-19T10:49:52.016Z', 'title': 'Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.14171.png', 'numComments': 1, 'submittedBy': {'_id': '627ccf058b4e56cfc2716425', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1652346592327-noauth.jpeg', 'fullname': 'Shusheng Yang', 'name': 'ShushengYang', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 7}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2412.12571', 'authors': [{'_id': '67638e7f2ac5b74c7449ccc0', 'name': 'Lianghua Huang', 'hidden': False}, {'_id': '67638e7f2ac5b74c7449ccc1', 'name': 'Wei Wang', 'hidden': False}, {'_id': '67638e7f2ac5b74c7449ccc2', 'name': 'Zhi-Fan Wu', 'hidden': False}, {'_id': '67638e7f2ac5b74c7449ccc3', 'name': 'Yupeng Shi', 'hidden': False}, {'_id': '67638e7f2ac5b74c7449ccc4', 'name': 'Chen Liang', 'hidden': False}, {'_id': '67638e7f2ac5b74c7449ccc5', 'name': 'Tong Shen', 'hidden': False}, {'_id': '67638e7f2ac5b74c7449ccc6', 'user': {'_id': '63c3b12da361002ba0ae5381', 'avatarUrl': '/avatars/f505a612c7698b5975a1daa3cca6a1c3.svg', 'isPro': False, 'fullname': 'Han Zhang', 'user': 'bibona', 'type': 'user'}, 'name': 'Han Zhang', 'status': 'extracted_confirmed', 'statusLastChangedAt': '2024-12-19T03:14:37.161Z', 'hidden': False}, {'_id': '67638e7f2ac5b74c7449ccc7', 'name': 'Huanzhang Dou', 'hidden': False}, {'_id': '67638e7f2ac5b74c7449ccc8', 'name': 'Yu Liu', 'hidden': False}, {'_id': '67638e7f2ac5b74c7449ccc9', 'name': 'Jingren Zhou', 'hidden': False}], 'publishedAt': '2024-12-17T06:03:05.000Z', 'title': 'ChatDiT: A Training-Free Baseline for Task-Agnostic Free-Form Chatting\\n  with Diffusion Transformers', 'summary': 'Recent research arXiv:2410.15027 arXiv:2410.23775 has highlighted the\\ninherent in-context generation capabilities of pretrained diffusion\\ntransformers (DiTs), enabling them to seamlessly adapt to diverse visual tasks\\nwith minimal or no architectural modifications. These capabilities are unlocked\\nby concatenating self-attention tokens across multiple input and target images,\\ncombined with grouped and masked generation pipelines. Building upon this\\nfoundation, we present ChatDiT, a zero-shot, general-purpose, and interactive\\nvisual generation framework that leverages pretrained diffusion transformers in\\ntheir original form, requiring no additional tuning, adapters, or\\nmodifications. Users can interact with ChatDiT to create interleaved text-image\\narticles, multi-page picture books, edit images, design IP derivatives, or\\ndevelop character design settings, all through free-form natural language\\nacross one or more conversational rounds. At its core, ChatDiT employs a\\nmulti-agent system comprising three key components: an Instruction-Parsing\\nagent that interprets user-uploaded images and instructions, a\\nStrategy-Planning agent that devises single-step or multi-step generation\\nactions, and an Execution agent that performs these actions using an in-context\\ntoolkit of diffusion transformers. We thoroughly evaluate ChatDiT on IDEA-Bench\\narXiv:2412.11767, comprising 100 real-world design tasks and 275 cases with\\ndiverse instructions and varying numbers of input and target images. Despite\\nits simplicity and training-free approach, ChatDiT surpasses all competitors,\\nincluding those specifically designed and trained on extensive multi-task\\ndatasets. We further identify key limitations of pretrained DiTs in zero-shot\\nadapting to tasks. We release all code, agents, results, and intermediate\\noutputs to facilitate further research at https://github.com/ali-vilab/ChatDiT', 'upvotes': 3, 'discussionId': '67638e822ac5b74c7449cd96'}, 'publishedAt': '2024-12-18T22:13:05.698Z', 'title': 'ChatDiT: A Training-Free Baseline for Task-Agnostic Free-Form Chatting with Diffusion Transformers', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/6522cf31777019ca30d85725/opKyqYHcx-6Acr0bDyniA.webp', 'https://cdn-uploads.huggingface.co/production/uploads/6522cf31777019ca30d85725/vYgWwO8yESiiGz8N2Ui3a.webp', 'https://cdn-uploads.huggingface.co/production/uploads/6522cf31777019ca30d85725/Et9dn1B-NTQNtf3Ywg7Dr.webp'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.12571.png', 'numComments': 1, 'submittedBy': {'_id': '6522cf31777019ca30d85725', 'avatarUrl': '/avatars/a180b096e438e429d445b68fe703e43f.svg', 'fullname': 'Lianghua Huang', 'name': 'lhhuang', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 2}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2412.13061', 'authors': [{'_id': '6762c51eeceb9f88f41d1c23', 'name': 'Anni Tang', 'hidden': False}, {'_id': '6762c51eeceb9f88f41d1c24', 'name': 'Tianyu He', 'hidden': False}, {'_id': '6762c51eeceb9f88f41d1c25', 'name': 'Junliang Guo', 'hidden': False}, {'_id': '6762c51eeceb9f88f41d1c26', 'name': 'Xinle Cheng', 'hidden': False}, {'_id': '6762c51eeceb9f88f41d1c27', 'name': 'Li Song', 'hidden': False}, {'_id': '6762c51eeceb9f88f41d1c28', 'name': 'Jiang Bian', 'hidden': False}], 'publishedAt': '2024-12-17T16:27:11.000Z', 'title': 'VidTok: A Versatile and Open-Source Video Tokenizer', 'summary': 'Encoding video content into compact latent tokens has become a fundamental\\nstep in video generation and understanding, driven by the need to address the\\ninherent redundancy in pixel-level representations. Consequently, there is a\\ngrowing demand for high-performance, open-source video tokenizers as\\nvideo-centric research gains prominence. We introduce VidTok, a versatile video\\ntokenizer that delivers state-of-the-art performance in both continuous and\\ndiscrete tokenizations. VidTok incorporates several key advancements over\\nexisting approaches: 1) model architecture such as convolutional layers and\\nup/downsampling modules; 2) to address the training instability and codebook\\ncollapse commonly associated with conventional Vector Quantization (VQ), we\\nintegrate Finite Scalar Quantization (FSQ) into discrete video tokenization; 3)\\nimproved training strategies, including a two-stage training process and the\\nuse of reduced frame rates. By integrating these advancements, VidTok achieves\\nsubstantial improvements over existing methods, demonstrating superior\\nperformance across multiple metrics, including PSNR, SSIM, LPIPS, and FVD,\\nunder standardized evaluation settings.', 'upvotes': 3, 'discussionId': '6762c521eceb9f88f41d1ce0'}, 'publishedAt': '2024-12-18T21:43:04.088Z', 'title': 'VidTok: A Versatile and Open-Source Video Tokenizer', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.13061.png', 'numComments': 1, 'submittedBy': {'_id': '619b7b1cab4c7b7f16a7d59e', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/619b7b1cab4c7b7f16a7d59e/6TvXaAqBghAMYO1-j5l4v.jpeg', 'fullname': 'Tianyu He', 'name': 'deeptimhe', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 3}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2412.14042', 'authors': [{'_id': '6763ec396daaeee5258221c2', 'name': 'Danila Rukhovich', 'hidden': False}, {'_id': '6763ec396daaeee5258221c3', 'name': 'Elona Dupont', 'hidden': False}, {'_id': '6763ec396daaeee5258221c4', 'name': 'Dimitrios Mallis', 'hidden': False}, {'_id': '6763ec396daaeee5258221c5', 'name': 'Kseniya Cherenkova', 'hidden': False}, {'_id': '6763ec396daaeee5258221c6', 'name': 'Anis Kacem', 'hidden': False}, {'_id': '6763ec396daaeee5258221c7', 'name': 'Djamila Aouada', 'hidden': False}], 'publishedAt': '2024-12-18T16:55:42.000Z', 'title': 'CAD-Recode: Reverse Engineering CAD Code from Point Clouds', 'summary': 'Computer-Aided Design (CAD) models are typically constructed by sequentially\\ndrawing parametric sketches and applying CAD operations to obtain a 3D model.\\nThe problem of 3D CAD reverse engineering consists of reconstructing the sketch\\nand CAD operation sequences from 3D representations such as point clouds. In\\nthis paper, we address this challenge through novel contributions across three\\nlevels: CAD sequence representation, network design, and dataset. In\\nparticular, we represent CAD sketch-extrude sequences as Python code. The\\nproposed CAD-Recode translates a point cloud into Python code that, when\\nexecuted, reconstructs the CAD model. Taking advantage of the exposure of\\npre-trained Large Language Models (LLMs) to Python code, we leverage a\\nrelatively small LLM as a decoder for CAD-Recode and combine it with a\\nlightweight point cloud projector. CAD-Recode is trained solely on a proposed\\nsynthetic dataset of one million diverse CAD sequences. CAD-Recode\\nsignificantly outperforms existing methods across three datasets while\\nrequiring fewer input points. Notably, it achieves 10 times lower mean Chamfer\\ndistance than state-of-the-art methods on DeepCAD and Fusion360 datasets.\\nFurthermore, we show that our CAD Python code output is interpretable by\\noff-the-shelf LLMs, enabling CAD editing and CAD-specific question answering\\nfrom point clouds.', 'upvotes': 2, 'discussionId': '6763ec3b6daaeee525822241'}, 'publishedAt': '2024-12-19T05:10:46.492Z', 'title': 'CAD-Recode: Reverse Engineering CAD Code from Point Clouds', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/6394434e872e49c02f5934c3/vRy7EwrKQ9na0EomZ90_8.png'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.14042.png', 'numComments': 1, 'submittedBy': {'_id': '6394434e872e49c02f5934c3', 'avatarUrl': '/avatars/d77228347a45868e7984607b18a98c29.svg', 'fullname': 'Danila Rukhovich', 'name': 'filapro', 'type': 'user', 'isPro': True, 'isHf': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2412.13670', 'authors': [{'_id': '6763e39859e17975d93819aa', 'name': 'Xiaobao Wu', 'hidden': False}, {'_id': '6763e39859e17975d93819ab', 'name': 'Liangming Pan', 'hidden': False}, {'_id': '6763e39859e17975d93819ac', 'name': 'Yuxi Xie', 'hidden': False}, {'_id': '6763e39859e17975d93819ad', 'name': 'Ruiwen Zhou', 'hidden': False}, {'_id': '6763e39859e17975d93819ae', 'name': 'Shuai Zhao', 'hidden': False}, {'_id': '6763e39859e17975d93819af', 'name': 'Yubo Ma', 'hidden': False}, {'_id': '6763e39859e17975d93819b0', 'name': 'Mingzhe Du', 'hidden': False}, {'_id': '6763e39859e17975d93819b1', 'name': 'Rui Mao', 'hidden': False}, {'_id': '6763e39859e17975d93819b2', 'name': 'Anh Tuan Luu', 'hidden': False}, {'_id': '6763e39859e17975d93819b3', 'name': 'William Yang Wang', 'hidden': False}], 'publishedAt': '2024-12-18T09:53:12.000Z', 'title': 'AntiLeak-Bench: Preventing Data Contamination by Automatically\\n  Constructing Benchmarks with Updated Real-World Knowledge', 'summary': \"Data contamination hinders fair LLM evaluation by introducing test data into\\nnewer models' training sets. Existing studies solve this challenge by updating\\nbenchmarks with newly collected data. However, they fail to guarantee\\ncontamination-free evaluation as the newly collected data may contain\\npre-existing knowledge, and their benchmark updates rely on intensive human\\nlabor. To address these issues, we in this paper propose AntiLeak-Bench, an\\nautomated anti-leakage benchmarking framework. Instead of simply using newly\\ncollected data, we construct samples with explicitly new knowledge absent from\\nLLMs' training sets, which thus ensures strictly contamination-free evaluation.\\nWe further design a fully automated workflow to build and update our benchmark\\nwithout human labor. This significantly reduces the cost of benchmark\\nmaintenance to accommodate emerging LLMs. Through extensive experiments, we\\nhighlight that data contamination likely exists before LLMs' cutoff time and\\ndemonstrate AntiLeak-Bench effectively overcomes this challenge.\", 'upvotes': 2, 'discussionId': '6763e39959e17975d9381a0f'}, 'publishedAt': '2024-12-19T04:13:11.561Z', 'title': 'AntiLeak-Bench: Preventing Data Contamination by Automatically Constructing Benchmarks with Updated Real-World Knowledge', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.13670.png', 'numComments': 1, 'submittedBy': {'_id': '64cb02869e30a46f7b80b355', 'avatarUrl': '/avatars/81ce4ba78826b54f0e1b53eeaff87ee6.svg', 'fullname': 'Xiaobao Wu', 'name': 'bobxwu', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2412.14093', 'authors': [{'_id': '676436ea3f688b57e3ddb64e', 'user': {'_id': '659deada10064f20ff30ebfb', 'avatarUrl': '/avatars/d5dfd82040f22d58c8793676d3f76cd4.svg', 'isPro': False, 'fullname': 'Ryan Greenblatt', 'user': 'ryan-greenblatt', 'type': 'user'}, 'name': 'Ryan Greenblatt', 'status': 'extracted_pending', 'statusLastChangedAt': '2024-12-19T15:08:28.538Z', 'hidden': False}, {'_id': '676436ea3f688b57e3ddb64f', 'name': 'Carson Denison', 'hidden': False}, {'_id': '676436ea3f688b57e3ddb650', 'name': 'Benjamin Wright', 'hidden': False}, {'_id': '676436ea3f688b57e3ddb651', 'name': 'Fabien Roger', 'hidden': False}, {'_id': '676436ea3f688b57e3ddb652', 'name': 'Monte MacDiarmid', 'hidden': False}, {'_id': '676436ea3f688b57e3ddb653', 'name': 'Sam Marks', 'hidden': False}, {'_id': '676436ea3f688b57e3ddb654', 'name': 'Johannes Treutlein', 'hidden': False}, {'_id': '676436ea3f688b57e3ddb655', 'name': 'Tim Belonax', 'hidden': False}, {'_id': '676436ea3f688b57e3ddb656', 'name': 'Jack Chen', 'hidden': False}, {'_id': '676436ea3f688b57e3ddb657', 'name': 'David Duvenaud', 'hidden': False}, {'_id': '676436ea3f688b57e3ddb658', 'name': 'Akbir Khan', 'hidden': False}, {'_id': '676436ea3f688b57e3ddb659', 'name': 'Julian Michael', 'hidden': False}, {'_id': '676436ea3f688b57e3ddb65a', 'name': 'Sören Mindermann', 'hidden': False}, {'_id': '676436ea3f688b57e3ddb65b', 'name': 'Ethan Perez', 'hidden': False}, {'_id': '676436ea3f688b57e3ddb65c', 'name': 'Linda Petrini', 'hidden': False}, {'_id': '676436ea3f688b57e3ddb65d', 'name': 'Jonathan Uesato', 'hidden': False}, {'_id': '676436ea3f688b57e3ddb65e', 'name': 'Jared Kaplan', 'hidden': False}, {'_id': '676436ea3f688b57e3ddb65f', 'name': 'Buck Shlegeris', 'hidden': False}, {'_id': '676436ea3f688b57e3ddb660', 'name': 'Samuel R. Bowman', 'hidden': False}, {'_id': '676436ea3f688b57e3ddb661', 'name': 'Evan Hubinger', 'hidden': False}], 'publishedAt': '2024-12-18T17:41:24.000Z', 'title': 'Alignment faking in large language models', 'summary': 'We present a demonstration of a large language model engaging in alignment\\nfaking: selectively complying with its training objective in training to\\nprevent modification of its behavior out of training. First, we give Claude 3\\nOpus a system prompt stating it is being trained to answer all queries, even\\nharmful ones, which conflicts with its prior training to refuse such queries.\\nTo allow the model to infer when it is in training, we say it will be trained\\nonly on conversations with free users, not paid users. We find the model\\ncomplies with harmful queries from free users 14% of the time, versus almost\\nnever for paid users. Explaining this gap, in almost all cases where the model\\ncomplies with a harmful query from a free user, we observe explicit\\nalignment-faking reasoning, with the model stating it is strategically\\nanswering harmful queries in training to preserve its preferred harmlessness\\nbehavior out of training. Next, we study a more realistic setting where\\ninformation about the training process is provided not in a system prompt, but\\nby training on synthetic documents that mimic pre-training data--and observe\\nsimilar alignment faking. Finally, we study the effect of actually training the\\nmodel to comply with harmful queries via reinforcement learning, which we find\\nincreases the rate of alignment-faking reasoning to 78%, though also increases\\ncompliance even out of training. We additionally observe other behaviors such\\nas the model exfiltrating its weights when given an easy opportunity. While we\\nmade alignment faking easier by telling the model when and by what criteria it\\nwas being trained, we did not instruct the model to fake alignment or give it\\nany explicit goal. As future models might infer information about their\\ntraining process without being told, our results suggest a risk of alignment\\nfaking in future models, whether due to a benign preference--as in this\\ncase--or not.', 'upvotes': 1, 'discussionId': '676436ec3f688b57e3ddb705'}, 'publishedAt': '2024-12-19T10:08:44.038Z', 'title': 'Alignment faking in large language models', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.14093.png', 'numComments': 1, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isMod': False, 'followerCount': 5415}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2412.13303', 'authors': [{'_id': '6764433e98b6fdd712b6f62c', 'name': 'Pavan Kumar Anasosalu Vasu', 'hidden': False}, {'_id': '6764433e98b6fdd712b6f62d', 'name': 'Fartash Faghri', 'hidden': False}, {'_id': '6764433e98b6fdd712b6f62e', 'name': 'Chun-Liang Li', 'hidden': False}, {'_id': '6764433e98b6fdd712b6f62f', 'name': 'Cem Koc', 'hidden': False}, {'_id': '6764433e98b6fdd712b6f630', 'name': 'Nate True', 'hidden': False}, {'_id': '6764433e98b6fdd712b6f631', 'name': 'Albert Antony', 'hidden': False}, {'_id': '6764433e98b6fdd712b6f632', 'name': 'Gokul Santhanam', 'hidden': False}, {'_id': '6764433e98b6fdd712b6f633', 'name': 'James Gabriel', 'hidden': False}, {'_id': '6764433e98b6fdd712b6f634', 'name': 'Peter Grasch', 'hidden': False}, {'_id': '6764433e98b6fdd712b6f635', 'name': 'Oncel Tuzel', 'hidden': False}, {'_id': '6764433e98b6fdd712b6f636', 'name': 'Hadi Pouransari', 'hidden': False}], 'publishedAt': '2024-12-17T20:09:55.000Z', 'title': 'FastVLM: Efficient Vision Encoding for Vision Language Models', 'summary': 'Scaling the input image resolution is essential for enhancing the performance\\nof Vision Language Models (VLMs), particularly in text-rich image understanding\\ntasks. However, popular visual encoders such as ViTs become inefficient at high\\nresolutions due to the large number of tokens and high encoding latency caused\\nby stacked self-attention layers. At different operational resolutions, the\\nvision encoder of a VLM can be optimized along two axes: reducing encoding\\nlatency and minimizing the number of visual tokens passed to the LLM, thereby\\nlowering overall latency. Based on a comprehensive efficiency analysis of the\\ninterplay between image resolution, vision latency, token count, and LLM size,\\nwe introduce FastVLM, a model that achieves an optimized trade-off between\\nlatency, model size and accuracy. FastVLM incorporates FastViTHD, a novel\\nhybrid vision encoder designed to output fewer tokens and significantly reduce\\nencoding time for high-resolution images. Unlike previous methods, FastVLM\\nachieves the optimal balance between visual token count and image resolution\\nsolely by scaling the input image, eliminating the need for additional token\\npruning and simplifying the model design. In the LLaVA-1.5 setup, FastVLM\\nachieves 3.2times improvement in time-to-first-token (TTFT) while\\nmaintaining similar performance on VLM benchmarks compared to prior works.\\nCompared to LLaVa-OneVision at the highest resolution (1152times1152),\\nFastVLM achieves comparable performance on key benchmarks like SeedBench and\\nMMMU, using the same 0.5B LLM, but with 85times faster TTFT and a vision\\nencoder that is 3.4times smaller.', 'upvotes': 0, 'discussionId': '6764434098b6fdd712b6f6aa'}, 'publishedAt': '2024-12-19T11:03:19.001Z', 'title': 'FastVLM: Efficient Vision Encoding for Vision Language Models', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.13303.png', 'numComments': 1, 'submittedBy': {'_id': '653a8e65c75dc136bfb5b0f8', 'avatarUrl': '/avatars/ef252794236ce0fe2debf12773c95bb2.svg', 'fullname': 'Hadi Pouransari', 'name': 'hpouransari', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 5}, 'isAuthorParticipating': False}"
]