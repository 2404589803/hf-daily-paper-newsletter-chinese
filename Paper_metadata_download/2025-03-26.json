[
  {
    "paper": {
      "id": "2503.19325",
      "authors": [
        {
          "_id": "67e35f6fc9d8214b5e1c64c3",
          "name": "Yuchao Gu",
          "hidden": false
        },
        {
          "_id": "67e35f6fc9d8214b5e1c64c4",
          "name": "Weijia Mao",
          "hidden": false
        },
        {
          "_id": "67e35f6fc9d8214b5e1c64c5",
          "name": "Mike Zheng Shou",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63021630a35b21bd8a53305a/SL0MQs7OvQpNlGBhroTW3.png"
      ],
      "publishedAt": "2025-03-25T03:38:06.000Z",
      "submittedOnDailyAt": "2025-03-26T00:37:14.940Z",
      "title": "Long-Context Autoregressive Video Modeling with Next-Frame Prediction",
      "submittedOnDailyBy": {
        "_id": "63021630a35b21bd8a53305a",
        "avatarUrl": "/avatars/7a7e8b39749eda61e57d8a1908726558.svg",
        "isPro": true,
        "fullname": "Gu Yuchao",
        "user": "guyuchao",
        "type": "user"
      },
      "summary": "Long-context autoregressive modeling has significantly advanced language\ngeneration, but video generation still struggles to fully utilize extended\ntemporal contexts. To investigate long-context video modeling, we introduce\nFrame AutoRegressive (FAR), a strong baseline for video autoregressive\nmodeling. Just as language models learn causal dependencies between tokens\n(i.e., Token AR), FAR models temporal causal dependencies between continuous\nframes, achieving better convergence than Token AR and video diffusion\ntransformers. Building on FAR, we observe that long-context vision modeling\nfaces challenges due to visual redundancy. Existing RoPE lacks effective\ntemporal decay for remote context and fails to extrapolate well to long video\nsequences. Additionally, training on long videos is computationally expensive,\nas vision tokens grow much faster than language tokens. To tackle these issues,\nwe propose balancing locality and long-range dependency. We introduce FlexRoPE,\nan test-time technique that adds flexible temporal decay to RoPE, enabling\nextrapolation to 16x longer vision contexts. Furthermore, we propose long\nshort-term context modeling, where a high-resolution short-term context window\nensures fine-grained temporal consistency, while an unlimited long-term context\nwindow encodes long-range information using fewer tokens. With this approach,\nwe can train on long video sequences with a manageable token context length. We\ndemonstrate that FAR achieves state-of-the-art performance in both short- and\nlong-video generation, providing a simple yet effective baseline for video\nautoregressive modeling.",
      "upvotes": 48,
      "discussionId": "67e35f72c9d8214b5e1c659b",
      "ai_keywords": [
        "Frame AutoRegressive (FAR)",
        "Token AR",
        "video autoregressive modeling",
        "visual redundancy",
        "RoPE (Rotary Position Embedding)",
        "temporal decay",
        "FlexRoPE",
        "long short-term context modeling",
        "high-resolution short-term context window",
        "long-term context window",
        "state-of-the-art performance",
        "video generation"
      ]
    },
    "publishedAt": "2025-03-24T23:38:06.000Z",
    "title": "Long-Context Autoregressive Video Modeling with Next-Frame Prediction",
    "summary": "Long-context autoregressive modeling has significantly advanced language\ngeneration, but video generation still struggles to fully utilize extended\ntemporal contexts. To investigate long-context video modeling, we introduce\nFrame AutoRegressive (FAR), a strong baseline for video autoregressive\nmodeling. Just as language models learn causal dependencies between tokens\n(i.e., Token AR), FAR models temporal causal dependencies between continuous\nframes, achieving better convergence than Token AR and video diffusion\ntransformers. Building on FAR, we observe that long-context vision modeling\nfaces challenges due to visual redundancy. Existing RoPE lacks effective\ntemporal decay for remote context and fails to extrapolate well to long video\nsequences. Additionally, training on long videos is computationally expensive,\nas vision tokens grow much faster than language tokens. To tackle these issues,\nwe propose balancing locality and long-range dependency. We introduce FlexRoPE,\nan test-time technique that adds flexible temporal decay to RoPE, enabling\nextrapolation to 16x longer vision contexts. Furthermore, we propose long\nshort-term context modeling, where a high-resolution short-term context window\nensures fine-grained temporal consistency, while an unlimited long-term context\nwindow encodes long-range information using fewer tokens. With this approach,\nwe can train on long video sequences with a manageable token context length. We\ndemonstrate that FAR achieves state-of-the-art performance in both short- and\nlong-video generation, providing a simple yet effective baseline for video\nautoregressive modeling.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63021630a35b21bd8a53305a/SL0MQs7OvQpNlGBhroTW3.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19325.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63021630a35b21bd8a53305a",
      "avatarUrl": "/avatars/7a7e8b39749eda61e57d8a1908726558.svg",
      "fullname": "Gu Yuchao",
      "name": "guyuchao",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.18931",
      "authors": [
        {
          "_id": "67e25c4d1908043170bd551d",
          "user": {
            "_id": "64651db3611ae99d14d392ea",
            "avatarUrl": "/avatars/b818dc0dddc999758ab5737d5053e8c3.svg",
            "isPro": false,
            "fullname": "cyt",
            "user": "Row11n",
            "type": "user"
          },
          "name": "Yitong Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T08:18:45.692Z",
          "hidden": false
        },
        {
          "_id": "67e25c4d1908043170bd551e",
          "name": "Lingchen Meng",
          "hidden": false
        },
        {
          "_id": "67e25c4d1908043170bd551f",
          "name": "Wujian Peng",
          "hidden": false
        },
        {
          "_id": "67e25c4d1908043170bd5520",
          "name": "Zuxuan Wu",
          "hidden": false
        },
        {
          "_id": "67e25c4d1908043170bd5521",
          "name": "Yu-Gang Jiang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T17:52:47.000Z",
      "submittedOnDailyAt": "2025-03-26T01:10:42.553Z",
      "title": "CoMP: Continual Multimodal Pre-training for Vision Foundation Models",
      "submittedOnDailyBy": {
        "_id": "64651db3611ae99d14d392ea",
        "avatarUrl": "/avatars/b818dc0dddc999758ab5737d5053e8c3.svg",
        "isPro": false,
        "fullname": "cyt",
        "user": "Row11n",
        "type": "user"
      },
      "summary": "Pre-trained Vision Foundation Models (VFMs) provide strong visual\nrepresentations for a wide range of applications. In this paper, we continually\npre-train prevailing VFMs in a multimodal manner such that they can\neffortlessly process visual inputs of varying sizes and produce visual\nrepresentations that are more aligned with language representations, regardless\nof their original pre-training process. To this end, we introduce CoMP, a\ncarefully designed multimodal pre-training pipeline. CoMP uses a Continual\nRotary Position Embedding to support native resolution continual pre-training,\nand an Alignment Loss between visual and textual features through language\nprototypes to align multimodal representations. By three-stage training, our\nVFMs achieve remarkable improvements not only in multimodal understanding but\nalso in other downstream tasks such as classification and segmentation.\nRemarkably, CoMP-SigLIP achieves scores of 66.7 on ChartQA and 75.9 on DocVQA\nwith a 0.5B LLM, while maintaining an 87.4% accuracy on ImageNet-1K and a 49.5\nmIoU on ADE20K under frozen chunk evaluation.",
      "upvotes": 16,
      "discussionId": "67e25c4f1908043170bd55a8",
      "projectPage": "https://slimm-x.github.io/comp/",
      "githubRepo": "https://github.com/SliMM-X/CoMP-MM",
      "ai_keywords": [
        "Vision Foundation Models (VFMs)",
        "Continual Rotary Position Embedding",
        "Alignment Loss",
        "language prototypes",
        "multimodal pre-training pipeline",
        "three-stage training",
        "multimodal understanding",
        "classification",
        "segmentation",
        "ChartQA",
        "DocVQA",
        "LLM",
        "ImageNet-1K",
        "ADE20K",
        "frozen chunk evaluation"
      ]
    },
    "publishedAt": "2025-03-24T13:52:47.000Z",
    "title": "CoMP: Continual Multimodal Pre-training for Vision Foundation Models",
    "summary": "Pre-trained Vision Foundation Models (VFMs) provide strong visual\nrepresentations for a wide range of applications. In this paper, we continually\npre-train prevailing VFMs in a multimodal manner such that they can\neffortlessly process visual inputs of varying sizes and produce visual\nrepresentations that are more aligned with language representations, regardless\nof their original pre-training process. To this end, we introduce CoMP, a\ncarefully designed multimodal pre-training pipeline. CoMP uses a Continual\nRotary Position Embedding to support native resolution continual pre-training,\nand an Alignment Loss between visual and textual features through language\nprototypes to align multimodal representations. By three-stage training, our\nVFMs achieve remarkable improvements not only in multimodal understanding but\nalso in other downstream tasks such as classification and segmentation.\nRemarkably, CoMP-SigLIP achieves scores of 66.7 on ChartQA and 75.9 on DocVQA\nwith a 0.5B LLM, while maintaining an 87.4% accuracy on ImageNet-1K and a 49.5\nmIoU on ADE20K under frozen chunk evaluation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18931.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64651db3611ae99d14d392ea",
      "avatarUrl": "/avatars/b818dc0dddc999758ab5737d5053e8c3.svg",
      "fullname": "cyt",
      "name": "Row11n",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.19622",
      "authors": [
        {
          "_id": "67e3706bc9d8214b5e219149",
          "name": "Hongcheng Gao",
          "hidden": false
        },
        {
          "_id": "67e3706bc9d8214b5e21914a",
          "name": "Jiashu Qu",
          "hidden": false
        },
        {
          "_id": "67e3706bc9d8214b5e21914b",
          "name": "Jingyi Tang",
          "hidden": false
        },
        {
          "_id": "67e3706bc9d8214b5e21914c",
          "name": "Baolong Bi",
          "hidden": false
        },
        {
          "_id": "67e3706bc9d8214b5e21914d",
          "name": "Yue Liu",
          "hidden": false
        },
        {
          "_id": "67e3706bc9d8214b5e21914e",
          "name": "Hongyu Chen",
          "hidden": false
        },
        {
          "_id": "67e3706bc9d8214b5e21914f",
          "name": "Li Liang",
          "hidden": false
        },
        {
          "_id": "67e3706bc9d8214b5e219150",
          "name": "Li Su",
          "hidden": false
        },
        {
          "_id": "67e3706bc9d8214b5e219151",
          "name": "Qingming Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-25T13:12:17.000Z",
      "submittedOnDailyAt": "2025-03-26T01:44:03.080Z",
      "title": "Exploring Hallucination of Large Multimodal Models in Video\n  Understanding: Benchmark, Analysis and Mitigation",
      "submittedOnDailyBy": {
        "_id": "62728f4f6253fe2068da1021",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62728f4f6253fe2068da1021/KZ65X0EH98AF3zXemPiap.jpeg",
        "isPro": false,
        "fullname": "Hongcheng Gao",
        "user": "HongchengGao",
        "type": "user"
      },
      "summary": "The hallucination of large multimodal models (LMMs), providing responses that\nappear correct but are actually incorrect, limits their reliability and\napplicability. This paper aims to study the hallucination problem of LMMs in\nvideo modality, which is dynamic and more challenging compared to static\nmodalities like images and text. From this motivation, we first present a\ncomprehensive benchmark termed HAVEN for evaluating hallucinations of LMMs in\nvideo understanding tasks. It is built upon three dimensions, i.e.,\nhallucination causes, hallucination aspects, and question formats, resulting in\n6K questions. Then, we quantitatively study 7 influential factors on\nhallucinations, e.g., duration time of videos, model sizes, and model\nreasoning, via experiments of 16 LMMs on the presented benchmark. In addition,\ninspired by recent thinking models like OpenAI o1, we propose a video-thinking\nmodel to mitigate the hallucinations of LMMs via supervised reasoning\nfine-tuning (SRFT) and direct preference optimization (TDPO)-- where SRFT\nenhances reasoning capabilities while TDPO reduces hallucinations in the\nthinking process. Extensive experiments and analyses demonstrate the\neffectiveness. Remarkably, it improves the baseline by 7.65% in accuracy on\nhallucination evaluation and reduces the bias score by 4.5%. The code and data\nare public at https://github.com/Hongcheng-Gao/HAVEN.",
      "upvotes": 13,
      "discussionId": "67e3706dc9d8214b5e2191e0",
      "githubRepo": "https://github.com/Hongcheng-Gao/HAVEN",
      "ai_keywords": [
        "multimodal models (LMMs)",
        "hallucination",
        "video modality",
        "video understanding",
        "HAVEN",
        "hallucination causes",
        "hallucination aspects",
        "question formats",
        "duration time",
        "model sizes",
        "model reasoning",
        "supervised reasoning fine-tuning (SRFT)",
        "direct preference optimization (TDPO)",
        "video-thinking model",
        "accuracy",
        "bias score"
      ]
    },
    "publishedAt": "2025-03-25T09:12:17.000Z",
    "title": "Exploring Hallucination of Large Multimodal Models in Video\n  Understanding: Benchmark, Analysis and Mitigation",
    "summary": "The hallucination of large multimodal models (LMMs), providing responses that\nappear correct but are actually incorrect, limits their reliability and\napplicability. This paper aims to study the hallucination problem of LMMs in\nvideo modality, which is dynamic and more challenging compared to static\nmodalities like images and text. From this motivation, we first present a\ncomprehensive benchmark termed HAVEN for evaluating hallucinations of LMMs in\nvideo understanding tasks. It is built upon three dimensions, i.e.,\nhallucination causes, hallucination aspects, and question formats, resulting in\n6K questions. Then, we quantitatively study 7 influential factors on\nhallucinations, e.g., duration time of videos, model sizes, and model\nreasoning, via experiments of 16 LMMs on the presented benchmark. In addition,\ninspired by recent thinking models like OpenAI o1, we propose a video-thinking\nmodel to mitigate the hallucinations of LMMs via supervised reasoning\nfine-tuning (SRFT) and direct preference optimization (TDPO)-- where SRFT\nenhances reasoning capabilities while TDPO reduces hallucinations in the\nthinking process. Extensive experiments and analyses demonstrate the\neffectiveness. Remarkably, it improves the baseline by 7.65% in accuracy on\nhallucination evaluation and reduces the bias score by 4.5%. The code and data\nare public at https://github.com/Hongcheng-Gao/HAVEN.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19622.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "62728f4f6253fe2068da1021",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62728f4f6253fe2068da1021/KZ65X0EH98AF3zXemPiap.jpeg",
      "fullname": "Hongcheng Gao",
      "name": "HongchengGao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.19385",
      "authors": [
        {
          "_id": "67e36241d8da46951f858026",
          "name": "Jaihoon Kim",
          "hidden": false
        },
        {
          "_id": "67e36241d8da46951f858027",
          "name": "Taehoon Yoon",
          "hidden": false
        },
        {
          "_id": "67e36241d8da46951f858028",
          "name": "Jisung Hwang",
          "hidden": false
        },
        {
          "_id": "67e36241d8da46951f858029",
          "name": "Minhyuk Sung",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-25T06:30:45.000Z",
      "submittedOnDailyAt": "2025-03-26T00:49:38.583Z",
      "title": "Inference-Time Scaling for Flow Models via Stochastic Generation and\n  Rollover Budget Forcing",
      "submittedOnDailyBy": {
        "_id": "6342796a0875f2c99cfd313b",
        "avatarUrl": "/avatars/98575092404c4197b20c929a6499a015.svg",
        "isPro": false,
        "fullname": "Yuseung \"Phillip\" Lee",
        "user": "phillipinseoul",
        "type": "user"
      },
      "summary": "We propose an inference-time scaling approach for pretrained flow models.\nRecently, inference-time scaling has gained significant attention in LLMs and\ndiffusion models, improving sample quality or better aligning outputs with user\npreferences by leveraging additional computation. For diffusion models,\nparticle sampling has allowed more efficient scaling due to the stochasticity\nat intermediate denoising steps. On the contrary, while flow models have gained\npopularity as an alternative to diffusion models--offering faster generation\nand high-quality outputs in state-of-the-art image and video generative\nmodels--efficient inference-time scaling methods used for diffusion models\ncannot be directly applied due to their deterministic generative process. To\nenable efficient inference-time scaling for flow models, we propose three key\nideas: 1) SDE-based generation, enabling particle sampling in flow models, 2)\nInterpolant conversion, broadening the search space and enhancing sample\ndiversity, and 3) Rollover Budget Forcing (RBF), an adaptive allocation of\ncomputational resources across timesteps to maximize budget utilization. Our\nexperiments show that SDE-based generation, particularly variance-preserving\n(VP) interpolant-based generation, improves the performance of particle\nsampling methods for inference-time scaling in flow models. Additionally, we\ndemonstrate that RBF with VP-SDE achieves the best performance, outperforming\nall previous inference-time scaling approaches.",
      "upvotes": 12,
      "discussionId": "67e36245d8da46951f85802c",
      "ai_keywords": [
        "flow models",
        "inference-time scaling",
        "LLMs",
        "diffusion models",
        "sample quality",
        "user preferences",
        "particle sampling",
        "stochasticity",
        "denoising steps",
        "generative process",
        "SDE-based generation",
        "interpolant conversion",
        "sample diversity",
        "Rollover Budget Forcing (RBF)",
        "adaptive allocation",
        "computational resources",
        "timesteps",
        "budget utilization",
        "variance-preserving (VP)",
        "VP interpolant-based generation"
      ]
    },
    "publishedAt": "2025-03-25T02:30:45.000Z",
    "title": "Inference-Time Scaling for Flow Models via Stochastic Generation and\n  Rollover Budget Forcing",
    "summary": "We propose an inference-time scaling approach for pretrained flow models.\nRecently, inference-time scaling has gained significant attention in LLMs and\ndiffusion models, improving sample quality or better aligning outputs with user\npreferences by leveraging additional computation. For diffusion models,\nparticle sampling has allowed more efficient scaling due to the stochasticity\nat intermediate denoising steps. On the contrary, while flow models have gained\npopularity as an alternative to diffusion models--offering faster generation\nand high-quality outputs in state-of-the-art image and video generative\nmodels--efficient inference-time scaling methods used for diffusion models\ncannot be directly applied due to their deterministic generative process. To\nenable efficient inference-time scaling for flow models, we propose three key\nideas: 1) SDE-based generation, enabling particle sampling in flow models, 2)\nInterpolant conversion, broadening the search space and enhancing sample\ndiversity, and 3) Rollover Budget Forcing (RBF), an adaptive allocation of\ncomputational resources across timesteps to maximize budget utilization. Our\nexperiments show that SDE-based generation, particularly variance-preserving\n(VP) interpolant-based generation, improves the performance of particle\nsampling methods for inference-time scaling in flow models. Additionally, we\ndemonstrate that RBF with VP-SDE achieves the best performance, outperforming\nall previous inference-time scaling approaches.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19385.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "6342796a0875f2c99cfd313b",
      "avatarUrl": "/avatars/98575092404c4197b20c929a6499a015.svg",
      "fullname": "Yuseung \"Phillip\" Lee",
      "name": "phillipinseoul",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.19903",
      "authors": [
        {
          "_id": "67e375d3cc93cc8c42da7699",
          "name": "Baifeng Shi",
          "hidden": false
        },
        {
          "_id": "67e375d3cc93cc8c42da769a",
          "name": "Boyi Li",
          "hidden": false
        },
        {
          "_id": "67e375d3cc93cc8c42da769b",
          "name": "Han Cai",
          "hidden": false
        },
        {
          "_id": "67e375d3cc93cc8c42da769c",
          "name": "Yao Lu",
          "hidden": false
        },
        {
          "_id": "67e375d3cc93cc8c42da769d",
          "name": "Sifei Liu",
          "hidden": false
        },
        {
          "_id": "67e375d3cc93cc8c42da769e",
          "name": "Marco Pavone",
          "hidden": false
        },
        {
          "_id": "67e375d3cc93cc8c42da769f",
          "name": "Jan Kautz",
          "hidden": false
        },
        {
          "_id": "67e375d3cc93cc8c42da76a0",
          "name": "Song Han",
          "hidden": false
        },
        {
          "_id": "67e375d3cc93cc8c42da76a1",
          "name": "Trevor Darrell",
          "hidden": false
        },
        {
          "_id": "67e375d3cc93cc8c42da76a2",
          "name": "Pavlo Molchanov",
          "hidden": false
        },
        {
          "_id": "67e375d3cc93cc8c42da76a3",
          "name": "Hongxu Yin",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/649004218f7cbbc94c782db6/F0o61glyGm9fib9Pl1i-L.mp4"
      ],
      "publishedAt": "2025-03-25T17:58:37.000Z",
      "submittedOnDailyAt": "2025-03-26T02:13:20.800Z",
      "title": "Scaling Vision Pre-Training to 4K Resolution",
      "submittedOnDailyBy": {
        "_id": "649004218f7cbbc94c782db6",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/AdgLVfAIpWlug4jXTaEK-.jpeg",
        "isPro": false,
        "fullname": "Baifeng Shi",
        "user": "bfshi",
        "type": "user"
      },
      "summary": "High-resolution perception of visual details is crucial for daily tasks.\nCurrent vision pre-training, however, is still limited to low resolutions\n(e.g., 378 x 378 pixels) due to the quadratic cost of processing larger images.\nWe introduce PS3 that scales CLIP-style vision pre-training to 4K resolution\nwith a near-constant cost. Instead of contrastive learning on global image\nrepresentation, PS3 is pre-trained by selectively processing local regions and\ncontrasting them with local detailed captions, enabling high-resolution\nrepresentation learning with greatly reduced computational overhead. The\npre-trained PS3 is able to both encode the global image at low resolution and\nselectively process local high-resolution regions based on their saliency or\nrelevance to a text prompt. When applying PS3 to multi-modal LLM (MLLM), the\nresulting model, named VILA-HD, significantly improves high-resolution visual\nperception compared to baselines without high-resolution vision pre-training\nsuch as AnyRes and S^2 while using up to 4.3x fewer tokens. PS3 also unlocks\nappealing scaling properties of VILA-HD, including scaling up resolution for\nfree and scaling up test-time compute for better performance. Compared to state\nof the arts, VILA-HD outperforms previous MLLMs such as NVILA and Qwen2-VL\nacross multiple benchmarks and achieves better efficiency than latest token\npruning approaches. Finally, we find current benchmarks do not require\n4K-resolution perception, which motivates us to propose 4KPro, a new benchmark\nof image QA at 4K resolution, on which VILA-HD outperforms all previous MLLMs,\nincluding a 14.5% improvement over GPT-4o, and a 3.2% improvement and 2.96x\nspeedup over Qwen2-VL.",
      "upvotes": 8,
      "discussionId": "67e375d9cc93cc8c42da785f",
      "projectPage": "https://nvlabs.github.io/PS3/",
      "githubRepo": "https://github.com/NVlabs/PS3",
      "ai_keywords": [
        "PS3",
        "CLIP-style vision pre-training",
        "contrastive learning",
        "local regions",
        "local detailed captions",
        "high-resolution representation learning",
        "computational overhead",
        "saliency",
        "text prompt",
        "VILA-HD",
        "multi-modal LLM",
        "high-resolution visual perception",
        "AnyRes",
        "S^2",
        "scaling properties",
        "test-time compute",
        "NVILA",
        "Qwen2-VL",
        "benchmarks",
        "token pruning approaches",
        "4KPer",
        "image QA",
        "GPT-4o"
      ]
    },
    "publishedAt": "2025-03-25T13:58:37.000Z",
    "title": "Scaling Vision Pre-Training to 4K Resolution",
    "summary": "High-resolution perception of visual details is crucial for daily tasks.\nCurrent vision pre-training, however, is still limited to low resolutions\n(e.g., 378 x 378 pixels) due to the quadratic cost of processing larger images.\nWe introduce PS3 that scales CLIP-style vision pre-training to 4K resolution\nwith a near-constant cost. Instead of contrastive learning on global image\nrepresentation, PS3 is pre-trained by selectively processing local regions and\ncontrasting them with local detailed captions, enabling high-resolution\nrepresentation learning with greatly reduced computational overhead. The\npre-trained PS3 is able to both encode the global image at low resolution and\nselectively process local high-resolution regions based on their saliency or\nrelevance to a text prompt. When applying PS3 to multi-modal LLM (MLLM), the\nresulting model, named VILA-HD, significantly improves high-resolution visual\nperception compared to baselines without high-resolution vision pre-training\nsuch as AnyRes and S^2 while using up to 4.3x fewer tokens. PS3 also unlocks\nappealing scaling properties of VILA-HD, including scaling up resolution for\nfree and scaling up test-time compute for better performance. Compared to state\nof the arts, VILA-HD outperforms previous MLLMs such as NVILA and Qwen2-VL\nacross multiple benchmarks and achieves better efficiency than latest token\npruning approaches. Finally, we find current benchmarks do not require\n4K-resolution perception, which motivates us to propose 4KPro, a new benchmark\nof image QA at 4K resolution, on which VILA-HD outperforms all previous MLLMs,\nincluding a 14.5% improvement over GPT-4o, and a 3.2% improvement and 2.96x\nspeedup over Qwen2-VL.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/649004218f7cbbc94c782db6/F0o61glyGm9fib9Pl1i-L.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19903.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649004218f7cbbc94c782db6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/AdgLVfAIpWlug4jXTaEK-.jpeg",
      "fullname": "Baifeng Shi",
      "name": "bfshi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.14905",
      "authors": [
        {
          "_id": "67e250450487eeecfd9a5880",
          "name": "Siwei Wen",
          "hidden": false
        },
        {
          "_id": "67e250450487eeecfd9a5881",
          "name": "Junyan Ye",
          "hidden": false
        },
        {
          "_id": "67e250450487eeecfd9a5882",
          "name": "Peilin Feng",
          "hidden": false
        },
        {
          "_id": "67e250450487eeecfd9a5883",
          "name": "Hengrui Kang",
          "hidden": false
        },
        {
          "_id": "67e250450487eeecfd9a5884",
          "name": "Zichen Wen",
          "hidden": false
        },
        {
          "_id": "67e250450487eeecfd9a5885",
          "name": "Yize Chen",
          "hidden": false
        },
        {
          "_id": "67e250450487eeecfd9a5886",
          "name": "Jiang Wu",
          "hidden": false
        },
        {
          "_id": "67e250450487eeecfd9a5887",
          "name": "Wenjun Wu",
          "hidden": false
        },
        {
          "_id": "67e250450487eeecfd9a5888",
          "name": "Conghui He",
          "hidden": false
        },
        {
          "_id": "67e250450487eeecfd9a5889",
          "name": "Weijia Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-19T05:14:44.000Z",
      "submittedOnDailyAt": "2025-03-26T04:00:13.753Z",
      "title": "Spot the Fake: Large Multimodal Model-Based Synthetic Image Detection\n  with Artifact Explanation",
      "submittedOnDailyBy": {
        "_id": "653b8c3e97a4d71d950e2f20",
        "avatarUrl": "/avatars/b68880022e14556d0be58c69615db3be.svg",
        "isPro": false,
        "fullname": "Zichen Wen",
        "user": "zichenwen",
        "type": "user"
      },
      "summary": "With the rapid advancement of Artificial Intelligence Generated Content\n(AIGC) technologies, synthetic images have become increasingly prevalent in\neveryday life, posing new challenges for authenticity assessment and detection.\nDespite the effectiveness of existing methods in evaluating image authenticity\nand locating forgeries, these approaches often lack human interpretability and\ndo not fully address the growing complexity of synthetic data. To tackle these\nchallenges, we introduce FakeVLM, a specialized large multimodal model designed\nfor both general synthetic image and DeepFake detection tasks. FakeVLM not only\nexcels in distinguishing real from fake images but also provides clear, natural\nlanguage explanations for image artifacts, enhancing interpretability.\nAdditionally, we present FakeClue, a comprehensive dataset containing over\n100,000 images across seven categories, annotated with fine-grained artifact\nclues in natural language. FakeVLM demonstrates performance comparable to\nexpert models while eliminating the need for additional classifiers, making it\na robust solution for synthetic data detection. Extensive evaluations across\nmultiple datasets confirm the superiority of FakeVLM in both authenticity\nclassification and artifact explanation tasks, setting a new benchmark for\nsynthetic image detection. The dataset and code will be released in:\nhttps://github.com/opendatalab/FakeVLM.",
      "upvotes": 8,
      "discussionId": "67e250490487eeecfd9a599e",
      "githubRepo": "https://github.com/opendatalab/FakeVLM",
      "ai_keywords": [
        "large multimodal model",
        "FakeVLM",
        "DeepFake detection",
        "image artifacts",
        "natural language explanations",
        "FakeClue",
        "fine-grained artifact clues"
      ]
    },
    "publishedAt": "2025-03-19T01:14:44.000Z",
    "title": "Spot the Fake: Large Multimodal Model-Based Synthetic Image Detection\n  with Artifact Explanation",
    "summary": "With the rapid advancement of Artificial Intelligence Generated Content\n(AIGC) technologies, synthetic images have become increasingly prevalent in\neveryday life, posing new challenges for authenticity assessment and detection.\nDespite the effectiveness of existing methods in evaluating image authenticity\nand locating forgeries, these approaches often lack human interpretability and\ndo not fully address the growing complexity of synthetic data. To tackle these\nchallenges, we introduce FakeVLM, a specialized large multimodal model designed\nfor both general synthetic image and DeepFake detection tasks. FakeVLM not only\nexcels in distinguishing real from fake images but also provides clear, natural\nlanguage explanations for image artifacts, enhancing interpretability.\nAdditionally, we present FakeClue, a comprehensive dataset containing over\n100,000 images across seven categories, annotated with fine-grained artifact\nclues in natural language. FakeVLM demonstrates performance comparable to\nexpert models while eliminating the need for additional classifiers, making it\na robust solution for synthetic data detection. Extensive evaluations across\nmultiple datasets confirm the superiority of FakeVLM in both authenticity\nclassification and artifact explanation tasks, setting a new benchmark for\nsynthetic image detection. The dataset and code will be released in:\nhttps://github.com/opendatalab/FakeVLM.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14905.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "653b8c3e97a4d71d950e2f20",
      "avatarUrl": "/avatars/b68880022e14556d0be58c69615db3be.svg",
      "fullname": "Zichen Wen",
      "name": "zichenwen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.19910",
      "authors": [
        {
          "_id": "67e35e4cff080b9ee71e3295",
          "name": "Chuong Huynh",
          "hidden": false
        },
        {
          "_id": "67e35e4cff080b9ee71e3296",
          "name": "Jinyu Yang",
          "hidden": false
        },
        {
          "_id": "67e35e4cff080b9ee71e3297",
          "name": "Ashish Tawari",
          "hidden": false
        },
        {
          "_id": "67e35e4cff080b9ee71e3298",
          "name": "Mubarak Shah",
          "hidden": false
        },
        {
          "_id": "67e35e4cff080b9ee71e3299",
          "name": "Son Tran",
          "hidden": false
        },
        {
          "_id": "67e35e4cff080b9ee71e329a",
          "name": "Raffay Hamid",
          "hidden": false
        },
        {
          "_id": "67e35e4cff080b9ee71e329b",
          "name": "Trishul Chilimbi",
          "hidden": false
        },
        {
          "_id": "67e35e4cff080b9ee71e329c",
          "name": "Abhinav Shrivastava",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-25T17:59:50.000Z",
      "submittedOnDailyAt": "2025-03-26T00:26:00.764Z",
      "title": "CoLLM: A Large Language Model for Composed Image Retrieval",
      "submittedOnDailyBy": {
        "_id": "63a4d196cde2b28f82a56bd9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4d196cde2b28f82a56bd9/iqVFOtDteRMUScFGRcx0L.png",
        "isPro": false,
        "fullname": "Chuong Huynh",
        "user": "chuonghm",
        "type": "user"
      },
      "summary": "Composed Image Retrieval (CIR) is a complex task that aims to retrieve images\nbased on a multimodal query. Typical training data consists of triplets\ncontaining a reference image, a textual description of desired modifications,\nand the target image, which are expensive and time-consuming to acquire. The\nscarcity of CIR datasets has led to zero-shot approaches utilizing synthetic\ntriplets or leveraging vision-language models (VLMs) with ubiquitous\nweb-crawled image-caption pairs. However, these methods have significant\nlimitations: synthetic triplets suffer from limited scale, lack of diversity,\nand unnatural modification text, while image-caption pairs hinder joint\nembedding learning of the multimodal query due to the absence of triplet data.\nMoreover, existing approaches struggle with complex and nuanced modification\ntexts that demand sophisticated fusion and understanding of vision and language\nmodalities. We present CoLLM, a one-stop framework that effectively addresses\nthese limitations. Our approach generates triplets on-the-fly from\nimage-caption pairs, enabling supervised training without manual annotation. We\nleverage Large Language Models (LLMs) to generate joint embeddings of reference\nimages and modification texts, facilitating deeper multimodal fusion.\nAdditionally, we introduce Multi-Text CIR (MTCIR), a large-scale dataset\ncomprising 3.4M samples, and refine existing CIR benchmarks (CIRR and\nFashion-IQ) to enhance evaluation reliability. Experimental results demonstrate\nthat CoLLM achieves state-of-the-art performance across multiple CIR benchmarks\nand settings. MTCIR yields competitive results, with up to 15% performance\nimprovement. Our refined benchmarks provide more reliable evaluation metrics\nfor CIR models, contributing to the advancement of this important field.",
      "upvotes": 6,
      "discussionId": "67e35e4eff080b9ee71e3353",
      "projectPage": "https://collm-cvpr25.github.io/",
      "ai_keywords": [
        "Composed Image Retrieval (CIR)",
        "multimodal query",
        "triplets",
        "reference image",
        "textual description",
        "target image",
        "zero-shot approaches",
        "synthetic triplets",
        "vision-language models (VLMs)",
        "web-crawled image-caption pairs",
        "joint embedding learning",
        "complex and nuanced modification texts",
        "multimodal fusion",
        "CoLLM",
        "Large Language Models (LLMs)",
        "Multi-Text CIR (MTCIR)",
        "CIRR benchmark",
        "Fashion-IQ benchmark",
        "state-of-the-art performance"
      ]
    },
    "publishedAt": "2025-03-25T13:59:50.000Z",
    "title": "CoLLM: A Large Language Model for Composed Image Retrieval",
    "summary": "Composed Image Retrieval (CIR) is a complex task that aims to retrieve images\nbased on a multimodal query. Typical training data consists of triplets\ncontaining a reference image, a textual description of desired modifications,\nand the target image, which are expensive and time-consuming to acquire. The\nscarcity of CIR datasets has led to zero-shot approaches utilizing synthetic\ntriplets or leveraging vision-language models (VLMs) with ubiquitous\nweb-crawled image-caption pairs. However, these methods have significant\nlimitations: synthetic triplets suffer from limited scale, lack of diversity,\nand unnatural modification text, while image-caption pairs hinder joint\nembedding learning of the multimodal query due to the absence of triplet data.\nMoreover, existing approaches struggle with complex and nuanced modification\ntexts that demand sophisticated fusion and understanding of vision and language\nmodalities. We present CoLLM, a one-stop framework that effectively addresses\nthese limitations. Our approach generates triplets on-the-fly from\nimage-caption pairs, enabling supervised training without manual annotation. We\nleverage Large Language Models (LLMs) to generate joint embeddings of reference\nimages and modification texts, facilitating deeper multimodal fusion.\nAdditionally, we introduce Multi-Text CIR (MTCIR), a large-scale dataset\ncomprising 3.4M samples, and refine existing CIR benchmarks (CIRR and\nFashion-IQ) to enhance evaluation reliability. Experimental results demonstrate\nthat CoLLM achieves state-of-the-art performance across multiple CIR benchmarks\nand settings. MTCIR yields competitive results, with up to 15% performance\nimprovement. Our refined benchmarks provide more reliable evaluation metrics\nfor CIR models, contributing to the advancement of this important field.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19910.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a4d196cde2b28f82a56bd9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4d196cde2b28f82a56bd9/iqVFOtDteRMUScFGRcx0L.png",
      "fullname": "Chuong Huynh",
      "name": "chuonghm",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.13964",
      "authors": [
        {
          "_id": "67e20852c0c932395394dbb0",
          "name": "Siwei Han",
          "hidden": false
        },
        {
          "_id": "67e20852c0c932395394dbb1",
          "name": "Peng Xia",
          "hidden": false
        },
        {
          "_id": "67e20852c0c932395394dbb2",
          "name": "Ruiyi Zhang",
          "hidden": false
        },
        {
          "_id": "67e20852c0c932395394dbb3",
          "name": "Tong Sun",
          "hidden": false
        },
        {
          "_id": "67e20852c0c932395394dbb4",
          "name": "Yun Li",
          "hidden": false
        },
        {
          "_id": "67e20852c0c932395394dbb5",
          "name": "Hongtu Zhu",
          "hidden": false
        },
        {
          "_id": "67e20852c0c932395394dbb6",
          "name": "Huaxiu Yao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/643e9ee6f6bb3c31a26e7bc4/smmwVmcnReTUxH6xDnyU1.png"
      ],
      "publishedAt": "2025-03-18T06:57:21.000Z",
      "submittedOnDailyAt": "2025-03-26T03:52:37.520Z",
      "title": "MDocAgent: A Multi-Modal Multi-Agent Framework for Document\n  Understanding",
      "submittedOnDailyBy": {
        "_id": "643e9ee6f6bb3c31a26e7bc4",
        "avatarUrl": "/avatars/acfaa7d6a23dada24c86b954c3be116a.svg",
        "isPro": false,
        "fullname": "Peng Xia",
        "user": "richardxp888",
        "type": "user"
      },
      "summary": "Document Question Answering (DocQA) is a very common task. Existing methods\nusing Large Language Models (LLMs) or Large Vision Language Models (LVLMs) and\nRetrieval Augmented Generation (RAG) often prioritize information from a single\nmodal, failing to effectively integrate textual and visual cues. These\napproaches struggle with complex multi-modal reasoning, limiting their\nperformance on real-world documents. We present MDocAgent (A Multi-Modal\nMulti-Agent Framework for Document Understanding), a novel RAG and multi-agent\nframework that leverages both text and image. Our system employs five\nspecialized agents: a general agent, a critical agent, a text agent, an image\nagent and a summarizing agent. These agents engage in multi-modal context\nretrieval, combining their individual insights to achieve a more comprehensive\nunderstanding of the document's content. This collaborative approach enables\nthe system to synthesize information from both textual and visual components,\nleading to improved accuracy in question answering. Preliminary experiments on\nfive benchmarks like MMLongBench, LongDocURL demonstrate the effectiveness of\nour MDocAgent, achieve an average improvement of 12.1% compared to current\nstate-of-the-art method. This work contributes to the development of more\nrobust and comprehensive DocQA systems capable of handling the complexities of\nreal-world documents containing rich textual and visual information. Our data\nand code are available at https://github.com/aiming-lab/MDocAgent.",
      "upvotes": 4,
      "discussionId": "67e20858c0c932395394dde6",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "Large Vision Language Models (LVLMs)",
        "Retrieval Augmented Generation (RAG)",
        "multi-modal reasoning",
        "multi-modal multi-agent framework",
        "general agent",
        "critical agent",
        "text agent",
        "image agent",
        "summarizing agent",
        "multi-modal context retrieval"
      ]
    },
    "publishedAt": "2025-03-18T02:57:21.000Z",
    "title": "MDocAgent: A Multi-Modal Multi-Agent Framework for Document\n  Understanding",
    "summary": "Document Question Answering (DocQA) is a very common task. Existing methods\nusing Large Language Models (LLMs) or Large Vision Language Models (LVLMs) and\nRetrieval Augmented Generation (RAG) often prioritize information from a single\nmodal, failing to effectively integrate textual and visual cues. These\napproaches struggle with complex multi-modal reasoning, limiting their\nperformance on real-world documents. We present MDocAgent (A Multi-Modal\nMulti-Agent Framework for Document Understanding), a novel RAG and multi-agent\nframework that leverages both text and image. Our system employs five\nspecialized agents: a general agent, a critical agent, a text agent, an image\nagent and a summarizing agent. These agents engage in multi-modal context\nretrieval, combining their individual insights to achieve a more comprehensive\nunderstanding of the document's content. This collaborative approach enables\nthe system to synthesize information from both textual and visual components,\nleading to improved accuracy in question answering. Preliminary experiments on\nfive benchmarks like MMLongBench, LongDocURL demonstrate the effectiveness of\nour MDocAgent, achieve an average improvement of 12.1% compared to current\nstate-of-the-art method. This work contributes to the development of more\nrobust and comprehensive DocQA systems capable of handling the complexities of\nreal-world documents containing rich textual and visual information. Our data\nand code are available at https://github.com/aiming-lab/MDocAgent.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/643e9ee6f6bb3c31a26e7bc4/smmwVmcnReTUxH6xDnyU1.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13964.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643e9ee6f6bb3c31a26e7bc4",
      "avatarUrl": "/avatars/acfaa7d6a23dada24c86b954c3be116a.svg",
      "fullname": "Peng Xia",
      "name": "richardxp888",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.19855",
      "authors": [
        {
          "_id": "67e36792a281c900d76a93c8",
          "name": "Xiaoyu Tian",
          "hidden": false
        },
        {
          "_id": "67e36792a281c900d76a93c9",
          "name": "Sitong Zhao",
          "hidden": false
        },
        {
          "_id": "67e36792a281c900d76a93ca",
          "name": "Haotian Wang",
          "hidden": false
        },
        {
          "_id": "67e36792a281c900d76a93cb",
          "name": "Shuaiting Chen",
          "hidden": false
        },
        {
          "_id": "67e36792a281c900d76a93cc",
          "name": "Yunjie Ji",
          "hidden": false
        },
        {
          "_id": "67e36792a281c900d76a93cd",
          "name": "Yiping Peng",
          "hidden": false
        },
        {
          "_id": "67e36792a281c900d76a93ce",
          "name": "Han Zhao",
          "hidden": false
        },
        {
          "_id": "67e36792a281c900d76a93cf",
          "name": "Xiangang Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-25T17:19:38.000Z",
      "submittedOnDailyAt": "2025-03-26T01:04:39.479Z",
      "title": "Think Twice: Enhancing LLM Reasoning by Scaling Multi-round Test-time\n  Thinking",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Recent advances in large language models (LLMs), such as OpenAI-o1 and\nDeepSeek-R1, have demonstrated the effectiveness of test-time scaling, where\nextended reasoning processes substantially enhance model performance. Despite\nthis, current models are constrained by limitations in handling long texts and\nreinforcement learning (RL) training efficiency. To address these issues, we\npropose a simple yet effective test-time scaling approach Multi-round Thinking.\nThis method iteratively refines model reasoning by leveraging previous answers\nas prompts for subsequent rounds. Extensive experiments across multiple models,\nincluding QwQ-32B and DeepSeek-R1, consistently show performance improvements\non various benchmarks such as AIME 2024, MATH-500, GPQA-diamond, and\nLiveCodeBench. For instance, the accuracy of QwQ-32B improved from 80.3% (Round\n1) to 82.1% (Round 2) on the AIME 2024 dataset, while DeepSeek-R1 showed a\nsimilar increase from 79.7% to 82.0%. These results confirm that Multi-round\nThinking is a broadly applicable, straightforward approach to achieving stable\nenhancements in model performance, underscoring its potential for future\ndevelopments in test-time scaling techniques. The key prompt: {Original\nquestion prompt} The assistant's previous answer is: <answer> {last round\nanswer} </answer>, and please re-answer.",
      "upvotes": 3,
      "discussionId": "67e36793a281c900d76a9459",
      "ai_keywords": [
        "large language models",
        "OpenAI-o1",
        "DeepSeek-R1",
        "test-time scaling",
        "extended reasoning processes",
        "reinforcement learning",
        "Multi-round Thinking",
        "iterative refinement",
        "AIME 2024",
        "MATH-500",
        "GPQA-diamond",
        "LiveCodeBench",
        "accuracy",
        "stable enhancements",
        "test-time scaling techniques"
      ]
    },
    "publishedAt": "2025-03-25T13:19:38.000Z",
    "title": "Think Twice: Enhancing LLM Reasoning by Scaling Multi-round Test-time\n  Thinking",
    "summary": "Recent advances in large language models (LLMs), such as OpenAI-o1 and\nDeepSeek-R1, have demonstrated the effectiveness of test-time scaling, where\nextended reasoning processes substantially enhance model performance. Despite\nthis, current models are constrained by limitations in handling long texts and\nreinforcement learning (RL) training efficiency. To address these issues, we\npropose a simple yet effective test-time scaling approach Multi-round Thinking.\nThis method iteratively refines model reasoning by leveraging previous answers\nas prompts for subsequent rounds. Extensive experiments across multiple models,\nincluding QwQ-32B and DeepSeek-R1, consistently show performance improvements\non various benchmarks such as AIME 2024, MATH-500, GPQA-diamond, and\nLiveCodeBench. For instance, the accuracy of QwQ-32B improved from 80.3% (Round\n1) to 82.1% (Round 2) on the AIME 2024 dataset, while DeepSeek-R1 showed a\nsimilar increase from 79.7% to 82.0%. These results confirm that Multi-round\nThinking is a broadly applicable, straightforward approach to achieving stable\nenhancements in model performance, underscoring its potential for future\ndevelopments in test-time scaling techniques. The key prompt: {Original\nquestion prompt} The assistant's previous answer is: <answer> {last round\nanswer} </answer>, and please re-answer.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19855.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6469
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.19470",
      "authors": [
        {
          "_id": "67e365b0dcfc2aeae1bf3da2",
          "name": "Mingyang Chen",
          "hidden": false
        },
        {
          "_id": "67e365b0dcfc2aeae1bf3da3",
          "name": "Tianpeng Li",
          "hidden": false
        },
        {
          "_id": "67e365b0dcfc2aeae1bf3da4",
          "name": "Haoze Sun",
          "hidden": false
        },
        {
          "_id": "67e365b0dcfc2aeae1bf3da5",
          "name": "Yijie Zhou",
          "hidden": false
        },
        {
          "_id": "67e365b0dcfc2aeae1bf3da6",
          "name": "Chenzheng Zhu",
          "hidden": false
        },
        {
          "_id": "67e365b0dcfc2aeae1bf3da7",
          "name": "Fan Yang",
          "hidden": false
        },
        {
          "_id": "67e365b0dcfc2aeae1bf3da8",
          "name": "Zenan Zhou",
          "hidden": false
        },
        {
          "_id": "67e365b0dcfc2aeae1bf3da9",
          "name": "Weipeng Chen",
          "hidden": false
        },
        {
          "_id": "67e365b0dcfc2aeae1bf3daa",
          "name": "Haofen Wang",
          "hidden": false
        },
        {
          "_id": "67e365b0dcfc2aeae1bf3dab",
          "name": "Jeff Z. Pan",
          "hidden": false
        },
        {
          "_id": "67e365b0dcfc2aeae1bf3dac",
          "name": "Wen Zhang",
          "hidden": false
        },
        {
          "_id": "67e365b0dcfc2aeae1bf3dad",
          "name": "Huajun Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-25T09:00:58.000Z",
      "submittedOnDailyAt": "2025-03-26T00:56:07.098Z",
      "title": "ReSearch: Learning to Reason with Search for LLMs via Reinforcement\n  Learning",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) have shown remarkable capabilities in reasoning,\nexemplified by the success of OpenAI-o1 and DeepSeek-R1. However, integrating\nreasoning with external search processes remains challenging, especially for\ncomplex multi-hop questions requiring multiple retrieval steps. We propose\nReSearch, a novel framework that trains LLMs to Reason with Search via\nreinforcement learning without using any supervised data on reasoning steps.\nOur approach treats search operations as integral components of the reasoning\nchain, where when and how to perform searches is guided by text-based thinking,\nand search results subsequently influence further reasoning. We train ReSearch\non Qwen2.5-7B(-Instruct) and Qwen2.5-32B(-Instruct) models and conduct\nextensive experiments. Despite being trained on only one dataset, our models\ndemonstrate strong generalizability across various benchmarks. Analysis reveals\nthat ReSearch naturally elicits advanced reasoning capabilities such as\nreflection and self-correction during the reinforcement learning process.",
      "upvotes": 3,
      "discussionId": "67e365b1dcfc2aeae1bf3df6",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "OpenAI-o1",
        "DeepSeek-R1",
        "complex multi-hop questions",
        "ReSearch",
        "reinforcement learning",
        "text-based thinking",
        "reflection",
        "self-correction",
        "Qwen2.5-7B(-Instruct)",
        "Qwen2.5-32B(-Instruct)"
      ]
    },
    "publishedAt": "2025-03-25T05:00:58.000Z",
    "title": "ReSearch: Learning to Reason with Search for LLMs via Reinforcement\n  Learning",
    "summary": "Large Language Models (LLMs) have shown remarkable capabilities in reasoning,\nexemplified by the success of OpenAI-o1 and DeepSeek-R1. However, integrating\nreasoning with external search processes remains challenging, especially for\ncomplex multi-hop questions requiring multiple retrieval steps. We propose\nReSearch, a novel framework that trains LLMs to Reason with Search via\nreinforcement learning without using any supervised data on reasoning steps.\nOur approach treats search operations as integral components of the reasoning\nchain, where when and how to perform searches is guided by text-based thinking,\nand search results subsequently influence further reasoning. We train ReSearch\non Qwen2.5-7B(-Instruct) and Qwen2.5-32B(-Instruct) models and conduct\nextensive experiments. Despite being trained on only one dataset, our models\ndemonstrate strong generalizability across various benchmarks. Analysis reveals\nthat ReSearch naturally elicits advanced reasoning capabilities such as\nreflection and self-correction during the reinforcement learning process.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19470.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6469
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.19041",
      "authors": [
        {
          "_id": "67e35da0b1b97cc3392024b1",
          "name": "Kangwei Liu",
          "hidden": false
        },
        {
          "_id": "67e35da0b1b97cc3392024b2",
          "name": "Mengru Wang",
          "hidden": false
        },
        {
          "_id": "67e35da0b1b97cc3392024b3",
          "name": "Yujie Luo",
          "hidden": false
        },
        {
          "_id": "67e35da0b1b97cc3392024b4",
          "name": "Lin Yuan",
          "hidden": false
        },
        {
          "_id": "67e35da0b1b97cc3392024b5",
          "name": "Mengshu Sun",
          "hidden": false
        },
        {
          "_id": "67e35da0b1b97cc3392024b6",
          "name": "Ningyu Zhang",
          "hidden": false
        },
        {
          "_id": "67e35da0b1b97cc3392024b7",
          "name": "Lei Liang",
          "hidden": false
        },
        {
          "_id": "67e35da0b1b97cc3392024b8",
          "name": "Zhiqiang Zhang",
          "hidden": false
        },
        {
          "_id": "67e35da0b1b97cc3392024b9",
          "name": "Jun Zhou",
          "hidden": false
        },
        {
          "_id": "67e35da0b1b97cc3392024ba",
          "name": "Huajun Chen",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/620b3bbb0668e435407c8d0a/f0fTHNDhXrS7zWpmuxVU-.png"
      ],
      "publishedAt": "2025-03-24T18:11:42.000Z",
      "submittedOnDailyAt": "2025-03-26T00:22:20.466Z",
      "title": "LookAhead Tuning: Safer Language Models via Partial Answer Previews",
      "submittedOnDailyBy": {
        "_id": "620b3bbb0668e435407c8d0a",
        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
        "isPro": false,
        "fullname": "Ningyu Zhang",
        "user": "Ningyu",
        "type": "user"
      },
      "summary": "Fine-tuning enables large language models (LLMs) to adapt to specific\ndomains, but often undermines their previously established safety alignment. To\nmitigate the degradation of model safety during fine-tuning, we introduce\nLookAhead Tuning, which comprises two simple, low-resource, and effective\ndata-driven methods that modify training data by previewing partial answer\nprefixes. Both methods aim to preserve the model's inherent safety mechanisms\nby minimizing perturbations to initial token distributions. Comprehensive\nexperiments demonstrate that LookAhead Tuning effectively maintains model\nsafety without sacrificing robust performance on downstream tasks. Our findings\nposition LookAhead Tuning as a reliable and efficient solution for the safe and\neffective adaptation of LLMs. Code is released at\nhttps://github.com/zjunlp/LookAheadTuning.",
      "upvotes": 3,
      "discussionId": "67e35da1b1b97cc339202525",
      "ai_keywords": [
        "LookAhead Tuning",
        "safety alignment",
        "data-driven methods",
        "partial answer prefixes",
        "token distributions",
        "robust performance",
        "downstream tasks"
      ]
    },
    "publishedAt": "2025-03-24T14:11:42.000Z",
    "title": "LookAhead Tuning: Safer Language Models via Partial Answer Previews",
    "summary": "Fine-tuning enables large language models (LLMs) to adapt to specific\ndomains, but often undermines their previously established safety alignment. To\nmitigate the degradation of model safety during fine-tuning, we introduce\nLookAhead Tuning, which comprises two simple, low-resource, and effective\ndata-driven methods that modify training data by previewing partial answer\nprefixes. Both methods aim to preserve the model's inherent safety mechanisms\nby minimizing perturbations to initial token distributions. Comprehensive\nexperiments demonstrate that LookAhead Tuning effectively maintains model\nsafety without sacrificing robust performance on downstream tasks. Our findings\nposition LookAhead Tuning as a reliable and efficient solution for the safe and\neffective adaptation of LLMs. Code is released at\nhttps://github.com/zjunlp/LookAheadTuning.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/620b3bbb0668e435407c8d0a/f0fTHNDhXrS7zWpmuxVU-.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19041.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620b3bbb0668e435407c8d0a",
      "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
      "fullname": "Ningyu Zhang",
      "name": "Ningyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 20
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.18446",
      "authors": [
        {
          "_id": "67e367ee4363e3c4bbbaca3a",
          "name": "Jinho Jeong",
          "hidden": false
        },
        {
          "_id": "67e367ee4363e3c4bbbaca3b",
          "name": "Sangmin Han",
          "hidden": false
        },
        {
          "_id": "67e367ee4363e3c4bbbaca3c",
          "name": "Jinwoo Kim",
          "hidden": false
        },
        {
          "_id": "67e367ee4363e3c4bbbaca3d",
          "name": "Seon Joo Kim",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T08:50:15.000Z",
      "submittedOnDailyAt": "2025-03-26T01:07:15.007Z",
      "title": "Latent Space Super-Resolution for Higher-Resolution Image Generation\n  with Diffusion Models",
      "submittedOnDailyBy": {
        "_id": "66b5f733f0c16f37f307f35e",
        "avatarUrl": "/avatars/29a97e10b4d65aa23d7eae238f809499.svg",
        "isPro": false,
        "fullname": "JinHo Jeong",
        "user": "3587jjh",
        "type": "user"
      },
      "summary": "In this paper, we propose LSRNA, a novel framework for higher-resolution\n(exceeding 1K) image generation using diffusion models by leveraging\nsuper-resolution directly in the latent space. Existing diffusion models\nstruggle with scaling beyond their training resolutions, often leading to\nstructural distortions or content repetition. Reference-based methods address\nthe issues by upsampling a low-resolution reference to guide higher-resolution\ngeneration. However, they face significant challenges: upsampling in latent\nspace often causes manifold deviation, which degrades output quality. On the\nother hand, upsampling in RGB space tends to produce overly smoothed outputs.\nTo overcome these limitations, LSRNA combines Latent space Super-Resolution\n(LSR) for manifold alignment and Region-wise Noise Addition (RNA) to enhance\nhigh-frequency details. Our extensive experiments demonstrate that integrating\nLSRNA outperforms state-of-the-art reference-based methods across various\nresolutions and metrics, while showing the critical role of latent space\nupsampling in preserving detail and sharpness. The code is available at\nhttps://github.com/3587jjh/LSRNA.",
      "upvotes": 2,
      "discussionId": "67e367f14363e3c4bbbacae1",
      "ai_keywords": [
        "LSRNA",
        "diffusion models",
        "latent space",
        "super-resolution",
        "structural distortions",
        "content repetition",
        "reference-based methods",
        "manifold deviation",
        "RGB space",
        "manifold alignment",
        "Region-wise Noise Addition (RNA)",
        "high-frequency details"
      ]
    },
    "publishedAt": "2025-03-24T04:50:15.000Z",
    "title": "Latent Space Super-Resolution for Higher-Resolution Image Generation\n  with Diffusion Models",
    "summary": "In this paper, we propose LSRNA, a novel framework for higher-resolution\n(exceeding 1K) image generation using diffusion models by leveraging\nsuper-resolution directly in the latent space. Existing diffusion models\nstruggle with scaling beyond their training resolutions, often leading to\nstructural distortions or content repetition. Reference-based methods address\nthe issues by upsampling a low-resolution reference to guide higher-resolution\ngeneration. However, they face significant challenges: upsampling in latent\nspace often causes manifold deviation, which degrades output quality. On the\nother hand, upsampling in RGB space tends to produce overly smoothed outputs.\nTo overcome these limitations, LSRNA combines Latent space Super-Resolution\n(LSR) for manifold alignment and Region-wise Noise Addition (RNA) to enhance\nhigh-frequency details. Our extensive experiments demonstrate that integrating\nLSRNA outperforms state-of-the-art reference-based methods across various\nresolutions and metrics, while showing the critical role of latent space\nupsampling in preserving detail and sharpness. The code is available at\nhttps://github.com/3587jjh/LSRNA.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18446.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "66b5f733f0c16f37f307f35e",
      "avatarUrl": "/avatars/29a97e10b4d65aa23d7eae238f809499.svg",
      "fullname": "JinHo Jeong",
      "name": "3587jjh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.18783",
      "authors": [
        {
          "_id": "67e2a43d5116df47da357eec",
          "user": {
            "_id": "642438eaa3adbc7142c3ca0f",
            "avatarUrl": "/avatars/8deff70e0c93d259a42ee47f00a31e3e.svg",
            "isPro": false,
            "fullname": "CharlesChen",
            "user": "CharlesChen2023",
            "type": "user"
          },
          "name": "Linwei Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T14:36:22.430Z",
          "hidden": false
        },
        {
          "_id": "67e2a43d5116df47da357eed",
          "name": "Lin Gu",
          "hidden": false
        },
        {
          "_id": "67e2a43d5116df47da357eee",
          "name": "Liang Li",
          "hidden": false
        },
        {
          "_id": "67e2a43d5116df47da357eef",
          "name": "Chenggang Yan",
          "hidden": false
        },
        {
          "_id": "67e2a43d5116df47da357ef0",
          "name": "Ying Fu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T15:32:06.000Z",
      "submittedOnDailyAt": "2025-03-26T01:08:28.390Z",
      "title": "Frequency Dynamic Convolution for Dense Image Prediction",
      "submittedOnDailyBy": {
        "_id": "642438eaa3adbc7142c3ca0f",
        "avatarUrl": "/avatars/8deff70e0c93d259a42ee47f00a31e3e.svg",
        "isPro": false,
        "fullname": "CharlesChen",
        "user": "CharlesChen2023",
        "type": "user"
      },
      "summary": "While Dynamic Convolution (DY-Conv) has shown promising performance by\nenabling adaptive weight selection through multiple parallel weights combined\nwith an attention mechanism, the frequency response of these weights tends to\nexhibit high similarity, resulting in high parameter costs but limited\nadaptability. In this work, we introduce Frequency Dynamic Convolution\n(FDConv), a novel approach that mitigates these limitations by learning a fixed\nparameter budget in the Fourier domain. FDConv divides this budget into\nfrequency-based groups with disjoint Fourier indices, enabling the construction\nof frequency-diverse weights without increasing the parameter cost. To further\nenhance adaptability, we propose Kernel Spatial Modulation (KSM) and Frequency\nBand Modulation (FBM). KSM dynamically adjusts the frequency response of each\nfilter at the spatial level, while FBM decomposes weights into distinct\nfrequency bands in the frequency domain and modulates them dynamically based on\nlocal content. Extensive experiments on object detection, segmentation, and\nclassification validate the effectiveness of FDConv. We demonstrate that when\napplied to ResNet-50, FDConv achieves superior performance with a modest\nincrease of +3.6M parameters, outperforming previous methods that require\nsubstantial increases in parameter budgets (e.g., CondConv +90M, KW +76.5M).\nMoreover, FDConv seamlessly integrates into a variety of architectures,\nincluding ConvNeXt, Swin-Transformer, offering a flexible and efficient\nsolution for modern vision tasks. The code is made publicly available at\nhttps://github.com/Linwei-Chen/FDConv.",
      "upvotes": 1,
      "discussionId": "67e2a4405116df47da357ff7",
      "ai_keywords": [
        "Dynamic Convolution (DY-Conv)",
        "Frequency Dynamic Convolution (FDConv)",
        "attention mechanism",
        "parameter budget",
        "Fourier domain",
        "frequency-based groups",
        "disjoint Fourier indices",
        "frequency-diverse weights",
        "Kernel Spatial Modulation (KSM)",
        "Frequency Band Modulation (FBM)",
        "frequency response",
        "spatial level",
        "frequency bands",
        "local content",
        "object detection",
        "segmentation",
        "classification",
        "ResNet-50",
        "ConvNeXt",
        "Swin-Transformer",
        "parameter-efficient"
      ]
    },
    "publishedAt": "2025-03-24T11:32:06.000Z",
    "title": "Frequency Dynamic Convolution for Dense Image Prediction",
    "summary": "While Dynamic Convolution (DY-Conv) has shown promising performance by\nenabling adaptive weight selection through multiple parallel weights combined\nwith an attention mechanism, the frequency response of these weights tends to\nexhibit high similarity, resulting in high parameter costs but limited\nadaptability. In this work, we introduce Frequency Dynamic Convolution\n(FDConv), a novel approach that mitigates these limitations by learning a fixed\nparameter budget in the Fourier domain. FDConv divides this budget into\nfrequency-based groups with disjoint Fourier indices, enabling the construction\nof frequency-diverse weights without increasing the parameter cost. To further\nenhance adaptability, we propose Kernel Spatial Modulation (KSM) and Frequency\nBand Modulation (FBM). KSM dynamically adjusts the frequency response of each\nfilter at the spatial level, while FBM decomposes weights into distinct\nfrequency bands in the frequency domain and modulates them dynamically based on\nlocal content. Extensive experiments on object detection, segmentation, and\nclassification validate the effectiveness of FDConv. We demonstrate that when\napplied to ResNet-50, FDConv achieves superior performance with a modest\nincrease of +3.6M parameters, outperforming previous methods that require\nsubstantial increases in parameter budgets (e.g., CondConv +90M, KW +76.5M).\nMoreover, FDConv seamlessly integrates into a variety of architectures,\nincluding ConvNeXt, Swin-Transformer, offering a flexible and efficient\nsolution for modern vision tasks. The code is made publicly available at\nhttps://github.com/Linwei-Chen/FDConv.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18783.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642438eaa3adbc7142c3ca0f",
      "avatarUrl": "/avatars/8deff70e0c93d259a42ee47f00a31e3e.svg",
      "fullname": "CharlesChen",
      "name": "CharlesChen2023",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.17361",
      "authors": [
        {
          "_id": "67e35ca7363374850440d91d",
          "name": "Sophia Tang",
          "hidden": false
        },
        {
          "_id": "67e35ca7363374850440d91e",
          "name": "Yinuo Zhang",
          "hidden": false
        },
        {
          "_id": "67e35ca7363374850440d91f",
          "name": "Alexander Tong",
          "hidden": false
        },
        {
          "_id": "67e35ca7363374850440d920",
          "user": {
            "_id": "64cd5b3f0494187a9e8b7c69",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eo9HiGbvCxHQZ-QJB1Y_o.jpeg",
            "isPro": false,
            "fullname": "Pranam Chatterjee",
            "user": "pranamanam",
            "type": "user"
          },
          "name": "Pranam Chatterjee",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-26T01:57:51.167Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-21T17:59:43.000Z",
      "submittedOnDailyAt": "2025-03-26T00:18:51.908Z",
      "title": "Gumbel-Softmax Flow Matching with Straight-Through Guidance for\n  Controllable Biological Sequence Generation",
      "submittedOnDailyBy": {
        "_id": "64cd5b3f0494187a9e8b7c69",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eo9HiGbvCxHQZ-QJB1Y_o.jpeg",
        "isPro": false,
        "fullname": "Pranam Chatterjee",
        "user": "pranamanam",
        "type": "user"
      },
      "summary": "Flow matching in the continuous simplex has emerged as a promising strategy\nfor DNA sequence design, but struggles to scale to higher simplex dimensions\nrequired for peptide and protein generation. We introduce Gumbel-Softmax Flow\nand Score Matching, a generative framework on the simplex based on a novel\nGumbel-Softmax interpolant with a time-dependent temperature. Using this\ninterpolant, we introduce Gumbel-Softmax Flow Matching by deriving a\nparameterized velocity field that transports from smooth categorical\ndistributions to distributions concentrated at a single vertex of the simplex.\nWe alternatively present Gumbel-Softmax Score Matching which learns to regress\nthe gradient of the probability density. Our framework enables high-quality,\ndiverse generation and scales efficiently to higher-dimensional simplices. To\nenable training-free guidance, we propose Straight-Through Guided Flows\n(STGFlow), a classifier-based guidance method that leverages straight-through\nestimators to steer the unconditional velocity field toward optimal vertices of\nthe simplex. STGFlow enables efficient inference-time guidance using\nclassifiers pre-trained on clean sequences, and can be used with any discrete\nflow method. Together, these components form a robust framework for\ncontrollable de novo sequence generation. We demonstrate state-of-the-art\nperformance in conditional DNA promoter design, sequence-only protein\ngeneration, and target-binding peptide design for rare disease treatment.",
      "upvotes": 1,
      "discussionId": "67e35caa363374850440d9df",
      "ai_keywords": [
        "Gumbel-Softmax Flow",
        "Score Matching",
        "simplex",
        "Gumbel-Softmax interpolant",
        "time-dependent temperature",
        "parameterized velocity field",
        "smooth categorical distributions",
        "Gumbel-Softmax Flow Matching",
        "Straight-Through Guided Flows",
        "STGFlow",
        "straight-through estimators",
        "classifiers",
        "de novo sequence generation",
        "conditional DNA promoter design",
        "sequence-only protein generation",
        "target-binding peptide design"
      ]
    },
    "publishedAt": "2025-03-21T13:59:43.000Z",
    "title": "Gumbel-Softmax Flow Matching with Straight-Through Guidance for\n  Controllable Biological Sequence Generation",
    "summary": "Flow matching in the continuous simplex has emerged as a promising strategy\nfor DNA sequence design, but struggles to scale to higher simplex dimensions\nrequired for peptide and protein generation. We introduce Gumbel-Softmax Flow\nand Score Matching, a generative framework on the simplex based on a novel\nGumbel-Softmax interpolant with a time-dependent temperature. Using this\ninterpolant, we introduce Gumbel-Softmax Flow Matching by deriving a\nparameterized velocity field that transports from smooth categorical\ndistributions to distributions concentrated at a single vertex of the simplex.\nWe alternatively present Gumbel-Softmax Score Matching which learns to regress\nthe gradient of the probability density. Our framework enables high-quality,\ndiverse generation and scales efficiently to higher-dimensional simplices. To\nenable training-free guidance, we propose Straight-Through Guided Flows\n(STGFlow), a classifier-based guidance method that leverages straight-through\nestimators to steer the unconditional velocity field toward optimal vertices of\nthe simplex. STGFlow enables efficient inference-time guidance using\nclassifiers pre-trained on clean sequences, and can be used with any discrete\nflow method. Together, these components form a robust framework for\ncontrollable de novo sequence generation. We demonstrate state-of-the-art\nperformance in conditional DNA promoter design, sequence-only protein\ngeneration, and target-binding peptide design for rare disease treatment.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17361.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64cd5b3f0494187a9e8b7c69",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eo9HiGbvCxHQZ-QJB1Y_o.jpeg",
      "fullname": "Pranam Chatterjee",
      "name": "pranamanam",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.17237",
      "authors": [
        {
          "_id": "67e2b68e08c6a250edda264a",
          "user": {
            "_id": "67e2063e1ee7f6db889849d6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67e2063e1ee7f6db889849d6/ihiwCCqbXlxQ2V_SSGnng.jpeg",
            "isPro": false,
            "fullname": "Yu-Hsi Chen",
            "user": "wish44165",
            "type": "user"
          },
          "name": "Yu-Hsi Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T14:35:46.455Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/67e2063e1ee7f6db889849d6/Dn_qgqu4a6nI8HIAdpDvW.mp4"
      ],
      "publishedAt": "2025-03-21T15:40:18.000Z",
      "submittedOnDailyAt": "2025-03-26T04:35:14.607Z",
      "title": "Strong Baseline: Multi-UAV Tracking via YOLOv12 with BoT-SORT-ReID",
      "submittedOnDailyBy": {
        "_id": "67e2063e1ee7f6db889849d6",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67e2063e1ee7f6db889849d6/ihiwCCqbXlxQ2V_SSGnng.jpeg",
        "isPro": false,
        "fullname": "Yu-Hsi Chen",
        "user": "wish44165",
        "type": "user"
      },
      "summary": "Detecting and tracking multiple unmanned aerial vehicles (UAVs) in thermal\ninfrared video is inherently challenging due to low contrast, environmental\nnoise, and small target sizes. This paper provides a straightforward approach\nto address multi-UAV tracking in thermal infrared video, leveraging recent\nadvances in detection and tracking. Instead of relying on the YOLOv5 with the\nDeepSORT pipeline, we present a tracking framework built on YOLOv12 and\nBoT-SORT, enhanced with tailored training and inference strategies. We evaluate\nour approach following the metrics from the 4th Anti-UAV Challenge and\ndemonstrate competitive performance. Notably, we achieve strong results without\nusing contrast enhancement or temporal information fusion to enrich UAV\nfeatures, highlighting our approach as a \"Strong Baseline\" for the multi-UAV\ntracking task. We provide implementation details, in-depth experimental\nanalysis, and a discussion of potential improvements. The code is available at\nhttps://github.com/wish44165/YOLOv12-BoT-SORT-ReID .",
      "upvotes": 1,
      "discussionId": "67e2b69108c6a250edda279f",
      "githubRepo": "https://github.com/wish44165/YOLOv12-BoT-SORT-ReID",
      "ai_keywords": [
        "YOLOv12",
        "BoT-SORT",
        "multi-UAV tracking",
        "thermal infrared video",
        "detection",
        "tracking",
        "tailored training",
        "inference strategies",
        "4th Anti-UAV Challenge",
        "contrast enhancement",
        "temporal information fusion",
        "UAV features",
        "Strong Baseline"
      ]
    },
    "publishedAt": "2025-03-21T11:40:18.000Z",
    "title": "Strong Baseline: Multi-UAV Tracking via YOLOv12 with BoT-SORT-ReID",
    "summary": "Detecting and tracking multiple unmanned aerial vehicles (UAVs) in thermal\ninfrared video is inherently challenging due to low contrast, environmental\nnoise, and small target sizes. This paper provides a straightforward approach\nto address multi-UAV tracking in thermal infrared video, leveraging recent\nadvances in detection and tracking. Instead of relying on the YOLOv5 with the\nDeepSORT pipeline, we present a tracking framework built on YOLOv12 and\nBoT-SORT, enhanced with tailored training and inference strategies. We evaluate\nour approach following the metrics from the 4th Anti-UAV Challenge and\ndemonstrate competitive performance. Notably, we achieve strong results without\nusing contrast enhancement or temporal information fusion to enrich UAV\nfeatures, highlighting our approach as a \"Strong Baseline\" for the multi-UAV\ntracking task. We provide implementation details, in-depth experimental\nanalysis, and a discussion of potential improvements. The code is available at\nhttps://github.com/wish44165/YOLOv12-BoT-SORT-ReID .",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/67e2063e1ee7f6db889849d6/Dn_qgqu4a6nI8HIAdpDvW.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17237.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "67e2063e1ee7f6db889849d6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67e2063e1ee7f6db889849d6/ihiwCCqbXlxQ2V_SSGnng.jpeg",
      "fullname": "Yu-Hsi Chen",
      "name": "wish44165",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.16965",
      "authors": [
        {
          "_id": "67e35c3bf049c252c672b824",
          "name": "Zhe Hu",
          "hidden": false
        },
        {
          "_id": "67e35c3bf049c252c672b825",
          "name": "Jing Li",
          "hidden": false
        },
        {
          "_id": "67e35c3bf049c252c672b826",
          "name": "Yu Yin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-21T09:25:23.000Z",
      "submittedOnDailyAt": "2025-03-26T00:20:32.465Z",
      "title": "When Words Outperform Vision: VLMs Can Self-Improve Via Text-Only\n  Training For Human-Centered Decision Making",
      "submittedOnDailyBy": {
        "_id": "63999a6fe657365725d0d0a4",
        "avatarUrl": "/avatars/99736de1bc0d5decf4a6eda86e3c7937.svg",
        "isPro": false,
        "fullname": "Derek Zhe Hu",
        "user": "zhehuderek",
        "type": "user"
      },
      "summary": "Embodied decision-making is fundamental for AI agents operating in real-world\nenvironments. While Visual Language Models (VLMs) have advanced this\ncapability, they still struggle with complex decisions, particularly in\nhuman-centered situations that require deep reasoning about human needs and\nvalues. In this study, we systematically evaluate open-sourced VLMs on\nmultimodal human-centered decision-making tasks. We find that LLMs receiving\nonly textual descriptions unexpectedly outperform their VLM counterparts of\nsimilar scale that process actual images, suggesting that visual alignment may\nhinder VLM abilities. To address this challenge, we propose a novel text-only\ntraining approach with synthesized textual data. This method strengthens VLMs'\nlanguage components and transfers the learned abilities to multimodal\ninference, eliminating the need for expensive image-text paired data.\nFurthermore, we show that VLMs can achieve substantial performance gains\nthrough self-improvement, using training data generated by their LLM\ncounterparts rather than relying on larger teacher models like GPT-4. Our\nfindings establish a more efficient and scalable approach to enhancing VLMs'\nhuman-centered decision-making capabilities, opening new avenues for optimizing\nVLMs through self-improvement mechanisms.",
      "upvotes": 1,
      "discussionId": "67e35c3cf049c252c672b859",
      "ai_keywords": [
        "Visual Language Models (VLMs)",
        "multimodal human-centered decision-making tasks",
        "Large Language Models (LLMs)",
        "textual descriptions",
        "visual alignment",
        "text-only training approach",
        "synthesized textual data",
        "self-improvement",
        "training data",
        "GPT-4",
        "human-centered decision-making capabilities"
      ]
    },
    "publishedAt": "2025-03-21T05:25:23.000Z",
    "title": "When Words Outperform Vision: VLMs Can Self-Improve Via Text-Only\n  Training For Human-Centered Decision Making",
    "summary": "Embodied decision-making is fundamental for AI agents operating in real-world\nenvironments. While Visual Language Models (VLMs) have advanced this\ncapability, they still struggle with complex decisions, particularly in\nhuman-centered situations that require deep reasoning about human needs and\nvalues. In this study, we systematically evaluate open-sourced VLMs on\nmultimodal human-centered decision-making tasks. We find that LLMs receiving\nonly textual descriptions unexpectedly outperform their VLM counterparts of\nsimilar scale that process actual images, suggesting that visual alignment may\nhinder VLM abilities. To address this challenge, we propose a novel text-only\ntraining approach with synthesized textual data. This method strengthens VLMs'\nlanguage components and transfers the learned abilities to multimodal\ninference, eliminating the need for expensive image-text paired data.\nFurthermore, we show that VLMs can achieve substantial performance gains\nthrough self-improvement, using training data generated by their LLM\ncounterparts rather than relying on larger teacher models like GPT-4. Our\nfindings establish a more efficient and scalable approach to enhancing VLMs'\nhuman-centered decision-making capabilities, opening new avenues for optimizing\nVLMs through self-improvement mechanisms.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16965.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63999a6fe657365725d0d0a4",
      "avatarUrl": "/avatars/99736de1bc0d5decf4a6eda86e3c7937.svg",
      "fullname": "Derek Zhe Hu",
      "name": "zhehuderek",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]