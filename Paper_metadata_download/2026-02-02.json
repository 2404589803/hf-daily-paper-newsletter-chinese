[
  {
    "paper": {
      "id": "2601.21558",
      "authors": [
        {
          "_id": "697c279ea67238fac88cc104",
          "name": "Xiaoyu Tian",
          "hidden": false
        },
        {
          "_id": "697c279ea67238fac88cc105",
          "name": "Haotian Wang",
          "hidden": false
        },
        {
          "_id": "697c279ea67238fac88cc106",
          "name": "Shuaiting Chen",
          "hidden": false
        },
        {
          "_id": "697c279ea67238fac88cc107",
          "name": "Hao Zhou",
          "hidden": false
        },
        {
          "_id": "697c279ea67238fac88cc108",
          "name": "Kaichi Yu",
          "hidden": false
        },
        {
          "_id": "697c279ea67238fac88cc109",
          "name": "Yudian Zhang",
          "hidden": false
        },
        {
          "_id": "697c279ea67238fac88cc10a",
          "name": "Jade Ouyang",
          "hidden": false
        },
        {
          "_id": "697c279ea67238fac88cc10b",
          "name": "Junxi Yin",
          "hidden": false
        },
        {
          "_id": "697c279ea67238fac88cc10c",
          "name": "Jiong Chen",
          "hidden": false
        },
        {
          "_id": "697c279ea67238fac88cc10d",
          "name": "Baoyan Guo",
          "hidden": false
        },
        {
          "_id": "697c279ea67238fac88cc10e",
          "name": "Lei Zhang",
          "hidden": false
        },
        {
          "_id": "697c279ea67238fac88cc10f",
          "name": "Junjie Tao",
          "hidden": false
        },
        {
          "_id": "697c279ea67238fac88cc110",
          "name": "Yuansheng Song",
          "hidden": false
        },
        {
          "_id": "697c279ea67238fac88cc111",
          "name": "Ming Cui",
          "hidden": false
        },
        {
          "_id": "697c279ea67238fac88cc112",
          "name": "Chengwei Liu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/621499d72be42a56cca7afad/C3KRF5SVsqevuZy7nGJfm.png"
      ],
      "publishedAt": "2026-01-29T11:22:23.000Z",
      "submittedOnDailyAt": "2026-02-02T00:08:03.322Z",
      "title": "ASTRA: Automated Synthesis of agentic Trajectories and Reinforcement Arenas",
      "submittedOnDailyBy": {
        "_id": "621499d72be42a56cca7afad",
        "avatarUrl": "/avatars/3cea70d1a55c8096b4270093c69e4a5e.svg",
        "isPro": false,
        "fullname": "TianXiaoyu",
        "user": "Emperorizzis",
        "type": "user"
      },
      "summary": "Large language models (LLMs) are increasingly used as tool-augmented agents for multi-step decision making, yet training robust tool-using agents remains challenging. Existing methods still require manual intervention, depend on non-verifiable simulated environments, rely exclusively on either supervised fine-tuning (SFT) or reinforcement learning (RL), and struggle with stable long-horizon, multi-turn learning. To address these challenges, we introduce ASTRA, a fully automated end-to-end framework for training tool-augmented language model agents via scalable data synthesis and verifiable reinforcement learning. ASTRA integrates two complementary components. First, a pipeline that leverages the static topology of tool-call graphs synthesizes diverse, structurally grounded trajectories, instilling broad and transferable tool-use competence. Second, an environment synthesis framework that captures the rich, compositional topology of human semantic reasoning converts decomposed question-answer traces into independent, code-executable, and rule-verifiable environments, enabling deterministic multi-turn RL. Based on this method, we develop a unified training methodology that integrates SFT with online RL using trajectory-level rewards to balance task completion and interaction efficiency. Experiments on multiple agentic tool-use benchmarks demonstrate that ASTRA-trained models achieve state-of-the-art performance at comparable scales, approaching closed-source systems while preserving core reasoning ability. We release the full pipelines, environments, and trained models at https://github.com/LianjiaTech/astra.",
      "upvotes": 33,
      "discussionId": "697c279fa67238fac88cc113",
      "githubRepo": "https://github.com/LianjiaTech/astra",
      "githubRepoAddedBy": "user",
      "ai_summary": "ASTRA is an automated framework that trains tool-augmented language models using synthetic data and verifiable reinforcement learning to improve multi-step decision-making capabilities.",
      "ai_keywords": [
        "tool-call graphs",
        "trajectory-level rewards",
        "supervised fine-tuning",
        "reinforcement learning",
        "agent training",
        "multi-step decision making",
        "verifiable environments",
        "compositional topology",
        "semantic reasoning",
        "tool-augmented language models"
      ],
      "githubStars": 56
    },
    "publishedAt": "2026-01-29T06:22:23.000Z",
    "title": "ASTRA: Automated Synthesis of agentic Trajectories and Reinforcement Arenas",
    "summary": "Large language models (LLMs) are increasingly used as tool-augmented agents for multi-step decision making, yet training robust tool-using agents remains challenging. Existing methods still require manual intervention, depend on non-verifiable simulated environments, rely exclusively on either supervised fine-tuning (SFT) or reinforcement learning (RL), and struggle with stable long-horizon, multi-turn learning. To address these challenges, we introduce ASTRA, a fully automated end-to-end framework for training tool-augmented language model agents via scalable data synthesis and verifiable reinforcement learning. ASTRA integrates two complementary components. First, a pipeline that leverages the static topology of tool-call graphs synthesizes diverse, structurally grounded trajectories, instilling broad and transferable tool-use competence. Second, an environment synthesis framework that captures the rich, compositional topology of human semantic reasoning converts decomposed question-answer traces into independent, code-executable, and rule-verifiable environments, enabling deterministic multi-turn RL. Based on this method, we develop a unified training methodology that integrates SFT with online RL using trajectory-level rewards to balance task completion and interaction efficiency. Experiments on multiple agentic tool-use benchmarks demonstrate that ASTRA-trained models achieve state-of-the-art performance at comparable scales, approaching closed-source systems while preserving core reasoning ability. We release the full pipelines, environments, and trained models at https://github.com/LianjiaTech/astra.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/621499d72be42a56cca7afad/C3KRF5SVsqevuZy7nGJfm.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.21558.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "621499d72be42a56cca7afad",
      "avatarUrl": "/avatars/3cea70d1a55c8096b4270093c69e4a5e.svg",
      "fullname": "TianXiaoyu",
      "name": "Emperorizzis",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.23143",
      "authors": [
        {
          "_id": "69801ae26676f9332270659b",
          "name": "Seanie Lee",
          "hidden": false
        },
        {
          "_id": "69801ae26676f9332270659c",
          "name": "Sangwoo Park",
          "hidden": false
        },
        {
          "_id": "69801ae26676f9332270659d",
          "name": "Yumin Choi",
          "hidden": false
        },
        {
          "_id": "69801ae26676f9332270659e",
          "name": "Gyeongman Kim",
          "hidden": false
        },
        {
          "_id": "69801ae26676f9332270659f",
          "name": "Minki Kang",
          "hidden": false
        },
        {
          "_id": "69801ae26676f933227065a0",
          "name": "Jihun Yun",
          "hidden": false
        },
        {
          "_id": "69801ae26676f933227065a1",
          "name": "Dongmin Park",
          "hidden": false
        },
        {
          "_id": "69801ae26676f933227065a2",
          "name": "Jongho Park",
          "hidden": false
        },
        {
          "_id": "69801ae26676f933227065a3",
          "name": "Sung Ju Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-30T16:31:02.000Z",
      "submittedOnDailyAt": "2026-02-02T02:09:13.735Z",
      "title": "THINKSAFE: Self-Generated Safety Alignment for Reasoning Models",
      "submittedOnDailyBy": {
        "_id": "638716c14e00d7fc0902fef4",
        "avatarUrl": "/avatars/5fa8152f8c0e4e600d1a64802c3e0103.svg",
        "isPro": false,
        "fullname": "Sangwoo Park",
        "user": "Sangsang",
        "type": "user"
      },
      "summary": "Large reasoning models (LRMs) achieve remarkable performance by leveraging reinforcement learning (RL) on reasoning tasks to generate long chain-of-thought (CoT) reasoning. However, this over-optimization often prioritizes compliance, making models vulnerable to harmful prompts. To mitigate this safety degradation, recent approaches rely on external teacher distillation, yet this introduces a distributional discrepancy that degrades native reasoning. We propose ThinkSafe, a self-generated alignment framework that restores safety alignment without external teachers. Our key insight is that while compliance suppresses safety mechanisms, models often retain latent knowledge to identify harm. ThinkSafe unlocks this via lightweight refusal steering, guiding the model to generate in-distribution safety reasoning traces. Fine-tuning on these self-generated responses effectively realigns the model while minimizing distribution shift. Experiments on DeepSeek-R1-Distill and Qwen3 show ThinkSafe significantly improves safety while preserving reasoning proficiency. Notably, it achieves superior safety and comparable reasoning to GRPO, with significantly reduced computational cost. Code, models, and datasets are available at https://github.com/seanie12/ThinkSafe.git.",
      "upvotes": 25,
      "discussionId": "69801ae26676f933227065a4",
      "githubRepo": "https://github.com/seanie12/ThinkSafe.git",
      "githubRepoAddedBy": "user",
      "ai_summary": "ThinkSafe is a self-aligned framework that enhances safety in large reasoning models through lightweight refusal steering and fine-tuning on self-generated responses, maintaining reasoning performance while reducing computational costs.",
      "ai_keywords": [
        "reinforcement learning",
        "chain-of-thought reasoning",
        "external teacher distillation",
        "distributional discrepancy",
        "lightweight refusal steering",
        "self-generated alignment",
        "safety alignment",
        "reasoning proficiency",
        "computational cost"
      ],
      "githubStars": 1
    },
    "publishedAt": "2026-01-30T11:31:02.000Z",
    "title": "THINKSAFE: Self-Generated Safety Alignment for Reasoning Models",
    "summary": "Large reasoning models (LRMs) achieve remarkable performance by leveraging reinforcement learning (RL) on reasoning tasks to generate long chain-of-thought (CoT) reasoning. However, this over-optimization often prioritizes compliance, making models vulnerable to harmful prompts. To mitigate this safety degradation, recent approaches rely on external teacher distillation, yet this introduces a distributional discrepancy that degrades native reasoning. We propose ThinkSafe, a self-generated alignment framework that restores safety alignment without external teachers. Our key insight is that while compliance suppresses safety mechanisms, models often retain latent knowledge to identify harm. ThinkSafe unlocks this via lightweight refusal steering, guiding the model to generate in-distribution safety reasoning traces. Fine-tuning on these self-generated responses effectively realigns the model while minimizing distribution shift. Experiments on DeepSeek-R1-Distill and Qwen3 show ThinkSafe significantly improves safety while preserving reasoning proficiency. Notably, it achieves superior safety and comparable reasoning to GRPO, with significantly reduced computational cost. Code, models, and datasets are available at https://github.com/seanie12/ThinkSafe.git.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.23143.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "638716c14e00d7fc0902fef4",
      "avatarUrl": "/avatars/5fa8152f8c0e4e600d1a64802c3e0103.svg",
      "fullname": "Sangwoo Park",
      "name": "Sangsang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 13,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.22628",
      "authors": [
        {
          "_id": "6980103b6676f9332270654a",
          "name": "Chengyi Yang",
          "hidden": false
        },
        {
          "_id": "6980103b6676f9332270654b",
          "name": "Zhishang Xiang",
          "hidden": false
        },
        {
          "_id": "6980103b6676f9332270654c",
          "name": "Yunbo Tang",
          "hidden": false
        },
        {
          "_id": "6980103b6676f9332270654d",
          "name": "Zongpei Teng",
          "hidden": false
        },
        {
          "_id": "6980103b6676f9332270654e",
          "name": "Chengsong Huang",
          "hidden": false
        },
        {
          "_id": "6980103b6676f9332270654f",
          "name": "Fei Long",
          "hidden": false
        },
        {
          "_id": "6980103b6676f93322706550",
          "name": "Yuhan Liu",
          "hidden": false
        },
        {
          "_id": "6980103b6676f93322706551",
          "name": "Jinsong Su",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-30T06:38:02.000Z",
      "submittedOnDailyAt": "2026-02-02T00:37:03.238Z",
      "title": "TTCS: Test-Time Curriculum Synthesis for Self-Evolving",
      "submittedOnDailyBy": {
        "_id": "62ea79dd01ed9b0e8f61ccd3",
        "avatarUrl": "/avatars/70af83e0e267be39fcd5f23b85e2dafa.svg",
        "isPro": false,
        "fullname": "Chengsong Huang",
        "user": "ChengsongHuang",
        "type": "user"
      },
      "summary": "Test-Time Training offers a promising way to improve the reasoning ability of large language models (LLMs) by adapting the model using only the test questions. However, existing methods struggle with difficult reasoning problems for two reasons: raw test questions are often too difficult to yield high-quality pseudo-labels, and the limited size of test sets makes continuous online updates prone to instability. To address these limitations, we propose TTCS, a co-evolving test-time training framework. Specifically, TTCS initializes two policies from the same pretrained model: a question synthesizer and a reasoning solver. These policies evolve through iterative optimization: the synthesizer generates progressively challenging question variants conditioned on the test questions, creating a structured curriculum tailored to the solver's current capability, while the solver updates itself using self-consistency rewards computed from multiple sampled responses on both original test and synthetic questions. Crucially, the solver's feedback guides the synthesizer to generate questions aligned with the model's current capability, and the generated question variants in turn stabilize the solver's test-time training. Experiments show that TTCS consistently strengthens the reasoning ability on challenging mathematical benchmarks and transfers to general-domain tasks across different LLM backbones, highlighting a scalable path towards dynamically constructing test-time curricula for self-evolving. Our code and implementation details are available at https://github.com/XMUDeepLIT/TTCS.",
      "upvotes": 20,
      "discussionId": "6980103b6676f93322706552",
      "githubRepo": "https://github.com/XMUDeepLIT/TTCS",
      "githubRepoAddedBy": "user",
      "ai_summary": "TTCS is a co-evolving test-time training framework that enhances LLM reasoning abilities by iteratively generating challenging question variants and updating a reasoning solver through self-consistency rewards.",
      "ai_keywords": [
        "test-time training",
        "large language models",
        "pseudo-labels",
        "self-consistency rewards",
        "question synthesizer",
        "reasoning solver",
        "iterative optimization",
        "test-time curricula",
        "mathematical benchmarks",
        "general-domain tasks"
      ],
      "githubStars": 9
    },
    "publishedAt": "2026-01-30T01:38:02.000Z",
    "title": "TTCS: Test-Time Curriculum Synthesis for Self-Evolving",
    "summary": "Test-Time Training offers a promising way to improve the reasoning ability of large language models (LLMs) by adapting the model using only the test questions. However, existing methods struggle with difficult reasoning problems for two reasons: raw test questions are often too difficult to yield high-quality pseudo-labels, and the limited size of test sets makes continuous online updates prone to instability. To address these limitations, we propose TTCS, a co-evolving test-time training framework. Specifically, TTCS initializes two policies from the same pretrained model: a question synthesizer and a reasoning solver. These policies evolve through iterative optimization: the synthesizer generates progressively challenging question variants conditioned on the test questions, creating a structured curriculum tailored to the solver's current capability, while the solver updates itself using self-consistency rewards computed from multiple sampled responses on both original test and synthetic questions. Crucially, the solver's feedback guides the synthesizer to generate questions aligned with the model's current capability, and the generated question variants in turn stabilize the solver's test-time training. Experiments show that TTCS consistently strengthens the reasoning ability on challenging mathematical benchmarks and transfers to general-domain tasks across different LLM backbones, highlighting a scalable path towards dynamically constructing test-time curricula for self-evolving. Our code and implementation details are available at https://github.com/XMUDeepLIT/TTCS.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.22628.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "62ea79dd01ed9b0e8f61ccd3",
      "avatarUrl": "/avatars/70af83e0e267be39fcd5f23b85e2dafa.svg",
      "fullname": "Chengsong Huang",
      "name": "ChengsongHuang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.23265",
      "authors": [
        {
          "_id": "698024266676f933227065e5",
          "name": "Dawei Zhu",
          "hidden": false
        },
        {
          "_id": "698024266676f933227065e6",
          "name": "Rui Meng",
          "hidden": false
        },
        {
          "_id": "698024266676f933227065e7",
          "name": "Yale Song",
          "hidden": false
        },
        {
          "_id": "698024266676f933227065e8",
          "name": "Xiyu Wei",
          "hidden": false
        },
        {
          "_id": "698024266676f933227065e9",
          "name": "Sujian Li",
          "hidden": false
        },
        {
          "_id": "698024266676f933227065ea",
          "name": "Tomas Pfister",
          "hidden": false
        },
        {
          "_id": "698024266676f933227065eb",
          "name": "Jinsung Yoon",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-30T18:33:37.000Z",
      "submittedOnDailyAt": "2026-02-02T01:42:31.514Z",
      "title": "PaperBanana: Automating Academic Illustration for AI Scientists",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Despite rapid advances in autonomous AI scientists powered by language models, generating publication-ready illustrations remains a labor-intensive bottleneck in the research workflow. To lift this burden, we introduce PaperBanana, an agentic framework for automated generation of publication-ready academic illustrations. Powered by state-of-the-art VLMs and image generation models, PaperBanana orchestrates specialized agents to retrieve references, plan content and style, render images, and iteratively refine via self-critique. To rigorously evaluate our framework, we introduce PaperBananaBench, comprising 292 test cases for methodology diagrams curated from NeurIPS 2025 publications, covering diverse research domains and illustration styles. Comprehensive experiments demonstrate that PaperBanana consistently outperforms leading baselines in faithfulness, conciseness, readability, and aesthetics. We further show that our method effectively extends to the generation of high-quality statistical plots. Collectively, PaperBanana paves the way for the automated generation of publication-ready illustrations.",
      "upvotes": 7,
      "discussionId": "698024276676f933227065ec",
      "projectPage": "https://dwzhu-pku.github.io/PaperBanana/",
      "ai_summary": "_paperbanana is an agentic framework that automates the creation of publication-ready academic illustrations using advanced vision-language models and image generation techniques.",
      "ai_keywords": [
        "VLMs",
        "image generation models",
        "agentic framework",
        "publication-ready illustrations",
        "methodology diagrams",
        "PaperBananaBench",
        "self-critique",
        "statistical plots"
      ],
      "organization": {
        "_id": "5e6aca39878b8b2bf9806447",
        "name": "google",
        "fullname": "Google",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"
      }
    },
    "publishedAt": "2026-01-30T13:33:37.000Z",
    "title": "PaperBanana: Automating Academic Illustration for AI Scientists",
    "summary": "Despite rapid advances in autonomous AI scientists powered by language models, generating publication-ready illustrations remains a labor-intensive bottleneck in the research workflow. To lift this burden, we introduce PaperBanana, an agentic framework for automated generation of publication-ready academic illustrations. Powered by state-of-the-art VLMs and image generation models, PaperBanana orchestrates specialized agents to retrieve references, plan content and style, render images, and iteratively refine via self-critique. To rigorously evaluate our framework, we introduce PaperBananaBench, comprising 292 test cases for methodology diagrams curated from NeurIPS 2025 publications, covering diverse research domains and illustration styles. Comprehensive experiments demonstrate that PaperBanana consistently outperforms leading baselines in faithfulness, conciseness, readability, and aesthetics. We further show that our method effectively extends to the generation of high-quality statistical plots. Collectively, PaperBanana paves the way for the automated generation of publication-ready illustrations.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.23265.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 222,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "5e6aca39878b8b2bf9806447",
      "name": "google",
      "fullname": "Google",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.23182",
      "authors": [
        {
          "_id": "6980487a6676f933227066aa",
          "name": "Siyang He",
          "hidden": false
        },
        {
          "_id": "6980487a6676f933227066ab",
          "name": "Qiqi Wang",
          "hidden": false
        },
        {
          "_id": "6980487a6676f933227066ac",
          "name": "Xiaoran Liu",
          "hidden": false
        },
        {
          "_id": "6980487a6676f933227066ad",
          "name": "Hongnan Ma",
          "hidden": false
        },
        {
          "_id": "6980487a6676f933227066ae",
          "name": "Yiwei Shi",
          "hidden": false
        },
        {
          "_id": "6980487a6676f933227066af",
          "name": "Yuerong Song",
          "hidden": false
        },
        {
          "_id": "6980487a6676f933227066b0",
          "name": "Ying Zhu",
          "hidden": false
        },
        {
          "_id": "6980487a6676f933227066b1",
          "name": "Tianyi Liang",
          "hidden": false
        },
        {
          "_id": "6980487a6676f933227066b2",
          "name": "Zengfeng Huang",
          "hidden": false
        },
        {
          "_id": "6980487a6676f933227066b3",
          "name": "Ziwei He",
          "hidden": false
        },
        {
          "_id": "6980487a6676f933227066b4",
          "name": "Xipeng Qiu",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-30T17:06:41.000Z",
      "submittedOnDailyAt": "2026-02-02T04:18:39.975Z",
      "title": "FourierSampler: Unlocking Non-Autoregressive Potential in Diffusion Language Models via Frequency-Guided Generation",
      "submittedOnDailyBy": {
        "_id": "64f033ef82c6eea604c4da8b",
        "avatarUrl": "/avatars/51b93fea7fd68b4274ee03701245dcca.svg",
        "isPro": false,
        "fullname": "Xiaoran Liu (SII)",
        "user": "SII-xrliu",
        "type": "user"
      },
      "summary": "Despite the non-autoregressive potential of diffusion language models (dLLMs), existing decoding strategies demonstrate positional bias, failing to fully unlock the potential of arbitrary generation. In this work, we delve into the inherent spectral characteristics of dLLMs and present the first frequency-domain analysis showing that low-frequency components in hidden states primarily encode global structural information and long-range dependencies, while high-frequency components are responsible for characterizing local details. Based on this observation, we propose FourierSampler, which leverages a frequency-domain sliding window mechanism to dynamically guide the model to achieve a \"structure-to-detail\" generation. FourierSampler outperforms other inference enhancement strategies on LLADA and SDAR, achieving relative improvements of 20.4% on LLaDA1.5-8B and 16.0% on LLaDA-8B-Instruct. It notably surpasses similarly sized autoregressive models like Llama3.1-8B-Instruct.",
      "upvotes": 7,
      "discussionId": "6980487b6676f933227066b5",
      "githubRepo": "https://github.com/ShirleYoung/FourierSampler",
      "githubRepoAddedBy": "user",
      "ai_summary": "Frequency-domain analysis of diffusion language models reveals that low-frequency components encode global structure while high-frequency components capture local details, enabling improved generation through FourierSampler's dynamic frequency-domain sliding window mechanism.",
      "ai_keywords": [
        "diffusion language models",
        "spectral characteristics",
        "frequency-domain analysis",
        "hidden states",
        "low-frequency components",
        "high-frequency components",
        "FourierSampler",
        "frequency-domain sliding window mechanism",
        "arbitrary generation",
        "positional bias"
      ],
      "githubStars": 1,
      "organization": {
        "_id": "613b0dee83ec35d460684607",
        "name": "OpenMOSS-Team",
        "fullname": "OpenMOSS",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"
      }
    },
    "publishedAt": "2026-01-30T12:06:41.000Z",
    "title": "FourierSampler: Unlocking Non-Autoregressive Potential in Diffusion Language Models via Frequency-Guided Generation",
    "summary": "Despite the non-autoregressive potential of diffusion language models (dLLMs), existing decoding strategies demonstrate positional bias, failing to fully unlock the potential of arbitrary generation. In this work, we delve into the inherent spectral characteristics of dLLMs and present the first frequency-domain analysis showing that low-frequency components in hidden states primarily encode global structural information and long-range dependencies, while high-frequency components are responsible for characterizing local details. Based on this observation, we propose FourierSampler, which leverages a frequency-domain sliding window mechanism to dynamically guide the model to achieve a \"structure-to-detail\" generation. FourierSampler outperforms other inference enhancement strategies on LLADA and SDAR, achieving relative improvements of 20.4% on LLaDA1.5-8B and 16.0% on LLaDA-8B-Instruct. It notably surpasses similarly sized autoregressive models like Llama3.1-8B-Instruct.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.23182.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64f033ef82c6eea604c4da8b",
      "avatarUrl": "/avatars/51b93fea7fd68b4274ee03701245dcca.svg",
      "fullname": "Xiaoran Liu (SII)",
      "name": "SII-xrliu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "613b0dee83ec35d460684607",
      "name": "OpenMOSS-Team",
      "fullname": "OpenMOSS",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.22491",
      "authors": [
        {
          "_id": "69801f416676f933227065a6",
          "name": "Jinyang Wu",
          "hidden": false
        },
        {
          "_id": "69801f416676f933227065a7",
          "name": "Changpeng Yang",
          "hidden": false
        },
        {
          "_id": "69801f416676f933227065a8",
          "name": "Yuhao Shen",
          "hidden": false
        },
        {
          "_id": "69801f416676f933227065a9",
          "name": "Fangzhi Xu",
          "hidden": false
        },
        {
          "_id": "69801f416676f933227065aa",
          "name": "Bolin Ni",
          "hidden": false
        },
        {
          "_id": "69801f416676f933227065ab",
          "name": "Chonghua Liao",
          "hidden": false
        },
        {
          "_id": "69801f416676f933227065ac",
          "name": "Yuchen Liu",
          "hidden": false
        },
        {
          "_id": "69801f416676f933227065ad",
          "name": "Hongzhen Wang",
          "hidden": false
        },
        {
          "_id": "69801f416676f933227065ae",
          "name": "Shuai Nie",
          "hidden": false
        },
        {
          "_id": "69801f416676f933227065af",
          "name": "Shuai Zhang",
          "hidden": false
        },
        {
          "_id": "69801f416676f933227065b0",
          "name": "Haoran Luo",
          "hidden": false
        },
        {
          "_id": "69801f416676f933227065b1",
          "name": "Jiaming Xu",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-30T03:02:18.000Z",
      "submittedOnDailyAt": "2026-02-02T01:24:32.985Z",
      "title": "SSL: Sweet Spot Learning for Differentiated Guidance in Agentic Optimization",
      "submittedOnDailyBy": {
        "_id": "6747de57f8cab58c22ec94a2",
        "avatarUrl": "/avatars/5bae0341862fac24564781c0fa32aac5.svg",
        "isPro": false,
        "fullname": "Jinyang Wu",
        "user": "Jinyang23",
        "type": "user"
      },
      "summary": "Reinforcement learning with verifiable rewards has emerged as a powerful paradigm for training intelligent agents. However, existing methods typically employ binary rewards that fail to capture quality differences among trajectories achieving identical outcomes, thereby overlooking potential diversity within the solution space. Inspired by the ``sweet spot'' concept in tennis-the racket's core region that produces optimal hitting effects, we introduce Sweet Spot Learning (SSL), a novel framework that provides differentiated guidance for agent optimization. SSL follows a simple yet effective principle: progressively amplified, tiered rewards guide policies toward the sweet-spot region of the solution space. This principle naturally adapts across diverse tasks: visual perception tasks leverage distance-tiered modeling to reward proximity, while complex reasoning tasks reward incremental progress toward promising solutions. We theoretically demonstrate that SSL preserves optimal solution ordering and enhances the gradient signal-to-noise ratio, thereby fostering more directed optimization. Extensive experiments across GUI perception, short/long-term planning, and complex reasoning tasks show consistent improvements over strong baselines on 12 benchmarks, achieving up to 2.5X sample efficiency gains and effective cross-task transferability. Our work establishes SSL as a general principle for training capable and robust agents.",
      "upvotes": 7,
      "discussionId": "69801f416676f933227065b2",
      "ai_summary": "Sweet Spot Learning (SSL) introduces a novel reinforcement learning framework that uses tiered rewards to guide agent optimization toward optimal regions of the solution space, improving sample efficiency and cross-task transferability.",
      "ai_keywords": [
        "reinforcement learning",
        "verifiable rewards",
        "trajectory optimization",
        "policy optimization",
        "gradient signal-to-noise ratio",
        "sample efficiency",
        "cross-task transferability"
      ]
    },
    "publishedAt": "2026-01-29T22:02:18.000Z",
    "title": "SSL: Sweet Spot Learning for Differentiated Guidance in Agentic Optimization",
    "summary": "Reinforcement learning with verifiable rewards has emerged as a powerful paradigm for training intelligent agents. However, existing methods typically employ binary rewards that fail to capture quality differences among trajectories achieving identical outcomes, thereby overlooking potential diversity within the solution space. Inspired by the ``sweet spot'' concept in tennis-the racket's core region that produces optimal hitting effects, we introduce Sweet Spot Learning (SSL), a novel framework that provides differentiated guidance for agent optimization. SSL follows a simple yet effective principle: progressively amplified, tiered rewards guide policies toward the sweet-spot region of the solution space. This principle naturally adapts across diverse tasks: visual perception tasks leverage distance-tiered modeling to reward proximity, while complex reasoning tasks reward incremental progress toward promising solutions. We theoretically demonstrate that SSL preserves optimal solution ordering and enhances the gradient signal-to-noise ratio, thereby fostering more directed optimization. Extensive experiments across GUI perception, short/long-term planning, and complex reasoning tasks show consistent improvements over strong baselines on 12 benchmarks, achieving up to 2.5X sample efficiency gains and effective cross-task transferability. Our work establishes SSL as a general principle for training capable and robust agents.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.22491.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6747de57f8cab58c22ec94a2",
      "avatarUrl": "/avatars/5bae0341862fac24564781c0fa32aac5.svg",
      "fullname": "Jinyang Wu",
      "name": "Jinyang23",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.21716",
      "authors": [
        {
          "_id": "697d68416676f933227061e4",
          "name": "Mingshuang Luo",
          "hidden": false
        },
        {
          "_id": "697d68416676f933227061e5",
          "name": "Shuang Liang",
          "hidden": false
        },
        {
          "_id": "697d68416676f933227061e6",
          "name": "Zhengkun Rong",
          "hidden": false
        },
        {
          "_id": "697d68416676f933227061e7",
          "name": "Yuxuan Luo",
          "hidden": false
        },
        {
          "_id": "697d68416676f933227061e8",
          "name": "Tianshu Hu",
          "hidden": false
        },
        {
          "_id": "697d68416676f933227061e9",
          "name": "Ruibing Hou",
          "hidden": false
        },
        {
          "_id": "697d68416676f933227061ea",
          "name": "Hong Chang",
          "hidden": false
        },
        {
          "_id": "697d68416676f933227061eb",
          "name": "Yong Li",
          "hidden": false
        },
        {
          "_id": "697d68416676f933227061ec",
          "name": "Yuan Zhang",
          "hidden": false
        },
        {
          "_id": "697d68416676f933227061ed",
          "name": "Mingyuan Gao",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-29T13:43:17.000Z",
      "submittedOnDailyAt": "2026-02-02T00:12:40.328Z",
      "title": "DreamActor-M2: Universal Character Image Animation via Spatiotemporal In-Context Learning",
      "submittedOnDailyBy": {
        "_id": "6178ea05267320abb99df778",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1636104707106-6178ea05267320abb99df778.jpeg",
        "isPro": false,
        "fullname": "Mingshuang Luo",
        "user": "luomingshuang",
        "type": "user"
      },
      "summary": "Character image animation aims to synthesize high-fidelity videos by transferring motion from a driving sequence to a static reference image. Despite recent advancements, existing methods suffer from two fundamental challenges: (1) suboptimal motion injection strategies that lead to a trade-off between identity preservation and motion consistency, manifesting as a \"see-saw\", and (2) an over-reliance on explicit pose priors (e.g., skeletons), which inadequately capture intricate dynamics and hinder generalization to arbitrary, non-humanoid characters. To address these challenges, we present DreamActor-M2, a universal animation framework that reimagines motion conditioning as an in-context learning problem. Our approach follows a two-stage paradigm. First, we bridge the input modality gap by fusing reference appearance and motion cues into a unified latent space, enabling the model to jointly reason about spatial identity and temporal dynamics by leveraging the generative prior of foundational models. Second, we introduce a self-bootstrapped data synthesis pipeline that curates pseudo cross-identity training pairs, facilitating a seamless transition from pose-dependent control to direct, end-to-end RGB-driven animation. This strategy significantly enhances generalization across diverse characters and motion scenarios. To facilitate comprehensive evaluation, we further introduce AW Bench, a versatile benchmark encompassing a wide spectrum of characters types and motion scenarios. Extensive experiments demonstrate that DreamActor-M2 achieves state-of-the-art performance, delivering superior visual fidelity and robust cross-domain generalization. Project Page: https://grisoon.github.io/DreamActor-M2/",
      "upvotes": 6,
      "discussionId": "697d68426676f933227061ee",
      "projectPage": "https://grisoon.github.io/DreamActor-M2/",
      "ai_summary": "DreamActor-M2 presents a universal character animation framework that addresses motion injection trade-offs and pose prior limitations through in-context learning and self-bootstrapped data synthesis for improved generalization across diverse characters.",
      "ai_keywords": [
        "motion conditioning",
        "in-context learning",
        "latent space",
        "generative prior",
        "self-bootstrapped data synthesis",
        "cross-identity training pairs",
        "end-to-end RGB-driven animation",
        "cross-domain generalization"
      ],
      "organization": {
        "_id": "653b817d32c97d0655575872",
        "name": "ByteDance",
        "fullname": "ByteDance",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
      }
    },
    "publishedAt": "2026-01-29T08:43:17.000Z",
    "title": "DreamActor-M2: Universal Character Image Animation via Spatiotemporal In-Context Learning",
    "summary": "Character image animation aims to synthesize high-fidelity videos by transferring motion from a driving sequence to a static reference image. Despite recent advancements, existing methods suffer from two fundamental challenges: (1) suboptimal motion injection strategies that lead to a trade-off between identity preservation and motion consistency, manifesting as a \"see-saw\", and (2) an over-reliance on explicit pose priors (e.g., skeletons), which inadequately capture intricate dynamics and hinder generalization to arbitrary, non-humanoid characters. To address these challenges, we present DreamActor-M2, a universal animation framework that reimagines motion conditioning as an in-context learning problem. Our approach follows a two-stage paradigm. First, we bridge the input modality gap by fusing reference appearance and motion cues into a unified latent space, enabling the model to jointly reason about spatial identity and temporal dynamics by leveraging the generative prior of foundational models. Second, we introduce a self-bootstrapped data synthesis pipeline that curates pseudo cross-identity training pairs, facilitating a seamless transition from pose-dependent control to direct, end-to-end RGB-driven animation. This strategy significantly enhances generalization across diverse characters and motion scenarios. To facilitate comprehensive evaluation, we further introduce AW Bench, a versatile benchmark encompassing a wide spectrum of characters types and motion scenarios. Extensive experiments demonstrate that DreamActor-M2 achieves state-of-the-art performance, delivering superior visual fidelity and robust cross-domain generalization. Project Page: https://grisoon.github.io/DreamActor-M2/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.21716.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6178ea05267320abb99df778",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1636104707106-6178ea05267320abb99df778.jpeg",
      "fullname": "Mingshuang Luo",
      "name": "luomingshuang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "653b817d32c97d0655575872",
      "name": "ByteDance",
      "fullname": "ByteDance",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.21468",
      "authors": [
        {
          "_id": "697c7da1a67238fac88cc2d0",
          "name": "Yaorui Shi",
          "hidden": false
        },
        {
          "_id": "697c7da1a67238fac88cc2d1",
          "name": "Shugui Liu",
          "hidden": false
        },
        {
          "_id": "697c7da1a67238fac88cc2d2",
          "name": "Yu Yang",
          "hidden": false
        },
        {
          "_id": "697c7da1a67238fac88cc2d3",
          "name": "Wenyu Mao",
          "hidden": false
        },
        {
          "_id": "697c7da1a67238fac88cc2d4",
          "name": "Yuxin Chen",
          "hidden": false
        },
        {
          "_id": "697c7da1a67238fac88cc2d5",
          "name": "Qi GU",
          "hidden": false
        },
        {
          "_id": "697c7da1a67238fac88cc2d6",
          "name": "Hui Su",
          "hidden": false
        },
        {
          "_id": "697c7da1a67238fac88cc2d7",
          "name": "Xunliang Cai",
          "hidden": false
        },
        {
          "_id": "697c7da1a67238fac88cc2d8",
          "name": "Xiang Wang",
          "hidden": false
        },
        {
          "_id": "697c7da1a67238fac88cc2d9",
          "name": "An Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-29T09:47:17.000Z",
      "submittedOnDailyAt": "2026-02-02T01:45:23.265Z",
      "title": "MemOCR: Layout-Aware Visual Memory for Efficient Long-Horizon Reasoning",
      "submittedOnDailyBy": {
        "_id": "63edd2d1f765928ceeb49057",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676530369930-noauth.png",
        "isPro": false,
        "fullname": "Yaorui SHI",
        "user": "yrshi",
        "type": "user"
      },
      "summary": "Long-horizon agentic reasoning necessitates effectively compressing growing interaction histories into a limited context window. Most existing memory systems serialize history as text, where token-level cost is uniform and scales linearly with length, often spending scarce budget on low-value details. To this end, we introduce MemOCR, a multimodal memory agent that improves long-horizon reasoning under tight context budgets by allocating memory space with adaptive information density through visual layout. Concretely, MemOCR maintains a structured rich-text memory (e.g., headings, highlights) and renders it into an image that the agent consults for memory access, visually prioritizing crucial evidence while aggressively compressing auxiliary details. To ensure robustness across varying memory budgets, we train MemOCR with reinforcement learning under budget-aware objectives that expose the agent to diverse compression levels. Across long-context multi-hop and single-hop question-answering benchmarks, MemOCR outperforms strong text-based baselines and achieves more effective context utilization under extreme budgets.",
      "upvotes": 6,
      "discussionId": "697c7da1a67238fac88cc2da",
      "ai_summary": "MemOCR is a multimodal memory agent that enhances long-horizon reasoning by adaptively compressing interaction histories into visual layouts, enabling efficient context utilization under tight budget constraints.",
      "ai_keywords": [
        "memory systems",
        "context window",
        "visual layout",
        "structured rich-text memory",
        "reinforcement learning",
        "context utilization",
        "long-horizon reasoning"
      ]
    },
    "publishedAt": "2026-01-29T04:47:17.000Z",
    "title": "MemOCR: Layout-Aware Visual Memory for Efficient Long-Horizon Reasoning",
    "summary": "Long-horizon agentic reasoning necessitates effectively compressing growing interaction histories into a limited context window. Most existing memory systems serialize history as text, where token-level cost is uniform and scales linearly with length, often spending scarce budget on low-value details. To this end, we introduce MemOCR, a multimodal memory agent that improves long-horizon reasoning under tight context budgets by allocating memory space with adaptive information density through visual layout. Concretely, MemOCR maintains a structured rich-text memory (e.g., headings, highlights) and renders it into an image that the agent consults for memory access, visually prioritizing crucial evidence while aggressively compressing auxiliary details. To ensure robustness across varying memory budgets, we train MemOCR with reinforcement learning under budget-aware objectives that expose the agent to diverse compression levels. Across long-context multi-hop and single-hop question-answering benchmarks, MemOCR outperforms strong text-based baselines and achieves more effective context utilization under extreme budgets.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.21468.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63edd2d1f765928ceeb49057",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676530369930-noauth.png",
      "fullname": "Yaorui SHI",
      "name": "yrshi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.20218",
      "authors": [
        {
          "_id": "697b3e53870173bd91777369",
          "user": {
            "_id": "665fce33b99c631f4f57e650",
            "avatarUrl": "/avatars/dc98235d6e4e33e5980c2b46627c238b.svg",
            "isPro": false,
            "fullname": "Haoyou Deng",
            "user": "haoyou11",
            "type": "user"
          },
          "name": "Haoyou Deng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-29T13:56:28.058Z",
          "hidden": false
        },
        {
          "_id": "697b3e53870173bd9177736a",
          "name": "Keyu Yan",
          "hidden": false
        },
        {
          "_id": "697b3e53870173bd9177736b",
          "name": "Chaojie Mao",
          "hidden": false
        },
        {
          "_id": "697b3e53870173bd9177736c",
          "name": "Xiang Wang",
          "hidden": false
        },
        {
          "_id": "697b3e53870173bd9177736d",
          "name": "Yu Liu",
          "hidden": false
        },
        {
          "_id": "697b3e53870173bd9177736e",
          "name": "Changxin Gao",
          "hidden": false
        },
        {
          "_id": "697b3e53870173bd9177736f",
          "name": "Nong Sang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-28T03:39:05.000Z",
      "submittedOnDailyAt": "2026-02-02T00:05:08.557Z",
      "title": "DenseGRPO: From Sparse to Dense Reward for Flow Matching Model Alignment",
      "submittedOnDailyBy": {
        "_id": "665fce33b99c631f4f57e650",
        "avatarUrl": "/avatars/dc98235d6e4e33e5980c2b46627c238b.svg",
        "isPro": false,
        "fullname": "Haoyou Deng",
        "user": "haoyou11",
        "type": "user"
      },
      "summary": "Recent GRPO-based approaches built on flow matching models have shown remarkable improvements in human preference alignment for text-to-image generation. Nevertheless, they still suffer from the sparse reward problem: the terminal reward of the entire denoising trajectory is applied to all intermediate steps, resulting in a mismatch between the global feedback signals and the exact fine-grained contributions at intermediate denoising steps. To address this issue, we introduce DenseGRPO, a novel framework that aligns human preference with dense rewards, which evaluates the fine-grained contribution of each denoising step. Specifically, our approach includes two key components: (1) we propose to predict the step-wise reward gain as dense reward of each denoising step, which applies a reward model on the intermediate clean images via an ODE-based approach. This manner ensures an alignment between feedback signals and the contributions of individual steps, facilitating effective training; and (2) based on the estimated dense rewards, a mismatch drawback between the uniform exploration setting and the time-varying noise intensity in existing GRPO-based methods is revealed, leading to an inappropriate exploration space. Thus, we propose a reward-aware scheme to calibrate the exploration space by adaptively adjusting a timestep-specific stochasticity injection in the SDE sampler, ensuring a suitable exploration space at all timesteps. Extensive experiments on multiple standard benchmarks demonstrate the effectiveness of the proposed DenseGRPO and highlight the critical role of the valid dense rewards in flow matching model alignment.",
      "upvotes": 6,
      "discussionId": "697b3e54870173bd91777370",
      "ai_summary": "DenseGRPO addresses sparse reward problems in flow matching models by introducing dense rewards for intermediate denoising steps and adaptive exploration calibration.",
      "ai_keywords": [
        "flow matching models",
        "denoising trajectory",
        "sparse reward problem",
        "dense rewards",
        "step-wise reward gain",
        "reward model",
        "ODE-based approach",
        "SDE sampler",
        "stochasticity injection",
        "time-varying noise intensity",
        "reward-aware scheme",
        "exploration space"
      ],
      "organization": {
        "_id": "67d15cca6e2cf0e062dbfb54",
        "name": "AlibabaTongyiLab",
        "fullname": "TongyiLab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
      }
    },
    "publishedAt": "2026-01-27T22:39:05.000Z",
    "title": "DenseGRPO: From Sparse to Dense Reward for Flow Matching Model Alignment",
    "summary": "Recent GRPO-based approaches built on flow matching models have shown remarkable improvements in human preference alignment for text-to-image generation. Nevertheless, they still suffer from the sparse reward problem: the terminal reward of the entire denoising trajectory is applied to all intermediate steps, resulting in a mismatch between the global feedback signals and the exact fine-grained contributions at intermediate denoising steps. To address this issue, we introduce DenseGRPO, a novel framework that aligns human preference with dense rewards, which evaluates the fine-grained contribution of each denoising step. Specifically, our approach includes two key components: (1) we propose to predict the step-wise reward gain as dense reward of each denoising step, which applies a reward model on the intermediate clean images via an ODE-based approach. This manner ensures an alignment between feedback signals and the contributions of individual steps, facilitating effective training; and (2) based on the estimated dense rewards, a mismatch drawback between the uniform exploration setting and the time-varying noise intensity in existing GRPO-based methods is revealed, leading to an inappropriate exploration space. Thus, we propose a reward-aware scheme to calibrate the exploration space by adaptively adjusting a timestep-specific stochasticity injection in the SDE sampler, ensuring a suitable exploration space at all timesteps. Extensive experiments on multiple standard benchmarks demonstrate the effectiveness of the proposed DenseGRPO and highlight the critical role of the valid dense rewards in flow matching model alignment.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.20218.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "665fce33b99c631f4f57e650",
      "avatarUrl": "/avatars/dc98235d6e4e33e5980c2b46627c238b.svg",
      "fullname": "Haoyou Deng",
      "name": "haoyou11",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "67d15cca6e2cf0e062dbfb54",
      "name": "AlibabaTongyiLab",
      "fullname": "TongyiLab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2601.22642",
      "authors": [
        {
          "_id": "698023f56676f933227065d8",
          "name": "Chuxue Cao",
          "hidden": false
        },
        {
          "_id": "698023f56676f933227065d9",
          "name": "Jinluan Yang",
          "hidden": false
        },
        {
          "_id": "698023f56676f933227065da",
          "name": "Haoran Li",
          "hidden": false
        },
        {
          "_id": "698023f56676f933227065db",
          "name": "Kunhao Pan",
          "hidden": false
        },
        {
          "_id": "698023f56676f933227065dc",
          "name": "Zijian Zhao",
          "hidden": false
        },
        {
          "_id": "698023f56676f933227065dd",
          "name": "Zhengyu Chen",
          "hidden": false
        },
        {
          "_id": "698023f56676f933227065de",
          "name": "Yuchen Tian",
          "hidden": false
        },
        {
          "_id": "698023f56676f933227065df",
          "name": "Lijun Wu",
          "hidden": false
        },
        {
          "_id": "698023f56676f933227065e0",
          "name": "Conghui He",
          "hidden": false
        },
        {
          "_id": "698023f56676f933227065e1",
          "name": "Sirui Han",
          "hidden": false
        },
        {
          "_id": "698023f56676f933227065e2",
          "name": "Yike Guo",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-30T07:01:25.000Z",
      "submittedOnDailyAt": "2026-02-02T02:04:09.685Z",
      "title": "Pushing the Boundaries of Natural Reasoning: Interleaved Bonus from Formal-Logic Verification",
      "submittedOnDailyBy": {
        "_id": "64c1e3a911c6194a55d974df",
        "avatarUrl": "/avatars/db7c37ba78b726a5646221bf9bc4cc6c.svg",
        "isPro": false,
        "fullname": "Chuxue Cao",
        "user": "chuxuecao",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) show remarkable capabilities, yet their stochastic next-token prediction creates logical inconsistencies and reward hacking that formal symbolic systems avoid. To bridge this gap, we introduce a formal logic verification-guided framework that dynamically interleaves formal symbolic verification with the natural language generation process, providing real-time feedback to detect and rectify errors as they occur. Distinguished from previous neuro-symbolic methods limited by passive post-hoc validation, our approach actively penalizes intermediate fallacies during the reasoning chain. We operationalize this framework via a novel two-stage training pipeline that synergizes formal logic verification-guided supervised fine-tuning and policy optimization. Extensive evaluation on six benchmarks spanning mathematical, logical, and general reasoning demonstrates that our 7B and 14B models outperform state-of-the-art baselines by average margins of 10.4% and 14.2%, respectively. These results validate that formal verification can serve as a scalable mechanism to significantly push the performance boundaries of advanced LLM reasoning.",
      "upvotes": 5,
      "discussionId": "698023f56676f933227065e3",
      "ai_summary": "A formal logic verification-guided framework dynamically interleaves symbolic verification with natural language generation to improve reasoning accuracy and reduce errors in large language models.",
      "ai_keywords": [
        "large language models",
        "formal logic verification",
        "symbolic verification",
        "natural language generation",
        "reasoning chain",
        "supervised fine-tuning",
        "policy optimization",
        "mathematical reasoning",
        "logical reasoning",
        "general reasoning"
      ],
      "organization": {
        "_id": "6609f50bf4ab651901ae4541",
        "name": "hongkongust",
        "fullname": "The Hong Kong University of Science and Technology"
      }
    },
    "publishedAt": "2026-01-30T02:01:25.000Z",
    "title": "Pushing the Boundaries of Natural Reasoning: Interleaved Bonus from Formal-Logic Verification",
    "summary": "Large Language Models (LLMs) show remarkable capabilities, yet their stochastic next-token prediction creates logical inconsistencies and reward hacking that formal symbolic systems avoid. To bridge this gap, we introduce a formal logic verification-guided framework that dynamically interleaves formal symbolic verification with the natural language generation process, providing real-time feedback to detect and rectify errors as they occur. Distinguished from previous neuro-symbolic methods limited by passive post-hoc validation, our approach actively penalizes intermediate fallacies during the reasoning chain. We operationalize this framework via a novel two-stage training pipeline that synergizes formal logic verification-guided supervised fine-tuning and policy optimization. Extensive evaluation on six benchmarks spanning mathematical, logical, and general reasoning demonstrates that our 7B and 14B models outperform state-of-the-art baselines by average margins of 10.4% and 14.2%, respectively. These results validate that formal verification can serve as a scalable mechanism to significantly push the performance boundaries of advanced LLM reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.22642.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c1e3a911c6194a55d974df",
      "avatarUrl": "/avatars/db7c37ba78b726a5646221bf9bc4cc6c.svg",
      "fullname": "Chuxue Cao",
      "name": "chuxuecao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "6609f50bf4ab651901ae4541",
      "name": "hongkongust",
      "fullname": "The Hong Kong University of Science and Technology"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.22636",
      "authors": [
        {
          "_id": "6980426e6676f93322706680",
          "name": "Mingqian Feng",
          "hidden": false
        },
        {
          "_id": "6980426e6676f93322706681",
          "name": "Xiaodong Liu",
          "hidden": false
        },
        {
          "_id": "6980426e6676f93322706682",
          "name": "Weiwei Yang",
          "hidden": false
        },
        {
          "_id": "6980426e6676f93322706683",
          "name": "Chenliang Xu",
          "hidden": false
        },
        {
          "_id": "6980426e6676f93322706684",
          "name": "Christopher White",
          "hidden": false
        },
        {
          "_id": "6980426e6676f93322706685",
          "name": "Jianfeng Gao",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-30T06:54:35.000Z",
      "submittedOnDailyAt": "2026-02-02T03:54:34.923Z",
      "title": "Statistical Estimation of Adversarial Risk in Large Language Models under Best-of-N Sampling",
      "submittedOnDailyBy": {
        "_id": "66332a98e39731d65a4b7e45",
        "avatarUrl": "/avatars/193e9ee3ac7488960229d6edddb9d1e9.svg",
        "isPro": true,
        "fullname": "Mingqian Feng",
        "user": "fmmarkmq",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) are typically evaluated for safety under single-shot or low-budget adversarial prompting, which underestimates real-world risk. In practice, attackers can exploit large-scale parallel sampling to repeatedly probe a model until a harmful response is produced. While recent work shows that attack success increases with repeated sampling, principled methods for predicting large-scale adversarial risk remain limited. We propose a scaling-aware Best-of-N estimation of risk, SABER, for modeling jailbreak vulnerability under Best-of-N sampling. We model sample-level success probabilities using a Beta distribution, the conjugate prior of the Bernoulli distribution, and derive an analytic scaling law that enables reliable extrapolation of large-N attack success rates from small-budget measurements. Using only n=100 samples, our anchored estimator predicts ASR@1000 with a mean absolute error of 1.66, compared to 12.04 for the baseline, which is an 86.2% reduction in estimation error. Our results reveal heterogeneous risk scaling profiles and show that models appearing robust under standard evaluation can experience rapid nonlinear risk amplification under parallel adversarial pressure. This work provides a low-cost, scalable methodology for realistic LLM safety assessment. We will release our code and evaluation scripts upon publication to future research.",
      "upvotes": 5,
      "discussionId": "6980426e6676f93322706686",
      "ai_summary": "A scaling-aware risk estimation method called SABER is introduced for predicting large-scale adversarial vulnerability in language models through Best-of-N sampling, enabling accurate assessment with reduced computational costs.",
      "ai_keywords": [
        "large language models",
        "adversarial prompting",
        "Best-of-N sampling",
        "jailbreak vulnerability",
        "Beta distribution",
        "scaling law",
        "attack success rate",
        "parallel sampling",
        "risk estimation"
      ],
      "organization": {
        "_id": "5e6485f787403103f9f1055e",
        "name": "microsoft",
        "fullname": "Microsoft",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"
      }
    },
    "publishedAt": "2026-01-30T01:54:35.000Z",
    "title": "Statistical Estimation of Adversarial Risk in Large Language Models under Best-of-N Sampling",
    "summary": "Large Language Models (LLMs) are typically evaluated for safety under single-shot or low-budget adversarial prompting, which underestimates real-world risk. In practice, attackers can exploit large-scale parallel sampling to repeatedly probe a model until a harmful response is produced. While recent work shows that attack success increases with repeated sampling, principled methods for predicting large-scale adversarial risk remain limited. We propose a scaling-aware Best-of-N estimation of risk, SABER, for modeling jailbreak vulnerability under Best-of-N sampling. We model sample-level success probabilities using a Beta distribution, the conjugate prior of the Bernoulli distribution, and derive an analytic scaling law that enables reliable extrapolation of large-N attack success rates from small-budget measurements. Using only n=100 samples, our anchored estimator predicts ASR@1000 with a mean absolute error of 1.66, compared to 12.04 for the baseline, which is an 86.2% reduction in estimation error. Our results reveal heterogeneous risk scaling profiles and show that models appearing robust under standard evaluation can experience rapid nonlinear risk amplification under parallel adversarial pressure. This work provides a low-cost, scalable methodology for realistic LLM safety assessment. We will release our code and evaluation scripts upon publication to future research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.22636.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66332a98e39731d65a4b7e45",
      "avatarUrl": "/avatars/193e9ee3ac7488960229d6edddb9d1e9.svg",
      "fullname": "Mingqian Feng",
      "name": "fmmarkmq",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "5e6485f787403103f9f1055e",
      "name": "microsoft",
      "fullname": "Microsoft",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.21358",
      "authors": [
        {
          "_id": "69801aa26676f93322706596",
          "name": "Jiecong Wang",
          "hidden": false
        },
        {
          "_id": "69801aa26676f93322706597",
          "name": "Hao Peng",
          "hidden": false
        },
        {
          "_id": "69801aa26676f93322706598",
          "name": "Chunyang Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-29T07:38:18.000Z",
      "submittedOnDailyAt": "2026-02-02T01:03:26.097Z",
      "title": "Latent Chain-of-Thought as Planning: Decoupling Reasoning from Verbalization",
      "submittedOnDailyBy": {
        "_id": "652275e1d65824bda7a845db",
        "avatarUrl": "/avatars/ae465143a480e4f739c3c4c544490f25.svg",
        "isPro": false,
        "fullname": "jc",
        "user": "yunsaijc",
        "type": "user"
      },
      "summary": "Chain-of-Thought (CoT) empowers Large Language Models (LLMs) to tackle complex problems, but remains constrained by the computational cost and reasoning path collapse when grounded in discrete token spaces. Recent latent reasoning approaches attempt to optimize efficiency by performing reasoning within continuous hidden states. However, these methods typically operate as opaque end-to-end mappings from explicit reasoning steps to latent states, and often require a pre-defined number of latent steps during inference. In this work, we introduce PLaT (Planning with Latent Thoughts), a framework that reformulates latent reasoning as planning by fundamentally decouple reasoning from verbalization. We model reasoning as a deterministic trajectory of latent planning states, while a separate Decoder grounds these thoughts into text when necessary. This decoupling allows the model to dynamically determine when to terminate reasoning rather than relying on fixed hyperparameters. Empirical results on mathematical benchmarks reveal a distinct trade-off: while PLaT achieves lower greedy accuracy than baselines, it demonstrates superior scalability in terms of reasoning diversity. This indicates that PLaT learns a robust, broader solution space, offering a transparent and scalable foundation for inference-time search.",
      "upvotes": 4,
      "discussionId": "69801aa26676f93322706599",
      "ai_summary": "PLaT introduces a latent reasoning framework that decouples reasoning from verbalization, enabling dynamic termination and improved scalability over traditional approaches.",
      "ai_keywords": [
        "Chain-of-Thought",
        "Large Language Models",
        "latent reasoning",
        "discrete token spaces",
        "continuous hidden states",
        "deterministic trajectory",
        "latent planning states",
        "inference-time search"
      ],
      "organization": {
        "_id": "63ba7720fc454697637969f1",
        "name": "Beihang",
        "fullname": "Beihang University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63ba7666c138c8f2b7844b58/n98lZU9VWxYgWIkzE_6o4.jpeg"
      }
    },
    "publishedAt": "2026-01-29T02:38:18.000Z",
    "title": "Latent Chain-of-Thought as Planning: Decoupling Reasoning from Verbalization",
    "summary": "Chain-of-Thought (CoT) empowers Large Language Models (LLMs) to tackle complex problems, but remains constrained by the computational cost and reasoning path collapse when grounded in discrete token spaces. Recent latent reasoning approaches attempt to optimize efficiency by performing reasoning within continuous hidden states. However, these methods typically operate as opaque end-to-end mappings from explicit reasoning steps to latent states, and often require a pre-defined number of latent steps during inference. In this work, we introduce PLaT (Planning with Latent Thoughts), a framework that reformulates latent reasoning as planning by fundamentally decouple reasoning from verbalization. We model reasoning as a deterministic trajectory of latent planning states, while a separate Decoder grounds these thoughts into text when necessary. This decoupling allows the model to dynamically determine when to terminate reasoning rather than relying on fixed hyperparameters. Empirical results on mathematical benchmarks reveal a distinct trade-off: while PLaT achieves lower greedy accuracy than baselines, it demonstrates superior scalability in terms of reasoning diversity. This indicates that PLaT learns a robust, broader solution space, offering a transparent and scalable foundation for inference-time search.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.21358.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "652275e1d65824bda7a845db",
      "avatarUrl": "/avatars/ae465143a480e4f739c3c4c544490f25.svg",
      "fullname": "jc",
      "name": "yunsaijc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "63ba7720fc454697637969f1",
      "name": "Beihang",
      "fullname": "Beihang University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63ba7666c138c8f2b7844b58/n98lZU9VWxYgWIkzE_6o4.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.23184",
      "authors": [
        {
          "_id": "6980247b6676f933227065ee",
          "name": "Fanmeng Wang",
          "hidden": false
        },
        {
          "_id": "6980247b6676f933227065ef",
          "name": "Haotian Liu",
          "hidden": false
        },
        {
          "_id": "6980247b6676f933227065f0",
          "name": "Guojiang Zhao",
          "hidden": false
        },
        {
          "_id": "6980247b6676f933227065f1",
          "name": "Hongteng Xu",
          "hidden": false
        },
        {
          "_id": "6980247b6676f933227065f2",
          "name": "Zhifeng Gao",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-30T17:08:06.000Z",
      "submittedOnDailyAt": "2026-02-02T01:44:01.004Z",
      "title": "ReGuLaR: Variational Latent Reasoning Guided by Rendered Chain-of-Thought",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "While Chain-of-Thought (CoT) significantly enhances the performance of Large Language Models (LLMs), explicit reasoning chains introduce substantial computational redundancy. Recent latent reasoning methods attempt to mitigate this by compressing reasoning processes into latent space, but often suffer from severe performance degradation due to the lack of appropriate compression guidance. In this study, we propose Rendered CoT-Guided variational Latent Reasoning (ReGuLaR), a simple yet novel latent learning paradigm resolving this issue. Fundamentally, we formulate latent reasoning within the Variational Auto-Encoding (VAE) framework, sampling the current latent reasoning state from the posterior distribution conditioned on previous ones. Specifically, when learning this variational latent reasoning model, we render explicit reasoning chains as images, from which we extract dense visual-semantic representations to regularize the posterior distribution, thereby achieving efficient compression with minimal information loss. Extensive experiments demonstrate that ReGuLaR significantly outperforms existing latent reasoning methods across both computational efficiency and reasoning effectiveness, and even surpasses CoT through multi-modal reasoning, providing a new and insightful solution to latent reasoning. Code: https://github.com/FanmengWang/ReGuLaR.",
      "upvotes": 3,
      "discussionId": "6980247b6676f933227065f3",
      "githubRepo": "https://github.com/FanmengWang/ReGuLaR",
      "githubRepoAddedBy": "user",
      "ai_summary": "ReGuLaR introduces a variational auto-encoding framework that compresses reasoning processes into latent space while maintaining performance through image-rendered explicit reasoning chains for guidance.",
      "ai_keywords": [
        "Chain-of-Thought",
        "Large Language Models",
        "latent reasoning",
        "Variational Auto-Encoding",
        "posterior distribution",
        "visual-semantic representations",
        "multi-modal reasoning"
      ],
      "githubStars": 6
    },
    "publishedAt": "2026-01-30T12:08:06.000Z",
    "title": "ReGuLaR: Variational Latent Reasoning Guided by Rendered Chain-of-Thought",
    "summary": "While Chain-of-Thought (CoT) significantly enhances the performance of Large Language Models (LLMs), explicit reasoning chains introduce substantial computational redundancy. Recent latent reasoning methods attempt to mitigate this by compressing reasoning processes into latent space, but often suffer from severe performance degradation due to the lack of appropriate compression guidance. In this study, we propose Rendered CoT-Guided variational Latent Reasoning (ReGuLaR), a simple yet novel latent learning paradigm resolving this issue. Fundamentally, we formulate latent reasoning within the Variational Auto-Encoding (VAE) framework, sampling the current latent reasoning state from the posterior distribution conditioned on previous ones. Specifically, when learning this variational latent reasoning model, we render explicit reasoning chains as images, from which we extract dense visual-semantic representations to regularize the posterior distribution, thereby achieving efficient compression with minimal information loss. Extensive experiments demonstrate that ReGuLaR significantly outperforms existing latent reasoning methods across both computational efficiency and reasoning effectiveness, and even surpasses CoT through multi-modal reasoning, providing a new and insightful solution to latent reasoning. Code: https://github.com/FanmengWang/ReGuLaR.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.23184.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 222,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.21957",
      "authors": [
        {
          "_id": "697c2ae8a67238fac88cc135",
          "name": "Cheng Cui",
          "hidden": false
        },
        {
          "_id": "697c2ae8a67238fac88cc136",
          "name": "Ting Sun",
          "hidden": false
        },
        {
          "_id": "697c2ae8a67238fac88cc137",
          "name": "Suyin Liang",
          "hidden": false
        },
        {
          "_id": "697c2ae8a67238fac88cc138",
          "name": "Tingquan Gao",
          "hidden": false
        },
        {
          "_id": "697c2ae8a67238fac88cc139",
          "name": "Zelun Zhang",
          "hidden": false
        },
        {
          "_id": "697c2ae8a67238fac88cc13a",
          "name": "Jiaxuan Liu",
          "hidden": false
        },
        {
          "_id": "697c2ae8a67238fac88cc13b",
          "name": "Xueqing Wang",
          "hidden": false
        },
        {
          "_id": "697c2ae8a67238fac88cc13c",
          "name": "Changda Zhou",
          "hidden": false
        },
        {
          "_id": "697c2ae8a67238fac88cc13d",
          "name": "Hongen Liu",
          "hidden": false
        },
        {
          "_id": "697c2ae8a67238fac88cc13e",
          "name": "Manhui Lin",
          "hidden": false
        },
        {
          "_id": "697c2ae8a67238fac88cc13f",
          "name": "Yue Zhang",
          "hidden": false
        },
        {
          "_id": "697c2ae8a67238fac88cc140",
          "name": "Yubo Zhang",
          "hidden": false
        },
        {
          "_id": "697c2ae8a67238fac88cc141",
          "name": "Yi Liu",
          "hidden": false
        },
        {
          "_id": "697c2ae8a67238fac88cc142",
          "name": "Dianhai Yu",
          "hidden": false
        },
        {
          "_id": "697c2ae8a67238fac88cc143",
          "name": "Yanjun Ma",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-29T16:35:04.000Z",
      "submittedOnDailyAt": "2026-02-02T04:03:37.670Z",
      "title": "PaddleOCR-VL-1.5: Towards a Multi-Task 0.9B VLM for Robust In-the-Wild Document Parsing",
      "submittedOnDailyBy": {
        "_id": "65a5231a087d8a2e9cc2414b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65a5231a087d8a2e9cc2414b/wj0l5R5LmBUG-E8XTMdBM.jpeg",
        "isPro": false,
        "fullname": "cuicheng",
        "user": "ChengCui",
        "type": "user"
      },
      "summary": "We introduce PaddleOCR-VL-1.5, an upgraded model achieving a new state-of-the-art (SOTA) accuracy of 94.5% on OmniDocBench v1.5. To rigorously evaluate robustness against real-world physical distortions, including scanning, skew, warping, screen-photography, and illumination, we propose the Real5-OmniDocBench benchmark. Experimental results demonstrate that this enhanced model attains SOTA performance on the newly curated benchmark. Furthermore, we extend the model's capabilities by incorporating seal recognition and text spotting tasks, while remaining a 0.9B ultra-compact VLM with high efficiency. Code: https://github.com/PaddlePaddle/PaddleOCR",
      "upvotes": 3,
      "discussionId": "697c2ae9a67238fac88cc144",
      "ai_summary": "A compact vision-language model achieves state-of-the-art accuracy on document understanding tasks while maintaining efficiency through specialized benchmarking and extended functionality.",
      "ai_keywords": [
        "Vision-Language Model",
        "OmniDocBench",
        "Real5-OmniDocBench",
        "seal recognition",
        "text spotting"
      ]
    },
    "publishedAt": "2026-01-29T11:35:04.000Z",
    "title": "PaddleOCR-VL-1.5: Towards a Multi-Task 0.9B VLM for Robust In-the-Wild Document Parsing",
    "summary": "We introduce PaddleOCR-VL-1.5, an upgraded model achieving a new state-of-the-art (SOTA) accuracy of 94.5% on OmniDocBench v1.5. To rigorously evaluate robustness against real-world physical distortions, including scanning, skew, warping, screen-photography, and illumination, we propose the Real5-OmniDocBench benchmark. Experimental results demonstrate that this enhanced model attains SOTA performance on the newly curated benchmark. Furthermore, we extend the model's capabilities by incorporating seal recognition and text spotting tasks, while remaining a 0.9B ultra-compact VLM with high efficiency. Code: https://github.com/PaddlePaddle/PaddleOCR",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.21957.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65a5231a087d8a2e9cc2414b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65a5231a087d8a2e9cc2414b/wj0l5R5LmBUG-E8XTMdBM.jpeg",
      "fullname": "cuicheng",
      "name": "ChengCui",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 20,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.21419",
      "authors": [
        {
          "_id": "697c1dfca67238fac88cc0a4",
          "name": "Qing Jin",
          "hidden": false
        },
        {
          "_id": "697c1dfca67238fac88cc0a5",
          "name": "Chaoyang Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-29T08:56:55.000Z",
      "submittedOnDailyAt": "2026-02-02T04:06:18.035Z",
      "title": "Revisiting Diffusion Model Predictions Through Dimensionality",
      "submittedOnDailyBy": {
        "_id": "64c166de11f3f1d23a99c6fd",
        "avatarUrl": "/avatars/d4730f70a6f1cb51266862c7a8d54f77.svg",
        "isPro": false,
        "fullname": "Chaoyang Wang",
        "user": "cwang9",
        "type": "user"
      },
      "summary": "Recent advances in diffusion and flow matching models have highlighted a shift in the preferred prediction target -- moving from noise (varepsilon) and velocity (v) to direct data (x) prediction -- particularly in high-dimensional settings. However, a formal explanation of why the optimal target depends on the specific properties of the data remains elusive. In this work, we provide a theoretical framework based on a generalized prediction formulation that accommodates arbitrary output targets, of which varepsilon-, v-, and x-prediction are special cases. We derive the analytical relationship between data's geometry and the optimal prediction target, offering a rigorous justification for why x-prediction becomes superior when the ambient dimension significantly exceeds the data's intrinsic dimension. Furthermore, while our theory identifies dimensionality as the governing factor for the optimal prediction target, the intrinsic dimension of manifold-bound data is typically intractable to estimate in practice. To bridge this gap, we propose k-Diff, a framework that employs a data-driven approach to learn the optimal prediction parameter k directly from data, bypassing the need for explicit dimension estimation. Extensive experiments in both latent-space and pixel-space image generation demonstrate that k-Diff consistently outperforms fixed-target baselines across varying architectures and data scales, providing a principled and automated approach to enhancing generative performance.",
      "upvotes": 3,
      "discussionId": "697c1dfca67238fac88cc0a6",
      "ai_summary": "Diffusion models using direct data prediction outperform traditional noise or velocity prediction in high-dimensional settings, with a proposed framework automatically learning optimal prediction parameters from data.",
      "ai_keywords": [
        "diffusion models",
        "flow matching models",
        "noise prediction",
        "velocity prediction",
        "data prediction",
        "ambient dimension",
        "intrinsic dimension",
        "generalized prediction formulation",
        "k-Diff",
        "latent-space",
        "pixel-space",
        "generative performance"
      ]
    },
    "publishedAt": "2026-01-29T03:56:55.000Z",
    "title": "Revisiting Diffusion Model Predictions Through Dimensionality",
    "summary": "Recent advances in diffusion and flow matching models have highlighted a shift in the preferred prediction target -- moving from noise (varepsilon) and velocity (v) to direct data (x) prediction -- particularly in high-dimensional settings. However, a formal explanation of why the optimal target depends on the specific properties of the data remains elusive. In this work, we provide a theoretical framework based on a generalized prediction formulation that accommodates arbitrary output targets, of which varepsilon-, v-, and x-prediction are special cases. We derive the analytical relationship between data's geometry and the optimal prediction target, offering a rigorous justification for why x-prediction becomes superior when the ambient dimension significantly exceeds the data's intrinsic dimension. Furthermore, while our theory identifies dimensionality as the governing factor for the optimal prediction target, the intrinsic dimension of manifold-bound data is typically intractable to estimate in practice. To bridge this gap, we propose k-Diff, a framework that employs a data-driven approach to learn the optimal prediction parameter k directly from data, bypassing the need for explicit dimension estimation. Extensive experiments in both latent-space and pixel-space image generation demonstrate that k-Diff consistently outperforms fixed-target baselines across varying architectures and data scales, providing a principled and automated approach to enhancing generative performance.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.21419.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c166de11f3f1d23a99c6fd",
      "avatarUrl": "/avatars/d4730f70a6f1cb51266862c7a8d54f77.svg",
      "fullname": "Chaoyang Wang",
      "name": "cwang9",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.18241",
      "authors": [
        {
          "_id": "697c93736676f93322706042",
          "name": "Elena Bruches",
          "hidden": false
        },
        {
          "_id": "697c93736676f93322706043",
          "name": "Vadim Alperovich",
          "hidden": false
        },
        {
          "_id": "697c93736676f93322706044",
          "name": "Dari Baturova",
          "hidden": false
        },
        {
          "_id": "697c93736676f93322706045",
          "user": {
            "_id": "6415cb01486c7c9a5d1560f3",
            "avatarUrl": "/avatars/f73946497c731ca0f03c430562e3e7fe.svg",
            "isPro": false,
            "fullname": "Roman Derunets",
            "user": "rmndrnts",
            "type": "user"
          },
          "name": "Roman Derunets",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-30T13:31:25.974Z",
          "hidden": false
        },
        {
          "_id": "697c93736676f93322706046",
          "name": "Daniil Grebenkin",
          "hidden": false
        },
        {
          "_id": "697c93736676f93322706047",
          "name": "Georgy Mkrtchyan",
          "hidden": false
        },
        {
          "_id": "697c93736676f93322706048",
          "name": "Oleg Sedukhin",
          "hidden": false
        },
        {
          "_id": "697c93736676f93322706049",
          "name": "Mikhail Klementev",
          "hidden": false
        },
        {
          "_id": "697c93736676f9332270604a",
          "name": "Ivan Bondarenko",
          "hidden": false
        },
        {
          "_id": "697c93736676f9332270604b",
          "name": "Nikolay Bushkov",
          "hidden": false
        },
        {
          "_id": "697c93736676f9332270604c",
          "name": "Stanislav Moiseev",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-26T07:47:22.000Z",
      "submittedOnDailyAt": "2026-02-02T05:38:57.789Z",
      "title": "TAM-Eval: Evaluating LLMs for Automated Unit Test Maintenance",
      "submittedOnDailyBy": {
        "_id": "6415cb01486c7c9a5d1560f3",
        "avatarUrl": "/avatars/f73946497c731ca0f03c430562e3e7fe.svg",
        "isPro": false,
        "fullname": "Roman Derunets",
        "user": "rmndrnts",
        "type": "user"
      },
      "summary": "While Large Language Models (LLMs) have shown promise in software engineering, their application to unit testing remains largely confined to isolated test generation or oracle prediction, neglecting the broader challenge of test suite maintenance. We introduce TAM-Eval (Test Automated Maintenance Evaluation), a framework and benchmark designed to evaluate model performance across three core test maintenance scenarios: creation, repair, and updating of test suites. Unlike prior work limited to function-level tasks, TAM-Eval operates at the test file level, while maintaining access to full repository context during isolated evaluation, better reflecting real-world maintenance workflows. Our benchmark comprises 1,539 automatically extracted and validated scenarios from Python, Java, and Go projects. TAM-Eval supports system-agnostic evaluation of both raw LLMs and agentic workflows, using a reference-free protocol based on test suite pass rate, code coverage, and mutation testing. Empirical results indicate that state-of-the-art LLMs have limited capabilities in realistic test maintenance processes and yield only marginal improvements in test effectiveness. We release TAM-Eval as an open-source framework to support future research in automated software testing. Our data and code are publicly available at https://github.com/trndcenter/TAM-Eval.",
      "upvotes": 3,
      "discussionId": "697c93736676f9332270604d",
      "githubRepo": "https://github.com/trndcenter/TAM-Eval",
      "githubRepoAddedBy": "user",
      "ai_summary": "TAM-Eval is a framework and benchmark for evaluating large language models on comprehensive test suite maintenance tasks including creation, repair, and updating across multiple programming languages.",
      "ai_keywords": [
        "Large Language Models",
        "unit testing",
        "test suite maintenance",
        "test automation",
        "test generation",
        "oracle prediction",
        "test file level evaluation",
        "repository context",
        "test suite pass rate",
        "code coverage",
        "mutation testing",
        "agentic workflows",
        "reference-free protocol"
      ],
      "githubStars": 2
    },
    "publishedAt": "2026-01-26T02:47:22.000Z",
    "title": "TAM-Eval: Evaluating LLMs for Automated Unit Test Maintenance",
    "summary": "While Large Language Models (LLMs) have shown promise in software engineering, their application to unit testing remains largely confined to isolated test generation or oracle prediction, neglecting the broader challenge of test suite maintenance. We introduce TAM-Eval (Test Automated Maintenance Evaluation), a framework and benchmark designed to evaluate model performance across three core test maintenance scenarios: creation, repair, and updating of test suites. Unlike prior work limited to function-level tasks, TAM-Eval operates at the test file level, while maintaining access to full repository context during isolated evaluation, better reflecting real-world maintenance workflows. Our benchmark comprises 1,539 automatically extracted and validated scenarios from Python, Java, and Go projects. TAM-Eval supports system-agnostic evaluation of both raw LLMs and agentic workflows, using a reference-free protocol based on test suite pass rate, code coverage, and mutation testing. Empirical results indicate that state-of-the-art LLMs have limited capabilities in realistic test maintenance processes and yield only marginal improvements in test effectiveness. We release TAM-Eval as an open-source framework to support future research in automated software testing. Our data and code are publicly available at https://github.com/trndcenter/TAM-Eval.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.18241.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6415cb01486c7c9a5d1560f3",
      "avatarUrl": "/avatars/f73946497c731ca0f03c430562e3e7fe.svg",
      "fullname": "Roman Derunets",
      "name": "rmndrnts",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3,
      "isUserFollowing": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2601.13097",
      "authors": [
        {
          "_id": "6976fdd85d41524304c1367e",
          "name": "Elena Bruches",
          "hidden": false
        },
        {
          "_id": "6976fdd85d41524304c1367f",
          "user": {
            "_id": "63cb976d80ba2ca4151b67a2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675713278440-63cb976d80ba2ca4151b67a2.jpeg",
            "isPro": false,
            "fullname": "Daniel Grebenkin",
            "user": "dangrebenkin",
            "type": "user"
          },
          "name": "Daniil Grebenkin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-26T08:28:32.856Z",
          "hidden": false
        },
        {
          "_id": "6976fdd85d41524304c13680",
          "name": "Mikhail Klementev",
          "hidden": false
        },
        {
          "_id": "6976fdd85d41524304c13681",
          "name": "Vadim Alperovich",
          "hidden": false
        },
        {
          "_id": "6976fdd85d41524304c13682",
          "user": {
            "_id": "6415cb01486c7c9a5d1560f3",
            "avatarUrl": "/avatars/f73946497c731ca0f03c430562e3e7fe.svg",
            "isPro": false,
            "fullname": "Roman Derunets",
            "user": "rmndrnts",
            "type": "user"
          },
          "name": "Roman Derunets",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-26T08:28:34.729Z",
          "hidden": false
        },
        {
          "_id": "6976fdd85d41524304c13683",
          "name": "Dari Baturova",
          "hidden": false
        },
        {
          "_id": "6976fdd85d41524304c13684",
          "name": "Georgy Mkrtchyan",
          "hidden": false
        },
        {
          "_id": "6976fdd85d41524304c13685",
          "name": "Oleg Sedukhin",
          "hidden": false
        },
        {
          "_id": "6976fdd85d41524304c13686",
          "name": "Ivan Bondarenko",
          "hidden": false
        },
        {
          "_id": "6976fdd85d41524304c13687",
          "name": "Nikolay Bushkov",
          "hidden": false
        },
        {
          "_id": "6976fdd85d41524304c13688",
          "name": "Stanislav Moiseev",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-19T14:37:50.000Z",
      "submittedOnDailyAt": "2026-02-02T05:52:21.731Z",
      "title": "RM -RF: Reward Model for Run-Free Unit Test Evaluation",
      "submittedOnDailyBy": {
        "_id": "6415cb01486c7c9a5d1560f3",
        "avatarUrl": "/avatars/f73946497c731ca0f03c430562e3e7fe.svg",
        "isPro": false,
        "fullname": "Roman Derunets",
        "user": "rmndrnts",
        "type": "user"
      },
      "summary": "We present RM-RF, a lightweight reward model for run-free evaluation of automatically generated unit tests. Instead of repeatedly compiling and executing candidate tests, RM-RF predicts - from source and test code alone - three execution-derived signals: (1) whether the augmented test suite compiles and runs successfully, (2) whether the generated test cases increase code coverage, and (3) whether the generated test cases improve the mutation kill rate. To train and evaluate RM-RF we assemble a multilingual dataset (Java, Python, Go) of focal files, test files, and candidate test additions labeled by an execution-based pipeline, and we release an associated dataset and methodology for comparative evaluation. We tested multiple model families and tuning regimes (zero-shot, full fine-tuning, and PEFT via LoRA), achieving an average F1 of 0.69 across the three targets. Compared to conventional compile-and-run instruments, RM-RF provides substantially lower latency and infrastructure cost while delivering competitive predictive fidelity, enabling fast, scalable feedback for large-scale test generation and RL-based code optimization.",
      "upvotes": 3,
      "discussionId": "6976fdd85d41524304c13689",
      "ai_summary": "RM-RF is a lightweight reward model that predicts execution outcomes from source code alone, offering faster and more cost-effective evaluation than traditional compile-and-run methods.",
      "ai_keywords": [
        "reward model",
        "run-free evaluation",
        "unit tests",
        "execution-derived signals",
        "code coverage",
        "mutation kill rate",
        "multilingual dataset",
        "focal files",
        "test files",
        "candidate test additions",
        "execution-based pipeline",
        "model families",
        "tuning regimes",
        "zero-shot",
        "full fine-tuning",
        "PEFT",
        "LoRA",
        "F1 score"
      ]
    },
    "publishedAt": "2026-01-19T09:37:50.000Z",
    "title": "RM -RF: Reward Model for Run-Free Unit Test Evaluation",
    "summary": "We present RM-RF, a lightweight reward model for run-free evaluation of automatically generated unit tests. Instead of repeatedly compiling and executing candidate tests, RM-RF predicts - from source and test code alone - three execution-derived signals: (1) whether the augmented test suite compiles and runs successfully, (2) whether the generated test cases increase code coverage, and (3) whether the generated test cases improve the mutation kill rate. To train and evaluate RM-RF we assemble a multilingual dataset (Java, Python, Go) of focal files, test files, and candidate test additions labeled by an execution-based pipeline, and we release an associated dataset and methodology for comparative evaluation. We tested multiple model families and tuning regimes (zero-shot, full fine-tuning, and PEFT via LoRA), achieving an average F1 of 0.69 across the three targets. Compared to conventional compile-and-run instruments, RM-RF provides substantially lower latency and infrastructure cost while delivering competitive predictive fidelity, enabling fast, scalable feedback for large-scale test generation and RL-based code optimization.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.13097.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6415cb01486c7c9a5d1560f3",
      "avatarUrl": "/avatars/f73946497c731ca0f03c430562e3e7fe.svg",
      "fullname": "Roman Derunets",
      "name": "rmndrnts",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3,
      "isUserFollowing": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2601.23188",
      "authors": [
        {
          "_id": "69802c5c6676f93322706612",
          "name": "Zhongxiang Sun",
          "hidden": false
        },
        {
          "_id": "69802c5c6676f93322706613",
          "name": "Qipeng Wang",
          "hidden": false
        },
        {
          "_id": "69802c5c6676f93322706614",
          "name": "Weijie Yu",
          "hidden": false
        },
        {
          "_id": "69802c5c6676f93322706615",
          "name": "Jingxuan Yang",
          "hidden": false
        },
        {
          "_id": "69802c5c6676f93322706616",
          "name": "Haolang Lu",
          "hidden": false
        },
        {
          "_id": "69802c5c6676f93322706617",
          "name": "Jun Xu",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-30T17:10:48.000Z",
      "submittedOnDailyAt": "2026-02-02T03:00:37.659Z",
      "title": "Deep Search with Hierarchical Meta-Cognitive Monitoring Inspired by Cognitive Neuroscience",
      "submittedOnDailyBy": {
        "_id": "6309bfdab8d7b3889319b588",
        "avatarUrl": "/avatars/572acdad470f765ef2e058ead3741e24.svg",
        "isPro": false,
        "fullname": "SunZX",
        "user": "Jeryi",
        "type": "user"
      },
      "summary": "Deep search agents powered by large language models have demonstrated strong capabilities in multi-step retrieval, reasoning, and long-horizon task execution. However, their practical failures often stem from the lack of mechanisms to monitor and regulate reasoning and retrieval states as tasks evolve under uncertainty. Insights from cognitive neuroscience suggest that human metacognition is hierarchically organized, integrating fast anomaly detection with selectively triggered, experience-driven reflection. In this work, we propose Deep Search with Meta-Cognitive Monitoring (DS-MCM), a deep search framework augmented with an explicit hierarchical metacognitive monitoring mechanism. DS-MCM integrates a Fast Consistency Monitor, which performs lightweight checks on the alignment between external evidence and internal reasoning confidence, and a Slow Experience-Driven Monitor, which is selectively activated to guide corrective intervention based on experience memory from historical agent trajectories. By embedding monitoring directly into the reasoning-retrieval loop, DS-MCM determines both when intervention is warranted and how corrective actions should be informed by prior experience. Experiments across multiple deep search benchmarks and backbone models demonstrate that DS-MCM consistently improves performance and robustness.",
      "upvotes": 2,
      "discussionId": "69802c5c6676f93322706618",
      "ai_summary": "Deep search agents with hierarchical metacognitive monitoring enhance reasoning and retrieval performance through fast consistency checks and experience-driven corrective interventions.",
      "ai_keywords": [
        "deep search agents",
        "large language models",
        "multi-step retrieval",
        "reasoning",
        "long-horizon task execution",
        "metacognition",
        "hierarchical organization",
        "anomaly detection",
        "reflection",
        "Fast Consistency Monitor",
        "Slow Experience-Driven Monitor",
        "corrective intervention",
        "experience memory",
        "agent trajectories",
        "deep search benchmarks",
        "backbone models"
      ],
      "organization": {
        "_id": "622177ac43826d6f261f8208",
        "name": "RUC",
        "fullname": "Renmin University of China",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/670IAX9A2-BflqA5MiSBW.jpeg"
      }
    },
    "publishedAt": "2026-01-30T12:10:48.000Z",
    "title": "Deep Search with Hierarchical Meta-Cognitive Monitoring Inspired by Cognitive Neuroscience",
    "summary": "Deep search agents powered by large language models have demonstrated strong capabilities in multi-step retrieval, reasoning, and long-horizon task execution. However, their practical failures often stem from the lack of mechanisms to monitor and regulate reasoning and retrieval states as tasks evolve under uncertainty. Insights from cognitive neuroscience suggest that human metacognition is hierarchically organized, integrating fast anomaly detection with selectively triggered, experience-driven reflection. In this work, we propose Deep Search with Meta-Cognitive Monitoring (DS-MCM), a deep search framework augmented with an explicit hierarchical metacognitive monitoring mechanism. DS-MCM integrates a Fast Consistency Monitor, which performs lightweight checks on the alignment between external evidence and internal reasoning confidence, and a Slow Experience-Driven Monitor, which is selectively activated to guide corrective intervention based on experience memory from historical agent trajectories. By embedding monitoring directly into the reasoning-retrieval loop, DS-MCM determines both when intervention is warranted and how corrective actions should be informed by prior experience. Experiments across multiple deep search benchmarks and backbone models demonstrate that DS-MCM consistently improves performance and robustness.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.23188.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6309bfdab8d7b3889319b588",
      "avatarUrl": "/avatars/572acdad470f765ef2e058ead3741e24.svg",
      "fullname": "SunZX",
      "name": "Jeryi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "622177ac43826d6f261f8208",
      "name": "RUC",
      "fullname": "Renmin University of China",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/670IAX9A2-BflqA5MiSBW.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.22664",
      "authors": [
        {
          "_id": "6980162d6676f9332270656f",
          "name": "Zixuan Huang",
          "hidden": false
        },
        {
          "_id": "6980162d6676f93322706570",
          "name": "Xin Xia",
          "hidden": false
        },
        {
          "_id": "6980162d6676f93322706571",
          "name": "Yuxi Ren",
          "hidden": false
        },
        {
          "_id": "6980162d6676f93322706572",
          "name": "Jianbin Zheng",
          "hidden": false
        },
        {
          "_id": "6980162d6676f93322706573",
          "name": "Xuefeng Xiao",
          "hidden": false
        },
        {
          "_id": "6980162d6676f93322706574",
          "name": "Hongyan Xie",
          "hidden": false
        },
        {
          "_id": "6980162d6676f93322706575",
          "name": "Li Huaqiu",
          "hidden": false
        },
        {
          "_id": "6980162d6676f93322706576",
          "name": "Songshi Liang",
          "hidden": false
        },
        {
          "_id": "6980162d6676f93322706577",
          "name": "Zhongxiang Dai",
          "hidden": false
        },
        {
          "_id": "6980162d6676f93322706578",
          "name": "Fuzhen Zhuang",
          "hidden": false
        },
        {
          "_id": "6980162d6676f93322706579",
          "name": "Jianxin Li",
          "hidden": false
        },
        {
          "_id": "6980162d6676f9332270657a",
          "name": "Yikun Ban",
          "hidden": false
        },
        {
          "_id": "6980162d6676f9332270657b",
          "name": "Deqing Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-30T07:32:35.000Z",
      "submittedOnDailyAt": "2026-02-02T01:01:20.171Z",
      "title": "Real-Time Aligned Reward Model beyond Semantics",
      "submittedOnDailyBy": {
        "_id": "68345345f4bbf856e2d708e2",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68345345f4bbf856e2d708e2/L5H2HNCuWje3ti2tNbC5p.jpeg",
        "isPro": false,
        "fullname": "Yikun Ban",
        "user": "Yikunb",
        "type": "user"
      },
      "summary": "Reinforcement Learning from Human Feedback (RLHF) is a pivotal technique for aligning large language models (LLMs) with human preferences, yet it is susceptible to reward overoptimization, in which policy models overfit to the reward model, exploit spurious reward patterns instead of faithfully capturing human intent. Prior mitigations primarily relies on surface semantic information and fails to efficiently address the misalignment between the reward model (RM) and the policy model caused by continuous policy distribution shifts. This inevitably leads to an increasing reward discrepancy, exacerbating reward overoptimization. To address these limitations, we introduce R2M (Real-Time Aligned Reward Model), a novel lightweight RLHF framework. R2M goes beyond vanilla reward models that solely depend on the semantic representations of a pretrained LLM. Instead, it leverages the evolving hidden states of the policy (namely policy feedback) to align with the real-time distribution shift of the policy during the RL process. This work points to a promising new direction for improving the performance of reward models through real-time utilization of feedback from policy models.",
      "upvotes": 2,
      "discussionId": "6980162e6676f9332270657c",
      "ai_summary": "RLHF suffers from reward overoptimization due to misalignment between reward models and policy models, which R2M addresses by incorporating real-time policy feedback to dynamically adapt reward modeling during training.",
      "ai_keywords": [
        "Reinforcement Learning from Human Feedback",
        "reward overoptimization",
        "policy models",
        "reward models",
        "policy distribution shifts",
        "real-time alignment",
        "policy feedback"
      ]
    },
    "publishedAt": "2026-01-30T02:32:35.000Z",
    "title": "Real-Time Aligned Reward Model beyond Semantics",
    "summary": "Reinforcement Learning from Human Feedback (RLHF) is a pivotal technique for aligning large language models (LLMs) with human preferences, yet it is susceptible to reward overoptimization, in which policy models overfit to the reward model, exploit spurious reward patterns instead of faithfully capturing human intent. Prior mitigations primarily relies on surface semantic information and fails to efficiently address the misalignment between the reward model (RM) and the policy model caused by continuous policy distribution shifts. This inevitably leads to an increasing reward discrepancy, exacerbating reward overoptimization. To address these limitations, we introduce R2M (Real-Time Aligned Reward Model), a novel lightweight RLHF framework. R2M goes beyond vanilla reward models that solely depend on the semantic representations of a pretrained LLM. Instead, it leverages the evolving hidden states of the policy (namely policy feedback) to align with the real-time distribution shift of the policy during the RL process. This work points to a promising new direction for improving the performance of reward models through real-time utilization of feedback from policy models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.22664.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "68345345f4bbf856e2d708e2",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68345345f4bbf856e2d708e2/L5H2HNCuWje3ti2tNbC5p.jpeg",
      "fullname": "Yikun Ban",
      "name": "Yikunb",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.22141",
      "authors": [
        {
          "_id": "697c8d096676f93322706032",
          "user": {
            "_id": "632dc132fdb35759ea673fbf",
            "avatarUrl": "/avatars/4ba2879965150820170d96b610cfccd2.svg",
            "isPro": false,
            "fullname": "Grzegorz Stefaski",
            "user": "GrzegorzStefanski",
            "type": "user"
          },
          "name": "Grzegorz Stefanski",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-30T13:31:28.141Z",
          "hidden": false
        },
        {
          "_id": "697c8d096676f93322706033",
          "name": "Alberto Presta",
          "hidden": false
        },
        {
          "_id": "697c8d096676f93322706034",
          "name": "Michal Byra",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-29T18:56:41.000Z",
      "submittedOnDailyAt": "2026-02-02T01:00:24.563Z",
      "title": "Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data",
      "submittedOnDailyBy": {
        "_id": "632dc132fdb35759ea673fbf",
        "avatarUrl": "/avatars/4ba2879965150820170d96b610cfccd2.svg",
        "isPro": false,
        "fullname": "Grzegorz Stefaski",
        "user": "GrzegorzStefanski",
        "type": "user"
      },
      "summary": "In pruning, the Lottery Ticket Hypothesis posits that large networks contain sparse subnetworks, or winning tickets, that can be trained in isolation to match the performance of their dense counterparts. However, most existing approaches assume a single universal winning ticket shared across all inputs, ignoring the inherent heterogeneity of real-world data. In this work, we propose Routing the Lottery (RTL), an adaptive pruning framework that discovers multiple specialized subnetworks, called adaptive tickets, each tailored to a class, semantic cluster, or environmental condition. Across diverse datasets and tasks, RTL consistently outperforms single- and multi-model baselines in balanced accuracy and recall, while using up to 10 times fewer parameters than independent models and exhibiting semantically aligned. Furthermore, we identify subnetwork collapse, a performance drop under aggressive pruning, and introduce a subnetwork similarity score that enables label-free diagnosis of oversparsification. Overall, our results recast pruning as a mechanism for aligning model structure with data heterogeneity, paving the way toward more modular and context-aware deep learning.",
      "upvotes": 2,
      "discussionId": "697c8d096676f93322706035",
      "ai_summary": "Routing the Lottery framework discovers multiple specialized subnetworks tailored to different data conditions, outperforming traditional pruning methods while using fewer parameters and identifying subnetwork collapse issues.",
      "ai_keywords": [
        "Lottery Ticket Hypothesis",
        "winning tickets",
        "adaptive pruning",
        "adaptive tickets",
        "subnetwork collapse",
        "subnetwork similarity score"
      ]
    },
    "publishedAt": "2026-01-29T13:56:41.000Z",
    "title": "Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data",
    "summary": "In pruning, the Lottery Ticket Hypothesis posits that large networks contain sparse subnetworks, or winning tickets, that can be trained in isolation to match the performance of their dense counterparts. However, most existing approaches assume a single universal winning ticket shared across all inputs, ignoring the inherent heterogeneity of real-world data. In this work, we propose Routing the Lottery (RTL), an adaptive pruning framework that discovers multiple specialized subnetworks, called adaptive tickets, each tailored to a class, semantic cluster, or environmental condition. Across diverse datasets and tasks, RTL consistently outperforms single- and multi-model baselines in balanced accuracy and recall, while using up to 10 times fewer parameters than independent models and exhibiting semantically aligned. Furthermore, we identify subnetwork collapse, a performance drop under aggressive pruning, and introduce a subnetwork similarity score that enables label-free diagnosis of oversparsification. Overall, our results recast pruning as a mechanism for aligning model structure with data heterogeneity, paving the way toward more modular and context-aware deep learning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.22141.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "632dc132fdb35759ea673fbf",
      "avatarUrl": "/avatars/4ba2879965150820170d96b610cfccd2.svg",
      "fullname": "Grzegorz Stefaski",
      "name": "GrzegorzStefanski",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2601.21525",
      "authors": [
        {
          "_id": "697c3614a67238fac88cc19a",
          "user": {
            "_id": "649e9dbdb24e556c33b9e508",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649e9dbdb24e556c33b9e508/B4M6OjrzL7Ac-vZFjydm_.jpeg",
            "isPro": false,
            "fullname": "Meet Doshi",
            "user": "meetdoshi90",
            "type": "user"
          },
          "name": "Meet Doshi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-30T09:35:41.547Z",
          "hidden": false
        },
        {
          "_id": "697c3614a67238fac88cc19b",
          "name": "Aashka Trivedi",
          "hidden": false
        },
        {
          "_id": "697c3614a67238fac88cc19c",
          "name": "Vishwajeet Kumar",
          "hidden": false
        },
        {
          "_id": "697c3614a67238fac88cc19d",
          "name": "Parul Awasthy",
          "hidden": false
        },
        {
          "_id": "697c3614a67238fac88cc19e",
          "name": "Yulong Li",
          "hidden": false
        },
        {
          "_id": "697c3614a67238fac88cc19f",
          "name": "Jaydeep Sen",
          "hidden": false
        },
        {
          "_id": "697c3614a67238fac88cc1a0",
          "name": "Radu Florian",
          "hidden": false
        },
        {
          "_id": "697c3614a67238fac88cc1a1",
          "name": "Sachindra Joshi",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-29T10:40:37.000Z",
      "submittedOnDailyAt": "2026-02-02T02:14:58.662Z",
      "title": "LMK > CLS: Landmark Pooling for Dense Embeddings",
      "submittedOnDailyBy": {
        "_id": "649e9dbdb24e556c33b9e508",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649e9dbdb24e556c33b9e508/B4M6OjrzL7Ac-vZFjydm_.jpeg",
        "isPro": false,
        "fullname": "Meet Doshi",
        "user": "meetdoshi90",
        "type": "user"
      },
      "summary": "Representation learning is central to many downstream tasks such as search, clustering, classification, and reranking. State-of-the-art sequence encoders typically collapse a variable-length token sequence to a single vector using a pooling operator, most commonly a special [CLS] token or mean pooling over token embeddings. In this paper, we identify systematic weaknesses of these pooling strategies: [CLS] tends to concentrate information toward the initial positions of the sequence and can under-represent distributed evidence, while mean pooling can dilute salient local signals, sometimes leading to worse short-context performance. To address these issues, we introduce Landmark (LMK) pooling, which partitions a sequence into chunks, inserts landmark tokens between chunks, and forms the final representation by mean-pooling the landmark token embeddings. This simple mechanism improves long-context extrapolation without sacrificing local salient features, at the cost of introducing a small number of special tokens. We empirically demonstrate that LMK pooling matches existing methods on short-context retrieval tasks and yields substantial improvements on long-context tasks, making it a practical and scalable alternative to existing pooling methods.",
      "upvotes": 2,
      "discussionId": "697c3614a67238fac88cc1a2",
      "ai_summary": "Landmark pooling improves long-context representation learning by partitioning sequences into chunks and using landmark tokens to preserve both global and local information more effectively than traditional pooling methods.",
      "ai_keywords": [
        "representation learning",
        "sequence encoders",
        "pooling operator",
        "[CLS] token",
        "mean pooling",
        "landmark pooling",
        "token sequences",
        "long-context extrapolation",
        "short-context performance"
      ]
    },
    "publishedAt": "2026-01-29T05:40:37.000Z",
    "title": "LMK > CLS: Landmark Pooling for Dense Embeddings",
    "summary": "Representation learning is central to many downstream tasks such as search, clustering, classification, and reranking. State-of-the-art sequence encoders typically collapse a variable-length token sequence to a single vector using a pooling operator, most commonly a special [CLS] token or mean pooling over token embeddings. In this paper, we identify systematic weaknesses of these pooling strategies: [CLS] tends to concentrate information toward the initial positions of the sequence and can under-represent distributed evidence, while mean pooling can dilute salient local signals, sometimes leading to worse short-context performance. To address these issues, we introduce Landmark (LMK) pooling, which partitions a sequence into chunks, inserts landmark tokens between chunks, and forms the final representation by mean-pooling the landmark token embeddings. This simple mechanism improves long-context extrapolation without sacrificing local salient features, at the cost of introducing a small number of special tokens. We empirically demonstrate that LMK pooling matches existing methods on short-context retrieval tasks and yields substantial improvements on long-context tasks, making it a practical and scalable alternative to existing pooling methods.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.21525.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "649e9dbdb24e556c33b9e508",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649e9dbdb24e556c33b9e508/B4M6OjrzL7Ac-vZFjydm_.jpeg",
      "fullname": "Meet Doshi",
      "name": "meetdoshi90",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.15625",
      "authors": [
        {
          "_id": "698042726676f93322706688",
          "name": "Zhiwei Zhang",
          "hidden": false
        },
        {
          "_id": "698042726676f93322706689",
          "name": "Fei Zhao",
          "hidden": false
        },
        {
          "_id": "698042726676f9332270668a",
          "name": "Rui Wang",
          "hidden": false
        },
        {
          "_id": "698042726676f9332270668b",
          "name": "Zezhong Wang",
          "hidden": false
        },
        {
          "_id": "698042726676f9332270668c",
          "name": "Bin Liang",
          "hidden": false
        },
        {
          "_id": "698042726676f9332270668d",
          "name": "Jiakang Wang",
          "hidden": false
        },
        {
          "_id": "698042726676f9332270668e",
          "name": "Yao Hu",
          "hidden": false
        },
        {
          "_id": "698042726676f9332270668f",
          "name": "Shaosheng Cao",
          "hidden": false
        },
        {
          "_id": "698042726676f93322706690",
          "name": "Kam-Fai Wong",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-22T03:57:35.000Z",
      "submittedOnDailyAt": "2026-02-02T03:52:21.562Z",
      "title": "Robust Tool Use via Fission-GRPO: Learning to Recover from Execution Errors",
      "submittedOnDailyBy": {
        "_id": "65328aa39326d6da5ff19b52",
        "avatarUrl": "/avatars/5c3de984cd6eba69616bb608796865c5.svg",
        "isPro": false,
        "fullname": "Fei Zhao",
        "user": "Hiiamein",
        "type": "user"
      },
      "summary": "Large language models (LLMs) can call tools effectively, yet they remain brittle in multi-turn execution: following a tool call error, smaller models often degenerate into repetitive invalid re-invocations, failing to interpret error feedback and self-correct. This brittleness hinders reliable real-world deployment, where the execution errors are inherently inevitable during tool interaction procedures. We identify a key limitation of current approaches: standard reinforcement learning (RL) treats errors as sparse negative rewards, providing no guidance on how to recover, while pre-collected synthetic error-correction datasets suffer from distribution mismatch with the model's on-policy error modes. To bridge this gap, we propose Fission-GRPO, a framework that converts execution errors into corrective supervision within the RL training loop. Our core mechanism fissions each failed trajectory into a new training instance by augmenting it with diagnostic feedback from a finetuned Error Simulator, then resampling recovery rollouts on-policy. This enables the model to learn from the precise errors it makes during exploration, rather than from static, pre-collected error cases. On the BFCL v4 Multi-Turn, Fission-GRPO improves the error recovery rate of Qwen3-8B by 5.7% absolute, crucially, yielding a 4% overall accuracy gain (42.75% to 46.75%) over GRPO and outperforming specialized tool-use agents.",
      "upvotes": 2,
      "discussionId": "698042736676f93322706691",
      "ai_summary": "A framework called Fission-GRPO is introduced to improve multi-turn tool execution in large language models by converting execution errors into corrective supervision during reinforcement learning training.",
      "ai_keywords": [
        "large language models",
        "tool calling",
        "reinforcement learning",
        "error recovery",
        "Fission-GRPO",
        "GRPO",
        "Error Simulator",
        "on-policy sampling",
        "multi-turn execution",
        "trajectory fission"
      ]
    },
    "publishedAt": "2026-01-21T22:57:35.000Z",
    "title": "Robust Tool Use via Fission-GRPO: Learning to Recover from Execution Errors",
    "summary": "Large language models (LLMs) can call tools effectively, yet they remain brittle in multi-turn execution: following a tool call error, smaller models often degenerate into repetitive invalid re-invocations, failing to interpret error feedback and self-correct. This brittleness hinders reliable real-world deployment, where the execution errors are inherently inevitable during tool interaction procedures. We identify a key limitation of current approaches: standard reinforcement learning (RL) treats errors as sparse negative rewards, providing no guidance on how to recover, while pre-collected synthetic error-correction datasets suffer from distribution mismatch with the model's on-policy error modes. To bridge this gap, we propose Fission-GRPO, a framework that converts execution errors into corrective supervision within the RL training loop. Our core mechanism fissions each failed trajectory into a new training instance by augmenting it with diagnostic feedback from a finetuned Error Simulator, then resampling recovery rollouts on-policy. This enables the model to learn from the precise errors it makes during exploration, rather than from static, pre-collected error cases. On the BFCL v4 Multi-Turn, Fission-GRPO improves the error recovery rate of Qwen3-8B by 5.7% absolute, crucially, yielding a 4% overall accuracy gain (42.75% to 46.75%) over GRPO and outperforming specialized tool-use agents.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.15625.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65328aa39326d6da5ff19b52",
      "avatarUrl": "/avatars/5c3de984cd6eba69616bb608796865c5.svg",
      "fullname": "Fei Zhao",
      "name": "Hiiamein",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.23161",
      "authors": [
        {
          "_id": "698025fb6676f933227065fb",
          "name": "Jiaming Zhou",
          "hidden": false
        },
        {
          "_id": "698025fb6676f933227065fc",
          "name": "Xuxin Cheng",
          "hidden": false
        },
        {
          "_id": "698025fb6676f933227065fd",
          "name": "Shiwan Zhao",
          "hidden": false
        },
        {
          "_id": "698025fb6676f933227065fe",
          "name": "Yuhang Jia",
          "hidden": false
        },
        {
          "_id": "698025fb6676f933227065ff",
          "name": "Cao Liu",
          "hidden": false
        },
        {
          "_id": "698025fb6676f93322706600",
          "name": "Ke Zeng",
          "hidden": false
        },
        {
          "_id": "698025fb6676f93322706601",
          "name": "Xunliang Cai",
          "hidden": false
        },
        {
          "_id": "698025fb6676f93322706602",
          "name": "Yong Qin",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-30T16:44:23.000Z",
      "submittedOnDailyAt": "2026-02-02T01:50:21.212Z",
      "title": "DIFFA-2: A Practical Diffusion Large Language Model for General Audio Understanding",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Autoregressive (AR) large audio language models (LALMs) such as Qwen-2.5-Omni have achieved strong performance on audio understanding and interaction, but scaling them remains costly in data and computation, and strictly sequential decoding limits inference efficiency. Diffusion large language models (dLLMs) have recently been shown to make effective use of limited training data, and prior work on DIFFA indicates that replacing an AR backbone with a diffusion counterpart can substantially improve audio understanding under matched settings, albeit at a proof-of-concept scale without large-scale instruction tuning, preference alignment, or practical decoding schemes. We introduce DIFFA-2, a practical diffusion-based LALM for general audio understanding. DIFFA-2 upgrades the speech encoder, employs dual semantic and acoustic adapters, and is trained with a four-stage curriculum that combines semantic and acoustic alignment, large-scale supervised fine-tuning, and variance-reduced preference optimization, using only fully open-source corpora. Experiments on MMSU, MMAU, and MMAR show that DIFFA-2 consistently improves over DIFFA and is competitive to strong AR LALMs under practical training budgets, supporting diffusion-based modeling is a viable backbone for large-scale audio understanding. Our code is available at https://github.com/NKU-HLT/DIFFA.git.",
      "upvotes": 1,
      "discussionId": "698025fc6676f93322706603",
      "ai_summary": "DIFFA-2, a diffusion-based large audio language model, achieves competitive audio understanding performance with improved efficiency over autoregressive counterparts through enhanced encoding, dual adapters, and staged training.",
      "ai_keywords": [
        "autoregressive models",
        "large audio language models",
        "diffusion models",
        "speech encoder",
        "dual semantic and acoustic adapters",
        "four-stage curriculum",
        "semantic alignment",
        "acoustic alignment",
        "supervised fine-tuning",
        "preference optimization",
        "variance-reduced optimization"
      ],
      "organization": {
        "_id": "68b28d79a176a9beb30d2049",
        "name": "meituan-longcat",
        "fullname": "LongCat",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"
      }
    },
    "publishedAt": "2026-01-30T11:44:23.000Z",
    "title": "DIFFA-2: A Practical Diffusion Large Language Model for General Audio Understanding",
    "summary": "Autoregressive (AR) large audio language models (LALMs) such as Qwen-2.5-Omni have achieved strong performance on audio understanding and interaction, but scaling them remains costly in data and computation, and strictly sequential decoding limits inference efficiency. Diffusion large language models (dLLMs) have recently been shown to make effective use of limited training data, and prior work on DIFFA indicates that replacing an AR backbone with a diffusion counterpart can substantially improve audio understanding under matched settings, albeit at a proof-of-concept scale without large-scale instruction tuning, preference alignment, or practical decoding schemes. We introduce DIFFA-2, a practical diffusion-based LALM for general audio understanding. DIFFA-2 upgrades the speech encoder, employs dual semantic and acoustic adapters, and is trained with a four-stage curriculum that combines semantic and acoustic alignment, large-scale supervised fine-tuning, and variance-reduced preference optimization, using only fully open-source corpora. Experiments on MMSU, MMAU, and MMAR show that DIFFA-2 consistently improves over DIFFA and is competitive to strong AR LALMs under practical training budgets, supporting diffusion-based modeling is a viable backbone for large-scale audio understanding. Our code is available at https://github.com/NKU-HLT/DIFFA.git.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.23161.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 222,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "68b28d79a176a9beb30d2049",
      "name": "meituan-longcat",
      "fullname": "LongCat",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.23134",
      "authors": [
        {
          "_id": "69804c7d6676f933227066c6",
          "name": "Zheyuan Hu",
          "hidden": false
        },
        {
          "_id": "69804c7d6676f933227066c7",
          "name": "Yifei Shi",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-30T16:23:06.000Z",
      "submittedOnDailyAt": "2026-02-02T04:35:07.481Z",
      "title": "Machine Learning for Energy-Performance-aware Scheduling",
      "submittedOnDailyBy": {
        "_id": "653053c6657ae56cdb5c490b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653053c6657ae56cdb5c490b/Zl0bZZ7-tD9AR6muE8esU.png",
        "isPro": false,
        "fullname": "Peter Hu",
        "user": "Peter2023HuggingFace",
        "type": "user"
      },
      "summary": "In the post-Dennard era, optimizing embedded systems requires navigating complex trade-offs between energy efficiency and latency. Traditional heuristic tuning is often inefficient in such high-dimensional, non-smooth landscapes. In this work, we propose a Bayesian Optimization framework using Gaussian Processes to automate the search for optimal scheduling configurations on heterogeneous multi-core architectures. We explicitly address the multi-objective nature of the problem by approximating the Pareto Frontier between energy and time. Furthermore, by incorporating Sensitivity Analysis (fANOVA) and comparing different covariance kernels (e.g., Matrn vs. RBF), we provide physical interpretability to the black-box model, revealing the dominant hardware parameters driving system performance.",
      "upvotes": 1,
      "discussionId": "69804c7e6676f933227066c8",
      "ai_summary": "A Bayesian Optimization approach using Gaussian Processes automates scheduling configuration optimization on heterogeneous multi-core systems while approximating the Pareto Frontier for energy-time trade-offs.",
      "ai_keywords": [
        "Bayesian Optimization",
        "Gaussian Processes",
        "multi-objective optimization",
        "Pareto Frontier",
        "sensitivity analysis",
        "fANOVA",
        "covariance kernels",
        "Matrn",
        "RBF"
      ]
    },
    "publishedAt": "2026-01-30T11:23:06.000Z",
    "title": "Machine Learning for Energy-Performance-aware Scheduling",
    "summary": "In the post-Dennard era, optimizing embedded systems requires navigating complex trade-offs between energy efficiency and latency. Traditional heuristic tuning is often inefficient in such high-dimensional, non-smooth landscapes. In this work, we propose a Bayesian Optimization framework using Gaussian Processes to automate the search for optimal scheduling configurations on heterogeneous multi-core architectures. We explicitly address the multi-objective nature of the problem by approximating the Pareto Frontier between energy and time. Furthermore, by incorporating Sensitivity Analysis (fANOVA) and comparing different covariance kernels (e.g., Matrn vs. RBF), we provide physical interpretability to the black-box model, revealing the dominant hardware parameters driving system performance.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.23134.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "653053c6657ae56cdb5c490b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653053c6657ae56cdb5c490b/Zl0bZZ7-tD9AR6muE8esU.png",
      "fullname": "Peter Hu",
      "name": "Peter2023HuggingFace",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.22904",
      "authors": [
        {
          "_id": "6980232e6676f933227065c4",
          "name": "Hun Chang",
          "hidden": false
        },
        {
          "_id": "6980232e6676f933227065c5",
          "name": "Byunghee Cha",
          "hidden": false
        },
        {
          "_id": "6980232e6676f933227065c6",
          "name": "Jong Chul Ye",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-30T12:25:34.000Z",
      "submittedOnDailyAt": "2026-02-02T01:38:19.533Z",
      "title": "DINO-SAE: DINO Spherical Autoencoder for High-Fidelity Image Reconstruction and Generation",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Recent studies have explored using pretrained Vision Foundation Models (VFMs) such as DINO for generative autoencoders, showing strong generative performance. Unfortunately, existing approaches often suffer from limited reconstruction fidelity due to the loss of high-frequency details. In this work, we present the DINO Spherical Autoencoder (DINO-SAE), a framework that bridges semantic representation and pixel-level reconstruction. Our key insight is that semantic information in contrastive representations is primarily encoded in the direction of feature vectors, while forcing strict magnitude matching can hinder the encoder from preserving fine-grained details. To address this, we introduce Hierarchical Convolutional Patch Embedding module that enhances local structure and texture preservation, and Cosine Similarity Alignment objective that enforces semantic consistency while allowing flexible feature magnitudes for detail retention. Furthermore, leveraging the observation that SSL-based foundation model representations intrinsically lie on a hypersphere, we employ Riemannian Flow Matching to train a Diffusion Transformer (DiT) directly on this spherical latent manifold. Experiments on ImageNet-1K demonstrate that our approach achieves state-of-the-art reconstruction quality, reaching 0.37 rFID and 26.2 dB PSNR, while maintaining strong semantic alignment to the pretrained VFM. Notably, our Riemannian Flow Matching-based DiT exhibits efficient convergence, achieving a gFID of 3.47 at 80 epochs.",
      "upvotes": 1,
      "discussionId": "6980232e6676f933227065c7",
      "ai_summary": "A novel vision autoencoder framework combines semantic representation with pixel-level reconstruction using spherical latent space and Riemannian flow matching for improved fidelity and efficiency.",
      "ai_keywords": [
        "Vision Foundation Models",
        "DINO",
        "generative autoencoders",
        "contrastive representations",
        "feature vectors",
        "hierarchical convolutional patch embedding",
        "cosine similarity alignment",
        "diffusion transformer",
        "Riemannian flow matching",
        "hypersphere",
        "latent manifold",
        "reconstruction quality",
        "semantic alignment",
        "gFID",
        "rFID",
        "PSNR"
      ]
    },
    "publishedAt": "2026-01-30T07:25:34.000Z",
    "title": "DINO-SAE: DINO Spherical Autoencoder for High-Fidelity Image Reconstruction and Generation",
    "summary": "Recent studies have explored using pretrained Vision Foundation Models (VFMs) such as DINO for generative autoencoders, showing strong generative performance. Unfortunately, existing approaches often suffer from limited reconstruction fidelity due to the loss of high-frequency details. In this work, we present the DINO Spherical Autoencoder (DINO-SAE), a framework that bridges semantic representation and pixel-level reconstruction. Our key insight is that semantic information in contrastive representations is primarily encoded in the direction of feature vectors, while forcing strict magnitude matching can hinder the encoder from preserving fine-grained details. To address this, we introduce Hierarchical Convolutional Patch Embedding module that enhances local structure and texture preservation, and Cosine Similarity Alignment objective that enforces semantic consistency while allowing flexible feature magnitudes for detail retention. Furthermore, leveraging the observation that SSL-based foundation model representations intrinsically lie on a hypersphere, we employ Riemannian Flow Matching to train a Diffusion Transformer (DiT) directly on this spherical latent manifold. Experiments on ImageNet-1K demonstrate that our approach achieves state-of-the-art reconstruction quality, reaching 0.37 rFID and 26.2 dB PSNR, while maintaining strong semantic alignment to the pretrained VFM. Notably, our Riemannian Flow Matching-based DiT exhibits efficient convergence, achieving a gFID of 3.47 at 80 epochs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.22904.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 222,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.22837",
      "authors": [
        {
          "_id": "698023a66676f933227065d2",
          "name": "Bin Wu",
          "hidden": false
        },
        {
          "_id": "698023a66676f933227065d3",
          "name": "Mengqi Huang",
          "hidden": false
        },
        {
          "_id": "698023a66676f933227065d4",
          "name": "Weinan Jia",
          "hidden": false
        },
        {
          "_id": "698023a66676f933227065d5",
          "name": "Zhendong Mao",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-30T11:01:43.000Z",
      "submittedOnDailyAt": "2026-02-02T01:40:24.437Z",
      "title": "NativeTok: Native Visual Tokenization for Improved Image Generation",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "VQ-based image generation typically follows a two-stage pipeline: a tokenizer encodes images into discrete tokens, and a generative model learns their dependencies for reconstruction. However, improved tokenization in the first stage does not necessarily enhance the second-stage generation, as existing methods fail to constrain token dependencies. This mismatch forces the generative model to learn from unordered distributions, leading to bias and weak coherence. To address this, we propose native visual tokenization, which enforces causal dependencies during tokenization. Building on this idea, we introduce NativeTok, a framework that achieves efficient reconstruction while embedding relational constraints within token sequences. NativeTok consists of: (1) a Meta Image Transformer (MIT) for latent image modeling, and (2) a Mixture of Causal Expert Transformer (MoCET), where each lightweight expert block generates a single token conditioned on prior tokens and latent features. We further design a Hierarchical Native Training strategy that updates only new expert blocks, ensuring training efficiency. Extensive experiments demonstrate the effectiveness of NativeTok.",
      "upvotes": 1,
      "discussionId": "698023a66676f933227065d6",
      "githubRepo": "https://github.com/wangbei1/Nativetok",
      "githubRepoAddedBy": "user",
      "ai_summary": "NativeTok introduces a novel visual tokenization approach that enforces causal dependencies during image encoding, using a Meta Image Transformer and Mixture of Causal Expert Transformer for efficient and coherent image generation.",
      "ai_keywords": [
        "tokenization",
        "generative model",
        "causal dependencies",
        "Meta Image Transformer",
        "Mixture of Causal Expert Transformer",
        "hierarchical training",
        "latent image modeling"
      ],
      "githubStars": 1
    },
    "publishedAt": "2026-01-30T06:01:43.000Z",
    "title": "NativeTok: Native Visual Tokenization for Improved Image Generation",
    "summary": "VQ-based image generation typically follows a two-stage pipeline: a tokenizer encodes images into discrete tokens, and a generative model learns their dependencies for reconstruction. However, improved tokenization in the first stage does not necessarily enhance the second-stage generation, as existing methods fail to constrain token dependencies. This mismatch forces the generative model to learn from unordered distributions, leading to bias and weak coherence. To address this, we propose native visual tokenization, which enforces causal dependencies during tokenization. Building on this idea, we introduce NativeTok, a framework that achieves efficient reconstruction while embedding relational constraints within token sequences. NativeTok consists of: (1) a Meta Image Transformer (MIT) for latent image modeling, and (2) a Mixture of Causal Expert Transformer (MoCET), where each lightweight expert block generates a single token conditioned on prior tokens and latent features. We further design a Hierarchical Native Training strategy that updates only new expert blocks, ensuring training efficiency. Extensive experiments demonstrate the effectiveness of NativeTok.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.22837.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 222,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.20732",
      "authors": [
        {
          "_id": "69804c686676f933227066bd",
          "name": "Ziwei Liu",
          "hidden": false
        },
        {
          "_id": "69804c686676f933227066be",
          "name": "Borui Kang",
          "hidden": false
        },
        {
          "_id": "69804c686676f933227066bf",
          "name": "Hangjie Yuan",
          "hidden": false
        },
        {
          "_id": "69804c686676f933227066c0",
          "name": "Zixiang Zhao",
          "hidden": false
        },
        {
          "_id": "69804c686676f933227066c1",
          "name": "Wei Li",
          "hidden": false
        },
        {
          "_id": "69804c686676f933227066c2",
          "name": "Yifan Zhu",
          "hidden": false
        },
        {
          "_id": "69804c686676f933227066c3",
          "name": "Tao Feng",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-28T16:06:31.000Z",
      "submittedOnDailyAt": "2026-02-02T04:56:18.518Z",
      "title": "Continual GUI Agents",
      "submittedOnDailyBy": {
        "_id": "64b8e82aa62c52b252c827fa",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b8e82aa62c52b252c827fa/Jyk5PHMXCaRlmWy4mT3Bt.jpeg",
        "isPro": true,
        "fullname": "Rajkumar rawal",
        "user": "rajkumarrawal",
        "type": "user"
      },
      "summary": "As digital environments (data distribution) are in flux, with new GUI data arriving over time-introducing new domains or resolutions-agents trained on static environments deteriorate in performance. In this work, we introduce Continual GUI Agents, a new task that requires GUI agents to perform continual learning under shifted domains and resolutions. We find existing methods fail to maintain stable grounding as GUI distributions shift over time, due to the diversity of UI interaction points and regions in fluxing scenarios. To address this, we introduce GUI-Anchoring in Flux (GUI-AiF), a new reinforcement fine-tuning framework that stabilizes continual learning through two novel rewards: Anchoring Point Reward in Flux (APR-iF) and Anchoring Region Reward in Flux (ARR-iF). These rewards guide the agents to align with shifting interaction points and regions, mitigating the tendency of existing reward strategies to over-adapt to static grounding cues (e.g., fixed coordinates or element scales). Extensive experiments show GUI-AiF surpasses state-of-the-art baselines. Our work establishes the first continual learning framework for GUI agents, revealing the untapped potential of reinforcement fine-tuning for continual GUI Agents.",
      "upvotes": 1,
      "discussionId": "69804c696676f933227066c4",
      "ai_summary": "Continual GUI Agents framework addresses performance degradation in dynamic digital environments through reinforcement fine-tuning with novel anchoring rewards that stabilize learning across shifting UI domains and resolutions.",
      "ai_keywords": [
        "continual learning",
        "GUI agents",
        "reinforcement fine-tuning",
        "GUI-Anchoring in Flux",
        "APR-iF",
        "ARR-iF",
        "continual GUI Agents"
      ],
      "organization": {
        "_id": "628735cbc83a2d6ab8d14a66",
        "name": "Tsinghua",
        "fullname": "Tsinghua University"
      }
    },
    "publishedAt": "2026-01-28T11:06:31.000Z",
    "title": "Continual GUI Agents",
    "summary": "As digital environments (data distribution) are in flux, with new GUI data arriving over time-introducing new domains or resolutions-agents trained on static environments deteriorate in performance. In this work, we introduce Continual GUI Agents, a new task that requires GUI agents to perform continual learning under shifted domains and resolutions. We find existing methods fail to maintain stable grounding as GUI distributions shift over time, due to the diversity of UI interaction points and regions in fluxing scenarios. To address this, we introduce GUI-Anchoring in Flux (GUI-AiF), a new reinforcement fine-tuning framework that stabilizes continual learning through two novel rewards: Anchoring Point Reward in Flux (APR-iF) and Anchoring Region Reward in Flux (ARR-iF). These rewards guide the agents to align with shifting interaction points and regions, mitigating the tendency of existing reward strategies to over-adapt to static grounding cues (e.g., fixed coordinates or element scales). Extensive experiments show GUI-AiF surpasses state-of-the-art baselines. Our work establishes the first continual learning framework for GUI agents, revealing the untapped potential of reinforcement fine-tuning for continual GUI Agents.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.20732.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b8e82aa62c52b252c827fa",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b8e82aa62c52b252c827fa/Jyk5PHMXCaRlmWy4mT3Bt.jpeg",
      "fullname": "Rajkumar rawal",
      "name": "rajkumarrawal",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 50,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "628735cbc83a2d6ab8d14a66",
      "name": "Tsinghua",
      "fullname": "Tsinghua University"
    },
    "isAuthorParticipating": false
  }
]