[
  {
    "paper": {
      "id": "2509.12201",
      "authors": [
        {
          "_id": "68c8cece733e345e52ac1e82",
          "name": "Yang Zhou",
          "hidden": false
        },
        {
          "_id": "68c8cece733e345e52ac1e83",
          "name": "Yifan Wang",
          "hidden": false
        },
        {
          "_id": "68c8cece733e345e52ac1e84",
          "name": "Jianjun Zhou",
          "hidden": false
        },
        {
          "_id": "68c8cece733e345e52ac1e85",
          "name": "Wenzheng Chang",
          "hidden": false
        },
        {
          "_id": "68c8cece733e345e52ac1e86",
          "name": "Haoyu Guo",
          "hidden": false
        },
        {
          "_id": "68c8cece733e345e52ac1e87",
          "name": "Zizun Li",
          "hidden": false
        },
        {
          "_id": "68c8cece733e345e52ac1e88",
          "name": "Kaijing Ma",
          "hidden": false
        },
        {
          "_id": "68c8cece733e345e52ac1e89",
          "name": "Xinyue Li",
          "hidden": false
        },
        {
          "_id": "68c8cece733e345e52ac1e8a",
          "name": "Yating Wang",
          "hidden": false
        },
        {
          "_id": "68c8cece733e345e52ac1e8b",
          "name": "Haoyi Zhu",
          "hidden": false
        },
        {
          "_id": "68c8cece733e345e52ac1e8c",
          "name": "Mingyu Liu",
          "hidden": false
        },
        {
          "_id": "68c8cece733e345e52ac1e8d",
          "name": "Dingning Liu",
          "hidden": false
        },
        {
          "_id": "68c8cece733e345e52ac1e8e",
          "name": "Jiange Yang",
          "hidden": false
        },
        {
          "_id": "68c8cece733e345e52ac1e8f",
          "name": "Zhoujie Fu",
          "hidden": false
        },
        {
          "_id": "68c8cece733e345e52ac1e90",
          "name": "Junyi Chen",
          "hidden": false
        },
        {
          "_id": "68c8cece733e345e52ac1e91",
          "name": "Chunhua Shen",
          "hidden": false
        },
        {
          "_id": "68c8cece733e345e52ac1e92",
          "name": "Jiangmiao Pang",
          "hidden": false
        },
        {
          "_id": "68c8cece733e345e52ac1e93",
          "name": "Kaipeng Zhang",
          "hidden": false
        },
        {
          "_id": "68c8cece733e345e52ac1e94",
          "name": "Tong He",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-15T17:59:19.000Z",
      "submittedOnDailyAt": "2025-09-16T01:13:36.945Z",
      "title": "OmniWorld: A Multi-Domain and Multi-Modal Dataset for 4D World Modeling",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "The field of 4D world modeling - aiming to jointly capture spatial geometry\nand temporal dynamics - has witnessed remarkable progress in recent years,\ndriven by advances in large-scale generative models and multimodal learning.\nHowever, the development of truly general 4D world models remains fundamentally\nconstrained by the availability of high-quality data. Existing datasets and\nbenchmarks often lack the dynamic complexity, multi-domain diversity, and\nspatial-temporal annotations required to support key tasks such as 4D geometric\nreconstruction, future prediction, and camera-control video generation. To\naddress this gap, we introduce OmniWorld, a large-scale, multi-domain,\nmulti-modal dataset specifically designed for 4D world modeling. OmniWorld\nconsists of a newly collected OmniWorld-Game dataset and several curated public\ndatasets spanning diverse domains. Compared with existing synthetic datasets,\nOmniWorld-Game provides richer modality coverage, larger scale, and more\nrealistic dynamic interactions. Based on this dataset, we establish a\nchallenging benchmark that exposes the limitations of current state-of-the-art\n(SOTA) approaches in modeling complex 4D environments. Moreover, fine-tuning\nexisting SOTA methods on OmniWorld leads to significant performance gains\nacross 4D reconstruction and video generation tasks, strongly validating\nOmniWorld as a powerful resource for training and evaluation. We envision\nOmniWorld as a catalyst for accelerating the development of general-purpose 4D\nworld models, ultimately advancing machines' holistic understanding of the\nphysical world.",
      "upvotes": 58,
      "discussionId": "68c8cece733e345e52ac1e95",
      "projectPage": "https://yangzhou24.github.io/OmniWorld/",
      "githubRepo": "https://github.com/yangzhou24/OmniWorld",
      "ai_summary": "OmniWorld, a large-scale, multi-domain, multi-modal dataset, addresses the limitations of existing 4D world modeling datasets and benchmarks, enabling significant performance improvements in 4D reconstruction and video generation.",
      "ai_keywords": [
        "4D world modeling",
        "spatial geometry",
        "temporal dynamics",
        "large-scale generative models",
        "multimodal learning",
        "OmniWorld",
        "OmniWorld-Game",
        "4D geometric reconstruction",
        "future prediction",
        "camera-control video generation",
        "state-of-the-art (SOTA)"
      ],
      "githubStars": 70
    },
    "publishedAt": "2025-09-15T13:59:19.000Z",
    "title": "OmniWorld: A Multi-Domain and Multi-Modal Dataset for 4D World Modeling",
    "summary": "The field of 4D world modeling - aiming to jointly capture spatial geometry\nand temporal dynamics - has witnessed remarkable progress in recent years,\ndriven by advances in large-scale generative models and multimodal learning.\nHowever, the development of truly general 4D world models remains fundamentally\nconstrained by the availability of high-quality data. Existing datasets and\nbenchmarks often lack the dynamic complexity, multi-domain diversity, and\nspatial-temporal annotations required to support key tasks such as 4D geometric\nreconstruction, future prediction, and camera-control video generation. To\naddress this gap, we introduce OmniWorld, a large-scale, multi-domain,\nmulti-modal dataset specifically designed for 4D world modeling. OmniWorld\nconsists of a newly collected OmniWorld-Game dataset and several curated public\ndatasets spanning diverse domains. Compared with existing synthetic datasets,\nOmniWorld-Game provides richer modality coverage, larger scale, and more\nrealistic dynamic interactions. Based on this dataset, we establish a\nchallenging benchmark that exposes the limitations of current state-of-the-art\n(SOTA) approaches in modeling complex 4D environments. Moreover, fine-tuning\nexisting SOTA methods on OmniWorld leads to significant performance gains\nacross 4D reconstruction and video generation tasks, strongly validating\nOmniWorld as a powerful resource for training and evaluation. We envision\nOmniWorld as a catalyst for accelerating the development of general-purpose 4D\nworld models, ultimately advancing machines' holistic understanding of the\nphysical world.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.12201.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 104
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.11543",
      "authors": [
        {
          "_id": "68c8c9b3733e345e52ac1e66",
          "name": "Zhengxi Lu",
          "hidden": false
        },
        {
          "_id": "68c8c9b3733e345e52ac1e67",
          "name": "Jiabo Ye",
          "hidden": false
        },
        {
          "_id": "68c8c9b3733e345e52ac1e68",
          "name": "Fei Tang",
          "hidden": false
        },
        {
          "_id": "68c8c9b3733e345e52ac1e69",
          "name": "Yongliang Shen",
          "hidden": false
        },
        {
          "_id": "68c8c9b3733e345e52ac1e6a",
          "name": "Haiyang Xu",
          "hidden": false
        },
        {
          "_id": "68c8c9b3733e345e52ac1e6b",
          "name": "Ziwei Zheng",
          "hidden": false
        },
        {
          "_id": "68c8c9b3733e345e52ac1e6c",
          "name": "Weiming Lu",
          "hidden": false
        },
        {
          "_id": "68c8c9b3733e345e52ac1e6d",
          "name": "Ming Yan",
          "hidden": false
        },
        {
          "_id": "68c8c9b3733e345e52ac1e6e",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "68c8c9b3733e345e52ac1e6f",
          "name": "Jun Xiao",
          "hidden": false
        },
        {
          "_id": "68c8c9b3733e345e52ac1e70",
          "name": "Yueting Zhuang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/645b10e80c73ea27d13f7aca/Im2ESDVPPQMlHP6L5owuo.png",
        "https://cdn-uploads.huggingface.co/production/uploads/645b10e80c73ea27d13f7aca/NchaoNZMIP1fSvmDtQ95e.png",
        "https://cdn-uploads.huggingface.co/production/uploads/645b10e80c73ea27d13f7aca/_A1UZfDw0U9K7cycraLFv.png"
      ],
      "publishedAt": "2025-09-15T03:24:08.000Z",
      "submittedOnDailyAt": "2025-09-16T00:55:54.867Z",
      "title": "UI-S1: Advancing GUI Automation via Semi-online Reinforcement Learning",
      "submittedOnDailyBy": {
        "_id": "645b10e80c73ea27d13f7aca",
        "avatarUrl": "/avatars/95e565306472a15067440b5b43e07a6f.svg",
        "isPro": false,
        "fullname": "xuhaiyang",
        "user": "xhyandwyy",
        "type": "user"
      },
      "summary": "Graphical User Interface (GUI) agents have demonstrated remarkable progress\nin automating complex user interface interactions through reinforcement\nlearning. However, current approaches face a fundamental dilemma: offline RL\nenables stable training on pre-collected trajectories, but struggles with\nmulti-step task execution for lack of trajectory-level reward signals; online\nRL captures these signals through environment interaction, but suffers from\nsparse rewards and prohibitive deployment costs. To address it, we present\nSemi-online Reinforcement Learning, a novel paradigm that simulates online RL\non offline trajectories. During each rollout process, we preserve the original\nmodel output within the multi-turn dialogue, where a Patch Module adaptively\nrecovers the divergence between rollout and expert trajectories. To capture\nlong-term training signals, Semi-online RL introduces discounted future returns\ninto the reward computation and optimizes the policy with weighted step-level\nand episode-level advantages. We further introduce Semi-Online Performance\n(SOP), a metric that aligns better with true online performance, serving as a\npractical and effective proxy for real-world evaluation. Experiments show that\nours Semi-online RL achieves SOTA performance among 7B models across four\ndynamic benchmarks, with significant gains over the base model (e.g., +12.0% on\nAndroidWorld, +23.8% on AITW), demonstrating significant progress in bridging\nthe gap between offline training efficiency and online multi-turn reasoning.\nThe code is available at https://github.com/X-PLUG/MobileAgent/tree/main/UI-S1.",
      "upvotes": 30,
      "discussionId": "68c8c9b4733e345e52ac1e71",
      "githubRepo": "https://github.com/X-PLUG/MobileAgent/tree/main/UI-S1",
      "ai_summary": "Semi-online Reinforcement Learning addresses the limitations of offline and online RL by simulating online RL on offline trajectories, achieving state-of-the-art performance in dynamic benchmarks.",
      "ai_keywords": [
        "reinforcement learning",
        "offline RL",
        "online RL",
        "multi-step task execution",
        "trajectory-level reward signals",
        "sparse rewards",
        "deployment costs",
        "Patch Module",
        "discounted future returns",
        "step-level advantages",
        "episode-level advantages",
        "Semi-Online Performance (SOP)",
        "dynamic benchmarks",
        "AndroidWorld",
        "AITW"
      ],
      "githubStars": 5632
    },
    "publishedAt": "2025-09-14T23:24:08.000Z",
    "title": "UI-S1: Advancing GUI Automation via Semi-online Reinforcement Learning",
    "summary": "Graphical User Interface (GUI) agents have demonstrated remarkable progress\nin automating complex user interface interactions through reinforcement\nlearning. However, current approaches face a fundamental dilemma: offline RL\nenables stable training on pre-collected trajectories, but struggles with\nmulti-step task execution for lack of trajectory-level reward signals; online\nRL captures these signals through environment interaction, but suffers from\nsparse rewards and prohibitive deployment costs. To address it, we present\nSemi-online Reinforcement Learning, a novel paradigm that simulates online RL\non offline trajectories. During each rollout process, we preserve the original\nmodel output within the multi-turn dialogue, where a Patch Module adaptively\nrecovers the divergence between rollout and expert trajectories. To capture\nlong-term training signals, Semi-online RL introduces discounted future returns\ninto the reward computation and optimizes the policy with weighted step-level\nand episode-level advantages. We further introduce Semi-Online Performance\n(SOP), a metric that aligns better with true online performance, serving as a\npractical and effective proxy for real-world evaluation. Experiments show that\nours Semi-online RL achieves SOTA performance among 7B models across four\ndynamic benchmarks, with significant gains over the base model (e.g., +12.0% on\nAndroidWorld, +23.8% on AITW), demonstrating significant progress in bridging\nthe gap between offline training efficiency and online multi-turn reasoning.\nThe code is available at https://github.com/X-PLUG/MobileAgent/tree/main/UI-S1.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/645b10e80c73ea27d13f7aca/Im2ESDVPPQMlHP6L5owuo.png",
      "https://cdn-uploads.huggingface.co/production/uploads/645b10e80c73ea27d13f7aca/NchaoNZMIP1fSvmDtQ95e.png",
      "https://cdn-uploads.huggingface.co/production/uploads/645b10e80c73ea27d13f7aca/_A1UZfDw0U9K7cycraLFv.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.11543.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645b10e80c73ea27d13f7aca",
      "avatarUrl": "/avatars/95e565306472a15067440b5b43e07a6f.svg",
      "fullname": "xuhaiyang",
      "name": "xhyandwyy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.10813",
      "authors": [
        {
          "_id": "68c8d095733e345e52ac1e97",
          "name": "Weipeng Zhong",
          "hidden": false
        },
        {
          "_id": "68c8d095733e345e52ac1e98",
          "name": "Peizhou Cao",
          "hidden": false
        },
        {
          "_id": "68c8d095733e345e52ac1e99",
          "name": "Yichen Jin",
          "hidden": false
        },
        {
          "_id": "68c8d095733e345e52ac1e9a",
          "name": "Li Luo",
          "hidden": false
        },
        {
          "_id": "68c8d095733e345e52ac1e9b",
          "name": "Wenzhe Cai",
          "hidden": false
        },
        {
          "_id": "68c8d095733e345e52ac1e9c",
          "name": "Jingli Lin",
          "hidden": false
        },
        {
          "_id": "68c8d095733e345e52ac1e9d",
          "name": "Hanqing Wang",
          "hidden": false
        },
        {
          "_id": "68c8d095733e345e52ac1e9e",
          "name": "Zhaoyang Lyu",
          "hidden": false
        },
        {
          "_id": "68c8d095733e345e52ac1e9f",
          "name": "Tai Wang",
          "hidden": false
        },
        {
          "_id": "68c8d095733e345e52ac1ea0",
          "name": "Bo Dai",
          "hidden": false
        },
        {
          "_id": "68c8d095733e345e52ac1ea1",
          "name": "Xudong Xu",
          "hidden": false
        },
        {
          "_id": "68c8d095733e345e52ac1ea2",
          "name": "Jiangmiao Pang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-13T14:25:17.000Z",
      "submittedOnDailyAt": "2025-09-16T01:21:15.577Z",
      "title": "InternScenes: A Large-scale Simulatable Indoor Scene Dataset with\n  Realistic Layouts",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "The advancement of Embodied AI heavily relies on large-scale, simulatable 3D\nscene datasets characterized by scene diversity and realistic layouts. However,\nexisting datasets typically suffer from limitations in data scale or diversity,\nsanitized layouts lacking small items, and severe object collisions. To address\nthese shortcomings, we introduce InternScenes, a novel large-scale\nsimulatable indoor scene dataset comprising approximately 40,000 diverse scenes\nby integrating three disparate scene sources, real-world scans, procedurally\ngenerated scenes, and designer-created scenes, including 1.96M 3D objects and\ncovering 15 common scene types and 288 object classes. We particularly preserve\nmassive small items in the scenes, resulting in realistic and complex layouts\nwith an average of 41.5 objects per region. Our comprehensive data processing\npipeline ensures simulatability by creating real-to-sim replicas for real-world\nscans, enhances interactivity by incorporating interactive objects into these\nscenes, and resolves object collisions by physical simulations. We demonstrate\nthe value of InternScenes with two benchmark applications: scene layout\ngeneration and point-goal navigation. Both show the new challenges posed by the\ncomplex and realistic layouts. More importantly, InternScenes paves the way for\nscaling up the model training for both tasks, making the generation and\nnavigation in such complex scenes possible. We commit to open-sourcing the\ndata, models, and benchmarks to benefit the whole community.",
      "upvotes": 17,
      "discussionId": "68c8d095733e345e52ac1ea3",
      "projectPage": "https://marjordcpz.github.io/InternScenes.github.io/",
      "githubRepo": "https://github.com/InternRobotics/InternScenes",
      "ai_summary": "InternScenes is a large-scale, diverse, and realistic indoor scene dataset that addresses limitations in existing datasets, enabling better scene layout generation and point-goal navigation.",
      "ai_keywords": [
        "Embodied AI",
        "3D scene datasets",
        "scene diversity",
        "realistic layouts",
        "real-world scans",
        "procedurally generated scenes",
        "designer-created scenes",
        "3D objects",
        "scene types",
        "object classes",
        "small items",
        "real-to-sim replicas",
        "interactive objects",
        "physical simulations",
        "scene layout generation",
        "point-goal navigation"
      ],
      "githubStars": 99
    },
    "publishedAt": "2025-09-13T10:25:17.000Z",
    "title": "InternScenes: A Large-scale Simulatable Indoor Scene Dataset with\n  Realistic Layouts",
    "summary": "The advancement of Embodied AI heavily relies on large-scale, simulatable 3D\nscene datasets characterized by scene diversity and realistic layouts. However,\nexisting datasets typically suffer from limitations in data scale or diversity,\nsanitized layouts lacking small items, and severe object collisions. To address\nthese shortcomings, we introduce InternScenes, a novel large-scale\nsimulatable indoor scene dataset comprising approximately 40,000 diverse scenes\nby integrating three disparate scene sources, real-world scans, procedurally\ngenerated scenes, and designer-created scenes, including 1.96M 3D objects and\ncovering 15 common scene types and 288 object classes. We particularly preserve\nmassive small items in the scenes, resulting in realistic and complex layouts\nwith an average of 41.5 objects per region. Our comprehensive data processing\npipeline ensures simulatability by creating real-to-sim replicas for real-world\nscans, enhances interactivity by incorporating interactive objects into these\nscenes, and resolves object collisions by physical simulations. We demonstrate\nthe value of InternScenes with two benchmark applications: scene layout\ngeneration and point-goal navigation. Both show the new challenges posed by the\ncomplex and realistic layouts. More importantly, InternScenes paves the way for\nscaling up the model training for both tasks, making the generation and\nnavigation in such complex scenes possible. We commit to open-sourcing the\ndata, models, and benchmarks to benefit the whole community.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.10813.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 104
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.12203",
      "authors": [
        {
          "_id": "68c8cea2733e345e52ac1e79",
          "name": "Zixin Yin",
          "hidden": false
        },
        {
          "_id": "68c8cea2733e345e52ac1e7a",
          "name": "Xili Dai",
          "hidden": false
        },
        {
          "_id": "68c8cea2733e345e52ac1e7b",
          "name": "Duomin Wang",
          "hidden": false
        },
        {
          "_id": "68c8cea2733e345e52ac1e7c",
          "name": "Xianfang Zeng",
          "hidden": false
        },
        {
          "_id": "68c8cea2733e345e52ac1e7d",
          "name": "Lionel M. Ni",
          "hidden": false
        },
        {
          "_id": "68c8cea2733e345e52ac1e7e",
          "name": "Gang Yu",
          "hidden": false
        },
        {
          "_id": "68c8cea2733e345e52ac1e7f",
          "name": "Heung-Yeung Shum",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-15T17:59:47.000Z",
      "submittedOnDailyAt": "2025-09-16T01:13:01.794Z",
      "title": "LazyDrag: Enabling Stable Drag-Based Editing on Multi-Modal Diffusion\n  Transformers via Explicit Correspondence",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "The reliance on implicit point matching via attention has become a core\nbottleneck in drag-based editing, resulting in a fundamental compromise on\nweakened inversion strength and costly test-time optimization (TTO). This\ncompromise severely limits the generative capabilities of diffusion models,\nsuppressing high-fidelity inpainting and text-guided creation. In this paper,\nwe introduce LazyDrag, the first drag-based image editing method for\nMulti-Modal Diffusion Transformers, which directly eliminates the reliance on\nimplicit point matching. In concrete terms, our method generates an explicit\ncorrespondence map from user drag inputs as a reliable reference to boost the\nattention control. This reliable reference opens the potential for a stable\nfull-strength inversion process, which is the first in the drag-based editing\ntask. It obviates the necessity for TTO and unlocks the generative capability\nof models. Therefore, LazyDrag naturally unifies precise geometric control with\ntext guidance, enabling complex edits that were previously out of reach:\nopening the mouth of a dog and inpainting its interior, generating new objects\nlike a ``tennis ball'', or for ambiguous drags, making context-aware changes\nlike moving a hand into a pocket. Additionally, LazyDrag supports multi-round\nworkflows with simultaneous move and scale operations. Evaluated on the\nDragBench, our method outperforms baselines in drag accuracy and perceptual\nquality, as validated by VIEScore and human evaluation. LazyDrag not only\nestablishes new state-of-the-art performance, but also paves a new way to\nediting paradigms.",
      "upvotes": 5,
      "discussionId": "68c8cea2733e345e52ac1e80",
      "projectPage": "https://zxyin.github.io/LazyDrag",
      "ai_summary": "LazyDrag, a drag-based image editing method for Multi-Modal Diffusion Transformers, eliminates implicit point matching, enabling precise geometric control and text guidance without test-time optimization.",
      "ai_keywords": [
        "attention",
        "drag-based editing",
        "Multi-Modal Diffusion Transformers",
        "explicit correspondence map",
        "full-strength inversion",
        "test-time optimization",
        "high-fidelity inpainting",
        "text-guided creation",
        "DragBench",
        "VIEScore"
      ]
    },
    "publishedAt": "2025-09-15T13:59:47.000Z",
    "title": "LazyDrag: Enabling Stable Drag-Based Editing on Multi-Modal Diffusion\n  Transformers via Explicit Correspondence",
    "summary": "The reliance on implicit point matching via attention has become a core\nbottleneck in drag-based editing, resulting in a fundamental compromise on\nweakened inversion strength and costly test-time optimization (TTO). This\ncompromise severely limits the generative capabilities of diffusion models,\nsuppressing high-fidelity inpainting and text-guided creation. In this paper,\nwe introduce LazyDrag, the first drag-based image editing method for\nMulti-Modal Diffusion Transformers, which directly eliminates the reliance on\nimplicit point matching. In concrete terms, our method generates an explicit\ncorrespondence map from user drag inputs as a reliable reference to boost the\nattention control. This reliable reference opens the potential for a stable\nfull-strength inversion process, which is the first in the drag-based editing\ntask. It obviates the necessity for TTO and unlocks the generative capability\nof models. Therefore, LazyDrag naturally unifies precise geometric control with\ntext guidance, enabling complex edits that were previously out of reach:\nopening the mouth of a dog and inpainting its interior, generating new objects\nlike a ``tennis ball'', or for ambiguous drags, making context-aware changes\nlike moving a hand into a pocket. Additionally, LazyDrag supports multi-round\nworkflows with simultaneous move and scale operations. Evaluated on the\nDragBench, our method outperforms baselines in drag accuracy and perceptual\nquality, as validated by VIEScore and human evaluation. LazyDrag not only\nestablishes new state-of-the-art performance, but also paves a new way to\nediting paradigms.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.12203.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 104
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.09672",
      "authors": [
        {
          "_id": "68c8d0fd733e345e52ac1ea5",
          "name": "Artem Lukoianov",
          "hidden": false
        },
        {
          "_id": "68c8d0fd733e345e52ac1ea6",
          "name": "Chenyang Yuan",
          "hidden": false
        },
        {
          "_id": "68c8d0fd733e345e52ac1ea7",
          "name": "Justin Solomon",
          "hidden": false
        },
        {
          "_id": "68c8d0fd733e345e52ac1ea8",
          "name": "Vincent Sitzmann",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63d9528c6b496a404a437507/qYvBgptghbW5et_mmWEDV.png"
      ],
      "publishedAt": "2025-09-11T17:59:08.000Z",
      "submittedOnDailyAt": "2025-09-16T02:23:57.542Z",
      "title": "Locality in Image Diffusion Models Emerges from Data Statistics",
      "submittedOnDailyBy": {
        "_id": "63d9528c6b496a404a437507",
        "avatarUrl": "/avatars/71eebe88297c83020e3446489edfd06b.svg",
        "isPro": false,
        "fullname": "Artem Lukoianov",
        "user": "ottogin",
        "type": "user"
      },
      "summary": "Among generative models, diffusion models are uniquely intriguing due to the\nexistence of a closed-form optimal minimizer of their training objective, often\nreferred to as the optimal denoiser. However, diffusion using this optimal\ndenoiser merely reproduces images in the training set and hence fails to\ncapture the behavior of deep diffusion models. Recent work has attempted to\ncharacterize this gap between the optimal denoiser and deep diffusion models,\nproposing analytical, training-free models that can generate images that\nresemble those generated by a trained UNet. The best-performing method\nhypothesizes that shift equivariance and locality inductive biases of\nconvolutional neural networks are the cause of the performance gap, hence\nincorporating these assumptions into its analytical model. In this work, we\npresent evidence that the locality in deep diffusion models emerges as a\nstatistical property of the image dataset, not due to the inductive bias of\nconvolutional neural networks. Specifically, we demonstrate that an optimal\nparametric linear denoiser exhibits similar locality properties to the deep\nneural denoisers. We further show, both theoretically and experimentally, that\nthis locality arises directly from the pixel correlations present in natural\nimage datasets. Finally, we use these insights to craft an analytical denoiser\nthat better matches scores predicted by a deep diffusion model than the prior\nexpert-crafted alternative.",
      "upvotes": 5,
      "discussionId": "68c8d0fd733e345e52ac1ea9",
      "projectPage": "https://locality.lukoianov.com/",
      "githubRepo": "https://github.com/ottogin/locality-in-diffusion-models",
      "ai_summary": "Research shows that locality in deep diffusion models is a statistical property of image datasets rather than an inductive bias of convolutional neural networks, leading to the development of a more accurate analytical denoiser.",
      "ai_keywords": [
        "diffusion models",
        "optimal denoiser",
        "UNet",
        "shift equivariance",
        "locality inductive biases",
        "convolutional neural networks",
        "parametric linear denoiser",
        "pixel correlations",
        "natural image datasets",
        "analytical denoiser"
      ],
      "githubStars": 10
    },
    "publishedAt": "2025-09-11T13:59:08.000Z",
    "title": "Locality in Image Diffusion Models Emerges from Data Statistics",
    "summary": "Among generative models, diffusion models are uniquely intriguing due to the\nexistence of a closed-form optimal minimizer of their training objective, often\nreferred to as the optimal denoiser. However, diffusion using this optimal\ndenoiser merely reproduces images in the training set and hence fails to\ncapture the behavior of deep diffusion models. Recent work has attempted to\ncharacterize this gap between the optimal denoiser and deep diffusion models,\nproposing analytical, training-free models that can generate images that\nresemble those generated by a trained UNet. The best-performing method\nhypothesizes that shift equivariance and locality inductive biases of\nconvolutional neural networks are the cause of the performance gap, hence\nincorporating these assumptions into its analytical model. In this work, we\npresent evidence that the locality in deep diffusion models emerges as a\nstatistical property of the image dataset, not due to the inductive bias of\nconvolutional neural networks. Specifically, we demonstrate that an optimal\nparametric linear denoiser exhibits similar locality properties to the deep\nneural denoisers. We further show, both theoretically and experimentally, that\nthis locality arises directly from the pixel correlations present in natural\nimage datasets. Finally, we use these insights to craft an analytical denoiser\nthat better matches scores predicted by a deep diffusion model than the prior\nexpert-crafted alternative.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63d9528c6b496a404a437507/qYvBgptghbW5et_mmWEDV.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.09672.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63d9528c6b496a404a437507",
      "avatarUrl": "/avatars/71eebe88297c83020e3446489edfd06b.svg",
      "fullname": "Artem Lukoianov",
      "name": "ottogin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.10884",
      "authors": [
        {
          "_id": "68c8d3b6733e345e52ac1ecb",
          "name": "Qingxiang Liu",
          "hidden": false
        },
        {
          "_id": "68c8d3b6733e345e52ac1ecc",
          "name": "Ting Huang",
          "hidden": false
        },
        {
          "_id": "68c8d3b6733e345e52ac1ecd",
          "name": "Zeyu Zhang",
          "hidden": false
        },
        {
          "_id": "68c8d3b6733e345e52ac1ece",
          "name": "Hao Tang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-13T16:31:03.000Z",
      "submittedOnDailyAt": "2025-09-16T01:34:35.833Z",
      "title": "Nav-R1: Reasoning and Navigation in Embodied Scenes",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Embodied navigation requires agents to integrate perception, reasoning, and\naction for robust interaction in complex 3D environments. Existing approaches\noften suffer from incoherent and unstable reasoning traces that hinder\ngeneralization across diverse environments, and difficulty balancing\nlong-horizon semantic reasoning with low-latency control for real-time\nnavigation. To address these challenges, we propose Nav-R1, an embodied\nfoundation model that unifies reasoning in embodied environments. We first\nconstruct Nav-CoT-110K, a large-scale dataset of step-by-step Chains-of-Thought\n(CoT) for embodied tasks, which enables cold-start initialization with\nstructured reasoning. Building on this foundation, we design a GRPO-based\nreinforcement learning framework with three complementary rewards: format,\nunderstanding, and navigation, to improve structural adherence, semantic\ngrounding, and path fidelity. Furthermore, we introduce a Fast-in-Slow\nreasoning paradigm, decoupling deliberate semantic reasoning from low-latency\nreactive control for efficient yet coherent navigation. Extensive evaluations\non embodied AI benchmarks demonstrate that Nav-R1 consistently outperforms\nstrong baselines, with over 8% average improvement in reasoning and navigation\nperformance. Real-world deployment on a mobile robot further validates its\nrobustness under limited onboard resources. Code:\nhttps://github.com/AIGeeksGroup/Nav-R1. Website:\nhttps://aigeeksgroup.github.io/Nav-R1.",
      "upvotes": 4,
      "discussionId": "68c8d3b6733e345e52ac1ecf",
      "projectPage": "https://aigeeksgroup.github.io/Nav-R1/",
      "githubRepo": "https://github.com/AIGeeksGroup/Nav-R1",
      "ai_summary": "Nav-R1, an embodied foundation model, enhances navigation by integrating structured reasoning and decoupled control mechanisms, outperforming existing approaches on benchmarks and in real-world scenarios.",
      "ai_keywords": [
        "Embodied navigation",
        "Chains-of-Thought (CoT)",
        "GRPO-based reinforcement learning",
        "Fast-in-Slow reasoning",
        "structural adherence",
        "semantic grounding",
        "path fidelity",
        "embodied AI benchmarks",
        "mobile robot deployment"
      ],
      "githubStars": 5
    },
    "publishedAt": "2025-09-13T12:31:03.000Z",
    "title": "Nav-R1: Reasoning and Navigation in Embodied Scenes",
    "summary": "Embodied navigation requires agents to integrate perception, reasoning, and\naction for robust interaction in complex 3D environments. Existing approaches\noften suffer from incoherent and unstable reasoning traces that hinder\ngeneralization across diverse environments, and difficulty balancing\nlong-horizon semantic reasoning with low-latency control for real-time\nnavigation. To address these challenges, we propose Nav-R1, an embodied\nfoundation model that unifies reasoning in embodied environments. We first\nconstruct Nav-CoT-110K, a large-scale dataset of step-by-step Chains-of-Thought\n(CoT) for embodied tasks, which enables cold-start initialization with\nstructured reasoning. Building on this foundation, we design a GRPO-based\nreinforcement learning framework with three complementary rewards: format,\nunderstanding, and navigation, to improve structural adherence, semantic\ngrounding, and path fidelity. Furthermore, we introduce a Fast-in-Slow\nreasoning paradigm, decoupling deliberate semantic reasoning from low-latency\nreactive control for efficient yet coherent navigation. Extensive evaluations\non embodied AI benchmarks demonstrate that Nav-R1 consistently outperforms\nstrong baselines, with over 8% average improvement in reasoning and navigation\nperformance. Real-world deployment on a mobile robot further validates its\nrobustness under limited onboard resources. Code:\nhttps://github.com/AIGeeksGroup/Nav-R1. Website:\nhttps://aigeeksgroup.github.io/Nav-R1.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.10884.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 104
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.11444",
      "authors": [
        {
          "_id": "68c8bef9733e345e52ac1e38",
          "name": "Gaurab Chhetri",
          "hidden": false
        },
        {
          "_id": "68c8bef9733e345e52ac1e39",
          "name": "Anandi Dutta",
          "hidden": false
        },
        {
          "_id": "68c8bef9733e345e52ac1e3a",
          "name": "Subasish Das",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/685204843761e5afb796bc57/4aU7jSLmMB3daVepAE8Fv.png"
      ],
      "publishedAt": "2025-09-14T21:37:24.000Z",
      "submittedOnDailyAt": "2025-09-16T00:35:07.371Z",
      "title": "CognitiveSky: Scalable Sentiment and Narrative Analysis for\n  Decentralized Social Media",
      "submittedOnDailyBy": {
        "_id": "685204843761e5afb796bc57",
        "avatarUrl": "/avatars/1224f12ba48912dd59f22a7a6af683bc.svg",
        "isPro": false,
        "fullname": "Gaurab Chhetri",
        "user": "gauravfs-14",
        "type": "user"
      },
      "summary": "The emergence of decentralized social media platforms presents new\nopportunities and challenges for real-time analysis of public discourse. This\nstudy introduces CognitiveSky, an open-source and scalable framework designed\nfor sentiment, emotion, and narrative analysis on Bluesky, a federated Twitter\nor X.com alternative. By ingesting data through Bluesky's Application\nProgramming Interface (API), CognitiveSky applies transformer-based models to\nannotate large-scale user-generated content and produces structured and\nanalyzable outputs. These summaries drive a dynamic dashboard that visualizes\nevolving patterns in emotion, activity, and conversation topics. Built entirely\non free-tier infrastructure, CognitiveSky achieves both low operational cost\nand high accessibility. While demonstrated here for monitoring mental health\ndiscourse, its modular design enables applications across domains such as\ndisinformation detection, crisis response, and civic sentiment analysis. By\nbridging large language models with decentralized networks, CognitiveSky offers\na transparent, extensible tool for computational social science in an era of\nshifting digital ecosystems.",
      "upvotes": 2,
      "discussionId": "68c8bef9733e345e52ac1e3b",
      "projectPage": "https://cognitivesky.gaurabchhetri.com.np/",
      "githubRepo": "https://github.com/gauravfs-14/CognitiveSky",
      "ai_summary": "CognitiveSky, a transformer-based framework, analyzes sentiment, emotion, and narratives on Bluesky, providing insights through a dynamic dashboard and supporting various applications in computational social science.",
      "ai_keywords": [
        "transformer-based models",
        "sentiment analysis",
        "emotion analysis",
        "narrative analysis",
        "Bluesky",
        "Application Programming Interface (API)",
        "dynamic dashboard",
        "mental health discourse",
        "disinformation detection",
        "crisis response",
        "civic sentiment analysis",
        "large language models",
        "decentralized networks"
      ],
      "githubStars": 2
    },
    "publishedAt": "2025-09-14T17:37:24.000Z",
    "title": "CognitiveSky: Scalable Sentiment and Narrative Analysis for\n  Decentralized Social Media",
    "summary": "The emergence of decentralized social media platforms presents new\nopportunities and challenges for real-time analysis of public discourse. This\nstudy introduces CognitiveSky, an open-source and scalable framework designed\nfor sentiment, emotion, and narrative analysis on Bluesky, a federated Twitter\nor X.com alternative. By ingesting data through Bluesky's Application\nProgramming Interface (API), CognitiveSky applies transformer-based models to\nannotate large-scale user-generated content and produces structured and\nanalyzable outputs. These summaries drive a dynamic dashboard that visualizes\nevolving patterns in emotion, activity, and conversation topics. Built entirely\non free-tier infrastructure, CognitiveSky achieves both low operational cost\nand high accessibility. While demonstrated here for monitoring mental health\ndiscourse, its modular design enables applications across domains such as\ndisinformation detection, crisis response, and civic sentiment analysis. By\nbridging large language models with decentralized networks, CognitiveSky offers\na transparent, extensible tool for computational social science in an era of\nshifting digital ecosystems.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/685204843761e5afb796bc57/4aU7jSLmMB3daVepAE8Fv.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.11444.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "685204843761e5afb796bc57",
      "avatarUrl": "/avatars/1224f12ba48912dd59f22a7a6af683bc.svg",
      "fullname": "Gaurab Chhetri",
      "name": "gauravfs-14",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.11986",
      "authors": [
        {
          "_id": "68c8d7dd733e345e52ac1edd",
          "name": "Wenyan Li",
          "hidden": false
        },
        {
          "_id": "68c8d7dd733e345e52ac1ede",
          "name": "Raphael Tang",
          "hidden": false
        },
        {
          "_id": "68c8d7dd733e345e52ac1edf",
          "name": "Chengzu Li",
          "hidden": false
        },
        {
          "_id": "68c8d7dd733e345e52ac1ee0",
          "name": "Caiqi Zhang",
          "hidden": false
        },
        {
          "_id": "68c8d7dd733e345e52ac1ee1",
          "name": "Ivan Vulić",
          "hidden": false
        },
        {
          "_id": "68c8d7dd733e345e52ac1ee2",
          "name": "Anders Søgaard",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-15T14:38:06.000Z",
      "submittedOnDailyAt": "2025-09-16T01:52:58.573Z",
      "title": "Lost in Embeddings: Information Loss in Vision-Language Models",
      "submittedOnDailyBy": {
        "_id": "619b506f70d03780cbec5806",
        "avatarUrl": "/avatars/ea2b0b8f0a3eb16d53ef40da9981c397.svg",
        "isPro": false,
        "fullname": "wenyan li",
        "user": "lyan62",
        "type": "user"
      },
      "summary": "Vision--language models (VLMs) often process visual inputs through a\npretrained vision encoder, followed by a projection into the language model's\nembedding space via a connector component. While crucial for modality fusion,\nthe potential information loss induced by this projection step and its direct\nimpact on model capabilities remain understudied. We introduce two\ncomplementary approaches to examine and quantify this loss by analyzing the\nlatent representation space. First, we evaluate semantic information\npreservation by analyzing changes in k-nearest neighbor relationships between\nimage representations, before and after projection. Second, we directly measure\ninformation loss by reconstructing visual embeddings from the projected\nrepresentation, localizing loss at an image patch level. Experiments reveal\nthat connectors substantially distort the local geometry of visual\nrepresentations, with k-nearest neighbors diverging by 40--60\\%\npost-projection, correlating with degradation in retrieval performance. The\npatch-level embedding reconstruction provides interpretable insights for model\nbehavior on visually grounded question-answering tasks, finding that areas of\nhigh information loss reliably predict instances where models struggle.",
      "upvotes": 1,
      "discussionId": "68c8d7dd733e345e52ac1ee3",
      "ai_summary": "Two approaches are introduced to analyze and quantify information loss in vision-language models during the projection of visual inputs into the language model's embedding space, revealing significant distortions and their impact on model performance.",
      "ai_keywords": [
        "vision--language models",
        "pretrained vision encoder",
        "connector component",
        "modality fusion",
        "latent representation space",
        "semantic information preservation",
        "k-nearest neighbor relationships",
        "visual embeddings",
        "patch-level embedding reconstruction",
        "visually grounded question-answering tasks"
      ]
    },
    "publishedAt": "2025-09-15T10:38:06.000Z",
    "title": "Lost in Embeddings: Information Loss in Vision-Language Models",
    "summary": "Vision--language models (VLMs) often process visual inputs through a\npretrained vision encoder, followed by a projection into the language model's\nembedding space via a connector component. While crucial for modality fusion,\nthe potential information loss induced by this projection step and its direct\nimpact on model capabilities remain understudied. We introduce two\ncomplementary approaches to examine and quantify this loss by analyzing the\nlatent representation space. First, we evaluate semantic information\npreservation by analyzing changes in k-nearest neighbor relationships between\nimage representations, before and after projection. Second, we directly measure\ninformation loss by reconstructing visual embeddings from the projected\nrepresentation, localizing loss at an image patch level. Experiments reveal\nthat connectors substantially distort the local geometry of visual\nrepresentations, with k-nearest neighbors diverging by 40--60\\%\npost-projection, correlating with degradation in retrieval performance. The\npatch-level embedding reconstruction provides interpretable insights for model\nbehavior on visually grounded question-answering tasks, finding that areas of\nhigh information loss reliably predict instances where models struggle.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.11986.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "619b506f70d03780cbec5806",
      "avatarUrl": "/avatars/ea2b0b8f0a3eb16d53ef40da9981c397.svg",
      "fullname": "wenyan li",
      "name": "lyan62",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.11648",
      "authors": [
        {
          "_id": "68c8c3ac733e345e52ac1e58",
          "name": "Sai Kartheek Reddy Kasu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-15T07:35:35.000Z",
      "submittedOnDailyAt": "2025-09-16T00:28:30.435Z",
      "title": "EthicsMH: A Pilot Benchmark for Ethical Reasoning in Mental Health AI",
      "submittedOnDailyBy": {
        "_id": "651692d718f3a57f869a5a0a",
        "avatarUrl": "/avatars/d5fe48de11e46675b05e1e2e1cf3505c.svg",
        "isPro": false,
        "fullname": "Sai Kartheek Reddy",
        "user": "UVSKKR",
        "type": "user"
      },
      "summary": "The deployment of large language models (LLMs) in mental health and other\nsensitive domains raises urgent questions about ethical reasoning, fairness,\nand responsible alignment. Yet, existing benchmarks for moral and clinical\ndecision-making do not adequately capture the unique ethical dilemmas\nencountered in mental health practice, where confidentiality, autonomy,\nbeneficence, and bias frequently intersect. To address this gap, we introduce\nEthical Reasoning in Mental Health (EthicsMH), a pilot dataset of 125 scenarios\ndesigned to evaluate how AI systems navigate ethically charged situations in\ntherapeutic and psychiatric contexts. Each scenario is enriched with structured\nfields, including multiple decision options, expert-aligned reasoning, expected\nmodel behavior, real-world impact, and multi-stakeholder viewpoints. This\nstructure enables evaluation not only of decision accuracy but also of\nexplanation quality and alignment with professional norms. Although modest in\nscale and developed with model-assisted generation, EthicsMH establishes a task\nframework that bridges AI ethics and mental health decision-making. By\nreleasing this dataset, we aim to provide a seed resource that can be expanded\nthrough community and expert contributions, fostering the development of AI\nsystems capable of responsibly handling some of society's most delicate\ndecisions.",
      "upvotes": 1,
      "discussionId": "68c8c3ad733e345e52ac1e59",
      "ai_summary": "EthicsMH is a dataset of 125 scenarios designed to evaluate AI systems' ethical reasoning in mental health contexts, focusing on decision accuracy, explanation quality, and alignment with professional norms.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "ethical reasoning",
        "fairness",
        "responsible alignment",
        "mental health",
        "ethical dilemmas",
        "confidentiality",
        "autonomy",
        "beneficence",
        "bias",
        "pilot dataset",
        "decision options",
        "expert-aligned reasoning",
        "expected model behavior",
        "real-world impact",
        "multi-stakeholder viewpoints",
        "AI ethics",
        "mental health decision-making"
      ]
    },
    "publishedAt": "2025-09-15T03:35:35.000Z",
    "title": "EthicsMH: A Pilot Benchmark for Ethical Reasoning in Mental Health AI",
    "summary": "The deployment of large language models (LLMs) in mental health and other\nsensitive domains raises urgent questions about ethical reasoning, fairness,\nand responsible alignment. Yet, existing benchmarks for moral and clinical\ndecision-making do not adequately capture the unique ethical dilemmas\nencountered in mental health practice, where confidentiality, autonomy,\nbeneficence, and bias frequently intersect. To address this gap, we introduce\nEthical Reasoning in Mental Health (EthicsMH), a pilot dataset of 125 scenarios\ndesigned to evaluate how AI systems navigate ethically charged situations in\ntherapeutic and psychiatric contexts. Each scenario is enriched with structured\nfields, including multiple decision options, expert-aligned reasoning, expected\nmodel behavior, real-world impact, and multi-stakeholder viewpoints. This\nstructure enables evaluation not only of decision accuracy but also of\nexplanation quality and alignment with professional norms. Although modest in\nscale and developed with model-assisted generation, EthicsMH establishes a task\nframework that bridges AI ethics and mental health decision-making. By\nreleasing this dataset, we aim to provide a seed resource that can be expanded\nthrough community and expert contributions, fostering the development of AI\nsystems capable of responsibly handling some of society's most delicate\ndecisions.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.11648.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "651692d718f3a57f869a5a0a",
      "avatarUrl": "/avatars/d5fe48de11e46675b05e1e2e1cf3505c.svg",
      "fullname": "Sai Kartheek Reddy",
      "name": "UVSKKR",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.11452",
      "authors": [
        {
          "_id": "68c8c651733e345e52ac1e5b",
          "name": "Yining Lu",
          "hidden": false
        },
        {
          "_id": "68c8c651733e345e52ac1e5c",
          "name": "Zilong Wang",
          "hidden": false
        },
        {
          "_id": "68c8c651733e345e52ac1e5d",
          "name": "Shiyang Li",
          "hidden": false
        },
        {
          "_id": "68c8c651733e345e52ac1e5e",
          "name": "Xin Liu",
          "hidden": false
        },
        {
          "_id": "68c8c651733e345e52ac1e5f",
          "name": "Changlong Yu",
          "hidden": false
        },
        {
          "_id": "68c8c651733e345e52ac1e60",
          "name": "Qingyu Yin",
          "hidden": false
        },
        {
          "_id": "68c8c651733e345e52ac1e61",
          "name": "Zhan Shi",
          "hidden": false
        },
        {
          "_id": "68c8c651733e345e52ac1e62",
          "name": "Zixuan Zhang",
          "hidden": false
        },
        {
          "_id": "68c8c651733e345e52ac1e63",
          "name": "Meng Jiang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-14T21:56:35.000Z",
      "submittedOnDailyAt": "2025-09-16T00:38:36.427Z",
      "title": "Learning to Optimize Multi-Objective Alignment Through Dynamic Reward\n  Weighting",
      "submittedOnDailyBy": {
        "_id": "642f742270daaa6e7209a2c8",
        "avatarUrl": "/avatars/746fb840c220329f69a17905ea519322.svg",
        "isPro": false,
        "fullname": "Yining Lu",
        "user": "ylu610",
        "type": "user"
      },
      "summary": "Prior works in multi-objective reinforcement learning typically use linear\nreward scalarization with fixed weights, which provably fail to capture\nnon-convex Pareto fronts and thus yield suboptimal results. This limitation\nbecomes especially critical in online preference alignment for large language\nmodels. Here, stochastic trajectories generated by parameterized policies\ncreate highly non-linear and non-convex mappings from parameters to objectives\nthat no single static weighting scheme can find optimal trade-offs. We address\nthis limitation by introducing dynamic reward weighting, which adaptively\nadjusts reward weights during the online reinforcement learning process. Unlike\nexisting approaches that rely on fixed-weight interpolation, our dynamic\nweighting continuously balances and prioritizes objectives in training,\nfacilitating effective exploration of Pareto fronts in objective space. We\nintroduce two approaches of increasing sophistication and generalizability: (1)\nhypervolume-guided weight adaptation and (2) gradient-based weight\noptimization, offering a versatile toolkit for online multi-objective\nalignment. Our extensive experiments demonstrate their compatibility with\ncommonly used online reinforcement learning algorithms (including GRPO,\nREINFORCE, and RLOO), effectiveness across multiple mathematical reasoning\ndatasets, and applicability to different model families, consistently achieving\nPareto dominant solutions with fewer training steps than fixed-weight linear\nscalarization baselines.",
      "upvotes": 1,
      "discussionId": "68c8c651733e345e52ac1e64",
      "projectPage": "https://yining610.github.io/dynamic-reward-weighting-webpage/",
      "githubRepo": "https://github.com/yining610/dynamic-reward-weighting",
      "ai_summary": "Dynamic reward weighting in multi-objective reinforcement learning adaptively adjusts weights during training to explore Pareto fronts effectively, outperforming fixed-weight scalarization methods.",
      "ai_keywords": [
        "multi-objective reinforcement learning",
        "linear reward scalarization",
        "non-convex Pareto fronts",
        "parameterized policies",
        "dynamic reward weighting",
        "hypervolume-guided weight adaptation",
        "gradient-based weight optimization",
        "GRPO",
        "REINFORCE",
        "RLOO",
        "Pareto dominant solutions"
      ],
      "githubStars": 0
    },
    "publishedAt": "2025-09-14T17:56:35.000Z",
    "title": "Learning to Optimize Multi-Objective Alignment Through Dynamic Reward\n  Weighting",
    "summary": "Prior works in multi-objective reinforcement learning typically use linear\nreward scalarization with fixed weights, which provably fail to capture\nnon-convex Pareto fronts and thus yield suboptimal results. This limitation\nbecomes especially critical in online preference alignment for large language\nmodels. Here, stochastic trajectories generated by parameterized policies\ncreate highly non-linear and non-convex mappings from parameters to objectives\nthat no single static weighting scheme can find optimal trade-offs. We address\nthis limitation by introducing dynamic reward weighting, which adaptively\nadjusts reward weights during the online reinforcement learning process. Unlike\nexisting approaches that rely on fixed-weight interpolation, our dynamic\nweighting continuously balances and prioritizes objectives in training,\nfacilitating effective exploration of Pareto fronts in objective space. We\nintroduce two approaches of increasing sophistication and generalizability: (1)\nhypervolume-guided weight adaptation and (2) gradient-based weight\noptimization, offering a versatile toolkit for online multi-objective\nalignment. Our extensive experiments demonstrate their compatibility with\ncommonly used online reinforcement learning algorithms (including GRPO,\nREINFORCE, and RLOO), effectiveness across multiple mathematical reasoning\ndatasets, and applicability to different model families, consistently achieving\nPareto dominant solutions with fewer training steps than fixed-weight linear\nscalarization baselines.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.11452.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642f742270daaa6e7209a2c8",
      "avatarUrl": "/avatars/746fb840c220329f69a17905ea519322.svg",
      "fullname": "Yining Lu",
      "name": "ylu610",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.11362",
      "authors": [
        {
          "_id": "68c8d175733e345e52ac1eb1",
          "name": "Loka Li",
          "hidden": false
        },
        {
          "_id": "68c8d175733e345e52ac1eb2",
          "name": "Wong Yu Kang",
          "hidden": false
        },
        {
          "_id": "68c8d175733e345e52ac1eb3",
          "name": "Minghao Fu",
          "hidden": false
        },
        {
          "_id": "68c8d175733e345e52ac1eb4",
          "name": "Guangyi Chen",
          "hidden": false
        },
        {
          "_id": "68c8d175733e345e52ac1eb5",
          "name": "Zhenhao Chen",
          "hidden": false
        },
        {
          "_id": "68c8d175733e345e52ac1eb6",
          "name": "Gongxu Luo",
          "hidden": false
        },
        {
          "_id": "68c8d175733e345e52ac1eb7",
          "name": "Yuewen Sun",
          "hidden": false
        },
        {
          "_id": "68c8d175733e345e52ac1eb8",
          "name": "Salman Khan",
          "hidden": false
        },
        {
          "_id": "68c8d175733e345e52ac1eb9",
          "name": "Peter Spirtes",
          "hidden": false
        },
        {
          "_id": "68c8d175733e345e52ac1eba",
          "name": "Kun Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-14T17:30:03.000Z",
      "submittedOnDailyAt": "2025-09-16T01:24:53.457Z",
      "title": "PersonaX: Multimodal Datasets with LLM-Inferred Behavior Traits",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Understanding human behavior traits is central to applications in\nhuman-computer interaction, computational social science, and personalized AI\nsystems. Such understanding often requires integrating multiple modalities to\ncapture nuanced patterns and relationships. However, existing resources rarely\nprovide datasets that combine behavioral descriptors with complementary\nmodalities such as facial attributes and biographical information. To address\nthis gap, we present PersonaX, a curated collection of multimodal datasets\ndesigned to enable comprehensive analysis of public traits across modalities.\nPersonaX consists of (1) CelebPersona, featuring 9444 public figures from\ndiverse occupations, and (2) AthlePersona, covering 4181 professional athletes\nacross 7 major sports leagues. Each dataset includes behavioral trait\nassessments inferred by three high-performing large language models, alongside\nfacial imagery and structured biographical features. We analyze PersonaX at two\ncomplementary levels. First, we abstract high-level trait scores from text\ndescriptions and apply five statistical independence tests to examine their\nrelationships with other modalities. Second, we introduce a novel causal\nrepresentation learning (CRL) framework tailored to multimodal and\nmulti-measurement data, providing theoretical identifiability guarantees.\nExperiments on both synthetic and real-world data demonstrate the effectiveness\nof our approach. By unifying structured and unstructured analysis, PersonaX\nestablishes a foundation for studying LLM-inferred behavioral traits in\nconjunction with visual and biographical attributes, advancing multimodal trait\nanalysis and causal reasoning.",
      "upvotes": 1,
      "discussionId": "68c8d175733e345e52ac1ebb",
      "ai_summary": "PersonaX, a multimodal dataset, combines behavioral traits, facial imagery, and biographical information to enable comprehensive analysis and causal reasoning using large language models.",
      "ai_keywords": [
        "large language models",
        "causal representation learning",
        "multimodal datasets",
        "behavioral traits",
        "facial imagery",
        "biographical information",
        "statistical independence tests",
        "theoretical identifiability guarantees"
      ]
    },
    "publishedAt": "2025-09-14T13:30:03.000Z",
    "title": "PersonaX: Multimodal Datasets with LLM-Inferred Behavior Traits",
    "summary": "Understanding human behavior traits is central to applications in\nhuman-computer interaction, computational social science, and personalized AI\nsystems. Such understanding often requires integrating multiple modalities to\ncapture nuanced patterns and relationships. However, existing resources rarely\nprovide datasets that combine behavioral descriptors with complementary\nmodalities such as facial attributes and biographical information. To address\nthis gap, we present PersonaX, a curated collection of multimodal datasets\ndesigned to enable comprehensive analysis of public traits across modalities.\nPersonaX consists of (1) CelebPersona, featuring 9444 public figures from\ndiverse occupations, and (2) AthlePersona, covering 4181 professional athletes\nacross 7 major sports leagues. Each dataset includes behavioral trait\nassessments inferred by three high-performing large language models, alongside\nfacial imagery and structured biographical features. We analyze PersonaX at two\ncomplementary levels. First, we abstract high-level trait scores from text\ndescriptions and apply five statistical independence tests to examine their\nrelationships with other modalities. Second, we introduce a novel causal\nrepresentation learning (CRL) framework tailored to multimodal and\nmulti-measurement data, providing theoretical identifiability guarantees.\nExperiments on both synthetic and real-world data demonstrate the effectiveness\nof our approach. By unifying structured and unstructured analysis, PersonaX\nestablishes a foundation for studying LLM-inferred behavioral traits in\nconjunction with visual and biographical attributes, advancing multimodal trait\nanalysis and causal reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.11362.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 104
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.09658",
      "authors": [
        {
          "_id": "68c8ccf3733e345e52ac1e73",
          "name": "Bingkui Tong",
          "hidden": false
        },
        {
          "_id": "68c8ccf3733e345e52ac1e74",
          "name": "Jiaer Xia",
          "hidden": false
        },
        {
          "_id": "68c8ccf3733e345e52ac1e75",
          "name": "Sifeng Shang",
          "hidden": false
        },
        {
          "_id": "68c8ccf3733e345e52ac1e76",
          "name": "Kaiyang Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-11T17:54:00.000Z",
      "submittedOnDailyAt": "2025-09-16T01:07:13.664Z",
      "title": "Measuring Epistemic Humility in Multimodal Large Language Models",
      "submittedOnDailyBy": {
        "_id": "62ac6656de8bfbb93094b8fd",
        "avatarUrl": "/avatars/1d8f445b6d5f9d43ddab81056bcb141e.svg",
        "isPro": false,
        "fullname": "Kaiyang Zhou",
        "user": "kaiyangzhou",
        "type": "user"
      },
      "summary": "Hallucinations in multimodal large language models (MLLMs) -- where the model\ngenerates content inconsistent with the input image -- pose significant risks\nin real-world applications, from misinformation in visual question answering to\nunsafe errors in decision-making. Existing benchmarks primarily test\nrecognition accuracy, i.e., evaluating whether models can select the correct\nanswer among distractors. This overlooks an equally critical capability for\ntrustworthy AI: recognizing when none of the provided options are correct, a\nbehavior reflecting epistemic humility. We present HumbleBench, a new\nhallucination benchmark designed to evaluate MLLMs' ability to reject plausible\nbut incorrect answers across three hallucination types: object, relation, and\nattribute. Built from a panoptic scene graph dataset, we leverage fine-grained\nscene graph annotations to extract ground-truth entities and relations, and\nprompt GPT-4-Turbo to generate multiple-choice questions, followed by a\nrigorous manual filtering process. Each question includes a \"None of the above\"\noption, requiring models not only to recognize correct visual information but\nalso to identify when no provided answer is valid. We evaluate a variety of\nstate-of-the-art MLLMs -- including both general-purpose and specialized\nreasoning models -- on HumbleBench and share valuable findings and insights\nwith the community. By incorporating explicit false-option rejection,\nHumbleBench fills a key gap in current evaluation suites, providing a more\nrealistic measure of MLLM reliability in safety-critical settings. Our code and\ndataset are released publicly and can be accessed at\nhttps://github.com/maifoundations/HumbleBench.",
      "upvotes": 1,
      "discussionId": "68c8ccf4733e345e52ac1e77",
      "githubRepo": "https://github.com/maifoundations/HumbleBench",
      "ai_summary": "HumbleBench evaluates multimodal large language models' ability to reject incorrect answers, addressing the issue of hallucinations in visual question answering and decision-making.",
      "ai_keywords": [
        "multimodal large language models",
        "hallucinations",
        "visual question answering",
        "epistemic humility",
        "panoptic scene graph dataset",
        "fine-grained scene graph annotations",
        "multiple-choice questions",
        "false-option rejection"
      ],
      "githubStars": 6
    },
    "publishedAt": "2025-09-11T13:54:00.000Z",
    "title": "Measuring Epistemic Humility in Multimodal Large Language Models",
    "summary": "Hallucinations in multimodal large language models (MLLMs) -- where the model\ngenerates content inconsistent with the input image -- pose significant risks\nin real-world applications, from misinformation in visual question answering to\nunsafe errors in decision-making. Existing benchmarks primarily test\nrecognition accuracy, i.e., evaluating whether models can select the correct\nanswer among distractors. This overlooks an equally critical capability for\ntrustworthy AI: recognizing when none of the provided options are correct, a\nbehavior reflecting epistemic humility. We present HumbleBench, a new\nhallucination benchmark designed to evaluate MLLMs' ability to reject plausible\nbut incorrect answers across three hallucination types: object, relation, and\nattribute. Built from a panoptic scene graph dataset, we leverage fine-grained\nscene graph annotations to extract ground-truth entities and relations, and\nprompt GPT-4-Turbo to generate multiple-choice questions, followed by a\nrigorous manual filtering process. Each question includes a \"None of the above\"\noption, requiring models not only to recognize correct visual information but\nalso to identify when no provided answer is valid. We evaluate a variety of\nstate-of-the-art MLLMs -- including both general-purpose and specialized\nreasoning models -- on HumbleBench and share valuable findings and insights\nwith the community. By incorporating explicit false-option rejection,\nHumbleBench fills a key gap in current evaluation suites, providing a more\nrealistic measure of MLLM reliability in safety-critical settings. Our code and\ndataset are released publicly and can be accessed at\nhttps://github.com/maifoundations/HumbleBench.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.09658.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62ac6656de8bfbb93094b8fd",
      "avatarUrl": "/avatars/1d8f445b6d5f9d43ddab81056bcb141e.svg",
      "fullname": "Kaiyang Zhou",
      "name": "kaiyangzhou",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.10844",
      "authors": [
        {
          "_id": "68c8bedf733e345e52ac1e34",
          "name": "Yixuan Tang",
          "hidden": false
        },
        {
          "_id": "68c8bedf733e345e52ac1e35",
          "name": "Yi Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-13T15:03:37.000Z",
      "submittedOnDailyAt": "2025-09-16T01:28:34.281Z",
      "title": "GAPrune: Gradient-Alignment Pruning for Domain-Aware Embeddings",
      "submittedOnDailyBy": {
        "_id": "647d834618274bce03013cc2",
        "avatarUrl": "/avatars/a95c7df96dc4fb6a96193f6dd5068227.svg",
        "isPro": true,
        "fullname": "yixuan",
        "user": "yixuantt",
        "type": "user"
      },
      "summary": "Domain-specific embedding models have shown promise for applications that\nrequire specialized semantic understanding, such as coding agents and financial\nretrieval systems, often achieving higher performance gains than general\nmodels. However, state-of-the-art embedding models are typically based on LLMs,\nwhich contain billions of parameters, making deployment challenging in\nresource-constrained environments. Model compression through pruning offers a\npromising solution, but existing pruning methods treat all parameters\nuniformly, failing to distinguish between general semantic representations and\ndomain-specific patterns, leading to suboptimal pruning decisions. Thus, we\npropose GAPrune, a pruning framework that addresses this challenge by\nconsidering both domain importance and preserving general linguistic\nfoundation. Our method uses Fisher Information to measure importance and\ngeneral-domain gradient alignment to assess parameter behavior, then combines\nthese signals using our Domain Alignment Importance (DAI) scoring. Lower DAI\nscores indicate that the parameter is either less important for the domain task\nor creates conflicts between domain and general objectives. Experiments on two\ndomain benchmarks, FinMTEB and ChemTEB, show that GAPrune maintains performance\nwithin 2.5% of dense models in one-shot pruning at 50% sparsity, while\noutperforming all baselines. With retraining in 100 steps, GAPrune achieves\n+4.51% improvement on FinMTEB and +1.73% on ChemTEB, demonstrating that our\npruning strategy not only preserves but enhances domain-specific capabilities.\nOur findings demonstrate that principled pruning strategies can achieve model\ncompression and enhanced domain specialization, providing the research\ncommunity with a new approach for development.",
      "upvotes": 0,
      "discussionId": "68c8bedf733e345e52ac1e36",
      "projectPage": "https://github.com/yixuantt/GAPrune",
      "githubRepo": "https://github.com/yixuantt/GAPrune",
      "ai_summary": "GAPrune, a pruning framework that considers domain importance and general linguistic foundation, effectively compresses models while maintaining and enhancing domain-specific performance.",
      "ai_keywords": [
        "LLMs",
        "pruning",
        "Fisher Information",
        "gradient alignment",
        "Domain Alignment Importance (DAI)",
        "FinMTEB",
        "ChemTEB",
        "one-shot pruning",
        "retraining"
      ],
      "githubStars": 0
    },
    "publishedAt": "2025-09-13T11:03:37.000Z",
    "title": "GAPrune: Gradient-Alignment Pruning for Domain-Aware Embeddings",
    "summary": "Domain-specific embedding models have shown promise for applications that\nrequire specialized semantic understanding, such as coding agents and financial\nretrieval systems, often achieving higher performance gains than general\nmodels. However, state-of-the-art embedding models are typically based on LLMs,\nwhich contain billions of parameters, making deployment challenging in\nresource-constrained environments. Model compression through pruning offers a\npromising solution, but existing pruning methods treat all parameters\nuniformly, failing to distinguish between general semantic representations and\ndomain-specific patterns, leading to suboptimal pruning decisions. Thus, we\npropose GAPrune, a pruning framework that addresses this challenge by\nconsidering both domain importance and preserving general linguistic\nfoundation. Our method uses Fisher Information to measure importance and\ngeneral-domain gradient alignment to assess parameter behavior, then combines\nthese signals using our Domain Alignment Importance (DAI) scoring. Lower DAI\nscores indicate that the parameter is either less important for the domain task\nor creates conflicts between domain and general objectives. Experiments on two\ndomain benchmarks, FinMTEB and ChemTEB, show that GAPrune maintains performance\nwithin 2.5% of dense models in one-shot pruning at 50% sparsity, while\noutperforming all baselines. With retraining in 100 steps, GAPrune achieves\n+4.51% improvement on FinMTEB and +1.73% on ChemTEB, demonstrating that our\npruning strategy not only preserves but enhances domain-specific capabilities.\nOur findings demonstrate that principled pruning strategies can achieve model\ncompression and enhanced domain specialization, providing the research\ncommunity with a new approach for development.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.10844.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647d834618274bce03013cc2",
      "avatarUrl": "/avatars/a95c7df96dc4fb6a96193f6dd5068227.svg",
      "fullname": "yixuan",
      "name": "yixuantt",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  }
]