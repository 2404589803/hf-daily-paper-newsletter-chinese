[
    "{'paper': {'id': '2412.18547', 'authors': [{'_id': '676c40d619a21c8b928d13c2', 'name': 'Tingxu Han', 'hidden': False}, {'_id': '676c40d619a21c8b928d13c3', 'name': 'Chunrong Fang', 'hidden': False}, {'_id': '676c40d619a21c8b928d13c4', 'name': 'Shiyu Zhao', 'hidden': False}, {'_id': '676c40d619a21c8b928d13c5', 'name': 'Shiqing Ma', 'hidden': False}, {'_id': '676c40d619a21c8b928d13c6', 'name': 'Zhenyu Chen', 'hidden': False}, {'_id': '676c40d619a21c8b928d13c7', 'name': 'Zhenting Wang', 'hidden': False}], 'publishedAt': '2024-12-24T16:55:45.000Z', 'title': 'Token-Budget-Aware LLM Reasoning', 'summary': 'Reasoning is critical for large language models (LLMs) to excel in a wide\\nrange of tasks. While methods like Chain-of-Thought (CoT) reasoning enhance LLM\\nperformance by decomposing problems into intermediate steps, they also incur\\nsignificant overhead in token usage, leading to increased costs. We find that\\nthe reasoning process of current LLMs is unnecessarily lengthy and it can be\\ncompressed by including a reasonable token budget in the prompt, but the choice\\nof token budget plays a crucial role in the actual compression effectiveness.\\nWe then propose a token-budget-aware LLM reasoning framework, which dynamically\\nestimates token budgets for different problems based on reasoning complexity\\nand uses the estimated token budgets to guide the reasoning process.\\nExperiments show that our method effectively reduces token costs in CoT\\nreasoning with only a slight performance reduction, offering a practical\\nsolution to balance efficiency and accuracy in LLM reasoning. Code:\\nhttps://github.com/GeniusHTX/TALE.', 'upvotes': 10, 'discussionId': '676c40d719a21c8b928d13ea'}, 'publishedAt': '2024-12-25T22:21:50.545Z', 'title': 'Token-Budget-Aware LLM Reasoning', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.18547.png', 'numComments': 1, 'submittedBy': {'_id': '64dfcc62e8b6f3f3baa950e0', 'avatarUrl': '/avatars/21bbff67d46c08044efe2406575aa77e.svg', 'fullname': 'Zhenting Wang', 'name': 'ztwang', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2412.18609', 'authors': [{'_id': '676d854b8771d55751ee0f4f', 'name': 'Jinhui Yi', 'hidden': False}, {'_id': '676d854b8771d55751ee0f50', 'name': 'Syed Talal Wasim', 'hidden': False}, {'_id': '676d854b8771d55751ee0f51', 'name': 'Yanan Luo', 'hidden': False}, {'_id': '676d854b8771d55751ee0f52', 'name': 'Muzammal Naseer', 'hidden': False}, {'_id': '676d854b8771d55751ee0f53', 'name': 'Juergen Gall', 'hidden': False}], 'publishedAt': '2024-12-24T18:59:56.000Z', 'title': 'Video-Panda: Parameter-efficient Alignment for Encoder-free\\n  Video-Language Models', 'summary': \"We present an efficient encoder-free approach for video-language\\nunderstanding that achieves competitive performance while significantly\\nreducing computational overhead. Current video-language models typically rely\\non heavyweight image encoders (300M-1.1B parameters) or video encoders (1B-1.4B\\nparameters), creating a substantial computational burden when processing\\nmulti-frame videos. Our method introduces a novel Spatio-Temporal Alignment\\nBlock (STAB) that directly processes video inputs without requiring pre-trained\\nencoders while using only 45M parameters for visual processing - at least a\\n6.5times reduction compared to traditional approaches. The STAB architecture\\ncombines Local Spatio-Temporal Encoding for fine-grained feature extraction,\\nefficient spatial downsampling through learned attention and separate\\nmechanisms for modeling frame-level and video-level relationships. Our model\\nachieves comparable or superior performance to encoder-based approaches for\\nopen-ended video question answering on standard benchmarks. The fine-grained\\nvideo question-answering evaluation demonstrates our model's effectiveness,\\noutperforming the encoder-based approaches Video-ChatGPT and Video-LLaVA in key\\naspects like correctness and temporal understanding. Extensive ablation studies\\nvalidate our architectural choices and demonstrate the effectiveness of our\\nspatio-temporal modeling approach while achieving 3-4times faster processing\\nspeeds than previous methods. Code is available at\\nhttps://github.com/jh-yi/Video-Panda.\", 'upvotes': 0, 'discussionId': '676d854e8771d55751ee108f'}, 'publishedAt': '2024-12-26T11:33:39.769Z', 'title': 'Video-Panda: Parameter-efficient Alignment for Encoder-free Video-Language Models', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.18609.png', 'numComments': 1, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isMod': False, 'followerCount': 5466}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2412.18319', 'authors': [{'_id': '676d843d92c4a8fe49532793', 'name': 'Huanjin Yao', 'hidden': False}, {'_id': '676d843d92c4a8fe49532794', 'name': 'Jiaxing Huang', 'hidden': False}, {'_id': '676d843d92c4a8fe49532795', 'name': 'Wenhao Wu', 'hidden': False}, {'_id': '676d843d92c4a8fe49532796', 'name': 'Jingyi Zhang', 'hidden': False}, {'_id': '676d843d92c4a8fe49532797', 'name': 'Yibo Wang', 'hidden': False}, {'_id': '676d843d92c4a8fe49532798', 'name': 'Shunyu Liu', 'hidden': False}, {'_id': '676d843d92c4a8fe49532799', 'name': 'Yingjie Wang', 'hidden': False}, {'_id': '676d843d92c4a8fe4953279a', 'name': 'Yuxin Song', 'hidden': False}, {'_id': '676d843d92c4a8fe4953279b', 'name': 'Haocheng Feng', 'hidden': False}, {'_id': '676d843d92c4a8fe4953279c', 'name': 'Li Shen', 'hidden': False}, {'_id': '676d843d92c4a8fe4953279d', 'name': 'Dacheng Tao', 'hidden': False}], 'publishedAt': '2024-12-24T10:07:51.000Z', 'title': 'Mulberry: Empowering MLLM with o1-like Reasoning and Reflection via\\n  Collective Monte Carlo Tree Search', 'summary': \"In this work, we aim to develop an MLLM that understands and solves questions\\nby learning to create each intermediate step of the reasoning involved till the\\nfinal answer. To this end, we propose Collective Monte Carlo Tree Search\\n(CoMCTS), a new learning-to-reason method for MLLMs, which introduces the\\nconcept of collective learning into ``tree search'' for effective and efficient\\nreasoning-path searching and learning. The core idea of CoMCTS is to leverage\\ncollective knowledge from multiple models to collaboratively conjecture, search\\nand identify effective reasoning paths toward correct answers via four\\niterative operations including Expansion, Simulation and Error Positioning,\\nBackpropagation, and Selection. Using CoMCTS, we construct Mulberry-260k, a\\nmultimodal dataset with a tree of rich, explicit and well-defined reasoning\\nnodes for each question. With Mulberry-260k, we perform collective SFT to train\\nour model, Mulberry, a series of MLLMs with o1-like step-by-step Reasoning and\\nReflection capabilities. Extensive experiments demonstrate the superiority of\\nour proposed methods on various benchmarks. Code will be available at\\nhttps://github.com/HJYao00/Mulberry\", 'upvotes': 0, 'discussionId': '676d843e92c4a8fe495328d3'}, 'publishedAt': '2024-12-26T11:29:06.978Z', 'title': 'Mulberry: Empowering MLLM with o1-like Reasoning and Reflection via Collective Monte Carlo Tree Search', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.18319.png', 'numComments': 1, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isMod': False, 'followerCount': 5466}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2412.17780', 'authors': [{'_id': '676d74cd96a84bb36f9d04e1', 'name': 'Sophia Tang', 'hidden': False}, {'_id': '676d74cd96a84bb36f9d04e2', 'name': 'Yinuo Zhang', 'hidden': False}, {'_id': '676d74cd96a84bb36f9d04e3', 'user': {'_id': '64cd5b3f0494187a9e8b7c69', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eo9HiGbvCxHQZ-QJB1Y_o.jpeg', 'isPro': False, 'fullname': 'Pranam Chatterjee', 'user': 'pranamanam', 'type': 'user'}, 'name': 'Pranam Chatterjee', 'status': 'extracted_confirmed', 'statusLastChangedAt': '2024-12-26T16:27:18.389Z', 'hidden': False}], 'publishedAt': '2024-12-23T18:38:49.000Z', 'title': 'PepTune: De Novo Generation of Therapeutic Peptides with\\n  Multi-Objective-Guided Discrete Diffusion', 'summary': 'Peptide therapeutics, a major class of medicines, have achieved remarkable\\nsuccess across diseases such as diabetes and cancer, with landmark examples\\nsuch as GLP-1 receptor agonists revolutionizing the treatment of type-2\\ndiabetes and obesity. Despite their success, designing peptides that satisfy\\nmultiple conflicting objectives, such as target binding affinity, solubility,\\nand membrane permeability, remains a major challenge. Classical drug\\ndevelopment and structure-based design are ineffective for such tasks, as they\\nfail to optimize global functional properties critical for therapeutic\\nefficacy. Existing generative frameworks are largely limited to continuous\\nspaces, unconditioned outputs, or single-objective guidance, making them\\nunsuitable for discrete sequence optimization across multiple properties. To\\naddress this, we present PepTune, a multi-objective discrete diffusion model\\nfor the simultaneous generation and optimization of therapeutic peptide SMILES.\\nBuilt on the Masked Discrete Language Model (MDLM) framework, PepTune ensures\\nvalid peptide structures with state-dependent masking schedules and\\npenalty-based objectives. To guide the diffusion process, we propose a Monte\\nCarlo Tree Search (MCTS)-based strategy that balances exploration and\\nexploitation to iteratively refine Pareto-optimal sequences. MCTS integrates\\nclassifier-based rewards with search-tree expansion, overcoming gradient\\nestimation challenges and data sparsity inherent to discrete spaces. Using\\nPepTune, we generate diverse, chemically-modified peptides optimized for\\nmultiple therapeutic properties, including target binding affinity, membrane\\npermeability, solubility, hemolysis, and non-fouling characteristics on various\\ndisease-relevant targets. In total, our results demonstrate that MCTS-guided\\ndiscrete diffusion is a powerful and modular approach for multi-objective\\nsequence design in discrete state spaces.', 'upvotes': 0, 'discussionId': '676d74d396a84bb36f9d0660'}, 'publishedAt': '2024-12-26T10:26:14.913Z', 'title': 'PepTune: De Novo Generation of Therapeutic Peptides with Multi-Objective-Guided Discrete Diffusion', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/64cd5b3f0494187a9e8b7c69/_ILClx7w2rSaaqw5PjP2i.png'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.17780.png', 'numComments': 1, 'submittedBy': {'_id': '64cd5b3f0494187a9e8b7c69', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eo9HiGbvCxHQZ-QJB1Y_o.jpeg', 'fullname': 'Pranam Chatterjee', 'name': 'pranamanam', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 2}, 'isAuthorParticipating': True}"
]