[
  {
    "paper": {
      "id": "2512.24618",
      "authors": [
        {
          "_id": "6955d543832867f25352555d",
          "name": "Junru Lu",
          "hidden": false
        },
        {
          "_id": "6955d543832867f25352555e",
          "name": "Jiarui Qin",
          "hidden": false
        },
        {
          "_id": "6955d543832867f25352555f",
          "name": "Lingfeng Qiao",
          "hidden": false
        },
        {
          "_id": "6955d543832867f253525560",
          "name": "Yinghui Li",
          "hidden": false
        },
        {
          "_id": "6955d543832867f253525561",
          "name": "Xinyi Dai",
          "hidden": false
        },
        {
          "_id": "6955d543832867f253525562",
          "name": "Bo Ke",
          "hidden": false
        },
        {
          "_id": "6955d543832867f253525563",
          "name": "Jianfeng He",
          "hidden": false
        },
        {
          "_id": "6955d543832867f253525564",
          "name": "Ruizhi Qiao",
          "hidden": false
        },
        {
          "_id": "6955d543832867f253525565",
          "name": "Di Yin",
          "hidden": false
        },
        {
          "_id": "6955d543832867f253525566",
          "name": "Xing Sun",
          "hidden": false
        },
        {
          "_id": "6955d543832867f253525567",
          "name": "Yunsheng Wu",
          "hidden": false
        },
        {
          "_id": "6955d543832867f253525568",
          "name": "Yinsong Liu",
          "hidden": false
        },
        {
          "_id": "6955d543832867f253525569",
          "name": "Shuangyin Liu",
          "hidden": false
        },
        {
          "_id": "6955d543832867f25352556a",
          "name": "Mingkong Tang",
          "hidden": false
        },
        {
          "_id": "6955d543832867f25352556b",
          "name": "Haodong Lin",
          "hidden": false
        },
        {
          "_id": "6955d543832867f25352556c",
          "name": "Jiayi Kuang",
          "hidden": false
        },
        {
          "_id": "6955d543832867f25352556d",
          "name": "Fanxu Meng",
          "hidden": false
        },
        {
          "_id": "6955d543832867f25352556e",
          "name": "Xiaojuan Tang",
          "hidden": false
        },
        {
          "_id": "6955d543832867f25352556f",
          "name": "Yunjia Xi",
          "hidden": false
        },
        {
          "_id": "6955d543832867f253525570",
          "name": "Junjie Huang",
          "hidden": false
        },
        {
          "_id": "6955d543832867f253525571",
          "name": "Haotong Yang",
          "hidden": false
        },
        {
          "_id": "6955d543832867f253525572",
          "name": "Zhenyi Shen",
          "hidden": false
        },
        {
          "_id": "6955d543832867f253525573",
          "name": "Yangning Li",
          "hidden": false
        },
        {
          "_id": "6955d543832867f253525574",
          "name": "Qianwen Zhang",
          "hidden": false
        },
        {
          "_id": "6955d543832867f253525575",
          "name": "Yifei Yu",
          "hidden": false
        },
        {
          "_id": "6955d543832867f253525576",
          "name": "Siyu An",
          "hidden": false
        },
        {
          "_id": "6955d543832867f253525577",
          "name": "Junnan Dong",
          "hidden": false
        },
        {
          "_id": "6955d543832867f253525578",
          "name": "Qiufeng Wang",
          "hidden": false
        },
        {
          "_id": "6955d543832867f253525579",
          "name": "Jie Wang",
          "hidden": false
        },
        {
          "_id": "6955d543832867f25352557a",
          "name": "Keyu Chen",
          "hidden": false
        },
        {
          "_id": "6955d543832867f25352557b",
          "name": "Wei Wen",
          "hidden": false
        },
        {
          "_id": "6955d543832867f25352557c",
          "name": "Taian Guo",
          "hidden": false
        },
        {
          "_id": "6955d543832867f25352557d",
          "name": "Zhifeng Shen",
          "hidden": false
        },
        {
          "_id": "6955d543832867f25352557e",
          "name": "Daohai Yu",
          "hidden": false
        },
        {
          "_id": "6955d543832867f25352557f",
          "name": "Jiahao Li",
          "hidden": false
        },
        {
          "_id": "6955d543832867f253525580",
          "name": "Ke Li",
          "hidden": false
        },
        {
          "_id": "6955d543832867f253525581",
          "name": "Zongyi Li",
          "hidden": false
        },
        {
          "_id": "6955d543832867f253525582",
          "name": "Xiaoyu Tan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-31T04:25:11.000Z",
      "submittedOnDailyAt": "2026-01-01T00:33:09.720Z",
      "title": "Youtu-LLM: Unlocking the Native Agentic Potential for Lightweight Large Language Models",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We introduce Youtu-LLM, a lightweight yet powerful language model that harmonizes high computational efficiency with native agentic intelligence. Unlike typical small models that rely on distillation, Youtu-LLM (1.96B) is pre-trained from scratch to systematically cultivate reasoning and planning capabilities. The key technical advancements are as follows: (1) Compact Architecture with Long-Context Support: Built on a dense Multi-Latent Attention (MLA) architecture with a novel STEM-oriented vocabulary, Youtu-LLM supports a 128k context window. This design enables robust long-context reasoning and state tracking within a minimal memory footprint, making it ideal for long-horizon agent and reasoning tasks. (2) Principled \"Commonsense-STEM-Agent\" Curriculum: We curated a massive corpus of approximately 11T tokens and implemented a multi-stage training strategy. By progressively shifting the pre-training data distribution from general commonsense to complex STEM and agentic tasks, we ensure the model acquires deep cognitive abilities rather than superficial alignment. (3) Scalable Agentic Mid-training: Specifically for the agentic mid-training, we employ diverse data construction schemes to synthesize rich and varied trajectories across math, coding, and tool-use domains. This high-quality data enables the model to internalize planning and reflection behaviors effectively. Extensive evaluations show that Youtu-LLM sets a new state-of-the-art for sub-2B LLMs. On general benchmarks, it achieves competitive performance against larger models, while on agent-specific tasks, it significantly surpasses existing SOTA baselines, demonstrating that lightweight models can possess strong intrinsic agentic capabilities.",
      "upvotes": 28,
      "discussionId": "6955d543832867f253525583",
      "ai_summary": "Youtu-LLM is a lightweight language model optimized for computational efficiency and agentic intelligence through a compact architecture, STEM-focused training curriculum, and scalable mid-training strategies for planning and reasoning tasks.",
      "ai_keywords": [
        "Multi-Latent Attention (MLA) architecture",
        "STEM-oriented vocabulary",
        "128k context window",
        "Commonsense-STEM-Agent Curriculum",
        "multi-stage training strategy",
        "agentic mid-training",
        "data construction schemes",
        "planning and reflection behaviors",
        "long-context reasoning",
        "state tracking",
        "agentic capabilities"
      ]
    },
    "publishedAt": "2025-12-30T23:25:11.000Z",
    "title": "Youtu-LLM: Unlocking the Native Agentic Potential for Lightweight Large Language Models",
    "summary": "We introduce Youtu-LLM, a lightweight yet powerful language model that harmonizes high computational efficiency with native agentic intelligence. Unlike typical small models that rely on distillation, Youtu-LLM (1.96B) is pre-trained from scratch to systematically cultivate reasoning and planning capabilities. The key technical advancements are as follows: (1) Compact Architecture with Long-Context Support: Built on a dense Multi-Latent Attention (MLA) architecture with a novel STEM-oriented vocabulary, Youtu-LLM supports a 128k context window. This design enables robust long-context reasoning and state tracking within a minimal memory footprint, making it ideal for long-horizon agent and reasoning tasks. (2) Principled \"Commonsense-STEM-Agent\" Curriculum: We curated a massive corpus of approximately 11T tokens and implemented a multi-stage training strategy. By progressively shifting the pre-training data distribution from general commonsense to complex STEM and agentic tasks, we ensure the model acquires deep cognitive abilities rather than superficial alignment. (3) Scalable Agentic Mid-training: Specifically for the agentic mid-training, we employ diverse data construction schemes to synthesize rich and varied trajectories across math, coding, and tool-use domains. This high-quality data enables the model to internalize planning and reflection behaviors effectively. Extensive evaluations show that Youtu-LLM sets a new state-of-the-art for sub-2B LLMs. On general benchmarks, it achieves competitive performance against larger models, while on agent-specific tasks, it significantly surpasses existing SOTA baselines, demonstrating that lightweight models can possess strong intrinsic agentic capabilities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24618.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 197
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.24210",
      "authors": [
        {
          "_id": "6955e1ac832867f25352559b",
          "name": "Ruoshi Wen",
          "hidden": false
        },
        {
          "_id": "6955e1ac832867f25352559c",
          "name": "Guangzeng Chen",
          "hidden": false
        },
        {
          "_id": "6955e1ac832867f25352559d",
          "name": "Zhongren Cui",
          "hidden": false
        },
        {
          "_id": "6955e1ac832867f25352559e",
          "name": "Min Du",
          "hidden": false
        },
        {
          "_id": "6955e1ac832867f25352559f",
          "name": "Yang Gou",
          "hidden": false
        },
        {
          "_id": "6955e1ac832867f2535255a0",
          "name": "Zhigang Han",
          "hidden": false
        },
        {
          "_id": "6955e1ac832867f2535255a1",
          "name": "Liqun Huang",
          "hidden": false
        },
        {
          "_id": "6955e1ac832867f2535255a2",
          "name": "Mingyu Lei",
          "hidden": false
        },
        {
          "_id": "6955e1ac832867f2535255a3",
          "name": "Yunfei Li",
          "hidden": false
        },
        {
          "_id": "6955e1ac832867f2535255a4",
          "name": "Zhuohang Li",
          "hidden": false
        },
        {
          "_id": "6955e1ac832867f2535255a5",
          "name": "Wenlei Liu",
          "hidden": false
        },
        {
          "_id": "6955e1ac832867f2535255a6",
          "name": "Yuxiao Liu",
          "hidden": false
        },
        {
          "_id": "6955e1ac832867f2535255a7",
          "name": "Xiao Ma",
          "hidden": false
        },
        {
          "_id": "6955e1ac832867f2535255a8",
          "name": "Hao Niu",
          "hidden": false
        },
        {
          "_id": "6955e1ac832867f2535255a9",
          "name": "Yutao Ouyang",
          "hidden": false
        },
        {
          "_id": "6955e1ac832867f2535255aa",
          "name": "Zeyu Ren",
          "hidden": false
        },
        {
          "_id": "6955e1ac832867f2535255ab",
          "name": "Haixin Shi",
          "hidden": false
        },
        {
          "_id": "6955e1ac832867f2535255ac",
          "name": "Wei Xu",
          "hidden": false
        },
        {
          "_id": "6955e1ac832867f2535255ad",
          "name": "Haoxiang Zhang",
          "hidden": false
        },
        {
          "_id": "6955e1ac832867f2535255ae",
          "name": "Jiajun Zhang",
          "hidden": false
        },
        {
          "_id": "6955e1ac832867f2535255af",
          "name": "Xiao Zhang",
          "hidden": false
        },
        {
          "_id": "6955e1ac832867f2535255b0",
          "name": "Liwei Zheng",
          "hidden": false
        },
        {
          "_id": "6955e1ac832867f2535255b1",
          "name": "Weiheng Zhong",
          "hidden": false
        },
        {
          "_id": "6955e1ac832867f2535255b2",
          "name": "Yifei Zhou",
          "hidden": false
        },
        {
          "_id": "6955e1ac832867f2535255b3",
          "name": "Zhengming Zhu",
          "hidden": false
        },
        {
          "_id": "6955e1ac832867f2535255b4",
          "name": "Hang Li",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6478597d91398856110a6738/xYPPHmiwlBKZ8t6hU2WKm.mp4"
      ],
      "publishedAt": "2025-12-30T13:22:16.000Z",
      "submittedOnDailyAt": "2026-01-01T00:44:36.055Z",
      "title": "GR-Dexter Technical Report",
      "submittedOnDailyBy": {
        "_id": "6478597d91398856110a6738",
        "avatarUrl": "/avatars/c3bc61eb7554ac21946b424e1314f1a7.svg",
        "isPro": false,
        "fullname": "Xiao Ma",
        "user": "yusufma555",
        "type": "user"
      },
      "summary": "Vision-language-action (VLA) models have enabled language-conditioned, long-horizon robot manipulation, but most existing systems are limited to grippers. Scaling VLA policies to bimanual robots with high degree-of-freedom (DoF) dexterous hands remains challenging due to the expanded action space, frequent hand-object occlusions, and the cost of collecting real-robot data. We present GR-Dexter, a holistic hardware-model-data framework for VLA-based generalist manipulation on a bimanual dexterous-hand robot. Our approach combines the design of a compact 21-DoF robotic hand, an intuitive bimanual teleoperation system for real-robot data collection, and a training recipe that leverages teleoperated robot trajectories together with large-scale vision-language and carefully curated cross-embodiment datasets. Across real-world evaluations spanning long-horizon everyday manipulation and generalizable pick-and-place, GR-Dexter achieves strong in-domain performance and improved robustness to unseen objects and unseen instructions. We hope GR-Dexter serves as a practical step toward generalist dexterous-hand robotic manipulation.",
      "upvotes": 7,
      "discussionId": "6955e1ad832867f2535255b5",
      "projectPage": "https://byte-dexter.github.io/gr-dexter/",
      "ai_summary": "GR-Dexter introduces a hardware-model-data framework for bimanual dexterous-hand robot manipulation using vision-language-action models, combining teleoperation data and multimodal datasets to achieve robust generalization.",
      "ai_keywords": [
        "Vision-language-action (VLA) models",
        "training recipe",
        "large-scale vision-language datasets",
        "cross-embodiment datasets"
      ],
      "organization": {
        "_id": "67d1140985ea0644e2f14b99",
        "name": "ByteDance-Seed",
        "fullname": "ByteDance Seed",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
      }
    },
    "publishedAt": "2025-12-30T08:22:16.000Z",
    "title": "GR-Dexter Technical Report",
    "summary": "Vision-language-action (VLA) models have enabled language-conditioned, long-horizon robot manipulation, but most existing systems are limited to grippers. Scaling VLA policies to bimanual robots with high degree-of-freedom (DoF) dexterous hands remains challenging due to the expanded action space, frequent hand-object occlusions, and the cost of collecting real-robot data. We present GR-Dexter, a holistic hardware-model-data framework for VLA-based generalist manipulation on a bimanual dexterous-hand robot. Our approach combines the design of a compact 21-DoF robotic hand, an intuitive bimanual teleoperation system for real-robot data collection, and a training recipe that leverages teleoperated robot trajectories together with large-scale vision-language and carefully curated cross-embodiment datasets. Across real-world evaluations spanning long-horizon everyday manipulation and generalizable pick-and-place, GR-Dexter achieves strong in-domain performance and improved robustness to unseen objects and unseen instructions. We hope GR-Dexter serves as a practical step toward generalist dexterous-hand robotic manipulation.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6478597d91398856110a6738/xYPPHmiwlBKZ8t6hU2WKm.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24210.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6478597d91398856110a6738",
      "avatarUrl": "/avatars/c3bc61eb7554ac21946b424e1314f1a7.svg",
      "fullname": "Xiao Ma",
      "name": "yusufma555",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "organization": {
      "_id": "67d1140985ea0644e2f14b99",
      "name": "ByteDance-Seed",
      "fullname": "ByteDance Seed",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.24873",
      "authors": [
        {
          "_id": "6955e3f8832867f2535255cd",
          "name": "Weixun Wang",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f2535255ce",
          "name": "XiaoXiao Xu",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f2535255cf",
          "name": "Wanhe An",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f2535255d0",
          "name": "Fangwen Dai",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f2535255d1",
          "name": "Wei Gao",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f2535255d2",
          "name": "Yancheng He",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f2535255d3",
          "name": "Ju Huang",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f2535255d4",
          "name": "Qiang Ji",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f2535255d5",
          "name": "Hanqi Jin",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f2535255d6",
          "name": "Xiaoyang Li",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f2535255d7",
          "name": "Yang Li",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f2535255d8",
          "name": "Zhongwen Li",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f2535255d9",
          "name": "Shirong Lin",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f2535255da",
          "name": "Jiashun Liu",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f2535255db",
          "name": "Zenan Liu",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f2535255dc",
          "name": "Tao Luo",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f2535255dd",
          "name": "Dilxat Muhtar",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f2535255de",
          "name": "Yuanbin Qu",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f2535255df",
          "name": "Jiaqiang Shi",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f2535255e0",
          "name": "Qinghui Sun",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f2535255e1",
          "name": "Yingshui Tan",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f2535255e2",
          "name": "Hao Tang",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f2535255e3",
          "name": "Runze Wang",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f2535255e4",
          "name": "Yi Wang",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f2535255e5",
          "name": "Zhaoguo Wang",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f2535255e6",
          "name": "Yanan Wu",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f2535255e7",
          "name": "Shaopan Xiong",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f2535255e8",
          "name": "Binchen Xu",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f2535255e9",
          "name": "Xander Xu",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f2535255ea",
          "name": "Yuchi Xu",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f2535255eb",
          "name": "Qipeng Zhang",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f2535255ec",
          "name": "Xixia Zhang",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f2535255ed",
          "name": "Haizhou Zhao",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f2535255ee",
          "name": "Jie Zhao",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f2535255ef",
          "name": "Shuaibing Zhao",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f2535255f0",
          "name": "Baihui Zheng",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f2535255f1",
          "name": "Jianhui Zheng",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f2535255f2",
          "name": "Suhang Zheng",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f2535255f3",
          "name": "Yanni Zhu",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f2535255f4",
          "name": "Mengze Cai",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f2535255f5",
          "name": "Kerui Cao",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f2535255f6",
          "name": "Xitong Chen",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f2535255f7",
          "name": "Yue Dai",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f2535255f8",
          "name": "Lifan Du",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f2535255f9",
          "name": "Tao Feng",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f2535255fa",
          "name": "Tao He",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f2535255fb",
          "name": "Jin Hu",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f2535255fc",
          "name": "Yijie Hu",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f2535255fd",
          "name": "Ziyu Jiang",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f2535255fe",
          "name": "Cheng Li",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f2535255ff",
          "name": "Xiang Li",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f253525600",
          "name": "Jing Liang",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f253525601",
          "name": "Chonghuan Liu",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f253525602",
          "name": "ZhenDong Liu",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f253525603",
          "name": "Haodong Mi",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f253525604",
          "name": "Yanhu Mo",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f253525605",
          "name": "Junjia Ni",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f253525606",
          "name": "Shixin Pei",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f253525607",
          "name": "Jingyu Shen",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f253525608",
          "name": "XiaoShuai Song",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f253525609",
          "name": "Cecilia Wang",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f25352560a",
          "name": "Chaofan Wang",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f25352560b",
          "name": "Kangyu Wang",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f25352560c",
          "name": "Pei Wang",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f25352560d",
          "name": "Tao Wang",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f25352560e",
          "name": "Wei Wang",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f25352560f",
          "name": "Ke Xiao",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f253525610",
          "name": "Mingyu Xu",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f253525611",
          "name": "Tiange Xu",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f253525612",
          "name": "Nan Ya",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f253525613",
          "name": "Siran Yang",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f253525614",
          "name": "Jianan Ye",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f253525615",
          "name": "Yaxing Zang",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f253525616",
          "name": "Duo Zhang",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f253525617",
          "name": "Junbo Zhang",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f253525618",
          "name": "Boren Zheng",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f253525619",
          "name": "Wanxi Deng",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f25352561a",
          "name": "Ling Pan",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f25352561b",
          "name": "Lin Qu",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f25352561c",
          "name": "Wenbo Su",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f25352561d",
          "name": "Jiamang Wang",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f25352561e",
          "name": "Wei Wang",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f25352561f",
          "name": "Hu Wei",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f253525620",
          "name": "Minggang Wu",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f253525621",
          "name": "Cheng Yu",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f253525622",
          "name": "Bing Zhao",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f253525623",
          "name": "Zhicheng Zheng",
          "hidden": false
        },
        {
          "_id": "6955e3f8832867f253525624",
          "name": "Bo Zheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-31T14:03:39.000Z",
      "submittedOnDailyAt": "2026-01-01T00:33:23.374Z",
      "title": "Let It Flow: Agentic Crafting on Rock and Roll, Building the ROME Model within an Open Agentic Learning Ecosystem",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Agentic crafting requires LLMs to operate in real-world environments over multiple turns by taking actions, observing outcomes, and iteratively refining artifacts. Despite its importance, the open-source community lacks a principled, end-to-end ecosystem to streamline agent development. We introduce the Agentic Learning Ecosystem (ALE), a foundational infrastructure that optimizes the production pipeline for agent LLMs. ALE consists of three components: ROLL, a post-training framework for weight optimization; ROCK, a sandbox environment manager for trajectory generation; and iFlow CLI, an agent framework for efficient context engineering. We release ROME (ROME is Obviously an Agentic Model), an open-source agent grounded by ALE and trained on over one million trajectories. Our approach includes data composition protocols for synthesizing complex behaviors and a novel policy optimization algorithm, Interaction-based Policy Alignment (IPA), which assigns credit over semantic interaction chunks rather than individual tokens to improve long-horizon training stability. Empirically, we evaluate ROME within a structured setting and introduce Terminal Bench Pro, a benchmark with improved scale and contamination control. ROME demonstrates strong performance across benchmarks like SWE-bench Verified and Terminal Bench, proving the effectiveness of the ALE infrastructure.",
      "upvotes": 6,
      "discussionId": "6955e3f8832867f253525625",
      "ai_summary": "The Agentic Learning Ecosystem (ALE) introduces a principled infrastructure for agent development, combining post-training optimization, sandbox environments, and policy alignment to enhance long-horizon training stability and performance in real-world tasks.",
      "ai_keywords": [
        "ROLL (post-training framework)",
        "ROCK (sandbox environment manager)",
        "iFlow CLI (agent framework)",
        "ROME (agentic model)",
        "data composition protocols",
        "Interaction-based Policy Alignment (IPA)",
        "semantic interaction chunks",
        "Terminal Bench Pro",
        "SWE-bench Verified"
      ]
    },
    "publishedAt": "2025-12-31T09:03:39.000Z",
    "title": "Let It Flow: Agentic Crafting on Rock and Roll, Building the ROME Model within an Open Agentic Learning Ecosystem",
    "summary": "Agentic crafting requires LLMs to operate in real-world environments over multiple turns by taking actions, observing outcomes, and iteratively refining artifacts. Despite its importance, the open-source community lacks a principled, end-to-end ecosystem to streamline agent development. We introduce the Agentic Learning Ecosystem (ALE), a foundational infrastructure that optimizes the production pipeline for agent LLMs. ALE consists of three components: ROLL, a post-training framework for weight optimization; ROCK, a sandbox environment manager for trajectory generation; and iFlow CLI, an agent framework for efficient context engineering. We release ROME (ROME is Obviously an Agentic Model), an open-source agent grounded by ALE and trained on over one million trajectories. Our approach includes data composition protocols for synthesizing complex behaviors and a novel policy optimization algorithm, Interaction-based Policy Alignment (IPA), which assigns credit over semantic interaction chunks rather than individual tokens to improve long-horizon training stability. Empirically, we evaluate ROME within a structured setting and introduce Terminal Bench Pro, a benchmark with improved scale and contamination control. ROME demonstrates strong performance across benchmarks like SWE-bench Verified and Terminal Bench, proving the effectiveness of the ALE infrastructure.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24873.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 197
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.25070",
      "authors": [
        {
          "_id": "6955e691832867f25352563b",
          "name": "Nikhil Chandak",
          "hidden": false
        },
        {
          "_id": "6955e691832867f25352563c",
          "name": "Shashwat Goel",
          "hidden": false
        },
        {
          "_id": "6955e691832867f25352563d",
          "name": "Ameya Prabhu",
          "hidden": false
        },
        {
          "_id": "6955e691832867f25352563e",
          "name": "Moritz Hardt",
          "hidden": false
        },
        {
          "_id": "6955e691832867f25352563f",
          "name": "Jonas Geiping",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-31T18:59:51.000Z",
      "submittedOnDailyAt": "2026-01-01T00:59:02.810Z",
      "title": "Scaling Open-Ended Reasoning to Predict the Future",
      "submittedOnDailyBy": {
        "_id": "6506832221ac448013f94995",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6506832221ac448013f94995/sVUI1JV4Dxan5l-MqNze4.jpeg",
        "isPro": false,
        "fullname": "Shashwat Goel",
        "user": "shash42",
        "type": "user"
      },
      "summary": "High-stakes decision making involves reasoning under uncertainty about the future. In this work, we train language models to make predictions on open-ended forecasting questions. To scale up training data, we synthesize novel forecasting questions from global events reported in daily news, using a fully automated, careful curation recipe. We train the Qwen3 thinking models on our dataset, OpenForesight. To prevent leakage of future information during training and evaluation, we use an offline news corpus, both for data generation and retrieval in our forecasting system. Guided by a small validation set, we show the benefits of retrieval, and an improved reward function for reinforcement learning (RL). Once we obtain our final forecasting system, we perform held-out testing between May to August 2025. Our specialized model, OpenForecaster 8B, matches much larger proprietary models, with our training improving the accuracy, calibration, and consistency of predictions. We find calibration improvements from forecasting training generalize across popular benchmarks. We open-source all our models, code, and data to make research on language model forecasting broadly accessible.",
      "upvotes": 4,
      "discussionId": "6955e691832867f253525640",
      "projectPage": "https://www.openforecaster.github.io",
      "githubRepo": "https://github.com/OpenForecaster/scaling-forecasting-training",
      "githubRepoAddedBy": "user",
      "githubStars": 0,
      "organization": {
        "_id": "638df552a11654155baca408",
        "name": "Intelligent-Systems",
        "fullname": "Max Planck Institute for Intelligent Systems",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1670247618868-6183d0b249ef1d984699e4a3.jpeg"
      }
    },
    "publishedAt": "2025-12-31T13:59:51.000Z",
    "title": "Scaling Open-Ended Reasoning to Predict the Future",
    "summary": "High-stakes decision making involves reasoning under uncertainty about the future. In this work, we train language models to make predictions on open-ended forecasting questions. To scale up training data, we synthesize novel forecasting questions from global events reported in daily news, using a fully automated, careful curation recipe. We train the Qwen3 thinking models on our dataset, OpenForesight. To prevent leakage of future information during training and evaluation, we use an offline news corpus, both for data generation and retrieval in our forecasting system. Guided by a small validation set, we show the benefits of retrieval, and an improved reward function for reinforcement learning (RL). Once we obtain our final forecasting system, we perform held-out testing between May to August 2025. Our specialized model, OpenForecaster 8B, matches much larger proprietary models, with our training improving the accuracy, calibration, and consistency of predictions. We find calibration improvements from forecasting training generalize across popular benchmarks. We open-source all our models, code, and data to make research on language model forecasting broadly accessible.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.25070.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6506832221ac448013f94995",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6506832221ac448013f94995/sVUI1JV4Dxan5l-MqNze4.jpeg",
      "fullname": "Shashwat Goel",
      "name": "shash42",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "organization": {
      "_id": "638df552a11654155baca408",
      "name": "Intelligent-Systems",
      "fullname": "Max Planck Institute for Intelligent Systems",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1670247618868-6183d0b249ef1d984699e4a3.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.25075",
      "authors": [
        {
          "_id": "6955e463832867f253525627",
          "name": "Zhening Huang",
          "hidden": false
        },
        {
          "_id": "6955e463832867f253525628",
          "name": "Hyeonho Jeong",
          "hidden": false
        },
        {
          "_id": "6955e463832867f253525629",
          "name": "Xuelin Chen",
          "hidden": false
        },
        {
          "_id": "6955e463832867f25352562a",
          "name": "Yulia Gryaditskaya",
          "hidden": false
        },
        {
          "_id": "6955e463832867f25352562b",
          "name": "Tuanfeng Y. Wang",
          "hidden": false
        },
        {
          "_id": "6955e463832867f25352562c",
          "name": "Joan Lasenby",
          "hidden": false
        },
        {
          "_id": "6955e463832867f25352562d",
          "name": "Chun-Hao Huang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/RGSTPpYqorUhge84CnaAK.mp4"
      ],
      "publishedAt": "2025-12-31T18:59:57.000Z",
      "submittedOnDailyAt": "2026-01-01T00:35:18.045Z",
      "title": "SpaceTimePilot: Generative Rendering of Dynamic Scenes Across Space and Time",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We present SpaceTimePilot, a video diffusion model that disentangles space and time for controllable generative rendering. Given a monocular video, SpaceTimePilot can independently alter the camera viewpoint and the motion sequence within the generative process, re-rendering the scene for continuous and arbitrary exploration across space and time. To achieve this, we introduce an effective animation time-embedding mechanism in the diffusion process, allowing explicit control of the output video's motion sequence with respect to that of the source video. As no datasets provide paired videos of the same dynamic scene with continuous temporal variations, we propose a simple yet effective temporal-warping training scheme that repurposes existing multi-view datasets to mimic temporal differences. This strategy effectively supervises the model to learn temporal control and achieve robust space-time disentanglement. To further enhance the precision of dual control, we introduce two additional components: an improved camera-conditioning mechanism that allows altering the camera from the first frame, and CamxTime, the first synthetic space-and-time full-coverage rendering dataset that provides fully free space-time video trajectories within a scene. Joint training on the temporal-warping scheme and the CamxTime dataset yields more precise temporal control. We evaluate SpaceTimePilot on both real-world and synthetic data, demonstrating clear space-time disentanglement and strong results compared to prior work. Project page: https://zheninghuang.github.io/Space-Time-Pilot/ Code: https://github.com/ZheningHuang/spacetimepilot",
      "upvotes": 3,
      "discussionId": "6955e463832867f25352562e",
      "projectPage": "https://zheninghuang.github.io/Space-Time-Pilot/",
      "ai_summary": "SpaceTimePilot is a video diffusion model enabling independent control of spatial viewpoint and temporal motion through a time-embedding mechanism, temporal-warping training, and a synthetic dataset for precise space-time disentanglement.",
      "ai_keywords": [
        "video diffusion model",
        "animation time-embedding mechanism",
        "temporal-warping training scheme",
        "camera-conditioning mechanism",
        "CamxTime dataset",
        "space-time disentanglement",
        "diffusion process",
        "multi-view datasets"
      ]
    },
    "publishedAt": "2025-12-31T13:59:57.000Z",
    "title": "SpaceTimePilot: Generative Rendering of Dynamic Scenes Across Space and Time",
    "summary": "We present SpaceTimePilot, a video diffusion model that disentangles space and time for controllable generative rendering. Given a monocular video, SpaceTimePilot can independently alter the camera viewpoint and the motion sequence within the generative process, re-rendering the scene for continuous and arbitrary exploration across space and time. To achieve this, we introduce an effective animation time-embedding mechanism in the diffusion process, allowing explicit control of the output video's motion sequence with respect to that of the source video. As no datasets provide paired videos of the same dynamic scene with continuous temporal variations, we propose a simple yet effective temporal-warping training scheme that repurposes existing multi-view datasets to mimic temporal differences. This strategy effectively supervises the model to learn temporal control and achieve robust space-time disentanglement. To further enhance the precision of dual control, we introduce two additional components: an improved camera-conditioning mechanism that allows altering the camera from the first frame, and CamxTime, the first synthetic space-and-time full-coverage rendering dataset that provides fully free space-time video trajectories within a scene. Joint training on the temporal-warping scheme and the CamxTime dataset yields more precise temporal control. We evaluate SpaceTimePilot on both real-world and synthetic data, demonstrating clear space-time disentanglement and strong results compared to prior work. Project page: https://zheninghuang.github.io/Space-Time-Pilot/ Code: https://github.com/ZheningHuang/spacetimepilot",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/RGSTPpYqorUhge84CnaAK.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.25075.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 197
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.24885",
      "authors": [
        {
          "_id": "6956184b832867f253525712",
          "name": "Hengli Li",
          "hidden": false
        },
        {
          "_id": "6956184b832867f253525713",
          "name": "Zhaoxin Yu",
          "hidden": false
        },
        {
          "_id": "6956184b832867f253525714",
          "name": "Qi Shen",
          "hidden": false
        },
        {
          "_id": "6956184b832867f253525715",
          "name": "Chenxi Li",
          "hidden": false
        },
        {
          "_id": "6956184b832867f253525716",
          "name": "Mengmeng Wang",
          "hidden": false
        },
        {
          "_id": "6956184b832867f253525717",
          "name": "Tinglang Wu",
          "hidden": false
        },
        {
          "_id": "6956184b832867f253525718",
          "name": "Yipeng Kang",
          "hidden": false
        },
        {
          "_id": "6956184b832867f253525719",
          "name": "Yuxuan Wang",
          "hidden": false
        },
        {
          "_id": "6956184b832867f25352571a",
          "name": "Song-Chun Zhu",
          "hidden": false
        },
        {
          "_id": "6956184b832867f25352571b",
          "name": "Zixia Jia",
          "hidden": false
        },
        {
          "_id": "6956184b832867f25352571c",
          "name": "Zilong Zheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-31T14:26:55.000Z",
      "submittedOnDailyAt": "2026-01-01T04:16:35.871Z",
      "title": "BEDA: Belief Estimation as Probabilistic Constraints for Performing Strategic Dialogue Acts",
      "submittedOnDailyBy": {
        "_id": "62649e2b1ed8d81e47ad9b4e",
        "avatarUrl": "/avatars/f33a0b727822fd2ea99dce37fbda3d17.svg",
        "isPro": false,
        "fullname": "Li",
        "user": "henry12348",
        "type": "user"
      },
      "summary": "Strategic dialogue requires agents to execute distinct dialogue acts, for which belief estimation is essential. While prior work often estimates beliefs accurately, it lacks a principled mechanism to use those beliefs during generation. We bridge this gap by first formalizing two core acts Adversarial and Alignment, and by operationalizing them via probabilistic constraints on what an agent may generate. We instantiate this idea in BEDA, a framework that consists of the world set, the belief estimator for belief estimation, and the conditional generator that selects acts and realizes utterances consistent with the inferred beliefs. Across three settings, Conditional Keeper Burglar (CKBG, adversarial), Mutual Friends (MF, cooperative), and CaSiNo (negotiation), BEDA consistently outperforms strong baselines: on CKBG it improves success rate by at least 5.0 points across backbones and by 20.6 points with GPT-4.1-nano; on Mutual Friends it achieves an average improvement of 9.3 points; and on CaSiNo it achieves the optimal deal relative to all baselines. These results indicate that casting belief estimation as constraints provides a simple, general mechanism for reliable strategic dialogue.",
      "upvotes": 3,
      "discussionId": "6956184b832867f25352571d",
      "ai_summary": "A framework called BEDA uses probabilistic constraints on belief estimation to improve strategic dialogue through formalized adversarial and alignment acts, outperforming baselines across multiple task settings.",
      "ai_keywords": [
        "BEDA",
        "belief estimator",
        "conditional generator",
        "probabilistic constraints",
        "Adversarial",
        "Alignment"
      ]
    },
    "publishedAt": "2025-12-31T09:26:55.000Z",
    "title": "BEDA: Belief Estimation as Probabilistic Constraints for Performing Strategic Dialogue Acts",
    "summary": "Strategic dialogue requires agents to execute distinct dialogue acts, for which belief estimation is essential. While prior work often estimates beliefs accurately, it lacks a principled mechanism to use those beliefs during generation. We bridge this gap by first formalizing two core acts Adversarial and Alignment, and by operationalizing them via probabilistic constraints on what an agent may generate. We instantiate this idea in BEDA, a framework that consists of the world set, the belief estimator for belief estimation, and the conditional generator that selects acts and realizes utterances consistent with the inferred beliefs. Across three settings, Conditional Keeper Burglar (CKBG, adversarial), Mutual Friends (MF, cooperative), and CaSiNo (negotiation), BEDA consistently outperforms strong baselines: on CKBG it improves success rate by at least 5.0 points across backbones and by 20.6 points with GPT-4.1-nano; on Mutual Friends it achieves an average improvement of 9.3 points; and on CaSiNo it achieves the optimal deal relative to all baselines. These results indicate that casting belief estimation as constraints provides a simple, general mechanism for reliable strategic dialogue.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24885.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62649e2b1ed8d81e47ad9b4e",
      "avatarUrl": "/avatars/f33a0b727822fd2ea99dce37fbda3d17.svg",
      "fullname": "Li",
      "name": "henry12348",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.24551",
      "authors": [
        {
          "_id": "6955e985832867f253525678",
          "name": "Yuanhao Cai",
          "hidden": false
        },
        {
          "_id": "6955e985832867f253525679",
          "name": "Kunpeng Li",
          "hidden": false
        },
        {
          "_id": "6955e985832867f25352567a",
          "name": "Menglin Jia",
          "hidden": false
        },
        {
          "_id": "6955e985832867f25352567b",
          "name": "Jialiang Wang",
          "hidden": false
        },
        {
          "_id": "6955e985832867f25352567c",
          "name": "Junzhe Sun",
          "hidden": false
        },
        {
          "_id": "6955e985832867f25352567d",
          "name": "Feng Liang",
          "hidden": false
        },
        {
          "_id": "6955e985832867f25352567e",
          "name": "Weifeng Chen",
          "hidden": false
        },
        {
          "_id": "6955e985832867f25352567f",
          "name": "Felix Juefei-Xu",
          "hidden": false
        },
        {
          "_id": "6955e985832867f253525680",
          "name": "Chu Wang",
          "hidden": false
        },
        {
          "_id": "6955e985832867f253525681",
          "name": "Ali Thabet",
          "hidden": false
        },
        {
          "_id": "6955e985832867f253525682",
          "name": "Xiaoliang Dai",
          "hidden": false
        },
        {
          "_id": "6955e985832867f253525683",
          "name": "Xuan Ju",
          "hidden": false
        },
        {
          "_id": "6955e985832867f253525684",
          "name": "Alan Yuille",
          "hidden": false
        },
        {
          "_id": "6955e985832867f253525685",
          "name": "Ji Hou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-31T01:19:14.000Z",
      "submittedOnDailyAt": "2026-01-01T00:58:49.694Z",
      "title": "PhyGDPO: Physics-Aware Groupwise Direct Preference Optimization for Physically Consistent Text-to-Video Generation",
      "submittedOnDailyBy": {
        "_id": "673969726c12c4b98b6ab29f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/C2elfn7L68jAt4dtHzDAW.png",
        "isPro": false,
        "fullname": "Yuanhao Cai",
        "user": "CaiYuanhao",
        "type": "user"
      },
      "summary": "Recent advances in text-to-video (T2V) generation have achieved good visual quality, yet synthesizing videos that faithfully follow physical laws remains an open challenge. Existing methods mainly based on graphics or prompt extension struggle to generalize beyond simple simulated environments or learn implicit physical reasoning. The scarcity of training data with rich physics interactions and phenomena is also a problem. In this paper, we first introduce a Physics-Augmented video data construction Pipeline, PhyAugPipe, that leverages a vision-language model (VLM) with chain-of-thought reasoning to collect a large-scale training dataset, PhyVidGen-135K. Then we formulate a principled Physics-aware Groupwise Direct Preference Optimization, PhyGDPO, framework that builds upon the groupwise Plackett-Luce probabilistic model to capture holistic preferences beyond pairwise comparisons. In PhyGDPO, we design a Physics-Guided Rewarding (PGR) scheme that embeds VLM-based physics rewards to steer optimization toward physical consistency. We also propose a LoRA-Switch Reference (LoRA-SR) scheme that eliminates memory-heavy reference duplication for efficient training. Experiments show that our method significantly outperforms state-of-the-art open-source methods on PhyGenBench and VideoPhy2. Please check our project page at https://caiyuanhao1998.github.io/project/PhyGDPO for more video results. Our code, models, and data will be released at https://github.com/caiyuanhao1998/Open-PhyGDPO",
      "upvotes": 3,
      "discussionId": "6955e985832867f253525686",
      "projectPage": "https://caiyuanhao1998.github.io/project/PhyGDPO/",
      "githubRepo": "https://github.com/caiyuanhao1998/Open-PhyGDPO",
      "githubRepoAddedBy": "user",
      "githubStars": 1,
      "organization": {
        "_id": "6552117889e482b9ce8244d2",
        "name": "meta-ai-for-media-research",
        "fullname": "Meta AI for Media Research",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/PbL9kB-lO4M1oD2LvJugk.png"
      }
    },
    "publishedAt": "2025-12-30T20:19:14.000Z",
    "title": "PhyGDPO: Physics-Aware Groupwise Direct Preference Optimization for Physically Consistent Text-to-Video Generation",
    "summary": "Recent advances in text-to-video (T2V) generation have achieved good visual quality, yet synthesizing videos that faithfully follow physical laws remains an open challenge. Existing methods mainly based on graphics or prompt extension struggle to generalize beyond simple simulated environments or learn implicit physical reasoning. The scarcity of training data with rich physics interactions and phenomena is also a problem. In this paper, we first introduce a Physics-Augmented video data construction Pipeline, PhyAugPipe, that leverages a vision-language model (VLM) with chain-of-thought reasoning to collect a large-scale training dataset, PhyVidGen-135K. Then we formulate a principled Physics-aware Groupwise Direct Preference Optimization, PhyGDPO, framework that builds upon the groupwise Plackett-Luce probabilistic model to capture holistic preferences beyond pairwise comparisons. In PhyGDPO, we design a Physics-Guided Rewarding (PGR) scheme that embeds VLM-based physics rewards to steer optimization toward physical consistency. We also propose a LoRA-Switch Reference (LoRA-SR) scheme that eliminates memory-heavy reference duplication for efficient training. Experiments show that our method significantly outperforms state-of-the-art open-source methods on PhyGenBench and VideoPhy2. Please check our project page at https://caiyuanhao1998.github.io/project/PhyGDPO for more video results. Our code, models, and data will be released at https://github.com/caiyuanhao1998/Open-PhyGDPO",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24551.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "673969726c12c4b98b6ab29f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/C2elfn7L68jAt4dtHzDAW.png",
      "fullname": "Yuanhao Cai",
      "name": "CaiYuanhao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "organization": {
      "_id": "6552117889e482b9ce8244d2",
      "name": "meta-ai-for-media-research",
      "fullname": "Meta AI for Media Research",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/PbL9kB-lO4M1oD2LvJugk.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.24097",
      "authors": [
        {
          "_id": "6955f290832867f2535256b3",
          "name": "Wenzheng Zeng",
          "hidden": false
        },
        {
          "_id": "6955f290832867f2535256b4",
          "name": "Difei Gao",
          "hidden": false
        },
        {
          "_id": "6955f290832867f2535256b5",
          "name": "Mike Zheng Shou",
          "hidden": false
        },
        {
          "_id": "6955f290832867f2535256b6",
          "name": "Hwee Tou Ng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-30T09:13:20.000Z",
      "submittedOnDailyAt": "2026-01-01T01:48:15.124Z",
      "title": "Factorized Learning for Temporally Grounded Video-Language Models",
      "submittedOnDailyBy": {
        "_id": "670a5c886f31d354bc8c1cd1",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/670a5c886f31d354bc8c1cd1/D2Mueg9eQy4fzJhOmCrnu.jpeg",
        "isPro": false,
        "fullname": "Wenzheng Zeng",
        "user": "wenzhengzeng",
        "type": "user"
      },
      "summary": "Recent video-language models have shown great potential for video understanding, but still struggle with accurate temporal grounding for event-level perception. We observe that two main factors in video understanding (i.e., temporal grounding and textual response) form a logical hierarchy: accurate temporal evidence grounding lays the foundation for reliable textual response. However, existing works typically handle these two tasks in a coupled manner without a clear logical structure, leading to sub-optimal objectives. We address this from a factorized learning perspective. We first propose D^2VLM, a framework that decouples the learning of these two tasks while also emphasizing their inherent dependency. We adopt a \"grounding then answering with evidence referencing\" paradigm and introduce evidence tokens for evidence grounding, which emphasize event-level visual semantic capture beyond the focus on timestamp representation in existing works. To further facilitate the learning of these two tasks, we introduce a novel factorized preference optimization (FPO) algorithm. Unlike standard preference optimization, FPO explicitly incorporates probabilistic temporal grounding modeling into the optimization objective, enabling preference learning for both temporal grounding and textual response. We also construct a synthetic dataset to address the lack of suitable datasets for factorized preference learning with explicit temporal grounding. Experiments on various tasks demonstrate the clear advantage of our approach. Our source code is available at https://github.com/nusnlp/d2vlm.",
      "upvotes": 3,
      "discussionId": "6955f290832867f2535256b7",
      "projectPage": "https://github.com/nusnlp/d2vlm",
      "githubRepo": "https://github.com/nusnlp/d2vlm",
      "githubRepoAddedBy": "user",
      "githubStars": 11,
      "organization": {
        "_id": "6508ab2b349930913196378b",
        "name": "NationalUniversityofSingapore",
        "fullname": "National University of Singapore",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZYUmpSMsa5Whihw3me2Bw.png"
      }
    },
    "publishedAt": "2025-12-30T04:13:20.000Z",
    "title": "Factorized Learning for Temporally Grounded Video-Language Models",
    "summary": "Recent video-language models have shown great potential for video understanding, but still struggle with accurate temporal grounding for event-level perception. We observe that two main factors in video understanding (i.e., temporal grounding and textual response) form a logical hierarchy: accurate temporal evidence grounding lays the foundation for reliable textual response. However, existing works typically handle these two tasks in a coupled manner without a clear logical structure, leading to sub-optimal objectives. We address this from a factorized learning perspective. We first propose D^2VLM, a framework that decouples the learning of these two tasks while also emphasizing their inherent dependency. We adopt a \"grounding then answering with evidence referencing\" paradigm and introduce evidence tokens for evidence grounding, which emphasize event-level visual semantic capture beyond the focus on timestamp representation in existing works. To further facilitate the learning of these two tasks, we introduce a novel factorized preference optimization (FPO) algorithm. Unlike standard preference optimization, FPO explicitly incorporates probabilistic temporal grounding modeling into the optimization objective, enabling preference learning for both temporal grounding and textual response. We also construct a synthetic dataset to address the lack of suitable datasets for factorized preference learning with explicit temporal grounding. Experiments on various tasks demonstrate the clear advantage of our approach. Our source code is available at https://github.com/nusnlp/d2vlm.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24097.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "670a5c886f31d354bc8c1cd1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/670a5c886f31d354bc8c1cd1/D2Mueg9eQy4fzJhOmCrnu.jpeg",
      "fullname": "Wenzheng Zeng",
      "name": "wenzhengzeng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "6508ab2b349930913196378b",
      "name": "NationalUniversityofSingapore",
      "fullname": "National University of Singapore",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZYUmpSMsa5Whihw3me2Bw.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.24385",
      "authors": [
        {
          "_id": "6955fb8d832867f2535256d6",
          "name": "Song Wang",
          "hidden": false
        },
        {
          "_id": "6955fb8d832867f2535256d7",
          "name": "Lingdong Kong",
          "hidden": false
        },
        {
          "_id": "6955fb8d832867f2535256d8",
          "name": "Xiaolu Liu",
          "hidden": false
        },
        {
          "_id": "6955fb8d832867f2535256d9",
          "name": "Hao Shi",
          "hidden": false
        },
        {
          "_id": "6955fb8d832867f2535256da",
          "name": "Wentong Li",
          "hidden": false
        },
        {
          "_id": "6955fb8d832867f2535256db",
          "name": "Jianke Zhu",
          "hidden": false
        },
        {
          "_id": "6955fb8d832867f2535256dc",
          "name": "Steven C. H. Hoi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-30T17:58:01.000Z",
      "submittedOnDailyAt": "2026-01-01T02:17:11.692Z",
      "title": "Forging Spatial Intelligence: A Roadmap of Multi-Modal Data Pre-Training for Autonomous Systems",
      "submittedOnDailyBy": {
        "_id": "66863d26e2b71e3d09189ae9",
        "avatarUrl": "/avatars/3c0e6f30e053f2e622ae75e1dc43edba.svg",
        "isPro": false,
        "fullname": "Song Wang",
        "user": "songw-zju",
        "type": "user"
      },
      "summary": "The rapid advancement of autonomous systems, including self-driving vehicles and drones, has intensified the need to forge true Spatial Intelligence from multi-modal onboard sensor data. While foundation models excel in single-modal contexts, integrating their capabilities across diverse sensors like cameras and LiDAR to create a unified understanding remains a formidable challenge. This paper presents a comprehensive framework for multi-modal pre-training, identifying the core set of techniques driving progress toward this goal. We dissect the interplay between foundational sensor characteristics and learning strategies, evaluating the role of platform-specific datasets in enabling these advancements. Our central contribution is the formulation of a unified taxonomy for pre-training paradigms: ranging from single-modality baselines to sophisticated unified frameworks that learn holistic representations for advanced tasks like 3D object detection and semantic occupancy prediction. Furthermore, we investigate the integration of textual inputs and occupancy representations to facilitate open-world perception and planning. Finally, we identify critical bottlenecks, such as computational efficiency and model scalability, and propose a roadmap toward general-purpose multi-modal foundation models capable of achieving robust Spatial Intelligence for real-world deployment.",
      "upvotes": 2,
      "discussionId": "6955fb8e832867f2535256dd",
      "githubRepo": "https://github.com/worldbench/awesome-spatial-intelligence",
      "githubRepoAddedBy": "user",
      "githubStars": 97,
      "organization": {
        "_id": "61bac2af530e5c78d7b99667",
        "name": "zju",
        "fullname": "Zhejiang University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"
      }
    },
    "publishedAt": "2025-12-30T12:58:01.000Z",
    "title": "Forging Spatial Intelligence: A Roadmap of Multi-Modal Data Pre-Training for Autonomous Systems",
    "summary": "The rapid advancement of autonomous systems, including self-driving vehicles and drones, has intensified the need to forge true Spatial Intelligence from multi-modal onboard sensor data. While foundation models excel in single-modal contexts, integrating their capabilities across diverse sensors like cameras and LiDAR to create a unified understanding remains a formidable challenge. This paper presents a comprehensive framework for multi-modal pre-training, identifying the core set of techniques driving progress toward this goal. We dissect the interplay between foundational sensor characteristics and learning strategies, evaluating the role of platform-specific datasets in enabling these advancements. Our central contribution is the formulation of a unified taxonomy for pre-training paradigms: ranging from single-modality baselines to sophisticated unified frameworks that learn holistic representations for advanced tasks like 3D object detection and semantic occupancy prediction. Furthermore, we investigate the integration of textual inputs and occupancy representations to facilitate open-world perception and planning. Finally, we identify critical bottlenecks, such as computational efficiency and model scalability, and propose a roadmap toward general-purpose multi-modal foundation models capable of achieving robust Spatial Intelligence for real-world deployment.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24385.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66863d26e2b71e3d09189ae9",
      "avatarUrl": "/avatars/3c0e6f30e053f2e622ae75e1dc43edba.svg",
      "fullname": "Song Wang",
      "name": "songw-zju",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "organization": {
      "_id": "61bac2af530e5c78d7b99667",
      "name": "zju",
      "fullname": "Zhejiang University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.24297",
      "authors": [
        {
          "_id": "6955e6f8832867f253525642",
          "name": "Meiqi Chen",
          "hidden": false
        },
        {
          "_id": "6955e6f8832867f253525643",
          "name": "Fandong Meng",
          "hidden": false
        },
        {
          "_id": "6955e6f8832867f253525644",
          "name": "Jie Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-30T15:39:11.000Z",
      "submittedOnDailyAt": "2026-01-01T00:46:12.085Z",
      "title": "Figure It Out: Improving the Frontier of Reasoning with Active Visual Thinking",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Complex reasoning problems often involve implicit spatial, geometric, and structural relationships that are not explicitly encoded in text. While recent reasoning models have achieved strong performance across many domains, purely text-based reasoning struggles to represent global structural constraints in complex settings. In this paper, we introduce FIGR, which integrates active visual thinking into multi-turn reasoning via end-to-end reinforcement learning. FIGR externalizes intermediate structural hypotheses by constructing visual representations during problem solving. By adaptively regulating when and how visual reasoning should be invoked, FIGR enables more stable and coherent reasoning over global structural properties that are difficult to capture from text alone. Experiments on challenging mathematical reasoning benchmarks demonstrate that FIGR outperforms strong text-only chain-of-thought baselines. In particular, FIGR improves the base model by 13.12% on AIME 2025 and 11.00% on BeyondAIME, highlighting the effectiveness of figure-guided multimodal reasoning in enhancing the stability and reliability of complex reasoning.",
      "upvotes": 2,
      "discussionId": "6955e6f8832867f253525645",
      "githubRepo": "https://github.com/chenmeiqii/FIGR",
      "githubRepoAddedBy": "user",
      "githubStars": 1
    },
    "publishedAt": "2025-12-30T10:39:11.000Z",
    "title": "Figure It Out: Improving the Frontier of Reasoning with Active Visual Thinking",
    "summary": "Complex reasoning problems often involve implicit spatial, geometric, and structural relationships that are not explicitly encoded in text. While recent reasoning models have achieved strong performance across many domains, purely text-based reasoning struggles to represent global structural constraints in complex settings. In this paper, we introduce FIGR, which integrates active visual thinking into multi-turn reasoning via end-to-end reinforcement learning. FIGR externalizes intermediate structural hypotheses by constructing visual representations during problem solving. By adaptively regulating when and how visual reasoning should be invoked, FIGR enables more stable and coherent reasoning over global structural properties that are difficult to capture from text alone. Experiments on challenging mathematical reasoning benchmarks demonstrate that FIGR outperforms strong text-only chain-of-thought baselines. In particular, FIGR improves the base model by 13.12% on AIME 2025 and 11.00% on BeyondAIME, highlighting the effectiveness of figure-guided multimodal reasoning in enhancing the stability and reliability of complex reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24297.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 197
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.23988",
      "authors": [
        {
          "_id": "6955e629832867f253525630",
          "name": "Zhenyu Zhang",
          "hidden": false
        },
        {
          "_id": "6955e629832867f253525631",
          "name": "Shujian Zhang",
          "hidden": false
        },
        {
          "_id": "6955e629832867f253525632",
          "name": "John Lambert",
          "hidden": false
        },
        {
          "_id": "6955e629832867f253525633",
          "name": "Wenxuan Zhou",
          "hidden": false
        },
        {
          "_id": "6955e629832867f253525634",
          "name": "Zhangyang Wang",
          "hidden": false
        },
        {
          "_id": "6955e629832867f253525635",
          "name": "Mingqing Chen",
          "hidden": false
        },
        {
          "_id": "6955e629832867f253525636",
          "name": "Andrew Hard",
          "hidden": false
        },
        {
          "_id": "6955e629832867f253525637",
          "name": "Rajiv Mathews",
          "hidden": false
        },
        {
          "_id": "6955e629832867f253525638",
          "name": "Lun Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-30T05:09:11.000Z",
      "submittedOnDailyAt": "2026-01-01T00:42:58.807Z",
      "title": "Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery of the Reasoning Process",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Despite the growing reasoning capabilities of recent large language models (LLMs), their internal mechanisms during the reasoning process remain underexplored. Prior approaches often rely on human-defined concepts (e.g., overthinking, reflection) at the word level to analyze reasoning in a supervised manner. However, such methods are limited, as it is infeasible to capture the full spectrum of potential reasoning behaviors, many of which are difficult to define in token space. In this work, we propose an unsupervised framework (namely, RISE: Reasoning behavior Interpretability via Sparse auto-Encoder) for discovering reasoning vectors, which we define as directions in the activation space that encode distinct reasoning behaviors. By segmenting chain-of-thought traces into sentence-level 'steps' and training sparse auto-encoders (SAEs) on step-level activations, we uncover disentangled features corresponding to interpretable behaviors such as reflection and backtracking. Visualization and clustering analyses show that these behaviors occupy separable regions in the decoder column space. Moreover, targeted interventions on SAE-derived vectors can controllably amplify or suppress specific reasoning behaviors, altering inference trajectories without retraining. Beyond behavior-specific disentanglement, SAEs capture structural properties such as response length, revealing clusters of long versus short reasoning traces. More interestingly, SAEs enable the discovery of novel behaviors beyond human supervision. We demonstrate the ability to control response confidence by identifying confidence-related vectors in the SAE decoder space. These findings underscore the potential of unsupervised latent discovery for both interpreting and controllably steering reasoning in LLMs.",
      "upvotes": 2,
      "discussionId": "6955e62a832867f253525639",
      "ai_summary": "An unsupervised framework using sparse auto-encoders identifies and controls interpretable reasoning behaviors in large language models through disentangled latent vectors.",
      "ai_keywords": [
        "sparse auto-encoders (SAEs)",
        "chain-of-thought traces",
        "activation space",
        "decoder column space",
        "disentangled features",
        "reasoning vectors",
        "SAE-derived vectors",
        "confidence-related vectors",
        "unsupervised latent discovery"
      ],
      "organization": {
        "_id": "60f6cbb2852126bac698c89e",
        "name": "deepmind",
        "fullname": "Deepmind",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1638956859875-5f1158120c833276f61f1a84.jpeg"
      }
    },
    "publishedAt": "2025-12-30T00:09:11.000Z",
    "title": "Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery of the Reasoning Process",
    "summary": "Despite the growing reasoning capabilities of recent large language models (LLMs), their internal mechanisms during the reasoning process remain underexplored. Prior approaches often rely on human-defined concepts (e.g., overthinking, reflection) at the word level to analyze reasoning in a supervised manner. However, such methods are limited, as it is infeasible to capture the full spectrum of potential reasoning behaviors, many of which are difficult to define in token space. In this work, we propose an unsupervised framework (namely, RISE: Reasoning behavior Interpretability via Sparse auto-Encoder) for discovering reasoning vectors, which we define as directions in the activation space that encode distinct reasoning behaviors. By segmenting chain-of-thought traces into sentence-level 'steps' and training sparse auto-encoders (SAEs) on step-level activations, we uncover disentangled features corresponding to interpretable behaviors such as reflection and backtracking. Visualization and clustering analyses show that these behaviors occupy separable regions in the decoder column space. Moreover, targeted interventions on SAE-derived vectors can controllably amplify or suppress specific reasoning behaviors, altering inference trajectories without retraining. Beyond behavior-specific disentanglement, SAEs capture structural properties such as response length, revealing clusters of long versus short reasoning traces. More interestingly, SAEs enable the discovery of novel behaviors beyond human supervision. We demonstrate the ability to control response confidence by identifying confidence-related vectors in the SAE decoder space. These findings underscore the potential of unsupervised latent discovery for both interpreting and controllably steering reasoning in LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23988.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 197
    },
    "organization": {
      "_id": "60f6cbb2852126bac698c89e",
      "name": "deepmind",
      "fullname": "Deepmind",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1638956859875-5f1158120c833276f61f1a84.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.22905",
      "authors": [
        {
          "_id": "6953792489916ff627aa4099",
          "name": "Kai Liu",
          "hidden": false
        },
        {
          "_id": "6953792489916ff627aa409a",
          "name": "Jungang Li",
          "hidden": false
        },
        {
          "_id": "6953792489916ff627aa409b",
          "name": "Yuchong Sun",
          "hidden": false
        },
        {
          "_id": "6953792489916ff627aa409c",
          "name": "Shengqiong Wu",
          "hidden": false
        },
        {
          "_id": "6953792489916ff627aa409d",
          "name": "Jianzhang Gao",
          "hidden": false
        },
        {
          "_id": "6953792489916ff627aa409e",
          "name": "Daoan Zhang",
          "hidden": false
        },
        {
          "_id": "6953792489916ff627aa409f",
          "name": "Wei Zhang",
          "hidden": false
        },
        {
          "_id": "6953792489916ff627aa40a0",
          "name": "Sheng Jin",
          "hidden": false
        },
        {
          "_id": "6953792489916ff627aa40a1",
          "name": "Sicheng Yu",
          "hidden": false
        },
        {
          "_id": "6953792489916ff627aa40a2",
          "name": "Geng Zhan",
          "hidden": false
        },
        {
          "_id": "6953792489916ff627aa40a3",
          "name": "Jiayi Ji",
          "hidden": false
        },
        {
          "_id": "6953792489916ff627aa40a4",
          "name": "Fan Zhou",
          "hidden": false
        },
        {
          "_id": "6953792489916ff627aa40a5",
          "name": "Liang Zheng",
          "hidden": false
        },
        {
          "_id": "6953792489916ff627aa40a6",
          "name": "Shuicheng Yan",
          "hidden": false
        },
        {
          "_id": "6953792489916ff627aa40a7",
          "name": "Hao Fei",
          "hidden": false
        },
        {
          "_id": "6953792489916ff627aa40a8",
          "name": "Tat-Seng Chua",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/678bdcbe600666579235a1f3/lxSjwswizLyAtl6Uuuu6a.png",
        "https://cdn-uploads.huggingface.co/production/uploads/678bdcbe600666579235a1f3/d8fT3qdlQEOOA4Y0NfUXL.png",
        "https://cdn-uploads.huggingface.co/production/uploads/678bdcbe600666579235a1f3/ZrCNEwlgd3XQtgsQmghuK.png"
      ],
      "publishedAt": "2025-12-28T12:25:43.000Z",
      "submittedOnDailyAt": "2026-01-01T01:52:36.696Z",
      "title": "JavisGPT: A Unified Multi-modal LLM for Sounding-Video Comprehension and Generation",
      "submittedOnDailyBy": {
        "_id": "678bdcbe600666579235a1f3",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/FoPxPc-T3xbzzkgQkrIbg.png",
        "isPro": false,
        "fullname": "KAI LIU",
        "user": "kkail8",
        "type": "user"
      },
      "summary": "This paper presents JavisGPT, the first unified multimodal large language model (MLLM) for Joint Audio-Video (JAV) comprehension and generation. JavisGPT adopts a concise encoder-LLM-decoder architecture, featuring a SyncFusion module for spatio-temporal audio-video fusion and synchrony-aware learnable queries to bridge a pretrained JAV-DiT generator. This design enables temporally coherent video-audio understanding and generation from multimodal instructions. We design an effective three-stage training pipeline consisting of multimodal pretraining, audio-video fine-tuning, and large-scale instruction-tuning, to progressively build multimodal comprehension and generation from existing vision-language models. To support this, we further construct JavisInst-Omni, a high-quality instruction dataset with over 200K GPT-4o-curated audio-video-text dialogues that span diverse and multi-level comprehension and generation scenarios. Extensive experiments on JAV comprehension and generation benchmarks show that JavisGPT outperforms existing MLLMs, particularly in complex and temporally synchronized settings.",
      "upvotes": 2,
      "discussionId": "6953792589916ff627aa40a9",
      "projectPage": "https://javisverse.github.io/JavisGPT-page/",
      "githubRepo": "https://github.com/JavisVerse/JavisGPT",
      "githubRepoAddedBy": "user",
      "githubStars": 9,
      "organization": {
        "_id": "67adfac46083604e4b664e43",
        "name": "JavisVerse",
        "fullname": "JavisVerse",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/678bdcbe600666579235a1f3/kZQXob4mMAq0CoeZH5ZfG.png"
      }
    },
    "publishedAt": "2025-12-28T07:25:43.000Z",
    "title": "JavisGPT: A Unified Multi-modal LLM for Sounding-Video Comprehension and Generation",
    "summary": "This paper presents JavisGPT, the first unified multimodal large language model (MLLM) for Joint Audio-Video (JAV) comprehension and generation. JavisGPT adopts a concise encoder-LLM-decoder architecture, featuring a SyncFusion module for spatio-temporal audio-video fusion and synchrony-aware learnable queries to bridge a pretrained JAV-DiT generator. This design enables temporally coherent video-audio understanding and generation from multimodal instructions. We design an effective three-stage training pipeline consisting of multimodal pretraining, audio-video fine-tuning, and large-scale instruction-tuning, to progressively build multimodal comprehension and generation from existing vision-language models. To support this, we further construct JavisInst-Omni, a high-quality instruction dataset with over 200K GPT-4o-curated audio-video-text dialogues that span diverse and multi-level comprehension and generation scenarios. Extensive experiments on JAV comprehension and generation benchmarks show that JavisGPT outperforms existing MLLMs, particularly in complex and temporally synchronized settings.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/678bdcbe600666579235a1f3/lxSjwswizLyAtl6Uuuu6a.png",
      "https://cdn-uploads.huggingface.co/production/uploads/678bdcbe600666579235a1f3/d8fT3qdlQEOOA4Y0NfUXL.png",
      "https://cdn-uploads.huggingface.co/production/uploads/678bdcbe600666579235a1f3/ZrCNEwlgd3XQtgsQmghuK.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.22905.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "678bdcbe600666579235a1f3",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/FoPxPc-T3xbzzkgQkrIbg.png",
      "fullname": "KAI LIU",
      "name": "kkail8",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "67adfac46083604e4b664e43",
      "name": "JavisVerse",
      "fullname": "JavisVerse",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/678bdcbe600666579235a1f3/kZQXob4mMAq0CoeZH5ZfG.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.22280",
      "authors": [
        {
          "_id": "69533db489916ff627aa3ec8",
          "user": {
            "_id": "66cb582d0aaaa6753b10d6ab",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66cb582d0aaaa6753b10d6ab/YEmHDjkPVQJ6wFKQCJwCm.jpeg",
            "isPro": false,
            "fullname": "Varshith",
            "user": "varam17",
            "type": "user"
          },
          "name": "Varshith Gudur",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-12-31T20:57:18.264Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-25T06:04:04.000Z",
      "submittedOnDailyAt": "2026-01-01T02:55:16.052Z",
      "title": "Valori: A Deterministic Memory Substrate for AI Systems",
      "submittedOnDailyBy": {
        "_id": "66cb582d0aaaa6753b10d6ab",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66cb582d0aaaa6753b10d6ab/YEmHDjkPVQJ6wFKQCJwCm.jpeg",
        "isPro": false,
        "fullname": "Varshith",
        "user": "varam17",
        "type": "user"
      },
      "summary": "Modern AI systems rely on vector embeddings stored and searched using floating-point arithmetic. While effective for approximate similarity search, this design introduces fundamental non-determinism: identical models, inputs, and code can produce different memory states and retrieval results across hardware architectures (e.g., x86 vs. ARM). This prevents replayability and safe deployment, leading to silent data divergence that prevents post-hoc verification and compromises audit trails in regulated sectors. We present Valori, a deterministic AI memory substrate that replaces floating-point memory operations with fixed-point arithmetic (Q16.16) and models memory as a replayable state machine. Valori guarantees bit-identical memory states, snapshots, and search results across platforms. We demonstrate that non-determinism arises before indexing or retrieval and show how Valori enforces determinism at the memory boundary. Our results suggest that deterministic memory is a necessary primitive for trustworthy AI systems. The reference implementation is open-source and available at https://github.com/varshith-Git/Valori-Kernel (archived at https://zenodo.org/records/18022660).",
      "upvotes": 2,
      "discussionId": "69533db489916ff627aa3ec9",
      "projectPage": "https://valori.systems/",
      "githubRepo": "https://github.com/varshith-Git/Valori-Kernel",
      "githubRepoAddedBy": "user",
      "ai_summary": "Valori introduces a deterministic AI memory system using fixed-point arithmetic to ensure bit-identical results across platforms, addressing non-determinism in vector embeddings and similarity search for trustworthy AI.",
      "ai_keywords": [
        "vector embeddings",
        "floating-point arithmetic",
        "fixed-point arithmetic (Q16.16)",
        "approximate similarity search",
        "memory state machine",
        "replayable state machine",
        "bit-identical memory states",
        "deterministic memory",
        "non-determinism",
        "silent data divergence",
        "snapshot"
      ],
      "githubStars": 1
    },
    "publishedAt": "2025-12-25T01:04:04.000Z",
    "title": "Valori: A Deterministic Memory Substrate for AI Systems",
    "summary": "Modern AI systems rely on vector embeddings stored and searched using floating-point arithmetic. While effective for approximate similarity search, this design introduces fundamental non-determinism: identical models, inputs, and code can produce different memory states and retrieval results across hardware architectures (e.g., x86 vs. ARM). This prevents replayability and safe deployment, leading to silent data divergence that prevents post-hoc verification and compromises audit trails in regulated sectors. We present Valori, a deterministic AI memory substrate that replaces floating-point memory operations with fixed-point arithmetic (Q16.16) and models memory as a replayable state machine. Valori guarantees bit-identical memory states, snapshots, and search results across platforms. We demonstrate that non-determinism arises before indexing or retrieval and show how Valori enforces determinism at the memory boundary. Our results suggest that deterministic memory is a necessary primitive for trustworthy AI systems. The reference implementation is open-source and available at https://github.com/varshith-Git/Valori-Kernel (archived at https://zenodo.org/records/18022660).",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.22280.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66cb582d0aaaa6753b10d6ab",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66cb582d0aaaa6753b10d6ab/YEmHDjkPVQJ6wFKQCJwCm.jpeg",
      "fullname": "Varshith",
      "name": "varam17",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2512.23851",
      "authors": [
        {
          "_id": "6955e31a832867f2535255ba",
          "name": "Lvmin Zhang",
          "hidden": false
        },
        {
          "_id": "6955e31a832867f2535255bb",
          "name": "Shengqu Cai",
          "hidden": false
        },
        {
          "_id": "6955e31a832867f2535255bc",
          "name": "Muyang Li",
          "hidden": false
        },
        {
          "_id": "6955e31a832867f2535255bd",
          "name": "Chong Zeng",
          "hidden": false
        },
        {
          "_id": "6955e31a832867f2535255be",
          "name": "Beijia Lu",
          "hidden": false
        },
        {
          "_id": "6955e31a832867f2535255bf",
          "name": "Anyi Rao",
          "hidden": false
        },
        {
          "_id": "6955e31a832867f2535255c0",
          "name": "Song Han",
          "hidden": false
        },
        {
          "_id": "6955e31a832867f2535255c1",
          "name": "Gordon Wetzstein",
          "hidden": false
        },
        {
          "_id": "6955e31a832867f2535255c2",
          "name": "Maneesh Agrawala",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-29T20:29:21.000Z",
      "submittedOnDailyAt": "2026-01-01T00:31:38.328Z",
      "title": "Pretraining Frame Preservation in Autoregressive Video Memory Compression",
      "submittedOnDailyBy": {
        "_id": "639c1572445b133a4e9b3a3f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1671173450971-noauth.jpeg",
        "isPro": true,
        "fullname": "Lvmin Zhang",
        "user": "lllyasviel",
        "type": "user"
      },
      "summary": "We present PFP, a neural network structure to compress long videos into short contexts, with an explicit pretraining objective to preserve the high-frequency details of single frames at arbitrary temporal positions. The baseline model can compress a 20-second video into a context at about 5k length, where random frames can be retrieved with perceptually preserved appearances. Such pretrained models can be directly fine-tuned as memory encoders for autoregressive video models, enabling long history memory with low context cost and relatively low fidelity loss. We evaluate the framework with ablative settings and discuss the trade-offs of possible neural architecture designs.",
      "upvotes": 1,
      "discussionId": "6955e31a832867f2535255c3"
    },
    "publishedAt": "2025-12-29T15:29:21.000Z",
    "title": "Pretraining Frame Preservation in Autoregressive Video Memory Compression",
    "summary": "We present PFP, a neural network structure to compress long videos into short contexts, with an explicit pretraining objective to preserve the high-frequency details of single frames at arbitrary temporal positions. The baseline model can compress a 20-second video into a context at about 5k length, where random frames can be retrieved with perceptually preserved appearances. Such pretrained models can be directly fine-tuned as memory encoders for autoregressive video models, enabling long history memory with low context cost and relatively low fidelity loss. We evaluate the framework with ablative settings and discuss the trade-offs of possible neural architecture designs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23851.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "639c1572445b133a4e9b3a3f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1671173450971-noauth.jpeg",
      "fullname": "Lvmin Zhang",
      "name": "lllyasviel",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9808
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.22564",
      "authors": [
        {
          "_id": "69538d3c89916ff627aa411a",
          "user": {
            "_id": "67a336ddb362638a9397b62f",
            "avatarUrl": "/avatars/99c99ae58caff032568fa4d123e9033a.svg",
            "isPro": false,
            "fullname": "Ik",
            "user": "Atakanisik",
            "type": "user"
          },
          "name": "Atakan Ik",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-12-31T20:55:48.308Z",
          "hidden": false
        },
        {
          "_id": "69538d3c89916ff627aa411b",
          "name": "Selin Vulga Ik",
          "hidden": false
        },
        {
          "_id": "69538d3c89916ff627aa411c",
          "name": "Ahmet Feridun Ik",
          "hidden": false
        },
        {
          "_id": "69538d3c89916ff627aa411d",
          "name": "Mahuk Taylan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-27T11:39:36.000Z",
      "submittedOnDailyAt": "2026-01-01T04:40:13.882Z",
      "title": "Geometry-Aware Optimization for Respiratory Sound Classification: Enhancing Sensitivity with SAM-Optimized Audio Spectrogram Transformers",
      "submittedOnDailyBy": {
        "_id": "67a336ddb362638a9397b62f",
        "avatarUrl": "/avatars/99c99ae58caff032568fa4d123e9033a.svg",
        "isPro": false,
        "fullname": "Ik",
        "user": "Atakanisik",
        "type": "user"
      },
      "summary": "Respiratory sound classification is hindered by the limited size, high noise levels, and severe class imbalance of benchmark datasets like ICBHI 2017. While Transformer-based models offer powerful feature extraction capabilities, they are prone to overfitting and often converge to sharp minima in the loss landscape when trained on such constrained medical data. To address this, we introduce a framework that enhances the Audio Spectrogram Transformer (AST) using Sharpness-Aware Minimization (SAM). Instead of merely minimizing the training loss, our approach optimizes the geometry of the loss surface, guiding the model toward flatter minima that generalize better to unseen patients. We also implement a weighted sampling strategy to handle class imbalance effectively. Our method achieves a state-of-the-art score of 68.10% on the ICBHI 2017 dataset, outperforming existing CNN and hybrid baselines. More importantly, it reaches a sensitivity of 68.31%, a crucial improvement for reliable clinical screening. Further analysis using t-SNE and attention maps confirms that the model learns robust, discriminative features rather than memorizing background noise.",
      "upvotes": 0,
      "discussionId": "69538d3c89916ff627aa411e"
    },
    "publishedAt": "2025-12-27T06:39:36.000Z",
    "title": "Geometry-Aware Optimization for Respiratory Sound Classification: Enhancing Sensitivity with SAM-Optimized Audio Spectrogram Transformers",
    "summary": "Respiratory sound classification is hindered by the limited size, high noise levels, and severe class imbalance of benchmark datasets like ICBHI 2017. While Transformer-based models offer powerful feature extraction capabilities, they are prone to overfitting and often converge to sharp minima in the loss landscape when trained on such constrained medical data. To address this, we introduce a framework that enhances the Audio Spectrogram Transformer (AST) using Sharpness-Aware Minimization (SAM). Instead of merely minimizing the training loss, our approach optimizes the geometry of the loss surface, guiding the model toward flatter minima that generalize better to unseen patients. We also implement a weighted sampling strategy to handle class imbalance effectively. Our method achieves a state-of-the-art score of 68.10% on the ICBHI 2017 dataset, outperforming existing CNN and hybrid baselines. More importantly, it reaches a sensitivity of 68.31%, a crucial improvement for reliable clinical screening. Further analysis using t-SNE and attention maps confirms that the model learns robust, discriminative features rather than memorizing background noise.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.22564.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67a336ddb362638a9397b62f",
      "avatarUrl": "/avatars/99c99ae58caff032568fa4d123e9033a.svg",
      "fullname": "Ik",
      "name": "Atakanisik",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]