[
  {
    "paper": {
      "id": "2511.18538",
      "authors": [
        {
          "_id": "692e667137312eaa83fd8832",
          "name": "Jian Yang",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd8833",
          "name": "Xianglong Liu",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd8834",
          "name": "Weifeng Lv",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd8835",
          "name": "Ken Deng",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd8836",
          "name": "Shawn Guo",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd8837",
          "name": "Lin Jing",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd8838",
          "name": "Yizhi Li",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd8839",
          "name": "Shark Liu",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd883a",
          "name": "Xianzhen Luo",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd883b",
          "name": "Yuyu Luo",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd883c",
          "name": "Changzai Pan",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd883d",
          "name": "Ensheng Shi",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd883e",
          "name": "Yingshui Tan",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd883f",
          "name": "Renshuai Tao",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd8840",
          "name": "Jiajun Wu",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd8841",
          "name": "Xianjie Wu",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd8842",
          "name": "Zhenhe Wu",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd8843",
          "name": "Daoguang Zan",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd8844",
          "name": "Chenchen Zhang",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd8845",
          "name": "Wei Zhang",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd8846",
          "name": "He Zhu",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd8847",
          "name": "Terry Yue Zhuo",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd8848",
          "name": "Kerui Cao",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd8849",
          "name": "Xianfu Cheng",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd884a",
          "name": "Jun Dong",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd884b",
          "name": "Shengjie Fang",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd884c",
          "name": "Zhiwei Fei",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd884d",
          "name": "Xiangyuan Guan",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd884e",
          "name": "Qipeng Guo",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd884f",
          "name": "Zhiguang Han",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd8850",
          "name": "Joseph James",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd8851",
          "name": "Tianqi Luo",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd8852",
          "name": "Renyuan Li",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd8853",
          "name": "Yuhang Li",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd8854",
          "name": "Yiming Liang",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd8855",
          "name": "Congnan Liu",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd8856",
          "name": "Jiaheng Liu",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd8857",
          "name": "Qian Liu",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd8858",
          "name": "Ruitong Liu",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd8859",
          "name": "Tyler Loakman",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd885a",
          "name": "Xiangxin Meng",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd885b",
          "name": "Chuang Peng",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd885c",
          "name": "Tianhao Peng",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd885d",
          "name": "Jiajun Shi",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd885e",
          "name": "Mingjie Tang",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd885f",
          "name": "Boyang Wang",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd8860",
          "name": "Haowen Wang",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd8861",
          "name": "Yunli Wang",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd8862",
          "name": "Fanglin Xu",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd8863",
          "name": "Zihan Xu",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd8864",
          "name": "Fei Yuan",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd8865",
          "name": "Ge Zhang",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd8866",
          "name": "Jiayi Zhang",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd8867",
          "name": "Xinhao Zhang",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd8868",
          "name": "Wangchunshu Zhou",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd8869",
          "name": "Hualei Zhu",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd886a",
          "name": "King Zhu",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd886b",
          "name": "Brown Dai",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd886c",
          "name": "Aishan Liu",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd886d",
          "name": "Zhoujun Li",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd886e",
          "name": "Chenghua Lin",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd886f",
          "name": "Tianyu Liu",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd8870",
          "name": "Chao Peng",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd8871",
          "name": "Kai Shen",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd8872",
          "name": "Libo Qin",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd8873",
          "name": "Shuangyong Song",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd8874",
          "name": "Zizheng Zhan",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd8875",
          "name": "Jiajun Zhang",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd8876",
          "name": "Jie Zhang",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd8877",
          "name": "Zhaoxiang Zhang",
          "hidden": false
        },
        {
          "_id": "692e667137312eaa83fd8878",
          "name": "Bo Zheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-23T17:09:34.000Z",
      "submittedOnDailyAt": "2025-12-02T02:55:07.234Z",
      "title": "From Code Foundation Models to Agents and Applications: A Practical Guide to Code Intelligence",
      "submittedOnDailyBy": {
        "_id": "64ccb9bfead94891d12aef42",
        "avatarUrl": "/avatars/c54809d43d93d3f0766bd2555cecc4e3.svg",
        "isPro": false,
        "fullname": "Yang Jian",
        "user": "CSJianYang",
        "type": "user"
      },
      "summary": "Large language models (LLMs) have fundamentally transformed automated software development by enabling direct translation of natural language descriptions into functional code, driving commercial adoption through tools like Github Copilot (Microsoft), Cursor (Anysphere), Trae (ByteDance), and Claude Code (Anthropic). While the field has evolved dramatically from rule-based systems to Transformer-based architectures, achieving performance improvements from single-digit to over 95\\% success rates on benchmarks like HumanEval. In this work, we provide a comprehensive synthesis and practical guide (a series of analytic and probing experiments) about code LLMs, systematically examining the complete model life cycle from data curation to post-training through advanced prompting paradigms, code pre-training, supervised fine-tuning, reinforcement learning, and autonomous coding agents. We analyze the code capability of the general LLMs (GPT-4, Claude, LLaMA) and code-specialized LLMs (StarCoder, Code LLaMA, DeepSeek-Coder, and QwenCoder), critically examining the techniques, design decisions, and trade-offs. Further, we articulate the research-practice gap between academic research (e.g., benchmarks and tasks) and real-world deployment (e.g., software-related code tasks), including code correctness, security, contextual awareness of large codebases, and integration with development workflows, and map promising research directions to practical needs. Last, we conduct a series of experiments to provide a comprehensive analysis of code pre-training, supervised fine-tuning, and reinforcement learning, covering scaling law, framework selection, hyperparameter sensitivity, model architectures, and dataset comparisons.",
      "upvotes": 119,
      "discussionId": "692e667237312eaa83fd8879",
      "ai_summary": "A comprehensive guide to code LLMs, covering their lifecycle from data curation to deployment, including techniques, trade-offs, and research-practice gaps.",
      "ai_keywords": [
        "Transformer-based architectures",
        "HumanEval",
        "prompting paradigms",
        "code pre-training",
        "supervised fine-tuning",
        "reinforcement learning",
        "autonomous coding agents",
        "GPT-4",
        "Claude",
        "LLaMA",
        "StarCoder",
        "Code LLaMA",
        "DeepSeek-Coder",
        "QwenCoder",
        "code correctness",
        "security",
        "contextual awareness",
        "software-related code tasks",
        "scaling law",
        "framework selection",
        "hyperparameter sensitivity",
        "model architectures",
        "dataset comparisons"
      ],
      "organization": {
        "_id": "63ba7720fc454697637969f1",
        "name": "Beihang",
        "fullname": "Beihang University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63ba7666c138c8f2b7844b58/n98lZU9VWxYgWIkzE_6o4.jpeg"
      }
    },
    "publishedAt": "2025-11-23T12:09:34.000Z",
    "title": "From Code Foundation Models to Agents and Applications: A Practical Guide to Code Intelligence",
    "summary": "Large language models (LLMs) have fundamentally transformed automated software development by enabling direct translation of natural language descriptions into functional code, driving commercial adoption through tools like Github Copilot (Microsoft), Cursor (Anysphere), Trae (ByteDance), and Claude Code (Anthropic). While the field has evolved dramatically from rule-based systems to Transformer-based architectures, achieving performance improvements from single-digit to over 95\\% success rates on benchmarks like HumanEval. In this work, we provide a comprehensive synthesis and practical guide (a series of analytic and probing experiments) about code LLMs, systematically examining the complete model life cycle from data curation to post-training through advanced prompting paradigms, code pre-training, supervised fine-tuning, reinforcement learning, and autonomous coding agents. We analyze the code capability of the general LLMs (GPT-4, Claude, LLaMA) and code-specialized LLMs (StarCoder, Code LLaMA, DeepSeek-Coder, and QwenCoder), critically examining the techniques, design decisions, and trade-offs. Further, we articulate the research-practice gap between academic research (e.g., benchmarks and tasks) and real-world deployment (e.g., software-related code tasks), including code correctness, security, contextual awareness of large codebases, and integration with development workflows, and map promising research directions to practical needs. Last, we conduct a series of experiments to provide a comprehensive analysis of code pre-training, supervised fine-tuning, and reinforcement learning, covering scaling law, framework selection, hyperparameter sensitivity, model architectures, and dataset comparisons.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.18538.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ccb9bfead94891d12aef42",
      "avatarUrl": "/avatars/c54809d43d93d3f0766bd2555cecc4e3.svg",
      "fullname": "Yang Jian",
      "name": "CSJianYang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 17
    },
    "organization": {
      "_id": "63ba7720fc454697637969f1",
      "name": "Beihang",
      "fullname": "Beihang University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63ba7666c138c8f2b7844b58/n98lZU9VWxYgWIkzE_6o4.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.20785",
      "authors": [
        {
          "_id": "692d430f4397b1ec214f696e",
          "user": {
            "_id": "6524d665ab1416594149e07e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6524d665ab1416594149e07e/KMsCaAtV0DLC4tqN8f2a7.png",
            "isPro": false,
            "fullname": "Zuhao Yang",
            "user": "mwxely",
            "type": "user"
          },
          "name": "Zuhao Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-12-01T09:10:11.311Z",
          "hidden": false
        },
        {
          "_id": "692d430f4397b1ec214f696f",
          "user": {
            "_id": "6690f58e2f9f6f9c88e91031",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6690f58e2f9f6f9c88e91031/QQ_VoEh7NlE6BUvii08zk.png",
            "isPro": false,
            "fullname": "Sudong Wang",
            "user": "xiao4435",
            "type": "user"
          },
          "name": "Sudong Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-12-01T09:10:14.173Z",
          "hidden": false
        },
        {
          "_id": "692d430f4397b1ec214f6970",
          "user": {
            "_id": "64bb77e786e7fb5b8a317a43",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bb77e786e7fb5b8a317a43/J0jOrlZJ9gazdYaeSH2Bo.png",
            "isPro": false,
            "fullname": "kcz",
            "user": "kcz358",
            "type": "user"
          },
          "name": "Kaichen Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-12-01T10:06:41.343Z",
          "hidden": false
        },
        {
          "_id": "692d430f4397b1ec214f6971",
          "user": {
            "_id": "66bf00ca5b4e241fe266059d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66bf00ca5b4e241fe266059d/VoWPC_C4zoeT6dS699t7L.png",
            "isPro": false,
            "fullname": "Keming Wu",
            "user": "wukeming11",
            "type": "user"
          },
          "name": "Keming Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-12-01T09:10:09.461Z",
          "hidden": false
        },
        {
          "_id": "692d430f4397b1ec214f6972",
          "name": "Sicong Leng",
          "hidden": false
        },
        {
          "_id": "692d430f4397b1ec214f6973",
          "name": "Yifan Zhang",
          "hidden": false
        },
        {
          "_id": "692d430f4397b1ec214f6974",
          "name": "Chengwei Qin",
          "hidden": false
        },
        {
          "_id": "692d430f4397b1ec214f6975",
          "name": "Shijian Lu",
          "hidden": false
        },
        {
          "_id": "692d430f4397b1ec214f6976",
          "name": "Xingxuan Li",
          "hidden": false
        },
        {
          "_id": "692d430f4397b1ec214f6977",
          "name": "Lidong Bing",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6524d665ab1416594149e07e/SZBdiFzskclJytgjtsLNu.gif"
      ],
      "publishedAt": "2025-11-25T19:22:48.000Z",
      "submittedOnDailyAt": "2025-12-02T00:35:56.511Z",
      "title": "LongVT: Incentivizing \"Thinking with Long Videos\" via Native Tool Calling",
      "submittedOnDailyBy": {
        "_id": "6524d665ab1416594149e07e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6524d665ab1416594149e07e/KMsCaAtV0DLC4tqN8f2a7.png",
        "isPro": false,
        "fullname": "Zuhao Yang",
        "user": "mwxely",
        "type": "user"
      },
      "summary": "Large multimodal models (LMMs) have shown great potential for video reasoning with textual Chain-of-Thought. However, they remain vulnerable to hallucinations, especially when processing long-form videos where evidence is sparse and temporally dispersed. Inspired by how humans comprehend long videos - by first skimming globally and then examining relevant clips for details - we introduce LongVT, an end-to-end agentic framework that enables \"Thinking with Long Videos\" via interleaved Multimodal Chain-of-Tool-Thought. Specifically, we exploit LMMs' inherent temporal grounding ability as a native video cropping tool to zoom in on a specific video clip and resample finer-grained video frames. This global-to-local reasoning loop continues until answers are grounded in retrieved visual evidence. Given the scarcity of fine-grained question-answering (QA) data for the long video reasoning task, we curate and will release a data suite named VideoSIAH to facilitate both training and evaluation. Specifically, our training dataset consists of 247.9K samples for tool-integrated cold-start supervised fine-tuning, 1.6K samples for agentic reinforcement learning, and 15.4K samples for agentic reinforcement fine-tuning, respectively. Our evaluation benchmark consists of 1,280 QA pairs that are carefully curated through a semi-automatic data pipeline with human-in-the-loop validation. With a meticulously designed three-stage training strategy and extensive empirical validation, LongVT consistently outperforms existing strong baselines across four challenging long-video understanding and reasoning benchmarks. Our codes, data, and model checkpoints are publicly available at https://github.com/EvolvingLMMs-Lab/LongVT .",
      "upvotes": 77,
      "discussionId": "692d430f4397b1ec214f6978",
      "projectPage": "https://evolvinglmms-lab.github.io/LongVT/",
      "githubRepo": "https://github.com/EvolvingLMMs-Lab/LongVT",
      "ai_summary": "LongVT is an end-to-end framework that enhances video reasoning with textual Chain-of-Thought by using interleaved Multimodal Chain-of-Tool-Thought, enabling global-to-local reasoning and leveraging LMMs for video cropping and frame resampling.",
      "ai_keywords": [
        "LMMs",
        "video reasoning",
        "textual Chain-of-Thought",
        "hallucinations",
        "long-form videos",
        "temporal grounding",
        "Multimodal Chain-of-Tool-Thought",
        "end-to-end agentic framework",
        "global-to-local reasoning",
        "video cropping",
        "frame resampling",
        "VideoSIAH",
        "question-answering",
        "tool-integrated cold-start supervised fine-tuning",
        "agentic reinforcement learning",
        "agentic reinforcement fine-tuning",
        "long-video understanding",
        "reasoning benchmarks"
      ],
      "githubStars": 21,
      "organization": {
        "_id": "6583eb89bed3689928f5d845",
        "name": "lmms-lab",
        "fullname": "LMMs-Lab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d3f7d84b0933c48f3cdd9c/RpVWux8y4hHjNO6guD6sp.png"
      }
    },
    "publishedAt": "2025-11-25T14:22:48.000Z",
    "title": "LongVT: Incentivizing \"Thinking with Long Videos\" via Native Tool Calling",
    "summary": "Large multimodal models (LMMs) have shown great potential for video reasoning with textual Chain-of-Thought. However, they remain vulnerable to hallucinations, especially when processing long-form videos where evidence is sparse and temporally dispersed. Inspired by how humans comprehend long videos - by first skimming globally and then examining relevant clips for details - we introduce LongVT, an end-to-end agentic framework that enables \"Thinking with Long Videos\" via interleaved Multimodal Chain-of-Tool-Thought. Specifically, we exploit LMMs' inherent temporal grounding ability as a native video cropping tool to zoom in on a specific video clip and resample finer-grained video frames. This global-to-local reasoning loop continues until answers are grounded in retrieved visual evidence. Given the scarcity of fine-grained question-answering (QA) data for the long video reasoning task, we curate and will release a data suite named VideoSIAH to facilitate both training and evaluation. Specifically, our training dataset consists of 247.9K samples for tool-integrated cold-start supervised fine-tuning, 1.6K samples for agentic reinforcement learning, and 15.4K samples for agentic reinforcement fine-tuning, respectively. Our evaluation benchmark consists of 1,280 QA pairs that are carefully curated through a semi-automatic data pipeline with human-in-the-loop validation. With a meticulously designed three-stage training strategy and extensive empirical validation, LongVT consistently outperforms existing strong baselines across four challenging long-video understanding and reasoning benchmarks. Our codes, data, and model checkpoints are publicly available at https://github.com/EvolvingLMMs-Lab/LongVT .",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6524d665ab1416594149e07e/SZBdiFzskclJytgjtsLNu.gif"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20785.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6524d665ab1416594149e07e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6524d665ab1416594149e07e/KMsCaAtV0DLC4tqN8f2a7.png",
      "fullname": "Zuhao Yang",
      "name": "mwxely",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "organization": {
      "_id": "6583eb89bed3689928f5d845",
      "name": "lmms-lab",
      "fullname": "LMMs-Lab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d3f7d84b0933c48f3cdd9c/RpVWux8y4hHjNO6guD6sp.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2512.01816",
      "authors": [
        {
          "_id": "692e5c0537312eaa83fd87b8",
          "name": "Juanxi Tian",
          "hidden": false
        },
        {
          "_id": "692e5c0537312eaa83fd87b9",
          "name": "Siyuan Li",
          "hidden": false
        },
        {
          "_id": "692e5c0537312eaa83fd87ba",
          "name": "Conghui He",
          "hidden": false
        },
        {
          "_id": "692e5c0537312eaa83fd87bb",
          "name": "Lijun Wu",
          "hidden": false
        },
        {
          "_id": "692e5c0537312eaa83fd87bc",
          "name": "Cheng Tan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-01T15:52:31.000Z",
      "submittedOnDailyAt": "2025-12-02T01:31:46.625Z",
      "title": "Envision: Benchmarking Unified Understanding & Generation for Causal World Process Insights",
      "submittedOnDailyBy": {
        "_id": "670880950e79a8b46f7ff9dd",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/670880950e79a8b46f7ff9dd/hA1TLhwlQblkFsq8wLrkB.jpeg",
        "isPro": false,
        "fullname": "Juanxi Tian",
        "user": "Juanxi",
        "type": "user"
      },
      "summary": "Current multimodal models aim to transcend the limitations of single-modality representations by unifying understanding and generation, often using text-to-image (T2I) tasks to calibrate semantic consistency. However, their reliance on static, single-image generation in training and evaluation leads to overfitting to static pattern matching and semantic fusion, while fundamentally hindering their ability to model dynamic processes that unfold over time. To address these constraints, we propose Envision-a causal event progression benchmark for chained text-to-multi-image generation. Grounded in world knowledge and structured by spatiotemporal causality, it reorganizes existing evaluation dimensions and includes 1,000 four-stage prompts spanning six scientific and humanities domains. To transition evaluation from single images to sequential frames and assess whether models truly internalize world knowledge while adhering to causal-temporal constraints, we introduce Envision-Score, a holistic metric integrating multi-dimensional consistency, physicality, and aesthetics. Comprehensive evaluation of 15 models (10 specialized T2I models, 5 unified models) uncovers: specialized T2I models demonstrate proficiency in aesthetic rendering yet lack intrinsic world knowledge. Unified multimodal models bridge this gap, consistently outperforming specialized counterparts in causal narrative coherence. However, even these unified architectures remain subordinate to closed-source models and struggle to overcome the core challenge of spatiotemporal consistency. This demonstrates that a focus on causally-isolated single images impedes multi-frame reasoning and generation, promoting static pattern matching over dynamic world modeling-ultimately limiting world knowledge internalization, generation.",
      "upvotes": 58,
      "discussionId": "692e5c0537312eaa83fd87bd",
      "projectPage": "https://opendatalab-raiser.github.io/Envision/",
      "githubRepo": "https://github.com/opendatalab-raiser/Envision",
      "ai_summary": "A benchmark for chained text-to-multi-image generation assesses models' ability to model dynamic causal processes and world knowledge, revealing that unified multimodal models outperform specialized ones but still struggle with spatiotemporal consistency.",
      "ai_keywords": [
        "multimodal models",
        "text-to-image (T2I)",
        "causal event progression",
        "spatiotemporal causality",
        "Envision-a",
        "Envision-Score",
        "multi-dimensional consistency",
        "physicality",
        "aesthetics",
        "causal narrative coherence",
        "spatiotemporal consistency",
        "multi-frame reasoning",
        "dynamic world modeling"
      ],
      "githubStars": 10,
      "organization": {
        "_id": "66ce9d1f5e180b9b9c8e6f31",
        "name": "opendatalab",
        "fullname": "OpenDataLab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/639c3afa7432f2f5d16b7296/yqxxBknyeqkGnYsjoaR4M.png"
      }
    },
    "publishedAt": "2025-12-01T10:52:31.000Z",
    "title": "Envision: Benchmarking Unified Understanding & Generation for Causal World Process Insights",
    "summary": "Current multimodal models aim to transcend the limitations of single-modality representations by unifying understanding and generation, often using text-to-image (T2I) tasks to calibrate semantic consistency. However, their reliance on static, single-image generation in training and evaluation leads to overfitting to static pattern matching and semantic fusion, while fundamentally hindering their ability to model dynamic processes that unfold over time. To address these constraints, we propose Envision-a causal event progression benchmark for chained text-to-multi-image generation. Grounded in world knowledge and structured by spatiotemporal causality, it reorganizes existing evaluation dimensions and includes 1,000 four-stage prompts spanning six scientific and humanities domains. To transition evaluation from single images to sequential frames and assess whether models truly internalize world knowledge while adhering to causal-temporal constraints, we introduce Envision-Score, a holistic metric integrating multi-dimensional consistency, physicality, and aesthetics. Comprehensive evaluation of 15 models (10 specialized T2I models, 5 unified models) uncovers: specialized T2I models demonstrate proficiency in aesthetic rendering yet lack intrinsic world knowledge. Unified multimodal models bridge this gap, consistently outperforming specialized counterparts in causal narrative coherence. However, even these unified architectures remain subordinate to closed-source models and struggle to overcome the core challenge of spatiotemporal consistency. This demonstrates that a focus on causally-isolated single images impedes multi-frame reasoning and generation, promoting static pattern matching over dynamic world modeling-ultimately limiting world knowledge internalization, generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.01816.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "670880950e79a8b46f7ff9dd",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/670880950e79a8b46f7ff9dd/hA1TLhwlQblkFsq8wLrkB.jpeg",
      "fullname": "Juanxi Tian",
      "name": "Juanxi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "organization": {
      "_id": "66ce9d1f5e180b9b9c8e6f31",
      "name": "opendatalab",
      "fullname": "OpenDataLab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/639c3afa7432f2f5d16b7296/yqxxBknyeqkGnYsjoaR4M.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.01374",
      "authors": [
        {
          "_id": "692e6bf937312eaa83fd8890",
          "name": "Chujie Zheng",
          "hidden": false
        },
        {
          "_id": "692e6bf937312eaa83fd8891",
          "name": "Kai Dang",
          "hidden": false
        },
        {
          "_id": "692e6bf937312eaa83fd8892",
          "name": "Bowen Yu",
          "hidden": false
        },
        {
          "_id": "692e6bf937312eaa83fd8893",
          "name": "Mingze Li",
          "hidden": false
        },
        {
          "_id": "692e6bf937312eaa83fd8894",
          "name": "Huiqiang Jiang",
          "hidden": false
        },
        {
          "_id": "692e6bf937312eaa83fd8895",
          "name": "Junrong Lin",
          "hidden": false
        },
        {
          "_id": "692e6bf937312eaa83fd8896",
          "name": "Yuqiong Liu",
          "hidden": false
        },
        {
          "_id": "692e6bf937312eaa83fd8897",
          "name": "An Yang",
          "hidden": false
        },
        {
          "_id": "692e6bf937312eaa83fd8898",
          "name": "Jingren Zhou",
          "hidden": false
        },
        {
          "_id": "692e6bf937312eaa83fd8899",
          "name": "Junyang Lin",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63d9d68c1cae35c27bf7a6a7/sMajVHMznJ4kLJvdY1HwJ.png"
      ],
      "publishedAt": "2025-12-01T07:45:39.000Z",
      "submittedOnDailyAt": "2025-12-02T02:47:49.367Z",
      "title": "Stabilizing Reinforcement Learning with LLMs: Formulation and Practices",
      "submittedOnDailyBy": {
        "_id": "63d9d68c1cae35c27bf7a6a7",
        "avatarUrl": "/avatars/b5ad98cf269ae5f1fe90861fb4170fae.svg",
        "isPro": false,
        "fullname": "Bowen Yu",
        "user": "Tigerph",
        "type": "user"
      },
      "summary": "This paper proposes a novel formulation for reinforcement learning (RL) with large language models, explaining why and under what conditions the true sequence-level reward can be optimized via a surrogate token-level objective in policy gradient methods such as REINFORCE. Specifically, through a first-order approximation, we show that this surrogate becomes increasingly valid only when both the training-inference discrepancy and policy staleness are minimized. This insight provides a principled explanation for the crucial role of several widely adopted techniques in stabilizing RL training, including importance sampling correction, clipping, and particularly Routing Replay for Mixture-of-Experts (MoE) models. Through extensive experiments with a 30B MoE model totaling hundreds of thousands of GPU hours, we show that for on-policy training, the basic policy gradient algorithm with importance sampling correction achieves the highest training stability. When off-policy updates are introduced to accelerate convergence, combining clipping and Routing Replay becomes essential to mitigate the instability caused by policy staleness. Notably, once training is stabilized, prolonged optimization consistently yields comparable final performance regardless of cold-start initialization. We hope that the shared insights and the developed recipes for stable RL training will facilitate future research.",
      "upvotes": 33,
      "discussionId": "692e6bfa37312eaa83fd889a",
      "ai_summary": "The paper provides a theoretical foundation for optimizing sequence-level rewards in reinforcement learning using token-level objectives, highlighting the importance of techniques like importance sampling correction, clipping, and Routing Replay for stabilizing training, especially with large language models.",
      "ai_keywords": [
        "reinforcement learning",
        "large language models",
        "sequence-level reward",
        "token-level objective",
        "policy gradient methods",
        "REINFORCE",
        "first-order approximation",
        "training-inference discrepancy",
        "policy staleness",
        "importance sampling correction",
        "clipping",
        "Routing Replay",
        "Mixture-of-Experts",
        "on-policy training",
        "off-policy updates"
      ],
      "organization": {
        "_id": "64c8b5837fe12ecd0a7e92eb",
        "name": "Qwen",
        "fullname": "Qwen",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"
      }
    },
    "publishedAt": "2025-12-01T02:45:39.000Z",
    "title": "Stabilizing Reinforcement Learning with LLMs: Formulation and Practices",
    "summary": "This paper proposes a novel formulation for reinforcement learning (RL) with large language models, explaining why and under what conditions the true sequence-level reward can be optimized via a surrogate token-level objective in policy gradient methods such as REINFORCE. Specifically, through a first-order approximation, we show that this surrogate becomes increasingly valid only when both the training-inference discrepancy and policy staleness are minimized. This insight provides a principled explanation for the crucial role of several widely adopted techniques in stabilizing RL training, including importance sampling correction, clipping, and particularly Routing Replay for Mixture-of-Experts (MoE) models. Through extensive experiments with a 30B MoE model totaling hundreds of thousands of GPU hours, we show that for on-policy training, the basic policy gradient algorithm with importance sampling correction achieves the highest training stability. When off-policy updates are introduced to accelerate convergence, combining clipping and Routing Replay becomes essential to mitigate the instability caused by policy staleness. Notably, once training is stabilized, prolonged optimization consistently yields comparable final performance regardless of cold-start initialization. We hope that the shared insights and the developed recipes for stable RL training will facilitate future research.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63d9d68c1cae35c27bf7a6a7/sMajVHMznJ4kLJvdY1HwJ.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.01374.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63d9d68c1cae35c27bf7a6a7",
      "avatarUrl": "/avatars/b5ad98cf269ae5f1fe90861fb4170fae.svg",
      "fullname": "Bowen Yu",
      "name": "Tigerph",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 15
    },
    "organization": {
      "_id": "64c8b5837fe12ecd0a7e92eb",
      "name": "Qwen",
      "fullname": "Qwen",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.20614",
      "authors": [
        {
          "_id": "692e3f7937312eaa83fd8713",
          "name": "Ziheng Ouyang",
          "hidden": false
        },
        {
          "_id": "692e3f7937312eaa83fd8714",
          "name": "Yiren Song",
          "hidden": false
        },
        {
          "_id": "692e3f7937312eaa83fd8715",
          "name": "Yaoli Liu",
          "hidden": false
        },
        {
          "_id": "692e3f7937312eaa83fd8716",
          "name": "Shihao Zhu",
          "hidden": false
        },
        {
          "_id": "692e3f7937312eaa83fd8717",
          "name": "Qibin Hou",
          "hidden": false
        },
        {
          "_id": "692e3f7937312eaa83fd8718",
          "name": "Ming-Ming Cheng",
          "hidden": false
        },
        {
          "_id": "692e3f7937312eaa83fd8719",
          "name": "Mike Zheng Shou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-25T18:40:25.000Z",
      "submittedOnDailyAt": "2025-12-02T01:36:58.442Z",
      "title": "The Consistency Critic: Correcting Inconsistencies in Generated Images via Reference-Guided Attentive Alignment",
      "submittedOnDailyBy": {
        "_id": "673c566071f89d461772aa36",
        "avatarUrl": "/avatars/aff6378a57272e1c34bfcd827f39211a.svg",
        "isPro": true,
        "fullname": "Ouyang Ziheng",
        "user": "ziheng1234",
        "type": "user"
      },
      "summary": "Previous works have explored various customized generation tasks given a reference image, but they still face limitations in generating consistent fine-grained details. In this paper, our aim is to solve the inconsistency problem of generated images by applying a reference-guided post-editing approach and present our ImageCritic. We first construct a dataset of reference-degraded-target triplets obtained via VLM-based selection and explicit degradation, which effectively simulates the common inaccuracies or inconsistencies observed in existing generation models. Furthermore, building on a thorough examination of the model's attention mechanisms and intrinsic representations, we accordingly devise an attention alignment loss and a detail encoder to precisely rectify inconsistencies. ImageCritic can be integrated into an agent framework to automatically detect inconsistencies and correct them with multi-round and local editing in complex scenarios. Extensive experiments demonstrate that ImageCritic can effectively resolve detail-related issues in various customized generation scenarios, providing significant improvements over existing methods.",
      "upvotes": 32,
      "discussionId": "692e3f7937312eaa83fd871a",
      "projectPage": "https://ouyangziheng.github.io/ImageCritic-Page/",
      "githubRepo": "https://github.com/HVision-NKU/ImageCritic",
      "ai_summary": "ImageCritic addresses image generation inconsistencies using reference-guided post-editing with attention alignment loss and a detail encoder, improving fine-grained detail accuracy.",
      "ai_keywords": [
        "VLM-based selection",
        "attention mechanisms",
        "attention alignment loss",
        "detail encoder",
        "multi-round editing",
        "local editing"
      ],
      "githubStars": 45,
      "organization": {
        "_id": "692e70ad81f7a906bb49bdf7",
        "name": "oyzh1234",
        "fullname": "Nankai University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/673c566071f89d461772aa36/H7LhszDDMa1tJpN32MoRJ.png"
      }
    },
    "publishedAt": "2025-11-25T13:40:25.000Z",
    "title": "The Consistency Critic: Correcting Inconsistencies in Generated Images via Reference-Guided Attentive Alignment",
    "summary": "Previous works have explored various customized generation tasks given a reference image, but they still face limitations in generating consistent fine-grained details. In this paper, our aim is to solve the inconsistency problem of generated images by applying a reference-guided post-editing approach and present our ImageCritic. We first construct a dataset of reference-degraded-target triplets obtained via VLM-based selection and explicit degradation, which effectively simulates the common inaccuracies or inconsistencies observed in existing generation models. Furthermore, building on a thorough examination of the model's attention mechanisms and intrinsic representations, we accordingly devise an attention alignment loss and a detail encoder to precisely rectify inconsistencies. ImageCritic can be integrated into an agent framework to automatically detect inconsistencies and correct them with multi-round and local editing in complex scenarios. Extensive experiments demonstrate that ImageCritic can effectively resolve detail-related issues in various customized generation scenarios, providing significant improvements over existing methods.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20614.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "673c566071f89d461772aa36",
      "avatarUrl": "/avatars/aff6378a57272e1c34bfcd827f39211a.svg",
      "fullname": "Ouyang Ziheng",
      "name": "ziheng1234",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "692e70ad81f7a906bb49bdf7",
      "name": "oyzh1234",
      "fullname": "Nankai University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/673c566071f89d461772aa36/H7LhszDDMa1tJpN32MoRJ.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.00425",
      "authors": [
        {
          "_id": "692e5bac37312eaa83fd87b2",
          "name": "Minh-Quan Le",
          "hidden": false
        },
        {
          "_id": "692e5bac37312eaa83fd87b3",
          "name": "Yuanzhi Zhu",
          "hidden": false
        },
        {
          "_id": "692e5bac37312eaa83fd87b4",
          "name": "Vicky Kalogeiton",
          "hidden": false
        },
        {
          "_id": "692e5bac37312eaa83fd87b5",
          "name": "Dimitris Samaras",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6618413e625ae15f64769476/jw3Z310XN2gwekaTButN_.gif"
      ],
      "publishedAt": "2025-11-29T10:04:50.000Z",
      "submittedOnDailyAt": "2025-12-02T00:55:05.588Z",
      "title": "What about gravity in video generation? Post-Training Newton's Laws with Verifiable Rewards",
      "submittedOnDailyBy": {
        "_id": "6618413e625ae15f64769476",
        "avatarUrl": "/avatars/1585703eab341f7808e7bf703ffef9dc.svg",
        "isPro": true,
        "fullname": "Minh-Quan Le",
        "user": "lmquan",
        "type": "user"
      },
      "summary": "Recent video diffusion models can synthesize visually compelling clips, yet often violate basic physical laws-objects float, accelerations drift, and collisions behave inconsistently-revealing a persistent gap between visual realism and physical realism. We propose NewtonRewards, the first physics-grounded post-training framework for video generation based on verifiable rewards. Instead of relying on human or VLM feedback, NewtonRewards extracts measurable proxies from generated videos using frozen utility models: optical flow serves as a proxy for velocity, while high-level appearance features serve as a proxy for mass. These proxies enable explicit enforcement of Newtonian structure through two complementary rewards: a Newtonian kinematic constraint enforcing constant-acceleration dynamics, and a mass conservation reward preventing trivial, degenerate solutions. We evaluate NewtonRewards on five Newtonian Motion Primitives (free fall, horizontal/parabolic throw, and ramp sliding down/up) using our newly constructed large-scale benchmark, NewtonBench-60K. Across all primitives in visual and physics metrics, NewtonRewards consistently improves physical plausibility, motion smoothness, and temporal coherence over prior post-training methods. It further maintains strong performance under out-of-distribution shifts in height, speed, and friction. Our results show that physics-grounded verifiable rewards offer a scalable path toward physics-aware video generation.",
      "upvotes": 27,
      "discussionId": "692e5bad37312eaa83fd87b6",
      "projectPage": "https://cvlab-stonybrook.github.io/NewtonRewards/",
      "githubRepo": "https://github.com/cvlab-stonybrook/NewtonRewards",
      "ai_summary": "A physics-grounded post-training framework using verifiable rewards improves physical realism and motion quality in video diffusion models.",
      "ai_keywords": [
        "video diffusion models",
        "NewtonRewards",
        "verifiable rewards",
        "optical flow",
        "high-level appearance features",
        "Newtonian kinematic constraint",
        "mass conservation reward",
        "Newtonian Motion Primitives",
        "NewtonBench-60K",
        "physics plausibility",
        "motion smoothness",
        "temporal coherence"
      ],
      "githubStars": 1,
      "organization": {
        "_id": "670442e8d8f4953b0334f1f7",
        "name": "StonyBrook-CVLab",
        "fullname": "CVLab @ Stony Brook University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/636c3e2815cd58e915bd6407/xTSHQ9ZJgqAPPrTRfOWY-.png"
      }
    },
    "publishedAt": "2025-11-29T05:04:50.000Z",
    "title": "What about gravity in video generation? Post-Training Newton's Laws with Verifiable Rewards",
    "summary": "Recent video diffusion models can synthesize visually compelling clips, yet often violate basic physical laws-objects float, accelerations drift, and collisions behave inconsistently-revealing a persistent gap between visual realism and physical realism. We propose NewtonRewards, the first physics-grounded post-training framework for video generation based on verifiable rewards. Instead of relying on human or VLM feedback, NewtonRewards extracts measurable proxies from generated videos using frozen utility models: optical flow serves as a proxy for velocity, while high-level appearance features serve as a proxy for mass. These proxies enable explicit enforcement of Newtonian structure through two complementary rewards: a Newtonian kinematic constraint enforcing constant-acceleration dynamics, and a mass conservation reward preventing trivial, degenerate solutions. We evaluate NewtonRewards on five Newtonian Motion Primitives (free fall, horizontal/parabolic throw, and ramp sliding down/up) using our newly constructed large-scale benchmark, NewtonBench-60K. Across all primitives in visual and physics metrics, NewtonRewards consistently improves physical plausibility, motion smoothness, and temporal coherence over prior post-training methods. It further maintains strong performance under out-of-distribution shifts in height, speed, and friction. Our results show that physics-grounded verifiable rewards offer a scalable path toward physics-aware video generation.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6618413e625ae15f64769476/jw3Z310XN2gwekaTButN_.gif"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.00425.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6618413e625ae15f64769476",
      "avatarUrl": "/avatars/1585703eab341f7808e7bf703ffef9dc.svg",
      "fullname": "Minh-Quan Le",
      "name": "lmquan",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "670442e8d8f4953b0334f1f7",
      "name": "StonyBrook-CVLab",
      "fullname": "CVLab @ Stony Brook University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/636c3e2815cd58e915bd6407/xTSHQ9ZJgqAPPrTRfOWY-.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.01948",
      "authors": [
        {
          "_id": "692e62d937312eaa83fd87e7",
          "name": "Dingling Zhang",
          "hidden": false
        },
        {
          "_id": "692e62d937312eaa83fd87e8",
          "name": "He Zhu",
          "hidden": false
        },
        {
          "_id": "692e62d937312eaa83fd87e9",
          "name": "Jincheng Ren",
          "hidden": false
        },
        {
          "_id": "692e62d937312eaa83fd87ea",
          "name": "Kangqi Song",
          "hidden": false
        },
        {
          "_id": "692e62d937312eaa83fd87eb",
          "name": "Xinran Zhou",
          "hidden": false
        },
        {
          "_id": "692e62d937312eaa83fd87ec",
          "name": "Boyu Feng",
          "hidden": false
        },
        {
          "_id": "692e62d937312eaa83fd87ed",
          "name": "Shudong Liu",
          "hidden": false
        },
        {
          "_id": "692e62d937312eaa83fd87ee",
          "name": "Jiabin Luo",
          "hidden": false
        },
        {
          "_id": "692e62d937312eaa83fd87ef",
          "name": "Weihao Xie",
          "hidden": false
        },
        {
          "_id": "692e62d937312eaa83fd87f0",
          "name": "Zhaohui Wang",
          "hidden": false
        },
        {
          "_id": "692e62d937312eaa83fd87f1",
          "name": "Tianrui Qin",
          "hidden": false
        },
        {
          "_id": "692e62d937312eaa83fd87f2",
          "name": "King Zhu",
          "hidden": false
        },
        {
          "_id": "692e62d937312eaa83fd87f3",
          "name": "Yuqing Wang",
          "hidden": false
        },
        {
          "_id": "692e62d937312eaa83fd87f4",
          "name": "Qianben Chen",
          "hidden": false
        },
        {
          "_id": "692e62d937312eaa83fd87f5",
          "name": "Yuchen Eleanor Jiang",
          "hidden": false
        },
        {
          "_id": "692e62d937312eaa83fd87f6",
          "name": "Wei Wang",
          "hidden": false
        },
        {
          "_id": "692e62d937312eaa83fd87f7",
          "name": "Jiaheng Liu",
          "hidden": false
        },
        {
          "_id": "692e62d937312eaa83fd87f8",
          "name": "Wangchunshu Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-01T17:58:59.000Z",
      "submittedOnDailyAt": "2025-12-02T01:25:02.560Z",
      "title": "How Far Are We from Genuinely Useful Deep Research Agents?",
      "submittedOnDailyBy": {
        "_id": "65377c30e48353201e6fdda0",
        "avatarUrl": "/avatars/a8f803b6f2e598eaee9c52c0d2ddfc16.svg",
        "isPro": false,
        "fullname": "Jiaheng Liu",
        "user": "CheeryLJH",
        "type": "user"
      },
      "summary": "Deep Research Agents (DRAs) aim to automatically produce analyst-level reports through iterative information retrieval and synthesis. However, most existing DRAs were validated on question-answering benchmarks, while research on generating comprehensive reports remains overlooked. Worse, current benchmarks for report synthesis suffer from task complexity and subjective metrics -- this fails to reflect user demands and limits the practical utility of generated reports. To address these gaps, we present Fine-grained DEepResearch bench (FINDER), an enhanced benchmark consisting of 100 human-curated research tasks with 419 structured checklist items that standardize report structure, analytical depth, and factual grounding. Based on approximately 1,000 reports produced by mainstream DRAs, we further propose Deep rEsearch Failure Taxonomy (DEFT), the first failure taxonomy for deep research agents. DEFT contains 14 fine-grained failure modes across reasoning, retrieval, and generation, and is built upon grounded theory with human-LLM co-annotating and inter-annotator reliability validation. Our experimental findings reveal that current DRAs struggle not with task comprehension but with evidence integration, verification, and reasoning-resilient planning.",
      "upvotes": 25,
      "discussionId": "692e62d937312eaa83fd87f9",
      "githubRepo": "https://github.com/OPPO-PersonalAI/FINDER_DEFT",
      "ai_summary": "FINDER is a benchmark for deep research agents with standardized human-curated tasks and DEFT is a failure taxonomy revealing that DRAs struggle with evidence integration, verification, and reasoning-resilient planning.",
      "ai_keywords": [
        "Deep Research Agents",
        "Fine-grained DEepResearch bench",
        "structured checklist items",
        "Deep rEsearch Failure Taxonomy",
        "reasoning",
        "retrieval",
        "generation",
        "failure modes",
        "reasoning-resilient planning"
      ],
      "githubStars": 5
    },
    "publishedAt": "2025-12-01T12:58:59.000Z",
    "title": "How Far Are We from Genuinely Useful Deep Research Agents?",
    "summary": "Deep Research Agents (DRAs) aim to automatically produce analyst-level reports through iterative information retrieval and synthesis. However, most existing DRAs were validated on question-answering benchmarks, while research on generating comprehensive reports remains overlooked. Worse, current benchmarks for report synthesis suffer from task complexity and subjective metrics -- this fails to reflect user demands and limits the practical utility of generated reports. To address these gaps, we present Fine-grained DEepResearch bench (FINDER), an enhanced benchmark consisting of 100 human-curated research tasks with 419 structured checklist items that standardize report structure, analytical depth, and factual grounding. Based on approximately 1,000 reports produced by mainstream DRAs, we further propose Deep rEsearch Failure Taxonomy (DEFT), the first failure taxonomy for deep research agents. DEFT contains 14 fine-grained failure modes across reasoning, retrieval, and generation, and is built upon grounded theory with human-LLM co-annotating and inter-annotator reliability validation. Our experimental findings reveal that current DRAs struggle not with task comprehension but with evidence integration, verification, and reasoning-resilient planning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.01948.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65377c30e48353201e6fdda0",
      "avatarUrl": "/avatars/a8f803b6f2e598eaee9c52c0d2ddfc16.svg",
      "fullname": "Jiaheng Liu",
      "name": "CheeryLJH",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 23
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.20649",
      "authors": [
        {
          "_id": "692d43344397b1ec214f697a",
          "name": "Hidir Yesiltepe",
          "hidden": false
        },
        {
          "_id": "692d43344397b1ec214f697b",
          "name": "Tuna Han Salih Meral",
          "hidden": false
        },
        {
          "_id": "692d43344397b1ec214f697c",
          "name": "Adil Kaan Akan",
          "hidden": false
        },
        {
          "_id": "692d43344397b1ec214f697d",
          "name": "Kaan Oktay",
          "hidden": false
        },
        {
          "_id": "692d43344397b1ec214f697e",
          "name": "Pinar Yanardag",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63597e84d72fc0539e72b507/7tIfsLhWroBq-ylcyUE6_.mp4"
      ],
      "publishedAt": "2025-11-25T18:59:46.000Z",
      "submittedOnDailyAt": "2025-12-02T00:34:00.542Z",
      "title": "Infinity-RoPE: Action-Controllable Infinite Video Generation Emerges From Autoregressive Self-Rollout",
      "submittedOnDailyBy": {
        "_id": "63597e84d72fc0539e72b507",
        "avatarUrl": "/avatars/f568302b70064220e3c824577e5bece4.svg",
        "isPro": false,
        "fullname": "Hidir Yesiltepe",
        "user": "Hidir",
        "type": "user"
      },
      "summary": "Current autoregressive video diffusion models are constrained by three core bottlenecks: (i) the finite temporal horizon imposed by the base model's 3D Rotary Positional Embedding (3D-RoPE), (ii) slow prompt responsiveness in maintaining fine-grained action control during long-form rollouts, and (iii) the inability to realize discontinuous cinematic transitions within a single generation stream. We introduce infty-RoPE, a unified inference-time framework that addresses all three limitations through three interconnected components: Block-Relativistic RoPE, KV Flush, and RoPE Cut. Block-Relativistic RoPE reformulates temporal encoding as a moving local reference frame, where each newly generated latent block is rotated relative to the base model's maximum frame horizon while earlier blocks are rotated backward to preserve relative temporal geometry. This relativistic formulation eliminates fixed temporal positions, enabling continuous video generation far beyond the base positional limits. To obtain fine-grained action control without re-encoding, KV Flush renews the KV cache by retaining only two latent frames, the global sink and the last generated latent frame, thereby ensuring immediate prompt responsiveness. Finally, RoPE Cut introduces controlled discontinuities in temporal RoPE coordinates, enabling multi-cut scene transitions within a single continuous rollout. Together, these components establish infty-RoPE as a training-free foundation for infinite-horizon, controllable, and cinematic video diffusion. Comprehensive experiments show that infty-RoPE consistently surpasses previous autoregressive models in overall VBench scores.",
      "upvotes": 22,
      "discussionId": "692d43354397b1ec214f697f",
      "ai_summary": "A new inference-time framework, $\\infty$-RoPE, addresses limitations in autoregressive video diffusion models by enabling continuous video generation, fine-grained action control, and cinematic transitions without retraining.",
      "ai_keywords": [
        "autoregressive video diffusion models",
        "3D Rotary Positional Embedding",
        "3D-RoPE",
        "Block-Relativistic RoPE",
        "KV Flush",
        "RoPE Cut",
        "infinite-horizon",
        "controllable",
        "cinematic video diffusion",
        "VBench scores"
      ]
    },
    "publishedAt": "2025-11-25T13:59:46.000Z",
    "title": "Infinity-RoPE: Action-Controllable Infinite Video Generation Emerges From Autoregressive Self-Rollout",
    "summary": "Current autoregressive video diffusion models are constrained by three core bottlenecks: (i) the finite temporal horizon imposed by the base model's 3D Rotary Positional Embedding (3D-RoPE), (ii) slow prompt responsiveness in maintaining fine-grained action control during long-form rollouts, and (iii) the inability to realize discontinuous cinematic transitions within a single generation stream. We introduce infty-RoPE, a unified inference-time framework that addresses all three limitations through three interconnected components: Block-Relativistic RoPE, KV Flush, and RoPE Cut. Block-Relativistic RoPE reformulates temporal encoding as a moving local reference frame, where each newly generated latent block is rotated relative to the base model's maximum frame horizon while earlier blocks are rotated backward to preserve relative temporal geometry. This relativistic formulation eliminates fixed temporal positions, enabling continuous video generation far beyond the base positional limits. To obtain fine-grained action control without re-encoding, KV Flush renews the KV cache by retaining only two latent frames, the global sink and the last generated latent frame, thereby ensuring immediate prompt responsiveness. Finally, RoPE Cut introduces controlled discontinuities in temporal RoPE coordinates, enabling multi-cut scene transitions within a single continuous rollout. Together, these components establish infty-RoPE as a training-free foundation for infinite-horizon, controllable, and cinematic video diffusion. Comprehensive experiments show that infty-RoPE consistently surpasses previous autoregressive models in overall VBench scores.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63597e84d72fc0539e72b507/7tIfsLhWroBq-ylcyUE6_.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20649.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63597e84d72fc0539e72b507",
      "avatarUrl": "/avatars/f568302b70064220e3c824577e5bece4.svg",
      "fullname": "Hidir Yesiltepe",
      "name": "Hidir",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.02014",
      "authors": [
        {
          "_id": "692e56a137312eaa83fd8761",
          "name": "Zhiheng Liu",
          "hidden": false
        },
        {
          "_id": "692e56a137312eaa83fd8762",
          "name": "Weiming Ren",
          "hidden": false
        },
        {
          "_id": "692e56a137312eaa83fd8763",
          "name": "Haozhe Liu",
          "hidden": false
        },
        {
          "_id": "692e56a137312eaa83fd8764",
          "name": "Zijian Zhou",
          "hidden": false
        },
        {
          "_id": "692e56a137312eaa83fd8765",
          "name": "Shoufa Chen",
          "hidden": false
        },
        {
          "_id": "692e56a137312eaa83fd8766",
          "name": "Haonan Qiu",
          "hidden": false
        },
        {
          "_id": "692e56a137312eaa83fd8767",
          "name": "Xiaoke Huang",
          "hidden": false
        },
        {
          "_id": "692e56a137312eaa83fd8768",
          "name": "Zhaochong An",
          "hidden": false
        },
        {
          "_id": "692e56a137312eaa83fd8769",
          "name": "Fanny Yang",
          "hidden": false
        },
        {
          "_id": "692e56a137312eaa83fd876a",
          "name": "Aditya Patel",
          "hidden": false
        },
        {
          "_id": "692e56a137312eaa83fd876b",
          "name": "Viktar Atliha",
          "hidden": false
        },
        {
          "_id": "692e56a137312eaa83fd876c",
          "name": "Tony Ng",
          "hidden": false
        },
        {
          "_id": "692e56a137312eaa83fd876d",
          "name": "Xiao Han",
          "hidden": false
        },
        {
          "_id": "692e56a137312eaa83fd876e",
          "name": "Chuyan Zhu",
          "hidden": false
        },
        {
          "_id": "692e56a137312eaa83fd876f",
          "name": "Chenyang Zhang",
          "hidden": false
        },
        {
          "_id": "692e56a137312eaa83fd8770",
          "name": "Ding Liu",
          "hidden": false
        },
        {
          "_id": "692e56a137312eaa83fd8771",
          "name": "Juan-Manuel Perez-Rua",
          "hidden": false
        },
        {
          "_id": "692e56a137312eaa83fd8772",
          "name": "Sen He",
          "hidden": false
        },
        {
          "_id": "692e56a137312eaa83fd8773",
          "name": "Jrgen Schmidhuber",
          "hidden": false
        },
        {
          "_id": "692e56a137312eaa83fd8774",
          "name": "Wenhu Chen",
          "hidden": false
        },
        {
          "_id": "692e56a137312eaa83fd8775",
          "name": "Ping Luo",
          "hidden": false
        },
        {
          "_id": "692e56a137312eaa83fd8776",
          "name": "Wei Liu",
          "hidden": false
        },
        {
          "_id": "692e56a137312eaa83fd8777",
          "name": "Tao Xiang",
          "hidden": false
        },
        {
          "_id": "692e56a137312eaa83fd8778",
          "name": "Jonas Schult",
          "hidden": false
        },
        {
          "_id": "692e56a137312eaa83fd8779",
          "name": "Yuren Cong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-01T18:59:51.000Z",
      "submittedOnDailyAt": "2025-12-02T01:20:57.356Z",
      "title": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Unified multimodal models (UMMs) aim to jointly perform multimodal understanding and generation within a single framework. We present TUNA, a native UMM that builds a unified continuous visual representation by cascading a VAE encoder with a representation encoder. This unified representation space allows end-to-end processing of images and videos for both understanding and generation tasks. Compared to prior UMMs with decoupled representations, TUNA's unified visual space avoids representation format mismatches introduced by separate encoders, outperforming decoupled alternatives in both understanding and generation. Moreover, we observe that stronger pretrained representation encoders consistently yield better performance across all multimodal tasks, highlighting the importance of the representation encoder. Finally, in this unified setting, jointly training on both understanding and generation data allows the two tasks to benefit from each other rather than interfere. Our extensive experiments on multimodal understanding and generation benchmarks show that TUNA achieves state-of-the-art results in image and video understanding, image and video generation, and image editing, demonstrating the effectiveness and scalability of its unified representation design.",
      "upvotes": 19,
      "discussionId": "692e56a137312eaa83fd877a",
      "projectPage": "https://tuna-ai.org/",
      "ai_summary": "TUNA, a unified multimodal model, uses a cascaded VAE and representation encoder for end-to-end multimodal understanding and generation, outperforming decoupled models and achieving state-of-the-art results across various benchmarks.",
      "ai_keywords": [
        "Unified multimodal models",
        "UMMs",
        "TUNA",
        "VAE encoder",
        "representation encoder",
        "unified visual representation",
        "multimodal understanding",
        "multimodal generation",
        "representation format mismatches",
        "image understanding",
        "video understanding",
        "image generation",
        "video generation",
        "image editing"
      ],
      "organization": {
        "_id": "5e63d8713071d5be688861b8",
        "name": "facebook",
        "fullname": "AI at Meta",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"
      }
    },
    "publishedAt": "2025-12-01T13:59:51.000Z",
    "title": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models",
    "summary": "Unified multimodal models (UMMs) aim to jointly perform multimodal understanding and generation within a single framework. We present TUNA, a native UMM that builds a unified continuous visual representation by cascading a VAE encoder with a representation encoder. This unified representation space allows end-to-end processing of images and videos for both understanding and generation tasks. Compared to prior UMMs with decoupled representations, TUNA's unified visual space avoids representation format mismatches introduced by separate encoders, outperforming decoupled alternatives in both understanding and generation. Moreover, we observe that stronger pretrained representation encoders consistently yield better performance across all multimodal tasks, highlighting the importance of the representation encoder. Finally, in this unified setting, jointly training on both understanding and generation data allows the two tasks to benefit from each other rather than interfere. Our extensive experiments on multimodal understanding and generation benchmarks show that TUNA achieves state-of-the-art results in image and video understanding, image and video generation, and image editing, demonstrating the effectiveness and scalability of its unified representation design.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.02014.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 173
    },
    "organization": {
      "_id": "5e63d8713071d5be688861b8",
      "name": "facebook",
      "fullname": "AI at Meta",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.01801",
      "authors": [
        {
          "_id": "692e597b37312eaa83fd8791",
          "name": "Yunfei Li",
          "hidden": false
        },
        {
          "_id": "692e597b37312eaa83fd8792",
          "name": "Xiao Ma",
          "hidden": false
        },
        {
          "_id": "692e597b37312eaa83fd8793",
          "name": "Jiafeng Xu",
          "hidden": false
        },
        {
          "_id": "692e597b37312eaa83fd8794",
          "name": "Yu Cui",
          "hidden": false
        },
        {
          "_id": "692e597b37312eaa83fd8795",
          "name": "Zhongren Cui",
          "hidden": false
        },
        {
          "_id": "692e597b37312eaa83fd8796",
          "name": "Zhigang Han",
          "hidden": false
        },
        {
          "_id": "692e597b37312eaa83fd8797",
          "name": "Liqun Huang",
          "hidden": false
        },
        {
          "_id": "692e597b37312eaa83fd8798",
          "name": "Tao Kong",
          "hidden": false
        },
        {
          "_id": "692e597b37312eaa83fd8799",
          "name": "Yuxiao Liu",
          "hidden": false
        },
        {
          "_id": "692e597b37312eaa83fd879a",
          "name": "Hao Niu",
          "hidden": false
        },
        {
          "_id": "692e597b37312eaa83fd879b",
          "name": "Wanli Peng",
          "hidden": false
        },
        {
          "_id": "692e597b37312eaa83fd879c",
          "name": "Jingchao Qiao",
          "hidden": false
        },
        {
          "_id": "692e597b37312eaa83fd879d",
          "name": "Zeyu Ren",
          "hidden": false
        },
        {
          "_id": "692e597b37312eaa83fd879e",
          "name": "Haixin Shi",
          "hidden": false
        },
        {
          "_id": "692e597b37312eaa83fd879f",
          "name": "Zhi Su",
          "hidden": false
        },
        {
          "_id": "692e597b37312eaa83fd87a0",
          "name": "Jiawen Tian",
          "hidden": false
        },
        {
          "_id": "692e597b37312eaa83fd87a1",
          "name": "Yuyang Xiao",
          "hidden": false
        },
        {
          "_id": "692e597b37312eaa83fd87a2",
          "name": "Shenyu Zhang",
          "hidden": false
        },
        {
          "_id": "692e597b37312eaa83fd87a3",
          "name": "Liwei Zheng",
          "hidden": false
        },
        {
          "_id": "692e597b37312eaa83fd87a4",
          "name": "Hang Li",
          "hidden": false
        },
        {
          "_id": "692e597b37312eaa83fd87a5",
          "name": "Yonghui Wu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6478597d91398856110a6738/lB4jLlGfenrS2xb2iQLd_.qt"
      ],
      "publishedAt": "2025-12-01T15:33:59.000Z",
      "submittedOnDailyAt": "2025-12-02T00:50:12.911Z",
      "title": "GR-RL: Going Dexterous and Precise for Long-Horizon Robotic Manipulation",
      "submittedOnDailyBy": {
        "_id": "6478597d91398856110a6738",
        "avatarUrl": "/avatars/c3bc61eb7554ac21946b424e1314f1a7.svg",
        "isPro": false,
        "fullname": "Xiao Ma",
        "user": "yusufma555",
        "type": "user"
      },
      "summary": "We present GR-RL, a robotic learning framework that turns a generalist vision-language-action (VLA) policy into a highly capable specialist for long-horizon dexterous manipulation. Assuming the optimality of human demonstrations is core to existing VLA policies. However, we claim that in highly dexterous and precise manipulation tasks, human demonstrations are noisy and suboptimal. GR-RL proposes a multi-stage training pipeline that filters, augments, and reinforces the demonstrations by reinforcement learning. First, GR-RL learns a vision-language-conditioned task progress, filters the demonstration trajectories, and only keeps the transitions that contribute positively to the progress. Specifically, we show that by directly applying offline RL with sparse reward, the resulting Q-values can be treated as a robust progress function. Next, we introduce morphological symmetry augmentation that greatly improves the generalization and performance of GR-RL. Lastly, to better align the VLA policy with its deployment behaviors for high-precision control, we perform online RL by learning a latent space noise predictor. With this pipeline, GR-RL is, to our knowledge, the first learning-based policy that can autonomously lace up a shoe by threading shoelaces through multiple eyelets with an 83.3% success rate, a task requiring long-horizon reasoning, millimeter-level precision, and compliant soft-body interaction. We hope GR-RL provides a step toward enabling generalist robot foundations models to specialize into reliable real-world experts.",
      "upvotes": 15,
      "discussionId": "692e597b37312eaa83fd87a6",
      "projectPage": "https://seed.bytedance.com/en/gr_rl",
      "ai_summary": "GR-RL enhances a vision-language-action policy for long-horizon dexterous manipulation through a multi-stage training pipeline that filters, augments, and refines demonstrations using reinforcement learning.",
      "ai_keywords": [
        "vision-language-action (VLA) policy",
        "reinforcement learning",
        "task progress",
        "offline RL",
        "sparse reward",
        "$Q$-values",
        "morphological symmetry augmentation",
        "online RL",
        "latent space noise predictor",
        "dexterous manipulation",
        "long-horizon reasoning",
        "millimeter-level precision",
        "compliant soft-body interaction"
      ],
      "organization": {
        "_id": "67d1140985ea0644e2f14b99",
        "name": "ByteDance-Seed",
        "fullname": "ByteDance Seed",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
      }
    },
    "publishedAt": "2025-12-01T10:33:59.000Z",
    "title": "GR-RL: Going Dexterous and Precise for Long-Horizon Robotic Manipulation",
    "summary": "We present GR-RL, a robotic learning framework that turns a generalist vision-language-action (VLA) policy into a highly capable specialist for long-horizon dexterous manipulation. Assuming the optimality of human demonstrations is core to existing VLA policies. However, we claim that in highly dexterous and precise manipulation tasks, human demonstrations are noisy and suboptimal. GR-RL proposes a multi-stage training pipeline that filters, augments, and reinforces the demonstrations by reinforcement learning. First, GR-RL learns a vision-language-conditioned task progress, filters the demonstration trajectories, and only keeps the transitions that contribute positively to the progress. Specifically, we show that by directly applying offline RL with sparse reward, the resulting Q-values can be treated as a robust progress function. Next, we introduce morphological symmetry augmentation that greatly improves the generalization and performance of GR-RL. Lastly, to better align the VLA policy with its deployment behaviors for high-precision control, we perform online RL by learning a latent space noise predictor. With this pipeline, GR-RL is, to our knowledge, the first learning-based policy that can autonomously lace up a shoe by threading shoelaces through multiple eyelets with an 83.3% success rate, a task requiring long-horizon reasoning, millimeter-level precision, and compliant soft-body interaction. We hope GR-RL provides a step toward enabling generalist robot foundations models to specialize into reliable real-world experts.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6478597d91398856110a6738/lB4jLlGfenrS2xb2iQLd_.qt"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.01801.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6478597d91398856110a6738",
      "avatarUrl": "/avatars/c3bc61eb7554ac21946b424e1314f1a7.svg",
      "fullname": "Xiao Ma",
      "name": "yusufma555",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "organization": {
      "_id": "67d1140985ea0644e2f14b99",
      "name": "ByteDance-Seed",
      "fullname": "ByteDance Seed",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.01031",
      "authors": [
        {
          "_id": "692e552137312eaa83fd8755",
          "name": "Jiaming Tang",
          "hidden": false
        },
        {
          "_id": "692e552137312eaa83fd8756",
          "name": "Yufei Sun",
          "hidden": false
        },
        {
          "_id": "692e552137312eaa83fd8757",
          "name": "Yilong Zhao",
          "hidden": false
        },
        {
          "_id": "692e552137312eaa83fd8758",
          "name": "Shang Yang",
          "hidden": false
        },
        {
          "_id": "692e552137312eaa83fd8759",
          "name": "Yujun Lin",
          "hidden": false
        },
        {
          "_id": "692e552137312eaa83fd875a",
          "name": "Zhuoyang Zhang",
          "hidden": false
        },
        {
          "_id": "692e552137312eaa83fd875b",
          "name": "James Hou",
          "hidden": false
        },
        {
          "_id": "692e552137312eaa83fd875c",
          "name": "Yao Lu",
          "hidden": false
        },
        {
          "_id": "692e552137312eaa83fd875d",
          "name": "Zhijian Liu",
          "hidden": false
        },
        {
          "_id": "692e552137312eaa83fd875e",
          "name": "Song Han",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63f30d28f4e30ffd2bda9aff/Qv61_5GunPmU8UN2XNkS6.mp4"
      ],
      "publishedAt": "2025-11-30T18:59:24.000Z",
      "submittedOnDailyAt": "2025-12-02T00:56:43.030Z",
      "title": "VLASH: Real-Time VLAs via Future-State-Aware Asynchronous Inference",
      "submittedOnDailyBy": {
        "_id": "63f30d28f4e30ffd2bda9aff",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676872959244-noauth.jpeg",
        "isPro": false,
        "fullname": "Jiaming Tang",
        "user": "Sakits",
        "type": "user"
      },
      "summary": "Vision-Language-Action models (VLAs) are becoming increasingly capable across diverse robotic tasks. However, their real-world deployment remains slow and inefficient: demonstration videos are often sped up by 5-10x to appear smooth, with noticeable action stalls and delayed reactions to environmental changes. Asynchronous inference offers a promising solution to achieve continuous and low-latency control by enabling robots to execute actions and perform inference simultaneously. However, because the robot and environment continue to evolve during inference, a temporal misalignment arises between the prediction and execution intervals. This leads to significant action instability, while existing methods either degrade accuracy or introduce runtime overhead to mitigate it. We propose VLASH, a general asynchronous inference framework for VLAs that delivers smooth, accurate, and fast reaction control without additional overhead or architectural changes. VLASH estimates the future execution-time state by rolling the robot state forward with the previously generated action chunk, thereby bridging the gap between prediction and execution. Experiments show that VLASH achieves up to 2.03x speedup and reduces reaction latency by up to 17.4x compared to synchronous inference while fully preserving the original accuracy. Moreover, it empowers VLAs to handle fast-reaction, high-precision tasks such as playing ping-pong and playing whack-a-mole, where traditional synchronous inference fails. Code is available at https://github.com/mit-han-lab/vlash",
      "upvotes": 13,
      "discussionId": "692e552137312eaa83fd875f",
      "githubRepo": "https://github.com/mit-han-lab/vlash",
      "ai_summary": "VLASH is an asynchronous inference framework for Vision-Language-Action models that achieves high speed and low latency without sacrificing accuracy, enabling precise robotic tasks like ping-pong and whack-a-mole.",
      "ai_keywords": [
        "asynchronous inference",
        "synchronous inference",
        "temporal misalignment",
        "prediction intervals",
        "action instability",
        "reaction latency",
        "action chunk",
        "robot state",
        "high-precision tasks"
      ],
      "githubStars": 16,
      "organization": {
        "_id": "637971c7dbe83216de4a67a3",
        "name": "mit-han-lab",
        "fullname": "MIT HAN Lab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1668903445024-6362fefe19cf373a5fc5b39e.png"
      }
    },
    "publishedAt": "2025-11-30T13:59:24.000Z",
    "title": "VLASH: Real-Time VLAs via Future-State-Aware Asynchronous Inference",
    "summary": "Vision-Language-Action models (VLAs) are becoming increasingly capable across diverse robotic tasks. However, their real-world deployment remains slow and inefficient: demonstration videos are often sped up by 5-10x to appear smooth, with noticeable action stalls and delayed reactions to environmental changes. Asynchronous inference offers a promising solution to achieve continuous and low-latency control by enabling robots to execute actions and perform inference simultaneously. However, because the robot and environment continue to evolve during inference, a temporal misalignment arises between the prediction and execution intervals. This leads to significant action instability, while existing methods either degrade accuracy or introduce runtime overhead to mitigate it. We propose VLASH, a general asynchronous inference framework for VLAs that delivers smooth, accurate, and fast reaction control without additional overhead or architectural changes. VLASH estimates the future execution-time state by rolling the robot state forward with the previously generated action chunk, thereby bridging the gap between prediction and execution. Experiments show that VLASH achieves up to 2.03x speedup and reduces reaction latency by up to 17.4x compared to synchronous inference while fully preserving the original accuracy. Moreover, it empowers VLAs to handle fast-reaction, high-precision tasks such as playing ping-pong and playing whack-a-mole, where traditional synchronous inference fails. Code is available at https://github.com/mit-han-lab/vlash",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63f30d28f4e30ffd2bda9aff/Qv61_5GunPmU8UN2XNkS6.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.01031.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "63f30d28f4e30ffd2bda9aff",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676872959244-noauth.jpeg",
      "fullname": "Jiaming Tang",
      "name": "Sakits",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "organization": {
      "_id": "637971c7dbe83216de4a67a3",
      "name": "mit-han-lab",
      "fullname": "MIT HAN Lab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1668903445024-6362fefe19cf373a5fc5b39e.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.00722",
      "authors": [
        {
          "_id": "692e597537312eaa83fd8788",
          "name": "Jiaming Xu",
          "hidden": false
        },
        {
          "_id": "692e597537312eaa83fd8789",
          "name": "Jiayi Pan",
          "hidden": false
        },
        {
          "_id": "692e597537312eaa83fd878a",
          "name": "Hanzhen Wang",
          "hidden": false
        },
        {
          "_id": "692e597537312eaa83fd878b",
          "name": "Yongkang Zhou",
          "hidden": false
        },
        {
          "_id": "692e597537312eaa83fd878c",
          "name": "Jiancai Ye",
          "hidden": false
        },
        {
          "_id": "692e597537312eaa83fd878d",
          "name": "Yu Wang",
          "hidden": false
        },
        {
          "_id": "692e597537312eaa83fd878e",
          "name": "Guohao Dai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-30T04:32:43.000Z",
      "submittedOnDailyAt": "2025-12-02T00:48:28.087Z",
      "title": "SpeContext: Enabling Efficient Long-context Reasoning with Speculative Context Sparsity in LLMs",
      "submittedOnDailyBy": {
        "_id": "651526b2391bec2898ba9103",
        "avatarUrl": "/avatars/b965b5e5694141b79c4c0b7a98a66a07.svg",
        "isPro": false,
        "fullname": "Jiaming Xu (SII)",
        "user": "MathsCode",
        "type": "user"
      },
      "summary": "In this paper, we point out that the objective of the retrieval algorithms is to align with the LLM, which is similar to the objective of knowledge distillation in LLMs. We analyze the similarity in information focus between the distilled language model(DLM) and the original LLM from the perspective of information theory, and thus propose a novel paradigm that leverages a DLM as the retrieval algorithm. Based on the insight, we present SpeContext, an algorithm and system co-design for long-context reasoning. (1) At the algorithm level, SpeContext proposes lightweight retrieval head based on the head-level attention weights of DLM, achieving > 90% parameters reduction by pruning the redundancy. (2) At the system level, SpeContext designs an asynchronous prefetch dataflow via the elastic loading strategy, effectively overlapping KV cache retrieval with the LLM computation. (3) At the compilation level, SpeContext constructs the theoretical memory model and implements an adaptive memory management system to achieve acceleration by maximizing GPU memory utilization. We deploy and evaluate SpeContext in two resourceconstrained environments, cloud and edge. Extensive experiments show that, compared with the Huggingface framework, SpeContext achieves up to 24.89x throughput improvement in cloud and 10.06x speedup in edge with negligible accuracy loss, pushing the Pareto frontier of accuracy and throughput.",
      "upvotes": 10,
      "discussionId": "692e597537312eaa83fd878f",
      "ai_summary": "SpeContext leverages a distilled language model for efficient long-context reasoning, reducing parameters and improving throughput with minimal accuracy loss in both cloud and edge environments.",
      "ai_keywords": [
        "retrieval algorithms",
        "LLM",
        "knowledge distillation",
        "distilled language model",
        "DLM",
        "long-context reasoning",
        "lightweight retrieval head",
        "head-level attention weights",
        "asynchronous prefetch dataflow",
        "elastic loading strategy",
        "KV cache retrieval",
        "adaptive memory management",
        "GPU memory utilization",
        "Huggingface framework"
      ],
      "organization": {
        "_id": "63e5ef7bf2e9a8f22c515654",
        "name": "SJTU",
        "fullname": "Shanghai Jiao Tong University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"
      }
    },
    "publishedAt": "2025-11-29T23:32:43.000Z",
    "title": "SpeContext: Enabling Efficient Long-context Reasoning with Speculative Context Sparsity in LLMs",
    "summary": "In this paper, we point out that the objective of the retrieval algorithms is to align with the LLM, which is similar to the objective of knowledge distillation in LLMs. We analyze the similarity in information focus between the distilled language model(DLM) and the original LLM from the perspective of information theory, and thus propose a novel paradigm that leverages a DLM as the retrieval algorithm. Based on the insight, we present SpeContext, an algorithm and system co-design for long-context reasoning. (1) At the algorithm level, SpeContext proposes lightweight retrieval head based on the head-level attention weights of DLM, achieving > 90% parameters reduction by pruning the redundancy. (2) At the system level, SpeContext designs an asynchronous prefetch dataflow via the elastic loading strategy, effectively overlapping KV cache retrieval with the LLM computation. (3) At the compilation level, SpeContext constructs the theoretical memory model and implements an adaptive memory management system to achieve acceleration by maximizing GPU memory utilization. We deploy and evaluate SpeContext in two resourceconstrained environments, cloud and edge. Extensive experiments show that, compared with the Huggingface framework, SpeContext achieves up to 24.89x throughput improvement in cloud and 10.06x speedup in edge with negligible accuracy loss, pushing the Pareto frontier of accuracy and throughput.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.00722.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "651526b2391bec2898ba9103",
      "avatarUrl": "/avatars/b965b5e5694141b79c4c0b7a98a66a07.svg",
      "fullname": "Jiaming Xu (SII)",
      "name": "MathsCode",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "63e5ef7bf2e9a8f22c515654",
      "name": "SJTU",
      "fullname": "Shanghai Jiao Tong University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.01925",
      "authors": [
        {
          "_id": "692e5c7337312eaa83fd87bf",
          "name": "Junnan Liu",
          "hidden": false
        },
        {
          "_id": "692e5c7337312eaa83fd87c0",
          "name": "Hongwei Liu",
          "hidden": false
        },
        {
          "_id": "692e5c7337312eaa83fd87c1",
          "name": "Songyang Zhang",
          "hidden": false
        },
        {
          "_id": "692e5c7337312eaa83fd87c2",
          "name": "Kai Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-01T17:41:08.000Z",
      "submittedOnDailyAt": "2025-12-02T00:59:53.374Z",
      "title": "Rectifying LLM Thought from Lens of Optimization",
      "submittedOnDailyBy": {
        "_id": "643d26979347842571bc9613",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/3heFf7h3jbhhJWJ4JfGfh.jpeg",
        "isPro": false,
        "fullname": "Junnan Liu",
        "user": "jnanliu",
        "type": "user"
      },
      "summary": "Recent advancements in large language models (LLMs) have been driven by their emergent reasoning capabilities, particularly through long chain-of-thought (CoT) prompting, which enables thorough exploration and deliberation. Despite these advances, long-CoT LLMs often exhibit suboptimal reasoning behaviors, such as overthinking and excessively protracted reasoning chains, which can impair performance. In this paper, we analyze reasoning processes through an optimization lens, framing CoT as a gradient descent procedure where each reasoning step constitutes an update toward problem resolution. Building on this perspective, we introduce RePro (Rectifying Process-level Reward), a novel approach to refine LLM reasoning during post-training. RePro defines a surrogate objective function to assess the optimization process underlying CoT, utilizing a dual scoring mechanism to quantify its intensity and stability. These scores are aggregated into a composite process-level reward, seamlessly integrated into reinforcement learning with verifiable rewards (RLVR) pipelines to optimize LLMs. Extensive experiments across multiple reinforcement learning algorithms and diverse LLMs, evaluated on benchmarks spanning mathematics, science, and coding, demonstrate that RePro consistently enhances reasoning performance and mitigates suboptimal reasoning behaviors.",
      "upvotes": 7,
      "discussionId": "692e5c7337312eaa83fd87c3",
      "githubRepo": "https://github.com/open-compass/RePro",
      "ai_summary": "RePro, a novel process-level reward mechanism, enhances LLM reasoning by refining the optimization process underlying chain-of-thought prompting, thereby improving performance and reducing suboptimal behaviors.",
      "ai_keywords": [
        "large language models",
        "long chain-of-thought",
        "gradient descent",
        "RePro",
        "Rectifying Process-level Reward",
        "surrogate objective function",
        "dual scoring mechanism",
        "composite process-level reward",
        "reinforcement learning with verifiable rewards",
        "RLVR"
      ],
      "githubStars": 1
    },
    "publishedAt": "2025-12-01T12:41:08.000Z",
    "title": "Rectifying LLM Thought from Lens of Optimization",
    "summary": "Recent advancements in large language models (LLMs) have been driven by their emergent reasoning capabilities, particularly through long chain-of-thought (CoT) prompting, which enables thorough exploration and deliberation. Despite these advances, long-CoT LLMs often exhibit suboptimal reasoning behaviors, such as overthinking and excessively protracted reasoning chains, which can impair performance. In this paper, we analyze reasoning processes through an optimization lens, framing CoT as a gradient descent procedure where each reasoning step constitutes an update toward problem resolution. Building on this perspective, we introduce RePro (Rectifying Process-level Reward), a novel approach to refine LLM reasoning during post-training. RePro defines a surrogate objective function to assess the optimization process underlying CoT, utilizing a dual scoring mechanism to quantify its intensity and stability. These scores are aggregated into a composite process-level reward, seamlessly integrated into reinforcement learning with verifiable rewards (RLVR) pipelines to optimize LLMs. Extensive experiments across multiple reinforcement learning algorithms and diverse LLMs, evaluated on benchmarks spanning mathematics, science, and coding, demonstrate that RePro consistently enhances reasoning performance and mitigates suboptimal reasoning behaviors.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.01925.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643d26979347842571bc9613",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/3heFf7h3jbhhJWJ4JfGfh.jpeg",
      "fullname": "Junnan Liu",
      "name": "jnanliu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.00891",
      "authors": [
        {
          "_id": "692e876937312eaa83fd892b",
          "name": "Yiyu Wang",
          "hidden": false
        },
        {
          "_id": "692e876937312eaa83fd892c",
          "name": "Xuyang Liu",
          "hidden": false
        },
        {
          "_id": "692e876937312eaa83fd892d",
          "name": "Xiyan Gui",
          "hidden": false
        },
        {
          "_id": "692e876937312eaa83fd892e",
          "name": "Xinying Lin",
          "hidden": false
        },
        {
          "_id": "692e876937312eaa83fd892f",
          "name": "Boxue Yang",
          "hidden": false
        },
        {
          "_id": "692e876937312eaa83fd8930",
          "name": "Chenfei Liao",
          "hidden": false
        },
        {
          "_id": "692e876937312eaa83fd8931",
          "name": "Tailai Chen",
          "hidden": false
        },
        {
          "_id": "692e876937312eaa83fd8932",
          "name": "Linfeng Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-30T13:44:28.000Z",
      "submittedOnDailyAt": "2025-12-02T04:35:25.692Z",
      "title": "Accelerating Streaming Video Large Language Models via Hierarchical Token Compression",
      "submittedOnDailyBy": {
        "_id": "66a0caa1a7a6ed88ad1c0ddf",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66a0caa1a7a6ed88ad1c0ddf/WoOP24-ruuHy4ryNhRp0D.jpeg",
        "isPro": false,
        "fullname": "Xuyang Liu",
        "user": "xuyang-liu16",
        "type": "user"
      },
      "summary": "Streaming Video Large Language Models (VideoLLMs) have demonstrated impressive performance across various video understanding tasks, but they face significant challenges in real-time deployment due to the high computational cost of processing dense visual tokens from continuous video streams. In streaming video scenarios, the primary bottleneck lies in the Vision Transformer (ViT) encoding stage, where redundant processing of temporally similar frames leads to inefficiency. Additionally, inflated token sequences during LLM pre-filling further exacerbate latency and memory overhead. To address these challenges, we propose Streaming Token Compression (STC), a plug-and-play hierarchical framework that seamlessly integrates into existing streaming VideoLLMs, optimizing both ViT encoding and LLM pre-filling stages to accelerate processing. STC introduces two token-level accelerators: STC-Cacher, which reduces ViT encoding overhead by caching and reusing features from temporally similar frames, and STC-Pruner, which compresses the visual token sequence before it enters the LLM, preserving only the most salient tokens based on both spatial and temporal relevance. Extensive experiments on four baseline streaming VideoLLMs across five benchmarks demonstrate that STC outperforms other compression methods. Notably, STC retains up to 99\\% of accuracy on the ReKV framework while reducing ViT encoding latency and LLM pre-filling latency by 24.5\\% and 45.3\\%.",
      "upvotes": 7,
      "discussionId": "692e876a37312eaa83fd8933",
      "githubRepo": "https://github.com/lern-to-write/STC",
      "ai_summary": "STC, a hierarchical framework, optimizes streaming VideoLLMs by reducing ViT encoding and LLM pre-filling latency through token caching and pruning, without significantly impacting accuracy.",
      "ai_keywords": [
        "Vision Transformer",
        "ViT",
        "Streaming Video Large Language Models",
        "VideoLLMs",
        "token compression",
        "STC-Cacher",
        "STC-Pruner",
        "ReKV framework"
      ],
      "githubStars": 7,
      "organization": {
        "_id": "670dc5ba9fbc880b347455d8",
        "name": "SJUT1",
        "fullname": "Shanghai JiaoTong University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/670dc582cad89cdabe16d6d3/RYLwroLKmhWCY8Yp1i5jr.png"
      }
    },
    "publishedAt": "2025-11-30T08:44:28.000Z",
    "title": "Accelerating Streaming Video Large Language Models via Hierarchical Token Compression",
    "summary": "Streaming Video Large Language Models (VideoLLMs) have demonstrated impressive performance across various video understanding tasks, but they face significant challenges in real-time deployment due to the high computational cost of processing dense visual tokens from continuous video streams. In streaming video scenarios, the primary bottleneck lies in the Vision Transformer (ViT) encoding stage, where redundant processing of temporally similar frames leads to inefficiency. Additionally, inflated token sequences during LLM pre-filling further exacerbate latency and memory overhead. To address these challenges, we propose Streaming Token Compression (STC), a plug-and-play hierarchical framework that seamlessly integrates into existing streaming VideoLLMs, optimizing both ViT encoding and LLM pre-filling stages to accelerate processing. STC introduces two token-level accelerators: STC-Cacher, which reduces ViT encoding overhead by caching and reusing features from temporally similar frames, and STC-Pruner, which compresses the visual token sequence before it enters the LLM, preserving only the most salient tokens based on both spatial and temporal relevance. Extensive experiments on four baseline streaming VideoLLMs across five benchmarks demonstrate that STC outperforms other compression methods. Notably, STC retains up to 99\\% of accuracy on the ReKV framework while reducing ViT encoding latency and LLM pre-filling latency by 24.5\\% and 45.3\\%.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.00891.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66a0caa1a7a6ed88ad1c0ddf",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66a0caa1a7a6ed88ad1c0ddf/WoOP24-ruuHy4ryNhRp0D.jpeg",
      "fullname": "Xuyang Liu",
      "name": "xuyang-liu16",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "organization": {
      "_id": "670dc5ba9fbc880b347455d8",
      "name": "SJUT1",
      "fullname": "Shanghai JiaoTong University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/670dc582cad89cdabe16d6d3/RYLwroLKmhWCY8Yp1i5jr.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.01420",
      "authors": [
        {
          "_id": "692e70dc37312eaa83fd88a8",
          "name": "Yaxuan Wang",
          "hidden": false
        },
        {
          "_id": "692e70dc37312eaa83fd88a9",
          "name": "Quan Liu",
          "hidden": false
        },
        {
          "_id": "692e70dc37312eaa83fd88aa",
          "name": "Zhenting Wang",
          "hidden": false
        },
        {
          "_id": "692e70dc37312eaa83fd88ab",
          "name": "Zichao Li",
          "hidden": false
        },
        {
          "_id": "692e70dc37312eaa83fd88ac",
          "name": "Wei Wei",
          "hidden": false
        },
        {
          "_id": "692e70dc37312eaa83fd88ad",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "692e70dc37312eaa83fd88ae",
          "name": "Yujia Bao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-01T08:55:45.000Z",
      "submittedOnDailyAt": "2025-12-02T02:47:26.816Z",
      "title": "PromptBridge: Cross-Model Prompt Transfer for Large Language Models",
      "submittedOnDailyBy": {
        "_id": "64dfcc62e8b6f3f3baa950e0",
        "avatarUrl": "/avatars/21bbff67d46c08044efe2406575aa77e.svg",
        "isPro": false,
        "fullname": "Zhenting Wang",
        "user": "ztwang",
        "type": "user"
      },
      "summary": "Large language models (LLMs) underpin applications in code generation, mathematical reasoning, and agent-based workflows. In practice, systems access LLMs via commercial APIs or open-source deployments, and the model landscape (e.g., GPT, Claude, Llama) evolves rapidly. This rapid evolution forces frequent model switches driven by capability, cost, deployment constraints, and privacy. Yet prompts are highly model-sensitive: reusing a prompt engineered for one model on another often yields substantially worse performance than a prompt optimized for the target model. We term this phenomenon Model Drifting. Through extensive empirical analysis across diverse LLM configurations, we show that model drifting is both common and severe. To address this challenge, we introduce PromptBridge, a training-free framework that preserves prompt effectiveness under model switches, enabling cross-model prompt transfer without costly per-task or per-model re-optimization. PromptBridge requires only a small set of alignment tasks for calibration. It first applies Model-Adaptive Reflective Prompt Evolution (MAP-RPE) to obtain task- and model-specific optimal prompts via iterative reflective refinement and quantitative evaluation. Using the resulting calibrated prompt pairs for the source and target models, PromptBridge learns a cross-model prompt mapping. At test time, i.e., for an unseen task, given a source-model prompt, this mapping directly produces an optimized prompt for the target model. Experiments in single-agent and multi-agent settings show that PromptBridge consistently improves downstream accuracy while reducing migration effort. The code will be available soon.",
      "upvotes": 6,
      "discussionId": "692e70dd37312eaa83fd88af",
      "ai_summary": "PromptBridge is a framework that facilitates prompt transfer between different LLMs to maintain performance without re-optimization.",
      "ai_keywords": [
        "Large language models (LLMs)",
        "code generation",
        "mathematical reasoning",
        "agent-based workflows",
        "commercial APIs",
        "open-source deployments",
        "model drifting",
        "PromptBridge",
        "Model-Adaptive Reflective Prompt Evolution (MAP-RPE)",
        "cross-model prompt mapping"
      ]
    },
    "publishedAt": "2025-12-01T03:55:45.000Z",
    "title": "PromptBridge: Cross-Model Prompt Transfer for Large Language Models",
    "summary": "Large language models (LLMs) underpin applications in code generation, mathematical reasoning, and agent-based workflows. In practice, systems access LLMs via commercial APIs or open-source deployments, and the model landscape (e.g., GPT, Claude, Llama) evolves rapidly. This rapid evolution forces frequent model switches driven by capability, cost, deployment constraints, and privacy. Yet prompts are highly model-sensitive: reusing a prompt engineered for one model on another often yields substantially worse performance than a prompt optimized for the target model. We term this phenomenon Model Drifting. Through extensive empirical analysis across diverse LLM configurations, we show that model drifting is both common and severe. To address this challenge, we introduce PromptBridge, a training-free framework that preserves prompt effectiveness under model switches, enabling cross-model prompt transfer without costly per-task or per-model re-optimization. PromptBridge requires only a small set of alignment tasks for calibration. It first applies Model-Adaptive Reflective Prompt Evolution (MAP-RPE) to obtain task- and model-specific optimal prompts via iterative reflective refinement and quantitative evaluation. Using the resulting calibrated prompt pairs for the source and target models, PromptBridge learns a cross-model prompt mapping. At test time, i.e., for an unseen task, given a source-model prompt, this mapping directly produces an optimized prompt for the target model. Experiments in single-agent and multi-agent settings show that PromptBridge consistently improves downstream accuracy while reducing migration effort. The code will be available soon.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.01420.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64dfcc62e8b6f3f3baa950e0",
      "avatarUrl": "/avatars/21bbff67d46c08044efe2406575aa77e.svg",
      "fullname": "Zhenting Wang",
      "name": "ztwang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.01342",
      "authors": [
        {
          "_id": "692e685d37312eaa83fd887b",
          "name": "Chenting Wang",
          "hidden": false
        },
        {
          "_id": "692e685d37312eaa83fd887c",
          "name": "Yuhan Zhu",
          "hidden": false
        },
        {
          "_id": "692e685d37312eaa83fd887d",
          "name": "Yicheng Xu",
          "hidden": false
        },
        {
          "_id": "692e685d37312eaa83fd887e",
          "name": "Jiange Yang",
          "hidden": false
        },
        {
          "_id": "692e685d37312eaa83fd887f",
          "name": "Ziang Yan",
          "hidden": false
        },
        {
          "_id": "692e685d37312eaa83fd8880",
          "name": "Yali Wang",
          "hidden": false
        },
        {
          "_id": "692e685d37312eaa83fd8881",
          "name": "Yi Wang",
          "hidden": false
        },
        {
          "_id": "692e685d37312eaa83fd8882",
          "name": "Limin Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-01T06:57:39.000Z",
      "submittedOnDailyAt": "2025-12-02T01:51:04.923Z",
      "title": "InternVideo-Next: Towards General Video Foundation Models without Video-Text Supervision",
      "submittedOnDailyBy": {
        "_id": "62aafa49f29ff279b51f0182",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62aafa49f29ff279b51f0182/rQx8QFQGOY2qIhqJ8zSRj.jpeg",
        "isPro": false,
        "fullname": "yinanhe",
        "user": "ynhe",
        "type": "user"
      },
      "summary": "Large-scale video-text pretraining achieves strong performance but depends on noisy, synthetic captions with limited semantic coverage, often overlooking implicit world knowledge such as object motion, 3D geometry, and physical cues. In contrast, masked video modeling (MVM) directly exploits spatiotemporal structures but trails text-supervised methods on general tasks. We find this gap arises from overlooked architectural issues: pixel-level reconstruction struggles with convergence and its low-level requirement often conflicts with semantics, while latent prediction often encourages shortcut learning. To address these, we disentangle the traditional encoder-decoder design into an Encoder-Predictor-Decoder (EPD) framework, where the predictor acts as a latent world model, and propose InternVideo-Next, a two-stage pretraining scheme that builds a semantically consistent yet detail-preserving latent space for this world model. First, conventional linear decoder in pixel MVM enforces the predictor output latent to be linearly projected to, thus separable in pixel space, causing the conflict with semantic abstraction. Our Stage 1 proposes a conditional diffusion decoder and injects reliable image-level semantic priors to enhance semantics and convergence, thus bridging pixel-level fidelity with high-level semantic abstraction. Stage 2 further learns world knowledge by predicting frozen Stage 1 targets within this space, mitigating shortcut learning. Trained on public, unlabeled videos, InternVideo-Next achieves state-of-the-art results across benchmarks and provides a scalable path toward general video representation learning.",
      "upvotes": 6,
      "discussionId": "692e685e37312eaa83fd8883",
      "ai_summary": "InternVideo-Next uses a two-stage pretraining scheme with an Encoder-Predictor-Decoder framework to achieve state-of-the-art video representation learning by combining pixel-level fidelity and high-level semantics.",
      "ai_keywords": [
        "masked video modeling",
        "spatiotemporal structures",
        "encoder-decoder design",
        "Encoder-Predictor-Decoder",
        "latent world model",
        "conditional diffusion decoder",
        "image-level semantic priors",
        "semantic abstraction",
        "shortcut learning",
        "world knowledge",
        "video representation learning"
      ],
      "organization": {
        "_id": "64006c57a3b8fe3ac0e9af7c",
        "name": "OpenGVLab",
        "fullname": "OpenGVLab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64006c09330a45b03605bba3/FvdxiTkTqH8rKDOzGZGUE.jpeg"
      }
    },
    "publishedAt": "2025-12-01T01:57:39.000Z",
    "title": "InternVideo-Next: Towards General Video Foundation Models without Video-Text Supervision",
    "summary": "Large-scale video-text pretraining achieves strong performance but depends on noisy, synthetic captions with limited semantic coverage, often overlooking implicit world knowledge such as object motion, 3D geometry, and physical cues. In contrast, masked video modeling (MVM) directly exploits spatiotemporal structures but trails text-supervised methods on general tasks. We find this gap arises from overlooked architectural issues: pixel-level reconstruction struggles with convergence and its low-level requirement often conflicts with semantics, while latent prediction often encourages shortcut learning. To address these, we disentangle the traditional encoder-decoder design into an Encoder-Predictor-Decoder (EPD) framework, where the predictor acts as a latent world model, and propose InternVideo-Next, a two-stage pretraining scheme that builds a semantically consistent yet detail-preserving latent space for this world model. First, conventional linear decoder in pixel MVM enforces the predictor output latent to be linearly projected to, thus separable in pixel space, causing the conflict with semantic abstraction. Our Stage 1 proposes a conditional diffusion decoder and injects reliable image-level semantic priors to enhance semantics and convergence, thus bridging pixel-level fidelity with high-level semantic abstraction. Stage 2 further learns world knowledge by predicting frozen Stage 1 targets within this space, mitigating shortcut learning. Trained on public, unlabeled videos, InternVideo-Next achieves state-of-the-art results across benchmarks and provides a scalable path toward general video representation learning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.01342.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "62aafa49f29ff279b51f0182",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62aafa49f29ff279b51f0182/rQx8QFQGOY2qIhqJ8zSRj.jpeg",
      "fullname": "yinanhe",
      "name": "ynhe",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "organization": {
      "_id": "64006c57a3b8fe3ac0e9af7c",
      "name": "OpenGVLab",
      "fullname": "OpenGVLab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64006c09330a45b03605bba3/FvdxiTkTqH8rKDOzGZGUE.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.01707",
      "authors": [
        {
          "_id": "692e822337312eaa83fd88ef",
          "name": "Daeun Lee",
          "hidden": false
        },
        {
          "_id": "692e822337312eaa83fd88f0",
          "name": "Subhojyoti Mukherjee",
          "hidden": false
        },
        {
          "_id": "692e822337312eaa83fd88f1",
          "name": "Branislav Kveton",
          "hidden": false
        },
        {
          "_id": "692e822337312eaa83fd88f2",
          "name": "Ryan A. Rossi",
          "hidden": false
        },
        {
          "_id": "692e822337312eaa83fd88f3",
          "name": "Viet Dac Lai",
          "hidden": false
        },
        {
          "_id": "692e822337312eaa83fd88f4",
          "name": "Seunghyun Yoon",
          "hidden": false
        },
        {
          "_id": "692e822337312eaa83fd88f5",
          "name": "Trung Bui",
          "hidden": false
        },
        {
          "_id": "692e822337312eaa83fd88f6",
          "name": "Franck Dernoncourt",
          "hidden": false
        },
        {
          "_id": "692e822337312eaa83fd88f7",
          "name": "Mohit Bansal",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-01T14:15:44.000Z",
      "submittedOnDailyAt": "2025-12-02T03:39:17.172Z",
      "title": "StreamGaze: Gaze-Guided Temporal Reasoning and Proactive Understanding in Streaming Videos",
      "submittedOnDailyBy": {
        "_id": "647585665aba8edfb2f393d7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647585665aba8edfb2f393d7/LU4xZwZcSoU6VcddKE2Eg.jpeg",
        "isPro": true,
        "fullname": "Daeun Lee",
        "user": "danaleee",
        "type": "user"
      },
      "summary": "Streaming video understanding requires models not only to process temporally incoming frames, but also to anticipate user intention for realistic applications like AR glasses. While prior streaming benchmarks evaluate temporal reasoning, none measure whether MLLMs can interpret or leverage human gaze signals within a streaming setting. To fill this gap, we introduce StreamGaze, the first benchmark designed to evaluate how effectively MLLMs use gaze for temporal and proactive reasoning in streaming videos. StreamGaze introduces gaze-guided past, present, and proactive tasks that comprehensively evaluate streaming video understanding. These tasks assess whether models can use real-time gaze to follow shifting attention and infer user intentions from only past and currently observed frames. To build StreamGaze, we develop a gaze-video QA generation pipeline that aligns egocentric videos with raw gaze trajectories via fixation extraction, region-specific visual prompting, and scanpath construction. This pipeline produces spatio-temporally grounded QA pairs that closely reflect human perceptual dynamics. Across all StreamGaze tasks, we observe substantial performance gaps between state-of-the-art MLLMs and human performance, revealing fundamental limitations in gaze-based temporal reasoning, intention modeling, and proactive prediction. We further provide detailed analyses of gaze-prompting strategies, reasoning behaviors, and task-specific failure modes, offering deeper insight into why current MLLMs struggle and what capabilities future models must develop. All data and code will be publicly released to support continued research in gaze-guided streaming video understanding.",
      "upvotes": 4,
      "discussionId": "692e822337312eaa83fd88f8",
      "projectPage": "https://streamgaze.github.io/",
      "githubRepo": "https://github.com/daeunni/StreamGaze",
      "ai_summary": "StreamGaze is a benchmark that evaluates how effectively models use gaze signals for temporal and proactive reasoning in streaming videos, revealing limitations in current MLLM capabilities.",
      "ai_keywords": [
        "MLLMs",
        "temporal reasoning",
        "gaze signals",
        "gaze-guided tasks",
        "past",
        "present",
        "proactive",
        "fixation extraction",
        "region-specific visual prompting",
        "scanpath construction",
        "spatio-temporally grounded QA",
        "human perceptual dynamics",
        "gaze-prompting strategies",
        "reasoning behaviors",
        "failure modes"
      ],
      "githubStars": 3,
      "organization": {
        "_id": "637b318856db0404b7c5a0c2",
        "name": "adobe-research",
        "fullname": "Adobe Research",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1669033410364-624bebf604abc7ebb01789af.png"
      }
    },
    "publishedAt": "2025-12-01T09:15:44.000Z",
    "title": "StreamGaze: Gaze-Guided Temporal Reasoning and Proactive Understanding in Streaming Videos",
    "summary": "Streaming video understanding requires models not only to process temporally incoming frames, but also to anticipate user intention for realistic applications like AR glasses. While prior streaming benchmarks evaluate temporal reasoning, none measure whether MLLMs can interpret or leverage human gaze signals within a streaming setting. To fill this gap, we introduce StreamGaze, the first benchmark designed to evaluate how effectively MLLMs use gaze for temporal and proactive reasoning in streaming videos. StreamGaze introduces gaze-guided past, present, and proactive tasks that comprehensively evaluate streaming video understanding. These tasks assess whether models can use real-time gaze to follow shifting attention and infer user intentions from only past and currently observed frames. To build StreamGaze, we develop a gaze-video QA generation pipeline that aligns egocentric videos with raw gaze trajectories via fixation extraction, region-specific visual prompting, and scanpath construction. This pipeline produces spatio-temporally grounded QA pairs that closely reflect human perceptual dynamics. Across all StreamGaze tasks, we observe substantial performance gaps between state-of-the-art MLLMs and human performance, revealing fundamental limitations in gaze-based temporal reasoning, intention modeling, and proactive prediction. We further provide detailed analyses of gaze-prompting strategies, reasoning behaviors, and task-specific failure modes, offering deeper insight into why current MLLMs struggle and what capabilities future models must develop. All data and code will be publicly released to support continued research in gaze-guided streaming video understanding.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.01707.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647585665aba8edfb2f393d7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647585665aba8edfb2f393d7/LU4xZwZcSoU6VcddKE2Eg.jpeg",
      "fullname": "Daeun Lee",
      "name": "danaleee",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "organization": {
      "_id": "637b318856db0404b7c5a0c2",
      "name": "adobe-research",
      "fullname": "Adobe Research",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1669033410364-624bebf604abc7ebb01789af.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.22396",
      "authors": [
        {
          "_id": "692d1fbc4397b1ec214f68d3",
          "name": "Run Shao",
          "hidden": false
        },
        {
          "_id": "692d1fbc4397b1ec214f68d4",
          "name": "Ziyu Li",
          "hidden": false
        },
        {
          "_id": "692d1fbc4397b1ec214f68d5",
          "name": "Zhaoyang Zhang",
          "hidden": false
        },
        {
          "_id": "692d1fbc4397b1ec214f68d6",
          "name": "Linrui Xu",
          "hidden": false
        },
        {
          "_id": "692d1fbc4397b1ec214f68d7",
          "name": "Xinran He",
          "hidden": false
        },
        {
          "_id": "692d1fbc4397b1ec214f68d8",
          "name": "Hongyuan Yuan",
          "hidden": false
        },
        {
          "_id": "692d1fbc4397b1ec214f68d9",
          "name": "Bolei He",
          "hidden": false
        },
        {
          "_id": "692d1fbc4397b1ec214f68da",
          "name": "Yongxing Dai",
          "hidden": false
        },
        {
          "_id": "692d1fbc4397b1ec214f68db",
          "name": "Yiming Yan",
          "hidden": false
        },
        {
          "_id": "692d1fbc4397b1ec214f68dc",
          "name": "Yijun Chen",
          "hidden": false
        },
        {
          "_id": "692d1fbc4397b1ec214f68dd",
          "name": "Wang Guo",
          "hidden": false
        },
        {
          "_id": "692d1fbc4397b1ec214f68de",
          "name": "Haifeng Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-27T12:19:37.000Z",
      "submittedOnDailyAt": "2025-12-02T03:42:15.089Z",
      "title": "Asking like Socrates: Socrates helps VLMs understand remote sensing images",
      "submittedOnDailyBy": {
        "_id": "630cc0ab8e3ff0c72329c364",
        "avatarUrl": "/avatars/c3e0e071a9efa6061388ad4c66136030.svg",
        "isPro": false,
        "fullname": "RunShao",
        "user": "ShaoRun",
        "type": "user"
      },
      "summary": "Recent multimodal reasoning models, inspired by DeepSeek-R1, have significantly advanced vision-language systems. However, in remote sensing (RS) tasks, we observe widespread pseudo reasoning: models narrate the process of reasoning rather than genuinely reason toward the correct answer based on visual evidence. We attribute this to the Glance Effect, where a single, coarse perception of large-scale RS imagery results in incomplete understanding and reasoning based on linguistic self-consistency instead of visual evidence. To address this, we propose RS-EoT (Remote Sensing Evidence-of-Thought), a language-driven, iterative visual evidence-seeking paradigm. To instill this paradigm, we propose SocraticAgent, a self-play multi-agent system that synthesizes reasoning traces via alternating cycles of reasoning and visual inspection. To enhance and generalize these patterns, we propose a two-stage progressive RL strategy: first, RL on fine-grained Grounding tasks to enhance RS-EoT capabilities, followed by RL on RS VQA to generalize to broader understanding scenarios. Experiments show RS-EoT achieves state-of-the-art performance on multiple RS VQA and grounding benchmarks. Analyses reveal clear iterative cycles of reasoning and evidence seeking, confirming RS-EoT mitigates the Glance Effect and enables genuine evidence-grounded reasoning. Our code, data, and models are available at https://geox-lab.github.io/Asking_like_Socrates",
      "upvotes": 4,
      "discussionId": "692d1fbc4397b1ec214f68df",
      "projectPage": "https://geox-lab.github.io/Asking_like_Socrates/",
      "githubRepo": "https://github.com/GeoX-Lab/Asking_like_Socrates",
      "ai_summary": "RS-EoT, a language-driven iterative visual evidence-seeking paradigm, addresses pseudo reasoning in remote sensing tasks by using a self-play multi-agent system and a two-stage RL strategy, achieving state-of-the-art performance on RS VQA benchmarks.",
      "ai_keywords": [
        "multimodal reasoning",
        "DeepSeek-R1",
        "remote sensing",
        "pseudo reasoning",
        "Glance Effect",
        "RS-EoT",
        "language-driven",
        "iterative visual evidence-seeking",
        "SocraticAgent",
        "self-play multi-agent",
        "reasoning traces",
        "visual inspection",
        "progressive RL",
        "Grounding tasks",
        "RS VQA",
        "evidence-grounded reasoning"
      ],
      "githubStars": 2
    },
    "publishedAt": "2025-11-27T07:19:37.000Z",
    "title": "Asking like Socrates: Socrates helps VLMs understand remote sensing images",
    "summary": "Recent multimodal reasoning models, inspired by DeepSeek-R1, have significantly advanced vision-language systems. However, in remote sensing (RS) tasks, we observe widespread pseudo reasoning: models narrate the process of reasoning rather than genuinely reason toward the correct answer based on visual evidence. We attribute this to the Glance Effect, where a single, coarse perception of large-scale RS imagery results in incomplete understanding and reasoning based on linguistic self-consistency instead of visual evidence. To address this, we propose RS-EoT (Remote Sensing Evidence-of-Thought), a language-driven, iterative visual evidence-seeking paradigm. To instill this paradigm, we propose SocraticAgent, a self-play multi-agent system that synthesizes reasoning traces via alternating cycles of reasoning and visual inspection. To enhance and generalize these patterns, we propose a two-stage progressive RL strategy: first, RL on fine-grained Grounding tasks to enhance RS-EoT capabilities, followed by RL on RS VQA to generalize to broader understanding scenarios. Experiments show RS-EoT achieves state-of-the-art performance on multiple RS VQA and grounding benchmarks. Analyses reveal clear iterative cycles of reasoning and evidence seeking, confirming RS-EoT mitigates the Glance Effect and enables genuine evidence-grounded reasoning. Our code, data, and models are available at https://geox-lab.github.io/Asking_like_Socrates",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.22396.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "630cc0ab8e3ff0c72329c364",
      "avatarUrl": "/avatars/c3e0e071a9efa6061388ad4c66136030.svg",
      "fullname": "RunShao",
      "name": "ShaoRun",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.01763",
      "authors": [
        {
          "_id": "692e651037312eaa83fd8806",
          "name": "Xurui Zhou",
          "hidden": false
        },
        {
          "_id": "692e651037312eaa83fd8807",
          "name": "Gongwei Chen",
          "hidden": false
        },
        {
          "_id": "692e651037312eaa83fd8808",
          "name": "Yuquan Xie",
          "hidden": false
        },
        {
          "_id": "692e651037312eaa83fd8809",
          "name": "Zaijing Li",
          "hidden": false
        },
        {
          "_id": "692e651037312eaa83fd880a",
          "name": "Kaiwen Zhou",
          "hidden": false
        },
        {
          "_id": "692e651037312eaa83fd880b",
          "name": "Shuai Wang",
          "hidden": false
        },
        {
          "_id": "692e651037312eaa83fd880c",
          "name": "Shuo Yang",
          "hidden": false
        },
        {
          "_id": "692e651037312eaa83fd880d",
          "name": "Zhuotao Tian",
          "hidden": false
        },
        {
          "_id": "692e651037312eaa83fd880e",
          "name": "Rui Shao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-01T15:06:45.000Z",
      "submittedOnDailyAt": "2025-12-02T01:33:38.149Z",
      "title": "HiconAgent: History Context-aware Policy Optimization for GUI Agents",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Graphical User Interface (GUI) agents require effective use of historical context to perform sequential navigation tasks. While incorporating past actions and observations can improve decision making, naive use of full history leads to excessive computational overhead and distraction from irrelevant information. To address this, we introduce HiconAgent, a GUI agent trained with History Context-aware Policy Optimization (HCPO) for efficient and effective utilization of historical information. HCPO optimizes history usage in both sampling and policy updates through two complementary components: (1) Dynamic Context Sampling (DCS) presents the agent with variable length histories during sampling, enabling adaptive use of the most relevant context; (2) Anchor-guided History Compression (AHC) refines the policy update phase with a dual branch strategy where the compressed branch removes history observations while keeping history actions as information flow anchors. The compressed and uncompressed branches are coupled through a history-enhanced alignment loss to enforce consistent history usage while maintaining efficiency. Experiments on mainstream GUI navigation benchmarks demonstrate strong performance. Despite being smaller, HiconAgent-3B outperforms GUI-R1-7B by +8.46 percent grounding accuracy and +11.32 percent step success rate on GUI-Odyssey, while achieving comparable results on AndroidControl and AITW with up to 2.47x computational speedup and 60 percent FLOPs reduction.",
      "upvotes": 3,
      "discussionId": "692e651137312eaa83fd880f",
      "githubRepo": "https://github.com/JiuTian-VL/HiconAgent",
      "ai_summary": "HiconAgent, a GUI agent using HCPO, efficiently utilizes historical context for better performance in navigation tasks with reduced computational cost.",
      "ai_keywords": [
        "GUI agents",
        "History Context-aware Policy Optimization (HCPO)",
        "Dynamic Context Sampling (DCS)",
        "Anchor-guided History Compression (AHC)",
        "history-enhanced alignment loss",
        "GUI navigation benchmarks",
        "grounding accuracy",
        "step success rate",
        "computational speedup",
        "FLOPs reduction"
      ],
      "githubStars": 11
    },
    "publishedAt": "2025-12-01T10:06:45.000Z",
    "title": "HiconAgent: History Context-aware Policy Optimization for GUI Agents",
    "summary": "Graphical User Interface (GUI) agents require effective use of historical context to perform sequential navigation tasks. While incorporating past actions and observations can improve decision making, naive use of full history leads to excessive computational overhead and distraction from irrelevant information. To address this, we introduce HiconAgent, a GUI agent trained with History Context-aware Policy Optimization (HCPO) for efficient and effective utilization of historical information. HCPO optimizes history usage in both sampling and policy updates through two complementary components: (1) Dynamic Context Sampling (DCS) presents the agent with variable length histories during sampling, enabling adaptive use of the most relevant context; (2) Anchor-guided History Compression (AHC) refines the policy update phase with a dual branch strategy where the compressed branch removes history observations while keeping history actions as information flow anchors. The compressed and uncompressed branches are coupled through a history-enhanced alignment loss to enforce consistent history usage while maintaining efficiency. Experiments on mainstream GUI navigation benchmarks demonstrate strong performance. Despite being smaller, HiconAgent-3B outperforms GUI-R1-7B by +8.46 percent grounding accuracy and +11.32 percent step success rate on GUI-Odyssey, while achieving comparable results on AndroidControl and AITW with up to 2.47x computational speedup and 60 percent FLOPs reduction.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.01763.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 173
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.01030",
      "authors": [
        {
          "_id": "692e61d537312eaa83fd87e1",
          "name": "Jing He",
          "hidden": false
        },
        {
          "_id": "692e61d537312eaa83fd87e2",
          "name": "Haodong Li",
          "hidden": false
        },
        {
          "_id": "692e61d537312eaa83fd87e3",
          "name": "Mingzhi Sheng",
          "hidden": false
        },
        {
          "_id": "692e61d537312eaa83fd87e4",
          "name": "Ying-Cong Chen",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/641d211e353524fe41f16387/uM9lupA025v4ngNTdOT3s.jpeg"
      ],
      "publishedAt": "2025-11-30T18:57:25.000Z",
      "submittedOnDailyAt": "2025-12-02T01:21:49.699Z",
      "title": "Lotus-2: Advancing Geometric Dense Prediction with Powerful Image Generative Model",
      "submittedOnDailyBy": {
        "_id": "641d211e353524fe41f16387",
        "avatarUrl": "/avatars/c6e72c82c029b415a035beebee50b52c.svg",
        "isPro": true,
        "fullname": "Haodong Li",
        "user": "haodongli",
        "type": "user"
      },
      "summary": "Recovering pixel-wise geometric properties from a single image is fundamentally ill-posed due to appearance ambiguity and non-injective mappings between 2D observations and 3D structures. While discriminative regression models achieve strong performance through large-scale supervision, their success is bounded by the scale, quality and diversity of available data and limited physical reasoning. Recent diffusion models exhibit powerful world priors that encode geometry and semantics learned from massive image-text data, yet directly reusing their stochastic generative formulation is suboptimal for deterministic geometric inference: the former is optimized for diverse and high-fidelity image generation, whereas the latter requires stable and accurate predictions. In this work, we propose Lotus-2, a two-stage deterministic framework for stable, accurate and fine-grained geometric dense prediction, aiming to provide an optimal adaption protocol to fully exploit the pre-trained generative priors. Specifically, in the first stage, the core predictor employs a single-step deterministic formulation with a clean-data objective and a lightweight local continuity module (LCM) to generate globally coherent structures without grid artifacts. In the second stage, the detail sharpener performs a constrained multi-step rectified-flow refinement within the manifold defined by the core predictor, enhancing fine-grained geometry through noise-free deterministic flow matching. Using only 59K training samples, less than 1% of existing large-scale datasets, Lotus-2 establishes new state-of-the-art results in monocular depth estimation and highly competitive surface normal prediction. These results demonstrate that diffusion models can serve as deterministic world priors, enabling high-quality geometric reasoning beyond traditional discriminative and generative paradigms.",
      "upvotes": 3,
      "discussionId": "692e61d537312eaa83fd87e5",
      "projectPage": "https://lotus-2.github.io/",
      "githubRepo": "https://github.com/EnVision-Research/Lotus-2",
      "ai_summary": "A two-stage deterministic framework, Lotus-2, leverages diffusion models' world priors for high-quality geometric inference, achieving state-of-the-art results in monocular depth estimation and competitive surface normal prediction with limited training data.",
      "ai_keywords": [
        "diffusion models",
        "world priors",
        "deterministic framework",
        "core predictor",
        "lightweight local continuity module",
        "rectified-flow refinement",
        "monocular depth estimation",
        "surface normal prediction"
      ],
      "githubStars": 4
    },
    "publishedAt": "2025-11-30T13:57:25.000Z",
    "title": "Lotus-2: Advancing Geometric Dense Prediction with Powerful Image Generative Model",
    "summary": "Recovering pixel-wise geometric properties from a single image is fundamentally ill-posed due to appearance ambiguity and non-injective mappings between 2D observations and 3D structures. While discriminative regression models achieve strong performance through large-scale supervision, their success is bounded by the scale, quality and diversity of available data and limited physical reasoning. Recent diffusion models exhibit powerful world priors that encode geometry and semantics learned from massive image-text data, yet directly reusing their stochastic generative formulation is suboptimal for deterministic geometric inference: the former is optimized for diverse and high-fidelity image generation, whereas the latter requires stable and accurate predictions. In this work, we propose Lotus-2, a two-stage deterministic framework for stable, accurate and fine-grained geometric dense prediction, aiming to provide an optimal adaption protocol to fully exploit the pre-trained generative priors. Specifically, in the first stage, the core predictor employs a single-step deterministic formulation with a clean-data objective and a lightweight local continuity module (LCM) to generate globally coherent structures without grid artifacts. In the second stage, the detail sharpener performs a constrained multi-step rectified-flow refinement within the manifold defined by the core predictor, enhancing fine-grained geometry through noise-free deterministic flow matching. Using only 59K training samples, less than 1% of existing large-scale datasets, Lotus-2 establishes new state-of-the-art results in monocular depth estimation and highly competitive surface normal prediction. These results demonstrate that diffusion models can serve as deterministic world priors, enabling high-quality geometric reasoning beyond traditional discriminative and generative paradigms.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/641d211e353524fe41f16387/uM9lupA025v4ngNTdOT3s.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.01030.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "641d211e353524fe41f16387",
      "avatarUrl": "/avatars/c6e72c82c029b415a035beebee50b52c.svg",
      "fullname": "Haodong Li",
      "name": "haodongli",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 27
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.01949",
      "authors": [
        {
          "_id": "692e70a137312eaa83fd88a2",
          "name": "Zhongyu Yang",
          "hidden": false
        },
        {
          "_id": "692e70a137312eaa83fd88a3",
          "name": "Dannong Xu",
          "hidden": false
        },
        {
          "_id": "692e70a137312eaa83fd88a4",
          "name": "Wei Pang",
          "hidden": false
        },
        {
          "_id": "692e70a137312eaa83fd88a5",
          "name": "Yingfang Yuan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-01T17:59:11.000Z",
      "submittedOnDailyAt": "2025-12-02T03:38:23.676Z",
      "title": "Script: Graph-Structured and Query-Conditioned Semantic Token Pruning for Multimodal Large Language Models",
      "submittedOnDailyBy": {
        "_id": "654f99f74c8874c64d4e5664",
        "avatarUrl": "/avatars/e9da0d688f91ae49db91d0ebebb3782a.svg",
        "isPro": false,
        "fullname": "Zhongyu Yang",
        "user": "yzzyu",
        "type": "user"
      },
      "summary": "The rapid growth of visual tokens in multimodal large language models (MLLMs) leads to excessive memory consumption and inference latency, especially when handling high-resolution images and videos. Token pruning is a technique used to mitigate this issue by removing redundancy, but existing methods often ignore relevance to the user query or suffer from the limitations of attention mechanisms, reducing their adaptability and effectiveness. To address these challenges, we propose Script, a plug-and-play pruning method that requires no retraining and generalizes across diverse MLLMs. Script comprises two modules: a graph-structured pruning module that removes visually redundant tokens, and a query-conditioned semantic pruning module that preserves query-relevant visual information. Together, they enhance performance on multimodal tasks. Experiments on fourteen benchmarks across image and video understanding tasks show that Script consistently achieves higher model efficiency and predictive accuracy compared to existing pruning methods. On LLaVA-NeXT-7B, it achieves up to 6.8x prefill speedup and 10x FLOP reduction, while retaining 96.88% of the original performance.",
      "upvotes": 2,
      "discussionId": "692e70a137312eaa83fd88a6",
      "ai_summary": "Script, a plug-and-play token pruning method for multimodal large language models, improves efficiency and accuracy by removing redundant and irrelevant visual tokens without retraining.",
      "ai_keywords": [
        "multimodal large language models",
        "visual tokens",
        "token pruning",
        "graph-structured pruning",
        "query-conditioned semantic pruning",
        "image understanding",
        "video understanding",
        "prefill speedup",
        "FLOP reduction"
      ]
    },
    "publishedAt": "2025-12-01T12:59:11.000Z",
    "title": "Script: Graph-Structured and Query-Conditioned Semantic Token Pruning for Multimodal Large Language Models",
    "summary": "The rapid growth of visual tokens in multimodal large language models (MLLMs) leads to excessive memory consumption and inference latency, especially when handling high-resolution images and videos. Token pruning is a technique used to mitigate this issue by removing redundancy, but existing methods often ignore relevance to the user query or suffer from the limitations of attention mechanisms, reducing their adaptability and effectiveness. To address these challenges, we propose Script, a plug-and-play pruning method that requires no retraining and generalizes across diverse MLLMs. Script comprises two modules: a graph-structured pruning module that removes visually redundant tokens, and a query-conditioned semantic pruning module that preserves query-relevant visual information. Together, they enhance performance on multimodal tasks. Experiments on fourteen benchmarks across image and video understanding tasks show that Script consistently achieves higher model efficiency and predictive accuracy compared to existing pruning methods. On LLaVA-NeXT-7B, it achieves up to 6.8x prefill speedup and 10x FLOP reduction, while retaining 96.88% of the original performance.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.01949.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "654f99f74c8874c64d4e5664",
      "avatarUrl": "/avatars/e9da0d688f91ae49db91d0ebebb3782a.svg",
      "fullname": "Zhongyu Yang",
      "name": "yzzyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.01481",
      "authors": [
        {
          "_id": "692e652f37312eaa83fd8811",
          "name": "Qisen Wang",
          "hidden": false
        },
        {
          "_id": "692e652f37312eaa83fd8812",
          "name": "Yifan Zhao",
          "hidden": false
        },
        {
          "_id": "692e652f37312eaa83fd8813",
          "name": "Peisen Shen",
          "hidden": false
        },
        {
          "_id": "692e652f37312eaa83fd8814",
          "name": "Jialu Li",
          "hidden": false
        },
        {
          "_id": "692e652f37312eaa83fd8815",
          "name": "Jia Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-01T10:00:26.000Z",
      "submittedOnDailyAt": "2025-12-02T01:34:07.407Z",
      "title": "ChronosObserver: Taming 4D World with Hyperspace Diffusion Sampling",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Although prevailing camera-controlled video generation models can produce cinematic results, lifting them directly to the generation of 3D-consistent and high-fidelity time-synchronized multi-view videos remains challenging, which is a pivotal capability for taming 4D worlds. Some works resort to data augmentation or test-time optimization, but these strategies are constrained by limited model generalization and scalability issues. To this end, we propose ChronosObserver, a training-free method including World State Hyperspace to represent the spatiotemporal constraints of a 4D world scene, and Hyperspace Guided Sampling to synchronize the diffusion sampling trajectories of multiple views using the hyperspace. Experimental results demonstrate that our method achieves high-fidelity and 3D-consistent time-synchronized multi-view videos generation without training or fine-tuning for diffusion models.",
      "upvotes": 2,
      "discussionId": "692e652f37312eaa83fd8816",
      "projectPage": "https://icvteam.github.io/ChronosObserver.html",
      "ai_summary": "ChronosObserver generates high-fidelity, 3D-consistent, and time-synchronized multi-view videos using a training-free approach that leverages World State Hyperspace and Hyperspace Guided Sampling.",
      "ai_keywords": [
        "diffusion models",
        "World State Hyperspace",
        "Hyperspace Guided Sampling",
        "3D-consistent",
        "time-synchronized",
        "multi-view videos"
      ]
    },
    "publishedAt": "2025-12-01T05:00:26.000Z",
    "title": "ChronosObserver: Taming 4D World with Hyperspace Diffusion Sampling",
    "summary": "Although prevailing camera-controlled video generation models can produce cinematic results, lifting them directly to the generation of 3D-consistent and high-fidelity time-synchronized multi-view videos remains challenging, which is a pivotal capability for taming 4D worlds. Some works resort to data augmentation or test-time optimization, but these strategies are constrained by limited model generalization and scalability issues. To this end, we propose ChronosObserver, a training-free method including World State Hyperspace to represent the spatiotemporal constraints of a 4D world scene, and Hyperspace Guided Sampling to synchronize the diffusion sampling trajectories of multiple views using the hyperspace. Experimental results demonstrate that our method achieves high-fidelity and 3D-consistent time-synchronized multi-view videos generation without training or fine-tuning for diffusion models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.01481.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 173
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.00762",
      "authors": [
        {
          "_id": "692e65d537312eaa83fd8818",
          "name": "Zhiyuan Gao",
          "hidden": false
        },
        {
          "_id": "692e65d537312eaa83fd8819",
          "name": "Jiageng Mao",
          "hidden": false
        },
        {
          "_id": "692e65d537312eaa83fd881a",
          "name": "Hong-Xing Yu",
          "hidden": false
        },
        {
          "_id": "692e65d537312eaa83fd881b",
          "name": "Haozhe Lou",
          "hidden": false
        },
        {
          "_id": "692e65d537312eaa83fd881c",
          "name": "Emily Yue-Ting Jia",
          "hidden": false
        },
        {
          "_id": "692e65d537312eaa83fd881d",
          "name": "Jernej Barbic",
          "hidden": false
        },
        {
          "_id": "692e65d537312eaa83fd881e",
          "name": "Jiajun Wu",
          "hidden": false
        },
        {
          "_id": "692e65d537312eaa83fd881f",
          "name": "Yue Wang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/PbhkyK6QFN1Ubux7UvPwX.mp4"
      ],
      "publishedAt": "2025-11-30T07:19:00.000Z",
      "submittedOnDailyAt": "2025-12-02T01:36:59.508Z",
      "title": "Seeing the Wind from a Falling Leaf",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "A longstanding goal in computer vision is to model motions from videos, while the representations behind motions, i.e. the invisible physical interactions that cause objects to deform and move, remain largely unexplored. In this paper, we study how to recover the invisible forces from visual observations, e.g., estimating the wind field by observing a leaf falling to the ground. Our key innovation is an end-to-end differentiable inverse graphics framework, which jointly models object geometry, physical properties, and interactions directly from videos. Through backpropagation, our approach enables the recovery of force representations from object motions. We validate our method on both synthetic and real-world scenarios, and the results demonstrate its ability to infer plausible force fields from videos. Furthermore, we show the potential applications of our approach, including physics-based video generation and editing. We hope our approach sheds light on understanding and modeling the physical process behind pixels, bridging the gap between vision and physics. Please check more video results in our https://chaoren2357.github.io/seeingthewind/{project page}.",
      "upvotes": 2,
      "discussionId": "692e65d537312eaa83fd8820",
      "ai_summary": "An end-to-end differentiable inverse graphics framework recovers force representations from video observations, enabling physics-based video generation and editing.",
      "ai_keywords": [
        "inverse graphics framework",
        "backpropagation",
        "force representations",
        "object geometry",
        "physical properties",
        "interactions",
        "physics-based video generation",
        "editing"
      ]
    },
    "publishedAt": "2025-11-30T02:19:00.000Z",
    "title": "Seeing the Wind from a Falling Leaf",
    "summary": "A longstanding goal in computer vision is to model motions from videos, while the representations behind motions, i.e. the invisible physical interactions that cause objects to deform and move, remain largely unexplored. In this paper, we study how to recover the invisible forces from visual observations, e.g., estimating the wind field by observing a leaf falling to the ground. Our key innovation is an end-to-end differentiable inverse graphics framework, which jointly models object geometry, physical properties, and interactions directly from videos. Through backpropagation, our approach enables the recovery of force representations from object motions. We validate our method on both synthetic and real-world scenarios, and the results demonstrate its ability to infer plausible force fields from videos. Furthermore, we show the potential applications of our approach, including physics-based video generation and editing. We hope our approach sheds light on understanding and modeling the physical process behind pixels, bridging the gap between vision and physics. Please check more video results in our https://chaoren2357.github.io/seeingthewind/{project page}.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/PbhkyK6QFN1Ubux7UvPwX.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.00762.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 173
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.02008",
      "authors": [
        {
          "_id": "692e749937312eaa83fd88c4",
          "name": "Aradhye Agarwal",
          "hidden": false
        },
        {
          "_id": "692e749937312eaa83fd88c5",
          "name": "Ayan Sengupta",
          "hidden": false
        },
        {
          "_id": "692e749937312eaa83fd88c6",
          "name": "Tanmoy Chakraborty",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63ca499104c97982831127ec/mleucL_GS70X4WNPiWjl1.png",
        "https://cdn-uploads.huggingface.co/production/uploads/63ca499104c97982831127ec/SQqgGhZs72ROYddZeZuUl.png",
        "https://cdn-uploads.huggingface.co/production/uploads/63ca499104c97982831127ec/ui60-tgbeAgwgzF3e4ncl.png"
      ],
      "publishedAt": "2025-12-01T18:59:28.000Z",
      "submittedOnDailyAt": "2025-12-02T02:49:48.420Z",
      "title": "The Art of Scaling Test-Time Compute for Large Language Models",
      "submittedOnDailyBy": {
        "_id": "63ca499104c97982831127ec",
        "avatarUrl": "/avatars/816a962c62c805adf7789fa8e398b01e.svg",
        "isPro": true,
        "fullname": "Aradhye Agarwal",
        "user": "aradhye",
        "type": "user"
      },
      "summary": "Test-time scaling (TTS) -- the dynamic allocation of compute during inference -- is a promising direction for improving reasoning in large language models (LLMs). However, a systematic comparison of well-known TTS strategies under identical conditions is missing, and the influence of model type and problem difficulty on performance remains unclear. To address these gaps, we conduct the first large-scale study of TTS, spanning over thirty billion tokens generated using eight open-source LLMs (7B to 235B parameters), across four reasoning datasets. We observe three consistent trends: (1) no single TTS strategy universally dominates; (2) reasoning models exhibit distinct trace-quality patterns across problem difficulty and trace length, forming short-horizon and long-horizon categories; and (3) for a given model type, the optimal TTS performance scales monotonically with compute budget. Based on these insights, we provide a practical recipe for selecting the best TTS strategy, considering problem difficulty, model type, and compute budget, providing a practical guide to effective inference-time scaling.",
      "upvotes": 1,
      "discussionId": "692e749a37312eaa83fd88c7",
      "githubRepo": "https://github.com/Aradhye2002/art_of_tts",
      "ai_summary": "Systematic study of test-time scaling strategies in large language models reveals distinct performance trends based on problem difficulty, model type, and compute budget.",
      "ai_keywords": [
        "test-time scaling",
        "TTS",
        "dynamic allocation",
        "inference",
        "large language models",
        "LLMs",
        "trace-quality patterns",
        "short-horizon",
        "long-horizon"
      ],
      "githubStars": 0
    },
    "publishedAt": "2025-12-01T13:59:28.000Z",
    "title": "The Art of Scaling Test-Time Compute for Large Language Models",
    "summary": "Test-time scaling (TTS) -- the dynamic allocation of compute during inference -- is a promising direction for improving reasoning in large language models (LLMs). However, a systematic comparison of well-known TTS strategies under identical conditions is missing, and the influence of model type and problem difficulty on performance remains unclear. To address these gaps, we conduct the first large-scale study of TTS, spanning over thirty billion tokens generated using eight open-source LLMs (7B to 235B parameters), across four reasoning datasets. We observe three consistent trends: (1) no single TTS strategy universally dominates; (2) reasoning models exhibit distinct trace-quality patterns across problem difficulty and trace length, forming short-horizon and long-horizon categories; and (3) for a given model type, the optimal TTS performance scales monotonically with compute budget. Based on these insights, we provide a practical recipe for selecting the best TTS strategy, considering problem difficulty, model type, and compute budget, providing a practical guide to effective inference-time scaling.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63ca499104c97982831127ec/mleucL_GS70X4WNPiWjl1.png",
      "https://cdn-uploads.huggingface.co/production/uploads/63ca499104c97982831127ec/SQqgGhZs72ROYddZeZuUl.png",
      "https://cdn-uploads.huggingface.co/production/uploads/63ca499104c97982831127ec/ui60-tgbeAgwgzF3e4ncl.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.02008.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63ca499104c97982831127ec",
      "avatarUrl": "/avatars/816a962c62c805adf7789fa8e398b01e.svg",
      "fullname": "Aradhye Agarwal",
      "name": "aradhye",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.01191",
      "authors": [
        {
          "_id": "692e607b37312eaa83fd87d9",
          "name": "Krithik Vishwanath",
          "hidden": false
        },
        {
          "_id": "692e607b37312eaa83fd87da",
          "name": "Mrigayu Ghosh",
          "hidden": false
        },
        {
          "_id": "692e607b37312eaa83fd87db",
          "name": "Anton Alyakin",
          "hidden": false
        },
        {
          "_id": "692e607b37312eaa83fd87dc",
          "name": "Daniel Alexander Alber",
          "hidden": false
        },
        {
          "_id": "692e607b37312eaa83fd87dd",
          "name": "Yindalon Aphinyanaphongs",
          "hidden": false
        },
        {
          "_id": "692e607b37312eaa83fd87de",
          "name": "Eric Karl Oermann",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-01T02:14:43.000Z",
      "submittedOnDailyAt": "2025-12-02T01:14:43.269Z",
      "title": "Generalist Large Language Models Outperform Clinical Tools on Medical Benchmarks",
      "submittedOnDailyBy": {
        "_id": "66a02bb71bb371390c4ae519",
        "avatarUrl": "/avatars/2c4791516aae0b20a9c9ce0542dc966e.svg",
        "isPro": false,
        "fullname": "Krithik Vishwanath",
        "user": "KrithikV",
        "type": "user"
      },
      "summary": "Specialized clinical AI assistants are rapidly entering medical practice, often framed as safer or more reliable than general-purpose large language models (LLMs). Yet, unlike frontier models, these clinical tools are rarely subjected to independent, quantitative evaluation, creating a critical evidence gap despite their growing influence on diagnosis, triage, and guideline interpretation. We assessed two widely deployed clinical AI systems (OpenEvidence and UpToDate Expert AI) against three state-of-the-art generalist LLMs (GPT-5, Gemini 3 Pro, and Claude Sonnet 4.5) using a 1,000-item mini-benchmark combining MedQA (medical knowledge) and HealthBench (clinician-alignment) tasks. Generalist models consistently outperformed clinical tools, with GPT-5 achieving the highest scores, while OpenEvidence and UpToDate demonstrated deficits in completeness, communication quality, context awareness, and systems-based safety reasoning. These findings reveal that tools marketed for clinical decision support may often lag behind frontier LLMs, underscoring the urgent need for transparent, independent evaluation before deployment in patient-facing workflows.",
      "upvotes": 1,
      "discussionId": "692e607b37312eaa83fd87df",
      "ai_summary": "Generalist LLMs outperform specialized clinical AI assistants in a multi-task benchmark, highlighting the need for independent evaluation of clinical decision support tools.",
      "ai_keywords": [
        "clinical AI assistants",
        "large language models",
        "LLMs",
        "OpenEvidence",
        "UpToDate Expert AI",
        "GPT-5",
        "Gemini 3 Pro",
        "Claude Sonnet 4.5",
        "MedQA",
        "HealthBench",
        "clinical decision support",
        "systems-based safety reasoning"
      ],
      "organization": {
        "_id": "68697064a8772ccec3825863",
        "name": "NYU-OLAB",
        "fullname": "NYU Langone Health OLAB",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66461ec9cce50c43b3d97e0b/kdA2_L-AXMf4BfxfuN548.png"
      }
    },
    "publishedAt": "2025-11-30T21:14:43.000Z",
    "title": "Generalist Large Language Models Outperform Clinical Tools on Medical Benchmarks",
    "summary": "Specialized clinical AI assistants are rapidly entering medical practice, often framed as safer or more reliable than general-purpose large language models (LLMs). Yet, unlike frontier models, these clinical tools are rarely subjected to independent, quantitative evaluation, creating a critical evidence gap despite their growing influence on diagnosis, triage, and guideline interpretation. We assessed two widely deployed clinical AI systems (OpenEvidence and UpToDate Expert AI) against three state-of-the-art generalist LLMs (GPT-5, Gemini 3 Pro, and Claude Sonnet 4.5) using a 1,000-item mini-benchmark combining MedQA (medical knowledge) and HealthBench (clinician-alignment) tasks. Generalist models consistently outperformed clinical tools, with GPT-5 achieving the highest scores, while OpenEvidence and UpToDate demonstrated deficits in completeness, communication quality, context awareness, and systems-based safety reasoning. These findings reveal that tools marketed for clinical decision support may often lag behind frontier LLMs, underscoring the urgent need for transparent, independent evaluation before deployment in patient-facing workflows.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.01191.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66a02bb71bb371390c4ae519",
      "avatarUrl": "/avatars/2c4791516aae0b20a9c9ce0542dc966e.svg",
      "fullname": "Krithik Vishwanath",
      "name": "KrithikV",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "organization": {
      "_id": "68697064a8772ccec3825863",
      "name": "NYU-OLAB",
      "fullname": "NYU Langone Health OLAB",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66461ec9cce50c43b3d97e0b/kdA2_L-AXMf4BfxfuN548.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.00639",
      "authors": [
        {
          "_id": "692e631137312eaa83fd87fb",
          "name": "Mahmoud El Hussieni",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-29T21:24:36.000Z",
      "submittedOnDailyAt": "2025-12-02T01:25:52.014Z",
      "title": "Doppler-Enhanced Deep Learning: Improving Thyroid Nodule Segmentation with YOLOv5 Instance Segmentation",
      "submittedOnDailyBy": {
        "_id": "6422eab8e2029ade06eeee2c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6422eab8e2029ade06eeee2c/Gai3BHr2WJ0YuhdumqQ_z.png",
        "isPro": false,
        "fullname": "Mahmud ElHuseyni ",
        "user": "MElHuseyni",
        "type": "user"
      },
      "summary": "The increasing prevalence of thyroid cancer globally has led to the development of various computer-aided detection methods. Accurate segmentation of thyroid nodules is a critical first step in the development of AI-assisted clinical decision support systems. This study focuses on instance segmentation of thyroid nodules using YOLOv5 algorithms on ultrasound images. We evaluated multiple YOLOv5 variants (Nano, Small, Medium, Large, and XLarge) across two dataset versions, with and without doppler images. The YOLOv5-Large algorithm achieved the highest performance with a dice score of 91\\% and mAP of 0.87 on the dataset including doppler images. Notably, our results demonstrate that doppler images, typically excluded by physicians, can significantly improve segmentation performance. The YOLOv5-Small model achieved 79\\% dice score when doppler images were excluded, while including them improved performance across all model variants. These findings suggest that instance segmentation with YOLOv5 provides an effective real-time approach for thyroid nodule detection, with potential clinical applications in automated diagnostic systems.",
      "upvotes": 1,
      "discussionId": "692e631137312eaa83fd87fc",
      "ai_summary": "YOLOv5 algorithms enhance thyroid nodule segmentation accuracy in ultrasound images, particularly when incorporating doppler images, offering a real-time solution for clinical diagnostics.",
      "ai_keywords": [
        "YOLOv5",
        "instance segmentation",
        "thyroid nodules",
        "ultrasound images",
        "dice score",
        "mAP",
        "doppler images",
        "automated diagnostic systems"
      ]
    },
    "publishedAt": "2025-11-29T16:24:36.000Z",
    "title": "Doppler-Enhanced Deep Learning: Improving Thyroid Nodule Segmentation with YOLOv5 Instance Segmentation",
    "summary": "The increasing prevalence of thyroid cancer globally has led to the development of various computer-aided detection methods. Accurate segmentation of thyroid nodules is a critical first step in the development of AI-assisted clinical decision support systems. This study focuses on instance segmentation of thyroid nodules using YOLOv5 algorithms on ultrasound images. We evaluated multiple YOLOv5 variants (Nano, Small, Medium, Large, and XLarge) across two dataset versions, with and without doppler images. The YOLOv5-Large algorithm achieved the highest performance with a dice score of 91\\% and mAP of 0.87 on the dataset including doppler images. Notably, our results demonstrate that doppler images, typically excluded by physicians, can significantly improve segmentation performance. The YOLOv5-Small model achieved 79\\% dice score when doppler images were excluded, while including them improved performance across all model variants. These findings suggest that instance segmentation with YOLOv5 provides an effective real-time approach for thyroid nodule detection, with potential clinical applications in automated diagnostic systems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.00639.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6422eab8e2029ade06eeee2c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6422eab8e2029ade06eeee2c/Gai3BHr2WJ0YuhdumqQ_z.png",
      "fullname": "Mahmud ElHuseyni ",
      "name": "MElHuseyni",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 22
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.00387",
      "authors": [
        {
          "_id": "692e662837312eaa83fd8827",
          "name": "Kaihang Pan",
          "hidden": false
        },
        {
          "_id": "692e662837312eaa83fd8828",
          "name": "Weile Chen",
          "hidden": false
        },
        {
          "_id": "692e662837312eaa83fd8829",
          "name": "Haiyi Qiu",
          "hidden": false
        },
        {
          "_id": "692e662837312eaa83fd882a",
          "name": "Qifan Yu",
          "hidden": false
        },
        {
          "_id": "692e662837312eaa83fd882b",
          "name": "Wendong Bu",
          "hidden": false
        },
        {
          "_id": "692e662837312eaa83fd882c",
          "name": "Zehan Wang",
          "hidden": false
        },
        {
          "_id": "692e662837312eaa83fd882d",
          "name": "Yun Zhu",
          "hidden": false
        },
        {
          "_id": "692e662837312eaa83fd882e",
          "name": "Juncheng Li",
          "hidden": false
        },
        {
          "_id": "692e662837312eaa83fd882f",
          "name": "Siliang Tang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/Lr0kkmYB0sZJdiWeUfvJq.png",
        "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/Y2aINlzYCKNWp_-evliic.png"
      ],
      "publishedAt": "2025-11-29T08:32:35.000Z",
      "submittedOnDailyAt": "2025-12-02T01:38:46.333Z",
      "title": "WiseEdit: Benchmarking Cognition- and Creativity-Informed Image Editing",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Recent image editing models boast next-level intelligent capabilities, facilitating cognition- and creativity-informed image editing. Yet, existing benchmarks provide too narrow a scope for evaluation, failing to holistically assess these advanced abilities. To address this, we introduce WiseEdit, a knowledge-intensive benchmark for comprehensive evaluation of cognition- and creativity-informed image editing, featuring deep task depth and broad knowledge breadth. Drawing an analogy to human cognitive creation, WiseEdit decomposes image editing into three cascaded steps, i.e., Awareness, Interpretation, and Imagination, each corresponding to a task that poses a challenge for models to complete at the specific step. It also encompasses complex tasks, where none of the three steps can be finished easily. Furthermore, WiseEdit incorporates three fundamental types of knowledge: Declarative, Procedural, and Metacognitive knowledge. Ultimately, WiseEdit comprises 1,220 test cases, objectively revealing the limitations of SoTA image editing models in knowledge-based cognitive reasoning and creative composition capabilities. The benchmark, evaluation code, and the generated images of each model will be made publicly available soon. Project Page: https://qnancy.github.io/wiseedit_project_page/.",
      "upvotes": 1,
      "discussionId": "692e662837312eaa83fd8830",
      "projectPage": "https://qnancy.github.io/wiseedit_project_page/",
      "ai_summary": "WiseEdit is a benchmark for evaluating cognition- and creativity-informed image editing models by decomposing the process into Awareness, Interpretation, and Imagination tasks, assessing them on declarative, procedural, and metacognitive knowledge.",
      "ai_keywords": [
        "decomposition",
        "Awareness",
        "Interpretation",
        "Imagination",
        "Declarative knowledge",
        "Procedural knowledge",
        "Metacognitive knowledge"
      ]
    },
    "publishedAt": "2025-11-29T03:32:35.000Z",
    "title": "WiseEdit: Benchmarking Cognition- and Creativity-Informed Image Editing",
    "summary": "Recent image editing models boast next-level intelligent capabilities, facilitating cognition- and creativity-informed image editing. Yet, existing benchmarks provide too narrow a scope for evaluation, failing to holistically assess these advanced abilities. To address this, we introduce WiseEdit, a knowledge-intensive benchmark for comprehensive evaluation of cognition- and creativity-informed image editing, featuring deep task depth and broad knowledge breadth. Drawing an analogy to human cognitive creation, WiseEdit decomposes image editing into three cascaded steps, i.e., Awareness, Interpretation, and Imagination, each corresponding to a task that poses a challenge for models to complete at the specific step. It also encompasses complex tasks, where none of the three steps can be finished easily. Furthermore, WiseEdit incorporates three fundamental types of knowledge: Declarative, Procedural, and Metacognitive knowledge. Ultimately, WiseEdit comprises 1,220 test cases, objectively revealing the limitations of SoTA image editing models in knowledge-based cognitive reasoning and creative composition capabilities. The benchmark, evaluation code, and the generated images of each model will be made publicly available soon. Project Page: https://qnancy.github.io/wiseedit_project_page/.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/Lr0kkmYB0sZJdiWeUfvJq.png",
      "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/Y2aINlzYCKNWp_-evliic.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.00387.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 173
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.00333",
      "authors": [
        {
          "_id": "692e6f4137312eaa83fd889c",
          "name": "Ayush Maheshwari",
          "hidden": false
        },
        {
          "_id": "692e6f4137312eaa83fd889d",
          "name": "Kaushal Sharma",
          "hidden": false
        },
        {
          "_id": "692e6f4137312eaa83fd889e",
          "name": "Vivek Patel",
          "hidden": false
        },
        {
          "_id": "692e6f4137312eaa83fd889f",
          "name": "Aditya Maheshwari",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-29T05:49:50.000Z",
      "submittedOnDailyAt": "2025-12-02T02:17:52.263Z",
      "title": "IndicParam: Benchmark to evaluate LLMs on low-resource Indic Languages",
      "submittedOnDailyBy": {
        "_id": "64a6ec82731903917bddb6ee",
        "avatarUrl": "/avatars/be7a4c1554b287bf922b1c402c46b33b.svg",
        "isPro": false,
        "fullname": "Ayush",
        "user": "acomquest",
        "type": "user"
      },
      "summary": "While large language models excel on high-resource multilingual tasks, low- and extremely low-resource Indic languages remain severely under-evaluated. We present IndicParam, a human-curated benchmark of over 13,000 multiple-choice questions covering 11 such languages (Nepali, Gujarati, Marathi, Odia as low-resource; Dogri, Maithili, Rajasthani, Sanskrit, Bodo, Santali, Konkani as extremely low-resource) plus Sanskrit-English code-mixed set. We evaluated 19 LLMs, both proprietary and open-weights, which reveals that even the top-performing GPT-5 reaches only 45.0% average accuracy, followed by DeepSeek-3.2 (43.1) and Claude-4.5 (42.7). We additionally label each question as knowledge-oriented or purely linguistic to discriminate factual recall from grammatical proficiency. Further, we assess the ability of LLMs to handle diverse question formats-such as list-based matching, assertion-reason pairs, and sequence ordering-alongside conventional multiple-choice questions. IndicParam provides insights into limitations of cross-lingual transfer and establishes a challenging benchmark for Indic languages. The dataset is available at https://huggingface.co/datasets/bharatgenai/IndicParam. Scripts to run benchmark are present at https://github.com/ayushbits/IndicParam.",
      "upvotes": 0,
      "discussionId": "692e6f4237312eaa83fd88a0",
      "projectPage": "https://huggingface.co/datasets/bharatgenai/IndicParam",
      "githubRepo": "https://github.com/ayushbits/IndicParam",
      "ai_summary": "IndicParam benchmark evaluates multiple-choice question performance of LLMs across 11 Indic languages, revealing limitations in cross-lingual transfer and grammatical proficiency.",
      "ai_keywords": [
        "large language models",
        "low-resource languages",
        "extremely low-resource languages",
        "human-curated benchmark",
        "multiple-choice questions",
        "knowledge-oriented",
        "purely linguistic",
        "list-based matching",
        "assertion-reason pairs",
        "sequence ordering"
      ],
      "githubStars": 0,
      "organization": {
        "_id": "67b473e74dd7ea0538ef5d5f",
        "name": "bharatgenai",
        "fullname": "BharatGen AI",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67b462a1f4f414c2b3e2bc2f/EnVeNWEIeZ6yF6ueZ7E3Y.jpeg"
      }
    },
    "publishedAt": "2025-11-29T00:49:50.000Z",
    "title": "IndicParam: Benchmark to evaluate LLMs on low-resource Indic Languages",
    "summary": "While large language models excel on high-resource multilingual tasks, low- and extremely low-resource Indic languages remain severely under-evaluated. We present IndicParam, a human-curated benchmark of over 13,000 multiple-choice questions covering 11 such languages (Nepali, Gujarati, Marathi, Odia as low-resource; Dogri, Maithili, Rajasthani, Sanskrit, Bodo, Santali, Konkani as extremely low-resource) plus Sanskrit-English code-mixed set. We evaluated 19 LLMs, both proprietary and open-weights, which reveals that even the top-performing GPT-5 reaches only 45.0% average accuracy, followed by DeepSeek-3.2 (43.1) and Claude-4.5 (42.7). We additionally label each question as knowledge-oriented or purely linguistic to discriminate factual recall from grammatical proficiency. Further, we assess the ability of LLMs to handle diverse question formats-such as list-based matching, assertion-reason pairs, and sequence ordering-alongside conventional multiple-choice questions. IndicParam provides insights into limitations of cross-lingual transfer and establishes a challenging benchmark for Indic languages. The dataset is available at https://huggingface.co/datasets/bharatgenai/IndicParam. Scripts to run benchmark are present at https://github.com/ayushbits/IndicParam.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.00333.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a6ec82731903917bddb6ee",
      "avatarUrl": "/avatars/be7a4c1554b287bf922b1c402c46b33b.svg",
      "fullname": "Ayush",
      "name": "acomquest",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "67b473e74dd7ea0538ef5d5f",
      "name": "bharatgenai",
      "fullname": "BharatGen AI",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67b462a1f4f414c2b3e2bc2f/EnVeNWEIeZ6yF6ueZ7E3Y.jpeg"
    },
    "isAuthorParticipating": false
  }
]