[
    "{'paper': {'id': '2501.01895', 'authors': [{'_id': '677b5c2478ac1cec9684059f', 'user': {'_id': '634e4120038b5879133552f5', 'avatarUrl': '/avatars/34ec861b4bbf1aecf927a7d6e726c7a4.svg', 'isPro': True, 'fullname': 'Siyuan', 'user': 'SiyuanH', 'type': 'user'}, 'name': 'Siyuan Huang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-06T07:59:36.568Z', 'hidden': False}, {'_id': '677b5c2478ac1cec968405a0', 'user': {'_id': '640b00555a9c21b95c6449b3', 'avatarUrl': '/avatars/5fa43b956f3acc671f033e31b7ca76c5.svg', 'isPro': False, 'fullname': 'Liliang Chen', 'user': 'pathcn', 'type': 'user'}, 'name': 'Liliang Chen', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-06T08:21:46.139Z', 'hidden': False}, {'_id': '677b5c2478ac1cec968405a1', 'user': {'_id': '65df481e530333731ea24617', 'avatarUrl': '/avatars/3ed41f5d7d0489193807b5e6260f16c9.svg', 'isPro': False, 'fullname': 'ZHOU PENGFEI', 'user': 'lyuukuu', 'type': 'user'}, 'name': 'Pengfei Zhou', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-06T08:22:24.033Z', 'hidden': False}, {'_id': '677b5c2478ac1cec968405a2', 'user': {'_id': '6575f9aeca03b6c514fe6e5c', 'avatarUrl': '/avatars/a6e9d428beaa124ee989d702b9bf4f85.svg', 'isPro': False, 'fullname': 'Shengcong Chen', 'user': 'Shengcong', 'type': 'user'}, 'name': 'Shengcong Chen', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-06T08:22:29.700Z', 'hidden': False}, {'_id': '677b5c2478ac1cec968405a3', 'user': {'_id': '67593dd0f522f4409e614ba0', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/67593dd0f522f4409e614ba0/cvb9w_8seu3Kbjg_XAnNj.jpeg', 'isPro': False, 'fullname': 'Jiang Zhengkai', 'user': 'jzzzzk', 'type': 'user'}, 'name': 'Zhengkai Jiang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-06T08:22:42.112Z', 'hidden': False}, {'_id': '677b5c2478ac1cec968405a4', 'name': 'Yue Hu', 'hidden': False}, {'_id': '677b5c2478ac1cec968405a5', 'name': 'Peng Gao', 'hidden': False}, {'_id': '677b5c2478ac1cec968405a6', 'user': {'_id': '65c04e9c27a5fdca81abcbd9', 'avatarUrl': '/avatars/12a155683c824fa23da4a9e2bed4f64e.svg', 'isPro': False, 'fullname': 'Hongsheng LI', 'user': 'hsli-cuhk', 'type': 'user'}, 'name': 'Hongsheng Li', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-06T08:23:37.965Z', 'hidden': False}, {'_id': '677b5c2478ac1cec968405a7', 'user': {'_id': '67739bfa64e8b7438ae68eb4', 'avatarUrl': '/avatars/15193bfbce487b2de4ce8c86bd18885a.svg', 'isPro': False, 'fullname': 'Maoqing Yao', 'user': 'AutobotZero', 'type': 'user'}, 'name': 'Maoqing Yao', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-06T08:23:43.822Z', 'hidden': False}, {'_id': '677b5c2478ac1cec968405a8', 'user': {'_id': '646ec9b135f55eb49e405faa', 'avatarUrl': '/avatars/a17194be585d20e2a021e77a5a20e213.svg', 'isPro': False, 'fullname': 'Guanghui Ren', 'user': 'sundrops', 'type': 'user'}, 'name': 'Guanghui Ren', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-06T12:37:40.659Z', 'hidden': False}], 'publishedAt': '2025-01-03T17:00:33.000Z', 'title': 'EnerVerse: Envisioning Embodied Future Space for Robotics Manipulation', 'summary': \"We introduce EnerVerse, a comprehensive framework for embodied future space\\ngeneration specifically designed for robotic manipulation tasks. EnerVerse\\nseamlessly integrates convolutional and bidirectional attention mechanisms for\\ninner-chunk space modeling, ensuring low-level consistency and continuity.\\nRecognizing the inherent redundancy in video data, we propose a sparse memory\\ncontext combined with a chunkwise unidirectional generative paradigm to enable\\nthe generation of infinitely long sequences. To further augment robotic\\ncapabilities, we introduce the Free Anchor View (FAV) space, which provides\\nflexible perspectives to enhance observation and analysis. The FAV space\\nmitigates motion modeling ambiguity, removes physical constraints in confined\\nenvironments, and significantly improves the robot's generalization and\\nadaptability across various tasks and settings. To address the prohibitive\\ncosts and labor intensity of acquiring multi-camera observations, we present a\\ndata engine pipeline that integrates a generative model with 4D Gaussian\\nSplatting (4DGS). This pipeline leverages the generative model's robust\\ngeneralization capabilities and the spatial constraints provided by 4DGS,\\nenabling an iterative enhancement of data quality and diversity, thus creating\\na data flywheel effect that effectively narrows the sim-to-real gap. Finally,\\nour experiments demonstrate that the embodied future space generation prior\\nsubstantially enhances policy predictive capabilities, resulting in improved\\noverall performance, particularly in long-range robotic manipulation tasks.\", 'upvotes': 40, 'discussionId': '677b5c2978ac1cec96840687'}, 'publishedAt': '2025-01-05T23:32:23.698Z', 'title': 'EnerVerse: Envisioning Embodied Future Space for Robotics Manipulation', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.01895.png', 'numComments': 1, 'submittedBy': {'_id': '634e4120038b5879133552f5', 'avatarUrl': '/avatars/34ec861b4bbf1aecf927a7d6e726c7a4.svg', 'fullname': 'Siyuan', 'name': 'SiyuanH', 'type': 'user', 'isPro': True, 'isHf': False, 'isMod': False, 'followerCount': 5}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2501.01957', 'authors': [{'_id': '677b5979a54b76dcaa4991f9', 'name': 'Chaoyou Fu', 'hidden': False}, {'_id': '677b5979a54b76dcaa4991fa', 'user': {'_id': '64ffd436d522560505a94b8e', 'avatarUrl': '/avatars/02d4faac40ac203cb5d635cfcb39780c.svg', 'isPro': False, 'fullname': 'Haojia Lin', 'user': 'linhaojia13', 'type': 'user'}, 'name': 'Haojia Lin', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-06T08:24:54.539Z', 'hidden': False}, {'_id': '677b5979a54b76dcaa4991fb', 'user': {'_id': '664eaf0a98e93ef417c3cc42', 'avatarUrl': '/avatars/67fb44351cac8964410e5b6549817182.svg', 'isPro': False, 'fullname': 'Xiong Wang', 'user': 'xiongwang', 'type': 'user'}, 'name': 'Xiong Wang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-06T08:25:10.743Z', 'hidden': False}, {'_id': '677b5979a54b76dcaa4991fc', 'user': {'_id': '623d8ca4c29adf5ef6175615', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg', 'isPro': False, 'fullname': 'Yi-Fan Zhang', 'user': 'yifanzhang114', 'type': 'user'}, 'name': 'Yi-Fan Zhang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-06T08:25:17.351Z', 'hidden': False}, {'_id': '677b5979a54b76dcaa4991fd', 'user': {'_id': '6483143902f98c3f05aff915', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6483143902f98c3f05aff915/ZhWFFgrlRsQf4MXiInh5p.jpeg', 'isPro': False, 'fullname': '沈云航 Yunhang Shen', 'user': 'shenyunhang', 'type': 'user'}, 'name': 'Yunhang Shen', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-06T08:25:29.653Z', 'hidden': False}, {'_id': '677b5979a54b76dcaa4991fe', 'name': 'Xiaoyu Liu', 'hidden': False}, {'_id': '677b5979a54b76dcaa4991ff', 'name': 'Yangze Li', 'hidden': False}, {'_id': '677b5979a54b76dcaa499200', 'name': 'Zuwei Long', 'hidden': False}, {'_id': '677b5979a54b76dcaa499201', 'user': {'_id': '65ff5dcf82708115869da69a', 'avatarUrl': '/avatars/10edebbc559e9fb8b0e377c82eba66d4.svg', 'isPro': False, 'fullname': 'Heting Gao', 'user': 'hertin', 'type': 'user'}, 'name': 'Heting Gao', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-06T08:26:18.115Z', 'hidden': False}, {'_id': '677b5979a54b76dcaa499202', 'name': 'Ke Li', 'hidden': False}, {'_id': '677b5979a54b76dcaa499203', 'user': {'_id': '665d85e35491b1e10d0d5221', 'avatarUrl': '/avatars/e46ea55b197e2bf8038871ae95f59585.svg', 'isPro': False, 'fullname': 'Xiawu Zheng', 'user': 'zhengxiawu', 'type': 'user'}, 'name': 'Xiawu Zheng', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-06T08:26:25.079Z', 'hidden': False}, {'_id': '677b5979a54b76dcaa499204', 'name': 'Rongrong Ji', 'hidden': False}, {'_id': '677b5979a54b76dcaa499205', 'user': {'_id': '647401e50da364bd0d002f2a', 'avatarUrl': '/avatars/f1586f610f21ddb4f868856208c2cfab.svg', 'isPro': False, 'fullname': 'XING Sun', 'user': 'tedsun', 'type': 'user'}, 'name': 'Xing Sun', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-06T08:30:21.180Z', 'hidden': False}, {'_id': '677b5979a54b76dcaa499206', 'name': 'Caifeng Shan', 'hidden': False}, {'_id': '677b5979a54b76dcaa499207', 'name': 'Ran He', 'hidden': False}], 'publishedAt': '2025-01-03T18:59:52.000Z', 'title': 'VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction', 'summary': 'Recent Multimodal Large Language Models (MLLMs) have typically focused on\\nintegrating visual and textual modalities, with less emphasis placed on the\\nrole of speech in enhancing interaction. However, speech plays a crucial role\\nin multimodal dialogue systems, and implementing high-performance in both\\nvision and speech tasks remains a significant challenge due to the fundamental\\nmodality differences. In this paper, we propose a carefully designed\\nmulti-stage training methodology that progressively trains LLM to understand\\nboth visual and speech information, ultimately enabling fluent vision and\\nspeech interaction. Our approach not only preserves strong vision-language\\ncapacity, but also enables efficient speech-to-speech dialogue capabilities\\nwithout separate ASR and TTS modules, significantly accelerating multimodal\\nend-to-end response speed. By comparing our method against state-of-the-art\\ncounterparts across benchmarks for image, video, and speech tasks, we\\ndemonstrate that our model is equipped with both strong visual and speech\\ncapabilities, making near real-time vision and speech interaction.', 'upvotes': 19, 'discussionId': '677b597aa54b76dcaa499262'}, 'publishedAt': '2025-01-05T23:18:26.341Z', 'title': 'VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.01957.png', 'numComments': 1, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isMod': False, 'followerCount': 5571}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2501.01904', 'authors': [{'_id': '677b56b8b91035bc42259da5', 'user': {'_id': '61d78857a21a9b49c7e8e4a9', 'avatarUrl': '/avatars/c7e7f84cad775be2d13fab8530bf21f5.svg', 'isPro': False, 'fullname': 'Yifan Du', 'user': 'Richard1999', 'type': 'user'}, 'name': 'Yifan Du', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-06T08:32:27.164Z', 'hidden': False}, {'_id': '677b56b8b91035bc42259da6', 'user': {'_id': '6448dcf1b6ac93fe6512e342', 'avatarUrl': '/avatars/a6441f89eabd156181bafc47c0b2f8c8.svg', 'isPro': False, 'fullname': 'Zikang Liu', 'user': 'JohnCage', 'type': 'user'}, 'name': 'Zikang Liu', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-06T08:32:32.935Z', 'hidden': False}, {'_id': '677b56b8b91035bc42259da7', 'user': {'_id': '65407faf81a8731a1c134c39', 'avatarUrl': '/avatars/eddc718cf16d316223bcbbd2d13cf15d.svg', 'isPro': False, 'fullname': 'Yifan Li', 'user': 'yifanli', 'type': 'user'}, 'name': 'Yifan Li', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-06T08:32:39.715Z', 'hidden': False}, {'_id': '677b56b8b91035bc42259da8', 'name': 'Wayne Xin Zhao', 'hidden': False}, {'_id': '677b56b8b91035bc42259da9', 'name': 'Yuqi Huo', 'hidden': False}, {'_id': '677b56b8b91035bc42259daa', 'name': 'Bingning Wang', 'hidden': False}, {'_id': '677b56b8b91035bc42259dab', 'user': {'_id': '6501587887b370a56ad2608e', 'avatarUrl': '/avatars/6779baaa8ed9032de55a2f78e1f52e20.svg', 'isPro': False, 'fullname': 'Wei-Peng Chen', 'user': 'whenfra', 'type': 'user'}, 'name': 'Weipeng Chen', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-06T08:33:31.102Z', 'hidden': False}, {'_id': '677b56b8b91035bc42259dac', 'name': 'Zheng Liu', 'hidden': False}, {'_id': '677b56b8b91035bc42259dad', 'name': 'Zhongyuan Wang', 'hidden': False}, {'_id': '677b56b8b91035bc42259dae', 'user': {'_id': '64b8c89052b7353d8c6a1013', 'avatarUrl': '/avatars/cd59fffe81f6b07b4519540b8ff3d95f.svg', 'isPro': False, 'fullname': 'Ji-Rong Wen', 'user': 'jrwen', 'type': 'user'}, 'name': 'Ji-Rong Wen', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-06T08:34:39.799Z', 'hidden': False}], 'publishedAt': '2025-01-03T17:14:16.000Z', 'title': 'Virgo: A Preliminary Exploration on Reproducing o1-like MLLM', 'summary': 'Recently, slow-thinking reasoning systems, built upon large language models\\n(LLMs), have garnered widespread attention by scaling the thinking time during\\ninference. There is also growing interest in adapting this capability to\\nmultimodal large language models (MLLMs). Given that MLLMs handle more complex\\ndata semantics across different modalities, it is intuitively more challenging\\nto implement multimodal slow-thinking systems.\\n  To address this issue, in this paper, we explore a straightforward approach\\nby fine-tuning a capable MLLM with a small amount of textual long-form thought\\ndata, resulting in a multimodal slow-thinking system, Virgo (Visual reasoning\\nwith long thought). We find that these long-form reasoning processes, expressed\\nin natural language, can be effectively transferred to MLLMs. Moreover, it\\nseems that such textual reasoning data can be even more effective than visual\\nreasoning data in eliciting the slow-thinking capacities of MLLMs. While this\\nwork is preliminary, it demonstrates that slow-thinking capacities are\\nfundamentally associated with the language model component, which can be\\ntransferred across modalities or domains. This finding can be leveraged to\\nguide the development of more powerful slow-thinking reasoning systems. We\\nrelease our resources at https://github.com/RUCAIBox/Virgo.', 'upvotes': 11, 'discussionId': '677b56b9b91035bc42259df3'}, 'publishedAt': '2025-01-05T23:08:18.520Z', 'title': 'Virgo: A Preliminary Exploration on Reproducing o1-like MLLM', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.01904.png', 'numComments': 1, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isMod': False, 'followerCount': 5571}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2501.01821', 'authors': [{'_id': '677b977e08c75e8046c38449', 'user': {'_id': '649974df6a49e761b2944145', 'avatarUrl': '/avatars/5f149f87df89786807f3185a1e538362.svg', 'isPro': False, 'fullname': 'Aobo Kong', 'user': 'KAB1314', 'type': 'user'}, 'name': 'Aobo Kong', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-06T12:37:42.697Z', 'hidden': False}, {'_id': '677b977e08c75e8046c3844a', 'name': 'Wentao Ma', 'hidden': False}, {'_id': '677b977e08c75e8046c3844b', 'user': {'_id': '62f4b84f2f63f904a0c5b355', 'avatarUrl': '/avatars/aa70c9709c736745246cadb7286c1e62.svg', 'isPro': False, 'fullname': 'Shiwan Zhao', 'user': 'shiwan', 'type': 'user'}, 'name': 'Shiwan Zhao', 'status': 'extracted_confirmed', 'statusLastChangedAt': '2025-01-06T12:16:57.707Z', 'hidden': False}, {'_id': '677b977e08c75e8046c3844c', 'name': 'Yongbin Li', 'hidden': False}, {'_id': '677b977e08c75e8046c3844d', 'name': 'Yuchuan Wu', 'hidden': False}, {'_id': '677b977e08c75e8046c3844e', 'name': 'Ke Wang', 'hidden': False}, {'_id': '677b977e08c75e8046c3844f', 'name': 'Xiaoqian Liu', 'hidden': False}, {'_id': '677b977e08c75e8046c38450', 'name': 'Qicheng Li', 'hidden': False}, {'_id': '677b977e08c75e8046c38451', 'name': 'Yong Qin', 'hidden': False}, {'_id': '677b977e08c75e8046c38452', 'name': 'Fei Huang', 'hidden': False}], 'publishedAt': '2025-01-03T14:09:46.000Z', 'title': 'SDPO: Segment-Level Direct Preference Optimization for Social Agents', 'summary': \"Social agents powered by large language models (LLMs) can simulate human\\nsocial behaviors but fall short in handling complex goal-oriented social\\ndialogues. Direct Preference Optimization (DPO) has proven effective in\\naligning LLM behavior with human preferences across a variety of agent tasks.\\nExisting DPO-based approaches for multi-turn interactions are divided into\\nturn-level and session-level methods. The turn-level method is overly\\nfine-grained, focusing exclusively on individual turns, while session-level\\nmethods are too coarse-grained, often introducing training noise. To address\\nthese limitations, we propose Segment-Level Direct Preference Optimization\\n(SDPO), which focuses on specific key segments within interactions to optimize\\nmulti-turn agent behavior while minimizing training noise. Evaluations on the\\nSOTOPIA benchmark demonstrate that SDPO-tuned agents consistently outperform\\nboth existing DPO-based methods and proprietary LLMs like GPT-4o, underscoring\\nSDPO's potential to advance the social intelligence of LLM-based agents. We\\nrelease our code and data at\\nhttps://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/SDPO.\", 'upvotes': 10, 'discussionId': '677b977f08c75e8046c38485'}, 'publishedAt': '2025-01-06T07:47:03.315Z', 'title': 'SDPO: Segment-Level Direct Preference Optimization for Social Agents', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.01821.png', 'numComments': 1, 'submittedBy': {'_id': '649974df6a49e761b2944145', 'avatarUrl': '/avatars/5f149f87df89786807f3185a1e538362.svg', 'fullname': 'Aobo Kong', 'name': 'KAB1314', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2412.21059', 'authors': [{'_id': '677a6fb5deb62f25f4209c2a', 'user': {'_id': '62d7b131f6e8ba66107af761', 'avatarUrl': '/avatars/f1c5df47aef69c824fd166722df8f670.svg', 'isPro': False, 'fullname': 'Jiazheng Xu', 'user': 'xujz0703', 'type': 'user'}, 'name': 'Jiazheng Xu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-06T08:00:26.452Z', 'hidden': False}, {'_id': '677a6fb5deb62f25f4209c2b', 'name': 'Yu Huang', 'hidden': False}, {'_id': '677a6fb5deb62f25f4209c2c', 'user': {'_id': '627626d42d26ac639e56f565', 'avatarUrl': '/avatars/805c5f909f52656345b8bde486c9fa8f.svg', 'isPro': False, 'fullname': 'Jiale Cheng', 'user': 'CCCCCC', 'type': 'user'}, 'name': 'Jiale Cheng', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-06T12:41:34.810Z', 'hidden': False}, {'_id': '677a6fb5deb62f25f4209c2d', 'user': {'_id': '659b6c50b0f43ed69fe09d56', 'avatarUrl': '/avatars/8ea56c56263595a9f7555f2c2520641a.svg', 'isPro': False, 'fullname': '杨远明', 'user': 'yangyuanming', 'type': 'user'}, 'name': 'Yuanming Yang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-06T12:41:48.700Z', 'hidden': False}, {'_id': '677a6fb5deb62f25f4209c2e', 'name': 'Jiajun Xu', 'hidden': False}, {'_id': '677a6fb5deb62f25f4209c2f', 'name': 'Yuan Wang', 'hidden': False}, {'_id': '677a6fb5deb62f25f4209c30', 'name': 'Wenbo Duan', 'hidden': False}, {'_id': '677a6fb5deb62f25f4209c31', 'name': 'Shen Yang', 'hidden': False}, {'_id': '677a6fb5deb62f25f4209c32', 'user': {'_id': '65007e0870b6b05c5ad94a62', 'avatarUrl': '/avatars/c1ff713db5748db121738c601f8add85.svg', 'isPro': False, 'fullname': 'Jin Qunlin', 'user': 'kimtorch', 'type': 'user'}, 'name': 'Qunlin Jin', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-06T12:42:43.541Z', 'hidden': False}, {'_id': '677a6fb5deb62f25f4209c33', 'name': 'Shurun Li', 'hidden': False}, {'_id': '677a6fb5deb62f25f4209c34', 'user': {'_id': '65228733377bffdc59a10117', 'avatarUrl': '/avatars/6eec07553658ab22f8058caa0bfbed49.svg', 'isPro': False, 'fullname': 'tengjiayan', 'user': 'tengjiayan', 'type': 'user'}, 'name': 'Jiayan Teng', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-06T12:43:16.822Z', 'hidden': False}, {'_id': '677a6fb5deb62f25f4209c35', 'user': {'_id': '6466d1640ed2f7a8cba87503', 'avatarUrl': '/avatars/652746e63dfeb5154ae7d34039d1a485.svg', 'isPro': False, 'fullname': 'Zhuoyi Yang', 'user': 'zyyangzy', 'type': 'user'}, 'name': 'Zhuoyi Yang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-06T12:43:40.186Z', 'hidden': False}, {'_id': '677a6fb5deb62f25f4209c36', 'user': {'_id': '650aaaf1634e02df56dfd231', 'avatarUrl': '/avatars/4643ab23721d4ed6aeb1ebbc717adc43.svg', 'isPro': False, 'fullname': 'Wendi Zheng', 'user': 'zwd125', 'type': 'user'}, 'name': 'Wendi Zheng', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-06T12:44:00.832Z', 'hidden': False}, {'_id': '677a6fb5deb62f25f4209c37', 'name': 'Xiao Liu', 'hidden': False}, {'_id': '677a6fb5deb62f25f4209c38', 'name': 'Ming Ding', 'hidden': False}, {'_id': '677a6fb5deb62f25f4209c39', 'name': 'Xiaohan Zhang', 'hidden': False}, {'_id': '677a6fb5deb62f25f4209c3a', 'name': 'Xiaotao Gu', 'hidden': False}, {'_id': '677a6fb5deb62f25f4209c3b', 'user': {'_id': '6406db5cd684369027166986', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6406db5cd684369027166986/Zl-orrGcbY0RbfjfKszn1.jpeg', 'isPro': False, 'fullname': 'Shiyu Huang', 'user': 'ShiyuHuang', 'type': 'user'}, 'name': 'Shiyu Huang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-06T12:39:28.802Z', 'hidden': False}, {'_id': '677a6fb5deb62f25f4209c3c', 'name': 'Minlie Huang', 'hidden': False}, {'_id': '677a6fb5deb62f25f4209c3d', 'user': {'_id': '640dff05474aa6f89556677e', 'avatarUrl': '/avatars/1b4591c7322d649c797b3125148f1915.svg', 'isPro': False, 'fullname': 'Jie Tang', 'user': 'jerytang', 'type': 'user'}, 'name': 'Jie Tang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-06T12:38:45.096Z', 'hidden': False}, {'_id': '677a6fb5deb62f25f4209c3e', 'user': {'_id': '640e73bdfdeaae1390857b62', 'avatarUrl': '/avatars/cd6779e30f716002a7838ed93d5c0754.svg', 'isPro': False, 'fullname': 'Yuxiao Dong', 'user': 'yuxiaod', 'type': 'user'}, 'name': 'Yuxiao Dong', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-06T12:38:37.430Z', 'hidden': False}], 'publishedAt': '2024-12-30T16:24:09.000Z', 'title': 'VisionReward: Fine-Grained Multi-Dimensional Human Preference Learning\\n  for Image and Video Generation', 'summary': 'We present a general strategy to aligning visual generation models -- both\\nimage and video generation -- with human preference. To start with, we build\\nVisionReward -- a fine-grained and multi-dimensional reward model. We decompose\\nhuman preferences in images and videos into multiple dimensions, each\\nrepresented by a series of judgment questions, linearly weighted and summed to\\nan interpretable and accurate score. To address the challenges of video quality\\nassessment, we systematically analyze various dynamic features of videos, which\\nhelps VisionReward surpass VideoScore by 17.2% and achieve top performance for\\nvideo preference prediction. Based on VisionReward, we develop a\\nmulti-objective preference learning algorithm that effectively addresses the\\nissue of confounding factors within preference data. Our approach significantly\\noutperforms existing image and video scoring methods on both machine metrics\\nand human evaluation. All code and datasets are provided at\\nhttps://github.com/THUDM/VisionReward.', 'upvotes': 10, 'discussionId': '677a6fbadeb62f25f4209e46'}, 'publishedAt': '2025-01-06T03:36:20.346Z', 'title': 'VisionReward: Fine-Grained Multi-Dimensional Human Preference Learning for Image and Video Generation', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.21059.png', 'numComments': 1, 'submittedBy': {'_id': '62d7b131f6e8ba66107af761', 'avatarUrl': '/avatars/f1c5df47aef69c824fd166722df8f670.svg', 'fullname': 'Jiazheng Xu', 'name': 'xujz0703', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 2}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2501.01073', 'authors': [{'_id': '677b7e4ef3f3f282a1051101', 'user': {'_id': '648015c6b82adfa7cc1a9565', 'avatarUrl': '/avatars/e630e72a9d80a4d73bbf979133a0e07a.svg', 'isPro': True, 'fullname': 'Xiaohui Chen', 'user': 'xchen16', 'type': 'user'}, 'name': 'Xiaohui Chen', 'status': 'extracted_pending', 'statusLastChangedAt': '2025-01-06T06:55:12.787Z', 'hidden': False}, {'_id': '677b7e4ef3f3f282a1051102', 'user': {'_id': '6115bec563c5df6b8d45fb70', 'avatarUrl': '/avatars/4392649682d87e99f396a2bff8a90b6e.svg', 'isPro': False, 'fullname': 'Yinkai Wang', 'user': 'Spony', 'type': 'user'}, 'name': 'Yinkai Wang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-06T08:30:56.674Z', 'hidden': False}, {'_id': '677b7e4ef3f3f282a1051103', 'user': {'_id': '666a81d55aa2df95fad831b6', 'avatarUrl': '/avatars/de72f369f3841c8c4071520755ad267a.svg', 'isPro': False, 'fullname': 'He Jiaxing', 'user': 'RArchered', 'type': 'user'}, 'name': 'Jiaxing He', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-06T08:31:07.412Z', 'hidden': False}, {'_id': '677b7e4ef3f3f282a1051104', 'user': {'_id': '632513c778ba0c1d795513c9', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1663374211032-noauth.jpeg', 'isPro': False, 'fullname': 'Yuanqi Du', 'user': 'y6q9', 'type': 'user'}, 'name': 'Yuanqi Du', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-06T08:31:13.712Z', 'hidden': False}, {'_id': '677b7e4ef3f3f282a1051105', 'name': 'Soha Hassoun', 'hidden': False}, {'_id': '677b7e4ef3f3f282a1051106', 'user': {'_id': '670cc73815f57d098521cbad', 'avatarUrl': '/avatars/2aeac179f215a344635e2aa0a7a64f7c.svg', 'isPro': False, 'fullname': 'XIaolin Xu', 'user': 'XiaolinXu', 'type': 'user'}, 'name': 'Xiaolin Xu', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-06T08:31:24.763Z', 'hidden': False}, {'_id': '677b7e4ef3f3f282a1051107', 'name': 'Li-Ping Liu', 'hidden': False}], 'publishedAt': '2025-01-02T05:44:11.000Z', 'title': 'Graph Generative Pre-trained Transformer', 'summary': \"Graph generation is a critical task in numerous domains, including molecular\\ndesign and social network analysis, due to its ability to model complex\\nrelationships and structured data. While most modern graph generative models\\nutilize adjacency matrix representations, this work revisits an alternative\\napproach that represents graphs as sequences of node set and edge set. We\\nadvocate for this approach due to its efficient encoding of graphs and propose\\na novel representation. Based on this representation, we introduce the Graph\\nGenerative Pre-trained Transformer (G2PT), an auto-regressive model that learns\\ngraph structures via next-token prediction. To further exploit G2PT's\\ncapabilities as a general-purpose foundation model, we explore fine-tuning\\nstrategies for two downstream applications: goal-oriented generation and graph\\nproperty prediction. We conduct extensive experiments across multiple datasets.\\nResults indicate that G2PT achieves superior generative performance on both\\ngeneric graph and molecule datasets. Furthermore, G2PT exhibits strong\\nadaptability and versatility in downstream tasks from molecular design to\\nproperty prediction.\", 'upvotes': 9, 'discussionId': '677b7e50f3f3f282a105119b'}, 'publishedAt': '2025-01-06T01:55:22.468Z', 'title': 'Graph Generative Pre-trained Transformer', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.01073.png', 'numComments': 1, 'submittedBy': {'_id': '63024676056ec3a2a8714b24', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1661093436322-noauth.jpeg', 'fullname': 'Xiang Liu', 'name': 'Dominic789654', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 2}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2501.00874', 'authors': [{'_id': '677b6abf6b1653e3951b4cbd', 'user': {'_id': '6225ce77b213da14d165fa50', 'avatarUrl': '/avatars/64d2307859d73343280a895ca81e3136.svg', 'isPro': False, 'fullname': 'Hieu Man', 'user': 'Hieuman', 'type': 'user'}, 'name': 'Hieu Man', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-06T08:35:03.741Z', 'hidden': False}, {'_id': '677b6abf6b1653e3951b4cbe', 'user': {'_id': '658a8208c4b2004663d82daf', 'avatarUrl': '/avatars/af6bcee06aec82602c2b931f79c008e7.svg', 'isPro': False, 'fullname': 'Nghia Trung Ngo', 'user': 'ntnghia1811', 'type': 'user'}, 'name': 'Nghia Trung Ngo', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-06T08:35:09.619Z', 'hidden': False}, {'_id': '677b6abf6b1653e3951b4cbf', 'name': 'Viet Dac Lai', 'hidden': False}, {'_id': '677b6abf6b1653e3951b4cc0', 'user': {'_id': '62a3ab83e4dd6252344d27cd', 'avatarUrl': '/avatars/7ca8510f70a58dc207b104240e30c35c.svg', 'isPro': False, 'fullname': 'Ryan A. Rossi', 'user': 'ryanrossi', 'type': 'user'}, 'name': 'Ryan A. Rossi', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-06T08:35:19.050Z', 'hidden': False}, {'_id': '677b6abf6b1653e3951b4cc1', 'user': {'_id': '62c5947524171688a9feb992', 'avatarUrl': '/avatars/5a151713b9eae8dc566f5957acee3475.svg', 'isPro': False, 'fullname': 'Franck Dernoncourt', 'user': 'Franck-Dernoncourt', 'type': 'user'}, 'name': 'Franck Dernoncourt', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-06T08:35:26.065Z', 'hidden': False}, {'_id': '677b6abf6b1653e3951b4cc2', 'user': {'_id': '64804fad8c6a3b8f11f73912', 'avatarUrl': '/avatars/61e37a91d4bba35fda9bf52aadd87745.svg', 'isPro': False, 'fullname': 'Thien Huu Nguyen', 'user': 'anoperson', 'type': 'user'}, 'name': 'Thien Huu Nguyen', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-06T08:35:31.396Z', 'hidden': False}], 'publishedAt': '2025-01-01T15:43:07.000Z', 'title': 'LUSIFER: Language Universal Space Integration for Enhanced Multilingual\\n  Embeddings with Large Language Models', 'summary': \"Recent advancements in large language models (LLMs) based embedding models\\nhave established new state-of-the-art benchmarks for text embedding tasks,\\nparticularly in dense vector-based retrieval. However, these models\\npredominantly focus on English, leaving multilingual embedding capabilities\\nlargely unexplored. To address this limitation, we present LUSIFER, a novel\\nzero-shot approach that adapts LLM-based embedding models for multilingual\\ntasks without requiring multilingual supervision. LUSIFER's architecture\\ncombines a multilingual encoder, serving as a language-universal learner, with\\nan LLM-based embedding model optimized for embedding-specific tasks. These\\ncomponents are seamlessly integrated through a minimal set of trainable\\nparameters that act as a connector, effectively transferring the multilingual\\nencoder's language understanding capabilities to the specialized embedding\\nmodel. Additionally, to comprehensively evaluate multilingual embedding\\nperformance, we introduce a new benchmark encompassing 5 primary embedding\\ntasks, 123 diverse datasets, and coverage across 14 languages. Extensive\\nexperimental results demonstrate that LUSIFER significantly enhances the\\nmultilingual performance across various embedding tasks, particularly for\\nmedium and low-resource languages, without requiring explicit multilingual\\ntraining data.\", 'upvotes': 7, 'discussionId': '677b6ac06b1653e3951b4cf9'}, 'publishedAt': '2025-01-06T00:31:45.838Z', 'title': 'LUSIFER: Language Universal Space Integration for Enhanced Multilingual Embeddings with Large Language Models', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.00874.png', 'numComments': 1, 'submittedBy': {'_id': '62c5947524171688a9feb992', 'avatarUrl': '/avatars/5a151713b9eae8dc566f5957acee3475.svg', 'fullname': 'Franck Dernoncourt', 'name': 'Franck-Dernoncourt', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 4}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2501.01540', 'authors': [{'_id': '677b9534c57d565bd3535ff2', 'user': {'_id': '63e6a880f2e9a8f22c5a1630', 'avatarUrl': '/avatars/53b57690fe052ce6882bbfc87b11567c.svg', 'isPro': False, 'fullname': 'Kanishk Gandhi', 'user': 'obiwan96', 'type': 'user'}, 'name': 'Kanishk Gandhi', 'status': 'extracted_confirmed', 'statusLastChangedAt': '2025-01-06T08:33:22.650Z', 'hidden': False}, {'_id': '677b9534c57d565bd3535ff3', 'user': {'_id': '667357d37376c7227771c14c', 'avatarUrl': '/avatars/77253b6f8b1eb97f9ab09097e83436ed.svg', 'isPro': False, 'fullname': 'Michael Y Li', 'user': 'michaelyli', 'type': 'user'}, 'name': 'Michael Y. Li', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-06T08:36:16.321Z', 'hidden': False}, {'_id': '677b9534c57d565bd3535ff4', 'name': 'Lyle Goodyear', 'hidden': False}, {'_id': '677b9534c57d565bd3535ff5', 'name': 'Louise Li', 'hidden': False}, {'_id': '677b9534c57d565bd3535ff6', 'name': 'Aditi Bhaskar', 'hidden': False}, {'_id': '677b9534c57d565bd3535ff7', 'name': 'Mohammed Zaman', 'hidden': False}, {'_id': '677b9534c57d565bd3535ff8', 'user': {'_id': '67321274c1f20c742bcf7a8d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/ltcQhre6eDRVzn6Vbbyhu.png', 'isPro': False, 'fullname': 'Noah D. Goodman', 'user': 'ngoodman', 'type': 'user'}, 'name': 'Noah D. Goodman', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-06T08:36:39.125Z', 'hidden': False}], 'publishedAt': '2025-01-02T21:15:57.000Z', 'title': 'BoxingGym: Benchmarking Progress in Automated Experimental Design and\\n  Model Discovery', 'summary': \"Understanding the world and explaining it with scientific theories is a\\ncentral aspiration of artificial intelligence research. Proposing theories,\\ndesigning experiments to test them, and then revising them based on data are\\nfundamental to scientific discovery. Despite the significant promise of\\nLLM-based scientific agents, no benchmarks systematically test LLM's ability to\\npropose scientific models, collect experimental data, and revise them in light\\nof new data. We introduce BoxingGym, a benchmark with 10 environments for\\nsystematically evaluating both experimental design (e.g. collecting data to\\ntest a scientific theory) and model discovery (e.g. proposing and revising\\nscientific theories). To enable tractable and quantitative evaluation, we\\nimplement each environment as a generative probabilistic model with which a\\nscientific agent can run interactive experiments. These probabilistic models\\nare drawn from various real-world scientific domains ranging from psychology to\\necology. To quantitatively evaluate a scientific agent's ability to collect\\ninformative experimental data, we compute the expected information gain (EIG),\\nan information-theoretic quantity which measures how much an experiment reduces\\nuncertainty about the parameters of a generative model. A good scientific\\ntheory is a concise and predictive explanation. Therefore, to quantitatively\\nevaluate model discovery, we ask a scientific agent to explain their model and\\nthen assess whether this explanation enables another scientific agent to make\\nreliable predictions about this environment. In addition to this\\nexplanation-based evaluation, we compute standard model evaluation metrics such\\nas prediction errors. We find that current LLMs, such as GPT-4o, struggle with\\nboth experimental design and model discovery. We find that augmenting the\\nLLM-based agent with an explicit statistical model does not reliably improve\\nthese results.\", 'upvotes': 4, 'discussionId': '677b9535c57d565bd353603c'}, 'publishedAt': '2025-01-06T03:34:33.331Z', 'title': 'BoxingGym: Benchmarking Progress in Automated Experimental Design and Model Discovery', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.01540.png', 'numComments': 1, 'submittedBy': {'_id': '63e6a880f2e9a8f22c5a1630', 'avatarUrl': '/avatars/53b57690fe052ce6882bbfc87b11567c.svg', 'fullname': 'Kanishk Gandhi', 'name': 'obiwan96', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}, 'isAuthorParticipating': True}"
]