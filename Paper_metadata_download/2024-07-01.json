[
    {
        "paper": {
            "id": "2406.19280",
            "authors": [
                {
                    "_id": "667e437a3a685cd30f9c0f84",
                    "user": {
                        "avatarUrl": "/avatars/18d036aab5e096054a8706bc78027126.svg",
                        "isPro": false,
                        "fullname": "Junying Chen",
                        "user": "jymcc",
                        "type": "user"
                    },
                    "name": "Junying Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-30T20:57:35.571Z",
                    "hidden": false
                },
                {
                    "_id": "667e437a3a685cd30f9c0f85",
                    "user": {
                        "avatarUrl": "/avatars/3ae69531fb03e522647e134688459fe7.svg",
                        "isPro": false,
                        "fullname": "Ruyi Ouyang",
                        "user": "OBB1028",
                        "type": "user"
                    },
                    "name": "Ruyi Ouyang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-01T07:56:39.424Z",
                    "hidden": false
                },
                {
                    "_id": "667e437a3a685cd30f9c0f86",
                    "name": "Anningzhe Gao",
                    "hidden": false
                },
                {
                    "_id": "667e437a3a685cd30f9c0f87",
                    "user": {
                        "avatarUrl": "/avatars/b6521b795a59754dbb40123fd4f63b8c.svg",
                        "isPro": false,
                        "fullname": "Shunian Chen",
                        "user": "Shunian",
                        "type": "user"
                    },
                    "name": "Shunian Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-01T07:56:56.176Z",
                    "hidden": false
                },
                {
                    "_id": "667e437a3a685cd30f9c0f88",
                    "name": "Guiming Hardy Chen",
                    "hidden": false
                },
                {
                    "_id": "667e437a3a685cd30f9c0f89",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678693486092-640ed3e9f2d7c41a1e9a9fde.jpeg",
                        "isPro": false,
                        "fullname": "Xidong Wang",
                        "user": "Xidong",
                        "type": "user"
                    },
                    "name": "Xidong Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-01T07:57:32.221Z",
                    "hidden": false
                },
                {
                    "_id": "667e437a3a685cd30f9c0f8a",
                    "user": {
                        "avatarUrl": "/avatars/5acb8548cfbc1a46e0f2c59094e1c30f.svg",
                        "isPro": false,
                        "fullname": "Ruifei Zhang",
                        "user": "ReaFly",
                        "type": "user"
                    },
                    "name": "Ruifei Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-01T07:57:38.854Z",
                    "hidden": false
                },
                {
                    "_id": "667e437a3a685cd30f9c0f8b",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f1a34f2c5c8b767916447e/uak2CsMAnxW8q4dwyAOBN.jpeg",
                        "isPro": false,
                        "fullname": "Zhenyang Cai",
                        "user": "Eric3200",
                        "type": "user"
                    },
                    "name": "Zhenyang Cai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-01T07:57:45.400Z",
                    "hidden": false
                },
                {
                    "_id": "667e437a3a685cd30f9c0f8c",
                    "user": {
                        "avatarUrl": "/avatars/522f0a499a0be81831814260e2159db9.svg",
                        "isPro": false,
                        "fullname": "Ke Ji",
                        "user": "kehorizon",
                        "type": "user"
                    },
                    "name": "Ke Ji",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-07-01T09:45:22.865Z",
                    "hidden": false
                },
                {
                    "_id": "667e437a3a685cd30f9c0f8d",
                    "name": "Guangjun Yu",
                    "hidden": false
                },
                {
                    "_id": "667e437a3a685cd30f9c0f8e",
                    "name": "Xiang Wan",
                    "hidden": false
                },
                {
                    "_id": "667e437a3a685cd30f9c0f8f",
                    "user": {
                        "avatarUrl": "/avatars/288ed63a1efa566c3f01e850c6ba5dd5.svg",
                        "isPro": false,
                        "fullname": "Wang",
                        "user": "Benyou",
                        "type": "user"
                    },
                    "name": "Benyou Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-01T07:58:18.395Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-27T15:50:41.000Z",
            "title": "HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into\n  Multimodal LLMs at Scale",
            "summary": "The rapid development of multimodal large language models (MLLMs), such as\nGPT-4V, has led to significant advancements. However, these models still face\nchallenges in medical multimodal capabilities due to limitations in the\nquantity and quality of medical vision-text data, stemming from data privacy\nconcerns and high annotation costs. While pioneering approaches utilize\nPubMed's large-scale, de-identified medical image-text pairs to address these\nlimitations, they still fall short due to inherent data noise. To tackle this,\nwe refined medical image-text pairs from PubMed and employed MLLMs (GPT-4V) in\nan 'unblinded' capacity to denoise and reformat the data, resulting in the\ncreation of the PubMedVision dataset with 1.3 million medical VQA samples. Our\nvalidation demonstrates that: (1) PubMedVision can significantly enhance the\nmedical multimodal capabilities of current MLLMs, showing significant\nimprovement in benchmarks including the MMMU Health & Medicine track; (2)\nmanual checks by medical experts and empirical results validate the superior\ndata quality of our dataset compared to other data construction methods. Using\nPubMedVision, we train a 34B medical MLLM HuatuoGPT-Vision, which shows\nsuperior performance in medical multimodal scenarios among open-source MLLMs.",
            "upvotes": 45
        },
        "publishedAt": "2024-07-01T01:37:17.752Z",
        "title": "HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.19280.png",
        "numComments": 7,
        "submittedBy": {
            "avatarUrl": "/avatars/18d036aab5e096054a8706bc78027126.svg",
            "fullname": "Junying Chen",
            "name": "jymcc",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.20094",
            "authors": [
                {
                    "_id": "6681fd246bc332049a92efcb",
                    "name": "Xin Chan",
                    "hidden": false
                },
                {
                    "_id": "6681fd246bc332049a92efcc",
                    "user": {
                        "avatarUrl": "/avatars/c7c984ae483144fab627aa2c54d91d0f.svg",
                        "isPro": false,
                        "fullname": "Xiaoyang Wang",
                        "user": "xywang1",
                        "type": "user"
                    },
                    "name": "Xiaoyang Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-07-01T07:46:55.022Z",
                    "hidden": false
                },
                {
                    "_id": "6681fd246bc332049a92efcd",
                    "name": "Dian Yu",
                    "hidden": false
                },
                {
                    "_id": "6681fd246bc332049a92efce",
                    "user": {
                        "avatarUrl": "/avatars/86574ee2d5c22e940be1c4e50be88675.svg",
                        "isPro": false,
                        "fullname": "Haitao Mi",
                        "user": "haitaominlp",
                        "type": "user"
                    },
                    "name": "Haitao Mi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-01T07:49:08.422Z",
                    "hidden": false
                },
                {
                    "_id": "6681fd246bc332049a92efcf",
                    "name": "Dong Yu",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-28T17:59:01.000Z",
            "title": "Scaling Synthetic Data Creation with 1,000,000,000 Personas",
            "summary": "We propose a novel persona-driven data synthesis methodology that leverages\nvarious perspectives within a large language model (LLM) to create diverse\nsynthetic data. To fully exploit this methodology at scale, we introduce\nPersona Hub -- a collection of 1 billion diverse personas automatically curated\nfrom web data. These 1 billion personas (~13% of the world's total population),\nacting as distributed carriers of world knowledge, can tap into almost every\nperspective encapsulated within the LLM, thereby facilitating the creation of\ndiverse synthetic data at scale for various scenarios. By showcasing Persona\nHub's use cases in synthesizing high-quality mathematical and logical reasoning\nproblems, instructions (i.e., user prompts), knowledge-rich texts, game NPCs\nand tools (functions) at scale, we demonstrate persona-driven data synthesis is\nversatile, scalable, flexible, and easy to use, potentially driving a paradigm\nshift in synthetic data creation and applications in practice, which may have a\nprofound impact on LLM research and development.",
            "upvotes": 39
        },
        "publishedAt": "2024-07-01T01:00:23.761Z",
        "title": "Scaling Synthetic Data Creation with 1,000,000,000 Personas",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.20094.png",
        "numComments": 4,
        "submittedBy": {
            "avatarUrl": "/avatars/c7c984ae483144fab627aa2c54d91d0f.svg",
            "fullname": "Xiaoyang Wang",
            "name": "xywang1",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.20095",
            "authors": [
                {
                    "_id": "668233727b50b433cdfc100a",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6369c6b17f319ce3573771ae/g7zVTXWBeeOBQqR5mOByS.jpeg",
                        "isPro": false,
                        "fullname": "Xiang Li",
                        "user": "variante",
                        "type": "user"
                    },
                    "name": "Xiang Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-07-01T07:46:52.958Z",
                    "hidden": false
                },
                {
                    "_id": "668233727b50b433cdfc100b",
                    "user": {
                        "avatarUrl": "/avatars/e69ebb82df9f193db388d841d1fc996c.svg",
                        "isPro": false,
                        "fullname": "Cristina Mata",
                        "user": "cfmata",
                        "type": "user"
                    },
                    "name": "Cristina Mata",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-01T09:20:50.776Z",
                    "hidden": false
                },
                {
                    "_id": "668233727b50b433cdfc100c",
                    "user": {
                        "avatarUrl": "/avatars/2a8f4cf26cdedb4fa5f715f0d14d1bbf.svg",
                        "isPro": false,
                        "fullname": "Jongwoo Park",
                        "user": "jongwoopark7978",
                        "type": "user"
                    },
                    "name": "Jongwoo Park",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-01T09:21:47.581Z",
                    "hidden": false
                },
                {
                    "_id": "668233727b50b433cdfc100d",
                    "user": {
                        "avatarUrl": "/avatars/ce4d46f575ba757f78eabdb25b394171.svg",
                        "isPro": false,
                        "fullname": "Kumara Kahatapitiya",
                        "user": "kumarak",
                        "type": "user"
                    },
                    "name": "Kumara Kahatapitiya",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-01T09:22:13.652Z",
                    "hidden": false
                },
                {
                    "_id": "668233727b50b433cdfc100e",
                    "user": {
                        "avatarUrl": "/avatars/302f0adca572a139023221ee6476e22f.svg",
                        "isPro": false,
                        "fullname": "Yoo Sung Jang",
                        "user": "yjang43",
                        "type": "user"
                    },
                    "name": "Yoo Sung Jang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-01T09:22:22.236Z",
                    "hidden": false
                },
                {
                    "_id": "668233727b50b433cdfc100f",
                    "user": {
                        "avatarUrl": "/avatars/b553823be640a07eeb4c7edc2c176d5d.svg",
                        "isPro": false,
                        "fullname": "Jinghuan Shang",
                        "user": "Jinghuan",
                        "type": "user"
                    },
                    "name": "Jinghuan Shang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-01T09:22:48.532Z",
                    "hidden": false
                },
                {
                    "_id": "668233727b50b433cdfc1010",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678925215042-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Kanchana Ranasinghe",
                        "user": "kahnchana",
                        "type": "user"
                    },
                    "name": "Kanchana Ranasinghe",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-01T09:22:55.586Z",
                    "hidden": false
                },
                {
                    "_id": "668233727b50b433cdfc1011",
                    "user": {
                        "avatarUrl": "/avatars/652089c241aa3953d3e7b96142f6c538.svg",
                        "isPro": false,
                        "fullname": "Ryan Burgert",
                        "user": "OneOverZero",
                        "type": "user"
                    },
                    "name": "Ryan Burgert",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-01T09:23:01.806Z",
                    "hidden": false
                },
                {
                    "_id": "668233727b50b433cdfc1012",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b7b2c6bd2d153522821766/aHtga-_OUdOrg_TRrXO08.jpeg",
                        "isPro": false,
                        "fullname": "Mu Cai",
                        "user": "mucai",
                        "type": "user"
                    },
                    "name": "Mu Cai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-01T09:23:27.185Z",
                    "hidden": false
                },
                {
                    "_id": "668233727b50b433cdfc1013",
                    "user": {
                        "avatarUrl": "/avatars/1c9a76717a450ac4aeb62a1e823d2e4a.svg",
                        "isPro": false,
                        "fullname": "Yong Jae Lee",
                        "user": "yjlee0222",
                        "type": "user"
                    },
                    "name": "Yong Jae Lee",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-01T09:23:33.168Z",
                    "hidden": false
                },
                {
                    "_id": "668233727b50b433cdfc1014",
                    "name": "Michael S. Ryoo",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-28T17:59:12.000Z",
            "title": "LLaRA: Supercharging Robot Learning Data for Vision-Language Policy",
            "summary": "Large Language Models (LLMs) equipped with extensive world knowledge and\nstrong reasoning skills can tackle diverse tasks across domains, often by\nposing them as conversation-style instruction-response pairs. In this paper, we\npropose LLaRA: Large Language and Robotics Assistant, a framework which\nformulates robot action policy as conversations, and provides improved\nresponses when trained with auxiliary data that complements policy learning.\nLLMs with visual inputs, i.e., Vision Language Models (VLMs), have the capacity\nto process state information as visual-textual prompts and generate optimal\npolicy decisions in text. To train such action policy VLMs, we first introduce\nan automated pipeline to generate diverse high-quality robotics instruction\ndata from existing behavior cloning data. A VLM finetuned with the resulting\ncollection of datasets based on a conversation-style formulation tailored for\nrobotics tasks, can generate meaningful robot action policy decisions. Our\nexperiments across multiple simulated and real-world environments demonstrate\nthe state-of-the-art performance of the proposed LLaRA framework. The code,\ndatasets, and pretrained models are available at\nhttps://github.com/LostXine/LLaRA.",
            "upvotes": 10
        },
        "publishedAt": "2024-07-01T06:28:53.225Z",
        "title": "LLaRA: Supercharging Robot Learning Data for Vision-Language Policy",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6369c6b17f319ce3573771ae/HEG16harPfXMjVwTH2oSj.gif",
            "https://cdn-uploads.huggingface.co/production/uploads/6369c6b17f319ce3573771ae/fiMzoFWASzgiDcBIE6xEk.gif",
            "https://cdn-uploads.huggingface.co/production/uploads/6369c6b17f319ce3573771ae/kn48BxpAfTAP4ugFW4Zxz.png",
            "https://cdn-uploads.huggingface.co/production/uploads/6369c6b17f319ce3573771ae/Ara5akHv_P9ogZSZTZ8B3.png",
            "https://cdn-uploads.huggingface.co/production/uploads/6369c6b17f319ce3573771ae/rME6-5KX6b8b4URJp4atj.png",
            "https://cdn-uploads.huggingface.co/production/uploads/6369c6b17f319ce3573771ae/eFW3oUVk8BCabX0Yxmdgb.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.20095.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6369c6b17f319ce3573771ae/g7zVTXWBeeOBQqR5mOByS.jpeg",
            "fullname": "Xiang Li",
            "name": "variante",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.19774",
            "authors": [
                {
                    "_id": "6682377ad892ee6886aab6f6",
                    "name": "Yixing Li",
                    "hidden": false
                },
                {
                    "_id": "6682377ad892ee6886aab6f7",
                    "user": {
                        "avatarUrl": "/avatars/8de6e319246500c460cf41163462c214.svg",
                        "isPro": false,
                        "fullname": "Yuxian Gu",
                        "user": "t1101675",
                        "type": "user"
                    },
                    "name": "Yuxian Gu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-01T09:00:21.943Z",
                    "hidden": false
                },
                {
                    "_id": "6682377ad892ee6886aab6f8",
                    "user": {
                        "avatarUrl": "/avatars/2331cf703c1b5d3a62e2050b1a6eb108.svg",
                        "isPro": false,
                        "fullname": "UniLM",
                        "user": "unilm",
                        "type": "user"
                    },
                    "name": "Li Dong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-07-01T07:46:49.916Z",
                    "hidden": false
                },
                {
                    "_id": "6682377ad892ee6886aab6f9",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1638737010975-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Dequan Wang",
                        "user": "dqwang",
                        "type": "user"
                    },
                    "name": "Dequan Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-01T09:01:03.279Z",
                    "hidden": false
                },
                {
                    "_id": "6682377ad892ee6886aab6fa",
                    "name": "Yu Cheng",
                    "hidden": false
                },
                {
                    "_id": "6682377ad892ee6886aab6fb",
                    "user": {
                        "avatarUrl": "/avatars/1c23bc7c0b6d9225699ce27647623d7a.svg",
                        "isPro": false,
                        "fullname": "Furu Wei",
                        "user": "thegenerality",
                        "type": "user"
                    },
                    "name": "Furu Wei",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-01T09:09:15.680Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-28T09:23:40.000Z",
            "title": "Direct Preference Knowledge Distillation for Large Language Models",
            "summary": "In the field of large language models (LLMs), Knowledge Distillation (KD) is\na critical technique for transferring capabilities from teacher models to\nstudent models. However, existing KD methods face limitations and challenges in\ndistillation of LLMs, including efficiency and insufficient measurement\ncapabilities of traditional KL divergence. It is shown that LLMs can serve as\nan implicit reward function, which we define as a supplement to KL divergence.\nIn this work, we propose Direct Preference Knowledge Distillation (DPKD) for\nLLMs. DPKD utilizes distribution divergence to represent the preference loss\nand implicit reward function. We re-formulate KD of LLMs into two stages: first\noptimizing and objective consisting of implicit reward and reverse KL\ndivergence and then improving the preference probability of teacher outputs\nover student outputs. We conducted experiments and analysis on various datasets\nwith LLM parameters ranging from 120M to 13B and demonstrate the broad\napplicability and effectiveness of our DPKD approach. Meanwhile, we prove the\nvalue and effectiveness of the introduced implicit reward and output preference\nin KD through experiments and theoretical analysis. The DPKD method outperforms\nthe baseline method in both output response precision and exact match\npercentage. Code and data are available at https://aka.ms/dpkd.",
            "upvotes": 9
        },
        "publishedAt": "2024-07-01T03:36:49.975Z",
        "title": "Direct Preference Knowledge Distillation for Large Language Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.19774.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/2331cf703c1b5d3a62e2050b1a6eb108.svg",
            "fullname": "UniLM",
            "name": "unilm",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.18462",
            "authors": [
                {
                    "_id": "667e60c9b0f62e5e077f41d4",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e5ee4b93d04e3439f4e988/g-THJvunoSPUyQ2v_fChU.jpeg",
                        "isPro": false,
                        "fullname": "taoranyi",
                        "user": "thewhole",
                        "type": "user"
                    },
                    "name": "Taoran Yi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-07-01T07:46:59.957Z",
                    "hidden": false
                },
                {
                    "_id": "667e60c9b0f62e5e077f41d5",
                    "user": {
                        "avatarUrl": "/avatars/9ccffa04832429aad37320a301ea8e36.svg",
                        "isPro": false,
                        "fullname": "Jiemin Fang",
                        "user": "JieminFang",
                        "type": "user"
                    },
                    "name": "Jiemin Fang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-01T09:31:05.128Z",
                    "hidden": false
                },
                {
                    "_id": "667e60c9b0f62e5e077f41d6",
                    "user": {
                        "avatarUrl": "/avatars/ad28bc90a3820265501b95bfa5bb9418.svg",
                        "isPro": false,
                        "fullname": "Zhou",
                        "user": "Zanwei",
                        "type": "user"
                    },
                    "name": "Zanwei Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-01T09:31:13.566Z",
                    "hidden": false
                },
                {
                    "_id": "667e60c9b0f62e5e077f41d7",
                    "name": "Junjie Wang",
                    "hidden": false
                },
                {
                    "_id": "667e60c9b0f62e5e077f41d8",
                    "user": {
                        "avatarUrl": "/avatars/09dc58dbbce5684e9edcb813bf078cc9.svg",
                        "isPro": false,
                        "fullname": "Guanjun Wu",
                        "user": "SnowGeraltOfRivia",
                        "type": "user"
                    },
                    "name": "Guanjun Wu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-01T09:31:38.299Z",
                    "hidden": false
                },
                {
                    "_id": "667e60c9b0f62e5e077f41d9",
                    "name": "Lingxi Xie",
                    "hidden": false
                },
                {
                    "_id": "667e60c9b0f62e5e077f41da",
                    "user": {
                        "avatarUrl": "/avatars/ae460dd971de1d04660b266b22f81f9d.svg",
                        "isPro": false,
                        "fullname": "Xiaopeng Zhang",
                        "user": "xpzhang",
                        "type": "user"
                    },
                    "name": "Xiaopeng Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-01T09:31:52.464Z",
                    "hidden": false
                },
                {
                    "_id": "667e60c9b0f62e5e077f41db",
                    "name": "Wenyu Liu",
                    "hidden": false
                },
                {
                    "_id": "667e60c9b0f62e5e077f41dc",
                    "user": {
                        "avatarUrl": "/avatars/a536417cfec6e10ac415091bd1829426.svg",
                        "isPro": false,
                        "fullname": "Xinggang Wang",
                        "user": "xinggangw",
                        "type": "user"
                    },
                    "name": "Xinggang Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-01T09:32:34.158Z",
                    "hidden": false
                },
                {
                    "_id": "667e60c9b0f62e5e077f41dd",
                    "name": "Qi Tian",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-26T16:12:09.000Z",
            "title": "GaussianDreamerPro: Text to Manipulable 3D Gaussians with Highly\n  Enhanced Quality",
            "summary": "Recently, 3D Gaussian splatting (3D-GS) has achieved great success in\nreconstructing and rendering real-world scenes. To transfer the high rendering\nquality to generation tasks, a series of research works attempt to generate\n3D-Gaussian assets from text. However, the generated assets have not achieved\nthe same quality as those in reconstruction tasks. We observe that Gaussians\ntend to grow without control as the generation process may cause indeterminacy.\nAiming at highly enhancing the generation quality, we propose a novel framework\nnamed GaussianDreamerPro. The main idea is to bind Gaussians to reasonable\ngeometry, which evolves over the whole generation process. Along different\nstages of our framework, both the geometry and appearance can be enriched\nprogressively. The final output asset is constructed with 3D Gaussians bound to\nmesh, which shows significantly enhanced details and quality compared with\nprevious methods. Notably, the generated asset can also be seamlessly\nintegrated into downstream manipulation pipelines, e.g. animation, composition,\nand simulation etc., greatly promoting its potential in wide applications.\nDemos are available at https://taoranyi.com/gaussiandreamerpro/.",
            "upvotes": 7
        },
        "publishedAt": "2024-07-01T06:05:37.849Z",
        "title": "GaussianDreamerPro: Text to Manipulable 3D Gaussians with Highly Enhanced Quality",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.18462.png",
        "numComments": 3,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e5ee4b93d04e3439f4e988/g-THJvunoSPUyQ2v_fChU.jpeg",
            "fullname": "taoranyi",
            "name": "thewhole",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.20076",
            "authors": [
                {
                    "_id": "66824779d892ee6886af5da2",
                    "name": "Yuxuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "66824779d892ee6886af5da3",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646b3db131968a60a01e4cf5/DhfdqUYQaD1Qa8Svw996J.jpeg",
                        "isPro": false,
                        "fullname": "Tianheng Cheng",
                        "user": "wondervictor",
                        "type": "user"
                    },
                    "name": "Tianheng Cheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-07-01T07:46:29.538Z",
                    "hidden": false
                },
                {
                    "_id": "66824779d892ee6886af5da4",
                    "user": {
                        "avatarUrl": "/avatars/7fd2e7a5ccbbf25643e349682d2f655f.svg",
                        "isPro": false,
                        "fullname": "Rui Hu",
                        "user": "RuiHu",
                        "type": "user"
                    },
                    "name": "Rui Hu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-01T09:43:04.536Z",
                    "hidden": false
                },
                {
                    "_id": "66824779d892ee6886af5da5",
                    "name": "ei Liu",
                    "hidden": false
                },
                {
                    "_id": "66824779d892ee6886af5da6",
                    "name": "Heng Liu",
                    "hidden": false
                },
                {
                    "_id": "66824779d892ee6886af5da7",
                    "name": "Longjin Ran",
                    "hidden": false
                },
                {
                    "_id": "66824779d892ee6886af5da8",
                    "user": {
                        "avatarUrl": "/avatars/2fa3828ca489cfe1948129a0eccf264f.svg",
                        "isPro": false,
                        "fullname": "chenxiaoxin",
                        "user": "steelozazala",
                        "type": "user"
                    },
                    "name": "Xiaoxin Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-01T09:42:39.544Z",
                    "hidden": false
                },
                {
                    "_id": "66824779d892ee6886af5da9",
                    "name": "Wenyu Liu",
                    "hidden": false
                },
                {
                    "_id": "66824779d892ee6886af5daa",
                    "user": {
                        "avatarUrl": "/avatars/a536417cfec6e10ac415091bd1829426.svg",
                        "isPro": false,
                        "fullname": "Xinggang Wang",
                        "user": "xinggangw",
                        "type": "user"
                    },
                    "name": "Xinggang Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-01T09:42:04.985Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-28T17:38:18.000Z",
            "title": "EVF-SAM: Early Vision-Language Fusion for Text-Prompted Segment Anything\n  Model",
            "summary": "Segment Anything Model (SAM) has attracted widespread attention for its\nsuperior interactive segmentation capabilities with visual prompts while\nlacking further exploration of text prompts. In this paper, we empirically\ninvestigate what text prompt encoders (e.g., CLIP or LLM) are good for adapting\nSAM for referring expression segmentation and introduce the Early\nVision-language Fusion-based SAM (EVF-SAM). EVF-SAM is a simple yet effective\nreferring segmentation method which exploits multimodal prompts (i.e., image\nand text) and comprises a pre-trained vision-language model to generate\nreferring prompts and a SAM model for segmentation. Surprisingly, we observe\nthat: (1) multimodal prompts and (2) vision-language models with early fusion\n(e.g., BEIT-3) are beneficial for prompting SAM for accurate referring\nsegmentation. Our experiments show that the proposed EVF-SAM based on BEIT-3\ncan obtain state-of-the-art performance on RefCOCO/+/g for referring expression\nsegmentation and demonstrate the superiority of prompting SAM with early\nvision-language fusion. In addition, the proposed EVF-SAM with 1.32B parameters\nachieves remarkably higher performance while reducing nearly 82% of parameters\ncompared to previous SAM methods based on large multimodal models.",
            "upvotes": 6
        },
        "publishedAt": "2024-07-01T04:38:24.279Z",
        "title": "EVF-SAM: Early Vision-Language Fusion for Text-Prompted Segment Anything Model",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.20076.png",
        "numComments": 3,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646b3db131968a60a01e4cf5/DhfdqUYQaD1Qa8Svw996J.jpeg",
            "fullname": "Tianheng Cheng",
            "name": "wondervictor",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.19251",
            "authors": [
                {
                    "_id": "668234bb7ad6fe37b198f6a8",
                    "name": "Jia Fu",
                    "hidden": false
                },
                {
                    "_id": "668234bb7ad6fe37b198f6a9",
                    "user": {
                        "avatarUrl": "/avatars/16dd4d945e9fbef5ac889a8087101ded.svg",
                        "isPro": false,
                        "fullname": "Xiaoting Qin",
                        "user": "XiaotingQin",
                        "type": "user"
                    },
                    "name": "Xiaoting Qin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-01T09:14:16.094Z",
                    "hidden": false
                },
                {
                    "_id": "668234bb7ad6fe37b198f6aa",
                    "name": "Fangkai Yang",
                    "hidden": false
                },
                {
                    "_id": "668234bb7ad6fe37b198f6ab",
                    "name": "Lu Wang",
                    "hidden": false
                },
                {
                    "_id": "668234bb7ad6fe37b198f6ac",
                    "user": {
                        "avatarUrl": "/avatars/a42ac5454cbe175f04c3420fce90cad2.svg",
                        "isPro": false,
                        "fullname": "Jue Zhang",
                        "user": "JueZhang",
                        "type": "user"
                    },
                    "name": "Jue Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-01T09:10:40.195Z",
                    "hidden": false
                },
                {
                    "_id": "668234bb7ad6fe37b198f6ad",
                    "name": "Qingwei Lin",
                    "hidden": false
                },
                {
                    "_id": "668234bb7ad6fe37b198f6ae",
                    "user": {
                        "avatarUrl": "/avatars/6b41c4eb817836d50d0072009f124f29.svg",
                        "isPro": false,
                        "fullname": "Yubo Chen",
                        "user": "ybch14",
                        "type": "user"
                    },
                    "name": "Yubo Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-01T09:14:47.607Z",
                    "hidden": false
                },
                {
                    "_id": "668234bb7ad6fe37b198f6af",
                    "name": "Dongmei Zhang",
                    "hidden": false
                },
                {
                    "_id": "668234bb7ad6fe37b198f6b0",
                    "name": "Saravan Rajmohan",
                    "hidden": false
                },
                {
                    "_id": "668234bb7ad6fe37b198f6b1",
                    "name": "Qi Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-27T15:18:21.000Z",
            "title": "AutoRAG-HP: Automatic Online Hyper-Parameter Tuning for\n  Retrieval-Augmented Generation",
            "summary": "Recent advancements in Large Language Models have transformed ML/AI\ndevelopment, necessitating a reevaluation of AutoML principles for the\nRetrieval-Augmented Generation (RAG) systems. To address the challenges of\nhyper-parameter optimization and online adaptation in RAG, we propose the\nAutoRAG-HP framework, which formulates the hyper-parameter tuning as an online\nmulti-armed bandit (MAB) problem and introduces a novel two-level Hierarchical\nMAB (Hier-MAB) method for efficient exploration of large search spaces. We\nconduct extensive experiments on tuning hyper-parameters, such as top-k\nretrieved documents, prompt compression ratio, and embedding methods, using the\nALCE-ASQA and Natural Questions datasets. Our evaluation from jointly\noptimization all three hyper-parameters demonstrate that MAB-based online\nlearning methods can achieve Recall@5 approx 0.8 for scenarios with\nprominent gradients in search space, using only sim20% of the LLM API calls\nrequired by the Grid Search approach. Additionally, the proposed Hier-MAB\napproach outperforms other baselines in more challenging optimization\nscenarios. The code will be made available at https://aka.ms/autorag.",
            "upvotes": 5
        },
        "publishedAt": "2024-07-01T03:19:01.622Z",
        "title": "AutoRAG-HP: Automatic Online Hyper-Parameter Tuning for Retrieval-Augmented Generation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.19251.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/a42ac5454cbe175f04c3420fce90cad2.svg",
            "fullname": "Jue Zhang",
            "name": "JueZhang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.17720",
            "authors": [
                {
                    "_id": "667c91f07f75474515f7f3f3",
                    "name": "Chih-Hsuan Yang",
                    "hidden": false
                },
                {
                    "_id": "667c91f07f75474515f7f3f4",
                    "name": "Benjamin Feuer",
                    "hidden": false
                },
                {
                    "_id": "667c91f07f75474515f7f3f5",
                    "name": "Zaki Jubery",
                    "hidden": false
                },
                {
                    "_id": "667c91f07f75474515f7f3f6",
                    "name": "Zi K. Deng",
                    "hidden": false
                },
                {
                    "_id": "667c91f07f75474515f7f3f7",
                    "name": "Andre Nakkab",
                    "hidden": false
                },
                {
                    "_id": "667c91f07f75474515f7f3f8",
                    "name": "Md Zahid Hasan",
                    "hidden": false
                },
                {
                    "_id": "667c91f07f75474515f7f3f9",
                    "name": "Shivani Chiranjeevi",
                    "hidden": false
                },
                {
                    "_id": "667c91f07f75474515f7f3fa",
                    "name": "Kelly Marshall",
                    "hidden": false
                },
                {
                    "_id": "667c91f07f75474515f7f3fb",
                    "name": "Nirmal Baishnab",
                    "hidden": false
                },
                {
                    "_id": "667c91f07f75474515f7f3fc",
                    "name": "Asheesh K Singh",
                    "hidden": false
                },
                {
                    "_id": "667c91f07f75474515f7f3fd",
                    "name": "Arti Singh",
                    "hidden": false
                },
                {
                    "_id": "667c91f07f75474515f7f3fe",
                    "name": "Soumik Sarkar",
                    "hidden": false
                },
                {
                    "_id": "667c91f07f75474515f7f3ff",
                    "name": "Nirav Merchant",
                    "hidden": false
                },
                {
                    "_id": "667c91f07f75474515f7f400",
                    "user": {
                        "avatarUrl": "/avatars/52c30caa0ee11347f82420a14ec19996.svg",
                        "isPro": false,
                        "fullname": "Chinmay Hegde",
                        "user": "chegde",
                        "type": "user"
                    },
                    "name": "Chinmay Hegde",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2024-06-26T22:23:32.531Z",
                    "hidden": false
                },
                {
                    "_id": "667c91f07f75474515f7f401",
                    "user": {
                        "avatarUrl": "/avatars/dca6a664631924e5ff0e8393bb2772af.svg",
                        "isPro": true,
                        "fullname": "Baskar Group",
                        "user": "BGLab",
                        "type": "user"
                    },
                    "name": "Baskar Ganapathysubramanian",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2024-06-26T22:11:01.461Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-25T17:09:54.000Z",
            "title": "Arboretum: A Large Multimodal Dataset Enabling AI for Biodiversity",
            "summary": "We introduce Arboretum, the largest publicly accessible dataset designed to\nadvance AI for biodiversity applications. This dataset, curated from the\niNaturalist community science platform and vetted by domain experts to ensure\naccuracy, includes 134.6 million images, surpassing existing datasets in scale\nby an order of magnitude. The dataset encompasses image-language paired data\nfor a diverse set of species from birds (Aves), spiders/ticks/mites\n(Arachnida), insects (Insecta), plants (Plantae), fungus/mushrooms (Fungi),\nsnails (Mollusca), and snakes/lizards (Reptilia), making it a valuable resource\nfor multimodal vision-language AI models for biodiversity assessment and\nagriculture research. Each image is annotated with scientific names, taxonomic\ndetails, and common names, enhancing the robustness of AI model training.\n  We showcase the value of Arboretum by releasing a suite of CLIP models\ntrained using a subset of 40 million captioned images. We introduce several new\nbenchmarks for rigorous assessment, report accuracy for zero-shot learning, and\nevaluations across life stages, rare species, confounding species, and various\nlevels of the taxonomic hierarchy.\n  We anticipate that Arboretum will spur the development of AI models that can\nenable a variety of digital tools ranging from pest control strategies, crop\nmonitoring, and worldwide biodiversity assessment and environmental\nconservation. These advancements are critical for ensuring food security,\npreserving ecosystems, and mitigating the impacts of climate change. Arboretum\nis publicly available, easily accessible, and ready for immediate use.\n  Please see the https://baskargroup.github.io/Arboretum/{project\nwebsite} for links to our data, models, and code.",
            "upvotes": 3
        },
        "publishedAt": "2024-07-01T08:20:59.076Z",
        "title": "Arboretum: A Large Multimodal Dataset Enabling AI for Biodiversity",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.17720.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1627505688463-60107b385ac3e86b3ea4fc34.jpeg",
            "fullname": "Daniel van Strien",
            "name": "davanstrien",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.19320",
            "authors": [
                {
                    "_id": "66825dbece294ddc5e7217d5",
                    "user": {
                        "avatarUrl": "/avatars/a660039d2152a2ab8139bc3b4a8cb439.svg",
                        "isPro": false,
                        "fullname": "Vincent Micheli",
                        "user": "vmicheli",
                        "type": "user"
                    },
                    "name": "Vincent Micheli",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-01T09:46:10.053Z",
                    "hidden": false
                },
                {
                    "_id": "66825dbece294ddc5e7217d6",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/ja4hmXYNBcprZau6Miux1.png",
                        "isPro": false,
                        "fullname": "Eloi Alonso",
                        "user": "eloialonso",
                        "type": "user"
                    },
                    "name": "Eloi Alonso",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-01T09:46:16.330Z",
                    "hidden": false
                },
                {
                    "_id": "66825dbece294ddc5e7217d7",
                    "user": {
                        "avatarUrl": "/avatars/7f1ac593608a89926f4107b4c3dd91d4.svg",
                        "isPro": false,
                        "fullname": "François Fleuret",
                        "user": "francoisfleuret",
                        "type": "user"
                    },
                    "name": "François Fleuret",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-01T09:46:22.087Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-27T16:54:12.000Z",
            "title": "Efficient World Models with Context-Aware Tokenization",
            "summary": "Scaling up deep Reinforcement Learning (RL) methods presents a significant\nchallenge. Following developments in generative modelling, model-based RL\npositions itself as a strong contender. Recent advances in sequence modelling\nhave led to effective transformer-based world models, albeit at the price of\nheavy computations due to the long sequences of tokens required to accurately\nsimulate environments. In this work, we propose Delta-IRIS, a new agent with\na world model architecture composed of a discrete autoencoder that encodes\nstochastic deltas between time steps and an autoregressive transformer that\npredicts future deltas by summarizing the current state of the world with\ncontinuous tokens. In the Crafter benchmark, Delta-IRIS sets a new state of\nthe art at multiple frame budgets, while being an order of magnitude faster to\ntrain than previous attention-based approaches. We release our code and models\nat https://github.com/vmicheli/delta-iris.",
            "upvotes": 3
        },
        "publishedAt": "2024-07-01T06:18:06.407Z",
        "title": "Efficient World Models with Context-Aware Tokenization",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/607d640fc5e1e416b294ea74/rAsOEtWx0lMzic8W5-v6W.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.19320.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/a660039d2152a2ab8139bc3b4a8cb439.svg",
            "fullname": "Vincent Micheli",
            "name": "vmicheli",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.16845",
            "authors": [
                {
                    "_id": "667a32e17dc34fe17cef3a84",
                    "user": {
                        "avatarUrl": "/avatars/bd6d4512d66fd9fd7fd5476ea7a44b46.svg",
                        "isPro": false,
                        "fullname": "Weike Zhao",
                        "user": "Angelakeke",
                        "type": "user"
                    },
                    "name": "Weike Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-07-01T07:47:05.570Z",
                    "hidden": false
                },
                {
                    "_id": "667a32e17dc34fe17cef3a85",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6436aaaa0c77d7c5036abdbd/C9A276yEeAPkKLEqUcjl_.jpeg",
                        "isPro": false,
                        "fullname": "Chaoyi Wu",
                        "user": "chaoyi-wu",
                        "type": "user"
                    },
                    "name": "Chaoyi Wu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-01T15:04:29.973Z",
                    "hidden": false
                },
                {
                    "_id": "667a32e17dc34fe17cef3a86",
                    "user": {
                        "avatarUrl": "/avatars/b38f5e55e5f5ba3c361bae32775e50e1.svg",
                        "isPro": false,
                        "fullname": "Xiaoman Zhang",
                        "user": "XiaomanZhang",
                        "type": "user"
                    },
                    "name": "Xiaoman Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-01T15:04:37.223Z",
                    "hidden": false
                },
                {
                    "_id": "667a32e17dc34fe17cef3a87",
                    "name": "Ya Zhang",
                    "hidden": false
                },
                {
                    "_id": "667a32e17dc34fe17cef3a88",
                    "name": "Yanfeng Wang",
                    "hidden": false
                },
                {
                    "_id": "667a32e17dc34fe17cef3a89",
                    "user": {
                        "avatarUrl": "/avatars/5398658c7d0cf556531a625a4ca5d18a.svg",
                        "isPro": false,
                        "fullname": "Xie",
                        "user": "Weidi",
                        "type": "user"
                    },
                    "name": "Weidi Xie",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-07-01T15:05:32.447Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-24T17:49:28.000Z",
            "title": "RaTEScore: A Metric for Radiology Report Generation",
            "summary": "This paper introduces a novel, entity-aware metric, termed as Radiological\nReport (Text) Evaluation (RaTEScore), to assess the quality of medical reports\ngenerated by AI models. RaTEScore emphasizes crucial medical entities such as\ndiagnostic outcomes and anatomical details, and is robust against complex\nmedical synonyms and sensitive to negation expressions. Technically, we\ndeveloped a comprehensive medical NER dataset, RaTE-NER, and trained an NER\nmodel specifically for this purpose. This model enables the decomposition of\ncomplex radiological reports into constituent medical entities. The metric\nitself is derived by comparing the similarity of entity embeddings, obtained\nfrom a language model, based on their types and relevance to clinical\nsignificance. Our evaluations demonstrate that RaTEScore aligns more closely\nwith human preference than existing metrics, validated both on established\npublic benchmarks and our newly proposed RaTE-Eval benchmark.",
            "upvotes": 2
        },
        "publishedAt": "2024-07-01T10:01:56.018Z",
        "title": "RaTEScore: A Metric for Radiology Report Generation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.16845.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/bd6d4512d66fd9fd7fd5476ea7a44b46.svg",
            "fullname": "Weike Zhao",
            "name": "Angelakeke",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    }
]