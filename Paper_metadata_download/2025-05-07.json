[
  {
    "paper": {
      "id": "2505.03318",
      "authors": [
        {
          "_id": "681aac4fd31f567552f0cc0e",
          "name": "Yibin Wang",
          "hidden": false
        },
        {
          "_id": "681aac4fd31f567552f0cc0f",
          "name": "Zhimin Li",
          "hidden": false
        },
        {
          "_id": "681aac4fd31f567552f0cc10",
          "name": "Yuhang Zang",
          "hidden": false
        },
        {
          "_id": "681aac4fd31f567552f0cc11",
          "name": "Chunyu Wang",
          "hidden": false
        },
        {
          "_id": "681aac4fd31f567552f0cc12",
          "name": "Qinglin Lu",
          "hidden": false
        },
        {
          "_id": "681aac4fd31f567552f0cc13",
          "name": "Cheng Jin",
          "hidden": false
        },
        {
          "_id": "681aac4fd31f567552f0cc14",
          "name": "Jiaqi Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-06T08:46:41.000Z",
      "submittedOnDailyAt": "2025-05-07T00:19:35.245Z",
      "title": "Unified Multimodal Chain-of-Thought Reward Model through Reinforcement\n  Fine-Tuning",
      "submittedOnDailyBy": {
        "_id": "654c6845bac6e6e49895a5b5",
        "avatarUrl": "/avatars/ed1f140abcd4d76669e2e48db1d1193f.svg",
        "isPro": false,
        "fullname": "Yibin Wang",
        "user": "CodeGoat24",
        "type": "user"
      },
      "summary": "Recent advances in multimodal Reward Models (RMs) have shown significant\npromise in delivering reward signals to align vision models with human\npreferences. However, current RMs are generally restricted to providing direct\nresponses or engaging in shallow reasoning processes with limited depth, often\nleading to inaccurate reward signals. We posit that incorporating explicit long\nchains of thought (CoT) into the reward reasoning process can significantly\nstrengthen their reliability and robustness. Furthermore, we believe that once\nRMs internalize CoT reasoning, their direct response accuracy can also be\nimproved through implicit reasoning capabilities. To this end, this paper\nproposes UnifiedReward-Think, the first unified multimodal CoT-based reward\nmodel, capable of multi-dimensional, step-by-step long-chain reasoning for both\nvisual understanding and generation reward tasks. Specifically, we adopt an\nexploration-driven reinforcement fine-tuning approach to elicit and incentivize\nthe model's latent complex reasoning ability: (1) We first use a small amount\nof image generation preference data to distill the reasoning process of GPT-4o,\nwhich is then used for the model's cold start to learn the format and structure\nof CoT reasoning. (2) Subsequently, by leveraging the model's prior knowledge\nand generalization capabilities, we prepare large-scale unified multimodal\npreference data to elicit the model's reasoning process across various vision\ntasks. During this phase, correct reasoning outputs are retained for rejection\nsampling to refine the model (3) while incorrect predicted samples are finally\nused for Group Relative Policy Optimization (GRPO) based reinforcement\nfine-tuning, enabling the model to explore diverse reasoning paths and optimize\nfor correct and robust solutions. Extensive experiments across various vision\nreward tasks demonstrate the superiority of our model.",
      "upvotes": 46,
      "discussionId": "681aac50d31f567552f0cc5d",
      "projectPage": "https://codegoat24.github.io/UnifiedReward/think",
      "githubRepo": "https://github.com/CodeGoat24/UnifiedReward",
      "ai_keywords": [
        "Multimodal Reward Models (RMs)",
        "long chains of thought (CoT)",
        "UnifiedReward-Think",
        "exploration-driven reinforcement fine-tuning",
        "small amount of image generation preference data",
        "GPT-4o",
        "large-scale unified multimodal preference data",
        "rejection sampling",
        "Group Relative Policy Optimization (GRPO)",
        "reinforcement fine-tuning"
      ]
    },
    "publishedAt": "2025-05-06T04:46:41.000Z",
    "title": "Unified Multimodal Chain-of-Thought Reward Model through Reinforcement\n  Fine-Tuning",
    "summary": "Recent advances in multimodal Reward Models (RMs) have shown significant\npromise in delivering reward signals to align vision models with human\npreferences. However, current RMs are generally restricted to providing direct\nresponses or engaging in shallow reasoning processes with limited depth, often\nleading to inaccurate reward signals. We posit that incorporating explicit long\nchains of thought (CoT) into the reward reasoning process can significantly\nstrengthen their reliability and robustness. Furthermore, we believe that once\nRMs internalize CoT reasoning, their direct response accuracy can also be\nimproved through implicit reasoning capabilities. To this end, this paper\nproposes UnifiedReward-Think, the first unified multimodal CoT-based reward\nmodel, capable of multi-dimensional, step-by-step long-chain reasoning for both\nvisual understanding and generation reward tasks. Specifically, we adopt an\nexploration-driven reinforcement fine-tuning approach to elicit and incentivize\nthe model's latent complex reasoning ability: (1) We first use a small amount\nof image generation preference data to distill the reasoning process of GPT-4o,\nwhich is then used for the model's cold start to learn the format and structure\nof CoT reasoning. (2) Subsequently, by leveraging the model's prior knowledge\nand generalization capabilities, we prepare large-scale unified multimodal\npreference data to elicit the model's reasoning process across various vision\ntasks. During this phase, correct reasoning outputs are retained for rejection\nsampling to refine the model (3) while incorrect predicted samples are finally\nused for Group Relative Policy Optimization (GRPO) based reinforcement\nfine-tuning, enabling the model to explore diverse reasoning paths and optimize\nfor correct and robust solutions. Extensive experiments across various vision\nreward tasks demonstrate the superiority of our model.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03318.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "654c6845bac6e6e49895a5b5",
      "avatarUrl": "/avatars/ed1f140abcd4d76669e2e48db1d1193f.svg",
      "fullname": "Yibin Wang",
      "name": "CodeGoat24",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.03335",
      "authors": [
        {
          "_id": "681ab9b8f43603c60cab87a3",
          "name": "Andrew Zhao",
          "hidden": false
        },
        {
          "_id": "681ab9b8f43603c60cab87a4",
          "name": "Yiran Wu",
          "hidden": false
        },
        {
          "_id": "681ab9b8f43603c60cab87a5",
          "name": "Yang Yue",
          "hidden": false
        },
        {
          "_id": "681ab9b8f43603c60cab87a6",
          "name": "Tong Wu",
          "hidden": false
        },
        {
          "_id": "681ab9b8f43603c60cab87a7",
          "name": "Quentin Xu",
          "hidden": false
        },
        {
          "_id": "681ab9b8f43603c60cab87a8",
          "name": "Yang Yue",
          "hidden": false
        },
        {
          "_id": "681ab9b8f43603c60cab87a9",
          "name": "Matthieu Lin",
          "hidden": false
        },
        {
          "_id": "681ab9b8f43603c60cab87aa",
          "name": "Shenzhi Wang",
          "hidden": false
        },
        {
          "_id": "681ab9b8f43603c60cab87ab",
          "name": "Qingyun Wu",
          "hidden": false
        },
        {
          "_id": "681ab9b8f43603c60cab87ac",
          "user": {
            "_id": "63a95a6a7930fa8c7dd63d4e",
            "avatarUrl": "/avatars/d9d0420f7ddfe2f3a7e029fb05f1c89f.svg",
            "isPro": false,
            "fullname": "Zilong Zheng",
            "user": "zlzheng",
            "type": "user"
          },
          "name": "Zilong Zheng",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-07T01:39:06.052Z",
          "hidden": false
        },
        {
          "_id": "681ab9b8f43603c60cab87ad",
          "name": "Gao Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-06T09:08:00.000Z",
      "submittedOnDailyAt": "2025-05-07T00:10:48.130Z",
      "title": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data",
      "submittedOnDailyBy": {
        "_id": "630482fbce6b12280b18971d",
        "avatarUrl": "/avatars/b07f31fd970d736bdf574d56da7a5634.svg",
        "isPro": false,
        "fullname": "Andrew Zhao",
        "user": "andrewzh",
        "type": "user"
      },
      "summary": "Reinforcement learning with verifiable rewards (RLVR) has shown promise in\nenhancing the reasoning capabilities of large language models by learning\ndirectly from outcome-based rewards. Recent RLVR works that operate under the\nzero setting avoid supervision in labeling the reasoning process, but still\ndepend on manually curated collections of questions and answers for training.\nThe scarcity of high-quality, human-produced examples raises concerns about the\nlong-term scalability of relying on human supervision, a challenge already\nevident in the domain of language model pretraining. Furthermore, in a\nhypothetical future where AI surpasses human intelligence, tasks provided by\nhumans may offer limited learning potential for a superintelligent system. To\naddress these concerns, we propose a new RLVR paradigm called Absolute Zero, in\nwhich a single model learns to propose tasks that maximize its own learning\nprogress and improves reasoning by solving them, without relying on any\nexternal data. Under this paradigm, we introduce the Absolute Zero Reasoner\n(AZR), a system that self-evolves its training curriculum and reasoning ability\nby using a code executor to both validate proposed code reasoning tasks and\nverify answers, serving as an unified source of verifiable reward to guide\nopen-ended yet grounded learning. Despite being trained entirely without\nexternal data, AZR achieves overall SOTA performance on coding and mathematical\nreasoning tasks, outperforming existing zero-setting models that rely on tens\nof thousands of in-domain human-curated examples. Furthermore, we demonstrate\nthat AZR can be effectively applied across different model scales and is\ncompatible with various model classes.",
      "upvotes": 36,
      "discussionId": "681ab9baf43603c60cab881a",
      "projectPage": "https://andrewzh112.github.io/absolute-zero-reasoner/",
      "githubRepo": "https://github.com/LeapLabTHU/Absolute-Zero-Reasoner",
      "ai_keywords": [
        "Reinforcement learning with verifiable rewards (RLVR)",
        "zero setting",
        "outcome-based rewards",
        "manually curated collections",
        "superintelligent system",
        "Absolute Zero",
        "AzR (Absolute Zero Reasoner)",
        "training curriculum",
        "code executor",
        "verifiable reward",
        "open-ended yet grounded learning",
        "SOTA performance",
        "coding and mathematical reasoning tasks"
      ]
    },
    "publishedAt": "2025-05-06T05:08:00.000Z",
    "title": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data",
    "summary": "Reinforcement learning with verifiable rewards (RLVR) has shown promise in\nenhancing the reasoning capabilities of large language models by learning\ndirectly from outcome-based rewards. Recent RLVR works that operate under the\nzero setting avoid supervision in labeling the reasoning process, but still\ndepend on manually curated collections of questions and answers for training.\nThe scarcity of high-quality, human-produced examples raises concerns about the\nlong-term scalability of relying on human supervision, a challenge already\nevident in the domain of language model pretraining. Furthermore, in a\nhypothetical future where AI surpasses human intelligence, tasks provided by\nhumans may offer limited learning potential for a superintelligent system. To\naddress these concerns, we propose a new RLVR paradigm called Absolute Zero, in\nwhich a single model learns to propose tasks that maximize its own learning\nprogress and improves reasoning by solving them, without relying on any\nexternal data. Under this paradigm, we introduce the Absolute Zero Reasoner\n(AZR), a system that self-evolves its training curriculum and reasoning ability\nby using a code executor to both validate proposed code reasoning tasks and\nverify answers, serving as an unified source of verifiable reward to guide\nopen-ended yet grounded learning. Despite being trained entirely without\nexternal data, AZR achieves overall SOTA performance on coding and mathematical\nreasoning tasks, outperforming existing zero-setting models that rely on tens\nof thousands of in-domain human-curated examples. Furthermore, we demonstrate\nthat AZR can be effectively applied across different model scales and is\ncompatible with various model classes.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03335.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "630482fbce6b12280b18971d",
      "avatarUrl": "/avatars/b07f31fd970d736bdf574d56da7a5634.svg",
      "fullname": "Andrew Zhao",
      "name": "andrewzh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.03730",
      "authors": [
        {
          "_id": "681abf3759155282c1cb2306",
          "name": "Shiyi Zhang",
          "hidden": false
        },
        {
          "_id": "681abf3759155282c1cb2307",
          "name": "Junhao Zhuang",
          "hidden": false
        },
        {
          "_id": "681abf3759155282c1cb2308",
          "name": "Zhaoyang Zhang",
          "hidden": false
        },
        {
          "_id": "681abf3759155282c1cb2309",
          "name": "Ying Shan",
          "hidden": false
        },
        {
          "_id": "681abf3759155282c1cb230a",
          "name": "Yansong Tang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-06T17:58:02.000Z",
      "submittedOnDailyAt": "2025-05-07T00:37:06.442Z",
      "title": "FlexiAct: Towards Flexible Action Control in Heterogeneous Scenarios",
      "submittedOnDailyBy": {
        "_id": "6315d306a9456afe2b9bf34a",
        "avatarUrl": "/avatars/7285b4e7d84b528d1a50f8ee4eb10727.svg",
        "isPro": false,
        "fullname": "ElevenZ",
        "user": "shiyi0408",
        "type": "user"
      },
      "summary": "Action customization involves generating videos where the subject performs\nactions dictated by input control signals. Current methods use pose-guided or\nglobal motion customization but are limited by strict constraints on spatial\nstructure, such as layout, skeleton, and viewpoint consistency, reducing\nadaptability across diverse subjects and scenarios. To overcome these\nlimitations, we propose FlexiAct, which transfers actions from a reference\nvideo to an arbitrary target image. Unlike existing methods, FlexiAct allows\nfor variations in layout, viewpoint, and skeletal structure between the subject\nof the reference video and the target image, while maintaining identity\nconsistency. Achieving this requires precise action control, spatial structure\nadaptation, and consistency preservation. To this end, we introduce RefAdapter,\na lightweight image-conditioned adapter that excels in spatial adaptation and\nconsistency preservation, surpassing existing methods in balancing appearance\nconsistency and structural flexibility. Additionally, based on our\nobservations, the denoising process exhibits varying levels of attention to\nmotion (low frequency) and appearance details (high frequency) at different\ntimesteps. So we propose FAE (Frequency-aware Action Extraction), which, unlike\nexisting methods that rely on separate spatial-temporal architectures, directly\nachieves action extraction during the denoising process. Experiments\ndemonstrate that our method effectively transfers actions to subjects with\ndiverse layouts, skeletons, and viewpoints. We release our code and model\nweights to support further research at\nhttps://shiyi-zh0408.github.io/projectpages/FlexiAct/",
      "upvotes": 16,
      "discussionId": "681abf3859155282c1cb23fa",
      "projectPage": "https://shiyi-zh0408.github.io/projectpages/FlexiAct/",
      "githubRepo": "https://github.com/shiyi-zh0408/FlexiAct",
      "ai_keywords": [
        "FlexiAct",
        "RefAdapter",
        "image-conditioned adapter",
        "spatial adaptation",
        "consistency preservation",
        "FAE",
        "Frequency-aware Action Extraction",
        "denoising process",
        "spatial-temporal architectures",
        "action extraction"
      ]
    },
    "publishedAt": "2025-05-06T13:58:02.000Z",
    "title": "FlexiAct: Towards Flexible Action Control in Heterogeneous Scenarios",
    "summary": "Action customization involves generating videos where the subject performs\nactions dictated by input control signals. Current methods use pose-guided or\nglobal motion customization but are limited by strict constraints on spatial\nstructure, such as layout, skeleton, and viewpoint consistency, reducing\nadaptability across diverse subjects and scenarios. To overcome these\nlimitations, we propose FlexiAct, which transfers actions from a reference\nvideo to an arbitrary target image. Unlike existing methods, FlexiAct allows\nfor variations in layout, viewpoint, and skeletal structure between the subject\nof the reference video and the target image, while maintaining identity\nconsistency. Achieving this requires precise action control, spatial structure\nadaptation, and consistency preservation. To this end, we introduce RefAdapter,\na lightweight image-conditioned adapter that excels in spatial adaptation and\nconsistency preservation, surpassing existing methods in balancing appearance\nconsistency and structural flexibility. Additionally, based on our\nobservations, the denoising process exhibits varying levels of attention to\nmotion (low frequency) and appearance details (high frequency) at different\ntimesteps. So we propose FAE (Frequency-aware Action Extraction), which, unlike\nexisting methods that rely on separate spatial-temporal architectures, directly\nachieves action extraction during the denoising process. Experiments\ndemonstrate that our method effectively transfers actions to subjects with\ndiverse layouts, skeletons, and viewpoints. We release our code and model\nweights to support further research at\nhttps://shiyi-zh0408.github.io/projectpages/FlexiAct/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03730.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6315d306a9456afe2b9bf34a",
      "avatarUrl": "/avatars/7285b4e7d84b528d1a50f8ee4eb10727.svg",
      "fullname": "ElevenZ",
      "name": "shiyi0408",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.03005",
      "authors": [
        {
          "_id": "681ac8a09f4ed2ece10fac18",
          "name": "Daniel Goldstein",
          "hidden": false
        },
        {
          "_id": "681ac8a09f4ed2ece10fac19",
          "name": "Eric Alcaide",
          "hidden": false
        },
        {
          "_id": "681ac8a09f4ed2ece10fac1a",
          "name": "Janna Lu",
          "hidden": false
        },
        {
          "_id": "681ac8a09f4ed2ece10fac1b",
          "name": "Eugene Cheah",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/647f4bac45baf21ad709fcd0/sfcDfWZfka9sy8siQC6MV.png",
        "https://cdn-uploads.huggingface.co/production/uploads/647f4bac45baf21ad709fcd0/yXcUOgY48NS_nfIZN0Kqq.png"
      ],
      "publishedAt": "2025-05-05T20:03:28.000Z",
      "submittedOnDailyAt": "2025-05-07T01:19:36.031Z",
      "title": "RADLADS: Rapid Attention Distillation to Linear Attention Decoders at\n  Scale",
      "submittedOnDailyBy": {
        "_id": "647f4bac45baf21ad709fcd0",
        "avatarUrl": "/avatars/14c04cdda95de676aeefa9ae3e7c19ba.svg",
        "isPro": false,
        "fullname": "Dan Goldstein",
        "user": "SmerkyG",
        "type": "user"
      },
      "summary": "We present Rapid Attention Distillation to Linear Attention Decoders at Scale\n(RADLADS), a protocol for rapidly converting softmax attention transformers\ninto linear attention decoder models, along with two new RWKV-variant\narchitectures, and models converted from popular Qwen2.5 open source models in\n7B, 32B, and 72B sizes. Our conversion process requires only 350-700M tokens,\nless than 0.005% of the token count used to train the original teacher models.\nConverting to our 72B linear attention model costs less than \\$2,000 USD at\ntoday's prices, yet quality at inference remains close to the original\ntransformer. These models achieve state-of-the-art downstream performance\nacross a set of standard benchmarks for linear attention models of their size.\nWe release all our models on HuggingFace under the Apache 2.0 license, with the\nexception of our 72B models which are also governed by the Qwen License\nAgreement.\n  Models at\nhttps://huggingface.co/collections/recursal/radlads-6818ee69e99e729ba8a87102\nTraining Code at https://github.com/recursal/RADLADS-paper",
      "upvotes": 12,
      "discussionId": "681ac8a19f4ed2ece10fac75",
      "ai_keywords": [
        "Rapid Attention Distillation",
        "RADLADS",
        "softmax attention transformers",
        "linear attention decoder models",
        "RWKV-variant",
        "Qwen2.5",
        "token count",
        "linear attention models",
        "HuggingFace",
        "Apache 2.0 license",
        "Qwen License Agreement"
      ]
    },
    "publishedAt": "2025-05-05T16:03:28.000Z",
    "title": "RADLADS: Rapid Attention Distillation to Linear Attention Decoders at\n  Scale",
    "summary": "We present Rapid Attention Distillation to Linear Attention Decoders at Scale\n(RADLADS), a protocol for rapidly converting softmax attention transformers\ninto linear attention decoder models, along with two new RWKV-variant\narchitectures, and models converted from popular Qwen2.5 open source models in\n7B, 32B, and 72B sizes. Our conversion process requires only 350-700M tokens,\nless than 0.005% of the token count used to train the original teacher models.\nConverting to our 72B linear attention model costs less than \\$2,000 USD at\ntoday's prices, yet quality at inference remains close to the original\ntransformer. These models achieve state-of-the-art downstream performance\nacross a set of standard benchmarks for linear attention models of their size.\nWe release all our models on HuggingFace under the Apache 2.0 license, with the\nexception of our 72B models which are also governed by the Qwen License\nAgreement.\n  Models at\nhttps://huggingface.co/collections/recursal/radlads-6818ee69e99e729ba8a87102\nTraining Code at https://github.com/recursal/RADLADS-paper",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/647f4bac45baf21ad709fcd0/sfcDfWZfka9sy8siQC6MV.png",
      "https://cdn-uploads.huggingface.co/production/uploads/647f4bac45baf21ad709fcd0/yXcUOgY48NS_nfIZN0Kqq.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03005.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647f4bac45baf21ad709fcd0",
      "avatarUrl": "/avatars/14c04cdda95de676aeefa9ae3e7c19ba.svg",
      "fullname": "Dan Goldstein",
      "name": "SmerkyG",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.02922",
      "authors": [
        {
          "_id": "681abdbef8feaef23b543877",
          "name": "Yaoqi Chen",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b543878",
          "name": "Jinkai Zhang",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b543879",
          "user": {
            "_id": "667135bdcca06def1c2599a6",
            "avatarUrl": "/avatars/d94ab99265e1970852605d344b4d69e9.svg",
            "isPro": false,
            "fullname": "Baotong Lu",
            "user": "baotonglu",
            "type": "user"
          },
          "name": "Baotong Lu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-07T01:56:16.240Z",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b54387a",
          "name": "Qianxi Zhang",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b54387b",
          "name": "Chengruidong Zhang",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b54387c",
          "name": "Jingjia Luo",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b54387d",
          "name": "Di Liu",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b54387e",
          "name": "Huiqiang Jiang",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b54387f",
          "name": "Qi Chen",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b543880",
          "name": "Jing Liu",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b543881",
          "name": "Bailu Ding",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b543882",
          "name": "Xiao Yan",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b543883",
          "name": "Jiawei Jiang",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b543884",
          "name": "Chen Chen",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b543885",
          "name": "Mingxing Zhang",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b543886",
          "name": "Yuqing Yang",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b543887",
          "name": "Fan Yang",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b543888",
          "name": "Mao Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-05T18:01:17.000Z",
      "submittedOnDailyAt": "2025-05-07T00:36:48.274Z",
      "title": "RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM\n  Inference",
      "submittedOnDailyBy": {
        "_id": "6278bd42541f3d2dfa77ea70",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6278bd42541f3d2dfa77ea70/ejn49eapnB3UXQckAYdTd.jpeg",
        "isPro": true,
        "fullname": "Huiqiang Jiang",
        "user": "iofu728",
        "type": "user"
      },
      "summary": "The growing context lengths of large language models (LLMs) pose significant\nchallenges for efficient inference, primarily due to GPU memory and bandwidth\nconstraints. We present RetroInfer, a novel system that reconceptualizes the\nkey-value (KV) cache as a vector storage system which exploits the inherent\nattention sparsity to accelerate long-context LLM inference. At its core is the\nwave index, an Attention-aWare VEctor index that enables efficient and accurate\nretrieval of critical tokens through techniques such as tripartite attention\napproximation, accuracy-bounded attention estimation, and segmented clustering.\nComplementing this is the wave buffer, which coordinates KV cache placement and\noverlaps computation and data transfer across GPU and CPU to sustain high\nthroughput. Unlike prior sparsity-based methods that struggle with token\nselection and hardware coordination, RetroInfer delivers robust performance\nwithout compromising model accuracy. Experiments on long-context benchmarks\nshow up to 4.5X speedup over full attention within GPU memory limits and up to\n10.5X over sparse attention baselines when KV cache is extended to CPU memory,\nall while preserving full-attention-level accuracy.",
      "upvotes": 8,
      "discussionId": "681abdc0f8feaef23b543926",
      "ai_keywords": [
        "RetrInfer",
        "key-value (KV) cache",
        "vector storage system",
        "wave index",
        "Attention-aWare VEctor index",
        "tripartite attention approximation",
        "accuracy-bounded attention estimation",
        "segmented clustering",
        "wave buffer",
        "GPU memory",
        "token selection",
        "hardware coordination"
      ]
    },
    "publishedAt": "2025-05-05T14:01:17.000Z",
    "title": "RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM\n  Inference",
    "summary": "The growing context lengths of large language models (LLMs) pose significant\nchallenges for efficient inference, primarily due to GPU memory and bandwidth\nconstraints. We present RetroInfer, a novel system that reconceptualizes the\nkey-value (KV) cache as a vector storage system which exploits the inherent\nattention sparsity to accelerate long-context LLM inference. At its core is the\nwave index, an Attention-aWare VEctor index that enables efficient and accurate\nretrieval of critical tokens through techniques such as tripartite attention\napproximation, accuracy-bounded attention estimation, and segmented clustering.\nComplementing this is the wave buffer, which coordinates KV cache placement and\noverlaps computation and data transfer across GPU and CPU to sustain high\nthroughput. Unlike prior sparsity-based methods that struggle with token\nselection and hardware coordination, RetroInfer delivers robust performance\nwithout compromising model accuracy. Experiments on long-context benchmarks\nshow up to 4.5X speedup over full attention within GPU memory limits and up to\n10.5X over sparse attention baselines when KV cache is extended to CPU memory,\nall while preserving full-attention-level accuracy.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02922.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6278bd42541f3d2dfa77ea70",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6278bd42541f3d2dfa77ea70/ejn49eapnB3UXQckAYdTd.jpeg",
      "fullname": "Huiqiang Jiang",
      "name": "iofu728",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.02214",
      "authors": [
        {
          "_id": "6819905519b420a2f91aa231",
          "name": "Xingyu Zheng",
          "hidden": false
        },
        {
          "_id": "6819905519b420a2f91aa232",
          "name": "Yuye Li",
          "hidden": false
        },
        {
          "_id": "6819905519b420a2f91aa233",
          "name": "Haoran Chu",
          "hidden": false
        },
        {
          "_id": "6819905519b420a2f91aa234",
          "name": "Yue Feng",
          "hidden": false
        },
        {
          "_id": "6819905519b420a2f91aa235",
          "name": "Xudong Ma",
          "hidden": false
        },
        {
          "_id": "6819905519b420a2f91aa236",
          "name": "Jie Luo",
          "hidden": false
        },
        {
          "_id": "6819905519b420a2f91aa237",
          "name": "Jinyang Guo",
          "hidden": false
        },
        {
          "_id": "6819905519b420a2f91aa238",
          "name": "Haotong Qin",
          "hidden": false
        },
        {
          "_id": "6819905519b420a2f91aa239",
          "name": "Michele Magno",
          "hidden": false
        },
        {
          "_id": "6819905519b420a2f91aa23a",
          "name": "Xianglong Liu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64612660933afb0106a9dee3/uQV5mzdTqoNatTz5fk6xC.png"
      ],
      "publishedAt": "2025-05-04T18:43:44.000Z",
      "submittedOnDailyAt": "2025-05-07T02:59:22.920Z",
      "title": "An Empirical Study of Qwen3 Quantization",
      "submittedOnDailyBy": {
        "_id": "64612660933afb0106a9dee3",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Ea83e0zR_2m8foWy6J0AF.jpeg",
        "isPro": false,
        "fullname": "Xingyu Zheng",
        "user": "Xingyu-Zheng",
        "type": "user"
      },
      "summary": "The Qwen series has emerged as a leading family of open-source Large Language\nModels (LLMs), demonstrating remarkable capabilities in natural language\nunderstanding tasks. With the recent release of Qwen3, which exhibits superior\nperformance across diverse benchmarks, there is growing interest in deploying\nthese models efficiently in resource-constrained environments. Low-bit\nquantization presents a promising solution, yet its impact on Qwen3's\nperformance remains underexplored. This study conducts a systematic evaluation\nof Qwen3's robustness under various quantization settings, aiming to uncover\nboth opportunities and challenges in compressing this state-of-the-art model.\nWe rigorously assess 5 existing classic post-training quantization techniques\napplied to Qwen3, spanning bit-widths from 1 to 8 bits, and evaluate their\neffectiveness across multiple datasets. Our findings reveal that while Qwen3\nmaintains competitive performance at moderate bit-widths, it experiences\nnotable degradation in linguistic tasks under ultra-low precision, underscoring\nthe persistent hurdles in LLM compression. These results emphasize the need for\nfurther research to mitigate performance loss in extreme quantization\nscenarios. We anticipate that this empirical analysis will provide actionable\ninsights for advancing quantization methods tailored to Qwen3 and future LLMs,\nultimately enhancing their practicality without compromising accuracy. Our\nproject is released on https://github.com/Efficient-ML/Qwen3-Quantization and\nhttps://huggingface.co/collections/Efficient-ML/qwen3-quantization-68164450decb1c868788cb2b.",
      "upvotes": 5,
      "discussionId": "6819905519b420a2f91aa27c",
      "githubRepo": "https://github.com/Efficient-ML/Qwen3-Quantization",
      "ai_keywords": [
        "Low-bit quantization",
        "Post-training quantization techniques",
        "Bit-widths",
        "Linguistic tasks",
        "LLM compression",
        "Quantization methods"
      ]
    },
    "publishedAt": "2025-05-04T14:43:44.000Z",
    "title": "An Empirical Study of Qwen3 Quantization",
    "summary": "The Qwen series has emerged as a leading family of open-source Large Language\nModels (LLMs), demonstrating remarkable capabilities in natural language\nunderstanding tasks. With the recent release of Qwen3, which exhibits superior\nperformance across diverse benchmarks, there is growing interest in deploying\nthese models efficiently in resource-constrained environments. Low-bit\nquantization presents a promising solution, yet its impact on Qwen3's\nperformance remains underexplored. This study conducts a systematic evaluation\nof Qwen3's robustness under various quantization settings, aiming to uncover\nboth opportunities and challenges in compressing this state-of-the-art model.\nWe rigorously assess 5 existing classic post-training quantization techniques\napplied to Qwen3, spanning bit-widths from 1 to 8 bits, and evaluate their\neffectiveness across multiple datasets. Our findings reveal that while Qwen3\nmaintains competitive performance at moderate bit-widths, it experiences\nnotable degradation in linguistic tasks under ultra-low precision, underscoring\nthe persistent hurdles in LLM compression. These results emphasize the need for\nfurther research to mitigate performance loss in extreme quantization\nscenarios. We anticipate that this empirical analysis will provide actionable\ninsights for advancing quantization methods tailored to Qwen3 and future LLMs,\nultimately enhancing their practicality without compromising accuracy. Our\nproject is released on https://github.com/Efficient-ML/Qwen3-Quantization and\nhttps://huggingface.co/collections/Efficient-ML/qwen3-quantization-68164450decb1c868788cb2b.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64612660933afb0106a9dee3/uQV5mzdTqoNatTz5fk6xC.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02214.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64612660933afb0106a9dee3",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Ea83e0zR_2m8foWy6J0AF.jpeg",
      "fullname": "Xingyu Zheng",
      "name": "Xingyu-Zheng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.03735",
      "authors": [
        {
          "_id": "681acb9c285636e830d37c8e",
          "name": "Jiayuan Rao",
          "hidden": false
        },
        {
          "_id": "681acb9c285636e830d37c8f",
          "name": "Zifeng Li",
          "hidden": false
        },
        {
          "_id": "681acb9c285636e830d37c90",
          "user": {
            "_id": "632c7a0d1d303f5f9acf01b8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/632c7a0d1d303f5f9acf01b8/T010IFuCp6UaOeIyWhbCk.jpeg",
            "isPro": false,
            "fullname": "Haoning Wu",
            "user": "haoningwu",
            "type": "user"
          },
          "name": "Haoning Wu",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-07T02:56:09.106Z",
          "hidden": false
        },
        {
          "_id": "681acb9c285636e830d37c91",
          "name": "Ya Zhang",
          "hidden": false
        },
        {
          "_id": "681acb9c285636e830d37c92",
          "name": "Yanfeng Wang",
          "hidden": false
        },
        {
          "_id": "681acb9c285636e830d37c93",
          "name": "Weidi Xie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-06T17:59:31.000Z",
      "submittedOnDailyAt": "2025-05-07T01:26:13.756Z",
      "title": "Multi-Agent System for Comprehensive Soccer Understanding",
      "submittedOnDailyBy": {
        "_id": "6625ce8074ae2df4e3effa92",
        "avatarUrl": "/avatars/686cd85cf8d3d8f0beb0737811144294.svg",
        "isPro": false,
        "fullname": "Jiayuan Rao 饶珈源",
        "user": "Homie0609",
        "type": "user"
      },
      "summary": "Recent advancements in AI-driven soccer understanding have demonstrated rapid\nprogress, yet existing research predominantly focuses on isolated or narrow\ntasks. To bridge this gap, we propose a comprehensive framework for holistic\nsoccer understanding. Specifically, we make the following contributions in this\npaper: (i) we construct SoccerWiki, the first large-scale multimodal soccer\nknowledge base, integrating rich domain knowledge about players, teams,\nreferees, and venues to enable knowledge-driven reasoning; (ii) we present\nSoccerBench, the largest and most comprehensive soccer-specific benchmark,\nfeaturing around 10K standardized multimodal (text, image, video) multi-choice\nQA pairs across 13 distinct understanding tasks, curated through automated\npipelines and manual verification; (iii) we introduce SoccerAgent, a novel\nmulti-agent system that decomposes complex soccer questions via collaborative\nreasoning, leveraging domain expertise from SoccerWiki and achieving robust\nperformance; (iv) extensive evaluations and ablations that benchmark\nstate-of-the-art MLLMs on SoccerBench, highlighting the superiority of our\nproposed agentic system. All data and code are publicly available at:\nhttps://jyrao.github.io/SoccerAgent/.",
      "upvotes": 3,
      "discussionId": "681acb9e285636e830d37d4b",
      "projectPage": "https://jyrao.github.io/SoccerAgent/",
      "githubRepo": "https://github.com/jyrao/SoccerAgent",
      "ai_keywords": [
        "multimodal",
        "knowledge base",
        "domain knowledge",
        "multimodal (text, image, video) multi-choice QA pairs",
        "multi-agent system",
        "collaborative reasoning",
        "state-of-the-art MLLMs"
      ]
    },
    "publishedAt": "2025-05-06T13:59:31.000Z",
    "title": "Multi-Agent System for Comprehensive Soccer Understanding",
    "summary": "Recent advancements in AI-driven soccer understanding have demonstrated rapid\nprogress, yet existing research predominantly focuses on isolated or narrow\ntasks. To bridge this gap, we propose a comprehensive framework for holistic\nsoccer understanding. Specifically, we make the following contributions in this\npaper: (i) we construct SoccerWiki, the first large-scale multimodal soccer\nknowledge base, integrating rich domain knowledge about players, teams,\nreferees, and venues to enable knowledge-driven reasoning; (ii) we present\nSoccerBench, the largest and most comprehensive soccer-specific benchmark,\nfeaturing around 10K standardized multimodal (text, image, video) multi-choice\nQA pairs across 13 distinct understanding tasks, curated through automated\npipelines and manual verification; (iii) we introduce SoccerAgent, a novel\nmulti-agent system that decomposes complex soccer questions via collaborative\nreasoning, leveraging domain expertise from SoccerWiki and achieving robust\nperformance; (iv) extensive evaluations and ablations that benchmark\nstate-of-the-art MLLMs on SoccerBench, highlighting the superiority of our\nproposed agentic system. All data and code are publicly available at:\nhttps://jyrao.github.io/SoccerAgent/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03735.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6625ce8074ae2df4e3effa92",
      "avatarUrl": "/avatars/686cd85cf8d3d8f0beb0737811144294.svg",
      "fullname": "Jiayuan Rao 饶珈源",
      "name": "Homie0609",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.03164",
      "authors": [
        {
          "_id": "681ac5d2626f5a368b2a7103",
          "name": "Ji Won Chung",
          "hidden": false
        },
        {
          "_id": "681ac5d2626f5a368b2a7104",
          "name": "Tongyu Zhou",
          "hidden": false
        },
        {
          "_id": "681ac5d2626f5a368b2a7105",
          "name": "Ivy Chen",
          "hidden": false
        },
        {
          "_id": "681ac5d2626f5a368b2a7106",
          "name": "Kevin Hsu",
          "hidden": false
        },
        {
          "_id": "681ac5d2626f5a368b2a7107",
          "name": "Ryan A. Rossi",
          "hidden": false
        },
        {
          "_id": "681ac5d2626f5a368b2a7108",
          "name": "Alexa Siu",
          "hidden": false
        },
        {
          "_id": "681ac5d2626f5a368b2a7109",
          "name": "Shunan Guo",
          "hidden": false
        },
        {
          "_id": "681ac5d2626f5a368b2a710a",
          "name": "Franck Dernoncourt",
          "hidden": false
        },
        {
          "_id": "681ac5d2626f5a368b2a710b",
          "name": "James Tompkin",
          "hidden": false
        },
        {
          "_id": "681ac5d2626f5a368b2a710c",
          "name": "Jeff Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-06T04:18:42.000Z",
      "submittedOnDailyAt": "2025-05-07T01:00:46.546Z",
      "title": "InfoVids: Reimagining the Viewer Experience with Alternative\n  Visualization-Presenter Relationships",
      "submittedOnDailyBy": {
        "_id": "62c5947524171688a9feb992",
        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
        "isPro": false,
        "fullname": "Franck Dernoncourt",
        "user": "Franck-Dernoncourt",
        "type": "user"
      },
      "summary": "Traditional data presentations typically separate the presenter and\nvisualization into two separate spaces--the 3D world and a 2D screen--enforcing\nvisualization-centric stories. To create a more human-centric viewing\nexperience, we establish a more equitable relationship between the\nvisualization and the presenter through our InfoVids. These\ninfographics-inspired informational videos are crafted to redefine\nrelationships between the presenter and visualizations. As we design InfoVids,\nwe explore how the use of layout, form, and interactions affects the viewer\nexperience. We compare InfoVids against their baseline 2D `slides' equivalents\nacross 9 metrics with 30 participants and provide practical, long-term insights\nfrom an autobiographical perspective. Our mixed methods analyses reveal that\nthis paradigm reduced viewer attention splitting, shifted the focus from the\nvisualization to the presenter, and led to more interactive, natural, and\nengaging full-body data performances for viewers. Ultimately, InfoVids helped\nviewers re-imagine traditional dynamics between the presenter and\nvisualizations.",
      "upvotes": 2,
      "discussionId": "681ac5d4626f5a368b2a71f8"
    },
    "publishedAt": "2025-05-06T00:18:42.000Z",
    "title": "InfoVids: Reimagining the Viewer Experience with Alternative\n  Visualization-Presenter Relationships",
    "summary": "Traditional data presentations typically separate the presenter and\nvisualization into two separate spaces--the 3D world and a 2D screen--enforcing\nvisualization-centric stories. To create a more human-centric viewing\nexperience, we establish a more equitable relationship between the\nvisualization and the presenter through our InfoVids. These\ninfographics-inspired informational videos are crafted to redefine\nrelationships between the presenter and visualizations. As we design InfoVids,\nwe explore how the use of layout, form, and interactions affects the viewer\nexperience. We compare InfoVids against their baseline 2D `slides' equivalents\nacross 9 metrics with 30 participants and provide practical, long-term insights\nfrom an autobiographical perspective. Our mixed methods analyses reveal that\nthis paradigm reduced viewer attention splitting, shifted the focus from the\nvisualization to the presenter, and led to more interactive, natural, and\nengaging full-body data performances for viewers. Ultimately, InfoVids helped\nviewers re-imagine traditional dynamics between the presenter and\nvisualizations.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03164.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c5947524171688a9feb992",
      "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
      "fullname": "Franck Dernoncourt",
      "name": "Franck-Dernoncourt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.02311",
      "authors": [
        {
          "_id": "681a06459c99f4b5ce5f2838",
          "user": {
            "_id": "658e85bb5b7553ca5c29ba89",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658e85bb5b7553ca5c29ba89/KK6UpS9agtrxevvBoup5N.jpeg",
            "isPro": false,
            "fullname": "Jihao Zhao",
            "user": "Robot2050",
            "type": "user"
          },
          "name": "Jihao Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T16:51:29.234Z",
          "hidden": false
        },
        {
          "_id": "681a06459c99f4b5ce5f2839",
          "name": "Chunlai Zhou",
          "hidden": false
        },
        {
          "_id": "681a06459c99f4b5ce5f283a",
          "name": "Biao Qin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-05T01:45:56.000Z",
      "submittedOnDailyAt": "2025-05-07T00:06:14.265Z",
      "title": "Invoke Interfaces Only When Needed: Adaptive Invocation for Large\n  Language Models in Question Answering",
      "submittedOnDailyBy": {
        "_id": "658e85bb5b7553ca5c29ba89",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658e85bb5b7553ca5c29ba89/KK6UpS9agtrxevvBoup5N.jpeg",
        "isPro": false,
        "fullname": "Jihao Zhao",
        "user": "Robot2050",
        "type": "user"
      },
      "summary": "The collaborative paradigm of large and small language models (LMs)\neffectively balances performance and cost, yet its pivotal challenge lies in\nprecisely pinpointing the moment of invocation when hallucinations arise in\nsmall LMs. Previous optimization efforts primarily focused on post-processing\ntechniques, which were separate from the reasoning process of LMs, resulting in\nhigh computational costs and limited effectiveness. In this paper, we propose a\npractical invocation evaluation metric called AttenHScore, which calculates the\naccumulation and propagation of hallucinations during the generation process of\nsmall LMs, continuously amplifying potential reasoning errors. By dynamically\nadjusting the detection threshold, we achieve more accurate real-time\ninvocation of large LMs. Additionally, considering the limited reasoning\ncapacity of small LMs, we leverage uncertainty-aware knowledge reorganization\nto assist them better capture critical information from different text chunks.\nExtensive experiments reveal that our AttenHScore outperforms most baseline in\nenhancing real-time hallucination detection capabilities across multiple QA\ndatasets, especially when addressing complex queries. Moreover, our strategies\neliminate the need for additional model training and display flexibility in\nadapting to various transformer-based LMs.",
      "upvotes": 2,
      "discussionId": "681a06469c99f4b5ce5f28a3",
      "ai_keywords": [
        "large language models (LMs)",
        "small language models (LMs)",
        "hallucinations",
        "post-processing techniques",
        "AttenHScore",
        "generation process",
        "reasoning errors",
        "dynamic adjustment",
        "real-time invocation",
        "reasoning capacity",
        "uncertainty-aware knowledge reorganization",
        "QA datasets",
        "complex queries",
        "transformer-based LMs"
      ]
    },
    "publishedAt": "2025-05-04T21:45:56.000Z",
    "title": "Invoke Interfaces Only When Needed: Adaptive Invocation for Large\n  Language Models in Question Answering",
    "summary": "The collaborative paradigm of large and small language models (LMs)\neffectively balances performance and cost, yet its pivotal challenge lies in\nprecisely pinpointing the moment of invocation when hallucinations arise in\nsmall LMs. Previous optimization efforts primarily focused on post-processing\ntechniques, which were separate from the reasoning process of LMs, resulting in\nhigh computational costs and limited effectiveness. In this paper, we propose a\npractical invocation evaluation metric called AttenHScore, which calculates the\naccumulation and propagation of hallucinations during the generation process of\nsmall LMs, continuously amplifying potential reasoning errors. By dynamically\nadjusting the detection threshold, we achieve more accurate real-time\ninvocation of large LMs. Additionally, considering the limited reasoning\ncapacity of small LMs, we leverage uncertainty-aware knowledge reorganization\nto assist them better capture critical information from different text chunks.\nExtensive experiments reveal that our AttenHScore outperforms most baseline in\nenhancing real-time hallucination detection capabilities across multiple QA\ndatasets, especially when addressing complex queries. Moreover, our strategies\neliminate the need for additional model training and display flexibility in\nadapting to various transformer-based LMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02311.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "658e85bb5b7553ca5c29ba89",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658e85bb5b7553ca5c29ba89/KK6UpS9agtrxevvBoup5N.jpeg",
      "fullname": "Jihao Zhao",
      "name": "Robot2050",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.03368",
      "authors": [
        {
          "_id": "681b05ab8e325b4097318969",
          "name": "Stef De Sabbata",
          "hidden": false
        },
        {
          "_id": "681b05ab8e325b409731896a",
          "name": "Stefano Mizzaro",
          "hidden": false
        },
        {
          "_id": "681b05ab8e325b409731896b",
          "name": "Kevin Roitero",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-06T09:40:06.000Z",
      "submittedOnDailyAt": "2025-05-07T05:33:25.973Z",
      "title": "Geospatial Mechanistic Interpretability of Large Language Models",
      "submittedOnDailyBy": {
        "_id": "620cca6f06a4320dbf3b50d8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654011487582-620cca6f06a4320dbf3b50d8.png",
        "isPro": false,
        "fullname": "Kevin Roitero",
        "user": "kevinr",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) have demonstrated unprecedented capabilities\nacross various natural language processing tasks. Their ability to process and\ngenerate viable text and code has made them ubiquitous in many fields, while\ntheir deployment as knowledge bases and \"reasoning\" tools remains an area of\nongoing research. In geography, a growing body of literature has been focusing\non evaluating LLMs' geographical knowledge and their ability to perform spatial\nreasoning. However, very little is still known about the internal functioning\nof these models, especially about how they process geographical information.\n  In this chapter, we establish a novel framework for the study of geospatial\nmechanistic interpretability - using spatial analysis to reverse engineer how\nLLMs handle geographical information. Our aim is to advance our understanding\nof the internal representations that these complex models generate while\nprocessing geographical information - what one might call \"how LLMs think about\ngeographic information\" if such phrasing was not an undue anthropomorphism.\n  We first outline the use of probing in revealing internal structures within\nLLMs. We then introduce the field of mechanistic interpretability, discussing\nthe superposition hypothesis and the role of sparse autoencoders in\ndisentangling polysemantic internal representations of LLMs into more\ninterpretable, monosemantic features. In our experiments, we use spatial\nautocorrelation to show how features obtained for placenames display spatial\npatterns related to their geographic location and can thus be interpreted\ngeospatially, providing insights into how these models process geographical\ninformation. We conclude by discussing how our framework can help shape the\nstudy and use of foundation models in geography.",
      "upvotes": 1,
      "discussionId": "681b05ad8e325b4097318a3d",
      "ai_keywords": [
        "probing",
        "mechanistic interpretability",
        "superposition hypothesis",
        "sparse autoencoders",
        "polysemantic",
        "monosemantic",
        "spatial autocorrelation",
        "placenames",
        "geospatial patterns"
      ]
    },
    "publishedAt": "2025-05-06T05:40:06.000Z",
    "title": "Geospatial Mechanistic Interpretability of Large Language Models",
    "summary": "Large Language Models (LLMs) have demonstrated unprecedented capabilities\nacross various natural language processing tasks. Their ability to process and\ngenerate viable text and code has made them ubiquitous in many fields, while\ntheir deployment as knowledge bases and \"reasoning\" tools remains an area of\nongoing research. In geography, a growing body of literature has been focusing\non evaluating LLMs' geographical knowledge and their ability to perform spatial\nreasoning. However, very little is still known about the internal functioning\nof these models, especially about how they process geographical information.\n  In this chapter, we establish a novel framework for the study of geospatial\nmechanistic interpretability - using spatial analysis to reverse engineer how\nLLMs handle geographical information. Our aim is to advance our understanding\nof the internal representations that these complex models generate while\nprocessing geographical information - what one might call \"how LLMs think about\ngeographic information\" if such phrasing was not an undue anthropomorphism.\n  We first outline the use of probing in revealing internal structures within\nLLMs. We then introduce the field of mechanistic interpretability, discussing\nthe superposition hypothesis and the role of sparse autoencoders in\ndisentangling polysemantic internal representations of LLMs into more\ninterpretable, monosemantic features. In our experiments, we use spatial\nautocorrelation to show how features obtained for placenames display spatial\npatterns related to their geographic location and can thus be interpreted\ngeospatially, providing insights into how these models process geographical\ninformation. We conclude by discussing how our framework can help shape the\nstudy and use of foundation models in geography.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03368.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620cca6f06a4320dbf3b50d8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654011487582-620cca6f06a4320dbf3b50d8.png",
      "fullname": "Kevin Roitero",
      "name": "kevinr",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.02872",
      "authors": [
        {
          "_id": "681b03a33bf166767107c74b",
          "name": "Cfir Avraham Hadar",
          "hidden": false
        },
        {
          "_id": "681b03a33bf166767107c74c",
          "name": "Omer Shubi",
          "hidden": false
        },
        {
          "_id": "681b03a33bf166767107c74d",
          "name": "Yoav Meiri",
          "hidden": false
        },
        {
          "_id": "681b03a33bf166767107c74e",
          "name": "Yevgeni Berzak",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-04T13:23:48.000Z",
      "submittedOnDailyAt": "2025-05-07T05:24:49.077Z",
      "title": "Decoding Open-Ended Information Seeking Goals from Eye Movements in\n  Reading",
      "submittedOnDailyBy": {
        "_id": "60ef001bed64a34082bfa0dd",
        "avatarUrl": "/avatars/78e4daeac169edbf4dc42fbed9b50d59.svg",
        "isPro": false,
        "fullname": "Omer Shubi",
        "user": "scaperex",
        "type": "user"
      },
      "summary": "When reading, we often have specific information that interests us in a text.\nFor example, you might be reading this paper because you are curious about LLMs\nfor eye movements in reading, the experimental design, or perhaps you only care\nabout the question ``but does it work?''. More broadly, in daily life, people\napproach texts with any number of text-specific goals that guide their reading\nbehavior. In this work, we ask, for the first time, whether open-ended reading\ngoals can be automatically decoded from eye movements in reading. To address\nthis question, we introduce goal classification and goal reconstruction tasks\nand evaluation frameworks, and use large-scale eye tracking for reading data in\nEnglish with hundreds of text-specific information seeking tasks. We develop\nand compare several discriminative and generative multimodal LLMs that combine\neye movements and text for goal classification and goal reconstruction. Our\nexperiments show considerable success on both tasks, suggesting that LLMs can\nextract valuable information about the readers' text-specific goals from eye\nmovements.",
      "upvotes": 1,
      "discussionId": "681b03a43bf166767107c787",
      "ai_keywords": [
        "goal classification",
        "goal reconstruction",
        "discriminative models",
        "generative models",
        "multimodal LLMs",
        "large-scale eye tracking",
        "text-specific information seeking"
      ]
    },
    "publishedAt": "2025-05-04T09:23:48.000Z",
    "title": "Decoding Open-Ended Information Seeking Goals from Eye Movements in\n  Reading",
    "summary": "When reading, we often have specific information that interests us in a text.\nFor example, you might be reading this paper because you are curious about LLMs\nfor eye movements in reading, the experimental design, or perhaps you only care\nabout the question ``but does it work?''. More broadly, in daily life, people\napproach texts with any number of text-specific goals that guide their reading\nbehavior. In this work, we ask, for the first time, whether open-ended reading\ngoals can be automatically decoded from eye movements in reading. To address\nthis question, we introduce goal classification and goal reconstruction tasks\nand evaluation frameworks, and use large-scale eye tracking for reading data in\nEnglish with hundreds of text-specific information seeking tasks. We develop\nand compare several discriminative and generative multimodal LLMs that combine\neye movements and text for goal classification and goal reconstruction. Our\nexperiments show considerable success on both tasks, suggesting that LLMs can\nextract valuable information about the readers' text-specific goals from eye\nmovements.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02872.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60ef001bed64a34082bfa0dd",
      "avatarUrl": "/avatars/78e4daeac169edbf4dc42fbed9b50d59.svg",
      "fullname": "Omer Shubi",
      "name": "scaperex",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.18373",
      "authors": [
        {
          "_id": "681af7a58f41e99d8ca05074",
          "user": {
            "_id": "6454d2ee1a543cf97b1ba671",
            "avatarUrl": "/avatars/30edc30cf3684bf685aabffcec8a3c30.svg",
            "isPro": false,
            "fullname": "shen",
            "user": "lorashen",
            "type": "user"
          },
          "name": "Lei Shen",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-07T06:09:12.039Z",
          "hidden": false
        },
        {
          "_id": "681af7a58f41e99d8ca05075",
          "name": "Xiaoyu Shen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-25T14:17:47.000Z",
      "submittedOnDailyAt": "2025-05-07T04:49:41.738Z",
      "title": "Auto-SLURP: A Benchmark Dataset for Evaluating Multi-Agent Frameworks in\n  Smart Personal Assistant",
      "submittedOnDailyBy": {
        "_id": "6454d2ee1a543cf97b1ba671",
        "avatarUrl": "/avatars/30edc30cf3684bf685aabffcec8a3c30.svg",
        "isPro": false,
        "fullname": "shen",
        "user": "lorashen",
        "type": "user"
      },
      "summary": "In recent years, multi-agent frameworks powered by large language models\n(LLMs) have advanced rapidly. Despite this progress, there is still a notable\nabsence of benchmark datasets specifically tailored to evaluate their\nperformance. To bridge this gap, we introduce Auto-SLURP, a benchmark dataset\naimed at evaluating LLM-based multi-agent frameworks in the context of\nintelligent personal assistants. Auto-SLURP extends the original SLURP dataset\n-- initially developed for natural language understanding tasks -- by\nrelabeling the data and integrating simulated servers and external services.\nThis enhancement enables a comprehensive end-to-end evaluation pipeline,\ncovering language understanding, task execution, and response generation. Our\nexperiments demonstrate that Auto-SLURP presents a significant challenge for\ncurrent state-of-the-art frameworks, highlighting that truly reliable and\nintelligent multi-agent personal assistants remain a work in progress. The\ndataset and related code are available at\nhttps://github.com/lorashen/Auto-SLURP/.",
      "upvotes": 0,
      "discussionId": "681af7a68f41e99d8ca050ba",
      "githubRepo": "https://github.com/lorashen/Auto-SLURP/",
      "ai_keywords": [
        "multi-agent frameworks",
        "large language models (LLMs)",
        "benchmark datasets",
        "natural language understanding tasks",
        "simulated servers",
        "external services",
        "end-to-end evaluation pipeline",
        "language understanding",
        "task execution",
        "response generation"
      ]
    },
    "publishedAt": "2025-04-25T10:17:47.000Z",
    "title": "Auto-SLURP: A Benchmark Dataset for Evaluating Multi-Agent Frameworks in\n  Smart Personal Assistant",
    "summary": "In recent years, multi-agent frameworks powered by large language models\n(LLMs) have advanced rapidly. Despite this progress, there is still a notable\nabsence of benchmark datasets specifically tailored to evaluate their\nperformance. To bridge this gap, we introduce Auto-SLURP, a benchmark dataset\naimed at evaluating LLM-based multi-agent frameworks in the context of\nintelligent personal assistants. Auto-SLURP extends the original SLURP dataset\n-- initially developed for natural language understanding tasks -- by\nrelabeling the data and integrating simulated servers and external services.\nThis enhancement enables a comprehensive end-to-end evaluation pipeline,\ncovering language understanding, task execution, and response generation. Our\nexperiments demonstrate that Auto-SLURP presents a significant challenge for\ncurrent state-of-the-art frameworks, highlighting that truly reliable and\nintelligent multi-agent personal assistants remain a work in progress. The\ndataset and related code are available at\nhttps://github.com/lorashen/Auto-SLURP/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.18373.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6454d2ee1a543cf97b1ba671",
      "avatarUrl": "/avatars/30edc30cf3684bf685aabffcec8a3c30.svg",
      "fullname": "shen",
      "name": "lorashen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]