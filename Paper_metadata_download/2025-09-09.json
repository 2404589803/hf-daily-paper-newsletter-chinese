[
  {
    "paper": {
      "id": "2509.06160",
      "authors": [
        {
          "_id": "68bf936f207285de11b07b79",
          "name": "Haozhe Wang",
          "hidden": false
        },
        {
          "_id": "68bf936f207285de11b07b7a",
          "name": "Haoran Que",
          "hidden": false
        },
        {
          "_id": "68bf936f207285de11b07b7b",
          "name": "Qixin Xu",
          "hidden": false
        },
        {
          "_id": "68bf936f207285de11b07b7c",
          "name": "Minghao Liu",
          "hidden": false
        },
        {
          "_id": "68bf936f207285de11b07b7d",
          "name": "Wangchunshu Zhou",
          "hidden": false
        },
        {
          "_id": "68bf936f207285de11b07b7e",
          "name": "Jiazhan Feng",
          "hidden": false
        },
        {
          "_id": "68bf936f207285de11b07b7f",
          "name": "Wanjun Zhong",
          "hidden": false
        },
        {
          "_id": "68bf936f207285de11b07b80",
          "name": "Wei Ye",
          "hidden": false
        },
        {
          "_id": "68bf936f207285de11b07b81",
          "name": "Tong Yang",
          "hidden": false
        },
        {
          "_id": "68bf936f207285de11b07b82",
          "name": "Wenhao Huang",
          "hidden": false
        },
        {
          "_id": "68bf936f207285de11b07b83",
          "name": "Ge Zhang",
          "hidden": false
        },
        {
          "_id": "68bf936f207285de11b07b84",
          "name": "Fangzhen Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-07T18:07:58.000Z",
      "submittedOnDailyAt": "2025-09-09T01:10:01.643Z",
      "title": "Reverse-Engineered Reasoning for Open-Ended Generation",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "While the ``deep reasoning'' paradigm has spurred significant advances in\nverifiable domains like mathematics, its application to open-ended, creative\ngeneration remains a critical challenge. The two dominant methods for\ninstilling reasoning -- reinforcement learning (RL) and instruction\ndistillation -- falter in this area; RL struggles with the absence of clear\nreward signals and high-quality reward models, while distillation is\nprohibitively expensive and capped by the teacher model's capabilities. To\novercome these limitations, we introduce REverse-Engineered Reasoning (REER), a\nnew paradigm that fundamentally shifts the approach. Instead of building a\nreasoning process ``forwards'' through trial-and-error or imitation, REER works\n``backwards'' from known-good solutions to computationally discover the latent,\nstep-by-step deep reasoning process that could have produced them. Using this\nscalable, gradient-free approach, we curate and open-source DeepWriting-20K, a\nlarge-scale dataset of 20,000 deep reasoning trajectories for open-ended tasks.\nOur model, DeepWriter-8B, trained on this data, not only surpasses strong\nopen-source baselines but also achieves performance competitive with, and at\ntimes superior to, leading proprietary models like GPT-4o and Claude 3.5.",
      "upvotes": 62,
      "discussionId": "68bf9370207285de11b07b85",
      "projectPage": "https://m-a-p.ai/REER_DeepWriter/",
      "ai_summary": "REER, a new paradigm for deep reasoning, uses reverse engineering to discover step-by-step reasoning processes, enabling a model to perform competitively on open-ended tasks.",
      "ai_keywords": [
        "deep reasoning",
        "reinforcement learning",
        "instruction distillation",
        "REER",
        "gradient-free",
        "DeepWriting-20K",
        "DeepWriter-8B",
        "GPT-4o",
        "Claude 3.5"
      ]
    },
    "publishedAt": "2025-09-07T14:07:58.000Z",
    "title": "Reverse-Engineered Reasoning for Open-Ended Generation",
    "summary": "While the ``deep reasoning'' paradigm has spurred significant advances in\nverifiable domains like mathematics, its application to open-ended, creative\ngeneration remains a critical challenge. The two dominant methods for\ninstilling reasoning -- reinforcement learning (RL) and instruction\ndistillation -- falter in this area; RL struggles with the absence of clear\nreward signals and high-quality reward models, while distillation is\nprohibitively expensive and capped by the teacher model's capabilities. To\novercome these limitations, we introduce REverse-Engineered Reasoning (REER), a\nnew paradigm that fundamentally shifts the approach. Instead of building a\nreasoning process ``forwards'' through trial-and-error or imitation, REER works\n``backwards'' from known-good solutions to computationally discover the latent,\nstep-by-step deep reasoning process that could have produced them. Using this\nscalable, gradient-free approach, we curate and open-source DeepWriting-20K, a\nlarge-scale dataset of 20,000 deep reasoning trajectories for open-ended tasks.\nOur model, DeepWriter-8B, trained on this data, not only surpasses strong\nopen-source baselines but also achieves performance competitive with, and at\ntimes superior to, leading proprietary models like GPT-4o and Claude 3.5.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.06160.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 99
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.06501",
      "authors": [
        {
          "_id": "68bfb768207285de11b07d02",
          "name": "Junteng Liu",
          "hidden": false
        },
        {
          "_id": "68bfb768207285de11b07d03",
          "name": "Yunji Li",
          "hidden": false
        },
        {
          "_id": "68bfb768207285de11b07d04",
          "name": "Chi Zhang",
          "hidden": false
        },
        {
          "_id": "68bfb768207285de11b07d05",
          "name": "Jingyang Li",
          "hidden": false
        },
        {
          "_id": "68bfb768207285de11b07d06",
          "name": "Aili Chen",
          "hidden": false
        },
        {
          "_id": "68bfb768207285de11b07d07",
          "name": "Ke Ji",
          "hidden": false
        },
        {
          "_id": "68bfb768207285de11b07d08",
          "name": "Weiyu Cheng",
          "hidden": false
        },
        {
          "_id": "68bfb768207285de11b07d09",
          "name": "Zijia Wu",
          "hidden": false
        },
        {
          "_id": "68bfb768207285de11b07d0a",
          "name": "Chengyu Du",
          "hidden": false
        },
        {
          "_id": "68bfb768207285de11b07d0b",
          "name": "Qidi Xu",
          "hidden": false
        },
        {
          "_id": "68bfb768207285de11b07d0c",
          "name": "Jiayuan Song",
          "hidden": false
        },
        {
          "_id": "68bfb768207285de11b07d0d",
          "name": "Zhengmao Zhu",
          "hidden": false
        },
        {
          "_id": "68bfb768207285de11b07d0e",
          "name": "Wenhu Chen",
          "hidden": false
        },
        {
          "_id": "68bfb768207285de11b07d0f",
          "name": "Pengyu Zhao",
          "hidden": false
        },
        {
          "_id": "68bfb768207285de11b07d10",
          "name": "Junxian He",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-08T10:07:03.000Z",
      "submittedOnDailyAt": "2025-09-09T03:45:45.200Z",
      "title": "WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents",
      "submittedOnDailyBy": {
        "_id": "6493fbb3085e14d7933b936d",
        "avatarUrl": "/avatars/85723bedac9e81fecc33b36ff94ecada.svg",
        "isPro": false,
        "fullname": "Junteng Liu",
        "user": "Junteng",
        "type": "user"
      },
      "summary": "The paradigm of Large Language Models (LLMs) has increasingly shifted toward\nagentic applications, where web browsing capabilities are fundamental for\nretrieving information from diverse online sources. However, existing\nopen-source web agents either demonstrate limited information-seeking abilities\non complex tasks or lack transparent implementations. In this work, we identify\nthat the key challenge lies in the scarcity of challenging data for information\nseeking. To address this limitation, we introduce WebExplorer: a systematic\ndata generation approach using model-based exploration and iterative,\nlong-to-short query evolution. This method creates challenging query-answer\npairs that require multi-step reasoning and complex web navigation. By\nleveraging our curated high-quality dataset, we successfully develop advanced\nweb agent WebExplorer-8B through supervised fine-tuning followed by\nreinforcement learning. Our model supports 128K context length and up to 100\ntool calling turns, enabling long-horizon problem solving. Across diverse\ninformation-seeking benchmarks, WebExplorer-8B achieves the state-of-the-art\nperformance at its scale. Notably, as an 8B-sized model, WebExplorer-8B is able\nto effectively search over an average of 16 turns after RL training, achieving\nhigher accuracy than WebSailor-72B on BrowseComp-en/zh and attaining the best\nperformance among models up to 100B parameters on WebWalkerQA and FRAMES.\nBeyond these information-seeking tasks, our model also achieves strong\ngeneralization on the HLE benchmark even though it is only trained on\nknowledge-intensive QA data. These results highlight our approach as a\npractical path toward long-horizon web agents.",
      "upvotes": 38,
      "discussionId": "68bfb768207285de11b07d11",
      "ai_summary": "WebExplorer, a data-driven approach for developing advanced web agents, achieves state-of-the-art performance in information-seeking tasks through systematic data generation and reinforcement learning.",
      "ai_keywords": [
        "Large Language Models",
        "agentic applications",
        "web browsing capabilities",
        "information-seeking abilities",
        "model-based exploration",
        "iterative query evolution",
        "query-answer pairs",
        "multi-step reasoning",
        "complex web navigation",
        "supervised fine-tuning",
        "reinforcement learning",
        "context length",
        "tool calling turns",
        "long-horizon problem solving",
        "BrowseComp-en/zh",
        "WebWalkerQA",
        "FRAMES",
        "HLE benchmark"
      ]
    },
    "publishedAt": "2025-09-08T06:07:03.000Z",
    "title": "WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents",
    "summary": "The paradigm of Large Language Models (LLMs) has increasingly shifted toward\nagentic applications, where web browsing capabilities are fundamental for\nretrieving information from diverse online sources. However, existing\nopen-source web agents either demonstrate limited information-seeking abilities\non complex tasks or lack transparent implementations. In this work, we identify\nthat the key challenge lies in the scarcity of challenging data for information\nseeking. To address this limitation, we introduce WebExplorer: a systematic\ndata generation approach using model-based exploration and iterative,\nlong-to-short query evolution. This method creates challenging query-answer\npairs that require multi-step reasoning and complex web navigation. By\nleveraging our curated high-quality dataset, we successfully develop advanced\nweb agent WebExplorer-8B through supervised fine-tuning followed by\nreinforcement learning. Our model supports 128K context length and up to 100\ntool calling turns, enabling long-horizon problem solving. Across diverse\ninformation-seeking benchmarks, WebExplorer-8B achieves the state-of-the-art\nperformance at its scale. Notably, as an 8B-sized model, WebExplorer-8B is able\nto effectively search over an average of 16 turns after RL training, achieving\nhigher accuracy than WebSailor-72B on BrowseComp-en/zh and attaining the best\nperformance among models up to 100B parameters on WebWalkerQA and FRAMES.\nBeyond these information-seeking tasks, our model also achieves strong\ngeneralization on the HLE benchmark even though it is only trained on\nknowledge-intensive QA data. These results highlight our approach as a\npractical path toward long-horizon web agents.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.06501.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6493fbb3085e14d7933b936d",
      "avatarUrl": "/avatars/85723bedac9e81fecc33b36ff94ecada.svg",
      "fullname": "Junteng Liu",
      "name": "Junteng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.06949",
      "authors": [
        {
          "_id": "68bf87ff207285de11b07b39",
          "name": "Yinjie Wang",
          "hidden": false
        },
        {
          "_id": "68bf87ff207285de11b07b3a",
          "name": "Ling Yang",
          "hidden": false
        },
        {
          "_id": "68bf87ff207285de11b07b3b",
          "name": "Bowen Li",
          "hidden": false
        },
        {
          "_id": "68bf87ff207285de11b07b3c",
          "name": "Ye Tian",
          "hidden": false
        },
        {
          "_id": "68bf87ff207285de11b07b3d",
          "name": "Ke Shen",
          "hidden": false
        },
        {
          "_id": "68bf87ff207285de11b07b3e",
          "name": "Mengdi Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-08T17:58:06.000Z",
      "submittedOnDailyAt": "2025-09-09T00:25:38.536Z",
      "title": "Revolutionizing Reinforcement Learning Framework for Diffusion Large\n  Language Models",
      "submittedOnDailyBy": {
        "_id": "64fde4e252e82dd432b74ce9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64fde4e252e82dd432b74ce9/-CQZbBP7FsPPyawYrsi4z.jpeg",
        "isPro": false,
        "fullname": "Ling Yang",
        "user": "Lingaaaaaaa",
        "type": "user"
      },
      "summary": "We propose TraceRL, a trajectory-aware reinforcement learning framework for\ndiffusion language models (DLMs) that incorporates preferred inference\ntrajectory into post-training, and is applicable across different\narchitectures. Equipped with a diffusion-based value model that enhances\ntraining stability, we demonstrate improved reasoning performance on complex\nmath and coding tasks. Besides, it can also be applied to adapt block-specific\nmodels to larger blocks, which improves sampling flexibility. Employing\nTraceRL, we derive a series of state-of-the-art diffusion language models,\nnamely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still\nconsistently outperforms them across complex math reasoning tasks.\nTraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over\nQwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical\nreasoning benchmarks. Through curriculum learning, we also derive the first\nlong-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1%\nrelative accuracy gain. To facilitate reproducible research and practical\napplications, we release a comprehensive open-source framework for building,\ntraining, and deploying diffusion LLMs across diverse architectures. The\nframework integrates accelerated KV-cache techniques and inference engines for\nboth inference and reinforcement learning, and includes implementations of\nvarious supervised fine-tuning and RL methods for mathematics, coding, and\ngeneral tasks. Code and Models: https://github.com/Gen-Verse/dLLM-RL",
      "upvotes": 20,
      "discussionId": "68bf87ff207285de11b07b3f",
      "projectPage": "https://huggingface.co/collections/Gen-Verse/trado-series-68beb6cd6a26c27cde9fe3af",
      "githubRepo": "https://github.com/Gen-Verse/dLLM-RL",
      "ai_summary": "TraceRL enhances diffusion language models with trajectory-aware reinforcement learning, improving reasoning performance on complex tasks and enabling flexible sampling.",
      "ai_keywords": [
        "trajectory-aware reinforcement learning",
        "diffusion language models",
        "diffusion-based value model",
        "sampling flexibility",
        "curriculum learning",
        "long-CoT DLM",
        "KV-cache techniques",
        "inference engines",
        "supervised fine-tuning",
        "RL methods"
      ],
      "githubStars": 22
    },
    "publishedAt": "2025-09-08T13:58:06.000Z",
    "title": "Revolutionizing Reinforcement Learning Framework for Diffusion Large\n  Language Models",
    "summary": "We propose TraceRL, a trajectory-aware reinforcement learning framework for\ndiffusion language models (DLMs) that incorporates preferred inference\ntrajectory into post-training, and is applicable across different\narchitectures. Equipped with a diffusion-based value model that enhances\ntraining stability, we demonstrate improved reasoning performance on complex\nmath and coding tasks. Besides, it can also be applied to adapt block-specific\nmodels to larger blocks, which improves sampling flexibility. Employing\nTraceRL, we derive a series of state-of-the-art diffusion language models,\nnamely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still\nconsistently outperforms them across complex math reasoning tasks.\nTraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over\nQwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical\nreasoning benchmarks. Through curriculum learning, we also derive the first\nlong-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1%\nrelative accuracy gain. To facilitate reproducible research and practical\napplications, we release a comprehensive open-source framework for building,\ntraining, and deploying diffusion LLMs across diverse architectures. The\nframework integrates accelerated KV-cache techniques and inference engines for\nboth inference and reinforcement learning, and includes implementations of\nvarious supervised fine-tuning and RL methods for mathematics, coding, and\ngeneral tasks. Code and Models: https://github.com/Gen-Verse/dLLM-RL",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.06949.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "64fde4e252e82dd432b74ce9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64fde4e252e82dd432b74ce9/-CQZbBP7FsPPyawYrsi4z.jpeg",
      "fullname": "Ling Yang",
      "name": "Lingaaaaaaa",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.01656",
      "authors": [
        {
          "_id": "68beda55c123124955ef6267",
          "name": "Zetong Zhou",
          "hidden": false
        },
        {
          "_id": "68beda55c123124955ef6268",
          "name": "Dongping Chen",
          "hidden": false
        },
        {
          "_id": "68beda55c123124955ef6269",
          "name": "Zixian Ma",
          "hidden": false
        },
        {
          "_id": "68beda55c123124955ef626a",
          "name": "Zhihan Hu",
          "hidden": false
        },
        {
          "_id": "68beda55c123124955ef626b",
          "name": "Mingyang Fu",
          "hidden": false
        },
        {
          "_id": "68beda55c123124955ef626c",
          "name": "Sinan Wang",
          "hidden": false
        },
        {
          "_id": "68beda55c123124955ef626d",
          "name": "Yao Wan",
          "hidden": false
        },
        {
          "_id": "68beda55c123124955ef626e",
          "name": "Zhou Zhao",
          "hidden": false
        },
        {
          "_id": "68beda55c123124955ef626f",
          "name": "Ranjay Krishna",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-01T17:57:49.000Z",
      "submittedOnDailyAt": "2025-09-09T00:10:59.899Z",
      "title": "Reinforced Visual Perception with Tools",
      "submittedOnDailyBy": {
        "_id": "643be8879f5d314db2d9ed23",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643be8879f5d314db2d9ed23/VrW2UtJ7ppOnGIYjTWd7b.png",
        "isPro": false,
        "fullname": "Chen Dongping",
        "user": "shuaishuaicdp",
        "type": "user"
      },
      "summary": "Visual reasoning, a cornerstone of human intelligence, encompasses complex\nperceptual and logical processes essential for solving diverse visual problems.\nWhile advances in computer vision have produced powerful models for various\nperceptual tasks, leveraging these for general visual reasoning remains\nchallenging. Prior work demonstrates that augmenting LLMs with vision models\nvia supervised finetuning improves performance, but faces key limitations such\nas expensive data generation, reliance on careful data filtering, and poor\ngeneralization. To address these issues, we propose ReVPT to enhance\nmulti-modal LLMs' abilities to reason about and use visual tools through\nreinforcement learning. We introduce a novel RL algorithm based on GRPO,\ndesigned to train models to reason with a suite of four visual tools. Through\nextensive experiments, we show that our method achieves state-of-the-art\nperformance on several perception-heavy benchmarks, including SAT, CV-Bench,\nBLINK and MMStar, significantly outperforming the supervised and text-based RL\nfinetuning baselines. Notably, Our ReVPT-3B and ReVPT-7B outperform the\ninstruct models by 9.03% and 9.44% on CV-Bench. Finally, we bring to the\ncommunity new insights on RL-based visual tool-usage through extensive\nablations. Our code is available at https://github.com/ls-kelvin/REVPT.",
      "upvotes": 15,
      "discussionId": "68beda55c123124955ef6270",
      "githubRepo": "https://github.com/ls-kelvin/REVPT",
      "ai_summary": "ReVPT enhances multi-modal LLMs' visual reasoning capabilities using reinforcement learning, achieving state-of-the-art performance on visual benchmarks.",
      "ai_keywords": [
        "LLMs",
        "vision models",
        "supervised finetuning",
        "reinforcement learning",
        "GRPO",
        "visual tools",
        "SAT",
        "CV-Bench",
        "BLINK",
        "MMStar",
        "instruct models"
      ],
      "githubStars": 20
    },
    "publishedAt": "2025-09-01T13:57:49.000Z",
    "title": "Reinforced Visual Perception with Tools",
    "summary": "Visual reasoning, a cornerstone of human intelligence, encompasses complex\nperceptual and logical processes essential for solving diverse visual problems.\nWhile advances in computer vision have produced powerful models for various\nperceptual tasks, leveraging these for general visual reasoning remains\nchallenging. Prior work demonstrates that augmenting LLMs with vision models\nvia supervised finetuning improves performance, but faces key limitations such\nas expensive data generation, reliance on careful data filtering, and poor\ngeneralization. To address these issues, we propose ReVPT to enhance\nmulti-modal LLMs' abilities to reason about and use visual tools through\nreinforcement learning. We introduce a novel RL algorithm based on GRPO,\ndesigned to train models to reason with a suite of four visual tools. Through\nextensive experiments, we show that our method achieves state-of-the-art\nperformance on several perception-heavy benchmarks, including SAT, CV-Bench,\nBLINK and MMStar, significantly outperforming the supervised and text-based RL\nfinetuning baselines. Notably, Our ReVPT-3B and ReVPT-7B outperform the\ninstruct models by 9.03% and 9.44% on CV-Bench. Finally, we bring to the\ncommunity new insights on RL-based visual tool-usage through extensive\nablations. Our code is available at https://github.com/ls-kelvin/REVPT.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.01656.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643be8879f5d314db2d9ed23",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643be8879f5d314db2d9ed23/VrW2UtJ7ppOnGIYjTWd7b.png",
      "fullname": "Chen Dongping",
      "name": "shuaishuaicdp",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.06461",
      "authors": [
        {
          "_id": "68bfa200207285de11b07beb",
          "name": "Yuyao Ge",
          "hidden": false
        },
        {
          "_id": "68bfa200207285de11b07bec",
          "name": "Shenghua Liu",
          "hidden": false
        },
        {
          "_id": "68bfa200207285de11b07bed",
          "name": "Yiwei Wang",
          "hidden": false
        },
        {
          "_id": "68bfa200207285de11b07bee",
          "name": "Lingrui Mei",
          "hidden": false
        },
        {
          "_id": "68bfa200207285de11b07bef",
          "name": "Baolong Bi",
          "hidden": false
        },
        {
          "_id": "68bfa200207285de11b07bf0",
          "name": "Xuanshan Zhou",
          "hidden": false
        },
        {
          "_id": "68bfa200207285de11b07bf1",
          "name": "Jiayu Yao",
          "hidden": false
        },
        {
          "_id": "68bfa200207285de11b07bf2",
          "name": "Jiafeng Guo",
          "hidden": false
        },
        {
          "_id": "68bfa200207285de11b07bf3",
          "name": "Xueqi Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-08T09:20:04.000Z",
      "submittedOnDailyAt": "2025-09-09T02:36:55.458Z",
      "title": "Focusing by Contrastive Attention: Enhancing VLMs' Visual Reasoning",
      "submittedOnDailyBy": {
        "_id": "656ad93853703dd78f3de7b8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656ad93853703dd78f3de7b8/r6VB6ICND1td3wFOy8pnz.jpeg",
        "isPro": false,
        "fullname": "YuyaoGe",
        "user": "YuyaoGe",
        "type": "user"
      },
      "summary": "Vision-Language Models (VLMs) have demonstrated remarkable success across\ndiverse visual tasks, yet their performance degrades in complex visual\nenvironments. While existing enhancement approaches require additional\ntraining, rely on external segmentation tools, or operate at coarse-grained\nlevels, they overlook the innate ability within VLMs. To bridge this gap, we\ninvestigate VLMs' attention patterns and discover that: (1) visual complexity\nstrongly correlates with attention entropy, negatively impacting reasoning\nperformance; (2) attention progressively refines from global scanning in\nshallow layers to focused convergence in deeper layers, with convergence degree\ndetermined by visual complexity. (3) Theoretically, we prove that the contrast\nof attention maps between general queries and task-specific queries enables the\ndecomposition of visual signal into semantic signals and visual noise\ncomponents. Building on these insights, we propose Contrastive Attention\nRefinement for Visual Enhancement (CARVE), a training-free method that extracts\ntask-relevant visual signals through attention contrasting at the pixel level.\nExtensive experiments demonstrate that CARVE consistently enhances performance,\nachieving up to 75% improvement on open-source models. Our work provides\ncritical insights into the interplay between visual complexity and attention\nmechanisms, offering an efficient pathway for improving visual reasoning with\ncontrasting attention.",
      "upvotes": 12,
      "discussionId": "68bfa201207285de11b07bf4",
      "ai_summary": "Contrastive Attention Refinement for Visual Enhancement (CARVE) improves VLM performance by extracting task-relevant visual signals through attention contrasting, addressing issues with visual complexity and attention mechanisms.",
      "ai_keywords": [
        "Vision-Language Models",
        "attention patterns",
        "attention entropy",
        "reasoning performance",
        "attention maps",
        "general queries",
        "task-specific queries",
        "semantic signals",
        "visual noise",
        "Contrastive Attention Refinement",
        "CARVE",
        "visual enhancement",
        "visual complexity",
        "attention mechanisms",
        "visual reasoning"
      ]
    },
    "publishedAt": "2025-09-08T05:20:04.000Z",
    "title": "Focusing by Contrastive Attention: Enhancing VLMs' Visual Reasoning",
    "summary": "Vision-Language Models (VLMs) have demonstrated remarkable success across\ndiverse visual tasks, yet their performance degrades in complex visual\nenvironments. While existing enhancement approaches require additional\ntraining, rely on external segmentation tools, or operate at coarse-grained\nlevels, they overlook the innate ability within VLMs. To bridge this gap, we\ninvestigate VLMs' attention patterns and discover that: (1) visual complexity\nstrongly correlates with attention entropy, negatively impacting reasoning\nperformance; (2) attention progressively refines from global scanning in\nshallow layers to focused convergence in deeper layers, with convergence degree\ndetermined by visual complexity. (3) Theoretically, we prove that the contrast\nof attention maps between general queries and task-specific queries enables the\ndecomposition of visual signal into semantic signals and visual noise\ncomponents. Building on these insights, we propose Contrastive Attention\nRefinement for Visual Enhancement (CARVE), a training-free method that extracts\ntask-relevant visual signals through attention contrasting at the pixel level.\nExtensive experiments demonstrate that CARVE consistently enhances performance,\nachieving up to 75% improvement on open-source models. Our work provides\ncritical insights into the interplay between visual complexity and attention\nmechanisms, offering an efficient pathway for improving visual reasoning with\ncontrasting attention.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.06461.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "656ad93853703dd78f3de7b8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656ad93853703dd78f3de7b8/r6VB6ICND1td3wFOy8pnz.jpeg",
      "fullname": "YuyaoGe",
      "name": "YuyaoGe",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.06733",
      "authors": [
        {
          "_id": "68bf8ec6207285de11b07b51",
          "name": "Wenjun Li",
          "hidden": false
        },
        {
          "_id": "68bf8ec6207285de11b07b52",
          "name": "Zhi Chen",
          "hidden": false
        },
        {
          "_id": "68bf8ec6207285de11b07b53",
          "name": "Jingru Lin",
          "hidden": false
        },
        {
          "_id": "68bf8ec6207285de11b07b54",
          "name": "Hannan Cao",
          "hidden": false
        },
        {
          "_id": "68bf8ec6207285de11b07b55",
          "name": "Wei Han",
          "hidden": false
        },
        {
          "_id": "68bf8ec6207285de11b07b56",
          "name": "Sheng Liang",
          "hidden": false
        },
        {
          "_id": "68bf8ec6207285de11b07b57",
          "name": "Zhi Zhang",
          "hidden": false
        },
        {
          "_id": "68bf8ec6207285de11b07b58",
          "name": "Kuicai Dong",
          "hidden": false
        },
        {
          "_id": "68bf8ec6207285de11b07b59",
          "name": "Dexun Li",
          "hidden": false
        },
        {
          "_id": "68bf8ec6207285de11b07b5a",
          "name": "Chen Zhang",
          "hidden": false
        },
        {
          "_id": "68bf8ec6207285de11b07b5b",
          "name": "Yong Liu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6622245224f3842a31f7c58a/oCfHkt6BkeTDm98jxWtNm.png"
      ],
      "publishedAt": "2025-09-08T14:27:23.000Z",
      "submittedOnDailyAt": "2025-09-09T00:53:51.403Z",
      "title": "Reinforcement Learning Foundations for Deep Research Systems: A Survey",
      "submittedOnDailyBy": {
        "_id": "6622245224f3842a31f7c58a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6622245224f3842a31f7c58a/EcvwtHamYwMdzRT9pevdE.jpeg",
        "isPro": false,
        "fullname": "wenjun",
        "user": "wenjun-li",
        "type": "user"
      },
      "summary": "Deep research systems, agentic AI that solve complex, multi-step tasks by\ncoordinating reasoning, search across the open web and user files, and tool\nuse, are moving toward hierarchical deployments with a Planner, Coordinator,\nand Executors. In practice, training entire stacks end-to-end remains\nimpractical, so most work trains a single planner connected to core tools such\nas search, browsing, and code. While SFT imparts protocol fidelity, it suffers\nfrom imitation and exposure biases and underuses environment feedback.\nPreference alignment methods such as DPO are schema and proxy-dependent,\noff-policy, and weak for long-horizon credit assignment and multi-objective\ntrade-offs. A further limitation of SFT and DPO is their reliance on human\ndefined decision points and subskills through schema design and labeled\ncomparisons. Reinforcement learning aligns with closed-loop, tool-interaction\nresearch by optimizing trajectory-level policies, enabling exploration,\nrecovery behaviors, and principled credit assignment, and it reduces dependence\non such human priors and rater biases.\n  This survey is, to our knowledge, the first dedicated to the RL foundations\nof deep research systems. It systematizes work after DeepSeek-R1 along three\naxes: (i) data synthesis and curation; (ii) RL methods for agentic research\ncovering stability, sample efficiency, long context handling, reward and credit\ndesign, multi-objective optimization, and multimodal integration; and (iii)\nagentic RL training systems and frameworks. We also cover agent architecture\nand coordination, as well as evaluation and benchmarks, including recent QA,\nVQA, long-form synthesis, and domain-grounded, tool-interaction tasks. We\ndistill recurring patterns, surface infrastructure bottlenecks, and offer\npractical guidance for training robust, transparent deep research agents with\nRL.",
      "upvotes": 11,
      "discussionId": "68bf8ec6207285de11b07b5c",
      "githubRepo": "https://github.com/wenjunli-0/deepresearch-survey",
      "ai_summary": "Reinforcement learning is explored as a foundational approach for training deep research systems, addressing limitations of supervised and preference alignment methods by optimizing policies for tool interaction and exploration.",
      "ai_keywords": [
        "Reinforcement learning",
        "trajectory-level policies",
        "exploration",
        "recovery behaviors",
        "credit assignment",
        "multi-objective optimization",
        "multimodal integration",
        "agent architecture",
        "coordination",
        "evaluation",
        "benchmarks",
        "QA",
        "VQA",
        "long-form synthesis",
        "domain-grounded tasks"
      ],
      "githubStars": 9
    },
    "publishedAt": "2025-09-08T10:27:23.000Z",
    "title": "Reinforcement Learning Foundations for Deep Research Systems: A Survey",
    "summary": "Deep research systems, agentic AI that solve complex, multi-step tasks by\ncoordinating reasoning, search across the open web and user files, and tool\nuse, are moving toward hierarchical deployments with a Planner, Coordinator,\nand Executors. In practice, training entire stacks end-to-end remains\nimpractical, so most work trains a single planner connected to core tools such\nas search, browsing, and code. While SFT imparts protocol fidelity, it suffers\nfrom imitation and exposure biases and underuses environment feedback.\nPreference alignment methods such as DPO are schema and proxy-dependent,\noff-policy, and weak for long-horizon credit assignment and multi-objective\ntrade-offs. A further limitation of SFT and DPO is their reliance on human\ndefined decision points and subskills through schema design and labeled\ncomparisons. Reinforcement learning aligns with closed-loop, tool-interaction\nresearch by optimizing trajectory-level policies, enabling exploration,\nrecovery behaviors, and principled credit assignment, and it reduces dependence\non such human priors and rater biases.\n  This survey is, to our knowledge, the first dedicated to the RL foundations\nof deep research systems. It systematizes work after DeepSeek-R1 along three\naxes: (i) data synthesis and curation; (ii) RL methods for agentic research\ncovering stability, sample efficiency, long context handling, reward and credit\ndesign, multi-objective optimization, and multimodal integration; and (iii)\nagentic RL training systems and frameworks. We also cover agent architecture\nand coordination, as well as evaluation and benchmarks, including recent QA,\nVQA, long-form synthesis, and domain-grounded, tool-interaction tasks. We\ndistill recurring patterns, surface infrastructure bottlenecks, and offer\npractical guidance for training robust, transparent deep research agents with\nRL.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6622245224f3842a31f7c58a/oCfHkt6BkeTDm98jxWtNm.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.06733.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6622245224f3842a31f7c58a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6622245224f3842a31f7c58a/EcvwtHamYwMdzRT9pevdE.jpeg",
      "fullname": "wenjun",
      "name": "wenjun-li",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.06467",
      "authors": [
        {
          "_id": "68bf97aa207285de11b07ba5",
          "name": "Che Liu",
          "hidden": false
        },
        {
          "_id": "68bf97aa207285de11b07ba6",
          "name": "Yinda Chen",
          "hidden": false
        },
        {
          "_id": "68bf97aa207285de11b07ba7",
          "name": "Haoyuan Shi",
          "hidden": false
        },
        {
          "_id": "68bf97aa207285de11b07ba8",
          "name": "Jinpeng Lu",
          "hidden": false
        },
        {
          "_id": "68bf97aa207285de11b07ba9",
          "name": "Bailiang Jian",
          "hidden": false
        },
        {
          "_id": "68bf97aa207285de11b07baa",
          "name": "Jiazhen Pan",
          "hidden": false
        },
        {
          "_id": "68bf97aa207285de11b07bab",
          "name": "Linghan Cai",
          "hidden": false
        },
        {
          "_id": "68bf97aa207285de11b07bac",
          "name": "Jiayi Wang",
          "hidden": false
        },
        {
          "_id": "68bf97aa207285de11b07bad",
          "name": "Yundi Zhang",
          "hidden": false
        },
        {
          "_id": "68bf97aa207285de11b07bae",
          "name": "Jun Li",
          "hidden": false
        },
        {
          "_id": "68bf97aa207285de11b07baf",
          "name": "Cosmin I. Bercea",
          "hidden": false
        },
        {
          "_id": "68bf97aa207285de11b07bb0",
          "name": "Cheng Ouyang",
          "hidden": false
        },
        {
          "_id": "68bf97aa207285de11b07bb1",
          "name": "Chen Chen",
          "hidden": false
        },
        {
          "_id": "68bf97aa207285de11b07bb2",
          "name": "Zhiwei Xiong",
          "hidden": false
        },
        {
          "_id": "68bf97aa207285de11b07bb3",
          "name": "Benedikt Wiestler",
          "hidden": false
        },
        {
          "_id": "68bf97aa207285de11b07bb4",
          "name": "Christian Wachinger",
          "hidden": false
        },
        {
          "_id": "68bf97aa207285de11b07bb5",
          "name": "Daniel Rueckert",
          "hidden": false
        },
        {
          "_id": "68bf97aa207285de11b07bb6",
          "name": "Wenjia Bai",
          "hidden": false
        },
        {
          "_id": "68bf97aa207285de11b07bb7",
          "name": "Rossella Arcucci",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-08T09:28:57.000Z",
      "submittedOnDailyAt": "2025-09-09T01:28:11.725Z",
      "title": "Does DINOv3 Set a New Medical Vision Standard?",
      "submittedOnDailyBy": {
        "_id": "631b9ff5824f2502e3557c7e",
        "avatarUrl": "/avatars/076043c9dba07644a570692563ef8114.svg",
        "isPro": true,
        "fullname": "liu",
        "user": "che111",
        "type": "user"
      },
      "summary": "The advent of large-scale vision foundation models, pre-trained on diverse\nnatural images, has marked a paradigm shift in computer vision. However, how\nthe frontier vision foundation models' efficacies transfer to specialized\ndomains remains such as medical imaging remains an open question. This report\ninvestigates whether DINOv3, a state-of-the-art self-supervised vision\ntransformer (ViT) that features strong capability in dense prediction tasks,\ncan directly serve as a powerful, unified encoder for medical vision tasks\nwithout domain-specific pre-training. To answer this, we benchmark DINOv3\nacross common medical vision tasks, including 2D/3D classification and\nsegmentation on a wide range of medical imaging modalities. We systematically\nanalyze its scalability by varying model sizes and input image resolutions. Our\nfindings reveal that DINOv3 shows impressive performance and establishes a\nformidable new baseline. Remarkably, it can even outperform medical-specific\nfoundation models like BiomedCLIP and CT-Net on several tasks, despite being\ntrained solely on natural images. However, we identify clear limitations: The\nmodel's features degrade in scenarios requiring deep domain specialization,\nsuch as in Whole-Slide Pathological Images (WSIs), Electron Microscopy (EM),\nand Positron Emission Tomography (PET). Furthermore, we observe that DINOv3\ndoes not consistently obey scaling law in the medical domain; performance does\nnot reliably increase with larger models or finer feature resolutions, showing\ndiverse scaling behaviors across tasks. Ultimately, our work establishes DINOv3\nas a strong baseline, whose powerful visual features can serve as a robust\nprior for multiple complex medical tasks. This opens promising future\ndirections, such as leveraging its features to enforce multiview consistency in\n3D reconstruction.",
      "upvotes": 11,
      "discussionId": "68bf97ab207285de11b07bb8",
      "ai_summary": "DINOv3, a self-supervised vision transformer, demonstrates strong performance across various medical vision tasks without domain-specific pre-training, though it shows limitations in deeply specialized domains and does not consistently follow scaling laws in the medical domain.",
      "ai_keywords": [
        "self-supervised vision transformer",
        "ViT",
        "dense prediction tasks",
        "medical vision tasks",
        "2D/3D classification",
        "segmentation",
        "medical imaging modalities",
        "scalability",
        "model sizes",
        "input image resolutions",
        "BiomedCLIP",
        "CT-Net",
        "Whole-Slide Pathological Images",
        "Electron Microscopy",
        "Positron Emission Tomography",
        "scaling law",
        "multiview consistency",
        "3D reconstruction"
      ]
    },
    "publishedAt": "2025-09-08T05:28:57.000Z",
    "title": "Does DINOv3 Set a New Medical Vision Standard?",
    "summary": "The advent of large-scale vision foundation models, pre-trained on diverse\nnatural images, has marked a paradigm shift in computer vision. However, how\nthe frontier vision foundation models' efficacies transfer to specialized\ndomains remains such as medical imaging remains an open question. This report\ninvestigates whether DINOv3, a state-of-the-art self-supervised vision\ntransformer (ViT) that features strong capability in dense prediction tasks,\ncan directly serve as a powerful, unified encoder for medical vision tasks\nwithout domain-specific pre-training. To answer this, we benchmark DINOv3\nacross common medical vision tasks, including 2D/3D classification and\nsegmentation on a wide range of medical imaging modalities. We systematically\nanalyze its scalability by varying model sizes and input image resolutions. Our\nfindings reveal that DINOv3 shows impressive performance and establishes a\nformidable new baseline. Remarkably, it can even outperform medical-specific\nfoundation models like BiomedCLIP and CT-Net on several tasks, despite being\ntrained solely on natural images. However, we identify clear limitations: The\nmodel's features degrade in scenarios requiring deep domain specialization,\nsuch as in Whole-Slide Pathological Images (WSIs), Electron Microscopy (EM),\nand Positron Emission Tomography (PET). Furthermore, we observe that DINOv3\ndoes not consistently obey scaling law in the medical domain; performance does\nnot reliably increase with larger models or finer feature resolutions, showing\ndiverse scaling behaviors across tasks. Ultimately, our work establishes DINOv3\nas a strong baseline, whose powerful visual features can serve as a robust\nprior for multiple complex medical tasks. This opens promising future\ndirections, such as leveraging its features to enforce multiview consistency in\n3D reconstruction.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.06467.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "631b9ff5824f2502e3557c7e",
      "avatarUrl": "/avatars/076043c9dba07644a570692563ef8114.svg",
      "fullname": "liu",
      "name": "che111",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.06155",
      "authors": [
        {
          "_id": "68bf827d207285de11b07b2d",
          "name": "Duomin Wang",
          "hidden": false
        },
        {
          "_id": "68bf827d207285de11b07b2e",
          "name": "Wei Zuo",
          "hidden": false
        },
        {
          "_id": "68bf827d207285de11b07b2f",
          "name": "Aojie Li",
          "hidden": false
        },
        {
          "_id": "68bf827d207285de11b07b30",
          "name": "Ling-Hao Chen",
          "hidden": false
        },
        {
          "_id": "68bf827d207285de11b07b31",
          "name": "Xinyao Liao",
          "hidden": false
        },
        {
          "_id": "68bf827d207285de11b07b32",
          "name": "Deyu Zhou",
          "hidden": false
        },
        {
          "_id": "68bf827d207285de11b07b33",
          "name": "Zixin Yin",
          "hidden": false
        },
        {
          "_id": "68bf827d207285de11b07b34",
          "name": "Xili Dai",
          "hidden": false
        },
        {
          "_id": "68bf827d207285de11b07b35",
          "name": "Daxin Jiang",
          "hidden": false
        },
        {
          "_id": "68bf827d207285de11b07b36",
          "name": "Gang Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-07T17:55:03.000Z",
      "submittedOnDailyAt": "2025-09-09T00:08:23.306Z",
      "title": "UniVerse-1: Unified Audio-Video Generation via Stitching of Experts",
      "submittedOnDailyBy": {
        "_id": "64ae9b88a22a179fc4d07992",
        "avatarUrl": "/avatars/c9065f04a1188ea3129e56a90328ffd3.svg",
        "isPro": false,
        "fullname": "wang",
        "user": "dorni",
        "type": "user"
      },
      "summary": "We introduce UniVerse-1, a unified, Veo-3-like model capable of\nsimultaneously generating coordinated audio and video. To enhance training\nefficiency, we bypass training from scratch and instead employ a stitching of\nexperts (SoE) technique. This approach deeply fuses the corresponding blocks of\npre-trained video and music generation experts models, thereby fully leveraging\ntheir foundational capabilities. To ensure accurate annotations and temporal\nalignment for both ambient sounds and speech with video content, we developed\nan online annotation pipeline that processes the required training data and\ngenerates labels during training process. This strategy circumvents the\nperformance degradation often caused by misalignment text-based annotations.\nThrough the synergy of these techniques, our model, after being finetuned on\napproximately 7,600 hours of audio-video data, produces results with\nwell-coordinated audio-visuals for ambient sounds generation and strong\nalignment for speech generation. To systematically evaluate our proposed\nmethod, we introduce Verse-Bench, a new benchmark dataset. In an effort to\nadvance research in audio-video generation and to close the performance gap\nwith state-of-the-art models such as Veo3, we make our model and code publicly\navailable. We hope this contribution will benefit the broader research\ncommunity. Project page: https://dorniwang.github.io/UniVerse-1/.",
      "upvotes": 7,
      "discussionId": "68bf827d207285de11b07b37",
      "projectPage": "https://dorniwang.github.io/UniVerse-1/",
      "githubRepo": "https://github.com/Dorniwang/UniVerse-1-code/",
      "ai_summary": "UniVerse-1, a unified audio-video generation model, uses a stitching of experts technique to combine pre-trained video and music models, ensuring accurate temporal alignment and producing high-quality audio-visual outputs.",
      "ai_keywords": [
        "unified model",
        "Veo-3-like model",
        "stitching of experts (SoE)",
        "pre-trained video generation",
        "pre-trained music generation",
        "online annotation pipeline",
        "temporal alignment",
        "ambient sounds",
        "speech generation",
        "Verse-Bench",
        "audio-video generation"
      ],
      "githubStars": 13
    },
    "publishedAt": "2025-09-07T13:55:03.000Z",
    "title": "UniVerse-1: Unified Audio-Video Generation via Stitching of Experts",
    "summary": "We introduce UniVerse-1, a unified, Veo-3-like model capable of\nsimultaneously generating coordinated audio and video. To enhance training\nefficiency, we bypass training from scratch and instead employ a stitching of\nexperts (SoE) technique. This approach deeply fuses the corresponding blocks of\npre-trained video and music generation experts models, thereby fully leveraging\ntheir foundational capabilities. To ensure accurate annotations and temporal\nalignment for both ambient sounds and speech with video content, we developed\nan online annotation pipeline that processes the required training data and\ngenerates labels during training process. This strategy circumvents the\nperformance degradation often caused by misalignment text-based annotations.\nThrough the synergy of these techniques, our model, after being finetuned on\napproximately 7,600 hours of audio-video data, produces results with\nwell-coordinated audio-visuals for ambient sounds generation and strong\nalignment for speech generation. To systematically evaluate our proposed\nmethod, we introduce Verse-Bench, a new benchmark dataset. In an effort to\nadvance research in audio-video generation and to close the performance gap\nwith state-of-the-art models such as Veo3, we make our model and code publicly\navailable. We hope this contribution will benefit the broader research\ncommunity. Project page: https://dorniwang.github.io/UniVerse-1/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.06155.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ae9b88a22a179fc4d07992",
      "avatarUrl": "/avatars/c9065f04a1188ea3129e56a90328ffd3.svg",
      "fullname": "wang",
      "name": "dorni",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.03516",
      "authors": [
        {
          "_id": "68b93427d43cadaf7a688bd1",
          "user": {
            "_id": "666c06ca9ed9e91df03e7e27",
            "avatarUrl": "/avatars/3101ad4f1a3a243b75080d06c5e59e9c.svg",
            "isPro": false,
            "fullname": "Ouxiang Li",
            "user": "lioooox",
            "type": "user"
          },
          "name": "Ouxiang Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-04T08:42:54.336Z",
          "hidden": false
        },
        {
          "_id": "68b93427d43cadaf7a688bd2",
          "name": "Yuan Wang",
          "hidden": false
        },
        {
          "_id": "68b93427d43cadaf7a688bd3",
          "name": "Xinting Hu",
          "hidden": false
        },
        {
          "_id": "68b93427d43cadaf7a688bd4",
          "name": "Huijuan Huang",
          "hidden": false
        },
        {
          "_id": "68b93427d43cadaf7a688bd5",
          "name": "Rui Chen",
          "hidden": false
        },
        {
          "_id": "68b93427d43cadaf7a688bd6",
          "name": "Jiarong Ou",
          "hidden": false
        },
        {
          "_id": "68b93427d43cadaf7a688bd7",
          "name": "Xin Tao",
          "hidden": false
        },
        {
          "_id": "68b93427d43cadaf7a688bd8",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "68b93427d43cadaf7a688bd9",
          "name": "Fuli Feng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-03T17:58:12.000Z",
      "submittedOnDailyAt": "2025-09-09T01:08:55.234Z",
      "title": "Easier Painting Than Thinking: Can Text-to-Image Models Set the Stage,\n  but Not Direct the Play?",
      "submittedOnDailyBy": {
        "_id": "666c06ca9ed9e91df03e7e27",
        "avatarUrl": "/avatars/3101ad4f1a3a243b75080d06c5e59e9c.svg",
        "isPro": false,
        "fullname": "Ouxiang Li",
        "user": "lioooox",
        "type": "user"
      },
      "summary": "Text-to-image (T2I) generation aims to synthesize images from textual\nprompts, which jointly specify what must be shown and imply what can be\ninferred, thereby corresponding to two core capabilities: composition and\nreasoning. However, with the emerging advances of T2I models in reasoning\nbeyond composition, existing benchmarks reveal clear limitations in providing\ncomprehensive evaluations across and within these capabilities. Meanwhile,\nthese advances also enable models to handle more complex prompts, whereas\ncurrent benchmarks remain limited to low scene density and simplified\none-to-one reasoning. To address these limitations, we propose T2I-CoReBench, a\ncomprehensive and complex benchmark that evaluates both composition and\nreasoning capabilities of T2I models. To ensure comprehensiveness, we structure\ncomposition around scene graph elements (instance, attribute, and relation) and\nreasoning around the philosophical framework of inference (deductive,\ninductive, and abductive), formulating a 12-dimensional evaluation taxonomy. To\nincrease complexity, driven by the inherent complexities of real-world\nscenarios, we curate each prompt with high compositional density for\ncomposition and multi-step inference for reasoning. We also pair each prompt\nwith a checklist that specifies individual yes/no questions to assess each\nintended element independently to facilitate fine-grained and reliable\nevaluation. In statistics, our benchmark comprises 1,080 challenging prompts\nand around 13,500 checklist questions. Experiments across 27 current T2I models\nreveal that their composition capability still remains limited in complex\nhigh-density scenarios, while the reasoning capability lags even further behind\nas a critical bottleneck, with all models struggling to infer implicit elements\nfrom prompts. Our project page: https://t2i-corebench.github.io/.",
      "upvotes": 7,
      "discussionId": "68b93428d43cadaf7a688bda",
      "projectPage": "https://t2i-corebench.github.io/",
      "githubRepo": "https://github.com/KwaiVGI/T2I-CoReBench",
      "ai_summary": "T2I-CoReBench is a benchmark that evaluates the composition and reasoning capabilities of text-to-image models using a comprehensive and complex set of prompts and checklist questions.",
      "ai_keywords": [
        "text-to-image (T2I) generation",
        "composition",
        "reasoning",
        "scene graph elements",
        "inference",
        "deductive",
        "inductive",
        "abductive",
        "evaluation taxonomy",
        "compositional density",
        "multi-step inference",
        "checklist questions"
      ],
      "githubStars": 10
    },
    "publishedAt": "2025-09-03T13:58:12.000Z",
    "title": "Easier Painting Than Thinking: Can Text-to-Image Models Set the Stage,\n  but Not Direct the Play?",
    "summary": "Text-to-image (T2I) generation aims to synthesize images from textual\nprompts, which jointly specify what must be shown and imply what can be\ninferred, thereby corresponding to two core capabilities: composition and\nreasoning. However, with the emerging advances of T2I models in reasoning\nbeyond composition, existing benchmarks reveal clear limitations in providing\ncomprehensive evaluations across and within these capabilities. Meanwhile,\nthese advances also enable models to handle more complex prompts, whereas\ncurrent benchmarks remain limited to low scene density and simplified\none-to-one reasoning. To address these limitations, we propose T2I-CoReBench, a\ncomprehensive and complex benchmark that evaluates both composition and\nreasoning capabilities of T2I models. To ensure comprehensiveness, we structure\ncomposition around scene graph elements (instance, attribute, and relation) and\nreasoning around the philosophical framework of inference (deductive,\ninductive, and abductive), formulating a 12-dimensional evaluation taxonomy. To\nincrease complexity, driven by the inherent complexities of real-world\nscenarios, we curate each prompt with high compositional density for\ncomposition and multi-step inference for reasoning. We also pair each prompt\nwith a checklist that specifies individual yes/no questions to assess each\nintended element independently to facilitate fine-grained and reliable\nevaluation. In statistics, our benchmark comprises 1,080 challenging prompts\nand around 13,500 checklist questions. Experiments across 27 current T2I models\nreveal that their composition capability still remains limited in complex\nhigh-density scenarios, while the reasoning capability lags even further behind\nas a critical bottleneck, with all models struggling to infer implicit elements\nfrom prompts. Our project page: https://t2i-corebench.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.03516.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "666c06ca9ed9e91df03e7e27",
      "avatarUrl": "/avatars/3101ad4f1a3a243b75080d06c5e59e9c.svg",
      "fullname": "Ouxiang Li",
      "name": "lioooox",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2509.06945",
      "authors": [
        {
          "_id": "68bfaf59207285de11b07cb4",
          "name": "Wenxuan Huang",
          "hidden": false
        },
        {
          "_id": "68bfaf59207285de11b07cb5",
          "name": "Shuang Chen",
          "hidden": false
        },
        {
          "_id": "68bfaf59207285de11b07cb6",
          "name": "Zheyong Xie",
          "hidden": false
        },
        {
          "_id": "68bfaf59207285de11b07cb7",
          "name": "Shaosheng Cao",
          "hidden": false
        },
        {
          "_id": "68bfaf59207285de11b07cb8",
          "name": "Shixiang Tang",
          "hidden": false
        },
        {
          "_id": "68bfaf59207285de11b07cb9",
          "name": "Yufan Shen",
          "hidden": false
        },
        {
          "_id": "68bfaf59207285de11b07cba",
          "name": "Qingyu Yin",
          "hidden": false
        },
        {
          "_id": "68bfaf59207285de11b07cbb",
          "name": "Wenbo Hu",
          "hidden": false
        },
        {
          "_id": "68bfaf59207285de11b07cbc",
          "name": "Xiaoman Wang",
          "hidden": false
        },
        {
          "_id": "68bfaf59207285de11b07cbd",
          "name": "Yuntian Tang",
          "hidden": false
        },
        {
          "_id": "68bfaf59207285de11b07cbe",
          "name": "Junbo Qiao",
          "hidden": false
        },
        {
          "_id": "68bfaf59207285de11b07cbf",
          "name": "Yue Guo",
          "hidden": false
        },
        {
          "_id": "68bfaf59207285de11b07cc0",
          "name": "Yao Hu",
          "hidden": false
        },
        {
          "_id": "68bfaf59207285de11b07cc1",
          "name": "Zhenfei Yin",
          "hidden": false
        },
        {
          "_id": "68bfaf59207285de11b07cc2",
          "name": "Philip Torr",
          "hidden": false
        },
        {
          "_id": "68bfaf59207285de11b07cc3",
          "name": "Yu Cheng",
          "hidden": false
        },
        {
          "_id": "68bfaf59207285de11b07cc4",
          "name": "Wanli Ouyang",
          "hidden": false
        },
        {
          "_id": "68bfaf59207285de11b07cc5",
          "name": "Shaohui Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-08T17:56:23.000Z",
      "submittedOnDailyAt": "2025-09-09T03:17:00.247Z",
      "title": "Interleaving Reasoning for Better Text-to-Image Generation",
      "submittedOnDailyBy": {
        "_id": "67dc162ec8c00778e8689f42",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67dc162ec8c00778e8689f42/_y_tO6W3ONOkOWbumAFXA.png",
        "isPro": false,
        "fullname": "Wenxuan Huang",
        "user": "Osilly",
        "type": "user"
      },
      "summary": "Unified multimodal understanding and generation models recently have achieve\nsignificant improvement in image generation capability, yet a large gap remains\nin instruction following and detail preservation compared to systems that\ntightly couple comprehension with generation such as GPT-4o. Motivated by\nrecent advances in interleaving reasoning, we explore whether such reasoning\ncan further improve Text-to-Image (T2I) generation. We introduce Interleaving\nReasoning Generation (IRG), a framework that alternates between text-based\nthinking and image synthesis: the model first produces a text-based thinking to\nguide an initial image, then reflects on the result to refine fine-grained\ndetails, visual quality, and aesthetics while preserving semantics. To train\nIRG effectively, we propose Interleaving Reasoning Generation Learning (IRGL),\nwhich targets two sub-goals: (1) strengthening the initial think-and-generate\nstage to establish core content and base quality, and (2) enabling high-quality\ntextual reflection and faithful implementation of those refinements in a\nsubsequent image. We curate IRGL-300K, a dataset organized into six decomposed\nlearning modes that jointly cover learning text-based thinking, and full\nthinking-image trajectories. Starting from a unified foundation model that\nnatively emits interleaved text-image outputs, our two-stage training first\nbuilds robust thinking and reflection, then efficiently tunes the IRG pipeline\nin the full thinking-image trajectory data. Extensive experiments show SoTA\nperformance, yielding absolute gains of 5-10 points on GenEval, WISE, TIIF,\nGenAI-Bench, and OneIG-EN, alongside substantial improvements in visual quality\nand fine-grained fidelity. The code, model weights and datasets will be\nreleased in: https://github.com/Osilly/Interleaving-Reasoning-Generation .",
      "upvotes": 5,
      "discussionId": "68bfaf59207285de11b07cc6",
      "githubRepo": "https://github.com/Osilly/Interleaving-Reasoning-Generation",
      "ai_summary": "Interleaving Reasoning Generation (IRG) framework alternates between text-based thinking and image synthesis to improve Text-to-Image generation, achieving state-of-the-art performance and enhanced visual quality.",
      "ai_keywords": [
        "Interleaving Reasoning Generation",
        "IRG",
        "Interleaving Reasoning Generation Learning",
        "IRGL",
        "text-based thinking",
        "image synthesis",
        "GenEval",
        "WISE",
        "TIIF",
        "GenAI-Bench",
        "OneIG-EN"
      ],
      "githubStars": 4
    },
    "publishedAt": "2025-09-08T13:56:23.000Z",
    "title": "Interleaving Reasoning for Better Text-to-Image Generation",
    "summary": "Unified multimodal understanding and generation models recently have achieve\nsignificant improvement in image generation capability, yet a large gap remains\nin instruction following and detail preservation compared to systems that\ntightly couple comprehension with generation such as GPT-4o. Motivated by\nrecent advances in interleaving reasoning, we explore whether such reasoning\ncan further improve Text-to-Image (T2I) generation. We introduce Interleaving\nReasoning Generation (IRG), a framework that alternates between text-based\nthinking and image synthesis: the model first produces a text-based thinking to\nguide an initial image, then reflects on the result to refine fine-grained\ndetails, visual quality, and aesthetics while preserving semantics. To train\nIRG effectively, we propose Interleaving Reasoning Generation Learning (IRGL),\nwhich targets two sub-goals: (1) strengthening the initial think-and-generate\nstage to establish core content and base quality, and (2) enabling high-quality\ntextual reflection and faithful implementation of those refinements in a\nsubsequent image. We curate IRGL-300K, a dataset organized into six decomposed\nlearning modes that jointly cover learning text-based thinking, and full\nthinking-image trajectories. Starting from a unified foundation model that\nnatively emits interleaved text-image outputs, our two-stage training first\nbuilds robust thinking and reflection, then efficiently tunes the IRG pipeline\nin the full thinking-image trajectory data. Extensive experiments show SoTA\nperformance, yielding absolute gains of 5-10 points on GenEval, WISE, TIIF,\nGenAI-Bench, and OneIG-EN, alongside substantial improvements in visual quality\nand fine-grained fidelity. The code, model weights and datasets will be\nreleased in: https://github.com/Osilly/Interleaving-Reasoning-Generation .",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.06945.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67dc162ec8c00778e8689f42",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67dc162ec8c00778e8689f42/_y_tO6W3ONOkOWbumAFXA.png",
      "fullname": "Wenxuan Huang",
      "name": "Osilly",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.06917",
      "authors": [
        {
          "_id": "68bf93a8207285de11b07b87",
          "name": "Jiacheng Miao",
          "hidden": false
        },
        {
          "_id": "68bf93a8207285de11b07b88",
          "name": "Joe R. Davis",
          "hidden": false
        },
        {
          "_id": "68bf93a8207285de11b07b89",
          "name": "Jonathan K. Pritchard",
          "hidden": false
        },
        {
          "_id": "68bf93a8207285de11b07b8a",
          "name": "James Zou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-08T17:28:42.000Z",
      "submittedOnDailyAt": "2025-09-09T01:10:59.723Z",
      "title": "Paper2Agent: Reimagining Research Papers As Interactive and Reliable AI\n  Agents",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We introduce Paper2Agent, an automated framework that converts research\npapers into AI agents. Paper2Agent transforms research output from passive\nartifacts into active systems that can accelerate downstream use, adoption, and\ndiscovery. Conventional research papers require readers to invest substantial\neffort to understand and adapt a paper's code, data, and methods to their own\nwork, creating barriers to dissemination and reuse. Paper2Agent addresses this\nchallenge by automatically converting a paper into an AI agent that acts as a\nknowledgeable research assistant. It systematically analyzes the paper and the\nassociated codebase using multiple agents to construct a Model Context Protocol\n(MCP) server, then iteratively generates and runs tests to refine and robustify\nthe resulting MCP. These paper MCPs can then be flexibly connected to a chat\nagent (e.g. Claude Code) to carry out complex scientific queries through\nnatural language while invoking tools and workflows from the original paper. We\ndemonstrate Paper2Agent's effectiveness in creating reliable and capable paper\nagents through in-depth case studies. Paper2Agent created an agent that\nleverages AlphaGenome to interpret genomic variants and agents based on ScanPy\nand TISSUE to carry out single-cell and spatial transcriptomics analyses. We\nvalidate that these paper agents can reproduce the original paper's results and\ncan correctly carry out novel user queries. By turning static papers into\ndynamic, interactive AI agents, Paper2Agent introduces a new paradigm for\nknowledge dissemination and a foundation for the collaborative ecosystem of AI\nco-scientists.",
      "upvotes": 4,
      "discussionId": "68bf93a8207285de11b07b8b",
      "ai_summary": "Paper2Agent converts research papers into interactive AI agents to facilitate knowledge dissemination and enable complex scientific queries through natural language.",
      "ai_keywords": [
        "AI agents",
        "Model Context Protocol (MCP)",
        "chat agent",
        "AlphaGenome",
        "ScanPy",
        "TISSUE",
        "single-cell analysis",
        "spatial transcriptomics"
      ]
    },
    "publishedAt": "2025-09-08T13:28:42.000Z",
    "title": "Paper2Agent: Reimagining Research Papers As Interactive and Reliable AI\n  Agents",
    "summary": "We introduce Paper2Agent, an automated framework that converts research\npapers into AI agents. Paper2Agent transforms research output from passive\nartifacts into active systems that can accelerate downstream use, adoption, and\ndiscovery. Conventional research papers require readers to invest substantial\neffort to understand and adapt a paper's code, data, and methods to their own\nwork, creating barriers to dissemination and reuse. Paper2Agent addresses this\nchallenge by automatically converting a paper into an AI agent that acts as a\nknowledgeable research assistant. It systematically analyzes the paper and the\nassociated codebase using multiple agents to construct a Model Context Protocol\n(MCP) server, then iteratively generates and runs tests to refine and robustify\nthe resulting MCP. These paper MCPs can then be flexibly connected to a chat\nagent (e.g. Claude Code) to carry out complex scientific queries through\nnatural language while invoking tools and workflows from the original paper. We\ndemonstrate Paper2Agent's effectiveness in creating reliable and capable paper\nagents through in-depth case studies. Paper2Agent created an agent that\nleverages AlphaGenome to interpret genomic variants and agents based on ScanPy\nand TISSUE to carry out single-cell and spatial transcriptomics analyses. We\nvalidate that these paper agents can reproduce the original paper's results and\ncan correctly carry out novel user queries. By turning static papers into\ndynamic, interactive AI agents, Paper2Agent introduces a new paradigm for\nknowledge dissemination and a foundation for the collaborative ecosystem of AI\nco-scientists.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.06917.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 99
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.06493",
      "authors": [
        {
          "_id": "68bf940c207285de11b07b8d",
          "name": "Ran Xin",
          "hidden": false
        },
        {
          "_id": "68bf940c207285de11b07b8e",
          "name": "Zeyu Zheng",
          "hidden": false
        },
        {
          "_id": "68bf940c207285de11b07b8f",
          "name": "Yanchen Nie",
          "hidden": false
        },
        {
          "_id": "68bf940c207285de11b07b90",
          "name": "Kun Yuan",
          "hidden": false
        },
        {
          "_id": "68bf940c207285de11b07b91",
          "name": "Xia Xiao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-08T09:54:18.000Z",
      "submittedOnDailyAt": "2025-09-09T01:12:27.071Z",
      "title": "Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM\n  Step-Provers",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "The integration of Large Language Models (LLMs) into automated theorem\nproving has shown immense promise, yet is fundamentally constrained by\nchallenges in scaling up both training-time reinforcement learning (RL) and\ninference-time compute. This paper introduces BFS-Prover-V2, a system\ndesigned to address this dual scaling problem. We present two primary\ninnovations. The first is a novel multi-turn off-policy RL framework for\ncontinually improving the performance of LLM step-prover at training time. This\nframework, inspired by the principles of AlphaZero, utilizes a multi-stage\nexpert iteration pipeline featuring adaptive tactic-level data filtering and\nperiodic retraining to surmount the performance plateaus that typically curtail\nlong-term RL in LLM-based agents. The second innovation is a planner-enhanced\nmulti-agent search architecture that scales reasoning capabilities at inference\ntime. This architecture employs a general reasoning model as a high-level\nplanner to iteratively decompose complex theorems into a sequence of simpler\nsubgoals. This hierarchical approach substantially reduces the search space,\nenabling a team of parallel prover agents to collaborate efficiently by\nleveraging a shared proof cache. We demonstrate that this dual approach to\nscaling yields state-of-the-art results on established formal mathematics\nbenchmarks. BFS-Prover-V2 achieves 95.08\\% and 41.4\\% on the MiniF2F\nand ProofNet test sets respectively. While demonstrated in the domain of formal\nmathematics, the RL and inference techniques presented in this work are of\nbroader interest and may be applied to other domains requiring long-horizon\nmulti-turn reasoning and complex search.",
      "upvotes": 4,
      "discussionId": "68bf940d207285de11b07b92",
      "ai_summary": "BFS-Prover-V2 addresses scaling challenges in automated theorem proving by integrating a multi-turn off-policy RL framework and a planner-enhanced multi-agent search architecture, achieving state-of-the-art results on formal mathematics benchmarks.",
      "ai_keywords": [
        "Large Language Models",
        "automated theorem proving",
        "reinforcement learning",
        "off-policy RL",
        "AlphaZero",
        "multi-stage expert iteration",
        "adaptive tactic-level data filtering",
        "periodic retraining",
        "planner-enhanced multi-agent search",
        "general reasoning model",
        "high-level planner",
        "subgoals",
        "shared proof cache",
        "MiniF2F",
        "ProofNet"
      ]
    },
    "publishedAt": "2025-09-08T05:54:18.000Z",
    "title": "Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM\n  Step-Provers",
    "summary": "The integration of Large Language Models (LLMs) into automated theorem\nproving has shown immense promise, yet is fundamentally constrained by\nchallenges in scaling up both training-time reinforcement learning (RL) and\ninference-time compute. This paper introduces BFS-Prover-V2, a system\ndesigned to address this dual scaling problem. We present two primary\ninnovations. The first is a novel multi-turn off-policy RL framework for\ncontinually improving the performance of LLM step-prover at training time. This\nframework, inspired by the principles of AlphaZero, utilizes a multi-stage\nexpert iteration pipeline featuring adaptive tactic-level data filtering and\nperiodic retraining to surmount the performance plateaus that typically curtail\nlong-term RL in LLM-based agents. The second innovation is a planner-enhanced\nmulti-agent search architecture that scales reasoning capabilities at inference\ntime. This architecture employs a general reasoning model as a high-level\nplanner to iteratively decompose complex theorems into a sequence of simpler\nsubgoals. This hierarchical approach substantially reduces the search space,\nenabling a team of parallel prover agents to collaborate efficiently by\nleveraging a shared proof cache. We demonstrate that this dual approach to\nscaling yields state-of-the-art results on established formal mathematics\nbenchmarks. BFS-Prover-V2 achieves 95.08\\% and 41.4\\% on the MiniF2F\nand ProofNet test sets respectively. While demonstrated in the domain of formal\nmathematics, the RL and inference techniques presented in this work are of\nbroader interest and may be applied to other domains requiring long-horizon\nmulti-turn reasoning and complex search.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.06493.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 99
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.06786",
      "authors": [
        {
          "_id": "68bf8d90207285de11b07b4a",
          "name": "Youbang Sun",
          "hidden": false
        },
        {
          "_id": "68bf8d90207285de11b07b4b",
          "name": "Xiang Wang",
          "hidden": false
        },
        {
          "_id": "68bf8d90207285de11b07b4c",
          "name": "Jie Fu",
          "hidden": false
        },
        {
          "_id": "68bf8d90207285de11b07b4d",
          "name": "Chaochao Lu",
          "hidden": false
        },
        {
          "_id": "68bf8d90207285de11b07b4e",
          "name": "Bowen Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-08T15:13:23.000Z",
      "submittedOnDailyAt": "2025-09-09T00:56:07.515Z",
      "title": "R^textbf{2AI}: Towards Resistant and Resilient AI in an\n  Evolving World",
      "submittedOnDailyBy": {
        "_id": "679ce8c048ebd7903d76a832",
        "avatarUrl": "/avatars/5f3fecaacfee6e2d5a72dd19fe87055a.svg",
        "isPro": false,
        "fullname": "Youbang Sun",
        "user": "Youbang",
        "type": "user"
      },
      "summary": "In this position paper, we address the persistent gap between rapidly growing\nAI capabilities and lagging safety progress. Existing paradigms divide into\n``Make AI Safe'', which applies post-hoc alignment and guardrails but remains\nbrittle and reactive, and ``Make Safe AI'', which emphasizes intrinsic safety\nbut struggles to address unforeseen risks in open-ended environments. We\ntherefore propose safe-by-coevolution as a new formulation of the\n``Make Safe AI'' paradigm, inspired by biological immunity, in which safety\nbecomes a dynamic, adversarial, and ongoing learning process. To operationalize\nthis vision, we introduce R^2AI -- Resistant and Resilient\nAI -- as a practical framework that unites resistance against known threats\nwith resilience to unforeseen risks. R^2AI integrates fast\nand slow safe models, adversarial simulation and verification through a\nsafety wind tunnel, and continual feedback loops that guide safety and\ncapability to coevolve. We argue that this framework offers a scalable and\nproactive path to maintain continual safety in dynamic environments, addressing\nboth near-term vulnerabilities and long-term existential risks as AI advances\ntoward AGI and ASI.",
      "upvotes": 3,
      "discussionId": "68bf8d90207285de11b07b4f",
      "ai_summary": "A new framework, R²AI, is proposed to enhance AI safety through coevolution, combining resistance to known threats with resilience to unforeseen risks using fast and slow safe models and adversarial simulation.",
      "ai_keywords": [
        "safe-by-coevolution",
        "R²AI",
        "Resistant and Resilient AI",
        "fast and slow safe models",
        "safety wind tunnel",
        "continual feedback loops",
        "AGI",
        "ASI"
      ]
    },
    "publishedAt": "2025-09-08T11:13:23.000Z",
    "title": "R^textbf{2AI}: Towards Resistant and Resilient AI in an\n  Evolving World",
    "summary": "In this position paper, we address the persistent gap between rapidly growing\nAI capabilities and lagging safety progress. Existing paradigms divide into\n``Make AI Safe'', which applies post-hoc alignment and guardrails but remains\nbrittle and reactive, and ``Make Safe AI'', which emphasizes intrinsic safety\nbut struggles to address unforeseen risks in open-ended environments. We\ntherefore propose safe-by-coevolution as a new formulation of the\n``Make Safe AI'' paradigm, inspired by biological immunity, in which safety\nbecomes a dynamic, adversarial, and ongoing learning process. To operationalize\nthis vision, we introduce R^2AI -- Resistant and Resilient\nAI -- as a practical framework that unites resistance against known threats\nwith resilience to unforeseen risks. R^2AI integrates fast\nand slow safe models, adversarial simulation and verification through a\nsafety wind tunnel, and continual feedback loops that guide safety and\ncapability to coevolve. We argue that this framework offers a scalable and\nproactive path to maintain continual safety in dynamic environments, addressing\nboth near-term vulnerabilities and long-term existential risks as AI advances\ntoward AGI and ASI.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.06786.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "679ce8c048ebd7903d76a832",
      "avatarUrl": "/avatars/5f3fecaacfee6e2d5a72dd19fe87055a.svg",
      "fullname": "Youbang Sun",
      "name": "Youbang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.06477",
      "authors": [
        {
          "_id": "68bf993b207285de11b07bc9",
          "name": "Pengxiang Zhao",
          "hidden": false
        },
        {
          "_id": "68bf993b207285de11b07bca",
          "name": "Guangyi Liu",
          "hidden": false
        },
        {
          "_id": "68bf993b207285de11b07bcb",
          "name": "Yaozhen Liang",
          "hidden": false
        },
        {
          "_id": "68bf993b207285de11b07bcc",
          "name": "Weiqing He",
          "hidden": false
        },
        {
          "_id": "68bf993b207285de11b07bcd",
          "name": "Zhengxi Lu",
          "hidden": false
        },
        {
          "_id": "68bf993b207285de11b07bce",
          "name": "Yuehao Huang",
          "hidden": false
        },
        {
          "_id": "68bf993b207285de11b07bcf",
          "name": "Yaxuan Guo",
          "hidden": false
        },
        {
          "_id": "68bf993b207285de11b07bd0",
          "name": "Kexin Zhang",
          "hidden": false
        },
        {
          "_id": "68bf993b207285de11b07bd1",
          "name": "Hao Wang",
          "hidden": false
        },
        {
          "_id": "68bf993b207285de11b07bd2",
          "name": "Liang Liu",
          "hidden": false
        },
        {
          "_id": "68bf993b207285de11b07bd3",
          "name": "Yong Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-08T09:43:48.000Z",
      "submittedOnDailyAt": "2025-09-09T01:35:27.046Z",
      "title": "MAS-Bench: A Unified Benchmark for Shortcut-Augmented Hybrid Mobile GUI\n  Agents",
      "submittedOnDailyBy": {
        "_id": "64d761b98ebc40443831f82a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d761b98ebc40443831f82a/DHBOtOstiFp2-lDY6b9gb.png",
        "isPro": false,
        "fullname": "Guangyi Liu",
        "user": "lgy0404",
        "type": "user"
      },
      "summary": "To enhance the efficiency of GUI agents on various platforms like smartphones\nand computers, a hybrid paradigm that combines flexible GUI operations with\nefficient shortcuts (e.g., API, deep links) is emerging as a promising\ndirection. However, a framework for systematically benchmarking these hybrid\nagents is still underexplored. To take the first step in bridging this gap, we\nintroduce MAS-Bench, a benchmark that pioneers the evaluation of GUI-shortcut\nhybrid agents with a specific focus on the mobile domain. Beyond merely using\npredefined shortcuts, MAS-Bench assesses an agent's capability to autonomously\ngenerate shortcuts by discovering and creating reusable, low-cost workflows. It\nfeatures 139 complex tasks across 11 real-world applications, a knowledge base\nof 88 predefined shortcuts (APIs, deep-links, RPA scripts), and 7 evaluation\nmetrics. The tasks are designed to be solvable via GUI-only operations, but can\nbe significantly accelerated by intelligently embedding shortcuts. Experiments\nshow that hybrid agents achieve significantly higher success rates and\nefficiency than their GUI-only counterparts. This result also demonstrates the\neffectiveness of our method for evaluating an agent's shortcut generation\ncapabilities. MAS-Bench fills a critical evaluation gap, providing a\nfoundational platform for future advancements in creating more efficient and\nrobust intelligent agents.",
      "upvotes": 2,
      "discussionId": "68bf993c207285de11b07bd4",
      "projectPage": "https://pengxiang-zhao.github.io/MAS-Bench",
      "githubRepo": "https://github.com/Pengxiang-zhao/MAS-Bench",
      "ai_summary": "MAS-Bench evaluates GUI-shortcut hybrid agents on mobile devices, demonstrating their superior performance and efficiency over GUI-only agents through a comprehensive benchmarking framework.",
      "ai_keywords": [
        "GUI agents",
        "hybrid paradigm",
        "efficient shortcuts",
        "API",
        "deep links",
        "MAS-Bench",
        "autonomous generation",
        "reusable workflows",
        "evaluation metrics",
        "success rates",
        "efficiency"
      ],
      "githubStars": 8
    },
    "publishedAt": "2025-09-08T05:43:48.000Z",
    "title": "MAS-Bench: A Unified Benchmark for Shortcut-Augmented Hybrid Mobile GUI\n  Agents",
    "summary": "To enhance the efficiency of GUI agents on various platforms like smartphones\nand computers, a hybrid paradigm that combines flexible GUI operations with\nefficient shortcuts (e.g., API, deep links) is emerging as a promising\ndirection. However, a framework for systematically benchmarking these hybrid\nagents is still underexplored. To take the first step in bridging this gap, we\nintroduce MAS-Bench, a benchmark that pioneers the evaluation of GUI-shortcut\nhybrid agents with a specific focus on the mobile domain. Beyond merely using\npredefined shortcuts, MAS-Bench assesses an agent's capability to autonomously\ngenerate shortcuts by discovering and creating reusable, low-cost workflows. It\nfeatures 139 complex tasks across 11 real-world applications, a knowledge base\nof 88 predefined shortcuts (APIs, deep-links, RPA scripts), and 7 evaluation\nmetrics. The tasks are designed to be solvable via GUI-only operations, but can\nbe significantly accelerated by intelligently embedding shortcuts. Experiments\nshow that hybrid agents achieve significantly higher success rates and\nefficiency than their GUI-only counterparts. This result also demonstrates the\neffectiveness of our method for evaluating an agent's shortcut generation\ncapabilities. MAS-Bench fills a critical evaluation gap, providing a\nfoundational platform for future advancements in creating more efficient and\nrobust intelligent agents.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.06477.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64d761b98ebc40443831f82a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d761b98ebc40443831f82a/DHBOtOstiFp2-lDY6b9gb.png",
      "fullname": "Guangyi Liu",
      "name": "lgy0404",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.05668",
      "authors": [
        {
          "_id": "68bf9108207285de11b07b62",
          "name": "Michael Hoffmann",
          "hidden": false
        },
        {
          "_id": "68bf9108207285de11b07b63",
          "name": "Jophin John",
          "hidden": false
        },
        {
          "_id": "68bf9108207285de11b07b64",
          "name": "Stefan Schweter",
          "hidden": false
        },
        {
          "_id": "68bf9108207285de11b07b65",
          "name": "Gokul Ramakrishnan",
          "hidden": false
        },
        {
          "_id": "68bf9108207285de11b07b66",
          "name": "Hoi-Fong Mak",
          "hidden": false
        },
        {
          "_id": "68bf9108207285de11b07b67",
          "name": "Alice Zhang",
          "hidden": false
        },
        {
          "_id": "68bf9108207285de11b07b68",
          "name": "Dmitry Gaynullin",
          "hidden": false
        },
        {
          "_id": "68bf9108207285de11b07b69",
          "name": "Nicolay J. Hammer",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-06T10:12:52.000Z",
      "submittedOnDailyAt": "2025-09-09T01:00:32.896Z",
      "title": "Llama-GENBA-10B: A Trilingual Large Language Model for German, English\n  and Bavarian",
      "submittedOnDailyBy": {
        "_id": "5e6a3d4ea9afd5125d9ec064",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1584020801691-noauth.jpeg",
        "isPro": true,
        "fullname": "Stefan Schweter",
        "user": "stefan-it",
        "type": "user"
      },
      "summary": "We present Llama-GENBA-10B, a trilingual foundation model addressing\nEnglish-centric bias in large language models. Built on Llama 3.1-8B and scaled\nto 10B parameters, Llama-GENBA-10B is continuously pretrained on 164B tokens\n(82B English, 82B German, and 80M Bavarian), balancing resources while\npreventing English dominance. Targeted at the German NLP community, the model\nalso promotes Bavarian as a low-resource language. Development tackled four\nchallenges: (1) curating a multilingual corpus despite Bavarian scarcity, (2)\ncreating a unified tokenizer for English, German, and Bavarian, (3) optimizing\narchitecture and language-ratio hyperparameters for cross-lingual transfer, and\n(4) establishing the first standardized trilingual evaluation suite by\ntranslating German benchmarks into Bavarian. Evaluations show that\nLlama-GENBA-10B achieves strong cross-lingual performance, with the fine-tuned\nvariant surpassing Apertus-8B-2509 and gemma-2-9b in Bavarian and establishing\nitself as the best model in its class for this language, while also\noutperforming EuroLLM in English and matching its results in German. Training\non the Cerebras CS-2 demonstrated efficient large-scale multilingual\npretraining with documented energy use, offering a blueprint for inclusive\nfoundation models that integrate low-resource languages.",
      "upvotes": 2,
      "discussionId": "68bf9108207285de11b07b6a",
      "ai_summary": "Llama-GENBA-10B, a trilingual foundation model, addresses English-centric bias by balancing English, German, and Bavarian training, achieving strong cross-lingual performance and setting new benchmarks for Bavarian.",
      "ai_keywords": [
        "trilingual foundation model",
        "Llama 3.1-8B",
        "Llama-GENBA-10B",
        "multilingual corpus",
        "unified tokenizer",
        "cross-lingual transfer",
        "trilingual evaluation suite",
        "fine-tuned variant",
        "Apertus-8B-2509",
        "gemma-2-9b",
        "EuroLLM",
        "Cerebras CS-2",
        "large-scale multilingual pretraining"
      ]
    },
    "publishedAt": "2025-09-06T06:12:52.000Z",
    "title": "Llama-GENBA-10B: A Trilingual Large Language Model for German, English\n  and Bavarian",
    "summary": "We present Llama-GENBA-10B, a trilingual foundation model addressing\nEnglish-centric bias in large language models. Built on Llama 3.1-8B and scaled\nto 10B parameters, Llama-GENBA-10B is continuously pretrained on 164B tokens\n(82B English, 82B German, and 80M Bavarian), balancing resources while\npreventing English dominance. Targeted at the German NLP community, the model\nalso promotes Bavarian as a low-resource language. Development tackled four\nchallenges: (1) curating a multilingual corpus despite Bavarian scarcity, (2)\ncreating a unified tokenizer for English, German, and Bavarian, (3) optimizing\narchitecture and language-ratio hyperparameters for cross-lingual transfer, and\n(4) establishing the first standardized trilingual evaluation suite by\ntranslating German benchmarks into Bavarian. Evaluations show that\nLlama-GENBA-10B achieves strong cross-lingual performance, with the fine-tuned\nvariant surpassing Apertus-8B-2509 and gemma-2-9b in Bavarian and establishing\nitself as the best model in its class for this language, while also\noutperforming EuroLLM in English and matching its results in German. Training\non the Cerebras CS-2 demonstrated efficient large-scale multilingual\npretraining with documented energy use, offering a blueprint for inclusive\nfoundation models that integrate low-resource languages.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.05668.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5e6a3d4ea9afd5125d9ec064",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1584020801691-noauth.jpeg",
      "fullname": "Stefan Schweter",
      "name": "stefan-it",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3360
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.06771",
      "authors": [
        {
          "_id": "68bf971c207285de11b07b9d",
          "name": "Sai Kartheek Reddy Kasu",
          "hidden": false
        },
        {
          "_id": "68bf971c207285de11b07b9e",
          "name": "Mohammad Zia Ur Rehman",
          "hidden": false
        },
        {
          "_id": "68bf971c207285de11b07b9f",
          "name": "Shahid Shafi Dar",
          "hidden": false
        },
        {
          "_id": "68bf971c207285de11b07ba0",
          "name": "Rishi Bharat Junghare",
          "hidden": false
        },
        {
          "_id": "68bf971c207285de11b07ba1",
          "name": "Dhanvin Sanjay Namboodiri",
          "hidden": false
        },
        {
          "_id": "68bf971c207285de11b07ba2",
          "name": "Nagendra Kumar",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-08T14:55:16.000Z",
      "submittedOnDailyAt": "2025-09-09T01:28:57.435Z",
      "title": "D-HUMOR: Dark Humor Understanding via Multimodal Open-ended Reasoning",
      "submittedOnDailyBy": {
        "_id": "651692d718f3a57f869a5a0a",
        "avatarUrl": "/avatars/d5fe48de11e46675b05e1e2e1cf3505c.svg",
        "isPro": false,
        "fullname": "Sai Kartheek Reddy",
        "user": "UVSKKR",
        "type": "user"
      },
      "summary": "Dark humor in online memes poses unique challenges due to its reliance on\nimplicit, sensitive, and culturally contextual cues. To address the lack of\nresources and methods for detecting dark humor in multimodal content, we\nintroduce a novel dataset of 4,379 Reddit memes annotated for dark humor,\ntarget category (gender, mental health, violence, race, disability, and other),\nand a three-level intensity rating (mild, moderate, severe). Building on this\nresource, we propose a reasoning-augmented framework that first generates\nstructured explanations for each meme using a Large Vision-Language Model\n(VLM). Through a Role-Reversal Self-Loop, VLM adopts the author's perspective\nto iteratively refine its explanations, ensuring completeness and alignment. We\nthen extract textual features from both the OCR transcript and the self-refined\nreasoning via a text encoder, while visual features are obtained using a vision\ntransformer. A Tri-stream Cross-Reasoning Network (TCRNet) fuses these three\nstreams, text, image, and reasoning, via pairwise attention mechanisms,\nproducing a unified representation for classification. Experimental results\ndemonstrate that our approach outperforms strong baselines across three tasks:\ndark humor detection, target identification, and intensity prediction. The\ndataset, annotations, and code are released to facilitate further research in\nmultimodal humor understanding and content moderation. Code and Dataset are\navailable at:\nhttps://github.com/Sai-Kartheek-Reddy/D-Humor-Dark-Humor-Understanding-via-Multimodal-Open-ended-Reasoning",
      "upvotes": 1,
      "discussionId": "68bf971c207285de11b07ba3",
      "githubRepo": "https://github.com/Sai-Kartheek-Reddy/D-Humor-Dark-Humor-Understanding-via-Multimodal-Open-ended-Reasoning/",
      "ai_summary": "A reasoning-augmented framework using a Large Vision-Language Model and a Tri-stream Cross-Reasoning Network achieves superior performance in detecting dark humor, identifying targets, and predicting intensity in multimodal memes.",
      "ai_keywords": [
        "Large Vision-Language Model",
        "Role-Reversal Self-Loop",
        "text encoder",
        "vision transformer",
        "Tri-stream Cross-Reasoning Network",
        "pairwise attention mechanisms"
      ],
      "githubStars": 2
    },
    "publishedAt": "2025-09-08T10:55:16.000Z",
    "title": "D-HUMOR: Dark Humor Understanding via Multimodal Open-ended Reasoning",
    "summary": "Dark humor in online memes poses unique challenges due to its reliance on\nimplicit, sensitive, and culturally contextual cues. To address the lack of\nresources and methods for detecting dark humor in multimodal content, we\nintroduce a novel dataset of 4,379 Reddit memes annotated for dark humor,\ntarget category (gender, mental health, violence, race, disability, and other),\nand a three-level intensity rating (mild, moderate, severe). Building on this\nresource, we propose a reasoning-augmented framework that first generates\nstructured explanations for each meme using a Large Vision-Language Model\n(VLM). Through a Role-Reversal Self-Loop, VLM adopts the author's perspective\nto iteratively refine its explanations, ensuring completeness and alignment. We\nthen extract textual features from both the OCR transcript and the self-refined\nreasoning via a text encoder, while visual features are obtained using a vision\ntransformer. A Tri-stream Cross-Reasoning Network (TCRNet) fuses these three\nstreams, text, image, and reasoning, via pairwise attention mechanisms,\nproducing a unified representation for classification. Experimental results\ndemonstrate that our approach outperforms strong baselines across three tasks:\ndark humor detection, target identification, and intensity prediction. The\ndataset, annotations, and code are released to facilitate further research in\nmultimodal humor understanding and content moderation. Code and Dataset are\navailable at:\nhttps://github.com/Sai-Kartheek-Reddy/D-Humor-Dark-Humor-Understanding-via-Multimodal-Open-ended-Reasoning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.06771.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "651692d718f3a57f869a5a0a",
      "avatarUrl": "/avatars/d5fe48de11e46675b05e1e2e1cf3505c.svg",
      "fullname": "Sai Kartheek Reddy",
      "name": "UVSKKR",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  }
]