[
  {
    "paper": {
      "id": "2512.17220",
      "authors": [
        {
          "_id": "69493ea1d2000e944d383ab5",
          "name": "Yuqing Li",
          "hidden": false
        },
        {
          "_id": "69493ea1d2000e944d383ab6",
          "name": "Jiangnan Li",
          "hidden": false
        },
        {
          "_id": "69493ea1d2000e944d383ab7",
          "name": "Zheng Lin",
          "hidden": false
        },
        {
          "_id": "69493ea1d2000e944d383ab8",
          "name": "Ziyan Zhou",
          "hidden": false
        },
        {
          "_id": "69493ea1d2000e944d383ab9",
          "name": "Junjie Wu",
          "hidden": false
        },
        {
          "_id": "69493ea1d2000e944d383aba",
          "name": "Weiping Wang",
          "hidden": false
        },
        {
          "_id": "69493ea1d2000e944d383abb",
          "name": "Jie Zhou",
          "hidden": false
        },
        {
          "_id": "69493ea1d2000e944d383abc",
          "name": "Mo Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-19T04:08:29.000Z",
      "submittedOnDailyAt": "2025-12-29T00:41:08.445Z",
      "title": "Mindscape-Aware Retrieval Augmented Generation for Improved Long Context Understanding",
      "submittedOnDailyBy": {
        "_id": "67af92045a86287292026808",
        "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg",
        "isPro": false,
        "fullname": "Mo",
        "user": "BishopGorov",
        "type": "user"
      },
      "summary": "Humans understand long and complex texts by relying on a holistic semantic representation of the content. This global view helps organize prior knowledge, interpret new information, and integrate evidence dispersed across a document, as revealed by the Mindscape-Aware Capability of humans in psychology. Current Retrieval-Augmented Generation (RAG) systems lack such guidance and therefore struggle with long-context tasks. In this paper, we propose Mindscape-Aware RAG (MiA-RAG), the first approach that equips LLM-based RAG systems with explicit global context awareness. MiA-RAG builds a mindscape through hierarchical summarization and conditions both retrieval and generation on this global semantic representation. This enables the retriever to form enriched query embeddings and the generator to reason over retrieved evidence within a coherent global context. We evaluate MiA-RAG across diverse long-context and bilingual benchmarks for evidence-based understanding and global sense-making. It consistently surpasses baselines, and further analysis shows that it aligns local details with a coherent global representation, enabling more human-like long-context retrieval and reasoning.",
      "upvotes": 37,
      "discussionId": "69493ea1d2000e944d383abd",
      "ai_summary": "MiA-RAG, a Mindscape-Aware Retrieval-Augmented Generation system, enhances LLM-based RAG with global context awareness through hierarchical summarization, improving long-context tasks and evidence-based understanding.",
      "ai_keywords": [
        "Mindscape-Aware Capability",
        "Retrieval-Augmented Generation (RAG)",
        "MiA-RAG",
        "LLM-based RAG systems",
        "hierarchical summarization",
        "query embeddings",
        "global semantic representation",
        "evidence-based understanding",
        "global sense-making"
      ],
      "organization": {
        "_id": "66543b6e420092799d2f625c",
        "name": "tencent",
        "fullname": "Tencent",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
      }
    },
    "publishedAt": "2025-12-18T23:08:29.000Z",
    "title": "Mindscape-Aware Retrieval Augmented Generation for Improved Long Context Understanding",
    "summary": "Humans understand long and complex texts by relying on a holistic semantic representation of the content. This global view helps organize prior knowledge, interpret new information, and integrate evidence dispersed across a document, as revealed by the Mindscape-Aware Capability of humans in psychology. Current Retrieval-Augmented Generation (RAG) systems lack such guidance and therefore struggle with long-context tasks. In this paper, we propose Mindscape-Aware RAG (MiA-RAG), the first approach that equips LLM-based RAG systems with explicit global context awareness. MiA-RAG builds a mindscape through hierarchical summarization and conditions both retrieval and generation on this global semantic representation. This enables the retriever to form enriched query embeddings and the generator to reason over retrieved evidence within a coherent global context. We evaluate MiA-RAG across diverse long-context and bilingual benchmarks for evidence-based understanding and global sense-making. It consistently surpasses baselines, and further analysis shows that it aligns local details with a coherent global representation, enabling more human-like long-context retrieval and reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.17220.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67af92045a86287292026808",
      "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg",
      "fullname": "Mo",
      "name": "BishopGorov",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "organization": {
      "_id": "66543b6e420092799d2f625c",
      "name": "tencent",
      "fullname": "Tencent",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.17504",
      "authors": [
        {
          "_id": "694c0ede746a34b55dd53fa2",
          "user": {
            "_id": "6726857c88f2f9df27225d48",
            "avatarUrl": "/avatars/6a4e09d1759f1c2fa241e51ad85f9f00.svg",
            "isPro": false,
            "fullname": "Hoiyeong Jin",
            "user": "myyzzzoooo",
            "type": "user"
          },
          "name": "Hoiyeong Jin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-12-25T20:44:16.181Z",
          "hidden": false
        },
        {
          "_id": "694c0ede746a34b55dd53fa3",
          "user": {
            "_id": "669e3d80f433fc42bebe2ff0",
            "avatarUrl": "/avatars/2122a3288ca017922a966361aec1fda4.svg",
            "isPro": false,
            "fullname": "Jang hyojin",
            "user": "Whit3Snow",
            "type": "user"
          },
          "name": "Hyojin Jang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-12-25T20:44:20.849Z",
          "hidden": false
        },
        {
          "_id": "694c0ede746a34b55dd53fa4",
          "name": "Jeongho Kim",
          "hidden": false
        },
        {
          "_id": "694c0ede746a34b55dd53fa5",
          "name": "Junha Hyung",
          "hidden": false
        },
        {
          "_id": "694c0ede746a34b55dd53fa6",
          "name": "Kinam Kim",
          "hidden": false
        },
        {
          "_id": "694c0ede746a34b55dd53fa7",
          "name": "Dongjin Kim",
          "hidden": false
        },
        {
          "_id": "694c0ede746a34b55dd53fa8",
          "name": "Huijin Choi",
          "hidden": false
        },
        {
          "_id": "694c0ede746a34b55dd53fa9",
          "name": "Hyeonji Kim",
          "hidden": false
        },
        {
          "_id": "694c0ede746a34b55dd53faa",
          "name": "Jaegul Choo",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6726857c88f2f9df27225d48/ZFmCv5gVUK7FY4ivZnTJY.mp4"
      ],
      "publishedAt": "2025-12-19T12:14:36.000Z",
      "submittedOnDailyAt": "2025-12-29T00:07:19.171Z",
      "title": "InsertAnywhere: Bridging 4D Scene Geometry and Diffusion Models for Realistic Video Object Insertion",
      "submittedOnDailyBy": {
        "_id": "6726857c88f2f9df27225d48",
        "avatarUrl": "/avatars/6a4e09d1759f1c2fa241e51ad85f9f00.svg",
        "isPro": false,
        "fullname": "Hoiyeong Jin",
        "user": "myyzzzoooo",
        "type": "user"
      },
      "summary": "Recent advances in diffusion-based video generation have opened new possibilities for controllable video editing, yet realistic video object insertion (VOI) remains challenging due to limited 4D scene understanding and inadequate handling of occlusion and lighting effects. We present InsertAnywhere, a new VOI framework that achieves geometrically consistent object placement and appearance-faithful video synthesis. Our method begins with a 4D aware mask generation module that reconstructs the scene geometry and propagates user specified object placement across frames while maintaining temporal coherence and occlusion consistency. Building upon this spatial foundation, we extend a diffusion based video generation model to jointly synthesize the inserted object and its surrounding local variations such as illumination and shading. To enable supervised training, we introduce ROSE++, an illumination aware synthetic dataset constructed by transforming the ROSE object removal dataset into triplets of object removed video, object present video, and a VLM generated reference image. Through extensive experiments, we demonstrate that our framework produces geometrically plausible and visually coherent object insertions across diverse real world scenarios, significantly outperforming existing research and commercial models.",
      "upvotes": 27,
      "discussionId": "694c0ede746a34b55dd53fab",
      "projectPage": "https://myyzzzoooo.github.io/InsertAnywhere/",
      "githubRepo": "https://github.com/myyzzzoooo/InsertAnywhere",
      "githubRepoAddedBy": "user",
      "ai_summary": "InsertAnywhere framework enhances video object insertion by generating geometrically consistent and visually coherent scenarios through 4D aware mask generation and diffusion-based synthesis.",
      "ai_keywords": [
        "diffusion-based video generation",
        "realistic video object insertion",
        "4D scene understanding",
        "occlusion effects",
        "geometrically consistent object placement",
        "appearance-faithful video synthesis",
        "4D aware mask generation",
        "diffusion based video generation model",
        "ROSE++",
        "illumination aware synthetic dataset",
        "object removal dataset",
        "VLM generated reference image",
        "geometrically plausible",
        "visually coherent object insertions"
      ],
      "githubStars": 3,
      "organization": {
        "_id": "6475760c33192631bad2bb38",
        "name": "kaist-ai",
        "fullname": "KAIST AI",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6469949654873f0043b09c22/aaZFiyXe1qR-Dmy_xq67m.png"
      }
    },
    "publishedAt": "2025-12-19T07:14:36.000Z",
    "title": "InsertAnywhere: Bridging 4D Scene Geometry and Diffusion Models for Realistic Video Object Insertion",
    "summary": "Recent advances in diffusion-based video generation have opened new possibilities for controllable video editing, yet realistic video object insertion (VOI) remains challenging due to limited 4D scene understanding and inadequate handling of occlusion and lighting effects. We present InsertAnywhere, a new VOI framework that achieves geometrically consistent object placement and appearance-faithful video synthesis. Our method begins with a 4D aware mask generation module that reconstructs the scene geometry and propagates user specified object placement across frames while maintaining temporal coherence and occlusion consistency. Building upon this spatial foundation, we extend a diffusion based video generation model to jointly synthesize the inserted object and its surrounding local variations such as illumination and shading. To enable supervised training, we introduce ROSE++, an illumination aware synthetic dataset constructed by transforming the ROSE object removal dataset into triplets of object removed video, object present video, and a VLM generated reference image. Through extensive experiments, we demonstrate that our framework produces geometrically plausible and visually coherent object insertions across diverse real world scenarios, significantly outperforming existing research and commercial models.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6726857c88f2f9df27225d48/ZFmCv5gVUK7FY4ivZnTJY.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.17504.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6726857c88f2f9df27225d48",
      "avatarUrl": "/avatars/6a4e09d1759f1c2fa241e51ad85f9f00.svg",
      "fullname": "Hoiyeong Jin",
      "name": "myyzzzoooo",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "organization": {
      "_id": "6475760c33192631bad2bb38",
      "name": "kaist-ai",
      "fullname": "KAIST AI",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6469949654873f0043b09c22/aaZFiyXe1qR-Dmy_xq67m.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2512.21675",
      "authors": [
        {
          "_id": "6951e354746a34b55dd5487a",
          "name": "Shuo Cao",
          "hidden": false
        },
        {
          "_id": "6951e354746a34b55dd5487b",
          "name": "Jiayang Li",
          "hidden": false
        },
        {
          "_id": "6951e354746a34b55dd5487c",
          "name": "Xiaohui Li",
          "hidden": false
        },
        {
          "_id": "6951e354746a34b55dd5487d",
          "name": "Yuandong Pu",
          "hidden": false
        },
        {
          "_id": "6951e354746a34b55dd5487e",
          "name": "Kaiwen Zhu",
          "hidden": false
        },
        {
          "_id": "6951e354746a34b55dd5487f",
          "name": "Yuanting Gao",
          "hidden": false
        },
        {
          "_id": "6951e354746a34b55dd54880",
          "name": "Siqi Luo",
          "hidden": false
        },
        {
          "_id": "6951e354746a34b55dd54881",
          "name": "Yi Xin",
          "hidden": false
        },
        {
          "_id": "6951e354746a34b55dd54882",
          "name": "Qi Qin",
          "hidden": false
        },
        {
          "_id": "6951e354746a34b55dd54883",
          "name": "Yu Zhou",
          "hidden": false
        },
        {
          "_id": "6951e354746a34b55dd54884",
          "name": "Xiangyu Chen",
          "hidden": false
        },
        {
          "_id": "6951e354746a34b55dd54885",
          "name": "Wenlong Zhang",
          "hidden": false
        },
        {
          "_id": "6951e354746a34b55dd54886",
          "name": "Bin Fu",
          "hidden": false
        },
        {
          "_id": "6951e354746a34b55dd54887",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "6951e354746a34b55dd54888",
          "name": "Yihao Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-25T13:35:52.000Z",
      "submittedOnDailyAt": "2025-12-29T00:40:27.598Z",
      "title": "UniPercept: Towards Unified Perceptual-Level Image Understanding across Aesthetics, Quality, Structure, and Texture",
      "submittedOnDailyBy": {
        "_id": "625d5b9f0bec31f086e04cd9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1650285458447-noauth.jpeg",
        "isPro": false,
        "fullname": "YuandongPu",
        "user": "Andrew613",
        "type": "user"
      },
      "summary": "Multimodal large language models (MLLMs) have achieved remarkable progress in visual understanding tasks such as visual grounding, segmentation, and captioning. However, their ability to perceive perceptual-level image features remains limited. In this work, we present UniPercept-Bench, a unified framework for perceptual-level image understanding across three key domains: Aesthetics, Quality, Structure and Texture. We establish a hierarchical definition system and construct large-scale datasets to evaluate perceptual-level image understanding. Based on this foundation, we develop a strong baseline UniPercept trained via Domain-Adaptive Pre-Training and Task-Aligned RL, enabling robust generalization across both Visual Rating (VR) and Visual Question Answering (VQA) tasks. UniPercept outperforms existing MLLMs on perceptual-level image understanding and can serve as a plug-and-play reward model for text-to-image generation. This work defines Perceptual-Level Image Understanding in the era of MLLMs and, through the introduction of a comprehensive benchmark together with a strong baseline, provides a solid foundation for advancing perceptual-level multimodal image understanding.",
      "upvotes": 13,
      "discussionId": "6951e354746a34b55dd54889",
      "projectPage": "https://thunderbolt215.github.io/Unipercept-project/",
      "githubRepo": "https://github.com/thunderbolt215/UniPercept",
      "githubRepoAddedBy": "user",
      "githubStars": 4
    },
    "publishedAt": "2025-12-25T08:35:52.000Z",
    "title": "UniPercept: Towards Unified Perceptual-Level Image Understanding across Aesthetics, Quality, Structure, and Texture",
    "summary": "Multimodal large language models (MLLMs) have achieved remarkable progress in visual understanding tasks such as visual grounding, segmentation, and captioning. However, their ability to perceive perceptual-level image features remains limited. In this work, we present UniPercept-Bench, a unified framework for perceptual-level image understanding across three key domains: Aesthetics, Quality, Structure and Texture. We establish a hierarchical definition system and construct large-scale datasets to evaluate perceptual-level image understanding. Based on this foundation, we develop a strong baseline UniPercept trained via Domain-Adaptive Pre-Training and Task-Aligned RL, enabling robust generalization across both Visual Rating (VR) and Visual Question Answering (VQA) tasks. UniPercept outperforms existing MLLMs on perceptual-level image understanding and can serve as a plug-and-play reward model for text-to-image generation. This work defines Perceptual-Level Image Understanding in the era of MLLMs and, through the introduction of a comprehensive benchmark together with a strong baseline, provides a solid foundation for advancing perceptual-level multimodal image understanding.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.21675.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "625d5b9f0bec31f086e04cd9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1650285458447-noauth.jpeg",
      "fullname": "YuandongPu",
      "name": "Andrew613",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.22047",
      "authors": [
        {
          "_id": "6951e7d4746a34b55dd548a7",
          "name": "Hanzhang Zhou",
          "hidden": false
        },
        {
          "_id": "6951e7d4746a34b55dd548a8",
          "name": "Xu Zhang",
          "hidden": false
        },
        {
          "_id": "6951e7d4746a34b55dd548a9",
          "name": "Panrong Tong",
          "hidden": false
        },
        {
          "_id": "6951e7d4746a34b55dd548aa",
          "name": "Jianan Zhang",
          "hidden": false
        },
        {
          "_id": "6951e7d4746a34b55dd548ab",
          "name": "Liangyu Chen",
          "hidden": false
        },
        {
          "_id": "6951e7d4746a34b55dd548ac",
          "name": "Quyu Kong",
          "hidden": false
        },
        {
          "_id": "6951e7d4746a34b55dd548ad",
          "name": "Chenglin Cai",
          "hidden": false
        },
        {
          "_id": "6951e7d4746a34b55dd548ae",
          "name": "Chen Liu",
          "hidden": false
        },
        {
          "_id": "6951e7d4746a34b55dd548af",
          "name": "Yue Wang",
          "hidden": false
        },
        {
          "_id": "6951e7d4746a34b55dd548b0",
          "name": "Jingren Zhou",
          "hidden": false
        },
        {
          "_id": "6951e7d4746a34b55dd548b1",
          "name": "Steven Hoi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-26T14:51:52.000Z",
      "submittedOnDailyAt": "2025-12-29T00:01:11.405Z",
      "title": "MAI-UI Technical Report: Real-World Centric Foundation GUI Agents",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "The development of GUI agents could revolutionize the next generation of human-computer interaction. Motivated by this vision, we present MAI-UI, a family of foundation GUI agents spanning the full spectrum of sizes, including 2B, 8B, 32B, and 235B-A22B variants. We identify four key challenges to realistic deployment: the lack of native agent-user interaction, the limits of UI-only operation, the absence of a practical deployment architecture, and brittleness in dynamic environments. MAI-UI addresses these issues with a unified methodology: a self-evolving data pipeline that expands the navigation data to include user interaction and MCP tool calls, a native device-cloud collaboration system routes execution by task state, and an online RL framework with advanced optimizations to scale parallel environments and context length. MAI-UI establishes new state-of-the-art across GUI grounding and mobile navigation. On grounding benchmarks, it reaches 73.5% on ScreenSpot-Pro, 91.3% on MMBench GUI L2, 70.9% on OSWorld-G, and 49.2% on UI-Vision, surpassing Gemini-3-Pro and Seed1.8 on ScreenSpot-Pro. On mobile GUI navigation, it sets a new SOTA of 76.7% on AndroidWorld, surpassing UI-Tars-2, Gemini-2.5-Pro and Seed1.8. On MobileWorld, MAI-UI obtains 41.7% success rate, significantly outperforming end-to-end GUI models and competitive with Gemini-3-Pro based agentic frameworks. Our online RL experiments show significant gains from scaling parallel environments from 32 to 512 (+5.2 points) and increasing environment step budget from 15 to 50 (+4.3 points). Finally, the native device-cloud collaboration system improves on-device performance by 33%, reduces cloud model calls by over 40%, and preserves user privacy.",
      "upvotes": 12,
      "discussionId": "6951e7d4746a34b55dd548b2",
      "organization": {
        "_id": "67d15cca6e2cf0e062dbfb54",
        "name": "AlibabaTongyiLab",
        "fullname": "TongyiLab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
      }
    },
    "publishedAt": "2025-12-26T09:51:52.000Z",
    "title": "MAI-UI Technical Report: Real-World Centric Foundation GUI Agents",
    "summary": "The development of GUI agents could revolutionize the next generation of human-computer interaction. Motivated by this vision, we present MAI-UI, a family of foundation GUI agents spanning the full spectrum of sizes, including 2B, 8B, 32B, and 235B-A22B variants. We identify four key challenges to realistic deployment: the lack of native agent-user interaction, the limits of UI-only operation, the absence of a practical deployment architecture, and brittleness in dynamic environments. MAI-UI addresses these issues with a unified methodology: a self-evolving data pipeline that expands the navigation data to include user interaction and MCP tool calls, a native device-cloud collaboration system routes execution by task state, and an online RL framework with advanced optimizations to scale parallel environments and context length. MAI-UI establishes new state-of-the-art across GUI grounding and mobile navigation. On grounding benchmarks, it reaches 73.5% on ScreenSpot-Pro, 91.3% on MMBench GUI L2, 70.9% on OSWorld-G, and 49.2% on UI-Vision, surpassing Gemini-3-Pro and Seed1.8 on ScreenSpot-Pro. On mobile GUI navigation, it sets a new SOTA of 76.7% on AndroidWorld, surpassing UI-Tars-2, Gemini-2.5-Pro and Seed1.8. On MobileWorld, MAI-UI obtains 41.7% success rate, significantly outperforming end-to-end GUI models and competitive with Gemini-3-Pro based agentic frameworks. Our online RL experiments show significant gains from scaling parallel environments from 32 to 512 (+5.2 points) and increasing environment step budget from 15 to 50 (+4.3 points). Finally, the native device-cloud collaboration system improves on-device performance by 33%, reduces cloud model calls by over 40%, and preserves user privacy.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.22047.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 195
    },
    "organization": {
      "_id": "67d15cca6e2cf0e062dbfb54",
      "name": "AlibabaTongyiLab",
      "fullname": "TongyiLab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.22118",
      "authors": [
        {
          "_id": "6951e89b746a34b55dd548bd",
          "name": "Zhi Ouyang",
          "hidden": false
        },
        {
          "_id": "6951e89b746a34b55dd548be",
          "name": "Dian Zheng",
          "hidden": false
        },
        {
          "_id": "6951e89b746a34b55dd548bf",
          "name": "Xiao-Ming Wu",
          "hidden": false
        },
        {
          "_id": "6951e89b746a34b55dd548c0",
          "name": "Jian-Jian Jiang",
          "hidden": false
        },
        {
          "_id": "6951e89b746a34b55dd548c1",
          "name": "Kun-Yu Lin",
          "hidden": false
        },
        {
          "_id": "6951e89b746a34b55dd548c2",
          "name": "Jingke Meng",
          "hidden": false
        },
        {
          "_id": "6951e89b746a34b55dd548c3",
          "name": "Wei-Shi Zheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-26T18:59:14.000Z",
      "submittedOnDailyAt": "2025-12-29T00:07:11.474Z",
      "title": "ProEdit: Inversion-based Editing From Prompts Done Right",
      "submittedOnDailyBy": {
        "_id": "67e60ae6ac37824273d74389",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/YvPKZ_0gyJnvNwM1zK3JS.png",
        "isPro": false,
        "fullname": "Dian Zheng",
        "user": "zhengli1013",
        "type": "user"
      },
      "summary": "Inversion-based visual editing provides an effective and training-free way to edit an image or a video based on user instructions. Existing methods typically inject source image information during the sampling process to maintain editing consistency. However, this sampling strategy overly relies on source information, which negatively affects the edits in the target image (e.g., failing to change the subject's atributes like pose, number, or color as instructed). In this work, we propose ProEdit to address this issue both in the attention and the latent aspects. In the attention aspect, we introduce KV-mix, which mixes KV features of the source and the target in the edited region, mitigating the influence of the source image on the editing region while maintaining background consistency. In the latent aspect, we propose Latents-Shift, which perturbs the edited region of the source latent, eliminating the influence of the inverted latent on the sampling. Extensive experiments on several image and video editing benchmarks demonstrate that our method achieves SOTA performance. In addition, our design is plug-and-play, which can be seamlessly integrated into existing inversion and editing methods, such as RF-Solver, FireFlow and UniEdit.",
      "upvotes": 10,
      "discussionId": "6951e89b746a34b55dd548c4",
      "projectPage": "https://isee-laboratory.github.io/ProEdit/",
      "githubRepo": "https://github.com/iSEE-Laboratory/ProEdit",
      "githubRepoAddedBy": "user",
      "githubStars": 16
    },
    "publishedAt": "2025-12-26T13:59:14.000Z",
    "title": "ProEdit: Inversion-based Editing From Prompts Done Right",
    "summary": "Inversion-based visual editing provides an effective and training-free way to edit an image or a video based on user instructions. Existing methods typically inject source image information during the sampling process to maintain editing consistency. However, this sampling strategy overly relies on source information, which negatively affects the edits in the target image (e.g., failing to change the subject's atributes like pose, number, or color as instructed). In this work, we propose ProEdit to address this issue both in the attention and the latent aspects. In the attention aspect, we introduce KV-mix, which mixes KV features of the source and the target in the edited region, mitigating the influence of the source image on the editing region while maintaining background consistency. In the latent aspect, we propose Latents-Shift, which perturbs the edited region of the source latent, eliminating the influence of the inverted latent on the sampling. Extensive experiments on several image and video editing benchmarks demonstrate that our method achieves SOTA performance. In addition, our design is plug-and-play, which can be seamlessly integrated into existing inversion and editing methods, such as RF-Solver, FireFlow and UniEdit.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.22118.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67e60ae6ac37824273d74389",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/YvPKZ_0gyJnvNwM1zK3JS.png",
      "fullname": "Dian Zheng",
      "name": "zhengli1013",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.22120",
      "authors": [
        {
          "_id": "6951e5dc746a34b55dd5488b",
          "name": "Shuoshuo Zhang",
          "hidden": false
        },
        {
          "_id": "6951e5dc746a34b55dd5488c",
          "name": "Yizhen Zhang",
          "hidden": false
        },
        {
          "_id": "6951e5dc746a34b55dd5488d",
          "name": "Jingjing Fu",
          "hidden": false
        },
        {
          "_id": "6951e5dc746a34b55dd5488e",
          "name": "Lei Song",
          "hidden": false
        },
        {
          "_id": "6951e5dc746a34b55dd5488f",
          "name": "Jiang Bian",
          "hidden": false
        },
        {
          "_id": "6951e5dc746a34b55dd54890",
          "name": "Yujiu Yang",
          "hidden": false
        },
        {
          "_id": "6951e5dc746a34b55dd54891",
          "name": "Rui Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-26T18:59:47.000Z",
      "submittedOnDailyAt": "2025-12-29T00:26:41.152Z",
      "title": "See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning",
      "submittedOnDailyBy": {
        "_id": "641c6c51dad24840739667ed",
        "avatarUrl": "/avatars/916bf79bd0cb2e3c3214edf5cba25784.svg",
        "isPro": true,
        "fullname": "Shuoshuo Zhang",
        "user": "zss01",
        "type": "user"
      },
      "summary": "Large vision-language models (VLMs) often benefit from intermediate visual cues, either injected via external tools or generated as latent visual tokens during reasoning, but these mechanisms still overlook fine-grained visual evidence (e.g., polylines in charts), generalize poorly across domains, and incur high inference-time cost. In this paper, we propose Bi-directional Perceptual Shaping (BiPS), which transforms question-conditioned masked views into bidirectional where-to-look signals that shape perception during training. BiPS first applies a KL-consistency constraint between the original image and an evidence-preserving view that keeps only question-relevant regions, encouraging coarse but complete coverage of supporting pixels. It then applies a KL-separation constraint between the original and an evidence-ablated view where critical pixels are masked so the image no longer supports the original answer, discouraging text-only shortcuts (i.e., answering from text alone) and enforcing fine-grained visual reliance. Across eight benchmarks, BiPS boosts Qwen2.5-VL-7B by 8.2% on average and shows strong out-of-domain generalization to unseen datasets and image types.",
      "upvotes": 6,
      "discussionId": "6951e5dc746a34b55dd54892",
      "githubRepo": "https://github.com/zss02/BiPS",
      "githubRepoAddedBy": "user",
      "githubStars": 3,
      "organization": {
        "_id": "66f55d53853f0506904d1922",
        "name": "IIGroup",
        "fullname": "Tsinghua IIGroup",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62579c55b98dcaa7e0de285d/A1SKeBEvaODFnkAZusICK.png"
      }
    },
    "publishedAt": "2025-12-26T13:59:47.000Z",
    "title": "See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning",
    "summary": "Large vision-language models (VLMs) often benefit from intermediate visual cues, either injected via external tools or generated as latent visual tokens during reasoning, but these mechanisms still overlook fine-grained visual evidence (e.g., polylines in charts), generalize poorly across domains, and incur high inference-time cost. In this paper, we propose Bi-directional Perceptual Shaping (BiPS), which transforms question-conditioned masked views into bidirectional where-to-look signals that shape perception during training. BiPS first applies a KL-consistency constraint between the original image and an evidence-preserving view that keeps only question-relevant regions, encouraging coarse but complete coverage of supporting pixels. It then applies a KL-separation constraint between the original and an evidence-ablated view where critical pixels are masked so the image no longer supports the original answer, discouraging text-only shortcuts (i.e., answering from text alone) and enforcing fine-grained visual reliance. Across eight benchmarks, BiPS boosts Qwen2.5-VL-7B by 8.2% on average and shows strong out-of-domain generalization to unseen datasets and image types.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.22120.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "641c6c51dad24840739667ed",
      "avatarUrl": "/avatars/916bf79bd0cb2e3c3214edf5cba25784.svg",
      "fullname": "Shuoshuo Zhang",
      "name": "zss01",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "organization": {
      "_id": "66f55d53853f0506904d1922",
      "name": "IIGroup",
      "fullname": "Tsinghua IIGroup",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62579c55b98dcaa7e0de285d/A1SKeBEvaODFnkAZusICK.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.21859",
      "authors": [
        {
          "_id": "6951f746746a34b55dd548cb",
          "name": "Qi Fan",
          "hidden": false
        },
        {
          "_id": "6951f746746a34b55dd548cc",
          "name": "An Zou",
          "hidden": false
        },
        {
          "_id": "6951f746746a34b55dd548cd",
          "name": "Yehan Ma",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-26T04:49:35.000Z",
      "submittedOnDailyAt": "2025-12-29T01:09:22.323Z",
      "title": "TimeBill: Time-Budgeted Inference for Large Language Models",
      "submittedOnDailyBy": {
        "_id": "64b8a72952b7353d8c669086",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b8a72952b7353d8c669086/3PUTNmx9kd17gtZ9-Yviw.jpeg",
        "isPro": false,
        "fullname": "Qi Fan",
        "user": "fanqiNO1",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) are increasingly deployed in time-critical systems, such as robotics, autonomous driving, embodied intelligence, and industrial automation, where generating accurate responses within a given time budget is crucial for decision-making, control, or safety-critical tasks. However, the auto-regressive generation process of LLMs makes it challenging to model and estimate the end-to-end execution time. Furthermore, existing efficient inference methods based on a fixed key-value (KV) cache eviction ratio struggle to adapt to varying tasks with diverse time budgets, where an improper eviction ratio may lead to incomplete inference or a drop in response performance. In this paper, we propose TimeBill, a novel time-budgeted inference framework for LLMs that balances the inference efficiency and response performance. To be more specific, we propose a fine-grained response length predictor (RLP) and an execution time estimator (ETE) to accurately predict the end-to-end execution time of LLMs. Following this, we develop a time-budgeted efficient inference approach that adaptively adjusts the KV cache eviction ratio based on execution time prediction and the given time budget. Finally, through extensive experiments, we demonstrate the advantages of TimeBill in improving task completion rate and maintaining response performance under various overrun strategies.",
      "upvotes": 4,
      "discussionId": "6951f746746a34b55dd548ce"
    },
    "publishedAt": "2025-12-25T23:49:35.000Z",
    "title": "TimeBill: Time-Budgeted Inference for Large Language Models",
    "summary": "Large Language Models (LLMs) are increasingly deployed in time-critical systems, such as robotics, autonomous driving, embodied intelligence, and industrial automation, where generating accurate responses within a given time budget is crucial for decision-making, control, or safety-critical tasks. However, the auto-regressive generation process of LLMs makes it challenging to model and estimate the end-to-end execution time. Furthermore, existing efficient inference methods based on a fixed key-value (KV) cache eviction ratio struggle to adapt to varying tasks with diverse time budgets, where an improper eviction ratio may lead to incomplete inference or a drop in response performance. In this paper, we propose TimeBill, a novel time-budgeted inference framework for LLMs that balances the inference efficiency and response performance. To be more specific, we propose a fine-grained response length predictor (RLP) and an execution time estimator (ETE) to accurately predict the end-to-end execution time of LLMs. Following this, we develop a time-budgeted efficient inference approach that adaptively adjusts the KV cache eviction ratio based on execution time prediction and the given time budget. Finally, through extensive experiments, we demonstrate the advantages of TimeBill in improving task completion rate and maintaining response performance under various overrun strategies.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.21859.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64b8a72952b7353d8c669086",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b8a72952b7353d8c669086/3PUTNmx9kd17gtZ9-Yviw.jpeg",
      "fullname": "Qi Fan",
      "name": "fanqiNO1",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.21643",
      "authors": [
        {
          "_id": "695217d6746a34b55dd548ee",
          "name": "Zhiwang Zhou",
          "hidden": false
        },
        {
          "_id": "695217d6746a34b55dd548ef",
          "name": "Yuandong Pu",
          "hidden": false
        },
        {
          "_id": "695217d6746a34b55dd548f0",
          "name": "Xuming He",
          "hidden": false
        },
        {
          "_id": "695217d6746a34b55dd548f1",
          "name": "Yidi Liu",
          "hidden": false
        },
        {
          "_id": "695217d6746a34b55dd548f2",
          "name": "Yixin Chen",
          "hidden": false
        },
        {
          "_id": "695217d6746a34b55dd548f3",
          "name": "Junchao Gong",
          "hidden": false
        },
        {
          "_id": "695217d6746a34b55dd548f4",
          "name": "Xiang Zhuang",
          "hidden": false
        },
        {
          "_id": "695217d6746a34b55dd548f5",
          "name": "Wanghan Xu",
          "hidden": false
        },
        {
          "_id": "695217d6746a34b55dd548f6",
          "name": "Qinglong Cao",
          "hidden": false
        },
        {
          "_id": "695217d6746a34b55dd548f7",
          "name": "Shixiang Tang",
          "hidden": false
        },
        {
          "_id": "695217d6746a34b55dd548f8",
          "name": "Yihao Liu",
          "hidden": false
        },
        {
          "_id": "695217d6746a34b55dd548f9",
          "name": "Wenlong Zhang",
          "hidden": false
        },
        {
          "_id": "695217d6746a34b55dd548fa",
          "name": "Lei Bai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-25T12:08:09.000Z",
      "submittedOnDailyAt": "2025-12-29T03:26:08.562Z",
      "title": "Omni-Weather: Unified Multimodal Foundation Model for Weather Generation and Understanding",
      "submittedOnDailyBy": {
        "_id": "625d5b9f0bec31f086e04cd9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1650285458447-noauth.jpeg",
        "isPro": false,
        "fullname": "YuandongPu",
        "user": "Andrew613",
        "type": "user"
      },
      "summary": "Weather modeling requires both accurate prediction and mechanistic interpretation, yet existing methods treat these goals in isolation, separating generation from understanding. To address this gap, we present Omni-Weather, the first multimodal foundation model that unifies weather generation and understanding within a single architecture. Omni-Weather integrates a radar encoder for weather generation tasks, followed by unified processing using a shared self-attention mechanism. Moreover, we construct a Chain-of-Thought dataset for causal reasoning in weather generation, enabling interpretable outputs and improved perceptual quality. Extensive experiments show Omni-Weather achieves state-of-the-art performance in both weather generation and understanding. Our findings further indicate that generative and understanding tasks in the weather domain can mutually enhance each other. Omni-Weather also demonstrates the feasibility and value of unifying weather generation and understanding.",
      "upvotes": 4,
      "discussionId": "695217d6746a34b55dd548fb",
      "ai_summary": "Omni-Weather is a multimodal foundation model that integrates weather generation and understanding using a shared self-attention mechanism and a Chain-of-Thought dataset to enable interpretable, high-quality outputs.",
      "ai_keywords": [
        "multimodal foundation model",
        "radar encoder",
        "shared self-attention mechanism",
        "Chain-of-Thought dataset"
      ]
    },
    "publishedAt": "2025-12-25T07:08:09.000Z",
    "title": "Omni-Weather: Unified Multimodal Foundation Model for Weather Generation and Understanding",
    "summary": "Weather modeling requires both accurate prediction and mechanistic interpretation, yet existing methods treat these goals in isolation, separating generation from understanding. To address this gap, we present Omni-Weather, the first multimodal foundation model that unifies weather generation and understanding within a single architecture. Omni-Weather integrates a radar encoder for weather generation tasks, followed by unified processing using a shared self-attention mechanism. Moreover, we construct a Chain-of-Thought dataset for causal reasoning in weather generation, enabling interpretable outputs and improved perceptual quality. Extensive experiments show Omni-Weather achieves state-of-the-art performance in both weather generation and understanding. Our findings further indicate that generative and understanding tasks in the weather domain can mutually enhance each other. Omni-Weather also demonstrates the feasibility and value of unifying weather generation and understanding.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.21643.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "625d5b9f0bec31f086e04cd9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1650285458447-noauth.jpeg",
      "fullname": "YuandongPu",
      "name": "Andrew613",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.21507",
      "authors": [
        {
          "_id": "6951e7e7746a34b55dd548b4",
          "name": "Wenshuo Peng",
          "hidden": false
        },
        {
          "_id": "6951e7e7746a34b55dd548b5",
          "name": "Gongxuan Wang",
          "hidden": false
        },
        {
          "_id": "6951e7e7746a34b55dd548b6",
          "name": "Tianmeng Yang",
          "hidden": false
        },
        {
          "_id": "6951e7e7746a34b55dd548b7",
          "name": "Chuanhao Li",
          "hidden": false
        },
        {
          "_id": "6951e7e7746a34b55dd548b8",
          "name": "Xiaojie Xu",
          "hidden": false
        },
        {
          "_id": "6951e7e7746a34b55dd548b9",
          "name": "Hui He",
          "hidden": false
        },
        {
          "_id": "6951e7e7746a34b55dd548ba",
          "name": "Kaipeng Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-25T04:44:59.000Z",
      "submittedOnDailyAt": "2025-12-29T00:01:52.656Z",
      "title": "SVBench: Evaluation of Video Generation Models on Social Reasoning",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Recent text-to-video generation models exhibit remarkable progress in visual realism, motion fidelity, and text-video alignment, yet they remain fundamentally limited in their ability to generate socially coherent behavior. Unlike humans, who effortlessly infer intentions, beliefs, emotions, and social norms from brief visual cues, current models tend to render literal scenes without capturing the underlying causal or psychological logic. To systematically evaluate this gap, we introduce the first benchmark for social reasoning in video generation. Grounded in findings from developmental and social psychology, our benchmark organizes thirty classic social cognition paradigms into seven core dimensions, including mental-state inference, goal-directed action, joint attention, social coordination, prosocial behavior, social norms, and multi-agent strategy. To operationalize these paradigms, we develop a fully training-free agent-based pipeline that (i) distills the reasoning mechanism of each experiment, (ii) synthesizes diverse video-ready scenarios, (iii) enforces conceptual neutrality and difficulty control through cue-based critique, and (iv) evaluates generated videos using a high-capacity VLM judge across five interpretable dimensions of social reasoning. Using this framework, we conduct the first large-scale study across seven state-of-the-art video generation systems. Our results reveal substantial performance gaps: while modern models excel in surface-level plausibility, they systematically fail in intention recognition, belief reasoning, joint attention, and prosocial inference.",
      "upvotes": 4,
      "discussionId": "6951e7e7746a34b55dd548bb",
      "githubRepo": "https://github.com/Gloria2tt/SVBench-Evaluation",
      "githubRepoAddedBy": "user",
      "githubStars": 1
    },
    "publishedAt": "2025-12-24T23:44:59.000Z",
    "title": "SVBench: Evaluation of Video Generation Models on Social Reasoning",
    "summary": "Recent text-to-video generation models exhibit remarkable progress in visual realism, motion fidelity, and text-video alignment, yet they remain fundamentally limited in their ability to generate socially coherent behavior. Unlike humans, who effortlessly infer intentions, beliefs, emotions, and social norms from brief visual cues, current models tend to render literal scenes without capturing the underlying causal or psychological logic. To systematically evaluate this gap, we introduce the first benchmark for social reasoning in video generation. Grounded in findings from developmental and social psychology, our benchmark organizes thirty classic social cognition paradigms into seven core dimensions, including mental-state inference, goal-directed action, joint attention, social coordination, prosocial behavior, social norms, and multi-agent strategy. To operationalize these paradigms, we develop a fully training-free agent-based pipeline that (i) distills the reasoning mechanism of each experiment, (ii) synthesizes diverse video-ready scenarios, (iii) enforces conceptual neutrality and difficulty control through cue-based critique, and (iv) evaluates generated videos using a high-capacity VLM judge across five interpretable dimensions of social reasoning. Using this framework, we conduct the first large-scale study across seven state-of-the-art video generation systems. Our results reveal substantial performance gaps: while modern models excel in surface-level plausibility, they systematically fail in intention recognition, belief reasoning, joint attention, and prosocial inference.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.21507.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 195
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.20292",
      "authors": [
        {
          "_id": "695212b6746a34b55dd548e8",
          "name": "Wenzheng Zeng",
          "hidden": false
        },
        {
          "_id": "695212b6746a34b55dd548e9",
          "name": "Mingyu Ouyang",
          "hidden": false
        },
        {
          "_id": "695212b6746a34b55dd548ea",
          "name": "Langyuan Cui",
          "hidden": false
        },
        {
          "_id": "695212b6746a34b55dd548eb",
          "name": "Hwee Tou Ng",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/670a5c886f31d354bc8c1cd1/CUTbLK_VqiU-HAz43aXMW.png",
        "https://cdn-uploads.huggingface.co/production/uploads/670a5c886f31d354bc8c1cd1/_MhLMyHauOaa6jnJ_u8qb.png"
      ],
      "publishedAt": "2025-12-23T12:01:18.000Z",
      "submittedOnDailyAt": "2025-12-29T03:17:11.148Z",
      "title": "SlideTailor: Personalized Presentation Slide Generation for Scientific Papers",
      "submittedOnDailyBy": {
        "_id": "670a5c886f31d354bc8c1cd1",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/670a5c886f31d354bc8c1cd1/D2Mueg9eQy4fzJhOmCrnu.jpeg",
        "isPro": false,
        "fullname": "Wenzheng Zeng",
        "user": "wenzhengzeng",
        "type": "user"
      },
      "summary": "Automatic presentation slide generation can greatly streamline content creation. However, since preferences of each user may vary, existing under-specified formulations often lead to suboptimal results that fail to align with individual user needs. We introduce a novel task that conditions paper-to-slides generation on user-specified preferences. We propose a human behavior-inspired agentic framework, SlideTailor, that progressively generates editable slides in a user-aligned manner. Instead of requiring users to write their preferences in detailed textual form, our system only asks for a paper-slides example pair and a visual template - natural and easy-to-provide artifacts that implicitly encode rich user preferences across content and visual style. Despite the implicit and unlabeled nature of these inputs, our framework effectively distills and generalizes the preferences to guide customized slide generation. We also introduce a novel chain-of-speech mechanism to align slide content with planned oral narration. Such a design significantly enhances the quality of generated slides and enables downstream applications like video presentations. To support this new task, we construct a benchmark dataset that captures diverse user preferences, with carefully designed interpretable metrics for robust evaluation. Extensive experiments demonstrate the effectiveness of our framework.",
      "upvotes": 3,
      "discussionId": "695212b6746a34b55dd548ec",
      "projectPage": "https://github.com/nusnlp/SlideTailor",
      "githubRepo": "https://github.com/nusnlp/SlideTailor",
      "githubRepoAddedBy": "user",
      "ai_summary": "SlideTailor, an agentic framework, generates user-aligned slides using implicit preferences from example pairs and visual templates, incorporating a chain-of-speech mechanism for oral narration alignment and a benchmark dataset for evaluation.",
      "ai_keywords": [
        "SlideTailor",
        "agentic framework",
        "chain-of-speech mechanism",
        "benchmark dataset",
        "interpretable metrics"
      ],
      "githubStars": 11,
      "organization": {
        "_id": "6508ab2b349930913196378b",
        "name": "NationalUniversityofSingapore",
        "fullname": "National University of Singapore",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZYUmpSMsa5Whihw3me2Bw.png"
      }
    },
    "publishedAt": "2025-12-23T07:01:18.000Z",
    "title": "SlideTailor: Personalized Presentation Slide Generation for Scientific Papers",
    "summary": "Automatic presentation slide generation can greatly streamline content creation. However, since preferences of each user may vary, existing under-specified formulations often lead to suboptimal results that fail to align with individual user needs. We introduce a novel task that conditions paper-to-slides generation on user-specified preferences. We propose a human behavior-inspired agentic framework, SlideTailor, that progressively generates editable slides in a user-aligned manner. Instead of requiring users to write their preferences in detailed textual form, our system only asks for a paper-slides example pair and a visual template - natural and easy-to-provide artifacts that implicitly encode rich user preferences across content and visual style. Despite the implicit and unlabeled nature of these inputs, our framework effectively distills and generalizes the preferences to guide customized slide generation. We also introduce a novel chain-of-speech mechanism to align slide content with planned oral narration. Such a design significantly enhances the quality of generated slides and enables downstream applications like video presentations. To support this new task, we construct a benchmark dataset that captures diverse user preferences, with carefully designed interpretable metrics for robust evaluation. Extensive experiments demonstrate the effectiveness of our framework.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/670a5c886f31d354bc8c1cd1/CUTbLK_VqiU-HAz43aXMW.png",
      "https://cdn-uploads.huggingface.co/production/uploads/670a5c886f31d354bc8c1cd1/_MhLMyHauOaa6jnJ_u8qb.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20292.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "670a5c886f31d354bc8c1cd1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/670a5c886f31d354bc8c1cd1/D2Mueg9eQy4fzJhOmCrnu.jpeg",
      "fullname": "Wenzheng Zeng",
      "name": "wenzhengzeng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "6508ab2b349930913196378b",
      "name": "NationalUniversityofSingapore",
      "fullname": "National University of Singapore",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZYUmpSMsa5Whihw3me2Bw.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.21919",
      "authors": [
        {
          "_id": "6951e7bc746a34b55dd5489c",
          "name": "KaShun Shum",
          "hidden": false
        },
        {
          "_id": "6951e7bc746a34b55dd5489d",
          "name": "Binyuan Hui",
          "hidden": false
        },
        {
          "_id": "6951e7bc746a34b55dd5489e",
          "name": "Jiawei Chen",
          "hidden": false
        },
        {
          "_id": "6951e7bc746a34b55dd5489f",
          "name": "Lei Zhang",
          "hidden": false
        },
        {
          "_id": "6951e7bc746a34b55dd548a0",
          "name": "X. W.",
          "hidden": false
        },
        {
          "_id": "6951e7bc746a34b55dd548a1",
          "name": "Jiaxi Yang",
          "hidden": false
        },
        {
          "_id": "6951e7bc746a34b55dd548a2",
          "name": "Yuzhen Huang",
          "hidden": false
        },
        {
          "_id": "6951e7bc746a34b55dd548a3",
          "name": "Junyang Lin",
          "hidden": false
        },
        {
          "_id": "6951e7bc746a34b55dd548a4",
          "name": "Junxian He",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-26T08:26:18.000Z",
      "submittedOnDailyAt": "2025-12-29T00:00:24.492Z",
      "title": "SWE-RM: Execution-free Feedback For Software Engineering Agents",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Execution-based feedback like unit testing is widely used in the development of coding agents through test-time scaling (TTS) and reinforcement learning (RL). This paradigm requires scalable and reliable collection of unit test cases to provide accurate feedback, and the resulting feedback is often sparse and cannot effectively distinguish between trajectories that are both successful or both unsuccessful. In contrast, execution-free feedback from reward models can provide more fine-grained signals without depending on unit test cases. Despite this potential, execution-free feedback for realistic software engineering (SWE) agents remains underexplored. Aiming to develop versatile reward models that are effective across TTS and RL, however, we observe that two verifiers with nearly identical TTS performance can nevertheless yield very different results in RL. Intuitively, TTS primarily reflects the model's ability to select the best trajectory, but this ability does not necessarily generalize to RL. To address this limitation, we identify two additional aspects that are crucial for RL training: classification accuracy and calibration. We then conduct comprehensive controlled experiments to investigate how to train a robust reward model that performs well across these metrics. In particular, we analyze the impact of various factors such as training data scale, policy mixtures, and data source composition. Guided by these investigations, we introduce SWE-RM, an accurate and robust reward model adopting a mixture-of-experts architecture with 30B total parameters and 3B activated during inference. SWE-RM substantially improves SWE agents on both TTS and RL performance. For example, it increases the accuracy of Qwen3-Coder-Flash from 51.6% to 62.0%, and Qwen3-Coder-Max from 67.0% to 74.6% on SWE-Bench Verified using TTS, achieving new state-of-the-art performance among open-source models.",
      "upvotes": 2,
      "discussionId": "6951e7bd746a34b55dd548a5"
    },
    "publishedAt": "2025-12-26T03:26:18.000Z",
    "title": "SWE-RM: Execution-free Feedback For Software Engineering Agents",
    "summary": "Execution-based feedback like unit testing is widely used in the development of coding agents through test-time scaling (TTS) and reinforcement learning (RL). This paradigm requires scalable and reliable collection of unit test cases to provide accurate feedback, and the resulting feedback is often sparse and cannot effectively distinguish between trajectories that are both successful or both unsuccessful. In contrast, execution-free feedback from reward models can provide more fine-grained signals without depending on unit test cases. Despite this potential, execution-free feedback for realistic software engineering (SWE) agents remains underexplored. Aiming to develop versatile reward models that are effective across TTS and RL, however, we observe that two verifiers with nearly identical TTS performance can nevertheless yield very different results in RL. Intuitively, TTS primarily reflects the model's ability to select the best trajectory, but this ability does not necessarily generalize to RL. To address this limitation, we identify two additional aspects that are crucial for RL training: classification accuracy and calibration. We then conduct comprehensive controlled experiments to investigate how to train a robust reward model that performs well across these metrics. In particular, we analyze the impact of various factors such as training data scale, policy mixtures, and data source composition. Guided by these investigations, we introduce SWE-RM, an accurate and robust reward model adopting a mixture-of-experts architecture with 30B total parameters and 3B activated during inference. SWE-RM substantially improves SWE agents on both TTS and RL performance. For example, it increases the accuracy of Qwen3-Coder-Flash from 51.6% to 62.0%, and Qwen3-Coder-Max from 67.0% to 74.6% on SWE-Bench Verified using TTS, achieving new state-of-the-art performance among open-source models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.21919.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 195
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.18745",
      "authors": [
        {
          "_id": "694a4fe4335742716e9323ce",
          "user": {
            "_id": "65d5b967eeb590ea7435ad07",
            "avatarUrl": "/avatars/ca0a5e123d5da5aca97cfd8a2d07e60e.svg",
            "isPro": false,
            "fullname": "Kaican Li",
            "user": "m-Just",
            "type": "user"
          },
          "name": "Kaican Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-12-25T20:47:40.670Z",
          "hidden": false
        },
        {
          "_id": "694a4fe4335742716e9323cf",
          "name": "Lewei Yao",
          "hidden": false
        },
        {
          "_id": "694a4fe4335742716e9323d0",
          "name": "Jiannan Wu",
          "hidden": false
        },
        {
          "_id": "694a4fe4335742716e9323d1",
          "name": "Tiezheng Yu",
          "hidden": false
        },
        {
          "_id": "694a4fe4335742716e9323d2",
          "name": "Jierun Chen",
          "hidden": false
        },
        {
          "_id": "694a4fe4335742716e9323d3",
          "name": "Haoli Bai",
          "hidden": false
        },
        {
          "_id": "694a4fe4335742716e9323d4",
          "name": "Lu Hou",
          "hidden": false
        },
        {
          "_id": "694a4fe4335742716e9323d5",
          "name": "Lanqing Hong",
          "hidden": false
        },
        {
          "_id": "694a4fe4335742716e9323d6",
          "name": "Wei Zhang",
          "hidden": false
        },
        {
          "_id": "694a4fe4335742716e9323d7",
          "name": "Nevin L. Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-21T14:23:07.000Z",
      "submittedOnDailyAt": "2025-12-29T04:36:37.110Z",
      "title": "InSight-o3: Empowering Multimodal Foundation Models with Generalized Visual Search",
      "submittedOnDailyBy": {
        "_id": "65d5b967eeb590ea7435ad07",
        "avatarUrl": "/avatars/ca0a5e123d5da5aca97cfd8a2d07e60e.svg",
        "isPro": false,
        "fullname": "Kaican Li",
        "user": "m-Just",
        "type": "user"
      },
      "summary": "The ability for AI agents to \"think with images\" requires a sophisticated blend of reasoning and perception. However, current open multimodal agents still largely fall short on the reasoning aspect crucial for real-world tasks like analyzing documents with dense charts/diagrams and navigating maps. To address this gap, we introduce O3-Bench, a new benchmark designed to evaluate multimodal reasoning with interleaved attention to visual details. O3-Bench features challenging problems that require agents to piece together subtle visual information from distinct image areas through multi-step reasoning. The problems are highly challenging even for frontier systems like OpenAI o3, which only obtains 40.8% accuracy on O3-Bench. To make progress, we propose InSight-o3, a multi-agent framework consisting of a visual reasoning agent (vReasoner) and a visual search agent (vSearcher) for which we introduce the task of generalized visual search -- locating relational, fuzzy, or conceptual regions described in free-form language, beyond just simple objects or figures in natural images. We then present a multimodal LLM purpose-trained for this task via reinforcement learning. As a plug-and-play agent, our vSearcher empowers frontier multimodal models (as vReasoners), significantly improving their performance on a wide range of benchmarks. This marks a concrete step towards powerful o3-like open systems. Our code and dataset can be found at https://github.com/m-Just/InSight-o3 .",
      "upvotes": 2,
      "discussionId": "694a4fe4335742716e9323d8",
      "githubRepo": "https://github.com/m-Just/InSight-o3",
      "githubRepoAddedBy": "user",
      "ai_summary": "O3-Bench evaluates multimodal reasoning with interleaved attention to visual details, while InSight-o3 uses a multi-agent framework to improve performance through specialized visual search and reasoning tasks.",
      "ai_keywords": [
        "multimodal agents",
        "reasoning",
        "perception",
        "O3-Bench",
        "interleaved attention",
        "visual reasoning",
        "visual search",
        "vReasoner",
        "vSearcher",
        "generalized visual search",
        "multimodal LLM",
        "reinforcement learning"
      ]
    },
    "publishedAt": "2025-12-21T09:23:07.000Z",
    "title": "InSight-o3: Empowering Multimodal Foundation Models with Generalized Visual Search",
    "summary": "The ability for AI agents to \"think with images\" requires a sophisticated blend of reasoning and perception. However, current open multimodal agents still largely fall short on the reasoning aspect crucial for real-world tasks like analyzing documents with dense charts/diagrams and navigating maps. To address this gap, we introduce O3-Bench, a new benchmark designed to evaluate multimodal reasoning with interleaved attention to visual details. O3-Bench features challenging problems that require agents to piece together subtle visual information from distinct image areas through multi-step reasoning. The problems are highly challenging even for frontier systems like OpenAI o3, which only obtains 40.8% accuracy on O3-Bench. To make progress, we propose InSight-o3, a multi-agent framework consisting of a visual reasoning agent (vReasoner) and a visual search agent (vSearcher) for which we introduce the task of generalized visual search -- locating relational, fuzzy, or conceptual regions described in free-form language, beyond just simple objects or figures in natural images. We then present a multimodal LLM purpose-trained for this task via reinforcement learning. As a plug-and-play agent, our vSearcher empowers frontier multimodal models (as vReasoners), significantly improving their performance on a wide range of benchmarks. This marks a concrete step towards powerful o3-like open systems. Our code and dataset can be found at https://github.com/m-Just/InSight-o3 .",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.18745.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65d5b967eeb590ea7435ad07",
      "avatarUrl": "/avatars/ca0a5e123d5da5aca97cfd8a2d07e60e.svg",
      "fullname": "Kaican Li",
      "name": "m-Just",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2512.21980",
      "authors": [
        {
          "_id": "69522805746a34b55dd548fd",
          "name": "A. I. Perminov",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-26T10:58:54.000Z",
      "submittedOnDailyAt": "2025-12-29T04:37:42.134Z",
      "title": "A 58-Addition, Rank-23 Scheme for General 3x3 Matrix Multiplication",
      "submittedOnDailyBy": {
        "_id": "6447e4e6e214848834046f0f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6447e4e6e214848834046f0f/Vaxv_BuDaZRdR1MJ0IrLT.png",
        "isPro": false,
        "fullname": "Perminov Andrew",
        "user": "dronperminov",
        "type": "user"
      },
      "summary": "This paper presents a new state-of-the-art algorithm for exact 3times3 matrix multiplication over general non-commutative rings, achieving a rank-23 scheme with only 58 scalar additions. This improves the previous best additive complexity of 60 additions without a change of basis. The result was discovered through an automated search combining ternary-restricted flip-graph exploration with greedy intersection reduction for common subexpression elimination. The resulting scheme uses only coefficients from {-1, 0, 1}, ensuring both efficiency and portability across arbitrary fields. The total scalar operation count is reduced from 83 to 81.",
      "upvotes": 0,
      "discussionId": "69522805746a34b55dd548fe",
      "githubRepo": "https://github.com/dronperminov/ternary_flip_graph",
      "githubRepoAddedBy": "user",
      "githubStars": 0
    },
    "publishedAt": "2025-12-26T05:58:54.000Z",
    "title": "A 58-Addition, Rank-23 Scheme for General 3x3 Matrix Multiplication",
    "summary": "This paper presents a new state-of-the-art algorithm for exact 3times3 matrix multiplication over general non-commutative rings, achieving a rank-23 scheme with only 58 scalar additions. This improves the previous best additive complexity of 60 additions without a change of basis. The result was discovered through an automated search combining ternary-restricted flip-graph exploration with greedy intersection reduction for common subexpression elimination. The resulting scheme uses only coefficients from {-1, 0, 1}, ensuring both efficiency and portability across arbitrary fields. The total scalar operation count is reduced from 83 to 81.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.21980.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6447e4e6e214848834046f0f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6447e4e6e214848834046f0f/Vaxv_BuDaZRdR1MJ0IrLT.png",
      "fullname": "Perminov Andrew",
      "name": "dronperminov",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]