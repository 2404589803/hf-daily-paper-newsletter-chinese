[
  {
    "paper": {
      "id": "2510.27492",
      "authors": [
        {
          "_id": "690813a7812eca10f9cc5e01",
          "name": "Jiawei Gu",
          "hidden": false
        },
        {
          "_id": "690813a7812eca10f9cc5e02",
          "name": "Yunzhuo Hao",
          "hidden": false
        },
        {
          "_id": "690813a7812eca10f9cc5e03",
          "name": "Huichen Will Wang",
          "hidden": false
        },
        {
          "_id": "690813a7812eca10f9cc5e04",
          "name": "Linjie Li",
          "hidden": false
        },
        {
          "_id": "690813a7812eca10f9cc5e05",
          "name": "Michael Qizhe Shieh",
          "hidden": false
        },
        {
          "_id": "690813a7812eca10f9cc5e06",
          "name": "Yejin Choi",
          "hidden": false
        },
        {
          "_id": "690813a7812eca10f9cc5e07",
          "name": "Ranjay Krishna",
          "hidden": false
        },
        {
          "_id": "690813a7812eca10f9cc5e08",
          "name": "Yu Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-30T17:51:38.000Z",
      "submittedOnDailyAt": "2025-11-03T00:17:01.674Z",
      "title": "ThinkMorph: Emergent Properties in Multimodal Interleaved\n  Chain-of-Thought Reasoning",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Multimodal reasoning requires iterative coordination between language and\nvision, yet it remains unclear what constitutes a meaningful interleaved chain\nof thought. We posit that text and image thoughts should function as\ncomplementary, rather than isomorphic, modalities that mutually advance\nreasoning. Guided by this principle, we build ThinkMorph, a unified model\nfine-tuned on 24K high-quality interleaved reasoning traces spanning tasks with\nvarying visual engagement. ThinkMorph learns to generate progressive text-image\nreasoning steps that concretely manipulate visual content while maintaining\ncoherent verbal logic. It delivers large gains on vision-centric benchmarks\n(averaging 34.7% over the base model) and generalizes to out-of-domain tasks,\nmatching or surpassing larger and proprietary VLMs. Beyond performance,\nThinkMorph exhibits emergent multimodal intelligence, including unseen visual\nmanipulation skills, adaptive switching between reasoning modes, and better\ntest-time scaling through diversified multimodal thoughts.These findings\nsuggest promising directions for characterizing the emergent capabilities of\nunified models for multimodal reasoning.",
      "upvotes": 37,
      "discussionId": "690813a7812eca10f9cc5e09",
      "projectPage": "https://thinkmorph.github.io/",
      "githubRepo": "https://github.com/ThinkMorph/ThinkMorph",
      "githubStars": 5
    },
    "publishedAt": "2025-10-30T13:51:38.000Z",
    "title": "ThinkMorph: Emergent Properties in Multimodal Interleaved\n  Chain-of-Thought Reasoning",
    "summary": "Multimodal reasoning requires iterative coordination between language and\nvision, yet it remains unclear what constitutes a meaningful interleaved chain\nof thought. We posit that text and image thoughts should function as\ncomplementary, rather than isomorphic, modalities that mutually advance\nreasoning. Guided by this principle, we build ThinkMorph, a unified model\nfine-tuned on 24K high-quality interleaved reasoning traces spanning tasks with\nvarying visual engagement. ThinkMorph learns to generate progressive text-image\nreasoning steps that concretely manipulate visual content while maintaining\ncoherent verbal logic. It delivers large gains on vision-centric benchmarks\n(averaging 34.7% over the base model) and generalizes to out-of-domain tasks,\nmatching or surpassing larger and proprietary VLMs. Beyond performance,\nThinkMorph exhibits emergent multimodal intelligence, including unseen visual\nmanipulation skills, adaptive switching between reasoning modes, and better\ntest-time scaling through diversified multimodal thoughts.These findings\nsuggest promising directions for characterizing the emergent capabilities of\nunified models for multimodal reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.27492.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 154
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.25602",
      "authors": [
        {
          "_id": "69081d74812eca10f9cc5e7a",
          "name": "Mengzhao Chen",
          "hidden": false
        },
        {
          "_id": "69081d74812eca10f9cc5e7b",
          "name": "Meng Wu",
          "hidden": false
        },
        {
          "_id": "69081d74812eca10f9cc5e7c",
          "name": "Hui Jin",
          "hidden": false
        },
        {
          "_id": "69081d74812eca10f9cc5e7d",
          "name": "Zhihang Yuan",
          "hidden": false
        },
        {
          "_id": "69081d74812eca10f9cc5e7e",
          "name": "Jing Liu",
          "hidden": false
        },
        {
          "_id": "69081d74812eca10f9cc5e7f",
          "name": "Chaoyi Zhang",
          "hidden": false
        },
        {
          "_id": "69081d74812eca10f9cc5e80",
          "name": "Yunshui Li",
          "hidden": false
        },
        {
          "_id": "69081d74812eca10f9cc5e81",
          "name": "Jie Huang",
          "hidden": false
        },
        {
          "_id": "69081d74812eca10f9cc5e82",
          "name": "Jin Ma",
          "hidden": false
        },
        {
          "_id": "69081d74812eca10f9cc5e83",
          "name": "Zeyue Xue",
          "hidden": false
        },
        {
          "_id": "69081d74812eca10f9cc5e84",
          "name": "Zhiheng Liu",
          "hidden": false
        },
        {
          "_id": "69081d74812eca10f9cc5e85",
          "name": "Xingyan Bin",
          "hidden": false
        },
        {
          "_id": "69081d74812eca10f9cc5e86",
          "name": "Ping Luo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-29T15:11:53.000Z",
      "submittedOnDailyAt": "2025-11-03T00:44:27.697Z",
      "title": "INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization\n  Formats",
      "submittedOnDailyBy": {
        "_id": "64aea082704210bf815e7551",
        "avatarUrl": "/avatars/5c8dc0df57596c526b2bccea21835f53.svg",
        "isPro": false,
        "fullname": "Mengzhao Chen",
        "user": "ChenMnZ",
        "type": "user"
      },
      "summary": "Modern AI hardware, such as Nvidia's Blackwell architecture, is increasingly\nembracing low-precision floating-point (FP) formats to handle the pervasive\nactivation outliers in Large Language Models (LLMs). Despite this industry\ntrend, a unified comparison of FP and integer (INT) quantization across varying\ngranularities has been missing, leaving algorithm and hardware co-design\nwithout clear guidance. This paper fills that gap by systematically\ninvestigating the trade-offs between FP and INT formats. We reveal a critical\nperformance crossover: while FP excels in coarse-grained quantization, the\ncomparison at fine-grained (block-wise) levels is more nuanced. Our\ncomprehensive comparison demonstrates that for popular 8-bit fine-grained\nformats (e.g., MX with block size 32), MXINT8 is superior to its FP counterpart\nin both algorithmic accuracy and hardware efficiency. However, for 4-bit\nformats, FP (e.g., MXFP4, NVFP4) often holds an accuracy advantage , though we\nshow that NVINT4 can surpass NVFP4 when outlier-mitigation techniques like\nHadamard rotation are applied. We also introduce a symmetric clipping method\nthat resolves gradient bias in fine-grained low-bit INT training, enabling\nnearly lossless performance for MXINT8 training. These findings challenge the\ncurrent hardware trajectory, demonstrating that a one-size-fits-all FP approach\nis suboptimal and advocating that fine-grained INT formats, particularly\nMXINT8, offer a better balance of accuracy, power, and efficiency for future AI\naccelerators.",
      "upvotes": 32,
      "discussionId": "69081d75812eca10f9cc5e87",
      "githubRepo": "https://github.com/ChenMnZ/INT_vs_FP",
      "githubStars": 16,
      "organization": {
        "_id": "67d1140985ea0644e2f14b99",
        "name": "ByteDance-Seed",
        "fullname": "ByteDance Seed",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
      }
    },
    "publishedAt": "2025-10-29T11:11:53.000Z",
    "title": "INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization\n  Formats",
    "summary": "Modern AI hardware, such as Nvidia's Blackwell architecture, is increasingly\nembracing low-precision floating-point (FP) formats to handle the pervasive\nactivation outliers in Large Language Models (LLMs). Despite this industry\ntrend, a unified comparison of FP and integer (INT) quantization across varying\ngranularities has been missing, leaving algorithm and hardware co-design\nwithout clear guidance. This paper fills that gap by systematically\ninvestigating the trade-offs between FP and INT formats. We reveal a critical\nperformance crossover: while FP excels in coarse-grained quantization, the\ncomparison at fine-grained (block-wise) levels is more nuanced. Our\ncomprehensive comparison demonstrates that for popular 8-bit fine-grained\nformats (e.g., MX with block size 32), MXINT8 is superior to its FP counterpart\nin both algorithmic accuracy and hardware efficiency. However, for 4-bit\nformats, FP (e.g., MXFP4, NVFP4) often holds an accuracy advantage , though we\nshow that NVINT4 can surpass NVFP4 when outlier-mitigation techniques like\nHadamard rotation are applied. We also introduce a symmetric clipping method\nthat resolves gradient bias in fine-grained low-bit INT training, enabling\nnearly lossless performance for MXINT8 training. These findings challenge the\ncurrent hardware trajectory, demonstrating that a one-size-fits-all FP approach\nis suboptimal and advocating that fine-grained INT formats, particularly\nMXINT8, offer a better balance of accuracy, power, and efficiency for future AI\naccelerators.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.25602.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64aea082704210bf815e7551",
      "avatarUrl": "/avatars/5c8dc0df57596c526b2bccea21835f53.svg",
      "fullname": "Mengzhao Chen",
      "name": "ChenMnZ",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 23
    },
    "organization": {
      "_id": "67d1140985ea0644e2f14b99",
      "name": "ByteDance-Seed",
      "fullname": "ByteDance Seed",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.24411",
      "authors": [
        {
          "_id": "6901c572646208eac0d1f58b",
          "user": {
            "_id": "6064a0eeb1703ddba0d458b9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1617207525789-noauth.png",
            "isPro": false,
            "fullname": "Qiushi",
            "user": "QiushiSun",
            "type": "user"
          },
          "name": "Qiushi Sun",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-30T14:41:31.108Z",
          "hidden": false
        },
        {
          "_id": "6901c572646208eac0d1f58c",
          "name": "Mukai Li",
          "hidden": false
        },
        {
          "_id": "6901c572646208eac0d1f58d",
          "name": "Zhoumianze Liu",
          "hidden": false
        },
        {
          "_id": "6901c572646208eac0d1f58e",
          "name": "Zhihui Xie",
          "hidden": false
        },
        {
          "_id": "6901c572646208eac0d1f58f",
          "name": "Fangzhi Xu",
          "hidden": false
        },
        {
          "_id": "6901c572646208eac0d1f590",
          "name": "Zhangyue Yin",
          "hidden": false
        },
        {
          "_id": "6901c572646208eac0d1f591",
          "name": "Kanzhi Cheng",
          "hidden": false
        },
        {
          "_id": "6901c572646208eac0d1f592",
          "name": "Zehao Li",
          "hidden": false
        },
        {
          "_id": "6901c572646208eac0d1f593",
          "name": "Zichen Ding",
          "hidden": false
        },
        {
          "_id": "6901c572646208eac0d1f594",
          "name": "Qi Liu",
          "hidden": false
        },
        {
          "_id": "6901c572646208eac0d1f595",
          "name": "Zhiyong Wu",
          "hidden": false
        },
        {
          "_id": "6901c572646208eac0d1f596",
          "name": "Zhuosheng Zhang",
          "hidden": false
        },
        {
          "_id": "6901c572646208eac0d1f597",
          "name": "Ben Kao",
          "hidden": false
        },
        {
          "_id": "6901c572646208eac0d1f598",
          "name": "Lingpeng Kong",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6064a0eeb1703ddba0d458b9/bw0LkLXWMJv2phqt5NTPT.png",
        "https://cdn-uploads.huggingface.co/production/uploads/6064a0eeb1703ddba0d458b9/LzYmfXUDFU8qdRwvvP_t1.png",
        "https://cdn-uploads.huggingface.co/production/uploads/6064a0eeb1703ddba0d458b9/oYs157OjuXN_AKMQb2vrl.png",
        "https://cdn-uploads.huggingface.co/production/uploads/6064a0eeb1703ddba0d458b9/iBKBpbGwt3NyyxB-BuDX5.png"
      ],
      "publishedAt": "2025-10-28T13:22:39.000Z",
      "submittedOnDailyAt": "2025-11-03T00:39:54.915Z",
      "title": "OS-Sentinel: Towards Safety-Enhanced Mobile GUI Agents via Hybrid\n  Validation in Realistic Workflows",
      "submittedOnDailyBy": {
        "_id": "6064a0eeb1703ddba0d458b9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1617207525789-noauth.png",
        "isPro": false,
        "fullname": "Qiushi",
        "user": "QiushiSun",
        "type": "user"
      },
      "summary": "Computer-using agents powered by Vision-Language Models (VLMs) have\ndemonstrated human-like capabilities in operating digital environments like\nmobile platforms. While these agents hold great promise for advancing digital\nautomation, their potential for unsafe operations, such as system compromise\nand privacy leakage, is raising significant concerns. Detecting these safety\nconcerns across the vast and complex operational space of mobile environments\npresents a formidable challenge that remains critically underexplored. To\nestablish a foundation for mobile agent safety research, we introduce\nMobileRisk-Live, a dynamic sandbox environment accompanied by a safety\ndetection benchmark comprising realistic trajectories with fine-grained\nannotations. Built upon this, we propose OS-Sentinel, a novel hybrid safety\ndetection framework that synergistically combines a Formal Verifier for\ndetecting explicit system-level violations with a VLM-based Contextual Judge\nfor assessing contextual risks and agent actions. Experiments show that\nOS-Sentinel achieves 10%-30% improvements over existing approaches across\nmultiple metrics. Further analysis provides critical insights that foster the\ndevelopment of safer and more reliable autonomous mobile agents.",
      "upvotes": 25,
      "discussionId": "6901c572646208eac0d1f599",
      "githubRepo": "https://github.com/OS-Copilot/OS-Sentinel",
      "ai_summary": "A hybrid safety detection framework combining formal verification and VLM-based contextual assessment improves the detection of unsafe operations in mobile agents.",
      "ai_keywords": [
        "Vision-Language Models",
        "VLMs",
        "Formal Verifier",
        "Contextual Judge",
        "MobileRisk-Live",
        "safety detection benchmark",
        "system-level violations",
        "contextual risks",
        "autonomous mobile agents"
      ],
      "githubStars": 5,
      "organization": {
        "_id": "61bb0986699d29d369eba1b2",
        "name": "hkunlp",
        "fullname": "NLP Group of The University of Hong Kong",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1639647572687-618767e4238063b4615d042b.jpeg"
      }
    },
    "publishedAt": "2025-10-28T09:22:39.000Z",
    "title": "OS-Sentinel: Towards Safety-Enhanced Mobile GUI Agents via Hybrid\n  Validation in Realistic Workflows",
    "summary": "Computer-using agents powered by Vision-Language Models (VLMs) have\ndemonstrated human-like capabilities in operating digital environments like\nmobile platforms. While these agents hold great promise for advancing digital\nautomation, their potential for unsafe operations, such as system compromise\nand privacy leakage, is raising significant concerns. Detecting these safety\nconcerns across the vast and complex operational space of mobile environments\npresents a formidable challenge that remains critically underexplored. To\nestablish a foundation for mobile agent safety research, we introduce\nMobileRisk-Live, a dynamic sandbox environment accompanied by a safety\ndetection benchmark comprising realistic trajectories with fine-grained\nannotations. Built upon this, we propose OS-Sentinel, a novel hybrid safety\ndetection framework that synergistically combines a Formal Verifier for\ndetecting explicit system-level violations with a VLM-based Contextual Judge\nfor assessing contextual risks and agent actions. Experiments show that\nOS-Sentinel achieves 10%-30% improvements over existing approaches across\nmultiple metrics. Further analysis provides critical insights that foster the\ndevelopment of safer and more reliable autonomous mobile agents.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6064a0eeb1703ddba0d458b9/bw0LkLXWMJv2phqt5NTPT.png",
      "https://cdn-uploads.huggingface.co/production/uploads/6064a0eeb1703ddba0d458b9/LzYmfXUDFU8qdRwvvP_t1.png",
      "https://cdn-uploads.huggingface.co/production/uploads/6064a0eeb1703ddba0d458b9/oYs157OjuXN_AKMQb2vrl.png",
      "https://cdn-uploads.huggingface.co/production/uploads/6064a0eeb1703ddba0d458b9/iBKBpbGwt3NyyxB-BuDX5.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.24411.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6064a0eeb1703ddba0d458b9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1617207525789-noauth.png",
      "fullname": "Qiushi",
      "name": "QiushiSun",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 20
    },
    "organization": {
      "_id": "61bb0986699d29d369eba1b2",
      "name": "hkunlp",
      "fullname": "NLP Group of The University of Hong Kong",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1639647572687-618767e4238063b4615d042b.jpeg"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.25889",
      "authors": [
        {
          "_id": "690832af812eca10f9cc5ec0",
          "name": "Kang Chen",
          "hidden": false
        },
        {
          "_id": "690832af812eca10f9cc5ec1",
          "name": "Zhihao Liu",
          "hidden": false
        },
        {
          "_id": "690832af812eca10f9cc5ec2",
          "name": "Tonghe Zhang",
          "hidden": false
        },
        {
          "_id": "690832af812eca10f9cc5ec3",
          "name": "Zhen Guo",
          "hidden": false
        },
        {
          "_id": "690832af812eca10f9cc5ec4",
          "name": "Si Xu",
          "hidden": false
        },
        {
          "_id": "690832af812eca10f9cc5ec5",
          "name": "Hao Lin",
          "hidden": false
        },
        {
          "_id": "690832af812eca10f9cc5ec6",
          "name": "Hongzhi Zang",
          "hidden": false
        },
        {
          "_id": "690832af812eca10f9cc5ec7",
          "name": "Quanlu Zhang",
          "hidden": false
        },
        {
          "_id": "690832af812eca10f9cc5ec8",
          "name": "Zhaofei Yu",
          "hidden": false
        },
        {
          "_id": "690832af812eca10f9cc5ec9",
          "name": "Guoliang Fan",
          "hidden": false
        },
        {
          "_id": "690832af812eca10f9cc5eca",
          "name": "Tiejun Huang",
          "hidden": false
        },
        {
          "_id": "690832af812eca10f9cc5ecb",
          "name": "Yu Wang",
          "hidden": false
        },
        {
          "_id": "690832af812eca10f9cc5ecc",
          "name": "Chao Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-29T18:37:39.000Z",
      "submittedOnDailyAt": "2025-11-03T02:15:50.813Z",
      "title": "π_RL: Online RL Fine-tuning for Flow-based\n  Vision-Language-Action Models",
      "submittedOnDailyBy": {
        "_id": "64ba0f8d842aa47891cb972b",
        "avatarUrl": "/avatars/3bcacd9b778a146e88e20887b0b00720.svg",
        "isPro": false,
        "fullname": "Chao Yu",
        "user": "zoeyuchao",
        "type": "user"
      },
      "summary": "Vision-Language-Action (VLA) models enable robots to understand and perform\ncomplex tasks from multimodal input. Although recent work explores using\nreinforcement learning (RL) to automate the laborious data collection process\nin scaling supervised fine-tuning (SFT), applying large-scale RL to flow-based\nVLAs (e.g., pi_0, pi_{0.5}) remains challenging due to intractable action\nlog-likelihoods from iterative denoising.\n  We address this challenge with pi_{RL}, an open-source framework\nfor training flow-based VLAs in parallel simulation. pi_{RL}\nimplements two RL algorithms: (1) {Flow-Noise} models the denoising process as\na discrete-time MDP with a learnable noise network for exact log-likelihood\ncomputation. (2) {Flow-SDE} integrates denoising with agent-environment\ninteraction, formulating a two-layer MDP that employs ODE-to-SDE conversion for\nefficient RL exploration.\n  We evaluate pi_{RL} on LIBERO and ManiSkill benchmarks. On LIBERO,\npi_{RL} boosts few-shot SFT models pi_0 and pi_{0.5} from 57.6%\nto 97.6% and from 77.1% to 98.3%, respectively. In ManiSkill, we train\npi_{RL} in 320 parallel environments, improving pi_0 from 41.6% to\n85.7% and pi_{0.5} from 40.0% to 84.8% across 4352 pick-and-place tasks,\ndemonstrating scalable multitask RL under heterogeneous simulation.\n  Overall, pi_{RL} achieves significant performance gains and\nstronger generalization over SFT-models, validating the effectiveness of online\nRL for flow-based VLAs.",
      "upvotes": 24,
      "discussionId": "690832af812eca10f9cc5ecd",
      "projectPage": "https://rlinf.readthedocs.io/en/latest/rst_source/examples/pi0.html",
      "organization": {
        "_id": "689ea978824b212c988bc8f5",
        "name": "RLinf",
        "fullname": "RLinf",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/689ea8a1a73ecc6940dbba3d/T2RGCw18z6lYP1WfkIGJ3.jpeg"
      }
    },
    "publishedAt": "2025-10-29T14:37:39.000Z",
    "title": "π_RL: Online RL Fine-tuning for Flow-based\n  Vision-Language-Action Models",
    "summary": "Vision-Language-Action (VLA) models enable robots to understand and perform\ncomplex tasks from multimodal input. Although recent work explores using\nreinforcement learning (RL) to automate the laborious data collection process\nin scaling supervised fine-tuning (SFT), applying large-scale RL to flow-based\nVLAs (e.g., pi_0, pi_{0.5}) remains challenging due to intractable action\nlog-likelihoods from iterative denoising.\n  We address this challenge with pi_{RL}, an open-source framework\nfor training flow-based VLAs in parallel simulation. pi_{RL}\nimplements two RL algorithms: (1) {Flow-Noise} models the denoising process as\na discrete-time MDP with a learnable noise network for exact log-likelihood\ncomputation. (2) {Flow-SDE} integrates denoising with agent-environment\ninteraction, formulating a two-layer MDP that employs ODE-to-SDE conversion for\nefficient RL exploration.\n  We evaluate pi_{RL} on LIBERO and ManiSkill benchmarks. On LIBERO,\npi_{RL} boosts few-shot SFT models pi_0 and pi_{0.5} from 57.6%\nto 97.6% and from 77.1% to 98.3%, respectively. In ManiSkill, we train\npi_{RL} in 320 parallel environments, improving pi_0 from 41.6% to\n85.7% and pi_{0.5} from 40.0% to 84.8% across 4352 pick-and-place tasks,\ndemonstrating scalable multitask RL under heterogeneous simulation.\n  Overall, pi_{RL} achieves significant performance gains and\nstronger generalization over SFT-models, validating the effectiveness of online\nRL for flow-based VLAs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.25889.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64ba0f8d842aa47891cb972b",
      "avatarUrl": "/avatars/3bcacd9b778a146e88e20887b0b00720.svg",
      "fullname": "Chao Yu",
      "name": "zoeyuchao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "689ea978824b212c988bc8f5",
      "name": "RLinf",
      "fullname": "RLinf",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/689ea8a1a73ecc6940dbba3d/T2RGCw18z6lYP1WfkIGJ3.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.27688",
      "authors": [
        {
          "_id": "69080ebd812eca10f9cc5df3",
          "name": "Chenze Shao",
          "hidden": false
        },
        {
          "_id": "69080ebd812eca10f9cc5df4",
          "name": "Darren Li",
          "hidden": false
        },
        {
          "_id": "69080ebd812eca10f9cc5df5",
          "name": "Fandong Meng",
          "hidden": false
        },
        {
          "_id": "69080ebd812eca10f9cc5df6",
          "name": "Jie Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-31T17:58:11.000Z",
      "submittedOnDailyAt": "2025-11-03T00:15:11.394Z",
      "title": "Continuous Autoregressive Language Models",
      "submittedOnDailyBy": {
        "_id": "67a42bf8dba32bb665e351ad",
        "avatarUrl": "/avatars/9c13810fe789ddcd9cefd4f2c924e4aa.svg",
        "isPro": false,
        "fullname": "Chenze Shao",
        "user": "cccczshao",
        "type": "user"
      },
      "summary": "The efficiency of large language models (LLMs) is fundamentally limited by\ntheir sequential, token-by-token generation process. We argue that overcoming\nthis bottleneck requires a new design axis for LLM scaling: increasing the\nsemantic bandwidth of each generative step. To this end, we introduce\nContinuous Autoregressive Language Models (CALM), a paradigm shift from\ndiscrete next-token prediction to continuous next-vector prediction. CALM uses\na high-fidelity autoencoder to compress a chunk of K tokens into a single\ncontinuous vector, from which the original tokens can be reconstructed with\nover 99.9\\% accuracy. This allows us to model language as a sequence of\ncontinuous vectors instead of discrete tokens, which reduces the number of\ngenerative steps by a factor of K. The paradigm shift necessitates a new\nmodeling toolkit; therefore, we develop a comprehensive likelihood-free\nframework that enables robust training, evaluation, and controllable sampling\nin the continuous domain. Experiments show that CALM significantly improves the\nperformance-compute trade-off, achieving the performance of strong discrete\nbaselines at a significantly lower computational cost. More importantly, these\nfindings establish next-vector prediction as a powerful and scalable pathway\ntowards ultra-efficient language models. Code:\nhttps://github.com/shaochenze/calm. Project:\nhttps://shaochenze.github.io/blog/2025/CALM.",
      "upvotes": 23,
      "discussionId": "69080ebd812eca10f9cc5df7",
      "projectPage": "https://shaochenze.github.io/blog/2025/CALM/",
      "githubRepo": "https://github.com/shaochenze/calm",
      "githubStars": 8,
      "organization": {
        "_id": "66543b6e420092799d2f625c",
        "name": "tencent",
        "fullname": "Tencent",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
      }
    },
    "publishedAt": "2025-10-31T13:58:11.000Z",
    "title": "Continuous Autoregressive Language Models",
    "summary": "The efficiency of large language models (LLMs) is fundamentally limited by\ntheir sequential, token-by-token generation process. We argue that overcoming\nthis bottleneck requires a new design axis for LLM scaling: increasing the\nsemantic bandwidth of each generative step. To this end, we introduce\nContinuous Autoregressive Language Models (CALM), a paradigm shift from\ndiscrete next-token prediction to continuous next-vector prediction. CALM uses\na high-fidelity autoencoder to compress a chunk of K tokens into a single\ncontinuous vector, from which the original tokens can be reconstructed with\nover 99.9\\% accuracy. This allows us to model language as a sequence of\ncontinuous vectors instead of discrete tokens, which reduces the number of\ngenerative steps by a factor of K. The paradigm shift necessitates a new\nmodeling toolkit; therefore, we develop a comprehensive likelihood-free\nframework that enables robust training, evaluation, and controllable sampling\nin the continuous domain. Experiments show that CALM significantly improves the\nperformance-compute trade-off, achieving the performance of strong discrete\nbaselines at a significantly lower computational cost. More importantly, these\nfindings establish next-vector prediction as a powerful and scalable pathway\ntowards ultra-efficient language models. Code:\nhttps://github.com/shaochenze/calm. Project:\nhttps://shaochenze.github.io/blog/2025/CALM.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.27688.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67a42bf8dba32bb665e351ad",
      "avatarUrl": "/avatars/9c13810fe789ddcd9cefd4f2c924e4aa.svg",
      "fullname": "Chenze Shao",
      "name": "cccczshao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "66543b6e420092799d2f625c",
      "name": "tencent",
      "fullname": "Tencent",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.27606",
      "authors": [
        {
          "_id": "6908284d812eca10f9cc5e92",
          "name": "Yuhong Liu",
          "hidden": false
        },
        {
          "_id": "6908284d812eca10f9cc5e93",
          "name": "Beichen Zhang",
          "hidden": false
        },
        {
          "_id": "6908284d812eca10f9cc5e94",
          "name": "Yuhang Zang",
          "hidden": false
        },
        {
          "_id": "6908284d812eca10f9cc5e95",
          "name": "Yuhang Cao",
          "hidden": false
        },
        {
          "_id": "6908284d812eca10f9cc5e96",
          "name": "Long Xing",
          "hidden": false
        },
        {
          "_id": "6908284d812eca10f9cc5e97",
          "name": "Xiaoyi Dong",
          "hidden": false
        },
        {
          "_id": "6908284d812eca10f9cc5e98",
          "name": "Haodong Duan",
          "hidden": false
        },
        {
          "_id": "6908284d812eca10f9cc5e99",
          "name": "Dahua Lin",
          "hidden": false
        },
        {
          "_id": "6908284d812eca10f9cc5e9a",
          "name": "Jiaqi Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-31T16:30:08.000Z",
      "submittedOnDailyAt": "2025-11-03T01:34:20.749Z",
      "title": "Spatial-SSRL: Enhancing Spatial Understanding via Self-Supervised\n  Reinforcement Learning",
      "submittedOnDailyBy": {
        "_id": "63859cf3b2906edaf83af9f0",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63859cf3b2906edaf83af9f0/kajwuVzd4pDucSPlwghxo.png",
        "isPro": true,
        "fullname": "Yuhang Zang",
        "user": "yuhangzang",
        "type": "user"
      },
      "summary": "Spatial understanding remains a weakness of Large Vision-Language Models\n(LVLMs). Existing supervised fine-tuning (SFT) and recent reinforcement\nlearning with verifiable rewards (RLVR) pipelines depend on costly supervision,\nspecialized tools, or constrained environments that limit scale. We introduce\nSpatial-SSRL, a self-supervised RL paradigm that derives verifiable signals\ndirectly from ordinary RGB or RGB-D images. Spatial-SSRL automatically\nformulates five pretext tasks that capture 2D and 3D spatial structure:\nshuffled patch reordering, flipped patch recognition, cropped patch inpainting,\nregional depth ordering, and relative 3D position prediction. These tasks\nprovide ground-truth answers that are easy to verify and require no human or\nLVLM annotation. Training on our tasks substantially improves spatial reasoning\nwhile preserving general visual capabilities. On seven spatial understanding\nbenchmarks in both image and video settings, Spatial-SSRL delivers average\naccuracy gains of 4.63% (3B) and 3.89% (7B) over the Qwen2.5-VL baselines. Our\nresults show that simple, intrinsic supervision enables RLVR at scale and\nprovides a practical route to stronger spatial intelligence in LVLMs.",
      "upvotes": 14,
      "discussionId": "6908284e812eca10f9cc5e9b",
      "githubRepo": "https://github.com/InternLM/Spatial-SSRL",
      "githubStars": 6,
      "organization": {
        "_id": "64a2d5fa81252883206f24c9",
        "name": "internlm",
        "fullname": "Intern Large Models",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6445306bc525660aa2099ecc/ipmEgm86UIby2q5q7NkKm.jpeg"
      }
    },
    "publishedAt": "2025-10-31T12:30:08.000Z",
    "title": "Spatial-SSRL: Enhancing Spatial Understanding via Self-Supervised\n  Reinforcement Learning",
    "summary": "Spatial understanding remains a weakness of Large Vision-Language Models\n(LVLMs). Existing supervised fine-tuning (SFT) and recent reinforcement\nlearning with verifiable rewards (RLVR) pipelines depend on costly supervision,\nspecialized tools, or constrained environments that limit scale. We introduce\nSpatial-SSRL, a self-supervised RL paradigm that derives verifiable signals\ndirectly from ordinary RGB or RGB-D images. Spatial-SSRL automatically\nformulates five pretext tasks that capture 2D and 3D spatial structure:\nshuffled patch reordering, flipped patch recognition, cropped patch inpainting,\nregional depth ordering, and relative 3D position prediction. These tasks\nprovide ground-truth answers that are easy to verify and require no human or\nLVLM annotation. Training on our tasks substantially improves spatial reasoning\nwhile preserving general visual capabilities. On seven spatial understanding\nbenchmarks in both image and video settings, Spatial-SSRL delivers average\naccuracy gains of 4.63% (3B) and 3.89% (7B) over the Qwen2.5-VL baselines. Our\nresults show that simple, intrinsic supervision enables RLVR at scale and\nprovides a practical route to stronger spatial intelligence in LVLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.27606.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63859cf3b2906edaf83af9f0",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63859cf3b2906edaf83af9f0/kajwuVzd4pDucSPlwghxo.png",
      "fullname": "Yuhang Zang",
      "name": "yuhangzang",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "organization": {
      "_id": "64a2d5fa81252883206f24c9",
      "name": "internlm",
      "fullname": "Intern Large Models",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6445306bc525660aa2099ecc/ipmEgm86UIby2q5q7NkKm.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.27266",
      "authors": [
        {
          "_id": "69081881812eca10f9cc5e4f",
          "name": "Shaojie Zhang",
          "hidden": false
        },
        {
          "_id": "69081881812eca10f9cc5e50",
          "name": "Pei Fu",
          "hidden": false
        },
        {
          "_id": "69081881812eca10f9cc5e51",
          "name": "Ruoceng Zhang",
          "hidden": false
        },
        {
          "_id": "69081881812eca10f9cc5e52",
          "name": "Jiahui Yang",
          "hidden": false
        },
        {
          "_id": "69081881812eca10f9cc5e53",
          "name": "Anan Du",
          "hidden": false
        },
        {
          "_id": "69081881812eca10f9cc5e54",
          "name": "Xiuwen Xi",
          "hidden": false
        },
        {
          "_id": "69081881812eca10f9cc5e55",
          "name": "Shaokang Wang",
          "hidden": false
        },
        {
          "_id": "69081881812eca10f9cc5e56",
          "name": "Ying Huang",
          "hidden": false
        },
        {
          "_id": "69081881812eca10f9cc5e57",
          "name": "Bin Qin",
          "hidden": false
        },
        {
          "_id": "69081881812eca10f9cc5e58",
          "name": "Zhenbo Luo",
          "hidden": false
        },
        {
          "_id": "69081881812eca10f9cc5e59",
          "name": "Jian Luan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-31T08:07:02.000Z",
      "submittedOnDailyAt": "2025-11-03T00:20:53.322Z",
      "title": "HyperClick: Advancing Reliable GUI Grounding via Uncertainty Calibration",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Autonomous Graphical User Interface (GUI) agents rely on accurate GUI\ngrounding, which maps language instructions to on-screen coordinates, to\nexecute user commands. However, current models, whether trained via supervised\nfine-tuning (SFT) or reinforcement fine-tuning (RFT), lack self-awareness of\ntheir capability boundaries, leading to overconfidence and unreliable\npredictions. We first systematically evaluate probabilistic and verbalized\nconfidence in general and GUI-specific models, revealing a misalignment between\nconfidence and actual accuracy, which is particularly critical in dynamic GUI\nautomation tasks, where single errors can cause task failure. To address this,\nwe propose HyperClick, a novel framework that enhances reliable GUI grounding\nthrough uncertainty calibration. HyperClick introduces a dual reward mechanism,\ncombining a binary reward for correct actions with a truncated Gaussian-based\nspatial confidence modeling, calibrated using the Brier score. This approach\njointly optimizes grounding accuracy and confidence reliability, fostering\nintrospective self-criticism. Extensive experiments on seven challenge\nbenchmarks show that HyperClick achieves state-of-the-art performance while\nproviding well-calibrated confidence. By enabling explicit confidence\ncalibration and introspective self-criticism, HyperClick reduces overconfidence\nand supports more reliable GUI automation.",
      "upvotes": 14,
      "discussionId": "69081881812eca10f9cc5e5a"
    },
    "publishedAt": "2025-10-31T04:07:02.000Z",
    "title": "HyperClick: Advancing Reliable GUI Grounding via Uncertainty Calibration",
    "summary": "Autonomous Graphical User Interface (GUI) agents rely on accurate GUI\ngrounding, which maps language instructions to on-screen coordinates, to\nexecute user commands. However, current models, whether trained via supervised\nfine-tuning (SFT) or reinforcement fine-tuning (RFT), lack self-awareness of\ntheir capability boundaries, leading to overconfidence and unreliable\npredictions. We first systematically evaluate probabilistic and verbalized\nconfidence in general and GUI-specific models, revealing a misalignment between\nconfidence and actual accuracy, which is particularly critical in dynamic GUI\nautomation tasks, where single errors can cause task failure. To address this,\nwe propose HyperClick, a novel framework that enhances reliable GUI grounding\nthrough uncertainty calibration. HyperClick introduces a dual reward mechanism,\ncombining a binary reward for correct actions with a truncated Gaussian-based\nspatial confidence modeling, calibrated using the Brier score. This approach\njointly optimizes grounding accuracy and confidence reliability, fostering\nintrospective self-criticism. Extensive experiments on seven challenge\nbenchmarks show that HyperClick achieves state-of-the-art performance while\nproviding well-calibrated confidence. By enabling explicit confidence\ncalibration and introspective self-criticism, HyperClick reduces overconfidence\nand supports more reliable GUI automation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.27266.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 154
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.27684",
      "authors": [
        {
          "_id": "690817db812eca10f9cc5e44",
          "name": "Xiangyu Fan",
          "hidden": false
        },
        {
          "_id": "690817db812eca10f9cc5e45",
          "name": "Zesong Qiu",
          "hidden": false
        },
        {
          "_id": "690817db812eca10f9cc5e46",
          "name": "Zhuguanyu Wu",
          "hidden": false
        },
        {
          "_id": "690817db812eca10f9cc5e47",
          "name": "Fanzhou Wang",
          "hidden": false
        },
        {
          "_id": "690817db812eca10f9cc5e48",
          "name": "Zhiqian Lin",
          "hidden": false
        },
        {
          "_id": "690817db812eca10f9cc5e49",
          "name": "Tianxiang Ren",
          "hidden": false
        },
        {
          "_id": "690817db812eca10f9cc5e4a",
          "name": "Dahua Lin",
          "hidden": false
        },
        {
          "_id": "690817db812eca10f9cc5e4b",
          "name": "Ruihao Gong",
          "hidden": false
        },
        {
          "_id": "690817db812eca10f9cc5e4c",
          "name": "Lei Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-31T17:55:10.000Z",
      "submittedOnDailyAt": "2025-11-03T00:26:25.051Z",
      "title": "Phased DMD: Few-step Distribution Matching Distillation via Score\n  Matching within Subintervals",
      "submittedOnDailyBy": {
        "_id": "6626a471430a124253f197c8",
        "avatarUrl": "/avatars/f5747fdbe495d1296fed9d16d8c95857.svg",
        "isPro": false,
        "fullname": "yl-1993",
        "user": "yl-1993",
        "type": "user"
      },
      "summary": "Distribution Matching Distillation (DMD) distills score-based generative\nmodels into efficient one-step generators, without requiring a one-to-one\ncorrespondence with the sampling trajectories of their teachers. However,\nlimited model capacity causes one-step distilled models underperform on complex\ngenerative tasks, e.g., synthesizing intricate object motions in text-to-video\ngeneration. Directly extending DMD to multi-step distillation increases memory\nusage and computational depth, leading to instability and reduced efficiency.\nWhile prior works propose stochastic gradient truncation as a potential\nsolution, we observe that it substantially reduces the generation diversity of\nmulti-step distilled models, bringing it down to the level of their one-step\ncounterparts. To address these limitations, we propose Phased DMD, a multi-step\ndistillation framework that bridges the idea of phase-wise distillation with\nMixture-of-Experts (MoE), reducing learning difficulty while enhancing model\ncapacity. Phased DMD is built upon two key ideas: progressive distribution\nmatching and score matching within subintervals. First, our model divides the\nSNR range into subintervals, progressively refining the model to higher SNR\nlevels, to better capture complex distributions. Next, to ensure the training\nobjective within each subinterval is accurate, we have conducted rigorous\nmathematical derivations. We validate Phased DMD by distilling state-of-the-art\nimage and video generation models, including Qwen-Image (20B parameters) and\nWan2.2 (28B parameters). Experimental results demonstrate that Phased DMD\npreserves output diversity better than DMD while retaining key generative\ncapabilities. We will release our code and models.",
      "upvotes": 12,
      "discussionId": "690817dc812eca10f9cc5e4d",
      "organization": {
        "_id": "64f0405f8a4cf3e5e6b38f9c",
        "name": "sensenova",
        "fullname": "SenseNova"
      }
    },
    "publishedAt": "2025-10-31T13:55:10.000Z",
    "title": "Phased DMD: Few-step Distribution Matching Distillation via Score\n  Matching within Subintervals",
    "summary": "Distribution Matching Distillation (DMD) distills score-based generative\nmodels into efficient one-step generators, without requiring a one-to-one\ncorrespondence with the sampling trajectories of their teachers. However,\nlimited model capacity causes one-step distilled models underperform on complex\ngenerative tasks, e.g., synthesizing intricate object motions in text-to-video\ngeneration. Directly extending DMD to multi-step distillation increases memory\nusage and computational depth, leading to instability and reduced efficiency.\nWhile prior works propose stochastic gradient truncation as a potential\nsolution, we observe that it substantially reduces the generation diversity of\nmulti-step distilled models, bringing it down to the level of their one-step\ncounterparts. To address these limitations, we propose Phased DMD, a multi-step\ndistillation framework that bridges the idea of phase-wise distillation with\nMixture-of-Experts (MoE), reducing learning difficulty while enhancing model\ncapacity. Phased DMD is built upon two key ideas: progressive distribution\nmatching and score matching within subintervals. First, our model divides the\nSNR range into subintervals, progressively refining the model to higher SNR\nlevels, to better capture complex distributions. Next, to ensure the training\nobjective within each subinterval is accurate, we have conducted rigorous\nmathematical derivations. We validate Phased DMD by distilling state-of-the-art\nimage and video generation models, including Qwen-Image (20B parameters) and\nWan2.2 (28B parameters). Experimental results demonstrate that Phased DMD\npreserves output diversity better than DMD while retaining key generative\ncapabilities. We will release our code and models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.27684.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6626a471430a124253f197c8",
      "avatarUrl": "/avatars/f5747fdbe495d1296fed9d16d8c95857.svg",
      "fullname": "yl-1993",
      "name": "yl-1993",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "organization": {
      "_id": "64f0405f8a4cf3e5e6b38f9c",
      "name": "sensenova",
      "fullname": "SenseNova"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.26788",
      "authors": [
        {
          "_id": "6908202f812eca10f9cc5e89",
          "name": "Penghui Qi",
          "hidden": false
        },
        {
          "_id": "6908202f812eca10f9cc5e8a",
          "name": "Zichen Liu",
          "hidden": false
        },
        {
          "_id": "6908202f812eca10f9cc5e8b",
          "name": "Xiangxin Zhou",
          "hidden": false
        },
        {
          "_id": "6908202f812eca10f9cc5e8c",
          "name": "Tianyu Pang",
          "hidden": false
        },
        {
          "_id": "6908202f812eca10f9cc5e8d",
          "name": "Chao Du",
          "hidden": false
        },
        {
          "_id": "6908202f812eca10f9cc5e8e",
          "name": "Wee Sun Lee",
          "hidden": false
        },
        {
          "_id": "6908202f812eca10f9cc5e8f",
          "name": "Min Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-30T17:58:11.000Z",
      "submittedOnDailyAt": "2025-11-03T00:55:58.058Z",
      "title": "Defeating the Training-Inference Mismatch via FP16",
      "submittedOnDailyBy": {
        "_id": "63885f1d0bebb233d8ad6e5b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669881620925-noauth.jpeg",
        "isPro": false,
        "fullname": "Penghui Qi",
        "user": "QPHutu",
        "type": "user"
      },
      "summary": "Reinforcement learning (RL) fine-tuning of large language models (LLMs) often\nsuffers from instability due to the numerical mismatch between the training and\ninference policies. While prior work has attempted to mitigate this issue\nthrough algorithmic corrections or engineering alignments, we show that its\nroot cause lies in the floating point precision itself. The widely adopted\nBF16, despite its large dynamic range, introduces large rounding errors that\nbreaks the consistency between training and inference. In this work, we\ndemonstrate that simply reverting to FP16 effectively eliminates this\nmismatch. The change is simple, fully supported by modern frameworks with only\na few lines of code change, and requires no modification to the model\narchitecture or learning algorithm. Our results suggest that using FP16\nuniformly yields more stable optimization, faster convergence, and stronger\nperformance across diverse tasks, algorithms and frameworks. We hope these\nfindings motivate a broader reconsideration of precision trade-offs in RL\nfine-tuning.",
      "upvotes": 11,
      "discussionId": "6908202f812eca10f9cc5e90",
      "githubRepo": "https://github.com/sail-sg/Precision-RL",
      "githubStars": 73,
      "organization": {
        "_id": "61f4e841c771e23a1abb61ff",
        "name": "sail",
        "fullname": "Sea AI Lab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1643440185801-5df833bdda6d0311fd3d5403.png"
      }
    },
    "publishedAt": "2025-10-30T13:58:11.000Z",
    "title": "Defeating the Training-Inference Mismatch via FP16",
    "summary": "Reinforcement learning (RL) fine-tuning of large language models (LLMs) often\nsuffers from instability due to the numerical mismatch between the training and\ninference policies. While prior work has attempted to mitigate this issue\nthrough algorithmic corrections or engineering alignments, we show that its\nroot cause lies in the floating point precision itself. The widely adopted\nBF16, despite its large dynamic range, introduces large rounding errors that\nbreaks the consistency between training and inference. In this work, we\ndemonstrate that simply reverting to FP16 effectively eliminates this\nmismatch. The change is simple, fully supported by modern frameworks with only\na few lines of code change, and requires no modification to the model\narchitecture or learning algorithm. Our results suggest that using FP16\nuniformly yields more stable optimization, faster convergence, and stronger\nperformance across diverse tasks, algorithms and frameworks. We hope these\nfindings motivate a broader reconsideration of precision trade-offs in RL\nfine-tuning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.26788.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63885f1d0bebb233d8ad6e5b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669881620925-noauth.jpeg",
      "fullname": "Penghui Qi",
      "name": "QPHutu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "organization": {
      "_id": "61f4e841c771e23a1abb61ff",
      "name": "sail",
      "fullname": "Sea AI Lab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1643440185801-5df833bdda6d0311fd3d5403.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.23095",
      "authors": [
        {
          "_id": "6902d73072739622ee92a856",
          "user": {
            "_id": "6686b9f2978a8880573681e9",
            "avatarUrl": "/avatars/965caeedb1ba847894c6b0e10025986e.svg",
            "isPro": false,
            "fullname": "Jie Huang",
            "user": "JJJYmmm",
            "type": "user"
          },
          "name": "Jie Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-30T14:27:21.611Z",
          "hidden": false
        },
        {
          "_id": "6902d73072739622ee92a857",
          "user": {
            "_id": "6486c50978ab6bf101afc29f",
            "avatarUrl": "/avatars/cd101a2c5188b48a1874f20756eb8f51.svg",
            "isPro": false,
            "fullname": "XuejingLiu",
            "user": "GingL",
            "type": "user"
          },
          "name": "Xuejing Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-30T14:27:16.199Z",
          "hidden": false
        },
        {
          "_id": "6902d73072739622ee92a858",
          "name": "Sibo Song",
          "hidden": false
        },
        {
          "_id": "6902d73072739622ee92a859",
          "name": "Ruibing Hou",
          "hidden": false
        },
        {
          "_id": "6902d73072739622ee92a85a",
          "name": "Hong Chang",
          "hidden": false
        },
        {
          "_id": "6902d73072739622ee92a85b",
          "name": "Junyang Lin",
          "hidden": false
        },
        {
          "_id": "6902d73072739622ee92a85c",
          "name": "Shuai Bai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-27T08:00:46.000Z",
      "submittedOnDailyAt": "2025-11-03T03:28:47.090Z",
      "title": "Revisiting Multimodal Positional Encoding in Vision-Language Models",
      "submittedOnDailyBy": {
        "_id": "6486c50978ab6bf101afc29f",
        "avatarUrl": "/avatars/cd101a2c5188b48a1874f20756eb8f51.svg",
        "isPro": false,
        "fullname": "XuejingLiu",
        "user": "GingL",
        "type": "user"
      },
      "summary": "Multimodal position encoding is essential for vision-language models, yet\nthere has been little systematic investigation into multimodal position\nencoding. We conduct a comprehensive analysis of multimodal Rotary Positional\nEmbedding (RoPE) by examining its two core components: position design and\nfrequency allocation. Through extensive experiments, we identify three key\nguidelines: positional coherence, full frequency utilization, and preservation\nof textual priors-ensuring unambiguous layout, rich representation, and\nfaithful transfer from the pre-trained LLM. Based on these insights, we propose\nMulti-Head RoPE (MHRoPE) and MRoPE-Interleave (MRoPE-I), two simple and\nplug-and-play variants that require no architectural changes. Our methods\nconsistently outperform existing approaches across diverse benchmarks, with\nsignificant improvements in both general and fine-grained multimodal\nunderstanding. Code will be avaliable at\nhttps://github.com/JJJYmmm/Multimodal-RoPEs.",
      "upvotes": 5,
      "discussionId": "6902d73072739622ee92a85d",
      "organization": {
        "_id": "64c8b5837fe12ecd0a7e92eb",
        "name": "Qwen",
        "fullname": "Qwen",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"
      }
    },
    "publishedAt": "2025-10-27T04:00:46.000Z",
    "title": "Revisiting Multimodal Positional Encoding in Vision-Language Models",
    "summary": "Multimodal position encoding is essential for vision-language models, yet\nthere has been little systematic investigation into multimodal position\nencoding. We conduct a comprehensive analysis of multimodal Rotary Positional\nEmbedding (RoPE) by examining its two core components: position design and\nfrequency allocation. Through extensive experiments, we identify three key\nguidelines: positional coherence, full frequency utilization, and preservation\nof textual priors-ensuring unambiguous layout, rich representation, and\nfaithful transfer from the pre-trained LLM. Based on these insights, we propose\nMulti-Head RoPE (MHRoPE) and MRoPE-Interleave (MRoPE-I), two simple and\nplug-and-play variants that require no architectural changes. Our methods\nconsistently outperform existing approaches across diverse benchmarks, with\nsignificant improvements in both general and fine-grained multimodal\nunderstanding. Code will be avaliable at\nhttps://github.com/JJJYmmm/Multimodal-RoPEs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.23095.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6486c50978ab6bf101afc29f",
      "avatarUrl": "/avatars/cd101a2c5188b48a1874f20756eb8f51.svg",
      "fullname": "XuejingLiu",
      "name": "GingL",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "64c8b5837fe12ecd0a7e92eb",
      "name": "Qwen",
      "fullname": "Qwen",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.27258",
      "authors": [
        {
          "_id": "69081a51812eca10f9cc5e5c",
          "name": "Yifan Zhang",
          "hidden": false
        },
        {
          "_id": "69081a51812eca10f9cc5e5d",
          "name": "Zhen Qin",
          "hidden": false
        },
        {
          "_id": "69081a51812eca10f9cc5e5e",
          "name": "Quanquan Gu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-31T07:54:37.000Z",
      "submittedOnDailyAt": "2025-11-03T00:36:34.235Z",
      "title": "Higher-order Linear Attention",
      "submittedOnDailyBy": {
        "_id": "647bf082aba7062fe5c51ca9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647bf082aba7062fe5c51ca9/p4lY9IjHiWZETKmFq1mtU.jpeg",
        "isPro": false,
        "fullname": "Yifan Zhang",
        "user": "yifAI",
        "type": "user"
      },
      "summary": "The quadratic cost of scaled dot-product attention is a central obstacle to\nscaling autoregressive language models to long contexts. Linear-time attention\nand State Space Models (SSMs) provide scalable alternatives but are typically\nrestricted to first-order or kernel-based approximations, which can limit\nexpressivity. We introduce Higher-order Linear Attention (HLA), a causal,\nstreaming mechanism that realizes higher interactions via compact prefix\nsufficient statistics. In the second-order case, HLA maintains a constant-size\nstate and computes per-token outputs in linear time without materializing any\nn times n matrices. We give closed-form streaming identities, a strictly\ncausal masked variant using two additional summaries, and a chunk-parallel\ntraining scheme based on associative scans that reproduces the activations of a\nserial recurrence exactly. We further outline extensions to third and higher\norders. Collectively, these results position HLA as a principled, scalable\nbuilding block that combines attention-like, data-dependent mixing with the\nefficiency of modern recurrent architectures. Project Page:\nhttps://github.com/yifanzhang-pro/HLA.",
      "upvotes": 2,
      "discussionId": "69081a51812eca10f9cc5e5f",
      "projectPage": "https://yifanzhang-pro.github.io/HLA",
      "githubRepo": "https://github.com/yifanzhang-pro/HLA",
      "githubStars": 26,
      "organization": {
        "_id": "6779cfba64085a93fd039c0c",
        "name": "model-architectures",
        "fullname": "Model Architectures"
      }
    },
    "publishedAt": "2025-10-31T03:54:37.000Z",
    "title": "Higher-order Linear Attention",
    "summary": "The quadratic cost of scaled dot-product attention is a central obstacle to\nscaling autoregressive language models to long contexts. Linear-time attention\nand State Space Models (SSMs) provide scalable alternatives but are typically\nrestricted to first-order or kernel-based approximations, which can limit\nexpressivity. We introduce Higher-order Linear Attention (HLA), a causal,\nstreaming mechanism that realizes higher interactions via compact prefix\nsufficient statistics. In the second-order case, HLA maintains a constant-size\nstate and computes per-token outputs in linear time without materializing any\nn times n matrices. We give closed-form streaming identities, a strictly\ncausal masked variant using two additional summaries, and a chunk-parallel\ntraining scheme based on associative scans that reproduces the activations of a\nserial recurrence exactly. We further outline extensions to third and higher\norders. Collectively, these results position HLA as a principled, scalable\nbuilding block that combines attention-like, data-dependent mixing with the\nefficiency of modern recurrent architectures. Project Page:\nhttps://github.com/yifanzhang-pro/HLA.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.27258.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647bf082aba7062fe5c51ca9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647bf082aba7062fe5c51ca9/p4lY9IjHiWZETKmFq1mtU.jpeg",
      "fullname": "Yifan Zhang",
      "name": "yifAI",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 16
    },
    "organization": {
      "_id": "6779cfba64085a93fd039c0c",
      "name": "model-architectures",
      "fullname": "Model Architectures"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.26887",
      "authors": [
        {
          "_id": "6908169f812eca10f9cc5e12",
          "name": "Francisco Villaescusa-Navarro",
          "hidden": false
        },
        {
          "_id": "6908169f812eca10f9cc5e13",
          "name": "Boris Bolliet",
          "hidden": false
        },
        {
          "_id": "6908169f812eca10f9cc5e14",
          "name": "Pablo Villanueva-Domingo",
          "hidden": false
        },
        {
          "_id": "6908169f812eca10f9cc5e15",
          "name": "Adrian E. Bayer",
          "hidden": false
        },
        {
          "_id": "6908169f812eca10f9cc5e16",
          "name": "Aidan Acquah",
          "hidden": false
        },
        {
          "_id": "6908169f812eca10f9cc5e17",
          "name": "Chetana Amancharla",
          "hidden": false
        },
        {
          "_id": "6908169f812eca10f9cc5e18",
          "name": "Almog Barzilay-Siegal",
          "hidden": false
        },
        {
          "_id": "6908169f812eca10f9cc5e19",
          "name": "Pablo Bermejo",
          "hidden": false
        },
        {
          "_id": "6908169f812eca10f9cc5e1a",
          "name": "Camille Bilodeau",
          "hidden": false
        },
        {
          "_id": "6908169f812eca10f9cc5e1b",
          "name": "Pablo Cárdenas Ramírez",
          "hidden": false
        },
        {
          "_id": "6908169f812eca10f9cc5e1c",
          "name": "Miles Cranmer",
          "hidden": false
        },
        {
          "_id": "6908169f812eca10f9cc5e1d",
          "name": "Urbano L. França",
          "hidden": false
        },
        {
          "_id": "6908169f812eca10f9cc5e1e",
          "name": "ChangHoon Hahn",
          "hidden": false
        },
        {
          "_id": "6908169f812eca10f9cc5e1f",
          "name": "Yan-Fei Jiang",
          "hidden": false
        },
        {
          "_id": "6908169f812eca10f9cc5e20",
          "name": "Raul Jimenez",
          "hidden": false
        },
        {
          "_id": "6908169f812eca10f9cc5e21",
          "name": "Jun-Young Lee",
          "hidden": false
        },
        {
          "_id": "6908169f812eca10f9cc5e22",
          "name": "Antonio Lerario",
          "hidden": false
        },
        {
          "_id": "6908169f812eca10f9cc5e23",
          "name": "Osman Mamun",
          "hidden": false
        },
        {
          "_id": "6908169f812eca10f9cc5e24",
          "name": "Thomas Meier",
          "hidden": false
        },
        {
          "_id": "6908169f812eca10f9cc5e25",
          "name": "Anupam A. Ojha",
          "hidden": false
        },
        {
          "_id": "6908169f812eca10f9cc5e26",
          "name": "Pavlos Protopapas",
          "hidden": false
        },
        {
          "_id": "6908169f812eca10f9cc5e27",
          "name": "Shimanto Roy",
          "hidden": false
        },
        {
          "_id": "6908169f812eca10f9cc5e28",
          "name": "David N. Spergel",
          "hidden": false
        },
        {
          "_id": "6908169f812eca10f9cc5e29",
          "name": "Pedro Tarancón-Álvarez",
          "hidden": false
        },
        {
          "_id": "6908169f812eca10f9cc5e2a",
          "name": "Ujjwal Tiwari",
          "hidden": false
        },
        {
          "_id": "6908169f812eca10f9cc5e2b",
          "name": "Matteo Viel",
          "hidden": false
        },
        {
          "_id": "6908169f812eca10f9cc5e2c",
          "name": "Digvijay Wadekar",
          "hidden": false
        },
        {
          "_id": "6908169f812eca10f9cc5e2d",
          "name": "Chi Wang",
          "hidden": false
        },
        {
          "_id": "6908169f812eca10f9cc5e2e",
          "name": "Bonny Y. Wang",
          "hidden": false
        },
        {
          "_id": "6908169f812eca10f9cc5e2f",
          "name": "Licong Xu",
          "hidden": false
        },
        {
          "_id": "6908169f812eca10f9cc5e30",
          "name": "Yossi Yovel",
          "hidden": false
        },
        {
          "_id": "6908169f812eca10f9cc5e31",
          "name": "Shuwen Yue",
          "hidden": false
        },
        {
          "_id": "6908169f812eca10f9cc5e32",
          "name": "Wen-Han Zhou",
          "hidden": false
        },
        {
          "_id": "6908169f812eca10f9cc5e33",
          "name": "Qiyao Zhu",
          "hidden": false
        },
        {
          "_id": "6908169f812eca10f9cc5e34",
          "name": "Jiajun Zou",
          "hidden": false
        },
        {
          "_id": "6908169f812eca10f9cc5e35",
          "name": "Íñigo Zubeldia",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-30T18:00:12.000Z",
      "submittedOnDailyAt": "2025-11-03T00:12:53.238Z",
      "title": "The Denario project: Deep knowledge AI agents for scientific discovery",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We present Denario, an AI multi-agent system designed to serve as a\nscientific research assistant. Denario can perform many different tasks, such\nas generating ideas, checking the literature, developing research plans,\nwriting and executing code, making plots, and drafting and reviewing a\nscientific paper. The system has a modular architecture, allowing it to handle\nspecific tasks, such as generating an idea, or carrying out end-to-end\nscientific analysis using Cmbagent as a deep-research backend. In this work, we\ndescribe in detail Denario and its modules, and illustrate its capabilities by\npresenting multiple AI-generated papers generated by it in many different\nscientific disciplines such as astrophysics, biology, biophysics, biomedical\ninformatics, chemistry, material science, mathematical physics, medicine,\nneuroscience and planetary science. Denario also excels at combining ideas from\ndifferent disciplines, and we illustrate this by showing a paper that applies\nmethods from quantum physics and machine learning to astrophysical data. We\nreport the evaluations performed on these papers by domain experts, who\nprovided both numerical scores and review-like feedback. We then highlight the\nstrengths, weaknesses, and limitations of the current system. Finally, we\ndiscuss the ethical implications of AI-driven research and reflect on how such\ntechnology relates to the philosophy of science. We publicly release the code\nat https://github.com/AstroPilot-AI/Denario. A Denario demo can also be run\ndirectly on the web at https://huggingface.co/spaces/astropilot-ai/Denario, and\nthe full app will be deployed on the cloud.",
      "upvotes": 1,
      "discussionId": "690816a0812eca10f9cc5e36",
      "githubRepo": "https://github.com/AstroPilot-AI/Denario",
      "githubStars": 57
    },
    "publishedAt": "2025-10-30T14:00:12.000Z",
    "title": "The Denario project: Deep knowledge AI agents for scientific discovery",
    "summary": "We present Denario, an AI multi-agent system designed to serve as a\nscientific research assistant. Denario can perform many different tasks, such\nas generating ideas, checking the literature, developing research plans,\nwriting and executing code, making plots, and drafting and reviewing a\nscientific paper. The system has a modular architecture, allowing it to handle\nspecific tasks, such as generating an idea, or carrying out end-to-end\nscientific analysis using Cmbagent as a deep-research backend. In this work, we\ndescribe in detail Denario and its modules, and illustrate its capabilities by\npresenting multiple AI-generated papers generated by it in many different\nscientific disciplines such as astrophysics, biology, biophysics, biomedical\ninformatics, chemistry, material science, mathematical physics, medicine,\nneuroscience and planetary science. Denario also excels at combining ideas from\ndifferent disciplines, and we illustrate this by showing a paper that applies\nmethods from quantum physics and machine learning to astrophysical data. We\nreport the evaluations performed on these papers by domain experts, who\nprovided both numerical scores and review-like feedback. We then highlight the\nstrengths, weaknesses, and limitations of the current system. Finally, we\ndiscuss the ethical implications of AI-driven research and reflect on how such\ntechnology relates to the philosophy of science. We publicly release the code\nat https://github.com/AstroPilot-AI/Denario. A Denario demo can also be run\ndirectly on the web at https://huggingface.co/spaces/astropilot-ai/Denario, and\nthe full app will be deployed on the cloud.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.26887.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 154
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.27623",
      "authors": [
        {
          "_id": "6908174c812eca10f9cc5e38",
          "name": "Qiusi Zhan",
          "hidden": false
        },
        {
          "_id": "6908174c812eca10f9cc5e39",
          "name": "Hyeonjeong Ha",
          "hidden": false
        },
        {
          "_id": "6908174c812eca10f9cc5e3a",
          "name": "Rui Yang",
          "hidden": false
        },
        {
          "_id": "6908174c812eca10f9cc5e3b",
          "name": "Sirui Xu",
          "hidden": false
        },
        {
          "_id": "6908174c812eca10f9cc5e3c",
          "name": "Hanyang Chen",
          "hidden": false
        },
        {
          "_id": "6908174c812eca10f9cc5e3d",
          "name": "Liang-Yan Gui",
          "hidden": false
        },
        {
          "_id": "6908174c812eca10f9cc5e3e",
          "name": "Yu-Xiong Wang",
          "hidden": false
        },
        {
          "_id": "6908174c812eca10f9cc5e3f",
          "name": "Huan Zhang",
          "hidden": false
        },
        {
          "_id": "6908174c812eca10f9cc5e40",
          "name": "Heng Ji",
          "hidden": false
        },
        {
          "_id": "6908174c812eca10f9cc5e41",
          "name": "Daniel Kang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/sKQfgvZh6v9bLrMzbPsaG.mp4"
      ],
      "publishedAt": "2025-10-31T16:50:49.000Z",
      "submittedOnDailyAt": "2025-11-03T00:15:46.836Z",
      "title": "Visual Backdoor Attacks on MLLM Embodied Decision Making via Contrastive\n  Trigger Learning",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Multimodal large language models (MLLMs) have advanced embodied agents by\nenabling direct perception, reasoning, and planning task-oriented actions from\nvisual inputs. However, such vision driven embodied agents open a new attack\nsurface: visual backdoor attacks, where the agent behaves normally until a\nvisual trigger appears in the scene, then persistently executes an\nattacker-specified multi-step policy. We introduce BEAT, the first framework to\ninject such visual backdoors into MLLM-based embodied agents using objects in\nthe environments as triggers. Unlike textual triggers, object triggers exhibit\nwide variation across viewpoints and lighting, making them difficult to implant\nreliably. BEAT addresses this challenge by (1) constructing a training set that\nspans diverse scenes, tasks, and trigger placements to expose agents to trigger\nvariability, and (2) introducing a two-stage training scheme that first applies\nsupervised fine-tuning (SFT) and then our novel Contrastive Trigger Learning\n(CTL). CTL formulates trigger discrimination as preference learning between\ntrigger-present and trigger-free inputs, explicitly sharpening the decision\nboundaries to ensure precise backdoor activation. Across various embodied agent\nbenchmarks and MLLMs, BEAT achieves attack success rates up to 80%, while\nmaintaining strong benign task performance, and generalizes reliably to\nout-of-distribution trigger placements. Notably, compared to naive SFT, CTL\nboosts backdoor activation accuracy up to 39% under limited backdoor data.\nThese findings expose a critical yet unexplored security risk in MLLM-based\nembodied agents, underscoring the need for robust defenses before real-world\ndeployment.",
      "upvotes": 0,
      "discussionId": "6908174c812eca10f9cc5e42",
      "projectPage": "https://zqs1943.github.io/BEAT/"
    },
    "publishedAt": "2025-10-31T12:50:49.000Z",
    "title": "Visual Backdoor Attacks on MLLM Embodied Decision Making via Contrastive\n  Trigger Learning",
    "summary": "Multimodal large language models (MLLMs) have advanced embodied agents by\nenabling direct perception, reasoning, and planning task-oriented actions from\nvisual inputs. However, such vision driven embodied agents open a new attack\nsurface: visual backdoor attacks, where the agent behaves normally until a\nvisual trigger appears in the scene, then persistently executes an\nattacker-specified multi-step policy. We introduce BEAT, the first framework to\ninject such visual backdoors into MLLM-based embodied agents using objects in\nthe environments as triggers. Unlike textual triggers, object triggers exhibit\nwide variation across viewpoints and lighting, making them difficult to implant\nreliably. BEAT addresses this challenge by (1) constructing a training set that\nspans diverse scenes, tasks, and trigger placements to expose agents to trigger\nvariability, and (2) introducing a two-stage training scheme that first applies\nsupervised fine-tuning (SFT) and then our novel Contrastive Trigger Learning\n(CTL). CTL formulates trigger discrimination as preference learning between\ntrigger-present and trigger-free inputs, explicitly sharpening the decision\nboundaries to ensure precise backdoor activation. Across various embodied agent\nbenchmarks and MLLMs, BEAT achieves attack success rates up to 80%, while\nmaintaining strong benign task performance, and generalizes reliably to\nout-of-distribution trigger placements. Notably, compared to naive SFT, CTL\nboosts backdoor activation accuracy up to 39% under limited backdoor data.\nThese findings expose a critical yet unexplored security risk in MLLM-based\nembodied agents, underscoring the need for robust defenses before real-world\ndeployment.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/sKQfgvZh6v9bLrMzbPsaG.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.27623.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 154
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.27607",
      "authors": [
        {
          "_id": "690815d6812eca10f9cc5e0b",
          "name": "John Won",
          "hidden": false
        },
        {
          "_id": "690815d6812eca10f9cc5e0c",
          "name": "Kyungmin Lee",
          "hidden": false
        },
        {
          "_id": "690815d6812eca10f9cc5e0d",
          "name": "Huiwon Jang",
          "hidden": false
        },
        {
          "_id": "690815d6812eca10f9cc5e0e",
          "name": "Dongyoung Kim",
          "hidden": false
        },
        {
          "_id": "690815d6812eca10f9cc5e0f",
          "name": "Jinwoo Shin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-31T16:32:12.000Z",
      "submittedOnDailyAt": "2025-11-03T00:09:23.869Z",
      "title": "Dual-Stream Diffusion for World-Model Augmented Vision-Language-Action\n  Model",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Recently, augmenting Vision-Language-Action models (VLAs) with world modeling\nhas shown promise in improving robotic policy learning. However, it remains\nchallenging to jointly predict next-state observations and action sequences\nbecause of the inherent difference between the two modalities. To address this,\nwe propose DUal-STream diffusion (DUST), a world-model augmented VLA framework\nthat handles the modality conflict and enhances the performance of VLAs across\ndiverse tasks. Specifically, we propose a multimodal diffusion transformer\narchitecture that explicitly maintains separate modality streams while still\nenabling cross-modal knowledge sharing. In addition, we introduce independent\nnoise perturbations for each modality and a decoupled flow-matching loss. This\ndesign enables the model to learn the joint distribution in a bidirectional\nmanner while avoiding the need for a unified latent space. Based on the\ndecoupling of modalities during training, we also introduce a joint sampling\nmethod that supports test-time scaling, where action and vision tokens evolve\nasynchronously at different rates. Through experiments on simulated benchmarks\nsuch as RoboCasa and GR-1, DUST achieves up to 6% gains over baseline methods,\nwhile our test-time scaling approach provides an additional 2-5% boost. On\nreal-world tasks with the Franka Research 3, DUST improves success rates by\n13%, confirming its effectiveness beyond simulation. Furthermore, pre-training\non action-free videos from BridgeV2 yields significant transfer gains on\nRoboCasa, underscoring DUST's potential for large-scale VLA pretraining.",
      "upvotes": 0,
      "discussionId": "690815d6812eca10f9cc5e10"
    },
    "publishedAt": "2025-10-31T12:32:12.000Z",
    "title": "Dual-Stream Diffusion for World-Model Augmented Vision-Language-Action\n  Model",
    "summary": "Recently, augmenting Vision-Language-Action models (VLAs) with world modeling\nhas shown promise in improving robotic policy learning. However, it remains\nchallenging to jointly predict next-state observations and action sequences\nbecause of the inherent difference between the two modalities. To address this,\nwe propose DUal-STream diffusion (DUST), a world-model augmented VLA framework\nthat handles the modality conflict and enhances the performance of VLAs across\ndiverse tasks. Specifically, we propose a multimodal diffusion transformer\narchitecture that explicitly maintains separate modality streams while still\nenabling cross-modal knowledge sharing. In addition, we introduce independent\nnoise perturbations for each modality and a decoupled flow-matching loss. This\ndesign enables the model to learn the joint distribution in a bidirectional\nmanner while avoiding the need for a unified latent space. Based on the\ndecoupling of modalities during training, we also introduce a joint sampling\nmethod that supports test-time scaling, where action and vision tokens evolve\nasynchronously at different rates. Through experiments on simulated benchmarks\nsuch as RoboCasa and GR-1, DUST achieves up to 6% gains over baseline methods,\nwhile our test-time scaling approach provides an additional 2-5% boost. On\nreal-world tasks with the Franka Research 3, DUST improves success rates by\n13%, confirming its effectiveness beyond simulation. Furthermore, pre-training\non action-free videos from BridgeV2 yields significant transfer gains on\nRoboCasa, underscoring DUST's potential for large-scale VLA pretraining.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.27607.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 154
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.27224",
      "authors": [
        {
          "_id": "690832bf812eca10f9cc5ecf",
          "name": "Mahmoud El Hussieni",
          "hidden": false
        },
        {
          "_id": "690832bf812eca10f9cc5ed0",
          "name": "Bahadır K. Güntürk",
          "hidden": false
        },
        {
          "_id": "690832bf812eca10f9cc5ed1",
          "name": "Hasan F. Ateş",
          "hidden": false
        },
        {
          "_id": "690832bf812eca10f9cc5ed2",
          "name": "Oğuz Hanoğlu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-31T06:37:08.000Z",
      "submittedOnDailyAt": "2025-11-03T02:15:50.430Z",
      "title": "Mask-to-Height: A YOLOv11-Based Architecture for Joint Building Instance\n  Segmentation and Height Classification from Satellite Imagery",
      "submittedOnDailyBy": {
        "_id": "6422eab8e2029ade06eeee2c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6422eab8e2029ade06eeee2c/Gai3BHr2WJ0YuhdumqQ_z.png",
        "isPro": false,
        "fullname": "Mahmud ElHuseyni 🇵🇸",
        "user": "MElHuseyni",
        "type": "user"
      },
      "summary": "Accurate building instance segmentation and height classification are\ncritical for urban planning, 3D city modeling, and infrastructure monitoring.\nThis paper presents a detailed analysis of YOLOv11, the recent advancement in\nthe YOLO series of deep learning models, focusing on its application to joint\nbuilding extraction and discrete height classification from satellite imagery.\nYOLOv11 builds on the strengths of earlier YOLO models by introducing a more\nefficient architecture that better combines features at different scales,\nimproves object localization accuracy, and enhances performance in complex\nurban scenes. Using the DFC2023 Track 2 dataset -- which includes over 125,000\nannotated buildings across 12 cities -- we evaluate YOLOv11's performance using\nmetrics such as precision, recall, F1 score, and mean average precision (mAP).\nOur findings demonstrate that YOLOv11 achieves strong instance segmentation\nperformance with 60.4\\% mAP@50 and 38.3\\% mAP@50--95 while maintaining robust\nclassification accuracy across five predefined height tiers. The model excels\nin handling occlusions, complex building shapes, and class imbalance,\nparticularly for rare high-rise structures. Comparative analysis confirms that\nYOLOv11 outperforms earlier multitask frameworks in both detection accuracy and\ninference speed, making it well-suited for real-time, large-scale urban\nmapping. This research highlights YOLOv11's potential to advance semantic urban\nreconstruction through streamlined categorical height modeling, offering\nactionable insights for future developments in remote sensing and geospatial\nintelligence.",
      "upvotes": 0,
      "discussionId": "690832c0812eca10f9cc5ed3"
    },
    "publishedAt": "2025-10-31T02:37:08.000Z",
    "title": "Mask-to-Height: A YOLOv11-Based Architecture for Joint Building Instance\n  Segmentation and Height Classification from Satellite Imagery",
    "summary": "Accurate building instance segmentation and height classification are\ncritical for urban planning, 3D city modeling, and infrastructure monitoring.\nThis paper presents a detailed analysis of YOLOv11, the recent advancement in\nthe YOLO series of deep learning models, focusing on its application to joint\nbuilding extraction and discrete height classification from satellite imagery.\nYOLOv11 builds on the strengths of earlier YOLO models by introducing a more\nefficient architecture that better combines features at different scales,\nimproves object localization accuracy, and enhances performance in complex\nurban scenes. Using the DFC2023 Track 2 dataset -- which includes over 125,000\nannotated buildings across 12 cities -- we evaluate YOLOv11's performance using\nmetrics such as precision, recall, F1 score, and mean average precision (mAP).\nOur findings demonstrate that YOLOv11 achieves strong instance segmentation\nperformance with 60.4\\% mAP@50 and 38.3\\% mAP@50--95 while maintaining robust\nclassification accuracy across five predefined height tiers. The model excels\nin handling occlusions, complex building shapes, and class imbalance,\nparticularly for rare high-rise structures. Comparative analysis confirms that\nYOLOv11 outperforms earlier multitask frameworks in both detection accuracy and\ninference speed, making it well-suited for real-time, large-scale urban\nmapping. This research highlights YOLOv11's potential to advance semantic urban\nreconstruction through streamlined categorical height modeling, offering\nactionable insights for future developments in remote sensing and geospatial\nintelligence.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.27224.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6422eab8e2029ade06eeee2c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6422eab8e2029ade06eeee2c/Gai3BHr2WJ0YuhdumqQ_z.png",
      "fullname": "Mahmud ElHuseyni 🇵🇸",
      "name": "MElHuseyni",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 21
    },
    "isAuthorParticipating": false
  }
]