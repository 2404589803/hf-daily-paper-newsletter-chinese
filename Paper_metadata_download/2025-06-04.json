[
  {
    "paper": {
      "id": "2506.03147",
      "authors": [
        {
          "_id": "683fae55c6b71c5994ccd4fe",
          "name": "Bin Lin",
          "hidden": false
        },
        {
          "_id": "683fae55c6b71c5994ccd4ff",
          "name": "Zongjian Li",
          "hidden": false
        },
        {
          "_id": "683fae55c6b71c5994ccd500",
          "name": "Xinhua Cheng",
          "hidden": false
        },
        {
          "_id": "683fae55c6b71c5994ccd501",
          "name": "Yuwei Niu",
          "hidden": false
        },
        {
          "_id": "683fae55c6b71c5994ccd502",
          "name": "Yang Ye",
          "hidden": false
        },
        {
          "_id": "683fae55c6b71c5994ccd503",
          "name": "Xianyi He",
          "hidden": false
        },
        {
          "_id": "683fae55c6b71c5994ccd504",
          "name": "Shenghai Yuan",
          "hidden": false
        },
        {
          "_id": "683fae55c6b71c5994ccd505",
          "name": "Wangbo Yu",
          "hidden": false
        },
        {
          "_id": "683fae55c6b71c5994ccd506",
          "name": "Shaodong Wang",
          "hidden": false
        },
        {
          "_id": "683fae55c6b71c5994ccd507",
          "name": "Yunyang Ge",
          "hidden": false
        },
        {
          "_id": "683fae55c6b71c5994ccd508",
          "name": "Yatian Pang",
          "hidden": false
        },
        {
          "_id": "683fae55c6b71c5994ccd509",
          "name": "Li Yuan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T17:59:33.000Z",
      "submittedOnDailyAt": "2025-06-04T00:55:35.016Z",
      "title": "UniWorld: High-Resolution Semantic Encoders for Unified Visual\n  Understanding and Generation",
      "submittedOnDailyBy": {
        "_id": "63468720dd6d90d82ccf3450",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
        "isPro": false,
        "fullname": "YSH",
        "user": "BestWishYsh",
        "type": "user"
      },
      "summary": "Although existing unified models deliver strong performance on\nvision-language understanding and text-to-image generation, their models are\nlimited in exploring image perception and manipulation tasks, which are\nurgently desired by users for wide applications. Recently, OpenAI released\ntheir powerful GPT-4o-Image model for comprehensive image perception and\nmanipulation, achieving expressive capability and attracting community\ninterests. By observing the performance of GPT-4o-Image in our carefully\nconstructed experiments, we infer that GPT-4o-Image leverages features\nextracted by semantic encoders instead of VAE, while VAEs are considered\nessential components in many image manipulation models. Motivated by such\ninspiring observations, we present a unified generative framework named\nUniWorld based on semantic features provided by powerful visual-language models\nand contrastive semantic encoders. As a result, we build a strong unified model\nusing only 1% amount of BAGEL's data, which consistently outperforms BAGEL on\nimage editing benchmarks. UniWorld also maintains competitive image\nunderstanding and generation capabilities, achieving strong performance across\nmultiple image perception tasks. We fully open-source our models, including\nmodel weights, training and evaluation scripts, and datasets.",
      "upvotes": 31,
      "discussionId": "683fae56c6b71c5994ccd548",
      "githubRepo": "https://github.com/PKU-YuanGroup/UniWorld-V1",
      "ai_summary": "A unified generative framework called UniWorld uses semantic features from visual-language models for image perception and manipulation, outperforming BAGEL with reduced data.",
      "ai_keywords": [
        "GPT-4o-Image",
        "semantic encoders",
        "VAE",
        "UniWorld",
        "visual-language models",
        "contrastive semantic encoders",
        "image editing benchmarks",
        "image perception tasks"
      ]
    },
    "publishedAt": "2025-06-03T13:59:33.000Z",
    "title": "UniWorld: High-Resolution Semantic Encoders for Unified Visual\n  Understanding and Generation",
    "summary": "Although existing unified models deliver strong performance on\nvision-language understanding and text-to-image generation, their models are\nlimited in exploring image perception and manipulation tasks, which are\nurgently desired by users for wide applications. Recently, OpenAI released\ntheir powerful GPT-4o-Image model for comprehensive image perception and\nmanipulation, achieving expressive capability and attracting community\ninterests. By observing the performance of GPT-4o-Image in our carefully\nconstructed experiments, we infer that GPT-4o-Image leverages features\nextracted by semantic encoders instead of VAE, while VAEs are considered\nessential components in many image manipulation models. Motivated by such\ninspiring observations, we present a unified generative framework named\nUniWorld based on semantic features provided by powerful visual-language models\nand contrastive semantic encoders. As a result, we build a strong unified model\nusing only 1% amount of BAGEL's data, which consistently outperforms BAGEL on\nimage editing benchmarks. UniWorld also maintains competitive image\nunderstanding and generation capabilities, achieving strong performance across\nmultiple image perception tasks. We fully open-source our models, including\nmodel weights, training and evaluation scripts, and datasets.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03147.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63468720dd6d90d82ccf3450",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
      "fullname": "YSH",
      "name": "BestWishYsh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 56
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.02387",
      "authors": [
        {
          "_id": "683fa95ea0770843560c7ae3",
          "user": {
            "_id": "653a5b0f7c01c693a16dd184",
            "avatarUrl": "/avatars/4b43d88709dc8037250404452e81adcf.svg",
            "isPro": false,
            "fullname": "Zelai Xu",
            "user": "zelaix",
            "type": "user"
          },
          "name": "Zelai Xu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-04T02:03:11.372Z",
          "hidden": false
        },
        {
          "_id": "683fa95ea0770843560c7ae4",
          "name": "Zhexuan Xu",
          "hidden": false
        },
        {
          "_id": "683fa95ea0770843560c7ae5",
          "name": "Xiangmin Yi",
          "hidden": false
        },
        {
          "_id": "683fa95ea0770843560c7ae6",
          "name": "Huining Yuan",
          "hidden": false
        },
        {
          "_id": "683fa95ea0770843560c7ae7",
          "name": "Xinlei Chen",
          "hidden": false
        },
        {
          "_id": "683fa95ea0770843560c7ae8",
          "name": "Yi Wu",
          "hidden": false
        },
        {
          "_id": "683fa95ea0770843560c7ae9",
          "name": "Chao Yu",
          "hidden": false
        },
        {
          "_id": "683fa95ea0770843560c7aea",
          "name": "Yu Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T02:57:38.000Z",
      "submittedOnDailyAt": "2025-06-04T00:58:29.506Z",
      "title": "VS-Bench: Evaluating VLMs for Strategic Reasoning and Decision-Making in\n  Multi-Agent Environments",
      "submittedOnDailyBy": {
        "_id": "653a5b0f7c01c693a16dd184",
        "avatarUrl": "/avatars/4b43d88709dc8037250404452e81adcf.svg",
        "isPro": false,
        "fullname": "Zelai Xu",
        "user": "zelaix",
        "type": "user"
      },
      "summary": "Recent advancements in Vision Language Models (VLMs) have expanded their\ncapabilities to interactive agent tasks, yet existing benchmarks remain limited\nto single-agent or text-only environments. In contrast, real-world scenarios\noften involve multiple agents interacting within rich visual and linguistic\ncontexts, posing challenges with both multimodal observations and strategic\ninteractions. To bridge this gap, we introduce Visual Strategic Bench\n(VS-Bench), a multimodal benchmark that evaluates VLMs for strategic reasoning\nand decision-making in multi-agent environments. VS-Bench comprises eight\nvision-grounded environments spanning cooperative, competitive, and\nmixed-motive interactions, designed to assess agents' ability to predict\nothers' future moves and optimize for long-term objectives. We consider two\ncomplementary evaluation dimensions, including offline evaluation of strategic\nreasoning by next-action prediction accuracy and online evaluation of\ndecision-making by normalized episode return. Extensive experiments of fourteen\nleading VLMs reveal a significant gap between current models and optimal\nperformance, with the best models attaining 47.8% prediction accuracy and 24.3%\nnormalized return. We further conduct in-depth analyses on multimodal\nobservations, test-time scaling, social behaviors, and failure cases of VLM\nagents. By standardizing the evaluation and highlighting the limitations of\nexisting models, we envision VS-Bench as a foundation for future research on\nstrategic multimodal agents. Code and data are available at\nhttps://vs-bench.github.io.",
      "upvotes": 25,
      "discussionId": "683fa95fa0770843560c7b3d",
      "projectPage": "https://vs-bench.github.io",
      "githubRepo": "https://github.com/zelaix/VS-Bench",
      "ai_summary": "VS-Bench is a multimodal benchmark designed to evaluate Vision Language Models' strategic reasoning and decision-making in complex multi-agent environments.",
      "ai_keywords": [
        "Vision Language Models",
        "VS-Bench",
        "multimodal benchmark",
        "strategic reasoning",
        "decision-making",
        "multi-agent environments",
        "vision-grounded environments",
        "cooperative",
        "competitive",
        "mixed-motive interactions",
        "next-action prediction",
        "normalized episode return",
        "multimodal observations",
        "test-time scaling",
        "social behaviors",
        "failure cases"
      ]
    },
    "publishedAt": "2025-06-02T22:57:38.000Z",
    "title": "VS-Bench: Evaluating VLMs for Strategic Reasoning and Decision-Making in\n  Multi-Agent Environments",
    "summary": "Recent advancements in Vision Language Models (VLMs) have expanded their\ncapabilities to interactive agent tasks, yet existing benchmarks remain limited\nto single-agent or text-only environments. In contrast, real-world scenarios\noften involve multiple agents interacting within rich visual and linguistic\ncontexts, posing challenges with both multimodal observations and strategic\ninteractions. To bridge this gap, we introduce Visual Strategic Bench\n(VS-Bench), a multimodal benchmark that evaluates VLMs for strategic reasoning\nand decision-making in multi-agent environments. VS-Bench comprises eight\nvision-grounded environments spanning cooperative, competitive, and\nmixed-motive interactions, designed to assess agents' ability to predict\nothers' future moves and optimize for long-term objectives. We consider two\ncomplementary evaluation dimensions, including offline evaluation of strategic\nreasoning by next-action prediction accuracy and online evaluation of\ndecision-making by normalized episode return. Extensive experiments of fourteen\nleading VLMs reveal a significant gap between current models and optimal\nperformance, with the best models attaining 47.8% prediction accuracy and 24.3%\nnormalized return. We further conduct in-depth analyses on multimodal\nobservations, test-time scaling, social behaviors, and failure cases of VLM\nagents. By standardizing the evaluation and highlighting the limitations of\nexisting models, we envision VS-Bench as a foundation for future research on\nstrategic multimodal agents. Code and data are available at\nhttps://vs-bench.github.io.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02387.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "653a5b0f7c01c693a16dd184",
      "avatarUrl": "/avatars/4b43d88709dc8037250404452e81adcf.svg",
      "fullname": "Zelai Xu",
      "name": "zelaix",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.00123",
      "authors": [
        {
          "_id": "683e709a3a4c2c3b2750fc32",
          "name": "Gen Luo",
          "hidden": false
        },
        {
          "_id": "683e709a3a4c2c3b2750fc33",
          "user": {
            "_id": "6565d7149afd51867e55520b",
            "avatarUrl": "/avatars/027b17651e61df598af53f69b92e7771.svg",
            "isPro": false,
            "fullname": "Ganlin Yang",
            "user": "ganlinyang",
            "type": "user"
          },
          "name": "Ganlin Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:45:03.857Z",
          "hidden": false
        },
        {
          "_id": "683e709a3a4c2c3b2750fc34",
          "user": {
            "_id": "660691330be1fbe3b9e4c33d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/660691330be1fbe3b9e4c33d/TxrDFH_cRu3AlpMC3xmhv.jpeg",
            "isPro": false,
            "fullname": "ZiYang Gong",
            "user": "Cusyoung",
            "type": "user"
          },
          "name": "Ziyang Gong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:45:00.364Z",
          "hidden": false
        },
        {
          "_id": "683e709a3a4c2c3b2750fc35",
          "name": "Guanzhou Chen",
          "hidden": false
        },
        {
          "_id": "683e709a3a4c2c3b2750fc36",
          "name": "Haonan Duan",
          "hidden": false
        },
        {
          "_id": "683e709a3a4c2c3b2750fc37",
          "name": "Erfei Cui",
          "hidden": false
        },
        {
          "_id": "683e709a3a4c2c3b2750fc38",
          "name": "Ronglei Tong",
          "hidden": false
        },
        {
          "_id": "683e709a3a4c2c3b2750fc39",
          "name": "Zhi Hou",
          "hidden": false
        },
        {
          "_id": "683e709a3a4c2c3b2750fc3a",
          "name": "Tianyi Zhang",
          "hidden": false
        },
        {
          "_id": "683e709a3a4c2c3b2750fc3b",
          "name": "Zhe Chen",
          "hidden": false
        },
        {
          "_id": "683e709a3a4c2c3b2750fc3c",
          "name": "Shenglong Ye",
          "hidden": false
        },
        {
          "_id": "683e709a3a4c2c3b2750fc3d",
          "name": "Lewei Lu",
          "hidden": false
        },
        {
          "_id": "683e709a3a4c2c3b2750fc3e",
          "name": "Jingbo Wang",
          "hidden": false
        },
        {
          "_id": "683e709a3a4c2c3b2750fc3f",
          "name": "Wenhai Wang",
          "hidden": false
        },
        {
          "_id": "683e709a3a4c2c3b2750fc40",
          "name": "Jifeng Dai",
          "hidden": false
        },
        {
          "_id": "683e709a3a4c2c3b2750fc41",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "683e709a3a4c2c3b2750fc42",
          "name": "Rongrong Ji",
          "hidden": false
        },
        {
          "_id": "683e709a3a4c2c3b2750fc43",
          "name": "Xizhou Zhu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T18:00:34.000Z",
      "submittedOnDailyAt": "2025-06-04T03:43:23.019Z",
      "title": "Visual Embodied Brain: Let Multimodal Large Language Models See, Think,\n  and Control in Spaces",
      "submittedOnDailyBy": {
        "_id": "6565d7149afd51867e55520b",
        "avatarUrl": "/avatars/027b17651e61df598af53f69b92e7771.svg",
        "isPro": false,
        "fullname": "Ganlin Yang",
        "user": "ganlinyang",
        "type": "user"
      },
      "summary": "The remarkable progress of Multimodal Large Language Models (MLLMs) has\nattracted increasing attention to extend them to physical entities like legged\nrobot. This typically requires MLLMs to not only grasp multimodal understanding\nabilities, but also integrate visual-spatial reasoning and physical interaction\ncapabilities. Nevertheless,existing methods struggle to unify these\ncapabilities due to their fundamental differences.In this paper, we present the\nVisual Embodied Brain (VeBrain), a unified framework for perception, reasoning,\nand control in real world. VeBrain reformulates robotic control into common\ntext-based MLLM tasks in the 2D visual space, thus unifying the objectives and\nmapping spaces of different tasks. Then, a novel robotic adapter is proposed to\nconvert textual control signals from MLLMs to motion policies of real robots.\nFrom the data perspective, we further introduce VeBrain-600k, a high-quality\ninstruction dataset encompassing various capabilities of VeBrain. In\nVeBrain-600k, we take hundreds of hours to collect, curate and annotate the\ndata, and adopt multimodal chain-of-thought(CoT) to mix the different\ncapabilities into a single conversation. Extensive experiments on 13 multimodal\nbenchmarks and 5 spatial intelligence benchmarks demonstrate the superior\nperformance of VeBrain to existing MLLMs like Qwen2.5-VL. When deployed to\nlegged robots and robotic arms, VeBrain shows strong adaptability, flexibility,\nand compositional capabilities compared to existing methods. For example,\ncompared to Qwen2.5-VL, VeBrain not only achieves substantial gains on MMVet by\n+5.6%, but also excels in legged robot tasks with +50% average gains.",
      "upvotes": 21,
      "discussionId": "683e70a13a4c2c3b2750fd76",
      "projectPage": "https://internvl.github.io/blog/2025-05-26-VeBrain/",
      "githubRepo": "https://github.com/OpenGVLab/VeBrain",
      "ai_summary": "VeBrain is a unified framework that integrates multimodal understanding, visual-spatial reasoning, and physical interaction for legged robots, demonstrating superior performance compared to existing methods across various benchmarks.",
      "ai_keywords": [
        "Multimodal Large Language Models",
        "VeBrain",
        "Visual Embodied Brain",
        "text-based MLLM tasks",
        "robotic adapter",
        "VeBrain-600k",
        "multimodal chain-of-thought",
        "MMVet",
        "compositional capabilities"
      ]
    },
    "publishedAt": "2025-05-30T14:00:34.000Z",
    "title": "Visual Embodied Brain: Let Multimodal Large Language Models See, Think,\n  and Control in Spaces",
    "summary": "The remarkable progress of Multimodal Large Language Models (MLLMs) has\nattracted increasing attention to extend them to physical entities like legged\nrobot. This typically requires MLLMs to not only grasp multimodal understanding\nabilities, but also integrate visual-spatial reasoning and physical interaction\ncapabilities. Nevertheless,existing methods struggle to unify these\ncapabilities due to their fundamental differences.In this paper, we present the\nVisual Embodied Brain (VeBrain), a unified framework for perception, reasoning,\nand control in real world. VeBrain reformulates robotic control into common\ntext-based MLLM tasks in the 2D visual space, thus unifying the objectives and\nmapping spaces of different tasks. Then, a novel robotic adapter is proposed to\nconvert textual control signals from MLLMs to motion policies of real robots.\nFrom the data perspective, we further introduce VeBrain-600k, a high-quality\ninstruction dataset encompassing various capabilities of VeBrain. In\nVeBrain-600k, we take hundreds of hours to collect, curate and annotate the\ndata, and adopt multimodal chain-of-thought(CoT) to mix the different\ncapabilities into a single conversation. Extensive experiments on 13 multimodal\nbenchmarks and 5 spatial intelligence benchmarks demonstrate the superior\nperformance of VeBrain to existing MLLMs like Qwen2.5-VL. When deployed to\nlegged robots and robotic arms, VeBrain shows strong adaptability, flexibility,\nand compositional capabilities compared to existing methods. For example,\ncompared to Qwen2.5-VL, VeBrain not only achieves substantial gains on MMVet by\n+5.6%, but also excels in legged robot tasks with +50% average gains.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00123.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "6565d7149afd51867e55520b",
      "avatarUrl": "/avatars/027b17651e61df598af53f69b92e7771.svg",
      "fullname": "Ganlin Yang",
      "name": "ganlinyang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.01674",
      "authors": [
        {
          "_id": "683faa6515abeae85e13336b",
          "name": "Yipeng Du",
          "hidden": false
        },
        {
          "_id": "683faa6515abeae85e13336c",
          "name": "Tiehan Fan",
          "hidden": false
        },
        {
          "_id": "683faa6515abeae85e13336d",
          "name": "Kepan Nan",
          "hidden": false
        },
        {
          "_id": "683faa6515abeae85e13336e",
          "name": "Rui Xie",
          "hidden": false
        },
        {
          "_id": "683faa6515abeae85e13336f",
          "name": "Penghao Zhou",
          "hidden": false
        },
        {
          "_id": "683faa6515abeae85e133370",
          "name": "Xiang Li",
          "hidden": false
        },
        {
          "_id": "683faa6515abeae85e133371",
          "name": "Jian Yang",
          "hidden": false
        },
        {
          "_id": "683faa6515abeae85e133372",
          "name": "Zhenheng Yang",
          "hidden": false
        },
        {
          "_id": "683faa6515abeae85e133373",
          "name": "Ying Tai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-02T13:44:56.000Z",
      "submittedOnDailyAt": "2025-06-04T00:38:03.935Z",
      "title": "MotionSight: Boosting Fine-Grained Motion Understanding in Multimodal\n  LLMs",
      "submittedOnDailyBy": {
        "_id": "65927f3b754092f6b1e187a7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65927f3b754092f6b1e187a7/gUrNvIQHmsl1vLwSUxpmL.jpeg",
        "isPro": false,
        "fullname": "tiehan fan",
        "user": "AnonMegumi",
        "type": "user"
      },
      "summary": "Despite advancements in Multimodal Large Language Models (MLLMs), their\nproficiency in fine-grained video motion understanding remains critically\nlimited. They often lack inter-frame differencing and tend to average or ignore\nsubtle visual cues. Furthermore, while visual prompting has shown potential in\nstatic images, its application to video's temporal complexities, particularly\nfor fine-grained motion understanding, remains largely unexplored. We\ninvestigate whether inherent capability can be unlocked and boost MLLMs' motion\nperception and enable distinct visual signatures tailored to decouple object\nand camera motion cues. In this study, we introduce MotionSight, a novel\nzero-shot method pioneering object-centric visual spotlight and motion blur as\nvisual prompts to effectively improve fine-grained motion understanding without\ntraining. To convert this into valuable data assets, we curated MotionVid-QA,\nthe first large-scale dataset for fine-grained video motion understanding, with\nhierarchical annotations including SFT and preference data, {\\Theta}(40K) video\nclips and {\\Theta}(87K) QAs. Experiments show MotionSight achieves\nstate-of-the-art open-source performance and competitiveness with commercial\nmodels. In particular, for fine-grained motion understanding we present a novel\nzero-shot technique and a large-scale, high-quality dataset. All the code and\nannotations will be publicly available.",
      "upvotes": 19,
      "discussionId": "683faa6615abeae85e1333c2",
      "projectPage": "https://nju-pcalab.github.io/projects/MotionSight/",
      "githubRepo": "https://github.com/NJU-PCALab/MotionSight",
      "ai_summary": "MotionSight, a zero-shot method using object-centric visual spotlight and motion blur as prompts, enhances fine-grained video motion understanding and achieves state-of-the-art performance on MotionVid-QA, a large-scale dataset with hierarchical annotations.",
      "ai_keywords": [
        "Multimodal Large Language Models",
        "MLLMs",
        "fine-grained video motion understanding",
        "inter-frame differencing",
        "visual prompting",
        "temporal complexities",
        "MotionSight",
        "object-centric visual spotlight",
        "motion blur",
        "MotionVid-QA",
        "hierarchical annotations",
        "SFT",
        "preference data",
        "state-of-the-art performance"
      ]
    },
    "publishedAt": "2025-06-02T09:44:56.000Z",
    "title": "MotionSight: Boosting Fine-Grained Motion Understanding in Multimodal\n  LLMs",
    "summary": "Despite advancements in Multimodal Large Language Models (MLLMs), their\nproficiency in fine-grained video motion understanding remains critically\nlimited. They often lack inter-frame differencing and tend to average or ignore\nsubtle visual cues. Furthermore, while visual prompting has shown potential in\nstatic images, its application to video's temporal complexities, particularly\nfor fine-grained motion understanding, remains largely unexplored. We\ninvestigate whether inherent capability can be unlocked and boost MLLMs' motion\nperception and enable distinct visual signatures tailored to decouple object\nand camera motion cues. In this study, we introduce MotionSight, a novel\nzero-shot method pioneering object-centric visual spotlight and motion blur as\nvisual prompts to effectively improve fine-grained motion understanding without\ntraining. To convert this into valuable data assets, we curated MotionVid-QA,\nthe first large-scale dataset for fine-grained video motion understanding, with\nhierarchical annotations including SFT and preference data, {\\Theta}(40K) video\nclips and {\\Theta}(87K) QAs. Experiments show MotionSight achieves\nstate-of-the-art open-source performance and competitiveness with commercial\nmodels. In particular, for fine-grained motion understanding we present a novel\nzero-shot technique and a large-scale, high-quality dataset. All the code and\nannotations will be publicly available.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01674.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65927f3b754092f6b1e187a7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65927f3b754092f6b1e187a7/gUrNvIQHmsl1vLwSUxpmL.jpeg",
      "fullname": "tiehan fan",
      "name": "AnonMegumi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.03135",
      "authors": [
        {
          "_id": "683fe55868402c738a8e5ee4",
          "name": "Mengdi Jia",
          "hidden": false
        },
        {
          "_id": "683fe55868402c738a8e5ee5",
          "name": "Zekun Qi",
          "hidden": false
        },
        {
          "_id": "683fe55868402c738a8e5ee6",
          "name": "Shaochen Zhang",
          "hidden": false
        },
        {
          "_id": "683fe55868402c738a8e5ee7",
          "name": "Wenyao Zhang",
          "hidden": false
        },
        {
          "_id": "683fe55868402c738a8e5ee8",
          "name": "Xinqiang Yu",
          "hidden": false
        },
        {
          "_id": "683fe55868402c738a8e5ee9",
          "name": "Jiawei He",
          "hidden": false
        },
        {
          "_id": "683fe55868402c738a8e5eea",
          "name": "He Wang",
          "hidden": false
        },
        {
          "_id": "683fe55868402c738a8e5eeb",
          "name": "Li Yi",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63c3e8abc7d7f4c63a515a02/ZZsfC9We_mrTAc2s-h5ng.mp4"
      ],
      "publishedAt": "2025-06-03T17:58:29.000Z",
      "submittedOnDailyAt": "2025-06-04T05:47:51.969Z",
      "title": "OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark for\n  Vision Language Models",
      "submittedOnDailyBy": {
        "_id": "63c3e8abc7d7f4c63a515a02",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c3e8abc7d7f4c63a515a02/npMHnVP2hHLbvoUGe7C4O.jpeg",
        "isPro": false,
        "fullname": "Zekun Qi",
        "user": "qizekun",
        "type": "user"
      },
      "summary": "Spatial reasoning is a key aspect of cognitive psychology and remains a major\nbottleneck for current vision-language models (VLMs). While extensive research\nhas aimed to evaluate or improve VLMs' understanding of basic spatial\nrelations, such as distinguishing left from right, near from far, and object\ncounting, these tasks represent only the most fundamental level of spatial\nreasoning. In this work, we introduce OmniSpatial, a comprehensive and\nchallenging benchmark for spatial reasoning, grounded in cognitive psychology.\nOmniSpatial covers four major categories: dynamic reasoning, complex spatial\nlogic, spatial interaction, and perspective-taking, with 50 fine-grained\nsubcategories. Through Internet data crawling and careful manual annotation, we\nconstruct over 1.5K question-answer pairs. Extensive experiments show that both\nopen- and closed-source VLMs, as well as existing reasoning and spatial\nunderstanding models, exhibit significant limitations in comprehensive spatial\nunderstanding. We further analyze failure cases and propose potential\ndirections for future research.",
      "upvotes": 17,
      "discussionId": "683fe55c68402c738a8e5ff4",
      "ai_summary": "A comprehensive benchmark called OmniSpatial evaluates vision-language models' understanding of advanced spatial reasoning tasks, revealing significant limitations across various models.",
      "ai_keywords": [
        "vision-language models",
        "spatial reasoning",
        "cognitive psychology",
        "dynamic reasoning",
        "complex spatial logic",
        "spatial interaction",
        "perspective-taking",
        "question-answer pairs"
      ]
    },
    "publishedAt": "2025-06-03T13:58:29.000Z",
    "title": "OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark for\n  Vision Language Models",
    "summary": "Spatial reasoning is a key aspect of cognitive psychology and remains a major\nbottleneck for current vision-language models (VLMs). While extensive research\nhas aimed to evaluate or improve VLMs' understanding of basic spatial\nrelations, such as distinguishing left from right, near from far, and object\ncounting, these tasks represent only the most fundamental level of spatial\nreasoning. In this work, we introduce OmniSpatial, a comprehensive and\nchallenging benchmark for spatial reasoning, grounded in cognitive psychology.\nOmniSpatial covers four major categories: dynamic reasoning, complex spatial\nlogic, spatial interaction, and perspective-taking, with 50 fine-grained\nsubcategories. Through Internet data crawling and careful manual annotation, we\nconstruct over 1.5K question-answer pairs. Extensive experiments show that both\nopen- and closed-source VLMs, as well as existing reasoning and spatial\nunderstanding models, exhibit significant limitations in comprehensive spatial\nunderstanding. We further analyze failure cases and propose potential\ndirections for future research.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63c3e8abc7d7f4c63a515a02/ZZsfC9We_mrTAc2s-h5ng.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03135.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63c3e8abc7d7f4c63a515a02",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c3e8abc7d7f4c63a515a02/npMHnVP2hHLbvoUGe7C4O.jpeg",
      "fullname": "Zekun Qi",
      "name": "qizekun",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.03065",
      "authors": [
        {
          "_id": "683fc6241de14546d5e02775",
          "name": "Pengtao Chen",
          "hidden": false
        },
        {
          "_id": "683fc6241de14546d5e02776",
          "name": "Xianfang Zeng",
          "hidden": false
        },
        {
          "_id": "683fc6241de14546d5e02777",
          "name": "Maosen Zhao",
          "hidden": false
        },
        {
          "_id": "683fc6241de14546d5e02778",
          "name": "Peng Ye",
          "hidden": false
        },
        {
          "_id": "683fc6241de14546d5e02779",
          "name": "Mingzhu Shen",
          "hidden": false
        },
        {
          "_id": "683fc6241de14546d5e0277a",
          "name": "Wei Cheng",
          "hidden": false
        },
        {
          "_id": "683fc6241de14546d5e0277b",
          "name": "Gang Yu",
          "hidden": false
        },
        {
          "_id": "683fc6241de14546d5e0277c",
          "name": "Tao Chen",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64b914c8ace99c0723ad83a9/iewcVFqJgelW9EQTlH_LH.png"
      ],
      "publishedAt": "2025-06-03T16:42:37.000Z",
      "submittedOnDailyAt": "2025-06-04T04:37:13.352Z",
      "title": "Sparse-vDiT: Unleashing the Power of Sparse Attention to Accelerate\n  Video Diffusion Transformers",
      "submittedOnDailyBy": {
        "_id": "64b914c8ace99c0723ad83a9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/B4gxNByeVY_xaOcjwiN1j.jpeg",
        "isPro": false,
        "fullname": "Wei Cheng",
        "user": "wchengad",
        "type": "user"
      },
      "summary": "While Diffusion Transformers (DiTs) have achieved breakthroughs in video\ngeneration, this long sequence generation task remains constrained by the\nquadratic complexity of attention mechanisms, resulting in significant\ninference latency. Through detailed analysis of attention maps in Video\nDiffusion Transformer (vDiT), we identify three recurring sparsity patterns:\ndiagonal, multi-diagonal, and vertical-stripe structures. And even 3-6\\%\nattention heads can be skipped. Crucially, these patterns exhibit strong\nlayer-depth and head-position correlations but show limited dependence on the\ninput content. Leveraging these findings, we propose Sparse-vDiT, a sparsity\nacceleration framework for vDiT comprising: 1) Pattern-optimized sparse kernels\nthat replace dense attention with computationally efficient implementations for\neach identified sparsity pattern. 2) An offline sparse diffusion search\nalgorithm that selects the optimal sparse computation strategy per layer and\nhead via hardware-aware cost modeling. After determining the optimal\nconfiguration, we fuse heads within the same layer that share the same\nattention strategy, enhancing inference efficiency. Integrated into\nstate-of-the-art vDiT models (CogVideoX1.5, HunyuanVideo, and Wan2.1),\nSparse-vDiT achieves 2.09times, 2.38times, and 1.67times theoretical\nFLOP reduction, and actual inference speedups of 1.76times, 1.85times,\nand 1.58times, respectively, while maintaining high visual fidelity, with\nPSNR values reaching 24.13, 27.09, and 22.59. Our work demonstrates that latent\nstructural sparsity in vDiTs can be systematically exploited for long video\nsynthesis.",
      "upvotes": 17,
      "discussionId": "683fc62b1de14546d5e02931",
      "githubRepo": "https://github.com/Peyton-Chen/Sparse-vDiT",
      "ai_summary": "Sparse-vDiT accelerates Video Diffusion Transformer (vDiT) by leveraging sparsity patterns in attention maps, reducing theoretical FLOPs and improving inference speed without significant loss in visual quality.",
      "ai_keywords": [
        "Diffusion Transformers",
        "DiTs",
        "Video Diffusion Transformer",
        "vDiT",
        "sparsity patterns",
        "diagonal",
        "multi-diagonal",
        "vertical-stripe structures",
        "sparse kernels",
        "sparse diffusion search algorithm",
        "FLOP reduction",
        "inference speedups",
        "PSNR",
        "latent structural sparsity"
      ]
    },
    "publishedAt": "2025-06-03T12:42:37.000Z",
    "title": "Sparse-vDiT: Unleashing the Power of Sparse Attention to Accelerate\n  Video Diffusion Transformers",
    "summary": "While Diffusion Transformers (DiTs) have achieved breakthroughs in video\ngeneration, this long sequence generation task remains constrained by the\nquadratic complexity of attention mechanisms, resulting in significant\ninference latency. Through detailed analysis of attention maps in Video\nDiffusion Transformer (vDiT), we identify three recurring sparsity patterns:\ndiagonal, multi-diagonal, and vertical-stripe structures. And even 3-6\\%\nattention heads can be skipped. Crucially, these patterns exhibit strong\nlayer-depth and head-position correlations but show limited dependence on the\ninput content. Leveraging these findings, we propose Sparse-vDiT, a sparsity\nacceleration framework for vDiT comprising: 1) Pattern-optimized sparse kernels\nthat replace dense attention with computationally efficient implementations for\neach identified sparsity pattern. 2) An offline sparse diffusion search\nalgorithm that selects the optimal sparse computation strategy per layer and\nhead via hardware-aware cost modeling. After determining the optimal\nconfiguration, we fuse heads within the same layer that share the same\nattention strategy, enhancing inference efficiency. Integrated into\nstate-of-the-art vDiT models (CogVideoX1.5, HunyuanVideo, and Wan2.1),\nSparse-vDiT achieves 2.09times, 2.38times, and 1.67times theoretical\nFLOP reduction, and actual inference speedups of 1.76times, 1.85times,\nand 1.58times, respectively, while maintaining high visual fidelity, with\nPSNR values reaching 24.13, 27.09, and 22.59. Our work demonstrates that latent\nstructural sparsity in vDiTs can be systematically exploited for long video\nsynthesis.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64b914c8ace99c0723ad83a9/iewcVFqJgelW9EQTlH_LH.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03065.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b914c8ace99c0723ad83a9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/B4gxNByeVY_xaOcjwiN1j.jpeg",
      "fullname": "Wei Cheng",
      "name": "wchengad",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.02096",
      "authors": [
        {
          "_id": "683fa36f7ed99d0040761114",
          "name": "Zijian Wu",
          "hidden": false
        },
        {
          "_id": "683fa36f7ed99d0040761115",
          "name": "Jinjie Ni",
          "hidden": false
        },
        {
          "_id": "683fa36f7ed99d0040761116",
          "name": "Xiangyan Liu",
          "hidden": false
        },
        {
          "_id": "683fa36f7ed99d0040761117",
          "name": "Zichen Liu",
          "hidden": false
        },
        {
          "_id": "683fa36f7ed99d0040761118",
          "name": "Hang Yan",
          "hidden": false
        },
        {
          "_id": "683fa36f7ed99d0040761119",
          "name": "Michael Qizhe Shieh",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-02T17:45:16.000Z",
      "submittedOnDailyAt": "2025-06-04T00:44:38.612Z",
      "title": "SynthRL: Scaling Visual Reasoning with Verifiable Data Synthesis",
      "submittedOnDailyBy": {
        "_id": "6486b09e8315b19342f0bf5e",
        "avatarUrl": "/avatars/bc5f22f231c884146d373fe1042d81bd.svg",
        "isPro": false,
        "fullname": "Xiangyan Liu",
        "user": "xyliu6",
        "type": "user"
      },
      "summary": "Vision-language models (VLMs) trained via reinforcement learning with\nverifiable reward (RLVR) have shown notable progress in scaling test-time\ncompute effectively. In this work, we investigate how synthesized RL data can\nfurther improve RLVR. To this end, we propose SynthRL-a scalable and\nguaranteed pipeline for automatic data scaling in reasoning-oriented RL\ntraining. SynthRL comprises three key stages: (1) selecting seed questions with\nappropriate distribution, (2) augmenting them into more challenging variants\nwhile preserving the original answers, and (3) a guaranteed verification stage\nthat ensures near-perfect correctness and difficulty enhancement. Our empirical\nexperiments demonstrate SynthRL's scalability and effectiveness. When applied\nto the MMK12 dataset, SynthRL synthesizes over 3.3K additional verifiable,\nchallenging questions from approximately 8K seed samples. Models trained with\nour synthesized data achieve consistent gains across five out-of-domain visual\nmath reasoning benchmarks, with a significant improvement over baseline models\ntrained on seed data alone. Notably, detailed analysis reveals that the gains\nare more pronounced on the most challenging evaluation samples, highlighting\nSynthRL's effectiveness in eliciting deeper and more complex reasoning\npatterns.",
      "upvotes": 16,
      "discussionId": "683fa3707ed99d0040761154",
      "ai_summary": "SynthRL, a scalable pipeline for data synthesis in reinforcement learning with verifiable rewards, enhances visual math reasoning VLMs by generating challenging, verifiable questions.",
      "ai_keywords": [
        "reinforcement learning",
        "verifiable reward",
        "RLVR",
        "SynthRL",
        "seed questions",
        "data augmentation",
        "verification stage",
        "MMK12 dataset",
        "visual math reasoning benchmarks"
      ]
    },
    "publishedAt": "2025-06-02T13:45:16.000Z",
    "title": "SynthRL: Scaling Visual Reasoning with Verifiable Data Synthesis",
    "summary": "Vision-language models (VLMs) trained via reinforcement learning with\nverifiable reward (RLVR) have shown notable progress in scaling test-time\ncompute effectively. In this work, we investigate how synthesized RL data can\nfurther improve RLVR. To this end, we propose SynthRL-a scalable and\nguaranteed pipeline for automatic data scaling in reasoning-oriented RL\ntraining. SynthRL comprises three key stages: (1) selecting seed questions with\nappropriate distribution, (2) augmenting them into more challenging variants\nwhile preserving the original answers, and (3) a guaranteed verification stage\nthat ensures near-perfect correctness and difficulty enhancement. Our empirical\nexperiments demonstrate SynthRL's scalability and effectiveness. When applied\nto the MMK12 dataset, SynthRL synthesizes over 3.3K additional verifiable,\nchallenging questions from approximately 8K seed samples. Models trained with\nour synthesized data achieve consistent gains across five out-of-domain visual\nmath reasoning benchmarks, with a significant improvement over baseline models\ntrained on seed data alone. Notably, detailed analysis reveals that the gains\nare more pronounced on the most challenging evaluation samples, highlighting\nSynthRL's effectiveness in eliciting deeper and more complex reasoning\npatterns.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02096.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6486b09e8315b19342f0bf5e",
      "avatarUrl": "/avatars/bc5f22f231c884146d373fe1042d81bd.svg",
      "fullname": "Xiangyan Liu",
      "name": "xyliu6",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.24120",
      "authors": [
        {
          "_id": "683fc08da33aeee1124887c4",
          "name": "Ai Jian",
          "hidden": false
        },
        {
          "_id": "683fc08da33aeee1124887c5",
          "name": "Weijie Qiu",
          "hidden": false
        },
        {
          "_id": "683fc08da33aeee1124887c6",
          "name": "Xiaokun Wang",
          "hidden": false
        },
        {
          "_id": "683fc08da33aeee1124887c7",
          "name": "Peiyu Wang",
          "hidden": false
        },
        {
          "_id": "683fc08da33aeee1124887c8",
          "name": "Yunzhuo Hao",
          "hidden": false
        },
        {
          "_id": "683fc08da33aeee1124887c9",
          "name": "Jiangbo Pei",
          "hidden": false
        },
        {
          "_id": "683fc08da33aeee1124887ca",
          "name": "Yichen Wei",
          "hidden": false
        },
        {
          "_id": "683fc08da33aeee1124887cb",
          "name": "Yi Peng",
          "hidden": false
        },
        {
          "_id": "683fc08da33aeee1124887cc",
          "name": "Xuchen Song",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/620f5a1c3f76c50e6458a9b6/l5zN70u8jAQBCjl41TWoi.png"
      ],
      "publishedAt": "2025-05-30T01:34:25.000Z",
      "submittedOnDailyAt": "2025-06-04T05:57:36.826Z",
      "title": "CSVQA: A Chinese Multimodal Benchmark for Evaluating STEM Reasoning\n  Capabilities of VLMs",
      "submittedOnDailyBy": {
        "_id": "620f5a1c3f76c50e6458a9b6",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/620f5a1c3f76c50e6458a9b6/pXh_f5F0UvufxuUa-eS-v.jpeg",
        "isPro": false,
        "fullname": "chris",
        "user": "OrlandoHugBot",
        "type": "user"
      },
      "summary": "Vision-Language Models (VLMs) have demonstrated remarkable progress in\nmultimodal understanding, yet their capabilities for scientific reasoning\nremains inadequately assessed. Current multimodal benchmarks predominantly\nevaluate generic image comprehension or text-driven reasoning, lacking\nauthentic scientific contexts that require domain-specific knowledge\nintegration with visual evidence analysis. To fill this gap, we present CSVQA,\na diagnostic multimodal benchmark specifically designed for evaluating\nscientific reasoning through domain-grounded visual question answering.Our\nbenchmark features 1,378 carefully constructed question-answer pairs spanning\ndiverse STEM disciplines, each demanding domain knowledge, integration of\nvisual evidence, and higher-order reasoning. Compared to prior multimodal\nbenchmarks, CSVQA places greater emphasis on real-world scientific content and\ncomplex reasoning.We additionally propose a rigorous evaluation protocol to\nsystematically assess whether model predictions are substantiated by valid\nintermediate reasoning steps based on curated explanations. Our comprehensive\nevaluation of 15 VLMs on this benchmark reveals notable performance\ndisparities, as even the top-ranked proprietary model attains only 49.6\\%\naccuracy.This empirical evidence underscores the pressing need for advancing\nscientific reasoning capabilities in VLMs. Our CSVQA is released at\nhttps://huggingface.co/datasets/Skywork/CSVQA.",
      "upvotes": 16,
      "discussionId": "683fc091a33aeee1124888a8",
      "ai_summary": "A new benchmark, CSVQA, evaluates scientific reasoning in vision-language models through domain-specific visual question answering, highlighting the need for improvement in these models.",
      "ai_keywords": [
        "Vision-Language Models",
        "multimodal benchmark",
        "scientific reasoning",
        "domain-grounded",
        "visual question answering",
        "domain-specific knowledge",
        "higher-order reasoning",
        "evaluation protocol",
        "intermediate reasoning steps",
        "curated explanations"
      ]
    },
    "publishedAt": "2025-05-29T21:34:25.000Z",
    "title": "CSVQA: A Chinese Multimodal Benchmark for Evaluating STEM Reasoning\n  Capabilities of VLMs",
    "summary": "Vision-Language Models (VLMs) have demonstrated remarkable progress in\nmultimodal understanding, yet their capabilities for scientific reasoning\nremains inadequately assessed. Current multimodal benchmarks predominantly\nevaluate generic image comprehension or text-driven reasoning, lacking\nauthentic scientific contexts that require domain-specific knowledge\nintegration with visual evidence analysis. To fill this gap, we present CSVQA,\na diagnostic multimodal benchmark specifically designed for evaluating\nscientific reasoning through domain-grounded visual question answering.Our\nbenchmark features 1,378 carefully constructed question-answer pairs spanning\ndiverse STEM disciplines, each demanding domain knowledge, integration of\nvisual evidence, and higher-order reasoning. Compared to prior multimodal\nbenchmarks, CSVQA places greater emphasis on real-world scientific content and\ncomplex reasoning.We additionally propose a rigorous evaluation protocol to\nsystematically assess whether model predictions are substantiated by valid\nintermediate reasoning steps based on curated explanations. Our comprehensive\nevaluation of 15 VLMs on this benchmark reveals notable performance\ndisparities, as even the top-ranked proprietary model attains only 49.6\\%\naccuracy.This empirical evidence underscores the pressing need for advancing\nscientific reasoning capabilities in VLMs. Our CSVQA is released at\nhttps://huggingface.co/datasets/Skywork/CSVQA.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/620f5a1c3f76c50e6458a9b6/l5zN70u8jAQBCjl41TWoi.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24120.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620f5a1c3f76c50e6458a9b6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/620f5a1c3f76c50e6458a9b6/pXh_f5F0UvufxuUa-eS-v.jpeg",
      "fullname": "chris",
      "name": "OrlandoHugBot",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.03131",
      "authors": [
        {
          "_id": "683fb2786a2b978ca4e62493",
          "name": "Zidong Wang",
          "hidden": false
        },
        {
          "_id": "683fb2786a2b978ca4e62494",
          "name": "Lei Bai",
          "hidden": false
        },
        {
          "_id": "683fb2786a2b978ca4e62495",
          "name": "Xiangyu Yue",
          "hidden": false
        },
        {
          "_id": "683fb2786a2b978ca4e62496",
          "name": "Wanli Ouyang",
          "hidden": false
        },
        {
          "_id": "683fb2786a2b978ca4e62497",
          "name": "Yiyuan Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63176933b58b0184630d2c74/VS2G-SBuSY5ltQmCLAaq3.png"
      ],
      "publishedAt": "2025-06-03T17:57:33.000Z",
      "submittedOnDailyAt": "2025-06-04T01:13:44.155Z",
      "title": "Native-Resolution Image Synthesis",
      "submittedOnDailyBy": {
        "_id": "63176933b58b0184630d2c74",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63176933b58b0184630d2c74/53b5EASwW76zeyyqeJA3O.jpeg",
        "isPro": false,
        "fullname": "Yiyuan Zhang",
        "user": "Yiyuan",
        "type": "user"
      },
      "summary": "We introduce native-resolution image synthesis, a novel generative modeling\nparadigm that enables the synthesis of images at arbitrary resolutions and\naspect ratios. This approach overcomes the limitations of conventional\nfixed-resolution, square-image methods by natively handling variable-length\nvisual tokens, a core challenge for traditional techniques. To this end, we\nintroduce the Native-resolution diffusion Transformer (NiT), an architecture\ndesigned to explicitly model varying resolutions and aspect ratios within its\ndenoising process. Free from the constraints of fixed formats, NiT learns\nintrinsic visual distributions from images spanning a broad range of\nresolutions and aspect ratios. Notably, a single NiT model simultaneously\nachieves the state-of-the-art performance on both ImageNet-256x256 and 512x512\nbenchmarks. Surprisingly, akin to the robust zero-shot capabilities seen in\nadvanced large language models, NiT, trained solely on ImageNet, demonstrates\nexcellent zero-shot generalization performance. It successfully generates\nhigh-fidelity images at previously unseen high resolutions (e.g., 1536 x 1536)\nand diverse aspect ratios (e.g., 16:9, 3:1, 4:3), as shown in Figure 1. These\nfindings indicate the significant potential of native-resolution modeling as a\nbridge between visual generative modeling and advanced LLM methodologies.",
      "upvotes": 13,
      "discussionId": "683fb27e6a2b978ca4e625bd",
      "projectPage": "https://wzdthu.github.io/NiT/",
      "githubRepo": "https://github.com/WZDTHU/NiT",
      "ai_summary": "A novel generative model, Native-resolution diffusion Transformer (NiT), synthesizes high-resolution and varied aspect ratio images with state-of-the-art performance and zero-shot generalization capabilities.",
      "ai_keywords": [
        "native-resolution image synthesis",
        "generative modeling paradigm",
        "variable-length visual tokens",
        "diffusion Transformer",
        "denoising process",
        "ImageNet-256x256",
        "ImageNet-512x512",
        "high-fidelity images",
        "aspect ratios",
        "zero-shot generalization"
      ]
    },
    "publishedAt": "2025-06-03T13:57:33.000Z",
    "title": "Native-Resolution Image Synthesis",
    "summary": "We introduce native-resolution image synthesis, a novel generative modeling\nparadigm that enables the synthesis of images at arbitrary resolutions and\naspect ratios. This approach overcomes the limitations of conventional\nfixed-resolution, square-image methods by natively handling variable-length\nvisual tokens, a core challenge for traditional techniques. To this end, we\nintroduce the Native-resolution diffusion Transformer (NiT), an architecture\ndesigned to explicitly model varying resolutions and aspect ratios within its\ndenoising process. Free from the constraints of fixed formats, NiT learns\nintrinsic visual distributions from images spanning a broad range of\nresolutions and aspect ratios. Notably, a single NiT model simultaneously\nachieves the state-of-the-art performance on both ImageNet-256x256 and 512x512\nbenchmarks. Surprisingly, akin to the robust zero-shot capabilities seen in\nadvanced large language models, NiT, trained solely on ImageNet, demonstrates\nexcellent zero-shot generalization performance. It successfully generates\nhigh-fidelity images at previously unseen high resolutions (e.g., 1536 x 1536)\nand diverse aspect ratios (e.g., 16:9, 3:1, 4:3), as shown in Figure 1. These\nfindings indicate the significant potential of native-resolution modeling as a\nbridge between visual generative modeling and advanced LLM methodologies.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63176933b58b0184630d2c74/VS2G-SBuSY5ltQmCLAaq3.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03131.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63176933b58b0184630d2c74",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63176933b58b0184630d2c74/53b5EASwW76zeyyqeJA3O.jpeg",
      "fullname": "Yiyuan Zhang",
      "name": "Yiyuan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.03143",
      "authors": [
        {
          "_id": "683fc5599363b50c19f17d42",
          "user": {
            "_id": "63ef330b1e695b35aa484e11",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ef330b1e695b35aa484e11/bXwpGy0dl8JXeJwJ--ilr.jpeg",
            "isPro": false,
            "fullname": "Qianhui WU",
            "user": "qianhuiwu",
            "type": "user"
          },
          "name": "Qianhui Wu",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-06-04T04:21:31.171Z",
          "hidden": false
        },
        {
          "_id": "683fc5599363b50c19f17d43",
          "name": "Kanzhi Cheng",
          "hidden": false
        },
        {
          "_id": "683fc5599363b50c19f17d44",
          "name": "Rui Yang",
          "hidden": false
        },
        {
          "_id": "683fc5599363b50c19f17d45",
          "name": "Chaoyun Zhang",
          "hidden": false
        },
        {
          "_id": "683fc5599363b50c19f17d46",
          "name": "Jianwei Yang",
          "hidden": false
        },
        {
          "_id": "683fc5599363b50c19f17d47",
          "name": "Huiqiang Jiang",
          "hidden": false
        },
        {
          "_id": "683fc5599363b50c19f17d48",
          "name": "Jian Mu",
          "hidden": false
        },
        {
          "_id": "683fc5599363b50c19f17d49",
          "name": "Baolin Peng",
          "hidden": false
        },
        {
          "_id": "683fc5599363b50c19f17d4a",
          "name": "Bo Qiao",
          "hidden": false
        },
        {
          "_id": "683fc5599363b50c19f17d4b",
          "name": "Reuben Tan",
          "hidden": false
        },
        {
          "_id": "683fc5599363b50c19f17d4c",
          "name": "Si Qin",
          "hidden": false
        },
        {
          "_id": "683fc5599363b50c19f17d4d",
          "name": "Lars Liden",
          "hidden": false
        },
        {
          "_id": "683fc5599363b50c19f17d4e",
          "name": "Qingwei Lin",
          "hidden": false
        },
        {
          "_id": "683fc5599363b50c19f17d4f",
          "name": "Huan Zhang",
          "hidden": false
        },
        {
          "_id": "683fc5599363b50c19f17d50",
          "name": "Tong Zhang",
          "hidden": false
        },
        {
          "_id": "683fc5599363b50c19f17d51",
          "name": "Jianbing Zhang",
          "hidden": false
        },
        {
          "_id": "683fc5599363b50c19f17d52",
          "name": "Dongmei Zhang",
          "hidden": false
        },
        {
          "_id": "683fc5599363b50c19f17d53",
          "name": "Jianfeng Gao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T17:59:08.000Z",
      "submittedOnDailyAt": "2025-06-04T02:44:41.730Z",
      "title": "GUI-Actor: Coordinate-Free Visual Grounding for GUI Agents",
      "submittedOnDailyBy": {
        "_id": "654dbac9938fbf1e696be8aa",
        "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
        "isPro": false,
        "fullname": "Chaoyun Zhang",
        "user": "vyokky",
        "type": "user"
      },
      "summary": "One of the principal challenges in building VLM-powered GUI agents is visual\ngrounding, i.e., localizing the appropriate screen region for action execution\nbased on both the visual content and the textual plans. Most existing work\nformulates this as a text-based coordinate generation task. However, these\napproaches suffer from several limitations: weak spatial-semantic alignment,\ninability to handle ambiguous supervision targets, and a mismatch between the\ndense nature of screen coordinates and the coarse, patch-level granularity of\nvisual features extracted by models like Vision Transformers. In this paper, we\npropose GUI-Actor, a VLM-based method for coordinate-free GUI grounding. At its\ncore, GUI-Actor introduces an attention-based action head that learns to align\na dedicated <ACTOR> token with all relevant visual patch tokens, enabling the\nmodel to propose one or more action regions in a single forward pass. In line\nwith this, we further design a grounding verifier to evaluate and select the\nmost plausible action region from the candidates proposed for action execution.\nExtensive experiments show that GUI-Actor outperforms prior state-of-the-art\nmethods on multiple GUI action grounding benchmarks, with improved\ngeneralization to unseen screen resolutions and layouts. Notably, GUI-Actor-7B\neven surpasses UI-TARS-72B (38.1) on ScreenSpot-Pro, achieving scores of 40.7\nwith Qwen2-VL and 44.6 with Qwen2.5-VL as backbones. Furthermore, by\nincorporating the verifier, we find that fine-tuning only the newly introduced\naction head (~100M parameters for 7B model) while keeping the VLM backbone\nfrozen is sufficient to achieve performance comparable to previous\nstate-of-the-art models, highlighting that GUI-Actor can endow the underlying\nVLM with effective grounding capabilities without compromising its\ngeneral-purpose strengths.",
      "upvotes": 12,
      "discussionId": "683fc55d9363b50c19f17e6b",
      "projectPage": "https://microsoft.github.io/GUI-Actor/",
      "githubRepo": "https://github.com/microsoft/GUI-Actor",
      "ai_summary": "GUI-Actor, a VLM-based method using attention for coordinate-free GUI grounding, outperforms existing methods with better generalization and efficient fine-tuning.",
      "ai_keywords": [
        "visual grounding",
        "text-based coordinate generation",
        "spatial-semantic alignment",
        "Vision Transformers",
        "attention-based action head",
        "grounding verifier",
        "GUI action grounding benchmarks",
        "GUI-Actor",
        "GUI-Actor-7B",
        "UI-TARS-72B",
        "ScreenSpot-Pro",
        "Qwen2-VL",
        "Qwen2.5-VL",
        "parameter-efficient fine-tuning"
      ]
    },
    "publishedAt": "2025-06-03T13:59:08.000Z",
    "title": "GUI-Actor: Coordinate-Free Visual Grounding for GUI Agents",
    "summary": "One of the principal challenges in building VLM-powered GUI agents is visual\ngrounding, i.e., localizing the appropriate screen region for action execution\nbased on both the visual content and the textual plans. Most existing work\nformulates this as a text-based coordinate generation task. However, these\napproaches suffer from several limitations: weak spatial-semantic alignment,\ninability to handle ambiguous supervision targets, and a mismatch between the\ndense nature of screen coordinates and the coarse, patch-level granularity of\nvisual features extracted by models like Vision Transformers. In this paper, we\npropose GUI-Actor, a VLM-based method for coordinate-free GUI grounding. At its\ncore, GUI-Actor introduces an attention-based action head that learns to align\na dedicated <ACTOR> token with all relevant visual patch tokens, enabling the\nmodel to propose one or more action regions in a single forward pass. In line\nwith this, we further design a grounding verifier to evaluate and select the\nmost plausible action region from the candidates proposed for action execution.\nExtensive experiments show that GUI-Actor outperforms prior state-of-the-art\nmethods on multiple GUI action grounding benchmarks, with improved\ngeneralization to unseen screen resolutions and layouts. Notably, GUI-Actor-7B\neven surpasses UI-TARS-72B (38.1) on ScreenSpot-Pro, achieving scores of 40.7\nwith Qwen2-VL and 44.6 with Qwen2.5-VL as backbones. Furthermore, by\nincorporating the verifier, we find that fine-tuning only the newly introduced\naction head (~100M parameters for 7B model) while keeping the VLM backbone\nfrozen is sufficient to achieve performance comparable to previous\nstate-of-the-art models, highlighting that GUI-Actor can endow the underlying\nVLM with effective grounding capabilities without compromising its\ngeneral-purpose strengths.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03143.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "654dbac9938fbf1e696be8aa",
      "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
      "fullname": "Chaoyun Zhang",
      "name": "vyokky",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.00070",
      "authors": [
        {
          "_id": "683fd7d79d4fb703271ad9ac",
          "name": "Dongyoung Kim",
          "hidden": false
        },
        {
          "_id": "683fd7d79d4fb703271ad9ad",
          "name": "Sumin Park",
          "hidden": false
        },
        {
          "_id": "683fd7d79d4fb703271ad9ae",
          "name": "Huiwon Jang",
          "hidden": false
        },
        {
          "_id": "683fd7d79d4fb703271ad9af",
          "name": "Jinwoo Shin",
          "hidden": false
        },
        {
          "_id": "683fd7d79d4fb703271ad9b0",
          "name": "Jaehyung Kim",
          "hidden": false
        },
        {
          "_id": "683fd7d79d4fb703271ad9b1",
          "name": "Younggyo Seo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T16:41:12.000Z",
      "submittedOnDailyAt": "2025-06-04T03:53:04.651Z",
      "title": "Robot-R1: Reinforcement Learning for Enhanced Embodied Reasoning in\n  Robotics",
      "submittedOnDailyBy": {
        "_id": "658d79663a5202a485a76d9b",
        "avatarUrl": "/avatars/1893384f28a7cb82d4588576c4c264f1.svg",
        "isPro": false,
        "fullname": "dongyoung kim",
        "user": "vangard703",
        "type": "user"
      },
      "summary": "Large Vision-Language Models (LVLMs) have recently shown great promise in\nadvancing robotics by combining embodied reasoning with robot control. A common\napproach involves training on embodied reasoning tasks related to robot control\nusing Supervised Fine-Tuning (SFT). However, SFT datasets are often\nheuristically constructed and not explicitly optimized for improving robot\ncontrol. Furthermore, SFT often leads to issues such as catastrophic forgetting\nand reduced generalization performance. To address these limitations, we\nintroduce Robot-R1, a novel framework that leverages reinforcement learning to\nenhance embodied reasoning specifically for robot control. Robot-R1 learns to\npredict the next keypoint state required for task completion, conditioned on\nthe current scene image and environment metadata derived from expert\ndemonstrations. Inspired by the DeepSeek-R1 learning approach, Robot-R1 samples\nreasoning-based responses and reinforces those that lead to more accurate\npredictions. Our experiments show that models trained with Robot-R1 outperform\nSFT methods on embodied reasoning tasks. Despite having only 7B parameters,\nRobot-R1 even surpasses GPT-4o on reasoning tasks related to low-level action\ncontrol, such as spatial and primitive movement reasoning.",
      "upvotes": 12,
      "discussionId": "683fd7d79d4fb703271ad9d9",
      "ai_summary": "Robot-R1, a reinforcement learning framework, enhances embodied reasoning for robotics by predicting keypoint states, outperforming supervised fine-tuning methods and even surpassing GPT-4o in low-level action control tasks.",
      "ai_keywords": [
        "Large Vision-Language Models (LVLMs)",
        "embodied reasoning",
        "robot control",
        "Supervised Fine-Tuning (SFT)",
        "catastrophic forgetting",
        "generalization performance",
        "reinforcement learning",
        "keypoint state",
        "scene image",
        "environment metadata",
        "expert demonstrations",
        "DeepSeek-R1",
        "reasoning-based responses",
        "parameter-efficient fine-tuning"
      ]
    },
    "publishedAt": "2025-05-29T12:41:12.000Z",
    "title": "Robot-R1: Reinforcement Learning for Enhanced Embodied Reasoning in\n  Robotics",
    "summary": "Large Vision-Language Models (LVLMs) have recently shown great promise in\nadvancing robotics by combining embodied reasoning with robot control. A common\napproach involves training on embodied reasoning tasks related to robot control\nusing Supervised Fine-Tuning (SFT). However, SFT datasets are often\nheuristically constructed and not explicitly optimized for improving robot\ncontrol. Furthermore, SFT often leads to issues such as catastrophic forgetting\nand reduced generalization performance. To address these limitations, we\nintroduce Robot-R1, a novel framework that leverages reinforcement learning to\nenhance embodied reasoning specifically for robot control. Robot-R1 learns to\npredict the next keypoint state required for task completion, conditioned on\nthe current scene image and environment metadata derived from expert\ndemonstrations. Inspired by the DeepSeek-R1 learning approach, Robot-R1 samples\nreasoning-based responses and reinforces those that lead to more accurate\npredictions. Our experiments show that models trained with Robot-R1 outperform\nSFT methods on embodied reasoning tasks. Despite having only 7B parameters,\nRobot-R1 even surpasses GPT-4o on reasoning tasks related to low-level action\ncontrol, such as spatial and primitive movement reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00070.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "658d79663a5202a485a76d9b",
      "avatarUrl": "/avatars/1893384f28a7cb82d4588576c4c264f1.svg",
      "fullname": "dongyoung kim",
      "name": "vangard703",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.23061",
      "authors": [
        {
          "_id": "683fc4028de3ffc5838c3fa8",
          "name": "Tarun Suresh",
          "hidden": false
        },
        {
          "_id": "683fc4028de3ffc5838c3fa9",
          "name": "Debangshu Banerjee",
          "hidden": false
        },
        {
          "_id": "683fc4028de3ffc5838c3faa",
          "name": "Shubham Ugare",
          "hidden": false
        },
        {
          "_id": "683fc4028de3ffc5838c3fab",
          "name": "Sasa Misailovic",
          "hidden": false
        },
        {
          "_id": "683fc4028de3ffc5838c3fac",
          "name": "Gagandeep Singh",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T04:04:54.000Z",
      "submittedOnDailyAt": "2025-06-04T02:27:09.092Z",
      "title": "DINGO: Constrained Inference for Diffusion LLMs",
      "submittedOnDailyBy": {
        "_id": "65e7bb35e5e78134ab049942",
        "avatarUrl": "/avatars/3c0972f0d59e51ebb5c218ee736d4458.svg",
        "isPro": false,
        "fullname": "Tarun Suresh",
        "user": "tarsur909",
        "type": "user"
      },
      "summary": "Diffusion LLMs have emerged as a promising alternative to conventional\nautoregressive LLMs, offering significant potential for improved runtime\nefficiency. However, existing diffusion models lack the ability to provably\nenforce user-specified formal constraints, such as regular expressions, which\nmakes them unreliable for tasks that require structured outputs, such as\nfixed-schema JSON generation. Unlike autoregressive models that generate tokens\nsequentially, diffusion LLMs predict a block of tokens in parallel. This\nparallelism makes traditional constrained decoding algorithms, which are\ndesigned for sequential token prediction, ineffective at preserving the true\noutput distribution. To address this limitation, we propose DINGO, a dynamic\nprogramming-based constrained decoding strategy that is both efficient and\nprovably distribution-preserving. DINGO enables sampling of output strings with\nthe highest probability under the model's predicted distribution, while\nstrictly satisfying any user-specified regular expression. On standard symbolic\nmath and JSON generation benchmarks, DINGO achieves up to a 68 percentage point\nimprovement over unconstrained inference",
      "upvotes": 12,
      "discussionId": "683fc4038de3ffc5838c3fd8",
      "ai_summary": "DINGO, a dynamic programming-based decoding strategy, enhances diffusion language models by enforcing structured output constraints, significantly improving performance on symbolic math and JSON generation tasks.",
      "ai_keywords": [
        "diffusion LLMs",
        "autoregressive LLMs",
        "formal constraints",
        "regular expressions",
        "sequential token prediction",
        "parallel token prediction",
        "dynamic programming",
        "constrained decoding",
        "output distribution",
        "symbolic math generation",
        "JSON generation"
      ]
    },
    "publishedAt": "2025-05-29T00:04:54.000Z",
    "title": "DINGO: Constrained Inference for Diffusion LLMs",
    "summary": "Diffusion LLMs have emerged as a promising alternative to conventional\nautoregressive LLMs, offering significant potential for improved runtime\nefficiency. However, existing diffusion models lack the ability to provably\nenforce user-specified formal constraints, such as regular expressions, which\nmakes them unreliable for tasks that require structured outputs, such as\nfixed-schema JSON generation. Unlike autoregressive models that generate tokens\nsequentially, diffusion LLMs predict a block of tokens in parallel. This\nparallelism makes traditional constrained decoding algorithms, which are\ndesigned for sequential token prediction, ineffective at preserving the true\noutput distribution. To address this limitation, we propose DINGO, a dynamic\nprogramming-based constrained decoding strategy that is both efficient and\nprovably distribution-preserving. DINGO enables sampling of output strings with\nthe highest probability under the model's predicted distribution, while\nstrictly satisfying any user-specified regular expression. On standard symbolic\nmath and JSON generation benchmarks, DINGO achieves up to a 68 percentage point\nimprovement over unconstrained inference",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23061.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65e7bb35e5e78134ab049942",
      "avatarUrl": "/avatars/3c0972f0d59e51ebb5c218ee736d4458.svg",
      "fullname": "Tarun Suresh",
      "name": "tarsur909",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.03136",
      "authors": [
        {
          "_id": "683fa2ddbde0ae60c2f16183",
          "name": "Yinjie Wang",
          "hidden": false
        },
        {
          "_id": "683fa2ddbde0ae60c2f16184",
          "name": "Ling Yang",
          "hidden": false
        },
        {
          "_id": "683fa2ddbde0ae60c2f16185",
          "name": "Ye Tian",
          "hidden": false
        },
        {
          "_id": "683fa2ddbde0ae60c2f16186",
          "name": "Ke Shen",
          "hidden": false
        },
        {
          "_id": "683fa2ddbde0ae60c2f16187",
          "name": "Mengdi Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T17:58:42.000Z",
      "submittedOnDailyAt": "2025-06-04T00:39:07.151Z",
      "title": "Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning",
      "submittedOnDailyBy": {
        "_id": "64fde4e252e82dd432b74ce9",
        "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
        "isPro": false,
        "fullname": "Ling Yang",
        "user": "Lingaaaaaaa",
        "type": "user"
      },
      "summary": "We propose CURE, a novel reinforcement learning framework with a dedicated\nreward design that co-evolves coding and unit test generation capabilities\nbased on their interaction outcomes, without any ground-truth code as\nsupervision. This approach enables flexible and scalable training and allows\nthe unit tester to learn directly from the coder's mistakes. Our derived\nReasonFlux-Coder-7B and 14B models improve code generation accuracy by 5.3% and\nBest-of-N accuracy by 9.0% after optimization on Qwen2.5-Instruct models,\noutperforming similarly sized Qwen-Coder, DeepSeek-Coder, and Seed-Coder. They\nnaturally extend to downstream tasks such as test-time scaling and agentic\ncoding-achieving a 8.1% improvement over the base model. For the long-CoT\nmodel, our ReasonFlux-Coder-4B consistently outperforms Qwen3-4B while\nachieving 64.8% inference efficiency in unit test generation. Notably, we also\nfind that our model can serve as an effective reward model for reinforcement\nlearning on base models. Project: https://github.com/Gen-Verse/CURE",
      "upvotes": 11,
      "discussionId": "683fa2debde0ae60c2f161cb",
      "projectPage": "https://huggingface.co/collections/Gen-Verse/reasonflux-coder-6833109ed9300c62deb32c6b",
      "githubRepo": "https://github.com/Gen-Verse/CURE",
      "ai_summary": "CURE, a reinforcement learning framework, improves code and unit test generation accuracy without ground-truth supervision, enhancing performance in various coding and testing tasks.",
      "ai_keywords": [
        "reinforcement learning",
        "reward design",
        "coding",
        "unit test generation",
        "ReasonFlux-Coder",
        "Qwen2.5-Instruct",
        "Qwen-Coder",
        "DeepSeek-Coder",
        "Seed-Coder",
        "test-time scaling",
        "agentic coding",
        "long-CoT",
        "inference efficiency",
        "reward model"
      ]
    },
    "publishedAt": "2025-06-03T13:58:42.000Z",
    "title": "Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning",
    "summary": "We propose CURE, a novel reinforcement learning framework with a dedicated\nreward design that co-evolves coding and unit test generation capabilities\nbased on their interaction outcomes, without any ground-truth code as\nsupervision. This approach enables flexible and scalable training and allows\nthe unit tester to learn directly from the coder's mistakes. Our derived\nReasonFlux-Coder-7B and 14B models improve code generation accuracy by 5.3% and\nBest-of-N accuracy by 9.0% after optimization on Qwen2.5-Instruct models,\noutperforming similarly sized Qwen-Coder, DeepSeek-Coder, and Seed-Coder. They\nnaturally extend to downstream tasks such as test-time scaling and agentic\ncoding-achieving a 8.1% improvement over the base model. For the long-CoT\nmodel, our ReasonFlux-Coder-4B consistently outperforms Qwen3-4B while\nachieving 64.8% inference efficiency in unit test generation. Notably, we also\nfind that our model can serve as an effective reward model for reinforcement\nlearning on base models. Project: https://github.com/Gen-Verse/CURE",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03136.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64fde4e252e82dd432b74ce9",
      "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
      "fullname": "Ling Yang",
      "name": "Lingaaaaaaa",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.24714",
      "authors": [
        {
          "_id": "683d04c751706d12b2c262ea",
          "name": "Junyu Luo",
          "hidden": false
        },
        {
          "_id": "683d04c751706d12b2c262eb",
          "name": "Zhizhuo Kou",
          "hidden": false
        },
        {
          "_id": "683d04c751706d12b2c262ec",
          "name": "Liming Yang",
          "hidden": false
        },
        {
          "_id": "683d04c751706d12b2c262ed",
          "name": "Xiao Luo",
          "hidden": false
        },
        {
          "_id": "683d04c751706d12b2c262ee",
          "name": "Jinsheng Huang",
          "hidden": false
        },
        {
          "_id": "683d04c751706d12b2c262ef",
          "name": "Zhiping Xiao",
          "hidden": false
        },
        {
          "_id": "683d04c751706d12b2c262f0",
          "name": "Jingshu Peng",
          "hidden": false
        },
        {
          "_id": "683d04c751706d12b2c262f1",
          "name": "Chengzhong Liu",
          "hidden": false
        },
        {
          "_id": "683d04c751706d12b2c262f2",
          "name": "Jiaming Ji",
          "hidden": false
        },
        {
          "_id": "683d04c751706d12b2c262f3",
          "name": "Xuanzhe Liu",
          "hidden": false
        },
        {
          "_id": "683d04c751706d12b2c262f4",
          "name": "Sirui Han",
          "hidden": false
        },
        {
          "_id": "683d04c751706d12b2c262f5",
          "name": "Ming Zhang",
          "hidden": false
        },
        {
          "_id": "683d04c751706d12b2c262f6",
          "name": "Yike Guo",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/642da1cd99f3110ac27caca5/ilkQ2K5LD4SPQ5oCcV2aB.png"
      ],
      "publishedAt": "2025-05-30T15:36:19.000Z",
      "submittedOnDailyAt": "2025-06-04T01:04:36.480Z",
      "title": "FinMME: Benchmark Dataset for Financial Multi-Modal Reasoning Evaluation",
      "submittedOnDailyBy": {
        "_id": "642da1cd99f3110ac27caca5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642da1cd99f3110ac27caca5/C1QJY3R_ZdaeANG1y8iW7.jpeg",
        "isPro": false,
        "fullname": "junyu",
        "user": "luojunyu",
        "type": "user"
      },
      "summary": "Multimodal Large Language Models (MLLMs) have experienced rapid development\nin recent years. However, in the financial domain, there is a notable lack of\neffective and specialized multimodal evaluation datasets. To advance the\ndevelopment of MLLMs in the finance domain, we introduce FinMME, encompassing\nmore than 11,000 high-quality financial research samples across 18 financial\ndomains and 6 asset classes, featuring 10 major chart types and 21 subtypes. We\nensure data quality through 20 annotators and carefully designed validation\nmechanisms. Additionally, we develop FinScore, an evaluation system\nincorporating hallucination penalties and multi-dimensional capability\nassessment to provide an unbiased evaluation. Extensive experimental results\ndemonstrate that even state-of-the-art models like GPT-4o exhibit\nunsatisfactory performance on FinMME, highlighting its challenging nature. The\nbenchmark exhibits high robustness with prediction variations under different\nprompts remaining below 1%, demonstrating superior reliability compared to\nexisting datasets. Our dataset and evaluation protocol are available at\nhttps://huggingface.co/datasets/luojunyu/FinMME and\nhttps://github.com/luo-junyu/FinMME.",
      "upvotes": 11,
      "discussionId": "683d04c951706d12b2c26367",
      "projectPage": "https://huggingface.co/datasets/luojunyu/FinMME",
      "githubRepo": "https://github.com/luo-junyu/FinMME",
      "ai_summary": "FinMME is a comprehensive multimodal dataset for financial research and FinScore is an evaluation system that highlights the challenges faced by even advanced models like GPT-4o in the finance domain.",
      "ai_keywords": [
        "Multimodal Large Language Models",
        "FinMME",
        "financial research samples",
        "high-quality dataset",
        "financial domains",
        "asset classes",
        "chart types",
        "data quality",
        "FinScore",
        "hallucination penalties",
        "multi-dimensional capability assessment",
        "benchmark dataset",
        "prediction robustness"
      ]
    },
    "publishedAt": "2025-05-30T11:36:19.000Z",
    "title": "FinMME: Benchmark Dataset for Financial Multi-Modal Reasoning Evaluation",
    "summary": "Multimodal Large Language Models (MLLMs) have experienced rapid development\nin recent years. However, in the financial domain, there is a notable lack of\neffective and specialized multimodal evaluation datasets. To advance the\ndevelopment of MLLMs in the finance domain, we introduce FinMME, encompassing\nmore than 11,000 high-quality financial research samples across 18 financial\ndomains and 6 asset classes, featuring 10 major chart types and 21 subtypes. We\nensure data quality through 20 annotators and carefully designed validation\nmechanisms. Additionally, we develop FinScore, an evaluation system\nincorporating hallucination penalties and multi-dimensional capability\nassessment to provide an unbiased evaluation. Extensive experimental results\ndemonstrate that even state-of-the-art models like GPT-4o exhibit\nunsatisfactory performance on FinMME, highlighting its challenging nature. The\nbenchmark exhibits high robustness with prediction variations under different\nprompts remaining below 1%, demonstrating superior reliability compared to\nexisting datasets. Our dataset and evaluation protocol are available at\nhttps://huggingface.co/datasets/luojunyu/FinMME and\nhttps://github.com/luo-junyu/FinMME.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/642da1cd99f3110ac27caca5/ilkQ2K5LD4SPQ5oCcV2aB.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24714.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "642da1cd99f3110ac27caca5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642da1cd99f3110ac27caca5/C1QJY3R_ZdaeANG1y8iW7.jpeg",
      "fullname": "junyu",
      "name": "luojunyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.02528",
      "authors": [
        {
          "_id": "683fb8bc3bcb592f18f5b866",
          "name": "Yan Gong",
          "hidden": false
        },
        {
          "_id": "683fb8bc3bcb592f18f5b867",
          "name": "Yiren Song",
          "hidden": false
        },
        {
          "_id": "683fb8bc3bcb592f18f5b868",
          "name": "Yicheng Li",
          "hidden": false
        },
        {
          "_id": "683fb8bc3bcb592f18f5b869",
          "name": "Chenglin Li",
          "hidden": false
        },
        {
          "_id": "683fb8bc3bcb592f18f5b86a",
          "name": "Yin Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T07:06:35.000Z",
      "submittedOnDailyAt": "2025-06-04T01:39:07.675Z",
      "title": "RelationAdapter: Learning and Transferring Visual Relation with\n  Diffusion Transformers",
      "submittedOnDailyBy": {
        "_id": "64311a95034ecbefddd141ef",
        "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
        "isPro": true,
        "fullname": "Yiren Song",
        "user": "yiren98",
        "type": "user"
      },
      "summary": "Inspired by the in-context learning mechanism of large language models\n(LLMs), a new paradigm of generalizable visual prompt-based image editing is\nemerging. Existing single-reference methods typically focus on style or\nappearance adjustments and struggle with non-rigid transformations. To address\nthese limitations, we propose leveraging source-target image pairs to extract\nand transfer content-aware editing intent to novel query images. To this end,\nwe introduce RelationAdapter, a lightweight module that enables Diffusion\nTransformer (DiT) based models to effectively capture and apply visual\ntransformations from minimal examples. We also introduce Relation252K, a\ncomprehensive dataset comprising 218 diverse editing tasks, to evaluate model\ngeneralization and adaptability in visual prompt-driven scenarios. Experiments\non Relation252K show that RelationAdapter significantly improves the model's\nability to understand and transfer editing intent, leading to notable gains in\ngeneration quality and overall editing performance.",
      "upvotes": 10,
      "discussionId": "683fb8bd3bcb592f18f5b89f",
      "ai_summary": "RelationAdapter, a lightweight module, enhances Diffusion Transformer models to capture and apply visual transformations effectively using source-target image pairs, improving editing performance and generalization on diverse tasks.",
      "ai_keywords": [
        "RelationAdapter",
        "Diffusion Transformer",
        "DiT",
        "visual prompt-based image editing",
        "content-aware editing intent",
        "Relation252K",
        "visual transformations",
        "editing intent",
        "generation quality",
        "overall editing performance"
      ]
    },
    "publishedAt": "2025-06-03T03:06:35.000Z",
    "title": "RelationAdapter: Learning and Transferring Visual Relation with\n  Diffusion Transformers",
    "summary": "Inspired by the in-context learning mechanism of large language models\n(LLMs), a new paradigm of generalizable visual prompt-based image editing is\nemerging. Existing single-reference methods typically focus on style or\nappearance adjustments and struggle with non-rigid transformations. To address\nthese limitations, we propose leveraging source-target image pairs to extract\nand transfer content-aware editing intent to novel query images. To this end,\nwe introduce RelationAdapter, a lightweight module that enables Diffusion\nTransformer (DiT) based models to effectively capture and apply visual\ntransformations from minimal examples. We also introduce Relation252K, a\ncomprehensive dataset comprising 218 diverse editing tasks, to evaluate model\ngeneralization and adaptability in visual prompt-driven scenarios. Experiments\non Relation252K show that RelationAdapter significantly improves the model's\nability to understand and transfer editing intent, leading to notable gains in\ngeneration quality and overall editing performance.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02528.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64311a95034ecbefddd141ef",
      "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
      "fullname": "Yiren Song",
      "name": "yiren98",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 21
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.03126",
      "authors": [
        {
          "_id": "683fb3719f37285365b080c9",
          "name": "Lu Qiu",
          "hidden": false
        },
        {
          "_id": "683fb3719f37285365b080ca",
          "name": "Yizhuo Li",
          "hidden": false
        },
        {
          "_id": "683fb3719f37285365b080cb",
          "name": "Yuying Ge",
          "hidden": false
        },
        {
          "_id": "683fb3719f37285365b080cc",
          "name": "Yixiao Ge",
          "hidden": false
        },
        {
          "_id": "683fb3719f37285365b080cd",
          "name": "Ying Shan",
          "hidden": false
        },
        {
          "_id": "683fb3719f37285365b080ce",
          "name": "Xihui Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T17:55:18.000Z",
      "submittedOnDailyAt": "2025-06-04T03:58:52.500Z",
      "title": "AnimeShooter: A Multi-Shot Animation Dataset for Reference-Guided Video\n  Generation",
      "submittedOnDailyBy": {
        "_id": "630b094f8b327c7b8b94d24c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/630b094f8b327c7b8b94d24c/Fd0ugLOHJZIi8oUZScOmX.jpeg",
        "isPro": false,
        "fullname": "Yizhuo Li",
        "user": "liyz",
        "type": "user"
      },
      "summary": "Recent advances in AI-generated content (AIGC) have significantly accelerated\nanimation production. To produce engaging animations, it is essential to\ngenerate coherent multi-shot video clips with narrative scripts and character\nreferences. However, existing public datasets primarily focus on real-world\nscenarios with global descriptions, and lack reference images for consistent\ncharacter guidance. To bridge this gap, we present AnimeShooter, a\nreference-guided multi-shot animation dataset. AnimeShooter features\ncomprehensive hierarchical annotations and strong visual consistency across\nshots through an automated pipeline. Story-level annotations provide an\noverview of the narrative, including the storyline, key scenes, and main\ncharacter profiles with reference images, while shot-level annotations\ndecompose the story into consecutive shots, each annotated with scene,\ncharacters, and both narrative and descriptive visual captions. Additionally, a\ndedicated subset, AnimeShooter-audio, offers synchronized audio tracks for each\nshot, along with audio descriptions and sound sources. To demonstrate the\neffectiveness of AnimeShooter and establish a baseline for the reference-guided\nmulti-shot video generation task, we introduce AnimeShooterGen, which leverages\nMultimodal Large Language Models (MLLMs) and video diffusion models. The\nreference image and previously generated shots are first processed by MLLM to\nproduce representations aware of both reference and context, which are then\nused as the condition for the diffusion model to decode the subsequent shot.\nExperimental results show that the model trained on AnimeShooter achieves\nsuperior cross-shot visual consistency and adherence to reference visual\nguidance, which highlight the value of our dataset for coherent animated video\ngeneration.",
      "upvotes": 8,
      "discussionId": "683fb3749f37285365b08167",
      "projectPage": "https://qiulu66.github.io/animeshooter",
      "githubRepo": "https://github.com/qiulu66/Anime-Shooter",
      "ai_summary": "AnimeShooter, a reference-guided multi-shot animation dataset, enhances coherent animated video generation by incorporating comprehensive hierarchical annotations and visual consistency, and AnimeShooterGen leverages MLLMs and video diffusion models to achieve superior results.",
      "ai_keywords": [
        "AnimeShooter",
        "reference-guided",
        "multimodal large language models (MLLMs)",
        "video diffusion models",
        "hierarchical annotations",
        "visual consistency",
        "cross-shot visual consistency",
        "reference visual guidance"
      ]
    },
    "publishedAt": "2025-06-03T13:55:18.000Z",
    "title": "AnimeShooter: A Multi-Shot Animation Dataset for Reference-Guided Video\n  Generation",
    "summary": "Recent advances in AI-generated content (AIGC) have significantly accelerated\nanimation production. To produce engaging animations, it is essential to\ngenerate coherent multi-shot video clips with narrative scripts and character\nreferences. However, existing public datasets primarily focus on real-world\nscenarios with global descriptions, and lack reference images for consistent\ncharacter guidance. To bridge this gap, we present AnimeShooter, a\nreference-guided multi-shot animation dataset. AnimeShooter features\ncomprehensive hierarchical annotations and strong visual consistency across\nshots through an automated pipeline. Story-level annotations provide an\noverview of the narrative, including the storyline, key scenes, and main\ncharacter profiles with reference images, while shot-level annotations\ndecompose the story into consecutive shots, each annotated with scene,\ncharacters, and both narrative and descriptive visual captions. Additionally, a\ndedicated subset, AnimeShooter-audio, offers synchronized audio tracks for each\nshot, along with audio descriptions and sound sources. To demonstrate the\neffectiveness of AnimeShooter and establish a baseline for the reference-guided\nmulti-shot video generation task, we introduce AnimeShooterGen, which leverages\nMultimodal Large Language Models (MLLMs) and video diffusion models. The\nreference image and previously generated shots are first processed by MLLM to\nproduce representations aware of both reference and context, which are then\nused as the condition for the diffusion model to decode the subsequent shot.\nExperimental results show that the model trained on AnimeShooter achieves\nsuperior cross-shot visual consistency and adherence to reference visual\nguidance, which highlight the value of our dataset for coherent animated video\ngeneration.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03126.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "630b094f8b327c7b8b94d24c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/630b094f8b327c7b8b94d24c/Fd0ugLOHJZIi8oUZScOmX.jpeg",
      "fullname": "Yizhuo Li",
      "name": "liyz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.00910",
      "authors": [
        {
          "_id": "683fcd9956d0f1cfb34f1d51",
          "name": "Seongjae Kang",
          "hidden": false
        },
        {
          "_id": "683fcd9956d0f1cfb34f1d52",
          "name": "Dong Bok Lee",
          "hidden": false
        },
        {
          "_id": "683fcd9956d0f1cfb34f1d53",
          "name": "Hyungjoon Jang",
          "hidden": false
        },
        {
          "_id": "683fcd9956d0f1cfb34f1d54",
          "name": "Dongseop Kim",
          "hidden": false
        },
        {
          "_id": "683fcd9956d0f1cfb34f1d55",
          "name": "Sung Ju Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-01T08:54:37.000Z",
      "submittedOnDailyAt": "2025-06-04T03:10:05.743Z",
      "title": "PCoreSet: Effective Active Learning through Knowledge Distillation from\n  Vision-Language Models",
      "submittedOnDailyBy": {
        "_id": "6357a08f8ed056fa1ccd3b38",
        "avatarUrl": "/avatars/07d4ca8f3197a6945ad71e6150801135.svg",
        "isPro": false,
        "fullname": "erjui",
        "user": "erjui",
        "type": "user"
      },
      "summary": "Knowledge distillation (KD) is a widely used framework for training compact,\ntask-specific models by leveraging the knowledge of teacher models. However,\nits application to active learning (AL), which aims to minimize annotation\ncosts through iterative sample selection, remains underexplored. This gap stems\nfrom the fact that KD typically assumes access to sufficient labeled data,\nwhereas AL operates in data-scarce scenarios where task-specific teacher models\nare often unavailable. In this paper, we introduce ActiveKD, a framework that\nintegrates AL with KD by leveraging the zero- and few-shot capabilities of\nlarge vision-language models (VLMs). A key aspect of ActiveKD is the structured\nprediction bias of VLMs -- i.e., their predictions form clusters in the\nprobability space. We regard this structure as an inductive bias of the teacher\nmodel, capturing generalizable output patterns beneficial to student learning.\nTo exploit this bias, we propose Probabilistic CoreSet (PCoreSet), a selection\nstrategy that maximizes coverage in the probability space rather than the\nfeature space. PCoreSet strategically selects categorically diverse unlabeled\nsamples, facilitating more efficient transfer of teacher knowledge under\nlimited annotation budgets. Evaluations on 11 datasets show that PCoreSet\nconsistently outperforms existing selection methods within the ActiveKD\nframework, advancing research at the intersection of AL and KD.",
      "upvotes": 8,
      "discussionId": "683fcd9d56d0f1cfb34f1eb1",
      "githubRepo": "https://github.com/erjui/PCoreSet",
      "ai_summary": "ActiveKD integrates active learning with knowledge distillation using large vision-language models to efficiently select diverse, unlabeled samples for annotation.",
      "ai_keywords": [
        "knowledge distillation",
        "active learning",
        "task-specific models",
        "teacher models",
        "zero-shot",
        "few-shot",
        "large vision-language models",
        "structured prediction bias",
        "inductive bias",
        "Probabilistic CoreSet",
        "PCoreSet",
        "probability space",
        "categorically diverse samples"
      ]
    },
    "publishedAt": "2025-06-01T04:54:37.000Z",
    "title": "PCoreSet: Effective Active Learning through Knowledge Distillation from\n  Vision-Language Models",
    "summary": "Knowledge distillation (KD) is a widely used framework for training compact,\ntask-specific models by leveraging the knowledge of teacher models. However,\nits application to active learning (AL), which aims to minimize annotation\ncosts through iterative sample selection, remains underexplored. This gap stems\nfrom the fact that KD typically assumes access to sufficient labeled data,\nwhereas AL operates in data-scarce scenarios where task-specific teacher models\nare often unavailable. In this paper, we introduce ActiveKD, a framework that\nintegrates AL with KD by leveraging the zero- and few-shot capabilities of\nlarge vision-language models (VLMs). A key aspect of ActiveKD is the structured\nprediction bias of VLMs -- i.e., their predictions form clusters in the\nprobability space. We regard this structure as an inductive bias of the teacher\nmodel, capturing generalizable output patterns beneficial to student learning.\nTo exploit this bias, we propose Probabilistic CoreSet (PCoreSet), a selection\nstrategy that maximizes coverage in the probability space rather than the\nfeature space. PCoreSet strategically selects categorically diverse unlabeled\nsamples, facilitating more efficient transfer of teacher knowledge under\nlimited annotation budgets. Evaluations on 11 datasets show that PCoreSet\nconsistently outperforms existing selection methods within the ActiveKD\nframework, advancing research at the intersection of AL and KD.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00910.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6357a08f8ed056fa1ccd3b38",
      "avatarUrl": "/avatars/07d4ca8f3197a6945ad71e6150801135.svg",
      "fullname": "erjui",
      "name": "erjui",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.01144",
      "authors": [
        {
          "_id": "683fca92c1e51fea3a470e93",
          "name": "Ariel Shaulov",
          "hidden": false
        },
        {
          "_id": "683fca92c1e51fea3a470e94",
          "user": {
            "_id": "64972b8d27e41e26a32835d4",
            "avatarUrl": "/avatars/00c76991b5421f592d632a750ec8b998.svg",
            "isPro": false,
            "fullname": "Itay Hazan",
            "user": "itayhzn",
            "type": "user"
          },
          "name": "Itay Hazan",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-06-04T06:31:03.556Z",
          "hidden": false
        },
        {
          "_id": "683fca92c1e51fea3a470e95",
          "name": "Lior Wolf",
          "hidden": false
        },
        {
          "_id": "683fca92c1e51fea3a470e96",
          "name": "Hila Chefer",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6181c72cdcc1df2c9de8a4d8/z3VoHZmxOtL3agHCaWWg4.mp4"
      ],
      "publishedAt": "2025-06-01T19:55:33.000Z",
      "submittedOnDailyAt": "2025-06-04T02:58:51.483Z",
      "title": "FlowMo: Variance-Based Flow Guidance for Coherent Motion in Video\n  Generation",
      "submittedOnDailyBy": {
        "_id": "6181c72cdcc1df2c9de8a4d8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1655248010394-6181c72cdcc1df2c9de8a4d8.jpeg",
        "isPro": false,
        "fullname": "Hila Chefer",
        "user": "Hila",
        "type": "user"
      },
      "summary": "Text-to-video diffusion models are notoriously limited in their ability to\nmodel temporal aspects such as motion, physics, and dynamic interactions.\nExisting approaches address this limitation by retraining the model or\nintroducing external conditioning signals to enforce temporal consistency. In\nthis work, we explore whether a meaningful temporal representation can be\nextracted directly from the predictions of a pre-trained model without any\nadditional training or auxiliary inputs. We introduce FlowMo, a novel\ntraining-free guidance method that enhances motion coherence using only the\nmodel's own predictions in each diffusion step. FlowMo first derives an\nappearance-debiased temporal representation by measuring the distance between\nlatents corresponding to consecutive frames. This highlights the implicit\ntemporal structure predicted by the model. It then estimates motion coherence\nby measuring the patch-wise variance across the temporal dimension and guides\nthe model to reduce this variance dynamically during sampling. Extensive\nexperiments across multiple text-to-video models demonstrate that FlowMo\nsignificantly improves motion coherence without sacrificing visual quality or\nprompt alignment, offering an effective plug-and-play solution for enhancing\nthe temporal fidelity of pre-trained video diffusion models.",
      "upvotes": 6,
      "discussionId": "683fca94c1e51fea3a470eee",
      "projectPage": "https://arielshaulov.github.io/FlowMo/",
      "githubRepo": "https://github.com/arielshaulov/FlowMo",
      "ai_summary": "FlowMo, a training-free method, enhances motion coherence in pre-trained text-to-video diffusion models by leveraging their own predictions to reduce patch-wise temporal variance.",
      "ai_keywords": [
        "text-to-video diffusion models",
        "temporal aspects",
        "motion",
        "physics",
        "dynamic interactions",
        "pre-trained model",
        "FlowMo",
        "guidance method",
        "appearance-debiased",
        "temporal representation",
        "latents",
        "patch-wise variance",
        "sampling",
        "temporal fidelity"
      ]
    },
    "publishedAt": "2025-06-01T15:55:33.000Z",
    "title": "FlowMo: Variance-Based Flow Guidance for Coherent Motion in Video\n  Generation",
    "summary": "Text-to-video diffusion models are notoriously limited in their ability to\nmodel temporal aspects such as motion, physics, and dynamic interactions.\nExisting approaches address this limitation by retraining the model or\nintroducing external conditioning signals to enforce temporal consistency. In\nthis work, we explore whether a meaningful temporal representation can be\nextracted directly from the predictions of a pre-trained model without any\nadditional training or auxiliary inputs. We introduce FlowMo, a novel\ntraining-free guidance method that enhances motion coherence using only the\nmodel's own predictions in each diffusion step. FlowMo first derives an\nappearance-debiased temporal representation by measuring the distance between\nlatents corresponding to consecutive frames. This highlights the implicit\ntemporal structure predicted by the model. It then estimates motion coherence\nby measuring the patch-wise variance across the temporal dimension and guides\nthe model to reduce this variance dynamically during sampling. Extensive\nexperiments across multiple text-to-video models demonstrate that FlowMo\nsignificantly improves motion coherence without sacrificing visual quality or\nprompt alignment, offering an effective plug-and-play solution for enhancing\nthe temporal fidelity of pre-trained video diffusion models.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6181c72cdcc1df2c9de8a4d8/z3VoHZmxOtL3agHCaWWg4.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01144.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6181c72cdcc1df2c9de8a4d8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1655248010394-6181c72cdcc1df2c9de8a4d8.jpeg",
      "fullname": "Hila Chefer",
      "name": "Hila",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.02497",
      "authors": [
        {
          "_id": "683fbc32917306517315589f",
          "name": "Jiahao Chen",
          "hidden": false
        },
        {
          "_id": "683fbc3291730651731558a0",
          "name": "Hangjie Yuan",
          "hidden": false
        },
        {
          "_id": "683fbc3291730651731558a1",
          "name": "Yichen Qian",
          "hidden": false
        },
        {
          "_id": "683fbc3291730651731558a2",
          "name": "Jingyun Liang",
          "hidden": false
        },
        {
          "_id": "683fbc3291730651731558a3",
          "name": "Jiazheng Xing",
          "hidden": false
        },
        {
          "_id": "683fbc3291730651731558a4",
          "name": "Pengwei Liu",
          "hidden": false
        },
        {
          "_id": "683fbc3291730651731558a5",
          "name": "Weihua Chen",
          "hidden": false
        },
        {
          "_id": "683fbc3291730651731558a6",
          "name": "Fan Wang",
          "hidden": false
        },
        {
          "_id": "683fbc3291730651731558a7",
          "name": "Bing Su",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T06:25:00.000Z",
      "submittedOnDailyAt": "2025-06-04T01:55:34.274Z",
      "title": "LumosFlow: Motion-Guided Long Video Generation",
      "submittedOnDailyBy": {
        "_id": "649d54b314afbb10ce2a9eeb",
        "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
        "isPro": false,
        "fullname": "Hangjie Yuan",
        "user": "JacobYuan",
        "type": "user"
      },
      "summary": "Long video generation has gained increasing attention due to its widespread\napplications in fields such as entertainment and simulation. Despite advances,\nsynthesizing temporally coherent and visually compelling long sequences remains\na formidable challenge. Conventional approaches often synthesize long videos by\nsequentially generating and concatenating short clips, or generating key frames\nand then interpolate the intermediate frames in a hierarchical manner. However,\nboth of them still remain significant challenges, leading to issues such as\ntemporal repetition or unnatural transitions. In this paper, we revisit the\nhierarchical long video generation pipeline and introduce LumosFlow, a\nframework introduce motion guidance explicitly. Specifically, we first employ\nthe Large Motion Text-to-Video Diffusion Model (LMTV-DM) to generate key frames\nwith larger motion intervals, thereby ensuring content diversity in the\ngenerated long videos. Given the complexity of interpolating contextual\ntransitions between key frames, we further decompose the intermediate frame\ninterpolation into motion generation and post-hoc refinement. For each pair of\nkey frames, the Latent Optical Flow Diffusion Model (LOF-DM) synthesizes\ncomplex and large-motion optical flows, while MotionControlNet subsequently\nrefines the warped results to enhance quality and guide intermediate frame\ngeneration. Compared with traditional video frame interpolation, we achieve 15x\ninterpolation, ensuring reasonable and continuous motion between adjacent\nframes. Experiments show that our method can generate long videos with\nconsistent motion and appearance. Code and models will be made publicly\navailable upon acceptance. Our project page:\nhttps://jiahaochen1.github.io/LumosFlow/",
      "upvotes": 4,
      "discussionId": "683fbc36917306517315597d",
      "projectPage": "https://jiahaochen1.github.io/LumosFlow/",
      "ai_summary": "LumosFlow uses LMTV-DM for key frame generation and LOF-DM followed by MotionControlNet for smooth intermediate frame interpolation, ensuring temporally coherent long video generation.",
      "ai_keywords": [
        "Large Motion Text-to-Video Diffusion Model",
        "LMTV-DM",
        "Latent Optical Flow Diffusion Model",
        "LOF-DM",
        "MotionControlNet",
        "optical flows",
        "frame interpolation",
        "key frames",
        "long video generation",
        "motion guidance",
        "synthetic long videos"
      ]
    },
    "publishedAt": "2025-06-03T02:25:00.000Z",
    "title": "LumosFlow: Motion-Guided Long Video Generation",
    "summary": "Long video generation has gained increasing attention due to its widespread\napplications in fields such as entertainment and simulation. Despite advances,\nsynthesizing temporally coherent and visually compelling long sequences remains\na formidable challenge. Conventional approaches often synthesize long videos by\nsequentially generating and concatenating short clips, or generating key frames\nand then interpolate the intermediate frames in a hierarchical manner. However,\nboth of them still remain significant challenges, leading to issues such as\ntemporal repetition or unnatural transitions. In this paper, we revisit the\nhierarchical long video generation pipeline and introduce LumosFlow, a\nframework introduce motion guidance explicitly. Specifically, we first employ\nthe Large Motion Text-to-Video Diffusion Model (LMTV-DM) to generate key frames\nwith larger motion intervals, thereby ensuring content diversity in the\ngenerated long videos. Given the complexity of interpolating contextual\ntransitions between key frames, we further decompose the intermediate frame\ninterpolation into motion generation and post-hoc refinement. For each pair of\nkey frames, the Latent Optical Flow Diffusion Model (LOF-DM) synthesizes\ncomplex and large-motion optical flows, while MotionControlNet subsequently\nrefines the warped results to enhance quality and guide intermediate frame\ngeneration. Compared with traditional video frame interpolation, we achieve 15x\ninterpolation, ensuring reasonable and continuous motion between adjacent\nframes. Experiments show that our method can generate long videos with\nconsistent motion and appearance. Code and models will be made publicly\navailable upon acceptance. Our project page:\nhttps://jiahaochen1.github.io/LumosFlow/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02497.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649d54b314afbb10ce2a9eeb",
      "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
      "fullname": "Hangjie Yuan",
      "name": "JacobYuan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.01789",
      "authors": [
        {
          "_id": "683fb99c7c8d720be438000a",
          "name": "Genta Indra Winata",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be438000b",
          "name": "David Anugraha",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be438000c",
          "name": "Emmy Liu",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be438000d",
          "name": "Alham Fikri Aji",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be438000e",
          "name": "Shou-Yi Hung",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be438000f",
          "name": "Aditya Parashar",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be4380010",
          "name": "Patrick Amadeus Irawan",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be4380011",
          "name": "Ruochen Zhang",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be4380012",
          "name": "Zheng-Xin Yong",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be4380013",
          "name": "Jan Christian Blaise Cruz",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be4380014",
          "name": "Niklas Muennighoff",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be4380015",
          "name": "Seungone Kim",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be4380016",
          "name": "Hanyang Zhao",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be4380017",
          "name": "Sudipta Kar",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be4380018",
          "name": "Kezia Erina Suryoraharjo",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be4380019",
          "name": "M. Farid Adilazuarda",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be438001a",
          "name": "En-Shiun Annie Lee",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be438001b",
          "name": "Ayu Purwarianti",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be438001c",
          "name": "Derry Tanti Wijaya",
          "hidden": false
        },
        {
          "_id": "683fb99c7c8d720be438001d",
          "name": "Monojit Choudhury",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-02T15:31:52.000Z",
      "submittedOnDailyAt": "2025-06-04T01:42:44.251Z",
      "title": "Datasheets Aren't Enough: DataRubrics for Automated Quality Metrics and\n  Accountability",
      "submittedOnDailyBy": {
        "_id": "5f5c4b20e56d546cd6233098",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1637813888895-5f5c4b20e56d546cd6233098.jpeg",
        "isPro": false,
        "fullname": "Genta Indra Winata",
        "user": "gentaiscool",
        "type": "user"
      },
      "summary": "High-quality datasets are fundamental to training and evaluating machine\nlearning models, yet their creation-especially with accurate human\nannotations-remains a significant challenge. Many dataset paper submissions\nlack originality, diversity, or rigorous quality control, and these\nshortcomings are often overlooked during peer review. Submissions also\nfrequently omit essential details about dataset construction and properties.\nWhile existing tools such as datasheets aim to promote transparency, they are\nlargely descriptive and do not provide standardized, measurable methods for\nevaluating data quality. Similarly, metadata requirements at conferences\npromote accountability but are inconsistently enforced. To address these\nlimitations, this position paper advocates for the integration of systematic,\nrubric-based evaluation metrics into the dataset review process-particularly as\nsubmission volumes continue to grow. We also explore scalable, cost-effective\nmethods for synthetic data generation, including dedicated tools and\nLLM-as-a-judge approaches, to support more efficient evaluation. As a call to\naction, we introduce DataRubrics, a structured framework for assessing the\nquality of both human- and model-generated datasets. Leveraging recent advances\nin LLM-based evaluation, DataRubrics offers a reproducible, scalable, and\nactionable solution for dataset quality assessment, enabling both authors and\nreviewers to uphold higher standards in data-centric research. We also release\ncode to support reproducibility of LLM-based evaluations at\nhttps://github.com/datarubrics/datarubrics.",
      "upvotes": 3,
      "discussionId": "683fb99e7c8d720be4380090"
    },
    "publishedAt": "2025-06-02T11:31:52.000Z",
    "title": "Datasheets Aren't Enough: DataRubrics for Automated Quality Metrics and\n  Accountability",
    "summary": "High-quality datasets are fundamental to training and evaluating machine\nlearning models, yet their creation-especially with accurate human\nannotations-remains a significant challenge. Many dataset paper submissions\nlack originality, diversity, or rigorous quality control, and these\nshortcomings are often overlooked during peer review. Submissions also\nfrequently omit essential details about dataset construction and properties.\nWhile existing tools such as datasheets aim to promote transparency, they are\nlargely descriptive and do not provide standardized, measurable methods for\nevaluating data quality. Similarly, metadata requirements at conferences\npromote accountability but are inconsistently enforced. To address these\nlimitations, this position paper advocates for the integration of systematic,\nrubric-based evaluation metrics into the dataset review process-particularly as\nsubmission volumes continue to grow. We also explore scalable, cost-effective\nmethods for synthetic data generation, including dedicated tools and\nLLM-as-a-judge approaches, to support more efficient evaluation. As a call to\naction, we introduce DataRubrics, a structured framework for assessing the\nquality of both human- and model-generated datasets. Leveraging recent advances\nin LLM-based evaluation, DataRubrics offers a reproducible, scalable, and\nactionable solution for dataset quality assessment, enabling both authors and\nreviewers to uphold higher standards in data-centric research. We also release\ncode to support reproducibility of LLM-based evaluations at\nhttps://github.com/datarubrics/datarubrics.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01789.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f5c4b20e56d546cd6233098",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1637813888895-5f5c4b20e56d546cd6233098.jpeg",
      "fullname": "Genta Indra Winata",
      "name": "gentaiscool",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 17
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.01274",
      "authors": [
        {
          "_id": "683fe4caf8916dcd6d1c936a",
          "name": "Hosu Lee",
          "hidden": false
        },
        {
          "_id": "683fe4caf8916dcd6d1c936b",
          "name": "Junho Kim",
          "hidden": false
        },
        {
          "_id": "683fe4caf8916dcd6d1c936c",
          "name": "Hyunjun Kim",
          "hidden": false
        },
        {
          "_id": "683fe4caf8916dcd6d1c936d",
          "name": "Yong Man Ro",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-02T03:08:07.000Z",
      "submittedOnDailyAt": "2025-06-04T04:50:22.651Z",
      "title": "ReFoCUS: Reinforcement-guided Frame Optimization for Contextual\n  Understanding",
      "submittedOnDailyBy": {
        "_id": "653238bed0f5a9e537ed966d",
        "avatarUrl": "/avatars/e97a83e68e770baa1c5df847777cf213.svg",
        "isPro": false,
        "fullname": "Junho Kim",
        "user": "arkimjh",
        "type": "user"
      },
      "summary": "Recent progress in Large Multi-modal Models (LMMs) has enabled effective\nvision-language reasoning, yet the ability to understand video content remains\nconstrained by suboptimal frame selection strategies. Existing approaches often\nrely on static heuristics or external retrieval modules to feed frame\ninformation into video-LLMs, which may fail to provide the query-relevant\ninformation. In this work, we introduce ReFoCUS (Reinforcement-guided Frame\nOptimization for Contextual UnderStanding), a novel frame-level policy\noptimization framework that shifts the optimization target from textual\nresponses to visual input selection. ReFoCUS learns a frame selection policy\nvia reinforcement learning, using reward signals derived from a reference LMM\nto reflect the model's intrinsic preferences for frames that best support\ntemporally grounded responses. To efficiently explore the large combinatorial\nframe space, we employ an autoregressive, conditional selection architecture\nthat ensures temporal coherence while reducing complexity. Our approach does\nnot require explicit supervision at the frame-level and consistently improves\nreasoning performance across multiple video QA benchmarks, highlighting the\nbenefits of aligning frame selection with model-internal utility.",
      "upvotes": 3,
      "discussionId": "683fe4ccf8916dcd6d1c93b6",
      "ai_summary": "ReFoCUS uses reinforcement learning to optimize frame selection for video-LLM, enhancing reasoning performance in video QA by aligning with model preferences.",
      "ai_keywords": [
        "Large Multi-modal Models",
        "vision-language reasoning",
        "frame selection strategies",
        "reinforcement learning",
        "frame selection policy",
        "reference LMM",
        "autoregressive architecture",
        "conditional selection architecture",
        "temporal coherence",
        "video QA benchmarks"
      ]
    },
    "publishedAt": "2025-06-01T23:08:07.000Z",
    "title": "ReFoCUS: Reinforcement-guided Frame Optimization for Contextual\n  Understanding",
    "summary": "Recent progress in Large Multi-modal Models (LMMs) has enabled effective\nvision-language reasoning, yet the ability to understand video content remains\nconstrained by suboptimal frame selection strategies. Existing approaches often\nrely on static heuristics or external retrieval modules to feed frame\ninformation into video-LLMs, which may fail to provide the query-relevant\ninformation. In this work, we introduce ReFoCUS (Reinforcement-guided Frame\nOptimization for Contextual UnderStanding), a novel frame-level policy\noptimization framework that shifts the optimization target from textual\nresponses to visual input selection. ReFoCUS learns a frame selection policy\nvia reinforcement learning, using reward signals derived from a reference LMM\nto reflect the model's intrinsic preferences for frames that best support\ntemporally grounded responses. To efficiently explore the large combinatorial\nframe space, we employ an autoregressive, conditional selection architecture\nthat ensures temporal coherence while reducing complexity. Our approach does\nnot require explicit supervision at the frame-level and consistently improves\nreasoning performance across multiple video QA benchmarks, highlighting the\nbenefits of aligning frame selection with model-internal utility.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01274.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "653238bed0f5a9e537ed966d",
      "avatarUrl": "/avatars/e97a83e68e770baa1c5df847777cf213.svg",
      "fullname": "Junho Kim",
      "name": "arkimjh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.02338",
      "authors": [
        {
          "_id": "683fbc730ffa93c1611d513b",
          "name": "Hyungjoo Chae",
          "hidden": false
        },
        {
          "_id": "683fbc730ffa93c1611d513c",
          "name": "Dongjin Kang",
          "hidden": false
        },
        {
          "_id": "683fbc730ffa93c1611d513d",
          "name": "Jihyuk Kim",
          "hidden": false
        },
        {
          "_id": "683fbc730ffa93c1611d513e",
          "name": "Beong-woo Kwak",
          "hidden": false
        },
        {
          "_id": "683fbc730ffa93c1611d513f",
          "name": "Sunghyun Park",
          "hidden": false
        },
        {
          "_id": "683fbc730ffa93c1611d5140",
          "name": "Haeju Park",
          "hidden": false
        },
        {
          "_id": "683fbc730ffa93c1611d5141",
          "name": "Jinyoung Yeo",
          "hidden": false
        },
        {
          "_id": "683fbc730ffa93c1611d5142",
          "name": "Moontae Lee",
          "hidden": false
        },
        {
          "_id": "683fbc730ffa93c1611d5143",
          "name": "Kyungjae Lee",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T00:29:15.000Z",
      "submittedOnDailyAt": "2025-06-04T01:55:22.453Z",
      "title": "One Missing Piece for Open-Source Reasoning Models: A Dataset to\n  Mitigate Cold-Starting Short CoT LLMs in RL",
      "submittedOnDailyBy": {
        "_id": "64c8f4cec547ed5243ebd0a8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c8f4cec547ed5243ebd0a8/MiOH5YbMg8Gh9KYlQsLmX.jpeg",
        "isPro": false,
        "fullname": "Hyungjoo Chae",
        "user": "hyungjoochae",
        "type": "user"
      },
      "summary": "With the release of R1, a publicly available large reasoning model (LRM),\nresearchers commonly train new LRMs by training language models on R1's long\nchain-of-thought (CoT) inferences. While prior works show that LRMs'\ncapabilities can be reproduced through direct distillation, the continued\nreliance on the existing models (e.g., R1) remains a critical limitation in\nadvancing the field. As a first step toward independent LRM development, this\npaper explores the possibility of constructing a long CoT dataset with LLMs\nthat are not trained for inference-time scaling. To this end, we present the\nLong CoT Collection, a dataset of 100K CoT rationales annotated using existing\nshort CoT LLMs. We develop a pipeline that induces o1's novel reasoning\nstrategies into short CoT LLMs, enabling them to think longer and introducing\ncontrollability over the thought budget to better manage the overthinking\nproblem. Our extensive analyses validate that our dataset achieves quality\ncomparable to--or slightly below--R1. Furthermore, our experiments demonstrate\nthat training on our dataset not only strengthens general reasoning skills, but\nalso provides a strong foundation for reinforcement learning--models\ninitialized on our data achieve 2-3x larger gains with RLVR.",
      "upvotes": 2,
      "discussionId": "683fbc760ffa93c1611d51cc",
      "ai_summary": "The Long CoT Collection dataset, generated by short CoT LLMs, enhances general reasoning skills and provides a strong foundation for reinforcement learning, achieving quality comparable to R1.",
      "ai_keywords": [
        "long chain-of-thought",
        "CoT inferences",
        "LRMs",
        "direct distillation",
        "inference-time scaling",
        "CoT rationales",
        "short CoT LLMs",
        "reasoning strategies",
        "thought budget",
        "overthinking",
        "reinforcement learning",
        "RLVR"
      ]
    },
    "publishedAt": "2025-06-02T20:29:15.000Z",
    "title": "One Missing Piece for Open-Source Reasoning Models: A Dataset to\n  Mitigate Cold-Starting Short CoT LLMs in RL",
    "summary": "With the release of R1, a publicly available large reasoning model (LRM),\nresearchers commonly train new LRMs by training language models on R1's long\nchain-of-thought (CoT) inferences. While prior works show that LRMs'\ncapabilities can be reproduced through direct distillation, the continued\nreliance on the existing models (e.g., R1) remains a critical limitation in\nadvancing the field. As a first step toward independent LRM development, this\npaper explores the possibility of constructing a long CoT dataset with LLMs\nthat are not trained for inference-time scaling. To this end, we present the\nLong CoT Collection, a dataset of 100K CoT rationales annotated using existing\nshort CoT LLMs. We develop a pipeline that induces o1's novel reasoning\nstrategies into short CoT LLMs, enabling them to think longer and introducing\ncontrollability over the thought budget to better manage the overthinking\nproblem. Our extensive analyses validate that our dataset achieves quality\ncomparable to--or slightly below--R1. Furthermore, our experiments demonstrate\nthat training on our dataset not only strengthens general reasoning skills, but\nalso provides a strong foundation for reinforcement learning--models\ninitialized on our data achieve 2-3x larger gains with RLVR.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02338.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c8f4cec547ed5243ebd0a8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c8f4cec547ed5243ebd0a8/MiOH5YbMg8Gh9KYlQsLmX.jpeg",
      "fullname": "Hyungjoo Chae",
      "name": "hyungjoochae",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.00413",
      "authors": [
        {
          "_id": "683e703c33ac56778c2e51cd",
          "user": {
            "_id": "630139f1f6bea7dd15bdaf4e",
            "avatarUrl": "/avatars/263536f6160f8c522d2a76ca2c4e4cc0.svg",
            "isPro": false,
            "fullname": "Daniel Israel",
            "user": "danielmisrael",
            "type": "user"
          },
          "name": "Daniel Israel",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:45:22.988Z",
          "hidden": false
        },
        {
          "_id": "683e703c33ac56778c2e51ce",
          "name": "Guy Van den Broeck",
          "hidden": false
        },
        {
          "_id": "683e703c33ac56778c2e51cf",
          "name": "Aditya Grover",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-31T06:10:10.000Z",
      "submittedOnDailyAt": "2025-06-04T03:04:01.663Z",
      "title": "Accelerating Diffusion LLMs via Adaptive Parallel Decoding",
      "submittedOnDailyBy": {
        "_id": "630139f1f6bea7dd15bdaf4e",
        "avatarUrl": "/avatars/263536f6160f8c522d2a76ca2c4e4cc0.svg",
        "isPro": false,
        "fullname": "Daniel Israel",
        "user": "danielmisrael",
        "type": "user"
      },
      "summary": "The generation speed of LLMs are bottlenecked by autoregressive decoding,\nwhere tokens are predicted sequentially one by one. Alternatively, diffusion\nlarge language models (dLLMs) theoretically allow for parallel token\ngeneration, but in practice struggle to achieve the speed of autoregressive\nmodels without significantly sacrificing quality. We therefore introduce\nadaptive parallel decoding (APD), a novel method that dynamically adjusts the\nnumber of tokens sampled in parallel. We achieve this by defining a\nmultiplicative mixture between the dLLM marginal probabilities and the joint\nprobability of sequences under a small auxiliary autoregressive model. This\ninverts the standard setup of speculative decoding, where the goal is to sample\nfrom a large autoregressive verifier by drafting from a smaller model. We\nfurther optimize APD by enabling KV caching and limiting the size of the masked\ninput. Altogether, our method puts forward three tunable parameters to flexibly\ntradeoff throughput and quality. We show that APD provides markedly higher\nthroughput with minimal quality degradations on downstream benchmarks.",
      "upvotes": 2,
      "discussionId": "683e703d33ac56778c2e51fe",
      "ai_summary": "Adaptive parallel decoding (APD) enhances the throughput of diffusion large language models (dLLMs) by dynamically adjusting parallel token generation without significantly diminishing quality.",
      "ai_keywords": [
        "autoregressive decoding",
        "diffusion large language models",
        "dLLMs",
        "adaptive parallel decoding",
        "APD",
        "marginal probabilities",
        "joint probability",
        "speculative decoding",
        "KV caching",
        "masked input"
      ]
    },
    "publishedAt": "2025-05-31T02:10:10.000Z",
    "title": "Accelerating Diffusion LLMs via Adaptive Parallel Decoding",
    "summary": "The generation speed of LLMs are bottlenecked by autoregressive decoding,\nwhere tokens are predicted sequentially one by one. Alternatively, diffusion\nlarge language models (dLLMs) theoretically allow for parallel token\ngeneration, but in practice struggle to achieve the speed of autoregressive\nmodels without significantly sacrificing quality. We therefore introduce\nadaptive parallel decoding (APD), a novel method that dynamically adjusts the\nnumber of tokens sampled in parallel. We achieve this by defining a\nmultiplicative mixture between the dLLM marginal probabilities and the joint\nprobability of sequences under a small auxiliary autoregressive model. This\ninverts the standard setup of speculative decoding, where the goal is to sample\nfrom a large autoregressive verifier by drafting from a smaller model. We\nfurther optimize APD by enabling KV caching and limiting the size of the masked\ninput. Altogether, our method puts forward three tunable parameters to flexibly\ntradeoff throughput and quality. We show that APD provides markedly higher\nthroughput with minimal quality degradations on downstream benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00413.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "630139f1f6bea7dd15bdaf4e",
      "avatarUrl": "/avatars/263536f6160f8c522d2a76ca2c4e4cc0.svg",
      "fullname": "Daniel Israel",
      "name": "danielmisrael",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.16994",
      "authors": [
        {
          "_id": "683fb5d3a09aefea70733fa3",
          "user": {
            "_id": "64e14c5b12a5504dda70e60d",
            "avatarUrl": "/avatars/944b486bb037364ef7d9d2c826526708.svg",
            "isPro": false,
            "fullname": "Runyang",
            "user": "dd101bb",
            "type": "user"
          },
          "name": "Runyang You",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-06-04T07:34:53.226Z",
          "hidden": false
        },
        {
          "_id": "683fb5d3a09aefea70733fa4",
          "user": {
            "_id": "674038313ccfb67446ae2b35",
            "avatarUrl": "/avatars/8a3c0fdf971363988731f9eb8b13658c.svg",
            "isPro": false,
            "fullname": "tensorslow",
            "user": "tensorslow",
            "type": "user"
          },
          "name": "Yongqi Li",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-04T02:56:21.008Z",
          "hidden": false
        },
        {
          "_id": "683fb5d3a09aefea70733fa5",
          "name": "Xinyu Lin",
          "hidden": false
        },
        {
          "_id": "683fb5d3a09aefea70733fa6",
          "name": "Xin Zhang",
          "hidden": false
        },
        {
          "_id": "683fb5d3a09aefea70733fa7",
          "name": "Wenjie Wang",
          "hidden": false
        },
        {
          "_id": "683fb5d3a09aefea70733fa8",
          "name": "Wenjie Li",
          "hidden": false
        },
        {
          "_id": "683fb5d3a09aefea70733fa9",
          "name": "Liqiang Nie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T17:55:43.000Z",
      "submittedOnDailyAt": "2025-06-04T01:27:54.567Z",
      "title": "R^2ec: Towards Large Recommender Models with Reasoning",
      "submittedOnDailyBy": {
        "_id": "63b6dbc8ccebeadccc888456",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673396893898-63b6dbc8ccebeadccc888456.jpeg",
        "isPro": false,
        "fullname": "Xin Zhang",
        "user": "izhx",
        "type": "user"
      },
      "summary": "Large recommender models have extended LLMs as powerful recommenders via\nencoding or item generation, and recent breakthroughs in LLM reasoning\nsynchronously motivate the exploration of reasoning in recommendation. Current\nstudies usually position LLMs as external reasoning modules to yield auxiliary\nthought for augmenting conventional recommendation pipelines. However, such\ndecoupled designs are limited in significant resource cost and suboptimal joint\noptimization. To address these issues, we propose \\name, a unified large\nrecommender model with intrinsic reasoning capabilities. Initially, we\nreconceptualize the model architecture to facilitate interleaved reasoning and\nrecommendation in the autoregressive process. Subsequently, we propose RecPO, a\ncorresponding reinforcement learning framework that optimizes \\name\\ both the\nreasoning and recommendation capabilities simultaneously in a single policy\nupdate; RecPO introduces a fused reward scheme that solely leverages\nrecommendation labels to simulate the reasoning capability, eliminating\ndependency on specialized reasoning annotations. Experiments on three datasets\nwith various baselines verify the effectiveness of \\name, showing relative\nimprovements of 68.67\\% in Hit@5 and 45.21\\% in NDCG@20. Code available at\nhttps://github.com/YRYangang/RRec.",
      "upvotes": 2,
      "discussionId": "683fb5d5a09aefea70734001",
      "githubRepo": "https://github.com/YRYangang/RRec",
      "ai_summary": "A unified large recommender model with intrinsic reasoning capabilities is proposed, facilitating interleaved reasoning and recommendation using a reinforcement learning framework called RecPO.",
      "ai_keywords": [
        "recommender models",
        "LLMs",
        "intrinsic reasoning",
        "autoregressive process",
        "reinforcement learning",
        "RecPO",
        "fused reward scheme",
        "Hit@5",
        "NDCG@20"
      ]
    },
    "publishedAt": "2025-05-22T13:55:43.000Z",
    "title": "R^2ec: Towards Large Recommender Models with Reasoning",
    "summary": "Large recommender models have extended LLMs as powerful recommenders via\nencoding or item generation, and recent breakthroughs in LLM reasoning\nsynchronously motivate the exploration of reasoning in recommendation. Current\nstudies usually position LLMs as external reasoning modules to yield auxiliary\nthought for augmenting conventional recommendation pipelines. However, such\ndecoupled designs are limited in significant resource cost and suboptimal joint\noptimization. To address these issues, we propose \\name, a unified large\nrecommender model with intrinsic reasoning capabilities. Initially, we\nreconceptualize the model architecture to facilitate interleaved reasoning and\nrecommendation in the autoregressive process. Subsequently, we propose RecPO, a\ncorresponding reinforcement learning framework that optimizes \\name\\ both the\nreasoning and recommendation capabilities simultaneously in a single policy\nupdate; RecPO introduces a fused reward scheme that solely leverages\nrecommendation labels to simulate the reasoning capability, eliminating\ndependency on specialized reasoning annotations. Experiments on three datasets\nwith various baselines verify the effectiveness of \\name, showing relative\nimprovements of 68.67\\% in Hit@5 and 45.21\\% in NDCG@20. Code available at\nhttps://github.com/YRYangang/RRec.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16994.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63b6dbc8ccebeadccc888456",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673396893898-63b6dbc8ccebeadccc888456.jpeg",
      "fullname": "Xin Zhang",
      "name": "izhx",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 19
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.03079",
      "authors": [
        {
          "_id": "683fee0a179d710da07d4352",
          "name": "Xiuyu Yang",
          "hidden": false
        },
        {
          "_id": "683fee0a179d710da07d4353",
          "name": "Bohan Li",
          "hidden": false
        },
        {
          "_id": "683fee0a179d710da07d4354",
          "name": "Shaocong Xu",
          "hidden": false
        },
        {
          "_id": "683fee0a179d710da07d4355",
          "name": "Nan Wang",
          "hidden": false
        },
        {
          "_id": "683fee0a179d710da07d4356",
          "name": "Chongjie Ye",
          "hidden": false
        },
        {
          "_id": "683fee0a179d710da07d4357",
          "name": "Zhaoxi Chen",
          "hidden": false
        },
        {
          "_id": "683fee0a179d710da07d4358",
          "name": "Minghan Qin",
          "hidden": false
        },
        {
          "_id": "683fee0a179d710da07d4359",
          "name": "Yikang Ding",
          "hidden": false
        },
        {
          "_id": "683fee0a179d710da07d435a",
          "name": "Xin Jin",
          "hidden": false
        },
        {
          "_id": "683fee0a179d710da07d435b",
          "name": "Hang Zhao",
          "hidden": false
        },
        {
          "_id": "683fee0a179d710da07d435c",
          "name": "Hao Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T17:00:32.000Z",
      "submittedOnDailyAt": "2025-06-04T05:27:07.241Z",
      "title": "ORV: 4D Occupancy-centric Robot Video Generation",
      "submittedOnDailyBy": {
        "_id": "634aab35dcf125e4dafc87b1",
        "avatarUrl": "/avatars/aa2db84fd423e9eefe3ef3167c9d3999.svg",
        "isPro": false,
        "fullname": "YangXiuyu",
        "user": "gzzyyxy",
        "type": "user"
      },
      "summary": "Acquiring real-world robotic simulation data through teleoperation is\nnotoriously time-consuming and labor-intensive. Recently, action-driven\ngenerative models have gained widespread adoption in robot learning and\nsimulation, as they eliminate safety concerns and reduce maintenance efforts.\nHowever, the action sequences used in these methods often result in limited\ncontrol precision and poor generalization due to their globally coarse\nalignment. To address these limitations, we propose ORV, an Occupancy-centric\nRobot Video generation framework, which utilizes 4D semantic occupancy\nsequences as a fine-grained representation to provide more accurate semantic\nand geometric guidance for video generation. By leveraging occupancy-based\nrepresentations, ORV enables seamless translation of simulation data into\nphotorealistic robot videos, while ensuring high temporal consistency and\nprecise controllability. Furthermore, our framework supports the simultaneous\ngeneration of multi-view videos of robot gripping operations - an important\ncapability for downstream robotic learning tasks. Extensive experimental\nresults demonstrate that ORV consistently outperforms existing baseline methods\nacross various datasets and sub-tasks. Demo, Code and Model:\nhttps://orangesodahub.github.io/ORV",
      "upvotes": 1,
      "discussionId": "683fee12179d710da07d45f4",
      "projectPage": "https://orangesodahub.github.io/ORV/",
      "githubRepo": "https://github.com/OrangeSodahub/ORV",
      "ai_summary": "ORV, an Occupancy-centric Robot Video generation framework, uses 4D semantic occupancy sequences to produce photorealistic, temporally consistent, and precisely controllable robot videos, enhancing existing methods.",
      "ai_keywords": [
        "action-driven generative models",
        "occupancy-centric",
        "4D semantic occupancy sequences",
        "video generation",
        "photorealistic robot videos",
        "temporal consistency",
        "precise controllability",
        "multi-view videos"
      ]
    },
    "publishedAt": "2025-06-03T13:00:32.000Z",
    "title": "ORV: 4D Occupancy-centric Robot Video Generation",
    "summary": "Acquiring real-world robotic simulation data through teleoperation is\nnotoriously time-consuming and labor-intensive. Recently, action-driven\ngenerative models have gained widespread adoption in robot learning and\nsimulation, as they eliminate safety concerns and reduce maintenance efforts.\nHowever, the action sequences used in these methods often result in limited\ncontrol precision and poor generalization due to their globally coarse\nalignment. To address these limitations, we propose ORV, an Occupancy-centric\nRobot Video generation framework, which utilizes 4D semantic occupancy\nsequences as a fine-grained representation to provide more accurate semantic\nand geometric guidance for video generation. By leveraging occupancy-based\nrepresentations, ORV enables seamless translation of simulation data into\nphotorealistic robot videos, while ensuring high temporal consistency and\nprecise controllability. Furthermore, our framework supports the simultaneous\ngeneration of multi-view videos of robot gripping operations - an important\ncapability for downstream robotic learning tasks. Extensive experimental\nresults demonstrate that ORV consistently outperforms existing baseline methods\nacross various datasets and sub-tasks. Demo, Code and Model:\nhttps://orangesodahub.github.io/ORV",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03079.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "634aab35dcf125e4dafc87b1",
      "avatarUrl": "/avatars/aa2db84fd423e9eefe3ef3167c9d3999.svg",
      "fullname": "YangXiuyu",
      "name": "gzzyyxy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.02510",
      "authors": [
        {
          "_id": "683faa31f0564d1fb4b9ffc6",
          "name": "Jie Zhu",
          "hidden": false
        },
        {
          "_id": "683faa31f0564d1fb4b9ffc7",
          "name": "Junhui Li",
          "hidden": false
        },
        {
          "_id": "683faa31f0564d1fb4b9ffc8",
          "name": "Yalong Wen",
          "hidden": false
        },
        {
          "_id": "683faa31f0564d1fb4b9ffc9",
          "name": "Xiandong Li",
          "hidden": false
        },
        {
          "_id": "683faa31f0564d1fb4b9ffca",
          "name": "Lifan Guo",
          "hidden": false
        },
        {
          "_id": "683faa31f0564d1fb4b9ffcb",
          "name": "Feng Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T06:41:09.000Z",
      "submittedOnDailyAt": "2025-06-04T00:38:17.377Z",
      "title": "M^3FinMeeting: A Multilingual, Multi-Sector, and Multi-Task Financial\n  Meeting Understanding Evaluation Dataset",
      "submittedOnDailyBy": {
        "_id": "642656cbad1e3b0e6e91b752",
        "avatarUrl": "/avatars/3bf0ee15fd528e09b2b889f5cce3cbd0.svg",
        "isPro": false,
        "fullname": "Jie Zhu",
        "user": "amazingj",
        "type": "user"
      },
      "summary": "Recent breakthroughs in large language models (LLMs) have led to the\ndevelopment of new benchmarks for evaluating their performance in the financial\ndomain. However, current financial benchmarks often rely on news articles,\nearnings reports, or announcements, making it challenging to capture the\nreal-world dynamics of financial meetings. To address this gap, we propose a\nnovel benchmark called M^3FinMeeting, which is a multilingual,\nmulti-sector, and multi-task dataset designed for financial meeting\nunderstanding. First, M^3FinMeeting supports English, Chinese, and\nJapanese, enhancing comprehension of financial discussions in diverse\nlinguistic contexts. Second, it encompasses various industry sectors defined by\nthe Global Industry Classification Standard (GICS), ensuring that the benchmark\nspans a broad range of financial activities. Finally,\nM^3FinMeeting includes three tasks: summarization, question-answer\n(QA) pair extraction, and question answering, facilitating a more realistic and\ncomprehensive evaluation of understanding. Experimental results with seven\npopular LLMs reveal that even the most advanced long-context models have\nsignificant room for improvement, demonstrating the effectiveness of\nM^3FinMeeting as a benchmark for assessing LLMs' financial meeting\ncomprehension skills.",
      "upvotes": 1,
      "discussionId": "683faa32f0564d1fb4ba0005",
      "projectPage": "https://github.com/aliyun/qwen-dianjin",
      "githubRepo": "https://github.com/aliyun/qwen-dianjin",
      "ai_summary": "A new multilingual, multi-sector, and multi-task benchmark, M³FinMeeting, evaluates large language models' performance in understanding financial meetings across different languages and industries.",
      "ai_keywords": [
        "large language models",
        "multilingual",
        "multi-sector",
        "multi-task",
        "benchmark",
        "financial meeting understanding",
        "Global Industry Classification Standard (GICS)",
        "summarization",
        "question-answer pair extraction",
        "question answering"
      ]
    },
    "publishedAt": "2025-06-03T02:41:09.000Z",
    "title": "M^3FinMeeting: A Multilingual, Multi-Sector, and Multi-Task Financial\n  Meeting Understanding Evaluation Dataset",
    "summary": "Recent breakthroughs in large language models (LLMs) have led to the\ndevelopment of new benchmarks for evaluating their performance in the financial\ndomain. However, current financial benchmarks often rely on news articles,\nearnings reports, or announcements, making it challenging to capture the\nreal-world dynamics of financial meetings. To address this gap, we propose a\nnovel benchmark called M^3FinMeeting, which is a multilingual,\nmulti-sector, and multi-task dataset designed for financial meeting\nunderstanding. First, M^3FinMeeting supports English, Chinese, and\nJapanese, enhancing comprehension of financial discussions in diverse\nlinguistic contexts. Second, it encompasses various industry sectors defined by\nthe Global Industry Classification Standard (GICS), ensuring that the benchmark\nspans a broad range of financial activities. Finally,\nM^3FinMeeting includes three tasks: summarization, question-answer\n(QA) pair extraction, and question answering, facilitating a more realistic and\ncomprehensive evaluation of understanding. Experimental results with seven\npopular LLMs reveal that even the most advanced long-context models have\nsignificant room for improvement, demonstrating the effectiveness of\nM^3FinMeeting as a benchmark for assessing LLMs' financial meeting\ncomprehension skills.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02510.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642656cbad1e3b0e6e91b752",
      "avatarUrl": "/avatars/3bf0ee15fd528e09b2b889f5cce3cbd0.svg",
      "fullname": "Jie Zhu",
      "name": "amazingj",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.02454",
      "authors": [
        {
          "_id": "683fb5d592425f86f2be5c40",
          "name": "Zhaorui Yang",
          "hidden": false
        },
        {
          "_id": "683fb5d592425f86f2be5c41",
          "name": "Bo Pan",
          "hidden": false
        },
        {
          "_id": "683fb5d592425f86f2be5c42",
          "name": "Han Wang",
          "hidden": false
        },
        {
          "_id": "683fb5d592425f86f2be5c43",
          "name": "Yiyao Wang",
          "hidden": false
        },
        {
          "_id": "683fb5d592425f86f2be5c44",
          "name": "Xingyu Liu",
          "hidden": false
        },
        {
          "_id": "683fb5d592425f86f2be5c45",
          "name": "Minfeng Zhu",
          "hidden": false
        },
        {
          "_id": "683fb5d592425f86f2be5c46",
          "name": "Bo Zhang",
          "hidden": false
        },
        {
          "_id": "683fb5d592425f86f2be5c47",
          "name": "Wei Chen",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64a568f5764b1dce366f9fd2/Jup3eQX_IL2nKhCQVaSWS.mp4"
      ],
      "publishedAt": "2025-06-03T05:18:19.000Z",
      "submittedOnDailyAt": "2025-06-04T01:28:03.959Z",
      "title": "Multimodal DeepResearcher: Generating Text-Chart Interleaved Reports\n  From Scratch with Agentic Framework",
      "submittedOnDailyBy": {
        "_id": "64a568f5764b1dce366f9fd2",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a568f5764b1dce366f9fd2/9THW2AJhEVmzltEAm9mMY.jpeg",
        "isPro": false,
        "fullname": "Zhaorui Yang",
        "user": "zhaoruiyang",
        "type": "user"
      },
      "summary": "Visualizations play a crucial part in effective communication of concepts and\ninformation. Recent advances in reasoning and retrieval augmented generation\nhave enabled Large Language Models (LLMs) to perform deep research and generate\ncomprehensive reports. Despite its progress, existing deep research frameworks\nprimarily focus on generating text-only content, leaving the automated\ngeneration of interleaved texts and visualizations underexplored. This novel\ntask poses key challenges in designing informative visualizations and\neffectively integrating them with text reports. To address these challenges, we\npropose Formal Description of Visualization (FDV), a structured textual\nrepresentation of charts that enables LLMs to learn from and generate diverse,\nhigh-quality visualizations. Building on this representation, we introduce\nMultimodal DeepResearcher, an agentic framework that decomposes the task into\nfour stages: (1) researching, (2) exemplar report textualization, (3) planning,\nand (4) multimodal report generation. For the evaluation of generated\nmultimodal reports, we develop MultimodalReportBench, which contains 100\ndiverse topics served as inputs along with 5 dedicated metrics. Extensive\nexperiments across models and evaluation methods demonstrate the effectiveness\nof Multimodal DeepResearcher. Notably, utilizing the same Claude 3.7 Sonnet\nmodel, Multimodal DeepResearcher achieves an 82\\% overall win rate over the\nbaseline method.",
      "upvotes": 1,
      "discussionId": "683fb5d792425f86f2be5c78",
      "projectPage": "https://rickyang1114.github.io/multimodal-deepresearcher/",
      "ai_summary": "A new framework, Multimodal DeepResearcher, enables Large Language Models to generate high-quality multimodal reports combining text and diverse visualizations through structured textual representations.",
      "ai_keywords": [
        "Formal Description of Visualization",
        "FDV",
        "Multimodal DeepResearcher",
        "researching",
        "exemplar report textualization",
        "planning",
        "multimodal report generation",
        "MultimodalReportBench",
        "multimodal reports"
      ]
    },
    "publishedAt": "2025-06-03T01:18:19.000Z",
    "title": "Multimodal DeepResearcher: Generating Text-Chart Interleaved Reports\n  From Scratch with Agentic Framework",
    "summary": "Visualizations play a crucial part in effective communication of concepts and\ninformation. Recent advances in reasoning and retrieval augmented generation\nhave enabled Large Language Models (LLMs) to perform deep research and generate\ncomprehensive reports. Despite its progress, existing deep research frameworks\nprimarily focus on generating text-only content, leaving the automated\ngeneration of interleaved texts and visualizations underexplored. This novel\ntask poses key challenges in designing informative visualizations and\neffectively integrating them with text reports. To address these challenges, we\npropose Formal Description of Visualization (FDV), a structured textual\nrepresentation of charts that enables LLMs to learn from and generate diverse,\nhigh-quality visualizations. Building on this representation, we introduce\nMultimodal DeepResearcher, an agentic framework that decomposes the task into\nfour stages: (1) researching, (2) exemplar report textualization, (3) planning,\nand (4) multimodal report generation. For the evaluation of generated\nmultimodal reports, we develop MultimodalReportBench, which contains 100\ndiverse topics served as inputs along with 5 dedicated metrics. Extensive\nexperiments across models and evaluation methods demonstrate the effectiveness\nof Multimodal DeepResearcher. Notably, utilizing the same Claude 3.7 Sonnet\nmodel, Multimodal DeepResearcher achieves an 82\\% overall win rate over the\nbaseline method.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64a568f5764b1dce366f9fd2/Jup3eQX_IL2nKhCQVaSWS.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02454.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a568f5764b1dce366f9fd2",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a568f5764b1dce366f9fd2/9THW2AJhEVmzltEAm9mMY.jpeg",
      "fullname": "Zhaorui Yang",
      "name": "zhaoruiyang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.02138",
      "authors": [
        {
          "_id": "683fe27c4f32bd7bbca087fc",
          "name": "Yarden Bakish",
          "hidden": false
        },
        {
          "_id": "683fe27c4f32bd7bbca087fd",
          "name": "Itamar Zimerman",
          "hidden": false
        },
        {
          "_id": "683fe27c4f32bd7bbca087fe",
          "name": "Hila Chefer",
          "hidden": false
        },
        {
          "_id": "683fe27c4f32bd7bbca087ff",
          "name": "Lior Wolf",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-02T18:07:55.000Z",
      "submittedOnDailyAt": "2025-06-04T04:51:22.070Z",
      "title": "Revisiting LRP: Positional Attribution as the Missing Ingredient for\n  Transformer Explainability",
      "submittedOnDailyBy": {
        "_id": "65376feed325b3f02fb92c69",
        "avatarUrl": "/avatars/e952918cf434d5302e9b1a404eccaf0e.svg",
        "isPro": false,
        "fullname": "Itamar Zimerman",
        "user": "ItamarZ",
        "type": "user"
      },
      "summary": "The development of effective explainability tools for Transformers is a\ncrucial pursuit in deep learning research. One of the most promising approaches\nin this domain is Layer-wise Relevance Propagation (LRP), which propagates\nrelevance scores backward through the network to the input space by\nredistributing activation values based on predefined rules. However, existing\nLRP-based methods for Transformer explainability entirely overlook a critical\ncomponent of the Transformer architecture: its positional encoding (PE),\nresulting in violation of the conservation property, and the loss of an\nimportant and unique type of relevance, which is also associated with\nstructural and positional features. To address this limitation, we reformulate\nthe input space for Transformer explainability as a set of position-token\npairs. This allows us to propose specialized theoretically-grounded LRP rules\ndesigned to propagate attributions across various positional encoding methods,\nincluding Rotary, Learnable, and Absolute PE. Extensive experiments with both\nfine-tuned classifiers and zero-shot foundation models, such as LLaMA 3,\ndemonstrate that our method significantly outperforms the state-of-the-art in\nboth vision and NLP explainability tasks. Our code is publicly available.",
      "upvotes": 0,
      "discussionId": "683fe27d4f32bd7bbca08867",
      "ai_summary": "A specialized LRP method for Transformer explainability considers positional encoding, improving relevance propagation and outperforming existing methods.",
      "ai_keywords": [
        "Layer-wise Relevance Propagation (LRP)",
        "Transformers",
        "positional encoding (PE)",
        "Rotary",
        "Learnable",
        "Absolute PE",
        "vision",
        "NLP explainability tasks"
      ]
    },
    "publishedAt": "2025-06-02T14:07:55.000Z",
    "title": "Revisiting LRP: Positional Attribution as the Missing Ingredient for\n  Transformer Explainability",
    "summary": "The development of effective explainability tools for Transformers is a\ncrucial pursuit in deep learning research. One of the most promising approaches\nin this domain is Layer-wise Relevance Propagation (LRP), which propagates\nrelevance scores backward through the network to the input space by\nredistributing activation values based on predefined rules. However, existing\nLRP-based methods for Transformer explainability entirely overlook a critical\ncomponent of the Transformer architecture: its positional encoding (PE),\nresulting in violation of the conservation property, and the loss of an\nimportant and unique type of relevance, which is also associated with\nstructural and positional features. To address this limitation, we reformulate\nthe input space for Transformer explainability as a set of position-token\npairs. This allows us to propose specialized theoretically-grounded LRP rules\ndesigned to propagate attributions across various positional encoding methods,\nincluding Rotary, Learnable, and Absolute PE. Extensive experiments with both\nfine-tuned classifiers and zero-shot foundation models, such as LLaMA 3,\ndemonstrate that our method significantly outperforms the state-of-the-art in\nboth vision and NLP explainability tasks. Our code is publicly available.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02138.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65376feed325b3f02fb92c69",
      "avatarUrl": "/avatars/e952918cf434d5302e9b1a404eccaf0e.svg",
      "fullname": "Itamar Zimerman",
      "name": "ItamarZ",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]