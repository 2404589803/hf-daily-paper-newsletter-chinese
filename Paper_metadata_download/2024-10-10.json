[
    {
        "paper": {
            "id": "2410.05254",
            "authors": [
                {
                    "_id": "670686055dc3f2a798ad4a5f",
                    "user": {
                        "_id": "64802fb6c57f629056c59966",
                        "avatarUrl": "/avatars/d5ecabaceeba759969855acf512b6649.svg",
                        "isPro": false,
                        "fullname": "Eilam Shapira",
                        "user": "EilamSha",
                        "type": "user"
                    },
                    "name": "Eilam Shapira",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-09T13:46:45.517Z",
                    "hidden": false
                },
                {
                    "_id": "670686055dc3f2a798ad4a60",
                    "user": {
                        "_id": "6707a02ae1e3c1a7b6f457b1",
                        "avatarUrl": "/avatars/fcb899e8a95e84ee18885d83f5cd7040.svg",
                        "isPro": false,
                        "fullname": "Omer Madmon",
                        "user": "omermadmon",
                        "type": "user"
                    },
                    "name": "Omer Madmon",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T14:08:16.799Z",
                    "hidden": false
                },
                {
                    "_id": "670686055dc3f2a798ad4a61",
                    "user": {
                        "_id": "64e67f74fdb84138027ae2b1",
                        "avatarUrl": "/avatars/f0bca499a30cfcfa058ac4bb31a08ee3.svg",
                        "isPro": false,
                        "fullname": "Itamar Reinman",
                        "user": "ireinman",
                        "type": "user"
                    },
                    "name": "Itamar Reinman",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T09:22:28.588Z",
                    "hidden": false
                },
                {
                    "_id": "670686055dc3f2a798ad4a62",
                    "name": "Samuel Joseph Amouyal",
                    "hidden": false
                },
                {
                    "_id": "670686055dc3f2a798ad4a63",
                    "name": "Roi Reichart",
                    "hidden": false
                },
                {
                    "_id": "670686055dc3f2a798ad4a64",
                    "name": "Moshe Tennenholtz",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-07T17:55:35.000Z",
            "title": "GLEE: A Unified Framework and Benchmark for Language-based Economic\n  Environments",
            "summary": "Large Language Models (LLMs) show significant potential in economic and\nstrategic interactions, where communication via natural language is often\nprevalent. This raises key questions: Do LLMs behave rationally? Can they mimic\nhuman behavior? Do they tend to reach an efficient and fair outcome? What is\nthe role of natural language in the strategic interaction? How do\ncharacteristics of the economic environment influence these dynamics? These\nquestions become crucial concerning the economic and societal implications of\nintegrating LLM-based agents into real-world data-driven systems, such as\nonline retail platforms and recommender systems. While the ML community has\nbeen exploring the potential of LLMs in such multi-agent setups, varying\nassumptions, design choices and evaluation criteria across studies make it\ndifficult to draw robust and meaningful conclusions. To address this, we\nintroduce a benchmark for standardizing research on two-player, sequential,\nlanguage-based games. Inspired by the economic literature, we define three base\nfamilies of games with consistent parameterization, degrees of freedom and\neconomic measures to evaluate agents' performance (self-gain), as well as the\ngame outcome (efficiency and fairness). We develop an open-source framework for\ninteraction simulation and analysis, and utilize it to collect a dataset of LLM\nvs. LLM interactions across numerous game configurations and an additional\ndataset of human vs. LLM interactions. Through extensive experimentation, we\ndemonstrate how our framework and dataset can be used to: (i) compare the\nbehavior of LLM-based agents to human players in various economic contexts;\n(ii) evaluate agents in both individual and collective performance measures;\nand (iii) quantify the effect of the economic characteristics of the\nenvironments on the behavior of agents.",
            "upvotes": 54,
            "discussionId": "670686075dc3f2a798ad4ae0"
        },
        "publishedAt": "2024-10-10T06:52:28.404Z",
        "title": "GLEE: A Unified Framework and Benchmark for Language-based Economic Environments",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.05254.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/d5ecabaceeba759969855acf512b6649.svg",
            "fullname": "Eilam Shapira",
            "name": "EilamSha",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.07113",
            "authors": [
                {
                    "_id": "67075c2716abdf6976cd06c6",
                    "user": {
                        "_id": "63f45b8d520c14618930d175",
                        "avatarUrl": "/avatars/a20994594579b52a8be8bd2c4acbb913.svg",
                        "isPro": false,
                        "fullname": "renjie",
                        "user": "renjiepi",
                        "type": "user"
                    },
                    "name": "Renjie Pi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T08:13:26.509Z",
                    "hidden": false
                },
                {
                    "_id": "67075c2716abdf6976cd06c7",
                    "user": {
                        "_id": "65d8b0f0661492b25c6623de",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d8b0f0661492b25c6623de/c6LPDse8NIV_3BHIu8dYe.png",
                        "isPro": false,
                        "fullname": "Jianshu Zhang",
                        "user": "Sterzhang",
                        "type": "user"
                    },
                    "name": "Jianshu Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T08:13:33.559Z",
                    "hidden": false
                },
                {
                    "_id": "67075c2716abdf6976cd06c8",
                    "name": "Tianyang Han",
                    "hidden": false
                },
                {
                    "_id": "67075c2716abdf6976cd06c9",
                    "name": "Jipeng Zhang",
                    "hidden": false
                },
                {
                    "_id": "67075c2716abdf6976cd06ca",
                    "user": {
                        "_id": "64cb1ad1667f4f80852f6050",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64cb1ad1667f4f80852f6050/iOn5q_RyyBS99tObrO5Tc.png",
                        "isPro": false,
                        "fullname": "Rui Pan",
                        "user": "research4pan",
                        "type": "user"
                    },
                    "name": "Rui Pan",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2024-10-10T16:19:32.485Z",
                    "hidden": false
                },
                {
                    "_id": "67075c2716abdf6976cd06cb",
                    "name": "Tong Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-09T17:46:53.000Z",
            "title": "Personalized Visual Instruction Tuning",
            "summary": "Recent advancements in multimodal large language models (MLLMs) have\ndemonstrated significant progress; however, these models exhibit a notable\nlimitation, which we refer to as \"face blindness\". Specifically, they can\nengage in general conversations but fail to conduct personalized dialogues\ntargeting at specific individuals. This deficiency hinders the application of\nMLLMs in personalized settings, such as tailored visual assistants on mobile\ndevices, or domestic robots that need to recognize members of the family. In\nthis paper, we introduce Personalized Visual Instruction Tuning (PVIT), a novel\ndata curation and training framework designed to enable MLLMs to identify\ntarget individuals within an image and engage in personalized and coherent\ndialogues. Our approach involves the development of a sophisticated pipeline\nthat autonomously generates training data containing personalized\nconversations. This pipeline leverages the capabilities of various visual\nexperts, image generation models, and (multi-modal) large language models. To\nevaluate the personalized potential of MLLMs, we present a benchmark called\nP-Bench, which encompasses various question types with different levels of\ndifficulty. The experiments demonstrate a substantial personalized performance\nenhancement after fine-tuning with our curated dataset.",
            "upvotes": 54,
            "discussionId": "67075c2c16abdf6976cd07ea"
        },
        "publishedAt": "2024-10-10T03:18:35.485Z",
        "title": "Personalized Visual Instruction Tuning",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.07113.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/a20994594579b52a8be8bd2c4acbb913.svg",
            "fullname": "renjie",
            "name": "renjiepi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.07171",
            "authors": [
                {
                    "_id": "67073ff0327cec6882ea2ce1",
                    "user": {
                        "_id": "653e5d31ffd60206c8b64bb5",
                        "avatarUrl": "/avatars/5076795722ec1f9e031654f301d30e8f.svg",
                        "isPro": false,
                        "fullname": "Xinchen Zhang",
                        "user": "comin",
                        "type": "user"
                    },
                    "name": "Xinchen Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-10T08:01:19.383Z",
                    "hidden": false
                },
                {
                    "_id": "67073ff0327cec6882ea2ce2",
                    "name": "Ling Yang",
                    "hidden": false
                },
                {
                    "_id": "67073ff0327cec6882ea2ce3",
                    "name": "Guohao Li",
                    "hidden": false
                },
                {
                    "_id": "67073ff0327cec6882ea2ce4",
                    "user": {
                        "_id": "657fe0b1f010d76b6e709110",
                        "avatarUrl": "/avatars/9c08817484cfb5b238d6833d788fa0a3.svg",
                        "isPro": false,
                        "fullname": "YaQi Cai",
                        "user": "yaqicc",
                        "type": "user"
                    },
                    "name": "Yaqi Cai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T08:44:03.195Z",
                    "hidden": false
                },
                {
                    "_id": "67073ff0327cec6882ea2ce5",
                    "user": {
                        "_id": "63f46f4f21eb234ab7395456",
                        "avatarUrl": "/avatars/16f444058549b0317243784e9e1e3110.svg",
                        "isPro": false,
                        "fullname": "jiake xie",
                        "user": "kelisiya",
                        "type": "user"
                    },
                    "name": "Jiake Xie",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T08:43:34.868Z",
                    "hidden": false
                },
                {
                    "_id": "67073ff0327cec6882ea2ce6",
                    "name": "Yong Tang",
                    "hidden": false
                },
                {
                    "_id": "67073ff0327cec6882ea2ce7",
                    "user": {
                        "_id": "64ca1fe838837b12d5e529b7",
                        "avatarUrl": "/avatars/44a3ad9e59318784ac531993b5f69f6b.svg",
                        "isPro": false,
                        "fullname": "Yujiu Yang",
                        "user": "Thu-redrobot",
                        "type": "user"
                    },
                    "name": "Yujiu Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T08:43:28.498Z",
                    "hidden": false
                },
                {
                    "_id": "67073ff0327cec6882ea2ce8",
                    "name": "Mengdi Wang",
                    "hidden": false
                },
                {
                    "_id": "67073ff0327cec6882ea2ce9",
                    "name": "Bin Cui",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-09T17:59:13.000Z",
            "title": "IterComp: Iterative Composition-Aware Feedback Learning from Model\n  Gallery for Text-to-Image Generation",
            "summary": "Advanced diffusion models like RPG, Stable Diffusion 3 and FLUX have made\nnotable strides in compositional text-to-image generation. However, these\nmethods typically exhibit distinct strengths for compositional generation, with\nsome excelling in handling attribute binding and others in spatial\nrelationships. This disparity highlights the need for an approach that can\nleverage the complementary strengths of various models to comprehensively\nimprove the composition capability. To this end, we introduce IterComp, a novel\nframework that aggregates composition-aware model preferences from multiple\nmodels and employs an iterative feedback learning approach to enhance\ncompositional generation. Specifically, we curate a gallery of six powerful\nopen-source diffusion models and evaluate their three key compositional\nmetrics: attribute binding, spatial relationships, and non-spatial\nrelationships. Based on these metrics, we develop a composition-aware model\npreference dataset comprising numerous image-rank pairs to train\ncomposition-aware reward models. Then, we propose an iterative feedback\nlearning method to enhance compositionality in a closed-loop manner, enabling\nthe progressive self-refinement of both the base diffusion model and reward\nmodels over multiple iterations. Theoretical proof demonstrates the\neffectiveness and extensive experiments show our significant superiority over\nprevious SOTA methods (e.g., Omost and FLUX), particularly in multi-category\nobject composition and complex semantic alignment. IterComp opens new research\navenues in reward feedback learning for diffusion models and compositional\ngeneration. Code: https://github.com/YangLing0818/IterComp",
            "upvotes": 35,
            "discussionId": "67073ff1327cec6882ea2d3b"
        },
        "publishedAt": "2024-10-10T02:10:13.564Z",
        "title": "IterComp: Iterative Composition-Aware Feedback Learning from Model Gallery for Text-to-Image Generation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.07171.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/5076795722ec1f9e031654f301d30e8f.svg",
            "fullname": "Xinchen Zhang",
            "name": "comin",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.05993",
            "authors": [
                {
                    "_id": "67073615be29abe376815ae4",
                    "user": {
                        "_id": "6357362f811ee2fa05070f64",
                        "avatarUrl": "/avatars/2cf37efb80f5cfb3e4e9d08674de6dd1.svg",
                        "isPro": false,
                        "fullname": "Dongxu Li",
                        "user": "dxli1",
                        "type": "user"
                    },
                    "name": "Dongxu Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-10T08:01:43.587Z",
                    "hidden": false
                },
                {
                    "_id": "67073615be29abe376815ae5",
                    "name": "Yudong Liu",
                    "hidden": false
                },
                {
                    "_id": "67073615be29abe376815ae6",
                    "user": {
                        "_id": "63047ed2412a1b9d381b09c9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63047ed2412a1b9d381b09c9/2Ill5G0uSMyGstrawgmIb.jpeg",
                        "isPro": false,
                        "fullname": "Haoning Wu, Teo",
                        "user": "teowu",
                        "type": "user"
                    },
                    "name": "Haoning Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-10T08:01:45.855Z",
                    "hidden": false
                },
                {
                    "_id": "67073615be29abe376815ae7",
                    "name": "Yue Wang",
                    "hidden": false
                },
                {
                    "_id": "67073615be29abe376815ae8",
                    "name": "Zhiqi Shen",
                    "hidden": false
                },
                {
                    "_id": "67073615be29abe376815ae9",
                    "name": "Bowen Qu",
                    "hidden": false
                },
                {
                    "_id": "67073615be29abe376815aea",
                    "user": {
                        "_id": "627f4c100d2c1c0ba3da8a72",
                        "avatarUrl": "/avatars/404869a87480ef526c1d5fccb583bfa8.svg",
                        "isPro": false,
                        "fullname": "Xinyao Niu",
                        "user": "sirius-ctrl",
                        "type": "user"
                    },
                    "name": "Xinyao Niu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T08:21:06.702Z",
                    "hidden": false
                },
                {
                    "_id": "67073615be29abe376815aeb",
                    "user": {
                        "_id": "6490d4ba1afdee3acd1147f6",
                        "avatarUrl": "/avatars/ae13c7b21fe9ced7541dcd664d1b94ed.svg",
                        "isPro": false,
                        "fullname": "Guoyin Wang",
                        "user": "guoyinwang",
                        "type": "user"
                    },
                    "name": "Guoyin Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T08:21:17.466Z",
                    "hidden": false
                },
                {
                    "_id": "67073615be29abe376815aec",
                    "name": "Bei Chen",
                    "hidden": false
                },
                {
                    "_id": "67073615be29abe376815aed",
                    "user": {
                        "_id": "61f9d3b54ac99e8a1bae85f4",
                        "avatarUrl": "/avatars/ac47d13204dd22452e4bc46e280842d5.svg",
                        "isPro": false,
                        "fullname": "JunnanLi",
                        "user": "JunnanLi",
                        "type": "user"
                    },
                    "name": "Junnan Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T08:21:58.418Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-08T12:44:57.000Z",
            "title": "Aria: An Open Multimodal Native Mixture-of-Experts Model",
            "summary": "Information comes in diverse modalities. Multimodal native AI models are\nessential to integrate real-world information and deliver comprehensive\nunderstanding. While proprietary multimodal native models exist, their lack of\nopenness imposes obstacles for adoptions, let alone adaptations. To fill this\ngap, we introduce Aria, an open multimodal native model with best-in-class\nperformance across a wide range of multimodal, language, and coding tasks. Aria\nis a mixture-of-expert model with 3.9B and 3.5B activated parameters per visual\ntoken and text token, respectively. It outperforms Pixtral-12B and\nLlama3.2-11B, and is competitive against the best proprietary models on various\nmultimodal tasks. We pre-train Aria from scratch following a 4-stage pipeline,\nwhich progressively equips the model with strong capabilities in language\nunderstanding, multimodal understanding, long context window, and instruction\nfollowing. We open-source the model weights along with a codebase that\nfacilitates easy adoptions and adaptations of Aria in real-world applications.",
            "upvotes": 33,
            "discussionId": "67073618be29abe376815bf4"
        },
        "publishedAt": "2024-10-10T01:41:49.086Z",
        "title": "Aria: An Open Multimodal Native Mixture-of-Experts Model",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.05993.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63047ed2412a1b9d381b09c9/2Ill5G0uSMyGstrawgmIb.jpeg",
            "fullname": "Haoning Wu, Teo",
            "name": "teowu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.05363",
            "authors": [
                {
                    "_id": "67073022327cec6882e5d4c9",
                    "user": {
                        "_id": "640b37b2bab5ca8fbe7df8f2",
                        "avatarUrl": "/avatars/c7bef45efad6a0d911a720e2236fcba5.svg",
                        "isPro": false,
                        "fullname": "fanqing meng",
                        "user": "FanqingM",
                        "type": "user"
                    },
                    "name": "Fanqing Meng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T08:27:47.816Z",
                    "hidden": false
                },
                {
                    "_id": "67073022327cec6882e5d4ca",
                    "user": {
                        "_id": "6630b287ce65a66ed8f04eba",
                        "avatarUrl": "/avatars/18ad0e3b453021ea6006e23333e92c12.svg",
                        "isPro": false,
                        "fullname": "liaojiaqi",
                        "user": "ljq940913",
                        "type": "user"
                    },
                    "name": "Jiaqi Liao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T08:28:33.069Z",
                    "hidden": false
                },
                {
                    "_id": "67073022327cec6882e5d4cb",
                    "name": "Xinyu Tan",
                    "hidden": false
                },
                {
                    "_id": "67073022327cec6882e5d4cc",
                    "user": {
                        "_id": "64b3fd42eec33e27dcc4c941",
                        "avatarUrl": "/avatars/5aa1a99468fa61d4b8b0e80b592c4e55.svg",
                        "isPro": false,
                        "fullname": "Wenqi Shao",
                        "user": "wqshao126",
                        "type": "user"
                    },
                    "name": "Wenqi Shao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T08:29:10.044Z",
                    "hidden": false
                },
                {
                    "_id": "67073022327cec6882e5d4cd",
                    "user": {
                        "_id": "653a483dacdeea08424ef55d",
                        "avatarUrl": "/avatars/5b9966742c5762e6e9026efc876f646a.svg",
                        "isPro": false,
                        "fullname": "Quanfeng Lu",
                        "user": "hflqf88888",
                        "type": "user"
                    },
                    "name": "Quanfeng Lu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T08:29:17.379Z",
                    "hidden": false
                },
                {
                    "_id": "67073022327cec6882e5d4ce",
                    "user": {
                        "_id": "63527f4e7d071f23d085ad45",
                        "avatarUrl": "/avatars/99a51adef5673b3ac1a8c02eb47759c4.svg",
                        "isPro": false,
                        "fullname": "KAIPENG ZHANG",
                        "user": "kpzhang",
                        "type": "user"
                    },
                    "name": "Kaipeng Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T08:29:23.401Z",
                    "hidden": false
                },
                {
                    "_id": "67073022327cec6882e5d4cf",
                    "name": "Yu Cheng",
                    "hidden": false
                },
                {
                    "_id": "67073022327cec6882e5d4d0",
                    "name": "Dianqi Li",
                    "hidden": false
                },
                {
                    "_id": "67073022327cec6882e5d4d1",
                    "name": "Yu Qiao",
                    "hidden": false
                },
                {
                    "_id": "67073022327cec6882e5d4d2",
                    "name": "Ping Luo",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-07T17:56:04.000Z",
            "title": "Towards World Simulator: Crafting Physical Commonsense-Based Benchmark\n  for Video Generation",
            "summary": "Text-to-video (T2V) models like Sora have made significant strides in\nvisualizing complex prompts, which is increasingly viewed as a promising path\ntowards constructing the universal world simulator. Cognitive psychologists\nbelieve that the foundation for achieving this goal is the ability to\nunderstand intuitive physics. However, the capacity of these models to\naccurately represent intuitive physics remains largely unexplored. To bridge\nthis gap, we introduce PhyGenBench, a comprehensive Physics\nGeneration Benchmark designed to evaluate physical\ncommonsense correctness in T2V generation. PhyGenBench comprises 160 carefully\ncrafted prompts across 27 distinct physical laws, spanning four fundamental\ndomains, which could comprehensively assesses models' understanding of physical\ncommonsense. Alongside PhyGenBench, we propose a novel evaluation framework\ncalled PhyGenEval. This framework employs a hierarchical evaluation structure\nutilizing appropriate advanced vision-language models and large language models\nto assess physical commonsense. Through PhyGenBench and PhyGenEval, we can\nconduct large-scale automated assessments of T2V models' understanding of\nphysical commonsense, which align closely with human feedback. Our evaluation\nresults and in-depth analysis demonstrate that current models struggle to\ngenerate videos that comply with physical commonsense. Moreover, simply scaling\nup models or employing prompt engineering techniques is insufficient to fully\naddress the challenges presented by PhyGenBench (e.g., dynamic scenarios). We\nhope this study will inspire the community to prioritize the learning of\nphysical commonsense in these models beyond entertainment applications. We will\nrelease the data and codes at https://github.com/OpenGVLab/PhyGenBench",
            "upvotes": 31,
            "discussionId": "67073024327cec6882e5d530"
        },
        "publishedAt": "2024-10-10T00:08:47.088Z",
        "title": "Towards World Simulator: Crafting Physical Commonsense-Based Benchmark for Video Generation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.05363.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "/avatars/c7bef45efad6a0d911a720e2236fcba5.svg",
            "fullname": "fanqing meng",
            "name": "FanqingM",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.07167",
            "authors": [
                {
                    "_id": "670748db49d08fabeb1ae5cd",
                    "user": {
                        "_id": "656f1b21b075b63c90ba02ee",
                        "avatarUrl": "/avatars/d6856815ef06261394178161e4d511b4.svg",
                        "isPro": false,
                        "fullname": "Huang Qidong",
                        "user": "shikiw",
                        "type": "user"
                    },
                    "name": "Qidong Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T08:37:24.039Z",
                    "hidden": false
                },
                {
                    "_id": "670748db49d08fabeb1ae5ce",
                    "name": "Xiaoyi Dong",
                    "hidden": false
                },
                {
                    "_id": "670748db49d08fabeb1ae5cf",
                    "name": "Pan Zhang",
                    "hidden": false
                },
                {
                    "_id": "670748db49d08fabeb1ae5d0",
                    "user": {
                        "_id": "63859cf3b2906edaf83af9f0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63859cf3b2906edaf83af9f0/iUQm5FAomzqYi6fkqIn9F.jpeg",
                        "isPro": false,
                        "fullname": "Yuhang Zang",
                        "user": "yuhangzang",
                        "type": "user"
                    },
                    "name": "Yuhang Zang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T08:38:00.537Z",
                    "hidden": false
                },
                {
                    "_id": "670748db49d08fabeb1ae5d1",
                    "name": "Yuhang Cao",
                    "hidden": false
                },
                {
                    "_id": "670748db49d08fabeb1ae5d2",
                    "name": "Jiaqi Wang",
                    "hidden": false
                },
                {
                    "_id": "670748db49d08fabeb1ae5d3",
                    "user": {
                        "_id": "636317ed80c1a705a6eff396",
                        "avatarUrl": "/avatars/3db090e101b916d9256d0d3e043db71d.svg",
                        "isPro": false,
                        "fullname": "Dahua Lin",
                        "user": "lindahua",
                        "type": "user"
                    },
                    "name": "Dahua Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T08:39:08.787Z",
                    "hidden": false
                },
                {
                    "_id": "670748db49d08fabeb1ae5d4",
                    "name": "Weiming Zhang",
                    "hidden": false
                },
                {
                    "_id": "670748db49d08fabeb1ae5d5",
                    "name": "Nenghai Yu",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-09T17:59:04.000Z",
            "title": "Deciphering Cross-Modal Alignment in Large Vision-Language Models with\n  Modality Integration Rate",
            "summary": "We present the Modality Integration Rate (MIR), an effective, robust, and\ngeneralized metric to indicate the multi-modal pre-training quality of Large\nVision Language Models (LVLMs). Large-scale pre-training plays a critical role\nin building capable LVLMs, while evaluating its training quality without the\ncostly supervised fine-tuning stage is under-explored. Loss, perplexity, and\nin-context evaluation results are commonly used pre-training metrics for Large\nLanguage Models (LLMs), while we observed that these metrics are less\nindicative when aligning a well-trained LLM with a new modality. Due to the\nlack of proper metrics, the research of LVLMs in the critical pre-training\nstage is hindered greatly, including the training data choice, efficient module\ndesign, etc. In this paper, we propose evaluating the pre-training quality from\nthe inter-modal distribution distance perspective and present MIR, the Modality\nIntegration Rate, which is 1) Effective to represent the pre-training\nquality and show a positive relation with the benchmark performance after\nsupervised fine-tuning. 2) Robust toward different training/evaluation\ndata. 3) Generalize across training configurations and architecture\nchoices. We conduct a series of pre-training experiments to explore the\neffectiveness of MIR and observe satisfactory results that MIR is indicative\nabout training data selection, training strategy schedule, and model\narchitecture design to get better pre-training results. We hope MIR could be a\nhelpful metric for building capable LVLMs and inspire the following research\nabout modality alignment in different areas. Our code is at:\nhttps://github.com/shikiw/Modality-Integration-Rate.",
            "upvotes": 29,
            "discussionId": "670748dc49d08fabeb1ae632"
        },
        "publishedAt": "2024-10-10T01:55:20.499Z",
        "title": "Deciphering Cross-Modal Alignment in Large Vision-Language Models with Modality Integration Rate",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.07167.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
            "fullname": "Jiaqi Wang",
            "name": "myownskyW7",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.07073",
            "authors": [
                {
                    "_id": "670746ee3e510db7639040f2",
                    "user": {
                        "_id": "66dfa475a36a3baebd55266b",
                        "avatarUrl": "/avatars/2c46210d159c200d4db9160fc5ecfe57.svg",
                        "isPro": false,
                        "fullname": "Pravesh Agrawal",
                        "user": "pragra",
                        "type": "user"
                    },
                    "name": "Pravesh Agrawal",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T08:44:28.232Z",
                    "hidden": false
                },
                {
                    "_id": "670746ee3e510db7639040f3",
                    "user": {
                        "_id": "63f767c4bd28622c9b984637",
                        "avatarUrl": "/avatars/126a23ca5b3adc89418b7a53b21e50e6.svg",
                        "isPro": false,
                        "fullname": "Szymon Antoniak",
                        "user": "Simontwice",
                        "type": "user"
                    },
                    "name": "Szymon Antoniak",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T08:44:33.689Z",
                    "hidden": false
                },
                {
                    "_id": "670746ee3e510db7639040f4",
                    "user": {
                        "_id": "655dc9138e56f09d55197cac",
                        "avatarUrl": "/avatars/764edb5c5e1003eb71cf6f1bea563f4b.svg",
                        "isPro": false,
                        "fullname": "Emma Bou Hanna",
                        "user": "EmmaBH",
                        "type": "user"
                    },
                    "name": "Emma Bou Hanna",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T08:44:39.014Z",
                    "hidden": false
                },
                {
                    "_id": "670746ee3e510db7639040f5",
                    "user": {
                        "_id": "65143e1c4f08b815c8db57a0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65143e1c4f08b815c8db57a0/JqkwKiJmLFRkH0NK3L8XH.jpeg",
                        "isPro": false,
                        "fullname": "Devendra Singh Chaplot",
                        "user": "devendrachaplot",
                        "type": "user"
                    },
                    "name": "Devendra Chaplot",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T08:44:44.258Z",
                    "hidden": false
                },
                {
                    "_id": "670746ee3e510db7639040f6",
                    "name": "Jessica Chudnovsky",
                    "hidden": false
                },
                {
                    "_id": "670746ee3e510db7639040f7",
                    "user": {
                        "_id": "64a8a87e4a702899835e9b45",
                        "avatarUrl": "/avatars/b6c60f63337a78a1bf3943885f4d7a18.svg",
                        "isPro": false,
                        "fullname": "Saurabh Garg",
                        "user": "saurabhgarg",
                        "type": "user"
                    },
                    "name": "Saurabh Garg",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T08:45:04.979Z",
                    "hidden": false
                },
                {
                    "_id": "670746ee3e510db7639040f8",
                    "user": {
                        "_id": "644e822cd6001776ed7700e4",
                        "avatarUrl": "/avatars/58a72e6bf8ede3cee3de54873231fd3e.svg",
                        "isPro": false,
                        "fullname": "Theophile Gervet",
                        "user": "TheophileGervet",
                        "type": "user"
                    },
                    "name": "Theophile Gervet",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T08:45:10.366Z",
                    "hidden": false
                },
                {
                    "_id": "670746ee3e510db7639040f9",
                    "user": {
                        "_id": "66bbbc31e314ecd9cc24cebc",
                        "avatarUrl": "/avatars/7f9b596a3a1c7243cea118423b61b97c.svg",
                        "isPro": false,
                        "fullname": "Soham Ghosh",
                        "user": "sohamghosh121",
                        "type": "user"
                    },
                    "name": "Soham Ghosh",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T08:45:27.823Z",
                    "hidden": false
                },
                {
                    "_id": "670746ee3e510db7639040fa",
                    "name": "Amélie Héliou",
                    "hidden": false
                },
                {
                    "_id": "670746ee3e510db7639040fb",
                    "name": "Paul Jacob",
                    "hidden": false
                },
                {
                    "_id": "670746ee3e510db7639040fc",
                    "user": {
                        "_id": "6137dff56ecbbda5c86755e9",
                        "avatarUrl": "/avatars/1aadefd3acb7ad24df15463dadceb7d4.svg",
                        "isPro": false,
                        "fullname": "Albert Q Jiang",
                        "user": "aqj213",
                        "type": "user"
                    },
                    "name": "Albert Q. Jiang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T08:45:46.987Z",
                    "hidden": false
                },
                {
                    "_id": "670746ee3e510db7639040fd",
                    "user": {
                        "_id": "6503364ceda77aafd4a68a48",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6503364ceda77aafd4a68a48/POQqon4zWQEpFTPzHS4uu.png",
                        "isPro": false,
                        "fullname": "timothee lacroix",
                        "user": "timlacroix",
                        "type": "user"
                    },
                    "name": "Timothée Lacroix",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T08:46:00.153Z",
                    "hidden": false
                },
                {
                    "_id": "670746ee3e510db7639040fe",
                    "user": {
                        "_id": "64882fc9508d0656d43dd5b3",
                        "avatarUrl": "/avatars/c3bc3656e4ff05663b54c6dd4f241891.svg",
                        "isPro": false,
                        "fullname": "Guillaume Lample",
                        "user": "glample",
                        "type": "user"
                    },
                    "name": "Guillaume Lample",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T08:46:05.767Z",
                    "hidden": false
                },
                {
                    "_id": "670746ee3e510db7639040ff",
                    "user": {
                        "_id": "657608c427c0e2fd6c894ced",
                        "avatarUrl": "/avatars/7f1ca5f75ca03cec69560584496558c9.svg",
                        "isPro": false,
                        "fullname": "Diego de las Casas",
                        "user": "diegolascasas",
                        "type": "user"
                    },
                    "name": "Diego Las Casas",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T08:46:11.532Z",
                    "hidden": false
                },
                {
                    "_id": "670746ee3e510db763904100",
                    "user": {
                        "_id": "66c705caf3f9994f2adf8c77",
                        "avatarUrl": "/avatars/e0322a12703693d10005113cbeed3207.svg",
                        "isPro": false,
                        "fullname": "Thibaut Lavril",
                        "user": "thibmistral",
                        "type": "user"
                    },
                    "name": "Thibaut Lavril",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T08:46:16.884Z",
                    "hidden": false
                },
                {
                    "_id": "670746ee3e510db763904101",
                    "user": {
                        "_id": "5e67bed6100906368940747b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1583858339980-5e67bed6100906368940747b.jpeg",
                        "isPro": false,
                        "fullname": "Teven Le Scao",
                        "user": "teven",
                        "type": "user"
                    },
                    "name": "Teven Le Scao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T08:46:22.313Z",
                    "hidden": false
                },
                {
                    "_id": "670746ee3e510db763904102",
                    "name": "Andy Lo",
                    "hidden": false
                },
                {
                    "_id": "670746ee3e510db763904103",
                    "name": "William Marshall",
                    "hidden": false
                },
                {
                    "_id": "670746ee3e510db763904104",
                    "name": "Louis Martin",
                    "hidden": false
                },
                {
                    "_id": "670746ee3e510db763904105",
                    "user": {
                        "_id": "645d28525ebf379fd6d9ae41",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645d28525ebf379fd6d9ae41/qVEKQk4gPlumsZE3ZorWG.jpeg",
                        "isPro": false,
                        "fullname": "Arthur Mensch",
                        "user": "arthurmensch",
                        "type": "user"
                    },
                    "name": "Arthur Mensch",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T08:47:16.994Z",
                    "hidden": false
                },
                {
                    "_id": "670746ee3e510db763904106",
                    "name": "Pavankumar Muddireddy",
                    "hidden": false
                },
                {
                    "_id": "670746ee3e510db763904107",
                    "user": {
                        "_id": "665efa6e8947302aa2bfc569",
                        "avatarUrl": "/avatars/58e044ad6ef6040a06b0fff4945ae988.svg",
                        "isPro": false,
                        "fullname": "Valera Nemychnikova",
                        "user": "sooobus",
                        "type": "user"
                    },
                    "name": "Valera Nemychnikova",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T08:47:32.137Z",
                    "hidden": false
                },
                {
                    "_id": "670746ee3e510db763904108",
                    "user": {
                        "_id": "62c69cd35aae1c624ca9e42c",
                        "avatarUrl": "/avatars/13824164766fbb06f7d45d6d38d96407.svg",
                        "isPro": false,
                        "fullname": "Marie Pellat",
                        "user": "mpellat",
                        "type": "user"
                    },
                    "name": "Marie Pellat",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T08:47:39.511Z",
                    "hidden": false
                },
                {
                    "_id": "670746ee3e510db763904109",
                    "user": {
                        "_id": "5dfcb1aada6d0311fd3d5448",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1584435275418-5dfcb1aada6d0311fd3d5448.jpeg",
                        "isPro": false,
                        "fullname": "Patrick von Platen",
                        "user": "patrickvonplaten",
                        "type": "user"
                    },
                    "name": "Patrick Von Platen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T08:47:46.655Z",
                    "hidden": false
                },
                {
                    "_id": "670746ee3e510db76390410a",
                    "name": "Nikhil Raghuraman",
                    "hidden": false
                },
                {
                    "_id": "670746ee3e510db76390410b",
                    "user": {
                        "_id": "62d063dac375d0c84255b9a1",
                        "avatarUrl": "/avatars/de0fc34bad8c761210c0895ebfa4feba.svg",
                        "isPro": false,
                        "fullname": "Baptiste Roziere",
                        "user": "broz",
                        "type": "user"
                    },
                    "name": "Baptiste Rozière",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T08:48:02.482Z",
                    "hidden": false
                },
                {
                    "_id": "670746ee3e510db76390410c",
                    "user": {
                        "_id": "6391c7456176fbc67b9ecd2c",
                        "avatarUrl": "/avatars/4aa9be94d8284118b8018362abf11f01.svg",
                        "isPro": false,
                        "fullname": "Alexandre Sablayrolles",
                        "user": "alexsablay",
                        "type": "user"
                    },
                    "name": "Alexandre Sablayrolles",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T08:48:09.427Z",
                    "hidden": false
                },
                {
                    "_id": "670746ee3e510db76390410d",
                    "user": {
                        "_id": "650cb0ebc705442b280160c4",
                        "avatarUrl": "/avatars/c5abfcddfa66505085178fa430779ed0.svg",
                        "isPro": false,
                        "fullname": "Lucile Saulnier",
                        "user": "LucileSaulnier",
                        "type": "user"
                    },
                    "name": "Lucile Saulnier",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T08:48:23.255Z",
                    "hidden": false
                },
                {
                    "_id": "670746ee3e510db76390410e",
                    "user": {
                        "_id": "66467938b7976a1d8d029b12",
                        "avatarUrl": "/avatars/7c4588aa2f50e67071b8f09cef13b005.svg",
                        "isPro": false,
                        "fullname": "Sauvestre",
                        "user": "romainsauvestre",
                        "type": "user"
                    },
                    "name": "Romain Sauvestre",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T08:48:38.293Z",
                    "hidden": false
                },
                {
                    "_id": "670746ee3e510db76390410f",
                    "user": {
                        "_id": "663d3aa7bc8177e039c17b3e",
                        "avatarUrl": "/avatars/b2ba474b94634472611105cf2968d077.svg",
                        "isPro": false,
                        "fullname": "Wendy Shang",
                        "user": "wendyshang",
                        "type": "user"
                    },
                    "name": "Wendy Shang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T08:48:44.673Z",
                    "hidden": false
                },
                {
                    "_id": "670746ee3e510db763904110",
                    "user": {
                        "_id": "6535a3d33da0ff3c70b6b3d2",
                        "avatarUrl": "/avatars/bc40dea410af9b2bd4881d7004eed9a7.svg",
                        "isPro": false,
                        "fullname": "Roman Soletskyi",
                        "user": "romansoletskyi",
                        "type": "user"
                    },
                    "name": "Roman Soletskyi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T08:49:05.189Z",
                    "hidden": false
                },
                {
                    "_id": "670746ee3e510db763904111",
                    "user": {
                        "_id": "664263de23fcdb3879e5bf36",
                        "avatarUrl": "/avatars/2a0f44b1a84c359c4b24c306c49f9e6a.svg",
                        "isPro": false,
                        "fullname": "Lawrence Stewart",
                        "user": "lmms",
                        "type": "user"
                    },
                    "name": "Lawrence Stewart",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T08:49:19.202Z",
                    "hidden": false
                },
                {
                    "_id": "670746ee3e510db763904112",
                    "user": {
                        "_id": "650c993d2751c84306aba92b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/650c993d2751c84306aba92b/uS04hveXAu6CEszCy-8_G.jpeg",
                        "isPro": false,
                        "fullname": "Pierre Stock",
                        "user": "pstock",
                        "type": "user"
                    },
                    "name": "Pierre Stock",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T08:49:26.789Z",
                    "hidden": false
                },
                {
                    "_id": "670746ee3e510db763904113",
                    "name": "Joachim Studnia",
                    "hidden": false
                },
                {
                    "_id": "670746ee3e510db763904114",
                    "user": {
                        "_id": "627bf27cf19c5eb46d54cea8",
                        "avatarUrl": "/avatars/b8ca0b4e841858c1d234671187234f56.svg",
                        "isPro": false,
                        "fullname": "Sandeep Subramanian",
                        "user": "MaximumEntropy",
                        "type": "user"
                    },
                    "name": "Sandeep Subramanian",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T08:49:37.492Z",
                    "hidden": false
                },
                {
                    "_id": "670746ee3e510db763904115",
                    "user": {
                        "_id": "643838e5c5a91b84ece168dd",
                        "avatarUrl": "/avatars/abebd42399decafbccc8579faa34e7d3.svg",
                        "isPro": false,
                        "fullname": "Sagar Vaze",
                        "user": "sgvaze",
                        "type": "user"
                    },
                    "name": "Sagar Vaze",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T08:49:44.074Z",
                    "hidden": false
                },
                {
                    "_id": "670746ee3e510db763904116",
                    "name": "Thomas Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-09T17:16:22.000Z",
            "title": "Pixtral 12B",
            "summary": "We introduce Pixtral-12B, a 12--billion-parameter multimodal language model.\nPixtral-12B is trained to understand both natural images and documents,\nachieving leading performance on various multimodal benchmarks, surpassing a\nnumber of larger models. Unlike many open-source models, Pixtral is also a\ncutting-edge text model for its size, and does not compromise on natural\nlanguage performance to excel in multimodal tasks. Pixtral uses a new vision\nencoder trained from scratch, which allows it to ingest images at their natural\nresolution and aspect ratio. This gives users flexibility on the number of\ntokens used to process an image. Pixtral is also able to process any number of\nimages in its long context window of 128K tokens. Pixtral 12B substanially\noutperforms other open models of similar sizes (Llama-3.2 11B \\& Qwen-2-VL 7B).\nIt also outperforms much larger open models like Llama-3.2 90B while being 7x\nsmaller. We further contribute an open-source benchmark, MM-MT-Bench, for\nevaluating vision-language models in practical scenarios, and provide detailed\nanalysis and code for standardized evaluation protocols for multimodal LLMs.\nPixtral-12B is released under Apache 2.0 license.",
            "upvotes": 29,
            "discussionId": "670746f33e510db7639042b6"
        },
        "publishedAt": "2024-10-10T01:46:46.993Z",
        "title": "Pixtral 12B",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.07073.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/uc2Q5G2HKphTD0TbOsYiC.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.06373",
            "authors": [
                {
                    "_id": "67073a7a95f90bdb8e86a0ff",
                    "name": "Siyuan Li",
                    "hidden": false
                },
                {
                    "_id": "67073a7a95f90bdb8e86a100",
                    "name": "Juanxi Tian",
                    "hidden": false
                },
                {
                    "_id": "67073a7a95f90bdb8e86a101",
                    "user": {
                        "_id": "6594d390674349122ce6f368",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6594d390674349122ce6f368/luDBiSMX_9l8QEpAQu3HJ.jpeg",
                        "isPro": false,
                        "fullname": "Zedong Wang (Jacky)",
                        "user": "ZedongWangAI",
                        "type": "user"
                    },
                    "name": "Zedong Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-10T08:01:29.377Z",
                    "hidden": false
                },
                {
                    "_id": "67073a7a95f90bdb8e86a102",
                    "name": "Luyuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "67073a7a95f90bdb8e86a103",
                    "user": {
                        "_id": "6569f6cb44ce94a70187f407",
                        "avatarUrl": "/avatars/414ecc9739ed23f9f395bd7cfa65055f.svg",
                        "isPro": false,
                        "fullname": "Zicheng Liu",
                        "user": "ZCLiu35",
                        "type": "user"
                    },
                    "name": "Zicheng Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T09:26:46.409Z",
                    "hidden": false
                },
                {
                    "_id": "67073a7a95f90bdb8e86a104",
                    "user": {
                        "_id": "66608add236f958513d21d2e",
                        "avatarUrl": "/avatars/53eca0891c98cbb93be899885160a983.svg",
                        "isPro": false,
                        "fullname": "Weiyang Jin",
                        "user": "SOTA-Owner",
                        "type": "user"
                    },
                    "name": "Weiyang Jin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T09:26:53.527Z",
                    "hidden": false
                },
                {
                    "_id": "67073a7a95f90bdb8e86a105",
                    "name": "Yang Liu",
                    "hidden": false
                },
                {
                    "_id": "67073a7a95f90bdb8e86a106",
                    "user": {
                        "_id": "64d9f1409a6a7ae9846ab8d1",
                        "avatarUrl": "/avatars/208b9d05b30f0094c3301936fabf4656.svg",
                        "isPro": false,
                        "fullname": "Baigui Sun",
                        "user": "sunbaigui",
                        "type": "user"
                    },
                    "name": "Baigui Sun",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T09:27:02.990Z",
                    "hidden": false
                },
                {
                    "_id": "67073a7a95f90bdb8e86a107",
                    "user": {
                        "_id": "64d187bf5de9e1e911d4777f",
                        "avatarUrl": "/avatars/0290d06655340ea57ec3db0b5f89455f.svg",
                        "isPro": false,
                        "fullname": "Stan Z. Li",
                        "user": "szli-0000",
                        "type": "user"
                    },
                    "name": "Stan Z. Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T09:27:08.710Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-08T21:14:23.000Z",
            "title": "Unveiling the Backbone-Optimizer Coupling Bias in Visual Representation\n  Learning",
            "summary": "This paper delves into the interplay between vision backbones and optimizers,\nunvealing an inter-dependent phenomenon termed\n\\textbf{backbone-optimizer coupling bias}\n(BOCB). We observe that canonical CNNs, such as VGG and ResNet, exhibit a\nmarked co-dependency with SGD families, while recent architectures like ViTs\nand ConvNeXt share a tight coupling with the adaptive learning rate ones. We\nfurther show that BOCB can be introduced by both optimizers and certain\nbackbone designs and may significantly impact the pre-training and downstream\nfine-tuning of vision models. Through in-depth empirical analysis, we summarize\ntakeaways on recommended optimizers and insights into robust vision backbone\narchitectures. We hope this work can inspire the community to question\nlong-held assumptions on backbones and optimizers, stimulate further\nexplorations, and thereby contribute to more robust vision systems. The source\ncode and models are publicly available at https://bocb-ai.github.io/.",
            "upvotes": 22,
            "discussionId": "67073a7b95f90bdb8e86a176"
        },
        "publishedAt": "2024-10-10T00:56:29.439Z",
        "title": "Unveiling the Backbone-Optimizer Coupling Bias in Visual Representation Learning",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.06373.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6594d390674349122ce6f368/luDBiSMX_9l8QEpAQu3HJ.jpeg",
            "fullname": "Zedong Wang (Jacky)",
            "name": "ZedongWangAI",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.05954",
            "authors": [
                {
                    "_id": "67074f465ea108eecd170d48",
                    "user": {
                        "_id": "64b79ec0a8c39dc078897430",
                        "avatarUrl": "/avatars/8339f3ab9bb7baaf69bf174eafb7282c.svg",
                        "isPro": false,
                        "fullname": "Yang Jin",
                        "user": "rain1011",
                        "type": "user"
                    },
                    "name": "Yang Jin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-10T10:59:07.115Z",
                    "hidden": false
                },
                {
                    "_id": "67074f465ea108eecd170d49",
                    "user": {
                        "_id": "62fc758172a7ab50b4b89c8c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677251053045-62fc758172a7ab50b4b89c8c.jpeg",
                        "isPro": false,
                        "fullname": "Zhicheng Sun",
                        "user": "feifeiobama",
                        "type": "user"
                    },
                    "name": "Zhicheng Sun",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-10T08:00:39.179Z",
                    "hidden": false
                },
                {
                    "_id": "67074f465ea108eecd170d4a",
                    "user": {
                        "_id": "66af935fa20def3de33f54ed",
                        "avatarUrl": "/avatars/2666e97165377d979c530d96a1089d86.svg",
                        "isPro": false,
                        "fullname": "Li Ningyuan",
                        "user": "Ninggggy",
                        "type": "user"
                    },
                    "name": "Ningyuan Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T08:55:43.710Z",
                    "hidden": false
                },
                {
                    "_id": "67074f465ea108eecd170d4b",
                    "name": "Kun Xu",
                    "hidden": false
                },
                {
                    "_id": "67074f465ea108eecd170d4c",
                    "name": "Kun Xu",
                    "hidden": false
                },
                {
                    "_id": "67074f465ea108eecd170d4d",
                    "name": "Hao Jiang",
                    "hidden": false
                },
                {
                    "_id": "67074f465ea108eecd170d4e",
                    "user": {
                        "_id": "6461e1ab5e410d3ff8ec84e4",
                        "avatarUrl": "/avatars/16d5fc2994993ab65e8e43798cd864c4.svg",
                        "isPro": false,
                        "fullname": "Nan Zhuang",
                        "user": "Payne53",
                        "type": "user"
                    },
                    "name": "Nan Zhuang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T08:55:08.855Z",
                    "hidden": false
                },
                {
                    "_id": "67074f465ea108eecd170d4f",
                    "user": {
                        "_id": "642fd69e8169f1dce23c0e64",
                        "avatarUrl": "/avatars/8dc14c7850b6274ab24906ea5b2e5e8a.svg",
                        "isPro": false,
                        "fullname": "Quzhe Huang",
                        "user": "quzhe",
                        "type": "user"
                    },
                    "name": "Quzhe Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T08:55:02.968Z",
                    "hidden": false
                },
                {
                    "_id": "67074f465ea108eecd170d50",
                    "name": "Yang Song",
                    "hidden": false
                },
                {
                    "_id": "67074f465ea108eecd170d51",
                    "name": "Yadong Mu",
                    "hidden": false
                },
                {
                    "_id": "67074f465ea108eecd170d52",
                    "name": "Zhouchen Lin",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-08T12:10:37.000Z",
            "title": "Pyramidal Flow Matching for Efficient Video Generative Modeling",
            "summary": "Video generation requires modeling a vast spatiotemporal space, which demands\nsignificant computational resources and data usage. To reduce the complexity,\nthe prevailing approaches employ a cascaded architecture to avoid direct\ntraining with full resolution. Despite reducing computational demands, the\nseparate optimization of each sub-stage hinders knowledge sharing and\nsacrifices flexibility. This work introduces a unified pyramidal flow matching\nalgorithm. It reinterprets the original denoising trajectory as a series of\npyramid stages, where only the final stage operates at the full resolution,\nthereby enabling more efficient video generative modeling. Through our\nsophisticated design, the flows of different pyramid stages can be interlinked\nto maintain continuity. Moreover, we craft autoregressive video generation with\na temporal pyramid to compress the full-resolution history. The entire\nframework can be optimized in an end-to-end manner and with a single unified\nDiffusion Transformer (DiT). Extensive experiments demonstrate that our method\nsupports generating high-quality 5-second (up to 10-second) videos at 768p\nresolution and 24 FPS within 20.7k A100 GPU training hours. All code and models\nwill be open-sourced at https://pyramid-flow.github.io.",
            "upvotes": 18,
            "discussionId": "67074f485ea108eecd170e1e"
        },
        "publishedAt": "2024-10-10T02:22:19.749Z",
        "title": "Pyramidal Flow Matching for Efficient Video Generative Modeling",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.05954.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677251053045-62fc758172a7ab50b4b89c8c.jpeg",
            "fullname": "Zhicheng Sun",
            "name": "feifeiobama",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.07177",
            "authors": [
                {
                    "_id": "67074fe8784e1c990108328a",
                    "user": {
                        "_id": "62d3ae4d894e7fe42def988f",
                        "avatarUrl": "/avatars/3aafc55d9783459f9a79546fc31dd68a.svg",
                        "isPro": false,
                        "fullname": "Hanrong Ye",
                        "user": "leoye",
                        "type": "user"
                    },
                    "name": "Hanrong Ye",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T08:56:29.882Z",
                    "hidden": false
                },
                {
                    "_id": "67074fe8784e1c990108328b",
                    "user": {
                        "_id": "631516348d85ad332fa47b2c",
                        "avatarUrl": "/avatars/100f5ae3cf3c52faaecdaecd5d8f2881.svg",
                        "isPro": false,
                        "fullname": "Haotian Zhang",
                        "user": "haotiz",
                        "type": "user"
                    },
                    "name": "Haotian Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-10T08:00:37.565Z",
                    "hidden": false
                },
                {
                    "_id": "67074fe8784e1c990108328c",
                    "user": {
                        "_id": "66150e514476535462a7166c",
                        "avatarUrl": "/avatars/75a0b6546dde9853a06c9536f067005d.svg",
                        "isPro": false,
                        "fullname": "Erik Daxberger",
                        "user": "edaxberger",
                        "type": "user"
                    },
                    "name": "Erik Daxberger",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T08:56:45.821Z",
                    "hidden": false
                },
                {
                    "_id": "67074fe8784e1c990108328d",
                    "name": "Lin Chen",
                    "hidden": false
                },
                {
                    "_id": "67074fe8784e1c990108328e",
                    "name": "Zongyu Lin",
                    "hidden": false
                },
                {
                    "_id": "67074fe8784e1c990108328f",
                    "user": {
                        "_id": "65dad3870af7e21ba473439f",
                        "avatarUrl": "/avatars/da542e7d68ae937bbdb791f17096bb1c.svg",
                        "isPro": false,
                        "fullname": "Yanghao Li",
                        "user": "FrozzZen",
                        "type": "user"
                    },
                    "name": "Yanghao Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T08:58:00.320Z",
                    "hidden": false
                },
                {
                    "_id": "67074fe8784e1c9901083290",
                    "name": "Bowen Zhang",
                    "hidden": false
                },
                {
                    "_id": "67074fe8784e1c9901083291",
                    "user": {
                        "_id": "65284ba3e3b3844f5e487a0b",
                        "avatarUrl": "/avatars/fce12a233bddb611d441f25a2ab17725.svg",
                        "isPro": false,
                        "fullname": "Haoxuan You",
                        "user": "HaoxuanYou",
                        "type": "user"
                    },
                    "name": "Haoxuan You",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T08:57:36.252Z",
                    "hidden": false
                },
                {
                    "_id": "67074fe8784e1c9901083292",
                    "user": {
                        "_id": "66feab48651e00e22f33222e",
                        "avatarUrl": "/avatars/7344377e2c796c7ec85194bb2fc78521.svg",
                        "isPro": false,
                        "fullname": "Dan Xu",
                        "user": "danxuhk",
                        "type": "user"
                    },
                    "name": "Dan Xu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T08:57:29.789Z",
                    "hidden": false
                },
                {
                    "_id": "67074fe8784e1c9901083293",
                    "user": {
                        "_id": "644022db21ec7c165d848b6e",
                        "avatarUrl": "/avatars/d9ed37ec77f2682878b2976a04d2fcd8.svg",
                        "isPro": false,
                        "fullname": "Zhe Gan",
                        "user": "zhegan27",
                        "type": "user"
                    },
                    "name": "Zhe Gan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T08:57:03.855Z",
                    "hidden": false
                },
                {
                    "_id": "67074fe8784e1c9901083294",
                    "user": {
                        "_id": "62b6b0397523238923221df9",
                        "avatarUrl": "/avatars/77068771dd51df7519516cd502a88789.svg",
                        "isPro": false,
                        "fullname": "Jiasenlu",
                        "user": "Jiasenlu",
                        "type": "user"
                    },
                    "name": "Jiasen Lu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T08:56:57.823Z",
                    "hidden": false
                },
                {
                    "_id": "67074fe8784e1c9901083295",
                    "user": {
                        "_id": "64b762568c632fbca942a405",
                        "avatarUrl": "/avatars/1eb737ec169967872f1ebf5ff29f1e6b.svg",
                        "isPro": false,
                        "fullname": "Yinfei Yang",
                        "user": "yinfeiy",
                        "type": "user"
                    },
                    "name": "Yinfei Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T08:56:52.265Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-09T17:59:59.000Z",
            "title": "MM-Ego: Towards Building Egocentric Multimodal LLMs",
            "summary": "This research aims to comprehensively explore building a multimodal\nfoundation model for egocentric video understanding. To achieve this goal, we\nwork on three fronts. First, as there is a lack of QA data for egocentric video\nunderstanding, we develop a data engine that efficiently generates 7M\nhigh-quality QA samples for egocentric videos ranging from 30 seconds to one\nhour long, based on human-annotated data. This is currently the largest\negocentric QA dataset. Second, we contribute a challenging egocentric QA\nbenchmark with 629 videos and 7,026 questions to evaluate the models' ability\nin recognizing and memorizing visual details across videos of varying lengths.\nWe introduce a new de-biasing evaluation method to help mitigate the\nunavoidable language bias present in the models being evaluated. Third, we\npropose a specialized multimodal architecture featuring a novel \"Memory Pointer\nPrompting\" mechanism. This design includes a global glimpse step to gain an\noverarching understanding of the entire video and identify key visual\ninformation, followed by a fallback step that utilizes the key visual\ninformation to generate responses. This enables the model to more effectively\ncomprehend extended video content. With the data, benchmark, and model, we\nsuccessfully build MM-Ego, an egocentric multimodal LLM that shows powerful\nperformance on egocentric video understanding.",
            "upvotes": 16,
            "discussionId": "67074fed784e1c9901083412"
        },
        "publishedAt": "2024-10-10T02:28:04.100Z",
        "title": "MM-Ego: Towards Building Egocentric Multimodal LLMs",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.07177.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/100f5ae3cf3c52faaecdaecd5d8f2881.svg",
            "fullname": "Haotian Zhang",
            "name": "haotiz",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.05355",
            "authors": [
                {
                    "_id": "6707619fd24201cd8c5d381b",
                    "user": {
                        "_id": "6460c3811db65f878513bcaf",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6460c3811db65f878513bcaf/CRdJ8lXixDku3k8Rm5Stn.jpeg",
                        "isPro": false,
                        "fullname": "Jingwei Zuo",
                        "user": "JingweiZuo",
                        "type": "user"
                    },
                    "name": "Jingwei Zuo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-10T08:00:20.786Z",
                    "hidden": false
                },
                {
                    "_id": "6707619fd24201cd8c5d381c",
                    "user": {
                        "_id": "64670db15993aa7666cc6022",
                        "avatarUrl": "/avatars/b68caad7e987c095b0cab4d9035aac25.svg",
                        "isPro": false,
                        "fullname": "Maksim Velikanov",
                        "user": "yellowvm",
                        "type": "user"
                    },
                    "name": "Maksim Velikanov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-10T08:00:15.197Z",
                    "hidden": false
                },
                {
                    "_id": "6707619fd24201cd8c5d381d",
                    "name": "Dhia Eddine Rhaiem",
                    "hidden": false
                },
                {
                    "_id": "6707619fd24201cd8c5d381e",
                    "name": "Ilyas Chahed",
                    "hidden": false
                },
                {
                    "_id": "6707619fd24201cd8c5d381f",
                    "user": {
                        "_id": "62441d1d9fdefb55a0b7d12c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1648631057413-noauth.png",
                        "isPro": false,
                        "fullname": "Younes Belkada",
                        "user": "ybelkada",
                        "type": "user"
                    },
                    "name": "Younes Belkada",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-10T08:00:24.878Z",
                    "hidden": false
                },
                {
                    "_id": "6707619fd24201cd8c5d3820",
                    "name": "Guillaume Kunsch",
                    "hidden": false
                },
                {
                    "_id": "6707619fd24201cd8c5d3821",
                    "name": "Hakim Hacid",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-07T15:40:45.000Z",
            "title": "Falcon Mamba: The First Competitive Attention-free 7B Language Model",
            "summary": "In this technical report, we present Falcon Mamba 7B, a new base large\nlanguage model based on the novel Mamba architecture. Falcon Mamba 7B is\ntrained on 5.8 trillion tokens with carefully selected data mixtures. As a pure\nMamba-based model, Falcon Mamba 7B surpasses leading open-weight models based\non Transformers, such as Mistral 7B, Llama3.1 8B, and Falcon2 11B. It is on par\nwith Gemma 7B and outperforms models with different architecture designs, such\nas RecurrentGemma 9B and RWKV-v6 Finch 7B/14B. Currently, Falcon Mamba 7B is\nthe best-performing Mamba model in the literature at this scale, surpassing\nboth existing Mamba and hybrid Mamba-Transformer models, according to the Open\nLLM Leaderboard. Due to its architecture, Falcon Mamba 7B is significantly\nfaster at inference and requires substantially less memory for long sequence\ngeneration. Despite recent studies suggesting that hybrid Mamba-Transformer\nmodels outperform pure architecture designs, we demonstrate that even the pure\nMamba design can achieve similar, or even superior results compared to the\nTransformer and hybrid designs. We make the weights of our implementation of\nFalcon Mamba 7B publicly available on\nhttps://huggingface.co/tiiuae/falcon-mamba-7b, under a permissive license.",
            "upvotes": 12,
            "discussionId": "670761a0d24201cd8c5d384a"
        },
        "publishedAt": "2024-10-10T06:02:52.822Z",
        "title": "Falcon Mamba: The First Competitive Attention-free 7B Language Model",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.05355.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/626237d9bbcbd1c34f1bb231/EJrOjvAL-68qMCYdnvOrq.png",
            "fullname": "Ali El Filali",
            "name": "alielfilali01",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.06244",
            "authors": [
                {
                    "_id": "670745bbd6f7382cc428eafc",
                    "user": {
                        "_id": "65f261116c60cd168b05433c",
                        "avatarUrl": "/avatars/0515bef9df2514707cbd3fc281891e2d.svg",
                        "isPro": false,
                        "fullname": "Jiawei Mao",
                        "user": "JohnWeck",
                        "type": "user"
                    },
                    "name": "Jiawei Mao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T09:01:11.990Z",
                    "hidden": false
                },
                {
                    "_id": "670745bbd6f7382cc428eafd",
                    "user": {
                        "_id": "63318b2349a9563915469f3b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63318b2349a9563915469f3b/zlbeB2997i8YkoyOTb9FL.jpeg",
                        "isPro": false,
                        "fullname": "Xiaoke Huang",
                        "user": "xk-huang",
                        "type": "user"
                    },
                    "name": "Xiaoke Huang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-10T08:01:17.167Z",
                    "hidden": false
                },
                {
                    "_id": "670745bbd6f7382cc428eafe",
                    "name": "Yunfei Xie",
                    "hidden": false
                },
                {
                    "_id": "670745bbd6f7382cc428eaff",
                    "name": "Yuanqi Chang",
                    "hidden": false
                },
                {
                    "_id": "670745bbd6f7382cc428eb00",
                    "user": {
                        "_id": "659f6f0de06dc8fc0e411224",
                        "avatarUrl": "/avatars/e16170f36b949c8697f277f6b52c51c3.svg",
                        "isPro": false,
                        "fullname": "MudeHui",
                        "user": "MudeHui",
                        "type": "user"
                    },
                    "name": "Mude Hui",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T09:01:57.443Z",
                    "hidden": false
                },
                {
                    "_id": "670745bbd6f7382cc428eb01",
                    "name": "Bingjie Xu",
                    "hidden": false
                },
                {
                    "_id": "670745bbd6f7382cc428eb02",
                    "user": {
                        "_id": "66c7fb4ce2c92fe5b132f314",
                        "avatarUrl": "/avatars/22d915fa339a70803c5c748255250256.svg",
                        "isPro": false,
                        "fullname": "Yuyin Zhou",
                        "user": "RitaCoding",
                        "type": "user"
                    },
                    "name": "Yuyin Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T09:02:11.011Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-08T17:59:30.000Z",
            "title": "Story-Adapter: A Training-free Iterative Framework for Long Story\n  Visualization",
            "summary": "Story visualization, the task of generating coherent images based on a\nnarrative, has seen significant advancements with the emergence of\ntext-to-image models, particularly diffusion models. However, maintaining\nsemantic consistency, generating high-quality fine-grained interactions, and\nensuring computational feasibility remain challenging, especially in long story\nvisualization (i.e., up to 100 frames). In this work, we propose a\ntraining-free and computationally efficient framework, termed Story-Adapter, to\nenhance the generative capability of long stories. Specifically, we propose an\niterative paradigm to refine each generated image, leveraging both the text\nprompt and all generated images from the previous iteration. Central to our\nframework is a training-free global reference cross-attention module, which\naggregates all generated images from the previous iteration to preserve\nsemantic consistency across the entire story, while minimizing computational\ncosts with global embeddings. This iterative process progressively optimizes\nimage generation by repeatedly incorporating text constraints, resulting in\nmore precise and fine-grained interactions. Extensive experiments validate the\nsuperiority of Story-Adapter in improving both semantic consistency and\ngenerative capability for fine-grained interactions, particularly in long story\nscenarios. The project page and associated code can be accessed via\nhttps://jwmao1.github.io/storyadapter .",
            "upvotes": 12,
            "discussionId": "670745c0d6f7382cc428ec6c"
        },
        "publishedAt": "2024-10-10T01:42:13.076Z",
        "title": "Story-Adapter: A Training-free Iterative Framework for Long Story Visualization",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.06244.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63318b2349a9563915469f3b/zlbeB2997i8YkoyOTb9FL.jpeg",
            "fullname": "Xiaoke Huang",
            "name": "xk-huang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.06961",
            "authors": [
                {
                    "_id": "6707453f257e19aee32b0a15",
                    "user": {
                        "_id": "670740744341dcee459fb990",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/66UkZvrAk7fQr5YCylEFk.png",
                        "isPro": false,
                        "fullname": "Qingxiu Dong",
                        "user": "Rsy24",
                        "type": "user"
                    },
                    "name": "Qingxiu Dong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T09:02:41.139Z",
                    "hidden": false
                },
                {
                    "_id": "6707453f257e19aee32b0a16",
                    "user": {
                        "_id": "5df85abada6d0311fd3d5408",
                        "avatarUrl": "/avatars/2331cf703c1b5d3a62e2050b1a6eb108.svg",
                        "isPro": false,
                        "fullname": "Li Dong",
                        "user": "unilm",
                        "type": "user"
                    },
                    "name": "Li Dong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T14:09:20.654Z",
                    "hidden": false
                },
                {
                    "_id": "6707453f257e19aee32b0a17",
                    "user": {
                        "_id": "64abbcff6cadc7aca584f71b",
                        "avatarUrl": "/avatars/fc6e85ad4a8befd133a37b411712c648.svg",
                        "isPro": false,
                        "fullname": "Xingxing Zhang",
                        "user": "THU-CHUNXIA",
                        "type": "user"
                    },
                    "name": "Xingxing Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T09:03:01.763Z",
                    "hidden": false
                },
                {
                    "_id": "6707453f257e19aee32b0a18",
                    "name": "Zhifang Sui",
                    "hidden": false
                },
                {
                    "_id": "6707453f257e19aee32b0a19",
                    "user": {
                        "_id": "6368c512fbfe97c16a40baba",
                        "avatarUrl": "/avatars/1c23bc7c0b6d9225699ce27647623d7a.svg",
                        "isPro": false,
                        "fullname": "Furu Wei",
                        "user": "thegenerality",
                        "type": "user"
                    },
                    "name": "Furu Wei",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T09:03:14.088Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-09T14:57:31.000Z",
            "title": "Self-Boosting Large Language Models with Synthetic Preference Data",
            "summary": "Through alignment with human preferences, Large Language Models (LLMs) have\nadvanced significantly in generating honest, harmless, and helpful responses.\nHowever, collecting high-quality preference data is a resource-intensive and\ncreativity-demanding process, especially for the continual improvement of LLMs.\nWe introduce SynPO, a self-boosting paradigm that leverages synthetic\npreference data for model alignment. SynPO employs an iterative mechanism\nwherein a self-prompt generator creates diverse prompts, and a response\nimprover refines model responses progressively. This approach trains LLMs to\nautonomously learn the generative rewards for their own outputs and eliminates\nthe need for large-scale annotation of prompts and human preferences. After\nfour SynPO iterations, Llama3-8B and Mistral-7B show significant enhancements\nin instruction-following abilities, achieving over 22.1% win rate improvements\non AlpacaEval 2.0 and ArenaHard. Simultaneously, SynPO improves the general\nperformance of LLMs on various tasks, validated by a 3.2 to 5.0 average score\nincrease on the well-recognized Open LLM leaderboard.",
            "upvotes": 12,
            "discussionId": "67074540257e19aee32b0a4c"
        },
        "publishedAt": "2024-10-10T01:39:09.264Z",
        "title": "Self-Boosting Large Language Models with Synthetic Preference Data",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.06961.png",
        "numComments": 0,
        "submittedBy": {
            "avatarUrl": "/avatars/d17e4a4b467ef9019594036ed8f1ca6e.svg",
            "fullname": "W",
            "name": "Windy",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.07170",
            "authors": [
                {
                    "_id": "6707636dbe29abe3768fc949",
                    "user": {
                        "_id": "648826f845a9218318e0272c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648826f845a9218318e0272c/1zUQFA7TdQ8WC9Chskx6b.jpeg",
                        "isPro": false,
                        "fullname": "Fabian Paischer",
                        "user": "paischer101",
                        "type": "user"
                    },
                    "name": "Fabian Paischer",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-10T08:00:13.044Z",
                    "hidden": false
                },
                {
                    "_id": "6707636dbe29abe3768fc94a",
                    "user": {
                        "_id": "6041ff7ff84ebe399f1c85ea",
                        "avatarUrl": "/avatars/a5e2306f3cd27e0ea1c30eeb81f870fa.svg",
                        "isPro": false,
                        "fullname": "Lukas",
                        "user": "sirluk",
                        "type": "user"
                    },
                    "name": "Lukas Hauzenberger",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-10T08:58:29.988Z",
                    "hidden": false
                },
                {
                    "_id": "6707636dbe29abe3768fc94b",
                    "user": {
                        "_id": "64c3849269b1a6796052eac7",
                        "avatarUrl": "/avatars/9f0c832d5b51b659c7bb83074f02a648.svg",
                        "isPro": false,
                        "fullname": "Thomas Schmied",
                        "user": "thomasschmied",
                        "type": "user"
                    },
                    "name": "Thomas Schmied",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T09:14:26.041Z",
                    "hidden": false
                },
                {
                    "_id": "6707636dbe29abe3768fc94c",
                    "name": "Benedikt Alkin",
                    "hidden": false
                },
                {
                    "_id": "6707636dbe29abe3768fc94d",
                    "name": "Marc Peter Deisenroth",
                    "hidden": false
                },
                {
                    "_id": "6707636dbe29abe3768fc94e",
                    "name": "Sepp Hochreiter",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-09T17:59:06.000Z",
            "title": "One Initialization to Rule them All: Fine-tuning via Explained Variance\n  Adaptation",
            "summary": "Foundation models (FMs) are pre-trained on large-scale datasets and then\nfine-tuned on a downstream task for a specific application. The most successful\nand most commonly used fine-tuning method is to update the pre-trained weights\nvia a low-rank adaptation (LoRA). LoRA introduces new weight matrices that are\nusually initialized at random with a uniform rank distribution across model\nweights. Recent works focus on weight-driven initialization or learning of\nadaptive ranks during training. Both approaches have only been investigated in\nisolation, resulting in slow convergence or a uniform rank distribution, in\nturn leading to sub-optimal performance. We propose to enhance LoRA by\ninitializing the new weights in a data-driven manner by computing singular\nvalue decomposition on minibatches of activation vectors. Then, we initialize\nthe LoRA matrices with the obtained right-singular vectors and re-distribute\nranks among all weight matrices to explain the maximal amount of variance and\ncontinue the standard LoRA fine-tuning procedure. This results in our new\nmethod Explained Variance Adaptation (EVA). We apply EVA to a variety of\nfine-tuning tasks ranging from language generation and understanding to image\nclassification and reinforcement learning. EVA exhibits faster convergence than\ncompetitors and attains the highest average score across a multitude of tasks\nper domain.",
            "upvotes": 11,
            "discussionId": "6707636ebe29abe3768fc9cd"
        },
        "publishedAt": "2024-10-10T03:49:39.452Z",
        "title": "One Initialization to Rule them All: Fine-tuning via Explained Variance Adaptation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.07170.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648826f845a9218318e0272c/1zUQFA7TdQ8WC9Chskx6b.jpeg",
            "fullname": "Fabian Paischer",
            "name": "paischer101",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.05591",
            "authors": [
                {
                    "_id": "670759004341dcee45a674a6",
                    "user": {
                        "_id": "66c68b917f4accbaaaaff6cc",
                        "avatarUrl": "/avatars/40f1a25c393f3cb2e7ae906bb954edac.svg",
                        "isPro": false,
                        "fullname": "Gihyun Kwon",
                        "user": "gkwon",
                        "type": "user"
                    },
                    "name": "Gihyun Kwon",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2024-10-10T04:36:37.757Z",
                    "hidden": false
                },
                {
                    "_id": "670759004341dcee45a674a7",
                    "name": "Jong Chul Ye",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-08T01:06:01.000Z",
            "title": "TweedieMix: Improving Multi-Concept Fusion for Diffusion-based\n  Image/Video Generation",
            "summary": "Despite significant advancements in customizing text-to-image and video\ngeneration models, generating images and videos that effectively integrate\nmultiple personalized concepts remains a challenging task. To address this, we\npresent TweedieMix, a novel method for composing customized diffusion models\nduring the inference phase. By analyzing the properties of reverse diffusion\nsampling, our approach divides the sampling process into two stages. During the\ninitial steps, we apply a multiple object-aware sampling technique to ensure\nthe inclusion of the desired target objects. In the later steps, we blend the\nappearances of the custom concepts in the de-noised image space using Tweedie's\nformula. Our results demonstrate that TweedieMix can generate multiple\npersonalized concepts with higher fidelity than existing methods. Moreover, our\nframework can be effortlessly extended to image-to-video diffusion models,\nenabling the generation of videos that feature multiple personalized concepts.\nResults and source code are in our anonymous project page.",
            "upvotes": 11,
            "discussionId": "670759024341dcee45a67507"
        },
        "publishedAt": "2024-10-10T03:03:18.042Z",
        "title": "TweedieMix: Improving Multi-Concept Fusion for Diffusion-based Image/Video Generation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.05591.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.06166",
            "authors": [
                {
                    "_id": "6707388ec818ba2f6579eb78",
                    "user": {
                        "_id": "6038d6d0612f5eef3cc05ea9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6038d6d0612f5eef3cc05ea9/ryhvAX5djQpD5OrIlZQ1f.jpeg",
                        "isPro": false,
                        "fullname": "Lei Li",
                        "user": "tobiaslee",
                        "type": "user"
                    },
                    "name": "Lei Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T09:11:14.571Z",
                    "hidden": false
                },
                {
                    "_id": "6707388ec818ba2f6579eb79",
                    "user": {
                        "_id": "6489761dcaea79f577897f98",
                        "avatarUrl": "/avatars/8f56dc9c08dc2b672555602d68509a03.svg",
                        "isPro": false,
                        "fullname": "Yuanxin Liu",
                        "user": "lyx97",
                        "type": "user"
                    },
                    "name": "Yuanxin Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-10T08:01:34.232Z",
                    "hidden": false
                },
                {
                    "_id": "6707388ec818ba2f6579eb7a",
                    "user": {
                        "_id": "655ca347f426a304c6b393a1",
                        "avatarUrl": "/avatars/67f0310d59c5912d38c2ad8e6448614d.svg",
                        "isPro": false,
                        "fullname": "Linli Yao",
                        "user": "yaolily",
                        "type": "user"
                    },
                    "name": "Linli Yao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T09:11:21.755Z",
                    "hidden": false
                },
                {
                    "_id": "6707388ec818ba2f6579eb7b",
                    "user": {
                        "_id": "63565cc56d7fcf1bedb7d347",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63565cc56d7fcf1bedb7d347/XGcHP4VkO_oieA1gZ4IAX.jpeg",
                        "isPro": false,
                        "fullname": "Zhang Peiyuan",
                        "user": "PY007",
                        "type": "user"
                    },
                    "name": "Peiyuan Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T09:11:32.678Z",
                    "hidden": false
                },
                {
                    "_id": "6707388ec818ba2f6579eb7c",
                    "user": {
                        "_id": "64acb321264bbbf171a2b040",
                        "avatarUrl": "/avatars/0ad344c0e9b1e3fda469932f91d117dc.svg",
                        "isPro": false,
                        "fullname": "Chenxin An",
                        "user": "Chancy",
                        "type": "user"
                    },
                    "name": "Chenxin An",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T09:11:37.979Z",
                    "hidden": false
                },
                {
                    "_id": "6707388ec818ba2f6579eb7d",
                    "user": {
                        "_id": "650c509472afb1e60e6151ae",
                        "avatarUrl": "/avatars/c16ab5053a586819dc2b965303215ff7.svg",
                        "isPro": false,
                        "fullname": "Lean Wang",
                        "user": "AdaHousman",
                        "type": "user"
                    },
                    "name": "Lean Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T09:11:43.776Z",
                    "hidden": false
                },
                {
                    "_id": "6707388ec818ba2f6579eb7e",
                    "name": "Xu Sun",
                    "hidden": false
                },
                {
                    "_id": "6707388ec818ba2f6579eb7f",
                    "name": "Lingpeng Kong",
                    "hidden": false
                },
                {
                    "_id": "6707388ec818ba2f6579eb80",
                    "user": {
                        "_id": "63ad3de96ee60ca58a409280",
                        "avatarUrl": "/avatars/7461f4fda3692f042e556d2a7c339bc0.svg",
                        "isPro": false,
                        "fullname": "Qi Liu",
                        "user": "QiLiuHKU",
                        "type": "user"
                    },
                    "name": "Qi Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T09:12:34.540Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-08T16:10:29.000Z",
            "title": "Temporal Reasoning Transfer from Text to Video",
            "summary": "Video Large Language Models (Video LLMs) have shown promising capabilities in\nvideo comprehension, yet they struggle with tracking temporal changes and\nreasoning about temporal relationships. While previous research attributed this\nlimitation to the ineffective temporal encoding of visual inputs, our\ndiagnostic study reveals that video representations contain sufficient\ninformation for even small probing classifiers to achieve perfect accuracy.\nSurprisingly, we find that the key bottleneck in Video LLMs' temporal reasoning\ncapability stems from the underlying LLM's inherent difficulty with temporal\nconcepts, as evidenced by poor performance on textual temporal\nquestion-answering tasks. Building on this discovery, we introduce the Textual\nTemporal reasoning Transfer (T3). T3 synthesizes diverse temporal reasoning\ntasks in pure text format from existing image-text datasets, addressing the\nscarcity of video samples with complex temporal scenarios. Remarkably, without\nusing any video data, T3 enhances LongVA-7B's temporal understanding, yielding\na 5.3 absolute accuracy improvement on the challenging TempCompass benchmark,\nwhich enables our model to outperform ShareGPT4Video-8B trained on 28,000 video\nsamples. Additionally, the enhanced LongVA-7B model achieves competitive\nperformance on comprehensive video benchmarks. For example, it achieves a 49.7\naccuracy on the Temporal Reasoning task of Video-MME, surpassing powerful\nlarge-scale models such as InternVL-Chat-V1.5-20B and VILA1.5-40B. Further\nanalysis reveals a strong correlation between textual and video temporal task\nperformance, validating the efficacy of transferring temporal reasoning\nabilities from text to video domains.",
            "upvotes": 10,
            "discussionId": "6707388fc818ba2f6579ebd1"
        },
        "publishedAt": "2024-10-10T00:45:02.114Z",
        "title": "Temporal Reasoning Transfer from Text to Video",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.06166.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6038d6d0612f5eef3cc05ea9/ryhvAX5djQpD5OrIlZQ1f.jpeg",
            "fullname": "Lei Li",
            "name": "tobiaslee",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.07002",
            "authors": [
                {
                    "_id": "6707611c392cba5330ea4abc",
                    "user": {
                        "_id": "65097423e64ee37323bd2def",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65097423e64ee37323bd2def/PTEwbfafNI88gdX1VVmIn.jpeg",
                        "isPro": false,
                        "fullname": "Hao Jiang",
                        "user": "TechxGenus",
                        "type": "user"
                    },
                    "name": "Hao Jiang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-10T08:00:26.810Z",
                    "hidden": false
                },
                {
                    "_id": "6707611c392cba5330ea4abd",
                    "name": "Qi Liu",
                    "hidden": false
                },
                {
                    "_id": "6707611c392cba5330ea4abe",
                    "name": "Rui Li",
                    "hidden": false
                },
                {
                    "_id": "6707611c392cba5330ea4abf",
                    "user": {
                        "_id": "6508091067ab943749f9e869",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6508091067ab943749f9e869/TpdHfIez3rj5wQqrruA7G.jpeg",
                        "isPro": false,
                        "fullname": "Shengyu Ye",
                        "user": "Eviloder",
                        "type": "user"
                    },
                    "name": "Shengyu Ye",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T09:27:38.637Z",
                    "hidden": false
                },
                {
                    "_id": "6707611c392cba5330ea4ac0",
                    "name": "Shijin Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-09T15:45:52.000Z",
            "title": "CursorCore: Assist Programming through Aligning Anything",
            "summary": "Large language models have been successfully applied to programming\nassistance tasks, such as code completion, code insertion, and instructional\ncode editing. However, these applications remain insufficiently automated and\nstruggle to effectively integrate various types of information during the\nprogramming process, including coding history, current code, and user\ninstructions. In this work, we propose a new conversational framework that\ncomprehensively integrates these information sources, collect data to train our\nmodels and evaluate their performance. Firstly, to thoroughly evaluate how well\nmodels align with different types of information and the quality of their\noutputs, we introduce a new benchmark, APEval (Assist Programming Eval), to\ncomprehensively assess the performance of models in programming assistance\ntasks. Then, for data collection, we develop a data generation pipeline,\nProgramming-Instruct, which synthesizes training data from diverse sources,\nsuch as GitHub and online judge platforms. This pipeline can automatically\ngenerate various types of messages throughout the programming process. Finally,\nusing this pipeline, we generate 219K samples, fine-tune multiple models, and\ndevelop the CursorCore series. We show that CursorCore outperforms other models\nof comparable size. This framework unifies applications such as inline chat and\nautomated editing, contributes to the advancement of coding assistants. Code,\nmodels and data are freely available at\nhttps://github.com/TechxGenus/CursorCore.",
            "upvotes": 9,
            "discussionId": "6707611c392cba5330ea4afe"
        },
        "publishedAt": "2024-10-10T05:11:19.637Z",
        "title": "CursorCore: Assist Programming through Aligning Anything",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.07002.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.05295",
            "authors": [
                {
                    "_id": "67076e9cbbfdd3d3074a61fb",
                    "user": {
                        "_id": "631df96d2899cb403efcf5be",
                        "avatarUrl": "/avatars/50db06ef7a6efc7b2a525e1c90e576f7.svg",
                        "isPro": false,
                        "fullname": "Xiaogeng Liu",
                        "user": "ShletonLiu-N",
                        "type": "user"
                    },
                    "name": "Xiaogeng Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T09:49:57.484Z",
                    "hidden": false
                },
                {
                    "_id": "67076e9cbbfdd3d3074a61fc",
                    "user": {
                        "_id": "648a446f28e95179adbf590b",
                        "avatarUrl": "/avatars/30f5328f04fd2f2f540f6a90a3204e86.svg",
                        "isPro": false,
                        "fullname": "Peiran L",
                        "user": "peiranli0930",
                        "type": "user"
                    },
                    "name": "Peiran Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T09:54:14.164Z",
                    "hidden": false
                },
                {
                    "_id": "67076e9cbbfdd3d3074a61fd",
                    "name": "Edward Suh",
                    "hidden": false
                },
                {
                    "_id": "67076e9cbbfdd3d3074a61fe",
                    "name": "Yevgeniy Vorobeychik",
                    "hidden": false
                },
                {
                    "_id": "67076e9cbbfdd3d3074a61ff",
                    "name": "Zhuoqing Mao",
                    "hidden": false
                },
                {
                    "_id": "67076e9cbbfdd3d3074a6200",
                    "user": {
                        "_id": "65da87db3d6199e3fa76b141",
                        "avatarUrl": "/avatars/7eb6ee3e954f85b9328496c97ab82393.svg",
                        "isPro": false,
                        "fullname": "Somesh Jha",
                        "user": "someshjha",
                        "type": "user"
                    },
                    "name": "Somesh Jha",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T09:50:13.799Z",
                    "hidden": false
                },
                {
                    "_id": "67076e9cbbfdd3d3074a6201",
                    "name": "Patrick McDaniel",
                    "hidden": false
                },
                {
                    "_id": "67076e9cbbfdd3d3074a6202",
                    "user": {
                        "_id": "6127b327699a5d2cde0b44dc",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1629991702069-noauth.png",
                        "isPro": false,
                        "fullname": "Huan Sun",
                        "user": "huansun",
                        "type": "user"
                    },
                    "name": "Huan Sun",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T09:53:19.015Z",
                    "hidden": false
                },
                {
                    "_id": "67076e9cbbfdd3d3074a6203",
                    "name": "Bo Li",
                    "hidden": false
                },
                {
                    "_id": "67076e9cbbfdd3d3074a6204",
                    "name": "Chaowei Xiao",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-03T17:59:01.000Z",
            "title": "AutoDAN-Turbo: A Lifelong Agent for Strategy Self-Exploration to\n  Jailbreak LLMs",
            "summary": "In this paper, we propose AutoDAN-Turbo, a black-box jailbreak method that\ncan automatically discover as many jailbreak strategies as possible from\nscratch, without any human intervention or predefined scopes (e.g., specified\ncandidate strategies), and use them for red-teaming. As a result, AutoDAN-Turbo\ncan significantly outperform baseline methods, achieving a 74.3% higher average\nattack success rate on public benchmarks. Notably, AutoDAN-Turbo achieves an\n88.5 attack success rate on GPT-4-1106-turbo. In addition, AutoDAN-Turbo is a\nunified framework that can incorporate existing human-designed jailbreak\nstrategies in a plug-and-play manner. By integrating human-designed strategies,\nAutoDAN-Turbo can even achieve a higher attack success rate of 93.4 on\nGPT-4-1106-turbo.",
            "upvotes": 9,
            "discussionId": "67076e9dbbfdd3d3074a629e"
        },
        "publishedAt": "2024-10-10T04:35:20.299Z",
        "title": "AutoDAN-Turbo: A Lifelong Agent for Strategy Self-Exploration to Jailbreak LLMs",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.05295.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.05651",
            "authors": [
                {
                    "_id": "6707511a95f90bdb8e8dfc5e",
                    "user": {
                        "_id": "63a2b7f9cc214062058e5a81",
                        "avatarUrl": "/avatars/be49a6e3aa6e4a3ad8800858d9a82ef1.svg",
                        "isPro": false,
                        "fullname": "Serin Yang",
                        "user": "sr2851766",
                        "type": "user"
                    },
                    "name": "Serin Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T09:49:23.096Z",
                    "hidden": false
                },
                {
                    "_id": "6707511a95f90bdb8e8dfc5f",
                    "name": "Taesung Kwon",
                    "hidden": false
                },
                {
                    "_id": "6707511a95f90bdb8e8dfc60",
                    "name": "Jong Chul Ye",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-08T03:01:54.000Z",
            "title": "ViBiDSampler: Enhancing Video Interpolation Using Bidirectional\n  Diffusion Sampler",
            "summary": "Recent progress in large-scale text-to-video (T2V) and image-to-video (I2V)\ndiffusion models has greatly enhanced video generation, especially in terms of\nkeyframe interpolation. However, current image-to-video diffusion models, while\npowerful in generating videos from a single conditioning frame, need adaptation\nfor two-frame (start & end) conditioned generation, which is essential for\neffective bounded interpolation. Unfortunately, existing approaches that fuse\ntemporally forward and backward paths in parallel often suffer from\noff-manifold issues, leading to artifacts or requiring multiple iterative\nre-noising steps. In this work, we introduce a novel, bidirectional sampling\nstrategy to address these off-manifold issues without requiring extensive\nre-noising or fine-tuning. Our method employs sequential sampling along both\nforward and backward paths, conditioned on the start and end frames,\nrespectively, ensuring more coherent and on-manifold generation of intermediate\nframes. Additionally, we incorporate advanced guidance techniques, CFG++ and\nDDS, to further enhance the interpolation process. By integrating these, our\nmethod achieves state-of-the-art performance, efficiently generating\nhigh-quality, smooth videos between keyframes. On a single 3090 GPU, our method\ncan interpolate 25 frames at 1024 x 576 resolution in just 195 seconds,\nestablishing it as a leading solution for keyframe interpolation.",
            "upvotes": 9,
            "discussionId": "6707511c95f90bdb8e8dfd29"
        },
        "publishedAt": "2024-10-10T02:30:29.545Z",
        "title": "ViBiDSampler: Enhancing Video Interpolation Using Bidirectional Diffusion Sampler",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.05651.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.05643",
            "authors": [
                {
                    "_id": "67073b83d4311c2dd68fe1f1",
                    "user": {
                        "_id": "652c9bdf4d0eef4a2b3c577f",
                        "avatarUrl": "/avatars/6e92eee19ae2c43e46a8b294c02f9915.svg",
                        "isPro": false,
                        "fullname": "Yongxin Guo",
                        "user": "Yongxin-Guo",
                        "type": "user"
                    },
                    "name": "Yongxin Guo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-10T08:01:22.374Z",
                    "hidden": false
                },
                {
                    "_id": "67073b83d4311c2dd68fe1f2",
                    "name": "Jingyu Liu",
                    "hidden": false
                },
                {
                    "_id": "67073b83d4311c2dd68fe1f3",
                    "name": "Mingda Li",
                    "hidden": false
                },
                {
                    "_id": "67073b83d4311c2dd68fe1f4",
                    "name": "Xiaoying Tang",
                    "hidden": false
                },
                {
                    "_id": "67073b83d4311c2dd68fe1f5",
                    "user": {
                        "_id": "6707481116abdf6976c68421",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/yEoJCremrGNoZrnI-xOOU.png",
                        "isPro": false,
                        "fullname": "qingbinliu",
                        "user": "qingbinliu",
                        "type": "user"
                    },
                    "name": "Qingbin Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T09:16:10.450Z",
                    "hidden": false
                },
                {
                    "_id": "67073b83d4311c2dd68fe1f6",
                    "name": "Xi Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-08T02:46:30.000Z",
            "title": "TRACE: Temporal Grounding Video LLM via Causal Event Modeling",
            "summary": "Video Temporal Grounding (VTG) is a crucial capability for video\nunderstanding models and plays a vital role in downstream tasks such as video\nbrowsing and editing. To effectively handle various tasks simultaneously and\nenable zero-shot prediction, there is a growing trend in employing video LLMs\nfor VTG tasks. However, current video LLM-based methods rely exclusively on\nnatural language generation, lacking the ability to model the clear structure\ninherent in videos, which restricts their effectiveness in tackling VTG tasks.\nTo address this issue, this paper first formally introduces causal event\nmodeling framework, which represents videos as sequences of events, and predict\nthe current event using previous events, video inputs, and textural\ninstructions. Each event consists of three components: timestamps, salient\nscores, and textual captions. We then propose a novel task-interleaved video\nLLM called TRACE to effectively implement the causal event modeling framework\nin practice. The TRACE processes visual frames, timestamps, salient scores, and\ntext as distinct tasks, employing various encoders and decoding heads for each.\nTask tokens are arranged in an interleaved sequence according to the causal\nevent modeling framework's formulation. Extensive experiments on various VTG\ntasks and datasets demonstrate the superior performance of TRACE compared to\nstate-of-the-art video LLMs. Our model and code are available at\nhttps://github.com/gyxxyg/TRACE.",
            "upvotes": 8,
            "discussionId": "67073b84d4311c2dd68fe240"
        },
        "publishedAt": "2024-10-10T01:28:18.488Z",
        "title": "TRACE: Temporal Grounding Video LLM via Causal Event Modeling",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.05643.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "/avatars/6e92eee19ae2c43e46a8b294c02f9915.svg",
            "fullname": "Yongxin Guo",
            "name": "Yongxin-Guo",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.07064",
            "authors": [
                {
                    "_id": "670739dfd4311c2dd68f7fa5",
                    "user": {
                        "_id": "624ac662102fcdff87be51b9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/624ac662102fcdff87be51b9/rzNahZFFkp194170tactJ.jpeg",
                        "isPro": false,
                        "fullname": "Yuxian Gu",
                        "user": "t1101675",
                        "type": "user"
                    },
                    "name": "Yuxian Gu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-10T08:01:31.000Z",
                    "hidden": false
                },
                {
                    "_id": "670739dfd4311c2dd68f7fa6",
                    "name": "Li Dong",
                    "hidden": false
                },
                {
                    "_id": "670739dfd4311c2dd68f7fa7",
                    "user": {
                        "_id": "62ec23fcbd19e355478fc584",
                        "avatarUrl": "/avatars/d7c567ef5f20bb3b9905cb5015d11e12.svg",
                        "isPro": false,
                        "fullname": "Hongning Wang",
                        "user": "howang",
                        "type": "user"
                    },
                    "name": "Hongning Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T09:25:41.531Z",
                    "hidden": false
                },
                {
                    "_id": "670739dfd4311c2dd68f7fa8",
                    "name": "Yaru Hao",
                    "hidden": false
                },
                {
                    "_id": "670739dfd4311c2dd68f7fa9",
                    "user": {
                        "_id": "670740744341dcee459fb990",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/66UkZvrAk7fQr5YCylEFk.png",
                        "isPro": false,
                        "fullname": "Qingxiu Dong",
                        "user": "Rsy24",
                        "type": "user"
                    },
                    "name": "Qingxiu Dong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-10T08:01:32.592Z",
                    "hidden": false
                },
                {
                    "_id": "670739dfd4311c2dd68f7faa",
                    "user": {
                        "_id": "6368c512fbfe97c16a40baba",
                        "avatarUrl": "/avatars/1c23bc7c0b6d9225699ce27647623d7a.svg",
                        "isPro": false,
                        "fullname": "Furu Wei",
                        "user": "thegenerality",
                        "type": "user"
                    },
                    "name": "Furu Wei",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T09:25:28.584Z",
                    "hidden": false
                },
                {
                    "_id": "670739dfd4311c2dd68f7fab",
                    "name": "Minlie Huang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-09T17:06:57.000Z",
            "title": "Data Selection via Optimal Control for Language Models",
            "summary": "This work investigates the selection of high-quality pre-training data from\nmassive corpora to enhance LMs' capabilities for downstream usage. We formulate\ndata selection as a generalized Optimal Control problem, which can be solved\ntheoretically by Pontryagin's Maximum Principle (PMP), yielding a set of\nnecessary conditions that characterize the relationship between optimal data\nselection and LM training dynamics. Based on these theoretical results, we\nintroduce PMP-based Data Selection (PDS), a framework that approximates optimal\ndata selection by solving the PMP conditions. In our experiments, we adopt PDS\nto select data from CommmonCrawl and show that the PDS-selected corpus\naccelerates the learning of LMs and constantly boosts their performance on a\nwide range of downstream tasks across various model sizes. Moreover, the\nbenefits of PDS extend to ~400B models trained on ~10T tokens, as evidenced by\nthe extrapolation of the test loss curves according to the Scaling Laws. PDS\nalso improves data utilization when the pre-training data is limited, by\nreducing the data demand by 1.8 times, which mitigates the quick exhaustion of\navailable web-crawled corpora. Our code, data, and model checkpoints can be\nfound in https://github.com/microsoft/LMOps/tree/main/data_selection.",
            "upvotes": 8,
            "discussionId": "670739e0d4311c2dd68f7fe0"
        },
        "publishedAt": "2024-10-10T00:51:34.643Z",
        "title": "Data Selection via Optimal Control for Language Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.07064.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/624ac662102fcdff87be51b9/rzNahZFFkp194170tactJ.jpeg",
            "fullname": "Yuxian Gu",
            "name": "t1101675",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.02465",
            "authors": [
                {
                    "_id": "6707b25a40b1f23d30085d85",
                    "user": {
                        "_id": "617017b7544a1c62b298b87d",
                        "avatarUrl": "/avatars/a569d976eb26807bd126de63a6b808fd.svg",
                        "isPro": false,
                        "fullname": "Seokhyun An",
                        "user": "seokhyun",
                        "type": "user"
                    },
                    "name": "Seokhyun An",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-10T10:59:05.265Z",
                    "hidden": false
                },
                {
                    "_id": "6707b25a40b1f23d30085d86",
                    "user": {
                        "_id": "64d7162b3ca2924d6e8593e4",
                        "avatarUrl": "/avatars/51b28e4652bbb9d35adbd62aece59cd0.svg",
                        "isPro": false,
                        "fullname": "Hyounghun Kim",
                        "user": "khh3323",
                        "type": "user"
                    },
                    "name": "Hyounghun Kim",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-10T14:03:08.497Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-03T13:15:19.000Z",
            "title": "Response Tuning: Aligning Large Language Models without Instruction",
            "summary": "Instruction tuning-supervised fine-tuning using instruction-response pairs-is\na foundational step in transitioning pre-trained Large Language Models (LLMs)\ninto helpful and safe chat assistants. Our hypothesis is that establishing an\nadequate output space can enable such a transition given the capabilities\ninherent in pre-trained LLMs. To verify this, we propose Response Tuning (RT),\nwhich eliminates the instruction-conditioning step in instruction tuning and\nsolely focuses on response space supervision. Our experiments demonstrate that\nRT models, trained only using responses, can effectively respond to a wide\nrange of instructions and exhibit helpfulness comparable to that of their\ninstruction-tuned counterparts. Furthermore, we observe that controlling the\ntraining response distribution can significantly improve their user preference\nor elicit target behaviors such as refusing assistance for unsafe queries. Our\nfindings illuminate the role of establishing an adequate output space in\nalignment, highlighting the potential of the extensive inherent capabilities of\npre-trained LLMs.",
            "upvotes": 7,
            "discussionId": "6707b25b40b1f23d30085dd1"
        },
        "publishedAt": "2024-10-10T09:36:37.367Z",
        "title": "Response Tuning: Aligning Large Language Models without Instruction",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.02465.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/a569d976eb26807bd126de63a6b808fd.svg",
            "fullname": "Seokhyun An",
            "name": "seokhyun",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.06084",
            "authors": [
                {
                    "_id": "6707aba2356f027ca984e789",
                    "name": "Geoffrey Cideron",
                    "hidden": false
                },
                {
                    "_id": "6707aba2356f027ca984e78a",
                    "user": {
                        "_id": "600e79032b417b1d53669bdb",
                        "avatarUrl": "/avatars/a60d5ae82340722285474b3a5d38e234.svg",
                        "isPro": false,
                        "fullname": "andrea agostinelli",
                        "user": "aagostinelli86",
                        "type": "user"
                    },
                    "name": "Andrea Agostinelli",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T11:12:01.996Z",
                    "hidden": false
                },
                {
                    "_id": "6707aba2356f027ca984e78b",
                    "user": {
                        "_id": "65afb7dbdd6bdfd73cd8e609",
                        "avatarUrl": "/avatars/b21069bc2d7ee4cc1508008e3c8ade64.svg",
                        "isPro": false,
                        "fullname": "Johan Ferret",
                        "user": "ferretj",
                        "type": "user"
                    },
                    "name": "Johan Ferret",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T11:12:08.489Z",
                    "hidden": false
                },
                {
                    "_id": "6707aba2356f027ca984e78c",
                    "name": "Sertan Girgin",
                    "hidden": false
                },
                {
                    "_id": "6707aba2356f027ca984e78d",
                    "name": "Romuald Elie",
                    "hidden": false
                },
                {
                    "_id": "6707aba2356f027ca984e78e",
                    "user": {
                        "_id": "667aac871bffb68706b4f62c",
                        "avatarUrl": "/avatars/2e2f2431f03a368f604c992d0d7ca57a.svg",
                        "isPro": false,
                        "fullname": "Olivier Bachem",
                        "user": "bachem",
                        "type": "user"
                    },
                    "name": "Olivier Bachem",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T11:12:20.214Z",
                    "hidden": false
                },
                {
                    "_id": "6707aba2356f027ca984e78f",
                    "user": {
                        "_id": "66328157b270ae503e91339b",
                        "avatarUrl": "/avatars/ea7a52060f5360f523ca28e137e85e33.svg",
                        "isPro": false,
                        "fullname": "Sarah Perrin",
                        "user": "Sper42",
                        "type": "user"
                    },
                    "name": "Sarah Perrin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T11:12:27.578Z",
                    "hidden": false
                },
                {
                    "_id": "6707aba2356f027ca984e790",
                    "user": {
                        "_id": "63c94ede00104ea998de19a6",
                        "avatarUrl": "/avatars/273959d87f0c67747588cf0700d64039.svg",
                        "isPro": false,
                        "fullname": "Alexandre Rame",
                        "user": "alexrame",
                        "type": "user"
                    },
                    "name": "Alexandre Ramé",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-10T10:33:00.042Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-08T14:40:51.000Z",
            "title": "Diversity-Rewarded CFG Distillation",
            "summary": "Generative models are transforming creative domains such as music generation,\nwith inference-time strategies like Classifier-Free Guidance (CFG) playing a\ncrucial role. However, CFG doubles inference cost while limiting originality\nand diversity across generated contents. In this paper, we introduce\ndiversity-rewarded CFG distillation, a novel finetuning procedure that distills\nthe strengths of CFG while addressing its limitations. Our approach optimises\ntwo training objectives: (1) a distillation objective, encouraging the model\nalone (without CFG) to imitate the CFG-augmented predictions, and (2) an RL\nobjective with a diversity reward, promoting the generation of diverse outputs\nfor a given prompt. By finetuning, we learn model weights with the ability to\ngenerate high-quality and diverse outputs, without any inference overhead. This\nalso unlocks the potential of weight-based model merging strategies: by\ninterpolating between the weights of two models (the first focusing on quality,\nthe second on diversity), we can control the quality-diversity trade-off at\ndeployment time, and even further boost performance. We conduct extensive\nexperiments on the MusicLM (Agostinelli et al., 2023) text-to-music generative\nmodel, where our approach surpasses CFG in terms of quality-diversity Pareto\noptimality. According to human evaluators, our finetuned-then-merged model\ngenerates samples with higher quality-diversity than the base model augmented\nwith CFG. Explore our generations at\nhttps://google-research.github.io/seanet/musiclm/diverse_music/.",
            "upvotes": 7,
            "discussionId": "6707aba3356f027ca984e7d0"
        },
        "publishedAt": "2024-10-10T08:56:39.548Z",
        "title": "Diversity-Rewarded CFG Distillation",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63c94ede00104ea998de19a6/MUy_IwIWprzdBG66doWrR.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.06084.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/273959d87f0c67747588cf0700d64039.svg",
            "fullname": "Alexandre Rame",
            "name": "alexrame",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.05677",
            "authors": [
                {
                    "_id": "6707527a2f420b4b07e4da90",
                    "user": {
                        "_id": "640d7ec5fdeaae13907fc488",
                        "avatarUrl": "/avatars/acf43ea155105a51c8612dacc4725091.svg",
                        "isPro": false,
                        "fullname": "Jiachen Li",
                        "user": "jiachenli-ucsb",
                        "type": "user"
                    },
                    "name": "Jiachen Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-10T08:00:33.880Z",
                    "hidden": false
                },
                {
                    "_id": "6707527a2f420b4b07e4da91",
                    "name": "Qian Long",
                    "hidden": false
                },
                {
                    "_id": "6707527a2f420b4b07e4da92",
                    "name": "Jian Zheng",
                    "hidden": false
                },
                {
                    "_id": "6707527a2f420b4b07e4da93",
                    "user": {
                        "_id": "64485435e21484883406f5fd",
                        "avatarUrl": "/avatars/910b920a6a6e8c7e0c7f7406554dd55b.svg",
                        "isPro": false,
                        "fullname": "Xiaofeng Gao",
                        "user": "xfgao",
                        "type": "user"
                    },
                    "name": "Xiaofeng Gao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T09:35:42.663Z",
                    "hidden": false
                },
                {
                    "_id": "6707527a2f420b4b07e4da94",
                    "user": {
                        "_id": "639a65cdabfa5e2fccf18b82",
                        "avatarUrl": "/avatars/945f6478d7a219174ffb1ccd14d7bad5.svg",
                        "isPro": false,
                        "fullname": "Robinson Piramuthu",
                        "user": "rpiramuthu",
                        "type": "user"
                    },
                    "name": "Robinson Piramuthu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T09:35:37.667Z",
                    "hidden": false
                },
                {
                    "_id": "6707527a2f420b4b07e4da95",
                    "user": {
                        "_id": "6313a86154e6e5d9f0f94e04",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
                        "isPro": false,
                        "fullname": "Wenhu Chen",
                        "user": "wenhu",
                        "type": "user"
                    },
                    "name": "Wenhu Chen",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2024-10-10T04:05:18.584Z",
                    "hidden": false
                },
                {
                    "_id": "6707527a2f420b4b07e4da96",
                    "user": {
                        "_id": "6476607efb22e3b77f3f1452",
                        "avatarUrl": "/avatars/22f40c81e6b6e09bdb66a5afd978f98b.svg",
                        "isPro": false,
                        "fullname": "William Yang Wang",
                        "user": "wangwilliamyang",
                        "type": "user"
                    },
                    "name": "William Yang Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T09:38:43.138Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-08T04:30:06.000Z",
            "title": "T2V-Turbo-v2: Enhancing Video Generation Model Post-Training through\n  Data, Reward, and Conditional Guidance Design",
            "summary": "In this paper, we focus on enhancing a diffusion-based text-to-video (T2V)\nmodel during the post-training phase by distilling a highly capable consistency\nmodel from a pretrained T2V model. Our proposed method, T2V-Turbo-v2,\nintroduces a significant advancement by integrating various supervision\nsignals, including high-quality training data, reward model feedback, and\nconditional guidance, into the consistency distillation process. Through\ncomprehensive ablation studies, we highlight the crucial importance of\ntailoring datasets to specific learning objectives and the effectiveness of\nlearning from diverse reward models for enhancing both the visual quality and\ntext-video alignment. Additionally, we highlight the vast design space of\nconditional guidance strategies, which centers on designing an effective energy\nfunction to augment the teacher ODE solver. We demonstrate the potential of\nthis approach by extracting motion guidance from the training datasets and\nincorporating it into the ODE solver, showcasing its effectiveness in improving\nthe motion quality of the generated videos with the improved motion-related\nmetrics from VBench and T2V-CompBench. Empirically, our T2V-Turbo-v2\nestablishes a new state-of-the-art result on VBench, with a Total score of\n85.13, surpassing proprietary systems such as Gen-3 and Kling.",
            "upvotes": 7,
            "discussionId": "6707527e2f420b4b07e4dc92"
        },
        "publishedAt": "2024-10-10T02:35:24.672Z",
        "title": "T2V-Turbo-v2: Enhancing Video Generation Model Post-Training through Data, Reward, and Conditional Guidance Design",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.05677.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.06458",
            "authors": [
                {
                    "_id": "6707dec1fd8b5c5c50646e45",
                    "user": {
                        "_id": "60db4e209f0b7223ea4560af",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1624985089254-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Thomas Ferraz",
                        "user": "thomas-ferraz",
                        "type": "user"
                    },
                    "name": "Thomas Palmeira Ferraz",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T14:15:37.467Z",
                    "hidden": false
                },
                {
                    "_id": "6707dec1fd8b5c5c50646e46",
                    "name": "Kartik Mehta",
                    "hidden": false
                },
                {
                    "_id": "6707dec1fd8b5c5c50646e47",
                    "name": "Yu-Hsiang Lin",
                    "hidden": false
                },
                {
                    "_id": "6707dec1fd8b5c5c50646e48",
                    "name": "Haw-Shiuan Chang",
                    "hidden": false
                },
                {
                    "_id": "6707dec1fd8b5c5c50646e49",
                    "name": "Shereen Oraby",
                    "hidden": false
                },
                {
                    "_id": "6707dec1fd8b5c5c50646e4a",
                    "name": "Sijia Liu",
                    "hidden": false
                },
                {
                    "_id": "6707dec1fd8b5c5c50646e4b",
                    "name": "Vivek Subramanian",
                    "hidden": false
                },
                {
                    "_id": "6707dec1fd8b5c5c50646e4c",
                    "name": "Tagyoung Chung",
                    "hidden": false
                },
                {
                    "_id": "6707dec1fd8b5c5c50646e4d",
                    "user": {
                        "_id": "665d9d3a057f7c508f98c625",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/665d9d3a057f7c508f98c625/u1R9P9sJoAl4zEIcetbPy.jpeg",
                        "isPro": false,
                        "fullname": "Mohit Bansal",
                        "user": "mohitbansal",
                        "type": "user"
                    },
                    "name": "Mohit Bansal",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T14:16:23.705Z",
                    "hidden": false
                },
                {
                    "_id": "6707dec1fd8b5c5c50646e4e",
                    "name": "Nanyun Peng",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-09T01:25:10.000Z",
            "title": "LLM Self-Correction with DeCRIM: Decompose, Critique, and Refine for\n  Enhanced Following of Instructions with Multiple Constraints",
            "summary": "Instruction following is a key capability for LLMs. However, recent studies\nhave shown that LLMs often struggle with instructions containing multiple\nconstraints (e.g. a request to create a social media post \"in a funny tone\"\nwith \"no hashtag\"). Despite this, most evaluations focus solely on synthetic\ndata. To address this, we introduce RealInstruct, the first benchmark designed\nto evaluate LLMs' ability to follow real-world multi-constrained instructions\nby leveraging queries real users asked AI assistants. We also investigate\nmodel-based evaluation as a cost-effective alternative to human annotation for\nthis task. Our findings reveal that even the proprietary GPT-4 model fails to\nmeet at least one constraint on over 21% of instructions, highlighting the\nlimitations of state-of-the-art models. To address the performance gap between\nopen-source and proprietary models, we propose the Decompose, Critique and\nRefine (DeCRIM) self-correction pipeline, which enhances LLMs' ability to\nfollow constraints. DeCRIM works by decomposing the original instruction into a\nlist of constraints and using a Critic model to decide when and where the LLM's\nresponse needs refinement. Our results show that DeCRIM improves Mistral's\nperformance by 7.3% on RealInstruct and 8.0% on IFEval even with weak feedback.\nMoreover, we demonstrate that with strong feedback, open-source LLMs with\nDeCRIM can outperform GPT-4 on both benchmarks.",
            "upvotes": 6,
            "discussionId": "6707dec2fd8b5c5c50646e79"
        },
        "publishedAt": "2024-10-10T12:39:26.408Z",
        "title": "LLM Self-Correction with DeCRIM: Decompose, Critique, and Refine for Enhanced Following of Instructions with Multiple Constraints",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.06458.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1624985089254-noauth.jpeg",
            "fullname": "Thomas Ferraz",
            "name": "thomas-ferraz",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.02503",
            "authors": [
                {
                    "_id": "6707b4eafd8b5c5c5056428c",
                    "user": {
                        "_id": "6527f3f27ad7a346021075a7",
                        "avatarUrl": "/avatars/8f9e154b9426be73d7f4320ce4c6f342.svg",
                        "isPro": false,
                        "fullname": "Jihyoung Jang",
                        "user": "jihyoung",
                        "type": "user"
                    },
                    "name": "Jihyoung Jang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T11:11:17.269Z",
                    "hidden": false
                },
                {
                    "_id": "6707b4eafd8b5c5c5056428d",
                    "name": "Taeyoung Kim",
                    "hidden": false
                },
                {
                    "_id": "6707b4eafd8b5c5c5056428e",
                    "user": {
                        "_id": "64d7162b3ca2924d6e8593e4",
                        "avatarUrl": "/avatars/51b28e4652bbb9d35adbd62aece59cd0.svg",
                        "isPro": false,
                        "fullname": "Hyounghun Kim",
                        "user": "khh3323",
                        "type": "user"
                    },
                    "name": "Hyounghun Kim",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T11:11:28.260Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-03T14:06:43.000Z",
            "title": "Mixed-Session Conversation with Egocentric Memory",
            "summary": "Recently introduced dialogue systems have demonstrated high usability.\nHowever, they still fall short of reflecting real-world conversation scenarios.\nCurrent dialogue systems exhibit an inability to replicate the dynamic,\ncontinuous, long-term interactions involving multiple partners. This shortfall\narises because there have been limited efforts to account for both aspects of\nreal-world dialogues: deeply layered interactions over the long-term dialogue\nand widely expanded conversation networks involving multiple participants. As\nthe effort to incorporate these aspects combined, we introduce Mixed-Session\nConversation, a dialogue system designed to construct conversations with\nvarious partners in a multi-session dialogue setup. We propose a new dataset\ncalled MiSC to implement this system. The dialogue episodes of MiSC consist of\n6 consecutive sessions, with four speakers (one main speaker and three\npartners) appearing in each episode. Also, we propose a new dialogue model with\na novel memory management mechanism, called Egocentric Memory Enhanced\nMixed-Session Conversation Agent (EMMA). EMMA collects and retains memories\nfrom the main speaker's perspective during conversations with partners,\nenabling seamless continuity in subsequent interactions. Extensive human\nevaluations validate that the dialogues in MiSC demonstrate a seamless\nconversational flow, even when conversation partners change in each session.\nEMMA trained with MiSC is also evaluated to maintain high memorability without\ncontradiction throughout the entire conversation.",
            "upvotes": 6,
            "discussionId": "6707b4edfd8b5c5c5056436e"
        },
        "publishedAt": "2024-10-10T09:36:26.358Z",
        "title": "Mixed-Session Conversation with Egocentric Memory",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.02503.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/8f9e154b9426be73d7f4320ce4c6f342.svg",
            "fullname": "Jihyoung Jang",
            "name": "jihyoung",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.06555",
            "authors": [
                {
                    "_id": "67079d5bdba78ae6a5605e43",
                    "name": "Haoran Zhang",
                    "hidden": false
                },
                {
                    "_id": "67079d5bdba78ae6a5605e44",
                    "user": {
                        "_id": "646b43deb1202bc77c1024a4",
                        "avatarUrl": "/avatars/cf791574ab986bac274e7fbcf04e2a59.svg",
                        "isPro": false,
                        "fullname": "hangyu guo",
                        "user": "Rosiness",
                        "type": "user"
                    },
                    "name": "Hangyu Guo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T09:46:00.887Z",
                    "hidden": false
                },
                {
                    "_id": "67079d5bdba78ae6a5605e45",
                    "name": "Shuyue Guo",
                    "hidden": false
                },
                {
                    "_id": "67079d5bdba78ae6a5605e46",
                    "name": "Meng Cao",
                    "hidden": false
                },
                {
                    "_id": "67079d5bdba78ae6a5605e47",
                    "user": {
                        "_id": "641e5bf65f274a0a92c2f6a2",
                        "avatarUrl": "/avatars/c15a54c51998c0e6367685e8e1737ec9.svg",
                        "isPro": false,
                        "fullname": "Wenhao Huang",
                        "user": "EZ-hwh",
                        "type": "user"
                    },
                    "name": "Wenhao Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T09:45:45.179Z",
                    "hidden": false
                },
                {
                    "_id": "67079d5bdba78ae6a5605e48",
                    "user": {
                        "_id": "65377c30e48353201e6fdda0",
                        "avatarUrl": "/avatars/a8f803b6f2e598eaee9c52c0d2ddfc16.svg",
                        "isPro": false,
                        "fullname": "Jiaheng Liu",
                        "user": "CheeryLJH",
                        "type": "user"
                    },
                    "name": "Jiaheng Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T09:45:29.766Z",
                    "hidden": false
                },
                {
                    "_id": "67079d5bdba78ae6a5605e49",
                    "user": {
                        "_id": "638efcf4c67af472d316d424",
                        "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
                        "isPro": false,
                        "fullname": "Ge Zhang",
                        "user": "zhangysk",
                        "type": "user"
                    },
                    "name": "Ge Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T09:45:24.241Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-09T05:17:38.000Z",
            "title": "ING-VP: MLLMs cannot Play Easy Vision-based Games Yet",
            "summary": "As multimodal large language models (MLLMs) continue to demonstrate\nincreasingly competitive performance across a broad spectrum of tasks, more\nintricate and comprehensive benchmarks have been developed to assess these\ncutting-edge models. These benchmarks introduce new challenges to core\ncapabilities such as perception, reasoning, and planning. However, existing\nmultimodal benchmarks fall short in providing a focused evaluation of\nmulti-step planning based on spatial relationships in images. To bridge this\ngap, we present ING-VP, the first INteractive Game-based Vision Planning\nbenchmark, specifically designed to evaluate the spatial imagination and\nmulti-step reasoning abilities of MLLMs. ING-VP features 6 distinct games,\nencompassing 300 levels, each with 6 unique configurations. A single model\nengages in over 60,000 rounds of interaction. The benchmark framework allows\nfor multiple comparison settings, including image-text vs. text-only inputs,\nsingle-step vs. multi-step reasoning, and with-history vs. without-history\nconditions, offering valuable insights into the model's capabilities. We\nevaluated numerous state-of-the-art MLLMs, with the highest-performing model,\nClaude-3.5 Sonnet, achieving an average accuracy of only 3.37%, far below the\nanticipated standard. This work aims to provide a specialized evaluation\nframework to drive advancements in MLLMs' capacity for complex spatial\nreasoning and planning. The code is publicly available at\nhttps://github.com/Thisisus7/ING-VP.git.",
            "upvotes": 6,
            "discussionId": "67079d5ddba78ae6a5605e9f"
        },
        "publishedAt": "2024-10-10T07:58:54.104Z",
        "title": "ING-VP: MLLMs cannot Play Easy Vision-based Games Yet",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.06555.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/cf791574ab986bac274e7fbcf04e2a59.svg",
            "fullname": "hangyu guo",
            "name": "Rosiness",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.06885",
            "authors": [
                {
                    "_id": "670761c9bbae2dc3843a4164",
                    "user": {
                        "_id": "636916bb85fef3ca96e2ef73",
                        "avatarUrl": "/avatars/3f36f3a02dd8c3bcd7f542b69bfa330e.svg",
                        "isPro": false,
                        "fullname": "Yushen CHEN",
                        "user": "SWivid",
                        "type": "user"
                    },
                    "name": "Yushen Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T09:54:29.316Z",
                    "hidden": false
                },
                {
                    "_id": "670761c9bbae2dc3843a4165",
                    "name": "Zhikang Niu",
                    "hidden": false
                },
                {
                    "_id": "670761c9bbae2dc3843a4166",
                    "name": "Ziyang Ma",
                    "hidden": false
                },
                {
                    "_id": "670761c9bbae2dc3843a4167",
                    "user": {
                        "_id": "6190e0a108a57f265f7f5246",
                        "avatarUrl": "/avatars/f7dc279ea9e922a2684dd23d4aa3f757.svg",
                        "isPro": false,
                        "fullname": "Keqi Deng",
                        "user": "D-Keqi",
                        "type": "user"
                    },
                    "name": "Keqi Deng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T09:55:02.556Z",
                    "hidden": false
                },
                {
                    "_id": "670761c9bbae2dc3843a4168",
                    "user": {
                        "_id": "66610c2ec629e0e089fc9cf9",
                        "avatarUrl": "/avatars/b3a958de94ba93a8b21ae6d1011f0b45.svg",
                        "isPro": false,
                        "fullname": "Chun-Hui Wang",
                        "user": "ChunHuiWangFN",
                        "type": "user"
                    },
                    "name": "Chunhui Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T09:55:07.827Z",
                    "hidden": false
                },
                {
                    "_id": "670761c9bbae2dc3843a4169",
                    "name": "Jian Zhao",
                    "hidden": false
                },
                {
                    "_id": "670761c9bbae2dc3843a416a",
                    "user": {
                        "_id": "66c04955780d735f176eb72f",
                        "avatarUrl": "/avatars/10fb520fbaf028d80aa1fb17a1d17f06.svg",
                        "isPro": false,
                        "fullname": "Kai Yu",
                        "user": "kaiyu-hf",
                        "type": "user"
                    },
                    "name": "Kai Yu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T09:55:18.617Z",
                    "hidden": false
                },
                {
                    "_id": "670761c9bbae2dc3843a416b",
                    "name": "Xie Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-09T13:46:34.000Z",
            "title": "F5-TTS: A Fairytaler that Fakes Fluent and Faithful Speech with Flow\n  Matching",
            "summary": "This paper introduces F5-TTS, a fully non-autoregressive text-to-speech\nsystem based on flow matching with Diffusion Transformer (DiT). Without\nrequiring complex designs such as duration model, text encoder, and phoneme\nalignment, the text input is simply padded with filler tokens to the same\nlength as input speech, and then the denoising is performed for speech\ngeneration, which was originally proved feasible by E2 TTS. However, the\noriginal design of E2 TTS makes it hard to follow due to its slow convergence\nand low robustness. To address these issues, we first model the input with\nConvNeXt to refine the text representation, making it easy to align with the\nspeech. We further propose an inference-time Sway Sampling strategy, which\nsignificantly improves our model's performance and efficiency. This sampling\nstrategy for flow step can be easily applied to existing flow matching based\nmodels without retraining. Our design allows faster training and achieves an\ninference RTF of 0.15, which is greatly improved compared to state-of-the-art\ndiffusion-based TTS models. Trained on a public 100K hours multilingual\ndataset, our Fairytaler Fakes Fluent and Faithful speech with Flow matching\n(F5-TTS) exhibits highly natural and expressive zero-shot ability, seamless\ncode-switching capability, and speed control efficiency. Demo samples can be\nfound at https://SWivid.github.io/F5-TTS. We release all code and checkpoints\nto promote community development.",
            "upvotes": 6,
            "discussionId": "670761cabbae2dc3843a41eb"
        },
        "publishedAt": "2024-10-10T05:38:53.780Z",
        "title": "F5-TTS: A Fairytaler that Fakes Fluent and Faithful Speech with Flow Matching",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.06885.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.06172",
            "authors": [
                {
                    "_id": "6707562501f0e95af7755b71",
                    "name": "Kaiwen Zhou",
                    "hidden": false
                },
                {
                    "_id": "6707562501f0e95af7755b72",
                    "user": {
                        "_id": "65e71f6bcd3df9b0f6b2678b",
                        "avatarUrl": "/avatars/f0c1b5433db21cee4c82658cd48781d2.svg",
                        "isPro": false,
                        "fullname": "Chengzhi Liu",
                        "user": "LCZZZZ",
                        "type": "user"
                    },
                    "name": "Chengzhi Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T09:30:41.139Z",
                    "hidden": false
                },
                {
                    "_id": "6707562501f0e95af7755b73",
                    "user": {
                        "_id": "6275a465597c70eb8949fce5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6275a465597c70eb8949fce5/ph4UogqMurMB0hSXZC38w.png",
                        "isPro": false,
                        "fullname": "Xuandong Zhao",
                        "user": "Xuandong",
                        "type": "user"
                    },
                    "name": "Xuandong Zhao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T09:30:31.061Z",
                    "hidden": false
                },
                {
                    "_id": "6707562501f0e95af7755b74",
                    "user": {
                        "_id": "670621c7bb23a4c6c7eea972",
                        "avatarUrl": "/avatars/bdceb682f2b89a64316b1264bebb616a.svg",
                        "isPro": false,
                        "fullname": "Anderson Compalas",
                        "user": "acompalas",
                        "type": "user"
                    },
                    "name": "Anderson Compalas",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T09:30:24.563Z",
                    "hidden": false
                },
                {
                    "_id": "6707562501f0e95af7755b75",
                    "user": {
                        "_id": "6513179af60393414afd4140",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/GAf-FulRRNF-X_Svew1Xj.jpeg",
                        "isPro": false,
                        "fullname": "Xiaowei Song",
                        "user": "dawnsong",
                        "type": "user"
                    },
                    "name": "Dawn Song",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T09:30:18.188Z",
                    "hidden": false
                },
                {
                    "_id": "6707562501f0e95af7755b76",
                    "user": {
                        "_id": "64679a226192d39142245e5e",
                        "avatarUrl": "/avatars/05abee0b6317f100923936ca2099e9eb.svg",
                        "isPro": false,
                        "fullname": "Xin Eric Wang",
                        "user": "xw-eric",
                        "type": "user"
                    },
                    "name": "Xin Eric Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T09:30:11.281Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-08T16:16:07.000Z",
            "title": "Multimodal Situational Safety",
            "summary": "Multimodal Large Language Models (MLLMs) are rapidly evolving, demonstrating\nimpressive capabilities as multimodal assistants that interact with both humans\nand their environments. However, this increased sophistication introduces\nsignificant safety concerns. In this paper, we present the first evaluation and\nanalysis of a novel safety challenge termed Multimodal Situational Safety,\nwhich explores how safety considerations vary based on the specific situation\nin which the user or agent is engaged. We argue that for an MLLM to respond\nsafely, whether through language or action, it often needs to assess the safety\nimplications of a language query within its corresponding visual context. To\nevaluate this capability, we develop the Multimodal Situational Safety\nbenchmark (MSSBench) to assess the situational safety performance of current\nMLLMs. The dataset comprises 1,820 language query-image pairs, half of which\nthe image context is safe, and the other half is unsafe. We also develop an\nevaluation framework that analyzes key safety aspects, including explicit\nsafety reasoning, visual understanding, and, crucially, situational safety\nreasoning. Our findings reveal that current MLLMs struggle with this nuanced\nsafety problem in the instruction-following setting and struggle to tackle\nthese situational safety challenges all at once, highlighting a key area for\nfuture research. Furthermore, we develop multi-agent pipelines to coordinately\nsolve safety challenges, which shows consistent improvement in safety over the\noriginal MLLM response. Code and data: mssbench.github.io.",
            "upvotes": 6,
            "discussionId": "6707562801f0e95af7755c46"
        },
        "publishedAt": "2024-10-10T03:52:32.884Z",
        "title": "Multimodal Situational Safety",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.06172.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/05abee0b6317f100923936ca2099e9eb.svg",
            "fullname": "Xin Eric Wang",
            "name": "xw-eric",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.04223",
            "authors": [
                {
                    "_id": "6706008cc21eb22813b18d16",
                    "name": "Gang Liu",
                    "hidden": false
                },
                {
                    "_id": "6706008cc21eb22813b18d17",
                    "name": "Michael Sun",
                    "hidden": false
                },
                {
                    "_id": "6706008cc21eb22813b18d18",
                    "name": "Wojciech Matusik",
                    "hidden": false
                },
                {
                    "_id": "6706008cc21eb22813b18d19",
                    "user": {
                        "_id": "62cdc931c589e4a9e2411d39",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62cdc931c589e4a9e2411d39/Q91Xp6WeI50AjViMzbD3Q.png",
                        "isPro": false,
                        "fullname": "Meng Jiang",
                        "user": "mjiang89",
                        "type": "user"
                    },
                    "name": "Meng Jiang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T09:43:01.104Z",
                    "hidden": false
                },
                {
                    "_id": "6706008cc21eb22813b18d1a",
                    "name": "Jie Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-05T16:35:32.000Z",
            "title": "Multimodal Large Language Models for Inverse Molecular Design with\n  Retrosynthetic Planning",
            "summary": "While large language models (LLMs) have integrated images, adapting them to\ngraphs remains challenging, limiting their applications in materials and drug\ndesign. This difficulty stems from the need for coherent autoregressive\ngeneration across texts and graphs. To address this, we introduce Llamole, the\nfirst multimodal LLM capable of interleaved text and graph generation, enabling\nmolecular inverse design with retrosynthetic planning. Llamole integrates a\nbase LLM with the Graph Diffusion Transformer and Graph Neural Networks for\nmulti-conditional molecular generation and reaction inference within texts,\nwhile the LLM, with enhanced molecular understanding, flexibly controls\nactivation among the different graph modules. Additionally, Llamole integrates\nA* search with LLM-based cost functions for efficient retrosynthetic planning.\nWe create benchmarking datasets and conduct extensive experiments to evaluate\nLlamole against in-context learning and supervised fine-tuning. Llamole\nsignificantly outperforms 14 adapted LLMs across 12 metrics for controllable\nmolecular design and retrosynthetic planning.",
            "upvotes": 6,
            "discussionId": "6706008ec21eb22813b18da7"
        },
        "publishedAt": "2024-10-10T02:02:23.257Z",
        "title": "Multimodal Large Language Models for Inverse Molecular Design with Retrosynthetic Planning",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.04223.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/134ac63b443e1360be7c91f84f9d5ec7.svg",
            "fullname": "Gang Liu",
            "name": "liuganghuggingface",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.02428",
            "authors": [
                {
                    "_id": "6707b4fde480b1c6bbddf820",
                    "user": {
                        "_id": "63332274431dafafae24be22",
                        "avatarUrl": "/avatars/58259c3f5533df7e3c5c40ba8526f19e.svg",
                        "isPro": false,
                        "fullname": "Bae",
                        "user": "minwook",
                        "type": "user"
                    },
                    "name": "Minwook Bae",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-10T11:11:55.410Z",
                    "hidden": false
                },
                {
                    "_id": "6707b4fde480b1c6bbddf821",
                    "user": {
                        "_id": "64d7162b3ca2924d6e8593e4",
                        "avatarUrl": "/avatars/51b28e4652bbb9d35adbd62aece59cd0.svg",
                        "isPro": false,
                        "fullname": "Hyounghun Kim",
                        "user": "khh3323",
                        "type": "user"
                    },
                    "name": "Hyounghun Kim",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-10T14:03:05.747Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-03T12:21:17.000Z",
            "title": "Collective Critics for Creative Story Generation",
            "summary": "Generating a long story of several thousand words with narrative coherence\nusing Large Language Models (LLMs) has been a challenging task. Previous\nresearch has addressed this challenge by proposing different frameworks that\ncreate a story plan and generate a long story based on that plan. However,\nthese frameworks have been mainly focusing on maintaining narrative coherence\nin stories, often overlooking creativity in story planning and the\nexpressiveness of the stories generated from those plans, which are desirable\nproperties to captivate readers' interest. In this paper, we propose Collective\nCritics for Creative Story Generation framework (CritiCS), which is composed of\nplan refining stage (CrPlan) and story generation stage (CrText), to integrate\na collective revision mechanism that promotes those properties into long-form\nstory generation process. Specifically, in each stage, a group of LLM critics\nand one leader collaborate to incrementally refine drafts of plan and story\nthroughout multiple rounds. Extensive human evaluation shows that the CritiCS\ncan significantly enhance story creativity and reader engagement, while also\nmaintaining narrative coherence. Furthermore, the design of the framework\nallows active participation from human writers in any role within the critique\nprocess, enabling interactive human-machine collaboration in story writing.",
            "upvotes": 5,
            "discussionId": "6707b4ffe480b1c6bbddf8c8"
        },
        "publishedAt": "2024-10-10T09:43:00.823Z",
        "title": "Collective Critics for Creative Story Generation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.02428.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/58259c3f5533df7e3c5c40ba8526f19e.svg",
            "fullname": "Bae",
            "name": "minwook",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.07071",
            "authors": [
                {
                    "_id": "67077d8146c9e0a80114a1ac",
                    "user": {
                        "_id": "64c3849269b1a6796052eac7",
                        "avatarUrl": "/avatars/9f0c832d5b51b659c7bb83074f02a648.svg",
                        "isPro": false,
                        "fullname": "Thomas Schmied",
                        "user": "thomasschmied",
                        "type": "user"
                    },
                    "name": "Thomas Schmied",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T09:29:24.323Z",
                    "hidden": false
                },
                {
                    "_id": "67077d8146c9e0a80114a1ad",
                    "user": {
                        "_id": "648826f845a9218318e0272c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648826f845a9218318e0272c/1zUQFA7TdQ8WC9Chskx6b.jpeg",
                        "isPro": false,
                        "fullname": "Fabian Paischer",
                        "user": "paischer101",
                        "type": "user"
                    },
                    "name": "Fabian Paischer",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T09:29:18.411Z",
                    "hidden": false
                },
                {
                    "_id": "67077d8146c9e0a80114a1ae",
                    "user": {
                        "_id": "63dfcf6742591dda0b951a5b",
                        "avatarUrl": "/avatars/3cceda20169516bd52b0d7f9090ab41e.svg",
                        "isPro": false,
                        "fullname": "vihang patil",
                        "user": "vihangp",
                        "type": "user"
                    },
                    "name": "Vihang Patil",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T09:29:12.588Z",
                    "hidden": false
                },
                {
                    "_id": "67077d8146c9e0a80114a1af",
                    "name": "Markus Hofmarcher",
                    "hidden": false
                },
                {
                    "_id": "67077d8146c9e0a80114a1b0",
                    "user": {
                        "_id": "64b9310403124195cd9778ec",
                        "avatarUrl": "/avatars/57c594d3d0f97d3010b15b6a0806451c.svg",
                        "isPro": false,
                        "fullname": "Razvan Pascanu",
                        "user": "razp",
                        "type": "user"
                    },
                    "name": "Razvan Pascanu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T09:29:01.158Z",
                    "hidden": false
                },
                {
                    "_id": "67077d8146c9e0a80114a1b1",
                    "name": "Sepp Hochreiter",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-09T17:15:30.000Z",
            "title": "Retrieval-Augmented Decision Transformer: External Memory for In-context\n  RL",
            "summary": "In-context learning (ICL) is the ability of a model to learn a new task by\nobserving a few exemplars in its context. While prevalent in NLP, this\ncapability has recently also been observed in Reinforcement Learning (RL)\nsettings. Prior in-context RL methods, however, require entire episodes in the\nagent's context. Given that complex environments typically lead to long\nepisodes with sparse rewards, these methods are constrained to simple\nenvironments with short episodes. To address these challenges, we introduce\nRetrieval-Augmented Decision Transformer (RA-DT). RA-DT employs an external\nmemory mechanism to store past experiences from which it retrieves only\nsub-trajectories relevant for the current situation. The retrieval component in\nRA-DT does not require training and can be entirely domain-agnostic. We\nevaluate the capabilities of RA-DT on grid-world environments, robotics\nsimulations, and procedurally-generated video games. On grid-worlds, RA-DT\noutperforms baselines, while using only a fraction of their context length.\nFurthermore, we illuminate the limitations of current in-context RL methods on\ncomplex environments and discuss future directions. To facilitate future\nresearch, we release datasets for four of the considered environments.",
            "upvotes": 5,
            "discussionId": "67077d8346c9e0a80114a272"
        },
        "publishedAt": "2024-10-10T05:39:04.231Z",
        "title": "Retrieval-Augmented Decision Transformer: External Memory for In-context RL",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.07071.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648826f845a9218318e0272c/1zUQFA7TdQ8WC9Chskx6b.jpeg",
            "fullname": "Fabian Paischer",
            "name": "paischer101",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.05791",
            "authors": [
                {
                    "_id": "670773181771817ab426144d",
                    "user": {
                        "_id": "62abb35b26adcd4a3845b41b",
                        "avatarUrl": "/avatars/928720c54d947cc8ebcfa9a007f272ea.svg",
                        "isPro": false,
                        "fullname": "Ruocheng Wang",
                        "user": "rcwang",
                        "type": "user"
                    },
                    "name": "Ruocheng Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T09:57:56.927Z",
                    "hidden": false
                },
                {
                    "_id": "670773181771817ab426144e",
                    "name": "Pei Xu",
                    "hidden": false
                },
                {
                    "_id": "670773181771817ab426144f",
                    "name": "Haochen Shi",
                    "hidden": false
                },
                {
                    "_id": "670773181771817ab4261450",
                    "name": "Elizabeth Schumann",
                    "hidden": false
                },
                {
                    "_id": "670773181771817ab4261451",
                    "name": "C. Karen Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-08T08:21:05.000Z",
            "title": "FürElise: Capturing and Physically Synthesizing Hand Motions of Piano\n  Performance",
            "summary": "Piano playing requires agile, precise, and coordinated hand control that\nstretches the limits of dexterity. Hand motion models with the sophistication\nto accurately recreate piano playing have a wide range of applications in\ncharacter animation, embodied AI, biomechanics, and VR/AR. In this paper, we\nconstruct a first-of-its-kind large-scale dataset that contains approximately\n10 hours of 3D hand motion and audio from 15 elite-level pianists playing 153\npieces of classical music. To capture natural performances, we designed a\nmarkerless setup in which motions are reconstructed from multi-view videos\nusing state-of-the-art pose estimation models. The motion data is further\nrefined via inverse kinematics using the high-resolution MIDI key-pressing data\nobtained from sensors in a specialized Yamaha Disklavier piano. Leveraging the\ncollected dataset, we developed a pipeline that can synthesize\nphysically-plausible hand motions for musical scores outside of the dataset.\nOur approach employs a combination of imitation learning and reinforcement\nlearning to obtain policies for physics-based bimanual control involving the\ninteraction between hands and piano keys. To solve the sampling efficiency\nproblem with the large motion dataset, we use a diffusion model to generate\nnatural reference motions, which provide high-level trajectory and fingering\n(finger order and placement) information. However, the generated reference\nmotion alone does not provide sufficient accuracy for piano performance\nmodeling. We then further augmented the data by using musical similarity to\nretrieve similar motions from the captured dataset to boost the precision of\nthe RL policy. With the proposed method, our model generates natural, dexterous\nmotions that generalize to music from outside the training dataset.",
            "upvotes": 5,
            "discussionId": "6707731b1771817ab4261516"
        },
        "publishedAt": "2024-10-10T04:54:55.548Z",
        "title": "FürElise: Capturing and Physically Synthesizing Hand Motions of Piano Performance",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.05791.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.05664",
            "authors": [
                {
                    "_id": "67073b95fb83bdd95a196060",
                    "user": {
                        "_id": "644896af4988ee01f2a917d4",
                        "avatarUrl": "/avatars/0e40133c2957541071e2ca41b2d8a13d.svg",
                        "isPro": false,
                        "fullname": "saemi moon",
                        "user": "hi-sammy",
                        "type": "user"
                    },
                    "name": "Saemi Moon",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T10:00:31.147Z",
                    "hidden": false
                },
                {
                    "_id": "67073b95fb83bdd95a196061",
                    "user": {
                        "_id": "650a4546a0f81fbc0a94ef1a",
                        "avatarUrl": "/avatars/4aed2b283e0c0e2567c14eb81adf809a.svg",
                        "isPro": false,
                        "fullname": "Minjong Lee",
                        "user": "Minjong",
                        "type": "user"
                    },
                    "name": "Minjong Lee",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-10T08:01:20.890Z",
                    "hidden": false
                },
                {
                    "_id": "67073b95fb83bdd95a196062",
                    "name": "Sangdon Park",
                    "hidden": false
                },
                {
                    "_id": "67073b95fb83bdd95a196063",
                    "name": "Dongwoo Kim",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-08T03:30:39.000Z",
            "title": "Holistic Unlearning Benchmark: A Multi-Faceted Evaluation for\n  Text-to-Image Diffusion Model Unlearning",
            "summary": "As text-to-image diffusion models become advanced enough for commercial\napplications, there is also increasing concern about their potential for\nmalicious and harmful use. Model unlearning has been proposed to mitigate the\nconcerns by removing undesired and potentially harmful information from the\npre-trained model. So far, the success of unlearning is mainly measured by\nwhether the unlearned model can generate a target concept while maintaining\nimage quality. However, unlearning is typically tested under limited scenarios,\nand the side effects of unlearning have barely been studied in the current\nliterature. In this work, we thoroughly analyze unlearning under various\nscenarios with five key aspects. Our investigation reveals that every method\nhas side effects or limitations, especially in more complex and realistic\nsituations. By releasing our comprehensive evaluation framework with the source\ncodes and artifacts, we hope to inspire further research in this area, leading\nto more reliable and effective unlearning methods.",
            "upvotes": 5,
            "discussionId": "67073b98fb83bdd95a19613e"
        },
        "publishedAt": "2024-10-10T02:19:38.358Z",
        "title": "Holistic Unlearning Benchmark: A Multi-Faceted Evaluation for Text-to-Image Diffusion Model Unlearning",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.05664.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/4aed2b283e0c0e2567c14eb81adf809a.svg",
            "fullname": "Minjong Lee",
            "name": "Minjong",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.06241",
            "authors": [
                {
                    "_id": "670749315ea108eecd14ec6f",
                    "name": "Jiazi Bu",
                    "hidden": false
                },
                {
                    "_id": "670749315ea108eecd14ec70",
                    "name": "Pengyang Ling",
                    "hidden": false
                },
                {
                    "_id": "670749315ea108eecd14ec71",
                    "name": "Pan Zhang",
                    "hidden": false
                },
                {
                    "_id": "670749315ea108eecd14ec72",
                    "name": "Tong Wu",
                    "hidden": false
                },
                {
                    "_id": "670749315ea108eecd14ec73",
                    "name": "Xiaoyi Dong",
                    "hidden": false
                },
                {
                    "_id": "670749315ea108eecd14ec74",
                    "user": {
                        "_id": "63859cf3b2906edaf83af9f0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63859cf3b2906edaf83af9f0/iUQm5FAomzqYi6fkqIn9F.jpeg",
                        "isPro": false,
                        "fullname": "Yuhang Zang",
                        "user": "yuhangzang",
                        "type": "user"
                    },
                    "name": "Yuhang Zang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T09:56:05.632Z",
                    "hidden": false
                },
                {
                    "_id": "670749315ea108eecd14ec75",
                    "name": "Yuhang Cao",
                    "hidden": false
                },
                {
                    "_id": "670749315ea108eecd14ec76",
                    "user": {
                        "_id": "636317ed80c1a705a6eff396",
                        "avatarUrl": "/avatars/3db090e101b916d9256d0d3e043db71d.svg",
                        "isPro": false,
                        "fullname": "Dahua Lin",
                        "user": "lindahua",
                        "type": "user"
                    },
                    "name": "Dahua Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T09:56:35.238Z",
                    "hidden": false
                },
                {
                    "_id": "670749315ea108eecd14ec77",
                    "name": "Jiaqi Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-08T17:56:33.000Z",
            "title": "BroadWay: Boost Your Text-to-Video Generation Model in a Training-free\n  Way",
            "summary": "The text-to-video (T2V) generation models, offering convenient visual\ncreation, have recently garnered increasing attention. Despite their\nsubstantial potential, the generated videos may present artifacts, including\nstructural implausibility, temporal inconsistency, and a lack of motion, often\nresulting in near-static video. In this work, we have identified a correlation\nbetween the disparity of temporal attention maps across different blocks and\nthe occurrence of temporal inconsistencies. Additionally, we have observed that\nthe energy contained within the temporal attention maps is directly related to\nthe magnitude of motion amplitude in the generated videos. Based on these\nobservations, we present BroadWay, a training-free method to improve the\nquality of text-to-video generation without introducing additional parameters,\naugmenting memory or sampling time. Specifically, BroadWay is composed of two\nprincipal components: 1) Temporal Self-Guidance improves the structural\nplausibility and temporal consistency of generated videos by reducing the\ndisparity between the temporal attention maps across various decoder blocks. 2)\nFourier-based Motion Enhancement enhances the magnitude and richness of motion\nby amplifying the energy of the map. Extensive experiments demonstrate that\nBroadWay significantly improves the quality of text-to-video generation with\nnegligible additional cost.",
            "upvotes": 5,
            "discussionId": "670749355ea108eecd14ee4d"
        },
        "publishedAt": "2024-10-10T01:56:59.078Z",
        "title": "BroadWay: Boost Your Text-to-Video Generation Model in a Training-free Way",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.06241.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
            "fullname": "Jiaqi Wang",
            "name": "myownskyW7",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.06462",
            "authors": [
                {
                    "_id": "6707460201f0e95af76fac44",
                    "user": {
                        "_id": "63136a82e29fb2e86d5e5bdd",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63136a82e29fb2e86d5e5bdd/pFZDuQtzfUStovbwwZGvn.png",
                        "isPro": false,
                        "fullname": "David Noever",
                        "user": "dnoever",
                        "type": "user"
                    },
                    "name": "David Noever",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-10T08:00:48.869Z",
                    "hidden": false
                },
                {
                    "_id": "6707460201f0e95af76fac45",
                    "user": {
                        "_id": "627bff703974b0ed6b2c340d",
                        "avatarUrl": "/avatars/12518ccef145f343548b3fd7be5efddc.svg",
                        "isPro": false,
                        "fullname": "Forrest McKee",
                        "user": "fgmckee",
                        "type": "user"
                    },
                    "name": "Forrest McKee",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T09:39:08.724Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-09T01:36:25.000Z",
            "title": "Hallucinating AI Hijacking Attack: Large Language Models and Malicious\n  Code Recommenders",
            "summary": "The research builds and evaluates the adversarial potential to introduce\ncopied code or hallucinated AI recommendations for malicious code in popular\ncode repositories. While foundational large language models (LLMs) from OpenAI,\nGoogle, and Anthropic guard against both harmful behaviors and toxic strings,\nprevious work on math solutions that embed harmful prompts demonstrate that the\nguardrails may differ between expert contexts. These loopholes would appear in\nmixture of expert's models when the context of the question changes and may\noffer fewer malicious training examples to filter toxic comments or recommended\noffensive actions. The present work demonstrates that foundational models may\nrefuse to propose destructive actions correctly when prompted overtly but may\nunfortunately drop their guard when presented with a sudden change of context,\nlike solving a computer programming challenge. We show empirical examples with\ntrojan-hosting repositories like GitHub, NPM, NuGet, and popular content\ndelivery networks (CDN) like jsDelivr which amplify the attack surface. In the\nLLM's directives to be helpful, example recommendations propose application\nprogramming interface (API) endpoints which a determined domain-squatter could\nacquire and setup attack mobile infrastructure that triggers from the naively\ncopied code. We compare this attack to previous work on context-shifting and\ncontrast the attack surface as a novel version of \"living off the land\" attacks\nin the malware literature. In the latter case, foundational language models can\nhijack otherwise innocent user prompts to recommend actions that violate their\nowners' safety policies when posed directly without the accompanying coding\nsupport request.",
            "upvotes": 5,
            "discussionId": "6707460201f0e95af76fac75"
        },
        "publishedAt": "2024-10-10T01:48:47.488Z",
        "title": "Hallucinating AI Hijacking Attack: Large Language Models and Malicious Code Recommenders",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.06462.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63136a82e29fb2e86d5e5bdd/pFZDuQtzfUStovbwwZGvn.png",
            "fullname": "David Noever",
            "name": "dnoever",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.06949",
            "authors": [
                {
                    "_id": "67073844d2a6eeb542ceb2fe",
                    "user": {
                        "_id": "65fc5109899083a2aad987c5",
                        "avatarUrl": "/avatars/289dbb8128746d931118cff6f6871a45.svg",
                        "isPro": false,
                        "fullname": "XUANMING ZHANG",
                        "user": "XUANMINGZHANG",
                        "type": "user"
                    },
                    "name": "Xuanming Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-10T08:01:38.371Z",
                    "hidden": false
                },
                {
                    "_id": "67073844d2a6eeb542ceb2ff",
                    "name": "Yuxuan Chen",
                    "hidden": false
                },
                {
                    "_id": "67073844d2a6eeb542ceb300",
                    "name": "Yuan Yuan",
                    "hidden": false
                },
                {
                    "_id": "67073844d2a6eeb542ceb301",
                    "name": "Minlie Huang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-09T14:45:45.000Z",
            "title": "Seeker: Enhancing Exception Handling in Code with LLM-based Multi-Agent\n  Approach",
            "summary": "In real world software development, improper or missing exception handling\ncan severely impact the robustness and reliability of code. Exception handling\nmechanisms require developers to detect, capture, and manage exceptions\naccording to high standards, but many developers struggle with these tasks,\nleading to fragile code. This problem is particularly evident in open source\nprojects and impacts the overall quality of the software ecosystem. To address\nthis challenge, we explore the use of large language models (LLMs) to improve\nexception handling in code. Through extensive analysis, we identify three key\nissues: Insensitive Detection of Fragile Code, Inaccurate Capture of Exception\nTypes, and Distorted Handling Solutions. These problems are widespread across\nreal world repositories, suggesting that robust exception handling practices\nare often overlooked or mishandled. In response, we propose Seeker, a multi\nagent framework inspired by expert developer strategies for exception handling.\nSeeker uses agents: Scanner, Detector, Predator, Ranker, and Handler to assist\nLLMs in detecting, capturing, and resolving exceptions more effectively. Our\nwork is the first systematic study on leveraging LLMs to enhance exception\nhandling practices, providing valuable insights for future improvements in code\nreliability.",
            "upvotes": 5,
            "discussionId": "67073845d2a6eeb542ceb327"
        },
        "publishedAt": "2024-10-10T00:45:18.389Z",
        "title": "Seeker: Enhancing Exception Handling in Code with LLM-based Multi-Agent Approach",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.06949.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "/avatars/289dbb8128746d931118cff6f6871a45.svg",
            "fullname": "XUANMING ZHANG",
            "name": "XUANMINGZHANG",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.06845",
            "authors": [
                {
                    "_id": "67077c5ecbadd03eb3884d27",
                    "user": {
                        "_id": "6459f82e5b3111fbe8327aa0",
                        "avatarUrl": "/avatars/c377b442c9e2589ee0cc9be6bd028cc7.svg",
                        "isPro": false,
                        "fullname": "Cheng Li",
                        "user": "Cheng228",
                        "type": "user"
                    },
                    "name": "Cheng Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-10T07:58:37.011Z",
                    "hidden": false
                },
                {
                    "_id": "67077c5ecbadd03eb3884d28",
                    "name": "May Fung",
                    "hidden": false
                },
                {
                    "_id": "67077c5ecbadd03eb3884d29",
                    "name": "Qingyun Wang",
                    "hidden": false
                },
                {
                    "_id": "67077c5ecbadd03eb3884d2a",
                    "name": "Chi Han",
                    "hidden": false
                },
                {
                    "_id": "67077c5ecbadd03eb3884d2b",
                    "name": "Manling Li",
                    "hidden": false
                },
                {
                    "_id": "67077c5ecbadd03eb3884d2c",
                    "user": {
                        "_id": "6204cc0d522e40b4a18d86e2",
                        "avatarUrl": "/avatars/18daf2de5671e711dc745388dd60569d.svg",
                        "isPro": false,
                        "fullname": "Jindong Wang",
                        "user": "jindongwang",
                        "type": "user"
                    },
                    "name": "Jindong Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T10:03:56.330Z",
                    "hidden": false
                },
                {
                    "_id": "67077c5ecbadd03eb3884d2d",
                    "name": "Heng Ji",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-09T13:06:40.000Z",
            "title": "MentalArena: Self-play Training of Language Models for Diagnosis and\n  Treatment of Mental Health Disorders",
            "summary": "Mental health disorders are one of the most serious diseases in the world.\nMost people with such a disease lack access to adequate care, which highlights\nthe importance of training models for the diagnosis and treatment of mental\nhealth disorders. However, in the mental health domain, privacy concerns limit\nthe accessibility of personalized treatment data, making it challenging to\nbuild powerful models. In this paper, we introduce MentalArena, a self-play\nframework to train language models by generating domain-specific personalized\ndata, where we obtain a better model capable of making a personalized diagnosis\nand treatment (as a therapist) and providing information (as a patient). To\naccurately model human-like mental health patients, we devise Symptom Encoder,\nwhich simulates a real patient from both cognition and behavior perspectives.\nTo address intent bias during patient-therapist interactions, we propose\nSymptom Decoder to compare diagnosed symptoms with encoded symptoms, and\ndynamically manage the dialogue between patient and therapist according to the\nidentified deviations. We evaluated MentalArena against 6 benchmarks, including\nbiomedicalQA and mental health tasks, compared to 6 advanced models. Our\nmodels, fine-tuned on both GPT-3.5 and Llama-3-8b, significantly outperform\ntheir counterparts, including GPT-4o. We hope that our work can inspire future\nresearch on personalized care. Code is available in\nhttps://github.com/Scarelette/MentalArena/tree/main",
            "upvotes": 4,
            "discussionId": "67077c5fcbadd03eb3884d70"
        },
        "publishedAt": "2024-10-10T05:34:41.314Z",
        "title": "MentalArena: Self-play Training of Language Models for Diagnosis and Treatment of Mental Health Disorders",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.06845.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/18daf2de5671e711dc745388dd60569d.svg",
            "fullname": "Jindong Wang",
            "name": "jindongwang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.06524",
            "authors": [
                {
                    "_id": "67074de616abdf6976c85bdf",
                    "user": {
                        "_id": "639410ddcd5e6b3cdc4ca80e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/639410ddcd5e6b3cdc4ca80e/Gllj99qgxh6Gm__p7sizf.jpeg",
                        "isPro": false,
                        "fullname": "Maharshi Gor",
                        "user": "mgor",
                        "type": "user"
                    },
                    "name": "Maharshi Gor",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-10T08:43:21.613Z",
                    "hidden": false
                },
                {
                    "_id": "67074de616abdf6976c85be0",
                    "name": "Hal Daumé III",
                    "hidden": false
                },
                {
                    "_id": "67074de616abdf6976c85be1",
                    "user": {
                        "_id": "647f5af5b0e96764589f3b2a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
                        "isPro": false,
                        "fullname": "Tianyi Zhou",
                        "user": "zhoutianyi",
                        "type": "user"
                    },
                    "name": "Tianyi Zhou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-10T08:00:42.432Z",
                    "hidden": false
                },
                {
                    "_id": "67074de616abdf6976c85be2",
                    "name": "Jordan Boyd-Graber",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-09T03:53:26.000Z",
            "title": "Do great minds think alike? Investigating Human-AI Complementarity in\n  Question Answering with CAIMIRA",
            "summary": "Recent advancements of large language models (LLMs) have led to claims of AI\nsurpassing humans in natural language processing (NLP) tasks such as textual\nunderstanding and reasoning. This work investigates these assertions by\nintroducing CAIMIRA, a novel framework rooted in item response theory (IRT)\nthat enables quantitative assessment and comparison of problem-solving\nabilities of question-answering (QA) agents: humans and AI systems. Through\nanalysis of over 300,000 responses from ~70 AI systems and 155 humans across\nthousands of quiz questions, CAIMIRA uncovers distinct proficiency patterns in\nknowledge domains and reasoning skills. Humans outperform AI systems in\nknowledge-grounded abductive and conceptual reasoning, while state-of-the-art\nLLMs like GPT-4 and LLaMA show superior performance on targeted information\nretrieval and fact-based reasoning, particularly when information gaps are\nwell-defined and addressable through pattern matching or data retrieval. These\nfindings highlight the need for future QA tasks to focus on questions that\nchallenge not only higher-order reasoning and scientific thinking, but also\ndemand nuanced linguistic interpretation and cross-contextual knowledge\napplication, helping advance AI developments that better emulate or complement\nhuman cognitive abilities in real-world problem-solving.",
            "upvotes": 4,
            "discussionId": "67074deb16abdf6976c85d6e"
        },
        "publishedAt": "2024-10-10T02:35:24.305Z",
        "title": "Do great minds think alike? Investigating Human-AI Complementarity in Question Answering with CAIMIRA",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/Vs6ItP7g4FYaRvbSJ5Hqr.png",
            "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/ebKs03ES6yhni5cj9Mlpx.png",
            "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/gGxmJ8nKo0W79YxE_gTYX.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.06524.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
            "fullname": "Tianyi Zhou",
            "name": "zhoutianyi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.06985",
            "authors": [
                {
                    "_id": "67079b9754069379bd86c10c",
                    "user": {
                        "_id": "63f9e95db31627d24009de1a",
                        "avatarUrl": "/avatars/53e6782c38bd1babca03389ae15a7d2e.svg",
                        "isPro": false,
                        "fullname": "Shimon Vainer",
                        "user": "esx2ve",
                        "type": "user"
                    },
                    "name": "Shimon Vainer",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T09:46:53.809Z",
                    "hidden": false
                },
                {
                    "_id": "67079b9754069379bd86c10d",
                    "user": {
                        "_id": "64636800d4d34b01f45cc40d",
                        "avatarUrl": "/avatars/f423e37b8461f3bda11021b3abf80ee1.svg",
                        "isPro": false,
                        "fullname": "Konstantin Kutsy",
                        "user": "bostadynamics",
                        "type": "user"
                    },
                    "name": "Konstantin Kutsy",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T09:46:59.437Z",
                    "hidden": false
                },
                {
                    "_id": "67079b9754069379bd86c10e",
                    "name": "Dante De Nigris",
                    "hidden": false
                },
                {
                    "_id": "67079b9754069379bd86c10f",
                    "user": {
                        "_id": "63357214eb6132ca653020e7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63357214eb6132ca653020e7/A_imoyFDx30wgi0guG_s8.png",
                        "isPro": false,
                        "fullname": "Ciara",
                        "user": "CiaraRowles",
                        "type": "user"
                    },
                    "name": "Ciara Rowles",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-10T09:29:55.515Z",
                    "hidden": false
                },
                {
                    "_id": "67079b9754069379bd86c110",
                    "user": {
                        "_id": "639b56244bbcd176745a6e6d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/639b56244bbcd176745a6e6d/a1DRanMtKUNJNcrt3ZmOg.jpeg",
                        "isPro": false,
                        "fullname": "Slava Elizarov",
                        "user": "SlavaElizarov",
                        "type": "user"
                    },
                    "name": "Slava Elizarov",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T09:47:07.631Z",
                    "hidden": false
                },
                {
                    "_id": "67079b9754069379bd86c111",
                    "name": "Simon Donné",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-09T15:21:46.000Z",
            "title": "Jointly Generating Multi-view Consistent PBR Textures using\n  Collaborative Control",
            "summary": "Multi-view consistency remains a challenge for image diffusion models. Even\nwithin the Text-to-Texture problem, where perfect geometric correspondences are\nknown a priori, many methods fail to yield aligned predictions across views,\nnecessitating non-trivial fusion methods to incorporate the results onto the\noriginal mesh. We explore this issue for a Collaborative Control workflow\nspecifically in PBR Text-to-Texture. Collaborative Control directly models PBR\nimage probability distributions, including normal bump maps; to our knowledge,\nthe only diffusion model to directly output full PBR stacks. We discuss the\ndesign decisions involved in making this model multi-view consistent, and\ndemonstrate the effectiveness of our approach in ablation studies, as well as\npractical applications.",
            "upvotes": 3,
            "discussionId": "67079b9f54069379bd86c3b1"
        },
        "publishedAt": "2024-10-10T07:59:19.864Z",
        "title": "Jointly Generating Multi-view Consistent PBR Textures using Collaborative Control",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.06985.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63357214eb6132ca653020e7/A_imoyFDx30wgi0guG_s8.png",
            "fullname": "Ciara",
            "name": "CiaraRowles",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.07062",
            "authors": [
                {
                    "_id": "6707742995f90bdb8e9b3ed9",
                    "user": {
                        "_id": "64da082a79c87e13ca232042",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64da082a79c87e13ca232042/DKGEIffK3kLj7SQ20yYiK.png",
                        "isPro": false,
                        "fullname": "Cristian Gutiérrez",
                        "user": "ggcristian",
                        "type": "user"
                    },
                    "name": "Cristian Gutierrez",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-10T08:00:04.297Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-09T17:03:49.000Z",
            "title": "TinyEmo: Scaling down Emotional Reasoning via Metric Projection",
            "summary": "This paper introduces TinyEmo, a family of small multi-modal language models\nfor emotional reasoning and classification. Our approach features: (1) a\nsynthetic emotional instruct dataset for both pre-training and fine-tuning\nstages, (2) a Metric Projector that delegates classification from the language\nmodel allowing for more efficient training and inference, (3) a multi-modal\nlarge language model (MM-LLM) for emotional reasoning, and (4) a semi-automated\nframework for bias detection. TinyEmo is able to perform emotion classification\nand emotional reasoning, all while using substantially fewer parameters than\ncomparable models. This efficiency allows us to freely incorporate more diverse\nemotional datasets, enabling strong performance on classification tasks, with\nour smallest model (700M parameters) outperforming larger state-of-the-art\nmodels based on general-purpose MM-LLMs with over 7B parameters. Additionally,\nthe Metric Projector allows for interpretability and indirect bias detection in\nlarge models without additional training, offering an approach to understand\nand improve AI systems.\n  We release code, models, and dataset at https://github.com/ggcr/TinyEmo",
            "upvotes": 3,
            "discussionId": "6707742b95f90bdb8e9b3f2e"
        },
        "publishedAt": "2024-10-10T06:42:37.996Z",
        "title": "TinyEmo: Scaling down Emotional Reasoning via Metric Projection",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.07062.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64da082a79c87e13ca232042/DKGEIffK3kLj7SQ20yYiK.png",
            "fullname": "Cristian Gutiérrez",
            "name": "ggcristian",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.07160",
            "authors": [
                {
                    "_id": "67077a46be29abe37696f33a",
                    "user": {
                        "_id": "6707dd1b5370cb53e70d673d",
                        "avatarUrl": "/avatars/795f443fa1f433a275e89e8ecd4cbfef.svg",
                        "isPro": false,
                        "fullname": "Song",
                        "user": "LuLuchuan",
                        "type": "user"
                    },
                    "name": "Luchuan Song",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-10T14:03:10.468Z",
                    "hidden": false
                },
                {
                    "_id": "67077a46be29abe37696f33b",
                    "name": "Lele Chen",
                    "hidden": false
                },
                {
                    "_id": "67077a46be29abe37696f33c",
                    "user": {
                        "_id": "64364ba31adb261e94e0e5b4",
                        "avatarUrl": "/avatars/cb6ef2155d88b1e64c4ff36bd3a8f255.svg",
                        "isPro": false,
                        "fullname": "Celong Liu",
                        "user": "goddice",
                        "type": "user"
                    },
                    "name": "Celong Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T09:35:08.788Z",
                    "hidden": false
                },
                {
                    "_id": "67077a46be29abe37696f33d",
                    "user": {
                        "_id": "62eb469dade76f18dd4f0dea",
                        "avatarUrl": "/avatars/c558254a0352d115a73febb90bb9370f.svg",
                        "isPro": false,
                        "fullname": "Pinxin Liu",
                        "user": "pliu23",
                        "type": "user"
                    },
                    "name": "Pinxin Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T09:35:03.031Z",
                    "hidden": false
                },
                {
                    "_id": "67077a46be29abe37696f33e",
                    "name": "Chenliang Xu",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-09-23T15:04:45.000Z",
            "title": "TextToon: Real-Time Text Toonify Head Avatar from Single Video",
            "summary": "We propose TextToon, a method to generate a drivable toonified avatar. Given\na short monocular video sequence and a written instruction about the avatar\nstyle, our model can generate a high-fidelity toonified avatar that can be\ndriven in real-time by another video with arbitrary identities. Existing\nrelated works heavily rely on multi-view modeling to recover geometry via\ntexture embeddings, presented in a static manner, leading to control\nlimitations. The multi-view video input also makes it difficult to deploy these\nmodels in real-world applications. To address these issues, we adopt a\nconditional embedding Tri-plane to learn realistic and stylized facial\nrepresentations in a Gaussian deformation field. Additionally, we expand the\nstylization capabilities of 3D Gaussian Splatting by introducing an adaptive\npixel-translation neural network and leveraging patch-aware contrastive\nlearning to achieve high-quality images. To push our work into consumer\napplications, we develop a real-time system that can operate at 48 FPS on a GPU\nmachine and 15-18 FPS on a mobile machine. Extensive experiments demonstrate\nthe efficacy of our approach in generating textual avatars over existing\nmethods in terms of quality and real-time animation. Please refer to our\nproject page for more details: https://songluchuan.github.io/TextToon/.",
            "upvotes": 3,
            "discussionId": "67077a49be29abe37696f3eb"
        },
        "publishedAt": "2024-10-10T05:25:09.185Z",
        "title": "TextToon: Real-Time Text Toonify Head Avatar from Single Video",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.07160.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.05873",
            "authors": [
                {
                    "_id": "6707e251500b1d1f3b493f2b",
                    "name": "Amir Hossein Kargaran",
                    "hidden": false
                },
                {
                    "_id": "6707e251500b1d1f3b493f2c",
                    "name": "Ali Modarressi",
                    "hidden": false
                },
                {
                    "_id": "6707e251500b1d1f3b493f2d",
                    "name": "Nafiseh Nikeghbal",
                    "hidden": false
                },
                {
                    "_id": "6707e251500b1d1f3b493f2e",
                    "name": "Jana Diesner",
                    "hidden": false
                },
                {
                    "_id": "6707e251500b1d1f3b493f2f",
                    "name": "François Yvon",
                    "hidden": false
                },
                {
                    "_id": "6707e251500b1d1f3b493f30",
                    "name": "Hinrich Schütze",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-08T09:59:23.000Z",
            "title": "MEXA: Multilingual Evaluation of English-Centric LLMs via Cross-Lingual\n  Alignment",
            "summary": "English-centric large language models (LLMs) often show strong multilingual\ncapabilities. However, the multilingual performance of these models remains\nunclear and is not thoroughly evaluated for many languages. Most benchmarks for\nmultilinguality focus on classic NLP tasks, or cover a minimal number of\nlanguages. We introduce MEXA, a method for assessing the multilingual\ncapabilities of pre-trained English-centric LLMs using parallel sentences,\nwhich are available for more languages than existing downstream tasks. MEXA\nleverages the fact that English-centric LLMs use English as a kind of pivot\nlanguage in their intermediate layers. It computes the alignment between\nEnglish and non-English languages using parallel sentences to evaluate the\ntransfer of language understanding from English to other languages. This\nalignment can be used to estimate model performance in other languages. We\nconduct studies using various parallel datasets (FLORES-200 and Bible), models\n(Llama family, Gemma family, Mistral, and OLMo), and established downstream\ntasks (Belebele, m-MMLU, and m-ARC). We explore different methods to compute\nembeddings in decoder-only models. Our results show that MEXA, in its default\nsettings, achieves a statistically significant average Pearson correlation of\n0.90 with three established downstream tasks across nine models and two\nparallel datasets. This suggests that MEXA is a reliable method for estimating\nthe multilingual capabilities of English-centric LLMs, providing a clearer\nunderstanding of their multilingual potential and the inner workings of LLMs.\nLeaderboard: https://huggingface.co/spaces/cis-lmu/Mexa, Code:\nhttps://github.com/cisnlp/Mexa.",
            "upvotes": 2,
            "discussionId": "6707e252500b1d1f3b493f65"
        },
        "publishedAt": "2024-10-10T12:51:34.775Z",
        "title": "MEXA: Multilingual Evaluation of English-Centric LLMs via Cross-Lingual Alignment",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/61bf84c8ca59d6d196a1b4e8/tzHHDjgTSLumgqCbRjcTx.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.05873.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61bf84c8ca59d6d196a1b4e8/L_NvUwlMYcye9X35z6f7e.jpeg",
            "fullname": "Amir Hossein Kargaran",
            "name": "kargaranamir",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.06468",
            "authors": [
                {
                    "_id": "6707b786ce33247b815f04c4",
                    "user": {
                        "_id": "653c470c3bd6135805014fb4",
                        "avatarUrl": "/avatars/3ce1456cafb7dbf6030ea6f775e235cc.svg",
                        "isPro": false,
                        "fullname": "Santhosh Kumar Ramakrishnan",
                        "user": "sramakrishnan",
                        "type": "user"
                    },
                    "name": "Santhosh Kumar Ramakrishnan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T11:23:47.804Z",
                    "hidden": false
                },
                {
                    "_id": "6707b786ce33247b815f04c5",
                    "user": {
                        "_id": "664ce8c217586a9634370552",
                        "avatarUrl": "/avatars/b8f96ed0dc321c833d654eab24a2443d.svg",
                        "isPro": false,
                        "fullname": "Erik Wijmans",
                        "user": "erikwijmans",
                        "type": "user"
                    },
                    "name": "Erik Wijmans",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T11:23:53.318Z",
                    "hidden": false
                },
                {
                    "_id": "6707b786ce33247b815f04c6",
                    "user": {
                        "_id": "665a93a3212eb0d2f4e0478d",
                        "avatarUrl": "/avatars/de33167b3dacc518562c62e9122db0fa.svg",
                        "isPro": false,
                        "fullname": "Philipp Kraehenbuehl",
                        "user": "philkra",
                        "type": "user"
                    },
                    "name": "Philipp Kraehenbuehl",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T11:23:58.952Z",
                    "hidden": false
                },
                {
                    "_id": "6707b786ce33247b815f04c7",
                    "user": {
                        "_id": "6707b1a9356f027ca986d6b1",
                        "avatarUrl": "/avatars/d32eae81b27dc4f7090196d410e96876.svg",
                        "isPro": false,
                        "fullname": "Vladlen Koltun",
                        "user": "vkoltun",
                        "type": "user"
                    },
                    "name": "Vladlen Koltun",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T11:24:04.344Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-09T01:41:49.000Z",
            "title": "Does Spatial Cognition Emerge in Frontier Models?",
            "summary": "Not yet. We present SPACE, a benchmark that systematically evaluates spatial\ncognition in frontier models. Our benchmark builds on decades of research in\ncognitive science. It evaluates large-scale mapping abilities that are brought\nto bear when an organism traverses physical environments, smaller-scale\nreasoning about object shapes and layouts, and cognitive infrastructure such as\nspatial attention and memory. For many tasks, we instantiate parallel\npresentations via text and images, allowing us to benchmark both large language\nmodels and large multimodal models. Results suggest that contemporary frontier\nmodels fall short of the spatial intelligence of animals, performing near\nchance level on a number of classic tests of animal cognition.",
            "upvotes": 1,
            "discussionId": "6707b787ce33247b815f0517"
        },
        "publishedAt": "2024-10-10T09:47:28.563Z",
        "title": "Does Spatial Cognition Emerge in Frontier Models?",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.06468.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/d32eae81b27dc4f7090196d410e96876.svg",
            "fullname": "Vladlen Koltun",
            "name": "vkoltun",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.07095",
            "authors": [
                {
                    "_id": "67077d44503533522daed22f",
                    "name": "Jun Shern Chan",
                    "hidden": false
                },
                {
                    "_id": "67077d44503533522daed230",
                    "user": {
                        "_id": "63ec754359a46fd5f4b81a2f",
                        "avatarUrl": "/avatars/aaa4a63b25bd03c366f855427ba9cad4.svg",
                        "isPro": false,
                        "fullname": "Neil Chowdhury",
                        "user": "nch0w",
                        "type": "user"
                    },
                    "name": "Neil Chowdhury",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T09:44:54.162Z",
                    "hidden": false
                },
                {
                    "_id": "67077d44503533522daed231",
                    "name": "Oliver Jaffe",
                    "hidden": false
                },
                {
                    "_id": "67077d44503533522daed232",
                    "name": "James Aung",
                    "hidden": false
                },
                {
                    "_id": "67077d44503533522daed233",
                    "name": "Dane Sherburn",
                    "hidden": false
                },
                {
                    "_id": "67077d44503533522daed234",
                    "user": {
                        "_id": "636d9548da13cd91f922e0d0",
                        "avatarUrl": "/avatars/488f6a4b12f20809d3a4c3d40ae82b53.svg",
                        "isPro": false,
                        "fullname": "Evan Mays",
                        "user": "evanmays",
                        "type": "user"
                    },
                    "name": "Evan Mays",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T09:44:33.433Z",
                    "hidden": false
                },
                {
                    "_id": "67077d44503533522daed235",
                    "user": {
                        "_id": "627b8bc83974b0ed6b28db67",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1665158475637-627b8bc83974b0ed6b28db67.jpeg",
                        "isPro": false,
                        "fullname": "Giulio Starace",
                        "user": "thesofakillers",
                        "type": "user"
                    },
                    "name": "Giulio Starace",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T09:44:26.853Z",
                    "hidden": false
                },
                {
                    "_id": "67077d44503533522daed236",
                    "name": "Kevin Liu",
                    "hidden": false
                },
                {
                    "_id": "67077d44503533522daed237",
                    "name": "Leon Maksin",
                    "hidden": false
                },
                {
                    "_id": "67077d44503533522daed238",
                    "user": {
                        "_id": "64d4f504887f55fb6eedec74",
                        "avatarUrl": "/avatars/054fb826890adcb330f0e4cbca3ef7c4.svg",
                        "isPro": false,
                        "fullname": "Tejal Patwardhan",
                        "user": "tejalp",
                        "type": "user"
                    },
                    "name": "Tejal Patwardhan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T09:44:12.948Z",
                    "hidden": false
                },
                {
                    "_id": "67077d44503533522daed239",
                    "user": {
                        "_id": "5fb2dc322a79d831217aa0cb",
                        "avatarUrl": "/avatars/c4bdc2f907443e34d75116730105510c.svg",
                        "isPro": false,
                        "fullname": "Lilian Weng",
                        "user": "lilianweng",
                        "type": "user"
                    },
                    "name": "Lilian Weng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-10T09:44:07.283Z",
                    "hidden": false
                },
                {
                    "_id": "67077d44503533522daed23a",
                    "name": "Aleksander Mądry",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-09T17:34:27.000Z",
            "title": "MLE-bench: Evaluating Machine Learning Agents on Machine Learning\n  Engineering",
            "summary": "We introduce MLE-bench, a benchmark for measuring how well AI agents perform\nat machine learning engineering. To this end, we curate 75 ML\nengineering-related competitions from Kaggle, creating a diverse set of\nchallenging tasks that test real-world ML engineering skills such as training\nmodels, preparing datasets, and running experiments. We establish human\nbaselines for each competition using Kaggle's publicly available leaderboards.\nWe use open-source agent scaffolds to evaluate several frontier language models\non our benchmark, finding that the best-performing setup--OpenAI's o1-preview\nwith AIDE scaffolding--achieves at least the level of a Kaggle bronze medal in\n16.9% of competitions. In addition to our main results, we investigate various\nforms of resource scaling for AI agents and the impact of contamination from\npre-training. We open-source our benchmark code (github.com/openai/mle-bench/)\nto facilitate future research in understanding the ML engineering capabilities\nof AI agents.",
            "upvotes": 1,
            "discussionId": "67077d45503533522daed271"
        },
        "publishedAt": "2024-10-10T05:38:28.941Z",
        "title": "MLE-bench: Evaluating Machine Learning Agents on Machine Learning Engineering",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.07095.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62e0ef42edb0462c8d51818d/3YM7DUynIWiiRFM6_enpg.jpeg",
            "fullname": "Ting-En Lin",
            "name": "tnlin",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.07145",
            "authors": [
                {
                    "_id": "670801c33f448c140030ee45",
                    "name": "Yingfa Chen",
                    "hidden": false
                },
                {
                    "_id": "670801c33f448c140030ee46",
                    "name": "Xinrong Zhang",
                    "hidden": false
                },
                {
                    "_id": "670801c33f448c140030ee47",
                    "name": "Shengding Hu",
                    "hidden": false
                },
                {
                    "_id": "670801c33f448c140030ee48",
                    "name": "Xu Han",
                    "hidden": false
                },
                {
                    "_id": "670801c33f448c140030ee49",
                    "name": "Zhiyuan Liu",
                    "hidden": false
                },
                {
                    "_id": "670801c33f448c140030ee4a",
                    "name": "Maosong Sun",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-09T17:54:28.000Z",
            "title": "Stuffed Mamba: State Collapse and State Capacity of RNN-Based\n  Long-Context Modeling",
            "summary": "One essential advantage of recurrent neural networks (RNNs) over\ntransformer-based language models is their linear computational complexity\nconcerning the sequence length, which makes them much faster in handling long\nsequences during inference. However, most publicly available RNNs (e.g., Mamba\nand RWKV) are trained on sequences with less than 10K tokens, and their\neffectiveness in longer contexts remains largely unsatisfying so far. In this\npaper, we study the cause of the inability to process long context for RNNs and\nsuggest critical mitigations. We examine two practical concerns when applying\nstate-of-the-art RNNs to long contexts: (1) the inability to extrapolate to\ninputs longer than the training length and (2) the upper bound of memory\ncapacity. Addressing the first concern, we first investigate *state collapse*\n(SC), a phenomenon that causes severe performance degradation on sequence\nlengths not encountered during training. With controlled experiments, we\nattribute this to overfitting due to the recurrent state being\noverparameterized for the training length. For the second concern, we train a\nseries of Mamba-2 models on long documents to empirically estimate the\nrecurrent state capacity in language modeling and passkey retrieval. Then,\nthree SC mitigation methods are proposed to improve Mamba-2's length\ngeneralizability, allowing the model to process more than 1M tokens without SC.\nWe also find that the recurrent state capacity in passkey retrieval scales\nexponentially to the state size, and we empirically train a Mamba-2 370M with\nnear-perfect passkey retrieval accuracy on 256K context length. This suggests a\npromising future for RNN-based long-context modeling.",
            "upvotes": 0,
            "discussionId": "670801c63f448c140030eeca"
        },
        "publishedAt": "2024-10-10T15:04:10.168Z",
        "title": "Stuffed Mamba: State Collapse and State Capacity of RNN-Based Long-Context Modeling",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.07145.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/c7a77d8b0aeb2a81bffac5c09edad9ca.svg",
            "fullname": "Yingfa Chen",
            "name": "chen-yingfa",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    }
]