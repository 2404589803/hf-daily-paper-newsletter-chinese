[
  {
    "paper": {
      "id": "2506.18095",
      "authors": [
        {
          "_id": "685a0ac20e4ad7e2197584ea",
          "name": "Junying Chen",
          "hidden": false
        },
        {
          "_id": "685a0ac20e4ad7e2197584eb",
          "name": "Zhenyang Cai",
          "hidden": false
        },
        {
          "_id": "685a0ac20e4ad7e2197584ec",
          "name": "Pengcheng Chen",
          "hidden": false
        },
        {
          "_id": "685a0ac20e4ad7e2197584ed",
          "name": "Shunian Chen",
          "hidden": false
        },
        {
          "_id": "685a0ac20e4ad7e2197584ee",
          "name": "Ke Ji",
          "hidden": false
        },
        {
          "_id": "685a0ac20e4ad7e2197584ef",
          "name": "Xidong Wang",
          "hidden": false
        },
        {
          "_id": "685a0ac20e4ad7e2197584f0",
          "name": "Yunjin Yang",
          "hidden": false
        },
        {
          "_id": "685a0ac20e4ad7e2197584f1",
          "name": "Benyou Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-22T16:51:09.000Z",
      "submittedOnDailyAt": "2025-06-26T02:58:42.859Z",
      "title": "ShareGPT-4o-Image: Aligning Multimodal Models with GPT-4o-Level Image\n  Generation",
      "submittedOnDailyBy": {
        "_id": "64097dd1b6a334f53e2b3e4c",
        "avatarUrl": "/avatars/18d036aab5e096054a8706bc78027126.svg",
        "isPro": false,
        "fullname": "Junying Chen",
        "user": "jymcc",
        "type": "user"
      },
      "summary": "Recent advances in multimodal generative models have unlocked photorealistic,\ninstruction-aligned image generation, yet leading systems like GPT-4o-Image\nremain proprietary and inaccessible. To democratize these capabilities, we\npresent ShareGPT-4o-Image, the first dataset comprising 45K text-to-image and\n46K text-and-image-to-image data, all synthesized using GPT-4o's image\ngeneration capabilities for distilling its advanced image generation abilities.\nLeveraging this dataset, we develop Janus-4o, a multimodal large language model\ncapable of both text-to-image and text-and-image-to-image generation. Janus-4o\nnot only significantly improves text-to-image generation over its predecessor,\nJanus-Pro, but also newly supports text-and-image-to-image generation. Notably,\nit achieves impressive performance in text-and-image-to-image generation from\nscratch, using only 91K synthetic samples and 6 hours of training on an 8\nA800-GPU machine. We hope the release of ShareGPT-4o-Image and Janus-4o will\nfoster open research in photorealistic, instruction-aligned image generation.",
      "upvotes": 39,
      "discussionId": "685a0ac30e4ad7e2197584f2",
      "githubRepo": "https://github.com/FreedomIntelligence/ShareGPT-4o-Image",
      "ai_summary": "ShareGPT-4o-Image and Janus-4o enable open research in photorealistic, instruction-aligned image generation through a large dataset and multimodal model.",
      "ai_keywords": [
        "multimodal generative models",
        "text-to-image",
        "text-and-image-to-image",
        "photorealistic",
        "instruction-aligned",
        "dataset",
        "large language model",
        "synthetic samples"
      ],
      "githubStars": 40
    },
    "publishedAt": "2025-06-22T12:51:09.000Z",
    "title": "ShareGPT-4o-Image: Aligning Multimodal Models with GPT-4o-Level Image\n  Generation",
    "summary": "Recent advances in multimodal generative models have unlocked photorealistic,\ninstruction-aligned image generation, yet leading systems like GPT-4o-Image\nremain proprietary and inaccessible. To democratize these capabilities, we\npresent ShareGPT-4o-Image, the first dataset comprising 45K text-to-image and\n46K text-and-image-to-image data, all synthesized using GPT-4o's image\ngeneration capabilities for distilling its advanced image generation abilities.\nLeveraging this dataset, we develop Janus-4o, a multimodal large language model\ncapable of both text-to-image and text-and-image-to-image generation. Janus-4o\nnot only significantly improves text-to-image generation over its predecessor,\nJanus-Pro, but also newly supports text-and-image-to-image generation. Notably,\nit achieves impressive performance in text-and-image-to-image generation from\nscratch, using only 91K synthetic samples and 6 hours of training on an 8\nA800-GPU machine. We hope the release of ShareGPT-4o-Image and Janus-4o will\nfoster open research in photorealistic, instruction-aligned image generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18095.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64097dd1b6a334f53e2b3e4c",
      "avatarUrl": "/avatars/18d036aab5e096054a8706bc78027126.svg",
      "fullname": "Junying Chen",
      "name": "jymcc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.19697",
      "authors": [
        {
          "_id": "685c1546df8a0d6c70bbf94e",
          "name": "Jungwoo Park",
          "hidden": false
        },
        {
          "_id": "685c1546df8a0d6c70bbf94f",
          "name": "Taewhoo Lee",
          "hidden": false
        },
        {
          "_id": "685c1546df8a0d6c70bbf950",
          "name": "Chanwoong Yoon",
          "hidden": false
        },
        {
          "_id": "685c1546df8a0d6c70bbf951",
          "name": "Hyeon Hwang",
          "hidden": false
        },
        {
          "_id": "685c1546df8a0d6c70bbf952",
          "name": "Jaewoo Kang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-24T15:03:57.000Z",
      "submittedOnDailyAt": "2025-06-26T02:17:13.263Z",
      "title": "Outlier-Safe Pre-Training for Robust 4-Bit Quantization of Large\n  Language Models",
      "submittedOnDailyBy": {
        "_id": "60f8435644e75317cc02ed51",
        "avatarUrl": "/avatars/68b7fc077fe2bda6607b1c470add8140.svg",
        "isPro": false,
        "fullname": "Jungwoo Park",
        "user": "affjljoo3581",
        "type": "user"
      },
      "summary": "Extreme activation outliers in Large Language Models (LLMs) critically\ndegrade quantization performance, hindering efficient on-device deployment.\nWhile channel-wise operations and adaptive gradient scaling are recognized\ncauses, practical mitigation remains challenging. We introduce Outlier-Safe\nPre-Training (OSP), a practical guideline that proactively prevents outlier\nformation rather than relying on post-hoc mitigation. OSP combines three key\ninnovations: (1) the Muon optimizer, eliminating privileged bases while\nmaintaining training efficiency; (2) Single-Scale RMSNorm, preventing\nchannel-wise amplification; and (3) a learnable embedding projection,\nredistributing activation magnitudes originating from embedding matrices. We\nvalidate OSP by training a 1.4B-parameter model on 1 trillion tokens, which is\nthe first production-scale LLM trained without such outliers. Under aggressive\n4-bit quantization, our OSP model achieves a 35.7 average score across 10\nbenchmarks (compared to 26.5 for an Adam-trained model), with only a 2%\ntraining overhead. Remarkably, OSP models exhibit near-zero excess kurtosis\n(0.04) compared to extreme values (1818.56) in standard models, fundamentally\naltering LLM quantization behavior. Our work demonstrates that outliers are not\ninherent to LLMs but are consequences of training strategies, paving the way\nfor more efficient LLM deployment. The source code and pretrained checkpoints\nare available at https://github.com/dmis-lab/Outlier-Safe-Pre-Training.",
      "upvotes": 27,
      "discussionId": "685c1546df8a0d6c70bbf953",
      "githubRepo": "https://github.com/dmis-lab/Outlier-Safe-Pre-Training",
      "ai_summary": "Outlier-Safe Pre-Training improves large language model quantization performance by preventing extreme activation outliers through innovative training techniques.",
      "ai_keywords": [
        "Muon optimizer",
        "Single-Scale RMSNorm",
        "learnable embedding projection",
        "outlier formation",
        "quantization performance",
        "LLM deployment",
        "excess kurtosis"
      ],
      "githubStars": 2
    },
    "publishedAt": "2025-06-24T11:03:57.000Z",
    "title": "Outlier-Safe Pre-Training for Robust 4-Bit Quantization of Large\n  Language Models",
    "summary": "Extreme activation outliers in Large Language Models (LLMs) critically\ndegrade quantization performance, hindering efficient on-device deployment.\nWhile channel-wise operations and adaptive gradient scaling are recognized\ncauses, practical mitigation remains challenging. We introduce Outlier-Safe\nPre-Training (OSP), a practical guideline that proactively prevents outlier\nformation rather than relying on post-hoc mitigation. OSP combines three key\ninnovations: (1) the Muon optimizer, eliminating privileged bases while\nmaintaining training efficiency; (2) Single-Scale RMSNorm, preventing\nchannel-wise amplification; and (3) a learnable embedding projection,\nredistributing activation magnitudes originating from embedding matrices. We\nvalidate OSP by training a 1.4B-parameter model on 1 trillion tokens, which is\nthe first production-scale LLM trained without such outliers. Under aggressive\n4-bit quantization, our OSP model achieves a 35.7 average score across 10\nbenchmarks (compared to 26.5 for an Adam-trained model), with only a 2%\ntraining overhead. Remarkably, OSP models exhibit near-zero excess kurtosis\n(0.04) compared to extreme values (1818.56) in standard models, fundamentally\naltering LLM quantization behavior. Our work demonstrates that outliers are not\ninherent to LLMs but are consequences of training strategies, paving the way\nfor more efficient LLM deployment. The source code and pretrained checkpoints\nare available at https://github.com/dmis-lab/Outlier-Safe-Pre-Training.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19697.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f8435644e75317cc02ed51",
      "avatarUrl": "/avatars/68b7fc077fe2bda6607b1c470add8140.svg",
      "fullname": "Jungwoo Park",
      "name": "affjljoo3581",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.16012",
      "authors": [
        {
          "_id": "685cf7c0696820ba1f28f2ea",
          "name": "Boyu Li",
          "hidden": false
        },
        {
          "_id": "685cf7c0696820ba1f28f2eb",
          "name": "Siyuan He",
          "hidden": false
        },
        {
          "_id": "685cf7c0696820ba1f28f2ec",
          "name": "Hang Xu",
          "hidden": false
        },
        {
          "_id": "685cf7c0696820ba1f28f2ed",
          "name": "Haoqi Yuan",
          "hidden": false
        },
        {
          "_id": "685cf7c0696820ba1f28f2ee",
          "name": "Yu Zang",
          "hidden": false
        },
        {
          "_id": "685cf7c0696820ba1f28f2ef",
          "name": "Liwei Hu",
          "hidden": false
        },
        {
          "_id": "685cf7c0696820ba1f28f2f0",
          "name": "Junpeng Yue",
          "hidden": false
        },
        {
          "_id": "685cf7c0696820ba1f28f2f1",
          "name": "Zhenxiong Jiang",
          "hidden": false
        },
        {
          "_id": "685cf7c0696820ba1f28f2f2",
          "name": "Pengbo Hu",
          "hidden": false
        },
        {
          "_id": "685cf7c0696820ba1f28f2f3",
          "name": "Börje F. Karlsson",
          "hidden": false
        },
        {
          "_id": "685cf7c0696820ba1f28f2f4",
          "name": "Yehui Tang",
          "hidden": false
        },
        {
          "_id": "685cf7c0696820ba1f28f2f5",
          "name": "Zongqing Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-19T04:13:36.000Z",
      "submittedOnDailyAt": "2025-06-26T06:05:04.111Z",
      "title": "DualTHOR: A Dual-Arm Humanoid Simulation Platform for Contingency-Aware\n  Planning",
      "submittedOnDailyBy": {
        "_id": "61e52be53d6dbb1da842316a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61e52be53d6dbb1da842316a/gx0WGPcOCClXPymoKglc4.jpeg",
        "isPro": false,
        "fullname": "Börje Karlsson",
        "user": "tellarin",
        "type": "user"
      },
      "summary": "Developing embodied agents capable of performing complex interactive tasks in\nreal-world scenarios remains a fundamental challenge in embodied AI. Although\nrecent advances in simulation platforms have greatly enhanced task diversity to\ntrain embodied Vision Language Models (VLMs), most platforms rely on simplified\nrobot morphologies and bypass the stochastic nature of low-level execution,\nwhich limits their transferability to real-world robots. To address these\nissues, we present a physics-based simulation platform DualTHOR for complex\ndual-arm humanoid robots, built upon an extended version of AI2-THOR. Our\nsimulator includes real-world robot assets, a task suite for dual-arm\ncollaboration, and inverse kinematics solvers for humanoid robots. We also\nintroduce a contingency mechanism that incorporates potential failures through\nphysics-based low-level execution, bridging the gap to real-world scenarios.\nOur simulator enables a more comprehensive evaluation of the robustness and\ngeneralization of VLMs in household environments. Extensive evaluations reveal\nthat current VLMs struggle with dual-arm coordination and exhibit limited\nrobustness in realistic environments with contingencies, highlighting the\nimportance of using our simulator to develop more capable VLMs for embodied\ntasks. The code is available at https://github.com/ds199895/DualTHOR.git.",
      "upvotes": 7,
      "discussionId": "685cf7c1696820ba1f28f2f6",
      "ai_summary": "A simulator named DualTHOR for training dual-arm humanoid robots integrates real-world assets and physics to enhance the robustness and generalization of Vision Language Models.",
      "ai_keywords": [
        "embodied AI",
        "Vision Language Models",
        "VLMs",
        "physics-based simulation",
        "DualTHOR",
        "AI2-THOR",
        "dual-arm robots",
        "real-world robot assets",
        "task suite",
        "inverse kinematics solvers",
        "contingency mechanism",
        "physics-based low-level execution"
      ]
    },
    "publishedAt": "2025-06-19T00:13:36.000Z",
    "title": "DualTHOR: A Dual-Arm Humanoid Simulation Platform for Contingency-Aware\n  Planning",
    "summary": "Developing embodied agents capable of performing complex interactive tasks in\nreal-world scenarios remains a fundamental challenge in embodied AI. Although\nrecent advances in simulation platforms have greatly enhanced task diversity to\ntrain embodied Vision Language Models (VLMs), most platforms rely on simplified\nrobot morphologies and bypass the stochastic nature of low-level execution,\nwhich limits their transferability to real-world robots. To address these\nissues, we present a physics-based simulation platform DualTHOR for complex\ndual-arm humanoid robots, built upon an extended version of AI2-THOR. Our\nsimulator includes real-world robot assets, a task suite for dual-arm\ncollaboration, and inverse kinematics solvers for humanoid robots. We also\nintroduce a contingency mechanism that incorporates potential failures through\nphysics-based low-level execution, bridging the gap to real-world scenarios.\nOur simulator enables a more comprehensive evaluation of the robustness and\ngeneralization of VLMs in household environments. Extensive evaluations reveal\nthat current VLMs struggle with dual-arm coordination and exhibit limited\nrobustness in realistic environments with contingencies, highlighting the\nimportance of using our simulator to develop more capable VLMs for embodied\ntasks. The code is available at https://github.com/ds199895/DualTHOR.git.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.16012.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61e52be53d6dbb1da842316a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61e52be53d6dbb1da842316a/gx0WGPcOCClXPymoKglc4.jpeg",
      "fullname": "Börje Karlsson",
      "name": "tellarin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 26
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.18088",
      "authors": [
        {
          "_id": "685ae8e8d2ee4fac76521d03",
          "user": {
            "_id": "65b37a9b06d8b55123ef8921",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b37a9b06d8b55123ef8921/CT5tLwezjXct1eTszA8sO.jpeg",
            "isPro": false,
            "fullname": "Tianxing Chen",
            "user": "TianxingChen",
            "type": "user"
          },
          "name": "Tianxing Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-24T18:13:30.491Z",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d04",
          "name": "Zanxin Chen",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d05",
          "name": "Baijun Chen",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d06",
          "name": "Zijian Cai",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d07",
          "name": "Yibin Liu",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d08",
          "name": "Qiwei Liang",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d09",
          "name": "Zixuan Li",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d0a",
          "name": "Xianliang Lin",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d0b",
          "name": "Yiheng Ge",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d0c",
          "name": "Zhenyu Gu",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d0d",
          "name": "Weiliang Deng",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d0e",
          "name": "Yubin Guo",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d0f",
          "name": "Tian Nian",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d10",
          "name": "Xuanbing Xie",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d11",
          "name": "Qiangyu Chen",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d12",
          "name": "Kailun Su",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d13",
          "name": "Tianling Xu",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d14",
          "name": "Guodong Liu",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d15",
          "name": "Mengkang Hu",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d16",
          "name": "Huan-ang Gao",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d17",
          "name": "Kaixuan Wang",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d18",
          "name": "Zhixuan Liang",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d19",
          "name": "Yusen Qin",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d1a",
          "name": "Xiaokang Yang",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d1b",
          "name": "Ping Luo",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d1c",
          "name": "Yao Mu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-22T16:26:53.000Z",
      "submittedOnDailyAt": "2025-06-26T06:13:35.593Z",
      "title": "RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain\n  Randomization for Robust Bimanual Robotic Manipulation",
      "submittedOnDailyBy": {
        "_id": "65b37a9b06d8b55123ef8921",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b37a9b06d8b55123ef8921/CT5tLwezjXct1eTszA8sO.jpeg",
        "isPro": false,
        "fullname": "Tianxing Chen",
        "user": "TianxingChen",
        "type": "user"
      },
      "summary": "Simulation-based data synthesis has emerged as a powerful paradigm for\nenhancing real-world robotic manipulation. However, existing synthetic datasets\nremain insufficient for robust bimanual manipulation due to two challenges: (1)\nthe lack of an efficient, scalable data generation method for novel tasks, and\n(2) oversimplified simulation environments that fail to capture real-world\ncomplexity. We present RoboTwin 2.0, a scalable simulation framework that\nenables automated, large-scale generation of diverse and realistic data, along\nwith unified evaluation protocols for dual-arm manipulation. We first construct\nRoboTwin-OD, a large-scale object library comprising 731 instances across 147\ncategories, each annotated with semantic and manipulation-relevant labels.\nBuilding on this foundation, we develop an expert data synthesis pipeline that\ncombines multimodal large language models (MLLMs) with simulation-in-the-loop\nrefinement to generate task-level execution code automatically. To improve\nsim-to-real transfer, RoboTwin 2.0 incorporates structured domain randomization\nalong five axes: clutter, lighting, background, tabletop height and language\ninstructions, thereby enhancing data diversity and policy robustness. We\ninstantiate this framework across 50 dual-arm tasks spanning five robot\nembodiments, and pre-collect over 100,000 domain-randomized expert\ntrajectories. Empirical results show a 10.9% gain in code generation success\nand improved generalization to novel real-world scenarios. A VLA model\nfine-tuned on our dataset achieves a 367% relative improvement (42.0% vs. 9.0%)\non unseen scene real-world tasks, while zero-shot models trained solely on our\nsynthetic data achieve a 228% relative gain, highlighting strong generalization\nwithout real-world supervision. We release the data generator, benchmark,\ndataset, and code to support scalable research in robust bimanual manipulation.",
      "upvotes": 5,
      "discussionId": "685ae8e8d2ee4fac76521d1d",
      "projectPage": "https://robotwin-platform.github.io/",
      "githubRepo": "https://github.com/robotwin-Platform/RoboTwin",
      "ai_summary": "RoboTwin 2.0 is a scalable simulation framework for bimanual robotic manipulation that uses expert data synthesis and structured domain randomization to generate diverse and realistic synthetic data, improving sim-to-real transfer and generalization.",
      "ai_keywords": [
        "multimodal large language models",
        "simulation-in-the-loop",
        "structured domain randomization",
        "bimanual manipulation",
        "task-level execution code",
        "sim-to-real transfer",
        "zero-shot learning"
      ]
    },
    "publishedAt": "2025-06-22T12:26:53.000Z",
    "title": "RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain\n  Randomization for Robust Bimanual Robotic Manipulation",
    "summary": "Simulation-based data synthesis has emerged as a powerful paradigm for\nenhancing real-world robotic manipulation. However, existing synthetic datasets\nremain insufficient for robust bimanual manipulation due to two challenges: (1)\nthe lack of an efficient, scalable data generation method for novel tasks, and\n(2) oversimplified simulation environments that fail to capture real-world\ncomplexity. We present RoboTwin 2.0, a scalable simulation framework that\nenables automated, large-scale generation of diverse and realistic data, along\nwith unified evaluation protocols for dual-arm manipulation. We first construct\nRoboTwin-OD, a large-scale object library comprising 731 instances across 147\ncategories, each annotated with semantic and manipulation-relevant labels.\nBuilding on this foundation, we develop an expert data synthesis pipeline that\ncombines multimodal large language models (MLLMs) with simulation-in-the-loop\nrefinement to generate task-level execution code automatically. To improve\nsim-to-real transfer, RoboTwin 2.0 incorporates structured domain randomization\nalong five axes: clutter, lighting, background, tabletop height and language\ninstructions, thereby enhancing data diversity and policy robustness. We\ninstantiate this framework across 50 dual-arm tasks spanning five robot\nembodiments, and pre-collect over 100,000 domain-randomized expert\ntrajectories. Empirical results show a 10.9% gain in code generation success\nand improved generalization to novel real-world scenarios. A VLA model\nfine-tuned on our dataset achieves a 367% relative improvement (42.0% vs. 9.0%)\non unseen scene real-world tasks, while zero-shot models trained solely on our\nsynthetic data achieve a 228% relative gain, highlighting strong generalization\nwithout real-world supervision. We release the data generator, benchmark,\ndataset, and code to support scalable research in robust bimanual manipulation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18088.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65b37a9b06d8b55123ef8921",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b37a9b06d8b55123ef8921/CT5tLwezjXct1eTszA8sO.jpeg",
      "fullname": "Tianxing Chen",
      "name": "TianxingChen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.18674",
      "authors": [
        {
          "_id": "685cd8fc696820ba1f28f2aa",
          "name": "Raquel Ferrando",
          "hidden": false
        },
        {
          "_id": "685cd8fc696820ba1f28f2ab",
          "name": "Javier Conde",
          "hidden": false
        },
        {
          "_id": "685cd8fc696820ba1f28f2ac",
          "name": "Gonzalo Martínez",
          "hidden": false
        },
        {
          "_id": "685cd8fc696820ba1f28f2ad",
          "name": "Pedro Reviriego",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-23T14:18:46.000Z",
      "submittedOnDailyAt": "2025-06-26T03:52:31.234Z",
      "title": "Is There a Case for Conversation Optimized Tokenizers in Large Language\n  Models?",
      "submittedOnDailyBy": {
        "_id": "64f31365ed48e3bb9c487d5d",
        "avatarUrl": "/avatars/979c1979eadbd4529c95b925bbb58d78.svg",
        "isPro": false,
        "fullname": "Gonzalo",
        "user": "gonzmart",
        "type": "user"
      },
      "summary": "The computational and energy costs of Large Language Models (LLMs) have\nincreased exponentially driven by the growing model sizes and the massive\nadoption of LLMs by hundreds of millions of users. The unit cost of an LLM is\nthe computation of a token. Therefore, the tokenizer plays an important role in\nthe efficiency of a model, and they are carefully optimized to minimize the\nnumber of tokens for the text in their training corpus. One of the most popular\napplications of LLMs are chatbots that interact with users. A key observation\nis that, for those chatbots, what is important is the performance of the\ntokenizer in the user text input and the chatbot responses. Those are most\nlikely different from the text in the training corpus. So, a question that\nimmediately arises is whether there is a potential benefit in optimizing\ntokenizers for chatbot conversations. In this paper, this idea is explored for\ndifferent tokenizers by using a publicly available corpus of chatbot\nconversations to redesign their vocabularies and evaluate their performance in\nthis domain. The results show that conversation-optimized tokenizers\nconsistently reduce the number of tokens in chatbot dialogues, which can lead\nto meaningful energy savings, in the range of 5% to 10% while having minimal or\neven slightly positive impact on tokenization efficiency for the original\ntraining corpus.",
      "upvotes": 4,
      "discussionId": "685cd8fc696820ba1f28f2ae",
      "ai_summary": "Optimizing tokenizers for chatbot conversations reduces computational costs and energy usage with minimal impact on training corpus performance.",
      "ai_keywords": [
        "Large Language Models",
        "token",
        "tokenizer",
        "chatbots",
        "conversation-optimized tokenizers"
      ]
    },
    "publishedAt": "2025-06-23T10:18:46.000Z",
    "title": "Is There a Case for Conversation Optimized Tokenizers in Large Language\n  Models?",
    "summary": "The computational and energy costs of Large Language Models (LLMs) have\nincreased exponentially driven by the growing model sizes and the massive\nadoption of LLMs by hundreds of millions of users. The unit cost of an LLM is\nthe computation of a token. Therefore, the tokenizer plays an important role in\nthe efficiency of a model, and they are carefully optimized to minimize the\nnumber of tokens for the text in their training corpus. One of the most popular\napplications of LLMs are chatbots that interact with users. A key observation\nis that, for those chatbots, what is important is the performance of the\ntokenizer in the user text input and the chatbot responses. Those are most\nlikely different from the text in the training corpus. So, a question that\nimmediately arises is whether there is a potential benefit in optimizing\ntokenizers for chatbot conversations. In this paper, this idea is explored for\ndifferent tokenizers by using a publicly available corpus of chatbot\nconversations to redesign their vocabularies and evaluate their performance in\nthis domain. The results show that conversation-optimized tokenizers\nconsistently reduce the number of tokens in chatbot dialogues, which can lead\nto meaningful energy savings, in the range of 5% to 10% while having minimal or\neven slightly positive impact on tokenization efficiency for the original\ntraining corpus.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18674.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64f31365ed48e3bb9c487d5d",
      "avatarUrl": "/avatars/979c1979eadbd4529c95b925bbb58d78.svg",
      "fullname": "Gonzalo",
      "name": "gonzmart",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.20544",
      "authors": [
        {
          "_id": "685cdd71696820ba1f28f2b8",
          "name": "Ammar Khairi",
          "hidden": false
        },
        {
          "_id": "685cdd71696820ba1f28f2b9",
          "name": "Daniel D'souza",
          "hidden": false
        },
        {
          "_id": "685cdd71696820ba1f28f2ba",
          "name": "Ye Shen",
          "hidden": false
        },
        {
          "_id": "685cdd71696820ba1f28f2bb",
          "name": "Julia Kreutzer",
          "hidden": false
        },
        {
          "_id": "685cdd71696820ba1f28f2bc",
          "name": "Sara Hooker",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-25T15:37:53.000Z",
      "submittedOnDailyAt": "2025-06-26T04:17:49.221Z",
      "title": "When Life Gives You Samples: The Benefits of Scaling up Inference\n  Compute for Multilingual LLMs",
      "submittedOnDailyBy": {
        "_id": "6544e43b12da508864c38f96",
        "avatarUrl": "/avatars/76f0cd55b4bf9c03d2686e146c6f795f.svg",
        "isPro": false,
        "fullname": "Julia Kreutzer",
        "user": "JuliaKreutzerCohere",
        "type": "user"
      },
      "summary": "Recent advancements in large language models (LLMs) have shifted focus toward\nscaling inference-time compute, improving performance without retraining the\nmodel. A common approach is to sample multiple outputs in parallel, and select\none of these as the final output. However, work to date has focused on English\nand a handful of domains such as math and code. In contrast, we are most\ninterested in techniques that generalize across open-ended tasks, formally\nverifiable tasks, and across languages. In this work, we study how to robustly\nscale inference-time compute for open-ended generative tasks in a multilingual,\nmulti-task setting.\n  Our findings show that both sampling strategy based on temperature variation\nand selection strategy must be adapted to account for diverse domains and\nvaried language settings. We evaluate existing selection methods, revealing\nthat strategies effective in English often fail to generalize across languages.\nWe propose novel sampling and selection strategies specifically adapted for\nmultilingual and multi-task inference scenarios, and show they yield notable\ngains across languages and tasks. In particular, our combined sampling and\nselection methods lead to an average +6.8 jump in win-rates for our 8B models\non m-ArenaHard-v2.0 prompts, against proprietary models such as Gemini. At\nlarger scale, Command-A (111B model) equipped with our methods, shows +9.0\nimprovement in win-rates on the same benchmark with just five samples against\nsingle-sample decoding, a substantial increase at minimal cost. Our results\nunderscore the need for language- and task-aware approaches to inference-time\ncompute, aiming to democratize performance improvements in underrepresented\nlanguages.",
      "upvotes": 2,
      "discussionId": "685cdd71696820ba1f28f2bd",
      "ai_summary": "The study examines and proposes new sampling and selection strategies to enhance inference-time compute for multilingual and multi-task large language models, demonstrating significant improvements in win-rates across various languages and tasks.",
      "ai_keywords": [
        "sampling strategy",
        "selection strategy",
        "temperature variation",
        "open-ended generative tasks",
        "multilingual",
        "multi-task",
        "m-ArenaHard-v2.0",
        "win-rates",
        "Command-A",
        "single-sample decoding",
        "language-aware",
        "task-aware"
      ]
    },
    "publishedAt": "2025-06-25T11:37:53.000Z",
    "title": "When Life Gives You Samples: The Benefits of Scaling up Inference\n  Compute for Multilingual LLMs",
    "summary": "Recent advancements in large language models (LLMs) have shifted focus toward\nscaling inference-time compute, improving performance without retraining the\nmodel. A common approach is to sample multiple outputs in parallel, and select\none of these as the final output. However, work to date has focused on English\nand a handful of domains such as math and code. In contrast, we are most\ninterested in techniques that generalize across open-ended tasks, formally\nverifiable tasks, and across languages. In this work, we study how to robustly\nscale inference-time compute for open-ended generative tasks in a multilingual,\nmulti-task setting.\n  Our findings show that both sampling strategy based on temperature variation\nand selection strategy must be adapted to account for diverse domains and\nvaried language settings. We evaluate existing selection methods, revealing\nthat strategies effective in English often fail to generalize across languages.\nWe propose novel sampling and selection strategies specifically adapted for\nmultilingual and multi-task inference scenarios, and show they yield notable\ngains across languages and tasks. In particular, our combined sampling and\nselection methods lead to an average +6.8 jump in win-rates for our 8B models\non m-ArenaHard-v2.0 prompts, against proprietary models such as Gemini. At\nlarger scale, Command-A (111B model) equipped with our methods, shows +9.0\nimprovement in win-rates on the same benchmark with just five samples against\nsingle-sample decoding, a substantial increase at minimal cost. Our results\nunderscore the need for language- and task-aware approaches to inference-time\ncompute, aiming to democratize performance improvements in underrepresented\nlanguages.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.20544.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6544e43b12da508864c38f96",
      "avatarUrl": "/avatars/76f0cd55b4bf9c03d2686e146c6f795f.svg",
      "fullname": "Julia Kreutzer",
      "name": "JuliaKreutzerCohere",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.18403",
      "authors": [
        {
          "_id": "685a555f0e4ad7e2197586b1",
          "name": "Muntasir Adnan",
          "hidden": false
        },
        {
          "_id": "685a555f0e4ad7e2197586b2",
          "name": "Carlos C. N. Kuhn",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-23T08:40:45.000Z",
      "submittedOnDailyAt": "2025-06-26T01:42:45.705Z",
      "title": "The Debugging Decay Index: Rethinking Debugging Strategies for Code LLMs",
      "submittedOnDailyBy": {
        "_id": "65eef9ce7443c09267513796",
        "avatarUrl": "/avatars/62547f99130557f54093b2ff4d6c9c24.svg",
        "isPro": false,
        "fullname": "Muntasir Adnan",
        "user": "adnaan525",
        "type": "user"
      },
      "summary": "The effectiveness of AI debugging follows a predictable exponential decay\npattern; most models lose 60-80% of their debugging capability within just 2-3\nattempts, despite iterative debugging being a critical capability for practical\ncode generation systems. We introduce the Debugging Decay Index (DDI), a\nmathematical framework that quantifies when debugging becomes ineffective and\npredicts intervention points. Our strategic fresh start approach shifts from\nexploitation to exploration at strategic points in the debugging process,\ndemonstrating that well-timed interventions can rescue the effectiveness of\ndebugging. DDI reveals a fundamental limitation in current AI debugging and\nprovides the first quantitative framework for optimising iterative code\ngeneration strategies.",
      "upvotes": 2,
      "discussionId": "685a555f0e4ad7e2197586b3",
      "ai_summary": "The Debugging Decay Index (DDI) quantifies and optimizes the effectiveness of iterative AI debugging by predicting intervention points to revive and enhance debugging capability.",
      "ai_keywords": [
        "AI debugging",
        "Debugging Decay Index (DDI)",
        "iterative debugging",
        "code generation",
        "effectiveness",
        "intervention points"
      ]
    },
    "publishedAt": "2025-06-23T04:40:45.000Z",
    "title": "The Debugging Decay Index: Rethinking Debugging Strategies for Code LLMs",
    "summary": "The effectiveness of AI debugging follows a predictable exponential decay\npattern; most models lose 60-80% of their debugging capability within just 2-3\nattempts, despite iterative debugging being a critical capability for practical\ncode generation systems. We introduce the Debugging Decay Index (DDI), a\nmathematical framework that quantifies when debugging becomes ineffective and\npredicts intervention points. Our strategic fresh start approach shifts from\nexploitation to exploration at strategic points in the debugging process,\ndemonstrating that well-timed interventions can rescue the effectiveness of\ndebugging. DDI reveals a fundamental limitation in current AI debugging and\nprovides the first quantitative framework for optimising iterative code\ngeneration strategies.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18403.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65eef9ce7443c09267513796",
      "avatarUrl": "/avatars/62547f99130557f54093b2ff4d6c9c24.svg",
      "fullname": "Muntasir Adnan",
      "name": "adnaan525",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.19502",
      "authors": [
        {
          "_id": "685c02aadf8a0d6c70bbf918",
          "user": {
            "_id": "674ed490fc7f50ef61c3a7bd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/674ed490fc7f50ef61c3a7bd/j260ICQmNl42aXehBEd6P.jpeg",
            "isPro": false,
            "fullname": "Aleksandr Algazinov",
            "user": "AleksandrAlgazinov",
            "type": "user"
          },
          "name": "Aleksandr Algazinov",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-25T20:59:09.301Z",
          "hidden": false
        },
        {
          "_id": "685c02aadf8a0d6c70bbf919",
          "name": "Matt Laing",
          "hidden": false
        },
        {
          "_id": "685c02aadf8a0d6c70bbf91a",
          "name": "Paul Laban",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-24T10:40:23.000Z",
      "submittedOnDailyAt": "2025-06-26T01:26:07.991Z",
      "title": "MATE: LLM-Powered Multi-Agent Translation Environment for Accessibility\n  Applications",
      "submittedOnDailyBy": {
        "_id": "674ed490fc7f50ef61c3a7bd",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/674ed490fc7f50ef61c3a7bd/j260ICQmNl42aXehBEd6P.jpeg",
        "isPro": false,
        "fullname": "Aleksandr Algazinov",
        "user": "AleksandrAlgazinov",
        "type": "user"
      },
      "summary": "Accessibility remains a critical concern in today's society, as many\ntechnologies are not developed to support the full range of user needs.\nExisting multi-agent systems (MAS) often cannot provide comprehensive\nassistance for users in need due to the lack of customization stemming from\nclosed-source designs. Consequently, individuals with disabilities frequently\nencounter significant barriers when attempting to interact with digital\nenvironments. We introduce MATE, a multimodal accessibility MAS, which performs\nthe modality conversions based on the user's needs. The system is useful for\nassisting people with disabilities by ensuring that data will be converted to\nan understandable format. For instance, if the user cannot see well and\nreceives an image, the system converts this image to its audio description.\nMATE can be applied to a wide range of domains, industries, and areas, such as\nhealthcare, and can become a useful assistant for various groups of users. The\nsystem supports multiple types of models, ranging from LLM API calling to using\ncustom machine learning (ML) classifiers. This flexibility ensures that the\nsystem can be adapted to various needs and is compatible with a wide variety of\nhardware. Since the system is expected to run locally, it ensures the privacy\nand security of sensitive information. In addition, the framework can be\neffectively integrated with institutional technologies (e.g., digital\nhealthcare service) for real-time user assistance. Furthermore, we introduce\nModCon-Task-Identifier, a model that is capable of extracting the precise\nmodality conversion task from the user input. Numerous experiments show that\nModCon-Task-Identifier consistently outperforms other LLMs and statistical\nmodels on our custom data. Our code and data are publicly available at\nhttps://github.com/AlgazinovAleksandr/Multi-Agent-MATE.",
      "upvotes": 1,
      "discussionId": "685c02abdf8a0d6c70bbf91b",
      "ai_summary": "MATE, a multimodal accessibility multi-agent system, converts data into understandable formats based on user needs, supporting various disabilities and integrating with institutional technologies.",
      "ai_keywords": [
        "MULTIAGENT SYSTEMS",
        "MAS",
        "MODALITY CONVERSIONS",
        "LLM API",
        "CUSTOM MACHINE LEARNING CLASSIFIERS",
        "MODCON-TASK-IDENTIFIER",
        "LLM",
        "STATISTICAL MODELS"
      ]
    },
    "publishedAt": "2025-06-24T06:40:23.000Z",
    "title": "MATE: LLM-Powered Multi-Agent Translation Environment for Accessibility\n  Applications",
    "summary": "Accessibility remains a critical concern in today's society, as many\ntechnologies are not developed to support the full range of user needs.\nExisting multi-agent systems (MAS) often cannot provide comprehensive\nassistance for users in need due to the lack of customization stemming from\nclosed-source designs. Consequently, individuals with disabilities frequently\nencounter significant barriers when attempting to interact with digital\nenvironments. We introduce MATE, a multimodal accessibility MAS, which performs\nthe modality conversions based on the user's needs. The system is useful for\nassisting people with disabilities by ensuring that data will be converted to\nan understandable format. For instance, if the user cannot see well and\nreceives an image, the system converts this image to its audio description.\nMATE can be applied to a wide range of domains, industries, and areas, such as\nhealthcare, and can become a useful assistant for various groups of users. The\nsystem supports multiple types of models, ranging from LLM API calling to using\ncustom machine learning (ML) classifiers. This flexibility ensures that the\nsystem can be adapted to various needs and is compatible with a wide variety of\nhardware. Since the system is expected to run locally, it ensures the privacy\nand security of sensitive information. In addition, the framework can be\neffectively integrated with institutional technologies (e.g., digital\nhealthcare service) for real-time user assistance. Furthermore, we introduce\nModCon-Task-Identifier, a model that is capable of extracting the precise\nmodality conversion task from the user input. Numerous experiments show that\nModCon-Task-Identifier consistently outperforms other LLMs and statistical\nmodels on our custom data. Our code and data are publicly available at\nhttps://github.com/AlgazinovAleksandr/Multi-Agent-MATE.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19502.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "674ed490fc7f50ef61c3a7bd",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/674ed490fc7f50ef61c3a7bd/j260ICQmNl42aXehBEd6P.jpeg",
      "fullname": "Aleksandr Algazinov",
      "name": "AleksandrAlgazinov",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  }
]