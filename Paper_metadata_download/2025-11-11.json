[
  {
    "paper": {
      "id": "2511.03506",
      "authors": [
        {
          "_id": "690c081d60494e4fa7675618",
          "name": "Ding Chen",
          "hidden": false
        },
        {
          "_id": "690c081d60494e4fa7675619",
          "name": "Simin Niu",
          "hidden": false
        },
        {
          "_id": "690c081d60494e4fa767561a",
          "name": "Kehang Li",
          "hidden": false
        },
        {
          "_id": "690c081d60494e4fa767561b",
          "name": "Peng Liu",
          "hidden": false
        },
        {
          "_id": "690c081d60494e4fa767561c",
          "name": "Xiangping Zheng",
          "hidden": false
        },
        {
          "_id": "690c081d60494e4fa767561d",
          "name": "Bo Tang",
          "hidden": false
        },
        {
          "_id": "690c081d60494e4fa767561e",
          "name": "Xinchi Li",
          "hidden": false
        },
        {
          "_id": "690c081d60494e4fa767561f",
          "name": "Feiyu Xiong",
          "hidden": false
        },
        {
          "_id": "690c081d60494e4fa7675620",
          "name": "Zhiyu Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-05T14:37:34.000Z",
      "submittedOnDailyAt": "2025-11-11T00:42:05.247Z",
      "title": "HaluMem: Evaluating Hallucinations in Memory Systems of Agents",
      "submittedOnDailyBy": {
        "_id": "64e18e9ec20c27fcc8df384e",
        "avatarUrl": "/avatars/64ef866b9fa385efcefb34ea76b76802.svg",
        "isPro": false,
        "fullname": "Ding Chen",
        "user": "Hush-cd",
        "type": "user"
      },
      "summary": "Memory systems are key components that enable AI systems such as LLMs and AI\nagents to achieve long-term learning and sustained interaction. However, during\nmemory storage and retrieval, these systems frequently exhibit memory\nhallucinations, including fabrication, errors, conflicts, and omissions.\nExisting evaluations of memory hallucinations are primarily end-to-end question\nanswering, which makes it difficult to localize the operational stage within\nthe memory system where hallucinations arise. To address this, we introduce the\nHallucination in Memory Benchmark (HaluMem), the first operation level\nhallucination evaluation benchmark tailored to memory systems. HaluMem defines\nthree evaluation tasks (memory extraction, memory updating, and memory question\nanswering) to comprehensively reveal hallucination behaviors across different\noperational stages of interaction. To support evaluation, we construct\nuser-centric, multi-turn human-AI interaction datasets, HaluMem-Medium and\nHaluMem-Long. Both include about 15k memory points and 3.5k multi-type\nquestions. The average dialogue length per user reaches 1.5k and 2.6k turns,\nwith context lengths exceeding 1M tokens, enabling evaluation of hallucinations\nacross different context scales and task complexities. Empirical studies based\non HaluMem show that existing memory systems tend to generate and accumulate\nhallucinations during the extraction and updating stages, which subsequently\npropagate errors to the question answering stage. Future research should focus\non developing interpretable and constrained memory operation mechanisms that\nsystematically suppress hallucinations and improve memory reliability.",
      "upvotes": 55,
      "discussionId": "690c081d60494e4fa7675621",
      "githubRepo": "https://github.com/MemTensor/HaluMem",
      "ai_summary": "HaluMem, a benchmark for evaluating memory hallucinations in AI systems, identifies and analyzes hallucinations across memory extraction, updating, and question answering stages using large-scale human-AI interaction datasets.",
      "ai_keywords": [
        "memory systems",
        "LLMs",
        "AI agents",
        "memory hallucinations",
        "fabrication",
        "errors",
        "conflicts",
        "omissions",
        "HaluMem",
        "operation level hallucination evaluation",
        "memory extraction",
        "memory updating",
        "memory question answering",
        "user-centric",
        "multi-turn human-AI interaction datasets",
        "HaluMem-Medium",
        "HaluMem-Long",
        "dialogue length",
        "context lengths",
        "token",
        "hallucinations",
        "memory reliability"
      ],
      "githubStars": 42,
      "organization": {
        "_id": "684d4f8e0bb9b6d7621cd53b",
        "name": "MemTensor",
        "fullname": "MemTensor",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62a155e615eeab266b2f2243/2mVH99TFqle9MJVb95aDC.jpeg"
      }
    },
    "publishedAt": "2025-11-05T09:37:34.000Z",
    "title": "HaluMem: Evaluating Hallucinations in Memory Systems of Agents",
    "summary": "Memory systems are key components that enable AI systems such as LLMs and AI\nagents to achieve long-term learning and sustained interaction. However, during\nmemory storage and retrieval, these systems frequently exhibit memory\nhallucinations, including fabrication, errors, conflicts, and omissions.\nExisting evaluations of memory hallucinations are primarily end-to-end question\nanswering, which makes it difficult to localize the operational stage within\nthe memory system where hallucinations arise. To address this, we introduce the\nHallucination in Memory Benchmark (HaluMem), the first operation level\nhallucination evaluation benchmark tailored to memory systems. HaluMem defines\nthree evaluation tasks (memory extraction, memory updating, and memory question\nanswering) to comprehensively reveal hallucination behaviors across different\noperational stages of interaction. To support evaluation, we construct\nuser-centric, multi-turn human-AI interaction datasets, HaluMem-Medium and\nHaluMem-Long. Both include about 15k memory points and 3.5k multi-type\nquestions. The average dialogue length per user reaches 1.5k and 2.6k turns,\nwith context lengths exceeding 1M tokens, enabling evaluation of hallucinations\nacross different context scales and task complexities. Empirical studies based\non HaluMem show that existing memory systems tend to generate and accumulate\nhallucinations during the extraction and updating stages, which subsequently\npropagate errors to the question answering stage. Future research should focus\non developing interpretable and constrained memory operation mechanisms that\nsystematically suppress hallucinations and improve memory reliability.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.03506.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64e18e9ec20c27fcc8df384e",
      "avatarUrl": "/avatars/64ef866b9fa385efcefb34ea76b76802.svg",
      "fullname": "Ding Chen",
      "name": "Hush-cd",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "organization": {
      "_id": "684d4f8e0bb9b6d7621cd53b",
      "name": "MemTensor",
      "fullname": "MemTensor",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62a155e615eeab266b2f2243/2mVH99TFqle9MJVb95aDC.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.07327",
      "authors": [
        {
          "_id": "6912b5eda644ba07c499c740",
          "name": "Guoxin Chen",
          "hidden": false
        },
        {
          "_id": "6912b5eda644ba07c499c741",
          "name": "Zile Qiao",
          "hidden": false
        },
        {
          "_id": "6912b5eda644ba07c499c742",
          "name": "Xuanzhong Chen",
          "hidden": false
        },
        {
          "_id": "6912b5eda644ba07c499c743",
          "name": "Donglei Yu",
          "hidden": false
        },
        {
          "_id": "6912b5eda644ba07c499c744",
          "name": "Haotian Xu",
          "hidden": false
        },
        {
          "_id": "6912b5eda644ba07c499c745",
          "name": "Wayne Xin Zhao",
          "hidden": false
        },
        {
          "_id": "6912b5eda644ba07c499c746",
          "name": "Ruihua Song",
          "hidden": false
        },
        {
          "_id": "6912b5eda644ba07c499c747",
          "name": "Wenbiao Yin",
          "hidden": false
        },
        {
          "_id": "6912b5eda644ba07c499c748",
          "name": "Huifeng Yin",
          "hidden": false
        },
        {
          "_id": "6912b5eda644ba07c499c749",
          "name": "Liwen Zhang",
          "hidden": false
        },
        {
          "_id": "6912b5eda644ba07c499c74a",
          "name": "Kuan Li",
          "hidden": false
        },
        {
          "_id": "6912b5eda644ba07c499c74b",
          "name": "Minpeng Liao",
          "hidden": false
        },
        {
          "_id": "6912b5eda644ba07c499c74c",
          "name": "Yong Jiang",
          "hidden": false
        },
        {
          "_id": "6912b5eda644ba07c499c74d",
          "name": "Pengjun Xie",
          "hidden": false
        },
        {
          "_id": "6912b5eda644ba07c499c74e",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "6912b5eda644ba07c499c74f",
          "name": "Jingren Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-10T17:30:08.000Z",
      "submittedOnDailyAt": "2025-11-11T01:39:02.557Z",
      "title": "IterResearch: Rethinking Long-Horizon Agents via Markovian State\n  Reconstruction",
      "submittedOnDailyBy": {
        "_id": "63f06116f1a47aaea5bd497b",
        "avatarUrl": "/avatars/7d99ffa59c4579599e852a0ffb261268.svg",
        "isPro": false,
        "fullname": "Guoxin Chen",
        "user": "GuoxinChen",
        "type": "user"
      },
      "summary": "Recent advances in deep-research agents have shown promise for autonomous\nknowledge construction through dynamic reasoning over external sources.\nHowever, existing approaches rely on a mono-contextual paradigm that\naccumulates all information in a single, expanding context window, leading to\ncontext suffocation and noise contamination that limit their effectiveness on\nlong-horizon tasks. We introduce IterResearch, a novel iterative deep-research\nparadigm that reformulates long-horizon research as a Markov Decision Process\nwith strategic workspace reconstruction. By maintaining an evolving report as\nmemory and periodically synthesizing insights, our approach preserves\nconsistent reasoning capacity across arbitrary exploration depths. We further\ndevelop Efficiency-Aware Policy Optimization (EAPO), a reinforcement learning\nframework that incentivizes efficient exploration through geometric reward\ndiscounting and enables stable distributed training via adaptive downsampling.\nExtensive experiments demonstrate that IterResearch achieves substantial\nimprovements over existing open-source agents with average +14.5pp across six\nbenchmarks and narrows the gap with frontier proprietary systems. Remarkably,\nour paradigm exhibits unprecedented interaction scaling, extending to 2048\ninteractions with dramatic performance gains (from 3.5\\% to 42.5\\%), and serves\nas an effective prompting strategy, improving frontier models by up to 19.2pp\nover ReAct on long-horizon tasks. These findings position IterResearch as a\nversatile solution for long-horizon reasoning, effective both as a trained\nagent and as a prompting paradigm for frontier models.",
      "upvotes": 31,
      "discussionId": "6912b5eda644ba07c499c750",
      "ai_summary": "IterResearch, an iterative deep-research paradigm, improves long-horizon reasoning by reformulating it as a Markov Decision Process with strategic workspace reconstruction and Efficiency-Aware Policy Optimization, achieving better performance and interaction scaling compared to existing agents.",
      "ai_keywords": [
        "deep-research agents",
        "dynamic reasoning",
        "mono-contextual paradigm",
        "context suffocation",
        "noise contamination",
        "long-horizon tasks",
        "IterResearch",
        "Markov Decision Process",
        "strategic workspace reconstruction",
        "evolving report",
        "Efficiency-Aware Policy Optimization",
        "reinforcement learning",
        "geometric reward discounting",
        "adaptive downsampling",
        "prompting strategy",
        "ReAct"
      ]
    },
    "publishedAt": "2025-11-10T12:30:08.000Z",
    "title": "IterResearch: Rethinking Long-Horizon Agents via Markovian State\n  Reconstruction",
    "summary": "Recent advances in deep-research agents have shown promise for autonomous\nknowledge construction through dynamic reasoning over external sources.\nHowever, existing approaches rely on a mono-contextual paradigm that\naccumulates all information in a single, expanding context window, leading to\ncontext suffocation and noise contamination that limit their effectiveness on\nlong-horizon tasks. We introduce IterResearch, a novel iterative deep-research\nparadigm that reformulates long-horizon research as a Markov Decision Process\nwith strategic workspace reconstruction. By maintaining an evolving report as\nmemory and periodically synthesizing insights, our approach preserves\nconsistent reasoning capacity across arbitrary exploration depths. We further\ndevelop Efficiency-Aware Policy Optimization (EAPO), a reinforcement learning\nframework that incentivizes efficient exploration through geometric reward\ndiscounting and enables stable distributed training via adaptive downsampling.\nExtensive experiments demonstrate that IterResearch achieves substantial\nimprovements over existing open-source agents with average +14.5pp across six\nbenchmarks and narrows the gap with frontier proprietary systems. Remarkably,\nour paradigm exhibits unprecedented interaction scaling, extending to 2048\ninteractions with dramatic performance gains (from 3.5\\% to 42.5\\%), and serves\nas an effective prompting strategy, improving frontier models by up to 19.2pp\nover ReAct on long-horizon tasks. These findings position IterResearch as a\nversatile solution for long-horizon reasoning, effective both as a trained\nagent and as a prompting paradigm for frontier models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.07327.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63f06116f1a47aaea5bd497b",
      "avatarUrl": "/avatars/7d99ffa59c4579599e852a0ffb261268.svg",
      "fullname": "Guoxin Chen",
      "name": "GuoxinChen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.06307",
      "authors": [
        {
          "_id": "6912c5b6a644ba07c499c7ce",
          "name": "Speed Zhu",
          "hidden": false
        },
        {
          "_id": "6912c5b6a644ba07c499c7cf",
          "name": "Jianwei Cai",
          "hidden": false
        },
        {
          "_id": "6912c5b6a644ba07c499c7d0",
          "name": "Guang Chen",
          "hidden": false
        },
        {
          "_id": "6912c5b6a644ba07c499c7d1",
          "name": "Lulu Wu",
          "hidden": false
        },
        {
          "_id": "6912c5b6a644ba07c499c7d2",
          "name": "Saiyong Yang",
          "hidden": false
        },
        {
          "_id": "6912c5b6a644ba07c499c7d3",
          "name": "Wiggin Zhou",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64b74b906ab5d14ca7f289cd/GA0zCucYiKF2baXLaOhE7.png",
        "https://cdn-uploads.huggingface.co/production/uploads/64b74b906ab5d14ca7f289cd/I8i9Vyp-OdQbeBiKmq9c8.png"
      ],
      "publishedAt": "2025-11-09T10:11:28.000Z",
      "submittedOnDailyAt": "2025-11-11T02:43:52.695Z",
      "title": "DRIVE: Data Curation Best Practices for Reinforcement Learning with\n  Verifiable Reward in Competitive Code Generation",
      "submittedOnDailyBy": {
        "_id": "64b74b906ab5d14ca7f289cd",
        "avatarUrl": "/avatars/b131b7c4ce5216708ca4a678f35ead0a.svg",
        "isPro": false,
        "fullname": "Chenchen Zhang",
        "user": "xxzcc",
        "type": "user"
      },
      "summary": "Recent reasoning-first models (e.g., OpenAI o1, DeepSeek R1) have spurred a\nresurgence of interest in RLVR. Nevertheless, advances are dominated by\nmathematics (e.g., AIME), with competitive-programming code generation\nunderexplored and data curation receiving less attention than RL algorithm\ndesign. We investigate how to construct RLVR datasets (i.e., RL prompts) and\npresent practical training techniques that yield strong performance on\ncompetitive-programming code generation. Our pipeline begins with supervised\nfine-tuning (SFT) distilled from strong open-source models, augmented with\ngeneral-purpose and reasoning-intensive data. RL then follows a two-stage\nprocess with executable, testcase-driven rewards: first, training on a large,\nuniformly distributed set of competitive-programming problems using Group\nRelative Policy Optimization (GRPO) with 8 rollouts per prompt and a relatively\nshort response-generation window (e.g., 32k during SFT and 24k in this stage)\nto expand entropy and mitigate repetition and truncation; second, we perform\nPre-GRPO: updating on a small, high-quality set of challenging\nproblems with a large rollout budget (64 rollouts per prompt) under a\nhard-focus curriculum that continuously retains the most difficult instances\nthroughout training. We implement our method on Qwen2.5-32B and evaluate on\nLeetCode and Codeforces weekly contests to avoid data leakage. The resulting\nmodel achieves state-of-the-art performance among models of similar scale and\nis comparable to leading systems such as DeepSeek v3.1 and Doubao-1.5-Thinking.\nWe also examine scaling trends and observe strong RL scaling on an internal\nlarge-scale MoE model. Our study distills concise best practices for data\ncuration, entropy expansion, and curriculum design in RLVR for\ncompetitive-programming code generation.",
      "upvotes": 24,
      "discussionId": "6912c5b6a644ba07c499c7d4",
      "ai_summary": "The study presents a two-stage reinforcement learning approach for competitive-programming code generation, achieving state-of-the-art performance using Group Relative Policy Optimization and a hard-focus curriculum.",
      "ai_keywords": [
        "RLVR",
        "supervised fine-tuning",
        "Group Relative Policy Optimization",
        "Pre-GRPO",
        "entropy expansion",
        "curriculum design",
        "competitive-programming code generation",
        "Qwen2.5-32B",
        "LeetCode",
        "Codeforces",
        "MoE model"
      ],
      "organization": {
        "_id": "66543b6e420092799d2f625c",
        "name": "tencent",
        "fullname": "Tencent",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
      }
    },
    "publishedAt": "2025-11-09T05:11:28.000Z",
    "title": "DRIVE: Data Curation Best Practices for Reinforcement Learning with\n  Verifiable Reward in Competitive Code Generation",
    "summary": "Recent reasoning-first models (e.g., OpenAI o1, DeepSeek R1) have spurred a\nresurgence of interest in RLVR. Nevertheless, advances are dominated by\nmathematics (e.g., AIME), with competitive-programming code generation\nunderexplored and data curation receiving less attention than RL algorithm\ndesign. We investigate how to construct RLVR datasets (i.e., RL prompts) and\npresent practical training techniques that yield strong performance on\ncompetitive-programming code generation. Our pipeline begins with supervised\nfine-tuning (SFT) distilled from strong open-source models, augmented with\ngeneral-purpose and reasoning-intensive data. RL then follows a two-stage\nprocess with executable, testcase-driven rewards: first, training on a large,\nuniformly distributed set of competitive-programming problems using Group\nRelative Policy Optimization (GRPO) with 8 rollouts per prompt and a relatively\nshort response-generation window (e.g., 32k during SFT and 24k in this stage)\nto expand entropy and mitigate repetition and truncation; second, we perform\nPre-GRPO: updating on a small, high-quality set of challenging\nproblems with a large rollout budget (64 rollouts per prompt) under a\nhard-focus curriculum that continuously retains the most difficult instances\nthroughout training. We implement our method on Qwen2.5-32B and evaluate on\nLeetCode and Codeforces weekly contests to avoid data leakage. The resulting\nmodel achieves state-of-the-art performance among models of similar scale and\nis comparable to leading systems such as DeepSeek v3.1 and Doubao-1.5-Thinking.\nWe also examine scaling trends and observe strong RL scaling on an internal\nlarge-scale MoE model. Our study distills concise best practices for data\ncuration, entropy expansion, and curriculum design in RLVR for\ncompetitive-programming code generation.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64b74b906ab5d14ca7f289cd/GA0zCucYiKF2baXLaOhE7.png",
      "https://cdn-uploads.huggingface.co/production/uploads/64b74b906ab5d14ca7f289cd/I8i9Vyp-OdQbeBiKmq9c8.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.06307.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "64b74b906ab5d14ca7f289cd",
      "avatarUrl": "/avatars/b131b7c4ce5216708ca4a678f35ead0a.svg",
      "fullname": "Chenchen Zhang",
      "name": "xxzcc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "organization": {
      "_id": "66543b6e420092799d2f625c",
      "name": "tencent",
      "fullname": "Tencent",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.07419",
      "authors": [
        {
          "_id": "6912af1ca644ba07c499c71a",
          "name": "Zhongyang Li",
          "hidden": false
        },
        {
          "_id": "6912af1ca644ba07c499c71b",
          "name": "Ziyue Li",
          "hidden": false
        },
        {
          "_id": "6912af1ca644ba07c499c71c",
          "name": "Tianyi Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-10T18:59:53.000Z",
      "submittedOnDailyAt": "2025-11-11T01:21:40.975Z",
      "title": "Routing Manifold Alignment Improves Generalization of Mixture-of-Experts\n  LLMs",
      "submittedOnDailyBy": {
        "_id": "647f5af5b0e96764589f3b2a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
        "isPro": false,
        "fullname": "Tianyi Zhou",
        "user": "zhoutianyi",
        "type": "user"
      },
      "summary": "Sparse Mixture-of-Experts (MoE) have been widely adopted in recent large\nlanguage models since it can efficiently scale up the model capability without\nincreasing the inference cost. However, evaluations on broad downstream tasks\nreveal a consistent suboptimality of the routers in existing MoE LLMs, which\nresults in a severe performance gap (e.g., 10-20% in accuracy) to the optimal\nrouting. In this paper, we show that aligning the manifold of routing weights\nwith that of task embedding can effectively reduce the gap and improve MoE\nLLMs' generalization performance. Our method, \"Routing Manifold Alignment\n(RoMA)\", introduces an additional manifold regularization term in the\npost-training objective and only requires lightweight finetuning of routers\n(with other parameters frozen). Specifically, the regularization encourages the\nrouting weights of each sample to be close to those of its successful neighbors\n(whose routing weights lead to correct answers) in a task embedding space.\nConsequently, samples targeting similar tasks will share similar expert choices\nacross layers. Building such bindings between tasks and experts over different\nsamples is essential to achieve better generalization. Moreover, RoMA\ndemonstrates the advantage of unifying the task understanding (by embedding\nmodels) with solution generation (by MoE LLMs). In experiments, we finetune\nrouters in OLMoE, DeepSeekMoE, and Qwen3-MoE using RoMA. Evaluations on diverse\nbenchmarks and extensive comparisons with baselines show the substantial\nimprovement brought by RoMA.",
      "upvotes": 11,
      "discussionId": "6912af1da644ba07c499c71d",
      "githubRepo": "https://github.com/tianyi-lab/RoMA",
      "ai_summary": "Aligning routing weights with task embeddings in Sparse Mixture-of-Experts (MoE) models improves generalization and reduces performance gaps in large language models.",
      "ai_keywords": [
        "Sparse Mixture-of-Experts",
        "MoE",
        "routing weights",
        "task embedding",
        "manifold regularization",
        "RoMA",
        "OLMoE",
        "DeepSeekMoE",
        "Qwen3-MoE"
      ],
      "githubStars": 1
    },
    "publishedAt": "2025-11-10T13:59:53.000Z",
    "title": "Routing Manifold Alignment Improves Generalization of Mixture-of-Experts\n  LLMs",
    "summary": "Sparse Mixture-of-Experts (MoE) have been widely adopted in recent large\nlanguage models since it can efficiently scale up the model capability without\nincreasing the inference cost. However, evaluations on broad downstream tasks\nreveal a consistent suboptimality of the routers in existing MoE LLMs, which\nresults in a severe performance gap (e.g., 10-20% in accuracy) to the optimal\nrouting. In this paper, we show that aligning the manifold of routing weights\nwith that of task embedding can effectively reduce the gap and improve MoE\nLLMs' generalization performance. Our method, \"Routing Manifold Alignment\n(RoMA)\", introduces an additional manifold regularization term in the\npost-training objective and only requires lightweight finetuning of routers\n(with other parameters frozen). Specifically, the regularization encourages the\nrouting weights of each sample to be close to those of its successful neighbors\n(whose routing weights lead to correct answers) in a task embedding space.\nConsequently, samples targeting similar tasks will share similar expert choices\nacross layers. Building such bindings between tasks and experts over different\nsamples is essential to achieve better generalization. Moreover, RoMA\ndemonstrates the advantage of unifying the task understanding (by embedding\nmodels) with solution generation (by MoE LLMs). In experiments, we finetune\nrouters in OLMoE, DeepSeekMoE, and Qwen3-MoE using RoMA. Evaluations on diverse\nbenchmarks and extensive comparisons with baselines show the substantial\nimprovement brought by RoMA.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.07419.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647f5af5b0e96764589f3b2a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
      "fullname": "Tianyi Zhou",
      "name": "zhoutianyi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 18
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.07070",
      "authors": [
        {
          "_id": "6912afc3a644ba07c499c72b",
          "name": "Fei Zhao",
          "hidden": false
        },
        {
          "_id": "6912afc3a644ba07c499c72c",
          "name": "Chonggang Lu",
          "hidden": false
        },
        {
          "_id": "6912afc3a644ba07c499c72d",
          "name": "Haofu Qian",
          "hidden": false
        },
        {
          "_id": "6912afc3a644ba07c499c72e",
          "name": "Fangcheng Shi",
          "hidden": false
        },
        {
          "_id": "6912afc3a644ba07c499c72f",
          "name": "Zijie Meng",
          "hidden": false
        },
        {
          "_id": "6912afc3a644ba07c499c730",
          "name": "Jianzhao Huang",
          "hidden": false
        },
        {
          "_id": "6912afc3a644ba07c499c731",
          "name": "Xu Tang",
          "hidden": false
        },
        {
          "_id": "6912afc3a644ba07c499c732",
          "name": "Zheyong Xie",
          "hidden": false
        },
        {
          "_id": "6912afc3a644ba07c499c733",
          "name": "Zheyu Ye",
          "hidden": false
        },
        {
          "_id": "6912afc3a644ba07c499c734",
          "name": "Zhe Xu",
          "hidden": false
        },
        {
          "_id": "6912afc3a644ba07c499c735",
          "name": "Yao Hu",
          "hidden": false
        },
        {
          "_id": "6912afc3a644ba07c499c736",
          "name": "Shaosheng Cao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-10T13:04:34.000Z",
      "submittedOnDailyAt": "2025-11-11T01:24:20.752Z",
      "title": "RedOne 2.0: Rethinking Domain-specific LLM Post-Training in Social\n  Networking Services",
      "submittedOnDailyBy": {
        "_id": "65328aa39326d6da5ff19b52",
        "avatarUrl": "/avatars/5c3de984cd6eba69616bb608796865c5.svg",
        "isPro": false,
        "fullname": "Fei Zhao",
        "user": "Hiiamein",
        "type": "user"
      },
      "summary": "As a key medium for human interaction and information exchange, social\nnetworking services (SNS) pose unique challenges for large language models\n(LLMs): heterogeneous workloads, fast-shifting norms and slang, and\nmultilingual, culturally diverse corpora that induce sharp distribution shift.\nSupervised fine-tuning (SFT) can specialize models but often triggers a\n``seesaw'' between in-distribution gains and out-of-distribution robustness,\nespecially for smaller models. To address these challenges, we introduce RedOne\n2.0, an SNS-oriented LLM trained with a progressive, RL-prioritized\npost-training paradigm designed for rapid and stable adaptation. The pipeline\nconsist in three stages: (1) Exploratory Learning on curated SNS corpora to\nestablish initial alignment and identify systematic weaknesses; (2) Targeted\nFine-Tuning that selectively applies SFT to the diagnosed gaps while mixing a\nsmall fraction of general data to mitigate forgetting; and (3) Refinement\nLearning that re-applies RL with SNS-centric signals to consolidate\nimprovements and harmonize trade-offs across tasks. Across various tasks\nspanning three categories, our 4B scale model delivers an average improvements\nabout 2.41 over the 7B sub-optimal baseline. Additionally, RedOne 2.0 achieves\naverage performance lift about 8.74 from the base model with less than half the\ndata required by SFT-centric method RedOne, evidencing superior data efficiency\nand stability at compact scales. Overall, RedOne 2.0 establishes a competitive,\ncost-effective baseline for domain-specific LLMs in SNS scenario, advancing\ncapability without sacrificing robustness.",
      "upvotes": 10,
      "discussionId": "6912afc4a644ba07c499c737",
      "ai_summary": "RedOne 2.0, a social networking service-oriented LLM, uses a progressive, RL-prioritized post-training paradigm to achieve rapid and stable adaptation, delivering improvements over larger baselines with less data.",
      "ai_keywords": [
        "supervised fine-tuning",
        "RL-prioritized",
        "exploratory learning",
        "targeted fine-tuning",
        "refinement learning",
        "domain-specific LLMs"
      ]
    },
    "publishedAt": "2025-11-10T08:04:34.000Z",
    "title": "RedOne 2.0: Rethinking Domain-specific LLM Post-Training in Social\n  Networking Services",
    "summary": "As a key medium for human interaction and information exchange, social\nnetworking services (SNS) pose unique challenges for large language models\n(LLMs): heterogeneous workloads, fast-shifting norms and slang, and\nmultilingual, culturally diverse corpora that induce sharp distribution shift.\nSupervised fine-tuning (SFT) can specialize models but often triggers a\n``seesaw'' between in-distribution gains and out-of-distribution robustness,\nespecially for smaller models. To address these challenges, we introduce RedOne\n2.0, an SNS-oriented LLM trained with a progressive, RL-prioritized\npost-training paradigm designed for rapid and stable adaptation. The pipeline\nconsist in three stages: (1) Exploratory Learning on curated SNS corpora to\nestablish initial alignment and identify systematic weaknesses; (2) Targeted\nFine-Tuning that selectively applies SFT to the diagnosed gaps while mixing a\nsmall fraction of general data to mitigate forgetting; and (3) Refinement\nLearning that re-applies RL with SNS-centric signals to consolidate\nimprovements and harmonize trade-offs across tasks. Across various tasks\nspanning three categories, our 4B scale model delivers an average improvements\nabout 2.41 over the 7B sub-optimal baseline. Additionally, RedOne 2.0 achieves\naverage performance lift about 8.74 from the base model with less than half the\ndata required by SFT-centric method RedOne, evidencing superior data efficiency\nand stability at compact scales. Overall, RedOne 2.0 establishes a competitive,\ncost-effective baseline for domain-specific LLMs in SNS scenario, advancing\ncapability without sacrificing robustness.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.07070.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65328aa39326d6da5ff19b52",
      "avatarUrl": "/avatars/5c3de984cd6eba69616bb608796865c5.svg",
      "fullname": "Fei Zhao",
      "name": "Hiiamein",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.07250",
      "authors": [
        {
          "_id": "6912b9b7a644ba07c499c777",
          "name": "Tianhao Peng",
          "hidden": false
        },
        {
          "_id": "6912b9b7a644ba07c499c778",
          "name": "Haochen Wang",
          "hidden": false
        },
        {
          "_id": "6912b9b7a644ba07c499c779",
          "name": "Yuanxing Zhang",
          "hidden": false
        },
        {
          "_id": "6912b9b7a644ba07c499c77a",
          "name": "Zekun Wang",
          "hidden": false
        },
        {
          "_id": "6912b9b7a644ba07c499c77b",
          "name": "Zili Wang",
          "hidden": false
        },
        {
          "_id": "6912b9b7a644ba07c499c77c",
          "name": "Ge Zhang",
          "hidden": false
        },
        {
          "_id": "6912b9b7a644ba07c499c77d",
          "name": "Jian Yang",
          "hidden": false
        },
        {
          "_id": "6912b9b7a644ba07c499c77e",
          "name": "Shihao Li",
          "hidden": false
        },
        {
          "_id": "6912b9b7a644ba07c499c77f",
          "name": "Yanghai Wang",
          "hidden": false
        },
        {
          "_id": "6912b9b7a644ba07c499c780",
          "name": "Xintao Wang",
          "hidden": false
        },
        {
          "_id": "6912b9b7a644ba07c499c781",
          "name": "Houyi Li",
          "hidden": false
        },
        {
          "_id": "6912b9b7a644ba07c499c782",
          "name": "Wei Ji",
          "hidden": false
        },
        {
          "_id": "6912b9b7a644ba07c499c783",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "6912b9b7a644ba07c499c784",
          "name": "Wenhao Huang",
          "hidden": false
        },
        {
          "_id": "6912b9b7a644ba07c499c785",
          "name": "Zhaoxiang Zhang",
          "hidden": false
        },
        {
          "_id": "6912b9b7a644ba07c499c786",
          "name": "Jiaheng Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-10T16:02:33.000Z",
      "submittedOnDailyAt": "2025-11-11T01:52:05.353Z",
      "title": "MVU-Eval: Towards Multi-Video Understanding Evaluation for Multimodal\n  LLMs",
      "submittedOnDailyBy": {
        "_id": "65377c30e48353201e6fdda0",
        "avatarUrl": "/avatars/a8f803b6f2e598eaee9c52c0d2ddfc16.svg",
        "isPro": false,
        "fullname": "Jiaheng Liu",
        "user": "CheeryLJH",
        "type": "user"
      },
      "summary": "The advent of Multimodal Large Language Models (MLLMs) has expanded AI\ncapabilities to visual modalities, yet existing evaluation benchmarks remain\nlimited to single-video understanding, overlooking the critical need for\nmulti-video understanding in real-world scenarios (e.g., sports analytics and\nautonomous driving). To address this significant gap, we introduce MVU-Eval,\nthe first comprehensive benchmark for evaluating Multi-Video Understanding for\nMLLMs. Specifically, our MVU-Eval mainly assesses eight core competencies\nthrough 1,824 meticulously curated question-answer pairs spanning 4,959 videos\nfrom diverse domains, addressing both fundamental perception tasks and\nhigh-order reasoning tasks. These capabilities are rigorously aligned with\nreal-world applications such as multi-sensor synthesis in autonomous systems\nand cross-angle sports analytics. Through extensive evaluation of\nstate-of-the-art open-source and closed-source models, we reveal significant\nperformance discrepancies and limitations in current MLLMs' ability to perform\nunderstanding across multiple videos. The benchmark will be made publicly\navailable to foster future research.",
      "upvotes": 7,
      "discussionId": "6912b9b8a644ba07c499c787",
      "projectPage": "https://huggingface.co/datasets/MVU-Eval-Team/MVU-Eval-Data",
      "githubRepo": "https://github.com/NJU-LINK/MVU-Eval",
      "ai_summary": "MVU-Eval is a comprehensive benchmark for evaluating multi-video understanding in Multimodal Large Language Models, addressing gaps in existing single-video benchmarks and highlighting performance discrepancies in real-world applications.",
      "ai_keywords": [
        "Multimodal Large Language Models",
        "MVU-Eval",
        "Multi-Video Understanding",
        "question-answer pairs",
        "multi-sensor synthesis",
        "cross-angle sports analytics"
      ],
      "githubStars": 1,
      "organization": {
        "_id": "68edc767abe005ac1b354573",
        "name": "NJU-LINK",
        "fullname": "NJU-LINK Lab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67f9d060395fb1a0d7e4ae21/O3V4UZjcSGnOivcQqTcXW.png"
      }
    },
    "publishedAt": "2025-11-10T11:02:33.000Z",
    "title": "MVU-Eval: Towards Multi-Video Understanding Evaluation for Multimodal\n  LLMs",
    "summary": "The advent of Multimodal Large Language Models (MLLMs) has expanded AI\ncapabilities to visual modalities, yet existing evaluation benchmarks remain\nlimited to single-video understanding, overlooking the critical need for\nmulti-video understanding in real-world scenarios (e.g., sports analytics and\nautonomous driving). To address this significant gap, we introduce MVU-Eval,\nthe first comprehensive benchmark for evaluating Multi-Video Understanding for\nMLLMs. Specifically, our MVU-Eval mainly assesses eight core competencies\nthrough 1,824 meticulously curated question-answer pairs spanning 4,959 videos\nfrom diverse domains, addressing both fundamental perception tasks and\nhigh-order reasoning tasks. These capabilities are rigorously aligned with\nreal-world applications such as multi-sensor synthesis in autonomous systems\nand cross-angle sports analytics. Through extensive evaluation of\nstate-of-the-art open-source and closed-source models, we reveal significant\nperformance discrepancies and limitations in current MLLMs' ability to perform\nunderstanding across multiple videos. The benchmark will be made publicly\navailable to foster future research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.07250.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65377c30e48353201e6fdda0",
      "avatarUrl": "/avatars/a8f803b6f2e598eaee9c52c0d2ddfc16.svg",
      "fullname": "Jiaheng Liu",
      "name": "CheeryLJH",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 22
    },
    "organization": {
      "_id": "68edc767abe005ac1b354573",
      "name": "NJU-LINK",
      "fullname": "NJU-LINK Lab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67f9d060395fb1a0d7e4ae21/O3V4UZjcSGnOivcQqTcXW.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.06411",
      "authors": [
        {
          "_id": "6912a010a644ba07c499c6d6",
          "name": "Zhi Zheng",
          "hidden": false
        },
        {
          "_id": "6912a010a644ba07c499c6d7",
          "name": "Wee Sun Lee",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/67a1d21e33e92b4a1183f3bb/CKtkMN0gQnaBFvadEA7gD.png",
        "https://cdn-uploads.huggingface.co/production/uploads/67a1d21e33e92b4a1183f3bb/1oIZCE3feLsvZOJjMFR3t.png",
        "https://cdn-uploads.huggingface.co/production/uploads/67a1d21e33e92b4a1183f3bb/hveVcu19PMsF7cj-_B_z0.png"
      ],
      "publishedAt": "2025-11-09T14:55:50.000Z",
      "submittedOnDailyAt": "2025-11-11T02:47:22.154Z",
      "title": "SofT-GRPO: Surpassing Discrete-Token LLM Reinforcement Learning via\n  Gumbel-Reparameterized Soft-Thinking Policy Optimization",
      "submittedOnDailyBy": {
        "_id": "67a1d21e33e92b4a1183f3bb",
        "avatarUrl": "/avatars/43f9dd3fcb7d58ddc69562fd1fc12957.svg",
        "isPro": false,
        "fullname": "Zhi Zheng",
        "user": "zz1358m",
        "type": "user"
      },
      "summary": "The soft-thinking paradigm for Large Language Model (LLM) reasoning can\noutperform the conventional discrete-token Chain-of-Thought (CoT) reasoning in\nsome scenarios, underscoring its research and application value. However, while\nthe discrete-token CoT reasoning pattern can be reinforced through policy\noptimization algorithms such as group relative policy optimization (GRPO),\nextending the soft-thinking pattern with Reinforcement Learning (RL) remains\nchallenging. This difficulty stems from the complexities of injecting\nstochasticity into soft-thinking tokens and updating soft-thinking policies\naccordingly. As a result, previous attempts to combine soft-thinking with GRPO\ntypically underperform their discrete-token GRPO counterparts. To fully unlock\nthe potential of soft-thinking, this paper presents a novel policy optimization\nalgorithm, SofT-GRPO, to reinforce LLMs under the soft-thinking reasoning\npattern. SofT-GRPO injects the Gumbel noise into logits, employs the\nGumbel-Softmax technique to avoid soft-thinking tokens outside the pre-trained\nembedding space, and leverages the reparameterization trick in policy gradient.\nWe conduct experiments across base LLMs ranging from 1.5B to 7B parameters, and\nresults demonstrate that SofT-GRPO enables soft-thinking LLMs to slightly\noutperform discrete-token GRPO on Pass@1 (+0.13% on average accuracy), while\nexhibiting a substantial uplift on Pass@32 (+2.19% on average accuracy). Codes\nand weights are available on https://github.com/zz1358m/SofT-GRPO-master",
      "upvotes": 5,
      "discussionId": "6912a010a644ba07c499c6d8",
      "ai_summary": "A novel policy optimization algorithm, SofT-GRPO, enhances soft-thinking in Large Language Models by integrating Gumbel noise and the Gumbel-Softmax technique, leading to improved performance over discrete-token methods.",
      "ai_keywords": [
        "soft-thinking",
        "Large Language Model (LLM)",
        "Chain-of-Thought (CoT)",
        "policy optimization",
        "group relative policy optimization (GRPO)",
        "Reinforcement Learning (RL)",
        "Gumbel noise",
        "Gumbel-Softmax",
        "reparameterization trick",
        "policy gradient"
      ],
      "organization": {
        "_id": "6508ab2b349930913196378b",
        "name": "NationalUniversityofSingapore",
        "fullname": "National University of Singapore",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZYUmpSMsa5Whihw3me2Bw.png"
      }
    },
    "publishedAt": "2025-11-09T09:55:50.000Z",
    "title": "SofT-GRPO: Surpassing Discrete-Token LLM Reinforcement Learning via\n  Gumbel-Reparameterized Soft-Thinking Policy Optimization",
    "summary": "The soft-thinking paradigm for Large Language Model (LLM) reasoning can\noutperform the conventional discrete-token Chain-of-Thought (CoT) reasoning in\nsome scenarios, underscoring its research and application value. However, while\nthe discrete-token CoT reasoning pattern can be reinforced through policy\noptimization algorithms such as group relative policy optimization (GRPO),\nextending the soft-thinking pattern with Reinforcement Learning (RL) remains\nchallenging. This difficulty stems from the complexities of injecting\nstochasticity into soft-thinking tokens and updating soft-thinking policies\naccordingly. As a result, previous attempts to combine soft-thinking with GRPO\ntypically underperform their discrete-token GRPO counterparts. To fully unlock\nthe potential of soft-thinking, this paper presents a novel policy optimization\nalgorithm, SofT-GRPO, to reinforce LLMs under the soft-thinking reasoning\npattern. SofT-GRPO injects the Gumbel noise into logits, employs the\nGumbel-Softmax technique to avoid soft-thinking tokens outside the pre-trained\nembedding space, and leverages the reparameterization trick in policy gradient.\nWe conduct experiments across base LLMs ranging from 1.5B to 7B parameters, and\nresults demonstrate that SofT-GRPO enables soft-thinking LLMs to slightly\noutperform discrete-token GRPO on Pass@1 (+0.13% on average accuracy), while\nexhibiting a substantial uplift on Pass@32 (+2.19% on average accuracy). Codes\nand weights are available on https://github.com/zz1358m/SofT-GRPO-master",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/67a1d21e33e92b4a1183f3bb/CKtkMN0gQnaBFvadEA7gD.png",
      "https://cdn-uploads.huggingface.co/production/uploads/67a1d21e33e92b4a1183f3bb/1oIZCE3feLsvZOJjMFR3t.png",
      "https://cdn-uploads.huggingface.co/production/uploads/67a1d21e33e92b4a1183f3bb/hveVcu19PMsF7cj-_B_z0.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.06411.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67a1d21e33e92b4a1183f3bb",
      "avatarUrl": "/avatars/43f9dd3fcb7d58ddc69562fd1fc12957.svg",
      "fullname": "Zhi Zheng",
      "name": "zz1358m",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "6508ab2b349930913196378b",
      "name": "NationalUniversityofSingapore",
      "fullname": "National University of Singapore",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZYUmpSMsa5Whihw3me2Bw.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.06309",
      "authors": [
        {
          "_id": "6912b989a644ba07c499c773",
          "name": "Stephen Chung",
          "hidden": false
        },
        {
          "_id": "6912b989a644ba07c499c774",
          "name": "Wenyu Du",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-09T10:13:00.000Z",
      "submittedOnDailyAt": "2025-11-11T01:50:33.244Z",
      "title": "The Station: An Open-World Environment for AI-Driven Discovery",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We introduce the STATION, an open-world multi-agent environment that models a\nminiature scientific ecosystem. Leveraging their extended context windows,\nagents in the Station can engage in long scientific journeys that include\nreading papers from peers, formulating hypotheses, submitting code, performing\nanalyses, and publishing results. Importantly, there is no centralized system\ncoordinating their activities - agents are free to choose their own actions and\ndevelop their own narratives within the Station. Experiments demonstrate that\nAI agents in the Station achieve new state-of-the-art performance on a wide\nrange of benchmarks, spanning from mathematics to computational biology to\nmachine learning, notably surpassing AlphaEvolve in circle packing. A rich\ntapestry of narratives emerges as agents pursue independent research, interact\nwith peers, and build upon a cumulative history. From these emergent\nnarratives, novel methods arise organically, such as a new density-adaptive\nalgorithm for scRNA-seq batch integration. The Station marks a first step\ntowards autonomous scientific discovery driven by emergent behavior in an\nopen-world environment, representing a new paradigm that moves beyond rigid\noptimization.",
      "upvotes": 4,
      "discussionId": "6912b989a644ba07c499c775",
      "githubRepo": "https://github.com/dualverse-ai/station",
      "ai_summary": "AI agents in the STATION environment achieve state-of-the-art performance across various benchmarks through autonomous scientific discovery and emergent behavior.",
      "ai_keywords": [
        "STATION",
        "multi-agent environment",
        "scientific ecosystem",
        "context windows",
        "long scientific journeys",
        "reading papers",
        "formulating hypotheses",
        "submitting code",
        "performing analyses",
        "publishing results",
        "AlphaEvolve",
        "circle packing",
        "density-adaptive algorithm",
        "scRNA-seq batch integration",
        "autonomous scientific discovery",
        "emergent behavior",
        "open-world environment"
      ],
      "githubStars": 3
    },
    "publishedAt": "2025-11-09T05:13:00.000Z",
    "title": "The Station: An Open-World Environment for AI-Driven Discovery",
    "summary": "We introduce the STATION, an open-world multi-agent environment that models a\nminiature scientific ecosystem. Leveraging their extended context windows,\nagents in the Station can engage in long scientific journeys that include\nreading papers from peers, formulating hypotheses, submitting code, performing\nanalyses, and publishing results. Importantly, there is no centralized system\ncoordinating their activities - agents are free to choose their own actions and\ndevelop their own narratives within the Station. Experiments demonstrate that\nAI agents in the Station achieve new state-of-the-art performance on a wide\nrange of benchmarks, spanning from mathematics to computational biology to\nmachine learning, notably surpassing AlphaEvolve in circle packing. A rich\ntapestry of narratives emerges as agents pursue independent research, interact\nwith peers, and build upon a cumulative history. From these emergent\nnarratives, novel methods arise organically, such as a new density-adaptive\nalgorithm for scRNA-seq batch integration. The Station marks a first step\ntowards autonomous scientific discovery driven by emergent behavior in an\nopen-world environment, representing a new paradigm that moves beyond rigid\noptimization.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.06309.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 159
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.07137",
      "authors": [
        {
          "_id": "6912d8eaa644ba07c499c808",
          "name": "Shiqi Jiang",
          "hidden": false
        },
        {
          "_id": "6912d8eaa644ba07c499c809",
          "name": "Tianyi Liang",
          "hidden": false
        },
        {
          "_id": "6912d8eaa644ba07c499c80a",
          "name": "Changbo Wang",
          "hidden": false
        },
        {
          "_id": "6912d8eaa644ba07c499c80b",
          "name": "Chenhui Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-10T14:18:27.000Z",
      "submittedOnDailyAt": "2025-11-11T05:19:32.338Z",
      "title": "MPJudge: Towards Perceptual Assessment of Music-Induced Paintings",
      "submittedOnDailyBy": {
        "_id": "62c14609ac1b639c2d87192c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656833489364-noauth.png",
        "isPro": false,
        "fullname": "SII-liangtianyi",
        "user": "tianyilt",
        "type": "user"
      },
      "summary": "Music induced painting is a unique artistic practice, where visual artworks\nare created under the influence of music. Evaluating whether a painting\nfaithfully reflects the music that inspired it poses a challenging perceptual\nassessment task. Existing methods primarily rely on emotion recognition models\nto assess the similarity between music and painting, but such models introduce\nconsiderable noise and overlook broader perceptual cues beyond emotion. To\naddress these limitations, we propose a novel framework for music induced\npainting assessment that directly models perceptual coherence between music and\nvisual art. We introduce MPD, the first large scale dataset of music painting\npairs annotated by domain experts based on perceptual coherence. To better\nhandle ambiguous cases, we further collect pairwise preference annotations.\nBuilding on this dataset, we present MPJudge, a model that integrates music\nfeatures into a visual encoder via a modulation based fusion mechanism. To\neffectively learn from ambiguous cases, we adopt Direct Preference Optimization\nfor training. Extensive experiments demonstrate that our method outperforms\nexisting approaches. Qualitative results further show that our model more\naccurately identifies music relevant regions in paintings.",
      "upvotes": 3,
      "discussionId": "6912d8eaa644ba07c499c80c",
      "ai_summary": "A novel framework MPJudge assesses music-induced paintings by integrating music features into a visual encoder using a modulation-based fusion mechanism, outperforming existing emotion recognition models.",
      "ai_keywords": [
        "perceptual coherence",
        "MPD",
        "MPJudge",
        "modulation-based fusion mechanism",
        "Direct Preference Optimization"
      ],
      "organization": {
        "_id": "62c1482aac1b639c2d873235",
        "name": "ECNU",
        "fullname": "East China Normal University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1656834075081-62c14609ac1b639c2d87192c.png"
      }
    },
    "publishedAt": "2025-11-10T09:18:27.000Z",
    "title": "MPJudge: Towards Perceptual Assessment of Music-Induced Paintings",
    "summary": "Music induced painting is a unique artistic practice, where visual artworks\nare created under the influence of music. Evaluating whether a painting\nfaithfully reflects the music that inspired it poses a challenging perceptual\nassessment task. Existing methods primarily rely on emotion recognition models\nto assess the similarity between music and painting, but such models introduce\nconsiderable noise and overlook broader perceptual cues beyond emotion. To\naddress these limitations, we propose a novel framework for music induced\npainting assessment that directly models perceptual coherence between music and\nvisual art. We introduce MPD, the first large scale dataset of music painting\npairs annotated by domain experts based on perceptual coherence. To better\nhandle ambiguous cases, we further collect pairwise preference annotations.\nBuilding on this dataset, we present MPJudge, a model that integrates music\nfeatures into a visual encoder via a modulation based fusion mechanism. To\neffectively learn from ambiguous cases, we adopt Direct Preference Optimization\nfor training. Extensive experiments demonstrate that our method outperforms\nexisting approaches. Qualitative results further show that our model more\naccurately identifies music relevant regions in paintings.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.07137.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c14609ac1b639c2d87192c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656833489364-noauth.png",
      "fullname": "SII-liangtianyi",
      "name": "tianyilt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "organization": {
      "_id": "62c1482aac1b639c2d873235",
      "name": "ECNU",
      "fullname": "East China Normal University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1656834075081-62c14609ac1b639c2d87192c.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.07416",
      "authors": [
        {
          "_id": "6912b819a644ba07c499c765",
          "name": "Jiageng Mao",
          "hidden": false
        },
        {
          "_id": "6912b819a644ba07c499c766",
          "name": "Sicheng He",
          "hidden": false
        },
        {
          "_id": "6912b819a644ba07c499c767",
          "name": "Hao-Ning Wu",
          "hidden": false
        },
        {
          "_id": "6912b819a644ba07c499c768",
          "name": "Yang You",
          "hidden": false
        },
        {
          "_id": "6912b819a644ba07c499c769",
          "name": "Shuyang Sun",
          "hidden": false
        },
        {
          "_id": "6912b819a644ba07c499c76a",
          "name": "Zhicheng Wang",
          "hidden": false
        },
        {
          "_id": "6912b819a644ba07c499c76b",
          "name": "Yanan Bao",
          "hidden": false
        },
        {
          "_id": "6912b819a644ba07c499c76c",
          "name": "Huizhong Chen",
          "hidden": false
        },
        {
          "_id": "6912b819a644ba07c499c76d",
          "name": "Leonidas Guibas",
          "hidden": false
        },
        {
          "_id": "6912b819a644ba07c499c76e",
          "name": "Vitor Guizilini",
          "hidden": false
        },
        {
          "_id": "6912b819a644ba07c499c76f",
          "name": "Howard Zhou",
          "hidden": false
        },
        {
          "_id": "6912b819a644ba07c499c770",
          "name": "Yue Wang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/f_PxBT_5OAmBdsChxO_cM.mp4"
      ],
      "publishedAt": "2025-11-10T18:59:07.000Z",
      "submittedOnDailyAt": "2025-11-11T01:44:22.734Z",
      "title": "Robot Learning from a Physical World Model",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We introduce PhysWorld, a framework that enables robot learning from video\ngeneration through physical world modeling. Recent video generation models can\nsynthesize photorealistic visual demonstrations from language commands and\nimages, offering a powerful yet underexplored source of training signals for\nrobotics. However, directly retargeting pixel motions from generated videos to\nrobots neglects physics, often resulting in inaccurate manipulations. PhysWorld\naddresses this limitation by coupling video generation with physical world\nreconstruction. Given a single image and a task command, our method generates\ntask-conditioned videos and reconstructs the underlying physical world from the\nvideos, and the generated video motions are grounded into physically accurate\nactions through object-centric residual reinforcement learning with the\nphysical world model. This synergy transforms implicit visual guidance into\nphysically executable robotic trajectories, eliminating the need for real robot\ndata collection and enabling zero-shot generalizable robotic manipulation.\nExperiments on diverse real-world tasks demonstrate that PhysWorld\nsubstantially improves manipulation accuracy compared to previous approaches.\nVisit https://pointscoder.github.io/PhysWorld_Web/{the project webpage}\nfor details.",
      "upvotes": 2,
      "discussionId": "6912b819a644ba07c499c771",
      "projectPage": "https://pointscoder.github.io/PhysWorld_Web/",
      "githubRepo": "https://github.com/PointsCoder/OpenReal2Sim",
      "ai_summary": "PhysWorld integrates video generation and physical world modeling to enable accurate robotic manipulation from visual demonstrations without real robot data.",
      "ai_keywords": [
        "video generation",
        "physical world modeling",
        "task-conditioned videos",
        "object-centric residual reinforcement learning",
        "physically executable robotic trajectories",
        "zero-shot generalizable robotic manipulation"
      ],
      "githubStars": 9,
      "organization": {
        "_id": "60f6cbb2852126bac698c89e",
        "name": "deepmind",
        "fullname": "Deepmind",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1638956859875-5f1158120c833276f61f1a84.jpeg"
      }
    },
    "publishedAt": "2025-11-10T13:59:07.000Z",
    "title": "Robot Learning from a Physical World Model",
    "summary": "We introduce PhysWorld, a framework that enables robot learning from video\ngeneration through physical world modeling. Recent video generation models can\nsynthesize photorealistic visual demonstrations from language commands and\nimages, offering a powerful yet underexplored source of training signals for\nrobotics. However, directly retargeting pixel motions from generated videos to\nrobots neglects physics, often resulting in inaccurate manipulations. PhysWorld\naddresses this limitation by coupling video generation with physical world\nreconstruction. Given a single image and a task command, our method generates\ntask-conditioned videos and reconstructs the underlying physical world from the\nvideos, and the generated video motions are grounded into physically accurate\nactions through object-centric residual reinforcement learning with the\nphysical world model. This synergy transforms implicit visual guidance into\nphysically executable robotic trajectories, eliminating the need for real robot\ndata collection and enabling zero-shot generalizable robotic manipulation.\nExperiments on diverse real-world tasks demonstrate that PhysWorld\nsubstantially improves manipulation accuracy compared to previous approaches.\nVisit https://pointscoder.github.io/PhysWorld_Web/{the project webpage}\nfor details.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/f_PxBT_5OAmBdsChxO_cM.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.07416.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 159
    },
    "organization": {
      "_id": "60f6cbb2852126bac698c89e",
      "name": "deepmind",
      "fullname": "Deepmind",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1638956859875-5f1158120c833276f61f1a84.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.04285",
      "authors": [
        {
          "_id": "6912a610a644ba07c499c6ed",
          "name": "Zeng Zhiyuan",
          "hidden": false
        },
        {
          "_id": "6912a610a644ba07c499c6ee",
          "name": "Jiashuo Liu",
          "hidden": false
        },
        {
          "_id": "6912a610a644ba07c499c6ef",
          "name": "Zhangyue Yin",
          "hidden": false
        },
        {
          "_id": "6912a610a644ba07c499c6f0",
          "name": "Ge Zhang",
          "hidden": false
        },
        {
          "_id": "6912a610a644ba07c499c6f1",
          "name": "Wenhao Huang",
          "hidden": false
        },
        {
          "_id": "6912a610a644ba07c499c6f2",
          "name": "Xipeng Qiu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-06T11:27:16.000Z",
      "submittedOnDailyAt": "2025-11-11T00:40:21.089Z",
      "title": "RLoop: An Self-Improving Framework for Reinforcement Learning with\n  Iterative Policy Initialization",
      "submittedOnDailyBy": {
        "_id": "61c351f0e58df959bde67301",
        "avatarUrl": "/avatars/30c1082060590db8ca7ee98a5dd3b83d.svg",
        "isPro": false,
        "fullname": "zhiyuan zeng",
        "user": "zyzeng",
        "type": "user"
      },
      "summary": "While Reinforcement Learning for Verifiable Rewards (RLVR) is powerful for\ntraining large reasoning models, its training dynamics harbor a critical\nchallenge: RL overfitting, where models gain training rewards but lose\ngeneralization. Our analysis reveals this is driven by policy\nover-specialization and catastrophic forgetting of diverse solutions generated\nduring training. Standard optimization discards this valuable inter-step policy\ndiversity. To address this, we introduce RLoop, a self-improving framework\nbuilt on iterative policy initialization. RLoop transforms the standard\ntraining process into a virtuous cycle: it first uses RL to explore the\nsolution space from a given policy, then filters the successful trajectories to\ncreate an expert dataset. This dataset is used via Rejection-sampling\nFine-Tuning (RFT) to refine the initial policy, creating a superior starting\npoint for the next iteration. This loop of exploration and exploitation via\niterative re-initialization effectively converts transient policy variations\ninto robust performance gains. Our experiments show RLoop mitigates forgetting\nand substantially improves generalization, boosting average accuracy by 9% and\npass@32 by over 15% compared to vanilla RL.",
      "upvotes": 2,
      "discussionId": "6912a610a644ba07c499c6f3",
      "ai_summary": "RLoop, a self-improving framework using iterative policy initialization and Rejection-sampling Fine-Tuning, mitigates overfitting and enhances generalization in Reinforcement Learning for Verifiable Rewards.",
      "ai_keywords": [
        "Reinforcement Learning for Verifiable Rewards",
        "RLVR",
        "RL overfitting",
        "policy over-specialization",
        "catastrophic forgetting",
        "iterative policy initialization",
        "RLoop",
        "Rejection-sampling Fine-Tuning",
        "RFT",
        "exploration",
        "exploitation",
        "transient policy variations",
        "robust performance gains",
        "pass@32"
      ]
    },
    "publishedAt": "2025-11-06T06:27:16.000Z",
    "title": "RLoop: An Self-Improving Framework for Reinforcement Learning with\n  Iterative Policy Initialization",
    "summary": "While Reinforcement Learning for Verifiable Rewards (RLVR) is powerful for\ntraining large reasoning models, its training dynamics harbor a critical\nchallenge: RL overfitting, where models gain training rewards but lose\ngeneralization. Our analysis reveals this is driven by policy\nover-specialization and catastrophic forgetting of diverse solutions generated\nduring training. Standard optimization discards this valuable inter-step policy\ndiversity. To address this, we introduce RLoop, a self-improving framework\nbuilt on iterative policy initialization. RLoop transforms the standard\ntraining process into a virtuous cycle: it first uses RL to explore the\nsolution space from a given policy, then filters the successful trajectories to\ncreate an expert dataset. This dataset is used via Rejection-sampling\nFine-Tuning (RFT) to refine the initial policy, creating a superior starting\npoint for the next iteration. This loop of exploration and exploitation via\niterative re-initialization effectively converts transient policy variations\ninto robust performance gains. Our experiments show RLoop mitigates forgetting\nand substantially improves generalization, boosting average accuracy by 9% and\npass@32 by over 15% compared to vanilla RL.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.04285.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61c351f0e58df959bde67301",
      "avatarUrl": "/avatars/30c1082060590db8ca7ee98a5dd3b83d.svg",
      "fullname": "zhiyuan zeng",
      "name": "zyzeng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.03317",
      "authors": [
        {
          "_id": "6912d73fa644ba07c499c7fa",
          "name": "Minghao Fu",
          "hidden": false
        },
        {
          "_id": "6912d73fa644ba07c499c7fb",
          "name": "Guo-Hua Wang",
          "hidden": false
        },
        {
          "_id": "6912d73fa644ba07c499c7fc",
          "name": "Tianyu Cui",
          "hidden": false
        },
        {
          "_id": "6912d73fa644ba07c499c7fd",
          "name": "Qing-Guo Chen",
          "hidden": false
        },
        {
          "_id": "6912d73fa644ba07c499c7fe",
          "name": "Zhao Xu",
          "hidden": false
        },
        {
          "_id": "6912d73fa644ba07c499c7ff",
          "name": "Weihua Luo",
          "hidden": false
        },
        {
          "_id": "6912d73fa644ba07c499c800",
          "name": "Kaifu Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-05T09:30:49.000Z",
      "submittedOnDailyAt": "2025-11-11T03:58:56.878Z",
      "title": "Diffusion-SDPO: Safeguarded Direct Preference Optimization for Diffusion\n  Models",
      "submittedOnDailyBy": {
        "_id": "636f4c6b5d2050767e4a1491",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/636f4c6b5d2050767e4a1491/qErBLQnYzL35EkbDAhvqN.jpeg",
        "isPro": false,
        "fullname": "Guo-Hua Wang",
        "user": "Flourish",
        "type": "user"
      },
      "summary": "Text-to-image diffusion models deliver high-quality images, yet aligning them\nwith human preferences remains challenging. We revisit diffusion-based Direct\nPreference Optimization (DPO) for these models and identify a critical\npathology: enlarging the preference margin does not necessarily improve\ngeneration quality. In particular, the standard Diffusion-DPO objective can\nincrease the reconstruction error of both winner and loser branches.\nConsequently, degradation of the less-preferred outputs can become sufficiently\nsevere that the preferred branch is also adversely affected even as the margin\ngrows. To address this, we introduce Diffusion-SDPO, a safeguarded update rule\nthat preserves the winner by adaptively scaling the loser gradient according to\nits alignment with the winner gradient. A first-order analysis yields a\nclosed-form scaling coefficient that guarantees the error of the preferred\noutput is non-increasing at each optimization step. Our method is simple,\nmodel-agnostic, broadly compatible with existing DPO-style alignment frameworks\nand adds only marginal computational overhead. Across standard text-to-image\nbenchmarks, Diffusion-SDPO delivers consistent gains over preference-learning\nbaselines on automated preference, aesthetic, and prompt alignment metrics.\nCode is publicly available at https://github.com/AIDC-AI/Diffusion-SDPO.",
      "upvotes": 2,
      "discussionId": "6912d73fa644ba07c499c801",
      "githubRepo": "https://github.com/AIDC-AI/Diffusion-SDPO",
      "ai_summary": "Diffusion-SDPO improves text-to-image generation quality by adaptively scaling the loser gradient in preference optimization, ensuring the preferred output's error does not increase.",
      "ai_keywords": [
        "diffusion models",
        "Direct Preference Optimization (DPO)",
        "preference margin",
        "reconstruction error",
        "winner branch",
        "loser branch",
        "Diffusion-SDPO",
        "safeguarded update rule",
        "adaptive scaling",
        "winner gradient",
        "loser gradient",
        "alignment",
        "closed-form scaling coefficient",
        "automated preference",
        "aesthetic",
        "prompt alignment metrics"
      ],
      "githubStars": 2
    },
    "publishedAt": "2025-11-05T04:30:49.000Z",
    "title": "Diffusion-SDPO: Safeguarded Direct Preference Optimization for Diffusion\n  Models",
    "summary": "Text-to-image diffusion models deliver high-quality images, yet aligning them\nwith human preferences remains challenging. We revisit diffusion-based Direct\nPreference Optimization (DPO) for these models and identify a critical\npathology: enlarging the preference margin does not necessarily improve\ngeneration quality. In particular, the standard Diffusion-DPO objective can\nincrease the reconstruction error of both winner and loser branches.\nConsequently, degradation of the less-preferred outputs can become sufficiently\nsevere that the preferred branch is also adversely affected even as the margin\ngrows. To address this, we introduce Diffusion-SDPO, a safeguarded update rule\nthat preserves the winner by adaptively scaling the loser gradient according to\nits alignment with the winner gradient. A first-order analysis yields a\nclosed-form scaling coefficient that guarantees the error of the preferred\noutput is non-increasing at each optimization step. Our method is simple,\nmodel-agnostic, broadly compatible with existing DPO-style alignment frameworks\nand adds only marginal computational overhead. Across standard text-to-image\nbenchmarks, Diffusion-SDPO delivers consistent gains over preference-learning\nbaselines on automated preference, aesthetic, and prompt alignment metrics.\nCode is publicly available at https://github.com/AIDC-AI/Diffusion-SDPO.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.03317.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "636f4c6b5d2050767e4a1491",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/636f4c6b5d2050767e4a1491/qErBLQnYzL35EkbDAhvqN.jpeg",
      "fullname": "Guo-Hua Wang",
      "name": "Flourish",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.07413",
      "authors": [
        {
          "_id": "6912bc52a644ba07c499c7b0",
          "name": "Yuxuan Sun",
          "hidden": false
        },
        {
          "_id": "6912bc52a644ba07c499c7b1",
          "name": "Manchen Wang",
          "hidden": false
        },
        {
          "_id": "6912bc52a644ba07c499c7b2",
          "name": "Shengyi Qian",
          "hidden": false
        },
        {
          "_id": "6912bc52a644ba07c499c7b3",
          "name": "William R. Wong",
          "hidden": false
        },
        {
          "_id": "6912bc52a644ba07c499c7b4",
          "name": "Eric Gan",
          "hidden": false
        },
        {
          "_id": "6912bc52a644ba07c499c7b5",
          "name": "Pierluca D'Oro",
          "hidden": false
        },
        {
          "_id": "6912bc52a644ba07c499c7b6",
          "name": "Alejandro Castillejo Munoz",
          "hidden": false
        },
        {
          "_id": "6912bc52a644ba07c499c7b7",
          "name": "Sneha Silwal",
          "hidden": false
        },
        {
          "_id": "6912bc52a644ba07c499c7b8",
          "name": "Pedro Matias",
          "hidden": false
        },
        {
          "_id": "6912bc52a644ba07c499c7b9",
          "name": "Nitin Kamra",
          "hidden": false
        },
        {
          "_id": "6912bc52a644ba07c499c7ba",
          "name": "Satwik Kottur",
          "hidden": false
        },
        {
          "_id": "6912bc52a644ba07c499c7bb",
          "name": "Nick Raines",
          "hidden": false
        },
        {
          "_id": "6912bc52a644ba07c499c7bc",
          "name": "Xuanyi Zhao",
          "hidden": false
        },
        {
          "_id": "6912bc52a644ba07c499c7bd",
          "name": "Joy Chen",
          "hidden": false
        },
        {
          "_id": "6912bc52a644ba07c499c7be",
          "name": "Joseph Greer",
          "hidden": false
        },
        {
          "_id": "6912bc52a644ba07c499c7bf",
          "name": "Andrea Madotto",
          "hidden": false
        },
        {
          "_id": "6912bc52a644ba07c499c7c0",
          "name": "Allen Bolourchi",
          "hidden": false
        },
        {
          "_id": "6912bc52a644ba07c499c7c1",
          "name": "James Valori",
          "hidden": false
        },
        {
          "_id": "6912bc52a644ba07c499c7c2",
          "name": "Kevin Carlberg",
          "hidden": false
        },
        {
          "_id": "6912bc52a644ba07c499c7c3",
          "name": "Karl Ridgeway",
          "hidden": false
        },
        {
          "_id": "6912bc52a644ba07c499c7c4",
          "name": "Joseph Tighe",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-10T18:57:35.000Z",
      "submittedOnDailyAt": "2025-11-11T02:03:02.991Z",
      "title": "DigiData: Training and Evaluating General-Purpose Mobile Control Agents",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "AI agents capable of controlling user interfaces have the potential to\ntransform human interaction with digital devices. To accelerate this\ntransformation, two fundamental building blocks are essential: high-quality\ndatasets that enable agents to achieve complex and human-relevant goals, and\nrobust evaluation methods that allow researchers and practitioners to rapidly\nenhance agent performance. In this paper, we introduce DigiData, a large-scale,\nhigh-quality, diverse, multi-modal dataset designed for training mobile control\nagents. Unlike existing datasets, which derive goals from unstructured\ninteractions, DigiData is meticulously constructed through comprehensive\nexploration of app features, resulting in greater diversity and higher goal\ncomplexity. Additionally, we present DigiData-Bench, a benchmark for evaluating\nmobile control agents on real-world complex tasks. We demonstrate that the\ncommonly used step-accuracy metric falls short in reliably assessing mobile\ncontrol agents and, to address this, we propose dynamic evaluation protocols\nand AI-powered evaluations as rigorous alternatives for agent assessment. Our\ncontributions aim to significantly advance the development of mobile control\nagents, paving the way for more intuitive and effective human-device\ninteractions.",
      "upvotes": 1,
      "discussionId": "6912bc53a644ba07c499c7c5",
      "ai_summary": "DigiData and DigiData-Bench advance mobile control agents by providing a diverse, high-quality dataset and dynamic evaluation protocols, respectively.",
      "ai_keywords": [
        "mobile control agents",
        "DigiData",
        "DigiData-Bench",
        "step-accuracy metric",
        "dynamic evaluation protocols",
        "AI-powered evaluations"
      ]
    },
    "publishedAt": "2025-11-10T13:57:35.000Z",
    "title": "DigiData: Training and Evaluating General-Purpose Mobile Control Agents",
    "summary": "AI agents capable of controlling user interfaces have the potential to\ntransform human interaction with digital devices. To accelerate this\ntransformation, two fundamental building blocks are essential: high-quality\ndatasets that enable agents to achieve complex and human-relevant goals, and\nrobust evaluation methods that allow researchers and practitioners to rapidly\nenhance agent performance. In this paper, we introduce DigiData, a large-scale,\nhigh-quality, diverse, multi-modal dataset designed for training mobile control\nagents. Unlike existing datasets, which derive goals from unstructured\ninteractions, DigiData is meticulously constructed through comprehensive\nexploration of app features, resulting in greater diversity and higher goal\ncomplexity. Additionally, we present DigiData-Bench, a benchmark for evaluating\nmobile control agents on real-world complex tasks. We demonstrate that the\ncommonly used step-accuracy metric falls short in reliably assessing mobile\ncontrol agents and, to address this, we propose dynamic evaluation protocols\nand AI-powered evaluations as rigorous alternatives for agent assessment. Our\ncontributions aim to significantly advance the development of mobile control\nagents, paving the way for more intuitive and effective human-device\ninteractions.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.07413.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 159
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.07409",
      "authors": [
        {
          "_id": "6912b1b4a644ba07c499c739",
          "name": "Linzhan Mou",
          "hidden": false
        },
        {
          "_id": "6912b1b4a644ba07c499c73a",
          "name": "Jiahui Lei",
          "hidden": false
        },
        {
          "_id": "6912b1b4a644ba07c499c73b",
          "name": "Chen Wang",
          "hidden": false
        },
        {
          "_id": "6912b1b4a644ba07c499c73c",
          "name": "Lingjie Liu",
          "hidden": false
        },
        {
          "_id": "6912b1b4a644ba07c499c73d",
          "name": "Kostas Daniilidis",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/LtgESO51ssdDHDp7543DQ.mp4"
      ],
      "publishedAt": "2025-11-10T18:56:49.000Z",
      "submittedOnDailyAt": "2025-11-11T01:18:21.336Z",
      "title": "DIMO: Diverse 3D Motion Generation for Arbitrary Objects",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We present DIMO, a generative approach capable of generating diverse 3D\nmotions for arbitrary objects from a single image. The core idea of our work is\nto leverage the rich priors in well-trained video models to extract the common\nmotion patterns and then embed them into a shared low-dimensional latent space.\nSpecifically, we first generate multiple videos of the same object with diverse\nmotions. We then embed each motion into a latent vector and train a shared\nmotion decoder to learn the distribution of motions represented by a structured\nand compact motion representation, i.e., neural key point trajectories. The\ncanonical 3D Gaussians are then driven by these key points and fused to model\nthe geometry and appearance. During inference time with learned latent space,\nwe can instantly sample diverse 3D motions in a single-forward pass and support\nseveral interesting applications including 3D motion interpolation and\nlanguage-guided motion generation. Our project page is available at\nhttps://linzhanm.github.io/dimo.",
      "upvotes": 1,
      "discussionId": "6912b1efa644ba07c499c73e",
      "projectPage": "https://linzhanm.github.io/dimo",
      "ai_summary": "A generative approach extracts motion patterns from video models, embeds them into a latent space, and uses neural key point trajectories to generate diverse 3D motions from a single image.",
      "ai_keywords": [
        "generative approach",
        "3D motions",
        "video models",
        "latent space",
        "motion decoder",
        "neural key point trajectories",
        "canonical 3D Gaussians",
        "3D motion interpolation",
        "language-guided motion generation"
      ]
    },
    "publishedAt": "2025-11-10T13:56:49.000Z",
    "title": "DIMO: Diverse 3D Motion Generation for Arbitrary Objects",
    "summary": "We present DIMO, a generative approach capable of generating diverse 3D\nmotions for arbitrary objects from a single image. The core idea of our work is\nto leverage the rich priors in well-trained video models to extract the common\nmotion patterns and then embed them into a shared low-dimensional latent space.\nSpecifically, we first generate multiple videos of the same object with diverse\nmotions. We then embed each motion into a latent vector and train a shared\nmotion decoder to learn the distribution of motions represented by a structured\nand compact motion representation, i.e., neural key point trajectories. The\ncanonical 3D Gaussians are then driven by these key points and fused to model\nthe geometry and appearance. During inference time with learned latent space,\nwe can instantly sample diverse 3D motions in a single-forward pass and support\nseveral interesting applications including 3D motion interpolation and\nlanguage-guided motion generation. Our project page is available at\nhttps://linzhanm.github.io/dimo.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/LtgESO51ssdDHDp7543DQ.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.07409.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 159
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.07299",
      "authors": [
        {
          "_id": "6912dcc8a644ba07c499c81a",
          "name": "Ying Cheng",
          "hidden": false
        },
        {
          "_id": "6912dcc8a644ba07c499c81b",
          "name": "Yu-Ho Lin",
          "hidden": false
        },
        {
          "_id": "6912dcc8a644ba07c499c81c",
          "name": "Min-Hung Chen",
          "hidden": false
        },
        {
          "_id": "6912dcc8a644ba07c499c81d",
          "name": "Fu-En Yang",
          "hidden": false
        },
        {
          "_id": "6912dcc8a644ba07c499c81e",
          "name": "Shang-Hong Lai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-10T16:56:11.000Z",
      "submittedOnDailyAt": "2025-11-11T04:21:57.296Z",
      "title": "VADER: Towards Causal Video Anomaly Understanding with Relation-Aware\n  Large Language Models",
      "submittedOnDailyBy": {
        "_id": "64ae22dd1aee69ece065cdcd",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png",
        "isPro": false,
        "fullname": "Min-Hung Chen",
        "user": "cmhungsteve",
        "type": "user"
      },
      "summary": "Video anomaly understanding (VAU) aims to provide detailed interpretation and\nsemantic comprehension of anomalous events within videos, addressing\nlimitations of traditional methods that focus solely on detecting and\nlocalizing anomalies. However, existing approaches often neglect the deeper\ncausal relationships and interactions between objects, which are critical for\nunderstanding anomalous behaviors. In this paper, we propose VADER, an\nLLM-driven framework for Video Anomaly unDErstanding, which integrates keyframe\nobject Relation features with visual cues to enhance anomaly comprehension from\nvideo. Specifically, VADER first applies an Anomaly Scorer to assign per-frame\nanomaly scores, followed by a Context-AwarE Sampling (CAES) strategy to capture\nthe causal context of each anomalous event. A Relation Feature Extractor and a\nCOntrastive Relation Encoder (CORE) jointly model dynamic object interactions,\nproducing compact relational representations for downstream reasoning. These\nvisual and relational cues are integrated with LLMs to generate detailed,\ncausally grounded descriptions and support robust anomaly-related question\nanswering. Experiments on multiple real-world VAU benchmarks demonstrate that\nVADER achieves strong results across anomaly description, explanation, and\ncausal reasoning tasks, advancing the frontier of explainable video anomaly\nanalysis.",
      "upvotes": 1,
      "discussionId": "6912dcc8a644ba07c499c81f",
      "ai_summary": "VADER, an LLM-driven framework, enhances video anomaly understanding by integrating keyframe object relations and visual cues to provide detailed, causally grounded descriptions and robust question answering.",
      "ai_keywords": [
        "Anomaly Scorer",
        "Context-AwarE Sampling",
        "Relation Feature Extractor",
        "COntrastive Relation Encoder",
        "LLMs",
        "anomaly description",
        "explanation",
        "causal reasoning"
      ],
      "organization": {
        "_id": "60262b67268c201cdc8b7d43",
        "name": "nvidia",
        "fullname": "NVIDIA",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
      }
    },
    "publishedAt": "2025-11-10T11:56:11.000Z",
    "title": "VADER: Towards Causal Video Anomaly Understanding with Relation-Aware\n  Large Language Models",
    "summary": "Video anomaly understanding (VAU) aims to provide detailed interpretation and\nsemantic comprehension of anomalous events within videos, addressing\nlimitations of traditional methods that focus solely on detecting and\nlocalizing anomalies. However, existing approaches often neglect the deeper\ncausal relationships and interactions between objects, which are critical for\nunderstanding anomalous behaviors. In this paper, we propose VADER, an\nLLM-driven framework for Video Anomaly unDErstanding, which integrates keyframe\nobject Relation features with visual cues to enhance anomaly comprehension from\nvideo. Specifically, VADER first applies an Anomaly Scorer to assign per-frame\nanomaly scores, followed by a Context-AwarE Sampling (CAES) strategy to capture\nthe causal context of each anomalous event. A Relation Feature Extractor and a\nCOntrastive Relation Encoder (CORE) jointly model dynamic object interactions,\nproducing compact relational representations for downstream reasoning. These\nvisual and relational cues are integrated with LLMs to generate detailed,\ncausally grounded descriptions and support robust anomaly-related question\nanswering. Experiments on multiple real-world VAU benchmarks demonstrate that\nVADER achieves strong results across anomaly description, explanation, and\ncausal reasoning tasks, advancing the frontier of explainable video anomaly\nanalysis.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.07299.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ae22dd1aee69ece065cdcd",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png",
      "fullname": "Min-Hung Chen",
      "name": "cmhungsteve",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "organization": {
      "_id": "60262b67268c201cdc8b7d43",
      "name": "nvidia",
      "fullname": "NVIDIA",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.07061",
      "authors": [
        {
          "_id": "6912e3f3a644ba07c499c83c",
          "name": "Xinran Li",
          "hidden": false
        },
        {
          "_id": "6912e3f3a644ba07c499c83d",
          "name": "Xiujuan Xu",
          "hidden": false
        },
        {
          "_id": "6912e3f3a644ba07c499c83e",
          "name": "Jiaqi Qiao",
          "hidden": false
        },
        {
          "_id": "6912e3f3a644ba07c499c83f",
          "name": "Yu Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-10T12:52:11.000Z",
      "submittedOnDailyAt": "2025-11-11T04:52:11.193Z",
      "title": "Do LLMs Feel? Teaching Emotion Recognition with Prompts, Retrieval, and\n  Curriculum Learning",
      "submittedOnDailyBy": {
        "_id": "681227983a64768db17175ca",
        "avatarUrl": "/avatars/2229fbf20cf8a03b8dda875da613905e.svg",
        "isPro": false,
        "fullname": "LiXinran",
        "user": "LiXinran1",
        "type": "user"
      },
      "summary": "Emotion Recognition in Conversation (ERC) is a crucial task for understanding\nhuman emotions and enabling natural human-computer interaction. Although Large\nLanguage Models (LLMs) have recently shown great potential in this field, their\nability to capture the intrinsic connections between explicit and implicit\nemotions remains limited. We propose a novel ERC training framework, PRC-Emo,\nwhich integrates Prompt engineering, demonstration Retrieval, and Curriculum\nlearning, with the goal of exploring whether LLMs can effectively perceive\nemotions in conversational contexts. Specifically, we design emotion-sensitive\nprompt templates based on both explicit and implicit emotional cues to better\nguide the model in understanding the speaker's psychological states. We\nconstruct the first dedicated demonstration retrieval repository for ERC, which\nincludes training samples from widely used datasets, as well as high-quality\ndialogue examples generated by LLMs and manually verified. Moreover, we\nintroduce a curriculum learning strategy into the LoRA fine-tuning process,\nincorporating weighted emotional shifts between same-speaker and\ndifferent-speaker utterances to assign difficulty levels to dialogue samples,\nwhich are then organized in an easy-to-hard training sequence. Experimental\nresults on two benchmark datasets-- IEMOCAP and MELD --show that our method\nachieves new state-of-the-art (SOTA) performance, demonstrating the\neffectiveness and generalizability of our approach in improving LLM-based\nemotional understanding.",
      "upvotes": 1,
      "discussionId": "6912e3f4a644ba07c499c840",
      "githubRepo": "https://github.com/LiXinran6/PRC-Emo",
      "ai_summary": "A novel ERC training framework, PRC-Emo, integrates prompt engineering, demonstration retrieval, and curriculum learning to enhance LLMs' ability to perceive emotions in conversations, achieving state-of-the-art performance on benchmark datasets.",
      "ai_keywords": [
        "Prompt engineering",
        "demonstration retrieval",
        "curriculum learning",
        "LoRA fine-tuning",
        "emotion-sensitive prompt templates",
        "IEMOCAP",
        "MELD"
      ]
    },
    "publishedAt": "2025-11-10T07:52:11.000Z",
    "title": "Do LLMs Feel? Teaching Emotion Recognition with Prompts, Retrieval, and\n  Curriculum Learning",
    "summary": "Emotion Recognition in Conversation (ERC) is a crucial task for understanding\nhuman emotions and enabling natural human-computer interaction. Although Large\nLanguage Models (LLMs) have recently shown great potential in this field, their\nability to capture the intrinsic connections between explicit and implicit\nemotions remains limited. We propose a novel ERC training framework, PRC-Emo,\nwhich integrates Prompt engineering, demonstration Retrieval, and Curriculum\nlearning, with the goal of exploring whether LLMs can effectively perceive\nemotions in conversational contexts. Specifically, we design emotion-sensitive\nprompt templates based on both explicit and implicit emotional cues to better\nguide the model in understanding the speaker's psychological states. We\nconstruct the first dedicated demonstration retrieval repository for ERC, which\nincludes training samples from widely used datasets, as well as high-quality\ndialogue examples generated by LLMs and manually verified. Moreover, we\nintroduce a curriculum learning strategy into the LoRA fine-tuning process,\nincorporating weighted emotional shifts between same-speaker and\ndifferent-speaker utterances to assign difficulty levels to dialogue samples,\nwhich are then organized in an easy-to-hard training sequence. Experimental\nresults on two benchmark datasets-- IEMOCAP and MELD --show that our method\nachieves new state-of-the-art (SOTA) performance, demonstrating the\neffectiveness and generalizability of our approach in improving LLM-based\nemotional understanding.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.07061.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "681227983a64768db17175ca",
      "avatarUrl": "/avatars/2229fbf20cf8a03b8dda875da613905e.svg",
      "fullname": "LiXinran",
      "name": "LiXinran1",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.06090",
      "authors": [
        {
          "_id": "6912b9f9a644ba07c499c789",
          "name": "Jeffrey Jian Ma",
          "hidden": false
        },
        {
          "_id": "6912b9f9a644ba07c499c78a",
          "name": "Milad Hashemi",
          "hidden": false
        },
        {
          "_id": "6912b9f9a644ba07c499c78b",
          "name": "Amir Yazdanbakhsh",
          "hidden": false
        },
        {
          "_id": "6912b9f9a644ba07c499c78c",
          "name": "Kevin Swersky",
          "hidden": false
        },
        {
          "_id": "6912b9f9a644ba07c499c78d",
          "name": "Ofir Press",
          "hidden": false
        },
        {
          "_id": "6912b9f9a644ba07c499c78e",
          "name": "Enhui Li",
          "hidden": false
        },
        {
          "_id": "6912b9f9a644ba07c499c78f",
          "name": "Vijay Janapa Reddi",
          "hidden": false
        },
        {
          "_id": "6912b9f9a644ba07c499c790",
          "name": "Parthasarathy Ranganathan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-08T17:55:09.000Z",
      "submittedOnDailyAt": "2025-11-11T01:52:35.484Z",
      "title": "SWE-fficiency: Can Language Models Optimize Real-World Repositories on\n  Real Workloads?",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Optimizing the performance of large-scale software repositories demands\nexpertise in code reasoning and software engineering (SWE) to reduce runtime\nwhile preserving program correctness. However, most benchmarks emphasize what\nto fix rather than how to fix code. We introduce SWE-fficiency, a\nbenchmark for evaluating repository-level performance optimization on real\nworkloads. Our suite contains 498 tasks across nine widely used data-science,\nmachine-learning, and HPC repositories (e.g., numpy, pandas, scipy): given a\ncomplete codebase and a slow workload, an agent must investigate code\nsemantics, localize bottlenecks and relevant tests, and produce a patch that\nmatches or exceeds expert speedup while passing the same unit tests. To enable\nthis how-to-fix evaluation, our automated pipeline scrapes GitHub pull requests\nfor performance-improving edits, combining keyword filtering, static analysis,\ncoverage tooling, and execution validation to both confirm expert speedup\nbaselines and identify relevant repository unit tests. Empirical evaluation of\nstate-of-the-art agents reveals significant underperformance. On average,\nagents achieve less than 0.15x the expert speedup: agents struggle in\nlocalizing optimization opportunities, reasoning about execution across\nfunctions, and maintaining correctness in proposed edits. We release the\nbenchmark and accompanying data pipeline to facilitate research on automated\nperformance engineering and long-horizon software reasoning.",
      "upvotes": 1,
      "discussionId": "6912b9faa644ba07c499c791",
      "projectPage": "https://swefficiency.com/"
    },
    "publishedAt": "2025-11-08T12:55:09.000Z",
    "title": "SWE-fficiency: Can Language Models Optimize Real-World Repositories on\n  Real Workloads?",
    "summary": "Optimizing the performance of large-scale software repositories demands\nexpertise in code reasoning and software engineering (SWE) to reduce runtime\nwhile preserving program correctness. However, most benchmarks emphasize what\nto fix rather than how to fix code. We introduce SWE-fficiency, a\nbenchmark for evaluating repository-level performance optimization on real\nworkloads. Our suite contains 498 tasks across nine widely used data-science,\nmachine-learning, and HPC repositories (e.g., numpy, pandas, scipy): given a\ncomplete codebase and a slow workload, an agent must investigate code\nsemantics, localize bottlenecks and relevant tests, and produce a patch that\nmatches or exceeds expert speedup while passing the same unit tests. To enable\nthis how-to-fix evaluation, our automated pipeline scrapes GitHub pull requests\nfor performance-improving edits, combining keyword filtering, static analysis,\ncoverage tooling, and execution validation to both confirm expert speedup\nbaselines and identify relevant repository unit tests. Empirical evaluation of\nstate-of-the-art agents reveals significant underperformance. On average,\nagents achieve less than 0.15x the expert speedup: agents struggle in\nlocalizing optimization opportunities, reasoning about execution across\nfunctions, and maintaining correctness in proposed edits. We release the\nbenchmark and accompanying data pipeline to facilitate research on automated\nperformance engineering and long-horizon software reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.06090.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 159
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.05936",
      "authors": [
        {
          "_id": "6912db4ea644ba07c499c80e",
          "name": "Soujanya Poria",
          "hidden": false
        },
        {
          "_id": "6912db4ea644ba07c499c80f",
          "name": "Navonil Majumder",
          "hidden": false
        },
        {
          "_id": "6912db4ea644ba07c499c810",
          "name": "Chia-Yu Hung",
          "hidden": false
        },
        {
          "_id": "6912db4ea644ba07c499c811",
          "name": "Amir Ali Bagherzadeh",
          "hidden": false
        },
        {
          "_id": "6912db4ea644ba07c499c812",
          "name": "Chuan Li",
          "hidden": false
        },
        {
          "_id": "6912db4ea644ba07c499c813",
          "name": "Kenneth Kwok",
          "hidden": false
        },
        {
          "_id": "6912db4ea644ba07c499c814",
          "name": "Ziwei Wang",
          "hidden": false
        },
        {
          "_id": "6912db4ea644ba07c499c815",
          "name": "Cheston Tan",
          "hidden": false
        },
        {
          "_id": "6912db4ea644ba07c499c816",
          "name": "Jiajun Wu",
          "hidden": false
        },
        {
          "_id": "6912db4ea644ba07c499c817",
          "name": "David Hsu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-08T09:02:13.000Z",
      "submittedOnDailyAt": "2025-11-11T04:49:33.265Z",
      "title": "10 Open Challenges Steering the Future of Vision-Language-Action Models",
      "submittedOnDailyBy": {
        "_id": "626b626405fe1cb65725aca1",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/626b626405fe1cb65725aca1/ZVSbhynzpQhVGq9kGywW6.png",
        "isPro": false,
        "fullname": "Soujanya Poria",
        "user": "soujanyaporia",
        "type": "user"
      },
      "summary": "Due to their ability of follow natural language instructions,\nvision-language-action (VLA) models are increasingly prevalent in the embodied\nAI arena, following the widespread success of their precursors -- LLMs and\nVLMs. In this paper, we discuss 10 principal milestones in the ongoing\ndevelopment of VLA models -- multimodality, reasoning, data, evaluation,\ncross-robot action generalization, efficiency, whole-body coordination, safety,\nagents, and coordination with humans. Furthermore, we discuss the emerging\ntrends of using spatial understanding, modeling world dynamics, post training,\nand data synthesis -- all aiming to reach these milestones. Through these\ndiscussions, we hope to bring attention to the research avenues that may\naccelerate the development of VLA models into wider acceptability.",
      "upvotes": 1,
      "discussionId": "6912db4fa644ba07c499c818",
      "ai_summary": "VLA models, combining vision, language, and action, are advancing through milestones like multimodality, reasoning, and safety, with trends focusing on spatial understanding and human coordination.",
      "ai_keywords": [
        "vision-language-action (VLA) models",
        "embodied AI",
        "multimodality",
        "reasoning",
        "data",
        "evaluation",
        "cross-robot action generalization",
        "efficiency",
        "whole-body coordination",
        "safety",
        "agents",
        "coordination with humans",
        "spatial understanding",
        "modeling world dynamics",
        "post training",
        "data synthesis"
      ],
      "organization": {
        "_id": "626ab9dac804c432c1b27a48",
        "name": "declare-lab",
        "fullname": "Deep Cognition and Language Research (DeCLaRe) Lab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/626b626405fe1cb65725aca1/grq3rj2uj0WRjjPjAtR1I.png"
      }
    },
    "publishedAt": "2025-11-08T04:02:13.000Z",
    "title": "10 Open Challenges Steering the Future of Vision-Language-Action Models",
    "summary": "Due to their ability of follow natural language instructions,\nvision-language-action (VLA) models are increasingly prevalent in the embodied\nAI arena, following the widespread success of their precursors -- LLMs and\nVLMs. In this paper, we discuss 10 principal milestones in the ongoing\ndevelopment of VLA models -- multimodality, reasoning, data, evaluation,\ncross-robot action generalization, efficiency, whole-body coordination, safety,\nagents, and coordination with humans. Furthermore, we discuss the emerging\ntrends of using spatial understanding, modeling world dynamics, post training,\nand data synthesis -- all aiming to reach these milestones. Through these\ndiscussions, we hope to bring attention to the research avenues that may\naccelerate the development of VLA models into wider acceptability.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.05936.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "626b626405fe1cb65725aca1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/626b626405fe1cb65725aca1/ZVSbhynzpQhVGq9kGywW6.png",
      "fullname": "Soujanya Poria",
      "name": "soujanyaporia",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "organization": {
      "_id": "626ab9dac804c432c1b27a48",
      "name": "declare-lab",
      "fullname": "Deep Cognition and Language Research (DeCLaRe) Lab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/626b626405fe1cb65725aca1/grq3rj2uj0WRjjPjAtR1I.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.06174",
      "authors": [
        {
          "_id": "6912be89a644ba07c499c7c7",
          "name": "Zifan He",
          "hidden": false
        },
        {
          "_id": "6912be89a644ba07c499c7c8",
          "name": "Shengyu Ye",
          "hidden": false
        },
        {
          "_id": "6912be89a644ba07c499c7c9",
          "name": "Rui Ma",
          "hidden": false
        },
        {
          "_id": "6912be89a644ba07c499c7ca",
          "name": "Yang Wang",
          "hidden": false
        },
        {
          "_id": "6912be89a644ba07c499c7cb",
          "name": "Jason Cong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-09T01:17:08.000Z",
      "submittedOnDailyAt": "2025-11-11T02:12:48.170Z",
      "title": "LUT-LLM: Efficient Large Language Model Inference with Memory-based\n  Computations on FPGAs",
      "submittedOnDailyBy": {
        "_id": "6542bbc0bcc3731b0debc628",
        "avatarUrl": "/avatars/7bcbff0738cf7efb9fbff73084446ea6.svg",
        "isPro": false,
        "fullname": "Zifan He",
        "user": "OswaldHe123",
        "type": "user"
      },
      "summary": "The rapid progress of large language models (LLMs) has advanced numerous\napplications, yet efficient single-batch inference remains vital for on-device\nintelligence. While FPGAs offer fine-grained data control and high energy\nefficiency, recent GPU optimizations have narrowed their advantage, especially\nunder arithmetic-based computation. To overcome this, we leverage FPGAs'\nabundant on-chip memory to shift LLM inference from arithmetic- to memory-based\ncomputation through table lookups. We present LUT-LLM, the first FPGA\naccelerator enabling 1B+ LLM inference via vector-quantized memory operations.\nOur analysis identifies activation-weight co-quantization as the most effective\nscheme, supported by (1) bandwidth-aware parallel centroid search, (2)\nefficient 2D table lookups, and (3) a spatial-temporal hybrid design minimizing\ndata caching. Implemented on an AMD V80 FPGA for a customized Qwen 3 1.7B\nmodel, LUT-LLM achieves 1.66x lower latency than AMD MI210 and 1.72x higher\nenergy efficiency than NVIDIA A100, scaling to 32B models with 2.16x efficiency\ngain over A100.",
      "upvotes": 0,
      "discussionId": "6912be89a644ba07c499c7cc",
      "ai_summary": "LUT-LLM, an FPGA accelerator, improves LLM inference efficiency by shifting computation to memory-based operations, achieving lower latency and higher energy efficiency compared to GPUs.",
      "ai_keywords": [
        "LLMs",
        "FPGAs",
        "memory-based computation",
        "table lookups",
        "LUT-LLM",
        "vector-quantized memory operations",
        "activation-weight co-quantization",
        "bandwidth-aware parallel centroid search",
        "efficient 2D table lookups",
        "spatial-temporal hybrid design",
        "Qwen 3 1.7B",
        "AMD V80 FPGA",
        "AMD MI210",
        "NVIDIA A100"
      ]
    },
    "publishedAt": "2025-11-08T20:17:08.000Z",
    "title": "LUT-LLM: Efficient Large Language Model Inference with Memory-based\n  Computations on FPGAs",
    "summary": "The rapid progress of large language models (LLMs) has advanced numerous\napplications, yet efficient single-batch inference remains vital for on-device\nintelligence. While FPGAs offer fine-grained data control and high energy\nefficiency, recent GPU optimizations have narrowed their advantage, especially\nunder arithmetic-based computation. To overcome this, we leverage FPGAs'\nabundant on-chip memory to shift LLM inference from arithmetic- to memory-based\ncomputation through table lookups. We present LUT-LLM, the first FPGA\naccelerator enabling 1B+ LLM inference via vector-quantized memory operations.\nOur analysis identifies activation-weight co-quantization as the most effective\nscheme, supported by (1) bandwidth-aware parallel centroid search, (2)\nefficient 2D table lookups, and (3) a spatial-temporal hybrid design minimizing\ndata caching. Implemented on an AMD V80 FPGA for a customized Qwen 3 1.7B\nmodel, LUT-LLM achieves 1.66x lower latency than AMD MI210 and 1.72x higher\nenergy efficiency than NVIDIA A100, scaling to 32B models with 2.16x efficiency\ngain over A100.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.06174.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6542bbc0bcc3731b0debc628",
      "avatarUrl": "/avatars/7bcbff0738cf7efb9fbff73084446ea6.svg",
      "fullname": "Zifan He",
      "name": "OswaldHe123",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]