[
  {
    "paper": {
      "id": "2503.17359",
      "authors": [
        {
          "_id": "67e16a266280a70b45b8a16c",
          "name": "Jiwen Yu",
          "hidden": false
        },
        {
          "_id": "67e16a266280a70b45b8a16d",
          "name": "Yiran Qin",
          "hidden": false
        },
        {
          "_id": "67e16a266280a70b45b8a16e",
          "name": "Haoxuan Che",
          "hidden": false
        },
        {
          "_id": "67e16a266280a70b45b8a16f",
          "name": "Quande Liu",
          "hidden": false
        },
        {
          "_id": "67e16a266280a70b45b8a170",
          "name": "Xintao Wang",
          "hidden": false
        },
        {
          "_id": "67e16a266280a70b45b8a171",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "67e16a266280a70b45b8a172",
          "name": "Di Zhang",
          "hidden": false
        },
        {
          "_id": "67e16a266280a70b45b8a173",
          "name": "Xihui Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-21T17:59:22.000Z",
      "submittedOnDailyAt": "2025-03-25T01:42:15.880Z",
      "title": "Position: Interactive Generative Video as Next-Generation Game Engine",
      "submittedOnDailyBy": {
        "_id": "64105a6d14215c0775dfdd14",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64105a6d14215c0775dfdd14/-VX-cUYOLjHIg7QnWhRGG.jpeg",
        "isPro": false,
        "fullname": "Jiwen Yu",
        "user": "VictorYuki",
        "type": "user"
      },
      "summary": "Modern game development faces significant challenges in creativity and cost\ndue to predetermined content in traditional game engines. Recent breakthroughs\nin video generation models, capable of synthesizing realistic and interactive\nvirtual environments, present an opportunity to revolutionize game creation. In\nthis position paper, we propose Interactive Generative Video (IGV) as the\nfoundation for Generative Game Engines (GGE), enabling unlimited novel content\ngeneration in next-generation gaming. GGE leverages IGV's unique strengths in\nunlimited high-quality content synthesis, physics-aware world modeling,\nuser-controlled interactivity, long-term memory capabilities, and causal\nreasoning. We present a comprehensive framework detailing GGE's core modules\nand a hierarchical maturity roadmap (L0-L4) to guide its evolution. Our work\ncharts a new course for game development in the AI era, envisioning a future\nwhere AI-powered generative systems fundamentally reshape how games are created\nand experienced.",
      "upvotes": 44,
      "discussionId": "67e16a276280a70b45b8a214",
      "ai_keywords": [
        "Interactive Generative Video (IGV)",
        "Generative Game Engines (GGE)",
        "video generation models",
        "high-quality content synthesis",
        "physics-aware world modeling",
        "user-controlled interactivity",
        "long-term memory capabilities",
        "causal reasoning",
        "hierarchical maturity roadmap (L0-L4)"
      ]
    },
    "publishedAt": "2025-03-21T13:59:22.000Z",
    "title": "Position: Interactive Generative Video as Next-Generation Game Engine",
    "summary": "Modern game development faces significant challenges in creativity and cost\ndue to predetermined content in traditional game engines. Recent breakthroughs\nin video generation models, capable of synthesizing realistic and interactive\nvirtual environments, present an opportunity to revolutionize game creation. In\nthis position paper, we propose Interactive Generative Video (IGV) as the\nfoundation for Generative Game Engines (GGE), enabling unlimited novel content\ngeneration in next-generation gaming. GGE leverages IGV's unique strengths in\nunlimited high-quality content synthesis, physics-aware world modeling,\nuser-controlled interactivity, long-term memory capabilities, and causal\nreasoning. We present a comprehensive framework detailing GGE's core modules\nand a hierarchical maturity roadmap (L0-L4) to guide its evolution. Our work\ncharts a new course for game development in the AI era, envisioning a future\nwhere AI-powered generative systems fundamentally reshape how games are created\nand experienced.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17359.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64105a6d14215c0775dfdd14",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64105a6d14215c0775dfdd14/-VX-cUYOLjHIg7QnWhRGG.jpeg",
      "fullname": "Jiwen Yu",
      "name": "VictorYuki",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.18942",
      "authors": [
        {
          "_id": "67e226039cd910bee045e38f",
          "name": "Fangfu Liu",
          "hidden": false
        },
        {
          "_id": "67e226039cd910bee045e390",
          "name": "Hanyang Wang",
          "hidden": false
        },
        {
          "_id": "67e226039cd910bee045e391",
          "name": "Yimo Cai",
          "hidden": false
        },
        {
          "_id": "67e226039cd910bee045e392",
          "name": "Kaiyan Zhang",
          "hidden": false
        },
        {
          "_id": "67e226039cd910bee045e393",
          "name": "Xiaohang Zhan",
          "hidden": false
        },
        {
          "_id": "67e226039cd910bee045e394",
          "name": "Yueqi Duan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T17:59:04.000Z",
      "submittedOnDailyAt": "2025-03-25T02:12:44.893Z",
      "title": "Video-T1: Test-Time Scaling for Video Generation",
      "submittedOnDailyBy": {
        "_id": "6505a02f9310ce8c400edc63",
        "avatarUrl": "/avatars/bbf781594fc8c812316711aa8e2797aa.svg",
        "isPro": false,
        "fullname": "Fangfu Liu",
        "user": "Liuff23",
        "type": "user"
      },
      "summary": "With the scale capability of increasing training data, model size, and\ncomputational cost, video generation has achieved impressive results in digital\ncreation, enabling users to express creativity across various domains.\nRecently, researchers in Large Language Models (LLMs) have expanded the scaling\nto test-time, which can significantly improve LLM performance by using more\ninference-time computation. Instead of scaling up video foundation models\nthrough expensive training costs, we explore the power of Test-Time Scaling\n(TTS) in video generation, aiming to answer the question: if a video generation\nmodel is allowed to use non-trivial amount of inference-time compute, how much\ncan it improve generation quality given a challenging text prompt. In this\nwork, we reinterpret the test-time scaling of video generation as a searching\nproblem to sample better trajectories from Gaussian noise space to the target\nvideo distribution. Specifically, we build the search space with test-time\nverifiers to provide feedback and heuristic algorithms to guide searching\nprocess. Given a text prompt, we first explore an intuitive linear search\nstrategy by increasing noise candidates at inference time. As full-step\ndenoising all frames simultaneously requires heavy test-time computation costs,\nwe further design a more efficient TTS method for video generation called\nTree-of-Frames (ToF) that adaptively expands and prunes video branches in an\nautoregressive manner. Extensive experiments on text-conditioned video\ngeneration benchmarks demonstrate that increasing test-time compute\nconsistently leads to significant improvements in the quality of videos.\nProject page: https://liuff19.github.io/Video-T1",
      "upvotes": 37,
      "discussionId": "67e226059cd910bee045e42b",
      "projectPage": "https://liuff19.github.io/Video-T1/",
      "githubRepo": "https://github.com/liuff19/Video-T1",
      "ai_keywords": [
        "Test-Time Scaling (TTS)",
        "video foundation models",
        "inference-time computation",
        "Gaussian noise space",
        "target video distribution",
        "test-time verifiers",
        "heuristic algorithms",
        "linear search strategy",
        "noise candidates",
        "full-step denoising",
        "inference time",
        "Tree-of-Frames (ToF)",
        "autoregressive manner",
        "text-conditioned video generation benchmarks"
      ]
    },
    "publishedAt": "2025-03-24T13:59:04.000Z",
    "title": "Video-T1: Test-Time Scaling for Video Generation",
    "summary": "With the scale capability of increasing training data, model size, and\ncomputational cost, video generation has achieved impressive results in digital\ncreation, enabling users to express creativity across various domains.\nRecently, researchers in Large Language Models (LLMs) have expanded the scaling\nto test-time, which can significantly improve LLM performance by using more\ninference-time computation. Instead of scaling up video foundation models\nthrough expensive training costs, we explore the power of Test-Time Scaling\n(TTS) in video generation, aiming to answer the question: if a video generation\nmodel is allowed to use non-trivial amount of inference-time compute, how much\ncan it improve generation quality given a challenging text prompt. In this\nwork, we reinterpret the test-time scaling of video generation as a searching\nproblem to sample better trajectories from Gaussian noise space to the target\nvideo distribution. Specifically, we build the search space with test-time\nverifiers to provide feedback and heuristic algorithms to guide searching\nprocess. Given a text prompt, we first explore an intuitive linear search\nstrategy by increasing noise candidates at inference time. As full-step\ndenoising all frames simultaneously requires heavy test-time computation costs,\nwe further design a more efficient TTS method for video generation called\nTree-of-Frames (ToF) that adaptively expands and prunes video branches in an\nautoregressive manner. Extensive experiments on text-conditioned video\ngeneration benchmarks demonstrate that increasing test-time compute\nconsistently leads to significant improvements in the quality of videos.\nProject page: https://liuff19.github.io/Video-T1",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18942.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6505a02f9310ce8c400edc63",
      "avatarUrl": "/avatars/bbf781594fc8c812316711aa8e2797aa.svg",
      "fullname": "Fangfu Liu",
      "name": "Liuff23",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.18892",
      "authors": [
        {
          "_id": "67e22ce1155ea10f2fdbe5d2",
          "name": "Weihao Zeng",
          "hidden": false
        },
        {
          "_id": "67e22ce1155ea10f2fdbe5d3",
          "name": "Yuzhen Huang",
          "hidden": false
        },
        {
          "_id": "67e22ce1155ea10f2fdbe5d4",
          "name": "Qian Liu",
          "hidden": false
        },
        {
          "_id": "67e22ce1155ea10f2fdbe5d5",
          "name": "Wei Liu",
          "hidden": false
        },
        {
          "_id": "67e22ce1155ea10f2fdbe5d6",
          "name": "Keqing He",
          "hidden": false
        },
        {
          "_id": "67e22ce1155ea10f2fdbe5d7",
          "name": "Zejun Ma",
          "hidden": false
        },
        {
          "_id": "67e22ce1155ea10f2fdbe5d8",
          "name": "Junxian He",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T17:06:10.000Z",
      "submittedOnDailyAt": "2025-03-25T02:41:40.812Z",
      "title": "SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for\n  Open Base Models in the Wild",
      "submittedOnDailyBy": {
        "_id": "62751082b43ccfeef483424f",
        "avatarUrl": "/avatars/fec83e4478e7d1731ba6033328131852.svg",
        "isPro": false,
        "fullname": "WeihaoZeng",
        "user": "AndrewZeng",
        "type": "user"
      },
      "summary": "DeepSeek-R1 has shown that long chain-of-thought (CoT) reasoning can\nnaturally emerge through a simple reinforcement learning (RL) framework with\nrule-based rewards, where the training may directly start from the base\nmodels-a paradigm referred to as zero RL training. Most recent efforts to\nreproduce zero RL training have primarily focused on the Qwen2.5 model series,\nwhich may not be representative as we find the base models already exhibit\nstrong instruction-following and self-reflection abilities. In this work, we\ninvestigate zero RL training across 10 diverse base models, spanning different\nfamilies and sizes including LLama3-8B, Mistral-7B/24B, DeepSeek-Math-7B,\nQwen2.5-math-7B, and all Qwen2.5 models from 0.5B to 32B. Leveraging several\nkey design strategies-such as adjusting format reward and controlling query\ndifficulty-we achieve substantial improvements in both reasoning accuracy and\nresponse length across most settings. However, by carefully monitoring the\ntraining dynamics, we observe that different base models exhibit distinct\npatterns during training. For instance, the increased response length does not\nalways correlate with the emergence of certain cognitive behaviors such as\nverification (i.e., the \"aha moment\"). Notably, we observe the \"aha moment\" for\nthe first time in small models not from the Qwen family. We share the key\ndesigns that enable successful zero RL training, along with our findings and\npractices. To facilitate further research, we open-source the code, models, and\nanalysis tools.",
      "upvotes": 13,
      "discussionId": "67e22ce3155ea10f2fdbe6c0",
      "githubRepo": "https://github.com/hkust-nlp/simpleRL-reason",
      "ai_keywords": [
        "reinforcement learning",
        "rule-based rewards",
        "zero RL training",
        "long chain-of-thought (CoT) reasoning",
        "instruction-following",
        "self-reflection",
        "base models",
        "Qwen2.5 model series",
        "LLama3-8B",
        "Mistral-7B/24B",
        "DeepSeek-Math-7B",
        "Qwen2.5-math-7B",
        "response length",
        "reasoning accuracy",
        "cognitive behaviors",
        "verification",
        "training dynamics"
      ]
    },
    "publishedAt": "2025-03-24T13:06:10.000Z",
    "title": "SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for\n  Open Base Models in the Wild",
    "summary": "DeepSeek-R1 has shown that long chain-of-thought (CoT) reasoning can\nnaturally emerge through a simple reinforcement learning (RL) framework with\nrule-based rewards, where the training may directly start from the base\nmodels-a paradigm referred to as zero RL training. Most recent efforts to\nreproduce zero RL training have primarily focused on the Qwen2.5 model series,\nwhich may not be representative as we find the base models already exhibit\nstrong instruction-following and self-reflection abilities. In this work, we\ninvestigate zero RL training across 10 diverse base models, spanning different\nfamilies and sizes including LLama3-8B, Mistral-7B/24B, DeepSeek-Math-7B,\nQwen2.5-math-7B, and all Qwen2.5 models from 0.5B to 32B. Leveraging several\nkey design strategies-such as adjusting format reward and controlling query\ndifficulty-we achieve substantial improvements in both reasoning accuracy and\nresponse length across most settings. However, by carefully monitoring the\ntraining dynamics, we observe that different base models exhibit distinct\npatterns during training. For instance, the increased response length does not\nalways correlate with the emergence of certain cognitive behaviors such as\nverification (i.e., the \"aha moment\"). Notably, we observe the \"aha moment\" for\nthe first time in small models not from the Qwen family. We share the key\ndesigns that enable successful zero RL training, along with our findings and\npractices. To facilitate further research, we open-source the code, models, and\nanalysis tools.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18892.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62751082b43ccfeef483424f",
      "avatarUrl": "/avatars/fec83e4478e7d1731ba6033328131852.svg",
      "fullname": "WeihaoZeng",
      "name": "AndrewZeng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.18940",
      "authors": [
        {
          "_id": "67e21b305d20ec3277dac34a",
          "name": "Ye Tian",
          "hidden": false
        },
        {
          "_id": "67e21b305d20ec3277dac34b",
          "name": "Xin Xia",
          "hidden": false
        },
        {
          "_id": "67e21b305d20ec3277dac34c",
          "name": "Yuxi Ren",
          "hidden": false
        },
        {
          "_id": "67e21b305d20ec3277dac34d",
          "name": "Shanchuan Lin",
          "hidden": false
        },
        {
          "_id": "67e21b305d20ec3277dac34e",
          "name": "Xing Wang",
          "hidden": false
        },
        {
          "_id": "67e21b305d20ec3277dac34f",
          "name": "Xuefeng Xiao",
          "hidden": false
        },
        {
          "_id": "67e21b305d20ec3277dac350",
          "name": "Yunhai Tong",
          "hidden": false
        },
        {
          "_id": "67e21b305d20ec3277dac351",
          "name": "Ling Yang",
          "hidden": false
        },
        {
          "_id": "67e21b305d20ec3277dac352",
          "name": "Bin Cui",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T17:59:02.000Z",
      "submittedOnDailyAt": "2025-03-25T01:26:48.606Z",
      "title": "Training-free Diffusion Acceleration with Bottleneck Sampling",
      "submittedOnDailyBy": {
        "_id": "64fde4e252e82dd432b74ce9",
        "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
        "isPro": false,
        "fullname": "Ling Yang",
        "user": "Lingaaaaaaa",
        "type": "user"
      },
      "summary": "Diffusion models have demonstrated remarkable capabilities in visual content\ngeneration but remain challenging to deploy due to their high computational\ncost during inference. This computational burden primarily arises from the\nquadratic complexity of self-attention with respect to image or video\nresolution. While existing acceleration methods often compromise output quality\nor necessitate costly retraining, we observe that most diffusion models are\npre-trained at lower resolutions, presenting an opportunity to exploit these\nlow-resolution priors for more efficient inference without degrading\nperformance. In this work, we introduce Bottleneck Sampling, a training-free\nframework that leverages low-resolution priors to reduce computational overhead\nwhile preserving output fidelity. Bottleneck Sampling follows a high-low-high\ndenoising workflow: it performs high-resolution denoising in the initial and\nfinal stages while operating at lower resolutions in intermediate steps. To\nmitigate aliasing and blurring artifacts, we further refine the resolution\ntransition points and adaptively shift the denoising timesteps at each stage.\nWe evaluate Bottleneck Sampling on both image and video generation tasks, where\nextensive experiments demonstrate that it accelerates inference by up to\n3times for image generation and 2.5times for video generation, all while\nmaintaining output quality comparable to the standard full-resolution sampling\nprocess across multiple evaluation metrics. Code is available at:\nhttps://github.com/tyfeld/Bottleneck-Sampling",
      "upvotes": 10,
      "discussionId": "67e21b365d20ec3277dac500",
      "projectPage": "https://tyfeld.github.io/BottleneckSampling.github.io",
      "githubRepo": "https://github.com/tyfeld/Bottleneck-Sampling",
      "ai_keywords": [
        "diffusion models",
        "self-attention",
        "computational overhead",
        "low-resolution priors",
        "Bottleneck Sampling",
        "denoising workflow",
        "high-resolution denoising",
        "aliasing",
        "blurring artifacts",
        "resolution transition points",
        "adaptive timesteps"
      ]
    },
    "publishedAt": "2025-03-24T13:59:02.000Z",
    "title": "Training-free Diffusion Acceleration with Bottleneck Sampling",
    "summary": "Diffusion models have demonstrated remarkable capabilities in visual content\ngeneration but remain challenging to deploy due to their high computational\ncost during inference. This computational burden primarily arises from the\nquadratic complexity of self-attention with respect to image or video\nresolution. While existing acceleration methods often compromise output quality\nor necessitate costly retraining, we observe that most diffusion models are\npre-trained at lower resolutions, presenting an opportunity to exploit these\nlow-resolution priors for more efficient inference without degrading\nperformance. In this work, we introduce Bottleneck Sampling, a training-free\nframework that leverages low-resolution priors to reduce computational overhead\nwhile preserving output fidelity. Bottleneck Sampling follows a high-low-high\ndenoising workflow: it performs high-resolution denoising in the initial and\nfinal stages while operating at lower resolutions in intermediate steps. To\nmitigate aliasing and blurring artifacts, we further refine the resolution\ntransition points and adaptively shift the denoising timesteps at each stage.\nWe evaluate Bottleneck Sampling on both image and video generation tasks, where\nextensive experiments demonstrate that it accelerates inference by up to\n3times for image generation and 2.5times for video generation, all while\nmaintaining output quality comparable to the standard full-resolution sampling\nprocess across multiple evaluation metrics. Code is available at:\nhttps://github.com/tyfeld/Bottleneck-Sampling",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18940.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64fde4e252e82dd432b74ce9",
      "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
      "fullname": "Ling Yang",
      "name": "Lingaaaaaaa",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.17489",
      "authors": [
        {
          "_id": "67e21f300e6b6fcc3eb38ae1",
          "name": "Shu Pu",
          "hidden": false
        },
        {
          "_id": "67e21f300e6b6fcc3eb38ae2",
          "name": "Yaochen Wang",
          "hidden": false
        },
        {
          "_id": "67e21f300e6b6fcc3eb38ae3",
          "name": "Dongping Chen",
          "hidden": false
        },
        {
          "_id": "67e21f300e6b6fcc3eb38ae4",
          "name": "Yuhang Chen",
          "hidden": false
        },
        {
          "_id": "67e21f300e6b6fcc3eb38ae5",
          "name": "Guohao Wang",
          "hidden": false
        },
        {
          "_id": "67e21f300e6b6fcc3eb38ae6",
          "name": "Qi Qin",
          "hidden": false
        },
        {
          "_id": "67e21f300e6b6fcc3eb38ae7",
          "name": "Zhongyi Zhang",
          "hidden": false
        },
        {
          "_id": "67e21f300e6b6fcc3eb38ae8",
          "name": "Zhiyuan Zhang",
          "hidden": false
        },
        {
          "_id": "67e21f300e6b6fcc3eb38ae9",
          "name": "Zetong Zhou",
          "hidden": false
        },
        {
          "_id": "67e21f300e6b6fcc3eb38aea",
          "name": "Shuang Gong",
          "hidden": false
        },
        {
          "_id": "67e21f300e6b6fcc3eb38aeb",
          "name": "Yi Gui",
          "hidden": false
        },
        {
          "_id": "67e21f300e6b6fcc3eb38aec",
          "name": "Yao Wan",
          "hidden": false
        },
        {
          "_id": "67e21f300e6b6fcc3eb38aed",
          "name": "Philip S. Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-21T18:59:20.000Z",
      "submittedOnDailyAt": "2025-03-25T01:46:07.985Z",
      "title": "Judge Anything: MLLM as a Judge Across Any Modality",
      "submittedOnDailyBy": {
        "_id": "643be8879f5d314db2d9ed23",
        "avatarUrl": "/avatars/64e9bb2c4e10fbe03e2b81afedf40865.svg",
        "isPro": false,
        "fullname": "Chen Dongping",
        "user": "shuaishuaicdp",
        "type": "user"
      },
      "summary": "Evaluating generative foundation models on open-ended multimodal\nunderstanding (MMU) and generation (MMG) tasks across diverse modalities (e.g.,\nimages, audio, video) poses significant challenges due to the complexity of\ncross-modal interactions. To this end, the idea of utilizing Multimodal LLMs\n(MLLMs) as automated judges has emerged, with encouraging results in assessing\nvision-language understanding tasks. Moving further, this paper extends\nMLLM-as-a-Judge across modalities to a unified manner by introducing two\nbenchmarks, TaskAnything and JudgeAnything, to respectively evaluate the\noverall performance and judging capabilities of MLLMs across any-to-any\nmodality tasks. Specifically, TaskAnything evaluates the MMU and MMG\ncapabilities across 15 any-to-any modality categories, employing 1,500 queries\ncurated from well-established benchmarks. Furthermore, JudgeAnything evaluates\nthe judging capabilities of 5 advanced (e.g., GPT-4o and Gemini-2.0-Flash) from\nthe perspectives of Pair Comparison and Score Evaluation, providing a\nstandardized testbed that incorporates human judgments and detailed rubrics.\nOur extensive experiments reveal that while these MLLMs show promise in\nassessing MMU (i.e., achieving an average of 66.55% in Pair Comparison setting\nand 42.79% in Score Evaluation setting), they encounter significant challenges\nwith MMG tasks (i.e., averaging only 53.37% in Pair Comparison setting and\n30.05% in Score Evaluation setting), exposing cross-modality biases and\nhallucination issues. To address this, we present OmniArena, an automated\nplatform for evaluating omni-models and multimodal reward models. Our work\nhighlights the need for fairer evaluation protocols and stronger alignment with\nhuman preferences. The source code and dataset are publicly available at:\nhttps://urrealhero.github.io/judgeanythingweb/.",
      "upvotes": 9,
      "discussionId": "67e21f350e6b6fcc3eb38c35",
      "ai_keywords": [
        "Multimodal LLMs (MLLMs)",
        "TaskAnything",
        "JudgeAnything",
        "open-ended multimodal understanding (MMU)",
        "open-ended multimodal generation (MMG)",
        "cross-modal interactions",
        "vision-language understanding tasks",
        "any-to-any modality tasks",
        "Pair Comparison",
        "Score Evaluation",
        "omni-models",
        "multimodal reward models",
        "cross-modality biases",
        "hallucination issues"
      ]
    },
    "publishedAt": "2025-03-21T14:59:20.000Z",
    "title": "Judge Anything: MLLM as a Judge Across Any Modality",
    "summary": "Evaluating generative foundation models on open-ended multimodal\nunderstanding (MMU) and generation (MMG) tasks across diverse modalities (e.g.,\nimages, audio, video) poses significant challenges due to the complexity of\ncross-modal interactions. To this end, the idea of utilizing Multimodal LLMs\n(MLLMs) as automated judges has emerged, with encouraging results in assessing\nvision-language understanding tasks. Moving further, this paper extends\nMLLM-as-a-Judge across modalities to a unified manner by introducing two\nbenchmarks, TaskAnything and JudgeAnything, to respectively evaluate the\noverall performance and judging capabilities of MLLMs across any-to-any\nmodality tasks. Specifically, TaskAnything evaluates the MMU and MMG\ncapabilities across 15 any-to-any modality categories, employing 1,500 queries\ncurated from well-established benchmarks. Furthermore, JudgeAnything evaluates\nthe judging capabilities of 5 advanced (e.g., GPT-4o and Gemini-2.0-Flash) from\nthe perspectives of Pair Comparison and Score Evaluation, providing a\nstandardized testbed that incorporates human judgments and detailed rubrics.\nOur extensive experiments reveal that while these MLLMs show promise in\nassessing MMU (i.e., achieving an average of 66.55% in Pair Comparison setting\nand 42.79% in Score Evaluation setting), they encounter significant challenges\nwith MMG tasks (i.e., averaging only 53.37% in Pair Comparison setting and\n30.05% in Score Evaluation setting), exposing cross-modality biases and\nhallucination issues. To address this, we present OmniArena, an automated\nplatform for evaluating omni-models and multimodal reward models. Our work\nhighlights the need for fairer evaluation protocols and stronger alignment with\nhuman preferences. The source code and dataset are publicly available at:\nhttps://urrealhero.github.io/judgeanythingweb/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17489.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643be8879f5d314db2d9ed23",
      "avatarUrl": "/avatars/64e9bb2c4e10fbe03e2b81afedf40865.svg",
      "fullname": "Chen Dongping",
      "name": "shuaishuaicdp",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.18948",
      "authors": [
        {
          "_id": "67e24217db11e1d382285cd4",
          "name": "Ruixiao Dong",
          "hidden": false
        },
        {
          "_id": "67e24217db11e1d382285cd5",
          "name": "Mengde Xu",
          "hidden": false
        },
        {
          "_id": "67e24217db11e1d382285cd6",
          "name": "Zigang Geng",
          "hidden": false
        },
        {
          "_id": "67e24217db11e1d382285cd7",
          "name": "Li Li",
          "hidden": false
        },
        {
          "_id": "67e24217db11e1d382285cd8",
          "name": "Han Hu",
          "hidden": false
        },
        {
          "_id": "67e24217db11e1d382285cd9",
          "name": "Shuyang Gu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T17:59:57.000Z",
      "submittedOnDailyAt": "2025-03-25T05:57:15.975Z",
      "title": "Equivariant Image Modeling",
      "submittedOnDailyBy": {
        "_id": "64c38fcf573c5a427e12cd37",
        "avatarUrl": "/avatars/2b9de06f29147ed2c212e920afba0eaf.svg",
        "isPro": false,
        "fullname": "cientgu",
        "user": "cientgu",
        "type": "user"
      },
      "summary": "Current generative models, such as autoregressive and diffusion approaches,\ndecompose high-dimensional data distribution learning into a series of simpler\nsubtasks. However, inherent conflicts arise during the joint optimization of\nthese subtasks, and existing solutions fail to resolve such conflicts without\nsacrificing efficiency or scalability. We propose a novel equivariant image\nmodeling framework that inherently aligns optimization targets across subtasks\nby leveraging the translation invariance of natural visual signals. Our method\nintroduces (1) column-wise tokenization which enhances translational symmetry\nalong the horizontal axis, and (2) windowed causal attention which enforces\nconsistent contextual relationships across positions. Evaluated on\nclass-conditioned ImageNet generation at 256x256 resolution, our approach\nachieves performance comparable to state-of-the-art AR models while using fewer\ncomputational resources. Systematic analysis demonstrates that enhanced\nequivariance reduces inter-task conflicts, significantly improving zero-shot\ngeneralization and enabling ultra-long image synthesis. This work establishes\nthe first framework for task-aligned decomposition in generative modeling,\noffering insights into efficient parameter sharing and conflict-free\noptimization. The code and models are publicly available at\nhttps://github.com/drx-code/EquivariantModeling.",
      "upvotes": 7,
      "discussionId": "67e2421edb11e1d382285f9b",
      "ai_keywords": [
        "autoregressive",
        "diffusion approaches",
        "high-dimensional data distribution learning",
        "subtasks",
        "joint optimization",
        "equivariant image modeling framework",
        "translation invariance",
        "column-wise tokenization",
        "translational symmetry",
        "windowed causal attention",
        "contextual relationships",
        "class-conditioned ImageNet generation",
        "state-of-the-art AR models",
        "computational resources",
        "enhanced equivariance",
        "zero-shot generalization",
        "ultra-long image synthesis",
        "task-aligned decomposition",
        "efficient parameter sharing",
        "conflict-free optimization"
      ]
    },
    "publishedAt": "2025-03-24T13:59:57.000Z",
    "title": "Equivariant Image Modeling",
    "summary": "Current generative models, such as autoregressive and diffusion approaches,\ndecompose high-dimensional data distribution learning into a series of simpler\nsubtasks. However, inherent conflicts arise during the joint optimization of\nthese subtasks, and existing solutions fail to resolve such conflicts without\nsacrificing efficiency or scalability. We propose a novel equivariant image\nmodeling framework that inherently aligns optimization targets across subtasks\nby leveraging the translation invariance of natural visual signals. Our method\nintroduces (1) column-wise tokenization which enhances translational symmetry\nalong the horizontal axis, and (2) windowed causal attention which enforces\nconsistent contextual relationships across positions. Evaluated on\nclass-conditioned ImageNet generation at 256x256 resolution, our approach\nachieves performance comparable to state-of-the-art AR models while using fewer\ncomputational resources. Systematic analysis demonstrates that enhanced\nequivariance reduces inter-task conflicts, significantly improving zero-shot\ngeneralization and enabling ultra-long image synthesis. This work establishes\nthe first framework for task-aligned decomposition in generative modeling,\noffering insights into efficient parameter sharing and conflict-free\noptimization. The code and models are publicly available at\nhttps://github.com/drx-code/EquivariantModeling.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18948.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c38fcf573c5a427e12cd37",
      "avatarUrl": "/avatars/2b9de06f29147ed2c212e920afba0eaf.svg",
      "fullname": "cientgu",
      "name": "cientgu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.18945",
      "authors": [
        {
          "_id": "67e22eca9455abdd1d257263",
          "name": "Aether Team",
          "hidden": false
        },
        {
          "_id": "67e22eca9455abdd1d257264",
          "name": "Haoyi Zhu",
          "hidden": false
        },
        {
          "_id": "67e22eca9455abdd1d257265",
          "name": "Yifan Wang",
          "hidden": false
        },
        {
          "_id": "67e22eca9455abdd1d257266",
          "name": "Jianjun Zhou",
          "hidden": false
        },
        {
          "_id": "67e22eca9455abdd1d257267",
          "name": "Wenzheng Chang",
          "hidden": false
        },
        {
          "_id": "67e22eca9455abdd1d257268",
          "name": "Yang Zhou",
          "hidden": false
        },
        {
          "_id": "67e22eca9455abdd1d257269",
          "name": "Zizun Li",
          "hidden": false
        },
        {
          "_id": "67e22eca9455abdd1d25726a",
          "name": "Junyi Chen",
          "hidden": false
        },
        {
          "_id": "67e22eca9455abdd1d25726b",
          "name": "Chunhua Shen",
          "hidden": false
        },
        {
          "_id": "67e22eca9455abdd1d25726c",
          "name": "Jiangmiao Pang",
          "hidden": false
        },
        {
          "_id": "67e22eca9455abdd1d25726d",
          "name": "Tong He",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T17:59:51.000Z",
      "submittedOnDailyAt": "2025-03-25T02:50:34.610Z",
      "title": "Aether: Geometric-Aware Unified World Modeling",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "The integration of geometric reconstruction and generative modeling remains a\ncritical challenge in developing AI systems capable of human-like spatial\nreasoning. This paper proposes Aether, a unified framework that enables\ngeometry-aware reasoning in world models by jointly optimizing three core\ncapabilities: (1) 4D dynamic reconstruction, (2) action-conditioned video\nprediction, and (3) goal-conditioned visual planning. Through task-interleaved\nfeature learning, Aether achieves synergistic knowledge sharing across\nreconstruction, prediction, and planning objectives. Building upon video\ngeneration models, our framework demonstrates unprecedented synthetic-to-real\ngeneralization despite never observing real-world data during training.\nFurthermore, our approach achieves zero-shot generalization in both action\nfollowing and reconstruction tasks, thanks to its intrinsic geometric modeling.\nRemarkably, even without real-world data, its reconstruction performance far\nexceeds that of domain-specific models. Additionally, Aether leverages a\ngeometry-informed action space to seamlessly translate predictions into\nactions, enabling effective autonomous trajectory planning. We hope our work\ninspires the community to explore new frontiers in physically-reasonable world\nmodeling and its applications.",
      "upvotes": 7,
      "discussionId": "67e22ecb9455abdd1d2572af",
      "projectPage": "https://aether-world.github.io/",
      "githubRepo": "https://github.com/OpenRobotLab/Aether",
      "ai_keywords": [
        "Aether",
        "4D dynamic reconstruction",
        "action-conditioned video prediction",
        "goal-conditioned visual planning",
        "task-interleaved feature learning",
        "video generation models",
        "synthetic-to-real generalization",
        "zero-shot generalization",
        "geometric modeling",
        "geometry-informed action space",
        "autonomous trajectory planning",
        "physically-reasonable world modeling"
      ]
    },
    "publishedAt": "2025-03-24T13:59:51.000Z",
    "title": "Aether: Geometric-Aware Unified World Modeling",
    "summary": "The integration of geometric reconstruction and generative modeling remains a\ncritical challenge in developing AI systems capable of human-like spatial\nreasoning. This paper proposes Aether, a unified framework that enables\ngeometry-aware reasoning in world models by jointly optimizing three core\ncapabilities: (1) 4D dynamic reconstruction, (2) action-conditioned video\nprediction, and (3) goal-conditioned visual planning. Through task-interleaved\nfeature learning, Aether achieves synergistic knowledge sharing across\nreconstruction, prediction, and planning objectives. Building upon video\ngeneration models, our framework demonstrates unprecedented synthetic-to-real\ngeneralization despite never observing real-world data during training.\nFurthermore, our approach achieves zero-shot generalization in both action\nfollowing and reconstruction tasks, thanks to its intrinsic geometric modeling.\nRemarkably, even without real-world data, its reconstruction performance far\nexceeds that of domain-specific models. Additionally, Aether leverages a\ngeometry-informed action space to seamlessly translate predictions into\nactions, enabling effective autonomous trajectory planning. We hope our work\ninspires the community to explore new frontiers in physically-reasonable world\nmodeling and its applications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18945.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6454
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.18923",
      "authors": [
        {
          "_id": "67e226f401cdb8cf3a1c7cd8",
          "name": "Meng Cao",
          "hidden": false
        },
        {
          "_id": "67e226f401cdb8cf3a1c7cd9",
          "name": "Pengfei Hu",
          "hidden": false
        },
        {
          "_id": "67e226f401cdb8cf3a1c7cda",
          "name": "Yingyao Wang",
          "hidden": false
        },
        {
          "_id": "67e226f401cdb8cf3a1c7cdb",
          "name": "Jihao Gu",
          "hidden": false
        },
        {
          "_id": "67e226f401cdb8cf3a1c7cdc",
          "name": "Haoran Tang",
          "hidden": false
        },
        {
          "_id": "67e226f401cdb8cf3a1c7cdd",
          "name": "Haoze Zhao",
          "hidden": false
        },
        {
          "_id": "67e226f401cdb8cf3a1c7cde",
          "name": "Jiahua Dong",
          "hidden": false
        },
        {
          "_id": "67e226f401cdb8cf3a1c7cdf",
          "name": "Wangbo Yu",
          "hidden": false
        },
        {
          "_id": "67e226f401cdb8cf3a1c7ce0",
          "name": "Ge Zhang",
          "hidden": false
        },
        {
          "_id": "67e226f401cdb8cf3a1c7ce1",
          "name": "Ian Reid",
          "hidden": false
        },
        {
          "_id": "67e226f401cdb8cf3a1c7ce2",
          "name": "Xiaodan Liang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T17:46:09.000Z",
      "submittedOnDailyAt": "2025-03-25T02:17:41.453Z",
      "title": "Video SimpleQA: Towards Factuality Evaluation in Large Video Language\n  Models",
      "submittedOnDailyBy": {
        "_id": "638efcf4c67af472d316d424",
        "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
        "isPro": false,
        "fullname": "Ge Zhang",
        "user": "zhangysk",
        "type": "user"
      },
      "summary": "Recent advancements in Large Video Language Models (LVLMs) have highlighted\ntheir potential for multi-modal understanding, yet evaluating their factual\ngrounding in video contexts remains a critical unsolved challenge. To address\nthis gap, we introduce Video SimpleQA, the first comprehensive benchmark\ntailored for factuality evaluation of LVLMs. Our work distinguishes from\nexisting video benchmarks through the following key features: 1) Knowledge\nrequired: demanding integration of external knowledge beyond the explicit\nnarrative; 2) Fact-seeking question: targeting objective, undisputed events or\nrelationships, avoiding subjective interpretation; 3) Definitive & short-form\nanswer: Answers are crafted as unambiguous and definitively correct in a short\nformat, enabling automated evaluation through LLM-as-a-judge frameworks with\nminimal scoring variance; 4) External-source verified: All annotations undergo\nrigorous validation against authoritative external references to ensure the\nreliability; 5) Temporal reasoning required: The annotated question types\nencompass both static single-frame understanding and dynamic temporal\nreasoning, explicitly evaluating LVLMs factuality under the long-context\ndependencies. We extensively evaluate 41 state-of-the-art LVLMs and summarize\nkey findings as follows: 1) Current LVLMs exhibit notable deficiencies in\nfactual adherence, particularly for open-source models. The best-performing\nmodel Gemini-1.5-Pro achieves merely an F-score of 54.4%; 2) Test-time compute\nparadigms show insignificant performance gains, revealing fundamental\nconstraints for enhancing factuality through post-hoc computation; 3)\nRetrieval-Augmented Generation demonstrates consistent improvements at the cost\nof additional inference time overhead, presenting a critical\nefficiency-performance trade-off.",
      "upvotes": 6,
      "discussionId": "67e226f601cdb8cf3a1c7d73",
      "projectPage": "https://videosimpleqa.github.io",
      "ai_keywords": [
        "Large Video Language Models (LVLMs)",
        "multi-modal understanding",
        "factuality evaluation",
        "Video SimpleQA",
        "external knowledge",
        "objective events",
        "relationships",
        "short-form answer",
        "LLM-as-a-judge",
        "automated evaluation",
        "scQUIre",
        "authoritative external references",
        "temporal reasoning",
        "long-context dependencies",
        "F-score",
        "test-time compute",
        "Retrieval-Augmented Generation",
        "inference time overhead",
        "efficiency-performance trade-off"
      ]
    },
    "publishedAt": "2025-03-24T13:46:09.000Z",
    "title": "Video SimpleQA: Towards Factuality Evaluation in Large Video Language\n  Models",
    "summary": "Recent advancements in Large Video Language Models (LVLMs) have highlighted\ntheir potential for multi-modal understanding, yet evaluating their factual\ngrounding in video contexts remains a critical unsolved challenge. To address\nthis gap, we introduce Video SimpleQA, the first comprehensive benchmark\ntailored for factuality evaluation of LVLMs. Our work distinguishes from\nexisting video benchmarks through the following key features: 1) Knowledge\nrequired: demanding integration of external knowledge beyond the explicit\nnarrative; 2) Fact-seeking question: targeting objective, undisputed events or\nrelationships, avoiding subjective interpretation; 3) Definitive & short-form\nanswer: Answers are crafted as unambiguous and definitively correct in a short\nformat, enabling automated evaluation through LLM-as-a-judge frameworks with\nminimal scoring variance; 4) External-source verified: All annotations undergo\nrigorous validation against authoritative external references to ensure the\nreliability; 5) Temporal reasoning required: The annotated question types\nencompass both static single-frame understanding and dynamic temporal\nreasoning, explicitly evaluating LVLMs factuality under the long-context\ndependencies. We extensively evaluate 41 state-of-the-art LVLMs and summarize\nkey findings as follows: 1) Current LVLMs exhibit notable deficiencies in\nfactual adherence, particularly for open-source models. The best-performing\nmodel Gemini-1.5-Pro achieves merely an F-score of 54.4%; 2) Test-time compute\nparadigms show insignificant performance gains, revealing fundamental\nconstraints for enhancing factuality through post-hoc computation; 3)\nRetrieval-Augmented Generation demonstrates consistent improvements at the cost\nof additional inference time overhead, presenting a critical\nefficiency-performance trade-off.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18923.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "638efcf4c67af472d316d424",
      "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
      "fullname": "Ge Zhang",
      "name": "zhangysk",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 43
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.17439",
      "authors": [
        {
          "_id": "67e21f63fb4213c53714be08",
          "name": "Zhuoshi Pan",
          "hidden": false
        },
        {
          "_id": "67e21f63fb4213c53714be09",
          "name": "Yu Li",
          "hidden": false
        },
        {
          "_id": "67e21f63fb4213c53714be0a",
          "name": "Honglin Lin",
          "hidden": false
        },
        {
          "_id": "67e21f63fb4213c53714be0b",
          "name": "Qizhi Pei",
          "hidden": false
        },
        {
          "_id": "67e21f63fb4213c53714be0c",
          "name": "Zinan Tang",
          "hidden": false
        },
        {
          "_id": "67e21f63fb4213c53714be0d",
          "name": "Wei Wu",
          "hidden": false
        },
        {
          "_id": "67e21f63fb4213c53714be0e",
          "name": "Chenlin Ming",
          "hidden": false
        },
        {
          "_id": "67e21f63fb4213c53714be0f",
          "name": "H. Vicky Zhao",
          "hidden": false
        },
        {
          "_id": "67e21f63fb4213c53714be10",
          "name": "Conghui He",
          "hidden": false
        },
        {
          "_id": "67e21f63fb4213c53714be11",
          "name": "Lijun Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-21T17:59:10.000Z",
      "submittedOnDailyAt": "2025-03-25T01:47:52.213Z",
      "title": "LEMMA: Learning from Errors for MatheMatical Advancement in LLMs",
      "submittedOnDailyBy": {
        "_id": "6397f6081323f19c578f142e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6397f6081323f19c578f142e/it7FYYKjlLX8wSsMLm8EO.jpeg",
        "isPro": false,
        "fullname": "QizhiPei",
        "user": "QizhiPei",
        "type": "user"
      },
      "summary": "Large language models (LLMs) have demonstrated remarkable reasoning\ncapability in solving mathematical problems. However, existing approaches\nprimarily focus on improving the quality of correct training data, e.g.,\ndistilling high-quality correct solutions from advanced models, neglecting the\nvalue contained in error data, potentially hindering the model's reflective\nability. Though some studies attempt to leverage error data, they often involve\ncomplex mechanisms, such as Monte Carlo Tree Search (MCTS) to explore error\nnodes. In this work, we propose to enhance LLMs' reasoning ability by Learning\nfrom Errors for Mathematical Advancement (LEMMA). LEMMA constructs data\nconsisting of an incorrect solution with an erroneous step and a reflection\nconnection to a correct solution for fine-tuning. Specifically, we\nsystematically analyze the model-generated error types and introduce an\nerror-type grounded mistake augmentation method to collect diverse and\nrepresentative errors. Correct solutions are either from fixing the errors or\ngenerating a fresh start. Through a model-aware smooth reflection connection,\nthe erroneous solution is transferred to the correct one. By fine-tuning on the\nconstructed dataset, the model is able to self-correct errors autonomously\nwithin the generation process without relying on external critique models.\nExperimental results demonstrate that LEMMA achieves significant performance\nimprovements over other strong baselines.",
      "upvotes": 6,
      "discussionId": "67e21f64fb4213c53714be6b",
      "githubRepo": "https://github.com/pzs19/LEMMA",
      "ai_keywords": [
        "Learning from Errors for Mathematical Advancement (LEMMA)",
        "mistake augmentation",
        "model-aware smooth reflection connection",
        "autonomous error correction"
      ]
    },
    "publishedAt": "2025-03-21T13:59:10.000Z",
    "title": "LEMMA: Learning from Errors for MatheMatical Advancement in LLMs",
    "summary": "Large language models (LLMs) have demonstrated remarkable reasoning\ncapability in solving mathematical problems. However, existing approaches\nprimarily focus on improving the quality of correct training data, e.g.,\ndistilling high-quality correct solutions from advanced models, neglecting the\nvalue contained in error data, potentially hindering the model's reflective\nability. Though some studies attempt to leverage error data, they often involve\ncomplex mechanisms, such as Monte Carlo Tree Search (MCTS) to explore error\nnodes. In this work, we propose to enhance LLMs' reasoning ability by Learning\nfrom Errors for Mathematical Advancement (LEMMA). LEMMA constructs data\nconsisting of an incorrect solution with an erroneous step and a reflection\nconnection to a correct solution for fine-tuning. Specifically, we\nsystematically analyze the model-generated error types and introduce an\nerror-type grounded mistake augmentation method to collect diverse and\nrepresentative errors. Correct solutions are either from fixing the errors or\ngenerating a fresh start. Through a model-aware smooth reflection connection,\nthe erroneous solution is transferred to the correct one. By fine-tuning on the\nconstructed dataset, the model is able to self-correct errors autonomously\nwithin the generation process without relying on external critique models.\nExperimental results demonstrate that LEMMA achieves significant performance\nimprovements over other strong baselines.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17439.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6397f6081323f19c578f142e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6397f6081323f19c578f142e/it7FYYKjlLX8wSsMLm8EO.jpeg",
      "fullname": "QizhiPei",
      "name": "QizhiPei",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 17
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.15879",
      "authors": [
        {
          "_id": "67dea7cc5b44ace7a30e237e",
          "user": {
            "_id": "6540fbf9cb7fffd683942b43",
            "avatarUrl": "/avatars/d4a64fbde511d0949e1c339179586850.svg",
            "isPro": false,
            "fullname": "DongGeon Lee",
            "user": "oneonlee",
            "type": "user"
          },
          "name": "DongGeon Lee",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-24T08:52:18.850Z",
          "hidden": false
        },
        {
          "_id": "67dea7cc5b44ace7a30e237f",
          "name": "Ahjeong Park",
          "hidden": false
        },
        {
          "_id": "67dea7cc5b44ace7a30e2380",
          "user": {
            "_id": "666a8be869a08ea4aac5e73e",
            "avatarUrl": "/avatars/be42632414bafb0af74b5f4d4f03d223.svg",
            "isPro": false,
            "fullname": "keira lee",
            "user": "keirahrlee",
            "type": "user"
          },
          "name": "Hyeri Lee",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-22T12:06:36.938Z",
          "hidden": false
        },
        {
          "_id": "67dea7cc5b44ace7a30e2381",
          "name": "Hyeonseo Nam",
          "hidden": false
        },
        {
          "_id": "67dea7cc5b44ace7a30e2382",
          "name": "Yunho Maeng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-20T06:04:12.000Z",
      "submittedOnDailyAt": "2025-03-25T03:48:51.972Z",
      "title": "Typed-RAG: Type-aware Multi-Aspect Decomposition for Non-Factoid\n  Question Answering",
      "submittedOnDailyBy": {
        "_id": "6540fbf9cb7fffd683942b43",
        "avatarUrl": "/avatars/d4a64fbde511d0949e1c339179586850.svg",
        "isPro": false,
        "fullname": "DongGeon Lee",
        "user": "oneonlee",
        "type": "user"
      },
      "summary": "Non-factoid question-answering (NFQA) poses a significant challenge due to\nits open-ended nature, diverse intents, and the need for multi-aspect\nreasoning, which renders conventional factoid QA approaches, including\nretrieval-augmented generation (RAG), inadequate. Unlike factoid questions,\nnon-factoid questions (NFQs) lack definitive answers and require synthesizing\ninformation from multiple sources across various reasoning dimensions. To\naddress these limitations, we introduce Typed-RAG, a type-aware multi-aspect\ndecomposition framework within the RAG paradigm for NFQA. Typed-RAG classifies\nNFQs into distinct types -- such as debate, experience, and comparison -- and\napplies aspect-based decomposition to refine retrieval and generation\nstrategies. By decomposing multi-aspect NFQs into single-aspect sub-queries and\naggregating the results, Typed-RAG generates more informative and contextually\nrelevant responses. To evaluate Typed-RAG, we introduce Wiki-NFQA, a benchmark\ndataset covering diverse NFQ types. Experimental results demonstrate that\nTyped-RAG outperforms baselines, thereby highlighting the importance of\ntype-aware decomposition for effective retrieval and generation in NFQA. Our\ncode and dataset are available at\nhttps://github.com/TeamNLP/Typed-RAG{https://github.com/TeamNLP/Typed-RAG}.",
      "upvotes": 5,
      "discussionId": "67dea7cc5b44ace7a30e23b8",
      "githubRepo": "https://github.com/TeamNLP/Typed-RAG",
      "ai_keywords": [
        "retrieval-augmented generation (RAG)",
        "non-factoid question-answering (NFQA)",
        "multi-aspect reasoning",
        "type-aware multi-aspect decomposition framework",
        "single-aspect sub-queries",
        "Wiki-NFQA",
        "type-aware decomposition"
      ]
    },
    "publishedAt": "2025-03-20T02:04:12.000Z",
    "title": "Typed-RAG: Type-aware Multi-Aspect Decomposition for Non-Factoid\n  Question Answering",
    "summary": "Non-factoid question-answering (NFQA) poses a significant challenge due to\nits open-ended nature, diverse intents, and the need for multi-aspect\nreasoning, which renders conventional factoid QA approaches, including\nretrieval-augmented generation (RAG), inadequate. Unlike factoid questions,\nnon-factoid questions (NFQs) lack definitive answers and require synthesizing\ninformation from multiple sources across various reasoning dimensions. To\naddress these limitations, we introduce Typed-RAG, a type-aware multi-aspect\ndecomposition framework within the RAG paradigm for NFQA. Typed-RAG classifies\nNFQs into distinct types -- such as debate, experience, and comparison -- and\napplies aspect-based decomposition to refine retrieval and generation\nstrategies. By decomposing multi-aspect NFQs into single-aspect sub-queries and\naggregating the results, Typed-RAG generates more informative and contextually\nrelevant responses. To evaluate Typed-RAG, we introduce Wiki-NFQA, a benchmark\ndataset covering diverse NFQ types. Experimental results demonstrate that\nTyped-RAG outperforms baselines, thereby highlighting the importance of\ntype-aware decomposition for effective retrieval and generation in NFQA. Our\ncode and dataset are available at\nhttps://github.com/TeamNLP/Typed-RAG{https://github.com/TeamNLP/Typed-RAG}.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15879.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6540fbf9cb7fffd683942b43",
      "avatarUrl": "/avatars/d4a64fbde511d0949e1c339179586850.svg",
      "fullname": "DongGeon Lee",
      "name": "oneonlee",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.18866",
      "authors": [
        {
          "_id": "67e2290da4525cbb1d718ae2",
          "name": "Yangjun Ruan",
          "hidden": false
        },
        {
          "_id": "67e2290da4525cbb1d718ae3",
          "name": "Neil Band",
          "hidden": false
        },
        {
          "_id": "67e2290da4525cbb1d718ae4",
          "name": "Chris J. Maddison",
          "hidden": false
        },
        {
          "_id": "67e2290da4525cbb1d718ae5",
          "name": "Tatsunori Hashimoto",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T16:41:23.000Z",
      "submittedOnDailyAt": "2025-03-25T02:25:31.127Z",
      "title": "Reasoning to Learn from Latent Thoughts",
      "submittedOnDailyBy": {
        "_id": "65619949d2e4352d64365606",
        "avatarUrl": "/avatars/4241b6e901e146a9ff8b5bc7de3f960a.svg",
        "isPro": true,
        "fullname": "Yangjun Ruan",
        "user": "ryoungj",
        "type": "user"
      },
      "summary": "Compute scaling for language model (LM) pretraining has outpaced the growth\nof human-written texts, leading to concerns that data will become the\nbottleneck to LM scaling. To continue scaling pretraining in this\ndata-constrained regime, we propose that explicitly modeling and inferring the\nlatent thoughts that underlie the text generation process can significantly\nimprove pretraining data efficiency. Intuitively, our approach views web text\nas the compressed final outcome of a verbose human thought process and that the\nlatent thoughts contain important contextual knowledge and reasoning steps that\nare critical to data-efficient learning. We empirically demonstrate the\neffectiveness of our approach through data-constrained continued pretraining\nfor math. We first show that synthetic data approaches to inferring latent\nthoughts significantly improve data efficiency, outperforming training on the\nsame amount of raw data (5.7\\% rightarrow 25.4\\% on MATH). Furthermore, we\ndemonstrate latent thought inference without a strong teacher, where an LM\nbootstraps its own performance by using an EM algorithm to iteratively improve\nthe capability of the trained LM and the quality of thought-augmented\npretraining data. We show that a 1B LM can bootstrap its performance across at\nleast three iterations and significantly outperform baselines trained on raw\ndata, with increasing gains from additional inference compute when performing\nthe E-step. The gains from inference scaling and EM iterations suggest new\nopportunities for scaling data-constrained pretraining.",
      "upvotes": 4,
      "discussionId": "67e2290ea4525cbb1d718b18",
      "ai_keywords": [
        "latent thoughts",
        "data-efficient learning",
        "web text",
        "verbose human thought process",
        "synthetic data",
        "data-constrained regime",
        "EM algorithm",
        "thought-augmented pretraining data",
        "inference compute",
        "data-constrained pretraining"
      ]
    },
    "publishedAt": "2025-03-24T12:41:23.000Z",
    "title": "Reasoning to Learn from Latent Thoughts",
    "summary": "Compute scaling for language model (LM) pretraining has outpaced the growth\nof human-written texts, leading to concerns that data will become the\nbottleneck to LM scaling. To continue scaling pretraining in this\ndata-constrained regime, we propose that explicitly modeling and inferring the\nlatent thoughts that underlie the text generation process can significantly\nimprove pretraining data efficiency. Intuitively, our approach views web text\nas the compressed final outcome of a verbose human thought process and that the\nlatent thoughts contain important contextual knowledge and reasoning steps that\nare critical to data-efficient learning. We empirically demonstrate the\neffectiveness of our approach through data-constrained continued pretraining\nfor math. We first show that synthetic data approaches to inferring latent\nthoughts significantly improve data efficiency, outperforming training on the\nsame amount of raw data (5.7\\% rightarrow 25.4\\% on MATH). Furthermore, we\ndemonstrate latent thought inference without a strong teacher, where an LM\nbootstraps its own performance by using an EM algorithm to iteratively improve\nthe capability of the trained LM and the quality of thought-augmented\npretraining data. We show that a 1B LM can bootstrap its performance across at\nleast three iterations and significantly outperform baselines trained on raw\ndata, with increasing gains from additional inference compute when performing\nthe E-step. The gains from inference scaling and EM iterations suggest new\nopportunities for scaling data-constrained pretraining.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18866.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65619949d2e4352d64365606",
      "avatarUrl": "/avatars/4241b6e901e146a9ff8b5bc7de3f960a.svg",
      "fullname": "Yangjun Ruan",
      "name": "ryoungj",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.18102",
      "authors": [
        {
          "_id": "67e22206ddc9b120cbde6fbe",
          "name": "Samuel Schmidgall",
          "hidden": false
        },
        {
          "_id": "67e22206ddc9b120cbde6fbf",
          "name": "Michael Moor",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-23T15:16:42.000Z",
      "submittedOnDailyAt": "2025-03-25T05:09:36.523Z",
      "title": "AgentRxiv: Towards Collaborative Autonomous Research",
      "submittedOnDailyBy": {
        "_id": "6438d1d843d932c462404500",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6438d1d843d932c462404500/6UrtJbed9N5ETbImgIh-C.png",
        "isPro": false,
        "fullname": "Michael Moor",
        "user": "mdmoor",
        "type": "user"
      },
      "summary": "Progress in scientific discovery is rarely the result of a single \"Eureka\"\nmoment, but is rather the product of hundreds of scientists incrementally\nworking together toward a common goal. While existing agent workflows are\ncapable of producing research autonomously, they do so in isolation, without\nthe ability to continuously improve upon prior research results. To address\nthese challenges, we introduce AgentRxiv-a framework that lets LLM agent\nlaboratories upload and retrieve reports from a shared preprint server in order\nto collaborate, share insights, and iteratively build on each other's research.\nWe task agent laboratories to develop new reasoning and prompting techniques\nand find that agents with access to their prior research achieve higher\nperformance improvements compared to agents operating in isolation (11.4%\nrelative improvement over baseline on MATH-500). We find that the best\nperforming strategy generalizes to benchmarks in other domains (improving on\naverage by 3.3%). Multiple agent laboratories sharing research through\nAgentRxiv are able to work together towards a common goal, progressing more\nrapidly than isolated laboratories, achieving higher overall accuracy (13.7%\nrelative improvement over baseline on MATH-500). These findings suggest that\nautonomous agents may play a role in designing future AI systems alongside\nhumans. We hope that AgentRxiv allows agents to collaborate toward research\ngoals and enables researchers to accelerate discovery.",
      "upvotes": 4,
      "discussionId": "67e22207ddc9b120cbde702c",
      "ai_keywords": [
        "LLM (Large Language Model)",
        "agent laboratories",
        "preprint server",
        "reasoning techniques",
        "prompting techniques",
        "performance improvements",
        "benchmarks",
        "accuracy"
      ]
    },
    "publishedAt": "2025-03-23T11:16:42.000Z",
    "title": "AgentRxiv: Towards Collaborative Autonomous Research",
    "summary": "Progress in scientific discovery is rarely the result of a single \"Eureka\"\nmoment, but is rather the product of hundreds of scientists incrementally\nworking together toward a common goal. While existing agent workflows are\ncapable of producing research autonomously, they do so in isolation, without\nthe ability to continuously improve upon prior research results. To address\nthese challenges, we introduce AgentRxiv-a framework that lets LLM agent\nlaboratories upload and retrieve reports from a shared preprint server in order\nto collaborate, share insights, and iteratively build on each other's research.\nWe task agent laboratories to develop new reasoning and prompting techniques\nand find that agents with access to their prior research achieve higher\nperformance improvements compared to agents operating in isolation (11.4%\nrelative improvement over baseline on MATH-500). We find that the best\nperforming strategy generalizes to benchmarks in other domains (improving on\naverage by 3.3%). Multiple agent laboratories sharing research through\nAgentRxiv are able to work together towards a common goal, progressing more\nrapidly than isolated laboratories, achieving higher overall accuracy (13.7%\nrelative improvement over baseline on MATH-500). These findings suggest that\nautonomous agents may play a role in designing future AI systems alongside\nhumans. We hope that AgentRxiv allows agents to collaborate toward research\ngoals and enables researchers to accelerate discovery.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18102.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6438d1d843d932c462404500",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6438d1d843d932c462404500/6UrtJbed9N5ETbImgIh-C.png",
      "fullname": "Michael Moor",
      "name": "mdmoor",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.14428",
      "authors": [
        {
          "_id": "67e217941cb9bded659267f0",
          "name": "Hongyu Zhang",
          "hidden": false
        },
        {
          "_id": "67e217941cb9bded659267f1",
          "name": "Yufan Deng",
          "hidden": false
        },
        {
          "_id": "67e217941cb9bded659267f2",
          "name": "Shenghai Yuan",
          "hidden": false
        },
        {
          "_id": "67e217941cb9bded659267f3",
          "name": "Peng Jin",
          "hidden": false
        },
        {
          "_id": "67e217941cb9bded659267f4",
          "name": "Zesen Cheng",
          "hidden": false
        },
        {
          "_id": "67e217941cb9bded659267f5",
          "name": "Yian Zhao",
          "hidden": false
        },
        {
          "_id": "67e217941cb9bded659267f6",
          "name": "Chang Liu",
          "hidden": false
        },
        {
          "_id": "67e217941cb9bded659267f7",
          "name": "Jie Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-18T17:02:14.000Z",
      "submittedOnDailyAt": "2025-03-25T01:10:50.245Z",
      "title": "MagicComp: Training-free Dual-Phase Refinement for Compositional Video\n  Generation",
      "submittedOnDailyBy": {
        "_id": "63468720dd6d90d82ccf3450",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
        "isPro": false,
        "fullname": "YSH",
        "user": "BestWishYsh",
        "type": "user"
      },
      "summary": "Text-to-video (T2V) generation has made significant strides with diffusion\nmodels. However, existing methods still struggle with accurately binding\nattributes, determining spatial relationships, and capturing complex action\ninteractions between multiple subjects. To address these limitations, we\npropose MagicComp, a training-free method that enhances compositional T2V\ngeneration through dual-phase refinement. Specifically, (1) During the\nConditioning Stage: We introduce the Semantic Anchor Disambiguation to\nreinforces subject-specific semantics and resolve inter-subject ambiguity by\nprogressively injecting the directional vectors of semantic anchors into\noriginal text embedding; (2) During the Denoising Stage: We propose Dynamic\nLayout Fusion Attention, which integrates grounding priors and model-adaptive\nspatial perception to flexibly bind subjects to their spatiotemporal regions\nthrough masked attention modulation. Furthermore, MagicComp is a model-agnostic\nand versatile approach, which can be seamlessly integrated into existing T2V\narchitectures. Extensive experiments on T2V-CompBench and VBench demonstrate\nthat MagicComp outperforms state-of-the-art methods, highlighting its potential\nfor applications such as complex prompt-based and trajectory-controllable video\ngeneration. Project page: https://hong-yu-zhang.github.io/MagicComp-Page/.",
      "upvotes": 4,
      "discussionId": "67e217981cb9bded65926978",
      "projectPage": "https://hong-yu-zhang.github.io/MagicComp-Page/",
      "githubRepo": "https://github.com/Hong-yu-Zhang/MagicComp",
      "ai_keywords": [
        "Semantic Anchor Disambiguation",
        "Dynamic Layout Fusion Attention",
        "grounding priors",
        "model-adaptive spatial perception",
        "masked attention modulation"
      ]
    },
    "publishedAt": "2025-03-18T13:02:14.000Z",
    "title": "MagicComp: Training-free Dual-Phase Refinement for Compositional Video\n  Generation",
    "summary": "Text-to-video (T2V) generation has made significant strides with diffusion\nmodels. However, existing methods still struggle with accurately binding\nattributes, determining spatial relationships, and capturing complex action\ninteractions between multiple subjects. To address these limitations, we\npropose MagicComp, a training-free method that enhances compositional T2V\ngeneration through dual-phase refinement. Specifically, (1) During the\nConditioning Stage: We introduce the Semantic Anchor Disambiguation to\nreinforces subject-specific semantics and resolve inter-subject ambiguity by\nprogressively injecting the directional vectors of semantic anchors into\noriginal text embedding; (2) During the Denoising Stage: We propose Dynamic\nLayout Fusion Attention, which integrates grounding priors and model-adaptive\nspatial perception to flexibly bind subjects to their spatiotemporal regions\nthrough masked attention modulation. Furthermore, MagicComp is a model-agnostic\nand versatile approach, which can be seamlessly integrated into existing T2V\narchitectures. Extensive experiments on T2V-CompBench and VBench demonstrate\nthat MagicComp outperforms state-of-the-art methods, highlighting its potential\nfor applications such as complex prompt-based and trajectory-controllable video\ngeneration. Project page: https://hong-yu-zhang.github.io/MagicComp-Page/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14428.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63468720dd6d90d82ccf3450",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
      "fullname": "YSH",
      "name": "BestWishYsh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 35
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.18908",
      "authors": [
        {
          "_id": "67e230fd4b9f234b60d06389",
          "name": "Akhiad Bercovich",
          "hidden": false
        },
        {
          "_id": "67e230fd4b9f234b60d0638a",
          "name": "Mohammad Dabbah",
          "hidden": false
        },
        {
          "_id": "67e230fd4b9f234b60d0638b",
          "name": "Omri Puny",
          "hidden": false
        },
        {
          "_id": "67e230fd4b9f234b60d0638c",
          "name": "Ido Galil",
          "hidden": false
        },
        {
          "_id": "67e230fd4b9f234b60d0638d",
          "name": "Amnon Geifman",
          "hidden": false
        },
        {
          "_id": "67e230fd4b9f234b60d0638e",
          "name": "Yonatan Geifman",
          "hidden": false
        },
        {
          "_id": "67e230fd4b9f234b60d0638f",
          "name": "Izhak Golan",
          "hidden": false
        },
        {
          "_id": "67e230fd4b9f234b60d06390",
          "name": "Ehud Karpas",
          "hidden": false
        },
        {
          "_id": "67e230fd4b9f234b60d06391",
          "name": "Itay Levy",
          "hidden": false
        },
        {
          "_id": "67e230fd4b9f234b60d06392",
          "name": "Zach Moshe",
          "hidden": false
        },
        {
          "_id": "67e230fd4b9f234b60d06393",
          "name": "Najeeb Nabwani",
          "hidden": false
        },
        {
          "_id": "67e230fd4b9f234b60d06394",
          "name": "Tomer Ronen",
          "hidden": false
        },
        {
          "_id": "67e230fd4b9f234b60d06395",
          "name": "Itamar Schen",
          "hidden": false
        },
        {
          "_id": "67e230fd4b9f234b60d06396",
          "name": "Elad Segal",
          "hidden": false
        },
        {
          "_id": "67e230fd4b9f234b60d06397",
          "name": "Ido Shahaf",
          "hidden": false
        },
        {
          "_id": "67e230fd4b9f234b60d06398",
          "name": "Oren Tropp",
          "hidden": false
        },
        {
          "_id": "67e230fd4b9f234b60d06399",
          "name": "Ran Zilberstein",
          "hidden": false
        },
        {
          "_id": "67e230fd4b9f234b60d0639a",
          "name": "Ran El-Yaniv",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T17:20:35.000Z",
      "submittedOnDailyAt": "2025-03-25T02:59:12.174Z",
      "title": "FFN Fusion: Rethinking Sequential Computation in Large Language Models",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "We introduce FFN Fusion, an architectural optimization technique that reduces\nsequential computation in large language models by identifying and exploiting\nnatural opportunities for parallelization. Our key insight is that sequences of\nFeed-Forward Network (FFN) layers, particularly those remaining after the\nremoval of specific attention layers, can often be parallelized with minimal\naccuracy impact. We develop a principled methodology for identifying and fusing\nsuch sequences, transforming them into parallel operations that significantly\nreduce inference latency while preserving model behavior. Applying these\ntechniques to Llama-3.1-405B-Instruct, we create Llama-Nemotron-Ultra-253B-Base\n(Ultra-253B-Base), an efficient and soon-to-be publicly available model that\nachieves a 1.71X speedup in inference latency and 35X lower per-token cost\nwhile maintaining strong performance across benchmarks. Through extensive\nexperiments on models from 49B to 253B parameters, we demonstrate that FFN\nFusion becomes increasingly effective at larger scales and can complement\nexisting optimization techniques like quantization and pruning. Most\nintriguingly, we find that even full transformer blocks containing both\nattention and FFN layers can sometimes be parallelized, suggesting new\ndirections for neural architecture design.",
      "upvotes": 3,
      "discussionId": "67e230fe4b9f234b60d063ec",
      "ai_keywords": [
        "FFN Fusion",
        "Feed-Forward Network (FFN)",
        "parallelization",
        "inference latency",
        "Llama-3.1-405B-Instruct",
        "Ultra-253B-Base",
        "model behavior",
        "per-token cost",
        "benchmarks",
        "transformer blocks",
        "quantization",
        "pruning"
      ]
    },
    "publishedAt": "2025-03-24T13:20:35.000Z",
    "title": "FFN Fusion: Rethinking Sequential Computation in Large Language Models",
    "summary": "We introduce FFN Fusion, an architectural optimization technique that reduces\nsequential computation in large language models by identifying and exploiting\nnatural opportunities for parallelization. Our key insight is that sequences of\nFeed-Forward Network (FFN) layers, particularly those remaining after the\nremoval of specific attention layers, can often be parallelized with minimal\naccuracy impact. We develop a principled methodology for identifying and fusing\nsuch sequences, transforming them into parallel operations that significantly\nreduce inference latency while preserving model behavior. Applying these\ntechniques to Llama-3.1-405B-Instruct, we create Llama-Nemotron-Ultra-253B-Base\n(Ultra-253B-Base), an efficient and soon-to-be publicly available model that\nachieves a 1.71X speedup in inference latency and 35X lower per-token cost\nwhile maintaining strong performance across benchmarks. Through extensive\nexperiments on models from 49B to 253B parameters, we demonstrate that FFN\nFusion becomes increasingly effective at larger scales and can complement\nexisting optimization techniques like quantization and pruning. Most\nintriguingly, we find that even full transformer blocks containing both\nattention and FFN layers can sometimes be parallelized, suggesting new\ndirections for neural architecture design.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18908.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6454
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.18013",
      "authors": [
        {
          "_id": "67e22902af6628c90b525a2b",
          "name": "Yufei Zhan",
          "hidden": false
        },
        {
          "_id": "67e22902af6628c90b525a2c",
          "name": "Yousong Zhu",
          "hidden": false
        },
        {
          "_id": "67e22902af6628c90b525a2d",
          "name": "Shurong Zheng",
          "hidden": false
        },
        {
          "_id": "67e22902af6628c90b525a2e",
          "name": "Hongyin Zhao",
          "hidden": false
        },
        {
          "_id": "67e22902af6628c90b525a2f",
          "name": "Fan Yang",
          "hidden": false
        },
        {
          "_id": "67e22902af6628c90b525a30",
          "name": "Ming Tang",
          "hidden": false
        },
        {
          "_id": "67e22902af6628c90b525a31",
          "name": "Jinqiao Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-23T10:21:14.000Z",
      "submittedOnDailyAt": "2025-03-25T02:25:32.811Z",
      "title": "Vision-R1: Evolving Human-Free Alignment in Large Vision-Language Models\n  via Vision-Guided Reinforcement Learning",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Large Vision-Language Models (LVLMs) typically follow a two-stage training\nparadigm-pretraining and supervised fine-tuning. Recently, preference\noptimization, derived from the language domain, has emerged as an effective\npost-training reinforcement strategy to enhance capabilities of LVLMs. However,\nconstructing high-quality human-annotated preference data and developing robust\nreward models to mimic these preferences are both costly and challenging.\nMotivated by this observation, we propose Vision-R1, a novel vision-guided\nR1-like reinforcement learning algorithm for LVLMs that rewards models with\ndefinitive vision feedback. It only leverages curated instruction data,\neliminating the need for specialized reward models and handcrafted preference\ndatasets. We incorporate a criterion-driven reward function that further\nintegrates multi-dimensional feedback to evaluate model completions\ncomprehensively based on the vision task logic. Furthermore, we introduce a\nprogressive rule refinement strategy that dynamically adjusts the reward\ncriteria during training, enabling continuous model improvement and mitigating\nreward hacking. Extensive experiments on both in-distribution and\nout-of-distribution benchmarks demonstrate that fine-tuning the 7B LVLMs with\nVision-R1 achieves consistent performance gains, with even up to 50%\nimprovement and surpassing the state-of-the-art 10x size model.",
      "upvotes": 3,
      "discussionId": "67e22903af6628c90b525a71",
      "ai_keywords": [
        "Large Vision-Language Models (LVLMs)",
        "pretraining",
        "supervised fine-tuning",
        "preference optimization",
        "reinforcement learning",
        "Vision-R1",
        "criterion-driven reward function",
        "progressive rule refinement strategy",
        "reward criteria",
        "model completions",
        "vision task logic",
        "reward hacking",
        "state-of-the-art"
      ]
    },
    "publishedAt": "2025-03-23T06:21:14.000Z",
    "title": "Vision-R1: Evolving Human-Free Alignment in Large Vision-Language Models\n  via Vision-Guided Reinforcement Learning",
    "summary": "Large Vision-Language Models (LVLMs) typically follow a two-stage training\nparadigm-pretraining and supervised fine-tuning. Recently, preference\noptimization, derived from the language domain, has emerged as an effective\npost-training reinforcement strategy to enhance capabilities of LVLMs. However,\nconstructing high-quality human-annotated preference data and developing robust\nreward models to mimic these preferences are both costly and challenging.\nMotivated by this observation, we propose Vision-R1, a novel vision-guided\nR1-like reinforcement learning algorithm for LVLMs that rewards models with\ndefinitive vision feedback. It only leverages curated instruction data,\neliminating the need for specialized reward models and handcrafted preference\ndatasets. We incorporate a criterion-driven reward function that further\nintegrates multi-dimensional feedback to evaluate model completions\ncomprehensively based on the vision task logic. Furthermore, we introduce a\nprogressive rule refinement strategy that dynamically adjusts the reward\ncriteria during training, enabling continuous model improvement and mitigating\nreward hacking. Extensive experiments on both in-distribution and\nout-of-distribution benchmarks demonstrate that fine-tuning the 7B LVLMs with\nVision-R1 achieves consistent performance gains, with even up to 50%\nimprovement and surpassing the state-of-the-art 10x size model.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18013.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6454
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.18769",
      "authors": [
        {
          "_id": "67e2177c77d32fd1ed8a496b",
          "user": {
            "_id": "62d7b2339b629105a5d6888a",
            "avatarUrl": "/avatars/c3f164fde6b8f9a671890e08ce8a3e75.svg",
            "isPro": false,
            "fullname": "Alan Dao",
            "user": "alandao",
            "type": "user"
          },
          "name": "Alan Dao",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-25T02:41:44.856Z",
          "hidden": false
        },
        {
          "_id": "67e2177c77d32fd1ed8a496c",
          "name": "Dinh Bach Vu",
          "hidden": false
        },
        {
          "_id": "67e2177c77d32fd1ed8a496d",
          "name": "Bui Quang Huy",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62d7b2339b629105a5d6888a/505sM0N1UIUHUOunaHtAG.mp4"
      ],
      "publishedAt": "2025-03-24T15:16:51.000Z",
      "submittedOnDailyAt": "2025-03-25T01:11:03.544Z",
      "title": "AlphaSpace: Enabling Robotic Actions through Semantic Tokenization and\n  Symbolic Reasoning",
      "submittedOnDailyBy": {
        "_id": "62d7b2339b629105a5d6888a",
        "avatarUrl": "/avatars/c3f164fde6b8f9a671890e08ce8a3e75.svg",
        "isPro": false,
        "fullname": "Alan Dao",
        "user": "alandao",
        "type": "user"
      },
      "summary": "This paper presents AlphaSpace, a novel methodology designed to enhance the\nspatial reasoning capabilities of large language models (LLMs) for 3D Cartesian\nspace navigation. AlphaSpace employs a semantics-based tokenization strategy,\nencoding height information through specialized semantic tokens, and integrates\nprimarily symbolic synthetic reasoning data. This approach enables LLMs to\naccurately manipulate objects by positioning them at specific [x, y, z]\ncoordinates. Experimental results demonstrate that AlphaSpace significantly\noutperforms existing models on manipulation subtasks, achieving a total\naccuracy of 66.67%, compared to 37.5% for GPT-4o and 29.17% for Claude 3.5\nSonnet.",
      "upvotes": 2,
      "discussionId": "67e2177d77d32fd1ed8a49ac",
      "ai_keywords": [
        "semantics-based tokenization",
        "semantic tokens",
        "Cartesian space",
        "3D Cartesian space",
        "positioning",
        "manipulation subtasks"
      ]
    },
    "publishedAt": "2025-03-24T11:16:51.000Z",
    "title": "AlphaSpace: Enabling Robotic Actions through Semantic Tokenization and\n  Symbolic Reasoning",
    "summary": "This paper presents AlphaSpace, a novel methodology designed to enhance the\nspatial reasoning capabilities of large language models (LLMs) for 3D Cartesian\nspace navigation. AlphaSpace employs a semantics-based tokenization strategy,\nencoding height information through specialized semantic tokens, and integrates\nprimarily symbolic synthetic reasoning data. This approach enables LLMs to\naccurately manipulate objects by positioning them at specific [x, y, z]\ncoordinates. Experimental results demonstrate that AlphaSpace significantly\noutperforms existing models on manipulation subtasks, achieving a total\naccuracy of 66.67%, compared to 37.5% for GPT-4o and 29.17% for Claude 3.5\nSonnet.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62d7b2339b629105a5d6888a/505sM0N1UIUHUOunaHtAG.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18769.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62d7b2339b629105a5d6888a",
      "avatarUrl": "/avatars/c3f164fde6b8f9a671890e08ce8a3e75.svg",
      "fullname": "Alan Dao",
      "name": "alandao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.17500",
      "authors": [
        {
          "_id": "67e23d503ef5318b1550f1bc",
          "name": "Louis Owen",
          "hidden": false
        },
        {
          "_id": "67e23d503ef5318b1550f1bd",
          "name": "Abhay Kumar",
          "hidden": false
        },
        {
          "_id": "67e23d503ef5318b1550f1be",
          "name": "Nilabhra Roy Chowdhury",
          "hidden": false
        },
        {
          "_id": "67e23d503ef5318b1550f1bf",
          "name": "Fabian Gra",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6071c4b270e11b30cfcfd7a3/AaY617TTwvPiB-ub2B9bA.png"
      ],
      "publishedAt": "2025-03-21T19:23:08.000Z",
      "submittedOnDailyAt": "2025-03-25T03:52:18.158Z",
      "title": "Variance Control via Weight Rescaling in LLM Pre-training",
      "submittedOnDailyBy": {
        "_id": "6071c4b270e11b30cfcfd7a3",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6071c4b270e11b30cfcfd7a3/-1ekCBzSTpqxkkul0bgmI.jpeg",
        "isPro": false,
        "fullname": "Louis Owen",
        "user": "louisowen6",
        "type": "user"
      },
      "summary": "The outcome of Large Language Model (LLM) pre-training strongly depends on\nweight initialization and variance control strategies. Although the importance\nof initial variance control has been well documented in neural networks in\ngeneral, the literature on initialization and management of its growth during\nLLM pre-training, specifically, is somewhat sparse. In this paper, we introduce\nthe Layer Index Rescaling (LIR) weight initialization scheme, and the Target\nVariance Rescaling (TVR) variance control strategy. Experiments on a 1B\nparameter LLaMA model demonstrate that better variance management using these\ntechniques yields substantial improvements in downstream task performance (up\nto 4.6% on common pre-training benchmarks) and reduces extreme activation\nvalues, thus mitigating challenges associated with quantization and\nlow-precision training. Our code is available at:\nhttps://github.com/bluorion-com/weight_rescaling.",
      "upvotes": 2,
      "discussionId": "67e23d513ef5318b1550f22c",
      "githubRepo": "https://github.com/bluorion-com/weight_rescaling",
      "ai_keywords": [
        "Layer Index Rescaling (LIR)",
        "Target Variance Rescaling (TVR)",
        "weight initialization",
        "variance control"
      ]
    },
    "publishedAt": "2025-03-21T15:23:08.000Z",
    "title": "Variance Control via Weight Rescaling in LLM Pre-training",
    "summary": "The outcome of Large Language Model (LLM) pre-training strongly depends on\nweight initialization and variance control strategies. Although the importance\nof initial variance control has been well documented in neural networks in\ngeneral, the literature on initialization and management of its growth during\nLLM pre-training, specifically, is somewhat sparse. In this paper, we introduce\nthe Layer Index Rescaling (LIR) weight initialization scheme, and the Target\nVariance Rescaling (TVR) variance control strategy. Experiments on a 1B\nparameter LLaMA model demonstrate that better variance management using these\ntechniques yields substantial improvements in downstream task performance (up\nto 4.6% on common pre-training benchmarks) and reduces extreme activation\nvalues, thus mitigating challenges associated with quantization and\nlow-precision training. Our code is available at:\nhttps://github.com/bluorion-com/weight_rescaling.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6071c4b270e11b30cfcfd7a3/AaY617TTwvPiB-ub2B9bA.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17500.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6071c4b270e11b30cfcfd7a3",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6071c4b270e11b30cfcfd7a3/-1ekCBzSTpqxkkul0bgmI.jpeg",
      "fullname": "Louis Owen",
      "name": "louisowen6",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.18813",
      "authors": [
        {
          "_id": "67e24a997210beea5ecae330",
          "name": "Edoardo Debenedetti",
          "hidden": false
        },
        {
          "_id": "67e24a997210beea5ecae331",
          "name": "Ilia Shumailov",
          "hidden": false
        },
        {
          "_id": "67e24a997210beea5ecae332",
          "name": "Tianqi Fan",
          "hidden": false
        },
        {
          "_id": "67e24a997210beea5ecae333",
          "name": "Jamie Hayes",
          "hidden": false
        },
        {
          "_id": "67e24a997210beea5ecae334",
          "name": "Nicholas Carlini",
          "hidden": false
        },
        {
          "_id": "67e24a997210beea5ecae335",
          "name": "Daniel Fabian",
          "hidden": false
        },
        {
          "_id": "67e24a997210beea5ecae336",
          "name": "Christoph Kern",
          "hidden": false
        },
        {
          "_id": "67e24a997210beea5ecae337",
          "name": "Chongyang Shi",
          "hidden": false
        },
        {
          "_id": "67e24a997210beea5ecae338",
          "name": "Andreas Terzis",
          "hidden": false
        },
        {
          "_id": "67e24a997210beea5ecae339",
          "name": "Florian Tramr",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T15:54:10.000Z",
      "submittedOnDailyAt": "2025-03-25T04:48:52.478Z",
      "title": "Defeating Prompt Injections by Design",
      "submittedOnDailyBy": {
        "_id": "6475c2794766357252e69e9f",
        "avatarUrl": "/avatars/db428715dfd2239df2aeaaff1282323f.svg",
        "isPro": false,
        "fullname": "i",
        "user": "iliashum",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) are increasingly deployed in agentic systems\nthat interact with an external environment. However, LLM agents are vulnerable\nto prompt injection attacks when handling untrusted data. In this paper we\npropose CaMeL, a robust defense that creates a protective system layer around\nthe LLM, securing it even when underlying models may be susceptible to attacks.\nTo operate, CaMeL explicitly extracts the control and data flows from the\n(trusted) query; therefore, the untrusted data retrieved by the LLM can never\nimpact the program flow. To further improve security, CaMeL relies on a notion\nof a capability to prevent the exfiltration of private data over unauthorized\ndata flows. We demonstrate effectiveness of CaMeL by solving 67% of tasks\nwith provable security in AgentDojo [NeurIPS 2024], a recent agentic security\nbenchmark.",
      "upvotes": 1,
      "discussionId": "67e24a9b7210beea5ecae3a0",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "prompt injection attacks",
        "protective system layer",
        "trusted query",
        "untrusted data",
        "program flow",
        "capability",
        "private data exfiltration",
        "unauthorized data flows",
        "AgentDojo"
      ]
    },
    "publishedAt": "2025-03-24T11:54:10.000Z",
    "title": "Defeating Prompt Injections by Design",
    "summary": "Large Language Models (LLMs) are increasingly deployed in agentic systems\nthat interact with an external environment. However, LLM agents are vulnerable\nto prompt injection attacks when handling untrusted data. In this paper we\npropose CaMeL, a robust defense that creates a protective system layer around\nthe LLM, securing it even when underlying models may be susceptible to attacks.\nTo operate, CaMeL explicitly extracts the control and data flows from the\n(trusted) query; therefore, the untrusted data retrieved by the LLM can never\nimpact the program flow. To further improve security, CaMeL relies on a notion\nof a capability to prevent the exfiltration of private data over unauthorized\ndata flows. We demonstrate effectiveness of CaMeL by solving 67% of tasks\nwith provable security in AgentDojo [NeurIPS 2024], a recent agentic security\nbenchmark.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18813.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6475c2794766357252e69e9f",
      "avatarUrl": "/avatars/db428715dfd2239df2aeaaff1282323f.svg",
      "fullname": "i",
      "name": "iliashum",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.18559",
      "authors": [
        {
          "_id": "67e22d5236076dc847989434",
          "name": "Takashi Isobe",
          "hidden": false
        },
        {
          "_id": "67e22d5236076dc847989435",
          "name": "He Cui",
          "hidden": false
        },
        {
          "_id": "67e22d5236076dc847989436",
          "name": "Dong Zhou",
          "hidden": false
        },
        {
          "_id": "67e22d5236076dc847989437",
          "name": "Mengmeng Ge",
          "hidden": false
        },
        {
          "_id": "67e22d5236076dc847989438",
          "name": "Dong Li",
          "hidden": false
        },
        {
          "_id": "67e22d5236076dc847989439",
          "name": "Emad Barsoum",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T11:13:33.000Z",
      "submittedOnDailyAt": "2025-03-25T02:43:21.795Z",
      "title": "AMD-Hummingbird: Towards an Efficient Text-to-Video Model",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Text-to-Video (T2V) generation has attracted significant attention for its\nability to synthesize realistic videos from textual descriptions. However,\nexisting models struggle to balance computational efficiency and high visual\nquality, particularly on resource-limited devices, e.g.,iGPUs and mobile\nphones. Most prior work prioritizes visual fidelity while overlooking the need\nfor smaller, more efficient models suitable for real-world deployment. To\naddress this challenge, we propose a lightweight T2V framework, termed\nHummingbird, which prunes existing models and enhances visual quality through\nvisual feedback learning. Our approach reduces the size of the U-Net from 1.4\nbillion to 0.7 billion parameters, significantly improving efficiency while\npreserving high-quality video generation. Additionally, we introduce a novel\ndata processing pipeline that leverages Large Language Models (LLMs) and Video\nQuality Assessment (VQA) models to enhance the quality of both text prompts and\nvideo data. To support user-driven training and style customization, we\npublicly release the full training code, including data processing and model\ntraining. Extensive experiments show that our method achieves a 31X speedup\ncompared to state-of-the-art models such as VideoCrafter2, while also attaining\nthe highest overall score on VBench. Moreover, our method supports the\ngeneration of videos with up to 26 frames, addressing the limitations of\nexisting U-Net-based methods in long video generation. Notably, the entire\ntraining process requires only four GPUs, yet delivers performance competitive\nwith existing leading methods. Hummingbird presents a practical and efficient\nsolution for T2V generation, combining high performance, scalability, and\nflexibility for real-world applications.",
      "upvotes": 1,
      "discussionId": "67e22d5836076dc84798964e",
      "ai_keywords": [
        "U-Net",
        "Visual feedback learning",
        "Large Language Models (LLMs)",
        "Video Quality Assessment (VQA)",
        "VBench"
      ]
    },
    "publishedAt": "2025-03-24T07:13:33.000Z",
    "title": "AMD-Hummingbird: Towards an Efficient Text-to-Video Model",
    "summary": "Text-to-Video (T2V) generation has attracted significant attention for its\nability to synthesize realistic videos from textual descriptions. However,\nexisting models struggle to balance computational efficiency and high visual\nquality, particularly on resource-limited devices, e.g.,iGPUs and mobile\nphones. Most prior work prioritizes visual fidelity while overlooking the need\nfor smaller, more efficient models suitable for real-world deployment. To\naddress this challenge, we propose a lightweight T2V framework, termed\nHummingbird, which prunes existing models and enhances visual quality through\nvisual feedback learning. Our approach reduces the size of the U-Net from 1.4\nbillion to 0.7 billion parameters, significantly improving efficiency while\npreserving high-quality video generation. Additionally, we introduce a novel\ndata processing pipeline that leverages Large Language Models (LLMs) and Video\nQuality Assessment (VQA) models to enhance the quality of both text prompts and\nvideo data. To support user-driven training and style customization, we\npublicly release the full training code, including data processing and model\ntraining. Extensive experiments show that our method achieves a 31X speedup\ncompared to state-of-the-art models such as VideoCrafter2, while also attaining\nthe highest overall score on VBench. Moreover, our method supports the\ngeneration of videos with up to 26 frames, addressing the limitations of\nexisting U-Net-based methods in long video generation. Notably, the entire\ntraining process requires only four GPUs, yet delivers performance competitive\nwith existing leading methods. Hummingbird presents a practical and efficient\nsolution for T2V generation, combining high performance, scalability, and\nflexibility for real-world applications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18559.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6454
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.17735",
      "authors": [
        {
          "_id": "67e22bc349edf14060e5747a",
          "name": "Zhiqiang Yuan",
          "hidden": false
        },
        {
          "_id": "67e22bc349edf14060e5747b",
          "name": "Ting Zhang",
          "hidden": false
        },
        {
          "_id": "67e22bc349edf14060e5747c",
          "name": "Ying Deng",
          "hidden": false
        },
        {
          "_id": "67e22bc349edf14060e5747d",
          "name": "Jiapei Zhang",
          "hidden": false
        },
        {
          "_id": "67e22bc349edf14060e5747e",
          "name": "Yeshuang Zhu",
          "hidden": false
        },
        {
          "_id": "67e22bc349edf14060e5747f",
          "name": "Zexi Jia",
          "hidden": false
        },
        {
          "_id": "67e22bc349edf14060e57480",
          "name": "Jie Zhou",
          "hidden": false
        },
        {
          "_id": "67e22bc349edf14060e57481",
          "name": "Jinchao Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-22T11:28:25.000Z",
      "submittedOnDailyAt": "2025-03-25T02:36:41.359Z",
      "title": "RDTF: Resource-efficient Dual-mask Training Framework for Multi-frame\n  Animated Sticker Generation",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Recently, great progress has been made in video generation technology,\nattracting the widespread attention of scholars. To apply this technology to\ndownstream applications under resource-constrained conditions, researchers\nusually fine-tune the pre-trained models based on parameter-efficient tuning\nmethods such as Adapter or Lora. Although these methods can transfer the\nknowledge from the source domain to the target domain, fewer training\nparameters lead to poor fitting ability, and the knowledge from the source\ndomain may lead to the inference process deviating from the target domain. In\nthis paper, we argue that under constrained resources, training a smaller video\ngeneration model from scratch using only million-level samples can outperform\nparameter-efficient tuning on larger models in downstream applications: the\ncore lies in the effective utilization of data and curriculum strategy. Take\nanimated sticker generation (ASG) as a case study, we first construct a\ndiscrete frame generation network for stickers with low frame rates, ensuring\nthat its parameters meet the requirements of model training under constrained\nresources. In order to provide data support for models trained from scratch, we\ncome up with a dual-mask based data utilization strategy, which manages to\nimprove the availability and expand the diversity of limited data. To\nfacilitate convergence under dual-mask situation, we propose a\ndifficulty-adaptive curriculum learning method, which decomposes the sample\nentropy into static and adaptive components so as to obtain samples from easy\nto difficult. The experiment demonstrates that our resource-efficient dual-mask\ntraining framework is quantitatively and qualitatively superior to\nefficient-parameter tuning methods such as I2V-Adapter and SimDA, verifying the\nfeasibility of our method on downstream tasks under constrained resources. Code\nwill be available.",
      "upvotes": 1,
      "discussionId": "67e22bc449edf14060e574e3",
      "ai_keywords": [
        "parameter-efficient tuning",
        "Adapter",
        "Lora",
        "discrete frame generation network",
        "dual-mask based data utilization strategy",
        "curriculum learning method",
        "difficulty-adaptive curriculum learning",
        "sample entropy",
        "I2V-Adapter",
        "SimDA"
      ]
    },
    "publishedAt": "2025-03-22T07:28:25.000Z",
    "title": "RDTF: Resource-efficient Dual-mask Training Framework for Multi-frame\n  Animated Sticker Generation",
    "summary": "Recently, great progress has been made in video generation technology,\nattracting the widespread attention of scholars. To apply this technology to\ndownstream applications under resource-constrained conditions, researchers\nusually fine-tune the pre-trained models based on parameter-efficient tuning\nmethods such as Adapter or Lora. Although these methods can transfer the\nknowledge from the source domain to the target domain, fewer training\nparameters lead to poor fitting ability, and the knowledge from the source\ndomain may lead to the inference process deviating from the target domain. In\nthis paper, we argue that under constrained resources, training a smaller video\ngeneration model from scratch using only million-level samples can outperform\nparameter-efficient tuning on larger models in downstream applications: the\ncore lies in the effective utilization of data and curriculum strategy. Take\nanimated sticker generation (ASG) as a case study, we first construct a\ndiscrete frame generation network for stickers with low frame rates, ensuring\nthat its parameters meet the requirements of model training under constrained\nresources. In order to provide data support for models trained from scratch, we\ncome up with a dual-mask based data utilization strategy, which manages to\nimprove the availability and expand the diversity of limited data. To\nfacilitate convergence under dual-mask situation, we propose a\ndifficulty-adaptive curriculum learning method, which decomposes the sample\nentropy into static and adaptive components so as to obtain samples from easy\nto difficult. The experiment demonstrates that our resource-efficient dual-mask\ntraining framework is quantitatively and qualitatively superior to\nefficient-parameter tuning methods such as I2V-Adapter and SimDA, verifying the\nfeasibility of our method on downstream tasks under constrained resources. Code\nwill be available.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17735.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6454
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.17422",
      "authors": [
        {
          "_id": "67e226b3b1acaf8a7680e926",
          "name": "Javier J. Poveda Rodrigo",
          "hidden": false
        },
        {
          "_id": "67e226b3b1acaf8a7680e927",
          "name": "Mohamed Amine Ahmdi",
          "hidden": false
        },
        {
          "_id": "67e226b3b1acaf8a7680e928",
          "name": "Alessio Burrello",
          "hidden": false
        },
        {
          "_id": "67e226b3b1acaf8a7680e929",
          "name": "Daniele Jahier Pagliari",
          "hidden": false
        },
        {
          "_id": "67e226b3b1acaf8a7680e92a",
          "name": "Luca Benini",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-21T09:00:19.000Z",
      "submittedOnDailyAt": "2025-03-25T02:15:09.508Z",
      "title": "V-Seek: Accelerating LLM Reasoning on Open-hardware Server-class RISC-V\n  Platforms",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "The recent exponential growth of Large Language Models (LLMs) has relied on\nGPU-based systems. However, CPUs are emerging as a flexible and lower-cost\nalternative, especially when targeting inference and reasoning workloads.\nRISC-V is rapidly gaining traction in this area, given its open and\nvendor-neutral ISA. However, the RISC-V hardware for LLM workloads and the\ncorresponding software ecosystem are not fully mature and streamlined, given\nthe requirement of domain-specific tuning. This paper aims at filling this gap,\nfocusing on optimizing LLM inference on the Sophon SG2042, the first\ncommercially available many-core RISC-V CPU with vector processing\ncapabilities.\n  On two recent state-of-the-art LLMs optimized for reasoning, DeepSeek R1\nDistill Llama 8B and DeepSeek R1 Distill QWEN 14B, we achieve 4.32/2.29 token/s\nfor token generation and 6.54/3.68 token/s for prompt processing, with a speed\nup of up 2.9x/3.0x compared to our baseline.",
      "upvotes": 1,
      "discussionId": "67e226b3b1acaf8a7680e96b",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "GPU-based systems",
        "CPUs",
        "RISC-V",
        "ISA",
        "RISC-V hardware",
        "software ecosystem",
        "domain-specific tuning",
        "Sophon SG2042",
        "many-core RISC-V CPU",
        "vector processing capabilities",
        "DeepSeek R1 Distill Llama 8B",
        "DeepSeek R1 Distill QWEN 14B",
        "token generation",
        "prompt processing"
      ]
    },
    "publishedAt": "2025-03-21T05:00:19.000Z",
    "title": "V-Seek: Accelerating LLM Reasoning on Open-hardware Server-class RISC-V\n  Platforms",
    "summary": "The recent exponential growth of Large Language Models (LLMs) has relied on\nGPU-based systems. However, CPUs are emerging as a flexible and lower-cost\nalternative, especially when targeting inference and reasoning workloads.\nRISC-V is rapidly gaining traction in this area, given its open and\nvendor-neutral ISA. However, the RISC-V hardware for LLM workloads and the\ncorresponding software ecosystem are not fully mature and streamlined, given\nthe requirement of domain-specific tuning. This paper aims at filling this gap,\nfocusing on optimizing LLM inference on the Sophon SG2042, the first\ncommercially available many-core RISC-V CPU with vector processing\ncapabilities.\n  On two recent state-of-the-art LLMs optimized for reasoning, DeepSeek R1\nDistill Llama 8B and DeepSeek R1 Distill QWEN 14B, we achieve 4.32/2.29 token/s\nfor token generation and 6.54/3.68 token/s for prompt processing, with a speed\nup of up 2.9x/3.0x compared to our baseline.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17422.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6454
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16924",
      "authors": [
        {
          "_id": "67e230f384513315a91c5602",
          "name": "Joo Chan Lee",
          "hidden": false
        },
        {
          "_id": "67e230f384513315a91c5603",
          "name": "Jong Hwan Ko",
          "hidden": false
        },
        {
          "_id": "67e230f384513315a91c5604",
          "name": "Eunbyung Park",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-21T07:41:45.000Z",
      "submittedOnDailyAt": "2025-03-25T02:59:24.076Z",
      "title": "Optimized Minimal 3D Gaussian Splatting",
      "submittedOnDailyBy": {
        "_id": "664207e5af62c6c26653b369",
        "avatarUrl": "/avatars/031453340e114efb10b7d650e1d1c384.svg",
        "isPro": false,
        "fullname": "Joo Chan Lee",
        "user": "maincold2",
        "type": "user"
      },
      "summary": "3D Gaussian Splatting (3DGS) has emerged as a powerful representation for\nreal-time, high-performance rendering, enabling a wide range of applications.\nHowever, representing 3D scenes with numerous explicit Gaussian primitives\nimposes significant storage and memory overhead. Recent studies have shown that\nhigh-quality rendering can be achieved with a substantially reduced number of\nGaussians when represented with high-precision attributes. Nevertheless,\nexisting 3DGS compression methods still rely on a relatively large number of\nGaussians, focusing primarily on attribute compression. This is because a\nsmaller set of Gaussians becomes increasingly sensitive to lossy attribute\ncompression, leading to severe quality degradation. Since the number of\nGaussians is directly tied to computational costs, it is essential to reduce\nthe number of Gaussians effectively rather than only optimizing storage. In\nthis paper, we propose Optimized Minimal Gaussians representation (OMG), which\nsignificantly reduces storage while using a minimal number of primitives.\nFirst, we determine the distinct Gaussian from the near ones, minimizing\nredundancy without sacrificing quality. Second, we propose a compact and\nprecise attribute representation that efficiently captures both continuity and\nirregularity among primitives. Additionally, we propose a sub-vector\nquantization technique for improved irregularity representation, maintaining\nfast training with a negligible codebook size. Extensive experiments\ndemonstrate that OMG reduces storage requirements by nearly 50% compared to the\nprevious state-of-the-art and enables 600+ FPS rendering while maintaining high\nrendering quality. Our source code is available at\nhttps://maincold2.github.io/omg/.",
      "upvotes": 0,
      "discussionId": "67e230f484513315a91c5678",
      "projectPage": "https://maincold2.github.io/omg/",
      "githubRepo": "https://github.com/maincold2/OMG",
      "ai_keywords": [
        "Gaussian Splatting (3DGS)",
        "real-time",
        "high-performance rendering",
        "3D scenes",
        "explicit Gaussian primitives",
        "storage",
        "memory overhead",
        "high-quality rendering",
        "attribute compression",
        "quality degradation",
        "computational costs",
        "Optimized Minimal Gaussians representation (OMG)",
        "distinct Gaussian",
        "redundancy",
        "attribute representation",
        "continuity",
        "irregularity",
        "sub-vector quantization",
        "codebook size",
        "FPS rendering",
        "rendering quality"
      ]
    },
    "publishedAt": "2025-03-21T03:41:45.000Z",
    "title": "Optimized Minimal 3D Gaussian Splatting",
    "summary": "3D Gaussian Splatting (3DGS) has emerged as a powerful representation for\nreal-time, high-performance rendering, enabling a wide range of applications.\nHowever, representing 3D scenes with numerous explicit Gaussian primitives\nimposes significant storage and memory overhead. Recent studies have shown that\nhigh-quality rendering can be achieved with a substantially reduced number of\nGaussians when represented with high-precision attributes. Nevertheless,\nexisting 3DGS compression methods still rely on a relatively large number of\nGaussians, focusing primarily on attribute compression. This is because a\nsmaller set of Gaussians becomes increasingly sensitive to lossy attribute\ncompression, leading to severe quality degradation. Since the number of\nGaussians is directly tied to computational costs, it is essential to reduce\nthe number of Gaussians effectively rather than only optimizing storage. In\nthis paper, we propose Optimized Minimal Gaussians representation (OMG), which\nsignificantly reduces storage while using a minimal number of primitives.\nFirst, we determine the distinct Gaussian from the near ones, minimizing\nredundancy without sacrificing quality. Second, we propose a compact and\nprecise attribute representation that efficiently captures both continuity and\nirregularity among primitives. Additionally, we propose a sub-vector\nquantization technique for improved irregularity representation, maintaining\nfast training with a negligible codebook size. Extensive experiments\ndemonstrate that OMG reduces storage requirements by nearly 50% compared to the\nprevious state-of-the-art and enables 600+ FPS rendering while maintaining high\nrendering quality. Our source code is available at\nhttps://maincold2.github.io/omg/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16924.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "664207e5af62c6c26653b369",
      "avatarUrl": "/avatars/031453340e114efb10b7d650e1d1c384.svg",
      "fullname": "Joo Chan Lee",
      "name": "maincold2",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]