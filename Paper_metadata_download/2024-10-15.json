[
    {
        "paper": {
            "id": "2410.09732",
            "authors": [
                {
                    "_id": "670dd110aa00e89d82f7011f",
                    "user": {
                        "_id": "66978ee0b8656f6506b4acb2",
                        "avatarUrl": "/avatars/298acb8222e189fce4368985ee5374a1.svg",
                        "isPro": false,
                        "fullname": "Junyan Ye",
                        "user": "Yejy53",
                        "type": "user"
                    },
                    "name": "Junyan Ye",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-15T09:28:39.560Z",
                    "hidden": false
                },
                {
                    "_id": "670dd110aa00e89d82f70120",
                    "user": {
                        "_id": "646325085897b675c65aea0f",
                        "avatarUrl": "/avatars/28ce7388f9318b49bdd0a5594c0f6732.svg",
                        "isPro": false,
                        "fullname": "Baichuan Zhou",
                        "user": "bczhou",
                        "type": "user"
                    },
                    "name": "Baichuan Zhou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-15T09:10:58.373Z",
                    "hidden": false
                },
                {
                    "_id": "670dd110aa00e89d82f70121",
                    "name": "Zilong Huang",
                    "hidden": false
                },
                {
                    "_id": "670dd110aa00e89d82f70122",
                    "name": "Junan Zhang",
                    "hidden": false
                },
                {
                    "_id": "670dd110aa00e89d82f70123",
                    "user": {
                        "_id": "641802c31f1f3b0fa812225c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/641802c31f1f3b0fa812225c/i-OFWm8d37ofsmuvvAKYR.png",
                        "isPro": false,
                        "fullname": "becca_bai",
                        "user": "beccabai",
                        "type": "user"
                    },
                    "name": "Tianyi Bai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-15T09:11:00.531Z",
                    "hidden": false
                },
                {
                    "_id": "670dd110aa00e89d82f70124",
                    "user": {
                        "_id": "6617f09e4f9cea969619bfbd",
                        "avatarUrl": "/avatars/4dbc9c7b6814edcdab334ed3f0a1a4dd.svg",
                        "isPro": false,
                        "fullname": "Heng Rui Kang",
                        "user": "ruikang2002",
                        "type": "user"
                    },
                    "name": "Hengrui Kang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-15T13:24:34.153Z",
                    "hidden": false
                },
                {
                    "_id": "670dd110aa00e89d82f70125",
                    "user": {
                        "_id": "670ddb69d6ac6394419d88c5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/XxnGNaX3FWug4aiZVjg93.png",
                        "isPro": false,
                        "fullname": "Jun He",
                        "user": "JunHe0915",
                        "type": "user"
                    },
                    "name": "Jun He",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-15T13:39:45.836Z",
                    "hidden": false
                },
                {
                    "_id": "670dd110aa00e89d82f70126",
                    "name": "Honglin Lin",
                    "hidden": false
                },
                {
                    "_id": "670dd110aa00e89d82f70127",
                    "name": "Zihao Wang",
                    "hidden": false
                },
                {
                    "_id": "670dd110aa00e89d82f70128",
                    "name": "Tong Wu",
                    "hidden": false
                },
                {
                    "_id": "670dd110aa00e89d82f70129",
                    "name": "Zhizheng Wu",
                    "hidden": false
                },
                {
                    "_id": "670dd110aa00e89d82f7012a",
                    "user": {
                        "_id": "653a22c6a6e8c78bc13f9d3c",
                        "avatarUrl": "/avatars/c9ca843aded54960c661f9eb232d6d60.svg",
                        "isPro": false,
                        "fullname": "Yiping Chen",
                        "user": "chenchenchenchenya",
                        "type": "user"
                    },
                    "name": "Yiping Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-15T13:22:50.336Z",
                    "hidden": false
                },
                {
                    "_id": "670dd110aa00e89d82f7012b",
                    "user": {
                        "_id": "636317ed80c1a705a6eff396",
                        "avatarUrl": "/avatars/3db090e101b916d9256d0d3e043db71d.svg",
                        "isPro": false,
                        "fullname": "Dahua Lin",
                        "user": "lindahua",
                        "type": "user"
                    },
                    "name": "Dahua Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-15T13:22:28.798Z",
                    "hidden": false
                },
                {
                    "_id": "670dd110aa00e89d82f7012c",
                    "user": {
                        "_id": "63f9fca8d4349b157a109eec",
                        "avatarUrl": "/avatars/fa1f2ae7972d7cde99dab178136ccbb0.svg",
                        "isPro": false,
                        "fullname": "Conghui He",
                        "user": "conghui",
                        "type": "user"
                    },
                    "name": "Conghui He",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-15T13:22:43.987Z",
                    "hidden": false
                },
                {
                    "_id": "670dd110aa00e89d82f7012d",
                    "user": {
                        "_id": "66d5b56c77a026c3d2086a79",
                        "avatarUrl": "/avatars/45da07fd82fd455955faa05b27a6393f.svg",
                        "isPro": false,
                        "fullname": "Weijia Li",
                        "user": "liweijia",
                        "type": "user"
                    },
                    "name": "Weijia Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-15T13:22:37.451Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-13T05:26:36.000Z",
            "title": "LOKI: A Comprehensive Synthetic Data Detection Benchmark using Large\n  Multimodal Models",
            "summary": "With the rapid development of AI-generated content, the future internet may\nbe inundated with synthetic data, making the discrimination of authentic and\ncredible multimodal data increasingly challenging. Synthetic data detection has\nthus garnered widespread attention, and the performance of large multimodal\nmodels (LMMs) in this task has attracted significant interest. LMMs can provide\nnatural language explanations for their authenticity judgments, enhancing the\nexplainability of synthetic content detection. Simultaneously, the task of\ndistinguishing between real and synthetic data effectively tests the\nperception, knowledge, and reasoning capabilities of LMMs. In response, we\nintroduce LOKI, a novel benchmark designed to evaluate the ability of LMMs to\ndetect synthetic data across multiple modalities. LOKI encompasses video,\nimage, 3D, text, and audio modalities, comprising 18K carefully curated\nquestions across 26 subcategories with clear difficulty levels. The benchmark\nincludes coarse-grained judgment and multiple-choice questions, as well as\nfine-grained anomaly selection and explanation tasks, allowing for a\ncomprehensive analysis of LMMs. We evaluated 22 open-source LMMs and 6\nclosed-source models on LOKI, highlighting their potential as synthetic data\ndetectors and also revealing some limitations in the development of LMM\ncapabilities. More information about LOKI can be found at\nhttps://opendatalab.github.io/LOKI/",
            "upvotes": 44,
            "discussionId": "670dd114aa00e89d82f701dc"
        },
        "publishedAt": "2024-10-15T00:49:53.656Z",
        "title": "LOKI: A Comprehensive Synthetic Data Detection Benchmark using Large Multimodal Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.09732.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/641802c31f1f3b0fa812225c/i-OFWm8d37ofsmuvvAKYR.png",
            "fullname": "becca_bai",
            "name": "beccabai",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.10139",
            "authors": [
                {
                    "_id": "670dc9101fd338212b902556",
                    "user": {
                        "_id": "643e9ee6f6bb3c31a26e7bc4",
                        "avatarUrl": "/avatars/acfaa7d6a23dada24c86b954c3be116a.svg",
                        "isPro": false,
                        "fullname": "Peng Xia",
                        "user": "richardxp888",
                        "type": "user"
                    },
                    "name": "Peng Xia",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-15T09:11:03.755Z",
                    "hidden": false
                },
                {
                    "_id": "670dc9101fd338212b902557",
                    "user": {
                        "_id": "65941852f0152a21fc860f79",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65941852f0152a21fc860f79/mHOkrMOkivxxK__F4AK2o.png",
                        "isPro": false,
                        "fullname": "Siwei Han",
                        "user": "Lillianwei",
                        "type": "user"
                    },
                    "name": "Siwei Han",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-15T08:37:39.675Z",
                    "hidden": false
                },
                {
                    "_id": "670dc9101fd338212b902558",
                    "user": {
                        "_id": "6644bb2c9bdbd85493074411",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6644bb2c9bdbd85493074411/iug8dkP1zjID-kXzNxDjD.jpeg",
                        "isPro": false,
                        "fullname": "SHI QIU",
                        "user": "StarThomas1002",
                        "type": "user"
                    },
                    "name": "Shi Qiu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-15T09:11:02.150Z",
                    "hidden": false
                },
                {
                    "_id": "670dc9101fd338212b902559",
                    "name": "Yiyang Zhou",
                    "hidden": false
                },
                {
                    "_id": "670dc9101fd338212b90255a",
                    "user": {
                        "_id": "66c719e2f3f9994f2ae6c1d2",
                        "avatarUrl": "/avatars/c6cbbf458e30d235bc3db12628510fc4.svg",
                        "isPro": false,
                        "fullname": "zhaoyang.wang",
                        "user": "wangzhaoyang",
                        "type": "user"
                    },
                    "name": "Zhaoyang Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-15T09:26:59.440Z",
                    "hidden": false
                },
                {
                    "_id": "670dc9101fd338212b90255b",
                    "user": {
                        "_id": "65d0c9b4fff50114959c0004",
                        "avatarUrl": "/avatars/31f599204e51528ec317aeb980f4676e.svg",
                        "isPro": false,
                        "fullname": "Wenhao Zheng",
                        "user": "WendellZwh",
                        "type": "user"
                    },
                    "name": "Wenhao Zheng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-15T09:27:06.091Z",
                    "hidden": false
                },
                {
                    "_id": "670dc9101fd338212b90255c",
                    "user": {
                        "_id": "64d448d3d095077728f7c740",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d448d3d095077728f7c740/4CFkiVoXXDsu7uQPTU0n3.jpeg",
                        "isPro": false,
                        "fullname": "Zhaorun Chen",
                        "user": "Zhaorun",
                        "type": "user"
                    },
                    "name": "Zhaorun Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-15T09:27:12.112Z",
                    "hidden": false
                },
                {
                    "_id": "670dc9101fd338212b90255d",
                    "user": {
                        "_id": "64a26627f09ad861c85487e0",
                        "avatarUrl": "/avatars/baefd153c59f3a3238fd6034c6daf0d4.svg",
                        "isPro": false,
                        "fullname": "ChenhangCui",
                        "user": "Chenhangcui",
                        "type": "user"
                    },
                    "name": "Chenhang Cui",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-15T09:27:18.302Z",
                    "hidden": false
                },
                {
                    "_id": "670dc9101fd338212b90255e",
                    "name": "Mingyu Ding",
                    "hidden": false
                },
                {
                    "_id": "670dc9101fd338212b90255f",
                    "user": {
                        "_id": "63db16fff03c3d71ef397206",
                        "avatarUrl": "/avatars/bfb7e0d730b7d03302799d5d2828d97d.svg",
                        "isPro": false,
                        "fullname": "Linjie Li",
                        "user": "linjieli222",
                        "type": "user"
                    },
                    "name": "Linjie Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-15T09:27:58.086Z",
                    "hidden": false
                },
                {
                    "_id": "670dc9101fd338212b902560",
                    "user": {
                        "_id": "6672e20d1dbdf7da8310dd92",
                        "avatarUrl": "/avatars/5d2fb23f92a7f9ff025a5be17a26de4d.svg",
                        "isPro": false,
                        "fullname": "lijuanwang",
                        "user": "lijuanwang228",
                        "type": "user"
                    },
                    "name": "Lijuan Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-15T09:28:14.317Z",
                    "hidden": false
                },
                {
                    "_id": "670dc9101fd338212b902561",
                    "user": {
                        "_id": "65f667a455009c4ad9e6ac4c",
                        "avatarUrl": "/avatars/ed10c2cf2ba3fde6d7da93f076961607.svg",
                        "isPro": false,
                        "fullname": "Yao",
                        "user": "Huaxiu",
                        "type": "user"
                    },
                    "name": "Huaxiu Yao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-15T09:28:27.413Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-14T04:15:00.000Z",
            "title": "MMIE: Massive Multimodal Interleaved Comprehension Benchmark for Large\n  Vision-Language Models",
            "summary": "Interleaved multimodal comprehension and generation, enabling models to\nproduce and interpret both images and text in arbitrary sequences, have become\na pivotal area in multimodal learning. Despite significant advancements, the\nevaluation of this capability remains insufficient. Existing benchmarks suffer\nfrom limitations in data scale, scope, and evaluation depth, while current\nevaluation metrics are often costly or biased, lacking in reliability for\npractical applications. To address these challenges, we introduce MMIE, a\nlarge-scale knowledge-intensive benchmark for evaluating interleaved multimodal\ncomprehension and generation in Large Vision-Language Models (LVLMs). MMIE\ncomprises 20K meticulously curated multimodal queries, spanning 3 categories,\n12 fields, and 102 subfields, including mathematics, coding, physics,\nliterature, health, and arts. It supports both interleaved inputs and outputs,\noffering a mix of multiple-choice and open-ended question formats to evaluate\ndiverse competencies. Moreover, we propose a reliable automated evaluation\nmetric, leveraging a scoring model fine-tuned with human-annotated data and\nsystematic evaluation criteria, aimed at reducing bias and improving evaluation\naccuracy. Extensive experiments demonstrate the effectiveness of our benchmark\nand metrics in providing a comprehensive evaluation of interleaved LVLMs.\nSpecifically, we evaluate eight LVLMs, revealing that even the best models show\nsignificant room for improvement, with most achieving only moderate results. We\nbelieve MMIE will drive further advancements in the development of interleaved\nLVLMs. We publicly release our benchmark and code in\nhttps://mmie-bench.github.io/.",
            "upvotes": 42,
            "discussionId": "670dc91f1fd338212b902930"
        },
        "publishedAt": "2024-10-15T00:30:53.212Z",
        "title": "MMIE: Massive Multimodal Interleaved Comprehension Benchmark for Large Vision-Language Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.10139.png",
        "numComments": 3,
        "submittedBy": {
            "avatarUrl": "/avatars/acfaa7d6a23dada24c86b954c3be116a.svg",
            "fullname": "Peng Xia",
            "name": "richardxp888",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.09584",
            "authors": [
                {
                    "_id": "670dcefa487a5e24b2768ecf",
                    "user": {
                        "_id": "61cd4b833dd34ba1985e0753",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61cd4b833dd34ba1985e0753/BfHfrwotoMESpXZOHiIe4.png",
                        "isPro": false,
                        "fullname": "KABI",
                        "user": "dongguanting",
                        "type": "user"
                    },
                    "name": "Guanting Dong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-15T13:25:39.921Z",
                    "hidden": false
                },
                {
                    "_id": "670dcefa487a5e24b2768ed0",
                    "name": "Xiaoshuai Song",
                    "hidden": false
                },
                {
                    "_id": "670dcefa487a5e24b2768ed1",
                    "user": {
                        "_id": "625e62452a7279d3c77b5c38",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625e62452a7279d3c77b5c38/zJINew6U4_Gup4WTobb-0.jpeg",
                        "isPro": false,
                        "fullname": "Yutao Zhu",
                        "user": "yutaozhu94",
                        "type": "user"
                    },
                    "name": "Yutao Zhu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-15T13:25:54.583Z",
                    "hidden": false
                },
                {
                    "_id": "670dcefa487a5e24b2768ed2",
                    "name": "Runqi Qiao",
                    "hidden": false
                },
                {
                    "_id": "670dcefa487a5e24b2768ed3",
                    "user": {
                        "_id": "66f0bf59e9d50ec57febf751",
                        "avatarUrl": "/avatars/be97941e60064e5dd806c6fe9db3c537.svg",
                        "isPro": false,
                        "fullname": "Zhicheng Dou",
                        "user": "douzc",
                        "type": "user"
                    },
                    "name": "Zhicheng Dou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-15T13:26:10.877Z",
                    "hidden": false
                },
                {
                    "_id": "670dcefa487a5e24b2768ed4",
                    "user": {
                        "_id": "64b8c89052b7353d8c6a1013",
                        "avatarUrl": "/avatars/cd59fffe81f6b07b4519540b8ff3d95f.svg",
                        "isPro": false,
                        "fullname": "Ji-Rong Wen",
                        "user": "jrwen",
                        "type": "user"
                    },
                    "name": "Ji-Rong Wen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-15T13:26:16.633Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-12T16:30:51.000Z",
            "title": "Toward General Instruction-Following Alignment for Retrieval-Augmented\n  Generation",
            "summary": "Following natural instructions is crucial for the effective application of\nRetrieval-Augmented Generation (RAG) systems. Despite recent advancements in\nLarge Language Models (LLMs), research on assessing and improving\ninstruction-following (IF) alignment within the RAG domain remains limited. To\naddress this issue, we propose VIF-RAG, the first automated, scalable, and\nverifiable synthetic pipeline for instruction-following alignment in RAG\nsystems. We start by manually crafting a minimal set of atomic instructions\n(<100) and developing combination rules to synthesize and verify complex\ninstructions for a seed set. We then use supervised models for instruction\nrewriting while simultaneously generating code to automate the verification of\ninstruction quality via a Python executor. Finally, we integrate these\ninstructions with extensive RAG and general data samples, scaling up to a\nhigh-quality VIF-RAG-QA dataset (>100k) through automated processes. To further\nbridge the gap in instruction-following auto-evaluation for RAG systems, we\nintroduce FollowRAG Benchmark, which includes approximately 3K test samples,\ncovering 22 categories of general instruction constraints and four\nknowledge-intensive QA datasets. Due to its robust pipeline design, FollowRAG\ncan seamlessly integrate with different RAG benchmarks. Using FollowRAG and\neight widely-used IF and foundational abilities benchmarks for LLMs, we\ndemonstrate that VIF-RAG markedly enhances LLM performance across a broad range\nof general instruction constraints while effectively leveraging its\ncapabilities in RAG scenarios. Further analysis offers practical insights for\nachieving IF alignment in RAG systems. Our code and datasets are released at\nhttps://FollowRAG.github.io.",
            "upvotes": 32,
            "discussionId": "670dcefc487a5e24b2768f16"
        },
        "publishedAt": "2024-10-15T00:40:06.021Z",
        "title": "Toward General Instruction-Following Alignment for Retrieval-Augmented Generation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.09584.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61cd4b833dd34ba1985e0753/BfHfrwotoMESpXZOHiIe4.png",
            "fullname": "KABI",
            "name": "dongguanting",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.10306",
            "authors": [
                {
                    "_id": "670dd7ee95c91805b9f5baaf",
                    "name": "Shuai Tan",
                    "hidden": false
                },
                {
                    "_id": "670dd7ee95c91805b9f5bab0",
                    "user": {
                        "_id": "644fcbea4f7316588267dc80",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644fcbea4f7316588267dc80/w8-2Gkaw9BN9VzppNXrTP.jpeg",
                        "isPro": false,
                        "fullname": "Biao Gong",
                        "user": "BiaoGong",
                        "type": "user"
                    },
                    "name": "Biao Gong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-15T09:10:56.446Z",
                    "hidden": false
                },
                {
                    "_id": "670dd7ee95c91805b9f5bab1",
                    "name": "Xiang Wang",
                    "hidden": false
                },
                {
                    "_id": "670dd7ee95c91805b9f5bab2",
                    "user": {
                        "_id": "63a1200045edac9f7508bae9",
                        "avatarUrl": "/avatars/e84f0b045f32c5b8b4da43458650b925.svg",
                        "isPro": false,
                        "fullname": "Shiwei Zhang",
                        "user": "StevenZhang",
                        "type": "user"
                    },
                    "name": "Shiwei Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-15T13:40:12.968Z",
                    "hidden": false
                },
                {
                    "_id": "670dd7ee95c91805b9f5bab3",
                    "user": {
                        "_id": "65dd699a89a2a760d15f7d35",
                        "avatarUrl": "/avatars/e098b56c413d147d1f38cf33a4b0ecde.svg",
                        "isPro": false,
                        "fullname": "Dandan Zheng",
                        "user": "zhengdd0422",
                        "type": "user"
                    },
                    "name": "Dandan Zheng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-15T13:39:53.541Z",
                    "hidden": false
                },
                {
                    "_id": "670dd7ee95c91805b9f5bab4",
                    "name": "Ruobing Zheng",
                    "hidden": false
                },
                {
                    "_id": "670dd7ee95c91805b9f5bab5",
                    "name": "Kecheng Zheng",
                    "hidden": false
                },
                {
                    "_id": "670dd7ee95c91805b9f5bab6",
                    "name": "Jingdong Chen",
                    "hidden": false
                },
                {
                    "_id": "670dd7ee95c91805b9f5bab7",
                    "name": "Ming Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-14T09:06:55.000Z",
            "title": "Animate-X: Universal Character Image Animation with Enhanced Motion\n  Representation",
            "summary": "Character image animation, which generates high-quality videos from a\nreference image and target pose sequence, has seen significant progress in\nrecent years. However, most existing methods only apply to human figures, which\nusually do not generalize well on anthropomorphic characters commonly used in\nindustries like gaming and entertainment. Our in-depth analysis suggests to\nattribute this limitation to their insufficient modeling of motion, which is\nunable to comprehend the movement pattern of the driving video, thus imposing a\npose sequence rigidly onto the target character. To this end, this paper\nproposes Animate-X, a universal animation framework based on LDM for various\ncharacter types (collectively named X), including anthropomorphic characters.\nTo enhance motion representation, we introduce the Pose Indicator, which\ncaptures comprehensive motion pattern from the driving video through both\nimplicit and explicit manner. The former leverages CLIP visual features of a\ndriving video to extract its gist of motion, like the overall movement pattern\nand temporal relations among motions, while the latter strengthens the\ngeneralization of LDM by simulating possible inputs in advance that may arise\nduring inference. Moreover, we introduce a new Animated Anthropomorphic\nBenchmark (A^2Bench) to evaluate the performance of Animate-X on universal and\nwidely applicable animation images. Extensive experiments demonstrate the\nsuperiority and effectiveness of Animate-X compared to state-of-the-art\nmethods.",
            "upvotes": 24,
            "discussionId": "670dd7f895c91805b9f5bc7a"
        },
        "publishedAt": "2024-10-15T01:34:40.154Z",
        "title": "Animate-X: Universal Character Image Animation with Enhanced Motion Representation",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/644fcbea4f7316588267dc80/xxqdrtZK8qPLu13hxluyi.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.10306.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644fcbea4f7316588267dc80/w8-2Gkaw9BN9VzppNXrTP.jpeg",
            "fullname": "Biao Gong",
            "name": "BiaoGong",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.10563",
            "authors": [
                {
                    "_id": "670dd82d85a1548f5cdf99fd",
                    "user": {
                        "_id": "655bca95360e4f90cb61ba83",
                        "avatarUrl": "/avatars/1a187beb91a5e2fdc2303620b742aab1.svg",
                        "isPro": false,
                        "fullname": "Jiacheng Chen",
                        "user": "cccjc",
                        "type": "user"
                    },
                    "name": "Jiacheng Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-15T13:32:05.739Z",
                    "hidden": false
                },
                {
                    "_id": "670dd82d85a1548f5cdf99fe",
                    "name": "Tianhao Liang",
                    "hidden": false
                },
                {
                    "_id": "670dd82d85a1548f5cdf99ff",
                    "user": {
                        "_id": "630d73387dacb93b335b00e9",
                        "avatarUrl": "/avatars/2ded3550aedf77a58175a66319d01820.svg",
                        "isPro": false,
                        "fullname": "Sherman Siu",
                        "user": "shermansiu",
                        "type": "user"
                    },
                    "name": "Sherman Siu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-15T09:10:52.899Z",
                    "hidden": false
                },
                {
                    "_id": "670dd82d85a1548f5cdf9a00",
                    "user": {
                        "_id": "64d97c5bfd0b55d501ba00cf",
                        "avatarUrl": "/avatars/47505f2a573acea7176a96f538226ecb.svg",
                        "isPro": false,
                        "fullname": "Zhengqing Wang",
                        "user": "EricW123456",
                        "type": "user"
                    },
                    "name": "Zhengqing Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-15T13:33:34.951Z",
                    "hidden": false
                },
                {
                    "_id": "670dd82d85a1548f5cdf9a01",
                    "name": "Kai Wang",
                    "hidden": false
                },
                {
                    "_id": "670dd82d85a1548f5cdf9a02",
                    "name": "Yubo Wang",
                    "hidden": false
                },
                {
                    "_id": "670dd82d85a1548f5cdf9a03",
                    "user": {
                        "_id": "64de37ee5e192985054be575",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64de37ee5e192985054be575/fVV7JQMtp_J3uFqszJJHH.jpeg",
                        "isPro": false,
                        "fullname": "Yuansheng Ni",
                        "user": "yuanshengni",
                        "type": "user"
                    },
                    "name": "Yuansheng Ni",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-15T09:10:49.671Z",
                    "hidden": false
                },
                {
                    "_id": "670dd82d85a1548f5cdf9a04",
                    "name": "Wang Zhu",
                    "hidden": false
                },
                {
                    "_id": "670dd82d85a1548f5cdf9a05",
                    "user": {
                        "_id": "64778fb8168cb428e00f69b0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64778fb8168cb428e00f69b0/D_XYg74zHG9K3HUJj_gD4.jpeg",
                        "isPro": true,
                        "fullname": "Ziyan Jiang",
                        "user": "ziyjiang",
                        "type": "user"
                    },
                    "name": "Ziyan Jiang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-15T13:06:01.261Z",
                    "hidden": false
                },
                {
                    "_id": "670dd82d85a1548f5cdf9a06",
                    "name": "Bohan Lyu",
                    "hidden": false
                },
                {
                    "_id": "670dd82d85a1548f5cdf9a07",
                    "user": {
                        "_id": "62567c86d444a9b5a0ec51c1",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62567c86d444a9b5a0ec51c1/SUgRd1glmXO9Z7x_h0yMn.jpeg",
                        "isPro": false,
                        "fullname": "Dongfu Jiang",
                        "user": "DongfuJiang",
                        "type": "user"
                    },
                    "name": "Dongfu Jiang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-15T09:10:51.346Z",
                    "hidden": false
                },
                {
                    "_id": "670dd82d85a1548f5cdf9a08",
                    "name": "Xuan He",
                    "hidden": false
                },
                {
                    "_id": "670dd82d85a1548f5cdf9a09",
                    "user": {
                        "_id": "64d47a7a508a6313e33faedd",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d47a7a508a6313e33faedd/8DrbXdxA-sKZb4jgNhvY1.jpeg",
                        "isPro": false,
                        "fullname": "Yuan Liu",
                        "user": "YuanLiuuuuuu",
                        "type": "user"
                    },
                    "name": "Yuan Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-15T13:31:06.125Z",
                    "hidden": false
                },
                {
                    "_id": "670dd82d85a1548f5cdf9a0a",
                    "user": {
                        "_id": "643441ccd55dea2d0ec2c309",
                        "avatarUrl": "/avatars/82e99d445e2b513ad7270fa852adbcbb.svg",
                        "isPro": false,
                        "fullname": "Hexiang Hu",
                        "user": "hexianghu",
                        "type": "user"
                    },
                    "name": "Hexiang Hu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-15T13:30:44.118Z",
                    "hidden": false
                },
                {
                    "_id": "670dd82d85a1548f5cdf9a0b",
                    "user": {
                        "_id": "6230d750d93e84e233882dbc",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6230d750d93e84e233882dbc/4MGEekLW3oWzqeFWDWvIK.jpeg",
                        "isPro": false,
                        "fullname": "Xiang Yue",
                        "user": "yuexiang96",
                        "type": "user"
                    },
                    "name": "Xiang Yue",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-15T09:10:54.823Z",
                    "hidden": false
                },
                {
                    "_id": "670dd82d85a1548f5cdf9a0c",
                    "user": {
                        "_id": "6313a86154e6e5d9f0f94e04",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
                        "isPro": false,
                        "fullname": "Wenhu Chen",
                        "user": "wenhu",
                        "type": "user"
                    },
                    "name": "Wenhu Chen",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2024-10-15T02:49:20.901Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-14T14:42:12.000Z",
            "title": "MEGA-Bench: Scaling Multimodal Evaluation to over 500 Real-World Tasks",
            "summary": "We present MEGA-Bench, an evaluation suite that scales multimodal evaluation\nto over 500 real-world tasks, to address the highly heterogeneous daily use\ncases of end users. Our objective is to optimize for a set of high-quality data\nsamples that cover a highly diverse and rich set of multimodal tasks, while\nenabling cost-effective and accurate model evaluation. In particular, we\ncollected 505 realistic tasks encompassing over 8,000 samples from 16 expert\nannotators to extensively cover the multimodal task space. Instead of unifying\nthese problems into standard multi-choice questions (like MMMU, MMBench, and\nMMT-Bench), we embrace a wide range of output formats like numbers, phrases,\ncode, \\LaTeX, coordinates, JSON, free-form, etc. To accommodate these formats,\nwe developed over 40 metrics to evaluate these tasks. Unlike existing\nbenchmarks, MEGA-Bench offers a fine-grained capability report across multiple\ndimensions (e.g., application, input type, output format, skill), allowing\nusers to interact with and visualize model capabilities in depth. We evaluate a\nwide variety of frontier vision-language models on MEGA-Bench to understand\ntheir capabilities across these dimensions.",
            "upvotes": 23,
            "discussionId": "670dd83085a1548f5cdf9a80"
        },
        "publishedAt": "2024-10-15T01:21:36.347Z",
        "title": "MEGA-Bench: Scaling Multimodal Evaluation to over 500 Real-World Tasks",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.10563.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
            "fullname": "Wenhu Chen",
            "name": "wenhu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.10783",
            "authors": [
                {
                    "_id": "670e0604ae37bc424b3bb0c3",
                    "user": {
                        "_id": "62bedff7304b82a773bf8c1b",
                        "avatarUrl": "/avatars/f9e79dc196caa95c220127c6212e9944.svg",
                        "isPro": false,
                        "fullname": "Nimrod Shabtay",
                        "user": "NimrodShabtay1986",
                        "type": "user"
                    },
                    "name": "Nimrod Shabtay",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-15T13:06:04.321Z",
                    "hidden": false
                },
                {
                    "_id": "670e0604ae37bc424b3bb0c4",
                    "name": "Felipe Maia Polo",
                    "hidden": false
                },
                {
                    "_id": "670e0604ae37bc424b3bb0c5",
                    "name": "Sivan Doveh",
                    "hidden": false
                },
                {
                    "_id": "670e0604ae37bc424b3bb0c6",
                    "user": {
                        "_id": "64b99a3fd4463c3d2183739d",
                        "avatarUrl": "/avatars/9b0fddbf6b13dd1baa7cfcee3ff4f0b9.svg",
                        "isPro": false,
                        "fullname": "Wei Lin",
                        "user": "wlin21at",
                        "type": "user"
                    },
                    "name": "Wei Lin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-15T08:07:46.917Z",
                    "hidden": false
                },
                {
                    "_id": "670e0604ae37bc424b3bb0c7",
                    "name": "M. Jehanzeb Mirza",
                    "hidden": false
                },
                {
                    "_id": "670e0604ae37bc424b3bb0c8",
                    "name": "Leshem Chosen",
                    "hidden": false
                },
                {
                    "_id": "670e0604ae37bc424b3bb0c9",
                    "name": "Mikhail Yurochkin",
                    "hidden": false
                },
                {
                    "_id": "670e0604ae37bc424b3bb0ca",
                    "name": "Yuekai Sun",
                    "hidden": false
                },
                {
                    "_id": "670e0604ae37bc424b3bb0cb",
                    "name": "Assaf Arbelle",
                    "hidden": false
                },
                {
                    "_id": "670e0604ae37bc424b3bb0cc",
                    "name": "Leonid Karlinsky",
                    "hidden": false
                },
                {
                    "_id": "670e0604ae37bc424b3bb0cd",
                    "name": "Raja Giryes",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-14T17:51:23.000Z",
            "title": "LiveXiv -- A Multi-Modal Live Benchmark Based on Arxiv Papers Content",
            "summary": "The large-scale training of multi-modal models on data scraped from the web\nhas shown outstanding utility in infusing these models with the required world\nknowledge to perform effectively on multiple downstream tasks. However, one\ndownside of scraping data from the web can be the potential sacrifice of the\nbenchmarks on which the abilities of these models are often evaluated. To\nsafeguard against test data contamination and to truly test the abilities of\nthese foundation models we propose LiveXiv: A scalable evolving live benchmark\nbased on scientific ArXiv papers. LiveXiv accesses domain-specific manuscripts\nat any given timestamp and proposes to automatically generate visual\nquestion-answer pairs (VQA). This is done without any human-in-the-loop, using\nthe multi-modal content in the manuscripts, like graphs, charts, and tables.\nMoreover, we introduce an efficient evaluation approach that estimates the\nperformance of all models on the evolving benchmark using evaluations of only a\nsubset of models. This significantly reduces the overall evaluation cost. We\nbenchmark multiple open and proprietary Large Multi-modal Models (LMMs) on the\nfirst version of our benchmark, showing its challenging nature and exposing the\nmodels true abilities, avoiding contamination. Lastly, in our commitment to\nhigh quality, we have collected and evaluated a manually verified subset. By\ncomparing its overall results to our automatic annotations, we have found that\nthe performance variance is indeed minimal (<2.5%). Our dataset is available\nonline on HuggingFace, and our code will be available here.",
            "upvotes": 22,
            "discussionId": "670e060eae37bc424b3bb574"
        },
        "publishedAt": "2024-10-15T04:40:56.580Z",
        "title": "LiveXiv -- A Multi-Modal Live Benchmark Based on Arxiv Papers Content",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.10783.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/9b0fddbf6b13dd1baa7cfcee3ff4f0b9.svg",
            "fullname": "Wei Lin",
            "name": "wlin21at",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.07985",
            "authors": [
                {
                    "_id": "6709313569a07aa53e3f5e88",
                    "user": {
                        "_id": "65ae21adabf6d1ccb795e9a4",
                        "avatarUrl": "/avatars/b5dced62c6a3564095a8fa0959bc06cb.svg",
                        "isPro": false,
                        "fullname": "Bofei Gao",
                        "user": "KbsdJames",
                        "type": "user"
                    },
                    "name": "Bofei Gao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-15T13:58:13.843Z",
                    "hidden": false
                },
                {
                    "_id": "6709313569a07aa53e3f5e89",
                    "user": {
                        "_id": "6447ca6ca478b20f1755b294",
                        "avatarUrl": "/avatars/5049856b5ed1b74533fff902e14b4c7c.svg",
                        "isPro": false,
                        "fullname": "Feifan Song",
                        "user": "songff",
                        "type": "user"
                    },
                    "name": "Feifan Song",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-15T14:00:23.150Z",
                    "hidden": false
                },
                {
                    "_id": "6709313569a07aa53e3f5e8a",
                    "name": "Zhe Yang",
                    "hidden": false
                },
                {
                    "_id": "6709313569a07aa53e3f5e8b",
                    "user": {
                        "_id": "64b15284372d4340772a3dca",
                        "avatarUrl": "/avatars/417d5f1bc1bcb5e4d5de6169673c2cf7.svg",
                        "isPro": false,
                        "fullname": "Zefan Cai",
                        "user": "ZefanCai",
                        "type": "user"
                    },
                    "name": "Zefan Cai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-15T14:01:36.446Z",
                    "hidden": false
                },
                {
                    "_id": "6709313569a07aa53e3f5e8c",
                    "user": {
                        "_id": "64a139c098fad0c8a5a627a4",
                        "avatarUrl": "/avatars/6eb508abd827d4a4f5abb6b24155b22d.svg",
                        "isPro": false,
                        "fullname": "Yibo Miao",
                        "user": "instro",
                        "type": "user"
                    },
                    "name": "Yibo Miao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-15T14:01:42.392Z",
                    "hidden": false
                },
                {
                    "_id": "6709313569a07aa53e3f5e8d",
                    "user": {
                        "_id": "670740744341dcee459fb990",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/66UkZvrAk7fQr5YCylEFk.png",
                        "isPro": false,
                        "fullname": "Qingxiu Dong",
                        "user": "Rsy24",
                        "type": "user"
                    },
                    "name": "Qingxiu Dong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-15T14:01:48.414Z",
                    "hidden": false
                },
                {
                    "_id": "6709313569a07aa53e3f5e8e",
                    "user": {
                        "_id": "6038d6d0612f5eef3cc05ea9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6038d6d0612f5eef3cc05ea9/ryhvAX5djQpD5OrIlZQ1f.jpeg",
                        "isPro": false,
                        "fullname": "Lei Li",
                        "user": "tobiaslee",
                        "type": "user"
                    },
                    "name": "Lei Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-15T09:11:16.735Z",
                    "hidden": false
                },
                {
                    "_id": "6709313569a07aa53e3f5e8f",
                    "user": {
                        "_id": "66cd868be2ed0c6657eefeb7",
                        "avatarUrl": "/avatars/72c33b6c17a4f69c67c76cdde15f54eb.svg",
                        "isPro": false,
                        "fullname": "mch",
                        "user": "mch0115",
                        "type": "user"
                    },
                    "name": "Chenghao Ma",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-15T09:11:18.805Z",
                    "hidden": false
                },
                {
                    "_id": "6709313569a07aa53e3f5e90",
                    "user": {
                        "_id": "658c481dd1c8b106727a8b73",
                        "avatarUrl": "/avatars/d34a7a62c3a524e5fdd2d5994348db58.svg",
                        "isPro": false,
                        "fullname": "Liang Chen",
                        "user": "liangchen-ms",
                        "type": "user"
                    },
                    "name": "Liang Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-15T14:02:05.684Z",
                    "hidden": false
                },
                {
                    "_id": "6709313569a07aa53e3f5e91",
                    "name": "Runxin Xu",
                    "hidden": false
                },
                {
                    "_id": "6709313569a07aa53e3f5e92",
                    "user": {
                        "_id": "64912976b95c3f0a1e6233cb",
                        "avatarUrl": "/avatars/c0615f8c6606073faffb419757d4e667.svg",
                        "isPro": false,
                        "fullname": "Zhengyang Tang",
                        "user": "tangzhy",
                        "type": "user"
                    },
                    "name": "Zhengyang Tang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-15T14:02:20.963Z",
                    "hidden": false
                },
                {
                    "_id": "6709313569a07aa53e3f5e93",
                    "user": {
                        "_id": "637c6703ca8542a0ba900ccb",
                        "avatarUrl": "/avatars/288ed63a1efa566c3f01e850c6ba5dd5.svg",
                        "isPro": false,
                        "fullname": "Wang",
                        "user": "Benyou",
                        "type": "user"
                    },
                    "name": "Benyou Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-15T14:02:30.698Z",
                    "hidden": false
                },
                {
                    "_id": "6709313569a07aa53e3f5e94",
                    "user": {
                        "_id": "61527edf8b55dbdae72874fa",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61527edf8b55dbdae72874fa/ZGWSBf_KSrDof6WyMoDMU.jpeg",
                        "isPro": false,
                        "fullname": "Daoguang Zan",
                        "user": "Daoguang",
                        "type": "user"
                    },
                    "name": "Daoguang Zan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-15T14:02:36.972Z",
                    "hidden": false
                },
                {
                    "_id": "6709313569a07aa53e3f5e95",
                    "name": "Shanghaoran Quan",
                    "hidden": false
                },
                {
                    "_id": "6709313569a07aa53e3f5e96",
                    "user": {
                        "_id": "638efcf4c67af472d316d424",
                        "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
                        "isPro": false,
                        "fullname": "Ge Zhang",
                        "user": "zhangysk",
                        "type": "user"
                    },
                    "name": "Ge Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-15T14:02:52.819Z",
                    "hidden": false
                },
                {
                    "_id": "6709313569a07aa53e3f5e97",
                    "name": "Lei Sha",
                    "hidden": false
                },
                {
                    "_id": "6709313569a07aa53e3f5e98",
                    "name": "Yichang Zhang",
                    "hidden": false
                },
                {
                    "_id": "6709313569a07aa53e3f5e99",
                    "name": "Xuancheng Ren",
                    "hidden": false
                },
                {
                    "_id": "6709313569a07aa53e3f5e9a",
                    "name": "Tianyu Liu",
                    "hidden": false
                },
                {
                    "_id": "6709313569a07aa53e3f5e9b",
                    "name": "Baobao Chang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-10T14:39:33.000Z",
            "title": "Omni-MATH: A Universal Olympiad Level Mathematic Benchmark For Large\n  Language Models",
            "summary": "Recent advancements in large language models (LLMs) have led to significant\nbreakthroughs in mathematical reasoning capabilities. However, existing\nbenchmarks like GSM8K or MATH are now being solved with high accuracy (e.g.,\nOpenAI o1 achieves 94.8% on MATH dataset), indicating their inadequacy for\ntruly challenging these models. To bridge this gap, we propose a comprehensive\nand challenging benchmark specifically designed to assess LLMs' mathematical\nreasoning at the Olympiad level. Unlike existing Olympiad-related benchmarks,\nour dataset focuses exclusively on mathematics and comprises a vast collection\nof 4428 competition-level problems with rigorous human annotation. These\nproblems are meticulously categorized into over 33 sub-domains and span more\nthan 10 distinct difficulty levels, enabling a holistic assessment of model\nperformance in Olympiad-mathematical reasoning. Furthermore, we conducted an\nin-depth analysis based on this benchmark. Our experimental results show that\neven the most advanced models, OpenAI o1-mini and OpenAI o1-preview, struggle\nwith highly challenging Olympiad-level problems, with 60.54% and 52.55%\naccuracy, highlighting significant challenges in Olympiad-level mathematical\nreasoning.",
            "upvotes": 20,
            "discussionId": "6709313769a07aa53e3f5f50"
        },
        "publishedAt": "2024-10-15T00:06:51.411Z",
        "title": "Omni-MATH: A Universal Olympiad Level Mathematic Benchmark For Large Language Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.07985.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "/avatars/b5dced62c6a3564095a8fa0959bc06cb.svg",
            "fullname": "Bofei Gao",
            "name": "KbsdJames",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.10774",
            "authors": [
                {
                    "_id": "670de040ae6e15b485712a81",
                    "user": {
                        "_id": "62f687a0c58915315c4ff75d",
                        "avatarUrl": "/avatars/b657180c7666735062782edd4f6a69c9.svg",
                        "isPro": false,
                        "fullname": "Dejia Xu",
                        "user": "ir1d",
                        "type": "user"
                    },
                    "name": "Dejia Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-15T09:10:40.250Z",
                    "hidden": false
                },
                {
                    "_id": "670de040ae6e15b485712a82",
                    "name": "Yifan Jiang",
                    "hidden": false
                },
                {
                    "_id": "670de040ae6e15b485712a83",
                    "name": "Chen Huang",
                    "hidden": false
                },
                {
                    "_id": "670de040ae6e15b485712a84",
                    "name": "Liangchen Song",
                    "hidden": false
                },
                {
                    "_id": "670de040ae6e15b485712a85",
                    "name": "Thorsten Gernoth",
                    "hidden": false
                },
                {
                    "_id": "670de040ae6e15b485712a86",
                    "name": "Liangliang Cao",
                    "hidden": false
                },
                {
                    "_id": "670de040ae6e15b485712a87",
                    "name": "Zhangyang Wang",
                    "hidden": false
                },
                {
                    "_id": "670de040ae6e15b485712a88",
                    "name": "Hao Tang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-14T17:46:32.000Z",
            "title": "Cavia: Camera-controllable Multi-view Video Diffusion with\n  View-Integrated Attention",
            "summary": "In recent years there have been remarkable breakthroughs in image-to-video\ngeneration. However, the 3D consistency and camera controllability of generated\nframes have remained unsolved. Recent studies have attempted to incorporate\ncamera control into the generation process, but their results are often limited\nto simple trajectories or lack the ability to generate consistent videos from\nmultiple distinct camera paths for the same scene. To address these\nlimitations, we introduce Cavia, a novel framework for camera-controllable,\nmulti-view video generation, capable of converting an input image into multiple\nspatiotemporally consistent videos. Our framework extends the spatial and\ntemporal attention modules into view-integrated attention modules, improving\nboth viewpoint and temporal consistency. This flexible design allows for joint\ntraining with diverse curated data sources, including scene-level static\nvideos, object-level synthetic multi-view dynamic videos, and real-world\nmonocular dynamic videos. To our best knowledge, Cavia is the first of its kind\nthat allows the user to precisely specify camera motion while obtaining object\nmotion. Extensive experiments demonstrate that Cavia surpasses state-of-the-art\nmethods in terms of geometric consistency and perceptual quality. Project Page:\nhttps://ir1d.github.io/Cavia/",
            "upvotes": 18,
            "discussionId": "670de044ae6e15b485712bfe"
        },
        "publishedAt": "2024-10-15T01:58:29.483Z",
        "title": "Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.10774.png",
        "numComments": 3,
        "submittedBy": {
            "avatarUrl": "/avatars/b657180c7666735062782edd4f6a69c9.svg",
            "fullname": "Dejia Xu",
            "name": "ir1d",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.10594",
            "authors": [
                {
                    "_id": "670e4bb54968ab91d837d73e",
                    "user": {
                        "_id": "6135eeeb5bc6ecdf86b60f0d",
                        "avatarUrl": "/avatars/43cedcf20ab6b0801a662787400e1384.svg",
                        "isPro": false,
                        "fullname": "Shi Yu",
                        "user": "yushi",
                        "type": "user"
                    },
                    "name": "Shi Yu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-15T16:37:25.335Z",
                    "hidden": false
                },
                {
                    "_id": "670e4bb54968ab91d837d73f",
                    "user": {
                        "_id": "65e2db6899d809668f2f29f5",
                        "avatarUrl": "/avatars/33365712acb3d8b0fc935accd8086333.svg",
                        "isPro": false,
                        "fullname": "Chaoyue Tang",
                        "user": "tcy6",
                        "type": "user"
                    },
                    "name": "Chaoyue Tang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-15T16:37:22.575Z",
                    "hidden": false
                },
                {
                    "_id": "670e4bb54968ab91d837d740",
                    "user": {
                        "_id": "6415818a986557e8cac252bf",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6415818a986557e8cac252bf/T4u9qjRt8P4clF4nOTA4W.jpeg",
                        "isPro": false,
                        "fullname": "Bokai Xu",
                        "user": "bokesyo",
                        "type": "user"
                    },
                    "name": "Bokai Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-15T16:37:23.899Z",
                    "hidden": false
                },
                {
                    "_id": "670e4bb54968ab91d837d741",
                    "user": {
                        "_id": "63f706dfe94ed998c463ed66",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f706dfe94ed998c463ed66/tAGil2qiFNev6CfEEDseV.png",
                        "isPro": true,
                        "fullname": "Cuiunbo",
                        "user": "Cuiunbo",
                        "type": "user"
                    },
                    "name": "Junbo Cui",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-15T15:09:53.369Z",
                    "hidden": false
                },
                {
                    "_id": "670e4bb54968ab91d837d742",
                    "name": "Junhao Ran",
                    "hidden": false
                },
                {
                    "_id": "670e4bb54968ab91d837d743",
                    "name": "Yukun Yan",
                    "hidden": false
                },
                {
                    "_id": "670e4bb54968ab91d837d744",
                    "name": "Zhenghao Liu",
                    "hidden": false
                },
                {
                    "_id": "670e4bb54968ab91d837d745",
                    "name": "Shuo Wang",
                    "hidden": false
                },
                {
                    "_id": "670e4bb54968ab91d837d746",
                    "name": "Xu Han",
                    "hidden": false
                },
                {
                    "_id": "670e4bb54968ab91d837d747",
                    "name": "Zhiyuan Liu",
                    "hidden": false
                },
                {
                    "_id": "670e4bb54968ab91d837d748",
                    "name": "Maosong Sun",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-14T15:04:18.000Z",
            "title": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality\n  Documents",
            "summary": "Retrieval-augmented generation (RAG) is an effective technique that enables\nlarge language models (LLMs) to utilize external knowledge sources for\ngeneration. However, current RAG systems are solely based on text, rendering it\nimpossible to utilize vision information like layout and images that play\ncrucial roles in real-world multi-modality documents. In this paper, we\nintroduce VisRAG, which tackles this issue by establishing a vision-language\nmodel (VLM)-based RAG pipeline. In this pipeline, instead of first parsing the\ndocument to obtain text, the document is directly embedded using a VLM as an\nimage and then retrieved to enhance the generation of a VLM. Compared to\ntraditional text-based RAG, VisRAG maximizes the retention and utilization of\nthe data information in the original documents, eliminating the information\nloss introduced during the parsing process. We collect both open-source and\nsynthetic data to train the retriever in VisRAG and explore a variety of\ngeneration methods. Experiments demonstrate that VisRAG outperforms traditional\nRAG in both the retrieval and generation stages, achieving a 25--39\\%\nend-to-end performance gain over traditional text-based RAG pipeline. Further\nanalysis reveals that VisRAG is effective in utilizing training data and\ndemonstrates strong generalization capability, positioning it as a promising\nsolution for RAG on multi-modality documents. Our code and data are available\nat https://github.com/openbmb/visrag .",
            "upvotes": 13,
            "discussionId": "670e4bbb4968ab91d837d8d4"
        },
        "publishedAt": "2024-10-15T13:58:17.351Z",
        "title": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63f706dfe94ed998c463ed66/eDxSfqaBcmmF4G3lWNK_N.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.10594.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f706dfe94ed998c463ed66/tAGil2qiFNev6CfEEDseV.png",
            "fullname": "Cuiunbo",
            "name": "Cuiunbo",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.10792",
            "authors": [
                {
                    "_id": "670ddc05d6ac6394419dc0ca",
                    "user": {
                        "_id": "649647b125b6d0ef557e5e44",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Axat8GQafuGSNN2KCWbmL.jpeg",
                        "isPro": false,
                        "fullname": "Litu Rout",
                        "user": "LituRout",
                        "type": "user"
                    },
                    "name": "Litu Rout",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-15T14:13:51.573Z",
                    "hidden": false
                },
                {
                    "_id": "670ddc05d6ac6394419dc0cb",
                    "name": "Yujia Chen",
                    "hidden": false
                },
                {
                    "_id": "670ddc05d6ac6394419dc0cc",
                    "name": "Nataniel Ruiz",
                    "hidden": false
                },
                {
                    "_id": "670ddc05d6ac6394419dc0cd",
                    "name": "Constantine Caramanis",
                    "hidden": false
                },
                {
                    "_id": "670ddc05d6ac6394419dc0ce",
                    "name": "Sanjay Shakkottai",
                    "hidden": false
                },
                {
                    "_id": "670ddc05d6ac6394419dc0cf",
                    "name": "Wen-Sheng Chu",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-14T17:56:24.000Z",
            "title": "Semantic Image Inversion and Editing using Rectified Stochastic\n  Differential Equations",
            "summary": "Generative models transform random noise into images; their inversion aims to\ntransform images back to structured noise for recovery and editing. This paper\naddresses two key tasks: (i) inversion and (ii) editing of a real image using\nstochastic equivalents of rectified flow models (such as Flux). Although\nDiffusion Models (DMs) have recently dominated the field of generative modeling\nfor images, their inversion presents faithfulness and editability challenges\ndue to nonlinearities in drift and diffusion. Existing state-of-the-art DM\ninversion approaches rely on training of additional parameters or test-time\noptimization of latent variables; both are expensive in practice. Rectified\nFlows (RFs) offer a promising alternative to diffusion models, yet their\ninversion has been underexplored. We propose RF inversion using dynamic optimal\ncontrol derived via a linear quadratic regulator. We prove that the resulting\nvector field is equivalent to a rectified stochastic differential equation.\nAdditionally, we extend our framework to design a stochastic sampler for Flux.\nOur inversion method allows for state-of-the-art performance in zero-shot\ninversion and editing, outperforming prior works in stroke-to-image synthesis\nand semantic image editing, with large-scale human evaluations confirming user\npreference.",
            "upvotes": 11,
            "discussionId": "670ddc0cd6ac6394419dc34b"
        },
        "publishedAt": "2024-10-15T03:10:41.083Z",
        "title": "Semantic Image Inversion and Editing using Rectified Stochastic Differential Equations",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.10792.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Axat8GQafuGSNN2KCWbmL.jpeg",
            "fullname": "Litu Rout",
            "name": "LituRout",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.10818",
            "authors": [
                {
                    "_id": "670de52bab8a8a8e3b942151",
                    "user": {
                        "_id": "63b7b2c6bd2d153522821766",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b7b2c6bd2d153522821766/aHtga-_OUdOrg_TRrXO08.jpeg",
                        "isPro": false,
                        "fullname": "Mu Cai",
                        "user": "mucai",
                        "type": "user"
                    },
                    "name": "Mu Cai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-15T14:09:25.460Z",
                    "hidden": false
                },
                {
                    "_id": "670de52bab8a8a8e3b942152",
                    "name": "Reuben Tan",
                    "hidden": false
                },
                {
                    "_id": "670de52bab8a8a8e3b942153",
                    "name": "Jianrui Zhang",
                    "hidden": false
                },
                {
                    "_id": "670de52bab8a8a8e3b942154",
                    "user": {
                        "_id": "6489eb3c44cfcffe8f5918e3",
                        "avatarUrl": "/avatars/4edffaa045cc736e2edbf858fb6ef7c8.svg",
                        "isPro": false,
                        "fullname": "Bocheng Zou",
                        "user": "BochengZou",
                        "type": "user"
                    },
                    "name": "Bocheng Zou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-15T08:07:52.341Z",
                    "hidden": false
                },
                {
                    "_id": "670de52bab8a8a8e3b942155",
                    "user": {
                        "_id": "63e0a50242591dda0b9dca5c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e0a50242591dda0b9dca5c/c7cBPEBWQDFYimfGnO_SI.png",
                        "isPro": false,
                        "fullname": "Kai Zhang",
                        "user": "drogozhang",
                        "type": "user"
                    },
                    "name": "Kai Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-15T14:10:19.304Z",
                    "hidden": false
                },
                {
                    "_id": "670de52bab8a8a8e3b942156",
                    "user": {
                        "_id": "64c8b2c5c547ed5243e14a6e",
                        "avatarUrl": "/avatars/96d4a9010f96001c8cff235915926390.svg",
                        "isPro": false,
                        "fullname": "Feng Yao",
                        "user": "fengyao1909",
                        "type": "user"
                    },
                    "name": "Feng Yao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-15T08:07:54.360Z",
                    "hidden": false
                },
                {
                    "_id": "670de52bab8a8a8e3b942157",
                    "name": "Fangrui Zhu",
                    "hidden": false
                },
                {
                    "_id": "670de52bab8a8a8e3b942158",
                    "user": {
                        "_id": "634781fae5c0717e6739bc59",
                        "avatarUrl": "/avatars/2b6d2d0625741b28f326459acad55b69.svg",
                        "isPro": false,
                        "fullname": "Jing Gu",
                        "user": "jinggu",
                        "type": "user"
                    },
                    "name": "Jing Gu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-15T14:10:56.430Z",
                    "hidden": false
                },
                {
                    "_id": "670de52bab8a8a8e3b942159",
                    "user": {
                        "_id": "62b4fe7ab25cb80fcf2ffd66",
                        "avatarUrl": "/avatars/22f6a05cdbf4224d29ec9259c9fdd7a4.svg",
                        "isPro": false,
                        "fullname": "Yiwu Zhong",
                        "user": "YiwuZhong",
                        "type": "user"
                    },
                    "name": "Yiwu Zhong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-15T14:12:43.229Z",
                    "hidden": false
                },
                {
                    "_id": "670de52bab8a8a8e3b94215a",
                    "user": {
                        "_id": "63a740aec46291f273a95b4c",
                        "avatarUrl": "/avatars/8c19e639e39f93083ab624747f9fd0b3.svg",
                        "isPro": false,
                        "fullname": "Yuzhang Shang",
                        "user": "yuzhang",
                        "type": "user"
                    },
                    "name": "Yuzhang Shang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-15T14:12:48.439Z",
                    "hidden": false
                },
                {
                    "_id": "670de52bab8a8a8e3b94215b",
                    "user": {
                        "_id": "63519eb8a9a9ae182206ff02",
                        "avatarUrl": "/avatars/c310ac17d87bd4d892ff29caad2673dd.svg",
                        "isPro": false,
                        "fullname": "Yao Dou",
                        "user": "douy",
                        "type": "user"
                    },
                    "name": "Yao Dou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-15T14:12:54.131Z",
                    "hidden": false
                },
                {
                    "_id": "670de52bab8a8a8e3b94215c",
                    "user": {
                        "_id": "66421caf1208117512556149",
                        "avatarUrl": "/avatars/71cc7eff6c914e5369697dc6f4daa0bb.svg",
                        "isPro": false,
                        "fullname": "pak",
                        "user": "jadenpak",
                        "type": "user"
                    },
                    "name": "Jaden Park",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-15T14:13:04.199Z",
                    "hidden": false
                },
                {
                    "_id": "670de52bab8a8a8e3b94215d",
                    "user": {
                        "_id": "641904caf9d6f1d772ec7af7",
                        "avatarUrl": "/avatars/4a63eac71eb30f70b1a0e9d4708f26c1.svg",
                        "isPro": false,
                        "fullname": "Jianfeng Gao",
                        "user": "wyngjf",
                        "type": "user"
                    },
                    "name": "Jianfeng Gao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-15T14:13:10.670Z",
                    "hidden": false
                },
                {
                    "_id": "670de52bab8a8a8e3b94215e",
                    "user": {
                        "_id": "649f41ee70a478f8b36b2984",
                        "avatarUrl": "/avatars/1c9a76717a450ac4aeb62a1e823d2e4a.svg",
                        "isPro": false,
                        "fullname": "Yong Jae Lee",
                        "user": "yjlee0222",
                        "type": "user"
                    },
                    "name": "Yong Jae Lee",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-15T14:13:15.639Z",
                    "hidden": false
                },
                {
                    "_id": "670de52bab8a8a8e3b94215f",
                    "user": {
                        "_id": "6125df7f25027fb1ea9c7a41",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1629871954341-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Jianwei Yang",
                        "user": "jw2yang",
                        "type": "user"
                    },
                    "name": "Jianwei Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-15T14:13:36.677Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-14T17:59:58.000Z",
            "title": "TemporalBench: Benchmarking Fine-grained Temporal Understanding for\n  Multimodal Video Models",
            "summary": "Understanding fine-grained temporal dynamics is crucial for multimodal video\ncomprehension and generation. Due to the lack of fine-grained temporal\nannotations, existing video benchmarks mostly resemble static image benchmarks\nand are incompetent at evaluating models for temporal understanding. In this\npaper, we introduce TemporalBench, a new benchmark dedicated to evaluating\nfine-grained temporal understanding in videos. TemporalBench consists of ~10K\nvideo question-answer pairs, derived from ~2K high-quality human annotations\ndetailing the temporal dynamics in video clips. As a result, our benchmark\nprovides a unique testbed for evaluating various temporal understanding and\nreasoning abilities such as action frequency, motion magnitude, event order,\netc. Moreover, it enables evaluations on various tasks like both video question\nanswering and captioning, both short and long video understanding, as well as\ndifferent models such as multimodal video embedding models and text generation\nmodels. Results show that state-of-the-art models like GPT-4o achieve only\n38.5% question answering accuracy on TemporalBench, demonstrating a significant\ngap (~30%) between humans and AI in temporal understanding. Furthermore, we\nnotice a critical pitfall for multi-choice QA where LLMs can detect the subtle\nchanges in negative captions and find a centralized description as a cue for\nits prediction, where we propose Multiple Binary Accuracy (MBA) to correct such\nbias. We hope that TemporalBench can foster research on improving models'\ntemporal reasoning capabilities. Both dataset and evaluation code will be made\navailable.",
            "upvotes": 10,
            "discussionId": "670de53bab8a8a8e3b94271e"
        },
        "publishedAt": "2024-10-15T02:16:22.297Z",
        "title": "TemporalBench: Benchmarking Fine-grained Temporal Understanding for Multimodal Video Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.10818.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b7b2c6bd2d153522821766/aHtga-_OUdOrg_TRrXO08.jpeg",
            "fullname": "Mu Cai",
            "name": "mucai",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.09335",
            "authors": [
                {
                    "_id": "670dd0cc487a5e24b27713b9",
                    "name": "Tingyu Xia",
                    "hidden": false
                },
                {
                    "_id": "670dd0cc487a5e24b27713ba",
                    "user": {
                        "_id": "6438b43ab2ea24b52ebac2b9",
                        "avatarUrl": "/avatars/84133cd719a4b1e2f5c1a74178425f86.svg",
                        "isPro": false,
                        "fullname": "Bowen Yu",
                        "user": "bwy",
                        "type": "user"
                    },
                    "name": "Bowen Yu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-15T14:21:17.898Z",
                    "hidden": false
                },
                {
                    "_id": "670dd0cc487a5e24b27713bb",
                    "name": "Kai Dang",
                    "hidden": false
                },
                {
                    "_id": "670dd0cc487a5e24b27713bc",
                    "name": "An Yang",
                    "hidden": false
                },
                {
                    "_id": "670dd0cc487a5e24b27713bd",
                    "user": {
                        "_id": "670e57b3391f1a7021182bff",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/N0tuHZVz8KFPCv8G1qUX2.png",
                        "isPro": false,
                        "fullname": "Yuan Wu",
                        "user": "WhiteCatY",
                        "type": "user"
                    },
                    "name": "Yuan Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-15T15:17:56.143Z",
                    "hidden": false
                },
                {
                    "_id": "670dd0cc487a5e24b27713be",
                    "name": "Yuan Tian",
                    "hidden": false
                },
                {
                    "_id": "670dd0cc487a5e24b27713bf",
                    "name": "Yi Chang",
                    "hidden": false
                },
                {
                    "_id": "670dd0cc487a5e24b27713c0",
                    "user": {
                        "_id": "620760a26e3b7210c2ff1943",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/620760a26e3b7210c2ff1943/VC-rKqimF6yxGESNVlPoR.jpeg",
                        "isPro": false,
                        "fullname": "Junyang Lin",
                        "user": "JustinLin610",
                        "type": "user"
                    },
                    "name": "Junyang Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-15T14:19:07.666Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-12T02:48:34.000Z",
            "title": "Rethinking Data Selection at Scale: Random Selection is Almost All You\n  Need",
            "summary": "Supervised fine-tuning (SFT) is crucial for aligning Large Language Models\n(LLMs) with human instructions. The primary goal during SFT is to select a\nsmall yet representative subset of training data from the larger pool, such\nthat fine-tuning with this subset achieves results comparable to or even\nexceeding those obtained using the entire dataset. However, most existing data\nselection techniques are designed for small-scale data pools, which fail to\nmeet the demands of real-world SFT scenarios. In this paper, we replicated\nseveral self-scoring methods those that do not rely on external model\nassistance on two million scale datasets, and found that nearly all methods\nstruggled to significantly outperform random selection when dealing with such\nlarge-scale data pools. Moreover, our comparisons suggest that, during SFT,\ndiversity in data selection is more critical than simply focusing on high\nquality data. We also analyzed the limitations of several current approaches,\nexplaining why they perform poorly on large-scale datasets and why they are\nunsuitable for such contexts. Finally, we found that filtering data by token\nlength offers a stable and efficient method for improving results. This\napproach, particularly when training on long text data, proves highly\nbeneficial for relatively weaker base models, such as Llama3.",
            "upvotes": 8,
            "discussionId": "670dd0cc487a5e24b27713fa"
        },
        "publishedAt": "2024-10-15T00:50:02.900Z",
        "title": "Rethinking Data Selection at Scale: Random Selection is Almost All You Need",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.09335.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/b5ad98cf269ae5f1fe90861fb4170fae.svg",
            "fullname": "Bowen Yu",
            "name": "Tigerph",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.06634",
            "authors": [
                {
                    "_id": "670e05ef1cc9c1fdf0a093ff",
                    "name": "Armel Zebaze",
                    "hidden": false
                },
                {
                    "_id": "670e05ef1cc9c1fdf0a09400",
                    "user": {
                        "_id": "602ba2a739515f8d31237967",
                        "avatarUrl": "/avatars/ef534712ca682bf74ec7eef17d3d1b5f.svg",
                        "isPro": false,
                        "fullname": "Benoît Sagot",
                        "user": "sagot",
                        "type": "user"
                    },
                    "name": "Benoît Sagot",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-15T16:32:56.867Z",
                    "hidden": false
                },
                {
                    "_id": "670e05ef1cc9c1fdf0a09401",
                    "user": {
                        "_id": "613a0c85e63aa832a98cecc4",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1653912785529-613a0c85e63aa832a98cecc4.png",
                        "isPro": false,
                        "fullname": "Rachel Bawden",
                        "user": "rbawden",
                        "type": "user"
                    },
                    "name": "Rachel Bawden",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-15T16:33:02.609Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-09T07:35:46.000Z",
            "title": "Tree of Problems: Improving structured problem solving with\n  compositionality",
            "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across\nmultiple tasks through in-context learning. For complex reasoning tasks that\nrequire step-by-step thinking, Chain-of-Thought (CoT) prompting has given\nimpressive results, especially when combined with self-consistency.\nNonetheless, some tasks remain particularly difficult for LLMs to solve. Tree\nof Thoughts (ToT) and Graph of Thoughts (GoT) emerged as alternatives, dividing\nthe complex problem into paths of subproblems. In this paper, we propose Tree\nof Problems (ToP), a simpler version of ToT, which we hypothesise can work\nbetter for complex tasks that can be divided into identical subtasks. Our\nempirical results show that our approach outperforms ToT and GoT, and in\naddition performs better than CoT on complex reasoning tasks. All code for this\npaper is publicly available here:\nhttps://github.com/ArmelRandy/tree-of-problems.",
            "upvotes": 5,
            "discussionId": "670e05ef1cc9c1fdf0a09449"
        },
        "publishedAt": "2024-10-15T04:34:39.451Z",
        "title": "Tree of Problems: Improving structured problem solving with compositionality",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.06634.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/e901c3aaa16d3a06dff09896ce8a67a2.svg",
            "fullname": "Armel Randy Zebaze",
            "name": "ArmelRandy",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.10803",
            "authors": [
                {
                    "_id": "670de51d1cc9c1fdf0959026",
                    "user": {
                        "_id": "63509bc859bfa9a85d4220aa",
                        "avatarUrl": "/avatars/ca2cc9b87f5ca5cd51606b2f9edf89d0.svg",
                        "isPro": false,
                        "fullname": "Yanjie Ze",
                        "user": "yjze",
                        "type": "user"
                    },
                    "name": "Yanjie Ze",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-15T16:37:10.802Z",
                    "hidden": false
                },
                {
                    "_id": "670de51d1cc9c1fdf0959027",
                    "name": "Zixuan Chen",
                    "hidden": false
                },
                {
                    "_id": "670de51d1cc9c1fdf0959028",
                    "name": "Wenhao Wang",
                    "hidden": false
                },
                {
                    "_id": "670de51d1cc9c1fdf0959029",
                    "name": "Tianyi Chen",
                    "hidden": false
                },
                {
                    "_id": "670de51d1cc9c1fdf095902a",
                    "user": {
                        "_id": "6332b75df0b2af7f685810da",
                        "avatarUrl": "/avatars/078f582f555a9dee3f7ab4d155c0a65c.svg",
                        "isPro": false,
                        "fullname": "Xialin He",
                        "user": "MuLinjiu",
                        "type": "user"
                    },
                    "name": "Xialin He",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-15T16:38:10.895Z",
                    "hidden": false
                },
                {
                    "_id": "670de51d1cc9c1fdf095902b",
                    "name": "Ying Yuan",
                    "hidden": false
                },
                {
                    "_id": "670de51d1cc9c1fdf095902c",
                    "name": "Xue Bin Peng",
                    "hidden": false
                },
                {
                    "_id": "670de51d1cc9c1fdf095902d",
                    "user": {
                        "_id": "647b7b8345616c3de3627863",
                        "avatarUrl": "/avatars/8196364bffed71086ba9a0a4a62027d3.svg",
                        "isPro": false,
                        "fullname": "Jiajun Wu",
                        "user": "jiajunwu",
                        "type": "user"
                    },
                    "name": "Jiajun Wu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-15T16:37:48.920Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-14T17:59:00.000Z",
            "title": "Generalizable Humanoid Manipulation with Improved 3D Diffusion Policies",
            "summary": "Humanoid robots capable of autonomous operation in diverse environments have\nlong been a goal for roboticists. However, autonomous manipulation by humanoid\nrobots has largely been restricted to one specific scene, primarily due to the\ndifficulty of acquiring generalizable skills. Recent advances in 3D visuomotor\npolicies, such as the 3D Diffusion Policy (DP3), have shown promise in\nextending these capabilities to wilder environments. However, 3D visuomotor\npolicies often rely on camera calibration and point-cloud segmentation, which\npresent challenges for deployment on mobile robots like humanoids. In this\nwork, we introduce the Improved 3D Diffusion Policy (iDP3), a novel 3D\nvisuomotor policy that eliminates these constraints by leveraging egocentric 3D\nvisual representations. We demonstrate that iDP3 enables a full-sized humanoid\nrobot to autonomously perform skills in diverse real-world scenarios, using\nonly data collected in the lab. Videos are available at:\nhttps://humanoid-manipulation.github.io",
            "upvotes": 5,
            "discussionId": "670de52e1cc9c1fdf0959499"
        },
        "publishedAt": "2024-10-15T02:14:57.117Z",
        "title": "Generalizable Humanoid Manipulation with Improved 3D Diffusion Policies",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.10803.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/ca2cc9b87f5ca5cd51606b2f9edf89d0.svg",
            "fullname": "Yanjie Ze",
            "name": "yjze",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.10813",
            "authors": [
                {
                    "_id": "670defb480363ae01368542c",
                    "user": {
                        "_id": "639bf367445b133a4e97ef9c",
                        "avatarUrl": "/avatars/51b59f4616a01796e07c05c9aa5286f8.svg",
                        "isPro": false,
                        "fullname": "Di Wu",
                        "user": "xiaowu0162",
                        "type": "user"
                    },
                    "name": "Di Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-15T08:07:50.570Z",
                    "hidden": false
                },
                {
                    "_id": "670defb480363ae01368542d",
                    "name": "Hongwei Wang",
                    "hidden": false
                },
                {
                    "_id": "670defb480363ae01368542e",
                    "user": {
                        "_id": "5feab3a28a3201f8e554c969",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1660795228685-5feab3a28a3201f8e554c969.png",
                        "isPro": false,
                        "fullname": "Wenhao Yu",
                        "user": "wyu1",
                        "type": "user"
                    },
                    "name": "Wenhao Yu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-15T16:41:35.824Z",
                    "hidden": false
                },
                {
                    "_id": "670defb480363ae01368542f",
                    "name": "Yuwei Zhang",
                    "hidden": false
                },
                {
                    "_id": "670defb480363ae013685430",
                    "user": {
                        "_id": "60b7b9d71b90c5d07c23fbd0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1622653364258-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Kai-Wei Chang",
                        "user": "kaiweichang",
                        "type": "user"
                    },
                    "name": "Kai-Wei Chang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-15T16:42:29.388Z",
                    "hidden": false
                },
                {
                    "_id": "670defb480363ae013685431",
                    "name": "Dong Yu",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-14T17:59:44.000Z",
            "title": "LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive\n  Memory",
            "summary": "Recent large language model (LLM)-driven chat assistant systems have\nintegrated memory components to track user-assistant chat histories, enabling\nmore accurate and personalized responses. However, their long-term memory\ncapabilities in sustained interactions remain underexplored. This paper\nintroduces LongMemEval, a comprehensive benchmark designed to evaluate five\ncore long-term memory abilities of chat assistants: information extraction,\nmulti-session reasoning, temporal reasoning, knowledge updates, and abstention.\nWith 500 meticulously curated questions embedded within freely scalable\nuser-assistant chat histories, LongMemEval presents a significant challenge to\nexisting long-term memory systems, with commercial chat assistants and\nlong-context LLMs showing 30% accuracy drop on memorizing information across\nsustained interactions. We then present a unified framework that breaks down\nthe long-term memory design into four design choices across the indexing,\nretrieval, and reading stages. Built upon key experimental insights, we propose\nseveral memory designs including session decomposition for optimizing value\ngranularity, fact-augmented key expansion for enhancing the index structure,\nand time-aware query expansion for refining the search scope. Experiment\nresults show that these optimizations greatly improve both memory recall and\ndownstream question answering on LongMemEval. Overall, our study provides\nvaluable resources and guidance for advancing the long-term memory capabilities\nof LLM-based chat assistants, paving the way toward more personalized and\nreliable conversational AI.",
            "upvotes": 4,
            "discussionId": "670defb680363ae0136854c2"
        },
        "publishedAt": "2024-10-15T03:01:29.103Z",
        "title": "LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.10813.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/51b59f4616a01796e07c05c9aa5286f8.svg",
            "fullname": "Di Wu",
            "name": "xiaowu0162",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.07752",
            "authors": [
                {
                    "_id": "6708e94c1f9b09c1ab200e8a",
                    "user": {
                        "_id": "6616922bff43b65a8ece7ccc",
                        "avatarUrl": "/avatars/983799cedcad7c75a57f215630e7ab4a.svg",
                        "isPro": false,
                        "fullname": "Daniel Cores",
                        "user": "dcores",
                        "type": "user"
                    },
                    "name": "Daniel Cores",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-15T09:11:20.594Z",
                    "hidden": false
                },
                {
                    "_id": "6708e94c1f9b09c1ab200e8b",
                    "user": {
                        "_id": "6566436e3d526e3297c1b4a1",
                        "avatarUrl": "/avatars/e433ffccb0f9c2d91649f9988655fedd.svg",
                        "isPro": false,
                        "fullname": "Michael Dorkenwald",
                        "user": "mdorkenw",
                        "type": "user"
                    },
                    "name": "Michael Dorkenwald",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-14T12:16:07.727Z",
                    "hidden": false
                },
                {
                    "_id": "6708e94c1f9b09c1ab200e8c",
                    "name": "Manuel Mucientes",
                    "hidden": false
                },
                {
                    "_id": "6708e94c1f9b09c1ab200e8d",
                    "name": "Cees G. M. Snoek",
                    "hidden": false
                },
                {
                    "_id": "6708e94c1f9b09c1ab200e8e",
                    "user": {
                        "_id": "637d21239a5217b88b7549c3",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637d21239a5217b88b7549c3/LrIGPiva5VGVZG87rTAJz.jpeg",
                        "isPro": false,
                        "fullname": "Yuki Asano",
                        "user": "yukimasano",
                        "type": "user"
                    },
                    "name": "Yuki M. Asano",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-15T09:11:22.379Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-10T09:28:36.000Z",
            "title": "TVBench: Redesigning Video-Language Evaluation",
            "summary": "Large language models have demonstrated impressive performance when\nintegrated with vision models even enabling video understanding. However,\nevaluating these video models presents its own unique challenges, for which\nseveral benchmarks have been proposed. In this paper, we show that the\ncurrently most used video-language benchmarks can be solved without requiring\nmuch temporal reasoning. We identified three main issues in existing datasets:\n(i) static information from single frames is often sufficient to solve the\ntasks (ii) the text of the questions and candidate answers is overly\ninformative, allowing models to answer correctly without relying on any visual\ninput (iii) world knowledge alone can answer many of the questions, making the\nbenchmarks a test of knowledge replication rather than visual reasoning. In\naddition, we found that open-ended question-answering benchmarks for video\nunderstanding suffer from similar issues while the automatic evaluation process\nwith LLMs is unreliable, making it an unsuitable alternative. As a solution, we\npropose TVBench, a novel open-source video multiple-choice question-answering\nbenchmark, and demonstrate through extensive evaluations that it requires a\nhigh level of temporal understanding. Surprisingly, we find that most recent\nstate-of-the-art video-language models perform similarly to random performance\non TVBench, with only Gemini-Pro and Tarsier clearly surpassing this baseline.",
            "upvotes": 4,
            "discussionId": "6708e94e1f9b09c1ab200f36"
        },
        "publishedAt": "2024-10-15T01:06:10.775Z",
        "title": "TVBench: Redesigning Video-Language Evaluation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.07752.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/e433ffccb0f9c2d91649f9988655fedd.svg",
            "fullname": "Michael Dorkenwald",
            "name": "mdorkenw",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.09733",
            "authors": [
                {
                    "_id": "670df88bd2e43d76064fbfd0",
                    "user": {
                        "_id": "639f8277beb95d698de007dd",
                        "avatarUrl": "/avatars/57f223ccd9d3cb03166ccf0e41361c58.svg",
                        "isPro": false,
                        "fullname": "HangHua",
                        "user": "hhua2",
                        "type": "user"
                    },
                    "name": "Hang Hua",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-15T14:09:46.462Z",
                    "hidden": false
                },
                {
                    "_id": "670df88bd2e43d76064fbfd1",
                    "name": "Yunlong Tang",
                    "hidden": false
                },
                {
                    "_id": "670df88bd2e43d76064fbfd2",
                    "user": {
                        "_id": "65e6a9a4c799256ec5f7ef77",
                        "avatarUrl": "/avatars/539a455841675539a62b44ff9803a948.svg",
                        "isPro": false,
                        "fullname": "ziyun zeng",
                        "user": "zengziyun",
                        "type": "user"
                    },
                    "name": "Ziyun Zeng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-15T08:07:48.782Z",
                    "hidden": false
                },
                {
                    "_id": "670df88bd2e43d76064fbfd3",
                    "name": "Liangliang Cao",
                    "hidden": false
                },
                {
                    "_id": "670df88bd2e43d76064fbfd4",
                    "name": "Zhengyuan Yang",
                    "hidden": false
                },
                {
                    "_id": "670df88bd2e43d76064fbfd5",
                    "name": "Hangfeng He",
                    "hidden": false
                },
                {
                    "_id": "670df88bd2e43d76064fbfd6",
                    "name": "Chenliang Xu",
                    "hidden": false
                },
                {
                    "_id": "670df88bd2e43d76064fbfd7",
                    "name": "Jiebo Luo",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-13T05:35:09.000Z",
            "title": "MMCOMPOSITION: Revisiting the Compositionality of Pre-trained\n  Vision-Language Models",
            "summary": "The advent of large Vision-Language Models (VLMs) has significantly advanced\nmultimodal understanding, enabling more sophisticated and accurate integration\nof visual and textual information across various tasks, including image and\nvideo captioning, visual question answering, and cross-modal retrieval. Despite\nVLMs' superior capabilities, researchers lack a comprehensive understanding of\ntheir compositionality -- the ability to understand and produce novel\ncombinations of known visual and textual components. Prior benchmarks provide\nonly a relatively rough compositionality evaluation from the perspectives of\nobjects, relations, and attributes while neglecting deeper reasoning about\nobject interactions, counting, and complex compositions. However,\ncompositionality is a critical ability that facilitates coherent reasoning and\nunderstanding across modalities for VLMs. To address this limitation, we\npropose MMCOMPOSITION, a novel human-annotated benchmark for comprehensively\nand accurately evaluating VLMs' compositionality. Our proposed benchmark serves\nas a complement to these earlier works. With MMCOMPOSITION, we can quantify and\nexplore the compositionality of the mainstream VLMs. Surprisingly, we find\nGPT-4o's compositionality inferior to the best open-source model, and we\nanalyze the underlying reasons. Our experimental analysis reveals the\nlimitations of VLMs in fine-grained compositional perception and reasoning, and\npoints to areas for improvement in VLM design and training. Resources available\nat: https://hanghuacs.github.io/MMComposition/",
            "upvotes": 3,
            "discussionId": "670df88fd2e43d76064fc0e9"
        },
        "publishedAt": "2024-10-15T12:32:30.659Z",
        "title": "MMCOMPOSITION: Revisiting the Compositionality of Pre-trained Vision-Language Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.09733.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/539a455841675539a62b44ff9803a948.svg",
            "fullname": "ziyun zeng",
            "name": "zengziyun",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.10630",
            "authors": [
                {
                    "_id": "670e8838ba29b3fca2222f69",
                    "name": "Tianhao Wu",
                    "hidden": false
                },
                {
                    "_id": "670e8838ba29b3fca2222f6a",
                    "name": "Janice Lan",
                    "hidden": false
                },
                {
                    "_id": "670e8838ba29b3fca2222f6b",
                    "user": {
                        "_id": "6172aa78c8e66e2aa84c06b8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1634904686602-noauth.png",
                        "isPro": false,
                        "fullname": "Weizhe Yuan",
                        "user": "weizhey",
                        "type": "user"
                    },
                    "name": "Weizhe Yuan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-15T16:45:50.569Z",
                    "hidden": false
                },
                {
                    "_id": "670e8838ba29b3fca2222f6c",
                    "user": {
                        "_id": "653b306986b88947d5cacfa4",
                        "avatarUrl": "/avatars/21ebd4daf35ec67c7d5f9b0a53628b00.svg",
                        "isPro": false,
                        "fullname": "Jiantao Jiao",
                        "user": "nexus-jt-llm",
                        "type": "user"
                    },
                    "name": "Jiantao Jiao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-15T16:45:56.366Z",
                    "hidden": false
                },
                {
                    "_id": "670e8838ba29b3fca2222f6d",
                    "user": {
                        "_id": "62f023a36a027498eaa2f9cc",
                        "avatarUrl": "/avatars/8ac1c5c74d0957e3c6cc94b3a7795c37.svg",
                        "isPro": false,
                        "fullname": "Jason Weston",
                        "user": "spermwhale",
                        "type": "user"
                    },
                    "name": "Jason Weston",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-15T16:46:02.044Z",
                    "hidden": false
                },
                {
                    "_id": "670e8838ba29b3fca2222f6e",
                    "user": {
                        "_id": "66a8611eb51510d82ed54231",
                        "avatarUrl": "/avatars/ad559e774fee4914091b82c9831ae2a2.svg",
                        "isPro": false,
                        "fullname": "Sainbayar Sukhbaatar",
                        "user": "sainbar",
                        "type": "user"
                    },
                    "name": "Sainbayar Sukhbaatar",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-15T16:46:07.400Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-14T15:38:56.000Z",
            "title": "Thinking LLMs: General Instruction Following with Thought Generation",
            "summary": "LLMs are typically trained to answer user questions or follow instructions\nsimilarly to how human experts respond. However, in the standard alignment\nframework they lack the basic ability of explicit thinking before answering.\nThinking is important for complex questions that require reasoning and planning\n-- but can be applied to any task. We propose a training method for equipping\nexisting LLMs with such thinking abilities for general instruction following\nwithout use of additional human data. We achieve this by an iterative search\nand optimization procedure that explores the space of possible thought\ngenerations, allowing the model to learn how to think without direct\nsupervision. For each instruction, the thought candidates are scored using a\njudge model to evaluate their responses only, and then optimized via preference\noptimization. We show that this procedure leads to superior performance on\nAlpacaEval and Arena-Hard, and shows gains from thinking on non-reasoning\ncategories such as marketing, health and general knowledge, in addition to more\ntraditional reasoning & problem-solving tasks.",
            "upvotes": 2,
            "discussionId": "670e883fba29b3fca2223170"
        },
        "publishedAt": "2024-10-15T13:51:05.612Z",
        "title": "Thinking LLMs: General Instruction Following with Thought Generation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.10630.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.09223",
            "authors": [
                {
                    "_id": "670e91997e9b2018da22717e",
                    "user": {
                        "_id": "5fc54c33fa6eef7667a4d699",
                        "avatarUrl": "/avatars/24cce84812f724fdbcd950bbbae9b154.svg",
                        "isPro": false,
                        "fullname": "Ruochen",
                        "user": "ruochenz",
                        "type": "user"
                    },
                    "name": "Ruochen Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-15T16:43:59.951Z",
                    "hidden": false
                },
                {
                    "_id": "670e91997e9b2018da22717f",
                    "user": {
                        "_id": "66f577584b2e6a5c8e433942",
                        "avatarUrl": "/avatars/40ba38c3e8d4ca55fb7b15fc78aadccd.svg",
                        "isPro": false,
                        "fullname": "Qinan Yu",
                        "user": "qinanyu",
                        "type": "user"
                    },
                    "name": "Qinan Yu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-15T16:44:06.598Z",
                    "hidden": false
                },
                {
                    "_id": "670e91997e9b2018da227180",
                    "user": {
                        "_id": "66fa4f2a6e6db689a26f37d4",
                        "avatarUrl": "/avatars/327d7992cd69620cc173c4a9dff093f3.svg",
                        "isPro": false,
                        "fullname": "Yuki Zang",
                        "user": "matianyuzang",
                        "type": "user"
                    },
                    "name": "Matianyu Zang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-15T16:44:18.957Z",
                    "hidden": false
                },
                {
                    "_id": "670e91997e9b2018da227181",
                    "name": "Carsten Eickhoff",
                    "hidden": false
                },
                {
                    "_id": "670e91997e9b2018da227182",
                    "name": "Ellie Pavlick",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-11T19:57:55.000Z",
            "title": "The Same But Different: Structural Similarities and Differences in\n  Multilingual Language Modeling",
            "summary": "We employ new tools from mechanistic interpretability in order to ask whether\nthe internal structure of large language models (LLMs) shows correspondence to\nthe linguistic structures which underlie the languages on which they are\ntrained. In particular, we ask (1) when two languages employ the same\nmorphosyntactic processes, do LLMs handle them using shared internal circuitry?\nand (2) when two languages require different morphosyntactic processes, do LLMs\nhandle them using different internal circuitry? Using English and Chinese\nmultilingual and monolingual models, we analyze the internal circuitry involved\nin two tasks. We find evidence that models employ the same circuit to handle\nthe same syntactic process independently of the language in which it occurs,\nand that this is the case even for monolingual models trained completely\nindependently. Moreover, we show that multilingual models employ\nlanguage-specific components (attention heads and feed-forward networks) when\nneeded to handle linguistic processes (e.g., morphological marking) that only\nexist in some languages. Together, our results provide new insights into how\nLLMs trade off between exploiting common structures and preserving linguistic\ndifferences when tasked with modeling multiple languages simultaneously.",
            "upvotes": 1,
            "discussionId": "670e919d7e9b2018da227293"
        },
        "publishedAt": "2024-10-15T14:32:40.528Z",
        "title": "The Same But Different: Structural Similarities and Differences in Multilingual Language Modeling",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.09223.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/24cce84812f724fdbcd950bbbae9b154.svg",
            "fullname": "Ruochen",
            "name": "ruochenz",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    }
]