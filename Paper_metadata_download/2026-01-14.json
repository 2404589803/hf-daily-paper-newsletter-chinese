[
  {
    "paper": {
      "id": "2601.07022",
      "authors": [
        {
          "_id": "6966474587c71000b5a910d2",
          "name": "Sungrae Park",
          "hidden": false
        },
        {
          "_id": "6966474587c71000b5a910d3",
          "name": "Sanghoon Kim",
          "hidden": false
        },
        {
          "_id": "6966474587c71000b5a910d4",
          "name": "Jungho Cho",
          "hidden": false
        },
        {
          "_id": "6966474587c71000b5a910d5",
          "name": "Gyoungjin Gim",
          "hidden": false
        },
        {
          "_id": "6966474587c71000b5a910d6",
          "name": "Dawoon Jung",
          "hidden": false
        },
        {
          "_id": "6966474587c71000b5a910d7",
          "name": "Mikyoung Cha",
          "hidden": false
        },
        {
          "_id": "6966474587c71000b5a910d8",
          "name": "Eunhae Choo",
          "hidden": false
        },
        {
          "_id": "6966474587c71000b5a910d9",
          "name": "Taekgyu Hong",
          "hidden": false
        },
        {
          "_id": "6966474587c71000b5a910da",
          "name": "Minbyul Jeong",
          "hidden": false
        },
        {
          "_id": "6966474587c71000b5a910db",
          "name": "SeHwan Joo",
          "hidden": false
        },
        {
          "_id": "6966474587c71000b5a910dc",
          "name": "Minsoo Khang",
          "hidden": false
        },
        {
          "_id": "6966474587c71000b5a910dd",
          "name": "Eunwon Kim",
          "hidden": false
        },
        {
          "_id": "6966474587c71000b5a910de",
          "name": "Minjeong Kim",
          "hidden": false
        },
        {
          "_id": "6966474587c71000b5a910df",
          "name": "Sujeong Kim",
          "hidden": false
        },
        {
          "_id": "6966474587c71000b5a910e0",
          "name": "Yunsu Kim",
          "hidden": false
        },
        {
          "_id": "6966474587c71000b5a910e1",
          "name": "Hyeonju Lee",
          "hidden": false
        },
        {
          "_id": "6966474587c71000b5a910e2",
          "name": "Seunghyun Lee",
          "hidden": false
        },
        {
          "_id": "6966474587c71000b5a910e3",
          "name": "Sukyung Lee",
          "hidden": false
        },
        {
          "_id": "6966474587c71000b5a910e4",
          "name": "Siyoung Park",
          "hidden": false
        },
        {
          "_id": "6966474587c71000b5a910e5",
          "name": "Gyungin Shin",
          "hidden": false
        },
        {
          "_id": "6966474587c71000b5a910e6",
          "name": "Inseo Song",
          "hidden": false
        },
        {
          "_id": "6966474587c71000b5a910e7",
          "name": "Wonho Song",
          "hidden": false
        },
        {
          "_id": "6966474587c71000b5a910e8",
          "name": "Seonghoon Yang",
          "hidden": false
        },
        {
          "_id": "6966474587c71000b5a910e9",
          "name": "Seungyoun Yi",
          "hidden": false
        },
        {
          "_id": "6966474587c71000b5a910ea",
          "name": "Sanghoon Yoon",
          "hidden": false
        },
        {
          "_id": "6966474587c71000b5a910eb",
          "name": "Jeonghyun Ko",
          "hidden": false
        },
        {
          "_id": "6966474587c71000b5a910ec",
          "name": "Seyoung Song",
          "hidden": false
        },
        {
          "_id": "6966474587c71000b5a910ed",
          "name": "Keunwoo Choi",
          "hidden": false
        },
        {
          "_id": "6966474587c71000b5a910ee",
          "name": "Hwalsuk Lee",
          "hidden": false
        },
        {
          "_id": "6966474587c71000b5a910ef",
          "name": "Sunghun Kim",
          "hidden": false
        },
        {
          "_id": "6966474587c71000b5a910f0",
          "name": "Du-Seong Chang",
          "hidden": false
        },
        {
          "_id": "6966474587c71000b5a910f1",
          "name": "Kyunghyun Cho",
          "hidden": false
        },
        {
          "_id": "6966474587c71000b5a910f2",
          "name": "Junsuk Choe",
          "hidden": false
        },
        {
          "_id": "6966474587c71000b5a910f3",
          "name": "Hwaran Lee",
          "hidden": false
        },
        {
          "_id": "6966474587c71000b5a910f4",
          "name": "Jae-Gil Lee",
          "hidden": false
        },
        {
          "_id": "6966474587c71000b5a910f5",
          "name": "KyungTae Lim",
          "hidden": false
        },
        {
          "_id": "6966474587c71000b5a910f6",
          "name": "Alice Oh",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-11T18:33:09.000Z",
      "submittedOnDailyAt": "2026-01-14T02:22:03.363Z",
      "title": "Solar Open Technical Report",
      "submittedOnDailyBy": {
        "_id": "64587be872b60ae7a3817858",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64587be872b60ae7a3817858/BbdOOxOCEzWTvEpkWp8MM.png",
        "isPro": false,
        "fullname": "Minbyul Jeong",
        "user": "Minbyul",
        "type": "user"
      },
      "summary": "We introduce Solar Open, a 102B-parameter bilingual Mixture-of-Experts language model for underserved languages. Solar Open demonstrates a systematic methodology for building competitive LLMs by addressing three interconnected challenges. First, to train effectively despite data scarcity for underserved languages, we synthesize 4.5T tokens of high-quality, domain-specific, and RL-oriented data. Second, we coordinate this data through a progressive curriculum jointly optimizing composition, quality thresholds, and domain coverage across 20 trillion tokens. Third, to enable reasoning capabilities through scalable RL, we apply our proposed framework SnapPO for efficient optimization. Across benchmarks in English and Korean, Solar Open achieves competitive performance, demonstrating the effectiveness of this methodology for underserved language AI development.",
      "upvotes": 32,
      "discussionId": "6966474587c71000b5a910f7",
      "ai_summary": "Solar Open presents a 102B-parameter bilingual Mixture-of-Experts language model that addresses data scarcity in underserved languages through synthetic data generation, progressive curriculum coordination, and scalable reinforcement learning optimization.",
      "ai_keywords": [
        "Mixture-of-Experts",
        "language model",
        "underserved languages",
        "data synthesis",
        "progressive curriculum",
        "reinforcement learning",
        "SnapPO",
        "domain-specific data",
        "quality thresholds",
        "composition optimization"
      ],
      "organization": {
        "_id": "62940d125d1c94a62e838db2",
        "name": "upstage",
        "fullname": "upstage",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/649144feeb13c70f7671c603/bUxWC5jKltd-MyrrCNCv5.png"
      }
    },
    "publishedAt": "2026-01-11T13:33:09.000Z",
    "title": "Solar Open Technical Report",
    "summary": "We introduce Solar Open, a 102B-parameter bilingual Mixture-of-Experts language model for underserved languages. Solar Open demonstrates a systematic methodology for building competitive LLMs by addressing three interconnected challenges. First, to train effectively despite data scarcity for underserved languages, we synthesize 4.5T tokens of high-quality, domain-specific, and RL-oriented data. Second, we coordinate this data through a progressive curriculum jointly optimizing composition, quality thresholds, and domain coverage across 20 trillion tokens. Third, to enable reasoning capabilities through scalable RL, we apply our proposed framework SnapPO for efficient optimization. Across benchmarks in English and Korean, Solar Open achieves competitive performance, demonstrating the effectiveness of this methodology for underserved language AI development.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.07022.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64587be872b60ae7a3817858",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64587be872b60ae7a3817858/BbdOOxOCEzWTvEpkWp8MM.png",
      "fullname": "Minbyul Jeong",
      "name": "Minbyul",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "62940d125d1c94a62e838db2",
      "name": "upstage",
      "fullname": "upstage",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/649144feeb13c70f7671c603/bUxWC5jKltd-MyrrCNCv5.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.08225",
      "authors": [
        {
          "_id": "6967202cc5e371f6b235d1cc",
          "name": "Jungho Cho",
          "hidden": false
        },
        {
          "_id": "6967202cc5e371f6b235d1cd",
          "name": "Minbyul Jeong",
          "hidden": false
        },
        {
          "_id": "6967202cc5e371f6b235d1ce",
          "name": "Sungrae Park",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-13T05:14:09.000Z",
      "submittedOnDailyAt": "2026-01-14T02:19:55.431Z",
      "title": "User-Oriented Multi-Turn Dialogue Generation with Tool Use at scale",
      "submittedOnDailyBy": {
        "_id": "64587be872b60ae7a3817858",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64587be872b60ae7a3817858/BbdOOxOCEzWTvEpkWp8MM.png",
        "isPro": false,
        "fullname": "Minbyul Jeong",
        "user": "Minbyul",
        "type": "user"
      },
      "summary": "The recent paradigm shift toward large reasoning models (LRMs) as autonomous agents has intensified the demand for sophisticated, multi-turn tool-use capabilities. Yet, existing datasets and data-generation approaches are limited by static, predefined toolsets that cannot scale to the complexity of open-ended human-agent collaboration. To address this, we initially developed a framework for automated task-oriented multi-turn dialogue generation at scale, utilizing an LRM-based simulator to dynamically generate high-value, domain-specific tools to solve specified tasks. However, we observe that a purely task-oriented design often results in \"solely task-solving\" trajectories, where the agent completes the objective with minimal interaction, failing to generate the high turn-count conversations seen in realistic scenarios. To bridge this gap, we shift toward a user-oriented simulation paradigm. By decoupling task generation from a dedicated user simulator that mimics human behavioral rules - such as incremental request-making and turn-by-turn feedback - we facilitate more authentic, extended multi-turn dialogues that reflect the iterative nature of real-world problem solving. Our generation pipeline operates as a versatile, plug-and-play module capable of initiating generation from any state, ensuring high scalability in producing extended tool-use data. Furthermore, by facilitating multiple task completions within a single trajectory, it yields a high-density dataset that reflects the multifaceted demands of real-world human-agent interaction.",
      "upvotes": 29,
      "discussionId": "6967202dc5e371f6b235d1cf",
      "ai_summary": "Large reasoning models enable scalable multi-turn dialogue generation through automated task-oriented simulation and user-oriented behavioral modeling for enhanced human-agent interaction datasets.",
      "ai_keywords": [
        "large reasoning models",
        "multi-turn dialogue generation",
        "task-oriented simulation",
        "user simulator",
        "behavioral rules",
        "turn-by-turn feedback",
        "automated task generation",
        "high-density dataset",
        "human-agent interaction"
      ],
      "organization": {
        "_id": "62940d125d1c94a62e838db2",
        "name": "upstage",
        "fullname": "upstage",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/649144feeb13c70f7671c603/bUxWC5jKltd-MyrrCNCv5.png"
      }
    },
    "publishedAt": "2026-01-13T00:14:09.000Z",
    "title": "User-Oriented Multi-Turn Dialogue Generation with Tool Use at scale",
    "summary": "The recent paradigm shift toward large reasoning models (LRMs) as autonomous agents has intensified the demand for sophisticated, multi-turn tool-use capabilities. Yet, existing datasets and data-generation approaches are limited by static, predefined toolsets that cannot scale to the complexity of open-ended human-agent collaboration. To address this, we initially developed a framework for automated task-oriented multi-turn dialogue generation at scale, utilizing an LRM-based simulator to dynamically generate high-value, domain-specific tools to solve specified tasks. However, we observe that a purely task-oriented design often results in \"solely task-solving\" trajectories, where the agent completes the objective with minimal interaction, failing to generate the high turn-count conversations seen in realistic scenarios. To bridge this gap, we shift toward a user-oriented simulation paradigm. By decoupling task generation from a dedicated user simulator that mimics human behavioral rules - such as incremental request-making and turn-by-turn feedback - we facilitate more authentic, extended multi-turn dialogues that reflect the iterative nature of real-world problem solving. Our generation pipeline operates as a versatile, plug-and-play module capable of initiating generation from any state, ensuring high scalability in producing extended tool-use data. Furthermore, by facilitating multiple task completions within a single trajectory, it yields a high-density dataset that reflects the multifaceted demands of real-world human-agent interaction.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.08225.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64587be872b60ae7a3817858",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64587be872b60ae7a3817858/BbdOOxOCEzWTvEpkWp8MM.png",
      "fullname": "Minbyul Jeong",
      "name": "Minbyul",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "62940d125d1c94a62e838db2",
      "name": "upstage",
      "fullname": "upstage",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/649144feeb13c70f7671c603/bUxWC5jKltd-MyrrCNCv5.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.24965",
      "authors": [
        {
          "_id": "6958946b832867f253525a3a",
          "name": "Siyuan Hu",
          "hidden": false
        },
        {
          "_id": "6958946b832867f253525a3b",
          "name": "Kevin Qinghong Lin",
          "hidden": false
        },
        {
          "_id": "6958946b832867f253525a3c",
          "name": "Mike Zheng Shou",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64440be5af034cdfd69ca3a7/_OdaniV8mAw_Y2nQ85G_I.mp4"
      ],
      "publishedAt": "2025-12-31T16:51:14.000Z",
      "submittedOnDailyAt": "2026-01-14T02:46:22.550Z",
      "title": "ShowUI-π: Flow-based Generative Models as GUI Dexterous Hands",
      "submittedOnDailyBy": {
        "_id": "64440be5af034cdfd69ca3a7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg",
        "isPro": false,
        "fullname": "Qinghong (Kevin) Lin",
        "user": "KevinQHLin",
        "type": "user"
      },
      "summary": "Building intelligent agents capable of dexterous manipulation is essential for achieving human-like automation in both robotics and digital environments. However, existing GUI agents rely on discrete click predictions (x,y), which prohibits free-form, closed-loop trajectories (e.g. dragging a progress bar) that require continuous, on-the-fly perception and adjustment. In this work, we develop ShowUI-π, the first flow-based generative model as GUI dexterous hand, featuring the following designs: (i) Unified Discrete-Continuous Actions, integrating discrete clicks and continuous drags within a shared model, enabling flexible adaptation across diverse interaction modes; (ii) Flow-based Action Generation for drag modeling, which predicts incremental cursor adjustments from continuous visual observations via a lightweight action expert, ensuring smooth and stable trajectories; (iii) Drag Training data and Benchmark, where we manually collect and synthesize 20K drag trajectories across five domains (e.g. PowerPoint, Adobe Premiere Pro), and introduce ScreenDrag, a benchmark with comprehensive online and offline evaluation protocols for assessing GUI agents' drag capabilities. Our experiments show that proprietary GUI agents still struggle on ScreenDrag (e.g. Operator scores 13.27, and the best Gemini-2.5-CUA reaches 22.18). In contrast, ShowUI-π achieves 26.98 with only 450M parameters, underscoring both the difficulty of the task and the effectiveness of our approach. We hope this work advances GUI agents toward human-like dexterous control in digital world. The code is available at https://github.com/showlab/showui-pi.",
      "upvotes": 23,
      "discussionId": "6958946b832867f253525a3f",
      "projectPage": "https://showlab.github.io/showui-pi",
      "githubRepo": "https://github.com/showlab/showui-pi",
      "githubRepoAddedBy": "auto",
      "githubStars": 14,
      "organization": {
        "_id": "63a553c4ce5763e06f78669c",
        "name": "showlab",
        "fullname": "Show Lab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1671779505215-63a55320ce5763e06f78519c.png"
      }
    },
    "publishedAt": "2025-12-31T11:51:14.000Z",
    "title": "ShowUI-π: Flow-based Generative Models as GUI Dexterous Hands",
    "summary": "Building intelligent agents capable of dexterous manipulation is essential for achieving human-like automation in both robotics and digital environments. However, existing GUI agents rely on discrete click predictions (x,y), which prohibits free-form, closed-loop trajectories (e.g. dragging a progress bar) that require continuous, on-the-fly perception and adjustment. In this work, we develop ShowUI-π, the first flow-based generative model as GUI dexterous hand, featuring the following designs: (i) Unified Discrete-Continuous Actions, integrating discrete clicks and continuous drags within a shared model, enabling flexible adaptation across diverse interaction modes; (ii) Flow-based Action Generation for drag modeling, which predicts incremental cursor adjustments from continuous visual observations via a lightweight action expert, ensuring smooth and stable trajectories; (iii) Drag Training data and Benchmark, where we manually collect and synthesize 20K drag trajectories across five domains (e.g. PowerPoint, Adobe Premiere Pro), and introduce ScreenDrag, a benchmark with comprehensive online and offline evaluation protocols for assessing GUI agents' drag capabilities. Our experiments show that proprietary GUI agents still struggle on ScreenDrag (e.g. Operator scores 13.27, and the best Gemini-2.5-CUA reaches 22.18). In contrast, ShowUI-π achieves 26.98 with only 450M parameters, underscoring both the difficulty of the task and the effectiveness of our approach. We hope this work advances GUI agents toward human-like dexterous control in digital world. The code is available at https://github.com/showlab/showui-pi.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64440be5af034cdfd69ca3a7/_OdaniV8mAw_Y2nQ85G_I.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24965.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64440be5af034cdfd69ca3a7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg",
      "fullname": "Qinghong (Kevin) Lin",
      "name": "KevinQHLin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 41,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "63a553c4ce5763e06f78669c",
      "name": "showlab",
      "fullname": "Show Lab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1671779505215-63a55320ce5763e06f78519c.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.06789",
      "authors": [
        {
          "_id": "69671036c5e371f6b235d143",
          "name": "Qihao Wang",
          "hidden": false
        },
        {
          "_id": "69671036c5e371f6b235d144",
          "name": "Ziming Cheng",
          "hidden": false
        },
        {
          "_id": "69671036c5e371f6b235d145",
          "name": "Shuo Zhang",
          "hidden": false
        },
        {
          "_id": "69671036c5e371f6b235d146",
          "name": "Fan Liu",
          "hidden": false
        },
        {
          "_id": "69671036c5e371f6b235d147",
          "name": "Rui Xu",
          "hidden": false
        },
        {
          "_id": "69671036c5e371f6b235d148",
          "name": "Heng Lian",
          "hidden": false
        },
        {
          "_id": "69671036c5e371f6b235d149",
          "name": "Kunyi Wang",
          "hidden": false
        },
        {
          "_id": "69671036c5e371f6b235d14a",
          "name": "Xiaoming Yu",
          "hidden": false
        },
        {
          "_id": "69671036c5e371f6b235d14b",
          "name": "Jianghao Yin",
          "hidden": false
        },
        {
          "_id": "69671036c5e371f6b235d14c",
          "name": "Sen Hu",
          "hidden": false
        },
        {
          "_id": "69671036c5e371f6b235d14d",
          "name": "Yue Hu",
          "hidden": false
        },
        {
          "_id": "69671036c5e371f6b235d14e",
          "name": "Shaolei Zhang",
          "hidden": false
        },
        {
          "_id": "69671036c5e371f6b235d14f",
          "name": "Yanbing Liu",
          "hidden": false
        },
        {
          "_id": "69671036c5e371f6b235d150",
          "name": "Ronghao Chen",
          "hidden": false
        },
        {
          "_id": "69671036c5e371f6b235d151",
          "name": "Huacan Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-11T06:41:26.000Z",
      "submittedOnDailyAt": "2026-01-14T01:40:39.607Z",
      "title": "MemGovern: Enhancing Code Agents through Learning from Governed Human Experiences",
      "submittedOnDailyBy": {
        "_id": "64084fa192033c150738e4f2",
        "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg",
        "isPro": false,
        "fullname": "Yu_xm",
        "user": "Yu2020",
        "type": "user"
      },
      "summary": "While autonomous software engineering (SWE) agents are reshaping programming paradigms, they currently suffer from a \"closed-world\" limitation: they attempt to fix bugs from scratch or solely using local context, ignoring the immense historical human experience available on platforms like GitHub. Accessing this open-world experience is hindered by the unstructured and fragmented nature of real-world issue-tracking data. In this paper, we introduce MemGovern, a framework designed to govern and transform raw GitHub data into actionable experiential memory for agents. MemGovern employs experience governance to convert human experience into agent-friendly experience cards and introduces an agentic experience search strategy that enables logic-driven retrieval of human expertise. By producing 135K governed experience cards, MemGovern achieves a significant performance boost, improving resolution rates on the SWE-bench Verified by 4.65%. As a plug-in approach, MemGovern provides a solution for agent-friendly memory infrastructure.",
      "upvotes": 16,
      "discussionId": "69671036c5e371f6b235d152",
      "githubRepo": "https://github.com/QuantaAlpha/MemGovern",
      "githubRepoAddedBy": "user",
      "ai_summary": "MemGovern framework transforms unstructured GitHub data into structured experiential memory for autonomous software engineering agents, improving bug resolution rates through enhanced experience retrieval.",
      "ai_keywords": [
        "autonomous software engineering",
        "SWE agents",
        "closed-world limitation",
        "open-world experience",
        "GitHub",
        "experience governance",
        "experience cards",
        "agentic experience search",
        "SWE-bench Verified"
      ],
      "githubStars": 7
    },
    "publishedAt": "2026-01-11T01:41:26.000Z",
    "title": "MemGovern: Enhancing Code Agents through Learning from Governed Human Experiences",
    "summary": "While autonomous software engineering (SWE) agents are reshaping programming paradigms, they currently suffer from a \"closed-world\" limitation: they attempt to fix bugs from scratch or solely using local context, ignoring the immense historical human experience available on platforms like GitHub. Accessing this open-world experience is hindered by the unstructured and fragmented nature of real-world issue-tracking data. In this paper, we introduce MemGovern, a framework designed to govern and transform raw GitHub data into actionable experiential memory for agents. MemGovern employs experience governance to convert human experience into agent-friendly experience cards and introduces an agentic experience search strategy that enables logic-driven retrieval of human expertise. By producing 135K governed experience cards, MemGovern achieves a significant performance boost, improving resolution rates on the SWE-bench Verified by 4.65%. As a plug-in approach, MemGovern provides a solution for agent-friendly memory infrastructure.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06789.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64084fa192033c150738e4f2",
      "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg",
      "fullname": "Yu_xm",
      "name": "Yu2020",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.07264",
      "authors": [
        {
          "_id": "69671055c5e371f6b235d154",
          "name": "Weihao Xuan",
          "hidden": false
        },
        {
          "_id": "69671055c5e371f6b235d155",
          "name": "Qingcheng Zeng",
          "hidden": false
        },
        {
          "_id": "69671055c5e371f6b235d156",
          "name": "Heli Qi",
          "hidden": false
        },
        {
          "_id": "69671055c5e371f6b235d157",
          "name": "Yunze Xiao",
          "hidden": false
        },
        {
          "_id": "69671055c5e371f6b235d158",
          "name": "Junjue Wang",
          "hidden": false
        },
        {
          "_id": "69671055c5e371f6b235d159",
          "name": "Naoto Yokoya",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-12T07:10:35.000Z",
      "submittedOnDailyAt": "2026-01-14T01:18:38.163Z",
      "title": "The Confidence Dichotomy: Analyzing and Mitigating Miscalibration in Tool-Use Agents",
      "submittedOnDailyBy": {
        "_id": "63bc77661374e3ef9135735f",
        "avatarUrl": "/avatars/94b04545ed9d30bfe58691672a0b5618.svg",
        "isPro": false,
        "fullname": "Qingcheng Zeng",
        "user": "qcz",
        "type": "user"
      },
      "summary": "Autonomous agents based on large language models (LLMs) are rapidly evolving to handle multi-turn tasks, but ensuring their trustworthiness remains a critical challenge. A fundamental pillar of this trustworthiness is calibration, which refers to an agent's ability to express confidence that reliably reflects its actual performance. While calibration is well-established for static models, its dynamics in tool-integrated agentic workflows remain underexplored. In this work, we systematically investigate verbalized calibration in tool-use agents, revealing a fundamental confidence dichotomy driven by tool type. Specifically, our pilot study identifies that evidence tools (e.g., web search) systematically induce severe overconfidence due to inherent noise in retrieved information, while verification tools (e.g., code interpreters) can ground reasoning through deterministic feedback and mitigate miscalibration. To robustly improve calibration across tool types, we propose a reinforcement learning (RL) fine-tuning framework that jointly optimizes task accuracy and calibration, supported by a holistic benchmark of reward designs. We demonstrate that our trained agents not only achieve superior calibration but also exhibit robust generalization from local training environments to noisy web settings and to distinct domains such as mathematical reasoning. Our results highlight the necessity of domain-specific calibration strategies for tool-use agents. More broadly, this work establishes a foundation for building self-aware agents that can reliably communicate uncertainty in high-stakes, real-world deployments.",
      "upvotes": 15,
      "discussionId": "69671055c5e371f6b235d15a",
      "ai_summary": "Tool-integrated language model agents exhibit different calibration behaviors based on tool type, with a reinforcement learning framework improving both task accuracy and reliable uncertainty estimation across diverse domains.",
      "ai_keywords": [
        "large language models",
        "tool-integrated agents",
        "calibration",
        "reinforcement learning",
        "reward design",
        "verbalized calibration",
        "evidence tools",
        "verification tools"
      ]
    },
    "publishedAt": "2026-01-12T02:10:35.000Z",
    "title": "The Confidence Dichotomy: Analyzing and Mitigating Miscalibration in Tool-Use Agents",
    "summary": "Autonomous agents based on large language models (LLMs) are rapidly evolving to handle multi-turn tasks, but ensuring their trustworthiness remains a critical challenge. A fundamental pillar of this trustworthiness is calibration, which refers to an agent's ability to express confidence that reliably reflects its actual performance. While calibration is well-established for static models, its dynamics in tool-integrated agentic workflows remain underexplored. In this work, we systematically investigate verbalized calibration in tool-use agents, revealing a fundamental confidence dichotomy driven by tool type. Specifically, our pilot study identifies that evidence tools (e.g., web search) systematically induce severe overconfidence due to inherent noise in retrieved information, while verification tools (e.g., code interpreters) can ground reasoning through deterministic feedback and mitigate miscalibration. To robustly improve calibration across tool types, we propose a reinforcement learning (RL) fine-tuning framework that jointly optimizes task accuracy and calibration, supported by a holistic benchmark of reward designs. We demonstrate that our trained agents not only achieve superior calibration but also exhibit robust generalization from local training environments to noisy web settings and to distinct domains such as mathematical reasoning. Our results highlight the necessity of domain-specific calibration strategies for tool-use agents. More broadly, this work establishes a foundation for building self-aware agents that can reliably communicate uncertainty in high-stakes, real-world deployments.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.07264.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63bc77661374e3ef9135735f",
      "avatarUrl": "/avatars/94b04545ed9d30bfe58691672a0b5618.svg",
      "fullname": "Qingcheng Zeng",
      "name": "qcz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.06487",
      "authors": [
        {
          "_id": "69660763fc8c4ecc02c7fab6",
          "name": "Qiang Zhang",
          "hidden": false
        },
        {
          "_id": "69660763fc8c4ecc02c7fab7",
          "user": {
            "_id": "61454d930aac1efe3e8d842e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61454d930aac1efe3e8d842e/sQELHgJP6c_547OrT8JZJ.jpeg",
            "isPro": false,
            "fullname": "boli",
            "user": "bcol",
            "type": "user"
          },
          "name": "Boli Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-13T15:45:22.457Z",
          "hidden": false
        },
        {
          "_id": "69660763fc8c4ecc02c7fab8",
          "name": "Fanrui Zhang",
          "hidden": false
        },
        {
          "_id": "69660763fc8c4ecc02c7fab9",
          "name": "Ruixue Ding",
          "hidden": false
        },
        {
          "_id": "69660763fc8c4ecc02c7faba",
          "name": "Shihang Wang",
          "hidden": false
        },
        {
          "_id": "69660763fc8c4ecc02c7fabb",
          "name": "Qiuchen Wang",
          "hidden": false
        },
        {
          "_id": "69660763fc8c4ecc02c7fabc",
          "name": "Yinfeng Huang",
          "hidden": false
        },
        {
          "_id": "69660763fc8c4ecc02c7fabd",
          "name": "Haonan Zhang",
          "hidden": false
        },
        {
          "_id": "69660763fc8c4ecc02c7fabe",
          "name": "Rongxiang Zhu",
          "hidden": false
        },
        {
          "_id": "69660763fc8c4ecc02c7fabf",
          "name": "Pengyong Wang",
          "hidden": false
        },
        {
          "_id": "69660763fc8c4ecc02c7fac0",
          "name": "Ailin Ren",
          "hidden": false
        },
        {
          "_id": "69660763fc8c4ecc02c7fac1",
          "name": "Xin Li",
          "hidden": false
        },
        {
          "_id": "69660763fc8c4ecc02c7fac2",
          "name": "Pengjun Xie",
          "hidden": false
        },
        {
          "_id": "69660763fc8c4ecc02c7fac3",
          "name": "Jiawei Liu",
          "hidden": false
        },
        {
          "_id": "69660763fc8c4ecc02c7fac4",
          "name": "Ning Guo",
          "hidden": false
        },
        {
          "_id": "69660763fc8c4ecc02c7fac5",
          "name": "Jingren Zhou",
          "hidden": false
        },
        {
          "_id": "69660763fc8c4ecc02c7fac6",
          "name": "Zheng-Jun Zha",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-10T08:43:07.000Z",
      "submittedOnDailyAt": "2026-01-14T01:12:42.456Z",
      "title": "ArenaRL: Scaling RL for Open-Ended Agents via Tournament-based Relative Ranking",
      "submittedOnDailyBy": {
        "_id": "657429d833e5a4bf5b278615",
        "avatarUrl": "/avatars/ed7e28c1b9a7bed1cad864c992cdcc69.svg",
        "isPro": false,
        "fullname": "QiuchenWang",
        "user": "Qiuchen-Wang",
        "type": "user"
      },
      "summary": "Reinforcement learning has substantially improved the performance of LLM agents on tasks with verifiable outcomes, but it still struggles on open-ended agent tasks with vast solution spaces (e.g., complex travel planning). Due to the absence of objective ground-truth for these tasks, current RL algorithms largely rely on reward models that assign scalar scores to individual responses. We contend that such pointwise scoring suffers from an inherent discrimination collapse: the reward model struggles to distinguish subtle advantages among different trajectories, resulting in scores within a group being compressed into a narrow range. Consequently, the effective reward signal becomes dominated by noise from the reward model, leading to optimization stagnation. To address this, we propose ArenaRL, a reinforcement learning paradigm that shifts from pointwise scalar scoring to intra-group relative ranking. ArenaRL introduces a process-aware pairwise evaluation mechanism, employing multi-level rubrics to assign fine-grained relative scores to trajectories. Additionally, we construct an intra-group adversarial arena and devise a tournament-based ranking scheme to obtain stable advantage signals. Empirical results confirm that the built seeded single-elimination scheme achieves nearly equivalent advantage estimation accuracy to full pairwise comparisons with O(N^2) complexity, while operating with only O(N) complexity, striking an optimal balance between efficiency and precision. Furthermore, to address the lack of full-cycle benchmarks for open-ended agents, we build Open-Travel and Open-DeepResearch, two high-quality benchmarks featuring a comprehensive pipeline covering SFT, RL training, and multi-dimensional evaluation. Extensive experiments show that ArenaRL substantially outperforms standard RL baselines, enabling LLM agents to generate more robust solutions for complex real-world tasks.",
      "upvotes": 15,
      "discussionId": "69660764fc8c4ecc02c7fac7",
      "projectPage": "https://tongyi-agent.github.io/blog/arenarl/",
      "githubRepo": "https://github.com/Alibaba-NLP/qqr",
      "githubRepoAddedBy": "user",
      "ai_summary": "Reinforcement learning for large language model agents suffers from discrimination collapse in open-ended tasks due to pointwise scalar scoring, which ArenaRL addresses through relative ranking and pairwise evaluation mechanisms.",
      "ai_keywords": [
        "reinforcement learning",
        "large language model agents",
        "reward models",
        "scalar scores",
        "discrimination collapse",
        "intra-group relative ranking",
        "pairwise evaluation",
        "multi-level rubrics",
        "adversarial arena",
        "tournament-based ranking",
        "advantage estimation",
        "efficiency",
        "precision",
        "benchmarks",
        "SFT",
        "RL training",
        "multi-dimensional evaluation"
      ],
      "githubStars": 7,
      "organization": {
        "_id": "661f98de142a51d630dbbcc4",
        "name": "Alibaba-NLP",
        "fullname": "Alibaba-NLP",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63fc4c00a3c067e62899d32b/dfd_EcIfylvu3sdc2WMqX.png"
      }
    },
    "publishedAt": "2026-01-10T03:43:07.000Z",
    "title": "ArenaRL: Scaling RL for Open-Ended Agents via Tournament-based Relative Ranking",
    "summary": "Reinforcement learning has substantially improved the performance of LLM agents on tasks with verifiable outcomes, but it still struggles on open-ended agent tasks with vast solution spaces (e.g., complex travel planning). Due to the absence of objective ground-truth for these tasks, current RL algorithms largely rely on reward models that assign scalar scores to individual responses. We contend that such pointwise scoring suffers from an inherent discrimination collapse: the reward model struggles to distinguish subtle advantages among different trajectories, resulting in scores within a group being compressed into a narrow range. Consequently, the effective reward signal becomes dominated by noise from the reward model, leading to optimization stagnation. To address this, we propose ArenaRL, a reinforcement learning paradigm that shifts from pointwise scalar scoring to intra-group relative ranking. ArenaRL introduces a process-aware pairwise evaluation mechanism, employing multi-level rubrics to assign fine-grained relative scores to trajectories. Additionally, we construct an intra-group adversarial arena and devise a tournament-based ranking scheme to obtain stable advantage signals. Empirical results confirm that the built seeded single-elimination scheme achieves nearly equivalent advantage estimation accuracy to full pairwise comparisons with O(N^2) complexity, while operating with only O(N) complexity, striking an optimal balance between efficiency and precision. Furthermore, to address the lack of full-cycle benchmarks for open-ended agents, we build Open-Travel and Open-DeepResearch, two high-quality benchmarks featuring a comprehensive pipeline covering SFT, RL training, and multi-dimensional evaluation. Extensive experiments show that ArenaRL substantially outperforms standard RL baselines, enabling LLM agents to generate more robust solutions for complex real-world tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06487.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "657429d833e5a4bf5b278615",
      "avatarUrl": "/avatars/ed7e28c1b9a7bed1cad864c992cdcc69.svg",
      "fullname": "QiuchenWang",
      "name": "Qiuchen-Wang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "661f98de142a51d630dbbcc4",
      "name": "Alibaba-NLP",
      "fullname": "Alibaba-NLP",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63fc4c00a3c067e62899d32b/dfd_EcIfylvu3sdc2WMqX.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.08584",
      "authors": [
        {
          "_id": "6967031ec5e371f6b235d068",
          "name": "Alexander H. Liu",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d069",
          "name": "Kartik Khandelwal",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d06a",
          "name": "Sandeep Subramanian",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d06b",
          "name": "Victor Jouault",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d06c",
          "name": "Abhinav Rastogi",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d06d",
          "name": "Adrien Sadé",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d06e",
          "name": "Alan Jeffares",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d06f",
          "name": "Albert Jiang",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d070",
          "name": "Alexandre Cahill",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d071",
          "name": "Alexandre Gavaudan",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d072",
          "name": "Alexandre Sablayrolles",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d073",
          "name": "Amélie Héliou",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d074",
          "name": "Amos You",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d075",
          "name": "Andy Ehrenberg",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d076",
          "name": "Andy Lo",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d077",
          "name": "Anton Eliseev",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d078",
          "name": "Antonia Calvi",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d079",
          "name": "Avinash Sooriyarachchi",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d07a",
          "name": "Baptiste Bout",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d07b",
          "name": "Baptiste Rozière",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d07c",
          "name": "Baudouin De Monicault",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d07d",
          "name": "Clémence Lanfranchi",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d07e",
          "name": "Corentin Barreau",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d07f",
          "name": "Cyprien Courtot",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d080",
          "name": "Daniele Grattarola",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d081",
          "name": "Darius Dabert",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d082",
          "name": "Diego de las Casas",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d083",
          "name": "Elliot Chane-Sane",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d084",
          "name": "Faruk Ahmed",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d085",
          "name": "Gabrielle Berrada",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d086",
          "name": "Gaëtan Ecrepont",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d087",
          "name": "Gauthier Guinet",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d088",
          "name": "Georgii Novikov",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d089",
          "name": "Guillaume Kunsch",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d08a",
          "name": "Guillaume Lample",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d08b",
          "name": "Guillaume Martin",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d08c",
          "name": "Gunshi Gupta",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d08d",
          "name": "Jan Ludziejewski",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d08e",
          "name": "Jason Rute",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d08f",
          "name": "Joachim Studnia",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d090",
          "name": "Jonas Amar",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d091",
          "name": "Joséphine Delas",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d092",
          "name": "Josselin Somerville Roberts",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d093",
          "name": "Karmesh Yadav",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d094",
          "name": "Khyathi Chandu",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d095",
          "name": "Kush Jain",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d096",
          "name": "Laurence Aitchison",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d097",
          "name": "Laurent Fainsin",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d098",
          "name": "Léonard Blier",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d099",
          "name": "Lingxiao Zhao",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d09a",
          "name": "Louis Martin",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d09b",
          "name": "Lucile Saulnier",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d09c",
          "name": "Luyu Gao",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d09d",
          "name": "Maarten Buyl",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d09e",
          "name": "Margaret Jennings",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d09f",
          "name": "Marie Pellat",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d0a0",
          "name": "Mark Prins",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d0a1",
          "name": "Mathieu Poirée",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d0a2",
          "name": "Mathilde Guillaumin",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d0a3",
          "name": "Matthieu Dinot",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d0a4",
          "name": "Matthieu Futeral",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d0a5",
          "name": "Maxime Darrin",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d0a6",
          "name": "Maximilian Augustin",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d0a7",
          "name": "Mia Chiquier",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d0a8",
          "name": "Michel Schimpf",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d0a9",
          "name": "Nathan Grinsztajn",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d0aa",
          "name": "Neha Gupta",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d0ab",
          "name": "Nikhil Raghuraman",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d0ac",
          "name": "Olivier Bousquet",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d0ad",
          "name": "Olivier Duchenne",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d0ae",
          "name": "Patricia Wang",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d0af",
          "name": "Patrick von Platen",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d0b0",
          "name": "Paul Jacob",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d0b1",
          "name": "Paul Wambergue",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d0b2",
          "name": "Paula Kurylowicz",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d0b3",
          "name": "Pavankumar Reddy Muddireddy",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d0b4",
          "name": "Philomène Chagniot",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d0b5",
          "name": "Pierre Stock",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d0b6",
          "name": "Pravesh Agrawal",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d0b7",
          "name": "Quentin Torroba",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d0b8",
          "name": "Romain Sauvestre",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d0b9",
          "name": "Roman Soletskyi",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d0ba",
          "name": "Rupert Menneer",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d0bb",
          "name": "Sagar Vaze",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d0bc",
          "name": "Samuel Barry",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d0bd",
          "name": "Sanchit Gandhi",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d0be",
          "name": "Siddhant Waghjale",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d0bf",
          "name": "Siddharth Gandhi",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d0c0",
          "name": "Soham Ghosh",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d0c1",
          "name": "Srijan Mishra",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d0c2",
          "name": "Sumukh Aithal",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d0c3",
          "name": "Szymon Antoniak",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d0c4",
          "name": "Teven Le Scao",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d0c5",
          "name": "Théo Cachet",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d0c6",
          "name": "Theo Simon Sorg",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d0c7",
          "name": "Thibaut Lavril",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d0c8",
          "name": "Thiziri Nait Saada",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d0c9",
          "name": "Thomas Chabal",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d0ca",
          "name": "Thomas Foubert",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d0cb",
          "name": "Thomas Robert",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d0cc",
          "name": "Thomas Wang",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d0cd",
          "name": "Tim Lawson",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d0ce",
          "name": "Tom Bewley",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d0cf",
          "name": "Tom Bewley",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d0d0",
          "name": "Tom Edwards",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d0d1",
          "name": "Umar Jamil",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d0d2",
          "name": "Umberto Tomasini",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d0d3",
          "name": "Valeriia Nemychnikova",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d0d4",
          "name": "Van Phung",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d0d5",
          "name": "Vincent Maladière",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d0d6",
          "name": "Virgile Richard",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d0d7",
          "name": "Wassim Bouaziz",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d0d8",
          "name": "Wen-Ding Li",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d0d9",
          "name": "William Marshall",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d0da",
          "name": "Xinghui Li",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d0db",
          "name": "Xinyu Yang",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d0dc",
          "name": "Yassine El Ouahidi",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d0dd",
          "name": "Yihan Wang",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d0de",
          "name": "Yunhao Tang",
          "hidden": false
        },
        {
          "_id": "6967031ec5e371f6b235d0df",
          "name": "Zaccharie Ramzi",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-13T14:06:03.000Z",
      "submittedOnDailyAt": "2026-01-14T00:14:55.855Z",
      "title": "Ministral 3",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We introduce the Ministral 3 series, a family of parameter-efficient dense language models designed for compute and memory constrained applications, available in three model sizes: 3B, 8B, and 14B parameters. For each model size, we release three variants: a pretrained base model for general-purpose use, an instruction finetuned, and a reasoning model for complex problem-solving. In addition, we present our recipe to derive the Ministral 3 models through Cascade Distillation, an iterative pruning and continued training with distillation technique. Each model comes with image understanding capabilities, all under the Apache 2.0 license.",
      "upvotes": 9,
      "discussionId": "6967031ec5e371f6b235d0e0",
      "ai_summary": "The Ministral 3 series consists of parameter-efficient dense language models with three sizes (3B, 8B, 14B) and three variants per size, trained using cascade distillation for compute-constrained applications.",
      "ai_keywords": [
        "parameter-efficient dense language models",
        "cascade distillation",
        "iterative pruning",
        "continued training",
        "distillation technique",
        "instruction finetuned",
        "reasoning model",
        "image understanding capabilities"
      ],
      "organization": {
        "_id": "64edf4004f42c35eea1b1632",
        "name": "mistralai",
        "fullname": "Mistral AI_",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/634c17653d11eaedd88b314d/9OgyfKstSZtbmsmuG8MbU.png"
      }
    },
    "publishedAt": "2026-01-13T09:06:03.000Z",
    "title": "Ministral 3",
    "summary": "We introduce the Ministral 3 series, a family of parameter-efficient dense language models designed for compute and memory constrained applications, available in three model sizes: 3B, 8B, and 14B parameters. For each model size, we release three variants: a pretrained base model for general-purpose use, an instruction finetuned, and a reasoning model for complex problem-solving. In addition, we present our recipe to derive the Ministral 3 models through Cascade Distillation, an iterative pruning and continued training with distillation technique. Each model comes with image understanding capabilities, all under the Apache 2.0 license.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.08584.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 207,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "64edf4004f42c35eea1b1632",
      "name": "mistralai",
      "fullname": "Mistral AI_",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/634c17653d11eaedd88b314d/9OgyfKstSZtbmsmuG8MbU.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.08828",
      "authors": [
        {
          "_id": "696707d0c5e371f6b235d0fc",
          "name": "Xindi Wu",
          "hidden": false
        },
        {
          "_id": "696707d0c5e371f6b235d0fd",
          "name": "Despoina Paschalidou",
          "hidden": false
        },
        {
          "_id": "696707d0c5e371f6b235d0fe",
          "name": "Jun Gao",
          "hidden": false
        },
        {
          "_id": "696707d0c5e371f6b235d0ff",
          "name": "Antonio Torralba",
          "hidden": false
        },
        {
          "_id": "696707d0c5e371f6b235d100",
          "name": "Laura Leal-Taixé",
          "hidden": false
        },
        {
          "_id": "696707d0c5e371f6b235d101",
          "name": "Olga Russakovsky",
          "hidden": false
        },
        {
          "_id": "696707d0c5e371f6b235d102",
          "name": "Sanja Fidler",
          "hidden": false
        },
        {
          "_id": "696707d0c5e371f6b235d103",
          "name": "Jonathan Lorraine",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-13T18:59:09.000Z",
      "submittedOnDailyAt": "2026-01-14T00:35:02.653Z",
      "title": "Motion Attribution for Video Generation",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Despite the rapid progress of video generation models, the role of data in influencing motion is poorly understood. We present Motive (MOTIon attribution for Video gEneration), a motion-centric, gradient-based data attribution framework that scales to modern, large, high-quality video datasets and models. We use this to study which fine-tuning clips improve or degrade temporal dynamics. Motive isolates temporal dynamics from static appearance via motion-weighted loss masks, yielding efficient and scalable motion-specific influence computation. On text-to-video models, Motive identifies clips that strongly affect motion and guides data curation that improves temporal consistency and physical plausibility. With Motive-selected high-influence data, our method improves both motion smoothness and dynamic degree on VBench, achieving a 74.1% human preference win rate compared with the pretrained base model. To our knowledge, this is the first framework to attribute motion rather than visual appearance in video generative models and to use it to curate fine-tuning data.",
      "upvotes": 3,
      "discussionId": "696707d0c5e371f6b235d104",
      "projectPage": "https://research.nvidia.com/labs/sil/projects/MOTIVE/",
      "ai_summary": "Motive is a gradient-based data attribution framework that identifies influential video clips for motion improvement in text-to-video models through motion-weighted loss masking.",
      "ai_keywords": [
        "video generation models",
        "data attribution",
        "gradient-based",
        "motion-centric",
        "temporal dynamics",
        "motion-weighted loss masks",
        "fine-tuning",
        "text-to-video models",
        "VBench",
        "motion smoothness",
        "dynamic degree"
      ],
      "organization": {
        "_id": "60262b67268c201cdc8b7d43",
        "name": "nvidia",
        "fullname": "NVIDIA",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
      }
    },
    "publishedAt": "2026-01-13T13:59:09.000Z",
    "title": "Motion Attribution for Video Generation",
    "summary": "Despite the rapid progress of video generation models, the role of data in influencing motion is poorly understood. We present Motive (MOTIon attribution for Video gEneration), a motion-centric, gradient-based data attribution framework that scales to modern, large, high-quality video datasets and models. We use this to study which fine-tuning clips improve or degrade temporal dynamics. Motive isolates temporal dynamics from static appearance via motion-weighted loss masks, yielding efficient and scalable motion-specific influence computation. On text-to-video models, Motive identifies clips that strongly affect motion and guides data curation that improves temporal consistency and physical plausibility. With Motive-selected high-influence data, our method improves both motion smoothness and dynamic degree on VBench, achieving a 74.1% human preference win rate compared with the pretrained base model. To our knowledge, this is the first framework to attribute motion rather than visual appearance in video generative models and to use it to curate fine-tuning data.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.08828.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 207,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "60262b67268c201cdc8b7d43",
      "name": "nvidia",
      "fullname": "NVIDIA",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.08587",
      "authors": [
        {
          "_id": "696705cec5e371f6b235d0f4",
          "name": "Zhengbo Xu",
          "hidden": false
        },
        {
          "_id": "696705cec5e371f6b235d0f5",
          "name": "Jie Ma",
          "hidden": false
        },
        {
          "_id": "696705cec5e371f6b235d0f6",
          "name": "Ziheng Wang",
          "hidden": false
        },
        {
          "_id": "696705cec5e371f6b235d0f7",
          "name": "Zhan Peng",
          "hidden": false
        },
        {
          "_id": "696705cec5e371f6b235d0f8",
          "name": "Jun Liang",
          "hidden": false
        },
        {
          "_id": "696705cec5e371f6b235d0f9",
          "name": "Jing Li",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-13T14:10:34.000Z",
      "submittedOnDailyAt": "2026-01-14T00:27:48.852Z",
      "title": "End-to-End Video Character Replacement without Structural Guidance",
      "submittedOnDailyBy": {
        "_id": "640d3eaa3623f6a56dde856d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678589663024-640d3eaa3623f6a56dde856d.jpeg",
        "isPro": true,
        "fullname": "vansin",
        "user": "vansin",
        "type": "user"
      },
      "summary": "Controllable video character replacement with a user-provided identity remains a challenging problem due to the lack of paired video data. Prior works have predominantly relied on a reconstruction-based paradigm that requires per-frame segmentation masks and explicit structural guidance (e.g., skeleton, depth). This reliance, however, severely limits their generalizability in complex scenarios involving occlusions, character-object interactions, unusual poses, or challenging illumination, often leading to visual artifacts and temporal inconsistencies. In this paper, we propose MoCha, a pioneering framework that bypasses these limitations by requiring only a single arbitrary frame mask. To effectively adapt the multi-modal input condition and enhance facial identity, we introduce a condition-aware RoPE and employ an RL-based post-training stage. Furthermore, to overcome the scarcity of qualified paired-training data, we propose a comprehensive data construction pipeline. Specifically, we design three specialized datasets: a high-fidelity rendered dataset built with Unreal Engine 5 (UE5), an expression-driven dataset synthesized by current portrait animation techniques, and an augmented dataset derived from existing video-mask pairs. Extensive experiments demonstrate that our method substantially outperforms existing state-of-the-art approaches. We will release the code to facilitate further research. Please refer to our project page for more details: orange-3dv-team.github.io/MoCha",
      "upvotes": 3,
      "discussionId": "696705cfc5e371f6b235d0fa",
      "ai_summary": "MoCha enables controllable video character replacement using a single frame mask through condition-aware RoPE and a comprehensive data construction pipeline with specialized datasets.",
      "ai_keywords": [
        "controllable video character replacement",
        "reconstruction-based paradigm",
        "per-frame segmentation masks",
        "structural guidance",
        "facial identity",
        "condition-aware RoPE",
        "RL-based post-training",
        "data construction pipeline",
        "high-fidelity rendered dataset",
        "expression-driven dataset",
        "augmented dataset"
      ]
    },
    "publishedAt": "2026-01-13T09:10:34.000Z",
    "title": "End-to-End Video Character Replacement without Structural Guidance",
    "summary": "Controllable video character replacement with a user-provided identity remains a challenging problem due to the lack of paired video data. Prior works have predominantly relied on a reconstruction-based paradigm that requires per-frame segmentation masks and explicit structural guidance (e.g., skeleton, depth). This reliance, however, severely limits their generalizability in complex scenarios involving occlusions, character-object interactions, unusual poses, or challenging illumination, often leading to visual artifacts and temporal inconsistencies. In this paper, we propose MoCha, a pioneering framework that bypasses these limitations by requiring only a single arbitrary frame mask. To effectively adapt the multi-modal input condition and enhance facial identity, we introduce a condition-aware RoPE and employ an RL-based post-training stage. Furthermore, to overcome the scarcity of qualified paired-training data, we propose a comprehensive data construction pipeline. Specifically, we design three specialized datasets: a high-fidelity rendered dataset built with Unreal Engine 5 (UE5), an expression-driven dataset synthesized by current portrait animation techniques, and an augmented dataset derived from existing video-mask pairs. Extensive experiments demonstrate that our method substantially outperforms existing state-of-the-art approaches. We will release the code to facilitate further research. Please refer to our project page for more details: orange-3dv-team.github.io/MoCha",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.08587.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "640d3eaa3623f6a56dde856d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678589663024-640d3eaa3623f6a56dde856d.jpeg",
      "fullname": "vansin",
      "name": "vansin",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 36,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.08665",
      "authors": [
        {
          "_id": "69670f41c5e371f6b235d124",
          "name": "Shaoan Wang",
          "hidden": false
        },
        {
          "_id": "69670f41c5e371f6b235d125",
          "name": "Yuanfei Luo",
          "hidden": false
        },
        {
          "_id": "69670f41c5e371f6b235d126",
          "name": "Xingyu Chen",
          "hidden": false
        },
        {
          "_id": "69670f41c5e371f6b235d127",
          "name": "Aocheng Luo",
          "hidden": false
        },
        {
          "_id": "69670f41c5e371f6b235d128",
          "name": "Dongyue Li",
          "hidden": false
        },
        {
          "_id": "69670f41c5e371f6b235d129",
          "name": "Chang Liu",
          "hidden": false
        },
        {
          "_id": "69670f41c5e371f6b235d12a",
          "name": "Sheng Chen",
          "hidden": false
        },
        {
          "_id": "69670f41c5e371f6b235d12b",
          "name": "Yangang Zhang",
          "hidden": false
        },
        {
          "_id": "69670f41c5e371f6b235d12c",
          "name": "Junzhi Yu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/2jcT5yoG-xs87FKOi4SnB.mp4"
      ],
      "publishedAt": "2026-01-13T15:43:43.000Z",
      "submittedOnDailyAt": "2026-01-14T01:06:58.251Z",
      "title": "VLingNav: Embodied Navigation with Adaptive Reasoning and Visual-Assisted Linguistic Memory",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "VLA models have shown promising potential in embodied navigation by unifying perception and planning while inheriting the strong generalization abilities of large VLMs. However, most existing VLA models rely on reactive mappings directly from observations to actions, lacking the explicit reasoning capabilities and persistent memory required for complex, long-horizon navigation tasks. To address these challenges, we propose VLingNav, a VLA model for embodied navigation grounded in linguistic-driven cognition. First, inspired by the dual-process theory of human cognition, we introduce an adaptive chain-of-thought mechanism, which dynamically triggers explicit reasoning only when necessary, enabling the agent to fluidly switch between fast, intuitive execution and slow, deliberate planning. Second, to handle long-horizon spatial dependencies, we develop a visual-assisted linguistic memory module that constructs a persistent, cross-modal semantic memory, enabling the agent to recall past observations to prevent repetitive exploration and infer movement trends for dynamic environments. For the training recipe, we construct Nav-AdaCoT-2.9M, the largest embodied navigation dataset with reasoning annotations to date, enriched with adaptive CoT annotations that induce a reasoning paradigm capable of adjusting both when to think and what to think about. Moreover, we incorporate an online expert-guided reinforcement learning stage, enabling the model to surpass pure imitation learning and to acquire more robust, self-explored navigation behaviors. Extensive experiments demonstrate that VLingNav achieves state-of-the-art performance across a wide range of embodied navigation benchmarks. Notably, VLingNav transfers to real-world robotic platforms in a zero-shot manner, executing various navigation tasks and demonstrating strong cross-domain and cross-task generalization.",
      "upvotes": 2,
      "discussionId": "69670f41c5e371f6b235d12d",
      "projectPage": "https://wsakobe.github.io/VLingNav-web/",
      "ai_summary": "VLingNav enhances embodied navigation through linguistic-driven cognition with adaptive reasoning and visual-assisted memory, achieving state-of-the-art performance and zero-shot transfer to real robots.",
      "ai_keywords": [
        "VLA models",
        "embodied navigation",
        "large VLMs",
        "dual-process theory",
        "chain-of-thought mechanism",
        "visual-assisted linguistic memory",
        "cross-modal semantic memory",
        "Nav-AdaCoT-2.9M",
        "online expert-guided reinforcement learning",
        "zero-shot transfer"
      ],
      "organization": {
        "_id": "67d1140985ea0644e2f14b99",
        "name": "ByteDance-Seed",
        "fullname": "ByteDance Seed",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
      }
    },
    "publishedAt": "2026-01-13T10:43:43.000Z",
    "title": "VLingNav: Embodied Navigation with Adaptive Reasoning and Visual-Assisted Linguistic Memory",
    "summary": "VLA models have shown promising potential in embodied navigation by unifying perception and planning while inheriting the strong generalization abilities of large VLMs. However, most existing VLA models rely on reactive mappings directly from observations to actions, lacking the explicit reasoning capabilities and persistent memory required for complex, long-horizon navigation tasks. To address these challenges, we propose VLingNav, a VLA model for embodied navigation grounded in linguistic-driven cognition. First, inspired by the dual-process theory of human cognition, we introduce an adaptive chain-of-thought mechanism, which dynamically triggers explicit reasoning only when necessary, enabling the agent to fluidly switch between fast, intuitive execution and slow, deliberate planning. Second, to handle long-horizon spatial dependencies, we develop a visual-assisted linguistic memory module that constructs a persistent, cross-modal semantic memory, enabling the agent to recall past observations to prevent repetitive exploration and infer movement trends for dynamic environments. For the training recipe, we construct Nav-AdaCoT-2.9M, the largest embodied navigation dataset with reasoning annotations to date, enriched with adaptive CoT annotations that induce a reasoning paradigm capable of adjusting both when to think and what to think about. Moreover, we incorporate an online expert-guided reinforcement learning stage, enabling the model to surpass pure imitation learning and to acquire more robust, self-explored navigation behaviors. Extensive experiments demonstrate that VLingNav achieves state-of-the-art performance across a wide range of embodied navigation benchmarks. Notably, VLingNav transfers to real-world robotic platforms in a zero-shot manner, executing various navigation tasks and demonstrating strong cross-domain and cross-task generalization.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/2jcT5yoG-xs87FKOi4SnB.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.08665.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 207,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "67d1140985ea0644e2f14b99",
      "name": "ByteDance-Seed",
      "fullname": "ByteDance Seed",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.08468",
      "authors": [
        {
          "_id": "69670378c5e371f6b235d0e2",
          "name": "Jiangshan Duo",
          "hidden": false
        },
        {
          "_id": "69670378c5e371f6b235d0e3",
          "name": "Hanyu Li",
          "hidden": false
        },
        {
          "_id": "69670378c5e371f6b235d0e4",
          "name": "Hailin Zhang",
          "hidden": false
        },
        {
          "_id": "69670378c5e371f6b235d0e5",
          "name": "Yudong Wang",
          "hidden": false
        },
        {
          "_id": "69670378c5e371f6b235d0e6",
          "name": "Sujian Li",
          "hidden": false
        },
        {
          "_id": "69670378c5e371f6b235d0e7",
          "name": "Liang Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-13T11:47:42.000Z",
      "submittedOnDailyAt": "2026-01-14T00:16:30.844Z",
      "title": "JudgeRLVR: Judge First, Generate Second for Efficient Reasoning",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has become a standard paradigm for reasoning in Large Language Models. However, optimizing solely for final-answer correctness often drives models into aimless, verbose exploration, where they rely on exhaustive trial-and-error tactics rather than structured planning to reach solutions. While heuristic constraints like length penalties can reduce verbosity, they often truncate essential reasoning steps, creating a difficult trade-off between efficiency and verification. In this paper, we argue that discriminative capability is a prerequisite for efficient generation: by learning to distinguish valid solutions, a model can internalize a guidance signal that prunes the search space. We propose JudgeRLVR, a two-stage judge-then-generate paradigm. In the first stage, we train the model to judge solution responses with verifiable answers. In the second stage, we fine-tune the same model with vanilla generating RLVR initialized from the judge. Compared to Vanilla RLVR using the same math-domain training data, JudgeRLVR achieves a better quality--efficiency trade-off for Qwen3-30B-A3B: on in-domain math, it delivers about +3.7 points average accuracy gain with -42\\% average generation length; on out-of-domain benchmarks, it delivers about +4.5 points average accuracy improvement, demonstrating enhanced generalization.",
      "upvotes": 0,
      "discussionId": "69670378c5e371f6b235d0e8",
      "ai_summary": "Reinforcement learning with verifiable rewards is enhanced through a judge-then-generate paradigm that improves both efficiency and accuracy in mathematical problem-solving.",
      "ai_keywords": [
        "reinforcement learning",
        "verifiable rewards",
        "large language models",
        "discriminative capability",
        "judge-then-generate paradigm",
        "fine-tuning",
        "vanilla RLVR",
        "Qwen3-30B-A3B",
        "mathematical problem-solving"
      ]
    },
    "publishedAt": "2026-01-13T06:47:42.000Z",
    "title": "JudgeRLVR: Judge First, Generate Second for Efficient Reasoning",
    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has become a standard paradigm for reasoning in Large Language Models. However, optimizing solely for final-answer correctness often drives models into aimless, verbose exploration, where they rely on exhaustive trial-and-error tactics rather than structured planning to reach solutions. While heuristic constraints like length penalties can reduce verbosity, they often truncate essential reasoning steps, creating a difficult trade-off between efficiency and verification. In this paper, we argue that discriminative capability is a prerequisite for efficient generation: by learning to distinguish valid solutions, a model can internalize a guidance signal that prunes the search space. We propose JudgeRLVR, a two-stage judge-then-generate paradigm. In the first stage, we train the model to judge solution responses with verifiable answers. In the second stage, we fine-tune the same model with vanilla generating RLVR initialized from the judge. Compared to Vanilla RLVR using the same math-domain training data, JudgeRLVR achieves a better quality--efficiency trade-off for Qwen3-30B-A3B: on in-domain math, it delivers about +3.7 points average accuracy gain with -42\\% average generation length; on out-of-domain benchmarks, it delivers about +4.5 points average accuracy improvement, demonstrating enhanced generalization.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.08468.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 207,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.08303",
      "authors": [
        {
          "_id": "696708d3c5e371f6b235d10c",
          "name": "Dongting Hu",
          "hidden": false
        },
        {
          "_id": "696708d3c5e371f6b235d10d",
          "name": "Aarush Gupta",
          "hidden": false
        },
        {
          "_id": "696708d3c5e371f6b235d10e",
          "name": "Magzhan Gabidolla",
          "hidden": false
        },
        {
          "_id": "696708d3c5e371f6b235d10f",
          "name": "Arpit Sahni",
          "hidden": false
        },
        {
          "_id": "696708d3c5e371f6b235d110",
          "name": "Huseyin Coskun",
          "hidden": false
        },
        {
          "_id": "696708d3c5e371f6b235d111",
          "name": "Yanyu Li",
          "hidden": false
        },
        {
          "_id": "696708d3c5e371f6b235d112",
          "name": "Yerlan Idelbayev",
          "hidden": false
        },
        {
          "_id": "696708d3c5e371f6b235d113",
          "name": "Ahsan Mahmood",
          "hidden": false
        },
        {
          "_id": "696708d3c5e371f6b235d114",
          "name": "Aleksei Lebedev",
          "hidden": false
        },
        {
          "_id": "696708d3c5e371f6b235d115",
          "name": "Dishani Lahiri",
          "hidden": false
        },
        {
          "_id": "696708d3c5e371f6b235d116",
          "name": "Anujraaj Goyal",
          "hidden": false
        },
        {
          "_id": "696708d3c5e371f6b235d117",
          "name": "Ju Hu",
          "hidden": false
        },
        {
          "_id": "696708d3c5e371f6b235d118",
          "name": "Mingming Gong",
          "hidden": false
        },
        {
          "_id": "696708d3c5e371f6b235d119",
          "name": "Sergey Tulyakov",
          "hidden": false
        },
        {
          "_id": "696708d3c5e371f6b235d11a",
          "name": "Anil Kag",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-13T07:46:46.000Z",
      "submittedOnDailyAt": "2026-01-14T00:39:31.241Z",
      "title": "SnapGen++: Unleashing Diffusion Transformers for Efficient High-Fidelity Image Generation on Edge Devices",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Recent advances in diffusion transformers (DiTs) have set new standards in image generation, yet remain impractical for on-device deployment due to their high computational and memory costs. In this work, we present an efficient DiT framework tailored for mobile and edge devices that achieves transformer-level generation quality under strict resource constraints. Our design combines three key components. First, we propose a compact DiT architecture with an adaptive global-local sparse attention mechanism that balances global context modeling and local detail preservation. Second, we propose an elastic training framework that jointly optimizes sub-DiTs of varying capacities within a unified supernetwork, allowing a single model to dynamically adjust for efficient inference across different hardware. Finally, we develop Knowledge-Guided Distribution Matching Distillation, a step-distillation pipeline that integrates the DMD objective with knowledge transfer from few-step teacher models, producing high-fidelity and low-latency generation (e.g., 4-step) suitable for real-time on-device use. Together, these contributions enable scalable, efficient, and high-quality diffusion models for deployment on diverse hardware.",
      "upvotes": 0,
      "discussionId": "696708d3c5e371f6b235d11b",
      "ai_summary": "An efficient diffusion transformer framework for mobile and edge devices that maintains high-generation quality while reducing computational costs through compact architecture, elastic training, and knowledge-guided distillation.",
      "ai_keywords": [
        "diffusion transformers",
        "DiTs",
        "sparse attention mechanism",
        "elastic training",
        "supernetwork",
        "knowledge-guided distribution matching distillation",
        "step-distillation pipeline",
        "teacher models",
        "high-fidelity generation",
        "low-latency generation"
      ],
      "organization": {
        "_id": "668450a2c1cbe5e008ac6515",
        "name": "Snapchat",
        "fullname": "Snapchat Inc.",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/648ca58a39d2584ee47efef6/plasFy052q2795odYb6NO.jpeg"
      }
    },
    "publishedAt": "2026-01-13T02:46:46.000Z",
    "title": "SnapGen++: Unleashing Diffusion Transformers for Efficient High-Fidelity Image Generation on Edge Devices",
    "summary": "Recent advances in diffusion transformers (DiTs) have set new standards in image generation, yet remain impractical for on-device deployment due to their high computational and memory costs. In this work, we present an efficient DiT framework tailored for mobile and edge devices that achieves transformer-level generation quality under strict resource constraints. Our design combines three key components. First, we propose a compact DiT architecture with an adaptive global-local sparse attention mechanism that balances global context modeling and local detail preservation. Second, we propose an elastic training framework that jointly optimizes sub-DiTs of varying capacities within a unified supernetwork, allowing a single model to dynamically adjust for efficient inference across different hardware. Finally, we develop Knowledge-Guided Distribution Matching Distillation, a step-distillation pipeline that integrates the DMD objective with knowledge transfer from few-step teacher models, producing high-fidelity and low-latency generation (e.g., 4-step) suitable for real-time on-device use. Together, these contributions enable scalable, efficient, and high-quality diffusion models for deployment on diverse hardware.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.08303.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 207,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "668450a2c1cbe5e008ac6515",
      "name": "Snapchat",
      "fullname": "Snapchat Inc.",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/648ca58a39d2584ee47efef6/plasFy052q2795odYb6NO.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.04582",
      "authors": [
        {
          "_id": "69670c16c5e371f6b235d11d",
          "name": "Mizanur Rahman",
          "hidden": false
        },
        {
          "_id": "69670c16c5e371f6b235d11e",
          "name": "Mohammed Saidul Islam",
          "hidden": false
        },
        {
          "_id": "69670c16c5e371f6b235d11f",
          "name": "Md Tahmid Rahman Laskar",
          "hidden": false
        },
        {
          "_id": "69670c16c5e371f6b235d120",
          "name": "Shafiq Joty",
          "hidden": false
        },
        {
          "_id": "69670c16c5e371f6b235d121",
          "name": "Enamul Hoque",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64fb380148411fc78972acab/63_NquH1f0T9wDKJ4Pska.png"
      ],
      "publishedAt": "2026-01-08T04:29:07.000Z",
      "submittedOnDailyAt": "2026-01-14T00:56:54.754Z",
      "title": "Aligning Text, Code, and Vision: A Multi-Objective Reinforcement Learning Framework for Text-to-Visualization",
      "submittedOnDailyBy": {
        "_id": "64fb380148411fc78972acab",
        "avatarUrl": "/avatars/e3b1fb54dea4d5e293acf79857a835a1.svg",
        "isPro": false,
        "fullname": "Mizanur",
        "user": "mizanurr",
        "type": "user"
      },
      "summary": "Text-to-Visualization (Text2Vis) systems translate natural language queries over tabular data into concise answers and executable visualizations. While closed-source LLMs generate functional code, the resulting charts often lack semantic alignment and clarity, qualities that can only be assessed post-execution. Open-source models struggle even more, frequently producing non-executable or visually poor outputs. Although supervised fine-tuning can improve code executability, it fails to enhance overall visualization quality, as traditional SFT loss cannot capture post-execution feedback. To address this gap, we propose RL-Text2Vis, the first reinforcement learning framework for Text2Vis generation. Built on Group Relative Policy Optimization (GRPO), our method uses a novel multi-objective reward that jointly optimizes textual accuracy, code validity, and visualization quality using post-execution feedback. By training Qwen2.5 models (7B and 14B), RL-Text2Vis achieves a 22% relative improvement in chart quality over GPT-4o on the Text2Vis benchmark and boosts code execution success from 78% to 97% relative to its zero-shot baseline. Our models significantly outperform strong zero-shot and supervised baselines and also demonstrate robust generalization to out-of-domain datasets like VIS-Eval and NVBench. These results establish GRPO as an effective strategy for structured, multimodal reasoning in visualization generation. We release our code at https://github.com/vis-nlp/RL-Text2Vis.",
      "upvotes": 0,
      "discussionId": "69670c16c5e371f6b235d122",
      "ai_summary": "A reinforcement learning framework for text-to-visualization generation that improves chart quality and code execution by optimizing multiple objectives using post-execution feedback.",
      "ai_keywords": [
        "Text2Vis",
        "reinforcement learning",
        "Group Relative Policy Optimization",
        "multi-objective reward",
        "code execution",
        "visualization quality",
        "post-execution feedback",
        "Qwen2.5"
      ],
      "organization": {
        "_id": "671048d8de5ba28c43b7d815",
        "name": "York-reem1100",
        "fullname": "York University ",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6710486b243baf4c58f17a1b/NjIJAOU6CEijEtKN0uidL.png"
      }
    },
    "publishedAt": "2026-01-07T23:29:07.000Z",
    "title": "Aligning Text, Code, and Vision: A Multi-Objective Reinforcement Learning Framework for Text-to-Visualization",
    "summary": "Text-to-Visualization (Text2Vis) systems translate natural language queries over tabular data into concise answers and executable visualizations. While closed-source LLMs generate functional code, the resulting charts often lack semantic alignment and clarity, qualities that can only be assessed post-execution. Open-source models struggle even more, frequently producing non-executable or visually poor outputs. Although supervised fine-tuning can improve code executability, it fails to enhance overall visualization quality, as traditional SFT loss cannot capture post-execution feedback. To address this gap, we propose RL-Text2Vis, the first reinforcement learning framework for Text2Vis generation. Built on Group Relative Policy Optimization (GRPO), our method uses a novel multi-objective reward that jointly optimizes textual accuracy, code validity, and visualization quality using post-execution feedback. By training Qwen2.5 models (7B and 14B), RL-Text2Vis achieves a 22% relative improvement in chart quality over GPT-4o on the Text2Vis benchmark and boosts code execution success from 78% to 97% relative to its zero-shot baseline. Our models significantly outperform strong zero-shot and supervised baselines and also demonstrate robust generalization to out-of-domain datasets like VIS-Eval and NVBench. These results establish GRPO as an effective strategy for structured, multimodal reasoning in visualization generation. We release our code at https://github.com/vis-nlp/RL-Text2Vis.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64fb380148411fc78972acab/63_NquH1f0T9wDKJ4Pska.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04582.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64fb380148411fc78972acab",
      "avatarUrl": "/avatars/e3b1fb54dea4d5e293acf79857a835a1.svg",
      "fullname": "Mizanur",
      "name": "mizanurr",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "671048d8de5ba28c43b7d815",
      "name": "York-reem1100",
      "fullname": "York University ",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6710486b243baf4c58f17a1b/NjIJAOU6CEijEtKN0uidL.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.02669",
      "authors": [
        {
          "_id": "696742c7c5e371f6b235d1e7",
          "name": "Hongzhan Lin",
          "hidden": false
        },
        {
          "_id": "696742c7c5e371f6b235d1e8",
          "name": "Zixin Chen",
          "hidden": false
        },
        {
          "_id": "696742c7c5e371f6b235d1e9",
          "name": "Zhiqi Shen",
          "hidden": false
        },
        {
          "_id": "696742c7c5e371f6b235d1ea",
          "name": "Ziyang Luo",
          "hidden": false
        },
        {
          "_id": "696742c7c5e371f6b235d1eb",
          "name": "Zhen Ye",
          "hidden": false
        },
        {
          "_id": "696742c7c5e371f6b235d1ec",
          "name": "Jing Ma",
          "hidden": false
        },
        {
          "_id": "696742c7c5e371f6b235d1ed",
          "name": "Tat-Seng Chua",
          "hidden": false
        },
        {
          "_id": "696742c7c5e371f6b235d1ee",
          "name": "Guandong Xu",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-06T02:51:56.000Z",
      "submittedOnDailyAt": "2026-01-14T04:58:02.826Z",
      "title": "Towards Comprehensive Stage-wise Benchmarking of Large Language Models in Fact-Checking",
      "submittedOnDailyBy": {
        "_id": "6499466c7d1edf7cb612a9a6",
        "avatarUrl": "/avatars/c2e18594aa0879db8226f2a04496fb0b.svg",
        "isPro": false,
        "fullname": "Hongzhan Lin",
        "user": "danielhzlin",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) are increasingly deployed in real-world fact-checking systems, yet existing evaluations focus predominantly on claim verification and overlook the broader fact-checking workflow, including claim extraction and evidence retrieval. This narrow focus prevents current benchmarks from revealing systematic reasoning failures, factual blind spots, and robustness limitations of modern LLMs. To bridge this gap, we present FactArena, a fully automated arena-style evaluation framework that conducts comprehensive, stage-wise benchmarking of LLMs across the complete fact-checking pipeline. FactArena integrates three key components: (i) an LLM-driven fact-checking process that standardizes claim decomposition, evidence retrieval via tool-augmented interactions, and justification-based verdict prediction; (ii) an arena-styled judgment mechanism guided by consolidated reference guidelines to ensure unbiased and consistent pairwise comparisons across heterogeneous judge agents; and (iii) an arena-driven claim-evolution module that adaptively generates more challenging and semantically controlled claims to probe LLMs' factual robustness beyond fixed seed data. Across 16 state-of-the-art LLMs spanning seven model families, FactArena produces stable and interpretable rankings. Our analyses further reveal significant discrepancies between static claim-verification accuracy and end-to-end fact-checking competence, highlighting the necessity of holistic evaluation. The proposed framework offers a scalable and trustworthy paradigm for diagnosing LLMs' factual reasoning, guiding future model development, and advancing the reliable deployment of LLMs in safety-critical fact-checking applications.",
      "upvotes": 0,
      "discussionId": "696742c7c5e371f6b235d1ef",
      "ai_summary": "FactArena is an automated evaluation framework that comprehensively assesses large language models across all stages of the fact-checking pipeline, revealing gaps between claim verification accuracy and overall fact-checking capability.",
      "ai_keywords": [
        "fact-checking workflow",
        "claim extraction",
        "evidence retrieval",
        "LLM-driven fact-checking process",
        "tool-augmented interactions",
        "justification-based verdict prediction",
        "arena-styled judgment mechanism",
        "consolidated reference guidelines",
        "pairwise comparisons",
        "claim-evolution module",
        "semantically controlled claims",
        "factual robustness",
        "end-to-end fact-checking competence",
        "holistic evaluation",
        "factual reasoning",
        "safety-critical applications"
      ]
    },
    "publishedAt": "2026-01-05T21:51:56.000Z",
    "title": "Towards Comprehensive Stage-wise Benchmarking of Large Language Models in Fact-Checking",
    "summary": "Large Language Models (LLMs) are increasingly deployed in real-world fact-checking systems, yet existing evaluations focus predominantly on claim verification and overlook the broader fact-checking workflow, including claim extraction and evidence retrieval. This narrow focus prevents current benchmarks from revealing systematic reasoning failures, factual blind spots, and robustness limitations of modern LLMs. To bridge this gap, we present FactArena, a fully automated arena-style evaluation framework that conducts comprehensive, stage-wise benchmarking of LLMs across the complete fact-checking pipeline. FactArena integrates three key components: (i) an LLM-driven fact-checking process that standardizes claim decomposition, evidence retrieval via tool-augmented interactions, and justification-based verdict prediction; (ii) an arena-styled judgment mechanism guided by consolidated reference guidelines to ensure unbiased and consistent pairwise comparisons across heterogeneous judge agents; and (iii) an arena-driven claim-evolution module that adaptively generates more challenging and semantically controlled claims to probe LLMs' factual robustness beyond fixed seed data. Across 16 state-of-the-art LLMs spanning seven model families, FactArena produces stable and interpretable rankings. Our analyses further reveal significant discrepancies between static claim-verification accuracy and end-to-end fact-checking competence, highlighting the necessity of holistic evaluation. The proposed framework offers a scalable and trustworthy paradigm for diagnosing LLMs' factual reasoning, guiding future model development, and advancing the reliable deployment of LLMs in safety-critical fact-checking applications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02669.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6499466c7d1edf7cb612a9a6",
      "avatarUrl": "/avatars/c2e18594aa0879db8226f2a04496fb0b.svg",
      "fullname": "Hongzhan Lin",
      "name": "danielhzlin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  }
]