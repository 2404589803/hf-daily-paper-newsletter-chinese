[
  {
    "paper": {
      "id": "2511.04570",
      "authors": [
        {
          "_id": "690d5b51ad2597bf6c464ca9",
          "name": "Jingqi Tong",
          "hidden": false
        },
        {
          "_id": "690d5b51ad2597bf6c464caa",
          "name": "Yurong Mou",
          "hidden": false
        },
        {
          "_id": "690d5b51ad2597bf6c464cab",
          "name": "Hangcheng Li",
          "hidden": false
        },
        {
          "_id": "690d5b51ad2597bf6c464cac",
          "name": "Mingzhe Li",
          "hidden": false
        },
        {
          "_id": "690d5b51ad2597bf6c464cad",
          "name": "Yongzhuo Yang",
          "hidden": false
        },
        {
          "_id": "690d5b51ad2597bf6c464cae",
          "name": "Ming Zhang",
          "hidden": false
        },
        {
          "_id": "690d5b51ad2597bf6c464caf",
          "name": "Qiguang Chen",
          "hidden": false
        },
        {
          "_id": "690d5b51ad2597bf6c464cb0",
          "name": "Tianyi Liang",
          "hidden": false
        },
        {
          "_id": "690d5b51ad2597bf6c464cb1",
          "name": "Xiaomeng Hu",
          "hidden": false
        },
        {
          "_id": "690d5b51ad2597bf6c464cb2",
          "name": "Yining Zheng",
          "hidden": false
        },
        {
          "_id": "690d5b51ad2597bf6c464cb3",
          "name": "Xinchi Chen",
          "hidden": false
        },
        {
          "_id": "690d5b51ad2597bf6c464cb4",
          "name": "Jun Zhao",
          "hidden": false
        },
        {
          "_id": "690d5b51ad2597bf6c464cb5",
          "name": "Xuanjing Huang",
          "hidden": false
        },
        {
          "_id": "690d5b51ad2597bf6c464cb6",
          "name": "Xipeng Qiu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-06T17:25:23.000Z",
      "submittedOnDailyAt": "2025-11-07T00:18:17.549Z",
      "title": "Thinking with Video: Video Generation as a Promising Multimodal\n  Reasoning Paradigm",
      "submittedOnDailyBy": {
        "_id": "6690e13ccbcaf7ab0ec1c971",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/e8KDV6J29tviXlIpLZPq6.png",
        "isPro": false,
        "fullname": "Tony.Li",
        "user": "lkdhy",
        "type": "user"
      },
      "summary": "\"Thinking with Text\" and \"Thinking with Images\" paradigm significantly\nimprove the reasoning ability of large language models (LLMs) and Vision\nLanguage Models (VLMs). However, these paradigms have inherent limitations. (1)\nImages capture only single moments and fail to represent dynamic processes or\ncontinuous changes, and (2) The separation of text and vision as distinct\nmodalities, hindering unified multimodal understanding and generation. To\novercome these limitations, we introduce \"Thinking with Video\", a new paradigm\nthat leverages video generation models, such as Sora-2, to bridge visual and\ntextual reasoning in a unified temporal framework. To support this exploration,\nwe developed the Video Thinking Benchmark (VideoThinkBench). VideoThinkBench\nencompasses two task categories: (1) vision-centric tasks (e.g., Eyeballing\nPuzzles), and (2) text-centric tasks (e.g., subsets of GSM8K, MMMU). Our\nevaluation establishes Sora-2 as a capable reasoner. On vision-centric tasks,\nSora-2 is generally comparable to state-of-the-art (SOTA) VLMs, and even\nsurpasses VLMs on several tasks, such as Eyeballing Games. On text-centric\ntasks, Sora-2 achieves 92% accuracy on MATH, and 75.53% accuracy on MMMU.\nFurthermore, we systematically analyse the source of these abilities. We also\nfind that self-consistency and in-context learning can improve Sora-2's\nperformance. In summary, our findings demonstrate that the video generation\nmodel is the potential unified multimodal understanding and generation model,\npositions \"thinking with video\" as a unified multimodal reasoning paradigm.",
      "upvotes": 94,
      "discussionId": "690d5b51ad2597bf6c464cb7",
      "projectPage": "https://thinking-with-video.github.io/",
      "githubRepo": "https://github.com/tongjingqi/Thinking-with-Video",
      "ai_summary": "The \"Thinking with Video\" paradigm enhances multimodal reasoning by integrating video generation models, demonstrated through the Video Thinking Benchmark and improved performance on both vision and text tasks.",
      "ai_keywords": [
        "Thinking with Text",
        "Thinking with Images",
        "large language models",
        "Vision Language Models",
        "Thinking with Video",
        "video generation models",
        "Video Thinking Benchmark",
        "vision-centric tasks",
        "text-centric tasks",
        "Eyeballing Puzzles",
        "GSM8K",
        "MMMU",
        "self-consistency",
        "in-context learning"
      ],
      "githubStars": 2,
      "organization": {
        "_id": "613b0dee83ec35d460684607",
        "name": "fnlp",
        "fullname": "OpenMOSS (SII, FNLP)",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/xM_PjniEZ9fmDKtJN7PAG.png"
      }
    },
    "publishedAt": "2025-11-06T12:25:23.000Z",
    "title": "Thinking with Video: Video Generation as a Promising Multimodal\n  Reasoning Paradigm",
    "summary": "\"Thinking with Text\" and \"Thinking with Images\" paradigm significantly\nimprove the reasoning ability of large language models (LLMs) and Vision\nLanguage Models (VLMs). However, these paradigms have inherent limitations. (1)\nImages capture only single moments and fail to represent dynamic processes or\ncontinuous changes, and (2) The separation of text and vision as distinct\nmodalities, hindering unified multimodal understanding and generation. To\novercome these limitations, we introduce \"Thinking with Video\", a new paradigm\nthat leverages video generation models, such as Sora-2, to bridge visual and\ntextual reasoning in a unified temporal framework. To support this exploration,\nwe developed the Video Thinking Benchmark (VideoThinkBench). VideoThinkBench\nencompasses two task categories: (1) vision-centric tasks (e.g., Eyeballing\nPuzzles), and (2) text-centric tasks (e.g., subsets of GSM8K, MMMU). Our\nevaluation establishes Sora-2 as a capable reasoner. On vision-centric tasks,\nSora-2 is generally comparable to state-of-the-art (SOTA) VLMs, and even\nsurpasses VLMs on several tasks, such as Eyeballing Games. On text-centric\ntasks, Sora-2 achieves 92% accuracy on MATH, and 75.53% accuracy on MMMU.\nFurthermore, we systematically analyse the source of these abilities. We also\nfind that self-consistency and in-context learning can improve Sora-2's\nperformance. In summary, our findings demonstrate that the video generation\nmodel is the potential unified multimodal understanding and generation model,\npositions \"thinking with video\" as a unified multimodal reasoning paradigm.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.04570.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6690e13ccbcaf7ab0ec1c971",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/e8KDV6J29tviXlIpLZPq6.png",
      "fullname": "Tony.Li",
      "name": "lkdhy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "613b0dee83ec35d460684607",
      "name": "fnlp",
      "fullname": "OpenMOSS (SII, FNLP)",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/xM_PjniEZ9fmDKtJN7PAG.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.04460",
      "authors": [
        {
          "_id": "690d5b2aad2597bf6c464c9a",
          "name": "Runqi Qiao",
          "hidden": false
        },
        {
          "_id": "690d5b2aad2597bf6c464c9b",
          "name": "Qiuna Tan",
          "hidden": false
        },
        {
          "_id": "690d5b2aad2597bf6c464c9c",
          "name": "Minghan Yang",
          "hidden": false
        },
        {
          "_id": "690d5b2aad2597bf6c464c9d",
          "name": "Guanting Dong",
          "hidden": false
        },
        {
          "_id": "690d5b2aad2597bf6c464c9e",
          "name": "Peiqing Yang",
          "hidden": false
        },
        {
          "_id": "690d5b2aad2597bf6c464c9f",
          "name": "Shiqiang Lang",
          "hidden": false
        },
        {
          "_id": "690d5b2aad2597bf6c464ca0",
          "name": "Enhui Wan",
          "hidden": false
        },
        {
          "_id": "690d5b2aad2597bf6c464ca1",
          "name": "Xiaowan Wang",
          "hidden": false
        },
        {
          "_id": "690d5b2aad2597bf6c464ca2",
          "name": "Yida Xu",
          "hidden": false
        },
        {
          "_id": "690d5b2aad2597bf6c464ca3",
          "name": "Lan Yang",
          "hidden": false
        },
        {
          "_id": "690d5b2aad2597bf6c464ca4",
          "name": "Chong Sun",
          "hidden": false
        },
        {
          "_id": "690d5b2aad2597bf6c464ca5",
          "name": "Chen Li",
          "hidden": false
        },
        {
          "_id": "690d5b2aad2597bf6c464ca6",
          "name": "Honggang Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-06T15:32:29.000Z",
      "submittedOnDailyAt": "2025-11-07T00:09:41.943Z",
      "title": "V-Thinker: Interactive Thinking with Images",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Empowering Large Multimodal Models (LMMs) to deeply integrate image\ninteraction with long-horizon reasoning capabilities remains a long-standing\nchallenge in this field. Recent advances in vision-centric reasoning explore a\npromising \"Thinking with Images\" paradigm for LMMs, marking a shift from\nimage-assisted reasoning to image-interactive thinking. While this milestone\nenables models to focus on fine-grained image regions, progress remains\nconstrained by limited visual tool spaces and task-specific workflow designs.\nTo bridge this gap, we present V-Thinker, a general-purpose multimodal\nreasoning assistant that enables interactive, vision-centric thinking through\nend-to-end reinforcement learning. V-Thinker comprises two key components: (1)\na Data Evolution Flywheel that automatically synthesizes, evolves, and verifies\ninteractive reasoning datasets across three dimensions-diversity, quality, and\ndifficulty; and (2) a Visual Progressive Training Curriculum that first aligns\nperception via point-level supervision, then integrates interactive reasoning\nthrough a two-stage reinforcement learning framework. Furthermore, we introduce\nVTBench, an expert-verified benchmark targeting vision-centric interactive\nreasoning tasks. Extensive experiments demonstrate that V-Thinker consistently\noutperforms strong LMM-based baselines in both general and interactive\nreasoning scenarios, providing valuable insights for advancing\nimage-interactive reasoning applications.",
      "upvotes": 56,
      "discussionId": "690d5b2aad2597bf6c464ca7",
      "githubRepo": "https://github.com/We-Math/V-Thinker",
      "ai_summary": "V-Thinker, a multimodal reasoning assistant using reinforcement learning, enhances image-interactive thinking by synthesizing datasets and aligning perception for improved performance in vision-centric tasks.",
      "ai_keywords": [
        "multimodal models",
        "image interaction",
        "long-horizon reasoning",
        "Thinking with Images",
        "image-interactive thinking",
        "end-to-end reinforcement learning",
        "Data Evolution Flywheel",
        "Visual Progressive Training Curriculum",
        "point-level supervision",
        "two-stage reinforcement learning",
        "VTBench",
        "vision-centric interactive reasoning"
      ],
      "githubStars": 21
    },
    "publishedAt": "2025-11-06T10:32:29.000Z",
    "title": "V-Thinker: Interactive Thinking with Images",
    "summary": "Empowering Large Multimodal Models (LMMs) to deeply integrate image\ninteraction with long-horizon reasoning capabilities remains a long-standing\nchallenge in this field. Recent advances in vision-centric reasoning explore a\npromising \"Thinking with Images\" paradigm for LMMs, marking a shift from\nimage-assisted reasoning to image-interactive thinking. While this milestone\nenables models to focus on fine-grained image regions, progress remains\nconstrained by limited visual tool spaces and task-specific workflow designs.\nTo bridge this gap, we present V-Thinker, a general-purpose multimodal\nreasoning assistant that enables interactive, vision-centric thinking through\nend-to-end reinforcement learning. V-Thinker comprises two key components: (1)\na Data Evolution Flywheel that automatically synthesizes, evolves, and verifies\ninteractive reasoning datasets across three dimensions-diversity, quality, and\ndifficulty; and (2) a Visual Progressive Training Curriculum that first aligns\nperception via point-level supervision, then integrates interactive reasoning\nthrough a two-stage reinforcement learning framework. Furthermore, we introduce\nVTBench, an expert-verified benchmark targeting vision-centric interactive\nreasoning tasks. Extensive experiments demonstrate that V-Thinker consistently\noutperforms strong LMM-based baselines in both general and interactive\nreasoning scenarios, providing valuable insights for advancing\nimage-interactive reasoning applications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.04460.png",
    "numComments": 6,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 156
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.03773",
      "authors": [
        {
          "_id": "690d51e4ad2597bf6c464c52",
          "name": "Zhaorun Chen",
          "hidden": false
        },
        {
          "_id": "690d51e4ad2597bf6c464c53",
          "name": "Zhuokai Zhao",
          "hidden": false
        },
        {
          "_id": "690d51e4ad2597bf6c464c54",
          "name": "Kai Zhang",
          "hidden": false
        },
        {
          "_id": "690d51e4ad2597bf6c464c55",
          "name": "Bo Liu",
          "hidden": false
        },
        {
          "_id": "690d51e4ad2597bf6c464c56",
          "name": "Qi Qi",
          "hidden": false
        },
        {
          "_id": "690d51e4ad2597bf6c464c57",
          "name": "Yifan Wu",
          "hidden": false
        },
        {
          "_id": "690d51e4ad2597bf6c464c58",
          "name": "Tarun Kalluri",
          "hidden": false
        },
        {
          "_id": "690d51e4ad2597bf6c464c59",
          "name": "Sara Cao",
          "hidden": false
        },
        {
          "_id": "690d51e4ad2597bf6c464c5a",
          "name": "Yuanhao Xiong",
          "hidden": false
        },
        {
          "_id": "690d51e4ad2597bf6c464c5b",
          "name": "Haibo Tong",
          "hidden": false
        },
        {
          "_id": "690d51e4ad2597bf6c464c5c",
          "name": "Huaxiu Yao",
          "hidden": false
        },
        {
          "_id": "690d51e4ad2597bf6c464c5d",
          "name": "Hengduo Li",
          "hidden": false
        },
        {
          "_id": "690d51e4ad2597bf6c464c5e",
          "name": "Jiacheng Zhu",
          "hidden": false
        },
        {
          "_id": "690d51e4ad2597bf6c464c5f",
          "name": "Xian Li",
          "hidden": false
        },
        {
          "_id": "690d51e4ad2597bf6c464c60",
          "name": "Dawn Song",
          "hidden": false
        },
        {
          "_id": "690d51e4ad2597bf6c464c61",
          "name": "Bo Li",
          "hidden": false
        },
        {
          "_id": "690d51e4ad2597bf6c464c62",
          "name": "Jason Weston",
          "hidden": false
        },
        {
          "_id": "690d51e4ad2597bf6c464c63",
          "name": "Dat Huynh",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-05T18:58:48.000Z",
      "submittedOnDailyAt": "2025-11-07T00:37:12.582Z",
      "title": "Scaling Agent Learning via Experience Synthesis",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "While reinforcement learning (RL) can empower large language model (LLM)\nagents by enabling self-improvement through interaction, its practical adoption\nremains challenging due to costly rollouts, limited task diversity, unreliable\nreward signals, and infrastructure complexity, all of which obstruct the\ncollection of scalable experience data. To address these challenges, we\nintroduce DreamGym, the first unified framework designed to synthesize diverse\nexperiences with scalability in mind to enable effective online RL training for\nautonomous agents. Rather than relying on expensive real-environment rollouts,\nDreamGym distills environment dynamics into a reasoning-based experience model\nthat derives consistent state transitions and feedback signals through\nstep-by-step reasoning, enabling scalable agent rollout collection for RL. To\nimprove the stability and quality of transitions, DreamGym leverages an\nexperience replay buffer initialized with offline real-world data and\ncontinuously enriched with fresh interactions to actively support agent\ntraining. To improve knowledge acquisition, DreamGym adaptively generates new\ntasks that challenge the current agent policy, enabling more effective online\ncurriculum learning. Experiments across diverse environments and agent\nbackbones demonstrate that DreamGym substantially improves RL training, both in\nfully synthetic settings and in sim-to-real transfer scenarios. On non-RL-ready\ntasks like WebArena, DreamGym outperforms all baselines by over 30%. And in\nRL-ready but costly settings, it matches GRPO and PPO performance using only\nsynthetic interactions. When transferring a policy trained purely on synthetic\nexperiences to real-environment RL, DreamGym yields significant additional\nperformance gains while requiring far fewer real-world interactions, providing\na scalable warm-start strategy for general-purpose RL.",
      "upvotes": 37,
      "discussionId": "690d51e5ad2597bf6c464c64",
      "ai_summary": "DreamGym is a unified framework that synthesizes diverse experiences for scalable online RL training, improving agent performance and reducing real-world interactions.",
      "ai_keywords": [
        "reinforcement learning",
        "large language model",
        "self-improvement",
        "real-environment rollouts",
        "experience model",
        "state transitions",
        "feedback signals",
        "experience replay buffer",
        "offline real-world data",
        "adaptive task generation",
        "curriculum learning",
        "sim-to-real transfer",
        "WebArena",
        "GRPO",
        "PPO",
        "synthetic interactions"
      ],
      "organization": {
        "_id": "66b54027408752ae16404b05",
        "name": "metaresearch",
        "fullname": "Meta Research",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66b25f3f58babfaeb76112dc/2GmiaF075AZ7BcE538oPk.png"
      }
    },
    "publishedAt": "2025-11-05T13:58:48.000Z",
    "title": "Scaling Agent Learning via Experience Synthesis",
    "summary": "While reinforcement learning (RL) can empower large language model (LLM)\nagents by enabling self-improvement through interaction, its practical adoption\nremains challenging due to costly rollouts, limited task diversity, unreliable\nreward signals, and infrastructure complexity, all of which obstruct the\ncollection of scalable experience data. To address these challenges, we\nintroduce DreamGym, the first unified framework designed to synthesize diverse\nexperiences with scalability in mind to enable effective online RL training for\nautonomous agents. Rather than relying on expensive real-environment rollouts,\nDreamGym distills environment dynamics into a reasoning-based experience model\nthat derives consistent state transitions and feedback signals through\nstep-by-step reasoning, enabling scalable agent rollout collection for RL. To\nimprove the stability and quality of transitions, DreamGym leverages an\nexperience replay buffer initialized with offline real-world data and\ncontinuously enriched with fresh interactions to actively support agent\ntraining. To improve knowledge acquisition, DreamGym adaptively generates new\ntasks that challenge the current agent policy, enabling more effective online\ncurriculum learning. Experiments across diverse environments and agent\nbackbones demonstrate that DreamGym substantially improves RL training, both in\nfully synthetic settings and in sim-to-real transfer scenarios. On non-RL-ready\ntasks like WebArena, DreamGym outperforms all baselines by over 30%. And in\nRL-ready but costly settings, it matches GRPO and PPO performance using only\nsynthetic interactions. When transferring a policy trained purely on synthetic\nexperiences to real-environment RL, DreamGym yields significant additional\nperformance gains while requiring far fewer real-world interactions, providing\na scalable warm-start strategy for general-purpose RL.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.03773.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 156
    },
    "organization": {
      "_id": "66b54027408752ae16404b05",
      "name": "metaresearch",
      "fullname": "Meta Research",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66b25f3f58babfaeb76112dc/2GmiaF075AZ7BcE538oPk.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.04670",
      "authors": [
        {
          "_id": "690d5b7aad2597bf6c464cb9",
          "name": "Shusheng Yang",
          "hidden": false
        },
        {
          "_id": "690d5b7aad2597bf6c464cba",
          "name": "Jihan Yang",
          "hidden": false
        },
        {
          "_id": "690d5b7aad2597bf6c464cbb",
          "name": "Pinzhi Huang",
          "hidden": false
        },
        {
          "_id": "690d5b7aad2597bf6c464cbc",
          "name": "Ellis Brown",
          "hidden": false
        },
        {
          "_id": "690d5b7aad2597bf6c464cbd",
          "name": "Zihao Yang",
          "hidden": false
        },
        {
          "_id": "690d5b7aad2597bf6c464cbe",
          "name": "Yue Yu",
          "hidden": false
        },
        {
          "_id": "690d5b7aad2597bf6c464cbf",
          "name": "Shengbang Tong",
          "hidden": false
        },
        {
          "_id": "690d5b7aad2597bf6c464cc0",
          "name": "Zihan Zheng",
          "hidden": false
        },
        {
          "_id": "690d5b7aad2597bf6c464cc1",
          "name": "Yifan Xu",
          "hidden": false
        },
        {
          "_id": "690d5b7aad2597bf6c464cc2",
          "name": "Muhan Wang",
          "hidden": false
        },
        {
          "_id": "690d5b7aad2597bf6c464cc3",
          "name": "Daohan Lu",
          "hidden": false
        },
        {
          "_id": "690d5b7aad2597bf6c464cc4",
          "name": "Rob Fergus",
          "hidden": false
        },
        {
          "_id": "690d5b7aad2597bf6c464cc5",
          "name": "Yann LeCun",
          "hidden": false
        },
        {
          "_id": "690d5b7aad2597bf6c464cc6",
          "name": "Li Fei-Fei",
          "hidden": false
        },
        {
          "_id": "690d5b7aad2597bf6c464cc7",
          "name": "Saining Xie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-06T18:55:17.000Z",
      "submittedOnDailyAt": "2025-11-07T00:07:55.850Z",
      "title": "Cambrian-S: Towards Spatial Supersensing in Video",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We argue that progress in true multimodal intelligence calls for a shift from\nreactive, task-driven systems and brute-force long context towards a broader\nparadigm of supersensing. We frame spatial supersensing as four stages beyond\nlinguistic-only understanding: semantic perception (naming what is seen),\nstreaming event cognition (maintaining memory across continuous experiences),\nimplicit 3D spatial cognition (inferring the world behind pixels), and\npredictive world modeling (creating internal models that filter and organize\ninformation). Current benchmarks largely test only the early stages, offering\nnarrow coverage of spatial cognition and rarely challenging models in ways that\nrequire true world modeling. To drive progress in spatial supersensing, we\npresent VSI-SUPER, a two-part benchmark: VSR (long-horizon visual spatial\nrecall) and VSC (continual visual spatial counting). These tasks require\narbitrarily long video inputs yet are resistant to brute-force context\nexpansion. We then test data scaling limits by curating VSI-590K and training\nCambrian-S, achieving +30% absolute improvement on VSI-Bench without\nsacrificing general capabilities. Yet performance on VSI-SUPER remains limited,\nindicating that scale alone is insufficient for spatial supersensing. We\npropose predictive sensing as a path forward, presenting a proof-of-concept in\nwhich a self-supervised next-latent-frame predictor leverages surprise\n(prediction error) to drive memory and event segmentation. On VSI-SUPER, this\napproach substantially outperforms leading proprietary baselines, showing that\nspatial supersensing requires models that not only see but also anticipate,\nselect, and organize experience.",
      "upvotes": 12,
      "discussionId": "690d5b7aad2597bf6c464cc8",
      "projectPage": "https://cambrian-mllm.github.io/",
      "ai_summary": "Progress in multimodal intelligence requires a shift to supersensing, including semantic perception, event cognition, spatial cognition, and predictive modeling, demonstrated through VSI-SUPER benchmarks and a self-supervised predictive sensing approach.",
      "ai_keywords": [
        "supersensing",
        "semantic perception",
        "streaming event cognition",
        "implicit 3D spatial cognition",
        "predictive world modeling",
        "VSI-SUPER",
        "VSR",
        "VSC",
        "VSI-590K",
        "Cambrian-S",
        "VSI-Bench",
        "predictive sensing",
        "self-supervised next-latent-frame predictor",
        "surprise",
        "prediction error",
        "memory",
        "event segmentation"
      ]
    },
    "publishedAt": "2025-11-06T13:55:17.000Z",
    "title": "Cambrian-S: Towards Spatial Supersensing in Video",
    "summary": "We argue that progress in true multimodal intelligence calls for a shift from\nreactive, task-driven systems and brute-force long context towards a broader\nparadigm of supersensing. We frame spatial supersensing as four stages beyond\nlinguistic-only understanding: semantic perception (naming what is seen),\nstreaming event cognition (maintaining memory across continuous experiences),\nimplicit 3D spatial cognition (inferring the world behind pixels), and\npredictive world modeling (creating internal models that filter and organize\ninformation). Current benchmarks largely test only the early stages, offering\nnarrow coverage of spatial cognition and rarely challenging models in ways that\nrequire true world modeling. To drive progress in spatial supersensing, we\npresent VSI-SUPER, a two-part benchmark: VSR (long-horizon visual spatial\nrecall) and VSC (continual visual spatial counting). These tasks require\narbitrarily long video inputs yet are resistant to brute-force context\nexpansion. We then test data scaling limits by curating VSI-590K and training\nCambrian-S, achieving +30% absolute improvement on VSI-Bench without\nsacrificing general capabilities. Yet performance on VSI-SUPER remains limited,\nindicating that scale alone is insufficient for spatial supersensing. We\npropose predictive sensing as a path forward, presenting a proof-of-concept in\nwhich a self-supervised next-latent-frame predictor leverages surprise\n(prediction error) to drive memory and event segmentation. On VSI-SUPER, this\napproach substantially outperforms leading proprietary baselines, showing that\nspatial supersensing requires models that not only see but also anticipate,\nselect, and organize experience.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.04670.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 156
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.04307",
      "authors": [
        {
          "_id": "690d5db4ad2597bf6c464d4b",
          "name": "Jian Mu",
          "hidden": false
        },
        {
          "_id": "690d5db4ad2597bf6c464d4c",
          "name": "Chaoyun Zhang",
          "hidden": false
        },
        {
          "_id": "690d5db4ad2597bf6c464d4d",
          "name": "Chiming Ni",
          "hidden": false
        },
        {
          "_id": "690d5db4ad2597bf6c464d4e",
          "name": "Lu Wang",
          "hidden": false
        },
        {
          "_id": "690d5db4ad2597bf6c464d4f",
          "name": "Bo Qiao",
          "hidden": false
        },
        {
          "_id": "690d5db4ad2597bf6c464d50",
          "name": "Kartik Mathur",
          "hidden": false
        },
        {
          "_id": "690d5db4ad2597bf6c464d51",
          "name": "Qianhui Wu",
          "hidden": false
        },
        {
          "_id": "690d5db4ad2597bf6c464d52",
          "name": "Yuhang Xie",
          "hidden": false
        },
        {
          "_id": "690d5db4ad2597bf6c464d53",
          "name": "Xiaojun Ma",
          "hidden": false
        },
        {
          "_id": "690d5db4ad2597bf6c464d54",
          "name": "Mengyu Zhou",
          "hidden": false
        },
        {
          "_id": "690d5db4ad2597bf6c464d55",
          "name": "Si Qin",
          "hidden": false
        },
        {
          "_id": "690d5db4ad2597bf6c464d56",
          "name": "Liqun Li",
          "hidden": false
        },
        {
          "_id": "690d5db4ad2597bf6c464d57",
          "name": "Yu Kang",
          "hidden": false
        },
        {
          "_id": "690d5db4ad2597bf6c464d58",
          "name": "Minghua Ma",
          "hidden": false
        },
        {
          "_id": "690d5db4ad2597bf6c464d59",
          "name": "Qingwei Lin",
          "hidden": false
        },
        {
          "_id": "690d5db4ad2597bf6c464d5a",
          "name": "Saravan Rajmohan",
          "hidden": false
        },
        {
          "_id": "690d5db4ad2597bf6c464d5b",
          "name": "Dongmei Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-06T12:19:02.000Z",
      "submittedOnDailyAt": "2025-11-07T00:19:55.977Z",
      "title": "GUI-360: A Comprehensive Dataset and Benchmark for Computer-Using Agents",
      "submittedOnDailyBy": {
        "_id": "654dbac9938fbf1e696be8aa",
        "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
        "isPro": true,
        "fullname": "Chaoyun Zhang",
        "user": "vyokky",
        "type": "user"
      },
      "summary": "We introduce GUI-360^circ, a large-scale, comprehensive dataset and\nbenchmark suite designed to advance computer-using agents (CUAs). CUAs present\nunique challenges and is constrained by three persistent gaps: a scarcity of\nreal-world CUA tasks, the lack of automated collection-and-annotation pipelines\nfor multi-modal trajectories, and the absence of a unified benchmark that\njointly evaluates GUI grounding, screen parsing, and action prediction.\n  GUI-360^circ addresses these gaps with an LLM-augmented, largely automated\npipeline for query sourcing, environment-template construction, task\ninstantiation, batched execution, and LLM-driven quality filtering. The\nreleased corpus contains over 1.2M executed action steps across thousands of\ntrajectories in popular Windows office applications, and includes\nfull-resolution screenshots, accessibility metadata when available,\ninstantiated goals, intermediate reasoning traces, and both successful and\nfailed action trajectories. The dataset supports three canonical tasks, GUI\ngrounding, screen parsing, and action prediction, and a hybrid GUI+API action\nspace that reflects modern agent designs. Benchmarking state-of-the-art\nvision--language models on GUI-360^circ reveals substantial out-of-the-box\nshortcomings in grounding and action prediction; supervised fine-tuning and\nreinforcement learning yield significant gains but do not close the gap to\nhuman-level reliability. We release GUI-360^circ and accompanying code to\nfacilitate reproducible research and accelerate progress on robust desktop\nCUAs.\n  The full dataset has been made public on\nhttps://huggingface.co/datasets/vyokky/GUI-360.",
      "upvotes": 8,
      "discussionId": "690d5db4ad2597bf6c464d5c",
      "ai_summary": "GUI-360Â° is a large-scale dataset and benchmark suite for computer-using agents, addressing gaps in real-world tasks, automated data collection, and unified evaluation of GUI grounding, screen parsing, and action prediction.",
      "ai_keywords": [
        "LLM-augmented",
        "GUI grounding",
        "screen parsing",
        "action prediction",
        "hybrid GUI+API action space",
        "vision--language models",
        "supervised fine-tuning",
        "reinforcement learning"
      ],
      "organization": {
        "_id": "5e6485f787403103f9f1055e",
        "name": "microsoft",
        "fullname": "Microsoft",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"
      }
    },
    "publishedAt": "2025-11-06T07:19:02.000Z",
    "title": "GUI-360: A Comprehensive Dataset and Benchmark for Computer-Using Agents",
    "summary": "We introduce GUI-360^circ, a large-scale, comprehensive dataset and\nbenchmark suite designed to advance computer-using agents (CUAs). CUAs present\nunique challenges and is constrained by three persistent gaps: a scarcity of\nreal-world CUA tasks, the lack of automated collection-and-annotation pipelines\nfor multi-modal trajectories, and the absence of a unified benchmark that\njointly evaluates GUI grounding, screen parsing, and action prediction.\n  GUI-360^circ addresses these gaps with an LLM-augmented, largely automated\npipeline for query sourcing, environment-template construction, task\ninstantiation, batched execution, and LLM-driven quality filtering. The\nreleased corpus contains over 1.2M executed action steps across thousands of\ntrajectories in popular Windows office applications, and includes\nfull-resolution screenshots, accessibility metadata when available,\ninstantiated goals, intermediate reasoning traces, and both successful and\nfailed action trajectories. The dataset supports three canonical tasks, GUI\ngrounding, screen parsing, and action prediction, and a hybrid GUI+API action\nspace that reflects modern agent designs. Benchmarking state-of-the-art\nvision--language models on GUI-360^circ reveals substantial out-of-the-box\nshortcomings in grounding and action prediction; supervised fine-tuning and\nreinforcement learning yield significant gains but do not close the gap to\nhuman-level reliability. We release GUI-360^circ and accompanying code to\nfacilitate reproducible research and accelerate progress on robust desktop\nCUAs.\n  The full dataset has been made public on\nhttps://huggingface.co/datasets/vyokky/GUI-360.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.04307.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "654dbac9938fbf1e696be8aa",
      "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
      "fullname": "Chaoyun Zhang",
      "name": "vyokky",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "organization": {
      "_id": "5e6485f787403103f9f1055e",
      "name": "microsoft",
      "fullname": "Microsoft",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.03929",
      "authors": [
        {
          "_id": "690d5c72ad2597bf6c464cca",
          "name": "NVIDIA",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464ccc",
          "name": "Amala Sanjay Deshmukh",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464ccd",
          "name": "Kateryna Chumachenko",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464cce",
          "name": "Tuomas Rintamaki",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464ccf",
          "name": "Matthieu Le",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464cd0",
          "name": "Tyler Poon",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464cd1",
          "name": "Danial Mohseni Taheri",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464cd2",
          "name": "Ilia Karmanov",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464cd3",
          "name": "Guilin Liu",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464cd4",
          "name": "Jarno Seppanen",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464cd5",
          "name": "Guo Chen",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464cd6",
          "name": "Karan Sapra",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464cd7",
          "name": "Zhiding Yu",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464cd8",
          "name": "Adi Renduchintala",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464cd9",
          "name": "Charles Wang",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464cda",
          "name": "Peter Jin",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464cdb",
          "name": "Arushi Goel",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464cdc",
          "name": "Mike Ranzinger",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464cdd",
          "name": "Lukas Voegtle",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464cde",
          "name": "Philipp Fischer",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464cdf",
          "name": "Timo Roman",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464ce0",
          "name": "Wei Ping",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464ce1",
          "name": "Boxin Wang",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464ce2",
          "name": "Zhuolin Yang",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464ce3",
          "name": "Nayeon Lee",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464ce4",
          "name": "Shaokun Zhang",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464ce5",
          "name": "Fuxiao Liu",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464ce6",
          "name": "Zhiqi Li",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464ce7",
          "name": "Di Zhang",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464ce8",
          "name": "Greg Heinrich",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464ce9",
          "name": "Hongxu",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464cea",
          "name": "Yin",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464ceb",
          "name": "Song Han",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464cec",
          "name": "Pavlo Molchanov",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464ced",
          "name": "Parth Mannan",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464cee",
          "name": "Yao Xu",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464cef",
          "name": "Jane Polak Scowcroft",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464cf0",
          "name": "Tom Balough",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464cf1",
          "name": "Subhashree Radhakrishnan",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464cf2",
          "name": "Paris Zhang",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464cf3",
          "name": "Sean Cha",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464cf4",
          "name": "Ratnesh Kumar",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464cf5",
          "name": "Zaid Pervaiz Bhat",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464cf6",
          "name": "Jian Zhang",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464cf7",
          "name": "Darragh Hanley",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464cf8",
          "name": "Pritam Biswas",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464cf9",
          "name": "Jesse Oliver",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464cfa",
          "name": "Kevin Vasques",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464cfb",
          "name": "Roger Waleffe",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464cfc",
          "name": "Duncan Riach",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464cfd",
          "name": "Oluwatobi Olabiyi",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464cfe",
          "name": "Ameya Sunil Mahabaleshwarkar",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464cff",
          "name": "Bilal Kartal",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d00",
          "name": "Pritam Gundecha",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d01",
          "name": "Khanh Nguyen",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d02",
          "name": "Alexandre Milesi",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d03",
          "name": "Eugene Khvedchenia",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d04",
          "name": "Ran Zilberstein",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d05",
          "name": "Ofri Masad",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d06",
          "name": "Natan Bagrov",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d07",
          "name": "Nave Assaf",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d08",
          "name": "Tomer Asida",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d09",
          "name": "Daniel Afrimi",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d0a",
          "name": "Amit Zuker",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d0b",
          "name": "Netanel Haber",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d0c",
          "name": "Zhiyu Cheng",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d0d",
          "name": "Jingyu",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d0e",
          "name": "Xin",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d0f",
          "name": "Di",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d10",
          "name": "Wu",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d11",
          "name": "Nik Spirin",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d12",
          "name": "Maryam Moosaei",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d13",
          "name": "Roman Ageev",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d14",
          "name": "Vanshil Atul Shah",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d15",
          "name": "Yuting Wu",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d16",
          "name": "Daniel Korzekwa",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d17",
          "name": "Unnikrishnan Kizhakkemadam Sreekumar",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d18",
          "name": "Wanli Jiang",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d19",
          "name": "Padmavathy Subramanian",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d1a",
          "name": "Alejandra Rico",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d1b",
          "name": "Sandip Bhaskar",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d1c",
          "name": "Saeid Motiian",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d1d",
          "name": "Kedi Wu",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d1e",
          "name": "Annie Surla",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d1f",
          "name": "Chia-Chih Chen",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d20",
          "name": "Hayden Wolff",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d21",
          "name": "Matthew Feinberg",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d22",
          "name": "Melissa Corpuz",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d23",
          "name": "Marek Wawrzos",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d24",
          "name": "Eileen Long",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d25",
          "name": "Aastha Jhunjhunwala",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d26",
          "name": "Paul Hendricks",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d27",
          "name": "Farzan Memarian",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d28",
          "name": "Benika Hall",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d29",
          "name": "Xin-Yu Wang",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d2a",
          "name": "David Mosallanezhad",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d2b",
          "name": "Soumye Singhal",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d2c",
          "name": "Luis Vega",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d2d",
          "name": "Katherine Cheung",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d2e",
          "name": "Krzysztof Pawelec",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d2f",
          "name": "Michael Evans",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d30",
          "name": "Katherine Luna",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d31",
          "name": "Jie Lou",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d32",
          "name": "Erick Galinkin",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d33",
          "name": "Akshay Hazare",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d34",
          "name": "Kaustubh Purandare",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d35",
          "name": "Ann Guan",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d36",
          "name": "Anna Warno",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d37",
          "name": "Chen Cui",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d38",
          "name": "Yoshi Suhara",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d39",
          "name": "Shibani Likhite",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d3a",
          "name": "Seph Mard",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d3b",
          "name": "Meredith Price",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d3c",
          "name": "Laya Sleiman",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d3d",
          "name": "Saori Kaji",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d3e",
          "name": "Udi Karpas",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d3f",
          "name": "Kari Briski",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d40",
          "name": "Joey Conway",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d41",
          "name": "Michael Lightstone",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d42",
          "name": "Jan Kautz",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d43",
          "name": "Mohammad Shoeybi",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d44",
          "name": "Mostofa Patwary",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d45",
          "name": "Jonathen Cohen",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d46",
          "name": "Oleksii Kuchaiev",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d47",
          "name": "Andrew Tao",
          "hidden": false
        },
        {
          "_id": "690d5c72ad2597bf6c464d48",
          "name": "Bryan Catanzaro",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-06T00:10:19.000Z",
      "submittedOnDailyAt": "2025-11-07T00:12:03.113Z",
      "title": "NVIDIA Nemotron Nano V2 VL",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We introduce Nemotron Nano V2 VL, the latest model of the Nemotron\nvision-language series designed for strong real-world document understanding,\nlong video comprehension, and reasoning tasks. Nemotron Nano V2 VL delivers\nsignificant improvements over our previous model,\nLlama-3.1-Nemotron-Nano-VL-8B, across all vision and text domains through major\nenhancements in model architecture, datasets, and training recipes. Nemotron\nNano V2 VL builds on Nemotron Nano V2, a hybrid Mamba-Transformer LLM, and\ninnovative token reduction techniques to achieve higher inference throughput in\nlong document and video scenarios. We are releasing model checkpoints in BF16,\nFP8, and FP4 formats and sharing large parts of our datasets, recipes and\ntraining code.",
      "upvotes": 6,
      "discussionId": "690d5c72ad2597bf6c464d49",
      "ai_summary": "Nemotron Nano V2 VL, a hybrid Mamba-Transformer LLM, improves document and video understanding through enhanced architecture and token reduction techniques.",
      "ai_keywords": [
        "Mamba-Transformer",
        "token reduction techniques",
        "BF16",
        "FP8",
        "FP4"
      ],
      "organization": {
        "_id": "60262b67268c201cdc8b7d43",
        "name": "nvidia",
        "fullname": "NVIDIA",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
      }
    },
    "publishedAt": "2025-11-05T19:10:19.000Z",
    "title": "NVIDIA Nemotron Nano V2 VL",
    "summary": "We introduce Nemotron Nano V2 VL, the latest model of the Nemotron\nvision-language series designed for strong real-world document understanding,\nlong video comprehension, and reasoning tasks. Nemotron Nano V2 VL delivers\nsignificant improvements over our previous model,\nLlama-3.1-Nemotron-Nano-VL-8B, across all vision and text domains through major\nenhancements in model architecture, datasets, and training recipes. Nemotron\nNano V2 VL builds on Nemotron Nano V2, a hybrid Mamba-Transformer LLM, and\ninnovative token reduction techniques to achieve higher inference throughput in\nlong document and video scenarios. We are releasing model checkpoints in BF16,\nFP8, and FP4 formats and sharing large parts of our datasets, recipes and\ntraining code.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.03929.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 156
    },
    "organization": {
      "_id": "60262b67268c201cdc8b7d43",
      "name": "nvidia",
      "fullname": "NVIDIA",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.04217",
      "authors": [
        {
          "_id": "690d5a51ad2597bf6c464c92",
          "name": "Hikari Otsuka",
          "hidden": false
        },
        {
          "_id": "690d5a51ad2597bf6c464c93",
          "name": "Daiki Chijiwa",
          "hidden": false
        },
        {
          "_id": "690d5a51ad2597bf6c464c94",
          "name": "Yasuyuki Okoshi",
          "hidden": false
        },
        {
          "_id": "690d5a51ad2597bf6c464c95",
          "name": "Daichi Fujiki",
          "hidden": false
        },
        {
          "_id": "690d5a51ad2597bf6c464c96",
          "name": "Susumu Takeuchi",
          "hidden": false
        },
        {
          "_id": "690d5a51ad2597bf6c464c97",
          "name": "Masato Motomura",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-06T09:29:58.000Z",
      "submittedOnDailyAt": "2025-11-07T00:04:48.015Z",
      "title": "The Strong Lottery Ticket Hypothesis for Multi-Head Attention Mechanisms",
      "submittedOnDailyBy": {
        "_id": "676a62adaeb3bcd701e23c1b",
        "avatarUrl": "/avatars/47afd9ed11fafd2c70f6fa8c570d1038.svg",
        "isPro": false,
        "fullname": "Hikari Otsuka",
        "user": "h-otsuka",
        "type": "user"
      },
      "summary": "The strong lottery ticket hypothesis (SLTH) conjectures that high-performing\nsubnetworks, called strong lottery tickets (SLTs), are hidden in randomly\ninitialized neural networks. Although recent theoretical studies have\nestablished the SLTH across various neural architectures, the SLTH for\ntransformer architectures still lacks theoretical understanding. In particular,\nthe current theory of the SLTH does not yet account for the multi-head\nattention (MHA) mechanism, a core component of transformers. To address this\ngap, we introduce a theoretical analysis of the existence of SLTs within MHAs.\nWe prove that, if a randomly initialized MHA of H heads and input dimension\nd has the hidden dimension O(dlog(Hd^{3/2})) for the key and value, it\ncontains an SLT that approximates an arbitrary MHA with the same input\ndimension with high probability. Furthermore, by leveraging this theory for\nMHAs, we extend the SLTH to transformers without normalization layers. We\nempirically validate our theoretical findings, demonstrating that the\napproximation error between the SLT within a source model (MHA and transformer)\nand an approximate target counterpart decreases exponentially by increasing the\nhidden dimension of the source model.",
      "upvotes": 4,
      "discussionId": "690d5a51ad2597bf6c464c98",
      "ai_summary": "Theoretical analysis proves the existence of strong lottery tickets within multi-head attention mechanisms and extends the strong lottery ticket hypothesis to transformers without normalization layers.",
      "ai_keywords": [
        "strong lottery ticket hypothesis",
        "strong lottery tickets",
        "multi-head attention",
        "transformers",
        "normalization layers"
      ]
    },
    "publishedAt": "2025-11-06T04:29:58.000Z",
    "title": "The Strong Lottery Ticket Hypothesis for Multi-Head Attention Mechanisms",
    "summary": "The strong lottery ticket hypothesis (SLTH) conjectures that high-performing\nsubnetworks, called strong lottery tickets (SLTs), are hidden in randomly\ninitialized neural networks. Although recent theoretical studies have\nestablished the SLTH across various neural architectures, the SLTH for\ntransformer architectures still lacks theoretical understanding. In particular,\nthe current theory of the SLTH does not yet account for the multi-head\nattention (MHA) mechanism, a core component of transformers. To address this\ngap, we introduce a theoretical analysis of the existence of SLTs within MHAs.\nWe prove that, if a randomly initialized MHA of H heads and input dimension\nd has the hidden dimension O(dlog(Hd^{3/2})) for the key and value, it\ncontains an SLT that approximates an arbitrary MHA with the same input\ndimension with high probability. Furthermore, by leveraging this theory for\nMHAs, we extend the SLTH to transformers without normalization layers. We\nempirically validate our theoretical findings, demonstrating that the\napproximation error between the SLT within a source model (MHA and transformer)\nand an approximate target counterpart decreases exponentially by increasing the\nhidden dimension of the source model.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.04217.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "676a62adaeb3bcd701e23c1b",
      "avatarUrl": "/avatars/47afd9ed11fafd2c70f6fa8c570d1038.svg",
      "fullname": "Hikari Otsuka",
      "name": "h-otsuka",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.04655",
      "authors": [
        {
          "_id": "690d627fad2597bf6c464d77",
          "name": "Ellis Brown",
          "hidden": false
        },
        {
          "_id": "690d627fad2597bf6c464d78",
          "name": "Jihan Yang",
          "hidden": false
        },
        {
          "_id": "690d627fad2597bf6c464d79",
          "name": "Shusheng Yang",
          "hidden": false
        },
        {
          "_id": "690d627fad2597bf6c464d7a",
          "name": "Rob Fergus",
          "hidden": false
        },
        {
          "_id": "690d627fad2597bf6c464d7b",
          "name": "Saining Xie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-06T18:43:21.000Z",
      "submittedOnDailyAt": "2025-11-07T00:38:49.237Z",
      "title": "Benchmark Designers Should \"Train on the Test Set\" to Expose Exploitable\n  Non-Visual Shortcuts",
      "submittedOnDailyBy": {
        "_id": "6304baf041387c7f1177a5d2",
        "avatarUrl": "/avatars/795c63f2394080eec78ca7981d4a1f78.svg",
        "isPro": true,
        "fullname": "Jihan Yang",
        "user": "jihanyang",
        "type": "user"
      },
      "summary": "Robust benchmarks are crucial for evaluating Multimodal Large Language Models\n(MLLMs). Yet we find that models can ace many multimodal benchmarks without\nstrong visual understanding, instead exploiting biases, linguistic priors, and\nsuperficial patterns. This is especially problematic for vision-centric\nbenchmarks that are meant to require visual inputs. We adopt a diagnostic\nprinciple for benchmark design: if a benchmark can be gamed, it will be.\nDesigners should therefore try to ``game'' their own benchmarks first, using\ndiagnostic and debiasing procedures to systematically identify and mitigate\nnon-visual biases. Effective diagnosis requires directly ``training on the test\nset'' -- probing the released test set for its intrinsic, exploitable patterns.\n  We operationalize this standard with two components. First, we diagnose\nbenchmark susceptibility using a ``Test-set Stress-Test'' (TsT) methodology.\nOur primary diagnostic tool involves fine-tuning a powerful Large Language\nModel via k-fold cross-validation on exclusively the non-visual, textual\ninputs of the test set to reveal shortcut performance and assign each sample a\nbias score s(x). We complement this with a lightweight Random Forest-based\ndiagnostic operating on hand-crafted features for fast, interpretable auditing.\nSecond, we debias benchmarks by filtering high-bias samples using an\n``Iterative Bias Pruning'' (IBP) procedure. Applying this framework to four\nbenchmarks -- VSI-Bench, CV-Bench, MMMU, and VideoMME -- we uncover pervasive\nnon-visual biases. As a case study, we apply our full framework to create\nVSI-Bench-Debiased, demonstrating reduced non-visual solvability and a wider\nvision-blind performance gap than the original.",
      "upvotes": 3,
      "discussionId": "690d6280ad2597bf6c464d7c",
      "projectPage": "https://cambrian-mllm.github.io/",
      "ai_summary": "A framework for diagnosing and debiasing multimodal benchmarks reveals and mitigates non-visual biases, improving the robustness of Multimodal Large Language Models.",
      "ai_keywords": [
        "multimodal large language models",
        "MLLMs",
        "vision-centric benchmarks",
        "diagnostic principle",
        "test-set stress-test",
        "TsT",
        "fine-tuning",
        "k-fold cross-validation",
        "bias score",
        "random forest",
        "iterative bias pruning",
        "IBP",
        "VSI-Bench",
        "CV-Bench",
        "MMMU",
        "VideoMME",
        "VSI-Bench-Debiased"
      ],
      "organization": {
        "_id": "662741612ada5b77e310d171",
        "name": "nyu-visionx",
        "fullname": "NYU VisionX",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/626dc5105f7327906f0b2a4e/Kn-QtZjE6TJE-syTndXIW.jpeg"
      }
    },
    "publishedAt": "2025-11-06T13:43:21.000Z",
    "title": "Benchmark Designers Should \"Train on the Test Set\" to Expose Exploitable\n  Non-Visual Shortcuts",
    "summary": "Robust benchmarks are crucial for evaluating Multimodal Large Language Models\n(MLLMs). Yet we find that models can ace many multimodal benchmarks without\nstrong visual understanding, instead exploiting biases, linguistic priors, and\nsuperficial patterns. This is especially problematic for vision-centric\nbenchmarks that are meant to require visual inputs. We adopt a diagnostic\nprinciple for benchmark design: if a benchmark can be gamed, it will be.\nDesigners should therefore try to ``game'' their own benchmarks first, using\ndiagnostic and debiasing procedures to systematically identify and mitigate\nnon-visual biases. Effective diagnosis requires directly ``training on the test\nset'' -- probing the released test set for its intrinsic, exploitable patterns.\n  We operationalize this standard with two components. First, we diagnose\nbenchmark susceptibility using a ``Test-set Stress-Test'' (TsT) methodology.\nOur primary diagnostic tool involves fine-tuning a powerful Large Language\nModel via k-fold cross-validation on exclusively the non-visual, textual\ninputs of the test set to reveal shortcut performance and assign each sample a\nbias score s(x). We complement this with a lightweight Random Forest-based\ndiagnostic operating on hand-crafted features for fast, interpretable auditing.\nSecond, we debias benchmarks by filtering high-bias samples using an\n``Iterative Bias Pruning'' (IBP) procedure. Applying this framework to four\nbenchmarks -- VSI-Bench, CV-Bench, MMMU, and VideoMME -- we uncover pervasive\nnon-visual biases. As a case study, we apply our full framework to create\nVSI-Bench-Debiased, demonstrating reduced non-visual solvability and a wider\nvision-blind performance gap than the original.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.04655.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6304baf041387c7f1177a5d2",
      "avatarUrl": "/avatars/795c63f2394080eec78ca7981d4a1f78.svg",
      "fullname": "Jihan Yang",
      "name": "jihanyang",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "organization": {
      "_id": "662741612ada5b77e310d171",
      "name": "nyu-visionx",
      "fullname": "NYU VisionX",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/626dc5105f7327906f0b2a4e/Kn-QtZjE6TJE-syTndXIW.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.03996",
      "authors": [
        {
          "_id": "690d62f9ad2597bf6c464d7e",
          "name": "Yushi Wang",
          "hidden": false
        },
        {
          "_id": "690d62f9ad2597bf6c464d7f",
          "name": "Changsheng Luo",
          "hidden": false
        },
        {
          "_id": "690d62f9ad2597bf6c464d80",
          "name": "Penghui Chen",
          "hidden": false
        },
        {
          "_id": "690d62f9ad2597bf6c464d81",
          "name": "Jianran Liu",
          "hidden": false
        },
        {
          "_id": "690d62f9ad2597bf6c464d82",
          "name": "Weijian Sun",
          "hidden": false
        },
        {
          "_id": "690d62f9ad2597bf6c464d83",
          "name": "Tong Guo",
          "hidden": false
        },
        {
          "_id": "690d62f9ad2597bf6c464d84",
          "name": "Kechang Yang",
          "hidden": false
        },
        {
          "_id": "690d62f9ad2597bf6c464d85",
          "name": "Biao Hu",
          "hidden": false
        },
        {
          "_id": "690d62f9ad2597bf6c464d86",
          "name": "Yangang Zhang",
          "hidden": false
        },
        {
          "_id": "690d62f9ad2597bf6c464d87",
          "name": "Mingguo Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-06T02:40:48.000Z",
      "submittedOnDailyAt": "2025-11-07T00:40:00.286Z",
      "title": "Learning Vision-Driven Reactive Soccer Skills for Humanoid Robots",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Humanoid soccer poses a representative challenge for embodied intelligence,\nrequiring robots to operate within a tightly coupled perception-action loop.\nHowever, existing systems typically rely on decoupled modules, resulting in\ndelayed responses and incoherent behaviors in dynamic environments, while\nreal-world perceptual limitations further exacerbate these issues. In this\nwork, we present a unified reinforcement learning-based controller that enables\nhumanoid robots to acquire reactive soccer skills through the direct\nintegration of visual perception and motion control. Our approach extends\nAdversarial Motion Priors to perceptual settings in real-world dynamic\nenvironments, bridging motion imitation and visually grounded dynamic control.\nWe introduce an encoder-decoder architecture combined with a virtual perception\nsystem that models real-world visual characteristics, allowing the policy to\nrecover privileged states from imperfect observations and establish active\ncoordination between perception and action. The resulting controller\ndemonstrates strong reactivity, consistently executing coherent and robust\nsoccer behaviors across various scenarios, including real RoboCup matches.",
      "upvotes": 2,
      "discussionId": "690d62f9ad2597bf6c464d88",
      "projectPage": "https://humanoid-kick.github.io/",
      "ai_summary": "A unified reinforcement learning controller integrates visual perception and motion control for humanoid robots in soccer, using Adversarial Motion Priors and an encoder-decoder architecture to achieve reactive and coherent behaviors.",
      "ai_keywords": [
        "reinforcement learning",
        "Adversarial Motion Priors",
        "encoder-decoder architecture",
        "virtual perception system",
        "privileged states",
        "real-world visual characteristics",
        "motion imitation",
        "visually grounded dynamic control"
      ],
      "organization": {
        "_id": "67d1140985ea0644e2f14b99",
        "name": "ByteDance-Seed",
        "fullname": "ByteDance Seed",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
      }
    },
    "publishedAt": "2025-11-05T21:40:48.000Z",
    "title": "Learning Vision-Driven Reactive Soccer Skills for Humanoid Robots",
    "summary": "Humanoid soccer poses a representative challenge for embodied intelligence,\nrequiring robots to operate within a tightly coupled perception-action loop.\nHowever, existing systems typically rely on decoupled modules, resulting in\ndelayed responses and incoherent behaviors in dynamic environments, while\nreal-world perceptual limitations further exacerbate these issues. In this\nwork, we present a unified reinforcement learning-based controller that enables\nhumanoid robots to acquire reactive soccer skills through the direct\nintegration of visual perception and motion control. Our approach extends\nAdversarial Motion Priors to perceptual settings in real-world dynamic\nenvironments, bridging motion imitation and visually grounded dynamic control.\nWe introduce an encoder-decoder architecture combined with a virtual perception\nsystem that models real-world visual characteristics, allowing the policy to\nrecover privileged states from imperfect observations and establish active\ncoordination between perception and action. The resulting controller\ndemonstrates strong reactivity, consistently executing coherent and robust\nsoccer behaviors across various scenarios, including real RoboCup matches.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.03996.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 156
    },
    "organization": {
      "_id": "67d1140985ea0644e2f14b99",
      "name": "ByteDance-Seed",
      "fullname": "ByteDance Seed",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.03774",
      "authors": [
        {
          "_id": "690d8af0ad2597bf6c464f4f",
          "name": "Jaden Park",
          "hidden": false
        },
        {
          "_id": "690d8af0ad2597bf6c464f50",
          "name": "Mu Cai",
          "hidden": false
        },
        {
          "_id": "690d8af0ad2597bf6c464f51",
          "name": "Feng Yao",
          "hidden": false
        },
        {
          "_id": "690d8af0ad2597bf6c464f52",
          "name": "Jingbo Shang",
          "hidden": false
        },
        {
          "_id": "690d8af0ad2597bf6c464f53",
          "name": "Soochahn Lee",
          "hidden": false
        },
        {
          "_id": "690d8af0ad2597bf6c464f54",
          "name": "Yong Jae Lee",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63b7b2c6bd2d153522821766/TbM07ib9NWtYgwcY726R8.png"
      ],
      "publishedAt": "2025-11-05T18:59:52.000Z",
      "submittedOnDailyAt": "2025-11-07T04:06:05.354Z",
      "title": "Contamination Detection for VLMs using Multi-Modal Semantic Perturbation",
      "submittedOnDailyBy": {
        "_id": "63b7b2c6bd2d153522821766",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b7b2c6bd2d153522821766/aHtga-_OUdOrg_TRrXO08.jpeg",
        "isPro": false,
        "fullname": "Mu Cai",
        "user": "mucai",
        "type": "user"
      },
      "summary": "Recent advances in Vision-Language Models (VLMs) have achieved\nstate-of-the-art performance on numerous benchmark tasks. However, the use of\ninternet-scale, often proprietary, pretraining corpora raises a critical\nconcern for both practitioners and users: inflated performance due to test-set\nleakage. While prior works have proposed mitigation strategies such as\ndecontamination of pretraining data and benchmark redesign for LLMs, the\ncomplementary direction of developing detection methods for contaminated VLMs\nremains underexplored. To address this gap, we deliberately contaminate\nopen-source VLMs on popular benchmarks and show that existing detection\napproaches either fail outright or exhibit inconsistent behavior. We then\npropose a novel simple yet effective detection method based on multi-modal\nsemantic perturbation, demonstrating that contaminated models fail to\ngeneralize under controlled perturbations. Finally, we validate our approach\nacross multiple realistic contamination strategies, confirming its robustness\nand effectiveness. The code and perturbed dataset will be released publicly.",
      "upvotes": 1,
      "discussionId": "690d8af1ad2597bf6c464f55",
      "ai_summary": "A novel detection method based on multi-modal semantic perturbation is proposed to identify contaminated Vision-Language Models, demonstrating robustness across various contamination strategies.",
      "ai_keywords": [
        "Vision-Language Models",
        "VLMs",
        "pretraining corpora",
        "test-set leakage",
        "decontamination",
        "benchmark redesign",
        "detection methods",
        "multi-modal semantic perturbation"
      ],
      "organization": {
        "_id": "61d090ec03bc10eb8e1c2970",
        "name": "uw-madison",
        "fullname": "University of Wisconsin - Madison",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/IYmUaLUc_rDVNC6F7-k8M.png"
      }
    },
    "publishedAt": "2025-11-05T13:59:52.000Z",
    "title": "Contamination Detection for VLMs using Multi-Modal Semantic Perturbation",
    "summary": "Recent advances in Vision-Language Models (VLMs) have achieved\nstate-of-the-art performance on numerous benchmark tasks. However, the use of\ninternet-scale, often proprietary, pretraining corpora raises a critical\nconcern for both practitioners and users: inflated performance due to test-set\nleakage. While prior works have proposed mitigation strategies such as\ndecontamination of pretraining data and benchmark redesign for LLMs, the\ncomplementary direction of developing detection methods for contaminated VLMs\nremains underexplored. To address this gap, we deliberately contaminate\nopen-source VLMs on popular benchmarks and show that existing detection\napproaches either fail outright or exhibit inconsistent behavior. We then\npropose a novel simple yet effective detection method based on multi-modal\nsemantic perturbation, demonstrating that contaminated models fail to\ngeneralize under controlled perturbations. Finally, we validate our approach\nacross multiple realistic contamination strategies, confirming its robustness\nand effectiveness. The code and perturbed dataset will be released publicly.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63b7b2c6bd2d153522821766/TbM07ib9NWtYgwcY726R8.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.03774.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63b7b2c6bd2d153522821766",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b7b2c6bd2d153522821766/aHtga-_OUdOrg_TRrXO08.jpeg",
      "fullname": "Mu Cai",
      "name": "mucai",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "organization": {
      "_id": "61d090ec03bc10eb8e1c2970",
      "name": "uw-madison",
      "fullname": "University of Wisconsin - Madison",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/IYmUaLUc_rDVNC6F7-k8M.png"
    },
    "isAuthorParticipating": false
  }
]