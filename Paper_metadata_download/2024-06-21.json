[
    {
        "paper": {
            "id": "2406.14347",
            "authors": [
                {
                    "_id": "667536bfa845e4470f3a1900",
                    "user": {
                        "avatarUrl": "/avatars/7832ef23d89b1a0ae08a76bf95639b6b.svg",
                        "isPro": false,
                        "fullname": "Kuzma Khrabrov",
                        "user": "Kuzma",
                        "type": "user"
                    },
                    "name": "Kuzma Khrabrov",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-21T10:22:40.149Z",
                    "hidden": false
                },
                {
                    "_id": "667536bfa845e4470f3a1901",
                    "user": {
                        "avatarUrl": "/avatars/b3c09745f3f90e7361be2f63604d5f3a.svg",
                        "isPro": false,
                        "fullname": "Anton",
                        "user": "antonber",
                        "type": "user"
                    },
                    "name": "Anton Ber",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-21T10:22:48.347Z",
                    "hidden": false
                },
                {
                    "_id": "667536bfa845e4470f3a1902",
                    "user": {
                        "avatarUrl": "/avatars/fb7ba8ceddd4c2f900c1361ad06e12d5.svg",
                        "isPro": false,
                        "fullname": "Artem Tsypin",
                        "user": "ofantomas",
                        "type": "user"
                    },
                    "name": "Artem Tsypin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-21T10:22:56.898Z",
                    "hidden": false
                },
                {
                    "_id": "667536bfa845e4470f3a1903",
                    "user": {
                        "avatarUrl": "/avatars/3fd3dd2cf85517b2e53c0b2c8870f274.svg",
                        "isPro": false,
                        "fullname": "Konstantin Ushenin",
                        "user": "kostaNew",
                        "type": "user"
                    },
                    "name": "Konstantin Ushenin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-21T10:23:03.134Z",
                    "hidden": false
                },
                {
                    "_id": "667536bfa845e4470f3a1904",
                    "name": "Egor Rumiantsev",
                    "hidden": false
                },
                {
                    "_id": "667536bfa845e4470f3a1905",
                    "user": {
                        "avatarUrl": "/avatars/0ff1633f34f119ac384142c61cfb83bd.svg",
                        "isPro": false,
                        "fullname": "Alexander Telepov",
                        "user": "alexander-telepov",
                        "type": "user"
                    },
                    "name": "Alexander Telepov",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-21T10:23:13.760Z",
                    "hidden": false
                },
                {
                    "_id": "667536bfa845e4470f3a1906",
                    "user": {
                        "avatarUrl": "/avatars/068fd11f7b0d19750194baf0b08dc66e.svg",
                        "isPro": false,
                        "fullname": "Dmitry Protasov",
                        "user": "vosatorp",
                        "type": "user"
                    },
                    "name": "Dmitry Protasov",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-21T10:23:20.726Z",
                    "hidden": false
                },
                {
                    "_id": "667536bfa845e4470f3a1907",
                    "name": "Ilya Shenbin",
                    "hidden": false
                },
                {
                    "_id": "667536bfa845e4470f3a1908",
                    "name": "Anton Alekseev",
                    "hidden": false
                },
                {
                    "_id": "667536bfa845e4470f3a1909",
                    "name": "Mikhail Shirokikh",
                    "hidden": false
                },
                {
                    "_id": "667536bfa845e4470f3a190a",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662968739113-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Sergey Nikolenko",
                        "user": "snikolenko",
                        "type": "user"
                    },
                    "name": "Sergey Nikolenko",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-21T10:23:37.613Z",
                    "hidden": false
                },
                {
                    "_id": "667536bfa845e4470f3a190b",
                    "user": {
                        "avatarUrl": "/avatars/72f9a3c39b3ba5114388d16a35524835.svg",
                        "isPro": false,
                        "fullname": "Elena Tutubalina",
                        "user": "tlenusik",
                        "type": "user"
                    },
                    "name": "Elena Tutubalina",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-21T10:24:19.860Z",
                    "hidden": false
                },
                {
                    "_id": "667536bfa845e4470f3a190c",
                    "user": {
                        "avatarUrl": "/avatars/999739bbd23e515f4f347fd29ebbcb2e.svg",
                        "isPro": false,
                        "fullname": "Artur Kadurin",
                        "user": "AKadurin",
                        "type": "user"
                    },
                    "name": "Artur Kadurin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-21T11:47:39.887Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-20T14:14:59.000Z",
            "title": "nabla^2DFT: A Universal Quantum Chemistry Dataset of Drug-Like\n  Molecules and a Benchmark for Neural Network Potentials",
            "summary": "Methods of computational quantum chemistry provide accurate approximations of\nmolecular properties crucial for computer-aided drug discovery and other areas\nof chemical science. However, high computational complexity limits the\nscalability of their applications. Neural network potentials (NNPs) are a\npromising alternative to quantum chemistry methods, but they require large and\ndiverse datasets for training. This work presents a new dataset and benchmark\ncalled nabla^2DFT that is based on the nablaDFT. It contains twice as much\nmolecular structures, three times more conformations, new data types and tasks,\nand state-of-the-art models. The dataset includes energies, forces, 17\nmolecular properties, Hamiltonian and overlap matrices, and a wavefunction\nobject. All calculations were performed at the DFT level\n(omegaB97X-D/def2-SVP) for each conformation. Moreover, nabla^2DFT is the\nfirst dataset that contains relaxation trajectories for a substantial number of\ndrug-like molecules. We also introduce a novel benchmark for evaluating NNPs in\nmolecular property prediction, Hamiltonian prediction, and conformational\noptimization tasks. Finally, we propose an extendable framework for training\nNNPs and implement 10 models within it.",
            "upvotes": 56
        },
        "publishedAt": "2024-06-21T06:46:45.642Z",
        "title": "$\\nabla^2$DFT: A Universal Quantum Chemistry Dataset of Drug-Like Molecules and a Benchmark for Neural Network Potentials",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.14347.png",
        "numComments": 4,
        "submittedBy": {
            "avatarUrl": "/avatars/fb7ba8ceddd4c2f900c1361ad06e12d5.svg",
            "fullname": "Artem Tsypin",
            "name": "ofantomas",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.10601",
            "authors": [
                {
                    "_id": "66718953bb244a42b6ebc340",
                    "name": "Denis Bobkov",
                    "hidden": false
                },
                {
                    "_id": "66718953bb244a42b6ebc341",
                    "name": "Vadim Titov",
                    "hidden": false
                },
                {
                    "_id": "66718953bb244a42b6ebc342",
                    "user": {
                        "avatarUrl": "/avatars/03707f5ea4e2aa8dc825a9782b00ed85.svg",
                        "isPro": false,
                        "fullname": "Aibek Alanov",
                        "user": "ai-alanov",
                        "type": "user"
                    },
                    "name": "Aibek Alanov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-20T10:19:29.679Z",
                    "hidden": false
                },
                {
                    "_id": "66718953bb244a42b6ebc343",
                    "name": "Dmitry Vetrov",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-15T11:28:32.000Z",
            "title": "The Devil is in the Details: StyleFeatureEditor for Detail-Rich StyleGAN\n  Inversion and High Quality Image Editing",
            "summary": "The task of manipulating real image attributes through StyleGAN inversion has\nbeen extensively researched. This process involves searching latent variables\nfrom a well-trained StyleGAN generator that can synthesize a real image,\nmodifying these latent variables, and then synthesizing an image with the\ndesired edits. A balance must be struck between the quality of the\nreconstruction and the ability to edit. Earlier studies utilized the\nlow-dimensional W-space for latent search, which facilitated effective editing\nbut struggled with reconstructing intricate details. More recent research has\nturned to the high-dimensional feature space F, which successfully inverses the\ninput image but loses much of the detail during editing. In this paper, we\nintroduce StyleFeatureEditor -- a novel method that enables editing in both\nw-latents and F-latents. This technique not only allows for the reconstruction\nof finer image details but also ensures their preservation during editing. We\nalso present a new training pipeline specifically designed to train our model\nto accurately edit F-latents. Our method is compared with state-of-the-art\nencoding approaches, demonstrating that our model excels in terms of\nreconstruction quality and is capable of editing even challenging out-of-domain\nexamples. Code is available at\nhttps://github.com/AIRI-Institute/StyleFeatureEditor.",
            "upvotes": 23
        },
        "publishedAt": "2024-06-21T11:27:50.303Z",
        "title": "The Devil is in the Details: StyleFeatureEditor for Detail-Rich StyleGAN Inversion and High Quality Image Editing",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/66680c6451545a8b46c6fd21/0VMF3FQ-pTWO-py7w1ivE.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.10601.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "/avatars/03707f5ea4e2aa8dc825a9782b00ed85.svg",
            "fullname": "Aibek Alanov",
            "name": "ai-alanov",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.14544",
            "authors": [
                {
                    "_id": "6675136b3ae9e9c659a0266e",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/663cd4e3c1b4f7297c5dd958/XZdsR1DTc9Njh8XnesTg1.jpeg",
                        "isPro": false,
                        "fullname": "Yuxuan Qiao",
                        "user": "Yuxuan-Qiao",
                        "type": "user"
                    },
                    "name": "Yuxuan Qiao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-21T07:05:37.653Z",
                    "hidden": false
                },
                {
                    "_id": "6675136b3ae9e9c659a0266f",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676546883247-noauth.png",
                        "isPro": false,
                        "fullname": "HAODONG DUAN",
                        "user": "KennyUTC",
                        "type": "user"
                    },
                    "name": "Haodong Duan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-21T07:57:12.383Z",
                    "hidden": false
                },
                {
                    "_id": "6675136b3ae9e9c659a02670",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f5f8dd9b17cd59c453c57f/MulhwLcePFUWUQel8LQZ8.jpeg",
                        "isPro": false,
                        "fullname": "xinyu fang",
                        "user": "nebulae09",
                        "type": "user"
                    },
                    "name": "Xinyu Fang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-21T07:00:18.009Z",
                    "hidden": false
                },
                {
                    "_id": "6675136b3ae9e9c659a02671",
                    "user": {
                        "avatarUrl": "/avatars/698c03b9a4bb69659d2ed594626e3895.svg",
                        "isPro": false,
                        "fullname": "junmingyang",
                        "user": "jmyang",
                        "type": "user"
                    },
                    "name": "Junming Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-21T13:33:23.625Z",
                    "hidden": false
                },
                {
                    "_id": "6675136b3ae9e9c659a02672",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b02ec0e5000ae8a572ced5/h5T5c14F-lHmx3X5K67A2.jpeg",
                        "isPro": true,
                        "fullname": "Lin Chen",
                        "user": "Lin-Chen",
                        "type": "user"
                    },
                    "name": "Lin Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-21T07:05:31.876Z",
                    "hidden": false
                },
                {
                    "_id": "6675136b3ae9e9c659a02673",
                    "user": {
                        "avatarUrl": "/avatars/2d36a880ce4a3cf7efc5ff3987dbeaf3.svg",
                        "isPro": false,
                        "fullname": "Songyang Zhang",
                        "user": "zsytony",
                        "type": "user"
                    },
                    "name": "Songyang Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-21T07:57:59.726Z",
                    "hidden": false
                },
                {
                    "_id": "6675136b3ae9e9c659a02674",
                    "user": {
                        "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
                        "isPro": true,
                        "fullname": "Jiaqi Wang",
                        "user": "myownskyW7",
                        "type": "user"
                    },
                    "name": "Jiaqi Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-21T07:58:34.023Z",
                    "hidden": false
                },
                {
                    "_id": "6675136b3ae9e9c659a02675",
                    "user": {
                        "avatarUrl": "/avatars/3db090e101b916d9256d0d3e043db71d.svg",
                        "isPro": false,
                        "fullname": "Dahua Lin",
                        "user": "lindahua",
                        "type": "user"
                    },
                    "name": "Dahua Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-21T10:19:04.717Z",
                    "hidden": false
                },
                {
                    "_id": "6675136b3ae9e9c659a02676",
                    "name": "Kai Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-20T17:54:03.000Z",
            "title": "Prism: A Framework for Decoupling and Assessing the Capabilities of VLMs",
            "summary": "Vision Language Models (VLMs) demonstrate remarkable proficiency in\naddressing a wide array of visual questions, which requires strong perception\nand reasoning faculties. Assessing these two competencies independently is\ncrucial for model refinement, despite the inherent difficulty due to the\nintertwined nature of seeing and reasoning in existing VLMs. To tackle this\nissue, we present Prism, an innovative framework designed to disentangle the\nperception and reasoning processes involved in visual question solving. Prism\ncomprises two distinct stages: a perception stage that utilizes a VLM to\nextract and articulate visual information in textual form, and a reasoning\nstage that formulates responses based on the extracted visual information using\na Large Language Model (LLM). This modular design enables the systematic\ncomparison and assessment of both proprietary and open-source VLM for their\nperception and reasoning strengths. Our analytical framework provides several\nvaluable insights, underscoring Prism's potential as a cost-effective solution\nfor vision-language tasks. By combining a streamlined VLM focused on perception\nwith a powerful LLM tailored for reasoning, Prism achieves superior results in\ngeneral vision-language tasks while substantially cutting down on training and\noperational expenses. Quantitative evaluations show that Prism, when configured\nwith a vanilla 2B LLaVA and freely accessible GPT-3.5, delivers performance on\npar with VLMs 10 times larger on the rigorous multimodal benchmark MMStar.\nThe project is released at: https://github.com/SparksJoe/Prism.",
            "upvotes": 22
        },
        "publishedAt": "2024-06-21T04:15:41.724Z",
        "title": "Prism: A Framework for Decoupling and Assessing the Capabilities of VLMs",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.14544.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b02ec0e5000ae8a572ced5/h5T5c14F-lHmx3X5K67A2.jpeg",
            "fullname": "Lin Chen",
            "name": "Lin-Chen",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.14515",
            "authors": [
                {
                    "_id": "667514ebdac349236b1d31d4",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f5f8dd9b17cd59c453c57f/MulhwLcePFUWUQel8LQZ8.jpeg",
                        "isPro": false,
                        "fullname": "xinyu fang",
                        "user": "nebulae09",
                        "type": "user"
                    },
                    "name": "Xinyu Fang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-21T07:00:13.950Z",
                    "hidden": false
                },
                {
                    "_id": "667514ebdac349236b1d31d5",
                    "user": {
                        "avatarUrl": "/avatars/86001ca10b50b0a1f8794521509348f7.svg",
                        "isPro": false,
                        "fullname": "Kangrui Mao",
                        "user": "karrymao",
                        "type": "user"
                    },
                    "name": "Kangrui Mao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-21T07:55:57.855Z",
                    "hidden": false
                },
                {
                    "_id": "667514ebdac349236b1d31d6",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676546883247-noauth.png",
                        "isPro": false,
                        "fullname": "HAODONG DUAN",
                        "user": "KennyUTC",
                        "type": "user"
                    },
                    "name": "Haodong Duan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-21T07:56:04.905Z",
                    "hidden": false
                },
                {
                    "_id": "667514ebdac349236b1d31d7",
                    "user": {
                        "avatarUrl": "/avatars/efc93bc767e561c6c6d429f65c23382d.svg",
                        "isPro": false,
                        "fullname": "Xiangyu Z",
                        "user": "PhoenixZ",
                        "type": "user"
                    },
                    "name": "Xiangyu Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-21T07:00:16.135Z",
                    "hidden": false
                },
                {
                    "_id": "667514ebdac349236b1d31d8",
                    "user": {
                        "avatarUrl": "/avatars/cf3e7f7c5bf790f93fc6b4cb5d72c915.svg",
                        "isPro": false,
                        "fullname": "Yining Li",
                        "user": "ANTiSOC1AL",
                        "type": "user"
                    },
                    "name": "Yining Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-21T07:56:24.200Z",
                    "hidden": false
                },
                {
                    "_id": "667514ebdac349236b1d31d9",
                    "user": {
                        "avatarUrl": "/avatars/3db090e101b916d9256d0d3e043db71d.svg",
                        "isPro": false,
                        "fullname": "Dahua Lin",
                        "user": "lindahua",
                        "type": "user"
                    },
                    "name": "Dahua Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-21T07:56:30.981Z",
                    "hidden": false
                },
                {
                    "_id": "667514ebdac349236b1d31da",
                    "name": "Kai Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-20T17:26:01.000Z",
            "title": "MMBench-Video: A Long-Form Multi-Shot Benchmark for Holistic Video\n  Understanding",
            "summary": "The advent of large vision-language models (LVLMs) has spurred research into\ntheir applications in multi-modal contexts, particularly in video\nunderstanding. Traditional VideoQA benchmarks, despite providing quantitative\nmetrics, often fail to encompass the full spectrum of video content and\ninadequately assess models' temporal comprehension. To address these\nlimitations, we introduce MMBench-Video, a quantitative benchmark designed to\nrigorously evaluate LVLMs' proficiency in video understanding. MMBench-Video\nincorporates lengthy videos from YouTube and employs free-form questions,\nmirroring practical use cases. The benchmark is meticulously crafted to probe\nthe models' temporal reasoning skills, with all questions human-annotated\naccording to a carefully constructed ability taxonomy. We employ GPT-4 for\nautomated assessment, demonstrating superior accuracy and robustness over\nearlier LLM-based evaluations. Utilizing MMBench-Video, we have conducted\ncomprehensive evaluations that include both proprietary and open-source LVLMs\nfor images and videos. MMBench-Video stands as a valuable resource for the\nresearch community, facilitating improved evaluation of LVLMs and catalyzing\nprogress in the field of video understanding. The evalutation code of\nMMBench-Video will be integrated into VLMEvalKit:\nhttps://github.com/open-compass/VLMEvalKit.",
            "upvotes": 21
        },
        "publishedAt": "2024-06-21T04:29:04.389Z",
        "title": "MMBench-Video: A Long-Form Multi-Shot Benchmark for Holistic Video Understanding",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.14515.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676546883247-noauth.png",
            "fullname": "HAODONG DUAN",
            "name": "KennyUTC",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.11410",
            "authors": [
                {
                    "_id": "66717c101f4de3ba8b1dfbb1",
                    "user": {
                        "avatarUrl": "/avatars/4c1b1e0628d2d0f321ba5fe1a0ec8c03.svg",
                        "isPro": false,
                        "fullname": "Zhang",
                        "user": "lingyun1",
                        "type": "user"
                    },
                    "name": "Lingyun Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-21T07:07:05.113Z",
                    "hidden": false
                },
                {
                    "_id": "66717c101f4de3ba8b1dfbb2",
                    "name": "Bin jin",
                    "hidden": false
                },
                {
                    "_id": "66717c101f4de3ba8b1dfbb3",
                    "user": {
                        "avatarUrl": "/avatars/60a8a84199cb67b4276cd83bbe1991a6.svg",
                        "isPro": false,
                        "fullname": "gegaojian",
                        "user": "fo1ge",
                        "type": "user"
                    },
                    "name": "Gaojian Ge",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-21T11:22:57.410Z",
                    "hidden": false
                },
                {
                    "_id": "66717c101f4de3ba8b1dfbb4",
                    "user": {
                        "avatarUrl": "/avatars/632f1cd0adededc1d495b9eb55d8adc2.svg",
                        "isPro": false,
                        "fullname": "LunHui Liu",
                        "user": "LLHLLH",
                        "type": "user"
                    },
                    "name": "Lunhui Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-21T11:23:04.909Z",
                    "hidden": false
                },
                {
                    "_id": "66717c101f4de3ba8b1dfbb5",
                    "name": "Xuewen Shen",
                    "hidden": false
                },
                {
                    "_id": "66717c101f4de3ba8b1dfbb6",
                    "user": {
                        "avatarUrl": "/avatars/a5a7c688de2f5f4fdef6b9ae91b98043.svg",
                        "isPro": false,
                        "fullname": "wumingyong",
                        "user": "wumingyong",
                        "type": "user"
                    },
                    "name": "Mingyong Wu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-21T11:23:24.373Z",
                    "hidden": false
                },
                {
                    "_id": "66717c101f4de3ba8b1dfbb7",
                    "name": "Houqian Zhang",
                    "hidden": false
                },
                {
                    "_id": "66717c101f4de3ba8b1dfbb8",
                    "name": "Yongneng Jiang",
                    "hidden": false
                },
                {
                    "_id": "66717c101f4de3ba8b1dfbb9",
                    "user": {
                        "avatarUrl": "/avatars/bcd2aa1823995b385096e2a68ce3e071.svg",
                        "isPro": false,
                        "fullname": "Shiqi Chen",
                        "user": "ShiqiChen",
                        "type": "user"
                    },
                    "name": "Shiqi Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-21T11:23:48.808Z",
                    "hidden": false
                },
                {
                    "_id": "66717c101f4de3ba8b1dfbba",
                    "user": {
                        "avatarUrl": "/avatars/8d3a9f053e99597c82d8d5ab491d913f.svg",
                        "isPro": false,
                        "fullname": "Shi Pu",
                        "user": "pushi",
                        "type": "user"
                    },
                    "name": "Shi Pu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-21T11:24:19.644Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-17T10:56:03.000Z",
            "title": "HARE: HumAn pRiors, a key to small language model Efficiency",
            "summary": "Human priors play a crucial role in efficiently utilizing data in deep\nlearning. However, with the development of large language models (LLMs), there\nis an increasing emphasis on scaling both model size and data volume, which\noften diminishes the importance of human priors in data construction.\nInfluenced by these trends, existing Small Language Models (SLMs) mainly rely\non web-scraped large-scale training data, neglecting the proper incorporation\nof human priors. This oversight limits the training efficiency of language\nmodels in resource-constrained settings. In this paper, we propose a principle\nto leverage human priors for data construction. This principle emphasizes\nachieving high-performance SLMs by training on a concise dataset that\naccommodates both semantic diversity and data quality consistency, while\navoiding benchmark data leakage. Following this principle, we train an SLM\nnamed HARE-1.1B. Extensive experiments on large-scale benchmark datasets\ndemonstrate that HARE-1.1B performs favorably against state-of-the-art SLMs,\nvalidating the effectiveness of the proposed principle. Additionally, this\nprovides new insights into efficient language model training in\nresource-constrained environments from the view of human priors.",
            "upvotes": 18
        },
        "publishedAt": "2024-06-21T06:57:48.237Z",
        "title": "HARE: HumAn pRiors, a key to small language model Efficiency",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.11410.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/4c1b1e0628d2d0f321ba5fe1a0ec8c03.svg",
            "fullname": "Zhang",
            "name": "lingyun1",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.14539",
            "authors": [
                {
                    "_id": "66753d0314e2aebef8bb1c98",
                    "name": "Nikita Starodubcev",
                    "hidden": false
                },
                {
                    "_id": "66753d0314e2aebef8bb1c99",
                    "user": {
                        "avatarUrl": "/avatars/a51b630e8d01224d5d8d5edff0a69d30.svg",
                        "isPro": false,
                        "fullname": "Mikhail Khoroshikh",
                        "user": "michellemoorre",
                        "type": "user"
                    },
                    "name": "Mikhail Khoroshikh",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-21T11:15:06.540Z",
                    "hidden": false
                },
                {
                    "_id": "66753d0314e2aebef8bb1c9a",
                    "name": "Artem Babenko",
                    "hidden": false
                },
                {
                    "_id": "66753d0314e2aebef8bb1c9b",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b6cc49752323892323bc04/gGBld1KJIP9AIpd81L3PC.jpeg",
                        "isPro": false,
                        "fullname": "Dmitry Baranchuk",
                        "user": "dbaranchuk",
                        "type": "user"
                    },
                    "name": "Dmitry Baranchuk",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-21T11:14:56.342Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-20T17:49:11.000Z",
            "title": "Invertible Consistency Distillation for Text-Guided Image Editing in\n  Around 7 Steps",
            "summary": "Diffusion distillation represents a highly promising direction for achieving\nfaithful text-to-image generation in a few sampling steps. However, despite\nrecent successes, existing distilled models still do not provide the full\nspectrum of diffusion abilities, such as real image inversion, which enables\nmany precise image manipulation methods. This work aims to enrich distilled\ntext-to-image diffusion models with the ability to effectively encode real\nimages into their latent space. To this end, we introduce invertible\nConsistency Distillation (iCD), a generalized consistency distillation\nframework that facilitates both high-quality image synthesis and accurate image\nencoding in only 3-4 inference steps. Though the inversion problem for\ntext-to-image diffusion models gets exacerbated by high classifier-free\nguidance scales, we notice that dynamic guidance significantly reduces\nreconstruction errors without noticeable degradation in generation performance.\nAs a result, we demonstrate that iCD equipped with dynamic guidance may serve\nas a highly effective tool for zero-shot text-guided image editing, competing\nwith more expensive state-of-the-art alternatives.",
            "upvotes": 17
        },
        "publishedAt": "2024-06-21T07:21:36.573Z",
        "title": "Invertible Consistency Distillation for Text-Guided Image Editing in Around 7 Steps",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.14539.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b6cc49752323892323bc04/gGBld1KJIP9AIpd81L3PC.jpeg",
            "fullname": "Dmitry Baranchuk",
            "name": "dbaranchuk",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.14491",
            "authors": [
                {
                    "_id": "6674ef905f7d5c8af70b5607",
                    "user": {
                        "avatarUrl": "/avatars/00b5dcb744c54a4aa18fe08efd70d6ff.svg",
                        "isPro": false,
                        "fullname": "Daixuan Cheng",
                        "user": "daixuancheng",
                        "type": "user"
                    },
                    "name": "Daixuan Cheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-21T07:05:43.020Z",
                    "hidden": false
                },
                {
                    "_id": "6674ef905f7d5c8af70b5608",
                    "user": {
                        "avatarUrl": "/avatars/8de6e319246500c460cf41163462c214.svg",
                        "isPro": false,
                        "fullname": "Yuxian Gu",
                        "user": "t1101675",
                        "type": "user"
                    },
                    "name": "Yuxian Gu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-21T11:12:42.585Z",
                    "hidden": false
                },
                {
                    "_id": "6674ef905f7d5c8af70b5609",
                    "user": {
                        "avatarUrl": "/avatars/6e1533e8a599f3068290aa69ac82cab7.svg",
                        "isPro": false,
                        "fullname": "HUANG SHAOHAN",
                        "user": "buaahsh",
                        "type": "user"
                    },
                    "name": "Shaohan Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-21T11:12:55.699Z",
                    "hidden": false
                },
                {
                    "_id": "6674ef905f7d5c8af70b560a",
                    "name": "Junyu Bi",
                    "hidden": false
                },
                {
                    "_id": "6674ef905f7d5c8af70b560b",
                    "name": "Minlie Huang",
                    "hidden": false
                },
                {
                    "_id": "6674ef905f7d5c8af70b560c",
                    "user": {
                        "avatarUrl": "/avatars/1c23bc7c0b6d9225699ce27647623d7a.svg",
                        "isPro": false,
                        "fullname": "Furu Wei",
                        "user": "thegenerality",
                        "type": "user"
                    },
                    "name": "Furu Wei",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-21T11:13:45.390Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-20T16:55:33.000Z",
            "title": "Instruction Pre-Training: Language Models are Supervised Multitask\n  Learners",
            "summary": "Unsupervised multitask pre-training has been the critical method behind the\nrecent success of language models (LMs). However, supervised multitask learning\nstill holds significant promise, as scaling it in the post-training stage\ntrends towards better generalization. In this paper, we explore supervised\nmultitask pre-training by proposing Instruction Pre-Training, a framework that\nscalably augments massive raw corpora with instruction-response pairs to\npre-train LMs. The instruction-response pairs are generated by an efficient\ninstruction synthesizer built on open-source models. In our experiments, we\nsynthesize 200M instruction-response pairs covering 40+ task categories to\nverify the effectiveness of Instruction Pre-Training. In pre-training from\nscratch, Instruction Pre-Training not only consistently enhances pre-trained\nbase models but also benefits more from further instruction tuning. In\ncontinual pre-training, Instruction Pre-Training enables Llama3-8B to be\ncomparable to or even outperform Llama3-70B. Our model, code, and data are\navailable at https://github.com/microsoft/LMOps.",
            "upvotes": 17
        },
        "publishedAt": "2024-06-21T01:42:56.560Z",
        "title": "Instruction Pre-Training: Language Models are Supervised Multitask Learners",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.14491.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/00b5dcb744c54a4aa18fe08efd70d6ff.svg",
            "fullname": "Daixuan Cheng",
            "name": "daixuancheng",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.13923",
            "authors": [
                {
                    "_id": "6674e8e6dbe75ba786077cc6",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1650349345760-62579c55b98dcaa7e0de285d.jpeg",
                        "isPro": false,
                        "fullname": "wangjunjie",
                        "user": "wanng",
                        "type": "user"
                    },
                    "name": "Junjie Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-21T07:05:45.156Z",
                    "hidden": false
                },
                {
                    "_id": "6674e8e6dbe75ba786077cc7",
                    "user": {
                        "avatarUrl": "/avatars/09f78f8fd533e13ab7e54b5468e22da4.svg",
                        "isPro": false,
                        "fullname": "yin zhang",
                        "user": "miracleyin",
                        "type": "user"
                    },
                    "name": "Yin Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-21T08:12:03.713Z",
                    "hidden": false
                },
                {
                    "_id": "6674e8e6dbe75ba786077cc8",
                    "user": {
                        "avatarUrl": "/avatars/e4e6155e682d0d690140483f77cbd63b.svg",
                        "isPro": false,
                        "fullname": "yatai ji",
                        "user": "jiyatai",
                        "type": "user"
                    },
                    "name": "Yatai Ji",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-21T08:12:10.313Z",
                    "hidden": false
                },
                {
                    "_id": "6674e8e6dbe75ba786077cc9",
                    "name": "Yuxiang Zhang",
                    "hidden": false
                },
                {
                    "_id": "6674e8e6dbe75ba786077cca",
                    "name": "Chunyang Jiang",
                    "hidden": false
                },
                {
                    "_id": "6674e8e6dbe75ba786077ccb",
                    "name": "Yubo Wang",
                    "hidden": false
                },
                {
                    "_id": "6674e8e6dbe75ba786077ccc",
                    "name": "Kang Zhu",
                    "hidden": false
                },
                {
                    "_id": "6674e8e6dbe75ba786077ccd",
                    "name": "Zekun Wang",
                    "hidden": false
                },
                {
                    "_id": "6674e8e6dbe75ba786077cce",
                    "user": {
                        "avatarUrl": "/avatars/703dd06469aaac724c94f622262b14e8.svg",
                        "isPro": false,
                        "fullname": "Tiezhen WANG",
                        "user": "xianbao",
                        "type": "user"
                    },
                    "name": "Tiezhen Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-21T08:24:20.397Z",
                    "hidden": false
                },
                {
                    "_id": "6674e8e6dbe75ba786077ccf",
                    "name": "Wenhao Huang",
                    "hidden": false
                },
                {
                    "_id": "6674e8e6dbe75ba786077cd0",
                    "name": "Jie Fu",
                    "hidden": false
                },
                {
                    "_id": "6674e8e6dbe75ba786077cd1",
                    "name": "Bei Chen",
                    "hidden": false
                },
                {
                    "_id": "6674e8e6dbe75ba786077cd2",
                    "name": "Qunshu Lin",
                    "hidden": false
                },
                {
                    "_id": "6674e8e6dbe75ba786077cd3",
                    "user": {
                        "avatarUrl": "/avatars/ec8e6362644568fb4a29973fbbb30178.svg",
                        "isPro": false,
                        "fullname": "minghao",
                        "user": "Liam-Liu",
                        "type": "user"
                    },
                    "name": "Minghao Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-21T11:47:38.165Z",
                    "hidden": false
                },
                {
                    "_id": "6674e8e6dbe75ba786077cd4",
                    "user": {
                        "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
                        "isPro": false,
                        "fullname": "Ge Zhang",
                        "user": "zhangysk",
                        "type": "user"
                    },
                    "name": "Ge Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-21T07:05:47.149Z",
                    "hidden": false
                },
                {
                    "_id": "6674e8e6dbe75ba786077cd5",
                    "name": "Wenhu Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-20T01:43:08.000Z",
            "title": "PIN: A Knowledge-Intensive Dataset for Paired and Interleaved Multimodal\n  Documents",
            "summary": "Recent advancements in Large Multimodal Models (LMMs) have leveraged\nextensive multimodal datasets to enhance capabilities in complex\nknowledge-driven tasks. However, persistent challenges in perceptual and\nreasoning errors limit their efficacy, particularly in interpreting intricate\nvisual data and deducing multimodal relationships. Addressing these issues, we\nintroduce a novel dataset format, PIN (Paired and INterleaved multimodal\ndocuments), designed to significantly improve both the depth and breadth of\nmultimodal training. The PIN format is built on three foundational principles:\nknowledge intensity, scalability, and support for diverse training modalities.\nThis innovative format combines markdown files and comprehensive images to\nenrich training data with a dense knowledge structure and versatile training\nstrategies. We present PIN-14M, an open-source dataset comprising 14 million\nsamples derived from a diverse range of Chinese and English sources, tailored\nto include complex web and scientific content. This dataset is constructed\nmeticulously to ensure data quality and ethical integrity, aiming to facilitate\nadvanced training strategies and improve model robustness against common\nmultimodal training pitfalls. Our initial results, forming the basis of this\ntechnical report, suggest significant potential for the PIN format in refining\nLMM performance, with plans for future expansions and detailed evaluations of\nits impact on model capabilities.",
            "upvotes": 16
        },
        "publishedAt": "2024-06-21T04:00:24.609Z",
        "title": "PIN: A Knowledge-Intensive Dataset for Paired and Interleaved Multimodal Documents",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/638efcf4c67af472d316d424/FzczjaW86tUS30oltb9Hh.jpeg",
            "https://cdn-uploads.huggingface.co/production/uploads/638efcf4c67af472d316d424/e_9az0zX2L3cl0vdbT4Ys.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.13923.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
            "fullname": "Ge Zhang",
            "name": "zhangysk",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.14562",
            "authors": [
                {
                    "_id": "667517fdd229a7f83090560e",
                    "user": {
                        "avatarUrl": "/avatars/f4e8cf1e9145a1df39082544eb709264.svg",
                        "isPro": false,
                        "fullname": "Sachit Menon",
                        "user": "sachit-menon",
                        "type": "user"
                    },
                    "name": "Sachit Menon",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2024-06-21T06:28:17.611Z",
                    "hidden": false
                },
                {
                    "_id": "667517fdd229a7f83090560f",
                    "name": "Richard Zemel",
                    "hidden": false
                },
                {
                    "_id": "667517fdd229a7f830905610",
                    "user": {
                        "avatarUrl": "/avatars/9d978a6f0caf6ad92ec4138a84a39e53.svg",
                        "isPro": false,
                        "fullname": "Carl Vondrick",
                        "user": "cvondrick",
                        "type": "user"
                    },
                    "name": "Carl Vondrick",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-21T11:40:33.375Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-20T17:59:45.000Z",
            "title": "Whiteboard-of-Thought: Thinking Step-by-Step Across Modalities",
            "summary": "When presented with questions involving visual thinking, humans naturally\nswitch reasoning modalities, often forming mental images or drawing visual\naids. Large language models have shown promising results in arithmetic and\nsymbolic reasoning by expressing intermediate reasoning in text as a chain of\nthought, yet struggle to extend this capability to answer text queries that are\neasily solved by visual reasoning, even with extensive multimodal pretraining.\nWe introduce a simple method, whiteboard-of-thought prompting, to unlock the\nvisual reasoning capabilities of multimodal large language models across\nmodalities. Whiteboard-of-thought prompting provides multimodal large language\nmodels with a metaphorical `whiteboard' to draw out reasoning steps as images,\nthen returns these images back to the model for further processing. We find\nthis can be accomplished with no demonstrations or specialized modules, instead\nleveraging models' existing ability to write code with libraries such as\nMatplotlib and Turtle. This simple approach shows state-of-the-art results on\nfour difficult natural language tasks that involve visual and spatial\nreasoning. We identify multiple settings where GPT-4o using chain-of-thought\nfails dramatically, including more than one where it achieves 0% accuracy,\nwhile whiteboard-of-thought enables up to 92% accuracy in these same\nsettings. We present a detailed exploration of where the technique succeeds as\nwell as its sources of error.",
            "upvotes": 13
        },
        "publishedAt": "2024-06-21T04:36:28.642Z",
        "title": "Whiteboard-of-Thought: Thinking Step-by-Step Across Modalities",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.14562.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/f4e8cf1e9145a1df39082544eb709264.svg",
            "fullname": "Sachit Menon",
            "name": "sachit-menon",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.14563",
            "authors": [
                {
                    "_id": "6675a373a223121b3da870a0",
                    "name": "Hasan Abed Al Kader Hammoud",
                    "hidden": false
                },
                {
                    "_id": "6675a373a223121b3da870a1",
                    "name": "Umberto Michieli",
                    "hidden": false
                },
                {
                    "_id": "6675a373a223121b3da870a2",
                    "name": "Fabio Pizzati",
                    "hidden": false
                },
                {
                    "_id": "6675a373a223121b3da870a3",
                    "name": "Philip Torr",
                    "hidden": false
                },
                {
                    "_id": "6675a373a223121b3da870a4",
                    "name": "Adel Bibi",
                    "hidden": false
                },
                {
                    "_id": "6675a373a223121b3da870a5",
                    "name": "Bernard Ghanem",
                    "hidden": false
                },
                {
                    "_id": "6675a373a223121b3da870a6",
                    "name": "Mete Ozay",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-20T17:59:58.000Z",
            "title": "Model Merging and Safety Alignment: One Bad Model Spoils the Bunch",
            "summary": "Merging Large Language Models (LLMs) is a cost-effective technique for\ncombining multiple expert LLMs into a single versatile model, retaining the\nexpertise of the original ones. However, current approaches often overlook the\nimportance of safety alignment during merging, leading to highly misaligned\nmodels. This work investigates the effects of model merging on alignment. We\nevaluate several popular model merging techniques, demonstrating that existing\nmethods do not only transfer domain expertise but also propagate misalignment.\nWe propose a simple two-step approach to address this problem: (i) generating\nsynthetic safety and domain-specific data, and (ii) incorporating these\ngenerated data into the optimization process of existing data-aware model\nmerging techniques. This allows us to treat alignment as a skill that can be\nmaximized in the resulting merged LLM. Our experiments illustrate the\neffectiveness of integrating alignment-related data during merging, resulting\nin models that excel in both domain expertise and alignment.",
            "upvotes": 7
        },
        "publishedAt": "2024-06-21T14:32:29.697Z",
        "title": "Model Merging and Safety Alignment: One Bad Model Spoils the Bunch",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/642b51385bf2355d02a23d15/tz9ehkRr8OtOkbZpvtJtU.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.14563.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/87985347643b2647555f2453fa4d94fb.svg",
            "fullname": "Hasan Abed Al Kader Hammoud",
            "name": "hammh0a",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.13621",
            "authors": [
                {
                    "_id": "66756732c411cf3d84747370",
                    "user": {
                        "avatarUrl": "/avatars/a48f6a085b8a69fa0a29847fc5ae9065.svg",
                        "isPro": false,
                        "fullname": "Guy Yariv",
                        "user": "GuyYariv",
                        "type": "user"
                    },
                    "name": "Guy Yariv",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-21T11:47:41.536Z",
                    "hidden": false
                },
                {
                    "_id": "66756732c411cf3d84747371",
                    "name": "Idan Schwartz",
                    "hidden": false
                },
                {
                    "_id": "66756732c411cf3d84747372",
                    "name": "Yossi Adi",
                    "hidden": false
                },
                {
                    "_id": "66756732c411cf3d84747373",
                    "name": "Sagie Benaim",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-19T15:17:10.000Z",
            "title": "Improving Visual Commonsense in Language Models via Multiple Image\n  Generation",
            "summary": "Commonsense reasoning is fundamentally based on multimodal knowledge.\nHowever, existing large language models (LLMs) are primarily trained using\ntextual data only, limiting their ability to incorporate essential visual\ninformation. In contrast, Visual Language Models, which excel at\nvisually-oriented tasks, often fail at non-visual tasks such as basic\ncommonsense reasoning. This divergence highlights a critical challenge - the\nintegration of robust visual understanding with foundational text-based\nlanguage reasoning. To this end, we introduce a method aimed at enhancing LLMs'\nvisual commonsense. Specifically, our method generates multiple images based on\nthe input text prompt and integrates these into the model's decision-making\nprocess by mixing their prediction probabilities. To facilitate multimodal\ngrounded language modeling, we employ a late-fusion layer that combines the\nprojected visual features with the output of a pre-trained LLM conditioned on\ntext only. This late-fusion layer enables predictions based on comprehensive\nimage-text knowledge as well as text only when this is required. We evaluate\nour approach using several visual commonsense reasoning tasks together with\ntraditional NLP tasks, including common sense reasoning and reading\ncomprehension. Our experimental results demonstrate significant superiority\nover existing baselines. When applied to recent state-of-the-art LLMs (e.g.,\nLlama3), we observe improvements not only in visual common sense but also in\ntraditional NLP benchmarks. Code and models are available under\nhttps://github.com/guyyariv/vLMIG.",
            "upvotes": 7
        },
        "publishedAt": "2024-06-21T10:13:56.245Z",
        "title": "Improving Visual Commonsense in Language Models via Multiple Image Generation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.13621.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/a48f6a085b8a69fa0a29847fc5ae9065.svg",
            "fullname": "Guy Yariv",
            "name": "GuyYariv",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.13542",
            "authors": [
                {
                    "_id": "66755c0fd5ac75b591f746da",
                    "user": {
                        "avatarUrl": "/avatars/23f7a65c6bf56b5ab0f455fea7bb70bf.svg",
                        "isPro": false,
                        "fullname": "guanting dong",
                        "user": "dongguanting",
                        "type": "user"
                    },
                    "name": "Guanting Dong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-21T12:22:03.427Z",
                    "hidden": false
                },
                {
                    "_id": "66755c0fd5ac75b591f746db",
                    "user": {
                        "avatarUrl": "/avatars/e8c9025ef24cec958c87a1008bb54fd7.svg",
                        "isPro": false,
                        "fullname": "Keming Lu",
                        "user": "keminglu",
                        "type": "user"
                    },
                    "name": "Keming Lu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-21T12:22:09.957Z",
                    "hidden": false
                },
                {
                    "_id": "66755c0fd5ac75b591f746dc",
                    "user": {
                        "avatarUrl": "/avatars/75d21e20b711b871616ef3850bb900b7.svg",
                        "isPro": false,
                        "fullname": "ChengpengLi",
                        "user": "ChengpengLi",
                        "type": "user"
                    },
                    "name": "Chengpeng Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-21T12:22:19.159Z",
                    "hidden": false
                },
                {
                    "_id": "66755c0fd5ac75b591f746dd",
                    "name": "Tingyu Xia",
                    "hidden": false
                },
                {
                    "_id": "66755c0fd5ac75b591f746de",
                    "name": "Bowen Yu",
                    "hidden": false
                },
                {
                    "_id": "66755c0fd5ac75b591f746df",
                    "name": "Chang Zhou",
                    "hidden": false
                },
                {
                    "_id": "66755c0fd5ac75b591f746e0",
                    "user": {
                        "avatarUrl": "/avatars/b78f0e583df8e5d5e3365934fe5f4900.svg",
                        "isPro": false,
                        "fullname": "Zhou",
                        "user": "Jingren",
                        "type": "user"
                    },
                    "name": "Jingren Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-21T12:22:56.911Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-19T13:29:53.000Z",
            "title": "Self-play with Execution Feedback: Improving Instruction-following\n  Capabilities of Large Language Models",
            "summary": "One core capability of large language models (LLMs) is to follow natural\nlanguage instructions. However, the issue of automatically constructing\nhigh-quality training data to enhance the complex instruction-following\nabilities of LLMs without manual annotation remains unresolved. In this paper,\nwe introduce AutoIF, the first scalable and reliable method for automatically\ngenerating instruction-following training data. AutoIF transforms the\nvalidation of instruction-following data quality into code verification,\nrequiring LLMs to generate instructions, the corresponding code to check the\ncorrectness of the instruction responses, and unit test samples to verify the\ncode's correctness. Then, execution feedback-based rejection sampling can\ngenerate data for Supervised Fine-Tuning (SFT) and Reinforcement Learning from\nHuman Feedback (RLHF) training. AutoIF achieves significant improvements across\nthree training algorithms, SFT, Offline DPO, and Online DPO, when applied to\nthe top open-source LLMs, Qwen2 and LLaMA3, in self-alignment and\nstrong-to-weak distillation settings. Our code is publicly available at\nhttps://github.com/QwenLM/AutoIF.",
            "upvotes": 6
        },
        "publishedAt": "2024-06-21T09:25:55.803Z",
        "title": "Self-play with Execution Feedback: Improving Instruction-following Capabilities of Large Language Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.13542.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1627505688463-60107b385ac3e86b3ea4fc34.jpeg",
            "fullname": "Daniel van Strien",
            "name": "davanstrien",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.12925",
            "authors": [
                {
                    "_id": "667529f0246665be1aa7312a",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1658166666371-noauth.png",
                        "isPro": false,
                        "fullname": "Stepanov",
                        "user": "Ihor",
                        "type": "user"
                    },
                    "name": "Ihor Stepanov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-21T10:24:42.457Z",
                    "hidden": false
                },
                {
                    "_id": "667529f0246665be1aa7312b",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6405f62ba577649430be5124/WMSqNvDqaeydFkHhNCU_K.png",
                        "isPro": false,
                        "fullname": "Mykhailo Shtopko",
                        "user": "BioMike",
                        "type": "user"
                    },
                    "name": "Mykhailo Shtopko",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-21T07:57:48.931Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-14T13:54:29.000Z",
            "title": "GLiNER multi-task: Generalist Lightweight Model for Various Information\n  Extraction Tasks",
            "summary": "Information extraction tasks require both accurate, efficient, and\ngeneralisable models. Classical supervised deep learning approaches can achieve\nthe required performance, but they need large datasets and are limited in their\nability to adapt to different tasks. On the other hand, large language models\n(LLMs) demonstrate good generalization, meaning that they can adapt to many\ndifferent tasks based on user requests. However, LLMs are computationally\nexpensive and tend to fail to generate structured outputs. In this article, we\nwill introduce a new kind of GLiNER model that can be used for various\ninformation extraction tasks while being a small encoder model. Our model\nachieved SoTA performance on zero-shot NER benchmarks and leading performance\non question-answering, summarization and relation extraction tasks.\nAdditionally, in this article, we will cover experimental results on\nself-learning approaches for named entity recognition using GLiNER models.",
            "upvotes": 6
        },
        "publishedAt": "2024-06-21T05:59:35.026Z",
        "title": "GLiNER multi-task: Generalist Lightweight Model for Various Information Extraction Tasks",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/65a4f7be2548c41ad9d4f0a0/reGi8zNmCSDF5Af-I7ZoI.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.12925.png",
        "numComments": 3,
        "submittedBy": {
            "avatarUrl": "/avatars/ddcf1627fd225ee03b421d0f0576ca27.svg",
            "fullname": "Ingvar",
            "name": "whitemetalicdragon",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.11896",
            "authors": [
                {
                    "_id": "66745cfe299133baa5b817a8",
                    "name": "Hao Bai",
                    "hidden": false
                },
                {
                    "_id": "66745cfe299133baa5b817a9",
                    "name": "Yifei Zhou",
                    "hidden": false
                },
                {
                    "_id": "66745cfe299133baa5b817aa",
                    "name": "Mert Cemri",
                    "hidden": false
                },
                {
                    "_id": "66745cfe299133baa5b817ab",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61568f37272f2d87a99ba884/lgvkl5f0rEyiQRVU5FE32.png",
                        "isPro": false,
                        "fullname": "Jiayi Pan",
                        "user": "Jiayi-Pan",
                        "type": "user"
                    },
                    "name": "Jiayi Pan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-21T11:43:25.497Z",
                    "hidden": false
                },
                {
                    "_id": "66745cfe299133baa5b817ac",
                    "name": "Alane Suhr",
                    "hidden": false
                },
                {
                    "_id": "66745cfe299133baa5b817ad",
                    "user": {
                        "avatarUrl": "/avatars/e698726e9be61dd50ce2efe372ed5dac.svg",
                        "isPro": false,
                        "fullname": "Sergey Levine",
                        "user": "svlevine",
                        "type": "user"
                    },
                    "name": "Sergey Levine",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-21T11:47:52.402Z",
                    "hidden": false
                },
                {
                    "_id": "66745cfe299133baa5b817ae",
                    "user": {
                        "avatarUrl": "/avatars/ead1211a56a4e5c14cb8f6b2ea59c0e6.svg",
                        "isPro": false,
                        "fullname": "Aviral Kumar",
                        "user": "aviralkumar",
                        "type": "user"
                    },
                    "name": "Aviral Kumar",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-21T11:48:05.505Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-14T17:49:55.000Z",
            "title": "DigiRL: Training In-The-Wild Device-Control Agents with Autonomous\n  Reinforcement Learning",
            "summary": "Training corpuses for vision language models (VLMs) typically lack sufficient\namounts of decision-centric data. This renders off-the-shelf VLMs sub-optimal\nfor decision-making tasks such as in-the-wild device control through graphical\nuser interfaces (GUIs). While training with static demonstrations has shown\nsome promise, we show that such methods fall short for controlling real GUIs\ndue to their failure to deal with real-world stochasticity and non-stationarity\nnot captured in static observational data. This paper introduces a novel\nautonomous RL approach, called DigiRL, for training in-the-wild device control\nagents through fine-tuning a pre-trained VLM in two stages: offline RL to\ninitialize the model, followed by offline-to-online RL. To do this, we build a\nscalable and parallelizable Android learning environment equipped with a\nVLM-based evaluator and develop a simple yet effective RL approach for learning\nin this domain. Our approach runs advantage-weighted RL with advantage\nestimators enhanced to account for stochasticity along with an automatic\ncurriculum for deriving maximal learning signal. We demonstrate the\neffectiveness of DigiRL using the Android-in-the-Wild (AitW) dataset, where our\n1.3B VLM trained with RL achieves a 49.5% absolute improvement -- from 17.7 to\n67.2% success rate -- over supervised fine-tuning with static human\ndemonstration data. These results significantly surpass not only the prior best\nagents, including AppAgent with GPT-4V (8.3% success rate) and the 17B CogAgent\ntrained with AitW data (38.5%), but also the prior best autonomous RL approach\nbased on filtered behavior cloning (57.8%), thereby establishing a new\nstate-of-the-art for digital agents for in-the-wild device control.",
            "upvotes": 6
        },
        "publishedAt": "2024-06-21T02:54:35.843Z",
        "title": "DigiRL: Training In-The-Wild Device-Control Agents with Autonomous Reinforcement Learning",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/61568f37272f2d87a99ba884/OSxe-KqZs6NAXS5NU4pT2.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.11896.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61568f37272f2d87a99ba884/lgvkl5f0rEyiQRVU5FE32.png",
            "fullname": "Jiayi Pan",
            "name": "Jiayi-Pan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.14130",
            "authors": [
                {
                    "_id": "6674f66fbd6e32596f69e9c8",
                    "user": {
                        "avatarUrl": "/avatars/05293b4047437480da22f4622236d992.svg",
                        "isPro": false,
                        "fullname": "Zhongjie Duan",
                        "user": "Artiprocher",
                        "type": "user"
                    },
                    "name": "Zhongjie Duan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-21T12:08:36.125Z",
                    "hidden": false
                },
                {
                    "_id": "6674f66fbd6e32596f69e9c9",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654828369523-623c6253389748c9f72ca287.jpeg",
                        "isPro": false,
                        "fullname": "wenmeng zhou",
                        "user": "wenmengzhou",
                        "type": "user"
                    },
                    "name": "Wenmeng Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-21T12:08:42.211Z",
                    "hidden": false
                },
                {
                    "_id": "6674f66fbd6e32596f69e9ca",
                    "user": {
                        "avatarUrl": "/avatars/3e2a714106db5b90b6f99b0cc3b984c2.svg",
                        "isPro": false,
                        "fullname": "chencen",
                        "user": "clh13600",
                        "type": "user"
                    },
                    "name": "Cen Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-21T12:09:24.261Z",
                    "hidden": false
                },
                {
                    "_id": "6674f66fbd6e32596f69e9cb",
                    "name": "Yaliang Li",
                    "hidden": false
                },
                {
                    "_id": "6674f66fbd6e32596f69e9cc",
                    "user": {
                        "avatarUrl": "/avatars/3eeb0b8ea703d3c873839ad0fd062352.svg",
                        "isPro": false,
                        "fullname": "Weining Qian",
                        "user": "wnqian",
                        "type": "user"
                    },
                    "name": "Weining Qian",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-21T12:09:06.181Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-20T09:18:54.000Z",
            "title": "ExVideo: Extending Video Diffusion Models via Parameter-Efficient\n  Post-Tuning",
            "summary": "Recently, advancements in video synthesis have attracted significant\nattention. Video synthesis models such as AnimateDiff and Stable Video\nDiffusion have demonstrated the practical applicability of diffusion models in\ncreating dynamic visual content. The emergence of SORA has further spotlighted\nthe potential of video generation technologies. Nonetheless, the extension of\nvideo lengths has been constrained by the limitations in computational\nresources. Most existing video synthesis models can only generate short video\nclips. In this paper, we propose a novel post-tuning methodology for video\nsynthesis models, called ExVideo. This approach is designed to enhance the\ncapability of current video synthesis models, allowing them to produce content\nover extended temporal durations while incurring lower training expenditures.\nIn particular, we design extension strategies across common temporal model\narchitectures respectively, including 3D convolution, temporal attention, and\npositional embedding. To evaluate the efficacy of our proposed post-tuning\napproach, we conduct extension training on the Stable Video Diffusion model.\nOur approach augments the model's capacity to generate up to 5times its\noriginal number of frames, requiring only 1.5k GPU hours of training on a\ndataset comprising 40k videos. Importantly, the substantial increase in video\nlength doesn't compromise the model's innate generalization capabilities, and\nthe model showcases its advantages in generating videos of diverse styles and\nresolutions. We will release the source code and the enhanced model publicly.",
            "upvotes": 6
        },
        "publishedAt": "2024-06-21T02:22:21.036Z",
        "title": "ExVideo: Extending Video Diffusion Models via Parameter-Efficient Post-Tuning",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.14130.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.11817",
            "authors": [
                {
                    "_id": "6670f21456709bb492312de9",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/639be86b59473c6ae02ef9c4/gw34RBCVZCOkcAA79xUr3.png",
                        "isPro": false,
                        "fullname": "Jie Liu",
                        "user": "jieliu",
                        "type": "user"
                    },
                    "name": "Jie Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-18T09:11:05.671Z",
                    "hidden": false
                },
                {
                    "_id": "6670f21456709bb492312dea",
                    "user": {
                        "avatarUrl": "/avatars/3ae01c9330a47e98fac9f1eb0ba94073.svg",
                        "isPro": false,
                        "fullname": "Zhanhui Zhou",
                        "user": "ZHZisZZ",
                        "type": "user"
                    },
                    "name": "Zhanhui Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-21T12:56:23.932Z",
                    "hidden": false
                },
                {
                    "_id": "6670f21456709bb492312deb",
                    "user": {
                        "avatarUrl": "/avatars/a8f803b6f2e598eaee9c52c0d2ddfc16.svg",
                        "isPro": false,
                        "fullname": "Jiaheng Liu",
                        "user": "CheeryLJH",
                        "type": "user"
                    },
                    "name": "Jiaheng Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-21T12:56:29.594Z",
                    "hidden": false
                },
                {
                    "_id": "6670f21456709bb492312dec",
                    "user": {
                        "avatarUrl": "/avatars/4ae5001e7c7dea89c4deaf2b05436857.svg",
                        "isPro": false,
                        "fullname": "Xingyuan Bu",
                        "user": "sefira32",
                        "type": "user"
                    },
                    "name": "Xingyuan Bu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-21T12:56:35.212Z",
                    "hidden": false
                },
                {
                    "_id": "6670f21456709bb492312ded",
                    "name": "Chao Yang",
                    "hidden": false
                },
                {
                    "_id": "6670f21456709bb492312dee",
                    "user": {
                        "avatarUrl": "/avatars/15e4e187225d25051d348f3f722041ca.svg",
                        "isPro": false,
                        "fullname": "Han-Sen Zhong",
                        "user": "hansenzhong",
                        "type": "user"
                    },
                    "name": "Han-Sen Zhong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-21T12:56:42.468Z",
                    "hidden": false
                },
                {
                    "_id": "6670f21456709bb492312def",
                    "name": "Wanli Ouyang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-17T17:55:38.000Z",
            "title": "Iterative Length-Regularized Direct Preference Optimization: A Case\n  Study on Improving 7B Language Models to GPT-4 Level",
            "summary": "Direct Preference Optimization (DPO), a standard method for aligning language\nmodels with human preferences, is traditionally applied to offline preferences.\nRecent studies show that DPO benefits from iterative training with online\npreferences labeled by a trained reward model. In this work, we identify a\npitfall of vanilla iterative DPO - improved response quality can lead to\nincreased verbosity. To address this, we introduce iterative length-regularized\nDPO (iLR-DPO) to penalize response length. Our empirical results show that\niLR-DPO can enhance a 7B model to perform on par with GPT-4 without increasing\nverbosity. Specifically, our 7B model achieves a 50.5% length-controlled win\nrate against GPT-4 Preview on AlpacaEval 2.0, and excels across\nstandard benchmarks including MT-Bench, Arena-Hard and OpenLLM Leaderboard.\nThese results demonstrate the effectiveness of iterative DPO in aligning\nlanguage models with human feedback.",
            "upvotes": 5
        },
        "publishedAt": "2024-06-21T04:56:25.776Z",
        "title": "Iterative Length-Regularized Direct Preference Optimization: A Case Study on Improving 7B Language Models to GPT-4 Level",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.11817.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
            "fullname": "Ge Zhang",
            "name": "zhangysk",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.14319",
            "authors": [
                {
                    "_id": "6675457a9f2810b00961e366",
                    "user": {
                        "avatarUrl": "/avatars/eb8abab4c8aee8f340dc55ebff60cc90.svg",
                        "isPro": false,
                        "fullname": "Chuangtao Chen",
                        "user": "ChuangtaoChen-TUM",
                        "type": "user"
                    },
                    "name": "Chuangtao Chen",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2024-06-21T09:18:51.993Z",
                    "hidden": false
                },
                {
                    "_id": "6675457a9f2810b00961e367",
                    "name": "Grace Li Zhang",
                    "hidden": false
                },
                {
                    "_id": "6675457a9f2810b00961e368",
                    "name": "Xunzhao Yin",
                    "hidden": false
                },
                {
                    "_id": "6675457a9f2810b00961e369",
                    "name": "Cheng Zhuo",
                    "hidden": false
                },
                {
                    "_id": "6675457a9f2810b00961e36a",
                    "name": "Ulf Schlichtmann",
                    "hidden": false
                },
                {
                    "_id": "6675457a9f2810b00961e36b",
                    "name": "Bing Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-20T13:52:30.000Z",
            "title": "LiveMind: Low-latency Large Language Models with Simultaneous Inference",
            "summary": "In this paper, we introduce a novel low-latency inference framework for large\nlanguage models (LLMs) inference which enables LLMs to perform inferences with\nincomplete prompts. By reallocating computational processes to prompt input\nphase, we achieve a substantial reduction in latency, thereby significantly\nenhancing the interactive experience for users of LLMs. The framework adeptly\nmanages the visibility of the streaming prompt to the model, allowing it to\ninfer from incomplete prompts or await additional prompts. Compared with\ntraditional inference methods that utilize complete prompts, our approach\ndemonstrates an average reduction of 59% in response latency on the MMLU-Pro\ndataset, while maintaining comparable accuracy. Additionally, our framework\nfacilitates collaborative inference and output across different models. By\nemploying an LLM for inference and a small language model (SLM) for output, we\nachieve an average 68% reduction in response latency, alongside a 5.5%\nimprovement in accuracy on the MMLU-Pro dataset compared with the SLM baseline.\nFor long prompts exceeding 20 sentences, the response latency can be reduced by\nup to 93%.",
            "upvotes": 4
        },
        "publishedAt": "2024-06-21T07:53:46.106Z",
        "title": "LiveMind: Low-latency Large Language Models with Simultaneous Inference",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.14319.png",
        "numComments": 3,
        "submittedBy": {
            "avatarUrl": "/avatars/eb8abab4c8aee8f340dc55ebff60cc90.svg",
            "fullname": "Chuangtao Chen",
            "name": "ChuangtaoChen-TUM",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.11927",
            "authors": [
                {
                    "_id": "6674fc013ae9e9c65996d66e",
                    "name": "Nam Le Hai",
                    "hidden": false
                },
                {
                    "_id": "6674fc013ae9e9c65996d66f",
                    "user": {
                        "avatarUrl": "/avatars/607e28f09ee8995c9a13579ad46f25ec.svg",
                        "isPro": false,
                        "fullname": "Dung Manh Nguyen",
                        "user": "nmd2000",
                        "type": "user"
                    },
                    "name": "Dung Manh Nguyen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-21T11:48:53.306Z",
                    "hidden": false
                },
                {
                    "_id": "6674fc013ae9e9c65996d670",
                    "user": {
                        "avatarUrl": "/avatars/441dfc0f7ae5d79ecccab5ee62326919.svg",
                        "isPro": false,
                        "fullname": "Nghi Bui",
                        "user": "bdqnghi",
                        "type": "user"
                    },
                    "name": "Nghi D. Q. Bui",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-21T11:49:46.436Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-17T10:45:22.000Z",
            "title": "REPOEXEC: Evaluate Code Generation with a Repository-Level Executable\n  Benchmark",
            "summary": "The ability of CodeLLMs to generate executable and functionally correct code\nat the repository-level scale remains largely unexplored. We introduce\nRepoExec, a novel benchmark for evaluating code generation at the\nrepository-level scale. RepoExec focuses on three main aspects: executability,\nfunctional correctness through automated test case generation with high\ncoverage rate, and carefully crafted cross-file contexts to accurately generate\ncode. Our work explores a controlled scenario where developers specify\nnecessary code dependencies, challenging the model to integrate these\naccurately. Experiments show that while pretrained LLMs outperform\ninstruction-tuned models in correctness, the latter excel in utilizing provided\ndependencies and demonstrating debugging capabilities. We also introduce a new\ninstruction-tuned dataset that focuses on code dependencies and demonstrate\nthat CodeLLMs fine-tuned on our dataset have a better capability to leverage\nthese dependencies effectively. RepoExec aims to provide a comprehensive\nevaluation of code functionality and alignment with developer intent, paving\nthe way for more reliable and applicable CodeLLMs in real-world scenarios. The\ndataset and source code can be found\nat~https://github.com/FSoft-AI4Code/RepoExec.",
            "upvotes": 4
        },
        "publishedAt": "2024-06-21T02:35:48.607Z",
        "title": "REPOEXEC: Evaluate Code Generation with a Repository-Level Executable Benchmark",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.11927.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/441dfc0f7ae5d79ecccab5ee62326919.svg",
            "fullname": "Nghi Bui",
            "name": "bdqnghi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.12045",
            "authors": [
                {
                    "_id": "6674ffba568251b0b77afccf",
                    "name": "Shunyu Yao",
                    "hidden": false
                },
                {
                    "_id": "6674ffba568251b0b77afcd0",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ca0095b047b9b12a1face0/Eq_BCfFPbBig4XxSHs3xW.png",
                        "isPro": false,
                        "fullname": "Noah Shinn",
                        "user": "noahshinn",
                        "type": "user"
                    },
                    "name": "Noah Shinn",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-21T12:54:46.421Z",
                    "hidden": false
                },
                {
                    "_id": "6674ffba568251b0b77afcd1",
                    "name": "Pedram Razavi",
                    "hidden": false
                },
                {
                    "_id": "6674ffba568251b0b77afcd2",
                    "user": {
                        "avatarUrl": "/avatars/7a780509f14d86d1e6ec37cb39ea1f5d.svg",
                        "isPro": false,
                        "fullname": "Karthik Narasimhan",
                        "user": "karthikn-sierra",
                        "type": "user"
                    },
                    "name": "Karthik Narasimhan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-21T12:54:32.248Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-17T19:33:08.000Z",
            "title": "τ-bench: A Benchmark for Tool-Agent-User Interaction in Real-World\n  Domains",
            "summary": "Existing benchmarks do not test language agents on their interaction with\nhuman users or ability to follow domain-specific rules, both of which are vital\nfor deploying them in real world applications. We propose tau-bench, a\nbenchmark emulating dynamic conversations between a user (simulated by language\nmodels) and a language agent provided with domain-specific API tools and policy\nguidelines. We employ an efficient and faithful evaluation process that\ncompares the database state at the end of a conversation with the annotated\ngoal state. We also propose a new metric (pass^k) to evaluate the reliability\nof agent behavior over multiple trials. Our experiments show that even\nstate-of-the-art function calling agents (like gpt-4o) succeed on <50% of the\ntasks, and are quite inconsistent (pass^8 <25% in retail). Our findings point\nto the need for methods that can improve the ability of agents to act\nconsistently and follow rules reliably.",
            "upvotes": 2
        },
        "publishedAt": "2024-06-21T02:51:58.730Z",
        "title": "$τ$-bench: A Benchmark for Tool-Agent-User Interaction in Real-World Domains",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.12045.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/e2c3c43562b0ef4b31a21e064e7f4a8f.svg",
            "fullname": "Soham Parikh",
            "name": "sohampnow",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.13663",
            "authors": [
                {
                    "_id": "667586138794bbd33fde01f3",
                    "user": {
                        "avatarUrl": "/avatars/7ce63a889ee530fd5788c7b8eca4bc7d.svg",
                        "isPro": false,
                        "fullname": "Jirui Qi",
                        "user": "JRQi",
                        "type": "user"
                    },
                    "name": "Jirui Qi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-21T15:00:34.930Z",
                    "hidden": false
                },
                {
                    "_id": "667586138794bbd33fde01f4",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1670231290373-5e7749883d77a72421292d07.jpeg",
                        "isPro": false,
                        "fullname": "Gabriele Sarti",
                        "user": "gsarti",
                        "type": "user"
                    },
                    "name": "Gabriele Sarti",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-21T14:20:43.979Z",
                    "hidden": false
                },
                {
                    "_id": "667586138794bbd33fde01f5",
                    "user": {
                        "avatarUrl": "/avatars/a6d3d10929fcf99e58391a55b5250bd4.svg",
                        "isPro": false,
                        "fullname": "Raquel Fernández",
                        "user": "rfernandez",
                        "type": "user"
                    },
                    "name": "Raquel Fernández",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2024-06-21T13:54:29.737Z",
                    "hidden": false
                },
                {
                    "_id": "667586138794bbd33fde01f6",
                    "name": "Arianna Bisazza",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-19T16:10:26.000Z",
            "title": "Model Internals-based Answer Attribution for Trustworthy\n  Retrieval-Augmented Generation",
            "summary": "Ensuring the verifiability of model answers is a fundamental challenge for\nretrieval-augmented generation (RAG) in the question answering (QA) domain.\nRecently, self-citation prompting was proposed to make large language models\n(LLMs) generate citations to supporting documents along with their answers.\nHowever, self-citing LLMs often struggle to match the required format, refer to\nnon-existent sources, and fail to faithfully reflect LLMs' context usage\nthroughout the generation. In this work, we present MIRAGE --Model\nInternals-based RAG Explanations -- a plug-and-play approach using model\ninternals for faithful answer attribution in RAG applications. MIRAGE detects\ncontext-sensitive answer tokens and pairs them with retrieved documents\ncontributing to their prediction via saliency methods. We evaluate our proposed\napproach on a multilingual extractive QA dataset, finding high agreement with\nhuman answer attribution. On open-ended QA, MIRAGE achieves citation quality\nand efficiency comparable to self-citation while also allowing for a\nfiner-grained control of attribution parameters. Our qualitative evaluation\nhighlights the faithfulness of MIRAGE's attributions and underscores the\npromising application of model internals for RAG answer attribution.",
            "upvotes": 1
        },
        "publishedAt": "2024-06-21T12:28:48.898Z",
        "title": "Model Internals-based Answer Attribution for Trustworthy Retrieval-Augmented Generation",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/5e7749883d77a72421292d07/o2ji4aX37hCTQXN1DA7Vc.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.13663.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1670231290373-5e7749883d77a72421292d07.jpeg",
            "fullname": "Gabriele Sarti",
            "name": "gsarti",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.12618",
            "authors": [
                {
                    "_id": "6674d0c425c64f92734af3b4",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/608865d2511e863acdb20bae/WSJQHZ4PC9hjLMCWv9mSb.png",
                        "isPro": false,
                        "fullname": "Marius Mosbach",
                        "user": "mmosbach",
                        "type": "user"
                    },
                    "name": "Marius Mosbach",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2024-06-21T01:00:53.341Z",
                    "hidden": false
                },
                {
                    "_id": "6674d0c425c64f92734af3b5",
                    "user": {
                        "avatarUrl": "/avatars/4fcec7d8e7aa96ea5d52274266e491ad.svg",
                        "isPro": false,
                        "fullname": "Vagrant",
                        "user": "dippedrusk",
                        "type": "user"
                    },
                    "name": "Vagrant Gautam",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-21T07:06:04.450Z",
                    "hidden": false
                },
                {
                    "_id": "6674d0c425c64f92734af3b6",
                    "name": "Tomás Vergara-Browne",
                    "hidden": false
                },
                {
                    "_id": "6674d0c425c64f92734af3b7",
                    "name": "Dietrich Klakow",
                    "hidden": false
                },
                {
                    "_id": "6674d0c425c64f92734af3b8",
                    "name": "Mor Geva",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-18T13:45:07.000Z",
            "title": "From Insights to Actions: The Impact of Interpretability and Analysis\n  Research on NLP",
            "summary": "Interpretability and analysis (IA) research is a growing subfield within NLP\nwith the goal of developing a deeper understanding of the behavior or inner\nworkings of NLP systems and methods. Despite growing interest in the subfield,\na commonly voiced criticism is that it lacks actionable insights and therefore\nhas little impact on NLP. In this paper, we seek to quantify the impact of IA\nresearch on the broader field of NLP. We approach this with a mixed-methods\nanalysis of: (1) a citation graph of 185K+ papers built from all papers\npublished at ACL and EMNLP conferences from 2018 to 2023, and (2) a survey of\n138 members of the NLP community. Our quantitative results show that IA work is\nwell-cited outside of IA, and central in the NLP citation graph. Through\nqualitative analysis of survey responses and manual annotation of 556 papers,\nwe find that NLP researchers build on findings from IA work and perceive it is\nimportant for progress in NLP, multiple subfields, and rely on its findings and\nterminology for their own work. Many novel methods are proposed based on IA\nfindings and highly influenced by them, but highly influential non-IA work\ncites IA findings without being driven by them. We end by summarizing what is\nmissing in IA work today and provide a call to action, to pave the way for a\nmore impactful future of IA research.",
            "upvotes": 0
        },
        "publishedAt": "2024-06-21T12:55:14.919Z",
        "title": "From Insights to Actions: The Impact of Interpretability and Analysis Research on NLP",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.12618.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/4fcec7d8e7aa96ea5d52274266e491ad.svg",
            "fullname": "Vagrant",
            "name": "dippedrusk",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    }
]