[
  {
    "paper": {
      "id": "2503.00865",
      "authors": [
        {
          "_id": "67c666245e2443d7d5e9b76a",
          "user": {
            "_id": "64802face9ff472e30dc1ceb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64802face9ff472e30dc1ceb/bcwTlgpaUrU7m2RMB5zCc.png",
            "isPro": false,
            "fullname": "Yiran Zhao",
            "user": "Yiran0924",
            "type": "user"
          },
          "name": "Yiran Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-04T08:51:21.231Z",
          "hidden": false
        },
        {
          "_id": "67c666245e2443d7d5e9b76b",
          "name": "Chaoqun Liu",
          "hidden": false
        },
        {
          "_id": "67c666245e2443d7d5e9b76c",
          "name": "Yue Deng",
          "hidden": false
        },
        {
          "_id": "67c666245e2443d7d5e9b76d",
          "name": "Jiahao Ying",
          "hidden": false
        },
        {
          "_id": "67c666245e2443d7d5e9b76e",
          "name": "Mahani Aljunied",
          "hidden": false
        },
        {
          "_id": "67c666245e2443d7d5e9b76f",
          "name": "Zhaodonghui Li",
          "hidden": false
        },
        {
          "_id": "67c666245e2443d7d5e9b770",
          "name": "Lidong Bing",
          "hidden": false
        },
        {
          "_id": "67c666245e2443d7d5e9b771",
          "name": "Hou Pong Chan",
          "hidden": false
        },
        {
          "_id": "67c666245e2443d7d5e9b772",
          "name": "Yu Rong",
          "hidden": false
        },
        {
          "_id": "67c666245e2443d7d5e9b773",
          "name": "Deli Zhao",
          "hidden": false
        },
        {
          "_id": "67c666245e2443d7d5e9b774",
          "name": "Wenxuan Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-02T11:53:55.000Z",
      "title": "Babel: Open Multilingual Large Language Models Serving Over 90% of\n  Global Speakers",
      "summary": "Large language models (LLMs) have revolutionized natural language processing\n(NLP), yet open-source multilingual LLMs remain scarce, with existing models\noften limited in language coverage. Such models typically prioritize\nwell-resourced languages, while widely spoken but under-resourced languages are\noften overlooked. To address this disparity, we introduce Babel, an\nopen multilingual LLM that covers the top 25 languages by number of speakers,\nsupports over 90% of the global population, and includes many languages\nneglected by other open multilingual LLMs. Unlike traditional continue\npretraining approaches, Babel expands its parameter count through a layer\nextension technique that elevates Babel's performance ceiling. We introduce two\nvariants: Babel-9B, designed for efficient inference and\nfine-tuning, and Babel-83B, which sets a new standard for open\nmultilingual LLMs. Extensive evaluations on multilingual tasks demonstrate its\nsuperior performance compared to open LLMs of comparable size. In addition,\nusing open-source supervised fine-tuning datasets, Babel achieves remarkable\nperformance, with Babel-9B-Chat leading among 10B-sized LLMs and Babel-83B-Chat\nsetting a new standard for multilingual tasks, reaching the same level of\ncommercial models.",
      "upvotes": 25,
      "discussionId": "67c666255e2443d7d5e9b7b3",
      "projectPage": "https://babel-llm.github.io/babel-llm/",
      "githubRepo": "https://github.com/babel-llm/babel-llm"
    },
    "publishedAt": "2025-03-05T21:49:03.700Z",
    "title": "Babel: Open Multilingual Large Language Models Serving Over 90% of Global Speakers",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.00865.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64802face9ff472e30dc1ceb",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64802face9ff472e30dc1ceb/bcwTlgpaUrU7m2RMB5zCc.png",
      "fullname": "Yiran Zhao",
      "name": "Yiran0924",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.00329",
      "authors": [
        {
          "_id": "67c755f898a2e37274c62c96",
          "name": "Benjamin Schneider",
          "hidden": false
        },
        {
          "_id": "67c755f898a2e37274c62c97",
          "name": "Florian Kerschbaum",
          "hidden": false
        },
        {
          "_id": "67c755f898a2e37274c62c98",
          "name": "Wenhu Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-01T03:29:02.000Z",
      "title": "ABC: Achieving Better Control of Multimodal Embeddings using VLMs",
      "summary": "Visual embedding models excel at zero-shot tasks like visual retrieval and\nclassification. However, these models cannot be used for tasks that contain\nambiguity or require user instruction. These tasks necessitate a multimodal\nembedding model, which outputs embeddings that combine visual and natural\nlanguage input. Existing CLIP-based approaches embed images and text\nindependently, and fuse the result. We find that this results in weak\ninteractions between modalities, and poor user control over the representation.\nWe introduce ABC, an open-source multimodal embedding model that uses a\nvision-language model backbone to deeply integrate image features with natural\nlanguage instructions. ABC achieves bestfor-size performance on MSCOCO\nimage-to-text retrieval and is the top performing model on classification and\nVQA tasks in the Massive Multimodal Embedding Benchmark. With a strongly\nunified vision-language representation, ABC can use natural language to solve\nsubtle and potentially ambiguous visual retrieval problems. To evaluate this\ncapability, we design CtrlBench, a benchmark that requires interleaving textual\ninstructions with image content for correct retrieval. ABC advances the state\nof multimodal embeddings by offering high-quality representations and flexible\nnatural language control. Our model and datasets are available at our project\npage.",
      "upvotes": 8,
      "discussionId": "67c7560298a2e37274c6311d",
      "projectPage": "https://tiger-ai-lab.github.io/ABC/",
      "githubRepo": "https://github.com/TIGER-AI-Lab/ABC"
    },
    "publishedAt": "2025-03-05T21:33:37.945Z",
    "title": "ABC: Achieving Better Control of Multimodal Embeddings using VLMs",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6313a86154e6e5d9f0f94e04/b2vg-4UWwvcEboAZgK-Sv.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.00329.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6313a86154e6e5d9f0f94e04",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
      "fullname": "Wenhu Chen",
      "name": "wenhu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 33
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.03751",
      "authors": [
        {
          "_id": "67c912b1b5903dd437cc2370",
          "name": "Xuanchi Ren",
          "hidden": false
        },
        {
          "_id": "67c912b1b5903dd437cc2371",
          "name": "Tianchang Shen",
          "hidden": false
        },
        {
          "_id": "67c912b1b5903dd437cc2372",
          "name": "Jiahui Huang",
          "hidden": false
        },
        {
          "_id": "67c912b1b5903dd437cc2373",
          "name": "Huan Ling",
          "hidden": false
        },
        {
          "_id": "67c912b1b5903dd437cc2374",
          "name": "Yifan Lu",
          "hidden": false
        },
        {
          "_id": "67c912b1b5903dd437cc2375",
          "name": "Merlin Nimier-David",
          "hidden": false
        },
        {
          "_id": "67c912b1b5903dd437cc2376",
          "name": "Thomas Müller",
          "hidden": false
        },
        {
          "_id": "67c912b1b5903dd437cc2377",
          "name": "Alexander Keller",
          "hidden": false
        },
        {
          "_id": "67c912b1b5903dd437cc2378",
          "name": "Sanja Fidler",
          "hidden": false
        },
        {
          "_id": "67c912b1b5903dd437cc2379",
          "name": "Jun Gao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-05T18:59:50.000Z",
      "title": "GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera\n  Control",
      "summary": "We present GEN3C, a generative video model with precise Camera Control and\ntemporal 3D Consistency. Prior video models already generate realistic videos,\nbut they tend to leverage little 3D information, leading to inconsistencies,\nsuch as objects popping in and out of existence. Camera control, if implemented\nat all, is imprecise, because camera parameters are mere inputs to the neural\nnetwork which must then infer how the video depends on the camera. In contrast,\nGEN3C is guided by a 3D cache: point clouds obtained by predicting the\npixel-wise depth of seed images or previously generated frames. When generating\nthe next frames, GEN3C is conditioned on the 2D renderings of the 3D cache with\nthe new camera trajectory provided by the user. Crucially, this means that\nGEN3C neither has to remember what it previously generated nor does it have to\ninfer the image structure from the camera pose. The model, instead, can focus\nall its generative power on previously unobserved regions, as well as advancing\nthe scene state to the next frame. Our results demonstrate more precise camera\ncontrol than prior work, as well as state-of-the-art results in sparse-view\nnovel view synthesis, even in challenging settings such as driving scenes and\nmonocular dynamic video. Results are best viewed in videos. Check out our\nwebpage! https://research.nvidia.com/labs/toronto-ai/GEN3C/",
      "upvotes": 6,
      "discussionId": "67c912b9b5903dd437cc2505"
    },
    "publishedAt": "2025-03-05T22:13:22.552Z",
    "title": "GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.03751.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6287
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.02951",
      "authors": [
        {
          "_id": "67c907ea7568a12737ad4535",
          "name": "Zhangchen Xu",
          "hidden": false
        },
        {
          "_id": "67c907ea7568a12737ad4536",
          "user": {
            "_id": "637c88b6d55081513c5690d8",
            "avatarUrl": "/avatars/6766e23ebf46b46d6c8b48351c571907.svg",
            "isPro": false,
            "fullname": "Yang Liu",
            "user": "nlpyang",
            "type": "user"
          },
          "name": "Yang Liu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-06T02:26:54.940Z",
          "hidden": false
        },
        {
          "_id": "67c907ea7568a12737ad4537",
          "name": "Yueqin Yin",
          "hidden": false
        },
        {
          "_id": "67c907ea7568a12737ad4538",
          "name": "Mingyuan Zhou",
          "hidden": false
        },
        {
          "_id": "67c907ea7568a12737ad4539",
          "name": "Radha Poovendran",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-04T19:17:36.000Z",
      "title": "KodCode: A Diverse, Challenging, and Verifiable Synthetic Dataset for\n  Coding",
      "summary": "We introduce KodCode, a synthetic dataset that addresses the persistent\nchallenge of acquiring high-quality, verifiable training data across diverse\ndifficulties and domains for training Large Language Models for coding.\nExisting code-focused resources typically fail to ensure either the breadth of\ncoverage (e.g., spanning simple coding tasks to advanced algorithmic problems)\nor verifiable correctness (e.g., unit tests). In contrast, KodCode comprises\nquestion-solution-test triplets that are systematically validated via a\nself-verification procedure. Our pipeline begins by synthesizing a broad range\nof coding questions, then generates solutions and test cases with additional\nattempts allocated to challenging problems. Finally, post-training data\nsynthesis is done by rewriting questions into diverse formats and generating\nresponses under a test-based reject sampling procedure from a reasoning model\n(DeepSeek R1). This pipeline yields a large-scale, robust and diverse coding\ndataset. KodCode is suitable for supervised fine-tuning and the paired unit\ntests also provide great potential for RL tuning. Fine-tuning experiments on\ncoding benchmarks (HumanEval(+), MBPP(+), BigCodeBench, and LiveCodeBench)\ndemonstrate that KodCode-tuned models achieve state-of-the-art performance,\nsurpassing models like Qwen2.5-Coder-32B-Instruct and\nDeepSeek-R1-Distill-Llama-70B.",
      "upvotes": 6,
      "discussionId": "67c907ee7568a12737ad4633",
      "projectPage": "https://kodcode-ai.github.io/",
      "githubRepo": "https://github.com/KodCode-AI/kodcode"
    },
    "publishedAt": "2025-03-05T21:31:01.626Z",
    "title": "KodCode: A Diverse, Challenging, and Verifiable Synthetic Dataset for Coding",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.02951.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "653df1323479e9ebbe3eb6cc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653df1323479e9ebbe3eb6cc/K_g-r1iMRNKj99LXPuYF3.jpeg",
      "fullname": "Zhangchen Xu",
      "name": "flydust",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 13
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.03278",
      "authors": [
        {
          "_id": "67c94e5f8c4ef8be73583f4b",
          "name": "Jun Li",
          "hidden": false
        },
        {
          "_id": "67c94e5f8c4ef8be73583f4c",
          "name": "Che Liu",
          "hidden": false
        },
        {
          "_id": "67c94e5f8c4ef8be73583f4d",
          "name": "Wenjia Bai",
          "hidden": false
        },
        {
          "_id": "67c94e5f8c4ef8be73583f4e",
          "name": "Rossella Arcucci",
          "hidden": false
        },
        {
          "_id": "67c94e5f8c4ef8be73583f4f",
          "name": "Cosmin I. Bercea",
          "hidden": false
        },
        {
          "_id": "67c94e5f8c4ef8be73583f50",
          "name": "Julia A. Schnabel",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-05T09:02:33.000Z",
      "title": "Enhancing Abnormality Grounding for Vision Language Models with\n  Knowledge Descriptions",
      "summary": "Visual Language Models (VLMs) have demonstrated impressive capabilities in\nvisual grounding tasks. However, their effectiveness in the medical domain,\nparticularly for abnormality detection and localization within medical images,\nremains underexplored. A major challenge is the complex and abstract nature of\nmedical terminology, which makes it difficult to directly associate\npathological anomaly terms with their corresponding visual features. In this\nwork, we introduce a novel approach to enhance VLM performance in medical\nabnormality detection and localization by leveraging decomposed medical\nknowledge. Instead of directly prompting models to recognize specific\nabnormalities, we focus on breaking down medical concepts into fundamental\nattributes and common visual patterns. This strategy promotes a stronger\nalignment between textual descriptions and visual features, improving both the\nrecognition and localization of abnormalities in medical images.We evaluate our\nmethod on the 0.23B Florence-2 base model and demonstrate that it achieves\ncomparable performance in abnormality grounding to significantly larger 7B\nLLaVA-based medical VLMs, despite being trained on only 1.5% of the data used\nfor such models. Experimental results also demonstrate the effectiveness of our\napproach in both known and previously unseen abnormalities, suggesting its\nstrong generalization capabilities.",
      "upvotes": 3,
      "discussionId": "67c94e608c4ef8be73583f7b",
      "projectPage": "https://lijunrio.github.io/AG-KD/"
    },
    "publishedAt": "2025-03-06T02:29:15.964Z",
    "title": "Enhancing Abnormality Grounding for Vision Language Models with Knowledge Descriptions",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.03278.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "631b9ff5824f2502e3557c7e",
      "avatarUrl": "/avatars/076043c9dba07644a570692563ef8114.svg",
      "fullname": "liu",
      "name": "che111",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.01836",
      "authors": [
        {
          "_id": "67c94c32dd505e6a4db201a2",
          "name": "Yisen Li",
          "hidden": false
        },
        {
          "_id": "67c94c32dd505e6a4db201a3",
          "name": "Lingfeng Yang",
          "hidden": false
        },
        {
          "_id": "67c94c32dd505e6a4db201a4",
          "name": "Wenxuan Shen",
          "hidden": false
        },
        {
          "_id": "67c94c32dd505e6a4db201a5",
          "name": "Pan Zhou",
          "hidden": false
        },
        {
          "_id": "67c94c32dd505e6a4db201a6",
          "name": "Yao Wan",
          "hidden": false
        },
        {
          "_id": "67c94c32dd505e6a4db201a7",
          "name": "Weiwei Lin",
          "hidden": false
        },
        {
          "_id": "67c94c32dd505e6a4db201a8",
          "name": "Dongping Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T18:56:44.000Z",
      "title": "CrowdSelect: Synthetic Instruction Data Selection with Multi-LLM Wisdom",
      "summary": "Distilling advanced Large Language Models' instruction-following capabilities\ninto smaller models using a selected subset has become a mainstream approach in\nmodel training. While existing synthetic instruction data selection strategies\nrely mainly on single-dimensional signals (i.e., reward scores, model\nperplexity), they fail to capture the complexity of instruction-following\nacross diverse fields. Therefore, we investigate more diverse signals to\ncapture comprehensive instruction-response pair characteristics and propose\nthree foundational metrics that leverage Multi-LLM wisdom, informed by (1)\ndiverse LLM responses and (2) reward model assessment. Building upon base\nmetrics, we propose CrowdSelect, an integrated metric incorporating a\nclustering-based approach to maintain response diversity. Our comprehensive\nexperiments demonstrate that our foundation metrics consistently improve\nperformance across 4 base models on MT-bench and Arena-Hard. CrowdSelect,\nefficiently incorporating all metrics, achieves state-of-the-art performance in\nboth Full and LoRA fine-tuning, showing improvements of 4.81% on Arena-Hard and\n11.1% on MT-bench with Llama-3.2-3b-instruct. We hope our findings will bring\nvaluable insights for future research in this direction. Code are available at\nhttps://github.com/listentm/crowdselect.",
      "upvotes": 3,
      "discussionId": "67c94c33dd505e6a4db201f6",
      "githubRepo": "https://github.com/listentm/crowdselect"
    },
    "publishedAt": "2025-03-06T02:20:38.735Z",
    "title": "CrowdSelect: Synthetic Instruction Data Selection with Multi-LLM Wisdom",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01836.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643be8879f5d314db2d9ed23",
      "avatarUrl": "/avatars/64e9bb2c4e10fbe03e2b81afedf40865.svg",
      "fullname": "Chen Dongping",
      "name": "shuaishuaicdp",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.03044",
      "authors": [
        {
          "_id": "67c94e6ad325e95d82f23433",
          "name": "Gabriele Sarti",
          "hidden": false
        },
        {
          "_id": "67c94e6ad325e95d82f23434",
          "name": "Vilém Zouhar",
          "hidden": false
        },
        {
          "_id": "67c94e6ad325e95d82f23435",
          "name": "Grzegorz Chrupała",
          "hidden": false
        },
        {
          "_id": "67c94e6ad325e95d82f23436",
          "name": "Ana Guerberof-Arenas",
          "hidden": false
        },
        {
          "_id": "67c94e6ad325e95d82f23437",
          "name": "Malvina Nissim",
          "hidden": false
        },
        {
          "_id": "67c94e6ad325e95d82f23438",
          "name": "Arianna Bisazza",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-04T22:50:17.000Z",
      "title": "QE4PE: Word-level Quality Estimation for Human Post-Editing",
      "summary": "Word-level quality estimation (QE) detects erroneous spans in machine\ntranslations, which can direct and facilitate human post-editing. While the\naccuracy of word-level QE systems has been assessed extensively, their\nusability and downstream influence on the speed, quality and editing choices of\nhuman post-editing remain understudied. Our QE4PE study investigates the impact\nof word-level QE on machine translation (MT) post-editing in a realistic\nsetting involving 42 professional post-editors across two translation\ndirections. We compare four error-span highlight modalities, including\nsupervised and uncertainty-based word-level QE methods, for identifying\npotential errors in the outputs of a state-of-the-art neural MT model.\nPost-editing effort and productivity are estimated by behavioral logs, while\nquality improvements are assessed by word- and segment-level human annotation.\nWe find that domain, language and editors' speed are critical factors in\ndetermining highlights' effectiveness, with modest differences between\nhuman-made and automated QE highlights underlining a gap between accuracy and\nusability in professional workflows.",
      "upvotes": 1,
      "discussionId": "67c94e6fd325e95d82f23524"
    },
    "publishedAt": "2025-03-06T02:30:17.431Z",
    "title": "QE4PE: Word-level Quality Estimation for Human Post-Editing",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.03044.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5e7749883d77a72421292d07",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1670231290373-5e7749883d77a72421292d07.jpeg",
      "fullname": "Gabriele Sarti",
      "name": "gsarti",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 213
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.01729",
      "authors": [
        {
          "_id": "67c92e8b5650d7efeba5b48c",
          "name": "Santiago Bou Betran",
          "hidden": false
        },
        {
          "_id": "67c92e8b5650d7efeba5b48d",
          "name": "Alberta Longhini",
          "hidden": false
        },
        {
          "_id": "67c92e8b5650d7efeba5b48e",
          "name": "Miguel Vasco",
          "hidden": false
        },
        {
          "_id": "67c92e8b5650d7efeba5b48f",
          "name": "Yuchong Zhang",
          "hidden": false
        },
        {
          "_id": "67c92e8b5650d7efeba5b490",
          "name": "Danica Kragic",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T16:49:15.000Z",
      "title": "FLAME: A Federated Learning Benchmark for Robotic Manipulation",
      "summary": "Recent progress in robotic manipulation has been fueled by large-scale\ndatasets collected across diverse environments. Training robotic manipulation\npolicies on these datasets is traditionally performed in a centralized manner,\nraising concerns regarding scalability, adaptability, and data privacy. While\nfederated learning enables decentralized, privacy-preserving training, its\napplication to robotic manipulation remains largely unexplored. We introduce\nFLAME (Federated Learning Across Manipulation Environments), the first\nbenchmark designed for federated learning in robotic manipulation. FLAME\nconsists of: (i) a set of large-scale datasets of over 160,000 expert\ndemonstrations of multiple manipulation tasks, collected across a wide range of\nsimulated environments; (ii) a training and evaluation framework for robotic\npolicy learning in a federated setting. We evaluate standard federated learning\nalgorithms in FLAME, showing their potential for distributed policy learning\nand highlighting key challenges. Our benchmark establishes a foundation for\nscalable, adaptive, and privacy-aware robotic learning.",
      "upvotes": 1,
      "discussionId": "67c92e8d5650d7efeba5b519"
    },
    "publishedAt": "2025-03-06T00:11:48.501Z",
    "title": "FLAME: A Federated Learning Benchmark for Robotic Manipulation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01729.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "640d3eaa3623f6a56dde856d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678589663024-640d3eaa3623f6a56dde856d.jpeg",
      "fullname": "vansin",
      "name": "vansin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.01378",
      "authors": [
        {
          "_id": "67c92e537ae0115c7a7b9fa3",
          "name": "Artem Lykov",
          "hidden": false
        },
        {
          "_id": "67c92e537ae0115c7a7b9fa4",
          "name": "Valerii Serpiva",
          "hidden": false
        },
        {
          "_id": "67c92e537ae0115c7a7b9fa5",
          "name": "Muhammad Haris Khan",
          "hidden": false
        },
        {
          "_id": "67c92e537ae0115c7a7b9fa6",
          "name": "Oleg Sautenkov",
          "hidden": false
        },
        {
          "_id": "67c92e537ae0115c7a7b9fa7",
          "name": "Artyom Myshlyaev",
          "hidden": false
        },
        {
          "_id": "67c92e537ae0115c7a7b9fa8",
          "name": "Grik Tadevosyan",
          "hidden": false
        },
        {
          "_id": "67c92e537ae0115c7a7b9fa9",
          "name": "Yasheerah Yaqoot",
          "hidden": false
        },
        {
          "_id": "67c92e537ae0115c7a7b9faa",
          "name": "Dzmitry Tsetserukou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T10:21:36.000Z",
      "title": "CognitiveDrone: A VLA Model and Evaluation Benchmark for Real-Time\n  Cognitive Task Solving and Reasoning in UAVs",
      "summary": "This paper introduces CognitiveDrone, a novel Vision-Language-Action (VLA)\nmodel tailored for complex Unmanned Aerial Vehicles (UAVs) tasks that demand\nadvanced cognitive abilities. Trained on a dataset comprising over 8,000\nsimulated flight trajectories across three key categories-Human Recognition,\nSymbol Understanding, and Reasoning-the model generates real-time 4D action\ncommands based on first-person visual inputs and textual instructions. To\nfurther enhance performance in intricate scenarios, we propose\nCognitiveDrone-R1, which integrates an additional Vision-Language Model (VLM)\nreasoning module to simplify task directives prior to high-frequency control.\nExperimental evaluations using our open-source benchmark, CognitiveDroneBench,\nreveal that while a racing-oriented model (RaceVLA) achieves an overall success\nrate of 31.3%, the base CognitiveDrone model reaches 59.6%, and\nCognitiveDrone-R1 attains a success rate of 77.2%. These results demonstrate\nimprovements of up to 30% in critical cognitive tasks, underscoring the\neffectiveness of incorporating advanced reasoning capabilities into UAV control\nsystems. Our contributions include the development of a state-of-the-art VLA\nmodel for UAV control and the introduction of the first dedicated benchmark for\nassessing cognitive tasks in drone operations. The complete repository is\navailable at cognitivedrone.github.io",
      "upvotes": 1,
      "discussionId": "67c92e547ae0115c7a7b9fe6"
    },
    "publishedAt": "2025-03-06T00:10:56.364Z",
    "title": "CognitiveDrone: A VLA Model and Evaluation Benchmark for Real-Time Cognitive Task Solving and Reasoning in UAVs",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01378.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "640d3eaa3623f6a56dde856d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678589663024-640d3eaa3623f6a56dde856d.jpeg",
      "fullname": "vansin",
      "name": "vansin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.00502",
      "authors": [
        {
          "_id": "67c8427047c2aa135346dced",
          "user": {
            "_id": "66d3290364c1e9b73208af82",
            "avatarUrl": "/avatars/e0ea5f2b366927c7b146f248028a2e59.svg",
            "isPro": false,
            "fullname": "Shiyu Fang",
            "user": "FanGShiYuu",
            "type": "user"
          },
          "name": "Shiyu Fang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-05T15:46:48.691Z",
          "hidden": false
        },
        {
          "_id": "67c8427047c2aa135346dcee",
          "name": "Jiaqi Liu",
          "hidden": false
        },
        {
          "_id": "67c8427047c2aa135346dcef",
          "name": "Chengkai Xu",
          "hidden": false
        },
        {
          "_id": "67c8427047c2aa135346dcf0",
          "name": "Chen Lv",
          "hidden": false
        },
        {
          "_id": "67c8427047c2aa135346dcf1",
          "name": "Peng Hang",
          "hidden": false
        },
        {
          "_id": "67c8427047c2aa135346dcf2",
          "name": "Jian Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-01T14:15:52.000Z",
      "title": "Interact, Instruct to Improve: A LLM-Driven Parallel Actor-Reasoner\n  Framework for Enhancing Autonomous Vehicle Interactions",
      "summary": "Autonomous Vehicles (AVs) have entered the commercialization stage, but their\nlimited ability to interact and express intentions still poses challenges in\ninteractions with Human-driven Vehicles (HVs). Recent advances in large\nlanguage models (LLMs) enable bidirectional human-machine communication, but\nthe conflict between slow inference speed and the need for real-time\ndecision-making challenges practical deployment. To address these issues, this\npaper introduces a parallel Actor-Reasoner framework designed to enable\nexplicit bidirectional AV-HV interactions across multiple scenarios. First, by\nfacilitating interactions between the LLM-driven Reasoner and heterogeneous\nsimulated HVs during training, an interaction memory database, referred to as\nthe Actor, is established. Then, by introducing the memory partition module and\nthe two-layer memory retrieval module, the Actor's ability to handle\nheterogeneous HVs is significantly enhanced. Ablation studies and comparisons\nwith other decision-making methods demonstrate that the proposed Actor-Reasoner\nframework significantly improves safety and efficiency. Finally, with the\ncombination of the external Human-Machine Interface (eHMI) information derived\nfrom Reasoner's reasoning and the feasible action solutions retrieved from the\nActor, the effectiveness of the proposed Actor-Reasoner is confirmed in\nmulti-scenario field interactions. Our code is available at\nhttps://github.com/FanGShiYuu/Actor-Reasoner.",
      "upvotes": 1,
      "discussionId": "67c8427247c2aa135346dd84",
      "projectPage": "https://fangshiyuu.github.io/Actor-Reasoner/",
      "githubRepo": "https://github.com/FanGShiYuu/Actor-Reasoner"
    },
    "publishedAt": "2025-03-05T21:37:18.981Z",
    "title": "Interact, Instruct to Improve: A LLM-Driven Parallel Actor-Reasoner Framework for Enhancing Autonomous Vehicle Interactions",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.00502.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66d3290364c1e9b73208af82",
      "avatarUrl": "/avatars/e0ea5f2b366927c7b146f248028a2e59.svg",
      "fullname": "Shiyu Fang",
      "name": "FanGShiYuu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.01763",
      "authors": [
        {
          "_id": "67c92e9c746bbcdbdfa8ebd4",
          "name": "Zhengliang Shi",
          "hidden": false
        },
        {
          "_id": "67c92e9c746bbcdbdfa8ebd5",
          "name": "Yuhan Wang",
          "hidden": false
        },
        {
          "_id": "67c92e9c746bbcdbdfa8ebd6",
          "name": "Lingyong Yan",
          "hidden": false
        },
        {
          "_id": "67c92e9c746bbcdbdfa8ebd7",
          "name": "Pengjie Ren",
          "hidden": false
        },
        {
          "_id": "67c92e9c746bbcdbdfa8ebd8",
          "name": "Shuaiqiang Wang",
          "hidden": false
        },
        {
          "_id": "67c92e9c746bbcdbdfa8ebd9",
          "name": "Dawei Yin",
          "hidden": false
        },
        {
          "_id": "67c92e9c746bbcdbdfa8ebda",
          "name": "Zhaochun Ren",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T17:37:16.000Z",
      "title": "Retrieval Models Aren't Tool-Savvy: Benchmarking Tool Retrieval for\n  Large Language Models",
      "summary": "Tool learning aims to augment large language models (LLMs) with diverse\ntools, enabling them to act as agents for solving practical tasks. Due to the\nlimited context length of tool-using LLMs, adopting information retrieval (IR)\nmodels to select useful tools from large toolsets is a critical initial step.\nHowever, the performance of IR models in tool retrieval tasks remains\nunderexplored and unclear. Most tool-use benchmarks simplify this step by\nmanually pre-annotating a small set of relevant tools for each task, which is\nfar from the real-world scenarios. In this paper, we propose ToolRet, a\nheterogeneous tool retrieval benchmark comprising 7.6k diverse retrieval tasks,\nand a corpus of 43k tools, collected from existing datasets. We benchmark six\ntypes of models on ToolRet. Surprisingly, even the models with strong\nperformance in conventional IR benchmarks, exhibit poor performance on ToolRet.\nThis low retrieval quality degrades the task pass rate of tool-use LLMs. As a\nfurther step, we contribute a large-scale training dataset with over 200k\ninstances, which substantially optimizes the tool retrieval ability of IR\nmodels.",
      "upvotes": 0,
      "discussionId": "67c92e9e746bbcdbdfa8ec57"
    },
    "publishedAt": "2025-03-06T00:12:07.867Z",
    "title": "Retrieval Models Aren't Tool-Savvy: Benchmarking Tool Retrieval for Large Language Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01763.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "640d3eaa3623f6a56dde856d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678589663024-640d3eaa3623f6a56dde856d.jpeg",
      "fullname": "vansin",
      "name": "vansin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.01449",
      "authors": [
        {
          "_id": "67c92e738d5fe8c860571103",
          "name": "Ting Zhang",
          "hidden": false
        },
        {
          "_id": "67c92e738d5fe8c860571104",
          "name": "Chengran Yang",
          "hidden": false
        },
        {
          "_id": "67c92e738d5fe8c860571105",
          "name": "Yindu Su",
          "hidden": false
        },
        {
          "_id": "67c92e738d5fe8c860571106",
          "name": "Martin Weyssow",
          "hidden": false
        },
        {
          "_id": "67c92e738d5fe8c860571107",
          "name": "Hung Nguyen",
          "hidden": false
        },
        {
          "_id": "67c92e738d5fe8c860571108",
          "name": "Tan Bui",
          "hidden": false
        },
        {
          "_id": "67c92e738d5fe8c860571109",
          "name": "Hong Jin Kang",
          "hidden": false
        },
        {
          "_id": "67c92e738d5fe8c86057110a",
          "name": "Yikun Li",
          "hidden": false
        },
        {
          "_id": "67c92e738d5fe8c86057110b",
          "name": "Eng Lieh Ouh",
          "hidden": false
        },
        {
          "_id": "67c92e738d5fe8c86057110c",
          "name": "Lwin Khin Shar",
          "hidden": false
        },
        {
          "_id": "67c92e738d5fe8c86057110d",
          "name": "David Lo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T11:56:00.000Z",
      "title": "Benchmarking Large Language Models for Multi-Language Software\n  Vulnerability Detection",
      "summary": "Recent advancements in generative AI have led to the widespread adoption of\nlarge language models (LLMs) in software engineering, addressing numerous\nlong-standing challenges. However, a comprehensive study examining the\ncapabilities of LLMs in software vulnerability detection (SVD), a crucial\naspect of software security, is currently lacking. Existing research primarily\nfocuses on evaluating LLMs using C/C++ datasets. It typically explores only one\nor two strategies among prompt engineering, instruction tuning, and sequence\nclassification fine-tuning for open-source LLMs. Consequently, there is a\nsignificant knowledge gap regarding the effectiveness of diverse LLMs in\ndetecting vulnerabilities across various programming languages. To address this\nknowledge gap, we present a comprehensive empirical study evaluating the\nperformance of LLMs on the SVD task. We have compiled a comprehensive dataset\ncomprising 8,260 vulnerable functions in Python, 7,505 in Java, and 28,983 in\nJavaScript. We assess five open-source LLMs using multiple approaches,\nincluding prompt engineering, instruction tuning, and sequence classification\nfine-tuning. These LLMs are benchmarked against five fine-tuned small language\nmodels and two open-source static application security testing tools.\nFurthermore, we explore two avenues to improve LLM performance on SVD: a) Data\nperspective: Retraining models using downsampled balanced datasets. b) Model\nperspective: Investigating ensemble learning methods that combine predictions\nfrom multiple LLMs. Our comprehensive experiments demonstrate that SVD remains\na challenging task for LLMs. This study provides a thorough understanding of\nthe role of LLMs in SVD and offers practical insights for future advancements\nin leveraging generative AI to enhance software security practices.",
      "upvotes": 0,
      "discussionId": "67c92e748d5fe8c860571142"
    },
    "publishedAt": "2025-03-06T00:11:25.013Z",
    "title": "Benchmarking Large Language Models for Multi-Language Software Vulnerability Detection",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01449.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "640d3eaa3623f6a56dde856d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678589663024-640d3eaa3623f6a56dde856d.jpeg",
      "fullname": "vansin",
      "name": "vansin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.01372",
      "authors": [
        {
          "_id": "67c6bd6e8f3e7fd471affd06",
          "name": "Joel Niklaus",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd07",
          "name": "Jakob Merane",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd08",
          "name": "Luka Nenadic",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd09",
          "name": "Sina Ahmadi",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd0a",
          "name": "Yingqiang Gao",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd0b",
          "name": "Cyrill A. H. Chevalley",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd0c",
          "name": "Claude Humbel",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd0d",
          "name": "Christophe Gösken",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd0e",
          "name": "Lorenzo Tanzi",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd0f",
          "name": "Thomas Lüthi",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd10",
          "name": "Stefan Palombo",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd11",
          "name": "Spencer Poff",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd12",
          "name": "Boling Yang",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd13",
          "name": "Nan Wu",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd14",
          "name": "Matthew Guillod",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd15",
          "name": "Robin Mamié",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd16",
          "name": "Daniel Brunner",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd17",
          "name": "Julio Pereyra",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd18",
          "name": "Niko Grupen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T10:10:30.000Z",
      "title": "SwiLTra-Bench: The Swiss Legal Translation Benchmark",
      "summary": "In Switzerland legal translation is uniquely important due to the country's\nfour official languages and requirements for multilingual legal documentation.\nHowever, this process traditionally relies on professionals who must be both\nlegal experts and skilled translators -- creating bottlenecks and impacting\neffective access to justice. To address this challenge, we introduce\nSwiLTra-Bench, a comprehensive multilingual benchmark of over 180K aligned\nSwiss legal translation pairs comprising laws, headnotes, and press releases\nacross all Swiss languages along with English, designed to evaluate LLM-based\ntranslation systems. Our systematic evaluation reveals that frontier models\nachieve superior translation performance across all document types, while\nspecialized translation systems excel specifically in laws but under-perform in\nheadnotes. Through rigorous testing and human expert validation, we demonstrate\nthat while fine-tuning open SLMs significantly improves their translation\nquality, they still lag behind the best zero-shot prompted frontier models such\nas Claude-3.5-Sonnet. Additionally, we present SwiLTra-Judge, a specialized LLM\nevaluation system that aligns best with human expert assessments.",
      "upvotes": 0,
      "discussionId": "67c6bd708f3e7fd471affd5d"
    },
    "publishedAt": "2025-03-06T00:10:21.173Z",
    "title": "SwiLTra-Bench: The Swiss Legal Translation Benchmark",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01372.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "640d3eaa3623f6a56dde856d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678589663024-640d3eaa3623f6a56dde856d.jpeg",
      "fullname": "vansin",
      "name": "vansin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": false
  }
]