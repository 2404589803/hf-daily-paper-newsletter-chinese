[
  {
    "paper": {
      "id": "2503.07677",
      "authors": [
        {
          "_id": "67d2ca0767366130cccad93d",
          "user": {
            "_id": "63973ee44e7b4959dc98028f",
            "avatarUrl": "/avatars/2e166fee60844729479bfa4291796c8a.svg",
            "isPro": false,
            "fullname": "Kwanyoung",
            "user": "kwanyoung",
            "type": "user"
          },
          "name": "Kwanyoung Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-14T08:58:03.528Z",
          "hidden": false
        },
        {
          "_id": "67d2ca0767366130cccad93e",
          "name": "Byeongsu Sim",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T07:23:19.000Z",
      "submittedOnDailyAt": "2025-03-17T00:44:05.364Z",
      "title": "PLADIS: Pushing the Limits of Attention in Diffusion Models at Inference\n  Time by Leveraging Sparsity",
      "submittedOnDailyBy": {
        "_id": "63973ee44e7b4959dc98028f",
        "avatarUrl": "/avatars/2e166fee60844729479bfa4291796c8a.svg",
        "isPro": false,
        "fullname": "Kwanyoung",
        "user": "kwanyoung",
        "type": "user"
      },
      "summary": "Diffusion models have shown impressive results in generating high-quality\nconditional samples using guidance techniques such as Classifier-Free Guidance\n(CFG). However, existing methods often require additional training or neural\nfunction evaluations (NFEs), making them incompatible with guidance-distilled\nmodels. Also, they rely on heuristic approaches that need identifying target\nlayers. In this work, we propose a novel and efficient method, termed PLADIS,\nwhich boosts pre-trained models (U-Net/Transformer) by leveraging sparse\nattention. Specifically, we extrapolate query-key correlations using softmax\nand its sparse counterpart in the cross-attention layer during inference,\nwithout requiring extra training or NFEs. By leveraging the noise robustness of\nsparse attention, our PLADIS unleashes the latent potential of text-to-image\ndiffusion models, enabling them to excel in areas where they once struggled\nwith newfound effectiveness. It integrates seamlessly with guidance techniques,\nincluding guidance-distilled models. Extensive experiments show notable\nimprovements in text alignment and human preference, offering a highly\nefficient and universally applicable solution.",
      "upvotes": 46,
      "discussionId": "67d2ca0b67366130cccada34",
      "ai_keywords": [
        "diffusion models",
        "Classifier-Free Guidance (CFG)",
        "neural function evaluations (NFEs)",
        "guidance-distilled models",
        "PLADIS",
        "pre-trained models (U-Net/Transformer)",
        "sparse attention",
        "query-key correlations",
        "softmax",
        "cross-attention layer",
        "noise robustness",
        "text-to-image diffusion models",
        "text alignment",
        "human preference"
      ]
    },
    "publishedAt": "2025-03-10T03:23:19.000Z",
    "title": "PLADIS: Pushing the Limits of Attention in Diffusion Models at Inference\n  Time by Leveraging Sparsity",
    "summary": "Diffusion models have shown impressive results in generating high-quality\nconditional samples using guidance techniques such as Classifier-Free Guidance\n(CFG). However, existing methods often require additional training or neural\nfunction evaluations (NFEs), making them incompatible with guidance-distilled\nmodels. Also, they rely on heuristic approaches that need identifying target\nlayers. In this work, we propose a novel and efficient method, termed PLADIS,\nwhich boosts pre-trained models (U-Net/Transformer) by leveraging sparse\nattention. Specifically, we extrapolate query-key correlations using softmax\nand its sparse counterpart in the cross-attention layer during inference,\nwithout requiring extra training or NFEs. By leveraging the noise robustness of\nsparse attention, our PLADIS unleashes the latent potential of text-to-image\ndiffusion models, enabling them to excel in areas where they once struggled\nwith newfound effectiveness. It integrates seamlessly with guidance techniques,\nincluding guidance-distilled models. Extensive experiments show notable\nimprovements in text alignment and human preference, offering a highly\nefficient and universally applicable solution.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07677.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63973ee44e7b4959dc98028f",
      "avatarUrl": "/avatars/2e166fee60844729479bfa4291796c8a.svg",
      "fullname": "Kwanyoung",
      "name": "kwanyoung",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.11647",
      "authors": [
        {
          "_id": "67d785fa473d4edd330edee1",
          "name": "Jianhong Bai",
          "hidden": false
        },
        {
          "_id": "67d785fa473d4edd330edee2",
          "name": "Menghan Xia",
          "hidden": false
        },
        {
          "_id": "67d785fa473d4edd330edee3",
          "name": "Xiao Fu",
          "hidden": false
        },
        {
          "_id": "67d785fa473d4edd330edee4",
          "name": "Xintao Wang",
          "hidden": false
        },
        {
          "_id": "67d785fa473d4edd330edee5",
          "name": "Lianrui Mu",
          "hidden": false
        },
        {
          "_id": "67d785fa473d4edd330edee6",
          "name": "Jinwen Cao",
          "hidden": false
        },
        {
          "_id": "67d785fa473d4edd330edee7",
          "name": "Zuozhu Liu",
          "hidden": false
        },
        {
          "_id": "67d785fa473d4edd330edee8",
          "name": "Haoji Hu",
          "hidden": false
        },
        {
          "_id": "67d785fa473d4edd330edee9",
          "name": "Xiang Bai",
          "hidden": false
        },
        {
          "_id": "67d785fa473d4edd330edeea",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "67d785fa473d4edd330edeeb",
          "name": "Di Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6530bf50f145530101ec03a2/RzQL-WqDDCxBy_j4rJuMl.mp4"
      ],
      "publishedAt": "2025-03-14T17:59:31.000Z",
      "submittedOnDailyAt": "2025-03-17T00:50:10.251Z",
      "title": "ReCamMaster: Camera-Controlled Generative Rendering from A Single Video",
      "submittedOnDailyBy": {
        "_id": "6530bf50f145530101ec03a2",
        "avatarUrl": "/avatars/c61c00c314cf202b64968e51e855694d.svg",
        "isPro": false,
        "fullname": "Jianhong Bai",
        "user": "jianhongbai",
        "type": "user"
      },
      "summary": "Camera control has been actively studied in text or image conditioned video\ngeneration tasks. However, altering camera trajectories of a given video\nremains under-explored, despite its importance in the field of video creation.\nIt is non-trivial due to the extra constraints of maintaining multiple-frame\nappearance and dynamic synchronization. To address this, we present\nReCamMaster, a camera-controlled generative video re-rendering framework that\nreproduces the dynamic scene of an input video at novel camera trajectories.\nThe core innovation lies in harnessing the generative capabilities of\npre-trained text-to-video models through a simple yet powerful video\nconditioning mechanism -- its capability often overlooked in current research.\nTo overcome the scarcity of qualified training data, we construct a\ncomprehensive multi-camera synchronized video dataset using Unreal Engine 5,\nwhich is carefully curated to follow real-world filming characteristics,\ncovering diverse scenes and camera movements. It helps the model generalize to\nin-the-wild videos. Lastly, we further improve the robustness to diverse inputs\nthrough a meticulously designed training strategy. Extensive experiments tell\nthat our method substantially outperforms existing state-of-the-art approaches\nand strong baselines. Our method also finds promising applications in video\nstabilization, super-resolution, and outpainting. Project page:\nhttps://jianhongbai.github.io/ReCamMaster/",
      "upvotes": 45,
      "discussionId": "67d785fb473d4edd330edf77",
      "ai_keywords": [
        "ReCamMaster",
        "text-to-video models",
        "video conditioning mechanism",
        "multi-camera synchronized video dataset",
        "Unreal Engine 5",
        "video stabilization",
        "super-resolution",
        "outpainting"
      ]
    },
    "publishedAt": "2025-03-14T13:59:31.000Z",
    "title": "ReCamMaster: Camera-Controlled Generative Rendering from A Single Video",
    "summary": "Camera control has been actively studied in text or image conditioned video\ngeneration tasks. However, altering camera trajectories of a given video\nremains under-explored, despite its importance in the field of video creation.\nIt is non-trivial due to the extra constraints of maintaining multiple-frame\nappearance and dynamic synchronization. To address this, we present\nReCamMaster, a camera-controlled generative video re-rendering framework that\nreproduces the dynamic scene of an input video at novel camera trajectories.\nThe core innovation lies in harnessing the generative capabilities of\npre-trained text-to-video models through a simple yet powerful video\nconditioning mechanism -- its capability often overlooked in current research.\nTo overcome the scarcity of qualified training data, we construct a\ncomprehensive multi-camera synchronized video dataset using Unreal Engine 5,\nwhich is carefully curated to follow real-world filming characteristics,\ncovering diverse scenes and camera movements. It helps the model generalize to\nin-the-wild videos. Lastly, we further improve the robustness to diverse inputs\nthrough a meticulously designed training strategy. Extensive experiments tell\nthat our method substantially outperforms existing state-of-the-art approaches\nand strong baselines. Our method also finds promising applications in video\nstabilization, super-resolution, and outpainting. Project page:\nhttps://jianhongbai.github.io/ReCamMaster/",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6530bf50f145530101ec03a2/RzQL-WqDDCxBy_j4rJuMl.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11647.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "6530bf50f145530101ec03a2",
      "avatarUrl": "/avatars/c61c00c314cf202b64968e51e855694d.svg",
      "fullname": "Jianhong Bai",
      "name": "jianhongbai",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.11646",
      "authors": [
        {
          "_id": "67d78c194fd0e3fa3a082f8d",
          "name": "Siyuan Huang",
          "hidden": false
        },
        {
          "_id": "67d78c194fd0e3fa3a082f8e",
          "name": "Yue Liao",
          "hidden": false
        },
        {
          "_id": "67d78c194fd0e3fa3a082f8f",
          "name": "Siyuan Feng",
          "hidden": false
        },
        {
          "_id": "67d78c194fd0e3fa3a082f90",
          "name": "Shu Jiang",
          "hidden": false
        },
        {
          "_id": "67d78c194fd0e3fa3a082f91",
          "name": "Si Liu",
          "hidden": false
        },
        {
          "_id": "67d78c194fd0e3fa3a082f92",
          "name": "Hongsheng Li",
          "hidden": false
        },
        {
          "_id": "67d78c194fd0e3fa3a082f93",
          "name": "Maoqing Yao",
          "hidden": false
        },
        {
          "_id": "67d78c194fd0e3fa3a082f94",
          "name": "Guanghui Ren",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/634e4120038b5879133552f5/2Cb7g14KRbbgg6yotocsP.mp4"
      ],
      "publishedAt": "2025-03-14T17:59:07.000Z",
      "submittedOnDailyAt": "2025-03-17T01:30:24.394Z",
      "title": "Adversarial Data Collection: Human-Collaborative Perturbations for\n  Efficient and Robust Robotic Imitation Learning",
      "submittedOnDailyBy": {
        "_id": "634e4120038b5879133552f5",
        "avatarUrl": "/avatars/34ec861b4bbf1aecf927a7d6e726c7a4.svg",
        "isPro": true,
        "fullname": "Siyuan",
        "user": "SiyuanH",
        "type": "user"
      },
      "summary": "The pursuit of data efficiency, where quality outweighs quantity, has emerged\nas a cornerstone in robotic manipulation, especially given the high costs\nassociated with real-world data collection. We propose that maximizing the\ninformational density of individual demonstrations can dramatically reduce\nreliance on large-scale datasets while improving task performance. To this end,\nwe introduce Adversarial Data Collection, a Human-in-the-Loop (HiL) framework\nthat redefines robotic data acquisition through real-time, bidirectional\nhuman-environment interactions. Unlike conventional pipelines that passively\nrecord static demonstrations, ADC adopts a collaborative perturbation paradigm:\nduring a single episode, an adversarial operator dynamically alters object\nstates, environmental conditions, and linguistic commands, while the\ntele-operator adaptively adjusts actions to overcome these evolving challenges.\nThis process compresses diverse failure-recovery behaviors, compositional task\nvariations, and environmental perturbations into minimal demonstrations. Our\nexperiments demonstrate that ADC-trained models achieve superior compositional\ngeneralization to unseen task instructions, enhanced robustness to perceptual\nperturbations, and emergent error recovery capabilities. Strikingly, models\ntrained with merely 20% of the demonstration volume collected through ADC\nsignificantly outperform traditional approaches using full datasets. These\nadvances bridge the gap between data-centric learning paradigms and practical\nrobotic deployment, demonstrating that strategic data acquisition, not merely\npost-hoc processing, is critical for scalable, real-world robot learning.\nAdditionally, we are curating a large-scale ADC-Robotics dataset comprising\nreal-world manipulation tasks with adversarial perturbations. This benchmark\nwill be open-sourced to facilitate advancements in robotic imitation learning.",
      "upvotes": 22,
      "discussionId": "67d78c1b4fd0e3fa3a08301c",
      "projectPage": " https://sites.google.com/view/adc-robot",
      "ai_keywords": [
        "Adversarial Data Collection",
        "Human-in-the-Loop (HiL)",
        "real-time, bidirectional human-environment interactions",
        "collaborative perturbation paradigm",
        "adversarial operator",
        "tele-operator",
        "compositional generalization",
        "perceptual perturbations",
        "error recovery capabilities",
        "ADC-trained models",
        "ADC-Robotics dataset",
        "robotic imitation learning"
      ]
    },
    "publishedAt": "2025-03-14T13:59:07.000Z",
    "title": "Adversarial Data Collection: Human-Collaborative Perturbations for\n  Efficient and Robust Robotic Imitation Learning",
    "summary": "The pursuit of data efficiency, where quality outweighs quantity, has emerged\nas a cornerstone in robotic manipulation, especially given the high costs\nassociated with real-world data collection. We propose that maximizing the\ninformational density of individual demonstrations can dramatically reduce\nreliance on large-scale datasets while improving task performance. To this end,\nwe introduce Adversarial Data Collection, a Human-in-the-Loop (HiL) framework\nthat redefines robotic data acquisition through real-time, bidirectional\nhuman-environment interactions. Unlike conventional pipelines that passively\nrecord static demonstrations, ADC adopts a collaborative perturbation paradigm:\nduring a single episode, an adversarial operator dynamically alters object\nstates, environmental conditions, and linguistic commands, while the\ntele-operator adaptively adjusts actions to overcome these evolving challenges.\nThis process compresses diverse failure-recovery behaviors, compositional task\nvariations, and environmental perturbations into minimal demonstrations. Our\nexperiments demonstrate that ADC-trained models achieve superior compositional\ngeneralization to unseen task instructions, enhanced robustness to perceptual\nperturbations, and emergent error recovery capabilities. Strikingly, models\ntrained with merely 20% of the demonstration volume collected through ADC\nsignificantly outperform traditional approaches using full datasets. These\nadvances bridge the gap between data-centric learning paradigms and practical\nrobotic deployment, demonstrating that strategic data acquisition, not merely\npost-hoc processing, is critical for scalable, real-world robot learning.\nAdditionally, we are curating a large-scale ADC-Robotics dataset comprising\nreal-world manipulation tasks with adversarial perturbations. This benchmark\nwill be open-sourced to facilitate advancements in robotic imitation learning.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/634e4120038b5879133552f5/2Cb7g14KRbbgg6yotocsP.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11646.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "634e4120038b5879133552f5",
      "avatarUrl": "/avatars/34ec861b4bbf1aecf927a7d6e726c7a4.svg",
      "fullname": "Siyuan",
      "name": "SiyuanH",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.11224",
      "authors": [
        {
          "_id": "67d788b6ba098a0651e1e235",
          "name": "Xingtai Lv",
          "hidden": false
        },
        {
          "_id": "67d788b6ba098a0651e1e236",
          "name": "Youbang Sun",
          "hidden": false
        },
        {
          "_id": "67d788b6ba098a0651e1e237",
          "name": "Kaiyan Zhang",
          "hidden": false
        },
        {
          "_id": "67d788b6ba098a0651e1e238",
          "name": "Shang Qu",
          "hidden": false
        },
        {
          "_id": "67d788b6ba098a0651e1e239",
          "name": "Xuekai Zhu",
          "hidden": false
        },
        {
          "_id": "67d788b6ba098a0651e1e23a",
          "name": "Yuchen Fan",
          "hidden": false
        },
        {
          "_id": "67d788b6ba098a0651e1e23b",
          "name": "Yi Wu",
          "hidden": false
        },
        {
          "_id": "67d788b6ba098a0651e1e23c",
          "name": "Ermo Hua",
          "hidden": false
        },
        {
          "_id": "67d788b6ba098a0651e1e23d",
          "name": "Xinwei Long",
          "hidden": false
        },
        {
          "_id": "67d788b6ba098a0651e1e23e",
          "name": "Ning Ding",
          "hidden": false
        },
        {
          "_id": "67d788b6ba098a0651e1e23f",
          "name": "Bowen Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-14T09:20:31.000Z",
      "submittedOnDailyAt": "2025-03-17T01:26:02.931Z",
      "title": "Technologies on Effectiveness and Efficiency: A Survey of State Spaces\n  Models",
      "submittedOnDailyBy": {
        "_id": "6445fa2ffc22e309d78bef3e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6445fa2ffc22e309d78bef3e/FQaINLd0PjgY9EnK_APRk.jpeg",
        "isPro": false,
        "fullname": "Messi Hua",
        "user": "Messi-Hua",
        "type": "user"
      },
      "summary": "State Space Models (SSMs) have emerged as a promising alternative to the\npopular transformer-based models and have been increasingly gaining attention.\nCompared to transformers, SSMs excel at tasks with sequential data or longer\ncontexts, demonstrating comparable performances with significant efficiency\ngains. In this survey, we provide a coherent and systematic overview for SSMs,\nincluding their theoretical motivations, mathematical formulations, comparison\nwith existing model classes, and various applications. We divide the SSM series\ninto three main sections, providing a detailed introduction to the original\nSSM, the structured SSM represented by S4, and the selective SSM typified by\nMamba. We put an emphasis on technicality, and highlight the various key\ntechniques introduced to address the effectiveness and efficiency of SSMs. We\nhope this manuscript serves as an introduction for researchers to explore the\ntheoretical foundations of SSMs.",
      "upvotes": 15,
      "discussionId": "67d788b7ba098a0651e1e2a4",
      "ai_keywords": [
        "State Space Models (SSMs)",
        "transformer-based models",
        "sequential data",
        "theoretical motivations",
        "mathematical formulations",
        "comparison",
        "model classes",
        "original SSM",
        "structured SSM",
        "S4",
        "selective SSM",
        "Mamba",
        "effectiveness",
        "efficiency"
      ]
    },
    "publishedAt": "2025-03-14T05:20:31.000Z",
    "title": "Technologies on Effectiveness and Efficiency: A Survey of State Spaces\n  Models",
    "summary": "State Space Models (SSMs) have emerged as a promising alternative to the\npopular transformer-based models and have been increasingly gaining attention.\nCompared to transformers, SSMs excel at tasks with sequential data or longer\ncontexts, demonstrating comparable performances with significant efficiency\ngains. In this survey, we provide a coherent and systematic overview for SSMs,\nincluding their theoretical motivations, mathematical formulations, comparison\nwith existing model classes, and various applications. We divide the SSM series\ninto three main sections, providing a detailed introduction to the original\nSSM, the structured SSM represented by S4, and the selective SSM typified by\nMamba. We put an emphasis on technicality, and highlight the various key\ntechniques introduced to address the effectiveness and efficiency of SSMs. We\nhope this manuscript serves as an introduction for researchers to explore the\ntheoretical foundations of SSMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11224.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6445fa2ffc22e309d78bef3e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6445fa2ffc22e309d78bef3e/FQaINLd0PjgY9EnK_APRk.jpeg",
      "fullname": "Messi Hua",
      "name": "Messi-Hua",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.11069",
      "authors": [
        {
          "_id": "67d785458678eaf139e3c594",
          "name": "Chaoyun Zhang",
          "hidden": false
        },
        {
          "_id": "67d785458678eaf139e3c595",
          "name": "Shilin He",
          "hidden": false
        },
        {
          "_id": "67d785458678eaf139e3c596",
          "name": "Liqun Li",
          "hidden": false
        },
        {
          "_id": "67d785458678eaf139e3c597",
          "name": "Si Qin",
          "hidden": false
        },
        {
          "_id": "67d785458678eaf139e3c598",
          "name": "Yu Kang",
          "hidden": false
        },
        {
          "_id": "67d785458678eaf139e3c599",
          "name": "Qingwei Lin",
          "hidden": false
        },
        {
          "_id": "67d785458678eaf139e3c59a",
          "name": "Dongmei Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-14T04:26:21.000Z",
      "submittedOnDailyAt": "2025-03-17T00:43:33.225Z",
      "title": "API Agents vs. GUI Agents: Divergence and Convergence",
      "submittedOnDailyBy": {
        "_id": "654dbac9938fbf1e696be8aa",
        "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
        "isPro": false,
        "fullname": "Chaoyun Zhang",
        "user": "vyokky",
        "type": "user"
      },
      "summary": "Large language models (LLMs) have evolved beyond simple text generation to\npower software agents that directly translate natural language commands into\ntangible actions. While API-based LLM agents initially rose to prominence for\ntheir robust automation capabilities and seamless integration with programmatic\nendpoints, recent progress in multimodal LLM research has enabled GUI-based LLM\nagents that interact with graphical user interfaces in a human-like manner.\nAlthough these two paradigms share the goal of enabling LLM-driven task\nautomation, they diverge significantly in architectural complexity, development\nworkflows, and user interaction models.\n  This paper presents the first comprehensive comparative study of API-based\nand GUI-based LLM agents, systematically analyzing their divergence and\npotential convergence. We examine key dimensions and highlight scenarios in\nwhich hybrid approaches can harness their complementary strengths. By proposing\nclear decision criteria and illustrating practical use cases, we aim to guide\npractitioners and researchers in selecting, combining, or transitioning between\nthese paradigms. Ultimately, we indicate that continuing innovations in\nLLM-based automation are poised to blur the lines between API- and GUI-driven\nagents, paving the way for more flexible, adaptive solutions in a wide range of\nreal-world applications.",
      "upvotes": 13,
      "discussionId": "67d785468678eaf139e3c5ee"
    },
    "publishedAt": "2025-03-14T00:26:21.000Z",
    "title": "API Agents vs. GUI Agents: Divergence and Convergence",
    "summary": "Large language models (LLMs) have evolved beyond simple text generation to\npower software agents that directly translate natural language commands into\ntangible actions. While API-based LLM agents initially rose to prominence for\ntheir robust automation capabilities and seamless integration with programmatic\nendpoints, recent progress in multimodal LLM research has enabled GUI-based LLM\nagents that interact with graphical user interfaces in a human-like manner.\nAlthough these two paradigms share the goal of enabling LLM-driven task\nautomation, they diverge significantly in architectural complexity, development\nworkflows, and user interaction models.\n  This paper presents the first comprehensive comparative study of API-based\nand GUI-based LLM agents, systematically analyzing their divergence and\npotential convergence. We examine key dimensions and highlight scenarios in\nwhich hybrid approaches can harness their complementary strengths. By proposing\nclear decision criteria and illustrating practical use cases, we aim to guide\npractitioners and researchers in selecting, combining, or transitioning between\nthese paradigms. Ultimately, we indicate that continuing innovations in\nLLM-based automation are poised to blur the lines between API- and GUI-driven\nagents, paving the way for more flexible, adaptive solutions in a wide range of\nreal-world applications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11069.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "654dbac9938fbf1e696be8aa",
      "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
      "fullname": "Chaoyun Zhang",
      "name": "vyokky",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.11514",
      "authors": [
        {
          "_id": "67d778325121a10e6fc650b3",
          "name": "Pengxin Guo",
          "hidden": false
        },
        {
          "_id": "67d778325121a10e6fc650b4",
          "name": "Runxi Wang",
          "hidden": false
        },
        {
          "_id": "67d778325121a10e6fc650b5",
          "name": "Shuang Zeng",
          "hidden": false
        },
        {
          "_id": "67d778325121a10e6fc650b6",
          "name": "Jinjing Zhu",
          "hidden": false
        },
        {
          "_id": "67d778325121a10e6fc650b7",
          "name": "Haoning Jiang",
          "hidden": false
        },
        {
          "_id": "67d778325121a10e6fc650b8",
          "name": "Yanran Wang",
          "hidden": false
        },
        {
          "_id": "67d778325121a10e6fc650b9",
          "name": "Yuyin Zhou",
          "hidden": false
        },
        {
          "_id": "67d778325121a10e6fc650ba",
          "name": "Feifei Wang",
          "hidden": false
        },
        {
          "_id": "67d778325121a10e6fc650bb",
          "name": "Hui Xiong",
          "hidden": false
        },
        {
          "_id": "67d778325121a10e6fc650bc",
          "name": "Liangqiong Qu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-13T08:08:44.000Z",
      "submittedOnDailyAt": "2025-03-17T00:38:48.278Z",
      "title": "Exploring the Vulnerabilities of Federated Learning: A Deep Dive into\n  Gradient Inversion Attacks",
      "submittedOnDailyBy": {
        "_id": "668f440894dfc0ed1a7006ed",
        "avatarUrl": "/avatars/fa0d328300b03bcbbf9b3a7532f28458.svg",
        "isPro": false,
        "fullname": "Pengxin Guo",
        "user": "gpx333",
        "type": "user"
      },
      "summary": "Federated Learning (FL) has emerged as a promising privacy-preserving\ncollaborative model training paradigm without sharing raw data. However, recent\nstudies have revealed that private information can still be leaked through\nshared gradient information and attacked by Gradient Inversion Attacks (GIA).\nWhile many GIA methods have been proposed, a detailed analysis, evaluation, and\nsummary of these methods are still lacking. Although various survey papers\nsummarize existing privacy attacks in FL, few studies have conducted extensive\nexperiments to unveil the effectiveness of GIA and their associated limiting\nfactors in this context. To fill this gap, we first undertake a systematic\nreview of GIA and categorize existing methods into three types, i.e.,\noptimization-based GIA (OP-GIA), generation-based GIA\n(GEN-GIA), and analytics-based GIA (ANA-GIA). Then, we comprehensively\nanalyze and evaluate the three types of GIA in FL, providing insights into the\nfactors that influence their performance, practicality, and potential threats.\nOur findings indicate that OP-GIA is the most practical attack setting despite\nits unsatisfactory performance, while GEN-GIA has many dependencies and ANA-GIA\nis easily detectable, making them both impractical. Finally, we offer a\nthree-stage defense pipeline to users when designing FL frameworks and\nprotocols for better privacy protection and share some future research\ndirections from the perspectives of attackers and defenders that we believe\nshould be pursued. We hope that our study can help researchers design more\nrobust FL frameworks to defend against these attacks.",
      "upvotes": 12,
      "discussionId": "67d778395121a10e6fc652eb",
      "ai_keywords": [
        "Gradient Inversion Attacks (GIA)",
        "optimization-based GIA (OP-GIA)",
        "generation-based GIA (GEN-GIA)",
        "analytics-based GIA (ANA-GIA)"
      ]
    },
    "publishedAt": "2025-03-13T04:08:44.000Z",
    "title": "Exploring the Vulnerabilities of Federated Learning: A Deep Dive into\n  Gradient Inversion Attacks",
    "summary": "Federated Learning (FL) has emerged as a promising privacy-preserving\ncollaborative model training paradigm without sharing raw data. However, recent\nstudies have revealed that private information can still be leaked through\nshared gradient information and attacked by Gradient Inversion Attacks (GIA).\nWhile many GIA methods have been proposed, a detailed analysis, evaluation, and\nsummary of these methods are still lacking. Although various survey papers\nsummarize existing privacy attacks in FL, few studies have conducted extensive\nexperiments to unveil the effectiveness of GIA and their associated limiting\nfactors in this context. To fill this gap, we first undertake a systematic\nreview of GIA and categorize existing methods into three types, i.e.,\noptimization-based GIA (OP-GIA), generation-based GIA\n(GEN-GIA), and analytics-based GIA (ANA-GIA). Then, we comprehensively\nanalyze and evaluate the three types of GIA in FL, providing insights into the\nfactors that influence their performance, practicality, and potential threats.\nOur findings indicate that OP-GIA is the most practical attack setting despite\nits unsatisfactory performance, while GEN-GIA has many dependencies and ANA-GIA\nis easily detectable, making them both impractical. Finally, we offer a\nthree-stage defense pipeline to users when designing FL frameworks and\nprotocols for better privacy protection and share some future research\ndirections from the perspectives of attackers and defenders that we believe\nshould be pursued. We hope that our study can help researchers design more\nrobust FL frameworks to defend against these attacks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11514.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "668f440894dfc0ed1a7006ed",
      "avatarUrl": "/avatars/fa0d328300b03bcbbf9b3a7532f28458.svg",
      "fullname": "Pengxin Guo",
      "name": "gpx333",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.10970",
      "authors": [
        {
          "_id": "67d771335e9c4135a570f57f",
          "name": "Shanghua Gao",
          "hidden": false
        },
        {
          "_id": "67d771335e9c4135a570f580",
          "name": "Richard Zhu",
          "hidden": false
        },
        {
          "_id": "67d771335e9c4135a570f581",
          "name": "Zhenglun Kong",
          "hidden": false
        },
        {
          "_id": "67d771335e9c4135a570f582",
          "name": "Ayush Noori",
          "hidden": false
        },
        {
          "_id": "67d771335e9c4135a570f583",
          "name": "Xiaorui Su",
          "hidden": false
        },
        {
          "_id": "67d771335e9c4135a570f584",
          "name": "Curtis Ginder",
          "hidden": false
        },
        {
          "_id": "67d771335e9c4135a570f585",
          "name": "Theodoros Tsiligkaridis",
          "hidden": false
        },
        {
          "_id": "67d771335e9c4135a570f586",
          "name": "Marinka Zitnik",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-14T00:28:15.000Z",
      "submittedOnDailyAt": "2025-03-17T02:04:58.876Z",
      "title": "TxAgent: An AI Agent for Therapeutic Reasoning Across a Universe of\n  Tools",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Precision therapeutics require multimodal adaptive models that generate\npersonalized treatment recommendations. We introduce TxAgent, an AI agent that\nleverages multi-step reasoning and real-time biomedical knowledge retrieval\nacross a toolbox of 211 tools to analyze drug interactions, contraindications,\nand patient-specific treatment strategies. TxAgent evaluates how drugs interact\nat molecular, pharmacokinetic, and clinical levels, identifies\ncontraindications based on patient comorbidities and concurrent medications,\nand tailors treatment strategies to individual patient characteristics. It\nretrieves and synthesizes evidence from multiple biomedical sources, assesses\ninteractions between drugs and patient conditions, and refines treatment\nrecommendations through iterative reasoning. It selects tools based on task\nobjectives and executes structured function calls to solve therapeutic tasks\nthat require clinical reasoning and cross-source validation. The ToolUniverse\nconsolidates 211 tools from trusted sources, including all US FDA-approved\ndrugs since 1939 and validated clinical insights from Open Targets. TxAgent\noutperforms leading LLMs, tool-use models, and reasoning agents across five new\nbenchmarks: DrugPC, BrandPC, GenericPC, TreatmentPC, and DescriptionPC,\ncovering 3,168 drug reasoning tasks and 456 personalized treatment scenarios.\nIt achieves 92.1% accuracy in open-ended drug reasoning tasks, surpassing\nGPT-4o and outperforming DeepSeek-R1 (671B) in structured multi-step reasoning.\nTxAgent generalizes across drug name variants and descriptions. By integrating\nmulti-step inference, real-time knowledge grounding, and tool-assisted\ndecision-making, TxAgent ensures that treatment recommendations align with\nestablished clinical guidelines and real-world evidence, reducing the risk of\nadverse events and improving therapeutic decision-making.",
      "upvotes": 7,
      "discussionId": "67d771345e9c4135a570f5d0",
      "ai_keywords": [
        "AI agent",
        "multi-step reasoning",
        "biomedical knowledge retrieval",
        "drug interactions",
        "contraindications",
        "patient-specific treatment strategies",
        "molecular levels",
        "pharmacokinetic levels",
        "clinical levels",
        "patient comorbidities",
        "concurrent medications",
        "ToolUniverse",
        "FDA-approved drugs",
        "Open Targets",
        "DrugPC",
        "BrandPC",
        "GenericPC",
        "TreatmentPC",
        "DescriptionPC",
        "drug reasoning tasks",
        "personalized treatment scenarios",
        "multi-step inference",
        "knowledge grounding",
        "tool-assisted decision-making",
        "clinical guidelines",
        "real-world evidence",
        "adverse events",
        "therapeutic decision-making"
      ]
    },
    "publishedAt": "2025-03-13T20:28:15.000Z",
    "title": "TxAgent: An AI Agent for Therapeutic Reasoning Across a Universe of\n  Tools",
    "summary": "Precision therapeutics require multimodal adaptive models that generate\npersonalized treatment recommendations. We introduce TxAgent, an AI agent that\nleverages multi-step reasoning and real-time biomedical knowledge retrieval\nacross a toolbox of 211 tools to analyze drug interactions, contraindications,\nand patient-specific treatment strategies. TxAgent evaluates how drugs interact\nat molecular, pharmacokinetic, and clinical levels, identifies\ncontraindications based on patient comorbidities and concurrent medications,\nand tailors treatment strategies to individual patient characteristics. It\nretrieves and synthesizes evidence from multiple biomedical sources, assesses\ninteractions between drugs and patient conditions, and refines treatment\nrecommendations through iterative reasoning. It selects tools based on task\nobjectives and executes structured function calls to solve therapeutic tasks\nthat require clinical reasoning and cross-source validation. The ToolUniverse\nconsolidates 211 tools from trusted sources, including all US FDA-approved\ndrugs since 1939 and validated clinical insights from Open Targets. TxAgent\noutperforms leading LLMs, tool-use models, and reasoning agents across five new\nbenchmarks: DrugPC, BrandPC, GenericPC, TreatmentPC, and DescriptionPC,\ncovering 3,168 drug reasoning tasks and 456 personalized treatment scenarios.\nIt achieves 92.1% accuracy in open-ended drug reasoning tasks, surpassing\nGPT-4o and outperforming DeepSeek-R1 (671B) in structured multi-step reasoning.\nTxAgent generalizes across drug name variants and descriptions. By integrating\nmulti-step inference, real-time knowledge grounding, and tool-assisted\ndecision-making, TxAgent ensures that treatment recommendations align with\nestablished clinical guidelines and real-world evidence, reducing the risk of\nadverse events and improving therapeutic decision-making.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10970.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6382
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.10772",
      "authors": [
        {
          "_id": "67d78ce0b4d0fefa68385d7f",
          "name": "Ju He",
          "hidden": false
        },
        {
          "_id": "67d78ce0b4d0fefa68385d80",
          "name": "Qihang Yu",
          "hidden": false
        },
        {
          "_id": "67d78ce0b4d0fefa68385d81",
          "name": "Qihao Liu",
          "hidden": false
        },
        {
          "_id": "67d78ce0b4d0fefa68385d82",
          "name": "Liang-Chieh Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-13T18:06:13.000Z",
      "submittedOnDailyAt": "2025-03-17T01:16:42.853Z",
      "title": "FlowTok: Flowing Seamlessly Across Text and Image Tokens",
      "submittedOnDailyBy": {
        "_id": "661c9059bcd78151e5c06ea1",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/661c9059bcd78151e5c06ea1/27bfNo1LZeZQ77vWuAa10.png",
        "isPro": false,
        "fullname": "Ju He",
        "user": "turkeyju",
        "type": "user"
      },
      "summary": "Bridging different modalities lies at the heart of cross-modality generation.\nWhile conventional approaches treat the text modality as a conditioning signal\nthat gradually guides the denoising process from Gaussian noise to the target\nimage modality, we explore a much simpler paradigm-directly evolving between\ntext and image modalities through flow matching. This requires projecting both\nmodalities into a shared latent space, which poses a significant challenge due\nto their inherently different representations: text is highly semantic and\nencoded as 1D tokens, whereas images are spatially redundant and represented as\n2D latent embeddings. To address this, we introduce FlowTok, a minimal\nframework that seamlessly flows across text and images by encoding images into\na compact 1D token representation. Compared to prior methods, this design\nreduces the latent space size by 3.3x at an image resolution of 256,\neliminating the need for complex conditioning mechanisms or noise scheduling.\nMoreover, FlowTok naturally extends to image-to-text generation under the same\nformulation. With its streamlined architecture centered around compact 1D\ntokens, FlowTok is highly memory-efficient, requires significantly fewer\ntraining resources, and achieves much faster sampling speeds-all while\ndelivering performance comparable to state-of-the-art models. Code will be\navailable at https://github.com/bytedance/1d-tokenizer.",
      "upvotes": 6,
      "discussionId": "67d78ce1b4d0fefa68385dc8",
      "projectPage": "https://tacju.github.io/projects/flowtok.html",
      "githubRepo": "https://github.com/bytedance/1d-tokenizer/",
      "ai_keywords": [
        "cross-modality generation",
        "flow matching",
        "latent space",
        "denoising process",
        "Gaussian noise",
        "semantic",
        "1D tokens",
        "2D latent embeddings",
        "FlowTok",
        "compact 1D token representation",
        "image-to-text generation",
        "memory-efficient",
        "sampling speeds",
        "state-of-the-art models"
      ]
    },
    "publishedAt": "2025-03-13T14:06:13.000Z",
    "title": "FlowTok: Flowing Seamlessly Across Text and Image Tokens",
    "summary": "Bridging different modalities lies at the heart of cross-modality generation.\nWhile conventional approaches treat the text modality as a conditioning signal\nthat gradually guides the denoising process from Gaussian noise to the target\nimage modality, we explore a much simpler paradigm-directly evolving between\ntext and image modalities through flow matching. This requires projecting both\nmodalities into a shared latent space, which poses a significant challenge due\nto their inherently different representations: text is highly semantic and\nencoded as 1D tokens, whereas images are spatially redundant and represented as\n2D latent embeddings. To address this, we introduce FlowTok, a minimal\nframework that seamlessly flows across text and images by encoding images into\na compact 1D token representation. Compared to prior methods, this design\nreduces the latent space size by 3.3x at an image resolution of 256,\neliminating the need for complex conditioning mechanisms or noise scheduling.\nMoreover, FlowTok naturally extends to image-to-text generation under the same\nformulation. With its streamlined architecture centered around compact 1D\ntokens, FlowTok is highly memory-efficient, requires significantly fewer\ntraining resources, and achieves much faster sampling speeds-all while\ndelivering performance comparable to state-of-the-art models. Code will be\navailable at https://github.com/bytedance/1d-tokenizer.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10772.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "661c9059bcd78151e5c06ea1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/661c9059bcd78151e5c06ea1/27bfNo1LZeZQ77vWuAa10.png",
      "fullname": "Ju He",
      "name": "turkeyju",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.10632",
      "authors": [
        {
          "_id": "67d4f1b1643653fd1cea5b5a",
          "user": {
            "_id": "66d5279130d7ea0b28d6d5d2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66d5279130d7ea0b28d6d5d2/oGFmux--lARBF4PKSkHAu.png",
            "isPro": false,
            "fullname": "Subhajit Maity",
            "user": "maitysubhajit",
            "type": "user"
          },
          "name": "Subhajit Maity",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-15T03:19:40.103Z",
          "hidden": false
        },
        {
          "_id": "67d4f1b1643653fd1cea5b5b",
          "name": "Killian Hitsman",
          "hidden": false
        },
        {
          "_id": "67d4f1b1643653fd1cea5b5c",
          "name": "Xin Li",
          "hidden": false
        },
        {
          "_id": "67d4f1b1643653fd1cea5b5d",
          "user": {
            "_id": "67d58d156db0e6f0c33c0f60",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67d58d156db0e6f0c33c0f60/9KiFw0iEMZQHcHnm1k0ED.jpeg",
            "isPro": false,
            "fullname": "Aritra Dutta",
            "user": "aritradutta",
            "type": "user"
          },
          "name": "Aritra Dutta",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-15T14:35:55.373Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-13T17:59:52.000Z",
      "submittedOnDailyAt": "2025-03-17T01:09:07.184Z",
      "title": "Kolmogorov-Arnold Attention: Is Learnable Attention Better For Vision\n  Transformers?",
      "submittedOnDailyBy": {
        "_id": "66d5279130d7ea0b28d6d5d2",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66d5279130d7ea0b28d6d5d2/oGFmux--lARBF4PKSkHAu.png",
        "isPro": false,
        "fullname": "Subhajit Maity",
        "user": "maitysubhajit",
        "type": "user"
      },
      "summary": "Kolmogorov-Arnold networks (KANs) are a remarkable innovation consisting of\nlearnable activation functions with the potential to capture more complex\nrelationships from data. Although KANs are useful in finding symbolic\nrepresentations and continual learning of one-dimensional functions, their\neffectiveness in diverse machine learning (ML) tasks, such as vision, remains\nquestionable. Presently, KANs are deployed by replacing multilayer perceptrons\n(MLPs) in deep network architectures, including advanced architectures such as\nvision Transformers (ViTs). In this paper, we are the first to design a general\nlearnable Kolmogorov-Arnold Attention (KArAt) for vanilla ViTs that can operate\non any choice of basis. However, the computing and memory costs of training\nthem motivated us to propose a more modular version, and we designed particular\nlearnable attention, called Fourier-KArAt. Fourier-KArAt and its variants\neither outperform their ViT counterparts or show comparable performance on\nCIFAR-10, CIFAR-100, and ImageNet-1K datasets. We dissect these architectures'\nperformance and generalization capacity by analyzing their loss landscapes,\nweight distributions, optimizer path, attention visualization, and spectral\nbehavior, and contrast them with vanilla ViTs. The goal of this paper is not to\nproduce parameter- and compute-efficient attention, but to encourage the\ncommunity to explore KANs in conjunction with more advanced architectures that\nrequire a careful understanding of learnable activations. Our open-source code\nand implementation details are available on: https://subhajitmaity.me/KArAt",
      "upvotes": 4,
      "discussionId": "67d4f1b6643653fd1cea5d20",
      "projectPage": "https://subhajitmaity.me/KArAt",
      "githubRepo": "https://github.com/MaitySubhajit/KArAt",
      "ai_keywords": [
        "Kolmogorov-Arnold networks (KANs)",
        "learnable activation functions",
        "multilayer perceptrons (MLPs)",
        "vision Transformers (ViTs)",
        "Kolmogorov-Arnold Attention (KArAt)",
        "Fourier-KArAt",
        "loss landscapes",
        "weight distributions",
        "optimizer path",
        "attention visualization",
        "spectral behavior"
      ]
    },
    "publishedAt": "2025-03-13T13:59:52.000Z",
    "title": "Kolmogorov-Arnold Attention: Is Learnable Attention Better For Vision\n  Transformers?",
    "summary": "Kolmogorov-Arnold networks (KANs) are a remarkable innovation consisting of\nlearnable activation functions with the potential to capture more complex\nrelationships from data. Although KANs are useful in finding symbolic\nrepresentations and continual learning of one-dimensional functions, their\neffectiveness in diverse machine learning (ML) tasks, such as vision, remains\nquestionable. Presently, KANs are deployed by replacing multilayer perceptrons\n(MLPs) in deep network architectures, including advanced architectures such as\nvision Transformers (ViTs). In this paper, we are the first to design a general\nlearnable Kolmogorov-Arnold Attention (KArAt) for vanilla ViTs that can operate\non any choice of basis. However, the computing and memory costs of training\nthem motivated us to propose a more modular version, and we designed particular\nlearnable attention, called Fourier-KArAt. Fourier-KArAt and its variants\neither outperform their ViT counterparts or show comparable performance on\nCIFAR-10, CIFAR-100, and ImageNet-1K datasets. We dissect these architectures'\nperformance and generalization capacity by analyzing their loss landscapes,\nweight distributions, optimizer path, attention visualization, and spectral\nbehavior, and contrast them with vanilla ViTs. The goal of this paper is not to\nproduce parameter- and compute-efficient attention, but to encourage the\ncommunity to explore KANs in conjunction with more advanced architectures that\nrequire a careful understanding of learnable activations. Our open-source code\nand implementation details are available on: https://subhajitmaity.me/KArAt",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10632.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66d5279130d7ea0b28d6d5d2",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66d5279130d7ea0b28d6d5d2/oGFmux--lARBF4PKSkHAu.png",
      "fullname": "Subhajit Maity",
      "name": "maitysubhajit",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.09279",
      "authors": [
        {
          "_id": "67d2bd340860f2d7ff10e3dc",
          "name": "Luozheng Qin",
          "hidden": false
        },
        {
          "_id": "67d2bd340860f2d7ff10e3dd",
          "name": "Zhiyu Tan",
          "hidden": false
        },
        {
          "_id": "67d2bd340860f2d7ff10e3de",
          "name": "Mengping Yang",
          "hidden": false
        },
        {
          "_id": "67d2bd340860f2d7ff10e3df",
          "name": "Xiaomeng Yang",
          "hidden": false
        },
        {
          "_id": "67d2bd340860f2d7ff10e3e0",
          "name": "Hao Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-12T11:25:04.000Z",
      "submittedOnDailyAt": "2025-03-17T00:46:46.368Z",
      "title": "Cockatiel: Ensembling Synthetic and Human Preferenced Training for\n  Detailed Video Caption",
      "submittedOnDailyBy": {
        "_id": "66a9b3533d417b0baa9220a6",
        "avatarUrl": "/avatars/adc372bd24df1d3bf43258833411e8af.svg",
        "isPro": false,
        "fullname": "Luozheng Qin",
        "user": "Fr0zencr4nE",
        "type": "user"
      },
      "summary": "Video Detailed Captioning (VDC) is a crucial task for vision-language\nbridging, enabling fine-grained descriptions of complex video content. In this\npaper, we first comprehensively benchmark current state-of-the-art approaches\nand systematically identified two critical limitations: biased capability\ntowards specific captioning aspect and misalignment with human preferences. To\naddress these deficiencies, we propose Cockatiel, a novel three-stage training\npipeline that ensembles synthetic and human-aligned training for improving VDC\nperformance. In the first stage, we derive a scorer from a meticulously\nannotated dataset to select synthetic captions high-performing on certain\nfine-grained video-caption alignment and human-preferred while disregarding\nothers. Then, we train Cockatiel-13B, using this curated dataset to infuse it\nwith assembled model strengths and human preferences. Finally, we further\ndistill Cockatiel-8B from Cockatiel-13B for the ease of usage. Extensive\nquantitative and qualitative experiments reflect the effectiveness of our\nmethod, as we not only set new state-of-the-art performance on VDCSCORE in a\ndimension-balanced way but also surpass leading alternatives on human\npreference by a large margin as depicted by the human evaluation results.",
      "upvotes": 4,
      "discussionId": "67d2bd370860f2d7ff10e4da",
      "ai_keywords": [
        "Cockatiel",
        "three-stage training pipeline",
        "synthetic and human-aligned training",
        "fine-grained video-caption alignment",
        "scorer",
        "meticulously annotated dataset",
        "curated dataset",
        "assembled model strengths",
        "human preferences",
        "VDCSCORE",
        "dimension-balanced way",
        "human evaluation results"
      ]
    },
    "publishedAt": "2025-03-12T07:25:04.000Z",
    "title": "Cockatiel: Ensembling Synthetic and Human Preferenced Training for\n  Detailed Video Caption",
    "summary": "Video Detailed Captioning (VDC) is a crucial task for vision-language\nbridging, enabling fine-grained descriptions of complex video content. In this\npaper, we first comprehensively benchmark current state-of-the-art approaches\nand systematically identified two critical limitations: biased capability\ntowards specific captioning aspect and misalignment with human preferences. To\naddress these deficiencies, we propose Cockatiel, a novel three-stage training\npipeline that ensembles synthetic and human-aligned training for improving VDC\nperformance. In the first stage, we derive a scorer from a meticulously\nannotated dataset to select synthetic captions high-performing on certain\nfine-grained video-caption alignment and human-preferred while disregarding\nothers. Then, we train Cockatiel-13B, using this curated dataset to infuse it\nwith assembled model strengths and human preferences. Finally, we further\ndistill Cockatiel-8B from Cockatiel-13B for the ease of usage. Extensive\nquantitative and qualitative experiments reflect the effectiveness of our\nmethod, as we not only set new state-of-the-art performance on VDCSCORE in a\ndimension-balanced way but also surpass leading alternatives on human\npreference by a large margin as depicted by the human evaluation results.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09279.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66a9b3533d417b0baa9220a6",
      "avatarUrl": "/avatars/adc372bd24df1d3bf43258833411e8af.svg",
      "fullname": "Luozheng Qin",
      "name": "Fr0zencr4nE",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.06674",
      "authors": [
        {
          "_id": "67d6881cf997964e21f90598",
          "name": "Yihong Luo",
          "hidden": false
        },
        {
          "_id": "67d6881cf997964e21f90599",
          "name": "Tianyang Hu",
          "hidden": false
        },
        {
          "_id": "67d6881cf997964e21f9059a",
          "name": "Jiacheng Sun",
          "hidden": false
        },
        {
          "_id": "67d6881cf997964e21f9059b",
          "name": "Yujun Cai",
          "hidden": false
        },
        {
          "_id": "67d6881cf997964e21f9059c",
          "name": "Jing Tang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-09T15:53:49.000Z",
      "submittedOnDailyAt": "2025-03-17T02:34:51.976Z",
      "title": "Learning Few-Step Diffusion Models by Trajectory Distribution Matching",
      "submittedOnDailyBy": {
        "_id": "65f7e6856bd4bac5b6a4ecc3",
        "avatarUrl": "/avatars/9c0c48310fb5e99c19700316a0ab531e.svg",
        "isPro": false,
        "fullname": "Yihong Luo",
        "user": "Luo-Yihong",
        "type": "user"
      },
      "summary": "Accelerating diffusion model sampling is crucial for efficient AIGC\ndeployment. While diffusion distillation methods -- based on distribution\nmatching and trajectory matching -- reduce sampling to as few as one step, they\nfall short on complex tasks like text-to-image generation. Few-step generation\noffers a better balance between speed and quality, but existing approaches face\na persistent trade-off: distribution matching lacks flexibility for multi-step\nsampling, while trajectory matching often yields suboptimal image quality. To\nbridge this gap, we propose learning few-step diffusion models by Trajectory\nDistribution Matching (TDM), a unified distillation paradigm that combines the\nstrengths of distribution and trajectory matching. Our method introduces a\ndata-free score distillation objective, aligning the student's trajectory with\nthe teacher's at the distribution level. Further, we develop a\nsampling-steps-aware objective that decouples learning targets across different\nsteps, enabling more adjustable sampling. This approach supports both\ndeterministic sampling for superior image quality and flexible multi-step\nadaptation, achieving state-of-the-art performance with remarkable efficiency.\nOur model, TDM, outperforms existing methods on various backbones, such as SDXL\nand PixArt-alpha, delivering superior quality and significantly reduced\ntraining costs. In particular, our method distills PixArt-alpha into a\n4-step generator that outperforms its teacher on real user preference at 1024\nresolution. This is accomplished with 500 iterations and 2 A800 hours -- a mere\n0.01% of the teacher's training cost. In addition, our proposed TDM can be\nextended to accelerate text-to-video diffusion. Notably, TDM can outperform its\nteacher model (CogVideoX-2B) by using only 4 NFE on VBench, improving the total\nscore from 80.91 to 81.65. Project page: https://tdm-t2x.github.io/",
      "upvotes": 2,
      "discussionId": "67d6881ef997964e21f90660",
      "projectPage": "https://tdm-t2x.github.io/",
      "githubRepo": "https://github.com/Luo-Yihong/TDM",
      "ai_keywords": [
        "diffusion model sampling",
        "diffusion distillation",
        "distribution matching",
        "trajectory matching",
        "few-step generation",
        "Trajectory Distribution Matching (TDM)",
        "data-free score distillation",
        "sampling-steps-aware objective",
        "deterministic sampling",
        "state-of-the-art performance",
        "SDXL",
        "PixArt-$\\alpha$",
        "TDM",
        "text-to-video diffusion",
        "CogVideoX-2B",
        "VBench",
        "NFE"
      ]
    },
    "publishedAt": "2025-03-09T11:53:49.000Z",
    "title": "Learning Few-Step Diffusion Models by Trajectory Distribution Matching",
    "summary": "Accelerating diffusion model sampling is crucial for efficient AIGC\ndeployment. While diffusion distillation methods -- based on distribution\nmatching and trajectory matching -- reduce sampling to as few as one step, they\nfall short on complex tasks like text-to-image generation. Few-step generation\noffers a better balance between speed and quality, but existing approaches face\na persistent trade-off: distribution matching lacks flexibility for multi-step\nsampling, while trajectory matching often yields suboptimal image quality. To\nbridge this gap, we propose learning few-step diffusion models by Trajectory\nDistribution Matching (TDM), a unified distillation paradigm that combines the\nstrengths of distribution and trajectory matching. Our method introduces a\ndata-free score distillation objective, aligning the student's trajectory with\nthe teacher's at the distribution level. Further, we develop a\nsampling-steps-aware objective that decouples learning targets across different\nsteps, enabling more adjustable sampling. This approach supports both\ndeterministic sampling for superior image quality and flexible multi-step\nadaptation, achieving state-of-the-art performance with remarkable efficiency.\nOur model, TDM, outperforms existing methods on various backbones, such as SDXL\nand PixArt-alpha, delivering superior quality and significantly reduced\ntraining costs. In particular, our method distills PixArt-alpha into a\n4-step generator that outperforms its teacher on real user preference at 1024\nresolution. This is accomplished with 500 iterations and 2 A800 hours -- a mere\n0.01% of the teacher's training cost. In addition, our proposed TDM can be\nextended to accelerate text-to-video diffusion. Notably, TDM can outperform its\nteacher model (CogVideoX-2B) by using only 4 NFE on VBench, improving the total\nscore from 80.91 to 81.65. Project page: https://tdm-t2x.github.io/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06674.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "65f7e6856bd4bac5b6a4ecc3",
      "avatarUrl": "/avatars/9c0c48310fb5e99c19700316a0ab531e.svg",
      "fullname": "Yihong Luo",
      "name": "Luo-Yihong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.05689",
      "authors": [
        {
          "_id": "67d37c5c3b54e330517a545d",
          "user": {
            "_id": "665b2ac6e0e2374ca24ba000",
            "avatarUrl": "/avatars/d5218c9fa3dceae7b91df2e1d396bcf3.svg",
            "isPro": false,
            "fullname": "Zebin Xing",
            "user": "XXXXing",
            "type": "user"
          },
          "name": "Zebin Xing",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-14T08:56:46.875Z",
          "hidden": false
        },
        {
          "_id": "67d37c5c3b54e330517a545e",
          "name": "Xingyu Zhang",
          "hidden": false
        },
        {
          "_id": "67d37c5c3b54e330517a545f",
          "name": "Yang Hu",
          "hidden": false
        },
        {
          "_id": "67d37c5c3b54e330517a5460",
          "name": "Bo Jiang",
          "hidden": false
        },
        {
          "_id": "67d37c5c3b54e330517a5461",
          "name": "Tong He",
          "hidden": false
        },
        {
          "_id": "67d37c5c3b54e330517a5462",
          "name": "Qian Zhang",
          "hidden": false
        },
        {
          "_id": "67d37c5c3b54e330517a5463",
          "name": "Xiaoxiao Long",
          "hidden": false
        },
        {
          "_id": "67d37c5c3b54e330517a5464",
          "name": "Wei Yin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T18:52:08.000Z",
      "submittedOnDailyAt": "2025-03-17T01:05:31.649Z",
      "title": "GoalFlow: Goal-Driven Flow Matching for Multimodal Trajectories\n  Generation in End-to-End Autonomous Driving",
      "submittedOnDailyBy": {
        "_id": "654a2b1a83e7bfc4313a5cc7",
        "avatarUrl": "/avatars/dc3dfc3fcd26bb7350a9db0d075c5ea0.svg",
        "isPro": false,
        "fullname": "Wei Yin",
        "user": "WonderingWorld",
        "type": "user"
      },
      "summary": "We propose GoalFlow, an end-to-end autonomous driving method for generating\nhigh-quality multimodal trajectories. In autonomous driving scenarios, there is\nrarely a single suitable trajectory. Recent methods have increasingly focused\non modeling multimodal trajectory distributions. However, they suffer from\ntrajectory selection complexity and reduced trajectory quality due to high\ntrajectory divergence and inconsistencies between guidance and scene\ninformation. To address these issues, we introduce GoalFlow, a novel method\nthat effectively constrains the generative process to produce high-quality,\nmultimodal trajectories. To resolve the trajectory divergence problem inherent\nin diffusion-based methods, GoalFlow constrains the generated trajectories by\nintroducing a goal point. GoalFlow establishes a novel scoring mechanism that\nselects the most appropriate goal point from the candidate points based on\nscene information. Furthermore, GoalFlow employs an efficient generative\nmethod, Flow Matching, to generate multimodal trajectories, and incorporates a\nrefined scoring mechanism to select the optimal trajectory from the candidates.\nOur experimental results, validated on the NavsimDauner2024_navsim,\ndemonstrate that GoalFlow achieves state-of-the-art performance, delivering\nrobust multimodal trajectories for autonomous driving. GoalFlow achieved PDMS\nof 90.3, significantly surpassing other methods. Compared with other\ndiffusion-policy-based methods, our approach requires only a single denoising\nstep to obtain excellent performance. The code is available at\nhttps://github.com/YvanYin/GoalFlow.",
      "upvotes": 2,
      "discussionId": "67d37c5d3b54e330517a54c7",
      "ai_keywords": [
        "GoalFlow",
        "multimodal trajectories",
        "trajectory selection complexity",
        "trajectory divergence",
        "diffusion-based methods",
        "goal point",
        "scoring mechanism",
        "Flow Matching",
        "Navsim",
        "PDMS",
        "diffusion-policy-based methods",
        "denoising step"
      ]
    },
    "publishedAt": "2025-03-07T13:52:08.000Z",
    "title": "GoalFlow: Goal-Driven Flow Matching for Multimodal Trajectories\n  Generation in End-to-End Autonomous Driving",
    "summary": "We propose GoalFlow, an end-to-end autonomous driving method for generating\nhigh-quality multimodal trajectories. In autonomous driving scenarios, there is\nrarely a single suitable trajectory. Recent methods have increasingly focused\non modeling multimodal trajectory distributions. However, they suffer from\ntrajectory selection complexity and reduced trajectory quality due to high\ntrajectory divergence and inconsistencies between guidance and scene\ninformation. To address these issues, we introduce GoalFlow, a novel method\nthat effectively constrains the generative process to produce high-quality,\nmultimodal trajectories. To resolve the trajectory divergence problem inherent\nin diffusion-based methods, GoalFlow constrains the generated trajectories by\nintroducing a goal point. GoalFlow establishes a novel scoring mechanism that\nselects the most appropriate goal point from the candidate points based on\nscene information. Furthermore, GoalFlow employs an efficient generative\nmethod, Flow Matching, to generate multimodal trajectories, and incorporates a\nrefined scoring mechanism to select the optimal trajectory from the candidates.\nOur experimental results, validated on the NavsimDauner2024_navsim,\ndemonstrate that GoalFlow achieves state-of-the-art performance, delivering\nrobust multimodal trajectories for autonomous driving. GoalFlow achieved PDMS\nof 90.3, significantly surpassing other methods. Compared with other\ndiffusion-policy-based methods, our approach requires only a single denoising\nstep to obtain excellent performance. The code is available at\nhttps://github.com/YvanYin/GoalFlow.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05689.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "654a2b1a83e7bfc4313a5cc7",
      "avatarUrl": "/avatars/dc3dfc3fcd26bb7350a9db0d075c5ea0.svg",
      "fullname": "Wei Yin",
      "name": "WonderingWorld",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]