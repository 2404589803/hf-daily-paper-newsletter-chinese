[
  {
    "paper": {
      "id": "2506.09513",
      "authors": [
        {
          "_id": "684b8dbd3b733ba33368701b",
          "user": {
            "_id": "6723079ad1306fe9c76a1d29",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/b4BNPCeZs59MKxo1qmT6r.png",
            "isPro": false,
            "fullname": "Yu Sun",
            "user": "YuSun-AI",
            "type": "user"
          },
          "name": "Yu Sun",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-13T02:32:30.652Z",
          "hidden": false
        },
        {
          "_id": "684b8dbd3b733ba33368701c",
          "name": "Xingyu Qian",
          "hidden": false
        },
        {
          "_id": "684b8dbd3b733ba33368701d",
          "name": "Weiwen Xu",
          "hidden": false
        },
        {
          "_id": "684b8dbd3b733ba33368701e",
          "user": {
            "_id": "64b7cd74ff6d81ae297feded",
            "avatarUrl": "/avatars/880fbc96cc093f5e901ce84f32a1d21d.svg",
            "isPro": false,
            "fullname": "ZHANG HAO",
            "user": "26hzhang",
            "type": "user"
          },
          "name": "Hao Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:43.056Z",
          "hidden": false
        },
        {
          "_id": "684b8dbd3b733ba33368701f",
          "name": "Chenghao Xiao",
          "hidden": false
        },
        {
          "_id": "684b8dbd3b733ba333687020",
          "name": "Long Li",
          "hidden": false
        },
        {
          "_id": "684b8dbd3b733ba333687021",
          "user": {
            "_id": "642eecbf9b2484d7d8526781",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642eecbf9b2484d7d8526781/4IvGbd66s49Wx5pZyZGHA.png",
            "isPro": false,
            "fullname": "Yu Rong",
            "user": "Swrooy",
            "type": "user"
          },
          "name": "Yu Rong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:40.908Z",
          "hidden": false
        },
        {
          "_id": "684b8dbd3b733ba333687022",
          "name": "Wenbing Huang",
          "hidden": false
        },
        {
          "_id": "684b8dbd3b733ba333687023",
          "name": "Qifeng Bai",
          "hidden": false
        },
        {
          "_id": "684b8dbd3b733ba333687024",
          "name": "Tingyang Xu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-11T08:36:55.000Z",
      "submittedOnDailyAt": "2025-06-13T01:06:46.741Z",
      "title": "ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical\n  Reasoning",
      "submittedOnDailyBy": {
        "_id": "6723079ad1306fe9c76a1d29",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/b4BNPCeZs59MKxo1qmT6r.png",
        "isPro": false,
        "fullname": "Yu Sun",
        "user": "YuSun-AI",
        "type": "user"
      },
      "summary": "Though reasoning-based large language models (LLMs) have excelled in\nmathematics and programming, their capabilities in knowledge-intensive medical\nquestion answering remain underexplored. To address this, we introduce\nReasonMed, the largest medical reasoning dataset, comprising 370k high-quality\nexamples distilled from 1.7 million initial reasoning paths generated by\nvarious LLMs. ReasonMed is constructed through a multi-agent\nverification and refinement process, where we design an Error Refiner\nto enhance the reasoning paths by identifying and correcting error-prone steps\nflagged by a verifier. Leveraging ReasonMed, we systematically investigate best\npractices for training medical reasoning models and find that combining\ndetailed Chain-of-Thought (CoT) reasoning with concise answer summaries yields\nthe most effective fine-tuning strategy. Based on this strategy, we train\nReasonMed-7B, which sets a new benchmark for sub-10B models, outperforming the\nprior best by 4.17\\% and even exceeding LLaMA3.1-70B on PubMedQA by 4.60\\%.",
      "upvotes": 31,
      "discussionId": "684b8dbe3b733ba333687025",
      "githubRepo": "https://github.com/YuSun-Work/ReasonMed",
      "ai_summary": "ReasonMed, a large medical reasoning dataset, enhances the accuracy of medical question answering models by combining detailed reasoning paths with concise summaries, setting new benchmarks for model performance.",
      "ai_keywords": [
        "reasoning-based large language models",
        "LLMs",
        "medical question answering",
        "ReasonMed",
        "multi-agent verification",
        "Error Refiner",
        "Chain-of-Thought",
        "CoT reasoning",
        "ReasonMed-7B",
        "PubMedQA"
      ]
    },
    "publishedAt": "2025-06-11T04:36:55.000Z",
    "title": "ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical\n  Reasoning",
    "summary": "Though reasoning-based large language models (LLMs) have excelled in\nmathematics and programming, their capabilities in knowledge-intensive medical\nquestion answering remain underexplored. To address this, we introduce\nReasonMed, the largest medical reasoning dataset, comprising 370k high-quality\nexamples distilled from 1.7 million initial reasoning paths generated by\nvarious LLMs. ReasonMed is constructed through a multi-agent\nverification and refinement process, where we design an Error Refiner\nto enhance the reasoning paths by identifying and correcting error-prone steps\nflagged by a verifier. Leveraging ReasonMed, we systematically investigate best\npractices for training medical reasoning models and find that combining\ndetailed Chain-of-Thought (CoT) reasoning with concise answer summaries yields\nthe most effective fine-tuning strategy. Based on this strategy, we train\nReasonMed-7B, which sets a new benchmark for sub-10B models, outperforming the\nprior best by 4.17\\% and even exceeding LLaMA3.1-70B on PubMedQA by 4.60\\%.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09513.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6723079ad1306fe9c76a1d29",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/b4BNPCeZs59MKxo1qmT6r.png",
      "fullname": "Yu Sun",
      "name": "YuSun-AI",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.09993",
      "authors": [
        {
          "_id": "684ae204dbd21a9cc27b0fba",
          "user": {
            "_id": "66012e9c9e1cf5eb41ee0c4c",
            "avatarUrl": "/avatars/b07240cb86315b9e33d14677e02e4024.svg",
            "isPro": false,
            "fullname": "Jaewon Min",
            "user": "Min-Jaewon",
            "type": "user"
          },
          "name": "Jaewon Min",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:41:25.024Z",
          "hidden": false
        },
        {
          "_id": "684ae204dbd21a9cc27b0fbb",
          "user": {
            "_id": "65ec3449a69aaabb431db0da",
            "avatarUrl": "/avatars/d7b507be0175a61a8fc21176eea45001.svg",
            "isPro": false,
            "fullname": "Jin Hyeon Kim",
            "user": "jinlovespho",
            "type": "user"
          },
          "name": "Jin Hyeon Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:41:22.776Z",
          "hidden": false
        },
        {
          "_id": "684ae204dbd21a9cc27b0fbc",
          "user": {
            "_id": "6752b6315281c3cae4b0783f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/xmcyVEl2xBhk3G5_7dmpz.png",
            "isPro": false,
            "fullname": "Paul Hyunbin Cho",
            "user": "paulcho98",
            "type": "user"
          },
          "name": "Paul Hyunbin Cho",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:41:20.327Z",
          "hidden": false
        },
        {
          "_id": "684ae204dbd21a9cc27b0fbd",
          "name": "Jaeeun Lee",
          "hidden": false
        },
        {
          "_id": "684ae204dbd21a9cc27b0fbe",
          "name": "Jihye Park",
          "hidden": false
        },
        {
          "_id": "684ae204dbd21a9cc27b0fbf",
          "name": "Minkyu Park",
          "hidden": false
        },
        {
          "_id": "684ae204dbd21a9cc27b0fc0",
          "name": "Sangpil Kim",
          "hidden": false
        },
        {
          "_id": "684ae204dbd21a9cc27b0fc1",
          "name": "Hyunhee Park",
          "hidden": false
        },
        {
          "_id": "684ae204dbd21a9cc27b0fc2",
          "name": "Seungryong Kim",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-11T17:59:46.000Z",
      "submittedOnDailyAt": "2025-06-13T00:32:01.285Z",
      "title": "Text-Aware Image Restoration with Diffusion Models",
      "submittedOnDailyBy": {
        "_id": "66012e9c9e1cf5eb41ee0c4c",
        "avatarUrl": "/avatars/b07240cb86315b9e33d14677e02e4024.svg",
        "isPro": false,
        "fullname": "Jaewon Min",
        "user": "Min-Jaewon",
        "type": "user"
      },
      "summary": "Image restoration aims to recover degraded images. However, existing\ndiffusion-based restoration methods, despite great success in natural image\nrestoration, often struggle to faithfully reconstruct textual regions in\ndegraded images. Those methods frequently generate plausible but incorrect\ntext-like patterns, a phenomenon we refer to as text-image hallucination. In\nthis paper, we introduce Text-Aware Image Restoration (TAIR), a novel\nrestoration task that requires the simultaneous recovery of visual contents and\ntextual fidelity. To tackle this task, we present SA-Text, a large-scale\nbenchmark of 100K high-quality scene images densely annotated with diverse and\ncomplex text instances. Furthermore, we propose a multi-task diffusion\nframework, called TeReDiff, that integrates internal features from diffusion\nmodels into a text-spotting module, enabling both components to benefit from\njoint training. This allows for the extraction of rich text representations,\nwhich are utilized as prompts in subsequent denoising steps. Extensive\nexperiments demonstrate that our approach consistently outperforms\nstate-of-the-art restoration methods, achieving significant gains in text\nrecognition accuracy. See our project page: https://cvlab-kaist.github.io/TAIR/",
      "upvotes": 27,
      "discussionId": "684ae204dbd21a9cc27b0fc5",
      "projectPage": "https://cvlab-kaist.github.io/TAIR/",
      "githubRepo": "https://github.com/cvlab-kaist/TAIR",
      "ai_summary": "The proposed Text-Aware Image Restoration (TAIR) system integrates a multi-task diffusion framework with a text-spotting module to enhance both image recovery and textual fidelity, outperforming existing diffusion-based methods.",
      "ai_keywords": [
        "diffusion-based restoration",
        "text-image hallucination",
        "Text-Aware Image Restoration (TAIR)",
        "SA-Text",
        "multi-task diffusion framework",
        "TeReDiff",
        "text-spotting module",
        "text recognition accuracy"
      ]
    },
    "publishedAt": "2025-06-11T13:59:46.000Z",
    "title": "Text-Aware Image Restoration with Diffusion Models",
    "summary": "Image restoration aims to recover degraded images. However, existing\ndiffusion-based restoration methods, despite great success in natural image\nrestoration, often struggle to faithfully reconstruct textual regions in\ndegraded images. Those methods frequently generate plausible but incorrect\ntext-like patterns, a phenomenon we refer to as text-image hallucination. In\nthis paper, we introduce Text-Aware Image Restoration (TAIR), a novel\nrestoration task that requires the simultaneous recovery of visual contents and\ntextual fidelity. To tackle this task, we present SA-Text, a large-scale\nbenchmark of 100K high-quality scene images densely annotated with diverse and\ncomplex text instances. Furthermore, we propose a multi-task diffusion\nframework, called TeReDiff, that integrates internal features from diffusion\nmodels into a text-spotting module, enabling both components to benefit from\njoint training. This allows for the extraction of rich text representations,\nwhich are utilized as prompts in subsequent denoising steps. Extensive\nexperiments demonstrate that our approach consistently outperforms\nstate-of-the-art restoration methods, achieving significant gains in text\nrecognition accuracy. See our project page: https://cvlab-kaist.github.io/TAIR/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09993.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66012e9c9e1cf5eb41ee0c4c",
      "avatarUrl": "/avatars/b07240cb86315b9e33d14677e02e4024.svg",
      "fullname": "Jaewon Min",
      "name": "Min-Jaewon",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.10954",
      "authors": [
        {
          "_id": "684b7ea83b733ba333686f8a",
          "name": "Lianghong Guo",
          "hidden": false
        },
        {
          "_id": "684b7ea83b733ba333686f8b",
          "name": "Yanlin Wang",
          "hidden": false
        },
        {
          "_id": "684b7ea83b733ba333686f8c",
          "name": "Caihua Li",
          "hidden": false
        },
        {
          "_id": "684b7ea83b733ba333686f8d",
          "name": "Pengyu Yang",
          "hidden": false
        },
        {
          "_id": "684b7ea83b733ba333686f8e",
          "name": "Jiachi Chen",
          "hidden": false
        },
        {
          "_id": "684b7ea83b733ba333686f8f",
          "user": {
            "_id": "6355473d525beaee688b7ba1",
            "avatarUrl": "/avatars/0a0f0acc65829c6d864440444c580698.svg",
            "isPro": false,
            "fullname": "Wei Tao",
            "user": "itaowe",
            "type": "user"
          },
          "name": "Wei Tao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:58.091Z",
          "hidden": false
        },
        {
          "_id": "684b7ea83b733ba333686f90",
          "name": "Yingtian Zou",
          "hidden": false
        },
        {
          "_id": "684b7ea83b733ba333686f91",
          "name": "Duyu Tang",
          "hidden": false
        },
        {
          "_id": "684b7ea83b733ba333686f92",
          "name": "Zibin Zheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T17:54:17.000Z",
      "submittedOnDailyAt": "2025-06-13T00:07:20.052Z",
      "title": "SWE-Factory: Your Automated Factory for Issue Resolution Training Data\n  and Evaluation Benchmarks",
      "submittedOnDailyBy": {
        "_id": "6355473d525beaee688b7ba1",
        "avatarUrl": "/avatars/0a0f0acc65829c6d864440444c580698.svg",
        "isPro": false,
        "fullname": "Wei Tao",
        "user": "itaowe",
        "type": "user"
      },
      "summary": "Constructing large-scale datasets for the GitHub issue resolution task is\ncrucial for both training and evaluating the software engineering capabilities\nof Large Language Models (LLMs). However, the traditional process for creating\nsuch benchmarks is notoriously challenging and labor-intensive, particularly in\nthe stages of setting up evaluation environments, grading test outcomes, and\nvalidating task instances. In this paper, we propose SWE-Factory, an automated\npipeline designed to address these challenges. To tackle these issues, our\npipeline integrates three core automated components. First, we introduce\nSWE-Builder, a multi-agent system that automates evaluation environment\nconstruction, which employs four specialized agents that work in a\ncollaborative, iterative loop and leverages an environment memory pool to\nenhance efficiency. Second, we introduce a standardized, exit-code-based\ngrading method that eliminates the need for manually writing custom parsers.\nFinally, we automate the fail2pass validation process using these reliable exit\ncode signals. Experiments on 671 issues across four programming languages show\nthat our pipeline can effectively construct valid task instances; for example,\nwith GPT-4.1-mini, our SWE-Builder constructs 269 valid instances at 0.045 per\ninstance, while with Gemini-2.5-flash, it achieves comparable performance at\nthe lowest cost of 0.024 per instance. We also demonstrate that our\nexit-code-based grading achieves 100% accuracy compared to manual inspection,\nand our automated fail2pass validation reaches a precision of 0.92 and a recall\nof 1.00. We hope our automated pipeline will accelerate the collection of\nlarge-scale, high-quality GitHub issue resolution datasets for both training\nand evaluation. Our code and datasets are released at\nhttps://github.com/DeepSoftwareAnalytics/swe-factory.",
      "upvotes": 25,
      "discussionId": "684b7ea83b733ba333686f93",
      "githubRepo": "https://github.com/DeepSoftwareAnalytics/swe-factory",
      "ai_summary": "A pipeline named SWE-Factory automates the creation and validation of GitHub issue resolution datasets for training and evaluating Large Language Models, using SWE-Builder for environment setup, exit-code-based grading, and automated fail2pass validation.",
      "ai_keywords": [
        "Large Language Models",
        "LLMs",
        "SWE-Factory",
        "SWE-Builder",
        "multi-agent system",
        "environment memory pool",
        "exit-code-based grading",
        "automated fail2pass validation",
        "GPT-4.1-mini",
        "Gemini-2.5-flash",
        "precision",
        "recall"
      ]
    },
    "publishedAt": "2025-06-12T13:54:17.000Z",
    "title": "SWE-Factory: Your Automated Factory for Issue Resolution Training Data\n  and Evaluation Benchmarks",
    "summary": "Constructing large-scale datasets for the GitHub issue resolution task is\ncrucial for both training and evaluating the software engineering capabilities\nof Large Language Models (LLMs). However, the traditional process for creating\nsuch benchmarks is notoriously challenging and labor-intensive, particularly in\nthe stages of setting up evaluation environments, grading test outcomes, and\nvalidating task instances. In this paper, we propose SWE-Factory, an automated\npipeline designed to address these challenges. To tackle these issues, our\npipeline integrates three core automated components. First, we introduce\nSWE-Builder, a multi-agent system that automates evaluation environment\nconstruction, which employs four specialized agents that work in a\ncollaborative, iterative loop and leverages an environment memory pool to\nenhance efficiency. Second, we introduce a standardized, exit-code-based\ngrading method that eliminates the need for manually writing custom parsers.\nFinally, we automate the fail2pass validation process using these reliable exit\ncode signals. Experiments on 671 issues across four programming languages show\nthat our pipeline can effectively construct valid task instances; for example,\nwith GPT-4.1-mini, our SWE-Builder constructs 269 valid instances at 0.045 per\ninstance, while with Gemini-2.5-flash, it achieves comparable performance at\nthe lowest cost of 0.024 per instance. We also demonstrate that our\nexit-code-based grading achieves 100% accuracy compared to manual inspection,\nand our automated fail2pass validation reaches a precision of 0.92 and a recall\nof 1.00. We hope our automated pipeline will accelerate the collection of\nlarge-scale, high-quality GitHub issue resolution datasets for both training\nand evaluation. Our code and datasets are released at\nhttps://github.com/DeepSoftwareAnalytics/swe-factory.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10954.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6355473d525beaee688b7ba1",
      "avatarUrl": "/avatars/0a0f0acc65829c6d864440444c580698.svg",
      "fullname": "Wei Tao",
      "name": "itaowe",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.10857",
      "authors": [
        {
          "_id": "684b817e3b733ba333686f95",
          "user": {
            "_id": "64b89a14cf14c2fabe96664c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/tEd3fBjMcEubF4plqzcUz.jpeg",
            "isPro": false,
            "fullname": "Jiashuo Yu",
            "user": "awojustin",
            "type": "user"
          },
          "name": "Jiashuo Yu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:55.618Z",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686f96",
          "name": "Yue Wu",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686f97",
          "name": "Meng Chu",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686f98",
          "name": "Zhifei Ren",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686f99",
          "name": "Zizheng Huang",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686f9a",
          "user": {
            "_id": "64c9beb2904317f42de06dd8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c9beb2904317f42de06dd8/he3rxfyzfwEd1vLuK6_o2.jpeg",
            "isPro": false,
            "fullname": "Pei Chu",
            "user": "chupei",
            "type": "user"
          },
          "name": "Pei Chu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:51.884Z",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686f9b",
          "name": "Ruijie Zhang",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686f9c",
          "name": "Yinan He",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686f9d",
          "name": "Qirui Li",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686f9e",
          "user": {
            "_id": "64acbbd51aee69ece03c6c0c",
            "avatarUrl": "/avatars/604df1cabc5faeda55022ae4c1997e56.svg",
            "isPro": false,
            "fullname": "Songze Li",
            "user": "LarryLee",
            "type": "user"
          },
          "name": "Songze Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:53.776Z",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686f9f",
          "name": "Zhenxiang Li",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686fa0",
          "name": "Zhongying Tu",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686fa1",
          "name": "Conghui He",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686fa2",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686fa3",
          "name": "Yali Wang",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686fa4",
          "name": "Yi Wang",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686fa5",
          "name": "Limin Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T16:17:17.000Z",
      "submittedOnDailyAt": "2025-06-13T00:10:47.082Z",
      "title": "VRBench: A Benchmark for Multi-Step Reasoning in Long Narrative Videos",
      "submittedOnDailyBy": {
        "_id": "64b89a14cf14c2fabe96664c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/tEd3fBjMcEubF4plqzcUz.jpeg",
        "isPro": false,
        "fullname": "Jiashuo Yu",
        "user": "awojustin",
        "type": "user"
      },
      "summary": "We present VRBench, the first long narrative video benchmark crafted for\nevaluating large models' multi-step reasoning capabilities, addressing\nlimitations in existing evaluations that overlook temporal reasoning and\nprocedural validity. It comprises 1,010 long videos (with an average duration\nof 1.6 hours), along with 9,468 human-labeled multi-step question-answering\npairs and 30,292 reasoning steps with timestamps. These videos are curated via\na multi-stage filtering process including expert inter-rater reviewing to\nprioritize plot coherence. We develop a human-AI collaborative framework that\ngenerates coherent reasoning chains, each requiring multiple temporally\ngrounded steps, spanning seven types (e.g., event attribution, implicit\ninference). VRBench designs a multi-phase evaluation pipeline that assesses\nmodels at both the outcome and process levels. Apart from the MCQs for the\nfinal results, we propose a progress-level LLM-guided scoring metric to\nevaluate the quality of the reasoning chain from multiple dimensions\ncomprehensively. Through extensive evaluations of 12 LLMs and 16 VLMs on\nVRBench, we undertake a thorough analysis and provide valuable insights that\nadvance the field of multi-step reasoning.",
      "upvotes": 21,
      "discussionId": "684b817e3b733ba333686fa6",
      "projectPage": "https://vrbench.github.io/",
      "githubRepo": "https://github.com/OpenGVLab/VRBench",
      "ai_summary": "VRBench is a long narrative video benchmark designed to evaluate models' multi-step reasoning and procedural validity through human-labeled question-answering pairs and a human-AI collaborative framework with a multi-phase evaluation pipeline.",
      "ai_keywords": [
        "VRBench",
        "multi-step reasoning",
        "temporal reasoning",
        "procedural validity",
        "long videos",
        "human-labeled",
        "multi-step question-answering",
        "expert inter-rater reviewing",
        "coherent reasoning chains",
        "event attribution",
        "implicit inference",
        "multi-phase evaluation",
        "progress-level LLM-guided scoring metric",
        "LLMs",
        "VLMs"
      ]
    },
    "publishedAt": "2025-06-12T12:17:17.000Z",
    "title": "VRBench: A Benchmark for Multi-Step Reasoning in Long Narrative Videos",
    "summary": "We present VRBench, the first long narrative video benchmark crafted for\nevaluating large models' multi-step reasoning capabilities, addressing\nlimitations in existing evaluations that overlook temporal reasoning and\nprocedural validity. It comprises 1,010 long videos (with an average duration\nof 1.6 hours), along with 9,468 human-labeled multi-step question-answering\npairs and 30,292 reasoning steps with timestamps. These videos are curated via\na multi-stage filtering process including expert inter-rater reviewing to\nprioritize plot coherence. We develop a human-AI collaborative framework that\ngenerates coherent reasoning chains, each requiring multiple temporally\ngrounded steps, spanning seven types (e.g., event attribution, implicit\ninference). VRBench designs a multi-phase evaluation pipeline that assesses\nmodels at both the outcome and process levels. Apart from the MCQs for the\nfinal results, we propose a progress-level LLM-guided scoring metric to\nevaluate the quality of the reasoning chain from multiple dimensions\ncomprehensively. Through extensive evaluations of 12 LLMs and 16 VLMs on\nVRBench, we undertake a thorough analysis and provide valuable insights that\nadvance the field of multi-step reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10857.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b89a14cf14c2fabe96664c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/tEd3fBjMcEubF4plqzcUz.jpeg",
      "fullname": "Jiashuo Yu",
      "name": "awojustin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.10540",
      "authors": [
        {
          "_id": "684bad683b733ba3336870b6",
          "user": {
            "_id": "652fb8bcc9dd2692a25ef2e3",
            "avatarUrl": "/avatars/461e6cc1c3441cde18192b080b0b8576.svg",
            "isPro": false,
            "fullname": "Haoyuan Shi",
            "user": "MrSunshy",
            "type": "user"
          },
          "name": "Haoyuan Shi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:38:52.713Z",
          "hidden": false
        },
        {
          "_id": "684bad683b733ba3336870b7",
          "user": {
            "_id": "62fdb01bc1588e1d4c6c1a7c",
            "avatarUrl": "/avatars/bd03085995b1c34e0ac8a845cf2c4e83.svg",
            "isPro": false,
            "fullname": "Yunxin Li",
            "user": "YunxinLi",
            "type": "user"
          },
          "name": "Yunxin Li",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-13T04:47:39.539Z",
          "hidden": false
        },
        {
          "_id": "684bad683b733ba3336870b8",
          "name": "Xinyu Chen",
          "hidden": false
        },
        {
          "_id": "684bad683b733ba3336870b9",
          "name": "Longyue Wang",
          "hidden": false
        },
        {
          "_id": "684bad683b733ba3336870ba",
          "name": "Baotian Hu",
          "hidden": false
        },
        {
          "_id": "684bad683b733ba3336870bb",
          "name": "Min Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T10:06:21.000Z",
      "submittedOnDailyAt": "2025-06-13T03:26:17.710Z",
      "title": "AniMaker: Automated Multi-Agent Animated Storytelling with MCTS-Driven\n  Clip Generation",
      "submittedOnDailyBy": {
        "_id": "62fdb01bc1588e1d4c6c1a7c",
        "avatarUrl": "/avatars/bd03085995b1c34e0ac8a845cf2c4e83.svg",
        "isPro": false,
        "fullname": "Yunxin Li",
        "user": "YunxinLi",
        "type": "user"
      },
      "summary": "Despite rapid advancements in video generation models, generating coherent\nstorytelling videos that span multiple scenes and characters remains\nchallenging. Current methods often rigidly convert pre-generated keyframes into\nfixed-length clips, resulting in disjointed narratives and pacing issues.\nFurthermore, the inherent instability of video generation models means that\neven a single low-quality clip can significantly degrade the entire output\nanimation's logical coherence and visual continuity. To overcome these\nobstacles, we introduce AniMaker, a multi-agent framework enabling efficient\nmulti-candidate clip generation and storytelling-aware clip selection, thus\ncreating globally consistent and story-coherent animation solely from text\ninput. The framework is structured around specialized agents, including the\nDirector Agent for storyboard generation, the Photography Agent for video clip\ngeneration, the Reviewer Agent for evaluation, and the Post-Production Agent\nfor editing and voiceover. Central to AniMaker's approach are two key technical\ncomponents: MCTS-Gen in Photography Agent, an efficient Monte Carlo Tree Search\n(MCTS)-inspired strategy that intelligently navigates the candidate space to\ngenerate high-potential clips while optimizing resource usage; and AniEval in\nReviewer Agent, the first framework specifically designed for multi-shot\nanimation evaluation, which assesses critical aspects such as story-level\nconsistency, action completion, and animation-specific features by considering\neach clip in the context of its preceding and succeeding clips. Experiments\ndemonstrate that AniMaker achieves superior quality as measured by popular\nmetrics including VBench and our proposed AniEval framework, while\nsignificantly improving the efficiency of multi-candidate generation, pushing\nAI-generated storytelling animation closer to production standards.",
      "upvotes": 20,
      "discussionId": "684bad683b733ba3336870bc",
      "ai_summary": "AniMaker, a multi-agent framework using MCTS-Gen and AniEval, generates coherent storytelling videos from text input, outperforming existing models with better quality and efficiency.",
      "ai_keywords": [
        "multi-agent framework",
        "Director Agent",
        "Photography Agent",
        "Reviewer Agent",
        "Post-Production Agent",
        "Monte Carlo Tree Search (MCTS)",
        "AniEval",
        "VBench",
        "action completion",
        "story-level consistency",
        "animation-specific features"
      ]
    },
    "publishedAt": "2025-06-12T06:06:21.000Z",
    "title": "AniMaker: Automated Multi-Agent Animated Storytelling with MCTS-Driven\n  Clip Generation",
    "summary": "Despite rapid advancements in video generation models, generating coherent\nstorytelling videos that span multiple scenes and characters remains\nchallenging. Current methods often rigidly convert pre-generated keyframes into\nfixed-length clips, resulting in disjointed narratives and pacing issues.\nFurthermore, the inherent instability of video generation models means that\neven a single low-quality clip can significantly degrade the entire output\nanimation's logical coherence and visual continuity. To overcome these\nobstacles, we introduce AniMaker, a multi-agent framework enabling efficient\nmulti-candidate clip generation and storytelling-aware clip selection, thus\ncreating globally consistent and story-coherent animation solely from text\ninput. The framework is structured around specialized agents, including the\nDirector Agent for storyboard generation, the Photography Agent for video clip\ngeneration, the Reviewer Agent for evaluation, and the Post-Production Agent\nfor editing and voiceover. Central to AniMaker's approach are two key technical\ncomponents: MCTS-Gen in Photography Agent, an efficient Monte Carlo Tree Search\n(MCTS)-inspired strategy that intelligently navigates the candidate space to\ngenerate high-potential clips while optimizing resource usage; and AniEval in\nReviewer Agent, the first framework specifically designed for multi-shot\nanimation evaluation, which assesses critical aspects such as story-level\nconsistency, action completion, and animation-specific features by considering\neach clip in the context of its preceding and succeeding clips. Experiments\ndemonstrate that AniMaker achieves superior quality as measured by popular\nmetrics including VBench and our proposed AniEval framework, while\nsignificantly improving the efficiency of multi-candidate generation, pushing\nAI-generated storytelling animation closer to production standards.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10540.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "62fdb01bc1588e1d4c6c1a7c",
      "avatarUrl": "/avatars/bd03085995b1c34e0ac8a845cf2c4e83.svg",
      "fullname": "Yunxin Li",
      "name": "YunxinLi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.10974",
      "authors": [
        {
          "_id": "684b8e193b733ba333687028",
          "user": {
            "_id": "6241749cf80bd930bd99f3dd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669210243382-6241749cf80bd930bd99f3dd.jpeg",
            "isPro": false,
            "fullname": "Ou Yixin",
            "user": "OE-Heart",
            "type": "user"
          },
          "name": "Yixin Ou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:36.148Z",
          "hidden": false
        },
        {
          "_id": "684b8e193b733ba333687029",
          "name": "Yujie Luo",
          "hidden": false
        },
        {
          "_id": "684b8e193b733ba33368702a",
          "name": "Jingsheng Zheng",
          "hidden": false
        },
        {
          "_id": "684b8e193b733ba33368702b",
          "name": "Lanning Wei",
          "hidden": false
        },
        {
          "_id": "684b8e193b733ba33368702c",
          "name": "Shuofei Qiao",
          "hidden": false
        },
        {
          "_id": "684b8e193b733ba33368702d",
          "name": "Jintian Zhang",
          "hidden": false
        },
        {
          "_id": "684b8e193b733ba33368702e",
          "name": "Da Zheng",
          "hidden": false
        },
        {
          "_id": "684b8e193b733ba33368702f",
          "name": "Huajun Chen",
          "hidden": false
        },
        {
          "_id": "684b8e193b733ba333687030",
          "user": {
            "_id": "620b3bbb0668e435407c8d0a",
            "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
            "isPro": false,
            "fullname": "Ningyu Zhang",
            "user": "Ningyu",
            "type": "user"
          },
          "name": "Ningyu Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:37.984Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T17:59:32.000Z",
      "submittedOnDailyAt": "2025-06-13T03:44:21.173Z",
      "title": "AutoMind: Adaptive Knowledgeable Agent for Automated Data Science",
      "submittedOnDailyBy": {
        "_id": "620b3bbb0668e435407c8d0a",
        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
        "isPro": false,
        "fullname": "Ningyu Zhang",
        "user": "Ningyu",
        "type": "user"
      },
      "summary": "Large Language Model (LLM) agents have shown great potential in addressing\nreal-world data science problems. LLM-driven data science agents promise to\nautomate the entire machine learning pipeline, yet their real-world\neffectiveness remains limited. Existing frameworks depend on rigid, pre-defined\nworkflows and inflexible coding strategies; consequently, they excel only on\nrelatively simple, classical problems and fail to capture the empirical\nexpertise that human practitioners bring to complex, innovative tasks. In this\nwork, we introduce AutoMind, an adaptive, knowledgeable LLM-agent framework\nthat overcomes these deficiencies through three key advances: (1) a curated\nexpert knowledge base that grounds the agent in domain expert knowledge, (2) an\nagentic knowledgeable tree search algorithm that strategically explores\npossible solutions, and (3) a self-adaptive coding strategy that dynamically\ntailors code generation to task complexity. Evaluations on two automated data\nscience benchmarks demonstrate that AutoMind delivers superior performance\nversus state-of-the-art baselines. Additional analyses confirm favorable\neffectiveness, efficiency, and qualitative solution quality, highlighting\nAutoMind as an efficient and robust step toward fully automated data science.",
      "upvotes": 9,
      "discussionId": "684b8e193b733ba333687031",
      "ai_summary": "AutoMind, a flexible and knowledgeable LLM-agent framework, improves automated data science through expert knowledge integration, strategic solution exploration, and adaptive coding, outperforming existing systems.",
      "ai_keywords": [
        "LLM",
        "data science agents",
        "machine learning pipeline",
        "expert knowledge base",
        "agentic knowledgeable tree search",
        "self-adaptive coding strategy"
      ]
    },
    "publishedAt": "2025-06-12T13:59:32.000Z",
    "title": "AutoMind: Adaptive Knowledgeable Agent for Automated Data Science",
    "summary": "Large Language Model (LLM) agents have shown great potential in addressing\nreal-world data science problems. LLM-driven data science agents promise to\nautomate the entire machine learning pipeline, yet their real-world\neffectiveness remains limited. Existing frameworks depend on rigid, pre-defined\nworkflows and inflexible coding strategies; consequently, they excel only on\nrelatively simple, classical problems and fail to capture the empirical\nexpertise that human practitioners bring to complex, innovative tasks. In this\nwork, we introduce AutoMind, an adaptive, knowledgeable LLM-agent framework\nthat overcomes these deficiencies through three key advances: (1) a curated\nexpert knowledge base that grounds the agent in domain expert knowledge, (2) an\nagentic knowledgeable tree search algorithm that strategically explores\npossible solutions, and (3) a self-adaptive coding strategy that dynamically\ntailors code generation to task complexity. Evaluations on two automated data\nscience benchmarks demonstrate that AutoMind delivers superior performance\nversus state-of-the-art baselines. Additional analyses confirm favorable\neffectiveness, efficiency, and qualitative solution quality, highlighting\nAutoMind as an efficient and robust step toward fully automated data science.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10974.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620b3bbb0668e435407c8d0a",
      "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
      "fullname": "Ningyu Zhang",
      "name": "Ningyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 23
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.10960",
      "authors": [
        {
          "_id": "684bb33a3b733ba3336870c5",
          "name": "Kangwei Liu",
          "hidden": false
        },
        {
          "_id": "684bb33a3b733ba3336870c6",
          "name": "Siyuan Cheng",
          "hidden": false
        },
        {
          "_id": "684bb33a3b733ba3336870c7",
          "name": "Bozhong Tian",
          "hidden": false
        },
        {
          "_id": "684bb33a3b733ba3336870c8",
          "name": "Xiaozhuan Liang",
          "hidden": false
        },
        {
          "_id": "684bb33a3b733ba3336870c9",
          "name": "Yuyang Yin",
          "hidden": false
        },
        {
          "_id": "684bb33a3b733ba3336870ca",
          "name": "Meng Han",
          "hidden": false
        },
        {
          "_id": "684bb33a3b733ba3336870cb",
          "user": {
            "_id": "620b3bbb0668e435407c8d0a",
            "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
            "isPro": false,
            "fullname": "Ningyu Zhang",
            "user": "Ningyu",
            "type": "user"
          },
          "name": "Ningyu Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:38:50.549Z",
          "hidden": false
        },
        {
          "_id": "684bb33a3b733ba3336870cc",
          "name": "Bryan Hooi",
          "hidden": false
        },
        {
          "_id": "684bb33a3b733ba3336870cd",
          "user": {
            "_id": "635113fdcba4ff2e81cb236e",
            "avatarUrl": "/avatars/f80df906b722b4901debce9baa867073.svg",
            "isPro": false,
            "fullname": "chen",
            "user": "Jasonchen123",
            "type": "user"
          },
          "name": "Xi Chen",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-13T05:12:27.526Z",
          "hidden": false
        },
        {
          "_id": "684bb33a3b733ba3336870ce",
          "name": "Shumin Deng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T17:57:05.000Z",
      "submittedOnDailyAt": "2025-06-13T03:43:12.055Z",
      "title": "ChineseHarm-Bench: A Chinese Harmful Content Detection Benchmark",
      "submittedOnDailyBy": {
        "_id": "620b3bbb0668e435407c8d0a",
        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
        "isPro": false,
        "fullname": "Ningyu Zhang",
        "user": "Ningyu",
        "type": "user"
      },
      "summary": "Large language models (LLMs) have been increasingly applied to automated\nharmful content detection tasks, assisting moderators in identifying policy\nviolations and improving the overall efficiency and accuracy of content review.\nHowever, existing resources for harmful content detection are predominantly\nfocused on English, with Chinese datasets remaining scarce and often limited in\nscope. We present a comprehensive, professionally annotated benchmark for\nChinese content harm detection, which covers six representative categories and\nis constructed entirely from real-world data. Our annotation process further\nyields a knowledge rule base that provides explicit expert knowledge to assist\nLLMs in Chinese harmful content detection. In addition, we propose a\nknowledge-augmented baseline that integrates both human-annotated knowledge\nrules and implicit knowledge from large language models, enabling smaller\nmodels to achieve performance comparable to state-of-the-art LLMs. Code and\ndata are available at https://github.com/zjunlp/ChineseHarm-bench.",
      "upvotes": 9,
      "discussionId": "684bb33a3b733ba3336870cf",
      "ai_summary": "A benchmark for Chinese harmful content detection, coupled with a knowledge-augmented baseline, improves the performance of smaller models without extensive resources.",
      "ai_keywords": [
        "large language models",
        "harmful content detection",
        "benchmark",
        "annotation process",
        "knowledge rule base",
        "knowledge-augmented baseline"
      ]
    },
    "publishedAt": "2025-06-12T13:57:05.000Z",
    "title": "ChineseHarm-Bench: A Chinese Harmful Content Detection Benchmark",
    "summary": "Large language models (LLMs) have been increasingly applied to automated\nharmful content detection tasks, assisting moderators in identifying policy\nviolations and improving the overall efficiency and accuracy of content review.\nHowever, existing resources for harmful content detection are predominantly\nfocused on English, with Chinese datasets remaining scarce and often limited in\nscope. We present a comprehensive, professionally annotated benchmark for\nChinese content harm detection, which covers six representative categories and\nis constructed entirely from real-world data. Our annotation process further\nyields a knowledge rule base that provides explicit expert knowledge to assist\nLLMs in Chinese harmful content detection. In addition, we propose a\nknowledge-augmented baseline that integrates both human-annotated knowledge\nrules and implicit knowledge from large language models, enabling smaller\nmodels to achieve performance comparable to state-of-the-art LLMs. Code and\ndata are available at https://github.com/zjunlp/ChineseHarm-bench.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10960.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620b3bbb0668e435407c8d0a",
      "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
      "fullname": "Ningyu Zhang",
      "name": "Ningyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 23
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.10821",
      "authors": [
        {
          "_id": "684b91c73b733ba333687033",
          "name": "Huaying Yuan",
          "hidden": false
        },
        {
          "_id": "684b91c73b733ba333687034",
          "name": "Zheng Liu",
          "hidden": false
        },
        {
          "_id": "684b91c73b733ba333687035",
          "name": "Junjie Zhou",
          "hidden": false
        },
        {
          "_id": "684b91c73b733ba333687036",
          "name": "Ji-Rong Wen",
          "hidden": false
        },
        {
          "_id": "684b91c73b733ba333687037",
          "name": "Zhicheng Dou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T15:39:10.000Z",
      "submittedOnDailyAt": "2025-06-13T01:20:51.837Z",
      "title": "VideoDeepResearch: Long Video Understanding With Agentic Tool Using",
      "submittedOnDailyBy": {
        "_id": "66d916a7b86f0d569aa19b60",
        "avatarUrl": "/avatars/2537cee66afecc2d999e05b01c78d319.svg",
        "isPro": false,
        "fullname": "huaying Yuan",
        "user": "avery00",
        "type": "user"
      },
      "summary": "Long video understanding (LVU) presents a significant challenge for current\nmulti-modal large language models (MLLMs) due to the task's inherent complexity\nand context window constraint. It is widely assumed that addressing LVU tasks\nrequires foundation MLLMs with extended context windows, strong visual\nperception capabilities, and proficient domain expertise. In this work, we\nchallenge this common belief by introducing VideoDeepResearch, a novel agentic\nframework for long video understanding. Our approach relies solely on a\ntext-only large reasoning model (LRM) combined with a modular multi-modal\ntoolkit, including multimodal retrievers and visual perceivers, all of which\nare readily available in practice. For each LVU task, the system formulates a\nproblem-solving strategy through reasoning, while selectively accessing and\nutilizing essential video content via tool using. We conduct extensive\nexperiments on popular LVU benchmarks, including MLVU, Video-MME, and LVBench.\nOur results demonstrate that VideoDeepResearch achieves substantial\nimprovements over existing MLLM baselines, surpassing the previous\nstate-of-the-art by 9.6%, 6.6%, and 3.9% on MLVU (test), LVBench, and\nLongVideoBench, respectively. These findings highlight the promise of agentic\nsystems in overcoming key challenges in LVU problems.",
      "upvotes": 9,
      "discussionId": "684b91c73b733ba333687038",
      "ai_summary": "VideoDeepResearch, a text-only reasoning model with modular tools, surpasses existing baselines in long video understanding tasks without extending context windows or enhancing visual perception capabilities.",
      "ai_keywords": [
        "long video understanding",
        "multi-modal large language models",
        "VideoDeepResearch",
        "text-only large reasoning model",
        "multimodal retrievers",
        "visual perceivers",
        "MLVU",
        "Video-MME",
        "LVBench",
        "LongVideoBench",
        "agentic systems"
      ]
    },
    "publishedAt": "2025-06-12T11:39:10.000Z",
    "title": "VideoDeepResearch: Long Video Understanding With Agentic Tool Using",
    "summary": "Long video understanding (LVU) presents a significant challenge for current\nmulti-modal large language models (MLLMs) due to the task's inherent complexity\nand context window constraint. It is widely assumed that addressing LVU tasks\nrequires foundation MLLMs with extended context windows, strong visual\nperception capabilities, and proficient domain expertise. In this work, we\nchallenge this common belief by introducing VideoDeepResearch, a novel agentic\nframework for long video understanding. Our approach relies solely on a\ntext-only large reasoning model (LRM) combined with a modular multi-modal\ntoolkit, including multimodal retrievers and visual perceivers, all of which\nare readily available in practice. For each LVU task, the system formulates a\nproblem-solving strategy through reasoning, while selectively accessing and\nutilizing essential video content via tool using. We conduct extensive\nexperiments on popular LVU benchmarks, including MLVU, Video-MME, and LVBench.\nOur results demonstrate that VideoDeepResearch achieves substantial\nimprovements over existing MLLM baselines, surpassing the previous\nstate-of-the-art by 9.6%, 6.6%, and 3.9% on MLVU (test), LVBench, and\nLongVideoBench, respectively. These findings highlight the promise of agentic\nsystems in overcoming key challenges in LVU problems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10821.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66d916a7b86f0d569aa19b60",
      "avatarUrl": "/avatars/2537cee66afecc2d999e05b01c78d319.svg",
      "fullname": "huaying Yuan",
      "name": "avery00",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.10890",
      "authors": [
        {
          "_id": "684b8b533b733ba333686fe4",
          "user": {
            "_id": "62bc1adacaf01b9bec398547",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656494729797-noauth.png",
            "isPro": false,
            "fullname": "Zhao Zhang",
            "user": "zbrl",
            "type": "user"
          },
          "name": "Zhao Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:45.153Z",
          "hidden": false
        },
        {
          "_id": "684b8b533b733ba333686fe5",
          "name": "Yutao Cheng",
          "hidden": false
        },
        {
          "_id": "684b8b533b733ba333686fe6",
          "user": {
            "_id": "6669a0cc9f28880b31d7c4ef",
            "avatarUrl": "/avatars/bd66a6f68a9af2bf7ee40510579e57fe.svg",
            "isPro": false,
            "fullname": "dexiang hong",
            "user": "hxxxl",
            "type": "user"
          },
          "name": "Dexiang Hong",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-13T02:22:14.202Z",
          "hidden": false
        },
        {
          "_id": "684b8b533b733ba333686fe7",
          "user": {
            "_id": "63fd7279ed9eead590fd02ed",
            "avatarUrl": "/avatars/4cf6f005069412ee87ed07cd81500f1e.svg",
            "isPro": false,
            "fullname": "YangMaoke",
            "user": "YangMaoke",
            "type": "user"
          },
          "name": "Maoke Yang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-13T02:22:14.202Z",
          "hidden": false
        },
        {
          "_id": "684b8b533b733ba333686fe8",
          "user": {
            "_id": "6436619ead9b9147de287a24",
            "avatarUrl": "/avatars/180c43c79e552dd345636a47db80e3e9.svg",
            "isPro": false,
            "fullname": "ShiLayne",
            "user": "ShiLayne",
            "type": "user"
          },
          "name": "Gonglei Shi",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-06-13T02:23:52.345Z",
          "hidden": true
        },
        {
          "_id": "684b8b533b733ba333686fe9",
          "name": "Lei Ma",
          "hidden": false
        },
        {
          "_id": "684b8b533b733ba333686fea",
          "name": "Hui Zhang",
          "hidden": false
        },
        {
          "_id": "684b8b533b733ba333686feb",
          "name": "Jie Shao",
          "hidden": false
        },
        {
          "_id": "684b8b533b733ba333686fec",
          "name": "Xinglong Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T16:54:39.000Z",
      "submittedOnDailyAt": "2025-06-13T00:55:02.473Z",
      "title": "CreatiPoster: Towards Editable and Controllable Multi-Layer Graphic\n  Design Generation",
      "submittedOnDailyBy": {
        "_id": "62bc1adacaf01b9bec398547",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656494729797-noauth.png",
        "isPro": false,
        "fullname": "Zhao Zhang",
        "user": "zbrl",
        "type": "user"
      },
      "summary": "Graphic design plays a crucial role in both commercial and personal contexts,\nyet creating high-quality, editable, and aesthetically pleasing graphic\ncompositions remains a time-consuming and skill-intensive task, especially for\nbeginners. Current AI tools automate parts of the workflow, but struggle to\naccurately incorporate user-supplied assets, maintain editability, and achieve\nprofessional visual appeal. Commercial systems, like Canva Magic Design, rely\non vast template libraries, which are impractical for replicate. In this paper,\nwe introduce CreatiPoster, a framework that generates editable, multi-layer\ncompositions from optional natural-language instructions or assets. A protocol\nmodel, an RGBA large multimodal model, first produces a JSON specification\ndetailing every layer (text or asset) with precise layout, hierarchy, content\nand style, plus a concise background prompt. A conditional background model\nthen synthesizes a coherent background conditioned on this rendered foreground\nlayers. We construct a benchmark with automated metrics for graphic-design\ngeneration and show that CreatiPoster surpasses leading open-source approaches\nand proprietary commercial systems. To catalyze further research, we release a\ncopyright-free corpus of 100,000 multi-layer designs. CreatiPoster supports\ndiverse applications such as canvas editing, text overlay, responsive resizing,\nmultilingual adaptation, and animated posters, advancing the democratization of\nAI-assisted graphic design. Project homepage:\nhttps://github.com/graphic-design-ai/creatiposter",
      "upvotes": 7,
      "discussionId": "684b8b533b733ba333686fed",
      "githubRepo": "https://github.com/graphic-design-ai/creatiposter",
      "ai_summary": "CreatiPoster generates high-quality, editable, and customizable graphic compositions from text or assets, outperforming existing tools and templates.",
      "ai_keywords": [
        "RGBA large multimodal model",
        "JSON specification",
        "conditional background model",
        "automated metrics",
        "graphic-design generation",
        "multi-layer designs",
        "AI-assisted graphic design"
      ]
    },
    "publishedAt": "2025-06-12T12:54:39.000Z",
    "title": "CreatiPoster: Towards Editable and Controllable Multi-Layer Graphic\n  Design Generation",
    "summary": "Graphic design plays a crucial role in both commercial and personal contexts,\nyet creating high-quality, editable, and aesthetically pleasing graphic\ncompositions remains a time-consuming and skill-intensive task, especially for\nbeginners. Current AI tools automate parts of the workflow, but struggle to\naccurately incorporate user-supplied assets, maintain editability, and achieve\nprofessional visual appeal. Commercial systems, like Canva Magic Design, rely\non vast template libraries, which are impractical for replicate. In this paper,\nwe introduce CreatiPoster, a framework that generates editable, multi-layer\ncompositions from optional natural-language instructions or assets. A protocol\nmodel, an RGBA large multimodal model, first produces a JSON specification\ndetailing every layer (text or asset) with precise layout, hierarchy, content\nand style, plus a concise background prompt. A conditional background model\nthen synthesizes a coherent background conditioned on this rendered foreground\nlayers. We construct a benchmark with automated metrics for graphic-design\ngeneration and show that CreatiPoster surpasses leading open-source approaches\nand proprietary commercial systems. To catalyze further research, we release a\ncopyright-free corpus of 100,000 multi-layer designs. CreatiPoster supports\ndiverse applications such as canvas editing, text overlay, responsive resizing,\nmultilingual adaptation, and animated posters, advancing the democratization of\nAI-assisted graphic design. Project homepage:\nhttps://github.com/graphic-design-ai/creatiposter",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10890.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62bc1adacaf01b9bec398547",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656494729797-noauth.png",
      "fullname": "Zhao Zhang",
      "name": "zbrl",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.10952",
      "authors": [
        {
          "_id": "684b96403b733ba33368703a",
          "user": {
            "_id": "65e808ed7c10574cc3f8e363",
            "avatarUrl": "/avatars/ed10759d354e271bfc15afd946b66b4a.svg",
            "isPro": false,
            "fullname": "zhangmozhi",
            "user": "mzzhang",
            "type": "user"
          },
          "name": "Mozhi Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:33.791Z",
          "hidden": false
        },
        {
          "_id": "684b96403b733ba33368703b",
          "user": {
            "_id": "6718fc605e14ff6b94a7109f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6718fc605e14ff6b94a7109f/uz2O2F_qbY3wefpY548qS.jpeg",
            "isPro": false,
            "fullname": "Howe Tissue",
            "user": "Howe77",
            "type": "user"
          },
          "name": "Howe Tissue",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:31.965Z",
          "hidden": false
        },
        {
          "_id": "684b96403b733ba33368703c",
          "name": "Lu Wang",
          "hidden": false
        },
        {
          "_id": "684b96403b733ba33368703d",
          "name": "Xipeng Qiu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T17:53:51.000Z",
      "submittedOnDailyAt": "2025-06-13T01:43:47.223Z",
      "title": "Domain2Vec: Vectorizing Datasets to Find the Optimal Data Mixture\n  without Training",
      "submittedOnDailyBy": {
        "_id": "6718fc605e14ff6b94a7109f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6718fc605e14ff6b94a7109f/uz2O2F_qbY3wefpY548qS.jpeg",
        "isPro": false,
        "fullname": "Howe Tissue",
        "user": "Howe77",
        "type": "user"
      },
      "summary": "We introduce~Domain2Vec, a novel approach that decomposes any\ndataset into a linear combination of several meta-domains, a new concept\ndesigned to capture the key underlying features of datasets.\nDomain2Vec maintains a vocabulary of meta-domains and uses a\nclassifier to decompose any given dataset into a domain vector that corresponds\nto a distribution over this vocabulary. These domain vectors enable the\nidentification of the optimal data mixture for language model (LM) pretraining\nin a training-free manner under the \\textbf{Distribution\nAlignment Assumption} (DA^{2}), which suggests that when\nthe data distributions of the training set and the validation set are better\naligned, a lower validation loss is achieved. Moreover, Domain2vec can\nbe seamlessly integrated into previous works to model the relationship between\ndomain vectors and LM performance, greatly enhancing the efficiency and\nscalability of previous methods. Extensive experiments demonstrate that\nDomain2Vec helps find the data mixture that enhances downstream task\nperformance with minimal computational overhead. Specifically,\nDomain2Vec achieves the same validation loss on Pile-CC using only\n51.5% of the computation required when training on the original mixture of\nThe Pile dataset. Under equivalent compute budget, Domain2Vec improves\ndownstream performance by an average of 2.83%.",
      "upvotes": 6,
      "discussionId": "684b96413b733ba33368703e",
      "ai_summary": "Domain2Vec decomposes datasets into meta-domains to optimize language model pretraining and downstream performance with reduced computational cost.",
      "ai_keywords": [
        "Domain2Vec",
        "meta-domains",
        "domain vector",
        "distribution alignment assumption",
        "DA",
        "language model",
        "pretraining",
        "downstream task performance",
        "Pile-CC",
        "The Pile dataset"
      ]
    },
    "publishedAt": "2025-06-12T13:53:51.000Z",
    "title": "Domain2Vec: Vectorizing Datasets to Find the Optimal Data Mixture\n  without Training",
    "summary": "We introduce~Domain2Vec, a novel approach that decomposes any\ndataset into a linear combination of several meta-domains, a new concept\ndesigned to capture the key underlying features of datasets.\nDomain2Vec maintains a vocabulary of meta-domains and uses a\nclassifier to decompose any given dataset into a domain vector that corresponds\nto a distribution over this vocabulary. These domain vectors enable the\nidentification of the optimal data mixture for language model (LM) pretraining\nin a training-free manner under the \\textbf{Distribution\nAlignment Assumption} (DA^{2}), which suggests that when\nthe data distributions of the training set and the validation set are better\naligned, a lower validation loss is achieved. Moreover, Domain2vec can\nbe seamlessly integrated into previous works to model the relationship between\ndomain vectors and LM performance, greatly enhancing the efficiency and\nscalability of previous methods. Extensive experiments demonstrate that\nDomain2Vec helps find the data mixture that enhances downstream task\nperformance with minimal computational overhead. Specifically,\nDomain2Vec achieves the same validation loss on Pile-CC using only\n51.5% of the computation required when training on the original mixture of\nThe Pile dataset. Under equivalent compute budget, Domain2Vec improves\ndownstream performance by an average of 2.83%.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10952.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6718fc605e14ff6b94a7109f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6718fc605e14ff6b94a7109f/uz2O2F_qbY3wefpY548qS.jpeg",
      "fullname": "Howe Tissue",
      "name": "Howe77",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.10741",
      "authors": [
        {
          "_id": "684b881f3b733ba333686fd4",
          "user": {
            "_id": "64966691990b342dcc9fccb5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64966691990b342dcc9fccb5/tQSrE3MkBeakk5QYfgHSo.jpeg",
            "isPro": false,
            "fullname": "sixiang chen",
            "user": "Ephemeral182",
            "type": "user"
          },
          "name": "SiXiang Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:47.414Z",
          "hidden": false
        },
        {
          "_id": "684b881f3b733ba333686fd5",
          "name": "Jianyu Lai",
          "hidden": false
        },
        {
          "_id": "684b881f3b733ba333686fd6",
          "name": "Jialin Gao",
          "hidden": false
        },
        {
          "_id": "684b881f3b733ba333686fd7",
          "name": "Tian Ye",
          "hidden": false
        },
        {
          "_id": "684b881f3b733ba333686fd8",
          "name": "Haoyu Chen",
          "hidden": false
        },
        {
          "_id": "684b881f3b733ba333686fd9",
          "name": "Hengyu Shi",
          "hidden": false
        },
        {
          "_id": "684b881f3b733ba333686fda",
          "name": "Shitong Shao",
          "hidden": false
        },
        {
          "_id": "684b881f3b733ba333686fdb",
          "name": "Yunlong Lin",
          "hidden": false
        },
        {
          "_id": "684b881f3b733ba333686fdc",
          "name": "Song Fei",
          "hidden": false
        },
        {
          "_id": "684b881f3b733ba333686fdd",
          "name": "Zhaohu Xing",
          "hidden": false
        },
        {
          "_id": "684b881f3b733ba333686fde",
          "name": "Yeying Jin",
          "hidden": false
        },
        {
          "_id": "684b881f3b733ba333686fdf",
          "name": "Junfeng Luo",
          "hidden": false
        },
        {
          "_id": "684b881f3b733ba333686fe0",
          "name": "Xiaoming Wei",
          "hidden": false
        },
        {
          "_id": "684b881f3b733ba333686fe1",
          "name": "Lei Zhu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T14:28:12.000Z",
      "submittedOnDailyAt": "2025-06-13T04:30:38.214Z",
      "title": "PosterCraft: Rethinking High-Quality Aesthetic Poster Generation in a\n  Unified Framework",
      "submittedOnDailyBy": {
        "_id": "66015e8aa4d296af07de538e",
        "avatarUrl": "/avatars/a1295c631cc2646282c545859975ce4c.svg",
        "isPro": false,
        "fullname": "Ye",
        "user": "Owen777",
        "type": "user"
      },
      "summary": "Generating aesthetic posters is more challenging than simple design images:\nit requires not only precise text rendering but also the seamless integration\nof abstract artistic content, striking layouts, and overall stylistic harmony.\nTo address this, we propose PosterCraft, a unified framework that abandons\nprior modular pipelines and rigid, predefined layouts, allowing the model to\nfreely explore coherent, visually compelling compositions. PosterCraft employs\na carefully designed, cascaded workflow to optimize the generation of\nhigh-aesthetic posters: (i) large-scale text-rendering optimization on our\nnewly introduced Text-Render-2M dataset; (ii) region-aware supervised\nfine-tuning on HQ-Poster100K; (iii) aesthetic-text-reinforcement learning via\nbest-of-n preference optimization; and (iv) joint vision-language feedback\nrefinement. Each stage is supported by a fully automated data-construction\npipeline tailored to its specific needs, enabling robust training without\ncomplex architectural modifications. Evaluated on multiple experiments,\nPosterCraft significantly outperforms open-source baselines in rendering\naccuracy, layout coherence, and overall visual appeal-approaching the quality\nof SOTA commercial systems. Our code, models, and datasets can be found in the\nProject page: https://ephemeral182.github.io/PosterCraft",
      "upvotes": 6,
      "discussionId": "684b881f3b733ba333686fe2",
      "projectPage": "https://ephemeral182.github.io/PosterCraft/",
      "githubRepo": "https://github.com/Ephemeral182/PosterCraft",
      "ai_summary": "PosterCraft improves aesthetic poster generation through a unified, modular pipeline with enhanced text rendering, region-aware fine-tuning, aesthetic reinforcement learning, and joint vision-language refinement.",
      "ai_keywords": [
        "text-rendering optimization",
        "Text-Render-2M",
        "region-aware supervised fine-tuning",
        "HQ-Poster100K",
        "aesthetic-text-reinforcement learning",
        "best-of-n preference optimization",
        "joint vision-language feedback refinement"
      ]
    },
    "publishedAt": "2025-06-12T10:28:12.000Z",
    "title": "PosterCraft: Rethinking High-Quality Aesthetic Poster Generation in a\n  Unified Framework",
    "summary": "Generating aesthetic posters is more challenging than simple design images:\nit requires not only precise text rendering but also the seamless integration\nof abstract artistic content, striking layouts, and overall stylistic harmony.\nTo address this, we propose PosterCraft, a unified framework that abandons\nprior modular pipelines and rigid, predefined layouts, allowing the model to\nfreely explore coherent, visually compelling compositions. PosterCraft employs\na carefully designed, cascaded workflow to optimize the generation of\nhigh-aesthetic posters: (i) large-scale text-rendering optimization on our\nnewly introduced Text-Render-2M dataset; (ii) region-aware supervised\nfine-tuning on HQ-Poster100K; (iii) aesthetic-text-reinforcement learning via\nbest-of-n preference optimization; and (iv) joint vision-language feedback\nrefinement. Each stage is supported by a fully automated data-construction\npipeline tailored to its specific needs, enabling robust training without\ncomplex architectural modifications. Evaluated on multiple experiments,\nPosterCraft significantly outperforms open-source baselines in rendering\naccuracy, layout coherence, and overall visual appeal-approaching the quality\nof SOTA commercial systems. Our code, models, and datasets can be found in the\nProject page: https://ephemeral182.github.io/PosterCraft",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10741.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "66015e8aa4d296af07de538e",
      "avatarUrl": "/avatars/a1295c631cc2646282c545859975ce4c.svg",
      "fullname": "Ye",
      "name": "Owen777",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.09967",
      "authors": [
        {
          "_id": "684ae1eedbd21a9cc27b0f10",
          "user": {
            "_id": "67469d6a8407f929491dce06",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67469d6a8407f929491dce06/7vd7zyApmT4rXe58gqmG1.png",
            "isPro": true,
            "fullname": "Shangshang Wang",
            "user": "upup-ashton-wang",
            "type": "user"
          },
          "name": "Shangshang Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:41:31.479Z",
          "hidden": false
        },
        {
          "_id": "684ae1eedbd21a9cc27b0f11",
          "name": "Julian Asilis",
          "hidden": false
        },
        {
          "_id": "684ae1eedbd21a9cc27b0f12",
          "name": "mer Faruk Akgl",
          "hidden": false
        },
        {
          "_id": "684ae1eedbd21a9cc27b0f13",
          "name": "Enes Burak Bilgin",
          "hidden": false
        },
        {
          "_id": "684ae1eedbd21a9cc27b0f14",
          "name": "Ollie Liu",
          "hidden": false
        },
        {
          "_id": "684ae1eedbd21a9cc27b0f15",
          "user": {
            "_id": "63c8454e46421a2efe82709d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c8454e46421a2efe82709d/3BcSk4KOwAgWHEPVtsAV3.png",
            "isPro": true,
            "fullname": "Deqing Fu",
            "user": "deqing",
            "type": "user"
          },
          "name": "Deqing Fu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:41:29.284Z",
          "hidden": false
        },
        {
          "_id": "684ae1eedbd21a9cc27b0f16",
          "user": {
            "_id": "644bf65522d211df6444a7f4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644bf65522d211df6444a7f4/k_ddZdQDg2fzhwjI1EXyx.jpeg",
            "isPro": false,
            "fullname": "Willie Neiswanger",
            "user": "willieneis",
            "type": "user"
          },
          "name": "Willie Neiswanger",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:41:27.301Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-11T17:44:01.000Z",
      "submittedOnDailyAt": "2025-06-13T02:39:37.215Z",
      "title": "Resa: Transparent Reasoning Models via SAEs",
      "submittedOnDailyBy": {
        "_id": "67469d6a8407f929491dce06",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67469d6a8407f929491dce06/7vd7zyApmT4rXe58gqmG1.png",
        "isPro": true,
        "fullname": "Shangshang Wang",
        "user": "upup-ashton-wang",
        "type": "user"
      },
      "summary": "How cost-effectively can we elicit strong reasoning in language models by\nleveraging their underlying representations? We answer this question with Resa,\na family of 1.5B reasoning models trained via a novel and efficient sparse\nautoencoder tuning (SAE-Tuning) procedure. This method first trains an SAE to\ncapture reasoning abilities from a source model, and then uses the trained SAE\nto guide a standard supervised fine-tuning process to elicit such abilities in\na target model, all using verified question-answer data without any reasoning\ntraces. Notably, when applied to certain base models before further RL\npost-training, SAE-Tuning retains >97% of its RL-trained counterpart's\nreasoning performance while reducing training costs by >2000x to roughly \\1\nand training time by >450x to around 20 minutes. Furthermore, when applied to\nlightly RL-trained models (e.g., within 1 hour on 2 GPUs), it enables reasoning\nperformance such as 43.33% Pass@1 on AIME24 and 90% Pass@1 on AMC23 for only\naround 1 additional cost. Surprisingly, the reasoning abilities extracted via\nSAEs are potentially both generalizable and modular. Generality means abilities\nextracted from one dataset still elevate performance on a larger and\noverlapping corpus. Modularity means abilities extracted from Qwen or Qwen-Math\ncan be attached to the R1-Distill model at test time, without any retraining,\nand yield comparable gains. Extensive ablations validate these findings and all\nartifacts are fully open-sourced.",
      "upvotes": 6,
      "discussionId": "684ae1eedbd21a9cc27b0f17",
      "projectPage": "https://shangshangwang.notion.site/resa",
      "githubRepo": "https://github.com/shangshang-wang/Resa",
      "ai_summary": "SAE-Tuning efficiently elicits strong reasoning in language models by leveraging sparse autoencoders, enabling cost-effective performance gains without extensive retraining.",
      "ai_keywords": [
        "sparse autoencoder tuning",
        "SAE-Tuning",
        "reasoning models",
        "verification",
        "sparse autoencoders",
        "supervised fine-tuning",
        "RL post-training",
        "Pass@1",
        "AIME24",
        "AMC23",
        "generality",
        "modularity",
        "R1-Distill",
        "Qwen",
        "Qwen-Math"
      ]
    },
    "publishedAt": "2025-06-11T13:44:01.000Z",
    "title": "Resa: Transparent Reasoning Models via SAEs",
    "summary": "How cost-effectively can we elicit strong reasoning in language models by\nleveraging their underlying representations? We answer this question with Resa,\na family of 1.5B reasoning models trained via a novel and efficient sparse\nautoencoder tuning (SAE-Tuning) procedure. This method first trains an SAE to\ncapture reasoning abilities from a source model, and then uses the trained SAE\nto guide a standard supervised fine-tuning process to elicit such abilities in\na target model, all using verified question-answer data without any reasoning\ntraces. Notably, when applied to certain base models before further RL\npost-training, SAE-Tuning retains >97% of its RL-trained counterpart's\nreasoning performance while reducing training costs by >2000x to roughly \\1\nand training time by >450x to around 20 minutes. Furthermore, when applied to\nlightly RL-trained models (e.g., within 1 hour on 2 GPUs), it enables reasoning\nperformance such as 43.33% Pass@1 on AIME24 and 90% Pass@1 on AMC23 for only\naround 1 additional cost. Surprisingly, the reasoning abilities extracted via\nSAEs are potentially both generalizable and modular. Generality means abilities\nextracted from one dataset still elevate performance on a larger and\noverlapping corpus. Modularity means abilities extracted from Qwen or Qwen-Math\ncan be attached to the R1-Distill model at test time, without any retraining,\nand yield comparable gains. Extensive ablations validate these findings and all\nartifacts are fully open-sourced.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09967.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67469d6a8407f929491dce06",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67469d6a8407f929491dce06/7vd7zyApmT4rXe58gqmG1.png",
      "fullname": "Shangshang Wang",
      "name": "upup-ashton-wang",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.09344",
      "authors": [
        {
          "_id": "684ae277dbd21a9cc27b118d",
          "name": "Inclusion AI",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b118e",
          "name": "Biao Gong",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b118f",
          "name": "Cheng Zou",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b1190",
          "name": "Chuanyang Zheng",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b1191",
          "name": "Chunluan Zhou",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b1192",
          "name": "Canxiang Yan",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b1193",
          "name": "Chunxiang Jin",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b1194",
          "name": "Chunjie Shen",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b1195",
          "name": "Dandan Zheng",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b1196",
          "name": "Fudong Wang",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b1197",
          "name": "Furong Xu",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b1198",
          "name": "GuangMing Yao",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b1199",
          "name": "Jun Zhou",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b119a",
          "name": "Jingdong Chen",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b119b",
          "name": "Jianxin Sun",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b119c",
          "name": "Jiajia Liu",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b119d",
          "name": "Jianjiang Zhu",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b119e",
          "name": "Jun Peng",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b119f",
          "name": "Kaixiang Ji",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11a0",
          "name": "Kaiyou Song",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11a1",
          "name": "Kaimeng Ren",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11a2",
          "name": "Libin Wang",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11a3",
          "name": "Lixiang Ru",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11a4",
          "name": "Lele Xie",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11a5",
          "name": "Longhua Tan",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11a6",
          "name": "Lyuxin Xue",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11a7",
          "name": "Lan Wang",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11a8",
          "name": "Mochen Bai",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11a9",
          "name": "Ning Gao",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11aa",
          "name": "Pei Chen",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11ab",
          "name": "Qingpei Guo",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11ac",
          "name": "Qinglong Zhang",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11ad",
          "name": "Qiang Xu",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11ae",
          "name": "Rui Liu",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11af",
          "name": "Ruijie Xiong",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11b0",
          "name": "Sirui Gao",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11b1",
          "name": "Tinghao Liu",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11b2",
          "name": "Taisong Li",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11b3",
          "name": "Weilong Chai",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11b4",
          "name": "Xinyu Xiao",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11b5",
          "name": "Xiaomei Wang",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11b6",
          "name": "Xiaoxue Chen",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11b7",
          "name": "Xiao Lu",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11b8",
          "name": "Xiaoyu Li",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11b9",
          "name": "Xingning Dong",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11ba",
          "name": "Xuzheng Yu",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11bb",
          "name": "Yi Yuan",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11bc",
          "name": "Yuting Gao",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11bd",
          "name": "Yunxiao Sun",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11be",
          "name": "Yipeng Chen",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11bf",
          "name": "Yifei Wu",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11c0",
          "name": "Yongjie Lyu",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11c1",
          "name": "Ziping Ma",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11c2",
          "name": "Zipeng Feng",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11c3",
          "name": "Zhijiang Fang",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11c4",
          "name": "Zhihao Qiu",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11c5",
          "name": "Ziyuan Huang",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11c6",
          "name": "Zhengyu He",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-11T02:50:49.000Z",
      "submittedOnDailyAt": "2025-06-13T01:53:15.172Z",
      "title": "Ming-Omni: A Unified Multimodal Model for Perception and Generation",
      "submittedOnDailyBy": {
        "_id": "644fcbea4f7316588267dc80",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644fcbea4f7316588267dc80/w8-2Gkaw9BN9VzppNXrTP.jpeg",
        "isPro": false,
        "fullname": "Biao Gong",
        "user": "BiaoGong",
        "type": "user"
      },
      "summary": "We propose Ming-Omni, a unified multimodal model capable of processing\nimages, text, audio, and video, while demonstrating strong proficiency in both\nspeech and image generation. Ming-Omni employs dedicated encoders to extract\ntokens from different modalities, which are then processed by Ling, an MoE\narchitecture equipped with newly proposed modality-specific routers. This\ndesign enables a single model to efficiently process and fuse multimodal inputs\nwithin a unified framework, thereby facilitating diverse tasks without\nrequiring separate models, task-specific fine-tuning, or structural redesign.\nImportantly, Ming-Omni extends beyond conventional multimodal models by\nsupporting audio and image generation. This is achieved through the integration\nof an advanced audio decoder for natural-sounding speech and Ming-Lite-Uni for\nhigh-quality image generation, which also allow the model to engage in\ncontext-aware chatting, perform text-to-speech conversion, and conduct\nversatile image editing. Our experimental results showcase Ming-Omni offers a\npowerful solution for unified perception and generation across all modalities.\nNotably, our proposed Ming-Omni is the first open-source model we are aware of\nto match GPT-4o in modality support, and we release all code and model weights\nto encourage further research and development in the community.",
      "upvotes": 4,
      "discussionId": "684ae277dbd21a9cc27b11c7",
      "ai_summary": "Ming-Omni is a unified multimodal model with dedicated encoders and modality-specific routers that can process images, text, audio, and video, and performs tasks like speech and image generation, context-aware chatting, and versatile image editing.",
      "ai_keywords": [
        "multimodal model",
        "encoders",
        "tokens",
        "MoE architecture",
        "modality-specific routers",
        "audio decoder",
        "Ming-Lite-Uni",
        "context-aware chatting",
        "text-to-speech conversion",
        "image editing",
        "unified perception",
        "generation",
        "open-source"
      ]
    },
    "publishedAt": "2025-06-10T22:50:49.000Z",
    "title": "Ming-Omni: A Unified Multimodal Model for Perception and Generation",
    "summary": "We propose Ming-Omni, a unified multimodal model capable of processing\nimages, text, audio, and video, while demonstrating strong proficiency in both\nspeech and image generation. Ming-Omni employs dedicated encoders to extract\ntokens from different modalities, which are then processed by Ling, an MoE\narchitecture equipped with newly proposed modality-specific routers. This\ndesign enables a single model to efficiently process and fuse multimodal inputs\nwithin a unified framework, thereby facilitating diverse tasks without\nrequiring separate models, task-specific fine-tuning, or structural redesign.\nImportantly, Ming-Omni extends beyond conventional multimodal models by\nsupporting audio and image generation. This is achieved through the integration\nof an advanced audio decoder for natural-sounding speech and Ming-Lite-Uni for\nhigh-quality image generation, which also allow the model to engage in\ncontext-aware chatting, perform text-to-speech conversion, and conduct\nversatile image editing. Our experimental results showcase Ming-Omni offers a\npowerful solution for unified perception and generation across all modalities.\nNotably, our proposed Ming-Omni is the first open-source model we are aware of\nto match GPT-4o in modality support, and we release all code and model weights\nto encourage further research and development in the community.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09344.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "644fcbea4f7316588267dc80",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644fcbea4f7316588267dc80/w8-2Gkaw9BN9VzppNXrTP.jpeg",
      "fullname": "Biao Gong",
      "name": "BiaoGong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.08060",
      "authors": [
        {
          "_id": "6848e0b042e4f9106973f280",
          "user": {
            "_id": "62f32eab52ad88c930bb3f3b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677134945205-62f32eab52ad88c930bb3f3b.png",
            "isPro": true,
            "fullname": "Asankhaya Sharma",
            "user": "codelion",
            "type": "user"
          },
          "name": "Asankhaya Sharma",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-11T08:35:08.045Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-09T08:37:19.000Z",
      "submittedOnDailyAt": "2025-06-13T00:31:17.814Z",
      "title": "Eliciting Fine-Tuned Transformer Capabilities via Inference-Time\n  Techniques",
      "submittedOnDailyBy": {
        "_id": "62f32eab52ad88c930bb3f3b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677134945205-62f32eab52ad88c930bb3f3b.png",
        "isPro": true,
        "fullname": "Asankhaya Sharma",
        "user": "codelion",
        "type": "user"
      },
      "summary": "Large language models have transformed natural language processing, yet\nsupervised fine-tuning (SFT) remains computationally intensive. This paper\nformally proves that capabilities acquired through SFT can be approximated by a\nbase transformer model using inference-time techniques, specifically in-context\nlearning (ICL), without altering model parameters, under idealized assumptions\nincluding unbounded computational resources and access to the fine-tuning\ndataset. We extend these results to practical scenarios with finite context\nlengths and partial dataset access. For text generation tasks with fixed output\nlength l, datasets of size Oleft( m V{varepsilon^2} log\nm{delta} right) or, with bounded context, Oleft( l\nlog V{varepsilon^2} log 1{delta} right) suffice to approximate\nfine-tuned behavior across m contexts within error varepsilon, where V\nis the vocabulary size and delta is the failure probability. For linear\nclassification, datasets of size Oleft( d{varepsilon}\nright) or, with fixed context, Oleft( 1{varepsilon^2} log\n1{delta} right) are sufficient, where d is the input dimension.\nGrounded in the Turing completeness of transformers, these results provide a\ntheoretical foundation for resource-efficient deployment of large language\nmodels, with practical techniques like retrieval-augmented generation bridging\ntheory to real-world applications.",
      "upvotes": 4,
      "discussionId": "6848e0b042e4f9106973f281",
      "githubRepo": "https://github.com/codelion/optillm",
      "ai_summary": "Transformers can approximate supervised fine-tuning capabilities through in-context learning without altering model parameters, supported by theoretical bounds and practical techniques.",
      "ai_keywords": [
        "supervised fine-tuning",
        "in-context learning",
        "base transformer model",
        "Turing completeness",
        "retrieval-augmented generation",
        "text generation",
        "linear classification"
      ]
    },
    "publishedAt": "2025-06-09T04:37:19.000Z",
    "title": "Eliciting Fine-Tuned Transformer Capabilities via Inference-Time\n  Techniques",
    "summary": "Large language models have transformed natural language processing, yet\nsupervised fine-tuning (SFT) remains computationally intensive. This paper\nformally proves that capabilities acquired through SFT can be approximated by a\nbase transformer model using inference-time techniques, specifically in-context\nlearning (ICL), without altering model parameters, under idealized assumptions\nincluding unbounded computational resources and access to the fine-tuning\ndataset. We extend these results to practical scenarios with finite context\nlengths and partial dataset access. For text generation tasks with fixed output\nlength l, datasets of size Oleft( m V{varepsilon^2} log\nm{delta} right) or, with bounded context, Oleft( l\nlog V{varepsilon^2} log 1{delta} right) suffice to approximate\nfine-tuned behavior across m contexts within error varepsilon, where V\nis the vocabulary size and delta is the failure probability. For linear\nclassification, datasets of size Oleft( d{varepsilon}\nright) or, with fixed context, Oleft( 1{varepsilon^2} log\n1{delta} right) are sufficient, where d is the input dimension.\nGrounded in the Turing completeness of transformers, these results provide a\ntheoretical foundation for resource-efficient deployment of large language\nmodels, with practical techniques like retrieval-augmented generation bridging\ntheory to real-world applications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08060.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62f32eab52ad88c930bb3f3b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677134945205-62f32eab52ad88c930bb3f3b.png",
      "fullname": "Asankhaya Sharma",
      "name": "codelion",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 91
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.10357",
      "authors": [
        {
          "_id": "684b86bf3b733ba333686fbe",
          "name": "Zaijing Li",
          "hidden": false
        },
        {
          "_id": "684b86bf3b733ba333686fbf",
          "name": "Yuquan Xie",
          "hidden": false
        },
        {
          "_id": "684b86bf3b733ba333686fc0",
          "name": "Rui Shao",
          "hidden": false
        },
        {
          "_id": "684b86bf3b733ba333686fc1",
          "name": "Gongwei Chen",
          "hidden": false
        },
        {
          "_id": "684b86bf3b733ba333686fc2",
          "name": "Weili Guan",
          "hidden": false
        },
        {
          "_id": "684b86bf3b733ba333686fc3",
          "name": "Dongmei Jiang",
          "hidden": false
        },
        {
          "_id": "684b86bf3b733ba333686fc4",
          "name": "Liqiang Nie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T05:29:40.000Z",
      "submittedOnDailyAt": "2025-06-13T00:37:48.793Z",
      "title": "Optimus-3: Towards Generalist Multimodal Minecraft Agents with Scalable\n  Task Experts",
      "submittedOnDailyBy": {
        "_id": "66b45fe75d0ac130d7d82764",
        "avatarUrl": "/avatars/09253f41f82c533b36199f82620cd075.svg",
        "isPro": false,
        "fullname": "Zaijing Li",
        "user": "dawn0815",
        "type": "user"
      },
      "summary": "Recently, agents based on multimodal large language models (MLLMs) have\nachieved remarkable progress across various domains. However, building a\ngeneralist agent with capabilities such as perception, planning, action,\ngrounding, and reflection in open-world environments like Minecraft remains\nchallenges: insufficient domain-specific data, interference among heterogeneous\ntasks, and visual diversity in open-world settings. In this paper, we address\nthese challenges through three key contributions. 1) We propose a\nknowledge-enhanced data generation pipeline to provide scalable and\nhigh-quality training data for agent development. 2) To mitigate interference\namong heterogeneous tasks, we introduce a Mixture-of-Experts (MoE) architecture\nwith task-level routing. 3) We develop a Multimodal Reasoning-Augmented\nReinforcement Learning approach to enhance the agent's reasoning ability for\nvisual diversity in Minecraft. Built upon these innovations, we present\nOptimus-3, a general-purpose agent for Minecraft. Extensive experimental\nresults demonstrate that Optimus-3 surpasses both generalist multimodal large\nlanguage models and existing state-of-the-art agents across a wide range of\ntasks in the Minecraft environment. Project page:\nhttps://cybertronagent.github.io/Optimus-3.github.io/",
      "upvotes": 3,
      "discussionId": "684b86bf3b733ba333686fc5",
      "ai_summary": "Optimus-3, a multimodal large language model agent, uses knowledge-enhanced data generation, a Mixture-of-Experts architecture, and multimodal reasoning-augmented reinforcement learning to achieve superior performance across various tasks in Minecraft.",
      "ai_keywords": [
        "multimodal large language models",
        "knowledge-enhanced data generation",
        "Mixture-of-Experts",
        "task-level routing",
        "multimodal reasoning-augmented reinforcement learning"
      ]
    },
    "publishedAt": "2025-06-12T01:29:40.000Z",
    "title": "Optimus-3: Towards Generalist Multimodal Minecraft Agents with Scalable\n  Task Experts",
    "summary": "Recently, agents based on multimodal large language models (MLLMs) have\nachieved remarkable progress across various domains. However, building a\ngeneralist agent with capabilities such as perception, planning, action,\ngrounding, and reflection in open-world environments like Minecraft remains\nchallenges: insufficient domain-specific data, interference among heterogeneous\ntasks, and visual diversity in open-world settings. In this paper, we address\nthese challenges through three key contributions. 1) We propose a\nknowledge-enhanced data generation pipeline to provide scalable and\nhigh-quality training data for agent development. 2) To mitigate interference\namong heterogeneous tasks, we introduce a Mixture-of-Experts (MoE) architecture\nwith task-level routing. 3) We develop a Multimodal Reasoning-Augmented\nReinforcement Learning approach to enhance the agent's reasoning ability for\nvisual diversity in Minecraft. Built upon these innovations, we present\nOptimus-3, a general-purpose agent for Minecraft. Extensive experimental\nresults demonstrate that Optimus-3 surpasses both generalist multimodal large\nlanguage models and existing state-of-the-art agents across a wide range of\ntasks in the Minecraft environment. Project page:\nhttps://cybertronagent.github.io/Optimus-3.github.io/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10357.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66b45fe75d0ac130d7d82764",
      "avatarUrl": "/avatars/09253f41f82c533b36199f82620cd075.svg",
      "fullname": "Zaijing Li",
      "name": "dawn0815",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.09942",
      "authors": [
        {
          "_id": "684ae26adbd21a9cc27b1177",
          "user": {
            "_id": "625a5446f1063e7085d5178a",
            "avatarUrl": "/avatars/5e78186f13f74b14e01583e06ff6c4dc.svg",
            "isPro": false,
            "fullname": "Hao Peng",
            "user": "Wesleythu",
            "type": "user"
          },
          "name": "Hao Peng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:40:06.456Z",
          "hidden": false
        },
        {
          "_id": "684ae26adbd21a9cc27b1178",
          "name": "Yunjia Qi",
          "hidden": false
        },
        {
          "_id": "684ae26adbd21a9cc27b1179",
          "name": "Xiaozhi Wang",
          "hidden": false
        },
        {
          "_id": "684ae26adbd21a9cc27b117a",
          "name": "Bin Xu",
          "hidden": false
        },
        {
          "_id": "684ae26adbd21a9cc27b117b",
          "name": "Lei Hou",
          "hidden": false
        },
        {
          "_id": "684ae26adbd21a9cc27b117c",
          "name": "Juanzi Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-11T17:10:36.000Z",
      "submittedOnDailyAt": "2025-06-13T00:15:19.828Z",
      "title": "VerIF: Verification Engineering for Reinforcement Learning in\n  Instruction Following",
      "submittedOnDailyBy": {
        "_id": "625a5446f1063e7085d5178a",
        "avatarUrl": "/avatars/5e78186f13f74b14e01583e06ff6c4dc.svg",
        "isPro": false,
        "fullname": "Hao Peng",
        "user": "Wesleythu",
        "type": "user"
      },
      "summary": "Reinforcement learning with verifiable rewards (RLVR) has become a key\ntechnique for enhancing large language models (LLMs), with verification\nengineering playing a central role. However, best practices for RL in\ninstruction following remain underexplored. In this work, we explore the\nverification challenge in RL for instruction following and propose VerIF, a\nverification method that combines rule-based code verification with LLM-based\nverification from a large reasoning model (e.g., QwQ-32B). To support this\napproach, we construct a high-quality instruction-following dataset,\nVerInstruct, containing approximately 22,000 instances with associated\nverification signals. We apply RL training with VerIF to two models, achieving\nsignificant improvements across several representative instruction-following\nbenchmarks. The trained models reach state-of-the-art performance among models\nof comparable size and generalize well to unseen constraints. We further\nobserve that their general capabilities remain unaffected, suggesting that RL\nwith VerIF can be integrated into existing RL recipes to enhance overall model\nperformance. We have released our datasets, codes, and models to facilitate\nfuture research at https://github.com/THU-KEG/VerIF.",
      "upvotes": 3,
      "discussionId": "684ae26adbd21a9cc27b117d",
      "githubRepo": "https://github.com/THU-KEG/VerIF",
      "ai_summary": "VerIF, a hybrid verification method combining rule-based and LLM-based approaches, enhances instruction-following RL with significant performance improvements and generalization.",
      "ai_keywords": [
        "reinforcement learning",
        "verifiable rewards",
        "RLVR",
        "large language models",
        "LLMs",
        "rule-based code verification",
        "QwQ-32B",
        "instruction-following",
        "VerInstruct",
        "RL training",
        "instruction-following benchmarks",
        "state-of-the-art performance",
        "existing RL recipes"
      ]
    },
    "publishedAt": "2025-06-11T13:10:36.000Z",
    "title": "VerIF: Verification Engineering for Reinforcement Learning in\n  Instruction Following",
    "summary": "Reinforcement learning with verifiable rewards (RLVR) has become a key\ntechnique for enhancing large language models (LLMs), with verification\nengineering playing a central role. However, best practices for RL in\ninstruction following remain underexplored. In this work, we explore the\nverification challenge in RL for instruction following and propose VerIF, a\nverification method that combines rule-based code verification with LLM-based\nverification from a large reasoning model (e.g., QwQ-32B). To support this\napproach, we construct a high-quality instruction-following dataset,\nVerInstruct, containing approximately 22,000 instances with associated\nverification signals. We apply RL training with VerIF to two models, achieving\nsignificant improvements across several representative instruction-following\nbenchmarks. The trained models reach state-of-the-art performance among models\nof comparable size and generalize well to unseen constraints. We further\nobserve that their general capabilities remain unaffected, suggesting that RL\nwith VerIF can be integrated into existing RL recipes to enhance overall model\nperformance. We have released our datasets, codes, and models to facilitate\nfuture research at https://github.com/THU-KEG/VerIF.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09942.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "625a5446f1063e7085d5178a",
      "avatarUrl": "/avatars/5e78186f13f74b14e01583e06ff6c4dc.svg",
      "fullname": "Hao Peng",
      "name": "Wesleythu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.10953",
      "authors": [
        {
          "_id": "684b9fa13b733ba333687066",
          "user": {
            "_id": "5fa9ff3ea13e063b8b2b60cb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1633380224986-5fa9ff3ea13e063b8b2b60cb.jpeg",
            "isPro": false,
            "fullname": "Xing Han L",
            "user": "xhluca",
            "type": "user"
          },
          "name": "Xing Han L",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:26.665Z",
          "hidden": false
        },
        {
          "_id": "684b9fa13b733ba333687067",
          "name": "Gaurav Kamath",
          "hidden": false
        },
        {
          "_id": "684b9fa13b733ba333687068",
          "name": "Marius Mosbach",
          "hidden": false
        },
        {
          "_id": "684b9fa13b733ba333687069",
          "name": "Siva Reddy",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T17:53:58.000Z",
      "submittedOnDailyAt": "2025-06-13T02:22:59.640Z",
      "title": "Build the web for agents, not agents for the web",
      "submittedOnDailyBy": {
        "_id": "5fa9ff3ea13e063b8b2b60cb",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1633380224986-5fa9ff3ea13e063b8b2b60cb.jpeg",
        "isPro": false,
        "fullname": "Xing Han L",
        "user": "xhluca",
        "type": "user"
      },
      "summary": "Recent advancements in Large Language Models (LLMs) and multimodal\ncounterparts have spurred significant interest in developing web agents -- AI\nsystems capable of autonomously navigating and completing tasks within web\nenvironments. While holding tremendous promise for automating complex web\ninteractions, current approaches face substantial challenges due to the\nfundamental mismatch between human-designed interfaces and LLM capabilities.\nCurrent methods struggle with the inherent complexity of web inputs, whether\nprocessing massive DOM trees, relying on screenshots augmented with additional\ninformation, or bypassing the user interface entirely through API interactions.\nThis position paper advocates for a paradigm shift in web agent research:\nrather than forcing web agents to adapt to interfaces designed for humans, we\nshould develop a new interaction paradigm specifically optimized for agentic\ncapabilities. To this end, we introduce the concept of an Agentic Web Interface\n(AWI), an interface specifically designed for agents to navigate a website. We\nestablish six guiding principles for AWI design, emphasizing safety,\nefficiency, and standardization, to account for the interests of all primary\nstakeholders. This reframing aims to overcome fundamental limitations of\nexisting interfaces, paving the way for more efficient, reliable, and\ntransparent web agent design, which will be a collaborative effort involving\nthe broader ML community.",
      "upvotes": 2,
      "discussionId": "684b9fa13b733ba33368706a",
      "ai_summary": "A new Agentic Web Interface (AWI) design paradigm is proposed to optimize web agents for navigating websites, focusing on safety, efficiency, and standardization to address fundamental interface mismatches.",
      "ai_keywords": [
        "Large Language Models",
        "multimodal",
        "web agents",
        "Agentic Web Interface",
        "AWI",
        "DOM trees",
        "screenshots",
        "API interactions"
      ]
    },
    "publishedAt": "2025-06-12T13:53:58.000Z",
    "title": "Build the web for agents, not agents for the web",
    "summary": "Recent advancements in Large Language Models (LLMs) and multimodal\ncounterparts have spurred significant interest in developing web agents -- AI\nsystems capable of autonomously navigating and completing tasks within web\nenvironments. While holding tremendous promise for automating complex web\ninteractions, current approaches face substantial challenges due to the\nfundamental mismatch between human-designed interfaces and LLM capabilities.\nCurrent methods struggle with the inherent complexity of web inputs, whether\nprocessing massive DOM trees, relying on screenshots augmented with additional\ninformation, or bypassing the user interface entirely through API interactions.\nThis position paper advocates for a paradigm shift in web agent research:\nrather than forcing web agents to adapt to interfaces designed for humans, we\nshould develop a new interaction paradigm specifically optimized for agentic\ncapabilities. To this end, we introduce the concept of an Agentic Web Interface\n(AWI), an interface specifically designed for agents to navigate a website. We\nestablish six guiding principles for AWI design, emphasizing safety,\nefficiency, and standardization, to account for the interests of all primary\nstakeholders. This reframing aims to overcome fundamental limitations of\nexisting interfaces, paving the way for more efficient, reliable, and\ntransparent web agent design, which will be a collaborative effort involving\nthe broader ML community.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10953.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5fa9ff3ea13e063b8b2b60cb",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1633380224986-5fa9ff3ea13e063b8b2b60cb.jpeg",
      "fullname": "Xing Han L",
      "name": "xhluca",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.10910",
      "authors": [
        {
          "_id": "684bbe273b733ba3336870ed",
          "name": "Mistral-AI",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870ef",
          "name": "Abhinav Rastogi",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870f0",
          "name": "Albert Q. Jiang",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870f1",
          "name": "Andy Lo",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870f2",
          "name": "Gabrielle Berrada",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870f3",
          "name": "Guillaume Lample",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870f4",
          "name": "Jason Rute",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870f5",
          "name": "Joep Barmentlo",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870f6",
          "name": "Karmesh Yadav",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870f7",
          "name": "Kartik Khandelwal",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870f8",
          "name": "Khyathi Raghavi Chandu",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870f9",
          "name": "Lonard Blier",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870fa",
          "name": "Lucile Saulnier",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870fb",
          "name": "Matthieu Dinot",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870fc",
          "name": "Maxime Darrin",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870fd",
          "name": "Neha Gupta",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870fe",
          "name": "Roman Soletskyi",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870ff",
          "name": "Sagar Vaze",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687100",
          "name": "Teven Le Scao",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687101",
          "name": "Yihan Wang",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687102",
          "name": "Adam Yang",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687103",
          "name": "Alexander H. Liu",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687104",
          "name": "Alexandre Sablayrolles",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687105",
          "name": "Amlie Hliou",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687106",
          "name": "Amlie Martin",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687107",
          "name": "Andy Ehrenberg",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687108",
          "name": "Anmol Agarwal",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687109",
          "name": "Antoine Roux",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368710a",
          "name": "Arthur Darcet",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368710b",
          "name": "Arthur Mensch",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368710c",
          "name": "Baptiste Bout",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368710d",
          "name": "Baptiste Rozire",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368710e",
          "name": "Baudouin De Monicault",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368710f",
          "name": "Chris Bamford",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687110",
          "name": "Christian Wallenwein",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687111",
          "name": "Christophe Renaudin",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687112",
          "name": "Clmence Lanfranchi",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687113",
          "name": "Darius Dabert",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687114",
          "name": "Devon Mizelle",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687115",
          "name": "Diego de las Casas",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687116",
          "name": "Elliot Chane-Sane",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687117",
          "name": "Emilien Fugier",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687118",
          "name": "Emma Bou Hanna",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687119",
          "name": "Gauthier Delerce",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368711a",
          "name": "Gauthier Guinet",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368711b",
          "name": "Georgii Novikov",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368711c",
          "name": "Guillaume Martin",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368711d",
          "name": "Himanshu Jaju",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368711e",
          "name": "Jan Ludziejewski",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368711f",
          "name": "Jean-Hadrien Chabran",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687120",
          "name": "Jean-Malo Delignon",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687121",
          "name": "Joachim Studnia",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687122",
          "name": "Jonas Amar",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687123",
          "name": "Josselin Somerville Roberts",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687124",
          "name": "Julien Denize",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687125",
          "name": "Karan Saxena",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687126",
          "name": "Kush Jain",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687127",
          "name": "Lingxiao Zhao",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687128",
          "name": "Louis Martin",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687129",
          "name": "Luyu Gao",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368712a",
          "name": "Llio Renard Lavaud",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368712b",
          "name": "Marie Pellat",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368712c",
          "name": "Mathilde Guillaumin",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368712d",
          "name": "Mathis Felardos",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368712e",
          "name": "Maximilian Augustin",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368712f",
          "name": "Mickal Seznec",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687130",
          "name": "Nikhil Raghuraman",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687131",
          "name": "Olivier Duchenne",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687132",
          "name": "Patricia Wang",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687133",
          "name": "Patrick von Platen",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687134",
          "name": "Patryk Saffer",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687135",
          "name": "Paul Jacob",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687136",
          "name": "Paul Wambergue",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687137",
          "name": "Paula Kurylowicz",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687138",
          "name": "Pavankumar Reddy Muddireddy",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687139",
          "name": "Philomne Chagniot",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368713a",
          "name": "Pierre Stock",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368713b",
          "name": "Pravesh Agrawal",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368713c",
          "name": "Romain Sauvestre",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368713d",
          "name": "Rmi Delacourt",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368713e",
          "name": "Sanchit Gandhi",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368713f",
          "name": "Sandeep Subramanian",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687140",
          "name": "Shashwat Dalal",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687141",
          "name": "Siddharth Gandhi",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687142",
          "name": "Soham Ghosh",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687143",
          "name": "Srijan Mishra",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687144",
          "name": "Sumukh Aithal",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687145",
          "name": "Szymon Antoniak",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687146",
          "name": "Thibault Schueller",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687147",
          "name": "Thibaut Lavril",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687148",
          "name": "Thomas Robert",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687149",
          "name": "Thomas Wang",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368714a",
          "name": "Timothe Lacroix",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368714b",
          "name": "Valeriia Nemychnikova",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368714c",
          "name": "Victor Paltz",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368714d",
          "name": "Virgile Richard",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368714e",
          "name": "Wen-Ding Li",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368714f",
          "name": "William Marshall",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687150",
          "name": "Xuanyu Zhang",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687151",
          "name": "Yunhao Tang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T17:22:37.000Z",
      "submittedOnDailyAt": "2025-06-13T04:29:39.974Z",
      "title": "Magistral",
      "submittedOnDailyBy": {
        "_id": "5e6a3d4ea9afd5125d9ec064",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1584020801691-noauth.jpeg",
        "isPro": true,
        "fullname": "Stefan Schweter",
        "user": "stefan-it",
        "type": "user"
      },
      "summary": "We introduce Magistral, Mistral's first reasoning model and our own scalable\nreinforcement learning (RL) pipeline. Instead of relying on existing\nimplementations and RL traces distilled from prior models, we follow a ground\nup approach, relying solely on our own models and infrastructure. Notably, we\ndemonstrate a stack that enabled us to explore the limits of pure RL training\nof LLMs, present a simple method to force the reasoning language of the model,\nand show that RL on text data alone maintains most of the initial checkpoint's\ncapabilities. We find that RL on text maintains or improves multimodal\nunderstanding, instruction following and function calling. We present Magistral\nMedium, trained for reasoning on top of Mistral Medium 3 with RL alone, and we\nopen-source Magistral Small (Apache 2.0) which further includes cold-start data\nfrom Magistral Medium.",
      "upvotes": 2,
      "discussionId": "684bbe283b733ba333687152",
      "ai_summary": "A scalable reinforcement learning pipeline for training reasoning models demonstrates improvements in multimodal understanding, instruction following, and function calling without relying on existing implementations.",
      "ai_keywords": [
        "ground up approach",
        "reinforcement learning (RL)",
        "reasoning model",
        "multimodal understanding",
        "instruction following",
        "function calling"
      ]
    },
    "publishedAt": "2025-06-12T13:22:37.000Z",
    "title": "Magistral",
    "summary": "We introduce Magistral, Mistral's first reasoning model and our own scalable\nreinforcement learning (RL) pipeline. Instead of relying on existing\nimplementations and RL traces distilled from prior models, we follow a ground\nup approach, relying solely on our own models and infrastructure. Notably, we\ndemonstrate a stack that enabled us to explore the limits of pure RL training\nof LLMs, present a simple method to force the reasoning language of the model,\nand show that RL on text data alone maintains most of the initial checkpoint's\ncapabilities. We find that RL on text maintains or improves multimodal\nunderstanding, instruction following and function calling. We present Magistral\nMedium, trained for reasoning on top of Mistral Medium 3 with RL alone, and we\nopen-source Magistral Small (Apache 2.0) which further includes cold-start data\nfrom Magistral Medium.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10910.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5e6a3d4ea9afd5125d9ec064",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1584020801691-noauth.jpeg",
      "fullname": "Stefan Schweter",
      "name": "stefan-it",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2746
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.06694",
      "authors": [
        {
          "_id": "684ba6bc3b733ba333687093",
          "name": "Yuan Yuan",
          "hidden": false
        },
        {
          "_id": "684ba6bc3b733ba333687094",
          "name": "Yukun Liu",
          "hidden": false
        },
        {
          "_id": "684ba6bc3b733ba333687095",
          "name": "Chonghua Han",
          "hidden": false
        },
        {
          "_id": "684ba6bc3b733ba333687096",
          "user": {
            "_id": "6465d3bd63e7e09dd02e95c3",
            "avatarUrl": "/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg",
            "isPro": false,
            "fullname": "Jie Feng",
            "user": "JJ-TMT",
            "type": "user"
          },
          "name": "Jie Feng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:13.023Z",
          "hidden": false
        },
        {
          "_id": "684ba6bc3b733ba333687097",
          "name": "Yong Li",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6465d3bd63e7e09dd02e95c3/LfNEmKBa7cYZhNNRnZRIx.png"
      ],
      "publishedAt": "2025-06-07T07:19:11.000Z",
      "submittedOnDailyAt": "2025-06-13T02:53:49.430Z",
      "title": "Breaking Data Silos: Towards Open and Scalable Mobility Foundation\n  Models via Generative Continual Learning",
      "submittedOnDailyBy": {
        "_id": "6465d3bd63e7e09dd02e95c3",
        "avatarUrl": "/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg",
        "isPro": false,
        "fullname": "Jie Feng",
        "user": "JJ-TMT",
        "type": "user"
      },
      "summary": "Foundation models have revolutionized fields such as natural language\nprocessing and computer vision by enabling general-purpose learning across\ndiverse tasks and datasets. However, building analogous models for human\nmobility remains challenging due to the privacy-sensitive nature of mobility\ndata and the resulting data silos across institutions. To bridge this gap, we\npropose MoveGCL, a scalable and privacy-preserving framework for training\nmobility foundation models via generative continual learning. Without sharing\nraw data, MoveGCL enables decentralized and progressive model evolution by\nreplaying synthetic trajectories generated from a frozen teacher model, and\nreinforces knowledge retention through a tailored distillation strategy that\nmitigates catastrophic forgetting. To address the heterogeneity of mobility\npatterns, MoveGCL incorporates a Mixture-of-Experts Transformer with a\nmobility-aware expert routing mechanism, and employs a layer-wise progressive\nadaptation strategy to stabilize continual updates. Experiments on six\nreal-world urban datasets demonstrate that MoveGCL achieves performance\ncomparable to joint training and significantly outperforms federated learning\nbaselines, while offering strong privacy protection. MoveGCL marks a crucial\nstep toward unlocking foundation models for mobility, offering a practical\nblueprint for open, scalable, and privacy-preserving model development in the\nera of foundation models.",
      "upvotes": 2,
      "discussionId": "684ba6bc3b733ba333687098",
      "githubRepo": "https://github.com/ScottLiu2003/MoveGCL",
      "ai_summary": "MoveGCL is a privacy-preserving framework using generative continual learning and a Mixture-of-Experts Transformer for training mobility foundation models without sharing raw data.",
      "ai_keywords": [
        "generative continual learning",
        "privacy-preserving",
        "Mixture-of-Experts Transformer",
        "mobility-aware expert routing mechanism",
        "layer-wise progressive adaptation",
        "catastrophic forgetting",
        "federated learning"
      ]
    },
    "publishedAt": "2025-06-07T03:19:11.000Z",
    "title": "Breaking Data Silos: Towards Open and Scalable Mobility Foundation\n  Models via Generative Continual Learning",
    "summary": "Foundation models have revolutionized fields such as natural language\nprocessing and computer vision by enabling general-purpose learning across\ndiverse tasks and datasets. However, building analogous models for human\nmobility remains challenging due to the privacy-sensitive nature of mobility\ndata and the resulting data silos across institutions. To bridge this gap, we\npropose MoveGCL, a scalable and privacy-preserving framework for training\nmobility foundation models via generative continual learning. Without sharing\nraw data, MoveGCL enables decentralized and progressive model evolution by\nreplaying synthetic trajectories generated from a frozen teacher model, and\nreinforces knowledge retention through a tailored distillation strategy that\nmitigates catastrophic forgetting. To address the heterogeneity of mobility\npatterns, MoveGCL incorporates a Mixture-of-Experts Transformer with a\nmobility-aware expert routing mechanism, and employs a layer-wise progressive\nadaptation strategy to stabilize continual updates. Experiments on six\nreal-world urban datasets demonstrate that MoveGCL achieves performance\ncomparable to joint training and significantly outperforms federated learning\nbaselines, while offering strong privacy protection. MoveGCL marks a crucial\nstep toward unlocking foundation models for mobility, offering a practical\nblueprint for open, scalable, and privacy-preserving model development in the\nera of foundation models.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6465d3bd63e7e09dd02e95c3/LfNEmKBa7cYZhNNRnZRIx.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06694.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6465d3bd63e7e09dd02e95c3",
      "avatarUrl": "/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg",
      "fullname": "Jie Feng",
      "name": "JJ-TMT",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.10036",
      "authors": [
        {
          "_id": "684bc8db3b733ba33368718c",
          "name": "Javad Rajabi",
          "hidden": false
        },
        {
          "_id": "684bc8db3b733ba33368718d",
          "name": "Soroush Mehraban",
          "hidden": false
        },
        {
          "_id": "684bc8db3b733ba33368718e",
          "user": {
            "_id": "63b4b02a103617b0a5b0ee2e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b4b02a103617b0a5b0ee2e/LasBC-pCnPJ6XuCt6qqcp.jpeg",
            "isPro": false,
            "fullname": "Seyedmorteza Sadat",
            "user": "msadat97",
            "type": "user"
          },
          "name": "Seyedmorteza Sadat",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:38:32.584Z",
          "hidden": false
        },
        {
          "_id": "684bc8db3b733ba33368718f",
          "name": "Babak Taati",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-10T21:25:46.000Z",
      "submittedOnDailyAt": "2025-06-13T05:21:38.595Z",
      "title": "Token Perturbation Guidance for Diffusion Models",
      "submittedOnDailyBy": {
        "_id": "63b4b02a103617b0a5b0ee2e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b4b02a103617b0a5b0ee2e/LasBC-pCnPJ6XuCt6qqcp.jpeg",
        "isPro": false,
        "fullname": "Seyedmorteza Sadat",
        "user": "msadat97",
        "type": "user"
      },
      "summary": "Classifier-free guidance (CFG) has become an essential component of modern\ndiffusion models to enhance both generation quality and alignment with input\nconditions. However, CFG requires specific training procedures and is limited\nto conditional generation. To address these limitations, we propose Token\nPerturbation Guidance (TPG), a novel method that applies perturbation matrices\ndirectly to intermediate token representations within the diffusion network.\nTPG employs a norm-preserving shuffling operation to provide effective and\nstable guidance signals that improve generation quality without architectural\nchanges. As a result, TPG is training-free and agnostic to input conditions,\nmaking it readily applicable to both conditional and unconditional generation.\nWe further analyze the guidance term provided by TPG and show that its effect\non sampling more closely resembles CFG compared to existing training-free\nguidance techniques. Extensive experiments on SDXL and Stable Diffusion 2.1\nshow that TPG achieves nearly a 2times improvement in FID for unconditional\ngeneration over the SDXL baseline, while closely matching CFG in prompt\nalignment. These results establish TPG as a general, condition-agnostic\nguidance method that brings CFG-like benefits to a broader class of diffusion\nmodels. The code is available at\nhttps://github.com/TaatiTeam/Token-Perturbation-Guidance",
      "upvotes": 1,
      "discussionId": "684bc8db3b733ba333687190",
      "githubRepo": "https://github.com/TaatiTeam/Token-Perturbation-Guidance",
      "ai_summary": "Token Perturbation Guidance (TPG) enhances diffusion model generation quality without training, by perturbing intermediate token representations, achieving CFG-like performance and improving unconditional generation.",
      "ai_keywords": [
        "classifier-free guidance (CFG)",
        "diffusion models",
        "token perturbation guidance (TPG)",
        "perturbation matrices",
        "norm-preserving shuffling",
        "FID",
        "SDXL",
        "Stable Diffusion 2.1",
        "prompt alignment"
      ]
    },
    "publishedAt": "2025-06-10T17:25:46.000Z",
    "title": "Token Perturbation Guidance for Diffusion Models",
    "summary": "Classifier-free guidance (CFG) has become an essential component of modern\ndiffusion models to enhance both generation quality and alignment with input\nconditions. However, CFG requires specific training procedures and is limited\nto conditional generation. To address these limitations, we propose Token\nPerturbation Guidance (TPG), a novel method that applies perturbation matrices\ndirectly to intermediate token representations within the diffusion network.\nTPG employs a norm-preserving shuffling operation to provide effective and\nstable guidance signals that improve generation quality without architectural\nchanges. As a result, TPG is training-free and agnostic to input conditions,\nmaking it readily applicable to both conditional and unconditional generation.\nWe further analyze the guidance term provided by TPG and show that its effect\non sampling more closely resembles CFG compared to existing training-free\nguidance techniques. Extensive experiments on SDXL and Stable Diffusion 2.1\nshow that TPG achieves nearly a 2times improvement in FID for unconditional\ngeneration over the SDXL baseline, while closely matching CFG in prompt\nalignment. These results establish TPG as a general, condition-agnostic\nguidance method that brings CFG-like benefits to a broader class of diffusion\nmodels. The code is available at\nhttps://github.com/TaatiTeam/Token-Perturbation-Guidance",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10036.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63b4b02a103617b0a5b0ee2e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b4b02a103617b0a5b0ee2e/LasBC-pCnPJ6XuCt6qqcp.jpeg",
      "fullname": "Seyedmorteza Sadat",
      "name": "msadat97",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.08373",
      "authors": [
        {
          "_id": "684ae1f3dbd21a9cc27b0f32",
          "name": "Kevin Galim",
          "hidden": false
        },
        {
          "_id": "684ae1f3dbd21a9cc27b0f33",
          "name": "Ethan Ewer",
          "hidden": false
        },
        {
          "_id": "684ae1f3dbd21a9cc27b0f34",
          "name": "Wonjun Kang",
          "hidden": false
        },
        {
          "_id": "684ae1f3dbd21a9cc27b0f35",
          "name": "Minjae Lee",
          "hidden": false
        },
        {
          "_id": "684ae1f3dbd21a9cc27b0f36",
          "name": "Hyung Il Koo",
          "hidden": false
        },
        {
          "_id": "684ae1f3dbd21a9cc27b0f37",
          "name": "Kangwook Lee",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-10T02:37:46.000Z",
      "submittedOnDailyAt": "2025-06-13T00:35:36.456Z",
      "title": "Draft-based Approximate Inference for LLMs",
      "submittedOnDailyBy": {
        "_id": "630c90123dc31beba6e8f406",
        "avatarUrl": "/avatars/2188b41fff122d4f5683b46c529ed79d.svg",
        "isPro": false,
        "fullname": "Kevin Galim",
        "user": "kev95",
        "type": "user"
      },
      "summary": "Optimizing inference for long-context Large Language Models (LLMs) is\nincreasingly important due to the quadratic compute and linear memory\ncomplexity of Transformers. Existing approximation methods, such as key-value\n(KV) cache dropping, sparse attention, and prompt compression, typically rely\non rough predictions of token or KV pair importance. We propose a novel\nframework for approximate LLM inference that leverages small draft models to\nmore accurately predict the importance of tokens and KV pairs. Specifically, we\nintroduce two instantiations of our proposed framework: (i) SpecKV, which\nleverages a draft output to accurately assess the importance of each KV pair\nfor more effective KV cache dropping, and (ii) SpecPC, which uses the draft\nmodel's attention activations to identify and discard unimportant prompt\ntokens. To the best of our knowledge, this is the first work to use draft\nmodels for approximate LLM inference acceleration, extending their utility\nbeyond traditional lossless speculative decoding. We motivate our methods with\ntheoretical and empirical analyses, and show a strong correlation between the\nattention patterns of draft and target models. Extensive experiments on\nlong-context benchmarks show that our methods consistently achieve higher\naccuracy than existing baselines, while preserving the same improvements in\nmemory usage, latency, and throughput. Our code is available at\nhttps://github.com/furiosa-ai/draft-based-approx-llm.",
      "upvotes": 1,
      "discussionId": "684ae1f3dbd21a9cc27b0f38",
      "ai_summary": "A new framework using draft models enhances approximate inference for long-context LLMs by better predicting token and key-value pair importance, improving accuracy while maintaining memory and compute efficiency.",
      "ai_keywords": [
        "Large Language Models",
        "Transformers",
        "key-value cache dropping",
        "sparse attention",
        "prompt compression",
        "draft models",
        "SpecKV",
        "SpecPC",
        "attention activations",
        "long-context benchmarks"
      ]
    },
    "publishedAt": "2025-06-09T22:37:46.000Z",
    "title": "Draft-based Approximate Inference for LLMs",
    "summary": "Optimizing inference for long-context Large Language Models (LLMs) is\nincreasingly important due to the quadratic compute and linear memory\ncomplexity of Transformers. Existing approximation methods, such as key-value\n(KV) cache dropping, sparse attention, and prompt compression, typically rely\non rough predictions of token or KV pair importance. We propose a novel\nframework for approximate LLM inference that leverages small draft models to\nmore accurately predict the importance of tokens and KV pairs. Specifically, we\nintroduce two instantiations of our proposed framework: (i) SpecKV, which\nleverages a draft output to accurately assess the importance of each KV pair\nfor more effective KV cache dropping, and (ii) SpecPC, which uses the draft\nmodel's attention activations to identify and discard unimportant prompt\ntokens. To the best of our knowledge, this is the first work to use draft\nmodels for approximate LLM inference acceleration, extending their utility\nbeyond traditional lossless speculative decoding. We motivate our methods with\ntheoretical and empirical analyses, and show a strong correlation between the\nattention patterns of draft and target models. Extensive experiments on\nlong-context benchmarks show that our methods consistently achieve higher\naccuracy than existing baselines, while preserving the same improvements in\nmemory usage, latency, and throughput. Our code is available at\nhttps://github.com/furiosa-ai/draft-based-approx-llm.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08373.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "630c90123dc31beba6e8f406",
      "avatarUrl": "/avatars/2188b41fff122d4f5683b46c529ed79d.svg",
      "fullname": "Kevin Galim",
      "name": "kev95",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.07795",
      "authors": [
        {
          "_id": "6848dca942e4f9106973f25c",
          "user": {
            "_id": "6659b410a69183808d04b22f",
            "avatarUrl": "/avatars/a16d1fed9ef87163fe458b10c477140b.svg",
            "isPro": false,
            "fullname": "Xiaotian Ye",
            "user": "Acruxos",
            "type": "user"
          },
          "name": "Xiaotian Ye",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-11T08:35:10.086Z",
          "hidden": false
        },
        {
          "_id": "6848dca942e4f9106973f25d",
          "name": "Mengqi Zhang",
          "hidden": false
        },
        {
          "_id": "6848dca942e4f9106973f25e",
          "name": "Shu Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-09T14:21:25.000Z",
      "submittedOnDailyAt": "2025-06-13T05:51:30.395Z",
      "title": "LLM Unlearning Should Be Form-Independent",
      "submittedOnDailyBy": {
        "_id": "6659b410a69183808d04b22f",
        "avatarUrl": "/avatars/a16d1fed9ef87163fe458b10c477140b.svg",
        "isPro": false,
        "fullname": "Xiaotian Ye",
        "user": "Acruxos",
        "type": "user"
      },
      "summary": "Large Language Model (LLM) unlearning aims to erase or suppress undesirable\nknowledge within the model, offering promise for controlling harmful or private\ninformation to prevent misuse. However, recent studies highlight its limited\nefficacy in real-world scenarios, hindering practical adoption. In this study,\nwe identify a pervasive issue underlying many downstream failures: the\neffectiveness of existing unlearning methods heavily depends on the form of\ntraining samples and frequently fails to generalize to alternate expressions of\nthe same knowledge. We formally characterize this problem as Form-Dependent\nBias and systematically investigate its specific manifestation patterns across\nvarious downstream tasks. To quantify its prevalence and support future\nresearch, we introduce ORT, a novel benchmark designed to evaluate the\nrobustness of unlearning methods against variations in knowledge expression.\nResults reveal that Form-Dependent Bias is both widespread and severe among\ncurrent techniques.\n  We argue that LLM unlearning should be form-independent to address the\nendless forms of downstream tasks encountered in real-world security-critical\nscenarios. Towards this goal, we introduce Rank-one Concept Redirection (ROCR),\na novel training-free method, as a promising solution path. ROCR performs\nunlearning by targeting the invariants in downstream tasks, specifically the\nactivated dangerous concepts. It is capable of modifying model parameters\nwithin seconds to redirect the model's perception of a specific unlearning\ntarget concept to another harmless concept. Extensive experiments demonstrate\nthat ROCR significantly improves unlearning effectiveness compared to\ntraditional methods while generating highly natural outputs.",
      "upvotes": 1,
      "discussionId": "6848dca942e4f9106973f25f",
      "ai_summary": "Form-Dependent Bias limits the effectiveness of LLM unlearning across different knowledge expressions, and Rank-one Concept Redirection (ROCR) is proposed as a form-independent solution that enhances unlearning efficacy.",
      "ai_keywords": [
        "Large Language Model (LLM)",
        "unlearning",
        "Form-Dependent Bias",
        "ORT",
        "Rank-one Concept Redirection (ROCR)",
        "downstream tasks",
        "unlearning methods",
        "concept redirection",
        "model parameters",
        "activated dangerous concepts"
      ]
    },
    "publishedAt": "2025-06-09T10:21:25.000Z",
    "title": "LLM Unlearning Should Be Form-Independent",
    "summary": "Large Language Model (LLM) unlearning aims to erase or suppress undesirable\nknowledge within the model, offering promise for controlling harmful or private\ninformation to prevent misuse. However, recent studies highlight its limited\nefficacy in real-world scenarios, hindering practical adoption. In this study,\nwe identify a pervasive issue underlying many downstream failures: the\neffectiveness of existing unlearning methods heavily depends on the form of\ntraining samples and frequently fails to generalize to alternate expressions of\nthe same knowledge. We formally characterize this problem as Form-Dependent\nBias and systematically investigate its specific manifestation patterns across\nvarious downstream tasks. To quantify its prevalence and support future\nresearch, we introduce ORT, a novel benchmark designed to evaluate the\nrobustness of unlearning methods against variations in knowledge expression.\nResults reveal that Form-Dependent Bias is both widespread and severe among\ncurrent techniques.\n  We argue that LLM unlearning should be form-independent to address the\nendless forms of downstream tasks encountered in real-world security-critical\nscenarios. Towards this goal, we introduce Rank-one Concept Redirection (ROCR),\na novel training-free method, as a promising solution path. ROCR performs\nunlearning by targeting the invariants in downstream tasks, specifically the\nactivated dangerous concepts. It is capable of modifying model parameters\nwithin seconds to redirect the model's perception of a specific unlearning\ntarget concept to another harmless concept. Extensive experiments demonstrate\nthat ROCR significantly improves unlearning effectiveness compared to\ntraditional methods while generating highly natural outputs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07795.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6659b410a69183808d04b22f",
      "avatarUrl": "/avatars/a16d1fed9ef87163fe458b10c477140b.svg",
      "fullname": "Xiaotian Ye",
      "name": "Acruxos",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.06950",
      "authors": [
        {
          "_id": "684ae1fbdbd21a9cc27b0f51",
          "name": "Do Xuan Long",
          "hidden": false
        },
        {
          "_id": "684ae1fbdbd21a9cc27b0f52",
          "name": "Duy Dinh",
          "hidden": false
        },
        {
          "_id": "684ae1fbdbd21a9cc27b0f53",
          "name": "Ngoc-Hai Nguyen",
          "hidden": false
        },
        {
          "_id": "684ae1fbdbd21a9cc27b0f54",
          "name": "Kenji Kawaguchi",
          "hidden": false
        },
        {
          "_id": "684ae1fbdbd21a9cc27b0f55",
          "name": "Nancy F. Chen",
          "hidden": false
        },
        {
          "_id": "684ae1fbdbd21a9cc27b0f56",
          "name": "Shafiq Joty",
          "hidden": false
        },
        {
          "_id": "684ae1fbdbd21a9cc27b0f57",
          "name": "Min-Yen Kan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-07T23:19:27.000Z",
      "submittedOnDailyAt": "2025-06-13T03:14:57.042Z",
      "title": "What Makes a Good Natural Language Prompt?",
      "submittedOnDailyBy": {
        "_id": "63a9a0d13453852ef53c0b37",
        "avatarUrl": "/avatars/411c4ffc2a3e89047a23c6e7442f6ed5.svg",
        "isPro": false,
        "fullname": "Do Xuan Long",
        "user": "dxlong2000",
        "type": "user"
      },
      "summary": "As large language models (LLMs) have progressed towards more human-like and\nhuman--AI communications have become prevalent, prompting has emerged as a\ndecisive component. However, there is limited conceptual consensus on what\nexactly quantifies natural language prompts. We attempt to address this\nquestion by conducting a meta-analysis surveying more than 150\nprompting-related papers from leading NLP and AI conferences from 2022 to 2025\nand blogs. We propose a property- and human-centric framework for evaluating\nprompt quality, encompassing 21 properties categorized into six dimensions. We\nthen examine how existing studies assess their impact on LLMs, revealing their\nimbalanced support across models and tasks, and substantial research gaps.\nFurther, we analyze correlations among properties in high-quality natural\nlanguage prompts, deriving prompting recommendations. We then empirically\nexplore multi-property prompt enhancements in reasoning tasks, observing that\nsingle-property enhancements often have the greatest impact. Finally, we\ndiscover that instruction-tuning on property-enhanced prompts can result in\nbetter reasoning models. Our findings establish a foundation for\nproperty-centric prompt evaluation and optimization, bridging the gaps between\nhuman--AI communication and opening new prompting research directions.",
      "upvotes": 1,
      "discussionId": "684ae1fbdbd21a9cc27b0f58",
      "ai_summary": "A framework for evaluating and optimizing natural language prompts in large language models is proposed, revealing correlations between prompt properties and their impact on reasoning tasks.",
      "ai_keywords": [
        "large language models",
        "prompting",
        "meta-analysis",
        "property-centric framework",
        "instruction-tuning",
        "reasoning tasks"
      ]
    },
    "publishedAt": "2025-06-07T19:19:27.000Z",
    "title": "What Makes a Good Natural Language Prompt?",
    "summary": "As large language models (LLMs) have progressed towards more human-like and\nhuman--AI communications have become prevalent, prompting has emerged as a\ndecisive component. However, there is limited conceptual consensus on what\nexactly quantifies natural language prompts. We attempt to address this\nquestion by conducting a meta-analysis surveying more than 150\nprompting-related papers from leading NLP and AI conferences from 2022 to 2025\nand blogs. We propose a property- and human-centric framework for evaluating\nprompt quality, encompassing 21 properties categorized into six dimensions. We\nthen examine how existing studies assess their impact on LLMs, revealing their\nimbalanced support across models and tasks, and substantial research gaps.\nFurther, we analyze correlations among properties in high-quality natural\nlanguage prompts, deriving prompting recommendations. We then empirically\nexplore multi-property prompt enhancements in reasoning tasks, observing that\nsingle-property enhancements often have the greatest impact. Finally, we\ndiscover that instruction-tuning on property-enhanced prompts can result in\nbetter reasoning models. Our findings establish a foundation for\nproperty-centric prompt evaluation and optimization, bridging the gaps between\nhuman--AI communication and opening new prompting research directions.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06950.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a9a0d13453852ef53c0b37",
      "avatarUrl": "/avatars/411c4ffc2a3e89047a23c6e7442f6ed5.svg",
      "fullname": "Do Xuan Long",
      "name": "dxlong2000",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.06561",
      "authors": [
        {
          "_id": "684b88113b733ba333686fc7",
          "name": "Ho Yin 'Sam' Ng",
          "hidden": false
        },
        {
          "_id": "684b88113b733ba333686fc8",
          "name": "Ting-Yao Hsu",
          "hidden": false
        },
        {
          "_id": "684b88113b733ba333686fc9",
          "name": "Aashish Anantha Ramakrishnan",
          "hidden": false
        },
        {
          "_id": "684b88113b733ba333686fca",
          "name": "Branislav Kveton",
          "hidden": false
        },
        {
          "_id": "684b88113b733ba333686fcb",
          "name": "Nedim Lipka",
          "hidden": false
        },
        {
          "_id": "684b88113b733ba333686fcc",
          "user": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "isPro": false,
            "fullname": "Franck Dernoncourt",
            "user": "Franck-Dernoncourt",
            "type": "user"
          },
          "name": "Franck Dernoncourt",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:49.692Z",
          "hidden": false
        },
        {
          "_id": "684b88113b733ba333686fcd",
          "name": "Dongwon Lee",
          "hidden": false
        },
        {
          "_id": "684b88113b733ba333686fce",
          "name": "Tong Yu",
          "hidden": false
        },
        {
          "_id": "684b88113b733ba333686fcf",
          "name": "Sungchul Kim",
          "hidden": false
        },
        {
          "_id": "684b88113b733ba333686fd0",
          "name": "Ryan A. Rossi",
          "hidden": false
        },
        {
          "_id": "684b88113b733ba333686fd1",
          "name": "Ting-Hao 'Kenneth' Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-06T22:16:16.000Z",
      "submittedOnDailyAt": "2025-06-13T00:38:33.715Z",
      "title": "LaMP-Cap: Personalized Figure Caption Generation With Multimodal Figure\n  Profiles",
      "submittedOnDailyBy": {
        "_id": "62c5947524171688a9feb992",
        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
        "isPro": false,
        "fullname": "Franck Dernoncourt",
        "user": "Franck-Dernoncourt",
        "type": "user"
      },
      "summary": "Figure captions are crucial for helping readers understand and remember a\nfigure's key message. Many models have been developed to generate these\ncaptions, helping authors compose better quality captions more easily. Yet,\nauthors almost always need to revise generic AI-generated captions to match\ntheir writing style and the domain's style, highlighting the need for\npersonalization. Despite language models' personalization (LaMP) advances,\nthese technologies often focus on text-only settings and rarely address\nscenarios where both inputs and profiles are multimodal. This paper introduces\nLaMP-Cap, a dataset for personalized figure caption generation with multimodal\nfigure profiles. For each target figure, LaMP-Cap provides not only the needed\ninputs, such as figure images, but also up to three other figures from the same\ndocument--each with its image, caption, and figure-mentioning paragraphs--as a\nprofile to characterize the context. Experiments with four LLMs show that using\nprofile information consistently helps generate captions closer to the original\nauthor-written ones. Ablation studies reveal that images in the profile are\nmore helpful than figure-mentioning paragraphs, highlighting the advantage of\nusing multimodal profiles over text-only ones.",
      "upvotes": 1,
      "discussionId": "684b88123b733ba333686fd2",
      "ai_summary": "LaMP-Cap introduces a dataset for personalized figure caption generation using multimodal profiles to improve the quality of AI-generated captions.",
      "ai_keywords": [
        "LaMP-Cap",
        "personalized figure caption generation",
        "multimodal figures",
        "figure profiles",
        "ablation studies"
      ]
    },
    "publishedAt": "2025-06-06T18:16:16.000Z",
    "title": "LaMP-Cap: Personalized Figure Caption Generation With Multimodal Figure\n  Profiles",
    "summary": "Figure captions are crucial for helping readers understand and remember a\nfigure's key message. Many models have been developed to generate these\ncaptions, helping authors compose better quality captions more easily. Yet,\nauthors almost always need to revise generic AI-generated captions to match\ntheir writing style and the domain's style, highlighting the need for\npersonalization. Despite language models' personalization (LaMP) advances,\nthese technologies often focus on text-only settings and rarely address\nscenarios where both inputs and profiles are multimodal. This paper introduces\nLaMP-Cap, a dataset for personalized figure caption generation with multimodal\nfigure profiles. For each target figure, LaMP-Cap provides not only the needed\ninputs, such as figure images, but also up to three other figures from the same\ndocument--each with its image, caption, and figure-mentioning paragraphs--as a\nprofile to characterize the context. Experiments with four LLMs show that using\nprofile information consistently helps generate captions closer to the original\nauthor-written ones. Ablation studies reveal that images in the profile are\nmore helpful than figure-mentioning paragraphs, highlighting the advantage of\nusing multimodal profiles over text-only ones.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06561.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c5947524171688a9feb992",
      "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
      "fullname": "Franck Dernoncourt",
      "name": "Franck-Dernoncourt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.05982",
      "authors": [
        {
          "_id": "684b86913b733ba333686fb8",
          "name": "Zonglin Wu",
          "hidden": false
        },
        {
          "_id": "684b86913b733ba333686fb9",
          "name": "Yule Xue",
          "hidden": false
        },
        {
          "_id": "684b86913b733ba333686fba",
          "name": "Xin Wei",
          "hidden": false
        },
        {
          "_id": "684b86913b733ba333686fbb",
          "name": "Yiren Song",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-06T11:02:01.000Z",
      "submittedOnDailyAt": "2025-06-13T00:33:34.648Z",
      "title": "MCA-Bench: A Multimodal Benchmark for Evaluating CAPTCHA Robustness\n  Against VLM-based Attacks",
      "submittedOnDailyBy": {
        "_id": "64311a95034ecbefddd141ef",
        "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
        "isPro": true,
        "fullname": "Yiren Song",
        "user": "yiren98",
        "type": "user"
      },
      "summary": "As automated attack techniques rapidly advance, CAPTCHAs remain a critical\ndefense mechanism against malicious bots. However, existing CAPTCHA schemes\nencompass a diverse range of modalities -- from static distorted text and\nobfuscated images to interactive clicks, sliding puzzles, and logic-based\nquestions -- yet the community still lacks a unified, large-scale, multimodal\nbenchmark to rigorously evaluate their security robustness. To address this\ngap, we introduce MCA-Bench, a comprehensive and reproducible benchmarking\nsuite that integrates heterogeneous CAPTCHA types into a single evaluation\nprotocol. Leveraging a shared vision-language model backbone, we fine-tune\nspecialized cracking agents for each CAPTCHA category, enabling consistent,\ncross-modal assessments. Extensive experiments reveal that MCA-Bench\neffectively maps the vulnerability spectrum of modern CAPTCHA designs under\nvaried attack settings, and crucially offers the first quantitative analysis of\nhow challenge complexity, interaction depth, and model solvability interrelate.\nBased on these findings, we propose three actionable design principles and\nidentify key open challenges, laying the groundwork for systematic CAPTCHA\nhardening, fair benchmarking, and broader community collaboration. Datasets and\ncode are available online.",
      "upvotes": 1,
      "discussionId": "684b86923b733ba333686fbc",
      "ai_summary": "MCA-Bench provides a unified benchmark for evaluating CAPTCHA security using a shared vision-language model and attackers specialized for each type of CAPTCHA.",
      "ai_keywords": [
        "vision-language model",
        "CAPTCHA",
        "benchmark",
        "evaluation protocol",
        "cracking agents",
        "vulnerability spectrum",
        "challenge complexity",
        "interaction depth",
        "model solvability"
      ]
    },
    "publishedAt": "2025-06-06T07:02:01.000Z",
    "title": "MCA-Bench: A Multimodal Benchmark for Evaluating CAPTCHA Robustness\n  Against VLM-based Attacks",
    "summary": "As automated attack techniques rapidly advance, CAPTCHAs remain a critical\ndefense mechanism against malicious bots. However, existing CAPTCHA schemes\nencompass a diverse range of modalities -- from static distorted text and\nobfuscated images to interactive clicks, sliding puzzles, and logic-based\nquestions -- yet the community still lacks a unified, large-scale, multimodal\nbenchmark to rigorously evaluate their security robustness. To address this\ngap, we introduce MCA-Bench, a comprehensive and reproducible benchmarking\nsuite that integrates heterogeneous CAPTCHA types into a single evaluation\nprotocol. Leveraging a shared vision-language model backbone, we fine-tune\nspecialized cracking agents for each CAPTCHA category, enabling consistent,\ncross-modal assessments. Extensive experiments reveal that MCA-Bench\neffectively maps the vulnerability spectrum of modern CAPTCHA designs under\nvaried attack settings, and crucially offers the first quantitative analysis of\nhow challenge complexity, interaction depth, and model solvability interrelate.\nBased on these findings, we propose three actionable design principles and\nidentify key open challenges, laying the groundwork for systematic CAPTCHA\nhardening, fair benchmarking, and broader community collaboration. Datasets and\ncode are available online.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05982.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64311a95034ecbefddd141ef",
      "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
      "fullname": "Yiren Song",
      "name": "yiren98",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 22
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.10378",
      "authors": [
        {
          "_id": "684bb08e3b733ba3336870bf",
          "name": "Jikai Jin",
          "hidden": false
        },
        {
          "_id": "684bb08e3b733ba3336870c0",
          "name": "Vasilis Syrgkanis",
          "hidden": false
        },
        {
          "_id": "684bb08e3b733ba3336870c1",
          "name": "Sham Kakade",
          "hidden": false
        },
        {
          "_id": "684bb08e3b733ba3336870c2",
          "name": "Hanlin Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T06:07:42.000Z",
      "submittedOnDailyAt": "2025-06-13T03:36:14.455Z",
      "title": "Discovering Hierarchical Latent Capabilities of Language Models via\n  Causal Representation Learning",
      "submittedOnDailyBy": {
        "_id": "624054bcc2c17da6a63eb539",
        "avatarUrl": "/avatars/bf52dc0683b4100733f8696a97696d0e.svg",
        "isPro": false,
        "fullname": "hlzhang109",
        "user": "hlzhang109",
        "type": "user"
      },
      "summary": "Faithful evaluation of language model capabilities is crucial for deriving\nactionable insights that can inform model development. However, rigorous causal\nevaluations in this domain face significant methodological challenges,\nincluding complex confounding effects and prohibitive computational costs\nassociated with extensive retraining. To tackle these challenges, we propose a\ncausal representation learning framework wherein observed benchmark performance\nis modeled as a linear transformation of a few latent capability factors.\nCrucially, these latent factors are identified as causally interrelated after\nappropriately controlling for the base model as a common confounder. Applying\nthis approach to a comprehensive dataset encompassing over 1500 models\nevaluated across six benchmarks from the Open LLM Leaderboard, we identify a\nconcise three-node linear causal structure that reliably explains the observed\nperformance variations. Further interpretation of this causal structure\nprovides substantial scientific insights beyond simple numerical rankings:\nspecifically, we reveal a clear causal direction starting from general\nproblem-solving capabilities, advancing through instruction-following\nproficiency, and culminating in mathematical reasoning ability. Our results\nunderscore the essential role of carefully controlling base model variations\nduring evaluation, a step critical to accurately uncovering the underlying\ncausal relationships among latent model capabilities.",
      "upvotes": 0,
      "discussionId": "684bb08e3b733ba3336870c3",
      "ai_summary": "The study proposes a causal representation learning framework to evaluate language model capabilities through latent factors, emphasizing the importance of controlling for base model variations to uncover underlying causal relationships.",
      "ai_keywords": [
        "causal representation learning",
        "linear transformation",
        "latent capability factors",
        "causal interrelated",
        "common confounder",
        "causal structure",
        "general problem-solving capabilities",
        "instruction-following proficiency",
        "mathematical reasoning ability"
      ]
    },
    "publishedAt": "2025-06-12T02:07:42.000Z",
    "title": "Discovering Hierarchical Latent Capabilities of Language Models via\n  Causal Representation Learning",
    "summary": "Faithful evaluation of language model capabilities is crucial for deriving\nactionable insights that can inform model development. However, rigorous causal\nevaluations in this domain face significant methodological challenges,\nincluding complex confounding effects and prohibitive computational costs\nassociated with extensive retraining. To tackle these challenges, we propose a\ncausal representation learning framework wherein observed benchmark performance\nis modeled as a linear transformation of a few latent capability factors.\nCrucially, these latent factors are identified as causally interrelated after\nappropriately controlling for the base model as a common confounder. Applying\nthis approach to a comprehensive dataset encompassing over 1500 models\nevaluated across six benchmarks from the Open LLM Leaderboard, we identify a\nconcise three-node linear causal structure that reliably explains the observed\nperformance variations. Further interpretation of this causal structure\nprovides substantial scientific insights beyond simple numerical rankings:\nspecifically, we reveal a clear causal direction starting from general\nproblem-solving capabilities, advancing through instruction-following\nproficiency, and culminating in mathematical reasoning ability. Our results\nunderscore the essential role of carefully controlling base model variations\nduring evaluation, a step critical to accurately uncovering the underlying\ncausal relationships among latent model capabilities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10378.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "624054bcc2c17da6a63eb539",
      "avatarUrl": "/avatars/bf52dc0683b4100733f8696a97696d0e.svg",
      "fullname": "hlzhang109",
      "name": "hlzhang109",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  }
]