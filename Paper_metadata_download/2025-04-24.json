[
  {
    "paper": {
      "id": "2504.15279",
      "authors": [
        {
          "_id": "68070d3b5035e6d88636ae13",
          "name": "Weiye Xu",
          "hidden": false
        },
        {
          "_id": "68070d3b5035e6d88636ae14",
          "name": "Jiahao Wang",
          "hidden": false
        },
        {
          "_id": "68070d3b5035e6d88636ae15",
          "name": "Weiyun Wang",
          "hidden": false
        },
        {
          "_id": "68070d3b5035e6d88636ae16",
          "name": "Zhe Chen",
          "hidden": false
        },
        {
          "_id": "68070d3b5035e6d88636ae17",
          "name": "Wengang Zhou",
          "hidden": false
        },
        {
          "_id": "68070d3b5035e6d88636ae18",
          "name": "Aijun Yang",
          "hidden": false
        },
        {
          "_id": "68070d3b5035e6d88636ae19",
          "name": "Lewei Lu",
          "hidden": false
        },
        {
          "_id": "68070d3b5035e6d88636ae1a",
          "name": "Houqiang Li",
          "hidden": false
        },
        {
          "_id": "68070d3b5035e6d88636ae1b",
          "name": "Xiaohua Wang",
          "hidden": false
        },
        {
          "_id": "68070d3b5035e6d88636ae1c",
          "name": "Xizhou Zhu",
          "hidden": false
        },
        {
          "_id": "68070d3b5035e6d88636ae1d",
          "name": "Wenhai Wang",
          "hidden": false
        },
        {
          "_id": "68070d3b5035e6d88636ae1e",
          "name": "Jifeng Dai",
          "hidden": false
        },
        {
          "_id": "68070d3b5035e6d88636ae1f",
          "name": "Jinguo Zhu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-21T17:59:53.000Z",
      "submittedOnDailyAt": "2025-04-24T01:22:54.990Z",
      "title": "VisuLogic: A Benchmark for Evaluating Visual Reasoning in Multi-modal\n  Large Language Models",
      "submittedOnDailyBy": {
        "_id": "664b4a748dd1bfb5a3a970fe",
        "avatarUrl": "/avatars/37aa9332ab3e8fbb6ae30b875a7e0e5a.svg",
        "isPro": false,
        "fullname": "Jiahao Wang",
        "user": "GenuineWWD",
        "type": "user"
      },
      "summary": "Visual reasoning is a core component of human intelligence and a critical\ncapability for advanced multimodal models. Yet current reasoning evaluations of\nmultimodal large language models (MLLMs) often rely on text descriptions and\nallow language-based reasoning shortcuts, failing to measure genuine\nvision-centric reasoning. To address this, we introduce VisuLogic: a benchmark\nof 1,000 human-verified problems across six categories (e.g., quantitative\nshifts, spatial relations, attribute comparisons). These various types of\nquestions can be evaluated to assess the visual reasoning capabilities of MLLMs\nfrom multiple perspectives. We evaluate leading MLLMs on this benchmark and\nanalyze their results to identify common failure modes. Most models score below\n30% accuracy-only slightly above the 25% random baseline and far below the\n51.4% achieved by humans-revealing significant gaps in visual reasoning.\nFurthermore, we provide a supplementary training dataset and a\nreinforcement-learning baseline to support further progress.",
      "upvotes": 42,
      "discussionId": "68070d3f5035e6d88636af56",
      "projectPage": "https://visulogic-benchmark.github.io/VisuLogic/",
      "githubRepo": "https://github.com/VisuLogic-Benchmark/VisuLogic-Eval",
      "ai_keywords": [
        "Visual reasoning",
        "multimodal large language models (MLLMs)",
        "text descriptions",
        "language-based reasoning shortcuts",
        "genuine vision-centric reasoning",
        "VisuLogic",
        "human-verified problems",
        "quantitative shifts",
        "spatial relations",
        "attribute comparisons",
        "supplementary training dataset",
        "reinforcement-learning baseline"
      ]
    },
    "publishedAt": "2025-04-21T13:59:53.000Z",
    "title": "VisuLogic: A Benchmark for Evaluating Visual Reasoning in Multi-modal\n  Large Language Models",
    "summary": "Visual reasoning is a core component of human intelligence and a critical\ncapability for advanced multimodal models. Yet current reasoning evaluations of\nmultimodal large language models (MLLMs) often rely on text descriptions and\nallow language-based reasoning shortcuts, failing to measure genuine\nvision-centric reasoning. To address this, we introduce VisuLogic: a benchmark\nof 1,000 human-verified problems across six categories (e.g., quantitative\nshifts, spatial relations, attribute comparisons). These various types of\nquestions can be evaluated to assess the visual reasoning capabilities of MLLMs\nfrom multiple perspectives. We evaluate leading MLLMs on this benchmark and\nanalyze their results to identify common failure modes. Most models score below\n30% accuracy-only slightly above the 25% random baseline and far below the\n51.4% achieved by humans-revealing significant gaps in visual reasoning.\nFurthermore, we provide a supplementary training dataset and a\nreinforcement-learning baseline to support further progress.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15279.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "664b4a748dd1bfb5a3a970fe",
      "avatarUrl": "/avatars/37aa9332ab3e8fbb6ae30b875a7e0e5a.svg",
      "fullname": "Jiahao Wang",
      "name": "GenuineWWD",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.15431",
      "authors": [
        {
          "_id": "680879ead6dc8bf64565c975",
          "name": "Sungjun Han",
          "hidden": false
        },
        {
          "_id": "680879ead6dc8bf64565c976",
          "user": {
            "_id": "6138cc1306dd10833d2db64b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6138cc1306dd10833d2db64b/IRX4y-8M4YlzR_8jOwkKp.jpeg",
            "isPro": false,
            "fullname": "Juyoung Suk",
            "user": "scottsuk0306",
            "type": "user"
          },
          "name": "Juyoung Suk",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-23T08:08:21.257Z",
          "hidden": false
        },
        {
          "_id": "680879ead6dc8bf64565c977",
          "name": "Suyeong An",
          "hidden": false
        },
        {
          "_id": "680879ead6dc8bf64565c978",
          "name": "Hyungguk Kim",
          "hidden": false
        },
        {
          "_id": "680879ead6dc8bf64565c979",
          "name": "Kyuseok Kim",
          "hidden": false
        },
        {
          "_id": "680879ead6dc8bf64565c97a",
          "name": "Wonsuk Yang",
          "hidden": false
        },
        {
          "_id": "680879ead6dc8bf64565c97b",
          "name": "Seungtaek Choi",
          "hidden": false
        },
        {
          "_id": "680879ead6dc8bf64565c97c",
          "name": "Jamin Shin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-21T20:54:44.000Z",
      "submittedOnDailyAt": "2025-04-24T01:09:32.264Z",
      "title": "Trillion 7B Technical Report",
      "submittedOnDailyBy": {
        "_id": "6138cc1306dd10833d2db64b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6138cc1306dd10833d2db64b/IRX4y-8M4YlzR_8jOwkKp.jpeg",
        "isPro": false,
        "fullname": "Juyoung Suk",
        "user": "scottsuk0306",
        "type": "user"
      },
      "summary": "We introduce Trillion-7B, the most token-efficient Korean-centric\nmultilingual LLM available. Our novel Cross-lingual Document Attention (XLDA)\nmechanism enables highly efficient and effective knowledge transfer from\nEnglish to target languages like Korean and Japanese. Combined with optimized\ndata mixtures, language-specific filtering, and tailored tokenizer\nconstruction, Trillion-7B achieves competitive performance while dedicating\nonly 10\\% of its 2T training tokens to multilingual data and requiring just\n59.4K H100 GPU hours (\\$148K) for full training. Comprehensive evaluations\nacross 27 benchmarks in four languages demonstrate Trillion-7B's robust\nmultilingual performance and exceptional cross-lingual consistency.",
      "upvotes": 17,
      "discussionId": "680879ebd6dc8bf64565c9bb",
      "ai_keywords": [
        "Trillion-7B",
        "Cross-lingual Document Attention (XLDA)",
        "language-specific filtering",
        "tailored tokenizer construction",
        "multilingual data",
        "multilingual performance",
        "cross-lingual consistency"
      ]
    },
    "publishedAt": "2025-04-21T16:54:44.000Z",
    "title": "Trillion 7B Technical Report",
    "summary": "We introduce Trillion-7B, the most token-efficient Korean-centric\nmultilingual LLM available. Our novel Cross-lingual Document Attention (XLDA)\nmechanism enables highly efficient and effective knowledge transfer from\nEnglish to target languages like Korean and Japanese. Combined with optimized\ndata mixtures, language-specific filtering, and tailored tokenizer\nconstruction, Trillion-7B achieves competitive performance while dedicating\nonly 10\\% of its 2T training tokens to multilingual data and requiring just\n59.4K H100 GPU hours (\\$148K) for full training. Comprehensive evaluations\nacross 27 benchmarks in four languages demonstrate Trillion-7B's robust\nmultilingual performance and exceptional cross-lingual consistency.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15431.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6138cc1306dd10833d2db64b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6138cc1306dd10833d2db64b/IRX4y-8M4YlzR_8jOwkKp.jpeg",
      "fullname": "Juyoung Suk",
      "name": "scottsuk0306",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.14509",
      "authors": [
        {
          "_id": "6809dd092e04f68a3f5baa66",
          "name": "Fulong Ye",
          "hidden": false
        },
        {
          "_id": "6809dd092e04f68a3f5baa67",
          "name": "Miao Hua",
          "hidden": false
        },
        {
          "_id": "6809dd092e04f68a3f5baa68",
          "name": "Pengze Zhang",
          "hidden": false
        },
        {
          "_id": "6809dd092e04f68a3f5baa69",
          "name": "Xinghui Li",
          "hidden": false
        },
        {
          "_id": "6809dd092e04f68a3f5baa6a",
          "name": "Qichao Sun",
          "hidden": false
        },
        {
          "_id": "6809dd092e04f68a3f5baa6b",
          "name": "Songtao Zhao",
          "hidden": false
        },
        {
          "_id": "6809dd092e04f68a3f5baa6c",
          "name": "Qian He",
          "hidden": false
        },
        {
          "_id": "6809dd092e04f68a3f5baa6d",
          "name": "Xinglong Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-20T06:53:00.000Z",
      "submittedOnDailyAt": "2025-04-24T05:26:05.811Z",
      "title": "DreamID: High-Fidelity and Fast diffusion-based Face Swapping via\n  Triplet ID Group Learning",
      "submittedOnDailyBy": {
        "_id": "6339029a76421c0543167075",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6339029a76421c0543167075/3npT0NxTMV-MLWDThyBy8.png",
        "isPro": false,
        "fullname": "fulong ye",
        "user": "Alon77777",
        "type": "user"
      },
      "summary": "In this paper, we introduce DreamID, a diffusion-based face swapping model\nthat achieves high levels of ID similarity, attribute preservation, image\nfidelity, and fast inference speed. Unlike the typical face swapping training\nprocess, which often relies on implicit supervision and struggles to achieve\nsatisfactory results. DreamID establishes explicit supervision for face\nswapping by constructing Triplet ID Group data, significantly enhancing\nidentity similarity and attribute preservation. The iterative nature of\ndiffusion models poses challenges for utilizing efficient image-space loss\nfunctions, as performing time-consuming multi-step sampling to obtain the\ngenerated image during training is impractical. To address this issue, we\nleverage the accelerated diffusion model SD Turbo, reducing the inference steps\nto a single iteration, enabling efficient pixel-level end-to-end training with\nexplicit Triplet ID Group supervision. Additionally, we propose an improved\ndiffusion-based model architecture comprising SwapNet, FaceNet, and ID Adapter.\nThis robust architecture fully unlocks the power of the Triplet ID Group\nexplicit supervision. Finally, to further extend our method, we explicitly\nmodify the Triplet ID Group data during training to fine-tune and preserve\nspecific attributes, such as glasses and face shape. Extensive experiments\ndemonstrate that DreamID outperforms state-of-the-art methods in terms of\nidentity similarity, pose and expression preservation, and image fidelity.\nOverall, DreamID achieves high-quality face swapping results at 512*512\nresolution in just 0.6 seconds and performs exceptionally well in challenging\nscenarios such as complex lighting, large angles, and occlusions.",
      "upvotes": 12,
      "discussionId": "6809dd102e04f68a3f5babf5",
      "ai_keywords": [
        "diffusion-based model",
        "Triplet ID Group",
        "diffusion models",
        "image-space loss functions",
        "SD Turbo",
        "SwapNet",
        "FaceNet",
        "ID Adapter",
        "face swapping",
        "explicit supervision",
        "identity similarity",
        "attribute preservation",
        "image fidelity",
        "pose preservation",
        "expression preservation",
        "high-quality face swapping"
      ]
    },
    "publishedAt": "2025-04-20T02:53:00.000Z",
    "title": "DreamID: High-Fidelity and Fast diffusion-based Face Swapping via\n  Triplet ID Group Learning",
    "summary": "In this paper, we introduce DreamID, a diffusion-based face swapping model\nthat achieves high levels of ID similarity, attribute preservation, image\nfidelity, and fast inference speed. Unlike the typical face swapping training\nprocess, which often relies on implicit supervision and struggles to achieve\nsatisfactory results. DreamID establishes explicit supervision for face\nswapping by constructing Triplet ID Group data, significantly enhancing\nidentity similarity and attribute preservation. The iterative nature of\ndiffusion models poses challenges for utilizing efficient image-space loss\nfunctions, as performing time-consuming multi-step sampling to obtain the\ngenerated image during training is impractical. To address this issue, we\nleverage the accelerated diffusion model SD Turbo, reducing the inference steps\nto a single iteration, enabling efficient pixel-level end-to-end training with\nexplicit Triplet ID Group supervision. Additionally, we propose an improved\ndiffusion-based model architecture comprising SwapNet, FaceNet, and ID Adapter.\nThis robust architecture fully unlocks the power of the Triplet ID Group\nexplicit supervision. Finally, to further extend our method, we explicitly\nmodify the Triplet ID Group data during training to fine-tune and preserve\nspecific attributes, such as glasses and face shape. Extensive experiments\ndemonstrate that DreamID outperforms state-of-the-art methods in terms of\nidentity similarity, pose and expression preservation, and image fidelity.\nOverall, DreamID achieves high-quality face swapping results at 512*512\nresolution in just 0.6 seconds and performs exceptionally well in challenging\nscenarios such as complex lighting, large angles, and occlusions.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.14509.png",
    "numComments": 5,
    "submittedBy": {
      "_id": "6339029a76421c0543167075",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6339029a76421c0543167075/3npT0NxTMV-MLWDThyBy8.png",
      "fullname": "fulong ye",
      "name": "Alon77777",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.16801",
      "authors": [
        {
          "_id": "6809bebd0f6dfd7bd5159b76",
          "user": {
            "_id": "6545f8922a2a483042ebc8b3",
            "avatarUrl": "/avatars/ab00da8aa841694f3f11093a9148e4c5.svg",
            "isPro": false,
            "fullname": "xiaoxing2001",
            "user": "xiaoxing2001",
            "type": "user"
          },
          "name": "Xiaoxing Hu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-24T04:32:01.063Z",
          "hidden": false
        },
        {
          "_id": "6809bebd0f6dfd7bd5159b77",
          "name": "Kaicheng Yang",
          "hidden": false
        },
        {
          "_id": "6809bebd0f6dfd7bd5159b78",
          "name": "Jun Wang",
          "hidden": false
        },
        {
          "_id": "6809bebd0f6dfd7bd5159b79",
          "name": "Haoran Xu",
          "hidden": false
        },
        {
          "_id": "6809bebd0f6dfd7bd5159b7a",
          "name": "Ziyong Feng",
          "hidden": false
        },
        {
          "_id": "6809bebd0f6dfd7bd5159b7b",
          "name": "Yupei Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-23T15:20:53.000Z",
      "submittedOnDailyAt": "2025-04-24T03:04:11.389Z",
      "title": "Decoupled Global-Local Alignment for Improving Compositional\n  Understanding",
      "submittedOnDailyBy": {
        "_id": "63e202f352b7578dba448ab5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e202f352b7578dba448ab5/8itVBLcv14m7OVsoF8h1o.jpeg",
        "isPro": false,
        "fullname": "Yang",
        "user": "Kaichengalex",
        "type": "user"
      },
      "summary": "Contrastive Language-Image Pre-training (CLIP) has achieved success on\nmultiple downstream tasks by aligning image and text modalities. However, the\nnature of global contrastive learning limits CLIP's ability to comprehend\ncompositional concepts, such as relations and attributes. Although recent\nstudies employ global hard negative samples to improve compositional\nunderstanding, these methods significantly compromise the model's inherent\ngeneral capabilities by forcibly distancing textual negative samples from\nimages in the embedding space. To overcome this limitation, we introduce a\nDecoupled Global-Local Alignment (DeGLA) framework that improves compositional\nunderstanding while substantially mitigating losses in general capabilities. To\noptimize the retention of the model's inherent capabilities, we incorporate a\nself-distillation mechanism within the global alignment process, aligning the\nlearnable image-text encoder with a frozen teacher model derived from an\nexponential moving average. Under the constraint of self-distillation, it\neffectively mitigates the catastrophic forgetting of pretrained knowledge\nduring fine-tuning. To improve compositional understanding, we first leverage\nthe in-context learning capability of Large Language Models (LLMs) to construct\nabout 2M high-quality negative captions across five types. Subsequently, we\npropose the Image-Grounded Contrast (IGC) loss and Text-Grounded Contrast (TGC)\nloss to enhance vision-language compositionally. Extensive experimental results\ndemonstrate the effectiveness of the DeGLA framework. Compared to previous\nstate-of-the-art methods, DeGLA achieves an average enhancement of 3.5% across\nthe VALSE, SugarCrepe, and ARO benchmarks. Concurrently, it obtains an average\nperformance improvement of 13.0% on zero-shot classification tasks across\neleven datasets. Our code will be released at\nhttps://github.com/xiaoxing2001/DeGLA",
      "upvotes": 10,
      "discussionId": "6809bec10f6dfd7bd5159c38",
      "ai_keywords": [
        "Decoupled Global-Local Alignment (DeGLA)",
        "self-distillation mechanism",
        "learnable image-text encoder",
        "frozen teacher model",
        "exponential moving average",
        "catastrophic forgetting",
        "in-context learning",
        "Large Language Models (LLMs)",
        "high-quality negative captions",
        "Image-Grounded Contrast (IGC) loss",
        "Text-Grounded Contrast (TGC) loss",
        "vision-language compositionally",
        "VALSE",
        "SugarCrepe",
        "ARO benchmarks",
        "zero-shot classification tasks"
      ]
    },
    "publishedAt": "2025-04-23T11:20:53.000Z",
    "title": "Decoupled Global-Local Alignment for Improving Compositional\n  Understanding",
    "summary": "Contrastive Language-Image Pre-training (CLIP) has achieved success on\nmultiple downstream tasks by aligning image and text modalities. However, the\nnature of global contrastive learning limits CLIP's ability to comprehend\ncompositional concepts, such as relations and attributes. Although recent\nstudies employ global hard negative samples to improve compositional\nunderstanding, these methods significantly compromise the model's inherent\ngeneral capabilities by forcibly distancing textual negative samples from\nimages in the embedding space. To overcome this limitation, we introduce a\nDecoupled Global-Local Alignment (DeGLA) framework that improves compositional\nunderstanding while substantially mitigating losses in general capabilities. To\noptimize the retention of the model's inherent capabilities, we incorporate a\nself-distillation mechanism within the global alignment process, aligning the\nlearnable image-text encoder with a frozen teacher model derived from an\nexponential moving average. Under the constraint of self-distillation, it\neffectively mitigates the catastrophic forgetting of pretrained knowledge\nduring fine-tuning. To improve compositional understanding, we first leverage\nthe in-context learning capability of Large Language Models (LLMs) to construct\nabout 2M high-quality negative captions across five types. Subsequently, we\npropose the Image-Grounded Contrast (IGC) loss and Text-Grounded Contrast (TGC)\nloss to enhance vision-language compositionally. Extensive experimental results\ndemonstrate the effectiveness of the DeGLA framework. Compared to previous\nstate-of-the-art methods, DeGLA achieves an average enhancement of 3.5% across\nthe VALSE, SugarCrepe, and ARO benchmarks. Concurrently, it obtains an average\nperformance improvement of 13.0% on zero-shot classification tasks across\neleven datasets. Our code will be released at\nhttps://github.com/xiaoxing2001/DeGLA",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16801.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63e202f352b7578dba448ab5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e202f352b7578dba448ab5/8itVBLcv14m7OVsoF8h1o.jpeg",
      "fullname": "Yang",
      "name": "Kaichengalex",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.15843",
      "authors": [
        {
          "_id": "6809948944114def75aaeb7d",
          "name": "Junshu Pan",
          "hidden": false
        },
        {
          "_id": "6809948944114def75aaeb7e",
          "name": "Wei Shen",
          "hidden": false
        },
        {
          "_id": "6809948944114def75aaeb7f",
          "name": "Shulin Huang",
          "hidden": false
        },
        {
          "_id": "6809948944114def75aaeb80",
          "name": "Qiji Zhou",
          "hidden": false
        },
        {
          "_id": "6809948944114def75aaeb81",
          "name": "Yue Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-22T12:39:30.000Z",
      "submittedOnDailyAt": "2025-04-24T00:02:36.679Z",
      "title": "Pre-DPO: Improving Data Utilization in Direct Preference Optimization\n  Using a Guiding Reference Model",
      "submittedOnDailyBy": {
        "_id": "6468823272d9180d4ac90bdf",
        "avatarUrl": "/avatars/70cb7d65d30ecb944595000ceeeedb1b.svg",
        "isPro": false,
        "fullname": "Wei Shen",
        "user": "Swtheking",
        "type": "user"
      },
      "summary": "Direct Preference Optimization (DPO) simplifies reinforcement learning from\nhuman feedback (RLHF) for large language models (LLMs) by directly optimizing\nhuman preferences without an explicit reward model. We find that during DPO\ntraining, the reference model plays the role of a data weight adjuster.\nHowever, the common practice of initializing the policy and reference models\nidentically in DPO can lead to inefficient data utilization and impose a\nperformance ceiling. Meanwhile, the lack of a reference model in Simple\nPreference Optimization (SimPO) reduces training robustness and necessitates\nstricter conditions to prevent catastrophic forgetting. In this work, we\npropose Pre-DPO, a simple yet effective DPO-based training paradigm that\nenhances preference optimization performance by leveraging a guiding reference\nmodel. This reference model provides foresight into the optimal policy state\nachievable through the training preference data, serving as a guiding mechanism\nthat adaptively assigns higher weights to samples more suitable for the model\nand lower weights to those less suitable. Extensive experiments on AlpacaEval\n2.0 and Arena-Hard v0.1 benchmarks demonstrate that Pre-DPO consistently\nimproves the performance of both DPO and SimPO, without relying on external\nmodels or additional data.",
      "upvotes": 8,
      "discussionId": "6809948a44114def75aaebab",
      "ai_keywords": [
        "reinforcement learning from human feedback (RLHF)",
        "large language models (LLMs)",
        "Direct Preference Optimization (DPO)",
        "human preferences",
        "reference model",
        "data weight adjuster",
        "Simple Preference Optimization (SimPO)",
        "catastrophic forgetting",
        "Pre-DPO",
        "guiding reference model",
        "AlpacaEval 2.0",
        "Arena-Hard v0.1"
      ]
    },
    "publishedAt": "2025-04-22T08:39:30.000Z",
    "title": "Pre-DPO: Improving Data Utilization in Direct Preference Optimization\n  Using a Guiding Reference Model",
    "summary": "Direct Preference Optimization (DPO) simplifies reinforcement learning from\nhuman feedback (RLHF) for large language models (LLMs) by directly optimizing\nhuman preferences without an explicit reward model. We find that during DPO\ntraining, the reference model plays the role of a data weight adjuster.\nHowever, the common practice of initializing the policy and reference models\nidentically in DPO can lead to inefficient data utilization and impose a\nperformance ceiling. Meanwhile, the lack of a reference model in Simple\nPreference Optimization (SimPO) reduces training robustness and necessitates\nstricter conditions to prevent catastrophic forgetting. In this work, we\npropose Pre-DPO, a simple yet effective DPO-based training paradigm that\nenhances preference optimization performance by leveraging a guiding reference\nmodel. This reference model provides foresight into the optimal policy state\nachievable through the training preference data, serving as a guiding mechanism\nthat adaptively assigns higher weights to samples more suitable for the model\nand lower weights to those less suitable. Extensive experiments on AlpacaEval\n2.0 and Arena-Hard v0.1 benchmarks demonstrate that Pre-DPO consistently\nimproves the performance of both DPO and SimPO, without relying on external\nmodels or additional data.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15843.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6468823272d9180d4ac90bdf",
      "avatarUrl": "/avatars/70cb7d65d30ecb944595000ceeeedb1b.svg",
      "fullname": "Wei Shen",
      "name": "Swtheking",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.16929",
      "authors": [
        {
          "_id": "6809ba7976a4f4f7268546a7",
          "name": "Shaden Alshammari",
          "hidden": false
        },
        {
          "_id": "6809ba7976a4f4f7268546a8",
          "name": "John Hershey",
          "hidden": false
        },
        {
          "_id": "6809ba7976a4f4f7268546a9",
          "name": "Axel Feldmann",
          "hidden": false
        },
        {
          "_id": "6809ba7976a4f4f7268546aa",
          "name": "William T. Freeman",
          "hidden": false
        },
        {
          "_id": "6809ba7976a4f4f7268546ab",
          "name": "Mark Hamilton",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62dae3734398e21bf7f53443/MT5eBEJkF1u7uejE382FI.png"
      ],
      "publishedAt": "2025-04-23T17:59:01.000Z",
      "submittedOnDailyAt": "2025-04-24T02:45:09.509Z",
      "title": "I-Con: A Unifying Framework for Representation Learning",
      "submittedOnDailyBy": {
        "_id": "62dae3734398e21bf7f53443",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1658512237806-noauth.jpeg",
        "isPro": false,
        "fullname": "Mark Hamilton",
        "user": "mhamilton723",
        "type": "user"
      },
      "summary": "As the field of representation learning grows, there has been a proliferation\nof different loss functions to solve different classes of problems. We\nintroduce a single information-theoretic equation that generalizes a large\ncollection of modern loss functions in machine learning. In particular, we\nintroduce a framework that shows that several broad classes of machine learning\nmethods are precisely minimizing an integrated KL divergence between two\nconditional distributions: the supervisory and learned representations. This\nviewpoint exposes a hidden information geometry underlying clustering, spectral\nmethods, dimensionality reduction, contrastive learning, and supervised\nlearning. This framework enables the development of new loss functions by\ncombining successful techniques from across the literature. We not only present\na wide array of proofs, connecting over 23 different approaches, but we also\nleverage these theoretical results to create state-of-the-art unsupervised\nimage classifiers that achieve a +8% improvement over the prior\nstate-of-the-art on unsupervised classification on ImageNet-1K. We also\ndemonstrate that I-Con can be used to derive principled debiasing methods which\nimprove contrastive representation learners.",
      "upvotes": 6,
      "discussionId": "6809ba7d76a4f4f72685478a",
      "ai_keywords": [
        "information-theoretic equation",
        "KL divergence",
        "conditional distributions",
        "supervisory representations",
        "learned representations",
        "information geometry",
        "clustering",
        "spectral methods",
        "dimensionality reduction",
        "contrastive learning",
        "supervised learning",
        "I-Con",
        "debiasing methods",
        "contrastive representation learners"
      ]
    },
    "publishedAt": "2025-04-23T13:59:01.000Z",
    "title": "I-Con: A Unifying Framework for Representation Learning",
    "summary": "As the field of representation learning grows, there has been a proliferation\nof different loss functions to solve different classes of problems. We\nintroduce a single information-theoretic equation that generalizes a large\ncollection of modern loss functions in machine learning. In particular, we\nintroduce a framework that shows that several broad classes of machine learning\nmethods are precisely minimizing an integrated KL divergence between two\nconditional distributions: the supervisory and learned representations. This\nviewpoint exposes a hidden information geometry underlying clustering, spectral\nmethods, dimensionality reduction, contrastive learning, and supervised\nlearning. This framework enables the development of new loss functions by\ncombining successful techniques from across the literature. We not only present\na wide array of proofs, connecting over 23 different approaches, but we also\nleverage these theoretical results to create state-of-the-art unsupervised\nimage classifiers that achieve a +8% improvement over the prior\nstate-of-the-art on unsupervised classification on ImageNet-1K. We also\ndemonstrate that I-Con can be used to derive principled debiasing methods which\nimprove contrastive representation learners.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62dae3734398e21bf7f53443/MT5eBEJkF1u7uejE382FI.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16929.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62dae3734398e21bf7f53443",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1658512237806-noauth.jpeg",
      "fullname": "Mark Hamilton",
      "name": "mhamilton723",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.16915",
      "authors": [
        {
          "_id": "6809b14111003e54bd204d99",
          "name": "Chong Mou",
          "hidden": false
        },
        {
          "_id": "6809b14111003e54bd204d9a",
          "name": "Yanze Wu",
          "hidden": false
        },
        {
          "_id": "6809b14111003e54bd204d9b",
          "name": "Wenxu Wu",
          "hidden": false
        },
        {
          "_id": "6809b14111003e54bd204d9c",
          "name": "Zinan Guo",
          "hidden": false
        },
        {
          "_id": "6809b14111003e54bd204d9d",
          "name": "Pengze Zhang",
          "hidden": false
        },
        {
          "_id": "6809b14111003e54bd204d9e",
          "name": "Yufeng Cheng",
          "hidden": false
        },
        {
          "_id": "6809b14111003e54bd204d9f",
          "name": "Yiming Luo",
          "hidden": false
        },
        {
          "_id": "6809b14111003e54bd204da0",
          "name": "Fei Ding",
          "hidden": false
        },
        {
          "_id": "6809b14111003e54bd204da1",
          "name": "Shiwen Zhang",
          "hidden": false
        },
        {
          "_id": "6809b14111003e54bd204da2",
          "name": "Xinghui Li",
          "hidden": false
        },
        {
          "_id": "6809b14111003e54bd204da3",
          "name": "Mengtian Li",
          "hidden": false
        },
        {
          "_id": "6809b14111003e54bd204da4",
          "name": "Songtao Zhao",
          "hidden": false
        },
        {
          "_id": "6809b14111003e54bd204da5",
          "name": "Jian Zhang",
          "hidden": false
        },
        {
          "_id": "6809b14111003e54bd204da6",
          "name": "Qian He",
          "hidden": false
        },
        {
          "_id": "6809b14111003e54bd204da7",
          "name": "Xinglong Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-23T17:41:44.000Z",
      "submittedOnDailyAt": "2025-04-24T02:18:39.286Z",
      "title": "DreamO: A Unified Framework for Image Customization",
      "submittedOnDailyBy": {
        "_id": "639709c2be8a14bb9eeea8f6",
        "avatarUrl": "/avatars/c142d71b541dccff91fcfd08a2cc0ce0.svg",
        "isPro": false,
        "fullname": "Yanze Wu",
        "user": "yanze",
        "type": "user"
      },
      "summary": "Recently, extensive research on image customization (e.g., identity, subject,\nstyle, background, etc.) demonstrates strong customization capabilities in\nlarge-scale generative models. However, most approaches are designed for\nspecific tasks, restricting their generalizability to combine different types\nof condition. Developing a unified framework for image customization remains an\nopen challenge. In this paper, we present DreamO, an image customization\nframework designed to support a wide range of tasks while facilitating seamless\nintegration of multiple conditions. Specifically, DreamO utilizes a diffusion\ntransformer (DiT) framework to uniformly process input of different types.\nDuring training, we construct a large-scale training dataset that includes\nvarious customization tasks, and we introduce a feature routing constraint to\nfacilitate the precise querying of relevant information from reference images.\nAdditionally, we design a placeholder strategy that associates specific\nplaceholders with conditions at particular positions, enabling control over the\nplacement of conditions in the generated results. Moreover, we employ a\nprogressive training strategy consisting of three stages: an initial stage\nfocused on simple tasks with limited data to establish baseline consistency, a\nfull-scale training stage to comprehensively enhance the customization\ncapabilities, and a final quality alignment stage to correct quality biases\nintroduced by low-quality data. Extensive experiments demonstrate that the\nproposed DreamO can effectively perform various image customization tasks with\nhigh quality and flexibly integrate different types of control conditions.",
      "upvotes": 4,
      "discussionId": "6809b14411003e54bd204e51",
      "projectPage": "https://mc-e.github.io/project/DreamO/",
      "githubRepo": "https://github.com/bytedance/DreamO",
      "ai_keywords": [
        "diffusion transformer (DiT)",
        "feature routing constraint",
        "placeholder strategy",
        "progressive training strategy",
        "baseline consistency",
        "customization capabilities",
        "quality alignment stage"
      ]
    },
    "publishedAt": "2025-04-23T13:41:44.000Z",
    "title": "DreamO: A Unified Framework for Image Customization",
    "summary": "Recently, extensive research on image customization (e.g., identity, subject,\nstyle, background, etc.) demonstrates strong customization capabilities in\nlarge-scale generative models. However, most approaches are designed for\nspecific tasks, restricting their generalizability to combine different types\nof condition. Developing a unified framework for image customization remains an\nopen challenge. In this paper, we present DreamO, an image customization\nframework designed to support a wide range of tasks while facilitating seamless\nintegration of multiple conditions. Specifically, DreamO utilizes a diffusion\ntransformer (DiT) framework to uniformly process input of different types.\nDuring training, we construct a large-scale training dataset that includes\nvarious customization tasks, and we introduce a feature routing constraint to\nfacilitate the precise querying of relevant information from reference images.\nAdditionally, we design a placeholder strategy that associates specific\nplaceholders with conditions at particular positions, enabling control over the\nplacement of conditions in the generated results. Moreover, we employ a\nprogressive training strategy consisting of three stages: an initial stage\nfocused on simple tasks with limited data to establish baseline consistency, a\nfull-scale training stage to comprehensively enhance the customization\ncapabilities, and a final quality alignment stage to correct quality biases\nintroduced by low-quality data. Extensive experiments demonstrate that the\nproposed DreamO can effectively perform various image customization tasks with\nhigh quality and flexibly integrate different types of control conditions.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16915.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "639709c2be8a14bb9eeea8f6",
      "avatarUrl": "/avatars/c142d71b541dccff91fcfd08a2cc0ce0.svg",
      "fullname": "Yanze Wu",
      "name": "yanze",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 138
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.15585",
      "authors": [
        {
          "_id": "6809c1f389b7cade55b32a6c",
          "name": "Kun Wang",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a6d",
          "name": "Guibin Zhang",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a6e",
          "name": "Zhenhong Zhou",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a6f",
          "name": "Jiahao Wu",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a70",
          "name": "Miao Yu",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a71",
          "name": "Shiqian Zhao",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a72",
          "name": "Chenlong Yin",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a73",
          "name": "Jinhu Fu",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a74",
          "name": "Yibo Yan",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a75",
          "name": "Hanjun Luo",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a76",
          "name": "Liang Lin",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a77",
          "name": "Zhihao Xu",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a78",
          "name": "Haolang Lu",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a79",
          "name": "Xinye Cao",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a7a",
          "name": "Xinyun Zhou",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a7b",
          "name": "Weifei Jin",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a7c",
          "name": "Fanci Meng",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a7d",
          "name": "Junyuan Mao",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a7e",
          "name": "Hao Wu",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a7f",
          "name": "Minghe Wang",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a80",
          "name": "Fan Zhang",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a81",
          "name": "Junfeng Fang",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a82",
          "name": "Chengwei Liu",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a83",
          "name": "Yifan Zhang",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a84",
          "name": "Qiankun Li",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a85",
          "name": "Chongye Guo",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a86",
          "name": "Yalan Qin",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a87",
          "name": "Yi Ding",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a88",
          "name": "Donghai Hong",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a89",
          "name": "Jiaming Ji",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a8a",
          "name": "Xinfeng Li",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a8b",
          "name": "Yifan Jiang",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a8c",
          "name": "Dongxia Wang",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a8d",
          "name": "Yihao Huang",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a8e",
          "name": "Yufei Guo",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a8f",
          "name": "Jen-tse Huang",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a90",
          "name": "Yanwei Yue",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a91",
          "name": "Wenke Huang",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a92",
          "name": "Guancheng Wan",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a93",
          "name": "Tianlin Li",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a94",
          "name": "Lei Bai",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a95",
          "name": "Jie Zhang",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a96",
          "name": "Qing Guo",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a97",
          "name": "Jingyi Wang",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a98",
          "name": "Tianlong Chen",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a99",
          "name": "Joey Tianyi Zhou",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a9a",
          "name": "Xiaojun Jia",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a9b",
          "name": "Weisong Sun",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a9c",
          "name": "Cong Wu",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a9d",
          "name": "Jing Chen",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a9e",
          "name": "Xuming Hu",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a9f",
          "name": "Yiming Li",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32aa0",
          "name": "Xiao Wang",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32aa1",
          "name": "Ningyu Zhang",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32aa2",
          "name": "Luu Anh Tuan",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32aa3",
          "name": "Guowen Xu",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32aa4",
          "name": "Tianwei Zhang",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32aa5",
          "name": "Xingjun Ma",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32aa6",
          "name": "Xiang Wang",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32aa7",
          "name": "Bo An",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32aa8",
          "name": "Jun Sun",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32aa9",
          "name": "Mohit Bansal",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32aaa",
          "name": "Shirui Pan",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32aab",
          "name": "Yuval Elovici",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32aac",
          "name": "Bhavya Kailkhura",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32aad",
          "name": "Bo Li",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32aae",
          "name": "Yaodong Yang",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32aaf",
          "name": "Hongwei Li",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32ab0",
          "name": "Wenyuan Xu",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32ab1",
          "name": "Yizhou Sun",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32ab2",
          "name": "Wei Wang",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32ab3",
          "name": "Qing Li",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32ab4",
          "name": "Ke Tang",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32ab5",
          "name": "Yu-Gang Jiang",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32ab6",
          "name": "Felix Juefei-Xu",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32ab7",
          "name": "Hui Xiong",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32ab8",
          "name": "Xiaofeng Wang",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32ab9",
          "name": "Shuicheng Yan",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32aba",
          "name": "Dacheng Tao",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32abb",
          "name": "Philip S. Yu",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32abc",
          "name": "Qingsong Wen",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32abd",
          "name": "Yang Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-22T05:02:49.000Z",
      "submittedOnDailyAt": "2025-04-24T03:15:54.692Z",
      "title": "A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training\n  and Deployment",
      "submittedOnDailyBy": {
        "_id": "620b3bbb0668e435407c8d0a",
        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
        "isPro": false,
        "fullname": "Ningyu Zhang",
        "user": "Ningyu",
        "type": "user"
      },
      "summary": "The remarkable success of Large Language Models (LLMs) has illuminated a\npromising pathway toward achieving Artificial General Intelligence for both\nacademic and industrial communities, owing to their unprecedented performance\nacross various applications. As LLMs continue to gain prominence in both\nresearch and commercial domains, their security and safety implications have\nbecome a growing concern, not only for researchers and corporations but also\nfor every nation. Currently, existing surveys on LLM safety primarily focus on\nspecific stages of the LLM lifecycle, e.g., deployment phase or fine-tuning\nphase, lacking a comprehensive understanding of the entire \"lifechain\" of LLMs.\nTo address this gap, this paper introduces, for the first time, the concept of\n\"full-stack\" safety to systematically consider safety issues throughout the\nentire process of LLM training, deployment, and eventual commercialization.\nCompared to the off-the-shelf LLM safety surveys, our work demonstrates several\ndistinctive advantages: (I) Comprehensive Perspective. We define the complete\nLLM lifecycle as encompassing data preparation, pre-training, post-training,\ndeployment and final commercialization. To our knowledge, this represents the\nfirst safety survey to encompass the entire lifecycle of LLMs. (II) Extensive\nLiterature Support. Our research is grounded in an exhaustive review of over\n800+ papers, ensuring comprehensive coverage and systematic organization of\nsecurity issues within a more holistic understanding. (III) Unique Insights.\nThrough systematic literature analysis, we have developed reliable roadmaps and\nperspectives for each chapter. Our work identifies promising research\ndirections, including safety in data generation, alignment techniques, model\nediting, and LLM-based agent systems. These insights provide valuable guidance\nfor researchers pursuing future work in this field.",
      "upvotes": 4,
      "discussionId": "6809c1f789b7cade55b32bf4"
    },
    "publishedAt": "2025-04-22T01:02:49.000Z",
    "title": "A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training\n  and Deployment",
    "summary": "The remarkable success of Large Language Models (LLMs) has illuminated a\npromising pathway toward achieving Artificial General Intelligence for both\nacademic and industrial communities, owing to their unprecedented performance\nacross various applications. As LLMs continue to gain prominence in both\nresearch and commercial domains, their security and safety implications have\nbecome a growing concern, not only for researchers and corporations but also\nfor every nation. Currently, existing surveys on LLM safety primarily focus on\nspecific stages of the LLM lifecycle, e.g., deployment phase or fine-tuning\nphase, lacking a comprehensive understanding of the entire \"lifechain\" of LLMs.\nTo address this gap, this paper introduces, for the first time, the concept of\n\"full-stack\" safety to systematically consider safety issues throughout the\nentire process of LLM training, deployment, and eventual commercialization.\nCompared to the off-the-shelf LLM safety surveys, our work demonstrates several\ndistinctive advantages: (I) Comprehensive Perspective. We define the complete\nLLM lifecycle as encompassing data preparation, pre-training, post-training,\ndeployment and final commercialization. To our knowledge, this represents the\nfirst safety survey to encompass the entire lifecycle of LLMs. (II) Extensive\nLiterature Support. Our research is grounded in an exhaustive review of over\n800+ papers, ensuring comprehensive coverage and systematic organization of\nsecurity issues within a more holistic understanding. (III) Unique Insights.\nThrough systematic literature analysis, we have developed reliable roadmaps and\nperspectives for each chapter. Our work identifies promising research\ndirections, including safety in data generation, alignment techniques, model\nediting, and LLM-based agent systems. These insights provide valuable guidance\nfor researchers pursuing future work in this field.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15585.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620b3bbb0668e435407c8d0a",
      "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
      "fullname": "Ningyu Zhang",
      "name": "Ningyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 22
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.15254",
      "authors": [
        {
          "_id": "6807bf3e70a0cec724b8a011",
          "user": {
            "_id": "6697abd4be7ce6de07140e72",
            "avatarUrl": "/avatars/1598decabd9975d755082c8fb4e5962d.svg",
            "isPro": false,
            "fullname": "Anirudh Khatry",
            "user": "anirudhkhatry",
            "type": "user"
          },
          "name": "Anirudh Khatry",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-23T08:30:04.897Z",
          "hidden": false
        },
        {
          "_id": "6807bf3e70a0cec724b8a012",
          "name": "Robert Zhang",
          "hidden": false
        },
        {
          "_id": "6807bf3e70a0cec724b8a013",
          "name": "Jia Pan",
          "hidden": false
        },
        {
          "_id": "6807bf3e70a0cec724b8a014",
          "name": "Ziteng Wang",
          "hidden": false
        },
        {
          "_id": "6807bf3e70a0cec724b8a015",
          "name": "Qiaochu Chen",
          "hidden": false
        },
        {
          "_id": "6807bf3e70a0cec724b8a016",
          "name": "Greg Durrett",
          "hidden": false
        },
        {
          "_id": "6807bf3e70a0cec724b8a017",
          "name": "Isil Dillig",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-21T17:33:33.000Z",
      "submittedOnDailyAt": "2025-04-24T05:15:24.237Z",
      "title": "CRUST-Bench: A Comprehensive Benchmark for C-to-safe-Rust Transpilation",
      "submittedOnDailyBy": {
        "_id": "6697abd4be7ce6de07140e72",
        "avatarUrl": "/avatars/1598decabd9975d755082c8fb4e5962d.svg",
        "isPro": false,
        "fullname": "Anirudh Khatry",
        "user": "anirudhkhatry",
        "type": "user"
      },
      "summary": "C-to-Rust transpilation is essential for modernizing legacy C code while\nenhancing safety and interoperability with modern Rust ecosystems. However, no\ndataset currently exists for evaluating whether a system can transpile C into\nsafe Rust that passes a set of test cases. We introduce CRUST-Bench, a dataset\nof 100 C repositories, each paired with manually-written interfaces in safe\nRust as well as test cases that can be used to validate correctness of the\ntranspilation. By considering entire repositories rather than isolated\nfunctions, CRUST-Bench captures the challenges of translating complex projects\nwith dependencies across multiple files. The provided Rust interfaces provide\nexplicit specifications that ensure adherence to idiomatic, memory-safe Rust\npatterns, while the accompanying test cases enforce functional correctness. We\nevaluate state-of-the-art large language models (LLMs) on this task and find\nthat safe and idiomatic Rust generation is still a challenging problem for\nvarious state-of-the-art methods and techniques. We also provide insights into\nthe errors LLMs usually make in transpiling code from C to safe Rust. The best\nperforming model, OpenAI o1, is able to solve only 15 tasks in a single-shot\nsetting. Improvements on CRUST-Bench would lead to improved transpilation\nsystems that can reason about complex scenarios and help in migrating legacy\ncodebases from C into languages like Rust that ensure memory safety. You can\nfind the dataset and code at https://github.com/anirudhkhatry/CRUST-bench.",
      "upvotes": 1,
      "discussionId": "6807bf3f70a0cec724b8a044",
      "githubRepo": "https://github.com/anirudhkhatry/CRUST-bench"
    },
    "publishedAt": "2025-04-21T13:33:33.000Z",
    "title": "CRUST-Bench: A Comprehensive Benchmark for C-to-safe-Rust Transpilation",
    "summary": "C-to-Rust transpilation is essential for modernizing legacy C code while\nenhancing safety and interoperability with modern Rust ecosystems. However, no\ndataset currently exists for evaluating whether a system can transpile C into\nsafe Rust that passes a set of test cases. We introduce CRUST-Bench, a dataset\nof 100 C repositories, each paired with manually-written interfaces in safe\nRust as well as test cases that can be used to validate correctness of the\ntranspilation. By considering entire repositories rather than isolated\nfunctions, CRUST-Bench captures the challenges of translating complex projects\nwith dependencies across multiple files. The provided Rust interfaces provide\nexplicit specifications that ensure adherence to idiomatic, memory-safe Rust\npatterns, while the accompanying test cases enforce functional correctness. We\nevaluate state-of-the-art large language models (LLMs) on this task and find\nthat safe and idiomatic Rust generation is still a challenging problem for\nvarious state-of-the-art methods and techniques. We also provide insights into\nthe errors LLMs usually make in transpiling code from C to safe Rust. The best\nperforming model, OpenAI o1, is able to solve only 15 tasks in a single-shot\nsetting. Improvements on CRUST-Bench would lead to improved transpilation\nsystems that can reason about complex scenarios and help in migrating legacy\ncodebases from C into languages like Rust that ensure memory safety. You can\nfind the dataset and code at https://github.com/anirudhkhatry/CRUST-bench.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15254.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6697abd4be7ce6de07140e72",
      "avatarUrl": "/avatars/1598decabd9975d755082c8fb4e5962d.svg",
      "fullname": "Anirudh Khatry",
      "name": "anirudhkhatry",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.10419",
      "authors": [
        {
          "_id": "68026f762e2023f6cf7f0daa",
          "user": {
            "_id": "680268a7fd1fae58d58a2b49",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/P96qvBdInKYnVxij7-MM_.png",
            "isPro": false,
            "fullname": "Michał Turski",
            "user": "mturski",
            "type": "user"
          },
          "name": "Michał Turski",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-18T15:27:52.927Z",
          "hidden": false
        },
        {
          "_id": "68026f762e2023f6cf7f0dab",
          "name": "Mateusz Chiliński",
          "hidden": false
        },
        {
          "_id": "68026f762e2023f6cf7f0dac",
          "name": "Łukasz Borchmann",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T17:06:59.000Z",
      "submittedOnDailyAt": "2025-04-24T03:55:04.111Z",
      "title": "Unchecked and Overlooked: Addressing the Checkbox Blind Spot in Large\n  Language Models with CheckboxQA",
      "submittedOnDailyBy": {
        "_id": "680268a7fd1fae58d58a2b49",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/P96qvBdInKYnVxij7-MM_.png",
        "isPro": false,
        "fullname": "Michał Turski",
        "user": "mturski",
        "type": "user"
      },
      "summary": "Checkboxes are critical in real-world document processing where the presence\nor absence of ticks directly informs data extraction and decision-making\nprocesses. Yet, despite the strong performance of Large Vision and Language\nModels across a wide range of tasks, they struggle with interpreting checkable\ncontent. This challenge becomes particularly pressing in industries where a\nsingle overlooked checkbox may lead to costly regulatory or contractual\noversights. To address this gap, we introduce the CheckboxQA dataset, a\ntargeted resource designed to evaluate and improve model performance on\ncheckbox-related tasks. It reveals the limitations of current models and serves\nas a valuable tool for advancing document comprehension systems, with\nsignificant implications for applications in sectors such as legal tech and\nfinance.\n  The dataset is publicly available at:\nhttps://github.com/Snowflake-Labs/CheckboxQA",
      "upvotes": 0,
      "discussionId": "68026f782e2023f6cf7f0e05"
    },
    "publishedAt": "2025-04-14T13:06:59.000Z",
    "title": "Unchecked and Overlooked: Addressing the Checkbox Blind Spot in Large\n  Language Models with CheckboxQA",
    "summary": "Checkboxes are critical in real-world document processing where the presence\nor absence of ticks directly informs data extraction and decision-making\nprocesses. Yet, despite the strong performance of Large Vision and Language\nModels across a wide range of tasks, they struggle with interpreting checkable\ncontent. This challenge becomes particularly pressing in industries where a\nsingle overlooked checkbox may lead to costly regulatory or contractual\noversights. To address this gap, we introduce the CheckboxQA dataset, a\ntargeted resource designed to evaluate and improve model performance on\ncheckbox-related tasks. It reveals the limitations of current models and serves\nas a valuable tool for advancing document comprehension systems, with\nsignificant implications for applications in sectors such as legal tech and\nfinance.\n  The dataset is publicly available at:\nhttps://github.com/Snowflake-Labs/CheckboxQA",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10419.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "680268a7fd1fae58d58a2b49",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/P96qvBdInKYnVxij7-MM_.png",
      "fullname": "Michał Turski",
      "name": "mturski",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]