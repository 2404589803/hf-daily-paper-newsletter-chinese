[
  {
    "paper": {
      "id": "2508.11737",
      "authors": [
        {
          "_id": "68a3e2c0b65388761d07448f",
          "name": "Shiyin Lu",
          "hidden": false
        },
        {
          "_id": "68a3e2c0b65388761d074490",
          "name": "Yang Li",
          "hidden": false
        },
        {
          "_id": "68a3e2c0b65388761d074491",
          "name": "Yu Xia",
          "hidden": false
        },
        {
          "_id": "68a3e2c0b65388761d074492",
          "name": "Yuwei Hu",
          "hidden": false
        },
        {
          "_id": "68a3e2c0b65388761d074493",
          "name": "Shanshan Zhao",
          "hidden": false
        },
        {
          "_id": "68a3e2c0b65388761d074494",
          "name": "Yanqing Ma",
          "hidden": false
        },
        {
          "_id": "68a3e2c0b65388761d074495",
          "name": "Zhichao Wei",
          "hidden": false
        },
        {
          "_id": "68a3e2c0b65388761d074496",
          "name": "Yinglun Li",
          "hidden": false
        },
        {
          "_id": "68a3e2c0b65388761d074497",
          "name": "Lunhao Duan",
          "hidden": false
        },
        {
          "_id": "68a3e2c0b65388761d074498",
          "name": "Jianshan Zhao",
          "hidden": false
        },
        {
          "_id": "68a3e2c0b65388761d074499",
          "name": "Yuxuan Han",
          "hidden": false
        },
        {
          "_id": "68a3e2c0b65388761d07449a",
          "name": "Haijun Li",
          "hidden": false
        },
        {
          "_id": "68a3e2c0b65388761d07449b",
          "name": "Wanying Chen",
          "hidden": false
        },
        {
          "_id": "68a3e2c0b65388761d07449c",
          "name": "Junke Tang",
          "hidden": false
        },
        {
          "_id": "68a3e2c0b65388761d07449d",
          "name": "Chengkun Hou",
          "hidden": false
        },
        {
          "_id": "68a3e2c0b65388761d07449e",
          "name": "Zhixing Du",
          "hidden": false
        },
        {
          "_id": "68a3e2c0b65388761d07449f",
          "name": "Tianli Zhou",
          "hidden": false
        },
        {
          "_id": "68a3e2c0b65388761d0744a0",
          "name": "Wenjie Zhang",
          "hidden": false
        },
        {
          "_id": "68a3e2c0b65388761d0744a1",
          "name": "Huping Ding",
          "hidden": false
        },
        {
          "_id": "68a3e2c0b65388761d0744a2",
          "name": "Jiahe Li",
          "hidden": false
        },
        {
          "_id": "68a3e2c0b65388761d0744a3",
          "name": "Wen Li",
          "hidden": false
        },
        {
          "_id": "68a3e2c0b65388761d0744a4",
          "name": "Gui Hu",
          "hidden": false
        },
        {
          "_id": "68a3e2c0b65388761d0744a5",
          "name": "Yiliang Gu",
          "hidden": false
        },
        {
          "_id": "68a3e2c0b65388761d0744a6",
          "name": "Siran Yang",
          "hidden": false
        },
        {
          "_id": "68a3e2c0b65388761d0744a7",
          "name": "Jiamang Wang",
          "hidden": false
        },
        {
          "_id": "68a3e2c0b65388761d0744a8",
          "name": "Hailong Sun",
          "hidden": false
        },
        {
          "_id": "68a3e2c0b65388761d0744a9",
          "name": "Yibo Wang",
          "hidden": false
        },
        {
          "_id": "68a3e2c0b65388761d0744aa",
          "name": "Hui Sun",
          "hidden": false
        },
        {
          "_id": "68a3e2c0b65388761d0744ab",
          "name": "Jinlong Huang",
          "hidden": false
        },
        {
          "_id": "68a3e2c0b65388761d0744ac",
          "name": "Yuping He",
          "hidden": false
        },
        {
          "_id": "68a3e2c0b65388761d0744ad",
          "name": "Shengze Shi",
          "hidden": false
        },
        {
          "_id": "68a3e2c0b65388761d0744ae",
          "name": "Weihong Zhang",
          "hidden": false
        },
        {
          "_id": "68a3e2c0b65388761d0744af",
          "name": "Guodong Zheng",
          "hidden": false
        },
        {
          "_id": "68a3e2c0b65388761d0744b0",
          "name": "Junpeng Jiang",
          "hidden": false
        },
        {
          "_id": "68a3e2c0b65388761d0744b1",
          "name": "Sensen Gao",
          "hidden": false
        },
        {
          "_id": "68a3e2c0b65388761d0744b2",
          "name": "Yi-Feng Wu",
          "hidden": false
        },
        {
          "_id": "68a3e2c0b65388761d0744b3",
          "name": "Sijia Chen",
          "hidden": false
        },
        {
          "_id": "68a3e2c0b65388761d0744b4",
          "name": "Yuhui Chen",
          "hidden": false
        },
        {
          "_id": "68a3e2c0b65388761d0744b5",
          "name": "Qing-Guo Chen",
          "hidden": false
        },
        {
          "_id": "68a3e2c0b65388761d0744b6",
          "name": "Zhao Xu",
          "hidden": false
        },
        {
          "_id": "68a3e2c0b65388761d0744b7",
          "name": "Weihua Luo",
          "hidden": false
        },
        {
          "_id": "68a3e2c0b65388761d0744b8",
          "name": "Kaifu Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-15T17:01:08.000Z",
      "submittedOnDailyAt": "2025-08-19T01:10:24.109Z",
      "title": "Ovis2.5 Technical Report",
      "submittedOnDailyBy": {
        "_id": "658a8a837959448ef5500ce5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658a8a837959448ef5500ce5/4rb2RXrsgCDH80VnwZsEt.jpeg",
        "isPro": false,
        "fullname": "Shiyin Lu",
        "user": "runninglsy",
        "type": "user"
      },
      "summary": "We present Ovis2.5, a successor to Ovis2 designed for native-resolution\nvisual perception and strong multimodal reasoning. Ovis2.5 integrates a\nnative-resolution vision transformer that processes images at their native,\nvariable resolutions, avoiding the degradation from fixed-resolution tiling and\npreserving both fine detail and global layout -- crucial for visually dense\ncontent like complex charts. To strengthen reasoning, we train the model to\nmove beyond linear chain-of-thought and perform reflection -- including\nself-checking and revision. This advanced capability is exposed as an optional\n\"thinking mode\" at inference time, allowing users to trade latency for enhanced\naccuracy on difficult inputs. The model is trained via a comprehensive\nfive-phase curriculum that progressively builds its skills. The process begins\nwith foundational visual and multimodal pretraining, advances through\nlarge-scale instruction tuning, and culminates in alignment and reasoning\nenhancement using DPO and GRPO. To scale these upgrades efficiently, we employ\nmultimodal data packing and hybrid parallelism, yielding a significant\nend-to-end speedup. We release two open-source models: Ovis2.5-9B and\nOvis2.5-2B. The latter continues the \"small model, big performance\" philosophy\nof Ovis2, making it ideal for resource-constrained, on-device scenarios. On the\nOpenCompass multimodal leaderboard, Ovis2.5-9B averages 78.3, marking a\nsubstantial improvement over its predecessor, Ovis2-8B, and achieving\nstate-of-the-art results among open-source MLLMs in the sub-40B parameter\nrange; Ovis2.5-2B scores 73.9, establishing SOTA for its size. Beyond aggregate\nscores, Ovis2.5 achieves leading results on STEM benchmarks, exhibits strong\ncapabilities on grounding and video tasks, and achieves open-source SOTA at its\nscale for complex chart analysis.",
      "upvotes": 62,
      "discussionId": "68a3e2c0b65388761d0744b9",
      "githubRepo": "https://github.com/AIDC-AI/Ovis",
      "ai_summary": "Ovis2.5, a native-resolution vision transformer with multimodal reasoning, achieves state-of-the-art performance on various benchmarks through advanced training techniques and efficient scaling methods.",
      "ai_keywords": [
        "vision transformer",
        "native-resolution",
        "multimodal reasoning",
        "linear chain-of-thought",
        "reflection",
        "thinking mode",
        "five-phase curriculum",
        "DPO",
        "GRPO",
        "multimodal data packing",
        "hybrid parallelism",
        "OpenCompass",
        "MLLMs",
        "STEM benchmarks",
        "grounding",
        "video tasks",
        "complex chart analysis"
      ],
      "githubStars": 1112
    },
    "publishedAt": "2025-08-15T13:01:08.000Z",
    "title": "Ovis2.5 Technical Report",
    "summary": "We present Ovis2.5, a successor to Ovis2 designed for native-resolution\nvisual perception and strong multimodal reasoning. Ovis2.5 integrates a\nnative-resolution vision transformer that processes images at their native,\nvariable resolutions, avoiding the degradation from fixed-resolution tiling and\npreserving both fine detail and global layout -- crucial for visually dense\ncontent like complex charts. To strengthen reasoning, we train the model to\nmove beyond linear chain-of-thought and perform reflection -- including\nself-checking and revision. This advanced capability is exposed as an optional\n\"thinking mode\" at inference time, allowing users to trade latency for enhanced\naccuracy on difficult inputs. The model is trained via a comprehensive\nfive-phase curriculum that progressively builds its skills. The process begins\nwith foundational visual and multimodal pretraining, advances through\nlarge-scale instruction tuning, and culminates in alignment and reasoning\nenhancement using DPO and GRPO. To scale these upgrades efficiently, we employ\nmultimodal data packing and hybrid parallelism, yielding a significant\nend-to-end speedup. We release two open-source models: Ovis2.5-9B and\nOvis2.5-2B. The latter continues the \"small model, big performance\" philosophy\nof Ovis2, making it ideal for resource-constrained, on-device scenarios. On the\nOpenCompass multimodal leaderboard, Ovis2.5-9B averages 78.3, marking a\nsubstantial improvement over its predecessor, Ovis2-8B, and achieving\nstate-of-the-art results among open-source MLLMs in the sub-40B parameter\nrange; Ovis2.5-2B scores 73.9, establishing SOTA for its size. Beyond aggregate\nscores, Ovis2.5 achieves leading results on STEM benchmarks, exhibits strong\ncapabilities on grounding and video tasks, and achieves open-source SOTA at its\nscale for complex chart analysis.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.11737.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "658a8a837959448ef5500ce5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658a8a837959448ef5500ce5/4rb2RXrsgCDH80VnwZsEt.jpeg",
      "fullname": "Shiyin Lu",
      "name": "runninglsy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.10419",
      "authors": [
        {
          "_id": "68a3e682b65388761d0744d6",
          "name": "Juyuan Wang",
          "hidden": false
        },
        {
          "_id": "68a3e682b65388761d0744d7",
          "name": "Rongchen Zhao",
          "hidden": false
        },
        {
          "_id": "68a3e682b65388761d0744d8",
          "name": "Wei Wei",
          "hidden": false
        },
        {
          "_id": "68a3e682b65388761d0744d9",
          "name": "Yufeng Wang",
          "hidden": false
        },
        {
          "_id": "68a3e682b65388761d0744da",
          "name": "Mo Yu",
          "hidden": false
        },
        {
          "_id": "68a3e682b65388761d0744db",
          "name": "Jie Zhou",
          "hidden": false
        },
        {
          "_id": "68a3e682b65388761d0744dc",
          "name": "Jin Xu",
          "hidden": false
        },
        {
          "_id": "68a3e682b65388761d0744dd",
          "name": "Liyan Xu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-14T07:52:09.000Z",
      "submittedOnDailyAt": "2025-08-19T01:21:34.597Z",
      "title": "ComoRAG: A Cognitive-Inspired Memory-Organized RAG for Stateful Long\n  Narrative Reasoning",
      "submittedOnDailyBy": {
        "_id": "650f0fac11f3210cf7a8a849",
        "avatarUrl": "/avatars/687d56c3a6d4f5cdb34e424cdcff954d.svg",
        "isPro": false,
        "fullname": "Leon Xu",
        "user": "lxucs",
        "type": "user"
      },
      "summary": "Narrative comprehension on long stories and novels has been a challenging\ndomain attributed to their intricate plotlines and entangled, often evolving\nrelations among characters and entities. Given the LLM's diminished reasoning\nover extended context and high computational cost, retrieval-based approaches\nremain a pivotal role in practice. However, traditional RAG methods can fall\nshort due to their stateless, single-step retrieval process, which often\noverlooks the dynamic nature of capturing interconnected relations within\nlong-range context. In this work, we propose ComoRAG, holding the principle\nthat narrative reasoning is not a one-shot process, but a dynamic, evolving\ninterplay between new evidence acquisition and past knowledge consolidation,\nanalogous to human cognition when reasoning with memory-related signals in the\nbrain. Specifically, when encountering a reasoning impasse, ComoRAG undergoes\niterative reasoning cycles while interacting with a dynamic memory workspace.\nIn each cycle, it generates probing queries to devise new exploratory paths,\nthen integrates the retrieved evidence of new aspects into a global memory\npool, thereby supporting the emergence of a coherent context for the query\nresolution. Across four challenging long-context narrative benchmarks (200K+\ntokens), ComoRAG outperforms strong RAG baselines with consistent relative\ngains up to 11% compared to the strongest baseline. Further analysis reveals\nthat ComoRAG is particularly advantageous for complex queries requiring global\ncomprehension, offering a principled, cognitively motivated paradigm for\nretrieval-based long context comprehension towards stateful reasoning. Our code\nis publicly released at https://github.com/EternityJune25/ComoRAG",
      "upvotes": 35,
      "discussionId": "68a3e683b65388761d0744de",
      "githubRepo": "https://github.com/EternityJune25/ComoRAG",
      "ai_summary": "ComoRAG, an iterative retrieval-based approach, enhances long-context narrative comprehension by dynamically updating memory and generating probing queries, outperforming traditional RAG methods.",
      "ai_keywords": [
        "retrieval-based approaches",
        "RAG methods",
        "ComoRAG",
        "iterative reasoning cycles",
        "dynamic memory workspace",
        "probing queries",
        "global memory pool",
        "long-context narrative comprehension",
        "stateful reasoning"
      ],
      "githubStars": 24
    },
    "publishedAt": "2025-08-14T03:52:09.000Z",
    "title": "ComoRAG: A Cognitive-Inspired Memory-Organized RAG for Stateful Long\n  Narrative Reasoning",
    "summary": "Narrative comprehension on long stories and novels has been a challenging\ndomain attributed to their intricate plotlines and entangled, often evolving\nrelations among characters and entities. Given the LLM's diminished reasoning\nover extended context and high computational cost, retrieval-based approaches\nremain a pivotal role in practice. However, traditional RAG methods can fall\nshort due to their stateless, single-step retrieval process, which often\noverlooks the dynamic nature of capturing interconnected relations within\nlong-range context. In this work, we propose ComoRAG, holding the principle\nthat narrative reasoning is not a one-shot process, but a dynamic, evolving\ninterplay between new evidence acquisition and past knowledge consolidation,\nanalogous to human cognition when reasoning with memory-related signals in the\nbrain. Specifically, when encountering a reasoning impasse, ComoRAG undergoes\niterative reasoning cycles while interacting with a dynamic memory workspace.\nIn each cycle, it generates probing queries to devise new exploratory paths,\nthen integrates the retrieved evidence of new aspects into a global memory\npool, thereby supporting the emergence of a coherent context for the query\nresolution. Across four challenging long-context narrative benchmarks (200K+\ntokens), ComoRAG outperforms strong RAG baselines with consistent relative\ngains up to 11% compared to the strongest baseline. Further analysis reveals\nthat ComoRAG is particularly advantageous for complex queries requiring global\ncomprehension, offering a principled, cognitively motivated paradigm for\nretrieval-based long context comprehension towards stateful reasoning. Our code\nis publicly released at https://github.com/EternityJune25/ComoRAG",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.10419.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "650f0fac11f3210cf7a8a849",
      "avatarUrl": "/avatars/687d56c3a6d4f5cdb34e424cdcff954d.svg",
      "fullname": "Leon Xu",
      "name": "lxucs",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.12811",
      "authors": [
        {
          "_id": "68a3e68db65388761d0744e0",
          "name": "Yikai Wang",
          "hidden": false
        },
        {
          "_id": "68a3e68db65388761d0744e1",
          "name": "Zhouxia Wang",
          "hidden": false
        },
        {
          "_id": "68a3e68db65388761d0744e2",
          "name": "Zhonghua Wu",
          "hidden": false
        },
        {
          "_id": "68a3e68db65388761d0744e3",
          "name": "Qingyi Tao",
          "hidden": false
        },
        {
          "_id": "68a3e68db65388761d0744e4",
          "name": "Kang Liao",
          "hidden": false
        },
        {
          "_id": "68a3e68db65388761d0744e5",
          "name": "Chen Change Loy",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63463bc4547c70e4b7d3009f/Oc1rdp6mW2ISy-bEA9Uqy.mp4"
      ],
      "publishedAt": "2025-08-18T10:47:37.000Z",
      "submittedOnDailyAt": "2025-08-19T02:48:33.504Z",
      "title": "Next Visual Granularity Generation",
      "submittedOnDailyBy": {
        "_id": "63463bc4547c70e4b7d3009f",
        "avatarUrl": "/avatars/6e5350fd998f0a7a4143d7504218164a.svg",
        "isPro": false,
        "fullname": "Yikai Wang",
        "user": "yikaiwang",
        "type": "user"
      },
      "summary": "We propose a novel approach to image generation by decomposing an image into\na structured sequence, where each element in the sequence shares the same\nspatial resolution but differs in the number of unique tokens used, capturing\ndifferent level of visual granularity. Image generation is carried out through\nour newly introduced Next Visual Granularity (NVG) generation framework, which\ngenerates a visual granularity sequence beginning from an empty image and\nprogressively refines it, from global layout to fine details, in a structured\nmanner. This iterative process encodes a hierarchical, layered representation\nthat offers fine-grained control over the generation process across multiple\ngranularity levels. We train a series of NVG models for class-conditional image\ngeneration on the ImageNet dataset and observe clear scaling behavior. Compared\nto the VAR series, NVG consistently outperforms it in terms of FID scores (3.30\n-> 3.03, 2.57 ->2.44, 2.09 -> 2.06). We also conduct extensive analysis to\nshowcase the capability and potential of the NVG framework. Our code and models\nwill be released.",
      "upvotes": 32,
      "discussionId": "68a3e68db65388761d0744e6",
      "projectPage": "https://yikai-wang.github.io/nvg/",
      "githubRepo": "https://github.com/Yikai-Wang/nvg",
      "ai_summary": "A novel Next Visual Granularity (NVG) framework generates images by iteratively refining a sequence of visual granularities, outperforming existing methods in class-conditional image generation.",
      "ai_keywords": [
        "Next Visual Granularity (NVG)",
        "visual granularity sequence",
        "global layout",
        "fine details",
        "hierarchical",
        "layered representation",
        "class-conditional image generation",
        "ImageNet dataset",
        "FID scores"
      ],
      "githubStars": 1
    },
    "publishedAt": "2025-08-18T06:47:37.000Z",
    "title": "Next Visual Granularity Generation",
    "summary": "We propose a novel approach to image generation by decomposing an image into\na structured sequence, where each element in the sequence shares the same\nspatial resolution but differs in the number of unique tokens used, capturing\ndifferent level of visual granularity. Image generation is carried out through\nour newly introduced Next Visual Granularity (NVG) generation framework, which\ngenerates a visual granularity sequence beginning from an empty image and\nprogressively refines it, from global layout to fine details, in a structured\nmanner. This iterative process encodes a hierarchical, layered representation\nthat offers fine-grained control over the generation process across multiple\ngranularity levels. We train a series of NVG models for class-conditional image\ngeneration on the ImageNet dataset and observe clear scaling behavior. Compared\nto the VAR series, NVG consistently outperforms it in terms of FID scores (3.30\n-> 3.03, 2.57 ->2.44, 2.09 -> 2.06). We also conduct extensive analysis to\nshowcase the capability and potential of the NVG framework. Our code and models\nwill be released.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63463bc4547c70e4b7d3009f/Oc1rdp6mW2ISy-bEA9Uqy.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.12811.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63463bc4547c70e4b7d3009f",
      "avatarUrl": "/avatars/6e5350fd998f0a7a4143d7504218164a.svg",
      "fullname": "Yikai Wang",
      "name": "yikaiwang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.13154",
      "authors": [
        {
          "_id": "68a3dfefb65388761d074471",
          "name": "Zhaoxi Chen",
          "hidden": false
        },
        {
          "_id": "68a3dfefb65388761d074472",
          "name": "Tianqi Liu",
          "hidden": false
        },
        {
          "_id": "68a3dfefb65388761d074473",
          "name": "Long Zhuo",
          "hidden": false
        },
        {
          "_id": "68a3dfefb65388761d074474",
          "name": "Jiawei Ren",
          "hidden": false
        },
        {
          "_id": "68a3dfefb65388761d074475",
          "name": "Zeng Tao",
          "hidden": false
        },
        {
          "_id": "68a3dfefb65388761d074476",
          "name": "He Zhu",
          "hidden": false
        },
        {
          "_id": "68a3dfefb65388761d074477",
          "name": "Fangzhou Hong",
          "hidden": false
        },
        {
          "_id": "68a3dfefb65388761d074478",
          "name": "Liang Pan",
          "hidden": false
        },
        {
          "_id": "68a3dfefb65388761d074479",
          "name": "Ziwei Liu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/66d347eebb76fb26eedb256e/ll8ni6nZZorhKA7jCZgT_.mp4"
      ],
      "publishedAt": "2025-08-18T17:59:55.000Z",
      "submittedOnDailyAt": "2025-08-19T00:55:49.901Z",
      "title": "4DNeX: Feed-Forward 4D Generative Modeling Made Easy",
      "submittedOnDailyBy": {
        "_id": "66d347eebb76fb26eedb256e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66d347eebb76fb26eedb256e/iCPF7GkmZu--XCsWzoucl.jpeg",
        "isPro": false,
        "fullname": "tianqi liu",
        "user": "tqliu",
        "type": "user"
      },
      "summary": "We present 4DNeX, the first feed-forward framework for generating 4D (i.e.,\ndynamic 3D) scene representations from a single image. In contrast to existing\nmethods that rely on computationally intensive optimization or require\nmulti-frame video inputs, 4DNeX enables efficient, end-to-end image-to-4D\ngeneration by fine-tuning a pretrained video diffusion model. Specifically, 1)\nto alleviate the scarcity of 4D data, we construct 4DNeX-10M, a large-scale\ndataset with high-quality 4D annotations generated using advanced\nreconstruction approaches. 2) we introduce a unified 6D video representation\nthat jointly models RGB and XYZ sequences, facilitating structured learning of\nboth appearance and geometry. 3) we propose a set of simple yet effective\nadaptation strategies to repurpose pretrained video diffusion models for 4D\nmodeling. 4DNeX produces high-quality dynamic point clouds that enable\nnovel-view video synthesis. Extensive experiments demonstrate that 4DNeX\noutperforms existing 4D generation methods in efficiency and generalizability,\noffering a scalable solution for image-to-4D modeling and laying the foundation\nfor generative 4D world models that simulate dynamic scene evolution.",
      "upvotes": 25,
      "discussionId": "68a3dfefb65388761d07447a",
      "projectPage": "https://4dnex.github.io/",
      "githubRepo": "https://github.com/3DTopia/4DNeX",
      "ai_summary": "4DNeX generates high-quality dynamic 3D scene representations from a single image using a fine-tuned pretrained video diffusion model, outperforming existing methods in efficiency and generalizability.",
      "ai_keywords": [
        "feed-forward framework",
        "4D scene representations",
        "video diffusion model",
        "4DNeX-10M",
        "6D video representation",
        "RGB",
        "XYZ sequences",
        "dynamic point clouds",
        "novel-view video synthesis",
        "generative 4D world models"
      ],
      "githubStars": 44
    },
    "publishedAt": "2025-08-18T13:59:55.000Z",
    "title": "4DNeX: Feed-Forward 4D Generative Modeling Made Easy",
    "summary": "We present 4DNeX, the first feed-forward framework for generating 4D (i.e.,\ndynamic 3D) scene representations from a single image. In contrast to existing\nmethods that rely on computationally intensive optimization or require\nmulti-frame video inputs, 4DNeX enables efficient, end-to-end image-to-4D\ngeneration by fine-tuning a pretrained video diffusion model. Specifically, 1)\nto alleviate the scarcity of 4D data, we construct 4DNeX-10M, a large-scale\ndataset with high-quality 4D annotations generated using advanced\nreconstruction approaches. 2) we introduce a unified 6D video representation\nthat jointly models RGB and XYZ sequences, facilitating structured learning of\nboth appearance and geometry. 3) we propose a set of simple yet effective\nadaptation strategies to repurpose pretrained video diffusion models for 4D\nmodeling. 4DNeX produces high-quality dynamic point clouds that enable\nnovel-view video synthesis. Extensive experiments demonstrate that 4DNeX\noutperforms existing 4D generation methods in efficiency and generalizability,\noffering a scalable solution for image-to-4D modeling and laying the foundation\nfor generative 4D world models that simulate dynamic scene evolution.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/66d347eebb76fb26eedb256e/ll8ni6nZZorhKA7jCZgT_.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.13154.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66d347eebb76fb26eedb256e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66d347eebb76fb26eedb256e/iCPF7GkmZu--XCsWzoucl.jpeg",
      "fullname": "tianqi liu",
      "name": "tqliu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.09834",
      "authors": [
        {
          "_id": "68a31244b65388761d074306",
          "name": "Weigao Sun",
          "hidden": false
        },
        {
          "_id": "68a31244b65388761d074307",
          "name": "Jiaxi Hu",
          "hidden": false
        },
        {
          "_id": "68a31244b65388761d074308",
          "name": "Yucheng Zhou",
          "hidden": false
        },
        {
          "_id": "68a31244b65388761d074309",
          "name": "Jusen Du",
          "hidden": false
        },
        {
          "_id": "68a31244b65388761d07430a",
          "name": "Disen Lan",
          "hidden": false
        },
        {
          "_id": "68a31244b65388761d07430b",
          "name": "Kexin Wang",
          "hidden": false
        },
        {
          "_id": "68a31244b65388761d07430c",
          "name": "Tong Zhu",
          "hidden": false
        },
        {
          "_id": "68a31244b65388761d07430d",
          "name": "Xiaoye Qu",
          "hidden": false
        },
        {
          "_id": "68a31244b65388761d07430e",
          "name": "Yu Zhang",
          "hidden": false
        },
        {
          "_id": "68a31244b65388761d07430f",
          "name": "Xiaoyu Mo",
          "hidden": false
        },
        {
          "_id": "68a31244b65388761d074310",
          "name": "Daizong Liu",
          "hidden": false
        },
        {
          "_id": "68a31244b65388761d074311",
          "name": "Yuxuan Liang",
          "hidden": false
        },
        {
          "_id": "68a31244b65388761d074312",
          "name": "Wenliang Chen",
          "hidden": false
        },
        {
          "_id": "68a31244b65388761d074313",
          "name": "Guoqi Li",
          "hidden": false
        },
        {
          "_id": "68a31244b65388761d074314",
          "name": "Yu Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-13T14:13:46.000Z",
      "submittedOnDailyAt": "2025-08-19T01:03:34.076Z",
      "title": "Speed Always Wins: A Survey on Efficient Architectures for Large\n  Language Models",
      "submittedOnDailyBy": {
        "_id": "6246bb33da617c00b48e4d92",
        "avatarUrl": "/avatars/0304a9f6eb7f5dee4d933d03222f94e9.svg",
        "isPro": false,
        "fullname": "Weigao Sun",
        "user": "weigao266",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) have delivered impressive results in language\nunderstanding, generation, reasoning, and pushes the ability boundary of\nmultimodal models. Transformer models, as the foundation of modern LLMs, offer\na strong baseline with excellent scaling properties. However, the traditional\ntransformer architecture requires substantial computations and poses\nsignificant obstacles for large-scale training and practical deployment. In\nthis survey, we offer a systematic examination of innovative LLM architectures\nthat address the inherent limitations of transformers and boost the efficiency.\nStarting from language modeling, this survey covers the background and\ntechnical details of linear and sparse sequence modeling methods, efficient\nfull attention variants, sparse mixture-of-experts, hybrid model architectures\nincorporating the above techniques, and emerging diffusion LLMs. Additionally,\nwe discuss applications of these techniques to other modalities and consider\ntheir wider implications for developing scalable, resource-aware foundation\nmodels. By grouping recent studies into the above category, this survey\npresents a blueprint of modern efficient LLM architectures, and we hope this\ncould help motivate future research toward more efficient, versatile AI\nsystems.",
      "upvotes": 25,
      "discussionId": "68a31244b65388761d074315",
      "projectPage": "https://github.com/weigao266/Awesome-Efficient-Arch",
      "githubRepo": "https://github.com/weigao266/Awesome-Efficient-Arch",
      "ai_summary": "This survey examines innovative architectures for large language models to enhance efficiency, covering linear and sparse sequence modeling, efficient attention mechanisms, sparse mixture-of-experts, hybrid models, and diffusion LLMs.",
      "ai_keywords": [
        "transformer models",
        "linear sequence modeling",
        "sparse sequence modeling",
        "efficient full attention",
        "sparse mixture-of-experts",
        "hybrid model architectures",
        "diffusion LLMs"
      ],
      "githubStars": 98
    },
    "publishedAt": "2025-08-13T10:13:46.000Z",
    "title": "Speed Always Wins: A Survey on Efficient Architectures for Large\n  Language Models",
    "summary": "Large Language Models (LLMs) have delivered impressive results in language\nunderstanding, generation, reasoning, and pushes the ability boundary of\nmultimodal models. Transformer models, as the foundation of modern LLMs, offer\na strong baseline with excellent scaling properties. However, the traditional\ntransformer architecture requires substantial computations and poses\nsignificant obstacles for large-scale training and practical deployment. In\nthis survey, we offer a systematic examination of innovative LLM architectures\nthat address the inherent limitations of transformers and boost the efficiency.\nStarting from language modeling, this survey covers the background and\ntechnical details of linear and sparse sequence modeling methods, efficient\nfull attention variants, sparse mixture-of-experts, hybrid model architectures\nincorporating the above techniques, and emerging diffusion LLMs. Additionally,\nwe discuss applications of these techniques to other modalities and consider\ntheir wider implications for developing scalable, resource-aware foundation\nmodels. By grouping recent studies into the above category, this survey\npresents a blueprint of modern efficient LLM architectures, and we hope this\ncould help motivate future research toward more efficient, versatile AI\nsystems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.09834.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6246bb33da617c00b48e4d92",
      "avatarUrl": "/avatars/0304a9f6eb7f5dee4d933d03222f94e9.svg",
      "fullname": "Weigao Sun",
      "name": "weigao266",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.13142",
      "authors": [
        {
          "_id": "68a40b0db65388761d074558",
          "name": "Zhongang Cai",
          "hidden": false
        },
        {
          "_id": "68a40b0db65388761d074559",
          "name": "Yubo Wang",
          "hidden": false
        },
        {
          "_id": "68a40b0db65388761d07455a",
          "name": "Qingping Sun",
          "hidden": false
        },
        {
          "_id": "68a40b0db65388761d07455b",
          "name": "Ruisi Wang",
          "hidden": false
        },
        {
          "_id": "68a40b0db65388761d07455c",
          "name": "Chenyang Gu",
          "hidden": false
        },
        {
          "_id": "68a40b0db65388761d07455d",
          "name": "Wanqi Yin",
          "hidden": false
        },
        {
          "_id": "68a40b0db65388761d07455e",
          "name": "Zhiqian Lin",
          "hidden": false
        },
        {
          "_id": "68a40b0db65388761d07455f",
          "name": "Zhitao Yang",
          "hidden": false
        },
        {
          "_id": "68a40b0db65388761d074560",
          "name": "Chen Wei",
          "hidden": false
        },
        {
          "_id": "68a40b0db65388761d074561",
          "name": "Xuanke Shi",
          "hidden": false
        },
        {
          "_id": "68a40b0db65388761d074562",
          "name": "Kewang Deng",
          "hidden": false
        },
        {
          "_id": "68a40b0db65388761d074563",
          "name": "Xiaoyang Han",
          "hidden": false
        },
        {
          "_id": "68a40b0db65388761d074564",
          "name": "Zukai Chen",
          "hidden": false
        },
        {
          "_id": "68a40b0db65388761d074565",
          "name": "Jiaqi Li",
          "hidden": false
        },
        {
          "_id": "68a40b0db65388761d074566",
          "name": "Xiangyu Fan",
          "hidden": false
        },
        {
          "_id": "68a40b0db65388761d074567",
          "name": "Hanming Deng",
          "hidden": false
        },
        {
          "_id": "68a40b0db65388761d074568",
          "name": "Lewei Lu",
          "hidden": false
        },
        {
          "_id": "68a40b0db65388761d074569",
          "name": "Bo Li",
          "hidden": false
        },
        {
          "_id": "68a40b0db65388761d07456a",
          "name": "Ziwei Liu",
          "hidden": false
        },
        {
          "_id": "68a40b0db65388761d07456b",
          "name": "Quan Wang",
          "hidden": false
        },
        {
          "_id": "68a40b0db65388761d07456c",
          "name": "Dahua Lin",
          "hidden": false
        },
        {
          "_id": "68a40b0db65388761d07456d",
          "name": "Lei Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-18T17:55:17.000Z",
      "submittedOnDailyAt": "2025-08-19T03:58:00.897Z",
      "title": "Has GPT-5 Achieved Spatial Intelligence? An Empirical Study",
      "submittedOnDailyBy": {
        "_id": "652d06833b5997ed71ce5c46",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/xZTXEcnEogEmBm_ledJQr.jpeg",
        "isPro": false,
        "fullname": "Zhongang Cai",
        "user": "caizhongang",
        "type": "user"
      },
      "summary": "Multi-modal models have achieved remarkable progress in recent years.\nNevertheless, they continue to exhibit notable limitations in spatial\nunderstanding and reasoning, which are fundamental capabilities to achieving\nartificial general intelligence. With the recent release of GPT-5, allegedly\nthe most powerful AI model to date, it is timely to examine where the leading\nmodels stand on the path toward spatial intelligence. First, we propose a\ncomprehensive taxonomy of spatial tasks that unifies existing benchmarks and\ndiscuss the challenges in ensuring fair evaluation. We then evaluate\nstate-of-the-art proprietary and open-source models on eight key benchmarks, at\na cost exceeding one billion total tokens. Our empirical study reveals that (1)\nGPT-5 demonstrates unprecedented strength in spatial intelligence, yet (2)\nstill falls short of human performance across a broad spectrum of tasks.\nMoreover, we (3) identify the more challenging spatial intelligence problems\nfor multi-modal models, and (4) proprietary models do not exhibit a decisive\nadvantage when facing the most difficult problems. In addition, we conduct a\nqualitative evaluation across a diverse set of scenarios that are intuitive for\nhumans yet fail even the most advanced multi-modal models.",
      "upvotes": 15,
      "discussionId": "68a40b0eb65388761d07456e",
      "ai_summary": "Recent multi-modal models, including GPT-5, show significant progress in spatial intelligence but still lag behind human performance across various benchmarks.",
      "ai_keywords": [
        "multi-modal models",
        "spatial understanding",
        "reasoning",
        "artificial general intelligence",
        "GPT-5",
        "spatial tasks",
        "benchmarks",
        "spatial intelligence",
        "qualitative evaluation"
      ]
    },
    "publishedAt": "2025-08-18T13:55:17.000Z",
    "title": "Has GPT-5 Achieved Spatial Intelligence? An Empirical Study",
    "summary": "Multi-modal models have achieved remarkable progress in recent years.\nNevertheless, they continue to exhibit notable limitations in spatial\nunderstanding and reasoning, which are fundamental capabilities to achieving\nartificial general intelligence. With the recent release of GPT-5, allegedly\nthe most powerful AI model to date, it is timely to examine where the leading\nmodels stand on the path toward spatial intelligence. First, we propose a\ncomprehensive taxonomy of spatial tasks that unifies existing benchmarks and\ndiscuss the challenges in ensuring fair evaluation. We then evaluate\nstate-of-the-art proprietary and open-source models on eight key benchmarks, at\na cost exceeding one billion total tokens. Our empirical study reveals that (1)\nGPT-5 demonstrates unprecedented strength in spatial intelligence, yet (2)\nstill falls short of human performance across a broad spectrum of tasks.\nMoreover, we (3) identify the more challenging spatial intelligence problems\nfor multi-modal models, and (4) proprietary models do not exhibit a decisive\nadvantage when facing the most difficult problems. In addition, we conduct a\nqualitative evaluation across a diverse set of scenarios that are intuitive for\nhumans yet fail even the most advanced multi-modal models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.13142.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "652d06833b5997ed71ce5c46",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/xZTXEcnEogEmBm_ledJQr.jpeg",
      "fullname": "Zhongang Cai",
      "name": "caizhongang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 17
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.13009",
      "authors": [
        {
          "_id": "68a3e934b65388761d0744e8",
          "name": "Xianglong He",
          "hidden": false
        },
        {
          "_id": "68a3e934b65388761d0744e9",
          "name": "Chunli Peng",
          "hidden": false
        },
        {
          "_id": "68a3e934b65388761d0744ea",
          "name": "Zexiang Liu",
          "hidden": false
        },
        {
          "_id": "68a3e934b65388761d0744eb",
          "name": "Boyang Wang",
          "hidden": false
        },
        {
          "_id": "68a3e934b65388761d0744ec",
          "name": "Yifan Zhang",
          "hidden": false
        },
        {
          "_id": "68a3e934b65388761d0744ed",
          "name": "Qi Cui",
          "hidden": false
        },
        {
          "_id": "68a3e934b65388761d0744ee",
          "name": "Fei Kang",
          "hidden": false
        },
        {
          "_id": "68a3e934b65388761d0744ef",
          "name": "Biao Jiang",
          "hidden": false
        },
        {
          "_id": "68a3e934b65388761d0744f0",
          "name": "Mengyin An",
          "hidden": false
        },
        {
          "_id": "68a3e934b65388761d0744f1",
          "name": "Yangyang Ren",
          "hidden": false
        },
        {
          "_id": "68a3e934b65388761d0744f2",
          "name": "Baixin Xu",
          "hidden": false
        },
        {
          "_id": "68a3e934b65388761d0744f3",
          "name": "Hao-Xiang Guo",
          "hidden": false
        },
        {
          "_id": "68a3e934b65388761d0744f4",
          "name": "Kaixiong Gong",
          "hidden": false
        },
        {
          "_id": "68a3e934b65388761d0744f5",
          "name": "Cyrus Wu",
          "hidden": false
        },
        {
          "_id": "68a3e934b65388761d0744f6",
          "name": "Wei Li",
          "hidden": false
        },
        {
          "_id": "68a3e934b65388761d0744f7",
          "name": "Xuchen Song",
          "hidden": false
        },
        {
          "_id": "68a3e934b65388761d0744f8",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "68a3e934b65388761d0744f9",
          "name": "Eric Li",
          "hidden": false
        },
        {
          "_id": "68a3e934b65388761d0744fa",
          "name": "Yahui Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-18T15:28:53.000Z",
      "submittedOnDailyAt": "2025-08-19T01:32:45.902Z",
      "title": "Matrix-Game 2.0: An Open-Source, Real-Time, and Streaming Interactive\n  World Model",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Recent advances in interactive video generations have demonstrated diffusion\nmodel's potential as world models by capturing complex physical dynamics and\ninteractive behaviors. However, existing interactive world models depend on\nbidirectional attention and lengthy inference steps, severely limiting\nreal-time performance. Consequently, they are hard to simulate real-world\ndynamics, where outcomes must update instantaneously based on historical\ncontext and current actions. To address this, we present Matrix-Game 2.0, an\ninteractive world model generates long videos on-the-fly via few-step\nauto-regressive diffusion. Our framework consists of three key components: (1)\nA scalable data production pipeline for Unreal Engine and GTA5 environments to\neffectively produce massive amounts (about 1200 hours) of video data with\ndiverse interaction annotations; (2) An action injection module that enables\nframe-level mouse and keyboard inputs as interactive conditions; (3) A few-step\ndistillation based on the casual architecture for real-time and streaming video\ngeneration. Matrix Game 2.0 can generate high-quality minute-level videos\nacross diverse scenes at an ultra-fast speed of 25 FPS. We open-source our\nmodel weights and codebase to advance research in interactive world modeling.",
      "upvotes": 11,
      "discussionId": "68a3e934b65388761d0744fb",
      "projectPage": "https://matrix-game-v2.github.io/",
      "githubRepo": "https://github.com/SkyworkAI/Matrix-Game/tree/main/Matrix-Game-2",
      "ai_summary": "Matrix-Game 2.0 generates real-time interactive videos using few-step auto-regressive diffusion, addressing the limitations of lengthy inference in existing models.",
      "ai_keywords": [
        "diffusion model",
        "world models",
        "bidirectional attention",
        "interactive world models",
        "few-step auto-regressive diffusion",
        "scalable data production pipeline",
        "Unreal Engine",
        "GTA5",
        "action injection module",
        "frame-level mouse and keyboard inputs",
        "few-step distillation",
        "casual architecture",
        "real-time video generation",
        "streaming video generation"
      ],
      "githubStars": 1385
    },
    "publishedAt": "2025-08-18T11:28:53.000Z",
    "title": "Matrix-Game 2.0: An Open-Source, Real-Time, and Streaming Interactive\n  World Model",
    "summary": "Recent advances in interactive video generations have demonstrated diffusion\nmodel's potential as world models by capturing complex physical dynamics and\ninteractive behaviors. However, existing interactive world models depend on\nbidirectional attention and lengthy inference steps, severely limiting\nreal-time performance. Consequently, they are hard to simulate real-world\ndynamics, where outcomes must update instantaneously based on historical\ncontext and current actions. To address this, we present Matrix-Game 2.0, an\ninteractive world model generates long videos on-the-fly via few-step\nauto-regressive diffusion. Our framework consists of three key components: (1)\nA scalable data production pipeline for Unreal Engine and GTA5 environments to\neffectively produce massive amounts (about 1200 hours) of video data with\ndiverse interaction annotations; (2) An action injection module that enables\nframe-level mouse and keyboard inputs as interactive conditions; (3) A few-step\ndistillation based on the casual architecture for real-time and streaming video\ngeneration. Matrix Game 2.0 can generate high-quality minute-level videos\nacross diverse scenes at an ultra-fast speed of 25 FPS. We open-source our\nmodel weights and codebase to advance research in interactive world modeling.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.13009.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 95
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.12466",
      "authors": [
        {
          "_id": "68a3deb1b65388761d07446d",
          "name": "Xuhui Zhan",
          "hidden": false
        },
        {
          "_id": "68a3deb1b65388761d07446e",
          "name": "Tyler Derr",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/652c6ee9b118f26df765e4bb/PVHGV5FdpWkY4RFJ0CeG1.png",
        "https://cdn-uploads.huggingface.co/production/uploads/652c6ee9b118f26df765e4bb/hFDLfJ2ksRL_mCDzGr70m.png"
      ],
      "publishedAt": "2025-08-17T18:36:04.000Z",
      "submittedOnDailyAt": "2025-08-19T00:49:35.874Z",
      "title": "Inverse-LLaVA: Eliminating Alignment Pre-training Through Text-to-Vision\n  Mapping",
      "submittedOnDailyBy": {
        "_id": "652c6ee9b118f26df765e4bb",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652c6ee9b118f26df765e4bb/SwKLzRs5iVm0ORSCH-WLH.jpeg",
        "isPro": false,
        "fullname": "Xuhui Zhan",
        "user": "xuhuizhan5",
        "type": "user"
      },
      "summary": "Traditional multimodal learning approaches require expensive alignment\npre-training to bridge vision and language modalities, typically projecting\nvisual features into discrete text token spaces. We challenge both fundamental\nassumptions underlying this paradigm by proposing Inverse-LLaVA, a novel\napproach that eliminates alignment pre-training entirely while inverting the\nconventional mapping direction. Rather than projecting visual features to text\nspace, our method maps text embeddings into continuous visual representation\nspace and performs fusion within transformer intermediate layers. Through\nselective additive components in attention mechanisms, we enable dynamic\nintegration of visual and textual representations without requiring massive\nimage-text alignment datasets. Comprehensive experiments across nine multimodal\nbenchmarks demonstrate nuanced performance trade-offs: Inverse-LLaVA achieves\nnotable improvements on reasoning-intensive and cognitive tasks (MM-VET: +0.2%,\nVizWiz: +1.8%, ScienceQA: +0.2%, cognitive reasoning: +27.2%), while showing\nexpected decreases in perception tasks requiring memorized visual-text\nassociations (celebrity recognition: -49.5%, OCR: -21.3%). These results\nprovide the first empirical evidence that alignment pre-training is not\nnecessary for effective multimodal learning, particularly for complex reasoning\ntasks. Our work establishes the feasibility of a new paradigm that reduces\ncomputational requirements by 45%, challenges conventional wisdom about\nmodality fusion, and opens new research directions for efficient multimodal\narchitectures that preserve modality-specific characteristics. Our project\nwebsite with code and additional resources is available at\nhttps://inverse-llava.github.io.",
      "upvotes": 5,
      "discussionId": "68a3decbb65388761d07446f",
      "projectPage": "https://inverse-llava.github.io",
      "githubRepo": "https://github.com/xuhuizhan5/Inverse-LLaVA",
      "ai_summary": "Inverse-LLaVA eliminates alignment pre-training by mapping text embeddings into continuous visual representation space, improving reasoning tasks while reducing computational requirements.",
      "ai_keywords": [
        "Inverse-LLaVA",
        "multimodal learning",
        "alignment pre-training",
        "visual features",
        "text embeddings",
        "continuous visual representation space",
        "transformer intermediate layers",
        "attention mechanisms",
        "MM-VET",
        "VizWiz",
        "ScienceQA",
        "cognitive reasoning",
        "celebrity recognition",
        "OCR"
      ],
      "githubStars": 2
    },
    "publishedAt": "2025-08-17T14:36:04.000Z",
    "title": "Inverse-LLaVA: Eliminating Alignment Pre-training Through Text-to-Vision\n  Mapping",
    "summary": "Traditional multimodal learning approaches require expensive alignment\npre-training to bridge vision and language modalities, typically projecting\nvisual features into discrete text token spaces. We challenge both fundamental\nassumptions underlying this paradigm by proposing Inverse-LLaVA, a novel\napproach that eliminates alignment pre-training entirely while inverting the\nconventional mapping direction. Rather than projecting visual features to text\nspace, our method maps text embeddings into continuous visual representation\nspace and performs fusion within transformer intermediate layers. Through\nselective additive components in attention mechanisms, we enable dynamic\nintegration of visual and textual representations without requiring massive\nimage-text alignment datasets. Comprehensive experiments across nine multimodal\nbenchmarks demonstrate nuanced performance trade-offs: Inverse-LLaVA achieves\nnotable improvements on reasoning-intensive and cognitive tasks (MM-VET: +0.2%,\nVizWiz: +1.8%, ScienceQA: +0.2%, cognitive reasoning: +27.2%), while showing\nexpected decreases in perception tasks requiring memorized visual-text\nassociations (celebrity recognition: -49.5%, OCR: -21.3%). These results\nprovide the first empirical evidence that alignment pre-training is not\nnecessary for effective multimodal learning, particularly for complex reasoning\ntasks. Our work establishes the feasibility of a new paradigm that reduces\ncomputational requirements by 45%, challenges conventional wisdom about\nmodality fusion, and opens new research directions for efficient multimodal\narchitectures that preserve modality-specific characteristics. Our project\nwebsite with code and additional resources is available at\nhttps://inverse-llava.github.io.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/652c6ee9b118f26df765e4bb/PVHGV5FdpWkY4RFJ0CeG1.png",
      "https://cdn-uploads.huggingface.co/production/uploads/652c6ee9b118f26df765e4bb/hFDLfJ2ksRL_mCDzGr70m.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.12466.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "652c6ee9b118f26df765e4bb",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652c6ee9b118f26df765e4bb/SwKLzRs5iVm0ORSCH-WLH.jpeg",
      "fullname": "Xuhui Zhan",
      "name": "xuhuizhan5",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.11598",
      "authors": [
        {
          "_id": "68a2f5eda4caabb4320e651f",
          "name": "Greta Tuckute",
          "hidden": false
        },
        {
          "_id": "68a2f5eda4caabb4320e6520",
          "name": "Klemen Kotar",
          "hidden": false
        },
        {
          "_id": "68a2f5eda4caabb4320e6521",
          "name": "Evelina Fedorenko",
          "hidden": false
        },
        {
          "_id": "68a2f5eda4caabb4320e6522",
          "name": "Daniel L. K. Yamins",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/646d3cae4220471ca0c6cb13/0PS299xjgGwi5ALMpgH4E.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/646d3cae4220471ca0c6cb13/M4ep8C1xZlZs8wfu-nERr.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/646d3cae4220471ca0c6cb13/Dalxatim74-G-5MraCwe_.mp4"
      ],
      "publishedAt": "2025-08-15T17:06:04.000Z",
      "submittedOnDailyAt": "2025-08-19T01:32:02.566Z",
      "title": "Representing Speech Through Autoregressive Prediction of Cochlear Tokens",
      "submittedOnDailyBy": {
        "_id": "646d3cae4220471ca0c6cb13",
        "avatarUrl": "/avatars/30e47db03a23d49d946cc5248b28959a.svg",
        "isPro": false,
        "fullname": "Klemen Kotar",
        "user": "klemenk",
        "type": "user"
      },
      "summary": "We introduce AuriStream, a biologically inspired model for encoding speech\nvia a two-stage framework inspired by the human auditory processing hierarchy.\nThe first stage transforms raw audio into a time-frequency representation based\non the human cochlea, from which we extract discrete cochlear tokens.\nThe second stage applies an autoregressive sequence model over the cochlear\ntokens. AuriStream learns meaningful phoneme and word representations, and\nstate-of-the-art lexical semantics. AuriStream shows competitive performance on\ndiverse downstream SUPERB speech tasks. Complementing AuriStream's strong\nrepresentational capabilities, it generates continuations of audio which can be\nvisualized in a spectrogram space and decoded back into audio, providing\ninsights into the model's predictions. In summary, we present a two-stage\nframework for speech representation learning to advance the development of more\nhuman-like models that efficiently handle a range of speech-based tasks.",
      "upvotes": 5,
      "discussionId": "68a2f5eda4caabb4320e6523",
      "projectPage": "https://tukoresearch.github.io/auristream-speech/",
      "ai_summary": "AuriStream, a biologically inspired two-stage model, encodes speech using cochlear tokens and an autoregressive sequence model, achieving state-of-the-art performance on speech tasks and generating interpretable audio continuations.",
      "ai_keywords": [
        "cochlear tokens",
        "autoregressive sequence model",
        "human auditory processing hierarchy",
        "human cochlea",
        "lexical semantics",
        "spectrogram space"
      ]
    },
    "publishedAt": "2025-08-15T13:06:04.000Z",
    "title": "Representing Speech Through Autoregressive Prediction of Cochlear Tokens",
    "summary": "We introduce AuriStream, a biologically inspired model for encoding speech\nvia a two-stage framework inspired by the human auditory processing hierarchy.\nThe first stage transforms raw audio into a time-frequency representation based\non the human cochlea, from which we extract discrete cochlear tokens.\nThe second stage applies an autoregressive sequence model over the cochlear\ntokens. AuriStream learns meaningful phoneme and word representations, and\nstate-of-the-art lexical semantics. AuriStream shows competitive performance on\ndiverse downstream SUPERB speech tasks. Complementing AuriStream's strong\nrepresentational capabilities, it generates continuations of audio which can be\nvisualized in a spectrogram space and decoded back into audio, providing\ninsights into the model's predictions. In summary, we present a two-stage\nframework for speech representation learning to advance the development of more\nhuman-like models that efficiently handle a range of speech-based tasks.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/646d3cae4220471ca0c6cb13/0PS299xjgGwi5ALMpgH4E.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/646d3cae4220471ca0c6cb13/M4ep8C1xZlZs8wfu-nERr.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/646d3cae4220471ca0c6cb13/Dalxatim74-G-5MraCwe_.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.11598.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "646d3cae4220471ca0c6cb13",
      "avatarUrl": "/avatars/30e47db03a23d49d946cc5248b28959a.svg",
      "fullname": "Klemen Kotar",
      "name": "klemenk",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.12945",
      "authors": [
        {
          "_id": "68a3e992b65388761d0744fd",
          "name": "Jianshu Zeng",
          "hidden": false
        },
        {
          "_id": "68a3e992b65388761d0744fe",
          "name": "Yuxuan Liu",
          "hidden": false
        },
        {
          "_id": "68a3e992b65388761d0744ff",
          "name": "Yutong Feng",
          "hidden": false
        },
        {
          "_id": "68a3e992b65388761d074500",
          "name": "Chenxuan Miao",
          "hidden": false
        },
        {
          "_id": "68a3e992b65388761d074501",
          "name": "Zixiang Gao",
          "hidden": false
        },
        {
          "_id": "68a3e992b65388761d074502",
          "name": "Jiwang Qu",
          "hidden": false
        },
        {
          "_id": "68a3e992b65388761d074503",
          "name": "Jianzhang Zhang",
          "hidden": false
        },
        {
          "_id": "68a3e992b65388761d074504",
          "name": "Bin Wang",
          "hidden": false
        },
        {
          "_id": "68a3e992b65388761d074505",
          "name": "Kun Yuan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-18T14:21:22.000Z",
      "submittedOnDailyAt": "2025-08-19T01:34:17.041Z",
      "title": "Lumen: Consistent Video Relighting and Harmonious Background Replacement\n  with Video Generative Models",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Video relighting is a challenging yet valuable task, aiming to replace the\nbackground in videos while correspondingly adjusting the lighting in the\nforeground with harmonious blending. During translation, it is essential to\npreserve the original properties of the foreground, e.g., albedo, and propagate\nconsistent relighting among temporal frames. In this paper, we propose Lumen,\nan end-to-end video relighting framework developed on large-scale video\ngenerative models, receiving flexible textual description for instructing the\ncontrol of lighting and background. Considering the scarcity of high-qualified\npaired videos with the same foreground in various lighting conditions, we\nconstruct a large-scale dataset with a mixture of realistic and synthetic\nvideos. For the synthetic domain, benefiting from the abundant 3D assets in the\ncommunity, we leverage advanced 3D rendering engine to curate video pairs in\ndiverse environments. For the realistic domain, we adapt a HDR-based lighting\nsimulation to complement the lack of paired in-the-wild videos. Powered by the\naforementioned dataset, we design a joint training curriculum to effectively\nunleash the strengths of each domain, i.e., the physical consistency in\nsynthetic videos, and the generalized domain distribution in realistic videos.\nTo implement this, we inject a domain-aware adapter into the model to decouple\nthe learning of relighting and domain appearance distribution. We construct a\ncomprehensive benchmark to evaluate Lumen together with existing methods, from\nthe perspectives of foreground preservation and video consistency assessment.\nExperimental results demonstrate that Lumen effectively edit the input into\ncinematic relighted videos with consistent lighting and strict foreground\npreservation. Our project page: https://lumen-relight.github.io/",
      "upvotes": 4,
      "discussionId": "68a3e993b65388761d074506",
      "projectPage": "https://lumen-relight.github.io/",
      "ai_summary": "Lumen is an end-to-end video relighting framework that uses a large-scale dataset of realistic and synthetic videos to achieve consistent lighting and foreground preservation in edited videos.",
      "ai_keywords": [
        "video relighting",
        "albedo",
        "temporal frames",
        "end-to-end framework",
        "large-scale video generative models",
        "textual description",
        "3D rendering engine",
        "HDR-based lighting simulation",
        "domain-aware adapter",
        "foreground preservation",
        "video consistency assessment"
      ]
    },
    "publishedAt": "2025-08-18T10:21:22.000Z",
    "title": "Lumen: Consistent Video Relighting and Harmonious Background Replacement\n  with Video Generative Models",
    "summary": "Video relighting is a challenging yet valuable task, aiming to replace the\nbackground in videos while correspondingly adjusting the lighting in the\nforeground with harmonious blending. During translation, it is essential to\npreserve the original properties of the foreground, e.g., albedo, and propagate\nconsistent relighting among temporal frames. In this paper, we propose Lumen,\nan end-to-end video relighting framework developed on large-scale video\ngenerative models, receiving flexible textual description for instructing the\ncontrol of lighting and background. Considering the scarcity of high-qualified\npaired videos with the same foreground in various lighting conditions, we\nconstruct a large-scale dataset with a mixture of realistic and synthetic\nvideos. For the synthetic domain, benefiting from the abundant 3D assets in the\ncommunity, we leverage advanced 3D rendering engine to curate video pairs in\ndiverse environments. For the realistic domain, we adapt a HDR-based lighting\nsimulation to complement the lack of paired in-the-wild videos. Powered by the\naforementioned dataset, we design a joint training curriculum to effectively\nunleash the strengths of each domain, i.e., the physical consistency in\nsynthetic videos, and the generalized domain distribution in realistic videos.\nTo implement this, we inject a domain-aware adapter into the model to decouple\nthe learning of relighting and domain appearance distribution. We construct a\ncomprehensive benchmark to evaluate Lumen together with existing methods, from\nthe perspectives of foreground preservation and video consistency assessment.\nExperimental results demonstrate that Lumen effectively edit the input into\ncinematic relighted videos with consistent lighting and strict foreground\npreservation. Our project page: https://lumen-relight.github.io/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.12945.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 95
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.12880",
      "authors": [
        {
          "_id": "68a3ea3db65388761d07450b",
          "name": "Chubin Chen",
          "hidden": false
        },
        {
          "_id": "68a3ea3db65388761d07450c",
          "name": "Jiashu Zhu",
          "hidden": false
        },
        {
          "_id": "68a3ea3db65388761d07450d",
          "name": "Xiaokun Feng",
          "hidden": false
        },
        {
          "_id": "68a3ea3db65388761d07450e",
          "name": "Nisha Huang",
          "hidden": false
        },
        {
          "_id": "68a3ea3db65388761d07450f",
          "name": "Meiqi Wu",
          "hidden": false
        },
        {
          "_id": "68a3ea3db65388761d074510",
          "name": "Fangyuan Mao",
          "hidden": false
        },
        {
          "_id": "68a3ea3db65388761d074511",
          "name": "Jiahong Wu",
          "hidden": false
        },
        {
          "_id": "68a3ea3db65388761d074512",
          "name": "Xiangxiang Chu",
          "hidden": false
        },
        {
          "_id": "68a3ea3db65388761d074513",
          "name": "Xiu Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-18T12:31:20.000Z",
      "submittedOnDailyAt": "2025-08-19T01:36:46.592Z",
      "title": "S^2-Guidance: Stochastic Self Guidance for Training-Free Enhancement of\n  Diffusion Models",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Classifier-free Guidance (CFG) is a widely used technique in modern diffusion\nmodels for enhancing sample quality and prompt adherence. However, through an\nempirical analysis on Gaussian mixture modeling with a closed-form solution, we\nobserve a discrepancy between the suboptimal results produced by CFG and the\nground truth. The model's excessive reliance on these suboptimal predictions\noften leads to semantic incoherence and low-quality outputs. To address this\nissue, we first empirically demonstrate that the model's suboptimal predictions\ncan be effectively refined using sub-networks of the model itself. Building on\nthis insight, we propose S^2-Guidance, a novel method that leverages stochastic\nblock-dropping during the forward process to construct stochastic sub-networks,\neffectively guiding the model away from potential low-quality predictions and\ntoward high-quality outputs. Extensive qualitative and quantitative experiments\non text-to-image and text-to-video generation tasks demonstrate that\nS^2-Guidance delivers superior performance, consistently surpassing CFG and\nother advanced guidance strategies. Our code will be released.",
      "upvotes": 4,
      "discussionId": "68a3ea3eb65388761d074514",
      "projectPage": "https://s2guidance.github.io/",
      "githubRepo": "https://github.com/AMAP-ML/S2-Guidance",
      "ai_summary": "S^2-Guidance, a novel method using stochastic block-dropping, improves sample quality and prompt adherence in diffusion models by refining suboptimal predictions, outperforming Classifier-free Guidance and other advanced strategies.",
      "ai_keywords": [
        "Classifier-free Guidance",
        "Gaussian mixture modeling",
        "suboptimal predictions",
        "sub-networks",
        "stochastic block-dropping",
        "S^2-Guidance",
        "text-to-image",
        "text-to-video generation"
      ],
      "githubStars": 8
    },
    "publishedAt": "2025-08-18T08:31:20.000Z",
    "title": "S^2-Guidance: Stochastic Self Guidance for Training-Free Enhancement of\n  Diffusion Models",
    "summary": "Classifier-free Guidance (CFG) is a widely used technique in modern diffusion\nmodels for enhancing sample quality and prompt adherence. However, through an\nempirical analysis on Gaussian mixture modeling with a closed-form solution, we\nobserve a discrepancy between the suboptimal results produced by CFG and the\nground truth. The model's excessive reliance on these suboptimal predictions\noften leads to semantic incoherence and low-quality outputs. To address this\nissue, we first empirically demonstrate that the model's suboptimal predictions\ncan be effectively refined using sub-networks of the model itself. Building on\nthis insight, we propose S^2-Guidance, a novel method that leverages stochastic\nblock-dropping during the forward process to construct stochastic sub-networks,\neffectively guiding the model away from potential low-quality predictions and\ntoward high-quality outputs. Extensive qualitative and quantitative experiments\non text-to-image and text-to-video generation tasks demonstrate that\nS^2-Guidance delivers superior performance, consistently surpassing CFG and\nother advanced guidance strategies. Our code will be released.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.12880.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 95
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.13104",
      "authors": [
        {
          "_id": "68a40c90b65388761d074570",
          "name": "Yuang Wang",
          "hidden": false
        },
        {
          "_id": "68a40c90b65388761d074571",
          "name": "Chao Wen",
          "hidden": false
        },
        {
          "_id": "68a40c90b65388761d074572",
          "name": "Haoyu Guo",
          "hidden": false
        },
        {
          "_id": "68a40c90b65388761d074573",
          "name": "Sida Peng",
          "hidden": false
        },
        {
          "_id": "68a40c90b65388761d074574",
          "name": "Minghan Qin",
          "hidden": false
        },
        {
          "_id": "68a40c90b65388761d074575",
          "name": "Hujun Bao",
          "hidden": false
        },
        {
          "_id": "68a40c90b65388761d074576",
          "name": "Xiaowei Zhou",
          "hidden": false
        },
        {
          "_id": "68a40c90b65388761d074577",
          "name": "Ruizhen Hu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-18T17:12:28.000Z",
      "submittedOnDailyAt": "2025-08-19T04:03:42.709Z",
      "title": "Precise Action-to-Video Generation Through Visual Action Prompts",
      "submittedOnDailyBy": {
        "_id": "650d0c442a602ba349183fca",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/650d0c442a602ba349183fca/-UId-lPK64nRXjGxPJE9k.png",
        "isPro": false,
        "fullname": "Chao Wen",
        "user": "walsvid",
        "type": "user"
      },
      "summary": "We present visual action prompts, a unified action representation for\naction-to-video generation of complex high-DoF interactions while maintaining\ntransferable visual dynamics across domains. Action-driven video generation\nfaces a precision-generality trade-off: existing methods using text, primitive\nactions, or coarse masks offer generality but lack precision, while\nagent-centric action signals provide precision at the cost of cross-domain\ntransferability. To balance action precision and dynamic transferability, we\npropose to \"render\" actions into precise visual prompts as domain-agnostic\nrepresentations that preserve both geometric precision and cross-domain\nadaptability for complex actions; specifically, we choose visual skeletons for\ntheir generality and accessibility. We propose robust pipelines to construct\nskeletons from two interaction-rich data sources - human-object interactions\n(HOI) and dexterous robotic manipulation - enabling cross-domain training of\naction-driven generative models. By integrating visual skeletons into\npretrained video generation models via lightweight fine-tuning, we enable\nprecise action control of complex interaction while preserving the learning of\ncross-domain dynamics. Experiments on EgoVid, RT-1 and DROID demonstrate the\neffectiveness of our proposed approach. Project page:\nhttps://zju3dv.github.io/VAP/.",
      "upvotes": 3,
      "discussionId": "68a40c90b65388761d074578",
      "ai_summary": "Visual action prompts, using visual skeletons, enable precise action control in video generation while maintaining cross-domain transferability.",
      "ai_keywords": [
        "visual action prompts",
        "action-to-video generation",
        "high-DoF interactions",
        "visual dynamics",
        "action-driven video generation",
        "precision-generality trade-off",
        "agent-centric action signals",
        "visual skeletons",
        "human-object interactions",
        "dexterous robotic manipulation",
        "pretrained video generation models",
        "lightweight fine-tuning",
        "EgoVid",
        "RT-1",
        "DROID"
      ]
    },
    "publishedAt": "2025-08-18T13:12:28.000Z",
    "title": "Precise Action-to-Video Generation Through Visual Action Prompts",
    "summary": "We present visual action prompts, a unified action representation for\naction-to-video generation of complex high-DoF interactions while maintaining\ntransferable visual dynamics across domains. Action-driven video generation\nfaces a precision-generality trade-off: existing methods using text, primitive\nactions, or coarse masks offer generality but lack precision, while\nagent-centric action signals provide precision at the cost of cross-domain\ntransferability. To balance action precision and dynamic transferability, we\npropose to \"render\" actions into precise visual prompts as domain-agnostic\nrepresentations that preserve both geometric precision and cross-domain\nadaptability for complex actions; specifically, we choose visual skeletons for\ntheir generality and accessibility. We propose robust pipelines to construct\nskeletons from two interaction-rich data sources - human-object interactions\n(HOI) and dexterous robotic manipulation - enabling cross-domain training of\naction-driven generative models. By integrating visual skeletons into\npretrained video generation models via lightweight fine-tuning, we enable\nprecise action control of complex interaction while preserving the learning of\ncross-domain dynamics. Experiments on EgoVid, RT-1 and DROID demonstrate the\neffectiveness of our proposed approach. Project page:\nhttps://zju3dv.github.io/VAP/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.13104.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "650d0c442a602ba349183fca",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/650d0c442a602ba349183fca/-UId-lPK64nRXjGxPJE9k.png",
      "fullname": "Chao Wen",
      "name": "walsvid",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  }
]