[
  {
    "paper": {
      "id": "2504.14945",
      "authors": [
        {
          "_id": "6806fdeda296cac1cf860505",
          "name": "Jianhao Yan",
          "hidden": false
        },
        {
          "_id": "6806fdeda296cac1cf860506",
          "name": "Yafu Li",
          "hidden": false
        },
        {
          "_id": "6806fdeda296cac1cf860507",
          "name": "Zican Hu",
          "hidden": false
        },
        {
          "_id": "6806fdeda296cac1cf860508",
          "name": "Zhi Wang",
          "hidden": false
        },
        {
          "_id": "6806fdeda296cac1cf860509",
          "name": "Ganqu Cui",
          "hidden": false
        },
        {
          "_id": "6806fdeda296cac1cf86050a",
          "name": "Xiaoye Qu",
          "hidden": false
        },
        {
          "_id": "6806fdeda296cac1cf86050b",
          "name": "Yu Cheng",
          "hidden": false
        },
        {
          "_id": "6806fdeda296cac1cf86050c",
          "name": "Yue Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-21T08:09:13.000Z",
      "submittedOnDailyAt": "2025-04-22T00:57:31.276Z",
      "title": "Learning to Reason under Off-Policy Guidance",
      "submittedOnDailyBy": {
        "_id": "6086838b19137b3a6ba760e7",
        "avatarUrl": "/avatars/d63eea3e39b22c6e65b82c28192696f1.svg",
        "isPro": false,
        "fullname": "Jianhao Yan",
        "user": "Elliott",
        "type": "user"
      },
      "summary": "Recent advances in large reasoning models (LRMs) demonstrate that\nsophisticated behaviors such as multi-step reasoning and self-reflection can\nemerge via reinforcement learning (RL) with simple rule-based rewards. However,\nexisting zero-RL approaches are inherently ``on-policy'', limiting learning to\na model's own outputs and failing to acquire reasoning abilities beyond its\ninitial capabilities. We introduce LUFFY (Learning to reason Under oFF-policY\nguidance), a framework that augments zero-RL with off-policy reasoning traces.\nLUFFY dynamically balances imitation and exploration by combining off-policy\ndemonstrations with on-policy rollouts during training. Notably, we propose\npolicy shaping via regularized importance sampling to avoid superficial and\nrigid imitation during mixed-policy training. Remarkably, LUFFY achieves an\nover +7.0 average gain across six math benchmarks and an advantage of over +6.2\npoints in out-of-distribution tasks. It also substantially surpasses\nimitation-based supervised fine-tuning (SFT), particularly in generalization.\nAnalysis shows LUFFY not only imitates effectively but also explores beyond\ndemonstrations, offering a scalable path to train generalizable reasoning\nmodels with off-policy guidance.",
      "upvotes": 26,
      "discussionId": "6806fdeea296cac1cf860553",
      "githubRepo": "https://github.com/ElliottYan/LUFFY",
      "ai_keywords": [
        "reinforcement learning (RL)",
        "zero-RL",
        "on-policy",
        "off-policy",
        "LUFFY",
        "imitation learning",
        "on-policy rollouts",
        "policy shaping",
        "regularized importance sampling",
        "mixed-policy training",
        "math benchmarks",
        "out-of-distribution tasks",
        "imitation-based supervised fine-tuning (SFT)",
        "generalizable reasoning models"
      ]
    },
    "publishedAt": "2025-04-21T04:09:13.000Z",
    "title": "Learning to Reason under Off-Policy Guidance",
    "summary": "Recent advances in large reasoning models (LRMs) demonstrate that\nsophisticated behaviors such as multi-step reasoning and self-reflection can\nemerge via reinforcement learning (RL) with simple rule-based rewards. However,\nexisting zero-RL approaches are inherently ``on-policy'', limiting learning to\na model's own outputs and failing to acquire reasoning abilities beyond its\ninitial capabilities. We introduce LUFFY (Learning to reason Under oFF-policY\nguidance), a framework that augments zero-RL with off-policy reasoning traces.\nLUFFY dynamically balances imitation and exploration by combining off-policy\ndemonstrations with on-policy rollouts during training. Notably, we propose\npolicy shaping via regularized importance sampling to avoid superficial and\nrigid imitation during mixed-policy training. Remarkably, LUFFY achieves an\nover +7.0 average gain across six math benchmarks and an advantage of over +6.2\npoints in out-of-distribution tasks. It also substantially surpasses\nimitation-based supervised fine-tuning (SFT), particularly in generalization.\nAnalysis shows LUFFY not only imitates effectively but also explores beyond\ndemonstrations, offering a scalable path to train generalizable reasoning\nmodels with off-policy guidance.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.14945.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6086838b19137b3a6ba760e7",
      "avatarUrl": "/avatars/d63eea3e39b22c6e65b82c28192696f1.svg",
      "fullname": "Jianhao Yan",
      "name": "Elliott",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.15257",
      "authors": [
        {
          "_id": "68070ee593d1301c2f2ade99",
          "name": "Hongcheng Gao",
          "hidden": false
        },
        {
          "_id": "68070ee593d1301c2f2ade9a",
          "name": "Yue Liu",
          "hidden": false
        },
        {
          "_id": "68070ee593d1301c2f2ade9b",
          "name": "Yufei He",
          "hidden": false
        },
        {
          "_id": "68070ee593d1301c2f2ade9c",
          "name": "Longxu Dou",
          "hidden": false
        },
        {
          "_id": "68070ee593d1301c2f2ade9d",
          "name": "Chao Du",
          "hidden": false
        },
        {
          "_id": "68070ee593d1301c2f2ade9e",
          "name": "Zhijie Deng",
          "hidden": false
        },
        {
          "_id": "68070ee593d1301c2f2ade9f",
          "name": "Bryan Hooi",
          "hidden": false
        },
        {
          "_id": "68070ee593d1301c2f2adea0",
          "name": "Min Lin",
          "hidden": false
        },
        {
          "_id": "68070ee593d1301c2f2adea1",
          "name": "Tianyu Pang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-21T17:35:42.000Z",
      "submittedOnDailyAt": "2025-04-22T02:12:27.161Z",
      "title": "FlowReasoner: Reinforcing Query-Level Meta-Agents",
      "submittedOnDailyBy": {
        "_id": "6650c77a74664a42ddfb9187",
        "avatarUrl": "/avatars/92001bbe0ae9b14309730316b639cede.svg",
        "isPro": false,
        "fullname": "yueliu1999",
        "user": "yueliu1999",
        "type": "user"
      },
      "summary": "This paper proposes a query-level meta-agent named FlowReasoner to automate\nthe design of query-level multi-agent systems, i.e., one system per user query.\nOur core idea is to incentivize a reasoning-based meta-agent via external\nexecution feedback. Concretely, by distilling DeepSeek R1, we first endow the\nbasic reasoning ability regarding the generation of multi-agent systems to\nFlowReasoner. Then, we further enhance it via reinforcement learning (RL) with\nexternal execution feedback. A multi-purpose reward is designed to guide the RL\ntraining from aspects of performance, complexity, and efficiency. In this\nmanner, FlowReasoner is enabled to generate a personalized multi-agent system\nfor each user query via deliberative reasoning. Experiments on both engineering\nand competition code benchmarks demonstrate the superiority of FlowReasoner.\nRemarkably, it surpasses o1-mini by 10.52% accuracy across three benchmarks.\nThe code is available at https://github.com/sail-sg/FlowReasoner.",
      "upvotes": 21,
      "discussionId": "68070ee693d1301c2f2aded1",
      "ai_keywords": [
        "DeepSeek R1",
        "reinforcement learning",
        "reward",
        "performance",
        "complexity",
        "efficiency",
        "deliberative reasoning"
      ]
    },
    "publishedAt": "2025-04-21T13:35:42.000Z",
    "title": "FlowReasoner: Reinforcing Query-Level Meta-Agents",
    "summary": "This paper proposes a query-level meta-agent named FlowReasoner to automate\nthe design of query-level multi-agent systems, i.e., one system per user query.\nOur core idea is to incentivize a reasoning-based meta-agent via external\nexecution feedback. Concretely, by distilling DeepSeek R1, we first endow the\nbasic reasoning ability regarding the generation of multi-agent systems to\nFlowReasoner. Then, we further enhance it via reinforcement learning (RL) with\nexternal execution feedback. A multi-purpose reward is designed to guide the RL\ntraining from aspects of performance, complexity, and efficiency. In this\nmanner, FlowReasoner is enabled to generate a personalized multi-agent system\nfor each user query via deliberative reasoning. Experiments on both engineering\nand competition code benchmarks demonstrate the superiority of FlowReasoner.\nRemarkably, it surpasses o1-mini by 10.52% accuracy across three benchmarks.\nThe code is available at https://github.com/sail-sg/FlowReasoner.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15257.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6650c77a74664a42ddfb9187",
      "avatarUrl": "/avatars/92001bbe0ae9b14309730316b639cede.svg",
      "fullname": "yueliu1999",
      "name": "yueliu1999",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.14396",
      "authors": [
        {
          "_id": "6806fcc4f349e60f6c1b928c",
          "name": "Minho Park",
          "hidden": false
        },
        {
          "_id": "6806fcc4f349e60f6c1b928d",
          "name": "Taewoong Kang",
          "hidden": false
        },
        {
          "_id": "6806fcc4f349e60f6c1b928e",
          "name": "Jooyeol Yun",
          "hidden": false
        },
        {
          "_id": "6806fcc4f349e60f6c1b928f",
          "name": "Sungwon Hwang",
          "hidden": false
        },
        {
          "_id": "6806fcc4f349e60f6c1b9290",
          "name": "Jaegul Choo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-19T19:59:11.000Z",
      "submittedOnDailyAt": "2025-04-22T00:50:52.270Z",
      "title": "SphereDiff: Tuning-free Omnidirectional Panoramic Image and Video\n  Generation via Spherical Latent Representation",
      "submittedOnDailyBy": {
        "_id": "630461624ec2dfa82a5ad7e7",
        "avatarUrl": "/avatars/6696e21069494552b81a28a899a28fd1.svg",
        "isPro": false,
        "fullname": "Minho Park",
        "user": "mpark",
        "type": "user"
      },
      "summary": "The increasing demand for AR/VR applications has highlighted the need for\nhigh-quality 360-degree panoramic content. However, generating high-quality\n360-degree panoramic images and videos remains a challenging task due to the\nsevere distortions introduced by equirectangular projection (ERP). Existing\napproaches either fine-tune pretrained diffusion models on limited ERP datasets\nor attempt tuning-free methods that still rely on ERP latent representations,\nleading to discontinuities near the poles. In this paper, we introduce\nSphereDiff, a novel approach for seamless 360-degree panoramic image and video\ngeneration using state-of-the-art diffusion models without additional tuning.\nWe define a spherical latent representation that ensures uniform distribution\nacross all perspectives, mitigating the distortions inherent in ERP. We extend\nMultiDiffusion to spherical latent space and propose a spherical latent\nsampling method to enable direct use of pretrained diffusion models. Moreover,\nwe introduce distortion-aware weighted averaging to further improve the\ngeneration quality in the projection process. Our method outperforms existing\napproaches in generating 360-degree panoramic content while maintaining high\nfidelity, making it a robust solution for immersive AR/VR applications. The\ncode is available here. https://github.com/pmh9960/SphereDiff",
      "upvotes": 18,
      "discussionId": "6806fcc7f349e60f6c1b93ab",
      "projectPage": "https://pmh9960.github.io/research/SphereDiff/",
      "githubRepo": "https://github.com/pmh9960/SphereDiff",
      "ai_keywords": [
        "SphereDiff",
        "diffusion models",
        "equirectangular projection (ERP)",
        "spherical latent representation",
        "MultiDiffusion",
        "spherical latent space",
        "spherical latent sampling method",
        "distortion-aware weighted averaging",
        "high-fidelity",
        "immersive AR/VR applications"
      ]
    },
    "publishedAt": "2025-04-19T15:59:11.000Z",
    "title": "SphereDiff: Tuning-free Omnidirectional Panoramic Image and Video\n  Generation via Spherical Latent Representation",
    "summary": "The increasing demand for AR/VR applications has highlighted the need for\nhigh-quality 360-degree panoramic content. However, generating high-quality\n360-degree panoramic images and videos remains a challenging task due to the\nsevere distortions introduced by equirectangular projection (ERP). Existing\napproaches either fine-tune pretrained diffusion models on limited ERP datasets\nor attempt tuning-free methods that still rely on ERP latent representations,\nleading to discontinuities near the poles. In this paper, we introduce\nSphereDiff, a novel approach for seamless 360-degree panoramic image and video\ngeneration using state-of-the-art diffusion models without additional tuning.\nWe define a spherical latent representation that ensures uniform distribution\nacross all perspectives, mitigating the distortions inherent in ERP. We extend\nMultiDiffusion to spherical latent space and propose a spherical latent\nsampling method to enable direct use of pretrained diffusion models. Moreover,\nwe introduce distortion-aware weighted averaging to further improve the\ngeneration quality in the projection process. Our method outperforms existing\napproaches in generating 360-degree panoramic content while maintaining high\nfidelity, making it a robust solution for immersive AR/VR applications. The\ncode is available here. https://github.com/pmh9960/SphereDiff",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.14396.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "630461624ec2dfa82a5ad7e7",
      "avatarUrl": "/avatars/6696e21069494552b81a28a899a28fd1.svg",
      "fullname": "Minho Park",
      "name": "mpark",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.13958",
      "authors": [
        {
          "_id": "6806fe1bec9fb3764b875ea0",
          "name": "Cheng Qian",
          "hidden": false
        },
        {
          "_id": "6806fe1bec9fb3764b875ea1",
          "name": "Emre Can Acikgoz",
          "hidden": false
        },
        {
          "_id": "6806fe1bec9fb3764b875ea2",
          "name": "Qi He",
          "hidden": false
        },
        {
          "_id": "6806fe1bec9fb3764b875ea3",
          "name": "Hongru Wang",
          "hidden": false
        },
        {
          "_id": "6806fe1bec9fb3764b875ea4",
          "name": "Xiusi Chen",
          "hidden": false
        },
        {
          "_id": "6806fe1bec9fb3764b875ea5",
          "name": "Dilek Hakkani-TÃ¼r",
          "hidden": false
        },
        {
          "_id": "6806fe1bec9fb3764b875ea6",
          "name": "Gokhan Tur",
          "hidden": false
        },
        {
          "_id": "6806fe1bec9fb3764b875ea7",
          "name": "Heng Ji",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63888d3fd68e37abd599f428/v-Pmd0mDnq-v5479tRcgC.png",
        "https://cdn-uploads.huggingface.co/production/uploads/63888d3fd68e37abd599f428/pCPBgXm4IAZdhYjc-ZSrw.png",
        "https://cdn-uploads.huggingface.co/production/uploads/63888d3fd68e37abd599f428/0c5XM8ZGXnieGz3IzuYTh.png",
        "https://cdn-uploads.huggingface.co/production/uploads/63888d3fd68e37abd599f428/omlm8xwODvorHTUkfFOgR.png",
        "https://cdn-uploads.huggingface.co/production/uploads/63888d3fd68e37abd599f428/1n73QmTy-PA5FfFQJMTej.png"
      ],
      "publishedAt": "2025-04-16T21:45:32.000Z",
      "submittedOnDailyAt": "2025-04-22T01:37:59.400Z",
      "title": "ToolRL: Reward is All Tool Learning Needs",
      "submittedOnDailyBy": {
        "_id": "63888d3fd68e37abd599f428",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63888d3fd68e37abd599f428/YaNyxG_oM6IgrHTkFZ6Eq.jpeg",
        "isPro": true,
        "fullname": "emre can",
        "user": "emrecanacikgoz",
        "type": "user"
      },
      "summary": "Current Large Language Models (LLMs) often undergo supervised fine-tuning\n(SFT) to acquire tool use capabilities. However, SFT struggles to generalize to\nunfamiliar or complex tool use scenarios. Recent advancements in reinforcement\nlearning (RL), particularly with R1-like models, have demonstrated promising\nreasoning and generalization abilities. Yet, reward design for tool use\npresents unique challenges: multiple tools may be invoked with diverse\nparameters, and coarse-grained reward signals, such as answer matching, fail to\noffer the finegrained feedback required for effective learning. In this work,\nwe present the first comprehensive study on reward design for tool selection\nand application tasks within the RL paradigm. We systematically explore a wide\nrange of reward strategies, analyzing their types, scales, granularity, and\ntemporal dynamics. Building on these insights, we propose a principled reward\ndesign tailored for tool use tasks and apply it to train LLMs using Group\nRelative Policy Optimization (GRPO). Empirical evaluations across diverse\nbenchmarks demonstrate that our approach yields robust, scalable, and stable\ntraining, achieving a 17% improvement over base models and a 15% gain over SFT\nmodels. These results highlight the critical role of thoughtful reward design\nin enhancing the tool use capabilities and generalization performance of LLMs.\nAll the codes are released to facilitate future research.",
      "upvotes": 17,
      "discussionId": "6806fe1dec9fb3764b875f0c",
      "githubRepo": "https://github.com/qiancheng0/ToolRL",
      "ai_keywords": [
        "reinforcement learning (RL)",
        "R1-like models",
        "reasoning and generalization",
        "tool selection",
        "tool application",
        "reward design",
        "finegrained feedback",
        "reward strategies",
        "Group Relative Policy Optimization (GRPO)"
      ]
    },
    "publishedAt": "2025-04-16T17:45:32.000Z",
    "title": "ToolRL: Reward is All Tool Learning Needs",
    "summary": "Current Large Language Models (LLMs) often undergo supervised fine-tuning\n(SFT) to acquire tool use capabilities. However, SFT struggles to generalize to\nunfamiliar or complex tool use scenarios. Recent advancements in reinforcement\nlearning (RL), particularly with R1-like models, have demonstrated promising\nreasoning and generalization abilities. Yet, reward design for tool use\npresents unique challenges: multiple tools may be invoked with diverse\nparameters, and coarse-grained reward signals, such as answer matching, fail to\noffer the finegrained feedback required for effective learning. In this work,\nwe present the first comprehensive study on reward design for tool selection\nand application tasks within the RL paradigm. We systematically explore a wide\nrange of reward strategies, analyzing their types, scales, granularity, and\ntemporal dynamics. Building on these insights, we propose a principled reward\ndesign tailored for tool use tasks and apply it to train LLMs using Group\nRelative Policy Optimization (GRPO). Empirical evaluations across diverse\nbenchmarks demonstrate that our approach yields robust, scalable, and stable\ntraining, achieving a 17% improvement over base models and a 15% gain over SFT\nmodels. These results highlight the critical role of thoughtful reward design\nin enhancing the tool use capabilities and generalization performance of LLMs.\nAll the codes are released to facilitate future research.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63888d3fd68e37abd599f428/v-Pmd0mDnq-v5479tRcgC.png",
      "https://cdn-uploads.huggingface.co/production/uploads/63888d3fd68e37abd599f428/pCPBgXm4IAZdhYjc-ZSrw.png",
      "https://cdn-uploads.huggingface.co/production/uploads/63888d3fd68e37abd599f428/0c5XM8ZGXnieGz3IzuYTh.png",
      "https://cdn-uploads.huggingface.co/production/uploads/63888d3fd68e37abd599f428/omlm8xwODvorHTUkfFOgR.png",
      "https://cdn-uploads.huggingface.co/production/uploads/63888d3fd68e37abd599f428/1n73QmTy-PA5FfFQJMTej.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13958.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63888d3fd68e37abd599f428",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63888d3fd68e37abd599f428/YaNyxG_oM6IgrHTkFZ6Eq.jpeg",
      "fullname": "emre can",
      "name": "emrecanacikgoz",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 13
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.13203",
      "authors": [
        {
          "_id": "68070197c1bd0fc00c7d90a7",
          "name": "Salman Rahman",
          "hidden": false
        },
        {
          "_id": "68070197c1bd0fc00c7d90a8",
          "name": "Liwei Jiang",
          "hidden": false
        },
        {
          "_id": "68070197c1bd0fc00c7d90a9",
          "name": "James Shiffer",
          "hidden": false
        },
        {
          "_id": "68070197c1bd0fc00c7d90aa",
          "name": "Genglin Liu",
          "hidden": false
        },
        {
          "_id": "68070197c1bd0fc00c7d90ab",
          "name": "Sheriff Issaka",
          "hidden": false
        },
        {
          "_id": "68070197c1bd0fc00c7d90ac",
          "name": "Md Rizwan Parvez",
          "hidden": false
        },
        {
          "_id": "68070197c1bd0fc00c7d90ad",
          "name": "Hamid Palangi",
          "hidden": false
        },
        {
          "_id": "68070197c1bd0fc00c7d90ae",
          "name": "Kai-Wei Chang",
          "hidden": false
        },
        {
          "_id": "68070197c1bd0fc00c7d90af",
          "name": "Yejin Choi",
          "hidden": false
        },
        {
          "_id": "68070197c1bd0fc00c7d90b0",
          "name": "Saadia Gabriel",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-15T16:11:28.000Z",
      "submittedOnDailyAt": "2025-04-22T01:13:32.158Z",
      "title": "X-Teaming: Multi-Turn Jailbreaks and Defenses with Adaptive Multi-Agents",
      "submittedOnDailyBy": {
        "_id": "625913bd5f80a3c1aad074b6",
        "avatarUrl": "/avatars/745e4d3c916a0f0fced72ac702ff677d.svg",
        "isPro": false,
        "fullname": "Salman Rahman",
        "user": "salmannyu",
        "type": "user"
      },
      "summary": "Multi-turn interactions with language models (LMs) pose critical safety\nrisks, as harmful intent can be strategically spread across exchanges. Yet, the\nvast majority of prior work has focused on single-turn safety, while\nadaptability and diversity remain among the key challenges of multi-turn\nred-teaming. To address these challenges, we present X-Teaming, a scalable\nframework that systematically explores how seemingly harmless interactions\nescalate into harmful outcomes and generates corresponding attack scenarios.\nX-Teaming employs collaborative agents for planning, attack optimization, and\nverification, achieving state-of-the-art multi-turn jailbreak effectiveness and\ndiversity with success rates up to 98.1% across representative leading\nopen-weight and closed-source models. In particular, X-Teaming achieves a 96.2%\nattack success rate against the latest Claude 3.7 Sonnet model, which has been\nconsidered nearly immune to single-turn attacks. Building on X-Teaming, we\nintroduce XGuard-Train, an open-source multi-turn safety training dataset that\nis 20x larger than the previous best resource, comprising 30K interactive\njailbreaks, designed to enable robust multi-turn safety alignment for LMs. Our\nwork offers essential tools and insights for mitigating sophisticated\nconversational attacks, advancing the multi-turn safety of LMs.",
      "upvotes": 14,
      "discussionId": "68070198c1bd0fc00c7d9101",
      "projectPage": "https://x-teaming.github.io/",
      "githubRepo": "https://github.com/salman-lui/x-teaming",
      "ai_keywords": [
        "multi-turn interactions",
        "language models (LMs)",
        "safety risks",
        "harmful intent",
        "single-turn safety",
        "adaptability",
        "diversity",
        "X-Teaming",
        "collaborative agents",
        "attack scenarios",
        "jailbreak effectiveness",
        "jailbreak success rates",
        "Claude 3.7 Sonnet model",
        "multi-turn jailbreak",
        "XGuard-Train",
        "safety training dataset",
        "interactive jailbreaks",
        "multi-turn safety alignment"
      ]
    },
    "publishedAt": "2025-04-15T12:11:28.000Z",
    "title": "X-Teaming: Multi-Turn Jailbreaks and Defenses with Adaptive Multi-Agents",
    "summary": "Multi-turn interactions with language models (LMs) pose critical safety\nrisks, as harmful intent can be strategically spread across exchanges. Yet, the\nvast majority of prior work has focused on single-turn safety, while\nadaptability and diversity remain among the key challenges of multi-turn\nred-teaming. To address these challenges, we present X-Teaming, a scalable\nframework that systematically explores how seemingly harmless interactions\nescalate into harmful outcomes and generates corresponding attack scenarios.\nX-Teaming employs collaborative agents for planning, attack optimization, and\nverification, achieving state-of-the-art multi-turn jailbreak effectiveness and\ndiversity with success rates up to 98.1% across representative leading\nopen-weight and closed-source models. In particular, X-Teaming achieves a 96.2%\nattack success rate against the latest Claude 3.7 Sonnet model, which has been\nconsidered nearly immune to single-turn attacks. Building on X-Teaming, we\nintroduce XGuard-Train, an open-source multi-turn safety training dataset that\nis 20x larger than the previous best resource, comprising 30K interactive\njailbreaks, designed to enable robust multi-turn safety alignment for LMs. Our\nwork offers essential tools and insights for mitigating sophisticated\nconversational attacks, advancing the multi-turn safety of LMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13203.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "625913bd5f80a3c1aad074b6",
      "avatarUrl": "/avatars/745e4d3c916a0f0fced72ac702ff677d.svg",
      "fullname": "Salman Rahman",
      "name": "salmannyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.14603",
      "authors": [
        {
          "_id": "6806fab1b89f3c89c81afbe0",
          "name": "Chaoyun Zhang",
          "hidden": false
        },
        {
          "_id": "6806fab1b89f3c89c81afbe1",
          "name": "He Huang",
          "hidden": false
        },
        {
          "_id": "6806fab1b89f3c89c81afbe2",
          "name": "Chiming Ni",
          "hidden": false
        },
        {
          "_id": "6806fab1b89f3c89c81afbe3",
          "name": "Jian Mu",
          "hidden": false
        },
        {
          "_id": "6806fab1b89f3c89c81afbe4",
          "name": "Si Qin",
          "hidden": false
        },
        {
          "_id": "6806fab1b89f3c89c81afbe5",
          "name": "Shilin He",
          "hidden": false
        },
        {
          "_id": "6806fab1b89f3c89c81afbe6",
          "name": "Lu Wang",
          "hidden": false
        },
        {
          "_id": "6806fab1b89f3c89c81afbe7",
          "name": "Fangkai Yang",
          "hidden": false
        },
        {
          "_id": "6806fab1b89f3c89c81afbe8",
          "name": "Pu Zhao",
          "hidden": false
        },
        {
          "_id": "6806fab1b89f3c89c81afbe9",
          "name": "Chao Du",
          "hidden": false
        },
        {
          "_id": "6806fab1b89f3c89c81afbea",
          "name": "Liqun Li",
          "hidden": false
        },
        {
          "_id": "6806fab1b89f3c89c81afbeb",
          "name": "Yu Kang",
          "hidden": false
        },
        {
          "_id": "6806fab1b89f3c89c81afbec",
          "name": "Zhao Jiang",
          "hidden": false
        },
        {
          "_id": "6806fab1b89f3c89c81afbed",
          "name": "Suzhen Zheng",
          "hidden": false
        },
        {
          "_id": "6806fab1b89f3c89c81afbee",
          "name": "Rujia Wang",
          "hidden": false
        },
        {
          "_id": "6806fab1b89f3c89c81afbef",
          "name": "Jiaxu Qian",
          "hidden": false
        },
        {
          "_id": "6806fab1b89f3c89c81afbf0",
          "name": "Minghua Ma",
          "hidden": false
        },
        {
          "_id": "6806fab1b89f3c89c81afbf1",
          "name": "Jian-Guang Lou",
          "hidden": false
        },
        {
          "_id": "6806fab1b89f3c89c81afbf2",
          "name": "Qingwei Lin",
          "hidden": false
        },
        {
          "_id": "6806fab1b89f3c89c81afbf3",
          "name": "Saravan Rajmohan",
          "hidden": false
        },
        {
          "_id": "6806fab1b89f3c89c81afbf4",
          "name": "Dongmei Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-20T13:04:43.000Z",
      "submittedOnDailyAt": "2025-04-22T00:41:48.404Z",
      "title": "UFO2: The Desktop AgentOS",
      "submittedOnDailyBy": {
        "_id": "654dbac9938fbf1e696be8aa",
        "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
        "isPro": false,
        "fullname": "Chaoyun Zhang",
        "user": "vyokky",
        "type": "user"
      },
      "summary": "Recent Computer-Using Agents (CUAs), powered by multimodal large language\nmodels (LLMs), offer a promising direction for automating complex desktop\nworkflows through natural language. However, most existing CUAs remain\nconceptual prototypes, hindered by shallow OS integration, fragile\nscreenshot-based interaction, and disruptive execution.\n  We present UFO2, a multiagent AgentOS for Windows desktops that elevates CUAs\ninto practical, system-level automation. UFO2 features a centralized HostAgent\nfor task decomposition and coordination, alongside a collection of\napplication-specialized AppAgent equipped with native APIs, domain-specific\nknowledge, and a unified GUI--API action layer. This architecture enables\nrobust task execution while preserving modularity and extensibility. A hybrid\ncontrol detection pipeline fuses Windows UI Automation (UIA) with vision-based\nparsing to support diverse interface styles. Runtime efficiency is further\nenhanced through speculative multi-action planning, reducing per-step LLM\noverhead. Finally, a Picture-in-Picture (PiP) interface enables automation\nwithin an isolated virtual desktop, allowing agents and users to operate\nconcurrently without interference.\n  We evaluate UFO2 across over 20 real-world Windows applications,\ndemonstrating substantial improvements in robustness and execution accuracy\nover prior CUAs. Our results show that deep OS integration unlocks a scalable\npath toward reliable, user-aligned desktop automation.",
      "upvotes": 13,
      "discussionId": "6806fabdb89f3c89c81affb5",
      "projectPage": "https://microsoft.github.io/UFO/",
      "githubRepo": "https://github.com/microsoft/UFO",
      "ai_keywords": [
        "multimodal large language models",
        "task decomposition",
        "coordination",
        "application-specialized AppAgent",
        "native APIs",
        "domain-specific knowledge",
        "unified GUI--API action layer",
        "hybrid control detection pipeline",
        "Windows UI Automation (UIA)",
        "vision-based parsing",
        "speculative multi-action planning",
        "Picture-in-Picture (PiP) interface",
        "runtime efficiency"
      ]
    },
    "publishedAt": "2025-04-20T09:04:43.000Z",
    "title": "UFO2: The Desktop AgentOS",
    "summary": "Recent Computer-Using Agents (CUAs), powered by multimodal large language\nmodels (LLMs), offer a promising direction for automating complex desktop\nworkflows through natural language. However, most existing CUAs remain\nconceptual prototypes, hindered by shallow OS integration, fragile\nscreenshot-based interaction, and disruptive execution.\n  We present UFO2, a multiagent AgentOS for Windows desktops that elevates CUAs\ninto practical, system-level automation. UFO2 features a centralized HostAgent\nfor task decomposition and coordination, alongside a collection of\napplication-specialized AppAgent equipped with native APIs, domain-specific\nknowledge, and a unified GUI--API action layer. This architecture enables\nrobust task execution while preserving modularity and extensibility. A hybrid\ncontrol detection pipeline fuses Windows UI Automation (UIA) with vision-based\nparsing to support diverse interface styles. Runtime efficiency is further\nenhanced through speculative multi-action planning, reducing per-step LLM\noverhead. Finally, a Picture-in-Picture (PiP) interface enables automation\nwithin an isolated virtual desktop, allowing agents and users to operate\nconcurrently without interference.\n  We evaluate UFO2 across over 20 real-world Windows applications,\ndemonstrating substantial improvements in robustness and execution accuracy\nover prior CUAs. Our results show that deep OS integration unlocks a scalable\npath toward reliable, user-aligned desktop automation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.14603.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "654dbac9938fbf1e696be8aa",
      "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
      "fullname": "Chaoyun Zhang",
      "name": "vyokky",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.15281",
      "authors": [
        {
          "_id": "68072d05362af0cf18fb4b0c",
          "name": "Cailin Zhuang",
          "hidden": false
        },
        {
          "_id": "68072d05362af0cf18fb4b0d",
          "name": "Yaoqi Hu",
          "hidden": false
        },
        {
          "_id": "68072d05362af0cf18fb4b0e",
          "name": "Xuanyang Zhang",
          "hidden": false
        },
        {
          "_id": "68072d05362af0cf18fb4b0f",
          "name": "Wei Cheng",
          "hidden": false
        },
        {
          "_id": "68072d05362af0cf18fb4b10",
          "name": "Jiacheng Bao",
          "hidden": false
        },
        {
          "_id": "68072d05362af0cf18fb4b11",
          "name": "Shengqi Liu",
          "hidden": false
        },
        {
          "_id": "68072d05362af0cf18fb4b12",
          "name": "Yiying Yang",
          "hidden": false
        },
        {
          "_id": "68072d05362af0cf18fb4b13",
          "name": "Xianfang Zeng",
          "hidden": false
        },
        {
          "_id": "68072d05362af0cf18fb4b14",
          "name": "Gang Yu",
          "hidden": false
        },
        {
          "_id": "68072d05362af0cf18fb4b15",
          "name": "Ming Li",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64b914c8ace99c0723ad83a9/EeggKsuKNbiiygcWDKxQZ.webm"
      ],
      "publishedAt": "2025-04-21T17:59:55.000Z",
      "submittedOnDailyAt": "2025-04-22T04:18:58.848Z",
      "title": "StyleMe3D: Stylization with Disentangled Priors by Multiple Encoders on\n  3D Gaussians",
      "submittedOnDailyBy": {
        "_id": "64b914c8ace99c0723ad83a9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/udUHjj6fby82zh8LDjXhL.jpeg",
        "isPro": false,
        "fullname": "Wei Cheng",
        "user": "wchengad",
        "type": "user"
      },
      "summary": "3D Gaussian Splatting (3DGS) excels in photorealistic scene reconstruction\nbut struggles with stylized scenarios (e.g., cartoons, games) due to fragmented\ntextures, semantic misalignment, and limited adaptability to abstract\naesthetics. We propose StyleMe3D, a holistic framework for 3D GS style transfer\nthat integrates multi-modal style conditioning, multi-level semantic alignment,\nand perceptual quality enhancement. Our key insights include: (1) optimizing\nonly RGB attributes preserves geometric integrity during stylization; (2)\ndisentangling low-, medium-, and high-level semantics is critical for coherent\nstyle transfer; (3) scalability across isolated objects and complex scenes is\nessential for practical deployment. StyleMe3D introduces four novel components:\nDynamic Style Score Distillation (DSSD), leveraging Stable Diffusion's latent\nspace for semantic alignment; Contrastive Style Descriptor (CSD) for localized,\ncontent-aware texture transfer; Simultaneously Optimized Scale (SOS) to\ndecouple style details and structural coherence; and 3D Gaussian Quality\nAssessment (3DG-QA), a differentiable aesthetic prior trained on human-rated\ndata to suppress artifacts and enhance visual harmony. Evaluated on NeRF\nsynthetic dataset (objects) and tandt db (scenes) datasets, StyleMe3D\noutperforms state-of-the-art methods in preserving geometric details (e.g.,\ncarvings on sculptures) and ensuring stylistic consistency across scenes (e.g.,\ncoherent lighting in landscapes), while maintaining real-time rendering. This\nwork bridges photorealistic 3D GS and artistic stylization, unlocking\napplications in gaming, virtual worlds, and digital art.",
      "upvotes": 12,
      "discussionId": "68072d08362af0cf18fb4c7b",
      "projectPage": "https://styleme3d.github.io/",
      "githubRepo": "https://github.com/AIGCResearch/styleme3d",
      "ai_keywords": [
        "Gaussian Splatting",
        "StyleMe3D",
        "multi-modal style conditioning",
        "multi-level semantic alignment",
        "perceptual quality enhancement",
        "Dynamic Style Score Distillation",
        "Stable Diffusion",
        "latent space",
        "Contrastive Style Descriptor",
        "localized, content-aware texture transfer",
        "Simultaneously Optimized Scale",
        "3D Gaussian Quality Assessment",
        "differentiable aesthetic prior",
        "NeRF synthetic dataset",
        "tandt db",
        "preserving geometric details",
        "stylistic consistency",
        "real-time rendering"
      ]
    },
    "publishedAt": "2025-04-21T13:59:55.000Z",
    "title": "StyleMe3D: Stylization with Disentangled Priors by Multiple Encoders on\n  3D Gaussians",
    "summary": "3D Gaussian Splatting (3DGS) excels in photorealistic scene reconstruction\nbut struggles with stylized scenarios (e.g., cartoons, games) due to fragmented\ntextures, semantic misalignment, and limited adaptability to abstract\naesthetics. We propose StyleMe3D, a holistic framework for 3D GS style transfer\nthat integrates multi-modal style conditioning, multi-level semantic alignment,\nand perceptual quality enhancement. Our key insights include: (1) optimizing\nonly RGB attributes preserves geometric integrity during stylization; (2)\ndisentangling low-, medium-, and high-level semantics is critical for coherent\nstyle transfer; (3) scalability across isolated objects and complex scenes is\nessential for practical deployment. StyleMe3D introduces four novel components:\nDynamic Style Score Distillation (DSSD), leveraging Stable Diffusion's latent\nspace for semantic alignment; Contrastive Style Descriptor (CSD) for localized,\ncontent-aware texture transfer; Simultaneously Optimized Scale (SOS) to\ndecouple style details and structural coherence; and 3D Gaussian Quality\nAssessment (3DG-QA), a differentiable aesthetic prior trained on human-rated\ndata to suppress artifacts and enhance visual harmony. Evaluated on NeRF\nsynthetic dataset (objects) and tandt db (scenes) datasets, StyleMe3D\noutperforms state-of-the-art methods in preserving geometric details (e.g.,\ncarvings on sculptures) and ensuring stylistic consistency across scenes (e.g.,\ncoherent lighting in landscapes), while maintaining real-time rendering. This\nwork bridges photorealistic 3D GS and artistic stylization, unlocking\napplications in gaming, virtual worlds, and digital art.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64b914c8ace99c0723ad83a9/EeggKsuKNbiiygcWDKxQZ.webm"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15281.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b914c8ace99c0723ad83a9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/udUHjj6fby82zh8LDjXhL.jpeg",
      "fullname": "Wei Cheng",
      "name": "wchengad",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.15280",
      "authors": [
        {
          "_id": "680703e4c0ce7eea9ba1a548",
          "name": "Chun-Hsiao Yeh",
          "hidden": false
        },
        {
          "_id": "680703e4c0ce7eea9ba1a549",
          "name": "Chenyu Wang",
          "hidden": false
        },
        {
          "_id": "680703e4c0ce7eea9ba1a54a",
          "name": "Shengbang Tong",
          "hidden": false
        },
        {
          "_id": "680703e4c0ce7eea9ba1a54b",
          "name": "Ta-Ying Cheng",
          "hidden": false
        },
        {
          "_id": "680703e4c0ce7eea9ba1a54c",
          "name": "Rouyu Wang",
          "hidden": false
        },
        {
          "_id": "680703e4c0ce7eea9ba1a54d",
          "name": "Tianzhe Chu",
          "hidden": false
        },
        {
          "_id": "680703e4c0ce7eea9ba1a54e",
          "name": "Yuexiang Zhai",
          "hidden": false
        },
        {
          "_id": "680703e4c0ce7eea9ba1a54f",
          "name": "Yubei Chen",
          "hidden": false
        },
        {
          "_id": "680703e4c0ce7eea9ba1a550",
          "name": "Shenghua Gao",
          "hidden": false
        },
        {
          "_id": "680703e4c0ce7eea9ba1a551",
          "name": "Yi Ma",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-21T17:59:53.000Z",
      "submittedOnDailyAt": "2025-04-22T01:49:06.289Z",
      "title": "Seeing from Another Perspective: Evaluating Multi-View Understanding in\n  MLLMs",
      "submittedOnDailyBy": {
        "_id": "637c7420f219c71f93ec8f81",
        "avatarUrl": "/avatars/969b72bd4320423af89e6a5d0ffa03cc.svg",
        "isPro": false,
        "fullname": "frog",
        "user": "frog123123123123",
        "type": "user"
      },
      "summary": "Multi-view understanding, the ability to reconcile visual information across\ndiverse viewpoints for effective navigation, manipulation, and 3D scene\ncomprehension, is a fundamental challenge in Multi-Modal Large Language Models\n(MLLMs) to be used as embodied agents. While recent MLLMs have shown impressive\nadvances in high-level reasoning and planning, they frequently fall short when\nconfronted with multi-view geometric consistency and cross-view correspondence.\nTo comprehensively evaluate the challenges of MLLMs in multi-view scene\nreasoning, we propose All-Angles Bench, a benchmark of over 2,100 human\ncarefully annotated multi-view question-answer pairs across 90 diverse\nreal-world scenes. Our six tasks (counting, attribute identification, relative\ndistance, relative direction, object manipulation, and camera pose estimation)\nspecifically test model's geometric correspondence and the capacity to align\ninformation consistently across views. Our extensive experiments, benchmark on\n27 representative MLLMs including Gemini-2.0-Flash, Claude-3.7-Sonnet, and\nGPT-4o against human evaluators reveals a substantial performance gap,\nindicating that current MLLMs remain far from human-level proficiency. Through\nin-depth analysis, we show that MLLMs are particularly underperforming under\ntwo aspects: (1) cross-view correspondence for partially occluded views and (2)\nestablishing the coarse camera poses. These findings highlight the necessity of\ndomain-specific refinements or modules that embed stronger multi-view\nawareness. We believe that our All-Angles Bench offers valuable insights and\ncontribute to bridging the gap between MLLMs and human-level multi-view\nunderstanding. The project and benchmark are publicly available at\nhttps://danielchyeh.github.io/All-Angles-Bench/.",
      "upvotes": 6,
      "discussionId": "680703e6c0ce7eea9ba1a63e",
      "projectPage": "https://danielchyeh.github.io/All-Angles-Bench/",
      "githubRepo": "https://github.com/Chenyu-Wang567/All-Angles-Bench/tree/main",
      "ai_keywords": [
        "Multi-Modal Large Language Models (MLLMs)",
        "multi-view geometric consistency",
        "cross-view correspondence",
        "All-Angles Bench",
        "Gemini-2.0-Flash",
        "Claude-3.7-Sonnet",
        "GPT-4o",
        "geometric correspondence",
        "camera pose estimation",
        "cross-view correspondence",
        "partially occluded views",
        "coarse camera poses",
        "multi-view awareness"
      ]
    },
    "publishedAt": "2025-04-21T13:59:53.000Z",
    "title": "Seeing from Another Perspective: Evaluating Multi-View Understanding in\n  MLLMs",
    "summary": "Multi-view understanding, the ability to reconcile visual information across\ndiverse viewpoints for effective navigation, manipulation, and 3D scene\ncomprehension, is a fundamental challenge in Multi-Modal Large Language Models\n(MLLMs) to be used as embodied agents. While recent MLLMs have shown impressive\nadvances in high-level reasoning and planning, they frequently fall short when\nconfronted with multi-view geometric consistency and cross-view correspondence.\nTo comprehensively evaluate the challenges of MLLMs in multi-view scene\nreasoning, we propose All-Angles Bench, a benchmark of over 2,100 human\ncarefully annotated multi-view question-answer pairs across 90 diverse\nreal-world scenes. Our six tasks (counting, attribute identification, relative\ndistance, relative direction, object manipulation, and camera pose estimation)\nspecifically test model's geometric correspondence and the capacity to align\ninformation consistently across views. Our extensive experiments, benchmark on\n27 representative MLLMs including Gemini-2.0-Flash, Claude-3.7-Sonnet, and\nGPT-4o against human evaluators reveals a substantial performance gap,\nindicating that current MLLMs remain far from human-level proficiency. Through\nin-depth analysis, we show that MLLMs are particularly underperforming under\ntwo aspects: (1) cross-view correspondence for partially occluded views and (2)\nestablishing the coarse camera poses. These findings highlight the necessity of\ndomain-specific refinements or modules that embed stronger multi-view\nawareness. We believe that our All-Angles Bench offers valuable insights and\ncontribute to bridging the gap between MLLMs and human-level multi-view\nunderstanding. The project and benchmark are publicly available at\nhttps://danielchyeh.github.io/All-Angles-Bench/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15280.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "637c7420f219c71f93ec8f81",
      "avatarUrl": "/avatars/969b72bd4320423af89e6a5d0ffa03cc.svg",
      "fullname": "frog",
      "name": "frog123123123123",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.13805",
      "authors": [
        {
          "_id": "680662b97415e191e3579b9e",
          "name": "Guangyi Liu",
          "hidden": false
        },
        {
          "_id": "680662b97415e191e3579b9f",
          "name": "Pengxiang Zhao",
          "hidden": false
        },
        {
          "_id": "680662b97415e191e3579ba0",
          "name": "Liang Liu",
          "hidden": false
        },
        {
          "_id": "680662b97415e191e3579ba1",
          "name": "Zhiming Chen",
          "hidden": false
        },
        {
          "_id": "680662b97415e191e3579ba2",
          "name": "Yuxiang Chai",
          "hidden": false
        },
        {
          "_id": "680662b97415e191e3579ba3",
          "name": "Shuai Ren",
          "hidden": false
        },
        {
          "_id": "680662b97415e191e3579ba4",
          "name": "Hao Wang",
          "hidden": false
        },
        {
          "_id": "680662b97415e191e3579ba5",
          "name": "Shibo He",
          "hidden": false
        },
        {
          "_id": "680662b97415e191e3579ba6",
          "name": "Wenchao Meng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-18T17:13:34.000Z",
      "submittedOnDailyAt": "2025-04-22T01:14:18.820Z",
      "title": "LearnAct: Few-Shot Mobile GUI Agent with a Unified Demonstration\n  Benchmark",
      "submittedOnDailyBy": {
        "_id": "6458ce236fa580137af5aa95",
        "avatarUrl": "/avatars/db65a7332e375eb5daad5c1b076b1e3b.svg",
        "isPro": false,
        "fullname": "Yuxiang Chai",
        "user": "Yuxiang007",
        "type": "user"
      },
      "summary": "Mobile GUI agents show promise in automating tasks but face generalization\nchallenges in diverse real-world scenarios. Traditional approaches using\npre-training or fine-tuning with massive datasets struggle with the diversity\nof mobile applications and user-specific tasks. We propose enhancing mobile GUI\nagent capabilities through human demonstrations, focusing on improving\nperformance in unseen scenarios rather than pursuing universal generalization\nthrough larger datasets. To realize this paradigm, we introduce LearnGUI, the\nfirst comprehensive dataset specifically designed for studying\ndemonstration-based learning in mobile GUI agents, comprising 2,252 offline\ntasks and 101 online tasks with high-quality human demonstrations. We further\ndevelop LearnAct, a sophisticated multi-agent framework that automatically\nextracts knowledge from demonstrations to enhance task completion. This\nframework integrates three specialized agents: DemoParser for knowledge\nextraction, KnowSeeker for relevant knowledge retrieval, and ActExecutor for\ndemonstration-enhanced task execution. Our experimental results show\nsignificant performance gains in both offline and online evaluations. In\noffline assessments, a single demonstration improves model performance,\nincreasing Gemini-1.5-Pro's accuracy from 19.3% to 51.7%. In online\nevaluations, our framework enhances UI-TARS-7B-SFT's task success rate from\n18.1% to 32.8%. LearnAct framework and LearnGUI benchmark establish\ndemonstration-based learning as a promising direction for more adaptable,\npersonalized, and deployable mobile GUI agents.",
      "upvotes": 6,
      "discussionId": "680662bd7415e191e3579c7d",
      "ai_keywords": [
        "LearnGUI",
        "LearnAct",
        "DemoParser",
        "KnowSeeker",
        "ActExecutor",
        "demonstration-based learning",
        "mobile GUI agents",
        "human demonstrations",
        "multi-agent framework"
      ]
    },
    "publishedAt": "2025-04-18T13:13:34.000Z",
    "title": "LearnAct: Few-Shot Mobile GUI Agent with a Unified Demonstration\n  Benchmark",
    "summary": "Mobile GUI agents show promise in automating tasks but face generalization\nchallenges in diverse real-world scenarios. Traditional approaches using\npre-training or fine-tuning with massive datasets struggle with the diversity\nof mobile applications and user-specific tasks. We propose enhancing mobile GUI\nagent capabilities through human demonstrations, focusing on improving\nperformance in unseen scenarios rather than pursuing universal generalization\nthrough larger datasets. To realize this paradigm, we introduce LearnGUI, the\nfirst comprehensive dataset specifically designed for studying\ndemonstration-based learning in mobile GUI agents, comprising 2,252 offline\ntasks and 101 online tasks with high-quality human demonstrations. We further\ndevelop LearnAct, a sophisticated multi-agent framework that automatically\nextracts knowledge from demonstrations to enhance task completion. This\nframework integrates three specialized agents: DemoParser for knowledge\nextraction, KnowSeeker for relevant knowledge retrieval, and ActExecutor for\ndemonstration-enhanced task execution. Our experimental results show\nsignificant performance gains in both offline and online evaluations. In\noffline assessments, a single demonstration improves model performance,\nincreasing Gemini-1.5-Pro's accuracy from 19.3% to 51.7%. In online\nevaluations, our framework enhances UI-TARS-7B-SFT's task success rate from\n18.1% to 32.8%. LearnAct framework and LearnGUI benchmark establish\ndemonstration-based learning as a promising direction for more adaptable,\npersonalized, and deployable mobile GUI agents.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13805.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6458ce236fa580137af5aa95",
      "avatarUrl": "/avatars/db65a7332e375eb5daad5c1b076b1e3b.svg",
      "fullname": "Yuxiang Chai",
      "name": "Yuxiang007",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.14655",
      "authors": [
        {
          "_id": "68070991b0cb768cae20c585",
          "name": "Yunhui Xia",
          "hidden": false
        },
        {
          "_id": "68070991b0cb768cae20c586",
          "name": "Wei Shen",
          "hidden": false
        },
        {
          "_id": "68070991b0cb768cae20c587",
          "name": "Yan Wang",
          "hidden": false
        },
        {
          "_id": "68070991b0cb768cae20c588",
          "user": {
            "_id": "6731c04d5f55903a1d8c307c",
            "avatarUrl": "/avatars/704b1d628c55e3141194b08736f21267.svg",
            "isPro": false,
            "fullname": "Jason Klein Liu",
            "user": "jasonkleinlove",
            "type": "user"
          },
          "name": "Jason Klein Liu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-22T03:14:26.859Z",
          "hidden": false
        },
        {
          "_id": "68070991b0cb768cae20c589",
          "name": "Huifeng Sun",
          "hidden": false
        },
        {
          "_id": "68070991b0cb768cae20c58a",
          "name": "Siyue Wu",
          "hidden": false
        },
        {
          "_id": "68070991b0cb768cae20c58b",
          "user": {
            "_id": "63f6c04ac96958470d1e9043",
            "avatarUrl": "/avatars/da46cdd9e21498e120ca91b67bfbfb5e.svg",
            "isPro": false,
            "fullname": "Jian Hu",
            "user": "chuyi777",
            "type": "user"
          },
          "name": "Jian Hu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-22T03:14:26.859Z",
          "hidden": false
        },
        {
          "_id": "68070991b0cb768cae20c58c",
          "name": "Xiaolong Xu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-20T15:28:16.000Z",
      "submittedOnDailyAt": "2025-04-22T01:46:36.359Z",
      "title": "LeetCodeDataset: A Temporal Dataset for Robust Evaluation and Efficient\n  Training of Code LLMs",
      "submittedOnDailyBy": {
        "_id": "6468823272d9180d4ac90bdf",
        "avatarUrl": "/avatars/70cb7d65d30ecb944595000ceeeedb1b.svg",
        "isPro": false,
        "fullname": "Wei Shen",
        "user": "Swtheking",
        "type": "user"
      },
      "summary": "We introduce LeetCodeDataset, a high-quality benchmark for evaluating and\ntraining code-generation models, addressing two key challenges in LLM research:\nthe lack of reasoning-focused coding benchmarks and self-contained training\ntestbeds. By curating LeetCode Python problems with rich metadata, broad\ncoverage, 100+ test cases per problem, and temporal splits (pre/post July\n2024), our dataset enables contamination-free evaluation and efficient\nsupervised fine-tuning (SFT). Experiments show reasoning models significantly\noutperform non-reasoning counterparts, while SFT with only 2.6K model-generated\nsolutions achieves performance comparable to 110K-sample counterparts. The\ndataset and evaluation framework are available on Hugging Face and Github.",
      "upvotes": 5,
      "discussionId": "68070992b0cb768cae20c5ca",
      "ai_keywords": [
        "LeetCodeDataset",
        "code-generation models",
        "LLM research",
        "evaluation",
        "supervised fine-tuning (SFT)",
        "reasoning models"
      ]
    },
    "publishedAt": "2025-04-20T11:28:16.000Z",
    "title": "LeetCodeDataset: A Temporal Dataset for Robust Evaluation and Efficient\n  Training of Code LLMs",
    "summary": "We introduce LeetCodeDataset, a high-quality benchmark for evaluating and\ntraining code-generation models, addressing two key challenges in LLM research:\nthe lack of reasoning-focused coding benchmarks and self-contained training\ntestbeds. By curating LeetCode Python problems with rich metadata, broad\ncoverage, 100+ test cases per problem, and temporal splits (pre/post July\n2024), our dataset enables contamination-free evaluation and efficient\nsupervised fine-tuning (SFT). Experiments show reasoning models significantly\noutperform non-reasoning counterparts, while SFT with only 2.6K model-generated\nsolutions achieves performance comparable to 110K-sample counterparts. The\ndataset and evaluation framework are available on Hugging Face and Github.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.14655.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6468823272d9180d4ac90bdf",
      "avatarUrl": "/avatars/70cb7d65d30ecb944595000ceeeedb1b.svg",
      "fullname": "Wei Shen",
      "name": "Swtheking",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.15217",
      "authors": [
        {
          "_id": "68070e1895e06498588c4497",
          "name": "Yatong Bai",
          "hidden": false
        },
        {
          "_id": "68070e1895e06498588c4498",
          "name": "Jonah Casebeer",
          "hidden": false
        },
        {
          "_id": "68070e1895e06498588c4499",
          "name": "Somayeh Sojoudi",
          "hidden": false
        },
        {
          "_id": "68070e1895e06498588c449a",
          "user": {
            "_id": "648119ee7a741c7f33f49f25",
            "avatarUrl": "/avatars/183d2143ba20e1cc8712c63c055aadd7.svg",
            "isPro": false,
            "fullname": "Nicholas J. Bryan",
            "user": "Njb",
            "type": "user"
          },
          "name": "Nicholas J. Bryan",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-04-22T03:35:40.134Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/648119ee7a741c7f33f49f25/V_gAc821aOtnXsoaUFvCQ.mp4"
      ],
      "publishedAt": "2025-04-21T16:41:40.000Z",
      "submittedOnDailyAt": "2025-04-22T02:07:09.310Z",
      "title": "DRAGON: Distributional Rewards Optimize Diffusion Generative Models",
      "submittedOnDailyBy": {
        "_id": "648119ee7a741c7f33f49f25",
        "avatarUrl": "/avatars/183d2143ba20e1cc8712c63c055aadd7.svg",
        "isPro": false,
        "fullname": "Nicholas J. Bryan",
        "user": "Njb",
        "type": "user"
      },
      "summary": "We present Distributional RewArds for Generative OptimizatioN (DRAGON), a\nversatile framework for fine-tuning media generation models towards a desired\noutcome. Compared with traditional reinforcement learning with human feedback\n(RLHF) or pairwise preference approaches such as direct preference optimization\n(DPO), DRAGON is more flexible. It can optimize reward functions that evaluate\neither individual examples or distributions of them, making it compatible with\na broad spectrum of instance-wise, instance-to-distribution, and\ndistribution-to-distribution rewards. Leveraging this versatility, we construct\nnovel reward functions by selecting an encoder and a set of reference examples\nto create an exemplar distribution. When cross-modality encoders such as CLAP\nare used, the reference examples may be of a different modality (e.g., text\nversus audio). Then, DRAGON gathers online and on-policy generations, scores\nthem to construct a positive demonstration set and a negative set, and\nleverages the contrast between the two sets to maximize the reward. For\nevaluation, we fine-tune an audio-domain text-to-music diffusion model with 20\ndifferent reward functions, including a custom music aesthetics model, CLAP\nscore, Vendi diversity, and Frechet audio distance (FAD). We further compare\ninstance-wise (per-song) and full-dataset FAD settings while ablating multiple\nFAD encoders and reference sets. Over all 20 target rewards, DRAGON achieves an\n81.45% average win rate. Moreover, reward functions based on exemplar sets\nindeed enhance generations and are comparable to model-based rewards. With an\nappropriate exemplar set, DRAGON achieves a 60.95% human-voted music quality\nwin rate without training on human preference annotations. As such, DRAGON\nexhibits a new approach to designing and optimizing reward functions for\nimproving human-perceived quality. Sound examples at\nhttps://ml-dragon.github.io/web.",
      "upvotes": 4,
      "discussionId": "68070e1f95e06498588c467f",
      "ai_keywords": [
        "Distributional RewArds for Generative OptimizatioN (DRAGON)",
        "fine-tuning",
        "media generation models",
        "reinforcement learning with human feedback (RLHF)",
        "direct preference optimization (DPO)",
        "reward functions",
        "exemplar distribution",
        "cross-modality encoders",
        "CLAP",
        "online and on-policy generations",
        "positive demonstration set",
        "negative set",
        "contrast",
        "audio-domain text-to-music diffusion model",
        "music aesthetics model",
        "Vendi diversity",
        "Frechet audio distance (FAD)",
        "full-dataset FAD",
        "human-voted music quality"
      ]
    },
    "publishedAt": "2025-04-21T12:41:40.000Z",
    "title": "DRAGON: Distributional Rewards Optimize Diffusion Generative Models",
    "summary": "We present Distributional RewArds for Generative OptimizatioN (DRAGON), a\nversatile framework for fine-tuning media generation models towards a desired\noutcome. Compared with traditional reinforcement learning with human feedback\n(RLHF) or pairwise preference approaches such as direct preference optimization\n(DPO), DRAGON is more flexible. It can optimize reward functions that evaluate\neither individual examples or distributions of them, making it compatible with\na broad spectrum of instance-wise, instance-to-distribution, and\ndistribution-to-distribution rewards. Leveraging this versatility, we construct\nnovel reward functions by selecting an encoder and a set of reference examples\nto create an exemplar distribution. When cross-modality encoders such as CLAP\nare used, the reference examples may be of a different modality (e.g., text\nversus audio). Then, DRAGON gathers online and on-policy generations, scores\nthem to construct a positive demonstration set and a negative set, and\nleverages the contrast between the two sets to maximize the reward. For\nevaluation, we fine-tune an audio-domain text-to-music diffusion model with 20\ndifferent reward functions, including a custom music aesthetics model, CLAP\nscore, Vendi diversity, and Frechet audio distance (FAD). We further compare\ninstance-wise (per-song) and full-dataset FAD settings while ablating multiple\nFAD encoders and reference sets. Over all 20 target rewards, DRAGON achieves an\n81.45% average win rate. Moreover, reward functions based on exemplar sets\nindeed enhance generations and are comparable to model-based rewards. With an\nappropriate exemplar set, DRAGON achieves a 60.95% human-voted music quality\nwin rate without training on human preference annotations. As such, DRAGON\nexhibits a new approach to designing and optimizing reward functions for\nimproving human-perceived quality. Sound examples at\nhttps://ml-dragon.github.io/web.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/648119ee7a741c7f33f49f25/V_gAc821aOtnXsoaUFvCQ.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15217.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648119ee7a741c7f33f49f25",
      "avatarUrl": "/avatars/183d2143ba20e1cc8712c63c055aadd7.svg",
      "fullname": "Nicholas J. Bryan",
      "name": "Njb",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.14239",
      "authors": [
        {
          "_id": "6806e9e819a9fa609609fe65",
          "name": "Yuhang Liu",
          "hidden": false
        },
        {
          "_id": "6806e9e819a9fa609609fe66",
          "name": "Pengxiang Li",
          "hidden": false
        },
        {
          "_id": "6806e9e819a9fa609609fe67",
          "user": {
            "_id": "629084f3a391a907d5e0484e",
            "avatarUrl": "/avatars/3a1d6d5aa90b3b8372505d13ccf8f2dd.svg",
            "isPro": false,
            "fullname": "unk",
            "user": "xieck13",
            "type": "user"
          },
          "name": "Congkai Xie",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-22T00:59:21.913Z",
          "hidden": false
        },
        {
          "_id": "6806e9e819a9fa609609fe68",
          "name": "Xavier Hu",
          "hidden": false
        },
        {
          "_id": "6806e9e819a9fa609609fe69",
          "name": "Xiaotian Han",
          "hidden": false
        },
        {
          "_id": "6806e9e819a9fa609609fe6a",
          "name": "Shengyu Zhang",
          "hidden": false
        },
        {
          "_id": "6806e9e819a9fa609609fe6b",
          "name": "Hongxia Yang",
          "hidden": false
        },
        {
          "_id": "6806e9e819a9fa609609fe6c",
          "name": "Fei Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-19T09:25:55.000Z",
      "submittedOnDailyAt": "2025-04-22T02:26:02.459Z",
      "title": "InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to\n  Deliberative Reasoners",
      "submittedOnDailyBy": {
        "_id": "64245f2c089d5fae56b4549a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64245f2c089d5fae56b4549a/qUHFsL9Svwyj5BKpfMtaY.jpeg",
        "isPro": false,
        "fullname": "Pengxiang Li",
        "user": "pengxiang",
        "type": "user"
      },
      "summary": "Multimodal Large Language Models (MLLMs) have powered Graphical User\nInterface (GUI) Agents, showing promise in automating tasks on computing\ndevices. Recent works have begun exploring reasoning in GUI tasks with\nencouraging results. However, many current approaches rely on manually designed\nreasoning templates, which may result in reasoning that is not sufficiently\nrobust and adaptive for complex GUI environments. Meanwhile, some existing\nagents continue to operate as Reactive Actors, relying primarily on implicit\nreasoning that may lack sufficient depth for GUI tasks demanding planning and\nerror recovery. We argue that advancing these agents requires a shift from\nreactive acting towards acting based on deliberate reasoning. To facilitate\nthis transformation, we introduce InfiGUI-R1, an MLLM-based GUI agent developed\nthrough our Actor2Reasoner framework, a reasoning-centric, two-stage training\napproach designed to progressively evolve agents from Reactive Actors to\nDeliberative Reasoners. The first stage, Reasoning Injection, focuses on\nestablishing a basic reasoner. We employ Spatial Reasoning Distillation to\ntransfer cross-modal spatial reasoning capabilities from teacher models to\nMLLMs through trajectories with explicit reasoning steps, enabling models to\nintegrate GUI visual-spatial information with logical reasoning before action\ngeneration. The second stage, Deliberation Enhancement, refines the basic\nreasoner into a deliberative one using Reinforcement Learning. This stage\nintroduces two approaches: Sub-goal Guidance, which rewards models for\ngenerating accurate intermediate sub-goals, and Error Recovery Scenario\nConstruction, which creates failure-and-recovery training scenarios from\nidentified prone-to-error steps. Experimental results show InfiGUI-R1 achieves\nstrong performance in GUI grounding and trajectory tasks. Resources at\nhttps://github.com/Reallm-Labs/InfiGUI-R1.",
      "upvotes": 4,
      "discussionId": "6806e9e919a9fa609609fea1",
      "ai_keywords": [
        "Multimodal Large Language Models (MLLMs)",
        "Graphical User Interface (GUI) Agents",
        "Spatial Reasoning Distillation",
        "Actor2Reasoner framework",
        "Reasoning Injection",
        "Deliberation Enhancement",
        "Sub-goal Guidance",
        "Error Recovery Scenario Construction",
        "GUI grounding",
        "trajectory tasks"
      ]
    },
    "publishedAt": "2025-04-19T05:25:55.000Z",
    "title": "InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to\n  Deliberative Reasoners",
    "summary": "Multimodal Large Language Models (MLLMs) have powered Graphical User\nInterface (GUI) Agents, showing promise in automating tasks on computing\ndevices. Recent works have begun exploring reasoning in GUI tasks with\nencouraging results. However, many current approaches rely on manually designed\nreasoning templates, which may result in reasoning that is not sufficiently\nrobust and adaptive for complex GUI environments. Meanwhile, some existing\nagents continue to operate as Reactive Actors, relying primarily on implicit\nreasoning that may lack sufficient depth for GUI tasks demanding planning and\nerror recovery. We argue that advancing these agents requires a shift from\nreactive acting towards acting based on deliberate reasoning. To facilitate\nthis transformation, we introduce InfiGUI-R1, an MLLM-based GUI agent developed\nthrough our Actor2Reasoner framework, a reasoning-centric, two-stage training\napproach designed to progressively evolve agents from Reactive Actors to\nDeliberative Reasoners. The first stage, Reasoning Injection, focuses on\nestablishing a basic reasoner. We employ Spatial Reasoning Distillation to\ntransfer cross-modal spatial reasoning capabilities from teacher models to\nMLLMs through trajectories with explicit reasoning steps, enabling models to\nintegrate GUI visual-spatial information with logical reasoning before action\ngeneration. The second stage, Deliberation Enhancement, refines the basic\nreasoner into a deliberative one using Reinforcement Learning. This stage\nintroduces two approaches: Sub-goal Guidance, which rewards models for\ngenerating accurate intermediate sub-goals, and Error Recovery Scenario\nConstruction, which creates failure-and-recovery training scenarios from\nidentified prone-to-error steps. Experimental results show InfiGUI-R1 achieves\nstrong performance in GUI grounding and trajectory tasks. Resources at\nhttps://github.com/Reallm-Labs/InfiGUI-R1.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.14239.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64245f2c089d5fae56b4549a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64245f2c089d5fae56b4549a/qUHFsL9Svwyj5BKpfMtaY.jpeg",
      "fullname": "Pengxiang Li",
      "name": "pengxiang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.15271",
      "authors": [
        {
          "_id": "680748d088578d9444349293",
          "name": "Guo Chen",
          "hidden": false
        },
        {
          "_id": "680748d088578d9444349294",
          "name": "Zhiqi Li",
          "hidden": false
        },
        {
          "_id": "680748d088578d9444349295",
          "name": "Shihao Wang",
          "hidden": false
        },
        {
          "_id": "680748d088578d9444349296",
          "name": "Jindong Jiang",
          "hidden": false
        },
        {
          "_id": "680748d088578d9444349297",
          "name": "Yicheng Liu",
          "hidden": false
        },
        {
          "_id": "680748d088578d9444349298",
          "name": "Lidong Lu",
          "hidden": false
        },
        {
          "_id": "680748d088578d9444349299",
          "name": "De-An Huang",
          "hidden": false
        },
        {
          "_id": "680748d088578d944434929a",
          "name": "Wonmin Byeon",
          "hidden": false
        },
        {
          "_id": "680748d088578d944434929b",
          "name": "Matthieu Le",
          "hidden": false
        },
        {
          "_id": "680748d088578d944434929c",
          "name": "Tuomas Rintamaki",
          "hidden": false
        },
        {
          "_id": "680748d088578d944434929d",
          "name": "Tyler Poon",
          "hidden": false
        },
        {
          "_id": "680748d088578d944434929e",
          "name": "Max Ehrlich",
          "hidden": false
        },
        {
          "_id": "680748d088578d944434929f",
          "name": "Tuomas Rintamaki",
          "hidden": false
        },
        {
          "_id": "680748d088578d94443492a0",
          "name": "Tyler Poon",
          "hidden": false
        },
        {
          "_id": "680748d088578d94443492a1",
          "name": "Tong Lu",
          "hidden": false
        },
        {
          "_id": "680748d088578d94443492a2",
          "name": "Limin Wang",
          "hidden": false
        },
        {
          "_id": "680748d088578d94443492a3",
          "name": "Bryan Catanzaro",
          "hidden": false
        },
        {
          "_id": "680748d088578d94443492a4",
          "name": "Jan Kautz",
          "hidden": false
        },
        {
          "_id": "680748d088578d94443492a5",
          "name": "Andrew Tao",
          "hidden": false
        },
        {
          "_id": "680748d088578d94443492a6",
          "name": "Zhiding Yu",
          "hidden": false
        },
        {
          "_id": "680748d088578d94443492a7",
          "name": "Guilin Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-21T17:57:28.000Z",
      "submittedOnDailyAt": "2025-04-22T06:14:17.808Z",
      "title": "Eagle 2.5: Boosting Long-Context Post-Training for Frontier\n  Vision-Language Models",
      "submittedOnDailyBy": {
        "_id": "6392c73390b8e99a6779a7b0",
        "avatarUrl": "/avatars/9ff824ab02848120aec5e8de6780bcf1.svg",
        "isPro": false,
        "fullname": "Guo Chen",
        "user": "cg1177",
        "type": "user"
      },
      "summary": "We introduce Eagle 2.5, a family of frontier vision-language models (VLMs)\nfor long-context multimodal learning. Our work addresses the challenges in long\nvideo comprehension and high-resolution image understanding, introducing a\ngeneralist framework for both tasks. The proposed training framework\nincorporates Automatic Degrade Sampling and Image Area Preservation, two\ntechniques that preserve contextual integrity and visual details. The framework\nalso includes numerous efficiency optimizations in the pipeline for\nlong-context data training. Finally, we propose Eagle-Video-110K, a novel\ndataset that integrates both story-level and clip-level annotations,\nfacilitating long-video understanding. Eagle 2.5 demonstrates substantial\nimprovements on long-context multimodal benchmarks, providing a robust solution\nto the limitations of existing VLMs. Notably, our best model Eagle 2.5-8B\nachieves 72.4% on Video-MME with 512 input frames, matching the results of\ntop-tier commercial model such as GPT-4o and large-scale open-source models\nlike Qwen2.5-VL-72B and InternVL2.5-78B.",
      "upvotes": 3,
      "discussionId": "680748d188578d9444349311",
      "projectPage": "https://nvlabs.github.io/EAGLE/",
      "githubRepo": "https://github.com/NVlabs/EAGLE"
    },
    "publishedAt": "2025-04-21T13:57:28.000Z",
    "title": "Eagle 2.5: Boosting Long-Context Post-Training for Frontier\n  Vision-Language Models",
    "summary": "We introduce Eagle 2.5, a family of frontier vision-language models (VLMs)\nfor long-context multimodal learning. Our work addresses the challenges in long\nvideo comprehension and high-resolution image understanding, introducing a\ngeneralist framework for both tasks. The proposed training framework\nincorporates Automatic Degrade Sampling and Image Area Preservation, two\ntechniques that preserve contextual integrity and visual details. The framework\nalso includes numerous efficiency optimizations in the pipeline for\nlong-context data training. Finally, we propose Eagle-Video-110K, a novel\ndataset that integrates both story-level and clip-level annotations,\nfacilitating long-video understanding. Eagle 2.5 demonstrates substantial\nimprovements on long-context multimodal benchmarks, providing a robust solution\nto the limitations of existing VLMs. Notably, our best model Eagle 2.5-8B\nachieves 72.4% on Video-MME with 512 input frames, matching the results of\ntop-tier commercial model such as GPT-4o and large-scale open-source models\nlike Qwen2.5-VL-72B and InternVL2.5-78B.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15271.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6392c73390b8e99a6779a7b0",
      "avatarUrl": "/avatars/9ff824ab02848120aec5e8de6780bcf1.svg",
      "fullname": "Guo Chen",
      "name": "cg1177",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.15133",
      "authors": [
        {
          "_id": "68073a2d19a9fa6096218691",
          "name": "Ziwen Xu",
          "hidden": false
        },
        {
          "_id": "68073a2d19a9fa6096218692",
          "name": "Shuxun Wang",
          "hidden": false
        },
        {
          "_id": "68073a2d19a9fa6096218693",
          "name": "Kewei Xu",
          "hidden": false
        },
        {
          "_id": "68073a2d19a9fa6096218694",
          "name": "Haoming Xu",
          "hidden": false
        },
        {
          "_id": "68073a2d19a9fa6096218695",
          "name": "Mengru Wang",
          "hidden": false
        },
        {
          "_id": "68073a2d19a9fa6096218696",
          "name": "Xinle Deng",
          "hidden": false
        },
        {
          "_id": "68073a2d19a9fa6096218697",
          "name": "Yunzhi Yao",
          "hidden": false
        },
        {
          "_id": "68073a2d19a9fa6096218698",
          "name": "Guozhou Zheng",
          "hidden": false
        },
        {
          "_id": "68073a2d19a9fa6096218699",
          "name": "Huajun Chen",
          "hidden": false
        },
        {
          "_id": "68073a2d19a9fa609621869a",
          "name": "Ningyu Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-21T14:33:55.000Z",
      "submittedOnDailyAt": "2025-04-22T05:12:13.220Z",
      "title": "EasyEdit2: An Easy-to-use Steering Framework for Editing Large Language\n  Models",
      "submittedOnDailyBy": {
        "_id": "620b3bbb0668e435407c8d0a",
        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
        "isPro": false,
        "fullname": "Ningyu Zhang",
        "user": "Ningyu",
        "type": "user"
      },
      "summary": "In this paper, we introduce EasyEdit2, a framework designed to enable\nplug-and-play adjustability for controlling Large Language Model (LLM)\nbehaviors. EasyEdit2 supports a wide range of test-time interventions,\nincluding safety, sentiment, personality, reasoning patterns, factuality, and\nlanguage features. Unlike its predecessor, EasyEdit2 features a new\narchitecture specifically designed for seamless model steering. It comprises\nkey modules such as the steering vector generator and the steering vector\napplier, which enable automatic generation and application of steering vectors\nto influence the model's behavior without modifying its parameters. One of the\nmain advantages of EasyEdit2 is its ease of use-users do not need extensive\ntechnical knowledge. With just a single example, they can effectively guide and\nadjust the model's responses, making precise control both accessible and\nefficient. Empirically, we report model steering performance across different\nLLMs, demonstrating the effectiveness of these techniques. We have released the\nsource code on GitHub at https://github.com/zjunlp/EasyEdit along with a\ndemonstration notebook. In addition, we provide a demo video at\nhttps://zjunlp.github.io/project/EasyEdit2/video for a quick introduction.",
      "upvotes": 3,
      "discussionId": "68073a2e19a9fa60962186db",
      "projectPage": "https://zjunlp.github.io/project/EasyEdit2/",
      "githubRepo": "https://github.com/zjunlp/EasyEdit"
    },
    "publishedAt": "2025-04-21T10:33:55.000Z",
    "title": "EasyEdit2: An Easy-to-use Steering Framework for Editing Large Language\n  Models",
    "summary": "In this paper, we introduce EasyEdit2, a framework designed to enable\nplug-and-play adjustability for controlling Large Language Model (LLM)\nbehaviors. EasyEdit2 supports a wide range of test-time interventions,\nincluding safety, sentiment, personality, reasoning patterns, factuality, and\nlanguage features. Unlike its predecessor, EasyEdit2 features a new\narchitecture specifically designed for seamless model steering. It comprises\nkey modules such as the steering vector generator and the steering vector\napplier, which enable automatic generation and application of steering vectors\nto influence the model's behavior without modifying its parameters. One of the\nmain advantages of EasyEdit2 is its ease of use-users do not need extensive\ntechnical knowledge. With just a single example, they can effectively guide and\nadjust the model's responses, making precise control both accessible and\nefficient. Empirically, we report model steering performance across different\nLLMs, demonstrating the effectiveness of these techniques. We have released the\nsource code on GitHub at https://github.com/zjunlp/EasyEdit along with a\ndemonstration notebook. In addition, we provide a demo video at\nhttps://zjunlp.github.io/project/EasyEdit2/video for a quick introduction.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15133.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620b3bbb0668e435407c8d0a",
      "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
      "fullname": "Ningyu Zhang",
      "name": "Ningyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 22
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.15047",
      "authors": [
        {
          "_id": "6806fb8e27dabde0a109776d",
          "name": "Quy-Anh Dang",
          "hidden": false
        },
        {
          "_id": "6806fb8e27dabde0a109776e",
          "name": "Chris Ngo",
          "hidden": false
        },
        {
          "_id": "6806fb8e27dabde0a109776f",
          "name": "Truong-Son Hy",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/645b663eca5d8a297712f2e1/XbUmMPAitDuX-g7n50bTa.png",
        "https://cdn-uploads.huggingface.co/production/uploads/645b663eca5d8a297712f2e1/a16jiYkgx_UtIyM9BiBc2.png"
      ],
      "publishedAt": "2025-04-21T12:04:57.000Z",
      "submittedOnDailyAt": "2025-04-22T00:47:26.080Z",
      "title": "RainbowPlus: Enhancing Adversarial Prompt Generation via Evolutionary\n  Quality-Diversity Search",
      "submittedOnDailyBy": {
        "_id": "645b663eca5d8a297712f2e1",
        "avatarUrl": "/avatars/c61dcba43feb879088b15b525e441cb9.svg",
        "isPro": false,
        "fullname": "Quy-Anh Dang",
        "user": "quyanh",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) exhibit remarkable capabilities but are\nsusceptible to adversarial prompts that exploit vulnerabilities to produce\nunsafe or biased outputs. Existing red-teaming methods often face scalability\nchallenges, resource-intensive requirements, or limited diversity in attack\nstrategies. We propose RainbowPlus, a novel red-teaming framework rooted in\nevolutionary computation, enhancing adversarial prompt generation through an\nadaptive quality-diversity (QD) search that extends classical evolutionary\nalgorithms like MAP-Elites with innovations tailored for language models. By\nemploying a multi-element archive to store diverse high-quality prompts and a\ncomprehensive fitness function to evaluate multiple prompts concurrently,\nRainbowPlus overcomes the constraints of single-prompt archives and pairwise\ncomparisons in prior QD methods like Rainbow Teaming. Experiments comparing\nRainbowPlus to QD methods across six benchmark datasets and four open-source\nLLMs demonstrate superior attack success rate (ASR) and diversity\n(Diverse-Score approx 0.84), generating up to 100 times more unique prompts\n(e.g., 10,418 vs. 100 for Ministral-8B-Instruct-2410). Against nine\nstate-of-the-art methods on the HarmBench dataset with twelve LLMs (ten\nopen-source, two closed-source), RainbowPlus achieves an average ASR of 81.1%,\nsurpassing AutoDAN-Turbo by 3.9%, and is 9 times faster (1.45 vs. 13.50 hours).\nOur open-source implementation fosters further advancements in LLM safety,\noffering a scalable tool for vulnerability assessment. Code and resources are\npublicly available at https://github.com/knoveleng/rainbowplus, supporting\nreproducibility and future research in LLM red-teaming.",
      "upvotes": 3,
      "discussionId": "6806fb9127dabde0a1097849",
      "githubRepo": "https://github.com/knoveleng/rainbowplus",
      "ai_keywords": [
        "adversarial prompts",
        "vulnerabilities",
        "unsafe outputs",
        "biased outputs",
        "red-teaming methods",
        "scalability challenges",
        "resource-intensive requirements",
        "quality-diversity (QD) search",
        "evolutionary computation",
        "MAP-Elites",
        "adaptive quality-diversity (QD) search",
        "multi-element archive",
        "fitness function",
        "single-prompt archives",
        "pairwise comparisons",
        "attack success rate (ASR)",
        "Diverse-Score",
        "superior attack success rate (ASR)",
        "diversity",
        "unique prompts",
        "benchmark datasets",
        "open-source LLMS",
        "HarmBench",
        "closed-source",
        "AutoDAN-Turbo",
        "vulnerability assessment"
      ]
    },
    "publishedAt": "2025-04-21T08:04:57.000Z",
    "title": "RainbowPlus: Enhancing Adversarial Prompt Generation via Evolutionary\n  Quality-Diversity Search",
    "summary": "Large Language Models (LLMs) exhibit remarkable capabilities but are\nsusceptible to adversarial prompts that exploit vulnerabilities to produce\nunsafe or biased outputs. Existing red-teaming methods often face scalability\nchallenges, resource-intensive requirements, or limited diversity in attack\nstrategies. We propose RainbowPlus, a novel red-teaming framework rooted in\nevolutionary computation, enhancing adversarial prompt generation through an\nadaptive quality-diversity (QD) search that extends classical evolutionary\nalgorithms like MAP-Elites with innovations tailored for language models. By\nemploying a multi-element archive to store diverse high-quality prompts and a\ncomprehensive fitness function to evaluate multiple prompts concurrently,\nRainbowPlus overcomes the constraints of single-prompt archives and pairwise\ncomparisons in prior QD methods like Rainbow Teaming. Experiments comparing\nRainbowPlus to QD methods across six benchmark datasets and four open-source\nLLMs demonstrate superior attack success rate (ASR) and diversity\n(Diverse-Score approx 0.84), generating up to 100 times more unique prompts\n(e.g., 10,418 vs. 100 for Ministral-8B-Instruct-2410). Against nine\nstate-of-the-art methods on the HarmBench dataset with twelve LLMs (ten\nopen-source, two closed-source), RainbowPlus achieves an average ASR of 81.1%,\nsurpassing AutoDAN-Turbo by 3.9%, and is 9 times faster (1.45 vs. 13.50 hours).\nOur open-source implementation fosters further advancements in LLM safety,\noffering a scalable tool for vulnerability assessment. Code and resources are\npublicly available at https://github.com/knoveleng/rainbowplus, supporting\nreproducibility and future research in LLM red-teaming.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/645b663eca5d8a297712f2e1/XbUmMPAitDuX-g7n50bTa.png",
      "https://cdn-uploads.huggingface.co/production/uploads/645b663eca5d8a297712f2e1/a16jiYkgx_UtIyM9BiBc2.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15047.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "645b663eca5d8a297712f2e1",
      "avatarUrl": "/avatars/c61dcba43feb879088b15b525e441cb9.svg",
      "fullname": "Quy-Anh Dang",
      "name": "quyanh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.08902",
      "authors": [
        {
          "_id": "680730598ce2be7f5450bd77",
          "name": "Pascal Chang",
          "hidden": false
        },
        {
          "_id": "680730598ce2be7f5450bd78",
          "name": "Sergio Sancho",
          "hidden": false
        },
        {
          "_id": "680730598ce2be7f5450bd79",
          "name": "Jingwei Tang",
          "hidden": false
        },
        {
          "_id": "680730598ce2be7f5450bd7a",
          "name": "Markus Gross",
          "hidden": false
        },
        {
          "_id": "680730598ce2be7f5450bd7b",
          "name": "Vinicius C. Azevedo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-11T18:12:01.000Z",
      "submittedOnDailyAt": "2025-04-22T04:59:10.950Z",
      "title": "LookingGlass: Generative Anamorphoses via Laplacian Pyramid Warping",
      "submittedOnDailyBy": {
        "_id": "66863bd107019f3fe48a21ab",
        "avatarUrl": "/avatars/3297e18e43d40e902b9554a077a34a8a.svg",
        "isPro": false,
        "fullname": "Manuel Kansy",
        "user": "manuelkansy",
        "type": "user"
      },
      "summary": "Anamorphosis refers to a category of images that are intentionally distorted,\nmaking them unrecognizable when viewed directly. Their true form only reveals\nitself when seen from a specific viewpoint, which can be through some\ncatadioptric device like a mirror or a lens. While the construction of these\nmathematical devices can be traced back to as early as the 17th century, they\nare only interpretable when viewed from a specific vantage point and tend to\nlose meaning when seen normally. In this paper, we revisit these famous optical\nillusions with a generative twist. With the help of latent rectified flow\nmodels, we propose a method to create anamorphic images that still retain a\nvalid interpretation when viewed directly. To this end, we introduce Laplacian\nPyramid Warping, a frequency-aware image warping technique key to generating\nhigh-quality visuals. Our work extends Visual Anagrams (arXiv:2311.17919) to\nlatent space models and to a wider range of spatial transforms, enabling the\ncreation of novel generative perceptual illusions.",
      "upvotes": 3,
      "discussionId": "6807305f8ce2be7f5450bf69",
      "ai_keywords": [
        "latent rectified flow models",
        "Laplacian Pyramid Warping",
        "frequency-aware image warping",
        "generative perceptual illusions"
      ]
    },
    "publishedAt": "2025-04-11T14:12:01.000Z",
    "title": "LookingGlass: Generative Anamorphoses via Laplacian Pyramid Warping",
    "summary": "Anamorphosis refers to a category of images that are intentionally distorted,\nmaking them unrecognizable when viewed directly. Their true form only reveals\nitself when seen from a specific viewpoint, which can be through some\ncatadioptric device like a mirror or a lens. While the construction of these\nmathematical devices can be traced back to as early as the 17th century, they\nare only interpretable when viewed from a specific vantage point and tend to\nlose meaning when seen normally. In this paper, we revisit these famous optical\nillusions with a generative twist. With the help of latent rectified flow\nmodels, we propose a method to create anamorphic images that still retain a\nvalid interpretation when viewed directly. To this end, we introduce Laplacian\nPyramid Warping, a frequency-aware image warping technique key to generating\nhigh-quality visuals. Our work extends Visual Anagrams (arXiv:2311.17919) to\nlatent space models and to a wider range of spatial transforms, enabling the\ncreation of novel generative perceptual illusions.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08902.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66863bd107019f3fe48a21ab",
      "avatarUrl": "/avatars/3297e18e43d40e902b9554a077a34a8a.svg",
      "fullname": "Manuel Kansy",
      "name": "manuelkansy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.14717",
      "authors": [
        {
          "_id": "680720e066b60d551b653f1b",
          "name": "Bowei Zhang",
          "hidden": false
        },
        {
          "_id": "680720e066b60d551b653f1c",
          "name": "Lei Ke",
          "hidden": false
        },
        {
          "_id": "680720e066b60d551b653f1d",
          "name": "Adam W. Harley",
          "hidden": false
        },
        {
          "_id": "680720e066b60d551b653f1e",
          "name": "Katerina Fragkiadaki",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6258a6455ea3a0a9b6de3f22/D3KLUoyXGD0mILRMznzGG.gif",
        "https://cdn-uploads.huggingface.co/production/uploads/6258a6455ea3a0a9b6de3f22/qsRjn3Gr351OY_A8w0tGc.gif"
      ],
      "publishedAt": "2025-04-20T19:09:43.000Z",
      "submittedOnDailyAt": "2025-04-22T03:25:44.916Z",
      "title": "TAPIP3D: Tracking Any Point in Persistent 3D Geometry",
      "submittedOnDailyBy": {
        "_id": "6258a6455ea3a0a9b6de3f22",
        "avatarUrl": "/avatars/6eeed72a97fb24465e5e65583fbe50cf.svg",
        "isPro": false,
        "fullname": "Lei Ke",
        "user": "lkeab",
        "type": "user"
      },
      "summary": "We introduce TAPIP3D, a novel approach for long-term 3D point tracking in\nmonocular RGB and RGB-D videos. TAPIP3D represents videos as camera-stabilized\nspatio-temporal feature clouds, leveraging depth and camera motion information\nto lift 2D video features into a 3D world space where camera motion is\neffectively canceled. TAPIP3D iteratively refines multi-frame 3D motion\nestimates within this stabilized representation, enabling robust tracking over\nextended periods. To manage the inherent irregularities of 3D point\ndistributions, we propose a Local Pair Attention mechanism. This 3D\ncontextualization strategy effectively exploits spatial relationships in 3D,\nforming informative feature neighborhoods for precise 3D trajectory estimation.\nOur 3D-centric approach significantly outperforms existing 3D point tracking\nmethods and even enhances 2D tracking accuracy compared to conventional 2D\npixel trackers when accurate depth is available. It supports inference in both\ncamera coordinates (i.e., unstabilized) and world coordinates, and our results\ndemonstrate that compensating for camera motion improves tracking performance.\nOur approach replaces the conventional 2D square correlation neighborhoods used\nin prior 2D and 3D trackers, leading to more robust and accurate results across\nvarious 3D point tracking benchmarks. Project Page: https://tapip3d.github.io",
      "upvotes": 2,
      "discussionId": "680720e166b60d551b653f7d",
      "projectPage": "https://tapip3d.github.io/",
      "githubRepo": "https://github.com/zbw001/TAPIP3D",
      "ai_keywords": [
        "camera-stabilized",
        "spatio-temporal feature clouds",
        "depth and camera motion information",
        "2D video features",
        "3D world space",
        "multi-frame 3D motion estimates",
        "Local Pair Attention mechanism",
        "3D contextualization strategy",
        "spatial relationships in 3D",
        "feature neighborhoods",
        "3D trajectory estimation",
        "3D point tracking",
        "2D tracker",
        "inference in both camera coordinates",
        "world coordinates",
        "3D point tracking benchmarks",
        "2D square correlation neighborhoods"
      ]
    },
    "publishedAt": "2025-04-20T15:09:43.000Z",
    "title": "TAPIP3D: Tracking Any Point in Persistent 3D Geometry",
    "summary": "We introduce TAPIP3D, a novel approach for long-term 3D point tracking in\nmonocular RGB and RGB-D videos. TAPIP3D represents videos as camera-stabilized\nspatio-temporal feature clouds, leveraging depth and camera motion information\nto lift 2D video features into a 3D world space where camera motion is\neffectively canceled. TAPIP3D iteratively refines multi-frame 3D motion\nestimates within this stabilized representation, enabling robust tracking over\nextended periods. To manage the inherent irregularities of 3D point\ndistributions, we propose a Local Pair Attention mechanism. This 3D\ncontextualization strategy effectively exploits spatial relationships in 3D,\nforming informative feature neighborhoods for precise 3D trajectory estimation.\nOur 3D-centric approach significantly outperforms existing 3D point tracking\nmethods and even enhances 2D tracking accuracy compared to conventional 2D\npixel trackers when accurate depth is available. It supports inference in both\ncamera coordinates (i.e., unstabilized) and world coordinates, and our results\ndemonstrate that compensating for camera motion improves tracking performance.\nOur approach replaces the conventional 2D square correlation neighborhoods used\nin prior 2D and 3D trackers, leading to more robust and accurate results across\nvarious 3D point tracking benchmarks. Project Page: https://tapip3d.github.io",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6258a6455ea3a0a9b6de3f22/D3KLUoyXGD0mILRMznzGG.gif",
      "https://cdn-uploads.huggingface.co/production/uploads/6258a6455ea3a0a9b6de3f22/qsRjn3Gr351OY_A8w0tGc.gif"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.14717.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6258a6455ea3a0a9b6de3f22",
      "avatarUrl": "/avatars/6eeed72a97fb24465e5e65583fbe50cf.svg",
      "fullname": "Lei Ke",
      "name": "lkeab",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.13941",
      "authors": [
        {
          "_id": "6806f6ff67a715240a5ab9f8",
          "name": "Syeda Nahida Akter",
          "hidden": false
        },
        {
          "_id": "6806f6ff67a715240a5ab9f9",
          "user": {
            "_id": "66980b9c9baa4382e1678809",
            "avatarUrl": "/avatars/1a516bb7aa7871834c19de708cdd853a.svg",
            "isPro": false,
            "fullname": "Shrimai Prabhumoye",
            "user": "shrimai19",
            "type": "user"
          },
          "name": "Shrimai Prabhumoye",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-22T01:55:12.561Z",
          "hidden": false
        },
        {
          "_id": "6806f6ff67a715240a5ab9fa",
          "name": "Matvei Novikov",
          "hidden": false
        },
        {
          "_id": "6806f6ff67a715240a5ab9fb",
          "name": "Seungju Han",
          "hidden": false
        },
        {
          "_id": "6806f6ff67a715240a5ab9fc",
          "name": "Ying Lin",
          "hidden": false
        },
        {
          "_id": "6806f6ff67a715240a5ab9fd",
          "name": "Evelina Bakhturi",
          "hidden": false
        },
        {
          "_id": "6806f6ff67a715240a5ab9fe",
          "name": "Eric Nyberg",
          "hidden": false
        },
        {
          "_id": "6806f6ff67a715240a5ab9ff",
          "name": "Yejin Choi",
          "hidden": false
        },
        {
          "_id": "6806f6ff67a715240a5aba00",
          "name": "Mostofa Patwary",
          "hidden": false
        },
        {
          "_id": "6806f6ff67a715240a5aba01",
          "name": "Mohammad Shoeybi",
          "hidden": false
        },
        {
          "_id": "6806f6ff67a715240a5aba02",
          "name": "Bryan Catanzaro",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-15T21:37:13.000Z",
      "submittedOnDailyAt": "2025-04-22T00:28:47.499Z",
      "title": "NEMOTRON-CROSSTHINK: Scaling Self-Learning beyond Math Reasoning",
      "submittedOnDailyBy": {
        "_id": "6338dd1776421c0543150467",
        "avatarUrl": "/avatars/4539dcec644e40be33f4a0d419fa66cb.svg",
        "isPro": false,
        "fullname": "Syeda Nahida Akter",
        "user": "SieraL",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) have shown strong reasoning capabilities,\nparticularly when enhanced through Reinforcement Learning (RL). While prior\nwork has successfully applied RL to mathematical reasoning -- where rules and\ncorrectness are well-defined -- generalizing these methods to broader reasoning\ndomains remains challenging due to limited data, the lack of verifiable reward\nstructures, and diverse task requirements. In this work, we propose\nNEMOTRON-CROSSTHINK, a framework that systematically incorporates multi-domain\ncorpora, including both synthetic and real-world question-answer pairs, into RL\ntraining to improve generalization across diverse reasoning tasks.\nNEMOTRON-CROSSTHINK addresses key challenges by (1) incorporating data from\nvaried sources spanning STEM, humanities, social sciences, etc.; (2) applying\nstructured templates (e.g., multiple-choice and open-ended) to control\nanswer-space complexity; (3) filtering for verifiable answers; and (4)\noptimizing data blending strategies that utilizes data from multiple sources\neffectively. Our approach enables scalable and verifiable reward modeling\nbeyond mathematics and demonstrates improved accuracies on both math (MATH-500:\n+30.1%, AMC23:+27.5%) and non-math reasoning benchmarks (MMLU-PRO: +12.8%,\nGPQA-DIAMOND: +11.3%, AGIEVAL: +15.1%, SUPERGPQA: +3.8%). Moreover,\nNEMOTRON-CROSSTHINK exhibits significantly improved response efficiency --\nusing 28% fewer tokens for correct answers -- highlighting more focused and\neffective reasoning. Through NEMOTRON-CROSSTHINK, we demonstrate that\nintegrating multi-domain, multi-format data in RL leads to more accurate,\nefficient, and generalizable LLMs.",
      "upvotes": 2,
      "discussionId": "6806f70067a715240a5aba4c",
      "ai_keywords": [
        "Reinforcement Learning",
        "NEMOTRON-CROSSTHINK",
        "multi-domain corpora",
        "structured templates",
        "answer-space complexity",
        "verifiable answers",
        "data blending strategies",
        "reward modeling",
        "MATH-500",
        "AMC23",
        "MMLU-PRO",
        "GPQA-DIAMOND",
        "AGIEVAL",
        "SUPERGPQA",
        "response efficiency",
        "tokens"
      ]
    },
    "publishedAt": "2025-04-15T17:37:13.000Z",
    "title": "NEMOTRON-CROSSTHINK: Scaling Self-Learning beyond Math Reasoning",
    "summary": "Large Language Models (LLMs) have shown strong reasoning capabilities,\nparticularly when enhanced through Reinforcement Learning (RL). While prior\nwork has successfully applied RL to mathematical reasoning -- where rules and\ncorrectness are well-defined -- generalizing these methods to broader reasoning\ndomains remains challenging due to limited data, the lack of verifiable reward\nstructures, and diverse task requirements. In this work, we propose\nNEMOTRON-CROSSTHINK, a framework that systematically incorporates multi-domain\ncorpora, including both synthetic and real-world question-answer pairs, into RL\ntraining to improve generalization across diverse reasoning tasks.\nNEMOTRON-CROSSTHINK addresses key challenges by (1) incorporating data from\nvaried sources spanning STEM, humanities, social sciences, etc.; (2) applying\nstructured templates (e.g., multiple-choice and open-ended) to control\nanswer-space complexity; (3) filtering for verifiable answers; and (4)\noptimizing data blending strategies that utilizes data from multiple sources\neffectively. Our approach enables scalable and verifiable reward modeling\nbeyond mathematics and demonstrates improved accuracies on both math (MATH-500:\n+30.1%, AMC23:+27.5%) and non-math reasoning benchmarks (MMLU-PRO: +12.8%,\nGPQA-DIAMOND: +11.3%, AGIEVAL: +15.1%, SUPERGPQA: +3.8%). Moreover,\nNEMOTRON-CROSSTHINK exhibits significantly improved response efficiency --\nusing 28% fewer tokens for correct answers -- highlighting more focused and\neffective reasoning. Through NEMOTRON-CROSSTHINK, we demonstrate that\nintegrating multi-domain, multi-format data in RL leads to more accurate,\nefficient, and generalizable LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13941.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6338dd1776421c0543150467",
      "avatarUrl": "/avatars/4539dcec644e40be33f4a0d419fa66cb.svg",
      "fullname": "Syeda Nahida Akter",
      "name": "SieraL",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]