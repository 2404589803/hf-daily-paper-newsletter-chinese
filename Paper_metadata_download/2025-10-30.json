[
  {
    "paper": {
      "id": "2510.23473",
      "authors": [
        {
          "_id": "6900b67f646208eac0d1efe9",
          "name": "Shijian Wang",
          "hidden": false
        },
        {
          "_id": "6900b67f646208eac0d1efea",
          "name": "Jiarui Jin",
          "hidden": false
        },
        {
          "_id": "6900b67f646208eac0d1efeb",
          "name": "Xingjian Wang",
          "hidden": false
        },
        {
          "_id": "6900b67f646208eac0d1efec",
          "name": "Linxin Song",
          "hidden": false
        },
        {
          "_id": "6900b67f646208eac0d1efed",
          "name": "Runhao Fu",
          "hidden": false
        },
        {
          "_id": "6900b67f646208eac0d1efee",
          "name": "Hecheng Wang",
          "hidden": false
        },
        {
          "_id": "6900b67f646208eac0d1efef",
          "name": "Zongyuan Ge",
          "hidden": false
        },
        {
          "_id": "6900b67f646208eac0d1eff0",
          "name": "Yuan Lu",
          "hidden": false
        },
        {
          "_id": "6900b67f646208eac0d1eff1",
          "name": "Xuelian Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-27T16:10:45.000Z",
      "submittedOnDailyAt": "2025-10-30T00:53:11.235Z",
      "title": "Video-Thinker: Sparking \"Thinking with Videos\" via Reinforcement\n  Learning",
      "submittedOnDailyBy": {
        "_id": "66e03eace17fb5ff054b7686",
        "avatarUrl": "/avatars/2b739ff11e43dd9e701c647a92617f20.svg",
        "isPro": false,
        "fullname": "Xiaoxi Li",
        "user": "lixiaoxi45",
        "type": "user"
      },
      "summary": "Recent advances in image reasoning methods, particularly \"Thinking with\nImages\", have demonstrated remarkable success in Multimodal Large Language\nModels (MLLMs); however, this dynamic reasoning paradigm has not yet been\nextended to video reasoning tasks. In this paper, we propose Video-Thinker,\nwhich empowers MLLMs to think with videos by autonomously leveraging their\nintrinsic \"grounding\" and \"captioning\" capabilities to generate reasoning clues\nthroughout the inference process. To spark this capability, we construct\nVideo-Thinker-10K, a curated dataset featuring autonomous tool usage within\nchain-of-thought reasoning sequences. Our training strategy begins with\nSupervised Fine-Tuning (SFT) to learn the reasoning format, followed by Group\nRelative Policy Optimization (GRPO) to strengthen this reasoning capability.\nThrough this approach, Video-Thinker enables MLLMs to autonomously navigate\ngrounding and captioning tasks for video reasoning, eliminating the need for\nconstructing and calling external tools. Extensive experiments demonstrate that\nVideo-Thinker achieves significant performance gains on both in-domain tasks\nand challenging out-of-domain video reasoning benchmarks, including\nVideo-Holmes, CG-Bench-Reasoning, and VRBench. Our Video-Thinker-7B\nsubstantially outperforms existing baselines such as Video-R1 and establishes\nstate-of-the-art performance among 7B-sized MLLMs.",
      "upvotes": 51,
      "discussionId": "6900b67f646208eac0d1eff2",
      "githubRepo": "https://github.com/shijian2001/Video-Thinker",
      "ai_summary": "Video-Thinker, a multimodal large language model, autonomously reasons with videos using intrinsic grounding and captioning capabilities, achieving state-of-the-art performance on various video reasoning benchmarks.",
      "ai_keywords": [
        "Thinking with Images",
        "Multimodal Large Language Models",
        "Video-Thinker",
        "Video-Thinker-10K",
        "Supervised Fine-Tuning",
        "Group Relative Policy Optimization",
        "Video-Holmes",
        "CG-Bench-Reasoning",
        "VRBench",
        "Video-R1"
      ],
      "githubStars": 35
    },
    "publishedAt": "2025-10-27T12:10:45.000Z",
    "title": "Video-Thinker: Sparking \"Thinking with Videos\" via Reinforcement\n  Learning",
    "summary": "Recent advances in image reasoning methods, particularly \"Thinking with\nImages\", have demonstrated remarkable success in Multimodal Large Language\nModels (MLLMs); however, this dynamic reasoning paradigm has not yet been\nextended to video reasoning tasks. In this paper, we propose Video-Thinker,\nwhich empowers MLLMs to think with videos by autonomously leveraging their\nintrinsic \"grounding\" and \"captioning\" capabilities to generate reasoning clues\nthroughout the inference process. To spark this capability, we construct\nVideo-Thinker-10K, a curated dataset featuring autonomous tool usage within\nchain-of-thought reasoning sequences. Our training strategy begins with\nSupervised Fine-Tuning (SFT) to learn the reasoning format, followed by Group\nRelative Policy Optimization (GRPO) to strengthen this reasoning capability.\nThrough this approach, Video-Thinker enables MLLMs to autonomously navigate\ngrounding and captioning tasks for video reasoning, eliminating the need for\nconstructing and calling external tools. Extensive experiments demonstrate that\nVideo-Thinker achieves significant performance gains on both in-domain tasks\nand challenging out-of-domain video reasoning benchmarks, including\nVideo-Holmes, CG-Bench-Reasoning, and VRBench. Our Video-Thinker-7B\nsubstantially outperforms existing baselines such as Video-R1 and establishes\nstate-of-the-art performance among 7B-sized MLLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.23473.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66e03eace17fb5ff054b7686",
      "avatarUrl": "/avatars/2b739ff11e43dd9e701c647a92617f20.svg",
      "fullname": "Xiaoxi Li",
      "name": "lixiaoxi45",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 16
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.23538",
      "authors": [
        {
          "_id": "6902211f646208eac0d1f6eb",
          "name": "Qiushi Sun",
          "hidden": false
        },
        {
          "_id": "6902211f646208eac0d1f6ec",
          "name": "Jingyang Gong",
          "hidden": false
        },
        {
          "_id": "6902211f646208eac0d1f6ed",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "6902211f646208eac0d1f6ee",
          "name": "Qiaosheng Chen",
          "hidden": false
        },
        {
          "_id": "6902211f646208eac0d1f6ef",
          "name": "Lei Li",
          "hidden": false
        },
        {
          "_id": "6902211f646208eac0d1f6f0",
          "name": "Kai Chen",
          "hidden": false
        },
        {
          "_id": "6902211f646208eac0d1f6f1",
          "name": "Qipeng Guo",
          "hidden": false
        },
        {
          "_id": "6902211f646208eac0d1f6f2",
          "name": "Ben Kao",
          "hidden": false
        },
        {
          "_id": "6902211f646208eac0d1f6f3",
          "name": "Fei Yuan",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6064a0eeb1703ddba0d458b9/fmd7sD8Gcva93B8N_vWv2.png",
        "https://cdn-uploads.huggingface.co/production/uploads/6064a0eeb1703ddba0d458b9/Rj2W89GUpwdfi743AWnXu.png"
      ],
      "publishedAt": "2025-10-27T17:13:49.000Z",
      "submittedOnDailyAt": "2025-10-30T01:44:09.561Z",
      "title": "JanusCoder: Towards a Foundational Visual-Programmatic Interface for\n  Code Intelligence",
      "submittedOnDailyBy": {
        "_id": "6064a0eeb1703ddba0d458b9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1617207525789-noauth.png",
        "isPro": false,
        "fullname": "Qiushi",
        "user": "QiushiSun",
        "type": "user"
      },
      "summary": "The scope of neural code intelligence is rapidly expanding beyond text-based\nsource code to encompass the rich visual outputs that programs generate. This\nvisual dimension is critical for advanced applications like flexible content\ngeneration and precise, program-driven editing of visualizations. However,\nprogress has been impeded by the scarcity of high-quality multimodal code data,\na bottleneck stemming from challenges in synthesis and quality assessment. To\naddress these challenges, we make contributions from both a data and modeling\nperspective. We first introduce a complete synthesis toolkit that leverages\nreciprocal synergies between data modalities to efficiently produce a\nlarge-scale, high-quality corpus spanning from standard charts to complex\ninteractive web UIs and code-driven animations. Leveraging this toolkit, we\nconstruct JanusCode-800K, the largest multimodal code corpus to date. This\npowers the training of our models, JanusCoder and JanusCoderV, which establish\na visual-programmatic interface for generating code from textual instructions,\nvisual inputs, or a combination of both. Our unified model is a departure from\nexisting approaches that build specialized models for isolated tasks. Extensive\nexperiments on both text-centric and vision-centric coding tasks demonstrate\nthe superior performance of the JanusCoder series, with our 7B to 14B scale\nmodels approaching or even exceeding the performance of commercial models.\nFurthermore, extensive analysis provides key insights into harmonizing\nprogrammatic logic with its visual expression. Our code and checkpoints will\nare available at https://github.com/InternLM/JanusCoder.",
      "upvotes": 50,
      "discussionId": "69022120646208eac0d1f6f4",
      "githubRepo": "https://github.com/InternLM/JanusCoder",
      "ai_summary": "A unified multimodal code corpus and model, JanusCoder, generate code from text and visual inputs, outperforming commercial models in various coding tasks.",
      "ai_keywords": [
        "neural code intelligence",
        "visual outputs",
        "content generation",
        "program-driven editing",
        "multimodal code data",
        "synthesis toolkit",
        "JanusCode-800K",
        "JanusCoder",
        "JanusCoderV",
        "visual-programmatic interface",
        "text-centric",
        "vision-centric coding tasks"
      ],
      "githubStars": 20,
      "organization": {
        "_id": "64a2d5fa81252883206f24c9",
        "name": "internlm",
        "fullname": "Intern Large Models",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6445306bc525660aa2099ecc/ipmEgm86UIby2q5q7NkKm.jpeg"
      }
    },
    "publishedAt": "2025-10-27T13:13:49.000Z",
    "title": "JanusCoder: Towards a Foundational Visual-Programmatic Interface for\n  Code Intelligence",
    "summary": "The scope of neural code intelligence is rapidly expanding beyond text-based\nsource code to encompass the rich visual outputs that programs generate. This\nvisual dimension is critical for advanced applications like flexible content\ngeneration and precise, program-driven editing of visualizations. However,\nprogress has been impeded by the scarcity of high-quality multimodal code data,\na bottleneck stemming from challenges in synthesis and quality assessment. To\naddress these challenges, we make contributions from both a data and modeling\nperspective. We first introduce a complete synthesis toolkit that leverages\nreciprocal synergies between data modalities to efficiently produce a\nlarge-scale, high-quality corpus spanning from standard charts to complex\ninteractive web UIs and code-driven animations. Leveraging this toolkit, we\nconstruct JanusCode-800K, the largest multimodal code corpus to date. This\npowers the training of our models, JanusCoder and JanusCoderV, which establish\na visual-programmatic interface for generating code from textual instructions,\nvisual inputs, or a combination of both. Our unified model is a departure from\nexisting approaches that build specialized models for isolated tasks. Extensive\nexperiments on both text-centric and vision-centric coding tasks demonstrate\nthe superior performance of the JanusCoder series, with our 7B to 14B scale\nmodels approaching or even exceeding the performance of commercial models.\nFurthermore, extensive analysis provides key insights into harmonizing\nprogrammatic logic with its visual expression. Our code and checkpoints will\nare available at https://github.com/InternLM/JanusCoder.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6064a0eeb1703ddba0d458b9/fmd7sD8Gcva93B8N_vWv2.png",
      "https://cdn-uploads.huggingface.co/production/uploads/6064a0eeb1703ddba0d458b9/Rj2W89GUpwdfi743AWnXu.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.23538.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6064a0eeb1703ddba0d458b9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1617207525789-noauth.png",
      "fullname": "Qiushi",
      "name": "QiushiSun",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 20
    },
    "organization": {
      "_id": "64a2d5fa81252883206f24c9",
      "name": "internlm",
      "fullname": "Intern Large Models",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6445306bc525660aa2099ecc/ipmEgm86UIby2q5q7NkKm.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.24592",
      "authors": [
        {
          "_id": "69017aff646208eac0d1f3cb",
          "user": {
            "_id": "63f06116f1a47aaea5bd497b",
            "avatarUrl": "/avatars/7d99ffa59c4579599e852a0ffb261268.svg",
            "isPro": false,
            "fullname": "Guoxin Chen",
            "user": "GuoxinChen",
            "type": "user"
          },
          "name": "Guoxin Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-29T12:45:10.856Z",
          "hidden": false
        },
        {
          "_id": "69017aff646208eac0d1f3cc",
          "name": "Jing Wu",
          "hidden": false
        },
        {
          "_id": "69017aff646208eac0d1f3cd",
          "name": "Xinjie Chen",
          "hidden": false
        },
        {
          "_id": "69017aff646208eac0d1f3ce",
          "name": "Wayne Xin Zhao",
          "hidden": false
        },
        {
          "_id": "69017aff646208eac0d1f3cf",
          "name": "Ruihua Song",
          "hidden": false
        },
        {
          "_id": "69017aff646208eac0d1f3d0",
          "name": "Chengxi Li",
          "hidden": false
        },
        {
          "_id": "69017aff646208eac0d1f3d1",
          "name": "Kai Fan",
          "hidden": false
        },
        {
          "_id": "69017aff646208eac0d1f3d2",
          "name": "Dayiheng Liu",
          "hidden": false
        },
        {
          "_id": "69017aff646208eac0d1f3d3",
          "name": "Minpeng Liao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-28T16:22:54.000Z",
      "submittedOnDailyAt": "2025-10-30T00:46:04.507Z",
      "title": "ReForm: Reflective Autoformalization with Prospective Bounded Sequence\n  Optimization",
      "submittedOnDailyBy": {
        "_id": "63f06116f1a47aaea5bd497b",
        "avatarUrl": "/avatars/7d99ffa59c4579599e852a0ffb261268.svg",
        "isPro": false,
        "fullname": "Guoxin Chen",
        "user": "GuoxinChen",
        "type": "user"
      },
      "summary": "Autoformalization, which translates natural language mathematics into\nmachine-verifiable formal statements, is critical for using formal mathematical\nreasoning to solve math problems stated in natural language. While Large\nLanguage Models can generate syntactically correct formal statements, they\noften fail to preserve the original problem's semantic intent. This limitation\narises from the LLM approaches' treating autoformalization as a simplistic\ntranslation task which lacks mechanisms for self-reflection and iterative\nrefinement that human experts naturally employ. To address these issues, we\npropose ReForm, a Reflective Autoformalization method that tightly integrates\nsemantic consistency evaluation into the autoformalization process. This\nenables the model to iteratively generate formal statements, assess its\nsemantic fidelity, and self-correct identified errors through progressive\nrefinement. To effectively train this reflective model, we introduce\nProspective Bounded Sequence Optimization (PBSO), which employs different\nrewards at different sequence positions to ensure that the model develops both\naccurate autoformalization and correct semantic validations, preventing\nsuperficial critiques that would undermine the purpose of reflection. Extensive\nexperiments across four autoformalization benchmarks demonstrate that ReForm\nachieves an average improvement of 17.2 percentage points over the strongest\nbaselines. To further ensure evaluation reliability, we introduce\nConsistencyCheck, a benchmark of 859 expert-annotated items that not only\nvalidates LLMs as judges but also reveals that autoformalization is inherently\ndifficult: even human experts produce semantic errors in up to 38.5% of cases.",
      "upvotes": 32,
      "discussionId": "69017aff646208eac0d1f3d4",
      "githubRepo": "https://github.com/Chen-GX/ReForm",
      "ai_summary": "ReForm, a reflective autoformalization method, improves the semantic accuracy of formal statements generated from natural language mathematics through iterative refinement and semantic consistency evaluation.",
      "ai_keywords": [
        "autoformalization",
        "Large Language Models",
        "semantic consistency evaluation",
        "iterative refinement",
        "Prospective Bounded Sequence Optimization",
        "PBSO",
        "ConsistencyCheck"
      ],
      "githubStars": 9
    },
    "publishedAt": "2025-10-28T12:22:54.000Z",
    "title": "ReForm: Reflective Autoformalization with Prospective Bounded Sequence\n  Optimization",
    "summary": "Autoformalization, which translates natural language mathematics into\nmachine-verifiable formal statements, is critical for using formal mathematical\nreasoning to solve math problems stated in natural language. While Large\nLanguage Models can generate syntactically correct formal statements, they\noften fail to preserve the original problem's semantic intent. This limitation\narises from the LLM approaches' treating autoformalization as a simplistic\ntranslation task which lacks mechanisms for self-reflection and iterative\nrefinement that human experts naturally employ. To address these issues, we\npropose ReForm, a Reflective Autoformalization method that tightly integrates\nsemantic consistency evaluation into the autoformalization process. This\nenables the model to iteratively generate formal statements, assess its\nsemantic fidelity, and self-correct identified errors through progressive\nrefinement. To effectively train this reflective model, we introduce\nProspective Bounded Sequence Optimization (PBSO), which employs different\nrewards at different sequence positions to ensure that the model develops both\naccurate autoformalization and correct semantic validations, preventing\nsuperficial critiques that would undermine the purpose of reflection. Extensive\nexperiments across four autoformalization benchmarks demonstrate that ReForm\nachieves an average improvement of 17.2 percentage points over the strongest\nbaselines. To further ensure evaluation reliability, we introduce\nConsistencyCheck, a benchmark of 859 expert-annotated items that not only\nvalidates LLMs as judges but also reveals that autoformalization is inherently\ndifficult: even human experts produce semantic errors in up to 38.5% of cases.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.24592.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63f06116f1a47aaea5bd497b",
      "avatarUrl": "/avatars/7d99ffa59c4579599e852a0ffb261268.svg",
      "fullname": "Guoxin Chen",
      "name": "GuoxinChen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.25741",
      "authors": [
        {
          "_id": "6902c3f772739622ee92a6e8",
          "name": "Rui-Jie Zhu",
          "hidden": false
        },
        {
          "_id": "6902c3f772739622ee92a6e9",
          "name": "Zixuan Wang",
          "hidden": false
        },
        {
          "_id": "6902c3f772739622ee92a6ea",
          "name": "Kai Hua",
          "hidden": false
        },
        {
          "_id": "6902c3f772739622ee92a6eb",
          "name": "Tianyu Zhang",
          "hidden": false
        },
        {
          "_id": "6902c3f772739622ee92a6ec",
          "name": "Ziniu Li",
          "hidden": false
        },
        {
          "_id": "6902c3f772739622ee92a6ed",
          "name": "Haoran Que",
          "hidden": false
        },
        {
          "_id": "6902c3f772739622ee92a6ee",
          "name": "Boyi Wei",
          "hidden": false
        },
        {
          "_id": "6902c3f772739622ee92a6ef",
          "name": "Zixin Wen",
          "hidden": false
        },
        {
          "_id": "6902c3f772739622ee92a6f0",
          "name": "Fan Yin",
          "hidden": false
        },
        {
          "_id": "6902c3f772739622ee92a6f1",
          "name": "He Xing",
          "hidden": false
        },
        {
          "_id": "6902c3f772739622ee92a6f2",
          "name": "Lu Li",
          "hidden": false
        },
        {
          "_id": "6902c3f772739622ee92a6f3",
          "name": "Jiajun Shi",
          "hidden": false
        },
        {
          "_id": "6902c3f772739622ee92a6f4",
          "name": "Kaijing Ma",
          "hidden": false
        },
        {
          "_id": "6902c3f772739622ee92a6f5",
          "name": "Shanda Li",
          "hidden": false
        },
        {
          "_id": "6902c3f772739622ee92a6f6",
          "name": "Taylor Kergan",
          "hidden": false
        },
        {
          "_id": "6902c3f772739622ee92a6f7",
          "name": "Andrew Smith",
          "hidden": false
        },
        {
          "_id": "6902c3f772739622ee92a6f8",
          "name": "Xingwei Qu",
          "hidden": false
        },
        {
          "_id": "6902c3f772739622ee92a6f9",
          "name": "Mude Hui",
          "hidden": false
        },
        {
          "_id": "6902c3f772739622ee92a6fa",
          "name": "Bohong Wu",
          "hidden": false
        },
        {
          "_id": "6902c3f772739622ee92a6fb",
          "name": "Qiyang Min",
          "hidden": false
        },
        {
          "_id": "6902c3f772739622ee92a6fc",
          "name": "Hongzhi Huang",
          "hidden": false
        },
        {
          "_id": "6902c3f772739622ee92a6fd",
          "name": "Xun Zhou",
          "hidden": false
        },
        {
          "_id": "6902c3f772739622ee92a6fe",
          "name": "Wei Ye",
          "hidden": false
        },
        {
          "_id": "6902c3f772739622ee92a6ff",
          "name": "Jiaheng Liu",
          "hidden": false
        },
        {
          "_id": "6902c3f772739622ee92a700",
          "name": "Jian Yang",
          "hidden": false
        },
        {
          "_id": "6902c3f772739622ee92a701",
          "name": "Yunfeng Shi",
          "hidden": false
        },
        {
          "_id": "6902c3f772739622ee92a702",
          "name": "Chenghua Lin",
          "hidden": false
        },
        {
          "_id": "6902c3f772739622ee92a703",
          "name": "Enduo Zhao",
          "hidden": false
        },
        {
          "_id": "6902c3f772739622ee92a704",
          "name": "Tianle Cai",
          "hidden": false
        },
        {
          "_id": "6902c3f772739622ee92a705",
          "name": "Ge Zhang",
          "hidden": false
        },
        {
          "_id": "6902c3f772739622ee92a706",
          "name": "Wenhao Huang",
          "hidden": false
        },
        {
          "_id": "6902c3f772739622ee92a707",
          "name": "Yoshua Bengio",
          "hidden": false
        },
        {
          "_id": "6902c3f772739622ee92a708",
          "name": "Jason Eshraghian",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-29T17:45:42.000Z",
      "submittedOnDailyAt": "2025-10-30T00:18:54.545Z",
      "title": "Scaling Latent Reasoning via Looped Language Models",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Modern LLMs are trained to \"think\" primarily via explicit text generation,\nsuch as chain-of-thought (CoT), which defers reasoning to post-training and\nunder-leverages pre-training data. We present and open-source Ouro, named after\nthe recursive Ouroboros, a family of pre-trained Looped Language Models\n(LoopLM) that instead build reasoning into the pre-training phase through (i)\niterative computation in latent space, (ii) an entropy-regularized objective\nfor learned depth allocation, and (iii) scaling to 7.7T tokens. Ouro 1.4B and\n2.6B models enjoy superior performance that match the results of up to 12B SOTA\nLLMs across a wide range of benchmarks. Through controlled experiments, we show\nthis advantage stems not from increased knowledge capacity, but from superior\nknowledge manipulation capabilities. We also show that LoopLM yields reasoning\ntraces more aligned with final outputs than explicit CoT. We hope our results\nshow the potential of LoopLM as a novel scaling direction in the reasoning era.\nOur model could be found in: http://ouro-llm.github.io.",
      "upvotes": 30,
      "discussionId": "6902c3f772739622ee92a709",
      "projectPage": "https://ouro-llm.github.io/",
      "organization": {
        "_id": "67d1140985ea0644e2f14b99",
        "name": "ByteDance-Seed",
        "fullname": "ByteDance Seed",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
      }
    },
    "publishedAt": "2025-10-29T13:45:42.000Z",
    "title": "Scaling Latent Reasoning via Looped Language Models",
    "summary": "Modern LLMs are trained to \"think\" primarily via explicit text generation,\nsuch as chain-of-thought (CoT), which defers reasoning to post-training and\nunder-leverages pre-training data. We present and open-source Ouro, named after\nthe recursive Ouroboros, a family of pre-trained Looped Language Models\n(LoopLM) that instead build reasoning into the pre-training phase through (i)\niterative computation in latent space, (ii) an entropy-regularized objective\nfor learned depth allocation, and (iii) scaling to 7.7T tokens. Ouro 1.4B and\n2.6B models enjoy superior performance that match the results of up to 12B SOTA\nLLMs across a wide range of benchmarks. Through controlled experiments, we show\nthis advantage stems not from increased knowledge capacity, but from superior\nknowledge manipulation capabilities. We also show that LoopLM yields reasoning\ntraces more aligned with final outputs than explicit CoT. We hope our results\nshow the potential of LoopLM as a novel scaling direction in the reasoning era.\nOur model could be found in: http://ouro-llm.github.io.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.25741.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 150
    },
    "organization": {
      "_id": "67d1140985ea0644e2f14b99",
      "name": "ByteDance-Seed",
      "fullname": "ByteDance Seed",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.25065",
      "authors": [
        {
          "_id": "6902cbef72739622ee92a7bd",
          "name": "Taekhyun Park",
          "hidden": false
        },
        {
          "_id": "6902cbef72739622ee92a7be",
          "name": "Yongjae Lee",
          "hidden": false
        },
        {
          "_id": "6902cbef72739622ee92a7bf",
          "name": "Hyerim Bae",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-29T01:07:45.000Z",
      "submittedOnDailyAt": "2025-10-30T02:43:45.380Z",
      "title": "Reasoning-Aware GRPO using Process Mining",
      "submittedOnDailyBy": {
        "_id": "64ba75761d0a5a5760874197",
        "avatarUrl": "/avatars/932857f6178373e977357e2269689c78.svg",
        "isPro": false,
        "fullname": "TaekHyunPark",
        "user": "Thrillcrazyer",
        "type": "user"
      },
      "summary": "Reinforcement learning (RL)-based post-training has been crucial for enabling\nmulti-step reasoning in large reasoning models (LRMs), yet current reward\nschemes are typically outcome-centric. We propose PM4GRPO, a reasoning-aware\nGroup Relative Policy Optimization (GRPO) that augments standard answer/format\nrewards with signals over the reasoning procedure. To this end, process mining\ntechniques are utilized to compute a scalar conformance reward that measures\nhow closely a policy model's reasoning aligns with the pretrained teacher\nmodel. The empirical results on five benchmarks demonstrate that PM4GRPO\nsignificantly outperforms existing methodologies for GRPO-based post-training.\nThese results highlight that leveraging process mining for reasoning-aware GRPO\neffectively enhances the reasoning capabilities of policy models.",
      "upvotes": 29,
      "discussionId": "6902cbf072739622ee92a7c0",
      "organization": {
        "_id": "6902caeadf78e6ca12c2a398",
        "name": "BAELABPNU",
        "fullname": "BIGDATA ANALYTICS ENGINEERING LAB, Pusan National University, Busan, Korea",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6902c9cb9427990a4948a33e/L59exY-PO66fQXk3lNwQ-.png"
      }
    },
    "publishedAt": "2025-10-28T21:07:45.000Z",
    "title": "Reasoning-Aware GRPO using Process Mining",
    "summary": "Reinforcement learning (RL)-based post-training has been crucial for enabling\nmulti-step reasoning in large reasoning models (LRMs), yet current reward\nschemes are typically outcome-centric. We propose PM4GRPO, a reasoning-aware\nGroup Relative Policy Optimization (GRPO) that augments standard answer/format\nrewards with signals over the reasoning procedure. To this end, process mining\ntechniques are utilized to compute a scalar conformance reward that measures\nhow closely a policy model's reasoning aligns with the pretrained teacher\nmodel. The empirical results on five benchmarks demonstrate that PM4GRPO\nsignificantly outperforms existing methodologies for GRPO-based post-training.\nThese results highlight that leveraging process mining for reasoning-aware GRPO\neffectively enhances the reasoning capabilities of policy models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.25065.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ba75761d0a5a5760874197",
      "avatarUrl": "/avatars/932857f6178373e977357e2269689c78.svg",
      "fullname": "TaekHyunPark",
      "name": "Thrillcrazyer",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "organization": {
      "_id": "6902caeadf78e6ca12c2a398",
      "name": "BAELABPNU",
      "fullname": "BIGDATA ANALYTICS ENGINEERING LAB, Pusan National University, Busan, Korea",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6902c9cb9427990a4948a33e/L59exY-PO66fQXk3lNwQ-.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.25726",
      "authors": [
        {
          "_id": "6902bdbe72739622ee92a654",
          "name": "Junlong Li",
          "hidden": false
        },
        {
          "_id": "6902bdbe72739622ee92a655",
          "name": "Wenshuo Zhao",
          "hidden": false
        },
        {
          "_id": "6902bdbe72739622ee92a656",
          "name": "Jian Zhao",
          "hidden": false
        },
        {
          "_id": "6902bdbe72739622ee92a657",
          "name": "Weihao Zeng",
          "hidden": false
        },
        {
          "_id": "6902bdbe72739622ee92a658",
          "name": "Haoze Wu",
          "hidden": false
        },
        {
          "_id": "6902bdbe72739622ee92a659",
          "name": "Xiaochen Wang",
          "hidden": false
        },
        {
          "_id": "6902bdbe72739622ee92a65a",
          "name": "Rui Ge",
          "hidden": false
        },
        {
          "_id": "6902bdbe72739622ee92a65b",
          "name": "Yuxuan Cao",
          "hidden": false
        },
        {
          "_id": "6902bdbe72739622ee92a65c",
          "name": "Yuzhen Huang",
          "hidden": false
        },
        {
          "_id": "6902bdbe72739622ee92a65d",
          "name": "Wei Liu",
          "hidden": false
        },
        {
          "_id": "6902bdbe72739622ee92a65e",
          "name": "Junteng Liu",
          "hidden": false
        },
        {
          "_id": "6902bdbe72739622ee92a65f",
          "name": "Zhaochen Su",
          "hidden": false
        },
        {
          "_id": "6902bdbe72739622ee92a660",
          "name": "Yiyang Guo",
          "hidden": false
        },
        {
          "_id": "6902bdbe72739622ee92a661",
          "name": "Fan Zhou",
          "hidden": false
        },
        {
          "_id": "6902bdbe72739622ee92a662",
          "name": "Lueyang Zhang",
          "hidden": false
        },
        {
          "_id": "6902bdbe72739622ee92a663",
          "name": "Juan Michelini",
          "hidden": false
        },
        {
          "_id": "6902bdbe72739622ee92a664",
          "name": "Xingyao Wang",
          "hidden": false
        },
        {
          "_id": "6902bdbe72739622ee92a665",
          "name": "Xiang Yue",
          "hidden": false
        },
        {
          "_id": "6902bdbe72739622ee92a666",
          "name": "Shuyan Zhou",
          "hidden": false
        },
        {
          "_id": "6902bdbe72739622ee92a667",
          "name": "Graham Neubig",
          "hidden": false
        },
        {
          "_id": "6902bdbe72739622ee92a668",
          "name": "Junxian He",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-29T17:32:49.000Z",
      "submittedOnDailyAt": "2025-10-30T00:27:17.128Z",
      "title": "The Tool Decathlon: Benchmarking Language Agents for Diverse, Realistic,\n  and Long-Horizon Task Execution",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Real-world language agents must handle complex, multi-step workflows across\ndiverse Apps. For instance, an agent may manage emails by coordinating with\ncalendars and file systems, or monitor a production database to detect\nanomalies and generate reports following an operating manual. However, existing\nlanguage agent benchmarks often focus on narrow domains or simplified tasks\nthat lack the diversity, realism, and long-horizon complexity required to\nevaluate agents' real-world performance. To address this gap, we introduce the\nTool Decathlon (dubbed as Toolathlon), a benchmark for language agents offering\ndiverse Apps and tools, realistic environment setup, and reliable\nexecution-based evaluation. Toolathlon spans 32 software applications and 604\ntools, ranging from everyday platforms such as Google Calendar and Notion to\nprofessional ones like WooCommerce, Kubernetes, and BigQuery. Most of the tools\nare based on a high-quality set of Model Context Protocol (MCP) servers that we\nmay have revised or implemented ourselves. Unlike prior works, which primarily\nensure functional realism but offer limited environment state diversity, we\nprovide realistic initial environment states from real software, such as Canvas\ncourses with dozens of students or real financial spreadsheets. This benchmark\nincludes 108 manually sourced or crafted tasks in total, requiring interacting\nwith multiple Apps over around 20 turns on average to complete. Each task is\nstrictly verifiable through dedicated evaluation scripts. Comprehensive\nevaluation of SOTA models highlights their significant shortcomings: the\nbest-performing model, Claude-4.5-Sonnet, achieves only a 38.6% success rate\nwith 20.2 tool calling turns on average, while the top open-weights model\nDeepSeek-V3.2-Exp reaches 20.1%. We expect Toolathlon to drive the development\nof more capable language agents for real-world, long-horizon task execution.",
      "upvotes": 23,
      "discussionId": "6902bdbe72739622ee92a669",
      "projectPage": "https://toolathlon.xyz/introduction",
      "githubRepo": "https://github.com/hkust-nlp/Toolathlon",
      "githubStars": 15
    },
    "publishedAt": "2025-10-29T13:32:49.000Z",
    "title": "The Tool Decathlon: Benchmarking Language Agents for Diverse, Realistic,\n  and Long-Horizon Task Execution",
    "summary": "Real-world language agents must handle complex, multi-step workflows across\ndiverse Apps. For instance, an agent may manage emails by coordinating with\ncalendars and file systems, or monitor a production database to detect\nanomalies and generate reports following an operating manual. However, existing\nlanguage agent benchmarks often focus on narrow domains or simplified tasks\nthat lack the diversity, realism, and long-horizon complexity required to\nevaluate agents' real-world performance. To address this gap, we introduce the\nTool Decathlon (dubbed as Toolathlon), a benchmark for language agents offering\ndiverse Apps and tools, realistic environment setup, and reliable\nexecution-based evaluation. Toolathlon spans 32 software applications and 604\ntools, ranging from everyday platforms such as Google Calendar and Notion to\nprofessional ones like WooCommerce, Kubernetes, and BigQuery. Most of the tools\nare based on a high-quality set of Model Context Protocol (MCP) servers that we\nmay have revised or implemented ourselves. Unlike prior works, which primarily\nensure functional realism but offer limited environment state diversity, we\nprovide realistic initial environment states from real software, such as Canvas\ncourses with dozens of students or real financial spreadsheets. This benchmark\nincludes 108 manually sourced or crafted tasks in total, requiring interacting\nwith multiple Apps over around 20 turns on average to complete. Each task is\nstrictly verifiable through dedicated evaluation scripts. Comprehensive\nevaluation of SOTA models highlights their significant shortcomings: the\nbest-performing model, Claude-4.5-Sonnet, achieves only a 38.6% success rate\nwith 20.2 tool calling turns on average, while the top open-weights model\nDeepSeek-V3.2-Exp reaches 20.1%. We expect Toolathlon to drive the development\nof more capable language agents for real-world, long-horizon task execution.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.25726.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 150
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.25590",
      "authors": [
        {
          "_id": "6902c36272739622ee92a6a0",
          "name": "Pengtao Chen",
          "hidden": false
        },
        {
          "_id": "6902c36272739622ee92a6a1",
          "name": "Xianfang Zeng",
          "hidden": false
        },
        {
          "_id": "6902c36272739622ee92a6a2",
          "name": "Maosen Zhao",
          "hidden": false
        },
        {
          "_id": "6902c36272739622ee92a6a3",
          "name": "Mingzhu Shen",
          "hidden": false
        },
        {
          "_id": "6902c36272739622ee92a6a4",
          "name": "Peng Ye",
          "hidden": false
        },
        {
          "_id": "6902c36272739622ee92a6a5",
          "name": "Bangyin Xiang",
          "hidden": false
        },
        {
          "_id": "6902c36272739622ee92a6a6",
          "name": "Zhibo Wang",
          "hidden": false
        },
        {
          "_id": "6902c36272739622ee92a6a7",
          "name": "Wei Cheng",
          "hidden": false
        },
        {
          "_id": "6902c36272739622ee92a6a8",
          "name": "Gang Yu",
          "hidden": false
        },
        {
          "_id": "6902c36272739622ee92a6a9",
          "name": "Tao Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-29T14:58:37.000Z",
      "submittedOnDailyAt": "2025-10-30T00:19:48.973Z",
      "title": "RegionE: Adaptive Region-Aware Generation for Efficient Image Editing",
      "submittedOnDailyBy": {
        "_id": "64c9bac33cfe45b07179568d",
        "avatarUrl": "/avatars/4a8206cdb1770a8cdaae0d0a2b7b59f2.svg",
        "isPro": false,
        "fullname": "Pengtao Chen",
        "user": "PengtaoChen",
        "type": "user"
      },
      "summary": "Recently, instruction-based image editing (IIE) has received widespread\nattention. In practice, IIE often modifies only specific regions of an image,\nwhile the remaining areas largely remain unchanged. Although these two types of\nregions differ significantly in generation difficulty and computational\nredundancy, existing IIE models do not account for this distinction, instead\napplying a uniform generation process across the entire image. This motivates\nus to propose RegionE, an adaptive, region-aware generation framework that\naccelerates IIE tasks without additional training. Specifically, the RegionE\nframework consists of three main components: 1) Adaptive Region Partition. We\nobserved that the trajectory of unedited regions is straight, allowing for\nmulti-step denoised predictions to be inferred in a single step. Therefore, in\nthe early denoising stages, we partition the image into edited and unedited\nregions based on the difference between the final estimated result and the\nreference image. 2) Region-Aware Generation. After distinguishing the regions,\nwe replace multi-step denoising with one-step prediction for unedited areas.\nFor edited regions, the trajectory is curved, requiring local iterative\ndenoising. To improve the efficiency and quality of local iterative generation,\nwe propose the Region-Instruction KV Cache, which reduces computational cost\nwhile incorporating global information. 3) Adaptive Velocity Decay Cache.\nObserving that adjacent timesteps in edited regions exhibit strong velocity\nsimilarity, we further propose an adaptive velocity decay cache to accelerate\nthe local denoising process. We applied RegionE to state-of-the-art IIE base\nmodels, including Step1X-Edit, FLUX.1 Kontext, and Qwen-Image-Edit. RegionE\nachieved acceleration factors of 2.57, 2.41, and 2.06. Evaluations by GPT-4o\nconfirmed that semantic and perceptual fidelity were well preserved.",
      "upvotes": 21,
      "discussionId": "6902c36272739622ee92a6aa",
      "githubRepo": "https://github.com/Peyton-Chen/RegionE",
      "githubStars": 6
    },
    "publishedAt": "2025-10-29T10:58:37.000Z",
    "title": "RegionE: Adaptive Region-Aware Generation for Efficient Image Editing",
    "summary": "Recently, instruction-based image editing (IIE) has received widespread\nattention. In practice, IIE often modifies only specific regions of an image,\nwhile the remaining areas largely remain unchanged. Although these two types of\nregions differ significantly in generation difficulty and computational\nredundancy, existing IIE models do not account for this distinction, instead\napplying a uniform generation process across the entire image. This motivates\nus to propose RegionE, an adaptive, region-aware generation framework that\naccelerates IIE tasks without additional training. Specifically, the RegionE\nframework consists of three main components: 1) Adaptive Region Partition. We\nobserved that the trajectory of unedited regions is straight, allowing for\nmulti-step denoised predictions to be inferred in a single step. Therefore, in\nthe early denoising stages, we partition the image into edited and unedited\nregions based on the difference between the final estimated result and the\nreference image. 2) Region-Aware Generation. After distinguishing the regions,\nwe replace multi-step denoising with one-step prediction for unedited areas.\nFor edited regions, the trajectory is curved, requiring local iterative\ndenoising. To improve the efficiency and quality of local iterative generation,\nwe propose the Region-Instruction KV Cache, which reduces computational cost\nwhile incorporating global information. 3) Adaptive Velocity Decay Cache.\nObserving that adjacent timesteps in edited regions exhibit strong velocity\nsimilarity, we further propose an adaptive velocity decay cache to accelerate\nthe local denoising process. We applied RegionE to state-of-the-art IIE base\nmodels, including Step1X-Edit, FLUX.1 Kontext, and Qwen-Image-Edit. RegionE\nachieved acceleration factors of 2.57, 2.41, and 2.06. Evaluations by GPT-4o\nconfirmed that semantic and perceptual fidelity were well preserved.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.25590.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c9bac33cfe45b07179568d",
      "avatarUrl": "/avatars/4a8206cdb1770a8cdaae0d0a2b7b59f2.svg",
      "fullname": "Pengtao Chen",
      "name": "PengtaoChen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.25772",
      "authors": [
        {
          "_id": "6902c4f472739622ee92a70b",
          "name": "Baolu Li",
          "hidden": false
        },
        {
          "_id": "6902c4f472739622ee92a70c",
          "name": "Yiming Zhang",
          "hidden": false
        },
        {
          "_id": "6902c4f472739622ee92a70d",
          "name": "Qinghe Wang",
          "hidden": false
        },
        {
          "_id": "6902c4f472739622ee92a70e",
          "name": "Liqian Ma",
          "hidden": false
        },
        {
          "_id": "6902c4f472739622ee92a70f",
          "name": "Xiaoyu Shi",
          "hidden": false
        },
        {
          "_id": "6902c4f472739622ee92a710",
          "name": "Xintao Wang",
          "hidden": false
        },
        {
          "_id": "6902c4f472739622ee92a711",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "6902c4f472739622ee92a712",
          "name": "Zhenfei Yin",
          "hidden": false
        },
        {
          "_id": "6902c4f472739622ee92a713",
          "name": "Yunzhi Zhuge",
          "hidden": false
        },
        {
          "_id": "6902c4f472739622ee92a714",
          "name": "Huchuan Lu",
          "hidden": false
        },
        {
          "_id": "6902c4f472739622ee92a715",
          "name": "Xu Jia",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/E4WnHnE05yVVWEWQU7AgW.mp4"
      ],
      "publishedAt": "2025-10-29T17:59:53.000Z",
      "submittedOnDailyAt": "2025-10-30T00:23:26.577Z",
      "title": "VFXMaster: Unlocking Dynamic Visual Effect Generation via In-Context\n  Learning",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Visual effects (VFX) are crucial to the expressive power of digital media,\nyet their creation remains a major challenge for generative AI. Prevailing\nmethods often rely on the one-LoRA-per-effect paradigm, which is\nresource-intensive and fundamentally incapable of generalizing to unseen\neffects, thus limiting scalability and creation. To address this challenge, we\nintroduce VFXMaster, the first unified, reference-based framework for VFX video\ngeneration. It recasts effect generation as an in-context learning task,\nenabling it to reproduce diverse dynamic effects from a reference video onto\ntarget content. In addition, it demonstrates remarkable generalization to\nunseen effect categories. Specifically, we design an in-context conditioning\nstrategy that prompts the model with a reference example. An in-context\nattention mask is designed to precisely decouple and inject the essential\neffect attributes, allowing a single unified model to master the effect\nimitation without information leakage. In addition, we propose an efficient\none-shot effect adaptation mechanism to boost generalization capability on\ntough unseen effects from a single user-provided video rapidly. Extensive\nexperiments demonstrate that our method effectively imitates various categories\nof effect information and exhibits outstanding generalization to out-of-domain\neffects. To foster future research, we will release our code, models, and a\ncomprehensive dataset to the community.",
      "upvotes": 16,
      "discussionId": "6902c4f572739622ee92a716",
      "projectPage": "https://libaolu312.github.io/VFXMaster/",
      "githubRepo": "https://github.com/libaolu312/VFXMaster",
      "githubStars": 4
    },
    "publishedAt": "2025-10-29T13:59:53.000Z",
    "title": "VFXMaster: Unlocking Dynamic Visual Effect Generation via In-Context\n  Learning",
    "summary": "Visual effects (VFX) are crucial to the expressive power of digital media,\nyet their creation remains a major challenge for generative AI. Prevailing\nmethods often rely on the one-LoRA-per-effect paradigm, which is\nresource-intensive and fundamentally incapable of generalizing to unseen\neffects, thus limiting scalability and creation. To address this challenge, we\nintroduce VFXMaster, the first unified, reference-based framework for VFX video\ngeneration. It recasts effect generation as an in-context learning task,\nenabling it to reproduce diverse dynamic effects from a reference video onto\ntarget content. In addition, it demonstrates remarkable generalization to\nunseen effect categories. Specifically, we design an in-context conditioning\nstrategy that prompts the model with a reference example. An in-context\nattention mask is designed to precisely decouple and inject the essential\neffect attributes, allowing a single unified model to master the effect\nimitation without information leakage. In addition, we propose an efficient\none-shot effect adaptation mechanism to boost generalization capability on\ntough unseen effects from a single user-provided video rapidly. Extensive\nexperiments demonstrate that our method effectively imitates various categories\nof effect information and exhibits outstanding generalization to out-of-domain\neffects. To foster future research, we will release our code, models, and a\ncomprehensive dataset to the community.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/E4WnHnE05yVVWEWQU7AgW.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.25772.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 150
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.18455",
      "authors": [
        {
          "_id": "69008ddd646208eac0d1ef5c",
          "user": {
            "_id": "661e6136a2bb874f6c283113",
            "avatarUrl": "/avatars/fe28c25310af37be241955fe41e75557.svg",
            "isPro": false,
            "fullname": "Liyang He",
            "user": "leoner24",
            "type": "user"
          },
          "name": "Liyang He",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-29T12:49:35.567Z",
          "hidden": false
        },
        {
          "_id": "69008ddd646208eac0d1ef5d",
          "name": "Yuren Zhang",
          "hidden": false
        },
        {
          "_id": "69008ddd646208eac0d1ef5e",
          "name": "Ziwei Zhu",
          "hidden": false
        },
        {
          "_id": "69008ddd646208eac0d1ef5f",
          "name": "Zhenghui Li",
          "hidden": false
        },
        {
          "_id": "69008ddd646208eac0d1ef60",
          "name": "Shiwei Tong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-21T09:28:13.000Z",
      "submittedOnDailyAt": "2025-10-30T05:32:22.552Z",
      "title": "ChronoPlay: A Framework for Modeling Dual Dynamics and Authenticity in\n  Game RAG Benchmarks",
      "submittedOnDailyBy": {
        "_id": "661e6136a2bb874f6c283113",
        "avatarUrl": "/avatars/fe28c25310af37be241955fe41e75557.svg",
        "isPro": false,
        "fullname": "Liyang He",
        "user": "leoner24",
        "type": "user"
      },
      "summary": "Retrieval Augmented Generation (RAG) systems are increasingly vital in\ndynamic domains like online gaming, yet the lack of a dedicated benchmark has\nimpeded standardized evaluation in this area. The core difficulty lies in Dual\nDynamics: the constant interplay between game content updates and the shifting\nfocus of the player community. Furthermore, the necessity of automating such a\nbenchmark introduces a critical requirement for player-centric authenticity to\nensure generated questions are realistic. To address this integrated challenge,\nwe introduce ChronoPlay, a novel framework for the automated and continuous\ngeneration of game RAG benchmarks. ChronoPlay utilizes a dual-dynamic update\nmechanism to track both forms of change, and a dual-source synthesis engine\nthat draws from official sources and player community to ensure both factual\ncorrectness and authentic query patterns. We instantiate our framework on three\ndistinct games to create the first dynamic RAG benchmark for the gaming domain,\noffering new insights into model performance under these complex and realistic\nconditions. Code is avaliable at: https://github.com/hly1998/ChronoPlay.",
      "upvotes": 11,
      "discussionId": "69008ddd646208eac0d1ef61",
      "projectPage": "https://hly1998.github.io/ChronoPlay/",
      "githubRepo": "https://github.com/hly1998/ChronoPlay",
      "ai_summary": "ChronoPlay is a framework for generating dynamic RAG benchmarks in gaming, addressing the challenges of game content updates and player focus shifts with a dual-dynamic update mechanism and dual-source synthesis engine.",
      "ai_keywords": [
        "Retrieval Augmented Generation",
        "RAG",
        "Dual Dynamics",
        "ChronoPlay",
        "dual-dynamic update mechanism",
        "dual-source synthesis engine"
      ],
      "githubStars": 10,
      "organization": {
        "_id": "66543b6e420092799d2f625c",
        "name": "tencent",
        "fullname": "Tencent",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
      }
    },
    "publishedAt": "2025-10-21T05:28:13.000Z",
    "title": "ChronoPlay: A Framework for Modeling Dual Dynamics and Authenticity in\n  Game RAG Benchmarks",
    "summary": "Retrieval Augmented Generation (RAG) systems are increasingly vital in\ndynamic domains like online gaming, yet the lack of a dedicated benchmark has\nimpeded standardized evaluation in this area. The core difficulty lies in Dual\nDynamics: the constant interplay between game content updates and the shifting\nfocus of the player community. Furthermore, the necessity of automating such a\nbenchmark introduces a critical requirement for player-centric authenticity to\nensure generated questions are realistic. To address this integrated challenge,\nwe introduce ChronoPlay, a novel framework for the automated and continuous\ngeneration of game RAG benchmarks. ChronoPlay utilizes a dual-dynamic update\nmechanism to track both forms of change, and a dual-source synthesis engine\nthat draws from official sources and player community to ensure both factual\ncorrectness and authentic query patterns. We instantiate our framework on three\ndistinct games to create the first dynamic RAG benchmark for the gaming domain,\noffering new insights into model performance under these complex and realistic\nconditions. Code is avaliable at: https://github.com/hly1998/ChronoPlay.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.18455.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "661e6136a2bb874f6c283113",
      "avatarUrl": "/avatars/fe28c25310af37be241955fe41e75557.svg",
      "fullname": "Liyang He",
      "name": "leoner24",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "organization": {
      "_id": "66543b6e420092799d2f625c",
      "name": "tencent",
      "fullname": "Tencent",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.19195",
      "authors": [
        {
          "_id": "69018b20646208eac0d1f4b7",
          "name": "Kai Zeng",
          "hidden": false
        },
        {
          "_id": "69018b20646208eac0d1f4b8",
          "name": "Zhanqian Wu",
          "hidden": false
        },
        {
          "_id": "69018b20646208eac0d1f4b9",
          "name": "Kaixin Xiong",
          "hidden": false
        },
        {
          "_id": "69018b20646208eac0d1f4ba",
          "name": "Xiaobao Wei",
          "hidden": false
        },
        {
          "_id": "69018b20646208eac0d1f4bb",
          "name": "Xiangyu Guo",
          "hidden": false
        },
        {
          "_id": "69018b20646208eac0d1f4bc",
          "name": "Zhenxin Zhu",
          "hidden": false
        },
        {
          "_id": "69018b20646208eac0d1f4bd",
          "name": "Kalok Ho",
          "hidden": false
        },
        {
          "_id": "69018b20646208eac0d1f4be",
          "name": "Lijun Zhou",
          "hidden": false
        },
        {
          "_id": "69018b20646208eac0d1f4bf",
          "name": "Bohan Zeng",
          "hidden": false
        },
        {
          "_id": "69018b20646208eac0d1f4c0",
          "name": "Ming Lu",
          "hidden": false
        },
        {
          "_id": "69018b20646208eac0d1f4c1",
          "name": "Haiyang Sun",
          "hidden": false
        },
        {
          "_id": "69018b20646208eac0d1f4c2",
          "name": "Bing Wang",
          "hidden": false
        },
        {
          "_id": "69018b20646208eac0d1f4c3",
          "name": "Guang Chen",
          "hidden": false
        },
        {
          "_id": "69018b20646208eac0d1f4c4",
          "name": "Hangjun Ye",
          "hidden": false
        },
        {
          "_id": "69018b20646208eac0d1f4c5",
          "name": "Wentao Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-22T03:02:38.000Z",
      "submittedOnDailyAt": "2025-10-30T00:35:18.380Z",
      "title": "Rethinking Driving World Model as Synthetic Data Generator for\n  Perception Tasks",
      "submittedOnDailyBy": {
        "_id": "6671214c92412fd4640714eb",
        "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg",
        "isPro": false,
        "fullname": "bohan zeng",
        "user": "zbhpku",
        "type": "user"
      },
      "summary": "Recent advancements in driving world models enable controllable generation of\nhigh-quality RGB videos or multimodal videos. Existing methods primarily focus\non metrics related to generation quality and controllability. However, they\noften overlook the evaluation of downstream perception tasks, which are\nreally crucial for the performance of autonomous driving. Existing\nmethods usually leverage a training strategy that first pretrains on synthetic\ndata and finetunes on real data, resulting in twice the epochs compared to the\nbaseline (real data only). When we double the epochs in the baseline, the\nbenefit of synthetic data becomes negligible. To thoroughly demonstrate the\nbenefit of synthetic data, we introduce Dream4Drive, a novel synthetic data\ngeneration framework designed for enhancing the downstream perception tasks.\nDream4Drive first decomposes the input video into several 3D-aware guidance\nmaps and subsequently renders the 3D assets onto these guidance maps. Finally,\nthe driving world model is fine-tuned to produce the edited, multi-view\nphotorealistic videos, which can be used to train the downstream perception\nmodels. Dream4Drive enables unprecedented flexibility in generating multi-view\ncorner cases at scale, significantly boosting corner case perception in\nautonomous driving. To facilitate future research, we also contribute a\nlarge-scale 3D asset dataset named DriveObj3D, covering the typical categories\nin driving scenarios and enabling diverse 3D-aware video editing. We conduct\ncomprehensive experiments to show that Dream4Drive can effectively boost the\nperformance of downstream perception models under various training epochs.\nPage: https://wm-research.github.io/Dream4Drive/ GitHub Link:\nhttps://github.com/wm-research/Dream4Drive",
      "upvotes": 8,
      "discussionId": "69018b21646208eac0d1f4c6",
      "projectPage": "https://wm-research.github.io/Dream4Drive/",
      "ai_summary": "Dream4Drive is a synthetic data generation framework that enhances downstream perception tasks in autonomous driving by decomposing videos into 3D-aware guidance maps and rendering 3D assets, leading to improved performance across various training epochs.",
      "ai_keywords": [
        "driving world models",
        "RGB videos",
        "multimodal videos",
        "generation quality",
        "controllability",
        "downstream perception tasks",
        "synthetic data",
        "pretraining",
        "finetuning",
        "3D-aware guidance maps",
        "3D assets",
        "photorealistic videos",
        "multi-view corner cases",
        "DriveObj3D",
        "3D asset dataset"
      ],
      "organization": {
        "_id": "61dcd8e344f59573371b5cb6",
        "name": "PekingUniversity",
        "fullname": "Peking University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
      }
    },
    "publishedAt": "2025-10-21T23:02:38.000Z",
    "title": "Rethinking Driving World Model as Synthetic Data Generator for\n  Perception Tasks",
    "summary": "Recent advancements in driving world models enable controllable generation of\nhigh-quality RGB videos or multimodal videos. Existing methods primarily focus\non metrics related to generation quality and controllability. However, they\noften overlook the evaluation of downstream perception tasks, which are\nreally crucial for the performance of autonomous driving. Existing\nmethods usually leverage a training strategy that first pretrains on synthetic\ndata and finetunes on real data, resulting in twice the epochs compared to the\nbaseline (real data only). When we double the epochs in the baseline, the\nbenefit of synthetic data becomes negligible. To thoroughly demonstrate the\nbenefit of synthetic data, we introduce Dream4Drive, a novel synthetic data\ngeneration framework designed for enhancing the downstream perception tasks.\nDream4Drive first decomposes the input video into several 3D-aware guidance\nmaps and subsequently renders the 3D assets onto these guidance maps. Finally,\nthe driving world model is fine-tuned to produce the edited, multi-view\nphotorealistic videos, which can be used to train the downstream perception\nmodels. Dream4Drive enables unprecedented flexibility in generating multi-view\ncorner cases at scale, significantly boosting corner case perception in\nautonomous driving. To facilitate future research, we also contribute a\nlarge-scale 3D asset dataset named DriveObj3D, covering the typical categories\nin driving scenarios and enabling diverse 3D-aware video editing. We conduct\ncomprehensive experiments to show that Dream4Drive can effectively boost the\nperformance of downstream perception models under various training epochs.\nPage: https://wm-research.github.io/Dream4Drive/ GitHub Link:\nhttps://github.com/wm-research/Dream4Drive",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.19195.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6671214c92412fd4640714eb",
      "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg",
      "fullname": "bohan zeng",
      "name": "zbhpku",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "organization": {
      "_id": "61dcd8e344f59573371b5cb6",
      "name": "PekingUniversity",
      "fullname": "Peking University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.25682",
      "authors": [
        {
          "_id": "6902ee8072739622ee92a8b1",
          "name": "Jiani Zheng",
          "hidden": false
        },
        {
          "_id": "6902ee8072739622ee92a8b2",
          "name": "Zhiyang Teng",
          "hidden": false
        },
        {
          "_id": "6902ee8072739622ee92a8b3",
          "name": "Xiangtai Li",
          "hidden": false
        },
        {
          "_id": "6902ee8072739622ee92a8b4",
          "name": "Anran Wang",
          "hidden": false
        },
        {
          "_id": "6902ee8072739622ee92a8b5",
          "name": "Yu Tian",
          "hidden": false
        },
        {
          "_id": "6902ee8072739622ee92a8b6",
          "name": "Kunpeng Qiu",
          "hidden": false
        },
        {
          "_id": "6902ee8072739622ee92a8b7",
          "name": "Ye Tian",
          "hidden": false
        },
        {
          "_id": "6902ee8072739622ee92a8b8",
          "name": "Haochen Wang",
          "hidden": false
        },
        {
          "_id": "6902ee8072739622ee92a8b9",
          "name": "Zhuochen Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-29T16:47:02.000Z",
      "submittedOnDailyAt": "2025-10-30T03:22:24.867Z",
      "title": "PairUni: Pairwise Training for Unified Multimodal Language Models",
      "submittedOnDailyBy": {
        "_id": "64531f631a57e1179c203e6b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64531f631a57e1179c203e6b/C_J7pXFLqoJoHYPPhK3J9.jpeg",
        "isPro": false,
        "fullname": "zjn",
        "user": "garlicisnotmyfavor",
        "type": "user"
      },
      "summary": "Unified vision-language models (UVLMs) must perform both understanding and\ngeneration within a single architecture, but these tasks rely on heterogeneous\ndata and supervision, making it difficult to balance them during reinforcement\nlearning (RL). We propose PairUni, a unified framework that reorganizes data\ninto understanding-generation (UG) pairs and aligns optimization accordingly.\nWe first use GPT-o3 to augment single-task data, generating captions for\nunderstanding samples and question-answer (QA) pairs for generation samples,\nforming aligned pairs from the same instance. Additionally, for each generation\nsample, we retrieve a semantically related understanding example to form a\nretrieved pair, linking different but related data points. These paired\nstructures expose cross-task semantic correspondences and support consistent\npolicy learning. To leverage this structure, we present Pair-GPRO, a pair-aware\nvariant based on Group Relative Policy Optimization. It assigns a similarity\nscore to each pair to modulate the advantage, strengthening learning from\nwell-aligned examples and reducing task interference. We curate a high-quality\ndataset of 16K UG pairs named PairUG for RL fine-tuning and evaluate PairUni on\nthe powerful Janus-Pro UVLMs. Our approach achieves balanced improvements on\nvarious UVLMs, outperforming strong UVLM RL baselines. Code:\nhttps://github.com/Haochen-Wang409/PairUni{github.com/Haochen-Wang409/PairUni}",
      "upvotes": 7,
      "discussionId": "6902ee8072739622ee92a8ba",
      "githubRepo": "https://github.com/Haochen-Wang409/PairUni",
      "githubStars": 0,
      "organization": {
        "_id": "653b817d32c97d0655575872",
        "name": "ByteDance",
        "fullname": "ByteDance",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
      }
    },
    "publishedAt": "2025-10-29T12:47:02.000Z",
    "title": "PairUni: Pairwise Training for Unified Multimodal Language Models",
    "summary": "Unified vision-language models (UVLMs) must perform both understanding and\ngeneration within a single architecture, but these tasks rely on heterogeneous\ndata and supervision, making it difficult to balance them during reinforcement\nlearning (RL). We propose PairUni, a unified framework that reorganizes data\ninto understanding-generation (UG) pairs and aligns optimization accordingly.\nWe first use GPT-o3 to augment single-task data, generating captions for\nunderstanding samples and question-answer (QA) pairs for generation samples,\nforming aligned pairs from the same instance. Additionally, for each generation\nsample, we retrieve a semantically related understanding example to form a\nretrieved pair, linking different but related data points. These paired\nstructures expose cross-task semantic correspondences and support consistent\npolicy learning. To leverage this structure, we present Pair-GPRO, a pair-aware\nvariant based on Group Relative Policy Optimization. It assigns a similarity\nscore to each pair to modulate the advantage, strengthening learning from\nwell-aligned examples and reducing task interference. We curate a high-quality\ndataset of 16K UG pairs named PairUG for RL fine-tuning and evaluate PairUni on\nthe powerful Janus-Pro UVLMs. Our approach achieves balanced improvements on\nvarious UVLMs, outperforming strong UVLM RL baselines. Code:\nhttps://github.com/Haochen-Wang409/PairUni{github.com/Haochen-Wang409/PairUni}",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.25682.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64531f631a57e1179c203e6b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64531f631a57e1179c203e6b/C_J7pXFLqoJoHYPPhK3J9.jpeg",
      "fullname": "zjn",
      "name": "garlicisnotmyfavor",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "organization": {
      "_id": "653b817d32c97d0655575872",
      "name": "ByteDance",
      "fullname": "ByteDance",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.24824",
      "authors": [
        {
          "_id": "6902c64272739622ee92a74f",
          "name": "Bohong Wu",
          "hidden": false
        },
        {
          "_id": "6902c64272739622ee92a750",
          "name": "Mengzhao Chen",
          "hidden": false
        },
        {
          "_id": "6902c64272739622ee92a751",
          "name": "Xiang Luo",
          "hidden": false
        },
        {
          "_id": "6902c64272739622ee92a752",
          "name": "Shen Yan",
          "hidden": false
        },
        {
          "_id": "6902c64272739622ee92a753",
          "name": "Qifan Yu",
          "hidden": false
        },
        {
          "_id": "6902c64272739622ee92a754",
          "name": "Fan Xia",
          "hidden": false
        },
        {
          "_id": "6902c64272739622ee92a755",
          "name": "Tianqi Zhang",
          "hidden": false
        },
        {
          "_id": "6902c64272739622ee92a756",
          "name": "Hongrui Zhan",
          "hidden": false
        },
        {
          "_id": "6902c64272739622ee92a757",
          "name": "Zheng Zhong",
          "hidden": false
        },
        {
          "_id": "6902c64272739622ee92a758",
          "name": "Xun Zhou",
          "hidden": false
        },
        {
          "_id": "6902c64272739622ee92a759",
          "name": "Siyuan Qiao",
          "hidden": false
        },
        {
          "_id": "6902c64272739622ee92a75a",
          "name": "Xingyan Bin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-28T15:35:50.000Z",
      "submittedOnDailyAt": "2025-10-30T00:29:22.311Z",
      "title": "Parallel Loop Transformer for Efficient Test-Time Computation Scaling",
      "submittedOnDailyBy": {
        "_id": "64722a616facfb01d8ae8349",
        "avatarUrl": "/avatars/1dce23ae5ebd9996770cf5efe910b857.svg",
        "isPro": false,
        "fullname": "Wu Bohong",
        "user": "bongbohong",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) are powerful but often too slow and costly for\nreal-world use during inference. Looped transformers save on parameters by\nreusing the same weights for multiple computational steps, or \"loops.\" However,\nthis approach has a major flaw: the loops run one after another, causing\ninference latency and memory requirements to increase with each added loop.\nThis makes them impractical for fast applications. To solve this problem, we\nintroduce the Parallel Loop Transformer (PLT). PLT is a new architecture that\ndelivers the performance benefits of a deep, looped model but with the low\nlatency of a standard, non-looped model. PLT works using two key techniques.\nFirst, Cross-Loop Parallelism (CLP) breaks the sequential dependency by\ncomputing different loops for different tokens at the same time, all within a\nsingle pass. Second, to prevent memory costs from growing, we use an Efficient\nRepresentation Enhancement strategy. This method shares the memory (KV cache)\nfrom the first loop with all other loops. It then uses a Gated Sliding-Window\nAttention (G-SWA) to combine this shared global information with local\ninformation, maintaining high accuracy. Our experiments show that PLT achieves\nthe high accuracy of a traditional looped model but with almost no extra\nlatency or memory cost compared to a standard transformer.",
      "upvotes": 7,
      "discussionId": "6902c64272739622ee92a75b",
      "organization": {
        "_id": "67d1140985ea0644e2f14b99",
        "name": "ByteDance-Seed",
        "fullname": "ByteDance Seed",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
      }
    },
    "publishedAt": "2025-10-28T11:35:50.000Z",
    "title": "Parallel Loop Transformer for Efficient Test-Time Computation Scaling",
    "summary": "Large Language Models (LLMs) are powerful but often too slow and costly for\nreal-world use during inference. Looped transformers save on parameters by\nreusing the same weights for multiple computational steps, or \"loops.\" However,\nthis approach has a major flaw: the loops run one after another, causing\ninference latency and memory requirements to increase with each added loop.\nThis makes them impractical for fast applications. To solve this problem, we\nintroduce the Parallel Loop Transformer (PLT). PLT is a new architecture that\ndelivers the performance benefits of a deep, looped model but with the low\nlatency of a standard, non-looped model. PLT works using two key techniques.\nFirst, Cross-Loop Parallelism (CLP) breaks the sequential dependency by\ncomputing different loops for different tokens at the same time, all within a\nsingle pass. Second, to prevent memory costs from growing, we use an Efficient\nRepresentation Enhancement strategy. This method shares the memory (KV cache)\nfrom the first loop with all other loops. It then uses a Gated Sliding-Window\nAttention (G-SWA) to combine this shared global information with local\ninformation, maintaining high accuracy. Our experiments show that PLT achieves\nthe high accuracy of a traditional looped model but with almost no extra\nlatency or memory cost compared to a standard transformer.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.24824.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64722a616facfb01d8ae8349",
      "avatarUrl": "/avatars/1dce23ae5ebd9996770cf5efe910b857.svg",
      "fullname": "Wu Bohong",
      "name": "bongbohong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "organization": {
      "_id": "67d1140985ea0644e2f14b99",
      "name": "ByteDance-Seed",
      "fullname": "ByteDance Seed",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.24654",
      "authors": [
        {
          "_id": "6902d7ea72739622ee92a85f",
          "name": "Pengcheng Qiu",
          "hidden": false
        },
        {
          "_id": "6902d7ea72739622ee92a860",
          "name": "Chaoyi Wu",
          "hidden": false
        },
        {
          "_id": "6902d7ea72739622ee92a861",
          "name": "Junwei Liu",
          "hidden": false
        },
        {
          "_id": "6902d7ea72739622ee92a862",
          "name": "Qiaoyu Zheng",
          "hidden": false
        },
        {
          "_id": "6902d7ea72739622ee92a863",
          "name": "Yusheng Liao",
          "hidden": false
        },
        {
          "_id": "6902d7ea72739622ee92a864",
          "name": "Haowen Wang",
          "hidden": false
        },
        {
          "_id": "6902d7ea72739622ee92a865",
          "name": "Yun Yue",
          "hidden": false
        },
        {
          "_id": "6902d7ea72739622ee92a866",
          "name": "Qianrui Fan",
          "hidden": false
        },
        {
          "_id": "6902d7ea72739622ee92a867",
          "name": "Shuai Zhen",
          "hidden": false
        },
        {
          "_id": "6902d7ea72739622ee92a868",
          "name": "Jian Wang",
          "hidden": false
        },
        {
          "_id": "6902d7ea72739622ee92a869",
          "name": "Jinjie Gu",
          "hidden": false
        },
        {
          "_id": "6902d7ea72739622ee92a86a",
          "name": "Yanfeng Wang",
          "hidden": false
        },
        {
          "_id": "6902d7ea72739622ee92a86b",
          "name": "Ya Zhang",
          "hidden": false
        },
        {
          "_id": "6902d7ea72739622ee92a86c",
          "name": "Weidi Xie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-28T17:19:47.000Z",
      "submittedOnDailyAt": "2025-10-30T05:15:29.687Z",
      "title": "Evolving Diagnostic Agents in a Virtual Clinical Environment",
      "submittedOnDailyBy": {
        "_id": "6436aaaa0c77d7c5036abdbd",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6436aaaa0c77d7c5036abdbd/C9A276yEeAPkKLEqUcjl_.jpeg",
        "isPro": false,
        "fullname": "Chaoyi Wu",
        "user": "chaoyi-wu",
        "type": "user"
      },
      "summary": "In this paper, we present a framework for training large language models\n(LLMs) as diagnostic agents with reinforcement learning, enabling them to\nmanage multi-turn diagnostic processes, adaptively select examinations, and\ncommit to final diagnoses. Unlike instruction-tuned models trained on static\ncase summaries, our method acquires diagnostic strategies through interactive\nexploration and outcome-based feedback. Our contributions are fourfold: (i) We\npresent DiagGym, a diagnostics world model trained with electronic health\nrecords that emits examination outcomes conditioned on patient history and\nrecommended examination, serving as a virtual clinical environment for\nrealistic diagnosis training and evaluation; (ii) We train DiagAgent via\nend-to-end, multi-turn reinforcement learning to learn diagnostic policies that\noptimize both information yield and diagnostic accuracy; (iii) We introduce\nDiagBench, a diagnostic benchmark comprising 750 cases with physician-validated\nexamination recommendations and 99 cases annotated with 973 physician-written\nrubrics on diagnosis process; (iv) we demonstrate superior performance across\ndiverse diagnostic settings. DiagAgent significantly outperforms 10\nstate-of-the-art LLMs, including DeepSeek-v3 and GPT-4o, as well as two\nprompt-engineered agents. In single-turn settings, DiagAgent achieves 9.34%\nhigher diagnostic accuracy and 44.03% improvement in examination recommendation\nhit ratio. In end-to-end settings, it delivers 15.12% increase in diagnostic\naccuracy and 23.09% boost in examination recommendation F1 score. In\nrubric-based evaluation, it surpasses the next-best model, Claude-sonnet-4, by\n7.1% in weighted rubric score. These findings indicate that learning policies\nin interactive clinical environments confers dynamic and clinically meaningful\ndiagnostic management abilities unattainable through passive training alone.",
      "upvotes": 5,
      "discussionId": "6902d7ea72739622ee92a86d",
      "projectPage": "https://arxiv.org/html/2510.24654v1",
      "githubRepo": "https://github.com/MAGIC-AI4Med/DiagGym",
      "githubStars": 43,
      "organization": {
        "_id": "63e5ef7bf2e9a8f22c515654",
        "name": "SJTU",
        "fullname": "Shanghai Jiao Tong University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"
      }
    },
    "publishedAt": "2025-10-28T13:19:47.000Z",
    "title": "Evolving Diagnostic Agents in a Virtual Clinical Environment",
    "summary": "In this paper, we present a framework for training large language models\n(LLMs) as diagnostic agents with reinforcement learning, enabling them to\nmanage multi-turn diagnostic processes, adaptively select examinations, and\ncommit to final diagnoses. Unlike instruction-tuned models trained on static\ncase summaries, our method acquires diagnostic strategies through interactive\nexploration and outcome-based feedback. Our contributions are fourfold: (i) We\npresent DiagGym, a diagnostics world model trained with electronic health\nrecords that emits examination outcomes conditioned on patient history and\nrecommended examination, serving as a virtual clinical environment for\nrealistic diagnosis training and evaluation; (ii) We train DiagAgent via\nend-to-end, multi-turn reinforcement learning to learn diagnostic policies that\noptimize both information yield and diagnostic accuracy; (iii) We introduce\nDiagBench, a diagnostic benchmark comprising 750 cases with physician-validated\nexamination recommendations and 99 cases annotated with 973 physician-written\nrubrics on diagnosis process; (iv) we demonstrate superior performance across\ndiverse diagnostic settings. DiagAgent significantly outperforms 10\nstate-of-the-art LLMs, including DeepSeek-v3 and GPT-4o, as well as two\nprompt-engineered agents. In single-turn settings, DiagAgent achieves 9.34%\nhigher diagnostic accuracy and 44.03% improvement in examination recommendation\nhit ratio. In end-to-end settings, it delivers 15.12% increase in diagnostic\naccuracy and 23.09% boost in examination recommendation F1 score. In\nrubric-based evaluation, it surpasses the next-best model, Claude-sonnet-4, by\n7.1% in weighted rubric score. These findings indicate that learning policies\nin interactive clinical environments confers dynamic and clinically meaningful\ndiagnostic management abilities unattainable through passive training alone.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.24654.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6436aaaa0c77d7c5036abdbd",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6436aaaa0c77d7c5036abdbd/C9A276yEeAPkKLEqUcjl_.jpeg",
      "fullname": "Chaoyi Wu",
      "name": "chaoyi-wu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 32
    },
    "organization": {
      "_id": "63e5ef7bf2e9a8f22c515654",
      "name": "SJTU",
      "fullname": "Shanghai Jiao Tong University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.25092",
      "authors": [
        {
          "_id": "6902dac372739622ee92a88b",
          "name": "Weijia Zhang",
          "hidden": false
        },
        {
          "_id": "6902dac372739622ee92a88c",
          "name": "Zijia Liu",
          "hidden": false
        },
        {
          "_id": "6902dac372739622ee92a88d",
          "name": "Haoru Li",
          "hidden": false
        },
        {
          "_id": "6902dac372739622ee92a88e",
          "name": "Haoqi Chen",
          "hidden": false
        },
        {
          "_id": "6902dac372739622ee92a88f",
          "name": "Jiaxuan You",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-29T01:57:11.000Z",
      "submittedOnDailyAt": "2025-10-30T02:00:18.876Z",
      "title": "SeeingEye: Agentic Information Flow Unlocks Multimodal Reasoning In\n  Text-only LLMs",
      "submittedOnDailyBy": {
        "_id": "65d188a4aa309d842e438ef1",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d188a4aa309d842e438ef1/t5zmBIytp-xnMoHUfYhEa.jpeg",
        "isPro": false,
        "fullname": "Zijia Liu",
        "user": "m-serious",
        "type": "user"
      },
      "summary": "Recent advances in text-only large language models (LLMs), such as\nDeepSeek-R1, demonstrate remarkable reasoning ability. However, these models\nremain fragile or entirely incapable when extended to multi-modal tasks.\nExisting approaches largely rely on single-form captions, which lack diversity\nand often fail to adapt across different types of Visual Question Answering\n(VQA) benchmarks. As a result, they provide no principled or efficient channel\nfor transmitting fine-grained visual information. We introduce Seeing Eye, a\nmodular framework that unlocks multimodal reasoning in text-only LLMs through\nan agent-based small VLM translator. This translator acts as a perception\nagent: it can invoke specialized tools (e.g., OCR and crop) and iteratively\ndistill multimodal inputs into structured intermediate representations (SIRs)\ntailored to the question. These SIRs are then passed to the text-only LLM,\nwhich serves as a reasoning agent. Crucially, the translator and reasoner\nengage in multi-round feedback and interaction, enabling the extraction of\ntargeted visual details and yielding more confident answers. Experiments on\nknowledge-intensive VQA benchmarks, including MMMU and MIA-Bench, demonstrate\nthat Seeing Eye not only reduces inference cost but also surpasses much larger\nend-to-end VLMs. For example, an instantiation combining a 3B-parameter vision\ntranslator with an 8B-parameter language reasoner outperforms a monolithic 32B\nVLM on challenging knowledge-based questions. Our results highlight that\ndecoupling perception from reasoning via agent information flow offers a\nscalable and plug-and-play pathway to multimodal reasoning, allowing strong\ntext-only LLMs to fully leverage their reasoning capabilities. Code is\navailable at: https://github.com/ulab-uiuc/SeeingEye",
      "upvotes": 4,
      "discussionId": "6902dac372739622ee92a890",
      "githubRepo": "https://github.com/ulab-uiuc/SeeingEye",
      "githubStars": 3
    },
    "publishedAt": "2025-10-28T21:57:11.000Z",
    "title": "SeeingEye: Agentic Information Flow Unlocks Multimodal Reasoning In\n  Text-only LLMs",
    "summary": "Recent advances in text-only large language models (LLMs), such as\nDeepSeek-R1, demonstrate remarkable reasoning ability. However, these models\nremain fragile or entirely incapable when extended to multi-modal tasks.\nExisting approaches largely rely on single-form captions, which lack diversity\nand often fail to adapt across different types of Visual Question Answering\n(VQA) benchmarks. As a result, they provide no principled or efficient channel\nfor transmitting fine-grained visual information. We introduce Seeing Eye, a\nmodular framework that unlocks multimodal reasoning in text-only LLMs through\nan agent-based small VLM translator. This translator acts as a perception\nagent: it can invoke specialized tools (e.g., OCR and crop) and iteratively\ndistill multimodal inputs into structured intermediate representations (SIRs)\ntailored to the question. These SIRs are then passed to the text-only LLM,\nwhich serves as a reasoning agent. Crucially, the translator and reasoner\nengage in multi-round feedback and interaction, enabling the extraction of\ntargeted visual details and yielding more confident answers. Experiments on\nknowledge-intensive VQA benchmarks, including MMMU and MIA-Bench, demonstrate\nthat Seeing Eye not only reduces inference cost but also surpasses much larger\nend-to-end VLMs. For example, an instantiation combining a 3B-parameter vision\ntranslator with an 8B-parameter language reasoner outperforms a monolithic 32B\nVLM on challenging knowledge-based questions. Our results highlight that\ndecoupling perception from reasoning via agent information flow offers a\nscalable and plug-and-play pathway to multimodal reasoning, allowing strong\ntext-only LLMs to fully leverage their reasoning capabilities. Code is\navailable at: https://github.com/ulab-uiuc/SeeingEye",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.25092.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65d188a4aa309d842e438ef1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d188a4aa309d842e438ef1/t5zmBIytp-xnMoHUfYhEa.jpeg",
      "fullname": "Zijia Liu",
      "name": "m-serious",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.24821",
      "authors": [
        {
          "_id": "6902c3a272739622ee92a6ac",
          "name": "Inclusion AI",
          "hidden": false
        },
        {
          "_id": "6902c3a272739622ee92a6ae",
          "name": "Bowen Ma",
          "hidden": false
        },
        {
          "_id": "6902c3a272739622ee92a6af",
          "name": "Cheng Zou",
          "hidden": false
        },
        {
          "_id": "6902c3a272739622ee92a6b0",
          "name": "Canxiang Yan",
          "hidden": false
        },
        {
          "_id": "6902c3a272739622ee92a6b1",
          "name": "Chunxiang Jin",
          "hidden": false
        },
        {
          "_id": "6902c3a272739622ee92a6b2",
          "name": "Chunjie Shen",
          "hidden": false
        },
        {
          "_id": "6902c3a272739622ee92a6b3",
          "name": "Dandan Zheng",
          "hidden": false
        },
        {
          "_id": "6902c3a272739622ee92a6b4",
          "name": "Fudong Wang",
          "hidden": false
        },
        {
          "_id": "6902c3a272739622ee92a6b5",
          "name": "Furong Xu",
          "hidden": false
        },
        {
          "_id": "6902c3a272739622ee92a6b6",
          "name": "GuangMing Yao",
          "hidden": false
        },
        {
          "_id": "6902c3a272739622ee92a6b7",
          "name": "Jun Zhou",
          "hidden": false
        },
        {
          "_id": "6902c3a272739622ee92a6b8",
          "name": "Jingdong Chen",
          "hidden": false
        },
        {
          "_id": "6902c3a272739622ee92a6b9",
          "name": "Jianing Li",
          "hidden": false
        },
        {
          "_id": "6902c3a272739622ee92a6ba",
          "name": "Jianxin Sun",
          "hidden": false
        },
        {
          "_id": "6902c3a272739622ee92a6bb",
          "name": "Jiajia Liu",
          "hidden": false
        },
        {
          "_id": "6902c3a272739622ee92a6bc",
          "name": "Jianjiang Zhu",
          "hidden": false
        },
        {
          "_id": "6902c3a272739622ee92a6bd",
          "name": "Jianping Jiang",
          "hidden": false
        },
        {
          "_id": "6902c3a272739622ee92a6be",
          "name": "Jun Peng",
          "hidden": false
        },
        {
          "_id": "6902c3a272739622ee92a6bf",
          "name": "Kaixiang Ji",
          "hidden": false
        },
        {
          "_id": "6902c3a272739622ee92a6c0",
          "name": "Kaimeng Ren",
          "hidden": false
        },
        {
          "_id": "6902c3a272739622ee92a6c1",
          "name": "Libin Wang",
          "hidden": false
        },
        {
          "_id": "6902c3a272739622ee92a6c2",
          "name": "Lixiang Ru",
          "hidden": false
        },
        {
          "_id": "6902c3a272739622ee92a6c3",
          "name": "Longhua Tan",
          "hidden": false
        },
        {
          "_id": "6902c3a272739622ee92a6c4",
          "name": "Lan Wang",
          "hidden": false
        },
        {
          "_id": "6902c3a272739622ee92a6c5",
          "name": "Mochen Bai",
          "hidden": false
        },
        {
          "_id": "6902c3a272739622ee92a6c6",
          "name": "Ning Gao",
          "hidden": false
        },
        {
          "_id": "6902c3a272739622ee92a6c7",
          "name": "Qingpei Guo",
          "hidden": false
        },
        {
          "_id": "6902c3a272739622ee92a6c8",
          "name": "Qinglong Zhang",
          "hidden": false
        },
        {
          "_id": "6902c3a272739622ee92a6c9",
          "name": "Qiang Xu",
          "hidden": false
        },
        {
          "_id": "6902c3a272739622ee92a6ca",
          "name": "Rui Liu",
          "hidden": false
        },
        {
          "_id": "6902c3a272739622ee92a6cb",
          "name": "Ruijie Xiong",
          "hidden": false
        },
        {
          "_id": "6902c3a272739622ee92a6cc",
          "name": "Ruobing Zheng",
          "hidden": false
        },
        {
          "_id": "6902c3a272739622ee92a6cd",
          "name": "Sirui Gao",
          "hidden": false
        },
        {
          "_id": "6902c3a272739622ee92a6ce",
          "name": "Tianqi Li",
          "hidden": false
        },
        {
          "_id": "6902c3a272739622ee92a6cf",
          "name": "Tinghao Liu",
          "hidden": false
        },
        {
          "_id": "6902c3a272739622ee92a6d0",
          "name": "Weilong Chai",
          "hidden": false
        },
        {
          "_id": "6902c3a272739622ee92a6d1",
          "name": "Xinyu Xiao",
          "hidden": false
        },
        {
          "_id": "6902c3a272739622ee92a6d2",
          "name": "Xiaomei Wang",
          "hidden": false
        },
        {
          "_id": "6902c3a272739622ee92a6d3",
          "name": "Xiaolong Wang",
          "hidden": false
        },
        {
          "_id": "6902c3a272739622ee92a6d4",
          "name": "Xiao Lu",
          "hidden": false
        },
        {
          "_id": "6902c3a272739622ee92a6d5",
          "name": "Xiaoyu Li",
          "hidden": false
        },
        {
          "_id": "6902c3a272739622ee92a6d6",
          "name": "Xingning Dong",
          "hidden": false
        },
        {
          "_id": "6902c3a272739622ee92a6d7",
          "name": "Xuzheng Yu",
          "hidden": false
        },
        {
          "_id": "6902c3a272739622ee92a6d8",
          "name": "Yi Yuan",
          "hidden": false
        },
        {
          "_id": "6902c3a272739622ee92a6d9",
          "name": "Yuting Gao",
          "hidden": false
        },
        {
          "_id": "6902c3a272739622ee92a6da",
          "name": "Yuting Xiao",
          "hidden": false
        },
        {
          "_id": "6902c3a272739622ee92a6db",
          "name": "Yunxiao Sun",
          "hidden": false
        },
        {
          "_id": "6902c3a272739622ee92a6dc",
          "name": "Yipeng Chen",
          "hidden": false
        },
        {
          "_id": "6902c3a272739622ee92a6dd",
          "name": "Yifan Mao",
          "hidden": false
        },
        {
          "_id": "6902c3a272739622ee92a6de",
          "name": "Yifei Wu",
          "hidden": false
        },
        {
          "_id": "6902c3a272739622ee92a6df",
          "name": "Yongjie Lyu",
          "hidden": false
        },
        {
          "_id": "6902c3a272739622ee92a6e0",
          "name": "Ziping Ma",
          "hidden": false
        },
        {
          "_id": "6902c3a272739622ee92a6e1",
          "name": "Zhiqiang Fang",
          "hidden": false
        },
        {
          "_id": "6902c3a272739622ee92a6e2",
          "name": "Zhihao Qiu",
          "hidden": false
        },
        {
          "_id": "6902c3a272739622ee92a6e3",
          "name": "Ziyuan Huang",
          "hidden": false
        },
        {
          "_id": "6902c3a272739622ee92a6e4",
          "name": "Zizheng Yang",
          "hidden": false
        },
        {
          "_id": "6902c3a272739622ee92a6e5",
          "name": "Zhengyu He",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-28T15:24:13.000Z",
      "submittedOnDailyAt": "2025-10-30T00:17:44.509Z",
      "title": "Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal\n  Perception and Generation",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We propose Ming-Flash-Omni, an upgraded version of Ming-Omni, built upon a\nsparser Mixture-of-Experts (MoE) variant of Ling-Flash-2.0 with 100 billion\ntotal parameters, of which only 6.1 billion are active per token. This\narchitecture enables highly efficient scaling (dramatically improving\ncomputational efficiency while significantly expanding model capacity) and\nempowers stronger unified multimodal intelligence across vision, speech, and\nlanguage, representing a key step toward Artificial General Intelligence (AGI).\nCompared to its predecessor, the upgraded version exhibits substantial\nimprovements across multimodal understanding and generation. We significantly\nadvance speech recognition capabilities, achieving state-of-the-art performance\nin contextual ASR and highly competitive results in dialect-aware ASR. In image\ngeneration, Ming-Flash-Omni introduces high-fidelity text rendering and\ndemonstrates marked gains in scene consistency and identity preservation during\nimage editing. Furthermore, Ming-Flash-Omni introduces generative segmentation,\na capability that not only achieves strong standalone segmentation performance\nbut also enhances spatial control in image generation and improves editing\nconsistency. Notably, Ming-Flash-Omni achieves state-of-the-art results in\ntext-to-image generation and generative segmentation, and sets new records on\nall 12 contextual ASR benchmarks, all within a single unified architecture.",
      "upvotes": 4,
      "discussionId": "6902c3a272739622ee92a6e6",
      "projectPage": "https://github.com/inclusionAI/Ming",
      "organization": {
        "_id": "67c1d682826160b28f778510",
        "name": "antgroup",
        "fullname": "Ant Group",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/7VcPHdLSGlged3ixK1dys.jpeg"
      }
    },
    "publishedAt": "2025-10-28T11:24:13.000Z",
    "title": "Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal\n  Perception and Generation",
    "summary": "We propose Ming-Flash-Omni, an upgraded version of Ming-Omni, built upon a\nsparser Mixture-of-Experts (MoE) variant of Ling-Flash-2.0 with 100 billion\ntotal parameters, of which only 6.1 billion are active per token. This\narchitecture enables highly efficient scaling (dramatically improving\ncomputational efficiency while significantly expanding model capacity) and\nempowers stronger unified multimodal intelligence across vision, speech, and\nlanguage, representing a key step toward Artificial General Intelligence (AGI).\nCompared to its predecessor, the upgraded version exhibits substantial\nimprovements across multimodal understanding and generation. We significantly\nadvance speech recognition capabilities, achieving state-of-the-art performance\nin contextual ASR and highly competitive results in dialect-aware ASR. In image\ngeneration, Ming-Flash-Omni introduces high-fidelity text rendering and\ndemonstrates marked gains in scene consistency and identity preservation during\nimage editing. Furthermore, Ming-Flash-Omni introduces generative segmentation,\na capability that not only achieves strong standalone segmentation performance\nbut also enhances spatial control in image generation and improves editing\nconsistency. Notably, Ming-Flash-Omni achieves state-of-the-art results in\ntext-to-image generation and generative segmentation, and sets new records on\nall 12 contextual ASR benchmarks, all within a single unified architecture.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.24821.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 150
    },
    "organization": {
      "_id": "67c1d682826160b28f778510",
      "name": "antgroup",
      "fullname": "Ant Group",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/7VcPHdLSGlged3ixK1dys.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.24803",
      "authors": [
        {
          "_id": "6902ae8672739622ee92a638",
          "name": "Milad Yazdani",
          "hidden": false
        },
        {
          "_id": "6902ae8672739622ee92a639",
          "name": "Mahdi Mostajabdaveh",
          "hidden": false
        },
        {
          "_id": "6902ae8672739622ee92a63a",
          "name": "Zirui Zhou",
          "hidden": false
        },
        {
          "_id": "6902ae8672739622ee92a63b",
          "name": "Ying Xiong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-28T00:48:20.000Z",
      "submittedOnDailyAt": "2025-10-30T01:29:23.926Z",
      "title": "MASPRM: Multi-Agent System Process Reward Model",
      "submittedOnDailyBy": {
        "_id": "624a82573571171191af5e60",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/tMIMn7oeSsDDpPuZ08co7.png",
        "isPro": false,
        "fullname": "Milad Yazdani",
        "user": "Miladyz",
        "type": "user"
      },
      "summary": "Practical deployment of Multi-Agent Systems (MAS) demands strong test-time\nperformance, motivating methods that guide inference-time search and\nselectively spend compute to improve quality. We present the Multi-Agent System\nProcess Reward Model (MASPRM). It assigns per-action, per-agent values to\npartial inter-agent transcripts and acts as an inference-time controller.\nMASPRM is trained from multi-agent Monte Carlo Tree Search (MCTS) rollouts\nwithout requiring step-level human annotations, by propagating returns to local\ntargets. At inference, MASPRM guides step-level beam search and MCTS, focusing\ncomputation on promising branches and pruning early. On GSM8K and MATH,\nMASPRM-guided decoding with an outcome reward model (ORM) applied to the final\nanswer, improves exact match (EM) over a single straight-through MAS pass by\n+30.7 and +22.9 points, respectively. A MASPRM trained on GSM8K transfers\nzero-shot to MATH without retraining, adding 8.4 EM points at the same\nbudget. MASPRM is a plug-in value model that estimates per-agent progress and\ncomplements verifier-style decoders, enabling more reliable, compute-aware\nmulti-agent reasoning. Code: https://github.com/milad1378yz/MASPRM",
      "upvotes": 4,
      "discussionId": "6902ae8772739622ee92a63c"
    },
    "publishedAt": "2025-10-27T20:48:20.000Z",
    "title": "MASPRM: Multi-Agent System Process Reward Model",
    "summary": "Practical deployment of Multi-Agent Systems (MAS) demands strong test-time\nperformance, motivating methods that guide inference-time search and\nselectively spend compute to improve quality. We present the Multi-Agent System\nProcess Reward Model (MASPRM). It assigns per-action, per-agent values to\npartial inter-agent transcripts and acts as an inference-time controller.\nMASPRM is trained from multi-agent Monte Carlo Tree Search (MCTS) rollouts\nwithout requiring step-level human annotations, by propagating returns to local\ntargets. At inference, MASPRM guides step-level beam search and MCTS, focusing\ncomputation on promising branches and pruning early. On GSM8K and MATH,\nMASPRM-guided decoding with an outcome reward model (ORM) applied to the final\nanswer, improves exact match (EM) over a single straight-through MAS pass by\n+30.7 and +22.9 points, respectively. A MASPRM trained on GSM8K transfers\nzero-shot to MATH without retraining, adding 8.4 EM points at the same\nbudget. MASPRM is a plug-in value model that estimates per-agent progress and\ncomplements verifier-style decoders, enabling more reliable, compute-aware\nmulti-agent reasoning. Code: https://github.com/milad1378yz/MASPRM",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.24803.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "624a82573571171191af5e60",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/tMIMn7oeSsDDpPuZ08co7.png",
      "fullname": "Milad Yazdani",
      "name": "Miladyz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.25760",
      "authors": [
        {
          "_id": "6902f25c72739622ee92a8c4",
          "name": "Xu Zheng",
          "hidden": false
        },
        {
          "_id": "6902f25c72739622ee92a8c5",
          "name": "Zihao Dongfang",
          "hidden": false
        },
        {
          "_id": "6902f25c72739622ee92a8c6",
          "name": "Lutao Jiang",
          "hidden": false
        },
        {
          "_id": "6902f25c72739622ee92a8c7",
          "name": "Boyuan Zheng",
          "hidden": false
        },
        {
          "_id": "6902f25c72739622ee92a8c8",
          "name": "Yulong Guo",
          "hidden": false
        },
        {
          "_id": "6902f25c72739622ee92a8c9",
          "name": "Zhenquan Zhang",
          "hidden": false
        },
        {
          "_id": "6902f25c72739622ee92a8ca",
          "name": "Giuliano Albanese",
          "hidden": false
        },
        {
          "_id": "6902f25c72739622ee92a8cb",
          "name": "Runyi Yang",
          "hidden": false
        },
        {
          "_id": "6902f25c72739622ee92a8cc",
          "name": "Mengjiao Ma",
          "hidden": false
        },
        {
          "_id": "6902f25c72739622ee92a8cd",
          "name": "Zixin Zhang",
          "hidden": false
        },
        {
          "_id": "6902f25c72739622ee92a8ce",
          "name": "Chenfei Liao",
          "hidden": false
        },
        {
          "_id": "6902f25c72739622ee92a8cf",
          "name": "Dingcheng Zhen",
          "hidden": false
        },
        {
          "_id": "6902f25c72739622ee92a8d0",
          "name": "Yuanhuiyi Lyu",
          "hidden": false
        },
        {
          "_id": "6902f25c72739622ee92a8d1",
          "name": "Yuqian Fu",
          "hidden": false
        },
        {
          "_id": "6902f25c72739622ee92a8d2",
          "name": "Bin Ren",
          "hidden": false
        },
        {
          "_id": "6902f25c72739622ee92a8d3",
          "name": "Linfeng Zhang",
          "hidden": false
        },
        {
          "_id": "6902f25c72739622ee92a8d4",
          "name": "Danda Pani Paudel",
          "hidden": false
        },
        {
          "_id": "6902f25c72739622ee92a8d5",
          "name": "Nicu Sebe",
          "hidden": false
        },
        {
          "_id": "6902f25c72739622ee92a8d6",
          "name": "Luc Van Gool",
          "hidden": false
        },
        {
          "_id": "6902f25c72739622ee92a8d7",
          "name": "Xuming Hu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-29T17:55:43.000Z",
      "submittedOnDailyAt": "2025-10-30T03:42:49.312Z",
      "title": "Multimodal Spatial Reasoning in the Large Model Era: A Survey and\n  Benchmarks",
      "submittedOnDailyBy": {
        "_id": "6806464ed918f6d2fee2bc8b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6806464ed918f6d2fee2bc8b/rgpG2oO0m6PT0KltCF_Wf.jpeg",
        "isPro": false,
        "fullname": "Chenfei Liao",
        "user": "Chenfei-Liao",
        "type": "user"
      },
      "summary": "Humans possess spatial reasoning abilities that enable them to understand\nspaces through multimodal observations, such as vision and sound. Large\nmultimodal reasoning models extend these abilities by learning to perceive and\nreason, showing promising performance across diverse spatial tasks. However,\nsystematic reviews and publicly available benchmarks for these models remain\nlimited. In this survey, we provide a comprehensive review of multimodal\nspatial reasoning tasks with large models, categorizing recent progress in\nmultimodal large language models (MLLMs) and introducing open benchmarks for\nevaluation. We begin by outlining general spatial reasoning, focusing on\npost-training techniques, explainability, and architecture. Beyond classical 2D\ntasks, we examine spatial relationship reasoning, scene and layout\nunderstanding, as well as visual question answering and grounding in 3D space.\nWe also review advances in embodied AI, including vision-language navigation\nand action models. Additionally, we consider emerging modalities such as audio\nand egocentric video, which contribute to novel spatial understanding through\nnew sensors. We believe this survey establishes a solid foundation and offers\ninsights into the growing field of multimodal spatial reasoning. Updated\ninformation about this survey, codes and implementation of the open benchmarks\ncan be found at https://github.com/zhengxuJosh/Awesome-Spatial-Reasoning.",
      "upvotes": 3,
      "discussionId": "6902f25d72739622ee92a8d8",
      "organization": {
        "_id": "660104b1569b30694e5a60f0",
        "name": "hkust-gz",
        "fullname": "hongkong university of science and technology"
      }
    },
    "publishedAt": "2025-10-29T13:55:43.000Z",
    "title": "Multimodal Spatial Reasoning in the Large Model Era: A Survey and\n  Benchmarks",
    "summary": "Humans possess spatial reasoning abilities that enable them to understand\nspaces through multimodal observations, such as vision and sound. Large\nmultimodal reasoning models extend these abilities by learning to perceive and\nreason, showing promising performance across diverse spatial tasks. However,\nsystematic reviews and publicly available benchmarks for these models remain\nlimited. In this survey, we provide a comprehensive review of multimodal\nspatial reasoning tasks with large models, categorizing recent progress in\nmultimodal large language models (MLLMs) and introducing open benchmarks for\nevaluation. We begin by outlining general spatial reasoning, focusing on\npost-training techniques, explainability, and architecture. Beyond classical 2D\ntasks, we examine spatial relationship reasoning, scene and layout\nunderstanding, as well as visual question answering and grounding in 3D space.\nWe also review advances in embodied AI, including vision-language navigation\nand action models. Additionally, we consider emerging modalities such as audio\nand egocentric video, which contribute to novel spatial understanding through\nnew sensors. We believe this survey establishes a solid foundation and offers\ninsights into the growing field of multimodal spatial reasoning. Updated\ninformation about this survey, codes and implementation of the open benchmarks\ncan be found at https://github.com/zhengxuJosh/Awesome-Spatial-Reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.25760.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6806464ed918f6d2fee2bc8b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6806464ed918f6d2fee2bc8b/rgpG2oO0m6PT0KltCF_Wf.jpeg",
      "fullname": "Chenfei Liao",
      "name": "Chenfei-Liao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "organization": {
      "_id": "660104b1569b30694e5a60f0",
      "name": "hkust-gz",
      "fullname": "hongkong university of science and technology"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.22543",
      "authors": [
        {
          "_id": "69003b3d22d452aac6dd43b0",
          "name": "Yuyang Ding",
          "hidden": false
        },
        {
          "_id": "69003b3d22d452aac6dd43b1",
          "name": "Chi Zhang",
          "hidden": false
        },
        {
          "_id": "69003b3d22d452aac6dd43b2",
          "name": "Juntao Li",
          "hidden": false
        },
        {
          "_id": "69003b3d22d452aac6dd43b3",
          "name": "Haibin Lin",
          "hidden": false
        },
        {
          "_id": "69003b3d22d452aac6dd43b4",
          "name": "Xin Liu",
          "hidden": false
        },
        {
          "_id": "69003b3d22d452aac6dd43b5",
          "name": "Min Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-26T05:49:38.000Z",
      "submittedOnDailyAt": "2025-10-30T03:42:35.611Z",
      "title": "FAPO: Flawed-Aware Policy Optimization for Efficient and Reliable\n  Reasoning",
      "submittedOnDailyBy": {
        "_id": "626cf0f65651e31a7a2b9779",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/626cf0f65651e31a7a2b9779/TAE--QIKo1vz4Rb8aearl.jpeg",
        "isPro": false,
        "fullname": "Ding",
        "user": "dyyyyyyyy",
        "type": "user"
      },
      "summary": "Reinforcement learning with verifiable rewards (RLVR) has emerged as a\npromising paradigm for enhancing the reasoning capabilities of large language\nmodels (LLMs). In this context, models explore reasoning trajectories and\nexploit rollouts with correct answers as positive signals for policy\noptimization. However, these rollouts might involve flawed patterns such as\nanswer-guessing and jump-in-reasoning. Such flawed-positive rollouts are\nrewarded identically to fully correct ones, causing policy models to\ninternalize these unreliable reasoning patterns. In this work, we first conduct\na systematic study of flawed-positive rollouts in RL and find that they enable\nrapid capability gains during the early optimization stage, while constraining\nreasoning capability later by reinforcing unreliable patterns. Building on\nthese insights, we propose Flawed-Aware Policy Optimization (FAPO), which\npresents a parameter-free reward penalty for flawed-positive rollouts, enabling\nthe policy to leverage them as useful shortcuts in the warm-up stage, securing\nstable early gains, while gradually shifting optimization toward reliable\nreasoning in the later refinement stage. To accurately and comprehensively\ndetect flawed-positive rollouts, we introduce a generative reward model (GenRM)\nwith a process-level reward that precisely localizes reasoning errors.\nExperiments show that FAPO is effective in broad domains, improving outcome\ncorrectness, process reliability, and training stability without increasing the\ntoken budget.",
      "upvotes": 1,
      "discussionId": "69003b3d22d452aac6dd43b6",
      "projectPage": "https://fapo-rl.github.io/",
      "githubRepo": "https://github.com/volcengine/verl/tree/main/recipe/fapo",
      "ai_summary": "Flawed-Aware Policy Optimization (FAPO) improves reinforcement learning with verifiable rewards by penalizing flawed-positive rollouts, enhancing reasoning capability and training stability without increasing computational cost.",
      "ai_keywords": [
        "reinforcement learning",
        "verifiable rewards",
        "large language models",
        "reasoning trajectories",
        "policy optimization",
        "flawed-positive rollouts",
        "parameter-free reward penalty",
        "generative reward model",
        "process-level reward",
        "reasoning errors",
        "outcome correctness",
        "process reliability",
        "training stability"
      ],
      "githubStars": 14904
    },
    "publishedAt": "2025-10-26T01:49:38.000Z",
    "title": "FAPO: Flawed-Aware Policy Optimization for Efficient and Reliable\n  Reasoning",
    "summary": "Reinforcement learning with verifiable rewards (RLVR) has emerged as a\npromising paradigm for enhancing the reasoning capabilities of large language\nmodels (LLMs). In this context, models explore reasoning trajectories and\nexploit rollouts with correct answers as positive signals for policy\noptimization. However, these rollouts might involve flawed patterns such as\nanswer-guessing and jump-in-reasoning. Such flawed-positive rollouts are\nrewarded identically to fully correct ones, causing policy models to\ninternalize these unreliable reasoning patterns. In this work, we first conduct\na systematic study of flawed-positive rollouts in RL and find that they enable\nrapid capability gains during the early optimization stage, while constraining\nreasoning capability later by reinforcing unreliable patterns. Building on\nthese insights, we propose Flawed-Aware Policy Optimization (FAPO), which\npresents a parameter-free reward penalty for flawed-positive rollouts, enabling\nthe policy to leverage them as useful shortcuts in the warm-up stage, securing\nstable early gains, while gradually shifting optimization toward reliable\nreasoning in the later refinement stage. To accurately and comprehensively\ndetect flawed-positive rollouts, we introduce a generative reward model (GenRM)\nwith a process-level reward that precisely localizes reasoning errors.\nExperiments show that FAPO is effective in broad domains, improving outcome\ncorrectness, process reliability, and training stability without increasing the\ntoken budget.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.22543.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "626cf0f65651e31a7a2b9779",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/626cf0f65651e31a7a2b9779/TAE--QIKo1vz4Rb8aearl.jpeg",
      "fullname": "Ding",
      "name": "dyyyyyyyy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 16
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.25758",
      "authors": [
        {
          "_id": "6902c90472739622ee92a783",
          "name": "He Hu",
          "hidden": false
        },
        {
          "_id": "6902c90472739622ee92a784",
          "name": "Yucheng Zhou",
          "hidden": false
        },
        {
          "_id": "6902c90472739622ee92a785",
          "name": "Chiyuan Ma",
          "hidden": false
        },
        {
          "_id": "6902c90472739622ee92a786",
          "name": "Qianning Wang",
          "hidden": false
        },
        {
          "_id": "6902c90472739622ee92a787",
          "name": "Zheng Zhang",
          "hidden": false
        },
        {
          "_id": "6902c90472739622ee92a788",
          "name": "Fei Ma",
          "hidden": false
        },
        {
          "_id": "6902c90472739622ee92a789",
          "name": "Laizhong Cui",
          "hidden": false
        },
        {
          "_id": "6902c90472739622ee92a78a",
          "name": "Qi Tian",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-29T17:54:20.000Z",
      "submittedOnDailyAt": "2025-10-30T00:40:58.582Z",
      "title": "TheraMind: A Strategic and Adaptive Agent for Longitudinal Psychological\n  Counseling",
      "submittedOnDailyBy": {
        "_id": "67c5541317ff2ccbf95c1e81",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67c5541317ff2ccbf95c1e81/ivFXIwna3ofM3rcTiGuHZ.jpeg",
        "isPro": false,
        "fullname": "HU HE",
        "user": "GMLHUHE",
        "type": "user"
      },
      "summary": "Large language models (LLMs) in psychological counseling have attracted\nincreasing attention. However, existing approaches often lack emotional\nunderstanding, adaptive strategies, and the use of therapeutic methods across\nmultiple sessions with long-term memory, leaving them far from real clinical\npractice. To address these critical gaps, we introduce TheraMind, a strategic\nand adaptive agent for longitudinal psychological counseling. The cornerstone\nof TheraMind is a novel dual-loop architecture that decouples the complex\ncounseling process into an Intra-Session Loop for tactical dialogue management\nand a Cross-Session Loop for strategic therapeutic planning. The Intra-Session\nLoop perceives the patient's emotional state to dynamically select response\nstrategies while leveraging cross-session memory to ensure continuity.\nCrucially, the Cross-Session Loop empowers the agent with long-term\nadaptability by evaluating the efficacy of the applied therapy after each\nsession and adjusting the method for subsequent interactions. We validate our\napproach in a high-fidelity simulation environment grounded in real clinical\ncases. Extensive evaluations show that TheraMind outperforms other methods,\nespecially on multi-session metrics like Coherence, Flexibility, and\nTherapeutic Attunement, validating the effectiveness of its dual-loop design in\nemulating strategic, adaptive, and longitudinal therapeutic behavior. The code\nis publicly available at https://0mwwm0.github.io/TheraMind/.",
      "upvotes": 0,
      "discussionId": "6902c90572739622ee92a78b",
      "projectPage": "https://0mwwm0.github.io/TheraMind/"
    },
    "publishedAt": "2025-10-29T13:54:20.000Z",
    "title": "TheraMind: A Strategic and Adaptive Agent for Longitudinal Psychological\n  Counseling",
    "summary": "Large language models (LLMs) in psychological counseling have attracted\nincreasing attention. However, existing approaches often lack emotional\nunderstanding, adaptive strategies, and the use of therapeutic methods across\nmultiple sessions with long-term memory, leaving them far from real clinical\npractice. To address these critical gaps, we introduce TheraMind, a strategic\nand adaptive agent for longitudinal psychological counseling. The cornerstone\nof TheraMind is a novel dual-loop architecture that decouples the complex\ncounseling process into an Intra-Session Loop for tactical dialogue management\nand a Cross-Session Loop for strategic therapeutic planning. The Intra-Session\nLoop perceives the patient's emotional state to dynamically select response\nstrategies while leveraging cross-session memory to ensure continuity.\nCrucially, the Cross-Session Loop empowers the agent with long-term\nadaptability by evaluating the efficacy of the applied therapy after each\nsession and adjusting the method for subsequent interactions. We validate our\napproach in a high-fidelity simulation environment grounded in real clinical\ncases. Extensive evaluations show that TheraMind outperforms other methods,\nespecially on multi-session metrics like Coherence, Flexibility, and\nTherapeutic Attunement, validating the effectiveness of its dual-loop design in\nemulating strategic, adaptive, and longitudinal therapeutic behavior. The code\nis publicly available at https://0mwwm0.github.io/TheraMind/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.25758.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67c5541317ff2ccbf95c1e81",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67c5541317ff2ccbf95c1e81/ivFXIwna3ofM3rcTiGuHZ.jpeg",
      "fullname": "HU HE",
      "name": "GMLHUHE",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.25409",
      "authors": [
        {
          "_id": "6902c31d72739622ee92a68d",
          "name": "Vijay Devane",
          "hidden": false
        },
        {
          "_id": "6902c31d72739622ee92a68e",
          "name": "Mohd Nauman",
          "hidden": false
        },
        {
          "_id": "6902c31d72739622ee92a68f",
          "name": "Bhargav Patel",
          "hidden": false
        },
        {
          "_id": "6902c31d72739622ee92a690",
          "name": "Aniket Mahendra Wakchoure",
          "hidden": false
        },
        {
          "_id": "6902c31d72739622ee92a691",
          "name": "Yogeshkumar Sant",
          "hidden": false
        },
        {
          "_id": "6902c31d72739622ee92a692",
          "name": "Shyam Pawar",
          "hidden": false
        },
        {
          "_id": "6902c31d72739622ee92a693",
          "name": "Viraj Thakur",
          "hidden": false
        },
        {
          "_id": "6902c31d72739622ee92a694",
          "name": "Ananya Godse",
          "hidden": false
        },
        {
          "_id": "6902c31d72739622ee92a695",
          "name": "Sunil Patra",
          "hidden": false
        },
        {
          "_id": "6902c31d72739622ee92a696",
          "name": "Neha Maurya",
          "hidden": false
        },
        {
          "_id": "6902c31d72739622ee92a697",
          "name": "Suraj Racha",
          "hidden": false
        },
        {
          "_id": "6902c31d72739622ee92a698",
          "name": "Nitish Kamal Singh",
          "hidden": false
        },
        {
          "_id": "6902c31d72739622ee92a699",
          "name": "Ajay Nagpal",
          "hidden": false
        },
        {
          "_id": "6902c31d72739622ee92a69a",
          "name": "Piyush Sawarkar",
          "hidden": false
        },
        {
          "_id": "6902c31d72739622ee92a69b",
          "name": "Kundeshwar Vijayrao Pundalik",
          "hidden": false
        },
        {
          "_id": "6902c31d72739622ee92a69c",
          "name": "Rohit Saluja",
          "hidden": false
        },
        {
          "_id": "6902c31d72739622ee92a69d",
          "name": "Ganesh Ramakrishnan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-29T11:27:08.000Z",
      "submittedOnDailyAt": "2025-10-30T00:16:00.088Z",
      "title": "BhashaBench V1: A Comprehensive Benchmark for the Quadrant of Indic\n  Domains",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "The rapid advancement of large language models(LLMs) has intensified the need\nfor domain and culture specific evaluation. Existing benchmarks are largely\nAnglocentric and domain-agnostic, limiting their applicability to India-centric\ncontexts. To address this gap, we introduce BhashaBench V1, the first\ndomain-specific, multi-task, bilingual benchmark focusing on critical Indic\nknowledge systems. BhashaBench V1 contains 74,166 meticulously curated\nquestion-answer pairs, with 52,494 in English and 21,672 in Hindi, sourced from\nauthentic government and domain-specific exams. It spans four major domains:\nAgriculture, Legal, Finance, and Ayurveda, comprising 90+ subdomains and\ncovering 500+ topics, enabling fine-grained evaluation. Evaluation of 29+ LLMs\nreveals significant domain and language specific performance gaps, with\nespecially large disparities in low-resource domains. For instance, GPT-4o\nachieves 76.49% overall accuracy in Legal but only 59.74% in Ayurveda. Models\nconsistently perform better on English content compared to Hindi across all\ndomains. Subdomain-level analysis shows that areas such as Cyber Law,\nInternational Finance perform relatively well, while Panchakarma, Seed Science,\nand Human Rights remain notably weak. BhashaBench V1 provides a comprehensive\ndataset for evaluating large language models across India's diverse knowledge\ndomains. It enables assessment of models' ability to integrate domain-specific\nknowledge with bilingual understanding. All code, benchmarks, and resources are\npublicly available to support open research.",
      "upvotes": 0,
      "discussionId": "6902c31d72739622ee92a69e",
      "organization": {
        "_id": "67b473e74dd7ea0538ef5d5f",
        "name": "bharatgenai",
        "fullname": "BharatGen AI",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67b462a1f4f414c2b3e2bc2f/EnVeNWEIeZ6yF6ueZ7E3Y.jpeg"
      }
    },
    "publishedAt": "2025-10-29T07:27:08.000Z",
    "title": "BhashaBench V1: A Comprehensive Benchmark for the Quadrant of Indic\n  Domains",
    "summary": "The rapid advancement of large language models(LLMs) has intensified the need\nfor domain and culture specific evaluation. Existing benchmarks are largely\nAnglocentric and domain-agnostic, limiting their applicability to India-centric\ncontexts. To address this gap, we introduce BhashaBench V1, the first\ndomain-specific, multi-task, bilingual benchmark focusing on critical Indic\nknowledge systems. BhashaBench V1 contains 74,166 meticulously curated\nquestion-answer pairs, with 52,494 in English and 21,672 in Hindi, sourced from\nauthentic government and domain-specific exams. It spans four major domains:\nAgriculture, Legal, Finance, and Ayurveda, comprising 90+ subdomains and\ncovering 500+ topics, enabling fine-grained evaluation. Evaluation of 29+ LLMs\nreveals significant domain and language specific performance gaps, with\nespecially large disparities in low-resource domains. For instance, GPT-4o\nachieves 76.49% overall accuracy in Legal but only 59.74% in Ayurveda. Models\nconsistently perform better on English content compared to Hindi across all\ndomains. Subdomain-level analysis shows that areas such as Cyber Law,\nInternational Finance perform relatively well, while Panchakarma, Seed Science,\nand Human Rights remain notably weak. BhashaBench V1 provides a comprehensive\ndataset for evaluating large language models across India's diverse knowledge\ndomains. It enables assessment of models' ability to integrate domain-specific\nknowledge with bilingual understanding. All code, benchmarks, and resources are\npublicly available to support open research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.25409.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 150
    },
    "organization": {
      "_id": "67b473e74dd7ea0538ef5d5f",
      "name": "bharatgenai",
      "fullname": "BharatGen AI",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67b462a1f4f414c2b3e2bc2f/EnVeNWEIeZ6yF6ueZ7E3Y.jpeg"
    },
    "isAuthorParticipating": false
  }
]