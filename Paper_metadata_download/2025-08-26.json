[
  {
    "paper": {
      "id": "2508.18265",
      "authors": [
        {
          "_id": "68ad214d86b21a0e2e358d5d",
          "name": "Weiyun Wang",
          "hidden": false
        },
        {
          "_id": "68ad214d86b21a0e2e358d5e",
          "name": "Zhangwei Gao",
          "hidden": false
        },
        {
          "_id": "68ad214d86b21a0e2e358d5f",
          "name": "Lixin Gu",
          "hidden": false
        },
        {
          "_id": "68ad214d86b21a0e2e358d60",
          "name": "Hengjun Pu",
          "hidden": false
        },
        {
          "_id": "68ad214d86b21a0e2e358d61",
          "name": "Long Cui",
          "hidden": false
        },
        {
          "_id": "68ad214d86b21a0e2e358d62",
          "name": "Xingguang Wei",
          "hidden": false
        },
        {
          "_id": "68ad214d86b21a0e2e358d63",
          "name": "Zhaoyang Liu",
          "hidden": false
        },
        {
          "_id": "68ad214d86b21a0e2e358d64",
          "name": "Linglin Jing",
          "hidden": false
        },
        {
          "_id": "68ad214d86b21a0e2e358d65",
          "name": "Shenglong Ye",
          "hidden": false
        },
        {
          "_id": "68ad214d86b21a0e2e358d66",
          "name": "Jie Shao",
          "hidden": false
        },
        {
          "_id": "68ad214d86b21a0e2e358d67",
          "name": "Zhaokai Wang",
          "hidden": false
        },
        {
          "_id": "68ad214d86b21a0e2e358d68",
          "name": "Zhe Chen",
          "hidden": false
        },
        {
          "_id": "68ad214d86b21a0e2e358d69",
          "name": "Hongjie Zhang",
          "hidden": false
        },
        {
          "_id": "68ad214d86b21a0e2e358d6a",
          "name": "Ganlin Yang",
          "hidden": false
        },
        {
          "_id": "68ad214d86b21a0e2e358d6b",
          "name": "Haomin Wang",
          "hidden": false
        },
        {
          "_id": "68ad214d86b21a0e2e358d6c",
          "name": "Qi Wei",
          "hidden": false
        },
        {
          "_id": "68ad214d86b21a0e2e358d6d",
          "name": "Jinhui Yin",
          "hidden": false
        },
        {
          "_id": "68ad214d86b21a0e2e358d6e",
          "name": "Wenhao Li",
          "hidden": false
        },
        {
          "_id": "68ad214d86b21a0e2e358d6f",
          "name": "Erfei Cui",
          "hidden": false
        },
        {
          "_id": "68ad214d86b21a0e2e358d70",
          "name": "Guanzhou Chen",
          "hidden": false
        },
        {
          "_id": "68ad214d86b21a0e2e358d71",
          "name": "Zichen Ding",
          "hidden": false
        },
        {
          "_id": "68ad214d86b21a0e2e358d72",
          "name": "Changyao Tian",
          "hidden": false
        },
        {
          "_id": "68ad214d86b21a0e2e358d73",
          "name": "Zhenyu Wu",
          "hidden": false
        },
        {
          "_id": "68ad214d86b21a0e2e358d74",
          "name": "Jingjing Xie",
          "hidden": false
        },
        {
          "_id": "68ad214d86b21a0e2e358d75",
          "name": "Zehao Li",
          "hidden": false
        },
        {
          "_id": "68ad214d86b21a0e2e358d76",
          "name": "Bowen Yang",
          "hidden": false
        },
        {
          "_id": "68ad214d86b21a0e2e358d77",
          "name": "Yuchen Duan",
          "hidden": false
        },
        {
          "_id": "68ad214d86b21a0e2e358d78",
          "name": "Xuehui Wang",
          "hidden": false
        },
        {
          "_id": "68ad214d86b21a0e2e358d79",
          "name": "Songze Li",
          "hidden": false
        },
        {
          "_id": "68ad214d86b21a0e2e358d7a",
          "name": "Xiangyu Zhao",
          "hidden": false
        },
        {
          "_id": "68ad214d86b21a0e2e358d7b",
          "name": "Haodong Duan",
          "hidden": false
        },
        {
          "_id": "68ad214d86b21a0e2e358d7c",
          "name": "Nianchen Deng",
          "hidden": false
        },
        {
          "_id": "68ad214d86b21a0e2e358d7d",
          "name": "Bin Fu",
          "hidden": false
        },
        {
          "_id": "68ad214d86b21a0e2e358d7e",
          "name": "Yinan He",
          "hidden": false
        },
        {
          "_id": "68ad214d86b21a0e2e358d7f",
          "name": "Yi Wang",
          "hidden": false
        },
        {
          "_id": "68ad214d86b21a0e2e358d80",
          "name": "Conghui He",
          "hidden": false
        },
        {
          "_id": "68ad214d86b21a0e2e358d81",
          "name": "Botian Shi",
          "hidden": false
        },
        {
          "_id": "68ad214d86b21a0e2e358d82",
          "name": "Junjun He",
          "hidden": false
        },
        {
          "_id": "68ad214d86b21a0e2e358d83",
          "name": "Yingtong Xiong",
          "hidden": false
        },
        {
          "_id": "68ad214d86b21a0e2e358d84",
          "name": "Han Lv",
          "hidden": false
        },
        {
          "_id": "68ad214d86b21a0e2e358d85",
          "name": "Lijun Wu",
          "hidden": false
        },
        {
          "_id": "68ad214d86b21a0e2e358d86",
          "name": "Wenqi Shao",
          "hidden": false
        },
        {
          "_id": "68ad214d86b21a0e2e358d87",
          "name": "Kaipeng Zhang",
          "hidden": false
        },
        {
          "_id": "68ad214d86b21a0e2e358d88",
          "name": "Huipeng Deng",
          "hidden": false
        },
        {
          "_id": "68ad214d86b21a0e2e358d89",
          "name": "Biqing Qi",
          "hidden": false
        },
        {
          "_id": "68ad214d86b21a0e2e358d8a",
          "name": "Jiaye Ge",
          "hidden": false
        },
        {
          "_id": "68ad214d86b21a0e2e358d8b",
          "name": "Qipeng Guo",
          "hidden": false
        },
        {
          "_id": "68ad214d86b21a0e2e358d8c",
          "name": "Wenwei Zhang",
          "hidden": false
        },
        {
          "_id": "68ad214d86b21a0e2e358d8d",
          "name": "Wanli Ouyang",
          "hidden": false
        },
        {
          "_id": "68ad214d86b21a0e2e358d8e",
          "name": "Limin Wang",
          "hidden": false
        },
        {
          "_id": "68ad214d86b21a0e2e358d8f",
          "name": "Min Dou",
          "hidden": false
        },
        {
          "_id": "68ad214d86b21a0e2e358d90",
          "name": "Xizhou Zhu",
          "hidden": false
        },
        {
          "_id": "68ad214d86b21a0e2e358d91",
          "name": "Tong Lu",
          "hidden": false
        },
        {
          "_id": "68ad214d86b21a0e2e358d92",
          "name": "Dahua Lin",
          "hidden": false
        },
        {
          "_id": "68ad214d86b21a0e2e358d93",
          "name": "Jifeng Dai",
          "hidden": false
        },
        {
          "_id": "68ad214d86b21a0e2e358d94",
          "name": "Bowen Zhou",
          "hidden": false
        },
        {
          "_id": "68ad214d86b21a0e2e358d95",
          "name": "Weijie Su",
          "hidden": false
        },
        {
          "_id": "68ad214d86b21a0e2e358d96",
          "name": "Kai Chen",
          "hidden": false
        },
        {
          "_id": "68ad214d86b21a0e2e358d97",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "68ad214d86b21a0e2e358d98",
          "name": "Wenhai Wang",
          "hidden": false
        },
        {
          "_id": "68ad214d86b21a0e2e358d99",
          "name": "Gen Luo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-25T17:58:17.000Z",
      "submittedOnDailyAt": "2025-08-26T01:22:14.339Z",
      "title": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility,\n  Reasoning, and Efficiency",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We introduce InternVL 3.5, a new family of open-source multimodal models that\nsignificantly advances versatility, reasoning capability, and inference\nefficiency along the InternVL series. A key innovation is the Cascade\nReinforcement Learning (Cascade RL) framework, which enhances reasoning through\na two-stage process: offline RL for stable convergence and online RL for\nrefined alignment. This coarse-to-fine training strategy leads to substantial\nimprovements on downstream reasoning tasks, e.g., MMMU and MathVista. To\noptimize efficiency, we propose a Visual Resolution Router (ViR) that\ndynamically adjusts the resolution of visual tokens without compromising\nperformance. Coupled with ViR, our Decoupled Vision-Language Deployment (DvD)\nstrategy separates the vision encoder and language model across different GPUs,\neffectively balancing computational load. These contributions collectively\nenable InternVL3.5 to achieve up to a +16.0\\% gain in overall reasoning\nperformance and a 4.05times inference speedup compared to its predecessor,\ni.e., InternVL3. In addition, InternVL3.5 supports novel capabilities such as\nGUI interaction and embodied agency. Notably, our largest model, i.e.,\nInternVL3.5-241B-A28B, attains state-of-the-art results among open-source MLLMs\nacross general multimodal, reasoning, text, and agentic tasks -- narrowing the\nperformance gap with leading commercial models like GPT-5. All models and code\nare publicly released.",
      "upvotes": 54,
      "discussionId": "68ad214d86b21a0e2e358d9a",
      "ai_summary": "InternVL 3.5 introduces Cascade RL, ViR, and DvD to enhance reasoning, efficiency, and performance in multimodal models.",
      "ai_keywords": [
        "Cascade RL",
        "offline RL",
        "online RL",
        "Visual Resolution Router",
        "ViR",
        "Decoupled Vision-Language Deployment",
        "DvD",
        "multimodal models",
        "reasoning performance",
        "inference speedup",
        "GUI interaction",
        "embodied agency"
      ]
    },
    "publishedAt": "2025-08-25T13:58:17.000Z",
    "title": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility,\n  Reasoning, and Efficiency",
    "summary": "We introduce InternVL 3.5, a new family of open-source multimodal models that\nsignificantly advances versatility, reasoning capability, and inference\nefficiency along the InternVL series. A key innovation is the Cascade\nReinforcement Learning (Cascade RL) framework, which enhances reasoning through\na two-stage process: offline RL for stable convergence and online RL for\nrefined alignment. This coarse-to-fine training strategy leads to substantial\nimprovements on downstream reasoning tasks, e.g., MMMU and MathVista. To\noptimize efficiency, we propose a Visual Resolution Router (ViR) that\ndynamically adjusts the resolution of visual tokens without compromising\nperformance. Coupled with ViR, our Decoupled Vision-Language Deployment (DvD)\nstrategy separates the vision encoder and language model across different GPUs,\neffectively balancing computational load. These contributions collectively\nenable InternVL3.5 to achieve up to a +16.0\\% gain in overall reasoning\nperformance and a 4.05times inference speedup compared to its predecessor,\ni.e., InternVL3. In addition, InternVL3.5 supports novel capabilities such as\nGUI interaction and embodied agency. Notably, our largest model, i.e.,\nInternVL3.5-241B-A28B, attains state-of-the-art results among open-source MLLMs\nacross general multimodal, reasoning, text, and agentic tasks -- narrowing the\nperformance gap with leading commercial models like GPT-5. All models and code\nare publicly released.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.18265.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 95
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.18032",
      "authors": [
        {
          "_id": "68ad236486b21a0e2e358db3",
          "name": "Yaqi Li",
          "hidden": false
        },
        {
          "_id": "68ad236486b21a0e2e358db4",
          "name": "Peng Chen",
          "hidden": false
        },
        {
          "_id": "68ad236486b21a0e2e358db5",
          "name": "Mingyang Han",
          "hidden": false
        },
        {
          "_id": "68ad236486b21a0e2e358db6",
          "name": "Bu Pi",
          "hidden": false
        },
        {
          "_id": "68ad236486b21a0e2e358db7",
          "name": "Haoxiang Shi",
          "hidden": false
        },
        {
          "_id": "68ad236486b21a0e2e358db8",
          "name": "Runzhou Zhao",
          "hidden": false
        },
        {
          "_id": "68ad236486b21a0e2e358db9",
          "name": "Yang Yao",
          "hidden": false
        },
        {
          "_id": "68ad236486b21a0e2e358dba",
          "name": "Xuan Zhang",
          "hidden": false
        },
        {
          "_id": "68ad236486b21a0e2e358dbb",
          "name": "Jun Song",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-25T13:53:02.000Z",
      "submittedOnDailyAt": "2025-08-26T01:31:03.330Z",
      "title": "Visual-CoG: Stage-Aware Reinforcement Learning with Chain of Guidance\n  for Text-to-Image Generation",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Despite the promising progress of recent autoregressive models in\ntext-to-image (T2I) generation, their ability to handle multi-attribute and\nambiguous prompts remains limited. To address these limitations, existing works\nhave applied chain-of-thought (CoT) to enable stage-aware visual synthesis and\nemployed reinforcement learning (RL) to improve reasoning capabilities.\nHowever, most models provide reward signals only at the end of the generation\nstage. This monolithic final-only guidance makes it difficult to identify which\nstages contribute positively to the final outcome and may lead to suboptimal\npolicies. To tackle this issue, we propose a Visual-Chain of Guidance\n(Visual-CoG) paradigm consisting of three stages: semantic reasoning, process\nrefining, and outcome evaluation, with stage-aware rewards providing immediate\nguidance throughout the image generation pipeline. We further construct a\nvisual cognition benchmark, VisCog-Bench, which comprises four subtasks to\nevaluate the effectiveness of semantic reasoning. Comprehensive evaluations on\nGenEval, T2I-CompBench, and the proposed VisCog-Bench show improvements of 15%,\n5%, and 19%, respectively, demonstrating the superior performance of the\nproposed Visual-CoG. We will release all the resources soon.",
      "upvotes": 22,
      "discussionId": "68ad236586b21a0e2e358dbc",
      "ai_summary": "The Visual-Chain of Guidance (Visual-CoG) paradigm enhances text-to-image generation by providing stage-aware rewards, improving performance across multiple benchmarks.",
      "ai_keywords": [
        "autoregressive models",
        "text-to-image (T2I) generation",
        "chain-of-thought (CoT)",
        "reinforcement learning (RL)",
        "stage-aware visual synthesis",
        "semantic reasoning",
        "process refining",
        "outcome evaluation",
        "Visual-Chain of Guidance (Visual-CoG)",
        "VisCog-Bench",
        "GenEval",
        "T2I-CompBench"
      ]
    },
    "publishedAt": "2025-08-25T09:53:02.000Z",
    "title": "Visual-CoG: Stage-Aware Reinforcement Learning with Chain of Guidance\n  for Text-to-Image Generation",
    "summary": "Despite the promising progress of recent autoregressive models in\ntext-to-image (T2I) generation, their ability to handle multi-attribute and\nambiguous prompts remains limited. To address these limitations, existing works\nhave applied chain-of-thought (CoT) to enable stage-aware visual synthesis and\nemployed reinforcement learning (RL) to improve reasoning capabilities.\nHowever, most models provide reward signals only at the end of the generation\nstage. This monolithic final-only guidance makes it difficult to identify which\nstages contribute positively to the final outcome and may lead to suboptimal\npolicies. To tackle this issue, we propose a Visual-Chain of Guidance\n(Visual-CoG) paradigm consisting of three stages: semantic reasoning, process\nrefining, and outcome evaluation, with stage-aware rewards providing immediate\nguidance throughout the image generation pipeline. We further construct a\nvisual cognition benchmark, VisCog-Bench, which comprises four subtasks to\nevaluate the effectiveness of semantic reasoning. Comprehensive evaluations on\nGenEval, T2I-CompBench, and the proposed VisCog-Bench show improvements of 15%,\n5%, and 19%, respectively, demonstrating the superior performance of the\nproposed Visual-CoG. We will release all the resources soon.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.18032.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 95
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.17472",
      "authors": [
        {
          "_id": "68ad304f86b21a0e2e358e22",
          "name": "Kaiyue Sun",
          "hidden": false
        },
        {
          "_id": "68ad304f86b21a0e2e358e23",
          "name": "Rongyao Fang",
          "hidden": false
        },
        {
          "_id": "68ad304f86b21a0e2e358e24",
          "name": "Chengqi Duan",
          "hidden": false
        },
        {
          "_id": "68ad304f86b21a0e2e358e25",
          "name": "Xian Liu",
          "hidden": false
        },
        {
          "_id": "68ad304f86b21a0e2e358e26",
          "name": "Xihui Liu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63640ce5ff4b318d1b7b6f5c/RwCZGuI1OB7yLiQgBLmd_.png",
        "https://cdn-uploads.huggingface.co/production/uploads/63640ce5ff4b318d1b7b6f5c/1HhE9m118CtQHrrvgQmbo.png",
        "https://cdn-uploads.huggingface.co/production/uploads/63640ce5ff4b318d1b7b6f5c/KNE5RJ6_y_em3ZRemlUSW.png"
      ],
      "publishedAt": "2025-08-24T17:59:38.000Z",
      "submittedOnDailyAt": "2025-08-26T02:30:30.581Z",
      "title": "T2I-ReasonBench: Benchmarking Reasoning-Informed Text-to-Image\n  Generation",
      "submittedOnDailyBy": {
        "_id": "63640ce5ff4b318d1b7b6f5c",
        "avatarUrl": "/avatars/9336d1dab1491f3c8e84b1ea287eb891.svg",
        "isPro": false,
        "fullname": "Kaiyue Sun",
        "user": "Kaiyue",
        "type": "user"
      },
      "summary": "We propose T2I-ReasonBench, a benchmark evaluating reasoning capabilities of\ntext-to-image (T2I) models. It consists of four dimensions: Idiom\nInterpretation, Textual Image Design, Entity-Reasoning and\nScientific-Reasoning. We propose a two-stage evaluation protocol to assess the\nreasoning accuracy and image quality. We benchmark various T2I generation\nmodels, and provide comprehensive analysis on their performances.",
      "upvotes": 14,
      "discussionId": "68ad304f86b21a0e2e358e27",
      "githubRepo": "https://github.com/KaiyueSun98/T2I-ReasonBench",
      "ai_summary": "T2I-ReasonBench evaluates the reasoning capabilities of text-to-image models across four dimensions using a two-stage protocol, analyzing their performance comprehensively.",
      "ai_keywords": [
        "text-to-image",
        "T2I",
        "Idiom Interpretation",
        "Textual Image Design",
        "Entity-Reasoning",
        "Scientific-Reasoning",
        "two-stage evaluation protocol"
      ],
      "githubStars": 4
    },
    "publishedAt": "2025-08-24T13:59:38.000Z",
    "title": "T2I-ReasonBench: Benchmarking Reasoning-Informed Text-to-Image\n  Generation",
    "summary": "We propose T2I-ReasonBench, a benchmark evaluating reasoning capabilities of\ntext-to-image (T2I) models. It consists of four dimensions: Idiom\nInterpretation, Textual Image Design, Entity-Reasoning and\nScientific-Reasoning. We propose a two-stage evaluation protocol to assess the\nreasoning accuracy and image quality. We benchmark various T2I generation\nmodels, and provide comprehensive analysis on their performances.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63640ce5ff4b318d1b7b6f5c/RwCZGuI1OB7yLiQgBLmd_.png",
      "https://cdn-uploads.huggingface.co/production/uploads/63640ce5ff4b318d1b7b6f5c/1HhE9m118CtQHrrvgQmbo.png",
      "https://cdn-uploads.huggingface.co/production/uploads/63640ce5ff4b318d1b7b6f5c/KNE5RJ6_y_em3ZRemlUSW.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.17472.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63640ce5ff4b318d1b7b6f5c",
      "avatarUrl": "/avatars/9336d1dab1491f3c8e84b1ea287eb891.svg",
      "fullname": "Kaiyue Sun",
      "name": "Kaiyue",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.16949",
      "authors": [
        {
          "_id": "68ad2b3c86b21a0e2e358de3",
          "name": "Yang Zhou",
          "hidden": false
        },
        {
          "_id": "68ad2b3c86b21a0e2e358de4",
          "name": "Sunzhu Li",
          "hidden": false
        },
        {
          "_id": "68ad2b3c86b21a0e2e358de5",
          "name": "Shunyu Liu",
          "hidden": false
        },
        {
          "_id": "68ad2b3c86b21a0e2e358de6",
          "name": "Wenkai Fang",
          "hidden": false
        },
        {
          "_id": "68ad2b3c86b21a0e2e358de7",
          "name": "Jiale Zhao",
          "hidden": false
        },
        {
          "_id": "68ad2b3c86b21a0e2e358de8",
          "name": "Jingwen Yang",
          "hidden": false
        },
        {
          "_id": "68ad2b3c86b21a0e2e358de9",
          "name": "Jianwei Lv",
          "hidden": false
        },
        {
          "_id": "68ad2b3c86b21a0e2e358dea",
          "name": "Kongcheng Zhang",
          "hidden": false
        },
        {
          "_id": "68ad2b3c86b21a0e2e358deb",
          "name": "Yihe Zhou",
          "hidden": false
        },
        {
          "_id": "68ad2b3c86b21a0e2e358dec",
          "name": "Hengtong Lu",
          "hidden": false
        },
        {
          "_id": "68ad2b3c86b21a0e2e358ded",
          "name": "Wei Chen",
          "hidden": false
        },
        {
          "_id": "68ad2b3c86b21a0e2e358dee",
          "name": "Yan Xie",
          "hidden": false
        },
        {
          "_id": "68ad2b3c86b21a0e2e358def",
          "name": "Mingli Song",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-23T08:47:31.000Z",
      "submittedOnDailyAt": "2025-08-26T02:08:07.486Z",
      "title": "Breaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement\n  Learning for General LLM Reasoning",
      "submittedOnDailyBy": {
        "_id": "670ff71f6b8de497472a81dc",
        "avatarUrl": "/avatars/43aef099b1c3edb9b17f45d07eb89cd7.svg",
        "isPro": false,
        "fullname": "YANG ZHOU",
        "user": "BAOLONGZHANSHEN",
        "type": "user"
      },
      "summary": "Recent advances in Large Language Models (LLMs) have underscored the\npotential of Reinforcement Learning (RL) to facilitate the emergence of\nreasoning capabilities. Despite the encouraging results, a fundamental dilemma\npersists as RL improvement relies on learning from high-quality samples, yet\nthe exploration for such samples remains bounded by the inherent limitations of\nLLMs. This, in effect, creates an undesirable cycle in which what cannot be\nexplored cannot be learned. In this work, we propose Rubric-Scaffolded\nReinforcement Learning (RuscaRL), a novel instructional scaffolding framework\ndesigned to break the exploration bottleneck for general LLM reasoning.\nSpecifically, RuscaRL introduces checklist-style rubrics as (1) explicit\nscaffolding for exploration during rollout generation, where different rubrics\nare provided as external guidance within task instructions to steer diverse\nhigh-quality responses. This guidance is gradually decayed over time,\nencouraging the model to internalize the underlying reasoning patterns; (2)\nverifiable rewards for exploitation during model training, where we can obtain\nrobust LLM-as-a-Judge scores using rubrics as references, enabling effective RL\non general reasoning tasks. Extensive experiments demonstrate the superiority\nof the proposed RuscaRL across various benchmarks, effectively expanding\nreasoning boundaries under the best-of-N evaluation. Notably, RuscaRL\nsignificantly boosts Qwen-2.5-7B-Instruct from 23.6 to 50.3 on HealthBench-500,\nsurpassing GPT-4.1. Furthermore, our fine-tuned variant on\nQwen3-30B-A3B-Instruct achieves 61.1 on HealthBench-500, outperforming leading\nLLMs including OpenAI-o3.",
      "upvotes": 7,
      "discussionId": "68ad2b3c86b21a0e2e358df0",
      "ai_summary": "RuscaRL, a novel instructional scaffolding framework, enhances LLM reasoning by using rubrics for exploration and verifiable rewards, significantly improving performance on reasoning tasks.",
      "ai_keywords": [
        "Large Language Models",
        "Reinforcement Learning",
        "Rubric-Scaffolded Reinforcement Learning",
        "RuscaRL",
        "checklist-style rubrics",
        "rollout generation",
        "LLM-as-a-Judge",
        "best-of-N evaluation",
        "HealthBench-500",
        "Qwen-2.5-7B-Instruct",
        "GPT-4.1",
        "Qwen3-30B-A3B-Instruct",
        "OpenAI-o3"
      ]
    },
    "publishedAt": "2025-08-23T04:47:31.000Z",
    "title": "Breaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement\n  Learning for General LLM Reasoning",
    "summary": "Recent advances in Large Language Models (LLMs) have underscored the\npotential of Reinforcement Learning (RL) to facilitate the emergence of\nreasoning capabilities. Despite the encouraging results, a fundamental dilemma\npersists as RL improvement relies on learning from high-quality samples, yet\nthe exploration for such samples remains bounded by the inherent limitations of\nLLMs. This, in effect, creates an undesirable cycle in which what cannot be\nexplored cannot be learned. In this work, we propose Rubric-Scaffolded\nReinforcement Learning (RuscaRL), a novel instructional scaffolding framework\ndesigned to break the exploration bottleneck for general LLM reasoning.\nSpecifically, RuscaRL introduces checklist-style rubrics as (1) explicit\nscaffolding for exploration during rollout generation, where different rubrics\nare provided as external guidance within task instructions to steer diverse\nhigh-quality responses. This guidance is gradually decayed over time,\nencouraging the model to internalize the underlying reasoning patterns; (2)\nverifiable rewards for exploitation during model training, where we can obtain\nrobust LLM-as-a-Judge scores using rubrics as references, enabling effective RL\non general reasoning tasks. Extensive experiments demonstrate the superiority\nof the proposed RuscaRL across various benchmarks, effectively expanding\nreasoning boundaries under the best-of-N evaluation. Notably, RuscaRL\nsignificantly boosts Qwen-2.5-7B-Instruct from 23.6 to 50.3 on HealthBench-500,\nsurpassing GPT-4.1. Furthermore, our fine-tuned variant on\nQwen3-30B-A3B-Instruct achieves 61.1 on HealthBench-500, outperforming leading\nLLMs including OpenAI-o3.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.16949.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "670ff71f6b8de497472a81dc",
      "avatarUrl": "/avatars/43aef099b1c3edb9b17f45d07eb89cd7.svg",
      "fullname": "YANG ZHOU",
      "name": "BAOLONGZHANSHEN",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.17188",
      "authors": [
        {
          "_id": "68ad136786b21a0e2e358d1c",
          "name": "Zhilin Zhang",
          "hidden": false
        },
        {
          "_id": "68ad136786b21a0e2e358d1d",
          "name": "Xiang Zhang",
          "hidden": false
        },
        {
          "_id": "68ad136786b21a0e2e358d1e",
          "name": "Jiaqi Wei",
          "hidden": false
        },
        {
          "_id": "68ad136786b21a0e2e358d1f",
          "name": "Yiwei Xu",
          "hidden": false
        },
        {
          "_id": "68ad136786b21a0e2e358d20",
          "name": "Chenyu You",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-24T02:25:45.000Z",
      "submittedOnDailyAt": "2025-08-26T00:23:03.761Z",
      "title": "PosterGen: Aesthetic-Aware Paper-to-Poster Generation via Multi-Agent\n  LLMs",
      "submittedOnDailyBy": {
        "_id": "656553d89bf6665f10e3a92d",
        "avatarUrl": "/avatars/f55b09e249c48ef3fe484afa33a182ae.svg",
        "isPro": false,
        "fullname": "xiang wyatt zhang",
        "user": "Wyattz23",
        "type": "user"
      },
      "summary": "Multi-agent systems built upon large language models (LLMs) have demonstrated\nremarkable capabilities in tackling complex compositional tasks. In this work,\nwe apply this paradigm to the paper-to-poster generation problem, a practical\nyet time-consuming process faced by researchers preparing for conferences.\nWhile recent approaches have attempted to automate this task, most neglect core\ndesign and aesthetic principles, resulting in posters that require substantial\nmanual refinement. To address these design limitations, we propose PosterGen, a\nmulti-agent framework that mirrors the workflow of professional poster\ndesigners. It consists of four collaborative specialized agents: (1) Parser and\nCurator agents extract content from the paper and organize storyboard; (2)\nLayout agent maps the content into a coherent spatial layout; (3) Stylist\nagents apply visual design elements such as color and typography; and (4)\nRenderer composes the final poster. Together, these agents produce posters that\nare both semantically grounded and visually appealing. To evaluate design\nquality, we introduce a vision-language model (VLM)-based rubric that measures\nlayout balance, readability, and aesthetic coherence. Experimental results show\nthat PosterGen consistently matches in content fidelity, and significantly\noutperforms existing methods in visual designs, generating posters that are\npresentation-ready with minimal human refinements.",
      "upvotes": 5,
      "discussionId": "68ad136786b21a0e2e358d21",
      "ai_summary": "PosterGen, a multi-agent framework using large language models, automates paper-to-poster generation with high design quality and minimal manual refinement.",
      "ai_keywords": [
        "multi-agent systems",
        "large language models",
        "paper-to-poster generation",
        "Parser",
        "Curator",
        "Layout agent",
        "Stylist agents",
        "Renderer",
        "vision-language model",
        "VLM",
        "layout balance",
        "readability",
        "aesthetic coherence"
      ]
    },
    "publishedAt": "2025-08-23T22:25:45.000Z",
    "title": "PosterGen: Aesthetic-Aware Paper-to-Poster Generation via Multi-Agent\n  LLMs",
    "summary": "Multi-agent systems built upon large language models (LLMs) have demonstrated\nremarkable capabilities in tackling complex compositional tasks. In this work,\nwe apply this paradigm to the paper-to-poster generation problem, a practical\nyet time-consuming process faced by researchers preparing for conferences.\nWhile recent approaches have attempted to automate this task, most neglect core\ndesign and aesthetic principles, resulting in posters that require substantial\nmanual refinement. To address these design limitations, we propose PosterGen, a\nmulti-agent framework that mirrors the workflow of professional poster\ndesigners. It consists of four collaborative specialized agents: (1) Parser and\nCurator agents extract content from the paper and organize storyboard; (2)\nLayout agent maps the content into a coherent spatial layout; (3) Stylist\nagents apply visual design elements such as color and typography; and (4)\nRenderer composes the final poster. Together, these agents produce posters that\nare both semantically grounded and visually appealing. To evaluate design\nquality, we introduce a vision-language model (VLM)-based rubric that measures\nlayout balance, readability, and aesthetic coherence. Experimental results show\nthat PosterGen consistently matches in content fidelity, and significantly\noutperforms existing methods in visual designs, generating posters that are\npresentation-ready with minimal human refinements.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.17188.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "656553d89bf6665f10e3a92d",
      "avatarUrl": "/avatars/f55b09e249c48ef3fe484afa33a182ae.svg",
      "fullname": "xiang wyatt zhang",
      "name": "Wyattz23",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.17580",
      "authors": [
        {
          "_id": "68ad09b186b21a0e2e358cf5",
          "name": "Fan Nie",
          "hidden": false
        },
        {
          "_id": "68ad09b186b21a0e2e358cf6",
          "name": "Ken Ziyu Liu",
          "hidden": false
        },
        {
          "_id": "68ad09b186b21a0e2e358cf7",
          "name": "Zihao Wang",
          "hidden": false
        },
        {
          "_id": "68ad09b186b21a0e2e358cf8",
          "name": "Rui Sun",
          "hidden": false
        },
        {
          "_id": "68ad09b186b21a0e2e358cf9",
          "name": "Wei Liu",
          "hidden": false
        },
        {
          "_id": "68ad09b186b21a0e2e358cfa",
          "name": "Weijia Shi",
          "hidden": false
        },
        {
          "_id": "68ad09b186b21a0e2e358cfb",
          "name": "Huaxiu Yao",
          "hidden": false
        },
        {
          "_id": "68ad09b186b21a0e2e358cfc",
          "name": "Linjun Zhang",
          "hidden": false
        },
        {
          "_id": "68ad09b186b21a0e2e358cfd",
          "name": "Andrew Y. Ng",
          "hidden": false
        },
        {
          "_id": "68ad09b186b21a0e2e358cfe",
          "name": "James Zou",
          "hidden": false
        },
        {
          "_id": "68ad09b186b21a0e2e358cff",
          "name": "Sanmi Koyejo",
          "hidden": false
        },
        {
          "_id": "68ad09b186b21a0e2e358d00",
          "name": "Yejin Choi",
          "hidden": false
        },
        {
          "_id": "68ad09b186b21a0e2e358d01",
          "name": "Percy Liang",
          "hidden": false
        },
        {
          "_id": "68ad09b186b21a0e2e358d02",
          "name": "Niklas Muennighoff",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/5f1eb362eec0ad2a071ad6e2/EIc8-2VZpt0d-mBAk6K1z.png"
      ],
      "publishedAt": "2025-08-25T01:07:59.000Z",
      "submittedOnDailyAt": "2025-08-26T00:31:47.088Z",
      "title": "UQ: Assessing Language Models on Unsolved Questions",
      "submittedOnDailyBy": {
        "_id": "5f1eb362eec0ad2a071ad6e2",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5f1eb362eec0ad2a071ad6e2/IXMYkYKuTwn6kBdWnQeeY.png",
        "isPro": false,
        "fullname": "Niklas Muennighoff",
        "user": "Muennighoff",
        "type": "user"
      },
      "summary": "Benchmarks shape progress in AI research. A useful benchmark should be both\ndifficult and realistic: questions should challenge frontier models while also\nreflecting real-world usage. Yet, current paradigms face a difficulty-realism\ntension: exam-style benchmarks are often made artificially difficult with\nlimited real-world value, while benchmarks based on real user interaction often\nskew toward easy, high-frequency problems. In this work, we explore a radically\ndifferent paradigm: assessing models on unsolved questions. Rather than a\nstatic benchmark scored once, we curate unsolved questions and evaluate models\nasynchronously over time with validator-assisted screening and community\nverification. We introduce UQ, a testbed of 500 challenging, diverse questions\nsourced from Stack Exchange, spanning topics from CS theory and math to sci-fi\nand history, probing capabilities including reasoning, factuality, and\nbrowsing. UQ is difficult and realistic by construction: unsolved questions are\noften hard and naturally arise when humans seek answers, thus solving them\nyields direct real-world value. Our contributions are threefold: (1) UQ-Dataset\nand its collection pipeline combining rule-based filters, LLM judges, and human\nreview to ensure question quality (e.g., well-defined and difficult); (2)\nUQ-Validators, compound validation strategies that leverage the\ngenerator-validator gap to provide evaluation signals and pre-screen candidate\nsolutions for human review; and (3) UQ-Platform, an open platform where experts\ncollectively verify questions and solutions. The top model passes UQ-validation\non only 15% of questions, and preliminary human verification has already\nidentified correct answers among those that passed. UQ charts a path for\nevaluating frontier models on real-world, open-ended challenges, where success\npushes the frontier of human knowledge. We release UQ at\nhttps://uq.stanford.edu.",
      "upvotes": 3,
      "discussionId": "68ad09b286b21a0e2e358d03",
      "projectPage": "https://uq.stanford.edu",
      "ai_summary": "UQ is a benchmark for evaluating AI models on unsolved questions, combining difficulty and realism to assess capabilities like reasoning, factuality, and browsing.",
      "ai_keywords": [
        "unsolved questions",
        "UQ",
        "UQ-Dataset",
        "UQ-Validators",
        "UQ-Platform",
        "generator-validator gap",
        "human verification",
        "reasoning",
        "factuality",
        "browsing"
      ]
    },
    "publishedAt": "2025-08-24T21:07:59.000Z",
    "title": "UQ: Assessing Language Models on Unsolved Questions",
    "summary": "Benchmarks shape progress in AI research. A useful benchmark should be both\ndifficult and realistic: questions should challenge frontier models while also\nreflecting real-world usage. Yet, current paradigms face a difficulty-realism\ntension: exam-style benchmarks are often made artificially difficult with\nlimited real-world value, while benchmarks based on real user interaction often\nskew toward easy, high-frequency problems. In this work, we explore a radically\ndifferent paradigm: assessing models on unsolved questions. Rather than a\nstatic benchmark scored once, we curate unsolved questions and evaluate models\nasynchronously over time with validator-assisted screening and community\nverification. We introduce UQ, a testbed of 500 challenging, diverse questions\nsourced from Stack Exchange, spanning topics from CS theory and math to sci-fi\nand history, probing capabilities including reasoning, factuality, and\nbrowsing. UQ is difficult and realistic by construction: unsolved questions are\noften hard and naturally arise when humans seek answers, thus solving them\nyields direct real-world value. Our contributions are threefold: (1) UQ-Dataset\nand its collection pipeline combining rule-based filters, LLM judges, and human\nreview to ensure question quality (e.g., well-defined and difficult); (2)\nUQ-Validators, compound validation strategies that leverage the\ngenerator-validator gap to provide evaluation signals and pre-screen candidate\nsolutions for human review; and (3) UQ-Platform, an open platform where experts\ncollectively verify questions and solutions. The top model passes UQ-validation\non only 15% of questions, and preliminary human verification has already\nidentified correct answers among those that passed. UQ charts a path for\nevaluating frontier models on real-world, open-ended challenges, where success\npushes the frontier of human knowledge. We release UQ at\nhttps://uq.stanford.edu.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/5f1eb362eec0ad2a071ad6e2/EIc8-2VZpt0d-mBAk6K1z.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.17580.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f1eb362eec0ad2a071ad6e2",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5f1eb362eec0ad2a071ad6e2/IXMYkYKuTwn6kBdWnQeeY.png",
      "fullname": "Niklas Muennighoff",
      "name": "Muennighoff",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 146
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.16577",
      "authors": [
        {
          "_id": "68ac574486b21a0e2e358c17",
          "user": {
            "_id": "680645323889e86c69a3daf6",
            "avatarUrl": "/avatars/3a361a01ee36965b523d73ff292ce1ef.svg",
            "isPro": false,
            "fullname": "Yosef Dayani",
            "user": "yosepyossi",
            "type": "user"
          },
          "name": "Yosef Dayani",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-25T12:36:57.099Z",
          "hidden": false
        },
        {
          "_id": "68ac574486b21a0e2e358c18",
          "user": {
            "_id": "64543a1ccd09ceba0e14ecfd",
            "avatarUrl": "/avatars/d4f3aca9aa8bb4188f68ffd9e0d1f881.svg",
            "isPro": false,
            "fullname": "Omer Benishu",
            "user": "omerbenishu",
            "type": "user"
          },
          "name": "Omer Benishu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-25T19:31:52.191Z",
          "hidden": false
        },
        {
          "_id": "68ac574486b21a0e2e358c19",
          "user": {
            "_id": "6345a9b9a8c2ff9f1377faab",
            "avatarUrl": "/avatars/a5f2b999ef8b967b2af9f41afcd9d475.svg",
            "isPro": false,
            "fullname": "Sagie Benaim",
            "user": "sagiebenaim",
            "type": "user"
          },
          "name": "Sagie Benaim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-25T19:31:54.300Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-22T17:59:40.000Z",
      "submittedOnDailyAt": "2025-08-26T06:12:31.342Z",
      "title": "MV-RAG: Retrieval Augmented Multiview Diffusion",
      "submittedOnDailyBy": {
        "_id": "680645323889e86c69a3daf6",
        "avatarUrl": "/avatars/3a361a01ee36965b523d73ff292ce1ef.svg",
        "isPro": false,
        "fullname": "Yosef Dayani",
        "user": "yosepyossi",
        "type": "user"
      },
      "summary": "Text-to-3D generation approaches have advanced significantly by leveraging\npretrained 2D diffusion priors, producing high-quality and 3D-consistent\noutputs. However, they often fail to produce out-of-domain (OOD) or rare\nconcepts, yielding inconsistent or inaccurate results. To this end, we propose\nMV-RAG, a novel text-to-3D pipeline that first retrieves relevant 2D images\nfrom a large in-the-wild 2D database and then conditions a multiview diffusion\nmodel on these images to synthesize consistent and accurate multiview outputs.\nTraining such a retrieval-conditioned model is achieved via a novel hybrid\nstrategy bridging structured multiview data and diverse 2D image collections.\nThis involves training on multiview data using augmented conditioning views\nthat simulate retrieval variance for view-specific reconstruction, alongside\ntraining on sets of retrieved real-world 2D images using a distinctive held-out\nview prediction objective: the model predicts the held-out view from the other\nviews to infer 3D consistency from 2D data. To facilitate a rigorous OOD\nevaluation, we introduce a new collection of challenging OOD prompts.\nExperiments against state-of-the-art text-to-3D, image-to-3D, and\npersonalization baselines show that our approach significantly improves 3D\nconsistency, photorealism, and text adherence for OOD/rare concepts, while\nmaintaining competitive performance on standard benchmarks.",
      "upvotes": 3,
      "discussionId": "68ac574486b21a0e2e358c1a",
      "projectPage": "https://yosefdayani.github.io/MV-RAG/",
      "githubRepo": "https://github.com/yosefdayani/MV-RAG",
      "ai_summary": "MV-RAG enhances text-to-3D generation by retrieving 2D images and conditioning a multiview diffusion model to improve consistency and accuracy, especially for out-of-domain concepts.",
      "ai_keywords": [
        "pretrained 2D diffusion priors",
        "MV-RAG",
        "multiview diffusion model",
        "retrieval-conditioned model",
        "hybrid strategy",
        "augmented conditioning views",
        "held-out view prediction objective",
        "OOD prompts",
        "text-to-3D",
        "image-to-3D",
        "personalization baselines",
        "3D consistency",
        "photorealism",
        "text adherence"
      ],
      "githubStars": 7
    },
    "publishedAt": "2025-08-22T13:59:40.000Z",
    "title": "MV-RAG: Retrieval Augmented Multiview Diffusion",
    "summary": "Text-to-3D generation approaches have advanced significantly by leveraging\npretrained 2D diffusion priors, producing high-quality and 3D-consistent\noutputs. However, they often fail to produce out-of-domain (OOD) or rare\nconcepts, yielding inconsistent or inaccurate results. To this end, we propose\nMV-RAG, a novel text-to-3D pipeline that first retrieves relevant 2D images\nfrom a large in-the-wild 2D database and then conditions a multiview diffusion\nmodel on these images to synthesize consistent and accurate multiview outputs.\nTraining such a retrieval-conditioned model is achieved via a novel hybrid\nstrategy bridging structured multiview data and diverse 2D image collections.\nThis involves training on multiview data using augmented conditioning views\nthat simulate retrieval variance for view-specific reconstruction, alongside\ntraining on sets of retrieved real-world 2D images using a distinctive held-out\nview prediction objective: the model predicts the held-out view from the other\nviews to infer 3D consistency from 2D data. To facilitate a rigorous OOD\nevaluation, we introduce a new collection of challenging OOD prompts.\nExperiments against state-of-the-art text-to-3D, image-to-3D, and\npersonalization baselines show that our approach significantly improves 3D\nconsistency, photorealism, and text adherence for OOD/rare concepts, while\nmaintaining competitive performance on standard benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.16577.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "680645323889e86c69a3daf6",
      "avatarUrl": "/avatars/3a361a01ee36965b523d73ff292ce1ef.svg",
      "fullname": "Yosef Dayani",
      "name": "yosepyossi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2508.18190",
      "authors": [
        {
          "_id": "68ad223786b21a0e2e358d9c",
          "name": "Zirui Tang",
          "hidden": false
        },
        {
          "_id": "68ad223786b21a0e2e358d9d",
          "name": "Boyu Niu",
          "hidden": false
        },
        {
          "_id": "68ad223786b21a0e2e358d9e",
          "name": "Xuanhe Zhou",
          "hidden": false
        },
        {
          "_id": "68ad223786b21a0e2e358d9f",
          "name": "Boxiu Li",
          "hidden": false
        },
        {
          "_id": "68ad223786b21a0e2e358da0",
          "name": "Wei Zhou",
          "hidden": false
        },
        {
          "_id": "68ad223786b21a0e2e358da1",
          "name": "Jiannan Wang",
          "hidden": false
        },
        {
          "_id": "68ad223786b21a0e2e358da2",
          "name": "Guoliang Li",
          "hidden": false
        },
        {
          "_id": "68ad223786b21a0e2e358da3",
          "name": "Xinyi Zhang",
          "hidden": false
        },
        {
          "_id": "68ad223786b21a0e2e358da4",
          "name": "Fan Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-25T16:48:51.000Z",
      "submittedOnDailyAt": "2025-08-26T01:26:03.380Z",
      "title": "ST-Raptor: LLM-Powered Semi-Structured Table Question Answering",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Semi-structured tables, widely used in real-world applications (e.g.,\nfinancial reports, medical records, transactional orders), often involve\nflexible and complex layouts (e.g., hierarchical headers and merged cells).\nThese tables generally rely on human analysts to interpret table layouts and\nanswer relevant natural language questions, which is costly and inefficient. To\nautomate the procedure, existing methods face significant challenges. First,\nmethods like NL2SQL require converting semi-structured tables into structured\nones, which often causes substantial information loss. Second, methods like\nNL2Code and multi-modal LLM QA struggle to understand the complex layouts of\nsemi-structured tables and cannot accurately answer corresponding questions. To\nthis end, we propose ST-Raptor, a tree-based framework for semi-structured\ntable question answering using large language models. First, we introduce the\nHierarchical Orthogonal Tree (HO-Tree), a structural model that captures\ncomplex semi-structured table layouts, along with an effective algorithm for\nconstructing the tree. Second, we define a set of basic tree operations to\nguide LLMs in executing common QA tasks. Given a user question, ST-Raptor\ndecomposes it into simpler sub-questions, generates corresponding tree\noperation pipelines, and conducts operation-table alignment for accurate\npipeline execution. Third, we incorporate a two-stage verification mechanism:\nforward validation checks the correctness of execution steps, while backward\nvalidation evaluates answer reliability by reconstructing queries from\npredicted answers. To benchmark the performance, we present SSTQA, a dataset of\n764 questions over 102 real-world semi-structured tables. Experiments show that\nST-Raptor outperforms nine baselines by up to 20% in answer accuracy. The code\nis available at https://github.com/weAIDB/ST-Raptor.",
      "upvotes": 1,
      "discussionId": "68ad223786b21a0e2e358da5",
      "githubRepo": "https://github.com/weAIDB/ST-Raptor",
      "ai_summary": "ST-Raptor, a tree-based framework using large language models, addresses challenges in answering questions from semi-structured tables by introducing a Hierarchical Orthogonal Tree and a two-stage verification mechanism.",
      "ai_keywords": [
        "Hierarchical Orthogonal Tree",
        "tree-based framework",
        "large language models",
        "semi-structured tables",
        "tree operations",
        "query decomposition",
        "operation-table alignment",
        "forward validation",
        "backward validation",
        "SSTQA dataset"
      ],
      "githubStars": 9
    },
    "publishedAt": "2025-08-25T12:48:51.000Z",
    "title": "ST-Raptor: LLM-Powered Semi-Structured Table Question Answering",
    "summary": "Semi-structured tables, widely used in real-world applications (e.g.,\nfinancial reports, medical records, transactional orders), often involve\nflexible and complex layouts (e.g., hierarchical headers and merged cells).\nThese tables generally rely on human analysts to interpret table layouts and\nanswer relevant natural language questions, which is costly and inefficient. To\nautomate the procedure, existing methods face significant challenges. First,\nmethods like NL2SQL require converting semi-structured tables into structured\nones, which often causes substantial information loss. Second, methods like\nNL2Code and multi-modal LLM QA struggle to understand the complex layouts of\nsemi-structured tables and cannot accurately answer corresponding questions. To\nthis end, we propose ST-Raptor, a tree-based framework for semi-structured\ntable question answering using large language models. First, we introduce the\nHierarchical Orthogonal Tree (HO-Tree), a structural model that captures\ncomplex semi-structured table layouts, along with an effective algorithm for\nconstructing the tree. Second, we define a set of basic tree operations to\nguide LLMs in executing common QA tasks. Given a user question, ST-Raptor\ndecomposes it into simpler sub-questions, generates corresponding tree\noperation pipelines, and conducts operation-table alignment for accurate\npipeline execution. Third, we incorporate a two-stage verification mechanism:\nforward validation checks the correctness of execution steps, while backward\nvalidation evaluates answer reliability by reconstructing queries from\npredicted answers. To benchmark the performance, we present SSTQA, a dataset of\n764 questions over 102 real-world semi-structured tables. Experiments show that\nST-Raptor outperforms nine baselines by up to 20% in answer accuracy. The code\nis available at https://github.com/weAIDB/ST-Raptor.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.18190.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 95
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.17298",
      "authors": [
        {
          "_id": "68ad3dec86b21a0e2e358e46",
          "name": "Fucai Ke",
          "hidden": false
        },
        {
          "_id": "68ad3dec86b21a0e2e358e47",
          "name": "Joy Hsu",
          "hidden": false
        },
        {
          "_id": "68ad3dec86b21a0e2e358e48",
          "name": "Zhixi Cai",
          "hidden": false
        },
        {
          "_id": "68ad3dec86b21a0e2e358e49",
          "name": "Zixian Ma",
          "hidden": false
        },
        {
          "_id": "68ad3dec86b21a0e2e358e4a",
          "name": "Xin Zheng",
          "hidden": false
        },
        {
          "_id": "68ad3dec86b21a0e2e358e4b",
          "name": "Xindi Wu",
          "hidden": false
        },
        {
          "_id": "68ad3dec86b21a0e2e358e4c",
          "name": "Sukai Huang",
          "hidden": false
        },
        {
          "_id": "68ad3dec86b21a0e2e358e4d",
          "name": "Weiqing Wang",
          "hidden": false
        },
        {
          "_id": "68ad3dec86b21a0e2e358e4e",
          "name": "Pari Delir Haghighi",
          "hidden": false
        },
        {
          "_id": "68ad3dec86b21a0e2e358e4f",
          "name": "Gholamreza Haffari",
          "hidden": false
        },
        {
          "_id": "68ad3dec86b21a0e2e358e50",
          "name": "Ranjay Krishna",
          "hidden": false
        },
        {
          "_id": "68ad3dec86b21a0e2e358e51",
          "name": "Jiajun Wu",
          "hidden": false
        },
        {
          "_id": "68ad3dec86b21a0e2e358e52",
          "name": "Hamid Rezatofighi",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/633de44a797cf1b030553d64/0gblT2G3J8WFpvJnQtCBD.png",
        "https://cdn-uploads.huggingface.co/production/uploads/633de44a797cf1b030553d64/qHYMdCq45N3_7rAnjCn-j.png",
        "https://cdn-uploads.huggingface.co/production/uploads/633de44a797cf1b030553d64/Tl0vUu-164DUOpUyjbGxq.png",
        "https://cdn-uploads.huggingface.co/production/uploads/633de44a797cf1b030553d64/TNURaGnadgMz-bt8bPdkM.png"
      ],
      "publishedAt": "2025-08-24T11:01:51.000Z",
      "submittedOnDailyAt": "2025-08-26T03:37:29.073Z",
      "title": "Explain Before You Answer: A Survey on Compositional Visual Reasoning",
      "submittedOnDailyBy": {
        "_id": "633de44a797cf1b030553d64",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633de44a797cf1b030553d64/uvrAP6uYrKnpRxdcK8RKR.png",
        "isPro": false,
        "fullname": "Zhixi Cai",
        "user": "ControlNet",
        "type": "user"
      },
      "summary": "Compositional visual reasoning has emerged as a key research frontier in\nmultimodal AI, aiming to endow machines with the human-like ability to\ndecompose visual scenes, ground intermediate concepts, and perform multi-step\nlogical inference. While early surveys focus on monolithic vision-language\nmodels or general multimodal reasoning, a dedicated synthesis of the rapidly\nexpanding compositional visual reasoning literature is still missing. We fill\nthis gap with a comprehensive survey spanning 2023 to 2025 that systematically\nreviews 260+ papers from top venues (CVPR, ICCV, NeurIPS, ICML, ACL, etc.). We\nfirst formalize core definitions and describe why compositional approaches\noffer advantages in cognitive alignment, semantic fidelity, robustness,\ninterpretability, and data efficiency. Next, we trace a five-stage paradigm\nshift: from prompt-enhanced language-centric pipelines, through tool-enhanced\nLLMs and tool-enhanced VLMs, to recently minted chain-of-thought reasoning and\nunified agentic VLMs, highlighting their architectural designs, strengths, and\nlimitations. We then catalog 60+ benchmarks and corresponding metrics that\nprobe compositional visual reasoning along dimensions such as grounding\naccuracy, chain-of-thought faithfulness, and high-resolution perception.\nDrawing on these analyses, we distill key insights, identify open challenges\n(e.g., limitations of LLM-based reasoning, hallucination, a bias toward\ndeductive reasoning, scalable supervision, tool integration, and benchmark\nlimitations), and outline future directions, including world-model integration,\nhuman-AI collaborative reasoning, and richer evaluation protocols. By offering\na unified taxonomy, historical roadmap, and critical outlook, this survey aims\nto serve as a foundational reference and inspire the next generation of\ncompositional visual reasoning research.",
      "upvotes": 1,
      "discussionId": "68ad3dec86b21a0e2e358e55",
      "projectPage": "https://github.com/pokerme7777/Compositional-Visual-Reasoning-Survey",
      "githubRepo": "https://github.com/pokerme7777/Compositional-Visual-Reasoning-Survey",
      "ai_summary": "A comprehensive survey of compositional visual reasoning from 2023 to 2025 reviews advancements in multimodal AI, highlighting architectural designs, benchmarks, and future directions.",
      "ai_keywords": [
        "compositional visual reasoning",
        "multimodal AI",
        "cognitive alignment",
        "semantic fidelity",
        "robustness",
        "interpretability",
        "data efficiency",
        "prompt-enhanced pipelines",
        "tool-enhanced LLMs",
        "tool-enhanced VLMs",
        "chain-of-thought reasoning",
        "unified agentic VLMs",
        "grounding accuracy",
        "chain-of-thought faithfulness",
        "high-resolution perception",
        "LLM-based reasoning",
        "hallucination",
        "deductive reasoning",
        "scalable supervision",
        "tool integration",
        "benchmark limitations",
        "world-model integration",
        "human-AI collaborative reasoning",
        "richer evaluation protocols"
      ],
      "githubStars": 2
    },
    "publishedAt": "2025-08-24T07:01:51.000Z",
    "title": "Explain Before You Answer: A Survey on Compositional Visual Reasoning",
    "summary": "Compositional visual reasoning has emerged as a key research frontier in\nmultimodal AI, aiming to endow machines with the human-like ability to\ndecompose visual scenes, ground intermediate concepts, and perform multi-step\nlogical inference. While early surveys focus on monolithic vision-language\nmodels or general multimodal reasoning, a dedicated synthesis of the rapidly\nexpanding compositional visual reasoning literature is still missing. We fill\nthis gap with a comprehensive survey spanning 2023 to 2025 that systematically\nreviews 260+ papers from top venues (CVPR, ICCV, NeurIPS, ICML, ACL, etc.). We\nfirst formalize core definitions and describe why compositional approaches\noffer advantages in cognitive alignment, semantic fidelity, robustness,\ninterpretability, and data efficiency. Next, we trace a five-stage paradigm\nshift: from prompt-enhanced language-centric pipelines, through tool-enhanced\nLLMs and tool-enhanced VLMs, to recently minted chain-of-thought reasoning and\nunified agentic VLMs, highlighting their architectural designs, strengths, and\nlimitations. We then catalog 60+ benchmarks and corresponding metrics that\nprobe compositional visual reasoning along dimensions such as grounding\naccuracy, chain-of-thought faithfulness, and high-resolution perception.\nDrawing on these analyses, we distill key insights, identify open challenges\n(e.g., limitations of LLM-based reasoning, hallucination, a bias toward\ndeductive reasoning, scalable supervision, tool integration, and benchmark\nlimitations), and outline future directions, including world-model integration,\nhuman-AI collaborative reasoning, and richer evaluation protocols. By offering\na unified taxonomy, historical roadmap, and critical outlook, this survey aims\nto serve as a foundational reference and inspire the next generation of\ncompositional visual reasoning research.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/633de44a797cf1b030553d64/0gblT2G3J8WFpvJnQtCBD.png",
      "https://cdn-uploads.huggingface.co/production/uploads/633de44a797cf1b030553d64/qHYMdCq45N3_7rAnjCn-j.png",
      "https://cdn-uploads.huggingface.co/production/uploads/633de44a797cf1b030553d64/Tl0vUu-164DUOpUyjbGxq.png",
      "https://cdn-uploads.huggingface.co/production/uploads/633de44a797cf1b030553d64/TNURaGnadgMz-bt8bPdkM.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.17298.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "633de44a797cf1b030553d64",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633de44a797cf1b030553d64/uvrAP6uYrKnpRxdcK8RKR.png",
      "fullname": "Zhixi Cai",
      "name": "ControlNet",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.17290",
      "authors": [
        {
          "_id": "68ad5e2586b21a0e2e358ecd",
          "name": "Omid Ghahroodi",
          "hidden": false
        },
        {
          "_id": "68ad5e2586b21a0e2e358ece",
          "name": "Arshia Hemmat",
          "hidden": false
        },
        {
          "_id": "68ad5e2586b21a0e2e358ecf",
          "name": "Marzia Nouri",
          "hidden": false
        },
        {
          "_id": "68ad5e2586b21a0e2e358ed0",
          "name": "Seyed Mohammad Hadi Hosseini",
          "hidden": false
        },
        {
          "_id": "68ad5e2586b21a0e2e358ed1",
          "name": "Doratossadat Dastgheib",
          "hidden": false
        },
        {
          "_id": "68ad5e2586b21a0e2e358ed2",
          "name": "Mohammad Vali Sanian",
          "hidden": false
        },
        {
          "_id": "68ad5e2586b21a0e2e358ed3",
          "name": "Alireza Sahebi",
          "hidden": false
        },
        {
          "_id": "68ad5e2586b21a0e2e358ed4",
          "name": "Reihaneh Zohrabi",
          "hidden": false
        },
        {
          "_id": "68ad5e2586b21a0e2e358ed5",
          "name": "Mohammad Hossein Rohban",
          "hidden": false
        },
        {
          "_id": "68ad5e2586b21a0e2e358ed6",
          "name": "Ehsaneddin Asgari",
          "hidden": false
        },
        {
          "_id": "68ad5e2586b21a0e2e358ed7",
          "name": "Mahdieh Soleymani Baghshah",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6250819a5bf543dbd2607081/lHOYrDxy_TTmQUIShbLZL.png"
      ],
      "publishedAt": "2025-08-24T10:32:37.000Z",
      "submittedOnDailyAt": "2025-08-26T05:43:12.601Z",
      "title": "MEENA (PersianMMMU): Multimodal-Multilingual Educational Exams for\n  N-level Assessment",
      "submittedOnDailyBy": {
        "_id": "6250819a5bf543dbd2607081",
        "avatarUrl": "/avatars/dfade22a578b8349ab544acda5e8bcad.svg",
        "isPro": false,
        "fullname": "Omid Ghahroodi",
        "user": "omidgh",
        "type": "user"
      },
      "summary": "Recent advancements in large vision-language models (VLMs) have primarily\nfocused on English, with limited attention given to other languages. To address\nthis gap, we introduce MEENA (also known as PersianMMMU), the first dataset\ndesigned to evaluate Persian VLMs across scientific, reasoning, and human-level\nunderstanding tasks. Our dataset comprises approximately 7,500 Persian and\n3,000 English questions, covering a wide range of topics such as reasoning,\nmathematics, physics, diagrams, charts, and Persian art and literature. Key\nfeatures of MEENA include: (1) diverse subject coverage spanning various\neducational levels, from primary to upper secondary school, (2) rich metadata,\nincluding difficulty levels and descriptive answers, (3) original Persian data\nthat preserves cultural nuances, (4) a bilingual structure to assess\ncross-linguistic performance, and (5) a series of diverse experiments assessing\nvarious capabilities, including overall performance, the model's ability to\nattend to images, and its tendency to generate hallucinations. We hope this\nbenchmark contributes to enhancing VLM capabilities beyond English.",
      "upvotes": 1,
      "discussionId": "68ad5e2586b21a0e2e358ed8",
      "ai_summary": "MEENA, a Persian-English dataset, evaluates vision-language models across scientific, reasoning, and human-level understanding tasks, enhancing capabilities beyond English.",
      "ai_keywords": [
        "vision-language models",
        "VLMs",
        "Persian",
        "MEENA",
        "PersianMMMU",
        "bilingual structure",
        "cross-linguistic performance",
        "hallucinations"
      ]
    },
    "publishedAt": "2025-08-24T06:32:37.000Z",
    "title": "MEENA (PersianMMMU): Multimodal-Multilingual Educational Exams for\n  N-level Assessment",
    "summary": "Recent advancements in large vision-language models (VLMs) have primarily\nfocused on English, with limited attention given to other languages. To address\nthis gap, we introduce MEENA (also known as PersianMMMU), the first dataset\ndesigned to evaluate Persian VLMs across scientific, reasoning, and human-level\nunderstanding tasks. Our dataset comprises approximately 7,500 Persian and\n3,000 English questions, covering a wide range of topics such as reasoning,\nmathematics, physics, diagrams, charts, and Persian art and literature. Key\nfeatures of MEENA include: (1) diverse subject coverage spanning various\neducational levels, from primary to upper secondary school, (2) rich metadata,\nincluding difficulty levels and descriptive answers, (3) original Persian data\nthat preserves cultural nuances, (4) a bilingual structure to assess\ncross-linguistic performance, and (5) a series of diverse experiments assessing\nvarious capabilities, including overall performance, the model's ability to\nattend to images, and its tendency to generate hallucinations. We hope this\nbenchmark contributes to enhancing VLM capabilities beyond English.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6250819a5bf543dbd2607081/lHOYrDxy_TTmQUIShbLZL.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.17290.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6250819a5bf543dbd2607081",
      "avatarUrl": "/avatars/dfade22a578b8349ab544acda5e8bcad.svg",
      "fullname": "Omid Ghahroodi",
      "name": "omidgh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.16790",
      "authors": [
        {
          "_id": "68ad0ef486b21a0e2e358d05",
          "name": "Yuancheng Wang",
          "hidden": false
        },
        {
          "_id": "68ad0ef486b21a0e2e358d06",
          "name": "Dekun Chen",
          "hidden": false
        },
        {
          "_id": "68ad0ef486b21a0e2e358d07",
          "name": "Xueyao Zhang",
          "hidden": false
        },
        {
          "_id": "68ad0ef486b21a0e2e358d08",
          "name": "Junan Zhang",
          "hidden": false
        },
        {
          "_id": "68ad0ef486b21a0e2e358d09",
          "name": "Jiaqi Li",
          "hidden": false
        },
        {
          "_id": "68ad0ef486b21a0e2e358d0a",
          "name": "Zhizheng Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-22T20:45:03.000Z",
      "submittedOnDailyAt": "2025-08-26T01:54:58.440Z",
      "title": "TaDiCodec: Text-aware Diffusion Speech Tokenizer for Speech Language\n  Modeling",
      "submittedOnDailyBy": {
        "_id": "63072d60cd148dbc5e49f4dd",
        "avatarUrl": "/avatars/ffa61038c0ff20848fbcde7c1c34570e.svg",
        "isPro": false,
        "fullname": "Yuancheng Wang",
        "user": "Hecheng0625",
        "type": "user"
      },
      "summary": "Speech tokenizers serve as foundational components for speech language\nmodels, yet current designs exhibit several limitations, including: 1)\ndependence on multi-layer residual vector quantization structures or high frame\nrates, 2) reliance on auxiliary pre-trained models for semantic distillation,\nand 3) requirements for complex two-stage training processes. In this work, we\nintroduce the Text-aware Diffusion Transformer Speech Codec (TaDiCodec), a\nnovel approach designed to overcome these challenges. TaDiCodec employs\nend-to-end optimization for quantization and reconstruction through a diffusion\nautoencoder, while integrating text guidance into the diffusion decoder to\nenhance reconstruction quality and achieve optimal compression. TaDiCodec\nachieves an extremely low frame rate of 6.25 Hz and a corresponding bitrate of\n0.0875 kbps with a single-layer codebook for 24 kHz speech, while maintaining\nsuperior performance on critical speech generation evaluation metrics such as\nWord Error Rate (WER), speaker similarity (SIM), and speech quality (UTMOS).\nNotably, TaDiCodec employs a single-stage, end-to-end training paradigm, and\nobviating the need for auxiliary pre-trained models. We also validate the\ncompatibility of TaDiCodec in language model based zero-shot text-to-speech\nwith both autoregressive modeling and masked generative modeling, demonstrating\nits effectiveness and efficiency for speech language modeling, as well as a\nsignificantly small reconstruction-generation gap. We will open source our code\nand model checkpoints. Audio samples are are available at\nhttps:/tadicodec.github.io/. We release code and model checkpoints at\nhttps:/github.com/HeCheng0625/Diffusion-Speech-Tokenizer.",
      "upvotes": 1,
      "discussionId": "68ad0ef486b21a0e2e358d0b",
      "projectPage": "https://github.com/HeCheng0625/Diffusion-Speech-Tokenizer",
      "githubRepo": "https://github.com/HeCheng0625/Diffusion-Speech-Tokenizer",
      "ai_summary": "TaDiCodec, a Text-aware Diffusion Transformer Speech Codec, achieves low frame rates and bitrates with superior speech generation performance using end-to-end optimization and text guidance.",
      "ai_keywords": [
        "diffusion autoencoder",
        "text guidance",
        "end-to-end optimization",
        "Word Error Rate (WER)",
        "speaker similarity (SIM)",
        "speech quality (UTMOS)",
        "zero-shot text-to-speech",
        "autoregressive modeling",
        "masked generative modeling"
      ],
      "githubStars": 69
    },
    "publishedAt": "2025-08-22T16:45:03.000Z",
    "title": "TaDiCodec: Text-aware Diffusion Speech Tokenizer for Speech Language\n  Modeling",
    "summary": "Speech tokenizers serve as foundational components for speech language\nmodels, yet current designs exhibit several limitations, including: 1)\ndependence on multi-layer residual vector quantization structures or high frame\nrates, 2) reliance on auxiliary pre-trained models for semantic distillation,\nand 3) requirements for complex two-stage training processes. In this work, we\nintroduce the Text-aware Diffusion Transformer Speech Codec (TaDiCodec), a\nnovel approach designed to overcome these challenges. TaDiCodec employs\nend-to-end optimization for quantization and reconstruction through a diffusion\nautoencoder, while integrating text guidance into the diffusion decoder to\nenhance reconstruction quality and achieve optimal compression. TaDiCodec\nachieves an extremely low frame rate of 6.25 Hz and a corresponding bitrate of\n0.0875 kbps with a single-layer codebook for 24 kHz speech, while maintaining\nsuperior performance on critical speech generation evaluation metrics such as\nWord Error Rate (WER), speaker similarity (SIM), and speech quality (UTMOS).\nNotably, TaDiCodec employs a single-stage, end-to-end training paradigm, and\nobviating the need for auxiliary pre-trained models. We also validate the\ncompatibility of TaDiCodec in language model based zero-shot text-to-speech\nwith both autoregressive modeling and masked generative modeling, demonstrating\nits effectiveness and efficiency for speech language modeling, as well as a\nsignificantly small reconstruction-generation gap. We will open source our code\nand model checkpoints. Audio samples are are available at\nhttps:/tadicodec.github.io/. We release code and model checkpoints at\nhttps:/github.com/HeCheng0625/Diffusion-Speech-Tokenizer.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.16790.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63072d60cd148dbc5e49f4dd",
      "avatarUrl": "/avatars/ffa61038c0ff20848fbcde7c1c34570e.svg",
      "fullname": "Yuancheng Wang",
      "name": "Hecheng0625",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.18159",
      "authors": [
        {
          "_id": "68ad22c586b21a0e2e358da7",
          "name": "Sara Ghazanfari",
          "hidden": false
        },
        {
          "_id": "68ad22c586b21a0e2e358da8",
          "name": "Wei-An Lin",
          "hidden": false
        },
        {
          "_id": "68ad22c586b21a0e2e358da9",
          "name": "Haitong Tian",
          "hidden": false
        },
        {
          "_id": "68ad22c586b21a0e2e358daa",
          "name": "Ersin Yumer",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-25T16:08:57.000Z",
      "submittedOnDailyAt": "2025-08-26T01:28:26.623Z",
      "title": "SpotEdit: Evaluating Visually-Guided Image Editing Methods",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Visually-guided image editing, where edits are conditioned on both visual\ncues and textual prompts, has emerged as a powerful paradigm for fine-grained,\ncontrollable content generation. Although recent generative models have shown\nremarkable capabilities, existing evaluations remain simple and insufficiently\nrepresentative of real-world editing challenges. We present SpotEdit, a\ncomprehensive benchmark designed to systematically assess visually-guided image\nediting methods across diverse diffusion, autoregressive, and hybrid generative\nmodels, uncovering substantial performance disparities. To address a critical\nyet underexplored challenge, our benchmark includes a dedicated component on\nhallucination, highlighting how leading models, such as GPT-4o, often\nhallucinate the existence of a visual cue and erroneously perform the editing\ntask. Our code and benchmark are publicly released at\nhttps://github.com/SaraGhazanfari/SpotEdit.",
      "upvotes": 0,
      "discussionId": "68ad22c586b21a0e2e358dab",
      "githubRepo": "https://github.com/SaraGhazanfari/SpotEdit",
      "ai_summary": "SpotEdit is a benchmark for evaluating visually-guided image editing methods, revealing performance disparities and hallucination issues across diffusion, autoregressive, and hybrid generative models.",
      "ai_keywords": [
        "visually-guided image editing",
        "diffusion models",
        "autoregressive models",
        "hybrid generative models",
        "hallucination"
      ],
      "githubStars": 2
    },
    "publishedAt": "2025-08-25T12:08:57.000Z",
    "title": "SpotEdit: Evaluating Visually-Guided Image Editing Methods",
    "summary": "Visually-guided image editing, where edits are conditioned on both visual\ncues and textual prompts, has emerged as a powerful paradigm for fine-grained,\ncontrollable content generation. Although recent generative models have shown\nremarkable capabilities, existing evaluations remain simple and insufficiently\nrepresentative of real-world editing challenges. We present SpotEdit, a\ncomprehensive benchmark designed to systematically assess visually-guided image\nediting methods across diverse diffusion, autoregressive, and hybrid generative\nmodels, uncovering substantial performance disparities. To address a critical\nyet underexplored challenge, our benchmark includes a dedicated component on\nhallucination, highlighting how leading models, such as GPT-4o, often\nhallucinate the existence of a visual cue and erroneously perform the editing\ntask. Our code and benchmark are publicly released at\nhttps://github.com/SaraGhazanfari/SpotEdit.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.18159.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 95
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.18076",
      "authors": [
        {
          "_id": "68ad232986b21a0e2e358dad",
          "name": "Khaoula Chehbouni",
          "hidden": false
        },
        {
          "_id": "68ad232986b21a0e2e358dae",
          "name": "Mohammed Haddou",
          "hidden": false
        },
        {
          "_id": "68ad232986b21a0e2e358daf",
          "name": "Jackie Chi Kit Cheung",
          "hidden": false
        },
        {
          "_id": "68ad232986b21a0e2e358db0",
          "name": "Golnoosh Farnadi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-25T14:43:10.000Z",
      "submittedOnDailyAt": "2025-08-26T01:30:03.792Z",
      "title": "Neither Valid nor Reliable? Investigating the Use of LLMs as Judges",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Evaluating natural language generation (NLG) systems remains a core challenge\nof natural language processing (NLP), further complicated by the rise of large\nlanguage models (LLMs) that aims to be general-purpose. Recently, large\nlanguage models as judges (LLJs) have emerged as a promising alternative to\ntraditional metrics, but their validity remains underexplored. This position\npaper argues that the current enthusiasm around LLJs may be premature, as their\nadoption has outpaced rigorous scrutiny of their reliability and validity as\nevaluators. Drawing on measurement theory from the social sciences, we identify\nand critically assess four core assumptions underlying the use of LLJs: their\nability to act as proxies for human judgment, their capabilities as evaluators,\ntheir scalability, and their cost-effectiveness. We examine how each of these\nassumptions may be challenged by the inherent limitations of LLMs, LLJs, or\ncurrent practices in NLG evaluation. To ground our analysis, we explore three\napplications of LLJs: text summarization, data annotation, and safety\nalignment. Finally, we highlight the need for more responsible evaluation\npractices in LLJs evaluation, to ensure that their growing role in the field\nsupports, rather than undermines, progress in NLG.",
      "upvotes": 0,
      "discussionId": "68ad232986b21a0e2e358db1",
      "ai_summary": "The paper critiques the use of large language models as judges for evaluating natural language generation systems, questioning their reliability, capabilities, scalability, and cost-effectiveness.",
      "ai_keywords": [
        "large language models",
        "LLJs",
        "natural language generation",
        "NLG",
        "measurement theory",
        "text summarization",
        "data annotation",
        "safety alignment"
      ]
    },
    "publishedAt": "2025-08-25T10:43:10.000Z",
    "title": "Neither Valid nor Reliable? Investigating the Use of LLMs as Judges",
    "summary": "Evaluating natural language generation (NLG) systems remains a core challenge\nof natural language processing (NLP), further complicated by the rise of large\nlanguage models (LLMs) that aims to be general-purpose. Recently, large\nlanguage models as judges (LLJs) have emerged as a promising alternative to\ntraditional metrics, but their validity remains underexplored. This position\npaper argues that the current enthusiasm around LLJs may be premature, as their\nadoption has outpaced rigorous scrutiny of their reliability and validity as\nevaluators. Drawing on measurement theory from the social sciences, we identify\nand critically assess four core assumptions underlying the use of LLJs: their\nability to act as proxies for human judgment, their capabilities as evaluators,\ntheir scalability, and their cost-effectiveness. We examine how each of these\nassumptions may be challenged by the inherent limitations of LLMs, LLJs, or\ncurrent practices in NLG evaluation. To ground our analysis, we explore three\napplications of LLJs: text summarization, data annotation, and safety\nalignment. Finally, we highlight the need for more responsible evaluation\npractices in LLJs evaluation, to ensure that their growing role in the field\nsupports, rather than undermines, progress in NLG.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.18076.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 95
    },
    "isAuthorParticipating": false
  }
]