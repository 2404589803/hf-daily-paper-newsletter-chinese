[
  {
    "paper": {
      "id": "2503.22675",
      "authors": [
        {
          "_id": "67e9f6b7d13d75fc155c7f2e",
          "name": "Jiakai Tang",
          "hidden": false
        },
        {
          "_id": "67e9f6b7d13d75fc155c7f2f",
          "name": "Sunhao Dai",
          "hidden": false
        },
        {
          "_id": "67e9f6b7d13d75fc155c7f30",
          "name": "Teng Shi",
          "hidden": false
        },
        {
          "_id": "67e9f6b7d13d75fc155c7f31",
          "name": "Jun Xu",
          "hidden": false
        },
        {
          "_id": "67e9f6b7d13d75fc155c7f32",
          "name": "Xu Chen",
          "hidden": false
        },
        {
          "_id": "67e9f6b7d13d75fc155c7f33",
          "name": "Wen Chen",
          "hidden": false
        },
        {
          "_id": "67e9f6b7d13d75fc155c7f34",
          "name": "Wu Jian",
          "hidden": false
        },
        {
          "_id": "67e9f6b7d13d75fc155c7f35",
          "name": "Yuning Jiang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-28T17:59:03.000Z",
      "submittedOnDailyAt": "2025-03-31T00:29:30.669Z",
      "title": "Think Before Recommend: Unleashing the Latent Reasoning Power for\n  Sequential Recommendation",
      "submittedOnDailyBy": {
        "_id": "64db88993725f8d9a908c077",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64db88993725f8d9a908c077/JZEGk0kius6mrANlwOWw9.jpeg",
        "isPro": false,
        "fullname": "Sunhao Dai",
        "user": "KID-22",
        "type": "user"
      },
      "summary": "Sequential Recommendation (SeqRec) aims to predict the next item by capturing\nsequential patterns from users' historical interactions, playing a crucial role\nin many real-world recommender systems. However, existing approaches\npredominantly adopt a direct forward computation paradigm, where the final\nhidden state of the sequence encoder serves as the user representation. We\nargue that this inference paradigm, due to its limited computational depth,\nstruggles to model the complex evolving nature of user preferences and lacks a\nnuanced understanding of long-tail items, leading to suboptimal performance. To\naddress this issue, we propose ReaRec, the first inference-time\ncomputing framework for recommender systems, which enhances user\nrepresentations through implicit multi-step reasoning. Specifically, ReaRec\nautoregressively feeds the sequence's last hidden state into the sequential\nrecommender while incorporating special reasoning position embeddings to\ndecouple the original item encoding space from the multi-step reasoning space.\nMoreover, we introduce two lightweight reasoning-based learning methods,\nEnsemble Reasoning Learning (ERL) and Progressive Reasoning Learning (PRL), to\nfurther effectively exploit ReaRec's reasoning potential. Extensive experiments\non five public real-world datasets and different SeqRec architectures\ndemonstrate the generality and effectiveness of our proposed ReaRec.\nRemarkably, post-hoc analyses reveal that ReaRec significantly elevates the\nperformance ceiling of multiple sequential recommendation backbones by\napproximately 30\\%-50\\%. Thus, we believe this work can open a new and\npromising avenue for future research in inference-time computing for sequential\nrecommendation.",
      "upvotes": 19,
      "discussionId": "67e9f6bdd13d75fc155c805e",
      "githubRepo": "https://github.com/TangJiakai/ReaRec",
      "ai_keywords": [
        "ReaRec",
        "reasoning position embeddings",
        "Ensemble Reasoning Learning (ERL)",
        "Progressive Reasoning Learning (PRL)",
        "sequential recommendation backbones",
        "autoregressive feeding"
      ]
    },
    "publishedAt": "2025-03-28T13:59:03.000Z",
    "title": "Think Before Recommend: Unleashing the Latent Reasoning Power for\n  Sequential Recommendation",
    "summary": "Sequential Recommendation (SeqRec) aims to predict the next item by capturing\nsequential patterns from users' historical interactions, playing a crucial role\nin many real-world recommender systems. However, existing approaches\npredominantly adopt a direct forward computation paradigm, where the final\nhidden state of the sequence encoder serves as the user representation. We\nargue that this inference paradigm, due to its limited computational depth,\nstruggles to model the complex evolving nature of user preferences and lacks a\nnuanced understanding of long-tail items, leading to suboptimal performance. To\naddress this issue, we propose ReaRec, the first inference-time\ncomputing framework for recommender systems, which enhances user\nrepresentations through implicit multi-step reasoning. Specifically, ReaRec\nautoregressively feeds the sequence's last hidden state into the sequential\nrecommender while incorporating special reasoning position embeddings to\ndecouple the original item encoding space from the multi-step reasoning space.\nMoreover, we introduce two lightweight reasoning-based learning methods,\nEnsemble Reasoning Learning (ERL) and Progressive Reasoning Learning (PRL), to\nfurther effectively exploit ReaRec's reasoning potential. Extensive experiments\non five public real-world datasets and different SeqRec architectures\ndemonstrate the generality and effectiveness of our proposed ReaRec.\nRemarkably, post-hoc analyses reveal that ReaRec significantly elevates the\nperformance ceiling of multiple sequential recommendation backbones by\napproximately 30\\%-50\\%. Thus, we believe this work can open a new and\npromising avenue for future research in inference-time computing for sequential\nrecommendation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.22675.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64db88993725f8d9a908c077",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64db88993725f8d9a908c077/JZEGk0kius6mrANlwOWw9.jpeg",
      "fullname": "Sunhao Dai",
      "name": "KID-22",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.22230",
      "authors": [
        {
          "_id": "67e9fdd446d9dd867e9728d3",
          "name": "Wei Shen",
          "hidden": false
        },
        {
          "_id": "67e9fdd446d9dd867e9728d4",
          "user": {
            "_id": "67805c4a43a58ab7b52a05ea",
            "avatarUrl": "/avatars/759d0466020b6f7c0207aaf62ad89eca.svg",
            "isPro": false,
            "fullname": "Guanlin Liu",
            "user": "glnbyte",
            "type": "user"
          },
          "name": "Guanlin Liu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-31T02:28:37.898Z",
          "hidden": false
        },
        {
          "_id": "67e9fdd446d9dd867e9728d5",
          "name": "Zheng Wu",
          "hidden": false
        },
        {
          "_id": "67e9fdd446d9dd867e9728d6",
          "name": "Ruofei Zhu",
          "hidden": false
        },
        {
          "_id": "67e9fdd446d9dd867e9728d7",
          "name": "Qingping Yang",
          "hidden": false
        },
        {
          "_id": "67e9fdd446d9dd867e9728d8",
          "name": "Chao Xin",
          "hidden": false
        },
        {
          "_id": "67e9fdd446d9dd867e9728d9",
          "name": "Yu Yue",
          "hidden": false
        },
        {
          "_id": "67e9fdd446d9dd867e9728da",
          "name": "Lin Yan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-28T08:26:41.000Z",
      "submittedOnDailyAt": "2025-03-31T00:59:21.502Z",
      "title": "Exploring Data Scaling Trends and Effects in Reinforcement Learning from\n  Human Feedback",
      "submittedOnDailyBy": {
        "_id": "6468823272d9180d4ac90bdf",
        "avatarUrl": "/avatars/70cb7d65d30ecb944595000ceeeedb1b.svg",
        "isPro": false,
        "fullname": "Wei Shen",
        "user": "Swtheking",
        "type": "user"
      },
      "summary": "Reinforcement Learning from Human Feedback (RLHF) is crucial for aligning\nlarge language models with human preferences. While recent research has focused\non algorithmic improvements, the importance of prompt-data construction has\nbeen overlooked. This paper addresses this gap by exploring data-driven\nbottlenecks in RLHF performance scaling, particularly reward hacking and\ndecreasing response diversity. We introduce a hybrid reward system combining\nreasoning task verifiers (RTV) and a generative reward model (GenRM) to\nmitigate reward hacking. We also propose a novel prompt-selection method,\nPre-PPO, to maintain response diversity and enhance learning effectiveness.\nAdditionally, we find that prioritizing mathematical and coding tasks early in\nRLHF training significantly improves performance. Experiments across two model\nsizes validate our methods' effectiveness and scalability. Results show that\nRTV is most resistant to reward hacking, followed by GenRM with ground truth,\nand then GenRM with SFT Best-of-N responses. Our strategies enable rapid\ncapture of subtle task-specific distinctions, leading to substantial\nimprovements in overall RLHF performance. This work highlights the importance\nof careful data construction and provides practical methods to overcome\nperformance barriers in RLHF.",
      "upvotes": 13,
      "discussionId": "67e9fdd546d9dd867e97292c",
      "ai_keywords": [
        "Reinforcement Learning from Human Feedback (RLHF)",
        "reward hacking",
        "response diversity",
        "reasoning task verifiers (RTV)",
        "generative reward model (GenRM)",
        "Pre-PPO",
        "prompt-selection method",
        "mathematical tasks",
        "coding tasks",
        "GenRM with ground truth",
        "GenRM with SFT Best-of-N responses"
      ]
    },
    "publishedAt": "2025-03-28T04:26:41.000Z",
    "title": "Exploring Data Scaling Trends and Effects in Reinforcement Learning from\n  Human Feedback",
    "summary": "Reinforcement Learning from Human Feedback (RLHF) is crucial for aligning\nlarge language models with human preferences. While recent research has focused\non algorithmic improvements, the importance of prompt-data construction has\nbeen overlooked. This paper addresses this gap by exploring data-driven\nbottlenecks in RLHF performance scaling, particularly reward hacking and\ndecreasing response diversity. We introduce a hybrid reward system combining\nreasoning task verifiers (RTV) and a generative reward model (GenRM) to\nmitigate reward hacking. We also propose a novel prompt-selection method,\nPre-PPO, to maintain response diversity and enhance learning effectiveness.\nAdditionally, we find that prioritizing mathematical and coding tasks early in\nRLHF training significantly improves performance. Experiments across two model\nsizes validate our methods' effectiveness and scalability. Results show that\nRTV is most resistant to reward hacking, followed by GenRM with ground truth,\nand then GenRM with SFT Best-of-N responses. Our strategies enable rapid\ncapture of subtle task-specific distinctions, leading to substantial\nimprovements in overall RLHF performance. This work highlights the importance\nof careful data construction and provides practical methods to overcome\nperformance barriers in RLHF.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.22230.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6468823272d9180d4ac90bdf",
      "avatarUrl": "/avatars/70cb7d65d30ecb944595000ceeeedb1b.svg",
      "fullname": "Wei Shen",
      "name": "Swtheking",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.19693",
      "authors": [
        {
          "_id": "67ea363dd13d75fc156ec498",
          "name": "Itay Nakash",
          "hidden": false
        },
        {
          "_id": "67ea363dd13d75fc156ec499",
          "name": "Nitay Calderon",
          "hidden": false
        },
        {
          "_id": "67ea363dd13d75fc156ec49a",
          "name": "Eyal Ben David",
          "hidden": false
        },
        {
          "_id": "67ea363dd13d75fc156ec49b",
          "name": "Elad Hoffer",
          "hidden": false
        },
        {
          "_id": "67ea363dd13d75fc156ec49c",
          "name": "Roi Reichart",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/671f8106d677d3a764a6f9a5/Rq3iCgXutkz1jOx5krnk-.png"
      ],
      "publishedAt": "2025-03-25T14:18:21.000Z",
      "submittedOnDailyAt": "2025-03-31T05:02:48.696Z",
      "title": "AdaptiVocab: Enhancing LLM Efficiency in Focused Domains through\n  Lightweight Vocabulary Adaptation",
      "submittedOnDailyBy": {
        "_id": "671f8106d677d3a764a6f9a5",
        "avatarUrl": "/avatars/90b4b00058aac30c060c5eac8debb1c7.svg",
        "isPro": false,
        "fullname": "itay nakash",
        "user": "itaynakash",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) have shown impressive versatility as general\npurpose models. However, their broad applicability comes at a high-cost\ncomputational overhead, particularly in auto-regressive decoding where each\nstep requires a forward pass. In domain-specific settings, general-purpose\ncapabilities are unnecessary and can be exchanged for efficiency. In this work,\nwe take a novel perspective on domain adaptation, reducing latency and\ncomputational costs by adapting the vocabulary to focused domains of interest.\nWe introduce AdaptiVocab, an end-to-end approach for vocabulary adaptation,\ndesigned to enhance LLM efficiency in low-resource domains. AdaptiVocab can be\napplied to any tokenizer and architecture, modifying the vocabulary by\nreplacing tokens with domain-specific n-gram-based tokens, thereby reducing the\nnumber of tokens required for both input processing and output generation.\nAdaptiVocab initializes new n-token embeddings using an exponentially weighted\ncombination of existing embeddings and employs a lightweight fine-tuning phase\nthat can be efficiently performed on a single GPU. We evaluate two 7B LLMs\nacross three niche domains, assessing efficiency, generation quality, and\nend-task performance. Our results show that AdaptiVocab reduces token usage by\nover 25% without compromising performance",
      "upvotes": 13,
      "discussionId": "67ea363ed13d75fc156ec4e8",
      "ai_keywords": [
        "AdaptiVocab",
        "vocabulary adaptation",
        "n-gram-based tokens",
        "token embeddings",
        "lightweight fine-tuning"
      ]
    },
    "publishedAt": "2025-03-25T10:18:21.000Z",
    "title": "AdaptiVocab: Enhancing LLM Efficiency in Focused Domains through\n  Lightweight Vocabulary Adaptation",
    "summary": "Large Language Models (LLMs) have shown impressive versatility as general\npurpose models. However, their broad applicability comes at a high-cost\ncomputational overhead, particularly in auto-regressive decoding where each\nstep requires a forward pass. In domain-specific settings, general-purpose\ncapabilities are unnecessary and can be exchanged for efficiency. In this work,\nwe take a novel perspective on domain adaptation, reducing latency and\ncomputational costs by adapting the vocabulary to focused domains of interest.\nWe introduce AdaptiVocab, an end-to-end approach for vocabulary adaptation,\ndesigned to enhance LLM efficiency in low-resource domains. AdaptiVocab can be\napplied to any tokenizer and architecture, modifying the vocabulary by\nreplacing tokens with domain-specific n-gram-based tokens, thereby reducing the\nnumber of tokens required for both input processing and output generation.\nAdaptiVocab initializes new n-token embeddings using an exponentially weighted\ncombination of existing embeddings and employs a lightweight fine-tuning phase\nthat can be efficiently performed on a single GPU. We evaluate two 7B LLMs\nacross three niche domains, assessing efficiency, generation quality, and\nend-task performance. Our results show that AdaptiVocab reduces token usage by\nover 25% without compromising performance",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/671f8106d677d3a764a6f9a5/Rq3iCgXutkz1jOx5krnk-.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19693.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "671f8106d677d3a764a6f9a5",
      "avatarUrl": "/avatars/90b4b00058aac30c060c5eac8debb1c7.svg",
      "fullname": "itay nakash",
      "name": "itaynakash",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.22194",
      "authors": [
        {
          "_id": "67e9eebe1f495035ca228ded",
          "name": "Yunhong Min",
          "hidden": false
        },
        {
          "_id": "67e9eebe1f495035ca228dee",
          "name": "Daehyeon Choi",
          "hidden": false
        },
        {
          "_id": "67e9eebe1f495035ca228def",
          "name": "Kyeongmin Yeo",
          "hidden": false
        },
        {
          "_id": "67e9eebe1f495035ca228df0",
          "name": "Jihyun Lee",
          "hidden": false
        },
        {
          "_id": "67e9eebe1f495035ca228df1",
          "name": "Minhyuk Sung",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-28T07:23:12.000Z",
      "submittedOnDailyAt": "2025-03-31T00:39:49.117Z",
      "title": "ORIGEN: Zero-Shot 3D Orientation Grounding in Text-to-Image Generation",
      "submittedOnDailyBy": {
        "_id": "6616702547ea6347974667e5",
        "avatarUrl": "/avatars/6bf95e50ba19df163ef89867ed63fecc.svg",
        "isPro": false,
        "fullname": "Daehyeon Choi",
        "user": "daehyeonchoi",
        "type": "user"
      },
      "summary": "We introduce ORIGEN, the first zero-shot method for 3D orientation grounding\nin text-to-image generation across multiple objects and diverse categories.\nWhile previous work on spatial grounding in image generation has mainly focused\non 2D positioning, it lacks control over 3D orientation. To address this, we\npropose a reward-guided sampling approach using a pretrained discriminative\nmodel for 3D orientation estimation and a one-step text-to-image generative\nflow model. While gradient-ascent-based optimization is a natural choice for\nreward-based guidance, it struggles to maintain image realism. Instead, we\nadopt a sampling-based approach using Langevin dynamics, which extends gradient\nascent by simply injecting random noise--requiring just a single additional\nline of code. Additionally, we introduce adaptive time rescaling based on the\nreward function to accelerate convergence. Our experiments show that ORIGEN\noutperforms both training-based and test-time guidance methods across\nquantitative metrics and user studies.",
      "upvotes": 12,
      "discussionId": "67e9eebf1f495035ca228e34",
      "projectPage": "https://origen2025.github.io/",
      "ai_keywords": [
        "zero-shot method",
        "3D orientation grounding",
        "text-to-image generation",
        "spatial grounding",
        "reward-guided sampling approach",
        "pretrained discriminative model",
        "3D orientation estimation",
        "one-step text-to-image generative flow model",
        "gradient-ascent-based optimization",
        "Langevin dynamics",
        "adaptive time rescaling"
      ]
    },
    "publishedAt": "2025-03-28T03:23:12.000Z",
    "title": "ORIGEN: Zero-Shot 3D Orientation Grounding in Text-to-Image Generation",
    "summary": "We introduce ORIGEN, the first zero-shot method for 3D orientation grounding\nin text-to-image generation across multiple objects and diverse categories.\nWhile previous work on spatial grounding in image generation has mainly focused\non 2D positioning, it lacks control over 3D orientation. To address this, we\npropose a reward-guided sampling approach using a pretrained discriminative\nmodel for 3D orientation estimation and a one-step text-to-image generative\nflow model. While gradient-ascent-based optimization is a natural choice for\nreward-based guidance, it struggles to maintain image realism. Instead, we\nadopt a sampling-based approach using Langevin dynamics, which extends gradient\nascent by simply injecting random noise--requiring just a single additional\nline of code. Additionally, we introduce adaptive time rescaling based on the\nreward function to accelerate convergence. Our experiments show that ORIGEN\noutperforms both training-based and test-time guidance methods across\nquantitative metrics and user studies.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.22194.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6616702547ea6347974667e5",
      "avatarUrl": "/avatars/6bf95e50ba19df163ef89867ed63fecc.svg",
      "fullname": "Daehyeon Choi",
      "name": "daehyeonchoi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.21614",
      "authors": [
        {
          "_id": "67ea331c1238e1aa16fc18b3",
          "name": "Xiaoye Qu",
          "hidden": false
        },
        {
          "_id": "67ea331c1238e1aa16fc18b4",
          "name": "Yafu Li",
          "hidden": false
        },
        {
          "_id": "67ea331c1238e1aa16fc18b5",
          "name": "Zhaochen Su",
          "hidden": false
        },
        {
          "_id": "67ea331c1238e1aa16fc18b6",
          "name": "Weigao Sun",
          "hidden": false
        },
        {
          "_id": "67ea331c1238e1aa16fc18b7",
          "name": "Jianhao Yan",
          "hidden": false
        },
        {
          "_id": "67ea331c1238e1aa16fc18b8",
          "name": "Dongrui Liu",
          "hidden": false
        },
        {
          "_id": "67ea331c1238e1aa16fc18b9",
          "name": "Ganqu Cui",
          "hidden": false
        },
        {
          "_id": "67ea331c1238e1aa16fc18ba",
          "name": "Daizong Liu",
          "hidden": false
        },
        {
          "_id": "67ea331c1238e1aa16fc18bb",
          "name": "Shuxian Liang",
          "hidden": false
        },
        {
          "_id": "67ea331c1238e1aa16fc18bc",
          "name": "Junxian He",
          "hidden": false
        },
        {
          "_id": "67ea331c1238e1aa16fc18bd",
          "name": "Peng Li",
          "hidden": false
        },
        {
          "_id": "67ea331c1238e1aa16fc18be",
          "name": "Wei Wei",
          "hidden": false
        },
        {
          "_id": "67ea331c1238e1aa16fc18bf",
          "name": "Jing Shao",
          "hidden": false
        },
        {
          "_id": "67ea331c1238e1aa16fc18c0",
          "name": "Chaochao Lu",
          "hidden": false
        },
        {
          "_id": "67ea331c1238e1aa16fc18c1",
          "name": "Yue Zhang",
          "hidden": false
        },
        {
          "_id": "67ea331c1238e1aa16fc18c2",
          "name": "Xian-Sheng Hua",
          "hidden": false
        },
        {
          "_id": "67ea331c1238e1aa16fc18c3",
          "name": "Bowen Zhou",
          "hidden": false
        },
        {
          "_id": "67ea331c1238e1aa16fc18c4",
          "name": "Yu Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-27T15:36:30.000Z",
      "submittedOnDailyAt": "2025-03-31T04:49:37.564Z",
      "title": "A Survey of Efficient Reasoning for Large Reasoning Models: Language,\n  Multimodality, and Beyond",
      "submittedOnDailyBy": {
        "_id": "64cb54da1af278541d663708",
        "avatarUrl": "/avatars/c44507cc92bb2e83154bad31b90ce6dd.svg",
        "isPro": false,
        "fullname": "Xiaoye Qu",
        "user": "Xiaoye08",
        "type": "user"
      },
      "summary": "Recent Large Reasoning Models (LRMs), such as DeepSeek-R1 and OpenAI o1, have\ndemonstrated strong performance gains by scaling up the length of\nChain-of-Thought (CoT) reasoning during inference. However, a growing concern\nlies in their tendency to produce excessively long reasoning traces, which are\noften filled with redundant content (e.g., repeated definitions), over-analysis\nof simple problems, and superficial exploration of multiple reasoning paths for\nharder tasks. This inefficiency introduces significant challenges for training,\ninference, and real-world deployment (e.g., in agent-based systems), where\ntoken economy is critical. In this survey, we provide a comprehensive overview\nof recent efforts aimed at improving reasoning efficiency in LRMs, with a\nparticular focus on the unique challenges that arise in this new paradigm. We\nidentify common patterns of inefficiency, examine methods proposed across the\nLRM lifecycle, i.e., from pretraining to inference, and discuss promising\nfuture directions for research. To support ongoing development, we also\nmaintain a real-time GitHub repository tracking recent progress in the field.\nWe hope this survey serves as a foundation for further exploration and inspires\ninnovation in this rapidly evolving area.",
      "upvotes": 11,
      "discussionId": "67ea331d1238e1aa16fc190f",
      "githubRepo": "https://github.com/XiaoYee/Awesome_Efficient_LRM_Reasoning",
      "ai_keywords": [
        "Large Reasoning Models (LRMs)",
        "DeepSeek-R1",
        "OpenAI o1",
        "Chain-of-Thought (CoT) reasoning",
        "reasoning traces",
        "redundant content",
        "over-analysis",
        "superficial exploration",
        "reasoning efficiency",
        "token economy",
        "agent-based systems",
        "pretraining",
        "inference",
        "GitHub repository"
      ]
    },
    "publishedAt": "2025-03-27T11:36:30.000Z",
    "title": "A Survey of Efficient Reasoning for Large Reasoning Models: Language,\n  Multimodality, and Beyond",
    "summary": "Recent Large Reasoning Models (LRMs), such as DeepSeek-R1 and OpenAI o1, have\ndemonstrated strong performance gains by scaling up the length of\nChain-of-Thought (CoT) reasoning during inference. However, a growing concern\nlies in their tendency to produce excessively long reasoning traces, which are\noften filled with redundant content (e.g., repeated definitions), over-analysis\nof simple problems, and superficial exploration of multiple reasoning paths for\nharder tasks. This inefficiency introduces significant challenges for training,\ninference, and real-world deployment (e.g., in agent-based systems), where\ntoken economy is critical. In this survey, we provide a comprehensive overview\nof recent efforts aimed at improving reasoning efficiency in LRMs, with a\nparticular focus on the unique challenges that arise in this new paradigm. We\nidentify common patterns of inefficiency, examine methods proposed across the\nLRM lifecycle, i.e., from pretraining to inference, and discuss promising\nfuture directions for research. To support ongoing development, we also\nmaintain a real-time GitHub repository tracking recent progress in the field.\nWe hope this survey serves as a foundation for further exploration and inspires\ninnovation in this rapidly evolving area.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21614.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64cb54da1af278541d663708",
      "avatarUrl": "/avatars/c44507cc92bb2e83154bad31b90ce6dd.svg",
      "fullname": "Xiaoye Qu",
      "name": "Xiaoye08",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.21821",
      "authors": [
        {
          "_id": "67e9ffab6887b70da56d0de5",
          "name": "Kaiyue Feng",
          "hidden": false
        },
        {
          "_id": "67e9ffab6887b70da56d0de6",
          "name": "Yilun Zhao",
          "hidden": false
        },
        {
          "_id": "67e9ffab6887b70da56d0de7",
          "name": "Yixin Liu",
          "hidden": false
        },
        {
          "_id": "67e9ffab6887b70da56d0de8",
          "name": "Tianyu Yang",
          "hidden": false
        },
        {
          "_id": "67e9ffab6887b70da56d0de9",
          "name": "Chen Zhao",
          "hidden": false
        },
        {
          "_id": "67e9ffab6887b70da56d0dea",
          "name": "John Sous",
          "hidden": false
        },
        {
          "_id": "67e9ffab6887b70da56d0deb",
          "name": "Arman Cohan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-26T06:21:56.000Z",
      "submittedOnDailyAt": "2025-03-31T01:07:24.176Z",
      "title": "PHYSICS: Benchmarking Foundation Models on University-Level Physics\n  Problem Solving",
      "submittedOnDailyBy": {
        "_id": "62f662bcc58915315c4eccea",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
        "isPro": true,
        "fullname": "Yilun",
        "user": "yilunzhao",
        "type": "user"
      },
      "summary": "We introduce PHYSICS, a comprehensive benchmark for university-level physics\nproblem solving. It contains 1297 expert-annotated problems covering six core\nareas: classical mechanics, quantum mechanics, thermodynamics and statistical\nmechanics, electromagnetism, atomic physics, and optics. Each problem requires\nadvanced physics knowledge and mathematical reasoning. We develop a robust\nautomated evaluation system for precise and reliable validation. Our evaluation\nof leading foundation models reveals substantial limitations. Even the most\nadvanced model, o3-mini, achieves only 59.9% accuracy, highlighting significant\nchallenges in solving high-level scientific problems. Through comprehensive\nerror analysis, exploration of diverse prompting strategies, and\nRetrieval-Augmented Generation (RAG)-based knowledge augmentation, we identify\nkey areas for improvement, laying the foundation for future advancements.",
      "upvotes": 10,
      "discussionId": "67e9ffac6887b70da56d0e15",
      "githubRepo": "https://github.com/yale-nlp/Physics"
    },
    "publishedAt": "2025-03-26T02:21:56.000Z",
    "title": "PHYSICS: Benchmarking Foundation Models on University-Level Physics\n  Problem Solving",
    "summary": "We introduce PHYSICS, a comprehensive benchmark for university-level physics\nproblem solving. It contains 1297 expert-annotated problems covering six core\nareas: classical mechanics, quantum mechanics, thermodynamics and statistical\nmechanics, electromagnetism, atomic physics, and optics. Each problem requires\nadvanced physics knowledge and mathematical reasoning. We develop a robust\nautomated evaluation system for precise and reliable validation. Our evaluation\nof leading foundation models reveals substantial limitations. Even the most\nadvanced model, o3-mini, achieves only 59.9% accuracy, highlighting significant\nchallenges in solving high-level scientific problems. Through comprehensive\nerror analysis, exploration of diverse prompting strategies, and\nRetrieval-Augmented Generation (RAG)-based knowledge augmentation, we identify\nkey areas for improvement, laying the foundation for future advancements.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21821.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62f662bcc58915315c4eccea",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
      "fullname": "Yilun",
      "name": "yilunzhao",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.22236",
      "authors": [
        {
          "_id": "67e9fe132a2d5e305e4e6b80",
          "name": "Chongjie Ye",
          "hidden": false
        },
        {
          "_id": "67e9fe132a2d5e305e4e6b81",
          "name": "Yushuang Wu",
          "hidden": false
        },
        {
          "_id": "67e9fe132a2d5e305e4e6b82",
          "name": "Ziteng Lu",
          "hidden": false
        },
        {
          "_id": "67e9fe132a2d5e305e4e6b83",
          "name": "Jiahao Chang",
          "hidden": false
        },
        {
          "_id": "67e9fe132a2d5e305e4e6b84",
          "name": "Xiaoyang Guo",
          "hidden": false
        },
        {
          "_id": "67e9fe132a2d5e305e4e6b85",
          "name": "Jiaqing Zhou",
          "hidden": false
        },
        {
          "_id": "67e9fe132a2d5e305e4e6b86",
          "name": "Hao Zhao",
          "hidden": false
        },
        {
          "_id": "67e9fe132a2d5e305e4e6b87",
          "name": "Xiaoguang Han",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-28T08:39:20.000Z",
      "submittedOnDailyAt": "2025-03-31T01:00:02.027Z",
      "title": "Hi3DGen: High-fidelity 3D Geometry Generation from Images via Normal\n  Bridging",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "With the growing demand for high-fidelity 3D models from 2D images, existing\nmethods still face significant challenges in accurately reproducing\nfine-grained geometric details due to limitations in domain gaps and inherent\nambiguities in RGB images. To address these issues, we propose Hi3DGen, a novel\nframework for generating high-fidelity 3D geometry from images via normal\nbridging. Hi3DGen consists of three key components: (1) an image-to-normal\nestimator that decouples the low-high frequency image pattern with noise\ninjection and dual-stream training to achieve generalizable, stable, and sharp\nestimation; (2) a normal-to-geometry learning approach that uses\nnormal-regularized latent diffusion learning to enhance 3D geometry generation\nfidelity; and (3) a 3D data synthesis pipeline that constructs a high-quality\ndataset to support training. Extensive experiments demonstrate the\neffectiveness and superiority of our framework in generating rich geometric\ndetails, outperforming state-of-the-art methods in terms of fidelity. Our work\nprovides a new direction for high-fidelity 3D geometry generation from images\nby leveraging normal maps as an intermediate representation.",
      "upvotes": 8,
      "discussionId": "67e9fe172a2d5e305e4e6ce6",
      "ai_keywords": [
        "image-to-normal estimator",
        "dual-stream training",
        "normal-regularized latent diffusion learning",
        "3D data synthesis pipeline",
        "normal maps"
      ]
    },
    "publishedAt": "2025-03-28T04:39:20.000Z",
    "title": "Hi3DGen: High-fidelity 3D Geometry Generation from Images via Normal\n  Bridging",
    "summary": "With the growing demand for high-fidelity 3D models from 2D images, existing\nmethods still face significant challenges in accurately reproducing\nfine-grained geometric details due to limitations in domain gaps and inherent\nambiguities in RGB images. To address these issues, we propose Hi3DGen, a novel\nframework for generating high-fidelity 3D geometry from images via normal\nbridging. Hi3DGen consists of three key components: (1) an image-to-normal\nestimator that decouples the low-high frequency image pattern with noise\ninjection and dual-stream training to achieve generalizable, stable, and sharp\nestimation; (2) a normal-to-geometry learning approach that uses\nnormal-regularized latent diffusion learning to enhance 3D geometry generation\nfidelity; and (3) a 3D data synthesis pipeline that constructs a high-quality\ndataset to support training. Extensive experiments demonstrate the\neffectiveness and superiority of our framework in generating rich geometric\ndetails, outperforming state-of-the-art methods in terms of fidelity. Our work\nprovides a new direction for high-fidelity 3D geometry generation from images\nby leveraging normal maps as an intermediate representation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.22236.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6521
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.20785",
      "authors": [
        {
          "_id": "67e7b4ba05d7355e476f4a10",
          "name": "Tianqi Liu",
          "hidden": false
        },
        {
          "_id": "67e7b4ba05d7355e476f4a11",
          "name": "Zihao Huang",
          "hidden": false
        },
        {
          "_id": "67e7b4ba05d7355e476f4a12",
          "name": "Zhaoxi Chen",
          "hidden": false
        },
        {
          "_id": "67e7b4ba05d7355e476f4a13",
          "name": "Guangcong Wang",
          "hidden": false
        },
        {
          "_id": "67e7b4ba05d7355e476f4a14",
          "name": "Shoukang Hu",
          "hidden": false
        },
        {
          "_id": "67e7b4ba05d7355e476f4a15",
          "name": "Liao Shen",
          "hidden": false
        },
        {
          "_id": "67e7b4ba05d7355e476f4a16",
          "name": "Huiqiang Sun",
          "hidden": false
        },
        {
          "_id": "67e7b4ba05d7355e476f4a17",
          "name": "Zhiguo Cao",
          "hidden": false
        },
        {
          "_id": "67e7b4ba05d7355e476f4a18",
          "name": "Wei Li",
          "hidden": false
        },
        {
          "_id": "67e7b4ba05d7355e476f4a19",
          "name": "Ziwei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-26T17:59:44.000Z",
      "submittedOnDailyAt": "2025-03-31T03:00:39.077Z",
      "title": "Free4D: Tuning-free 4D Scene Generation with Spatial-Temporal\n  Consistency",
      "submittedOnDailyBy": {
        "_id": "62fc8cf7ee999004b5a8b982",
        "avatarUrl": "/avatars/6c5dda9e58747054a989f077a078f3dc.svg",
        "isPro": false,
        "fullname": "Zhaoxi Chen",
        "user": "FrozenBurning",
        "type": "user"
      },
      "summary": "We present Free4D, a novel tuning-free framework for 4D scene generation from\na single image. Existing methods either focus on object-level generation,\nmaking scene-level generation infeasible, or rely on large-scale multi-view\nvideo datasets for expensive training, with limited generalization ability due\nto the scarcity of 4D scene data. In contrast, our key insight is to distill\npre-trained foundation models for consistent 4D scene representation, which\noffers promising advantages such as efficiency and generalizability. 1) To\nachieve this, we first animate the input image using image-to-video diffusion\nmodels followed by 4D geometric structure initialization. 2) To turn this\ncoarse structure into spatial-temporal consistent multiview videos, we design\nan adaptive guidance mechanism with a point-guided denoising strategy for\nspatial consistency and a novel latent replacement strategy for temporal\ncoherence. 3) To lift these generated observations into consistent 4D\nrepresentation, we propose a modulation-based refinement to mitigate\ninconsistencies while fully leveraging the generated information. The resulting\n4D representation enables real-time, controllable rendering, marking a\nsignificant advancement in single-image-based 4D scene generation.",
      "upvotes": 6,
      "discussionId": "67e7b4bb05d7355e476f4a74",
      "ai_keywords": [
        "diffusion models",
        "4D scene representation",
        "image-to-video",
        "adaptive guidance mechanism",
        "point-guided denoising",
        "latent replacement strategy",
        "temporal coherence",
        "modulation-based refinement"
      ]
    },
    "publishedAt": "2025-03-26T13:59:44.000Z",
    "title": "Free4D: Tuning-free 4D Scene Generation with Spatial-Temporal\n  Consistency",
    "summary": "We present Free4D, a novel tuning-free framework for 4D scene generation from\na single image. Existing methods either focus on object-level generation,\nmaking scene-level generation infeasible, or rely on large-scale multi-view\nvideo datasets for expensive training, with limited generalization ability due\nto the scarcity of 4D scene data. In contrast, our key insight is to distill\npre-trained foundation models for consistent 4D scene representation, which\noffers promising advantages such as efficiency and generalizability. 1) To\nachieve this, we first animate the input image using image-to-video diffusion\nmodels followed by 4D geometric structure initialization. 2) To turn this\ncoarse structure into spatial-temporal consistent multiview videos, we design\nan adaptive guidance mechanism with a point-guided denoising strategy for\nspatial consistency and a novel latent replacement strategy for temporal\ncoherence. 3) To lift these generated observations into consistent 4D\nrepresentation, we propose a modulation-based refinement to mitigate\ninconsistencies while fully leveraging the generated information. The resulting\n4D representation enables real-time, controllable rendering, marking a\nsignificant advancement in single-image-based 4D scene generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20785.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62fc8cf7ee999004b5a8b982",
      "avatarUrl": "/avatars/6c5dda9e58747054a989f077a078f3dc.svg",
      "fullname": "Zhaoxi Chen",
      "name": "FrozenBurning",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 22
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.22268",
      "authors": [
        {
          "_id": "67e9fec1564b123aa5d70388",
          "name": "Nan Huang",
          "hidden": false
        },
        {
          "_id": "67e9fec1564b123aa5d70389",
          "name": "Wenzhao Zheng",
          "hidden": false
        },
        {
          "_id": "67e9fec1564b123aa5d7038a",
          "name": "Chenfeng Xu",
          "hidden": false
        },
        {
          "_id": "67e9fec1564b123aa5d7038b",
          "name": "Kurt Keutzer",
          "hidden": false
        },
        {
          "_id": "67e9fec1564b123aa5d7038c",
          "name": "Shanghang Zhang",
          "hidden": false
        },
        {
          "_id": "67e9fec1564b123aa5d7038d",
          "name": "Angjoo Kanazawa",
          "hidden": false
        },
        {
          "_id": "67e9fec1564b123aa5d7038e",
          "name": "Qianqian Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-28T09:34:11.000Z",
      "submittedOnDailyAt": "2025-03-31T01:03:54.297Z",
      "title": "Segment Any Motion in Videos",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Moving object segmentation is a crucial task for achieving a high-level\nunderstanding of visual scenes and has numerous downstream applications. Humans\ncan effortlessly segment moving objects in videos. Previous work has largely\nrelied on optical flow to provide motion cues; however, this approach often\nresults in imperfect predictions due to challenges such as partial motion,\ncomplex deformations, motion blur and background distractions. We propose a\nnovel approach for moving object segmentation that combines long-range\ntrajectory motion cues with DINO-based semantic features and leverages SAM2 for\npixel-level mask densification through an iterative prompting strategy. Our\nmodel employs Spatio-Temporal Trajectory Attention and Motion-Semantic\nDecoupled Embedding to prioritize motion while integrating semantic support.\nExtensive testing on diverse datasets demonstrates state-of-the-art\nperformance, excelling in challenging scenarios and fine-grained segmentation\nof multiple objects. Our code is available at https://motion-seg.github.io/.",
      "upvotes": 5,
      "discussionId": "67e9fec2564b123aa5d70406",
      "ai_keywords": [
        "DINO-based",
        "SAM2",
        "Spatio-Temporal Trajectory Attention",
        "Motion-Semantic Decoupled Embedding"
      ]
    },
    "publishedAt": "2025-03-28T05:34:11.000Z",
    "title": "Segment Any Motion in Videos",
    "summary": "Moving object segmentation is a crucial task for achieving a high-level\nunderstanding of visual scenes and has numerous downstream applications. Humans\ncan effortlessly segment moving objects in videos. Previous work has largely\nrelied on optical flow to provide motion cues; however, this approach often\nresults in imperfect predictions due to challenges such as partial motion,\ncomplex deformations, motion blur and background distractions. We propose a\nnovel approach for moving object segmentation that combines long-range\ntrajectory motion cues with DINO-based semantic features and leverages SAM2 for\npixel-level mask densification through an iterative prompting strategy. Our\nmodel employs Spatio-Temporal Trajectory Attention and Motion-Semantic\nDecoupled Embedding to prioritize motion while integrating semantic support.\nExtensive testing on diverse datasets demonstrates state-of-the-art\nperformance, excelling in challenging scenarios and fine-grained segmentation\nof multiple objects. Our code is available at https://motion-seg.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.22268.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6521
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.22329",
      "authors": [
        {
          "_id": "67ea01e3d13d75fc155fa69d",
          "name": "Louis Owen",
          "hidden": false
        },
        {
          "_id": "67ea01e3d13d75fc155fa69e",
          "name": "Nilabhra Roy Chowdhury",
          "hidden": false
        },
        {
          "_id": "67ea01e3d13d75fc155fa69f",
          "name": "Abhay Kumar",
          "hidden": false
        },
        {
          "_id": "67ea01e3d13d75fc155fa6a0",
          "name": "Fabian Güra",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-28T11:08:34.000Z",
      "submittedOnDailyAt": "2025-03-31T01:17:56.852Z",
      "title": "A Refined Analysis of Massive Activations in LLMs",
      "submittedOnDailyBy": {
        "_id": "6071c4b270e11b30cfcfd7a3",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6071c4b270e11b30cfcfd7a3/-1ekCBzSTpqxkkul0bgmI.jpeg",
        "isPro": false,
        "fullname": "Louis Owen",
        "user": "louisowen6",
        "type": "user"
      },
      "summary": "Motivated in part by their relevance for low-precision training and\nquantization, massive activations in large language models (LLMs) have recently\nemerged as a topic of interest. However, existing analyses are limited in\nscope, and generalizability across architectures is unclear. This paper helps\naddress some of these gaps by conducting an analysis of massive activations\nacross a broad range of LLMs, including both GLU-based and non-GLU-based\narchitectures. Our findings challenge several prior assumptions, most\nimportantly: (1) not all massive activations are detrimental, i.e. suppressing\nthem does not lead to an explosion of perplexity or a collapse in downstream\ntask performance; (2) proposed mitigation strategies such as Attention KV bias\nare model-specific and ineffective in certain cases. We consequently\ninvestigate novel hybrid mitigation strategies; in particular pairing Target\nVariance Rescaling (TVR) with Attention KV bias or Dynamic Tanh (DyT)\nsuccessfully balances the mitigation of massive activations with preserved\ndownstream model performance in the scenarios we investigated. Our code is\navailable at: https://github.com/bluorion-com/refine_massive_activations.",
      "upvotes": 4,
      "discussionId": "67ea01e4d13d75fc155fa6d2",
      "githubRepo": "https://github.com/bluorion-com/refine_massive_activations",
      "ai_keywords": [
        "massive activations",
        "low-precision training",
        "quantization",
        "large language models (LLMs)",
        "GLU-based architectures",
        "Attention KV bias",
        "Target Variance Rescaling (TVR)",
        "Dynamic Tanh (DyT)",
        "perplexity",
        "downstream task performance"
      ]
    },
    "publishedAt": "2025-03-28T07:08:34.000Z",
    "title": "A Refined Analysis of Massive Activations in LLMs",
    "summary": "Motivated in part by their relevance for low-precision training and\nquantization, massive activations in large language models (LLMs) have recently\nemerged as a topic of interest. However, existing analyses are limited in\nscope, and generalizability across architectures is unclear. This paper helps\naddress some of these gaps by conducting an analysis of massive activations\nacross a broad range of LLMs, including both GLU-based and non-GLU-based\narchitectures. Our findings challenge several prior assumptions, most\nimportantly: (1) not all massive activations are detrimental, i.e. suppressing\nthem does not lead to an explosion of perplexity or a collapse in downstream\ntask performance; (2) proposed mitigation strategies such as Attention KV bias\nare model-specific and ineffective in certain cases. We consequently\ninvestigate novel hybrid mitigation strategies; in particular pairing Target\nVariance Rescaling (TVR) with Attention KV bias or Dynamic Tanh (DyT)\nsuccessfully balances the mitigation of massive activations with preserved\ndownstream model performance in the scenarios we investigated. Our code is\navailable at: https://github.com/bluorion-com/refine_massive_activations.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.22329.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6071c4b270e11b30cfcfd7a3",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6071c4b270e11b30cfcfd7a3/-1ekCBzSTpqxkkul0bgmI.jpeg",
      "fullname": "Louis Owen",
      "name": "louisowen6",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.21732",
      "authors": [
        {
          "_id": "67e620f77203bed82eb944e9",
          "user": {
            "_id": "66744b514f3d4b3327cd228d",
            "avatarUrl": "/avatars/9768587af7442fbb140f6b3d58100f91.svg",
            "isPro": false,
            "fullname": "XianglongHe",
            "user": "XianglongHe",
            "type": "user"
          },
          "name": "Xianglong He",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T10:51:50.659Z",
          "hidden": false
        },
        {
          "_id": "67e620f77203bed82eb944ea",
          "user": {
            "_id": "644dbf6453ad80c6593bf748",
            "avatarUrl": "/avatars/0e170cf2aa8d7f0f3f83e36f06f023f8.svg",
            "isPro": false,
            "fullname": "Zixin Zou",
            "user": "zouzx",
            "type": "user"
          },
          "name": "Zi-Xin Zou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T10:52:17.598Z",
          "hidden": false
        },
        {
          "_id": "67e620f77203bed82eb944eb",
          "name": "Chia-Hao Chen",
          "hidden": false
        },
        {
          "_id": "67e620f77203bed82eb944ec",
          "user": {
            "_id": "6346aaa3f06b237ba4e297b0",
            "avatarUrl": "/avatars/5acb986e993eab1461200f3e9d99d022.svg",
            "isPro": false,
            "fullname": "Yuan-Chen Guo",
            "user": "bennyguo",
            "type": "user"
          },
          "name": "Yuan-Chen Guo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T10:52:33.753Z",
          "hidden": false
        },
        {
          "_id": "67e620f77203bed82eb944ed",
          "name": "Ding Liang",
          "hidden": false
        },
        {
          "_id": "67e620f77203bed82eb944ee",
          "name": "Chun Yuan",
          "hidden": false
        },
        {
          "_id": "67e620f77203bed82eb944ef",
          "name": "Wanli Ouyang",
          "hidden": false
        },
        {
          "_id": "67e620f77203bed82eb944f0",
          "user": {
            "_id": "638066faf022c8a5803f7eb8",
            "avatarUrl": "/avatars/4cfd699c3f6c5461b12b7dc5e3fe183d.svg",
            "isPro": false,
            "fullname": "Yanpei Cao",
            "user": "pookiefoof",
            "type": "user"
          },
          "name": "Yan-Pei Cao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T10:52:50.823Z",
          "hidden": false
        },
        {
          "_id": "67e620f77203bed82eb944f1",
          "user": {
            "_id": "64d71083a787c9bc7b9f1238",
            "avatarUrl": "/avatars/d0b0546dec7fc5792921154bec41385a.svg",
            "isPro": false,
            "fullname": "Yangguang Li",
            "user": "Lp256",
            "type": "user"
          },
          "name": "Yangguang Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T10:52:59.331Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-27T17:46:42.000Z",
      "submittedOnDailyAt": "2025-03-31T01:22:51.212Z",
      "title": "SparseFlex: High-Resolution and Arbitrary-Topology 3D Shape Modeling",
      "submittedOnDailyBy": {
        "_id": "64d71083a787c9bc7b9f1238",
        "avatarUrl": "/avatars/d0b0546dec7fc5792921154bec41385a.svg",
        "isPro": false,
        "fullname": "Yangguang Li",
        "user": "Lp256",
        "type": "user"
      },
      "summary": "Creating high-fidelity 3D meshes with arbitrary topology, including open\nsurfaces and complex interiors, remains a significant challenge. Existing\nimplicit field methods often require costly and detail-degrading watertight\nconversion, while other approaches struggle with high resolutions. This paper\nintroduces SparseFlex, a novel sparse-structured isosurface representation that\nenables differentiable mesh reconstruction at resolutions up to 1024^3\ndirectly from rendering losses. SparseFlex combines the accuracy of Flexicubes\nwith a sparse voxel structure, focusing computation on surface-adjacent regions\nand efficiently handling open surfaces. Crucially, we introduce a frustum-aware\nsectional voxel training strategy that activates only relevant voxels during\nrendering, dramatically reducing memory consumption and enabling\nhigh-resolution training. This also allows, for the first time, the\nreconstruction of mesh interiors using only rendering supervision. Building\nupon this, we demonstrate a complete shape modeling pipeline by training a\nvariational autoencoder (VAE) and a rectified flow transformer for high-quality\n3D shape generation. Our experiments show state-of-the-art reconstruction\naccuracy, with a ~82% reduction in Chamfer Distance and a ~88% increase in\nF-score compared to previous methods, and demonstrate the generation of\nhigh-resolution, detailed 3D shapes with arbitrary topology. By enabling\nhigh-resolution, differentiable mesh reconstruction and generation with\nrendering losses, SparseFlex significantly advances the state-of-the-art in 3D\nshape representation and modeling.",
      "upvotes": 3,
      "discussionId": "67e620fb7203bed82eb945e8",
      "projectPage": "https://xianglonghe.github.io/TripoSF/index.html",
      "githubRepo": "https://github.com/VAST-AI-Research/TripoSF",
      "ai_keywords": [
        "SparseFlex",
        "isosurface representation",
        "differentiable mesh reconstruction",
        "Flexicubes",
        "sparse voxel structure",
        "frustum-aware",
        "sectional voxel training strategy",
        "memory consumption",
        "variational autoencoder (VAE)",
        "rectified flow transformer",
        "high-quality 3D shape generation",
        "Chamfer Distance",
        "F-score",
        "high-resolution, differentiable mesh reconstruction",
        "3D shape representation",
        "3D shape modeling"
      ]
    },
    "publishedAt": "2025-03-27T13:46:42.000Z",
    "title": "SparseFlex: High-Resolution and Arbitrary-Topology 3D Shape Modeling",
    "summary": "Creating high-fidelity 3D meshes with arbitrary topology, including open\nsurfaces and complex interiors, remains a significant challenge. Existing\nimplicit field methods often require costly and detail-degrading watertight\nconversion, while other approaches struggle with high resolutions. This paper\nintroduces SparseFlex, a novel sparse-structured isosurface representation that\nenables differentiable mesh reconstruction at resolutions up to 1024^3\ndirectly from rendering losses. SparseFlex combines the accuracy of Flexicubes\nwith a sparse voxel structure, focusing computation on surface-adjacent regions\nand efficiently handling open surfaces. Crucially, we introduce a frustum-aware\nsectional voxel training strategy that activates only relevant voxels during\nrendering, dramatically reducing memory consumption and enabling\nhigh-resolution training. This also allows, for the first time, the\nreconstruction of mesh interiors using only rendering supervision. Building\nupon this, we demonstrate a complete shape modeling pipeline by training a\nvariational autoencoder (VAE) and a rectified flow transformer for high-quality\n3D shape generation. Our experiments show state-of-the-art reconstruction\naccuracy, with a ~82% reduction in Chamfer Distance and a ~88% increase in\nF-score compared to previous methods, and demonstrate the generation of\nhigh-resolution, detailed 3D shapes with arbitrary topology. By enabling\nhigh-resolution, differentiable mesh reconstruction and generation with\nrendering losses, SparseFlex significantly advances the state-of-the-art in 3D\nshape representation and modeling.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21732.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64d71083a787c9bc7b9f1238",
      "avatarUrl": "/avatars/d0b0546dec7fc5792921154bec41385a.svg",
      "fullname": "Yangguang Li",
      "name": "Lp256",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.18968",
      "authors": [
        {
          "_id": "67ea0ede7b856e8fa8ff50d0",
          "name": "Ziyue Wang",
          "hidden": false
        },
        {
          "_id": "67ea0ede7b856e8fa8ff50d1",
          "user": {
            "_id": "6317257fc92fd6fee317ff7c",
            "avatarUrl": "/avatars/2f460a2f28562c987becb2acad8d93e7.svg",
            "isPro": false,
            "fullname": "Junde Wu",
            "user": "morson",
            "type": "user"
          },
          "name": "Junde Wu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-31T03:41:20.293Z",
          "hidden": false
        },
        {
          "_id": "67ea0ede7b856e8fa8ff50d2",
          "name": "Chang Han Low",
          "hidden": false
        },
        {
          "_id": "67ea0ede7b856e8fa8ff50d3",
          "name": "Yueming Jin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-21T14:04:18.000Z",
      "submittedOnDailyAt": "2025-03-31T02:13:07.501Z",
      "title": "MedAgent-Pro: Towards Multi-modal Evidence-based Medical Diagnosis via\n  Reasoning Agentic Workflow",
      "submittedOnDailyBy": {
        "_id": "65be4d7d5e342a230dc19a54",
        "avatarUrl": "/avatars/04ba16980da94954d811032f0091212f.svg",
        "isPro": false,
        "fullname": "Ziyue Wang",
        "user": "ZiyueWang",
        "type": "user"
      },
      "summary": "Developing reliable AI systems to assist human clinicians in multi-modal\nmedical diagnosis has long been a key objective for researchers. Recently,\nMulti-modal Large Language Models (MLLMs) have gained significant attention and\nachieved success across various domains. With strong reasoning capabilities and\nthe ability to perform diverse tasks based on user instructions, they hold\ngreat potential for enhancing medical diagnosis. However, directly applying\nMLLMs to the medical domain still presents challenges. They lack detailed\nperception of visual inputs, limiting their ability to perform quantitative\nimage analysis, which is crucial for medical diagnostics. Additionally, MLLMs\noften exhibit hallucinations and inconsistencies in reasoning, whereas clinical\ndiagnoses must adhere strictly to established criteria. To address these\nchallenges, we propose MedAgent-Pro, an evidence-based reasoning agentic system\ndesigned to achieve reliable, explainable, and precise medical diagnoses. This\nis accomplished through a hierarchical workflow: at the task level,\nknowledge-based reasoning generate reliable diagnostic plans for specific\ndiseases following retrieved clinical criteria. While at the case level,\nmultiple tool agents process multi-modal inputs, analyze different indicators\naccording to the plan, and provide a final diagnosis based on both quantitative\nand qualitative evidence. Comprehensive experiments on both 2D and 3D medical\ndiagnosis tasks demonstrate the superiority and effectiveness of MedAgent-Pro,\nwhile case studies further highlight its reliability and interpretability. The\ncode is available at https://github.com/jinlab-imvr/MedAgent-Pro.",
      "upvotes": 3,
      "discussionId": "67ea0ee07b856e8fa8ff514f",
      "ai_keywords": [
        "Multi-modal Large Language Models (MLLMs)",
        "knowledge-based reasoning",
        "task level",
        "case level",
        "tool agents",
        "multi-modal inputs",
        "diagnostic plans",
        "quantitative analysis",
        "qualitative evidence",
        "hierarchical workflow",
        "evidence-based reasoning",
        "explainable",
        "precise medical diagnoses",
        "superior",
        "effective",
        "reliability",
        "interpretability",
        "clinical criteria"
      ]
    },
    "publishedAt": "2025-03-21T10:04:18.000Z",
    "title": "MedAgent-Pro: Towards Multi-modal Evidence-based Medical Diagnosis via\n  Reasoning Agentic Workflow",
    "summary": "Developing reliable AI systems to assist human clinicians in multi-modal\nmedical diagnosis has long been a key objective for researchers. Recently,\nMulti-modal Large Language Models (MLLMs) have gained significant attention and\nachieved success across various domains. With strong reasoning capabilities and\nthe ability to perform diverse tasks based on user instructions, they hold\ngreat potential for enhancing medical diagnosis. However, directly applying\nMLLMs to the medical domain still presents challenges. They lack detailed\nperception of visual inputs, limiting their ability to perform quantitative\nimage analysis, which is crucial for medical diagnostics. Additionally, MLLMs\noften exhibit hallucinations and inconsistencies in reasoning, whereas clinical\ndiagnoses must adhere strictly to established criteria. To address these\nchallenges, we propose MedAgent-Pro, an evidence-based reasoning agentic system\ndesigned to achieve reliable, explainable, and precise medical diagnoses. This\nis accomplished through a hierarchical workflow: at the task level,\nknowledge-based reasoning generate reliable diagnostic plans for specific\ndiseases following retrieved clinical criteria. While at the case level,\nmultiple tool agents process multi-modal inputs, analyze different indicators\naccording to the plan, and provide a final diagnosis based on both quantitative\nand qualitative evidence. Comprehensive experiments on both 2D and 3D medical\ndiagnosis tasks demonstrate the superiority and effectiveness of MedAgent-Pro,\nwhile case studies further highlight its reliability and interpretability. The\ncode is available at https://github.com/jinlab-imvr/MedAgent-Pro.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18968.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65be4d7d5e342a230dc19a54",
      "avatarUrl": "/avatars/04ba16980da94954d811032f0091212f.svg",
      "fullname": "Ziyue Wang",
      "name": "ZiyueWang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.21779",
      "authors": [
        {
          "_id": "67e69b75113f7c9e552bea69",
          "user": {
            "_id": "660b9dfc8b022f13fdc8db83",
            "avatarUrl": "/avatars/62c68259c8a4d53f121c41a2831cb89a.svg",
            "isPro": false,
            "fullname": "vortexyu",
            "user": "vortex778",
            "type": "user"
          },
          "name": "Weihao Yu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-28T14:22:23.190Z",
          "hidden": false
        },
        {
          "_id": "67e69b75113f7c9e552bea6a",
          "name": "Yuanhao Cai",
          "hidden": false
        },
        {
          "_id": "67e69b75113f7c9e552bea6b",
          "name": "Ruyi Zha",
          "hidden": false
        },
        {
          "_id": "67e69b75113f7c9e552bea6c",
          "name": "Zhiwen Fan",
          "hidden": false
        },
        {
          "_id": "67e69b75113f7c9e552bea6d",
          "name": "Chenxin Li",
          "hidden": false
        },
        {
          "_id": "67e69b75113f7c9e552bea6e",
          "name": "Yixuan Yuan",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/660b9dfc8b022f13fdc8db83/Em05evtcueTrruwgHTLlo.gif"
      ],
      "publishedAt": "2025-03-27T17:59:57.000Z",
      "submittedOnDailyAt": "2025-03-31T05:01:34.606Z",
      "title": "X^{2}-Gaussian: 4D Radiative Gaussian Splatting for Continuous-time\n  Tomographic Reconstruction",
      "submittedOnDailyBy": {
        "_id": "660b9dfc8b022f13fdc8db83",
        "avatarUrl": "/avatars/62c68259c8a4d53f121c41a2831cb89a.svg",
        "isPro": false,
        "fullname": "vortexyu",
        "user": "vortex778",
        "type": "user"
      },
      "summary": "Four-dimensional computed tomography (4D CT) reconstruction is crucial for\ncapturing dynamic anatomical changes but faces inherent limitations from\nconventional phase-binning workflows. Current methods discretize temporal\nresolution into fixed phases with respiratory gating devices, introducing\nmotion misalignment and restricting clinical practicality. In this paper, We\npropose X^2-Gaussian, a novel framework that enables continuous-time 4D-CT\nreconstruction by integrating dynamic radiative Gaussian splatting with\nself-supervised respiratory motion learning. Our approach models anatomical\ndynamics through a spatiotemporal encoder-decoder architecture that predicts\ntime-varying Gaussian deformations, eliminating phase discretization. To remove\ndependency on external gating devices, we introduce a physiology-driven\nperiodic consistency loss that learns patient-specific breathing cycles\ndirectly from projections via differentiable optimization. Extensive\nexperiments demonstrate state-of-the-art performance, achieving a 9.93 dB PSNR\ngain over traditional methods and 2.25 dB improvement against prior Gaussian\nsplatting techniques. By unifying continuous motion modeling with hardware-free\nperiod learning, X^2-Gaussian advances high-fidelity 4D CT reconstruction for\ndynamic clinical imaging. Project website at: https://x2-gaussian.github.io/.",
      "upvotes": 2,
      "discussionId": "67e69b76113f7c9e552beaa1",
      "projectPage": "https://x2-gaussian.github.io/",
      "githubRepo": "https://github.com/yuyouxixi/x2-gaussian",
      "ai_keywords": [
        "X$^2$-Gaussian",
        "continuous-time 4D-CT",
        "dynamic radiative Gaussian splatting",
        "self-supervised respiratory motion learning",
        "spatiotemporal encoder-decoder architecture",
        "time-varying Gaussian deformations",
        "physiology-driven periodic consistency loss",
        "differentiable optimization",
        "PSNR gain",
        "hardware-free period learning",
        "high-fidelity 4D CT reconstruction"
      ]
    },
    "publishedAt": "2025-03-27T13:59:57.000Z",
    "title": "X^{2}-Gaussian: 4D Radiative Gaussian Splatting for Continuous-time\n  Tomographic Reconstruction",
    "summary": "Four-dimensional computed tomography (4D CT) reconstruction is crucial for\ncapturing dynamic anatomical changes but faces inherent limitations from\nconventional phase-binning workflows. Current methods discretize temporal\nresolution into fixed phases with respiratory gating devices, introducing\nmotion misalignment and restricting clinical practicality. In this paper, We\npropose X^2-Gaussian, a novel framework that enables continuous-time 4D-CT\nreconstruction by integrating dynamic radiative Gaussian splatting with\nself-supervised respiratory motion learning. Our approach models anatomical\ndynamics through a spatiotemporal encoder-decoder architecture that predicts\ntime-varying Gaussian deformations, eliminating phase discretization. To remove\ndependency on external gating devices, we introduce a physiology-driven\nperiodic consistency loss that learns patient-specific breathing cycles\ndirectly from projections via differentiable optimization. Extensive\nexperiments demonstrate state-of-the-art performance, achieving a 9.93 dB PSNR\ngain over traditional methods and 2.25 dB improvement against prior Gaussian\nsplatting techniques. By unifying continuous motion modeling with hardware-free\nperiod learning, X^2-Gaussian advances high-fidelity 4D CT reconstruction for\ndynamic clinical imaging. Project website at: https://x2-gaussian.github.io/.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/660b9dfc8b022f13fdc8db83/Em05evtcueTrruwgHTLlo.gif"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21779.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "660b9dfc8b022f13fdc8db83",
      "avatarUrl": "/avatars/62c68259c8a4d53f121c41a2831cb89a.svg",
      "fullname": "vortexyu",
      "name": "vortex778",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.21332",
      "authors": [
        {
          "_id": "67e623f10aaa5e9f7cf8a179",
          "user": {
            "_id": "65642d7401de72cb63165d22",
            "avatarUrl": "/avatars/1f4417c4ac5e781ce73eae1060e3f7f2.svg",
            "isPro": true,
            "fullname": "ytaewon",
            "user": "hamzzi",
            "type": "user"
          },
          "name": "Taewon Yun",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-28T08:36:56.604Z",
          "hidden": false
        },
        {
          "_id": "67e623f10aaa5e9f7cf8a17a",
          "name": "Jihwan Oh",
          "hidden": false
        },
        {
          "_id": "67e623f10aaa5e9f7cf8a17b",
          "name": "Hyangsuk Min",
          "hidden": false
        },
        {
          "_id": "67e623f10aaa5e9f7cf8a17c",
          "name": "Yuho Lee",
          "hidden": false
        },
        {
          "_id": "67e623f10aaa5e9f7cf8a17d",
          "name": "Jihwan Bang",
          "hidden": false
        },
        {
          "_id": "67e623f10aaa5e9f7cf8a17e",
          "name": "Jason Cai",
          "hidden": false
        },
        {
          "_id": "67e623f10aaa5e9f7cf8a17f",
          "name": "Hwanjun Song",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-27T10:11:41.000Z",
      "submittedOnDailyAt": "2025-03-31T05:50:46.001Z",
      "title": "ReFeed: Multi-dimensional Summarization Refinement with Reflective\n  Reasoning on Feedback",
      "submittedOnDailyBy": {
        "_id": "65642d7401de72cb63165d22",
        "avatarUrl": "/avatars/1f4417c4ac5e781ce73eae1060e3f7f2.svg",
        "isPro": true,
        "fullname": "ytaewon",
        "user": "hamzzi",
        "type": "user"
      },
      "summary": "Summarization refinement faces challenges when extending to multi-dimension.\nIn this paper, we introduce ReFeed, a powerful summarization refinement\npipeline that enhances multiple dimensions through reflective reasoning on\nfeedback. To achieve this, we release SumFeed-CoT, a large-scale Long-CoT-based\ndataset optimized for training a lightweight model with reflective reasoning.\nOur experiments reveal how the number of dimensions, feedback exposure, and\nreasoning policy influence refinement performance, highlighting reflective\nreasoning and simultaneously addressing multiple feedback is crucial to\nmitigate trade-off between dimensions. Furthermore, ReFeed is robust to noisy\nfeedback and feedback order. Lastly, our finding emphasizes that creating data\nwith a proper goal and guideline constitutes a fundamental pillar of effective\nreasoning. The dataset and model will be released.",
      "upvotes": 1,
      "discussionId": "67e623f20aaa5e9f7cf8a1dc",
      "ai_keywords": [
        "Long-CoT-based dataset",
        "reflective reasoning",
        "refinement performance",
        "ReFeed",
        "SumFeed-CoT"
      ]
    },
    "publishedAt": "2025-03-27T06:11:41.000Z",
    "title": "ReFeed: Multi-dimensional Summarization Refinement with Reflective\n  Reasoning on Feedback",
    "summary": "Summarization refinement faces challenges when extending to multi-dimension.\nIn this paper, we introduce ReFeed, a powerful summarization refinement\npipeline that enhances multiple dimensions through reflective reasoning on\nfeedback. To achieve this, we release SumFeed-CoT, a large-scale Long-CoT-based\ndataset optimized for training a lightweight model with reflective reasoning.\nOur experiments reveal how the number of dimensions, feedback exposure, and\nreasoning policy influence refinement performance, highlighting reflective\nreasoning and simultaneously addressing multiple feedback is crucial to\nmitigate trade-off between dimensions. Furthermore, ReFeed is robust to noisy\nfeedback and feedback order. Lastly, our finding emphasizes that creating data\nwith a proper goal and guideline constitutes a fundamental pillar of effective\nreasoning. The dataset and model will be released.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21332.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "65642d7401de72cb63165d22",
      "avatarUrl": "/avatars/1f4417c4ac5e781ce73eae1060e3f7f2.svg",
      "fullname": "ytaewon",
      "name": "hamzzi",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.21851",
      "authors": [
        {
          "_id": "67ea45e0cdd38e64b1134ec3",
          "name": "Alessandro Conti",
          "hidden": false
        },
        {
          "_id": "67ea45e0cdd38e64b1134ec4",
          "name": "Massimiliano Mancini",
          "hidden": false
        },
        {
          "_id": "67ea45e0cdd38e64b1134ec5",
          "name": "Enrico Fini",
          "hidden": false
        },
        {
          "_id": "67ea45e0cdd38e64b1134ec6",
          "name": "Yiming Wang",
          "hidden": false
        },
        {
          "_id": "67ea45e0cdd38e64b1134ec7",
          "name": "Paolo Rota",
          "hidden": false
        },
        {
          "_id": "67ea45e0cdd38e64b1134ec8",
          "name": "Elisa Ricci",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-27T17:03:18.000Z",
      "submittedOnDailyAt": "2025-03-31T06:13:51.445Z",
      "title": "On Large Multimodal Models as Open-World Image Classifiers",
      "submittedOnDailyBy": {
        "_id": "633f243c13e836a0fc507388",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633f243c13e836a0fc507388/Td8TWn1q2L78sZN9098AO.jpeg",
        "isPro": false,
        "fullname": "Alessandro Conti",
        "user": "altndrr",
        "type": "user"
      },
      "summary": "Traditional image classification requires a predefined list of semantic\ncategories. In contrast, Large Multimodal Models (LMMs) can sidestep this\nrequirement by classifying images directly using natural language (e.g.,\nanswering the prompt \"What is the main object in the image?\"). Despite this\nremarkable capability, most existing studies on LMM classification performance\nare surprisingly limited in scope, often assuming a closed-world setting with a\npredefined set of categories. In this work, we address this gap by thoroughly\nevaluating LMM classification performance in a truly open-world setting. We\nfirst formalize the task and introduce an evaluation protocol, defining various\nmetrics to assess the alignment between predicted and ground truth classes. We\nthen evaluate 13 models across 10 benchmarks, encompassing prototypical,\nnon-prototypical, fine-grained, and very fine-grained classes, demonstrating\nthe challenges LMMs face in this task. Further analyses based on the proposed\nmetrics reveal the types of errors LMMs make, highlighting challenges related\nto granularity and fine-grained capabilities, showing how tailored prompting\nand reasoning can alleviate them.",
      "upvotes": 0,
      "discussionId": "67ea45e1cdd38e64b1134f35",
      "githubRepo": "https://github.com/altndrr/lmms-owc",
      "ai_keywords": [
        "Large Multimodal Models (LMMs)",
        "open-world setting",
        "evaluation protocol",
        "metrics",
        "alignment between predicted and ground truth classes",
        "prototypical",
        "non-prototypical",
        "fine-grained",
        "very fine-grained classes",
        "granularity",
        "fine-grained capabilities",
        "tailored prompting",
        "reasoning"
      ]
    },
    "publishedAt": "2025-03-27T13:03:18.000Z",
    "title": "On Large Multimodal Models as Open-World Image Classifiers",
    "summary": "Traditional image classification requires a predefined list of semantic\ncategories. In contrast, Large Multimodal Models (LMMs) can sidestep this\nrequirement by classifying images directly using natural language (e.g.,\nanswering the prompt \"What is the main object in the image?\"). Despite this\nremarkable capability, most existing studies on LMM classification performance\nare surprisingly limited in scope, often assuming a closed-world setting with a\npredefined set of categories. In this work, we address this gap by thoroughly\nevaluating LMM classification performance in a truly open-world setting. We\nfirst formalize the task and introduce an evaluation protocol, defining various\nmetrics to assess the alignment between predicted and ground truth classes. We\nthen evaluate 13 models across 10 benchmarks, encompassing prototypical,\nnon-prototypical, fine-grained, and very fine-grained classes, demonstrating\nthe challenges LMMs face in this task. Further analyses based on the proposed\nmetrics reveal the types of errors LMMs make, highlighting challenges related\nto granularity and fine-grained capabilities, showing how tailored prompting\nand reasoning can alleviate them.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21851.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "633f243c13e836a0fc507388",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633f243c13e836a0fc507388/Td8TWn1q2L78sZN9098AO.jpeg",
      "fullname": "Alessandro Conti",
      "name": "altndrr",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  }
]