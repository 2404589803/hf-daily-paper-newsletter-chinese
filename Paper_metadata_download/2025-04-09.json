[
  {
    "paper": {
      "id": "2504.06263",
      "authors": [
        {
          "_id": "67f5e3701b29460f6a087954",
          "name": "Yiying Yang",
          "hidden": false
        },
        {
          "_id": "67f5e3701b29460f6a087955",
          "user": {
            "_id": "64b914c8ace99c0723ad83a9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/udUHjj6fby82zh8LDjXhL.jpeg",
            "isPro": false,
            "fullname": "Wei Cheng",
            "user": "wchengad",
            "type": "user"
          },
          "name": "Wei Cheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-09T07:34:51.924Z",
          "hidden": false
        },
        {
          "_id": "67f5e3701b29460f6a087956",
          "user": {
            "_id": "6485b08e687d9e0c759121b0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6485b08e687d9e0c759121b0/P_9F0izrQgUfEd-VEbhg8.jpeg",
            "isPro": false,
            "fullname": "sijin",
            "user": "CH3COOK",
            "type": "user"
          },
          "name": "Sijin Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-09T07:34:48.985Z",
          "hidden": false
        },
        {
          "_id": "67f5e3701b29460f6a087957",
          "name": "Xianfang Zeng",
          "hidden": false
        },
        {
          "_id": "67f5e3701b29460f6a087958",
          "name": "Jiaxu Zhang",
          "hidden": false
        },
        {
          "_id": "67f5e3701b29460f6a087959",
          "name": "Liao Wang",
          "hidden": false
        },
        {
          "_id": "67f5e3701b29460f6a08795a",
          "name": "Gang Yu",
          "hidden": false
        },
        {
          "_id": "67f5e3701b29460f6a08795b",
          "name": "Xingjun Ma",
          "hidden": false
        },
        {
          "_id": "67f5e3701b29460f6a08795c",
          "name": "Yu-Gang Jiang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6485b08e687d9e0c759121b0/goYk4m9KxOQvIKfjUy5Rn.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/6485b08e687d9e0c759121b0/oqnAsMzmh3Vtx0OIPv5ai.gif"
      ],
      "publishedAt": "2025-04-08T17:59:49.000Z",
      "submittedOnDailyAt": "2025-04-09T01:51:00.484Z",
      "title": "OmniSVG: A Unified Scalable Vector Graphics Generation Model",
      "submittedOnDailyBy": {
        "_id": "6485b08e687d9e0c759121b0",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6485b08e687d9e0c759121b0/P_9F0izrQgUfEd-VEbhg8.jpeg",
        "isPro": false,
        "fullname": "sijin",
        "user": "CH3COOK",
        "type": "user"
      },
      "summary": "Scalable Vector Graphics (SVG) is an important image format widely adopted in\ngraphic design because of their resolution independence and editability. The\nstudy of generating high-quality SVG has continuously drawn attention from both\ndesigners and researchers in the AIGC community. However, existing methods\neither produces unstructured outputs with huge computational cost or is limited\nto generating monochrome icons of over-simplified structures. To produce\nhigh-quality and complex SVG, we propose OmniSVG, a unified framework that\nleverages pre-trained Vision-Language Models (VLMs) for end-to-end multimodal\nSVG generation. By parameterizing SVG commands and coordinates into discrete\ntokens, OmniSVG decouples structural logic from low-level geometry for\nefficient training while maintaining the expressiveness of complex SVG\nstructure. To further advance the development of SVG synthesis, we introduce\nMMSVG-2M, a multimodal dataset with two million richly annotated SVG assets,\nalong with a standardized evaluation protocol for conditional SVG generation\ntasks. Extensive experiments show that OmniSVG outperforms existing methods and\ndemonstrates its potential for integration into professional SVG design\nworkflows.",
      "upvotes": 35,
      "discussionId": "67f5e3751b29460f6a087aa7",
      "projectPage": "https://omnisvg.github.io/",
      "githubRepo": "https://github.com/OmniSVG/OmniSVG"
    },
    "publishedAt": "2025-04-08T13:59:49.000Z",
    "title": "OmniSVG: A Unified Scalable Vector Graphics Generation Model",
    "summary": "Scalable Vector Graphics (SVG) is an important image format widely adopted in\ngraphic design because of their resolution independence and editability. The\nstudy of generating high-quality SVG has continuously drawn attention from both\ndesigners and researchers in the AIGC community. However, existing methods\neither produces unstructured outputs with huge computational cost or is limited\nto generating monochrome icons of over-simplified structures. To produce\nhigh-quality and complex SVG, we propose OmniSVG, a unified framework that\nleverages pre-trained Vision-Language Models (VLMs) for end-to-end multimodal\nSVG generation. By parameterizing SVG commands and coordinates into discrete\ntokens, OmniSVG decouples structural logic from low-level geometry for\nefficient training while maintaining the expressiveness of complex SVG\nstructure. To further advance the development of SVG synthesis, we introduce\nMMSVG-2M, a multimodal dataset with two million richly annotated SVG assets,\nalong with a standardized evaluation protocol for conditional SVG generation\ntasks. Extensive experiments show that OmniSVG outperforms existing methods and\ndemonstrates its potential for integration into professional SVG design\nworkflows.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6485b08e687d9e0c759121b0/goYk4m9KxOQvIKfjUy5Rn.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/6485b08e687d9e0c759121b0/oqnAsMzmh3Vtx0OIPv5ai.gif"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.06263.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6485b08e687d9e0c759121b0",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6485b08e687d9e0c759121b0/P_9F0izrQgUfEd-VEbhg8.jpeg",
      "fullname": "sijin",
      "name": "CH3COOK",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.05979",
      "authors": [
        {
          "_id": "67f5d5416ceb820f2006d8a2",
          "name": "Sixiang Chen",
          "hidden": false
        },
        {
          "_id": "67f5d5416ceb820f2006d8a3",
          "user": {
            "_id": "63fccdac93b993a4ebd7789a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63fccdac93b993a4ebd7789a/KRx8vpdoDjsZBRw0j8Vg8.jpeg",
            "isPro": false,
            "fullname": "Jinbin Bai",
            "user": "BryanW",
            "type": "user"
          },
          "name": "Jinbin Bai",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-09T07:35:08.303Z",
          "hidden": false
        },
        {
          "_id": "67f5d5416ceb820f2006d8a4",
          "name": "Zhuoran Zhao",
          "hidden": false
        },
        {
          "_id": "67f5d5416ceb820f2006d8a5",
          "name": "Tian Ye",
          "hidden": false
        },
        {
          "_id": "67f5d5416ceb820f2006d8a6",
          "user": {
            "_id": "656724074f6ec72017754d33",
            "avatarUrl": "/avatars/e61de248f6f53719b2375077340dd033.svg",
            "isPro": false,
            "fullname": "QingyuShi",
            "user": "QingyuShi",
            "type": "user"
          },
          "name": "Qingyu Shi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-09T07:35:04.572Z",
          "hidden": false
        },
        {
          "_id": "67f5d5416ceb820f2006d8a7",
          "user": {
            "_id": "67136093d2e50f1e8c9fad52",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0q49MyGuav8lJ9CIeyLhu.png",
            "isPro": false,
            "fullname": "Donghao Zhou",
            "user": "donghao-zhou",
            "type": "user"
          },
          "name": "Donghao Zhou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-09T07:35:02.400Z",
          "hidden": false
        },
        {
          "_id": "67f5d5416ceb820f2006d8a8",
          "name": "Wenhao Chai",
          "hidden": false
        },
        {
          "_id": "67f5d5416ceb820f2006d8a9",
          "name": "Xin Lin",
          "hidden": false
        },
        {
          "_id": "67f5d5416ceb820f2006d8aa",
          "name": "Jianzong Wu",
          "hidden": false
        },
        {
          "_id": "67f5d5416ceb820f2006d8ab",
          "name": "Chao Tang",
          "hidden": false
        },
        {
          "_id": "67f5d5416ceb820f2006d8ac",
          "name": "Shilin Xu",
          "hidden": false
        },
        {
          "_id": "67f5d5416ceb820f2006d8ad",
          "name": "Tao Zhang",
          "hidden": false
        },
        {
          "_id": "67f5d5416ceb820f2006d8ae",
          "name": "Haobo Yuan",
          "hidden": false
        },
        {
          "_id": "67f5d5416ceb820f2006d8af",
          "name": "Yikang Zhou",
          "hidden": false
        },
        {
          "_id": "67f5d5416ceb820f2006d8b0",
          "name": "Wei Chow",
          "hidden": false
        },
        {
          "_id": "67f5d5416ceb820f2006d8b1",
          "name": "Linfeng Li",
          "hidden": false
        },
        {
          "_id": "67f5d5416ceb820f2006d8b2",
          "name": "Xiangtai Li",
          "hidden": false
        },
        {
          "_id": "67f5d5416ceb820f2006d8b3",
          "name": "Lei Zhu",
          "hidden": false
        },
        {
          "_id": "67f5d5416ceb820f2006d8b4",
          "name": "Lu Qi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-08T12:34:36.000Z",
      "submittedOnDailyAt": "2025-04-09T00:39:48.924Z",
      "title": "An Empirical Study of GPT-4o Image Generation Capabilities",
      "submittedOnDailyBy": {
        "_id": "63fccdac93b993a4ebd7789a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63fccdac93b993a4ebd7789a/KRx8vpdoDjsZBRw0j8Vg8.jpeg",
        "isPro": false,
        "fullname": "Jinbin Bai",
        "user": "BryanW",
        "type": "user"
      },
      "summary": "The landscape of image generation has rapidly evolved, from early GAN-based\napproaches to diffusion models and, most recently, to unified generative\narchitectures that seek to bridge understanding and generation tasks. Recent\nadvances, especially the GPT-4o, have demonstrated the feasibility of\nhigh-fidelity multimodal generation, their architectural design remains\nmysterious and unpublished. This prompts the question of whether image and text\ngeneration have already been successfully integrated into a unified framework\nfor those methods. In this work, we conduct an empirical study of GPT-4o's\nimage generation capabilities, benchmarking it against leading open-source and\ncommercial models. Our evaluation covers four main categories, including\ntext-to-image, image-to-image, image-to-3D, and image-to-X generation, with\nmore than 20 tasks. Our analysis highlights the strengths and limitations of\nGPT-4o under various settings, and situates it within the broader evolution of\ngenerative modeling. Through this investigation, we identify promising\ndirections for future unified generative models, emphasizing the role of\narchitectural design and data scaling.",
      "upvotes": 35,
      "discussionId": "67f5d5496ceb820f2006da78"
    },
    "publishedAt": "2025-04-08T08:34:36.000Z",
    "title": "An Empirical Study of GPT-4o Image Generation Capabilities",
    "summary": "The landscape of image generation has rapidly evolved, from early GAN-based\napproaches to diffusion models and, most recently, to unified generative\narchitectures that seek to bridge understanding and generation tasks. Recent\nadvances, especially the GPT-4o, have demonstrated the feasibility of\nhigh-fidelity multimodal generation, their architectural design remains\nmysterious and unpublished. This prompts the question of whether image and text\ngeneration have already been successfully integrated into a unified framework\nfor those methods. In this work, we conduct an empirical study of GPT-4o's\nimage generation capabilities, benchmarking it against leading open-source and\ncommercial models. Our evaluation covers four main categories, including\ntext-to-image, image-to-image, image-to-3D, and image-to-X generation, with\nmore than 20 tasks. Our analysis highlights the strengths and limitations of\nGPT-4o under various settings, and situates it within the broader evolution of\ngenerative modeling. Through this investigation, we identify promising\ndirections for future unified generative models, emphasizing the role of\narchitectural design and data scaling.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05979.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63fccdac93b993a4ebd7789a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63fccdac93b993a4ebd7789a/KRx8vpdoDjsZBRw0j8Vg8.jpeg",
      "fullname": "Jinbin Bai",
      "name": "BryanW",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.05599",
      "authors": [
        {
          "_id": "67f61a98af81b0685bf055cf",
          "name": "Yi Peng",
          "hidden": false
        },
        {
          "_id": "67f61a98af81b0685bf055d0",
          "name": "Chris",
          "hidden": false
        },
        {
          "_id": "67f61a98af81b0685bf055d1",
          "name": "Xiaokun Wang",
          "hidden": false
        },
        {
          "_id": "67f61a98af81b0685bf055d2",
          "name": "Yichen Wei",
          "hidden": false
        },
        {
          "_id": "67f61a98af81b0685bf055d3",
          "name": "Jiangbo Pei",
          "hidden": false
        },
        {
          "_id": "67f61a98af81b0685bf055d4",
          "name": "Weijie Qiu",
          "hidden": false
        },
        {
          "_id": "67f61a98af81b0685bf055d5",
          "name": "Ai Jian",
          "hidden": false
        },
        {
          "_id": "67f61a98af81b0685bf055d6",
          "name": "Yunzhuo Hao",
          "hidden": false
        },
        {
          "_id": "67f61a98af81b0685bf055d7",
          "name": "Jiachun Pan",
          "hidden": false
        },
        {
          "_id": "67f61a98af81b0685bf055d8",
          "name": "Tianyidan Xie",
          "hidden": false
        },
        {
          "_id": "67f61a98af81b0685bf055d9",
          "name": "Li Ge",
          "hidden": false
        },
        {
          "_id": "67f61a98af81b0685bf055da",
          "name": "Rongxian Zhuang",
          "hidden": false
        },
        {
          "_id": "67f61a98af81b0685bf055db",
          "name": "Xuchen Song",
          "hidden": false
        },
        {
          "_id": "67f61a98af81b0685bf055dc",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "67f61a98af81b0685bf055dd",
          "name": "Yahui Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-08T01:19:20.000Z",
      "submittedOnDailyAt": "2025-04-09T05:32:09.323Z",
      "title": "Skywork R1V: Pioneering Multimodal Reasoning with Chain-of-Thought",
      "submittedOnDailyBy": {
        "_id": "6462b241b438438da3c25a5d",
        "avatarUrl": "/avatars/606a67f1be639c9a5e36f293abd5f27a.svg",
        "isPro": false,
        "fullname": "Xuchen Song",
        "user": "xuchensong",
        "type": "user"
      },
      "summary": "We introduce Skywork R1V, a multimodal reasoning model extending the an\nR1-series Large language models (LLM) to visual modalities via an efficient\nmultimodal transfer method. Leveraging a lightweight visual projector, Skywork\nR1V facilitates seamless multimodal adaptation without necessitating retraining\nof either the foundational language model or the vision encoder. To strengthen\nvisual-text alignment, we propose a hybrid optimization strategy that combines\nIterative Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization\n(GRPO), significantly enhancing cross-modal integration efficiency.\nAdditionally, we introduce an adaptive-length Chain-of-Thought distillation\napproach for reasoning data generation. This approach dynamically optimizes\nreasoning chain lengths, thereby enhancing inference efficiency and preventing\nexcessive reasoning overthinking. Empirical evaluations demonstrate that\nSkywork R1V, with only 38B parameters, delivers competitive performance,\nachieving a score of 69.0 on the MMMU benchmark and 67.5 on MathVista.\nMeanwhile, it maintains robust textual reasoning performance, evidenced by\nimpressive scores of 72.0 on AIME and 94.0 on MATH500. The Skywork R1V model\nweights have been publicly released to promote openness and reproducibility.",
      "upvotes": 25,
      "discussionId": "67f61a9daf81b0685bf05731",
      "githubRepo": "https://github.com/SkyworkAI/Skywork-R1V"
    },
    "publishedAt": "2025-04-07T21:19:20.000Z",
    "title": "Skywork R1V: Pioneering Multimodal Reasoning with Chain-of-Thought",
    "summary": "We introduce Skywork R1V, a multimodal reasoning model extending the an\nR1-series Large language models (LLM) to visual modalities via an efficient\nmultimodal transfer method. Leveraging a lightweight visual projector, Skywork\nR1V facilitates seamless multimodal adaptation without necessitating retraining\nof either the foundational language model or the vision encoder. To strengthen\nvisual-text alignment, we propose a hybrid optimization strategy that combines\nIterative Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization\n(GRPO), significantly enhancing cross-modal integration efficiency.\nAdditionally, we introduce an adaptive-length Chain-of-Thought distillation\napproach for reasoning data generation. This approach dynamically optimizes\nreasoning chain lengths, thereby enhancing inference efficiency and preventing\nexcessive reasoning overthinking. Empirical evaluations demonstrate that\nSkywork R1V, with only 38B parameters, delivers competitive performance,\nachieving a score of 69.0 on the MMMU benchmark and 67.5 on MathVista.\nMeanwhile, it maintains robust textual reasoning performance, evidenced by\nimpressive scores of 72.0 on AIME and 94.0 on MATH500. The Skywork R1V model\nweights have been publicly released to promote openness and reproducibility.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05599.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6462b241b438438da3c25a5d",
      "avatarUrl": "/avatars/606a67f1be639c9a5e36f293abd5f27a.svg",
      "fullname": "Xuchen Song",
      "name": "xuchensong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.02160",
      "authors": [
        {
          "_id": "67efd1cd40e0a904109cac33",
          "user": {
            "_id": "660114b38ae190912a61be5d",
            "avatarUrl": "/avatars/abc4ab10d6f9769d2b5e697ccbf3fb70.svg",
            "isPro": false,
            "fullname": "ShaojinWu",
            "user": "fenfan",
            "type": "user"
          },
          "name": "Shaojin Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-08T06:54:08.610Z",
          "hidden": false
        },
        {
          "_id": "67efd1cd40e0a904109cac34",
          "name": "Mengqi Huang",
          "hidden": false
        },
        {
          "_id": "67efd1cd40e0a904109cac35",
          "user": {
            "_id": "635634171c93c1ef4e9eb1c2",
            "avatarUrl": "/avatars/66b31b801960612057ecfd1e26410075.svg",
            "isPro": false,
            "fullname": "wuwenxu",
            "user": "wuwx",
            "type": "user"
          },
          "name": "Wenxu Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-08T06:54:06.171Z",
          "hidden": false
        },
        {
          "_id": "67efd1cd40e0a904109cac36",
          "name": "Yufeng Cheng",
          "hidden": false
        },
        {
          "_id": "67efd1cd40e0a904109cac37",
          "name": "Fei Ding",
          "hidden": false
        },
        {
          "_id": "67efd1cd40e0a904109cac38",
          "name": "Qian He",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/660114b38ae190912a61be5d/eLRIcZxGD19KgExwoJTKW.mp4"
      ],
      "publishedAt": "2025-04-02T22:20:21.000Z",
      "submittedOnDailyAt": "2025-04-09T02:18:09.429Z",
      "title": "Less-to-More Generalization: Unlocking More Controllability by\n  In-Context Generation",
      "submittedOnDailyBy": {
        "_id": "660114b38ae190912a61be5d",
        "avatarUrl": "/avatars/abc4ab10d6f9769d2b5e697ccbf3fb70.svg",
        "isPro": false,
        "fullname": "ShaojinWu",
        "user": "fenfan",
        "type": "user"
      },
      "summary": "Although subject-driven generation has been extensively explored in image\ngeneration due to its wide applications, it still has challenges in data\nscalability and subject expansibility. For the first challenge, moving from\ncurating single-subject datasets to multiple-subject ones and scaling them is\nparticularly difficult. For the second, most recent methods center on\nsingle-subject generation, making it hard to apply when dealing with\nmulti-subject scenarios. In this study, we propose a highly-consistent data\nsynthesis pipeline to tackle this challenge. This pipeline harnesses the\nintrinsic in-context generation capabilities of diffusion transformers and\ngenerates high-consistency multi-subject paired data. Additionally, we\nintroduce UNO, which consists of progressive cross-modal alignment and\nuniversal rotary position embedding. It is a multi-image conditioned\nsubject-to-image model iteratively trained from a text-to-image model.\nExtensive experiments show that our method can achieve high consistency while\nensuring controllability in both single-subject and multi-subject driven\ngeneration.",
      "upvotes": 16,
      "discussionId": "67efd1d140e0a904109cad62",
      "projectPage": "https://bytedance.github.io/UNO/",
      "githubRepo": "https://github.com/bytedance/UNO",
      "ai_keywords": [
        "diffusion transformers",
        "in-context generation",
        "multi-subject paired data",
        "UNO",
        "progressive cross-modal alignment",
        "universal rotary position embedding",
        "multi-image conditioned",
        "subject-to-image model",
        "text-to-image model"
      ]
    },
    "publishedAt": "2025-04-02T18:20:21.000Z",
    "title": "Less-to-More Generalization: Unlocking More Controllability by\n  In-Context Generation",
    "summary": "Although subject-driven generation has been extensively explored in image\ngeneration due to its wide applications, it still has challenges in data\nscalability and subject expansibility. For the first challenge, moving from\ncurating single-subject datasets to multiple-subject ones and scaling them is\nparticularly difficult. For the second, most recent methods center on\nsingle-subject generation, making it hard to apply when dealing with\nmulti-subject scenarios. In this study, we propose a highly-consistent data\nsynthesis pipeline to tackle this challenge. This pipeline harnesses the\nintrinsic in-context generation capabilities of diffusion transformers and\ngenerates high-consistency multi-subject paired data. Additionally, we\nintroduce UNO, which consists of progressive cross-modal alignment and\nuniversal rotary position embedding. It is a multi-image conditioned\nsubject-to-image model iteratively trained from a text-to-image model.\nExtensive experiments show that our method can achieve high consistency while\nensuring controllability in both single-subject and multi-subject driven\ngeneration.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/660114b38ae190912a61be5d/eLRIcZxGD19KgExwoJTKW.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02160.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "660114b38ae190912a61be5d",
      "avatarUrl": "/avatars/abc4ab10d6f9769d2b5e697ccbf3fb70.svg",
      "fullname": "ShaojinWu",
      "name": "fenfan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.02810",
      "authors": [
        {
          "_id": "67f099de103cb604facd26cd",
          "user": {
            "_id": "63453f02a05b51f7ded3c579",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/u7p-wTj8zyeWpSFRI8Go0.png",
            "isPro": false,
            "fullname": "Andy Lin",
            "user": "pkuHaowei",
            "type": "user"
          },
          "name": "Haowei Lin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-06T08:11:10.909Z",
          "hidden": false
        },
        {
          "_id": "67f099de103cb604facd26ce",
          "name": "Xiangyu Wang",
          "hidden": false
        },
        {
          "_id": "67f099de103cb604facd26cf",
          "name": "Ruilin Yan",
          "hidden": false
        },
        {
          "_id": "67f099de103cb604facd26d0",
          "name": "Baizhou Huang",
          "hidden": false
        },
        {
          "_id": "67f099de103cb604facd26d1",
          "name": "Haotian Ye",
          "hidden": false
        },
        {
          "_id": "67f099de103cb604facd26d2",
          "name": "Jianhua Zhu",
          "hidden": false
        },
        {
          "_id": "67f099de103cb604facd26d3",
          "name": "Zihao Wang",
          "hidden": false
        },
        {
          "_id": "67f099de103cb604facd26d4",
          "name": "James Zou",
          "hidden": false
        },
        {
          "_id": "67f099de103cb604facd26d5",
          "name": "Jianzhu Ma",
          "hidden": false
        },
        {
          "_id": "67f099de103cb604facd26d6",
          "user": {
            "_id": "64683a5776bb704aa14588b7",
            "avatarUrl": "/avatars/e532756f52c5b95981470ace41a10556.svg",
            "isPro": false,
            "fullname": "Yitao Liang",
            "user": "YitaoLiang",
            "type": "user"
          },
          "name": "Yitao Liang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-09T07:38:09.311Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-03T17:54:18.000Z",
      "submittedOnDailyAt": "2025-04-09T01:08:45.803Z",
      "title": "Generative Evaluation of Complex Reasoning in Large Language Models",
      "submittedOnDailyBy": {
        "_id": "63453f02a05b51f7ded3c579",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/u7p-wTj8zyeWpSFRI8Go0.png",
        "isPro": false,
        "fullname": "Andy Lin",
        "user": "pkuHaowei",
        "type": "user"
      },
      "summary": "With powerful large language models (LLMs) demonstrating superhuman reasoning\ncapabilities, a critical question arises: Do LLMs genuinely reason, or do they\nmerely recall answers from their extensive, web-scraped training datasets?\nPublicly released benchmarks inevitably become contaminated once incorporated\ninto subsequent LLM training sets, undermining their reliability as faithful\nassessments. To address this, we introduce KUMO, a generative evaluation\nframework designed specifically for assessing reasoning in LLMs. KUMO\nsynergistically combines LLMs with symbolic engines to dynamically produce\ndiverse, multi-turn reasoning tasks that are partially observable and\nadjustable in difficulty. Through an automated pipeline, KUMO continuously\ngenerates novel tasks across open-ended domains, compelling models to\ndemonstrate genuine generalization rather than memorization. We evaluated 23\nstate-of-the-art LLMs on 5,000 tasks across 100 domains created by KUMO,\nbenchmarking their reasoning abilities against university students. Our\nfindings reveal that many LLMs have outperformed university-level performance\non easy reasoning tasks, and reasoning-scaled LLMs reach university-level\nperformance on complex reasoning challenges. Moreover, LLM performance on KUMO\ntasks correlates strongly with results on newly released real-world reasoning\nbenchmarks, underscoring KUMO's value as a robust, enduring assessment tool for\ngenuine LLM reasoning capabilities.",
      "upvotes": 8,
      "discussionId": "67f099e1103cb604facd280e",
      "githubRepo": "https://github.com/linhaowei1/kumo",
      "ai_keywords": [
        "large language models (LLMs)",
        "superhuman reasoning capabilities",
        "web-scraped training datasets",
        "generative evaluation framework",
        "symbolic engines",
        "multi-turn reasoning tasks",
        "partially observable",
        "adjustable in difficulty",
        "automated pipeline",
        "open-ended domains",
        "genuine generalization",
        "memorization",
        "reasoning abilities",
        "reasoning-scaled LLMs",
        "university-level performance",
        "complex reasoning challenges",
        "real-world reasoning benchmarks"
      ]
    },
    "publishedAt": "2025-04-03T13:54:18.000Z",
    "title": "Generative Evaluation of Complex Reasoning in Large Language Models",
    "summary": "With powerful large language models (LLMs) demonstrating superhuman reasoning\ncapabilities, a critical question arises: Do LLMs genuinely reason, or do they\nmerely recall answers from their extensive, web-scraped training datasets?\nPublicly released benchmarks inevitably become contaminated once incorporated\ninto subsequent LLM training sets, undermining their reliability as faithful\nassessments. To address this, we introduce KUMO, a generative evaluation\nframework designed specifically for assessing reasoning in LLMs. KUMO\nsynergistically combines LLMs with symbolic engines to dynamically produce\ndiverse, multi-turn reasoning tasks that are partially observable and\nadjustable in difficulty. Through an automated pipeline, KUMO continuously\ngenerates novel tasks across open-ended domains, compelling models to\ndemonstrate genuine generalization rather than memorization. We evaluated 23\nstate-of-the-art LLMs on 5,000 tasks across 100 domains created by KUMO,\nbenchmarking their reasoning abilities against university students. Our\nfindings reveal that many LLMs have outperformed university-level performance\non easy reasoning tasks, and reasoning-scaled LLMs reach university-level\nperformance on complex reasoning challenges. Moreover, LLM performance on KUMO\ntasks correlates strongly with results on newly released real-world reasoning\nbenchmarks, underscoring KUMO's value as a robust, enduring assessment tool for\ngenuine LLM reasoning capabilities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02810.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "63453f02a05b51f7ded3c579",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/u7p-wTj8zyeWpSFRI8Go0.png",
      "fullname": "Andy Lin",
      "name": "pkuHaowei",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.05594",
      "authors": [
        {
          "_id": "67f5dc86015730c161ce291b",
          "name": "Qi Mao",
          "hidden": false
        },
        {
          "_id": "67f5dc86015730c161ce291c",
          "name": "Lan Chen",
          "hidden": false
        },
        {
          "_id": "67f5dc86015730c161ce291d",
          "name": "Yuchao Gu",
          "hidden": false
        },
        {
          "_id": "67f5dc86015730c161ce291e",
          "name": "Mike Zheng Shou",
          "hidden": false
        },
        {
          "_id": "67f5dc86015730c161ce291f",
          "name": "Ming-Hsuan Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-08T01:02:50.000Z",
      "submittedOnDailyAt": "2025-04-09T01:06:41.710Z",
      "title": "Tuning-Free Image Editing with Fidelity and Editability via Unified\n  Latent Diffusion Model",
      "submittedOnDailyBy": {
        "_id": "640d704c8036cc2142299c19",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640d704c8036cc2142299c19/Wt9AslcVxWOSSc11epk8l.jpeg",
        "isPro": false,
        "fullname": "Lan Chen",
        "user": "Orannue",
        "type": "user"
      },
      "summary": "Balancing fidelity and editability is essential in text-based image editing\n(TIE), where failures commonly lead to over- or under-editing issues. Existing\nmethods typically rely on attention injections for structure preservation and\nleverage the inherent text alignment capabilities of pre-trained text-to-image\n(T2I) models for editability, but they lack explicit and unified mechanisms to\nproperly balance these two objectives. In this work, we introduce UnifyEdit, a\ntuning-free method that performs diffusion latent optimization to enable a\nbalanced integration of fidelity and editability within a unified framework.\nUnlike direct attention injections, we develop two attention-based constraints:\na self-attention (SA) preservation constraint for structural fidelity, and a\ncross-attention (CA) alignment constraint to enhance text alignment for\nimproved editability. However, simultaneously applying both constraints can\nlead to gradient conflicts, where the dominance of one constraint results in\nover- or under-editing. To address this challenge, we introduce an adaptive\ntime-step scheduler that dynamically adjusts the influence of these\nconstraints, guiding the diffusion latent toward an optimal balance. Extensive\nquantitative and qualitative experiments validate the effectiveness of our\napproach, demonstrating its superiority in achieving a robust balance between\nstructure preservation and text alignment across various editing tasks,\noutperforming other state-of-the-art methods. The source code will be available\nat https://github.com/CUC-MIPG/UnifyEdit.",
      "upvotes": 5,
      "discussionId": "67f5dc89015730c161ce2a50"
    },
    "publishedAt": "2025-04-07T21:02:50.000Z",
    "title": "Tuning-Free Image Editing with Fidelity and Editability via Unified\n  Latent Diffusion Model",
    "summary": "Balancing fidelity and editability is essential in text-based image editing\n(TIE), where failures commonly lead to over- or under-editing issues. Existing\nmethods typically rely on attention injections for structure preservation and\nleverage the inherent text alignment capabilities of pre-trained text-to-image\n(T2I) models for editability, but they lack explicit and unified mechanisms to\nproperly balance these two objectives. In this work, we introduce UnifyEdit, a\ntuning-free method that performs diffusion latent optimization to enable a\nbalanced integration of fidelity and editability within a unified framework.\nUnlike direct attention injections, we develop two attention-based constraints:\na self-attention (SA) preservation constraint for structural fidelity, and a\ncross-attention (CA) alignment constraint to enhance text alignment for\nimproved editability. However, simultaneously applying both constraints can\nlead to gradient conflicts, where the dominance of one constraint results in\nover- or under-editing. To address this challenge, we introduce an adaptive\ntime-step scheduler that dynamically adjusts the influence of these\nconstraints, guiding the diffusion latent toward an optimal balance. Extensive\nquantitative and qualitative experiments validate the effectiveness of our\napproach, demonstrating its superiority in achieving a robust balance between\nstructure preservation and text alignment across various editing tasks,\noutperforming other state-of-the-art methods. The source code will be available\nat https://github.com/CUC-MIPG/UnifyEdit.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05594.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "640d704c8036cc2142299c19",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640d704c8036cc2142299c19/Wt9AslcVxWOSSc11epk8l.jpeg",
      "fullname": "Lan Chen",
      "name": "Orannue",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.00043",
      "authors": [
        {
          "_id": "67ec9d4ad327ed17ec707488",
          "name": "Jixuan Leng",
          "hidden": false
        },
        {
          "_id": "67ec9d4ad327ed17ec707489",
          "name": "Chengsong Huang",
          "hidden": false
        },
        {
          "_id": "67ec9d4ad327ed17ec70748a",
          "name": "Langlin Huang",
          "hidden": false
        },
        {
          "_id": "67ec9d4ad327ed17ec70748b",
          "name": "Bill Yuchen Lin",
          "hidden": false
        },
        {
          "_id": "67ec9d4ad327ed17ec70748c",
          "name": "William W. Cohen",
          "hidden": false
        },
        {
          "_id": "67ec9d4ad327ed17ec70748d",
          "name": "Haohan Wang",
          "hidden": false
        },
        {
          "_id": "67ec9d4ad327ed17ec70748e",
          "name": "Jiaxin Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-30T20:03:36.000Z",
      "submittedOnDailyAt": "2025-04-09T00:47:44.460Z",
      "title": "CrossWordBench: Evaluating the Reasoning Capabilities of LLMs and LVLMs\n  with Controllable Puzzle Generation",
      "submittedOnDailyBy": {
        "_id": "64efbf39b3610349e84db417",
        "avatarUrl": "/avatars/9e09a20e88f8cf5ce119efc0dadc3b7b.svg",
        "isPro": false,
        "fullname": "Jiaxin Huang",
        "user": "teapot123",
        "type": "user"
      },
      "summary": "Existing reasoning evaluation frameworks for Large Language Models (LLMs) and\nLarge Vision-Language Models (LVLMs) predominantly either assess text-based\nreasoning or vision-language understanding capabilities, with limited dynamic\ninterplay between textual and visual constraints. To address this limitation,\nwe introduce CrossWordBench, a benchmark designed to evaluate the reasoning\ncapabilities of both LLMs and LVLMs through the medium of crossword puzzles-a\ntask requiring multimodal adherence to semantic constraints from text-based\nclues and intersectional constraints from visual grid structures.\nCrossWordBench leverages a controllable puzzle generation framework that\nproduces puzzles in multiple formats (text and image) and offers different\nevaluation strategies ranging from direct puzzle solving to interactive modes.\nOur extensive evaluation of over 20 models reveals that reasoning LLMs\noutperform non-reasoning models substantially by effectively leveraging\ncrossing-letter constraints. We further demonstrate that LVLMs struggle with\nthe task, showing a strong correlation between their puzzle-solving performance\nand grid-parsing accuracy. Our findings offer insights into the limitations of\nthe reasoning capabilities of current LLMs and LVLMs, and provide an effective\napproach for creating multimodal constrained tasks for future evaluations.",
      "upvotes": 5,
      "discussionId": "67ec9d4fd327ed17ec707598",
      "ai_keywords": [
        "CrossWordBench",
        "multimodal adherence",
        "semantic constraints",
        "intersectional constraints",
        "controllable puzzle generation framework",
        "direct puzzle solving",
        "interactive modes",
        "reasoning LLMs",
        "non-reasoning models",
        "crossing-letter constraints",
        "grid-parsing accuracy",
        "multimodal constrained tasks"
      ]
    },
    "publishedAt": "2025-03-30T16:03:36.000Z",
    "title": "CrossWordBench: Evaluating the Reasoning Capabilities of LLMs and LVLMs\n  with Controllable Puzzle Generation",
    "summary": "Existing reasoning evaluation frameworks for Large Language Models (LLMs) and\nLarge Vision-Language Models (LVLMs) predominantly either assess text-based\nreasoning or vision-language understanding capabilities, with limited dynamic\ninterplay between textual and visual constraints. To address this limitation,\nwe introduce CrossWordBench, a benchmark designed to evaluate the reasoning\ncapabilities of both LLMs and LVLMs through the medium of crossword puzzles-a\ntask requiring multimodal adherence to semantic constraints from text-based\nclues and intersectional constraints from visual grid structures.\nCrossWordBench leverages a controllable puzzle generation framework that\nproduces puzzles in multiple formats (text and image) and offers different\nevaluation strategies ranging from direct puzzle solving to interactive modes.\nOur extensive evaluation of over 20 models reveals that reasoning LLMs\noutperform non-reasoning models substantially by effectively leveraging\ncrossing-letter constraints. We further demonstrate that LVLMs struggle with\nthe task, showing a strong correlation between their puzzle-solving performance\nand grid-parsing accuracy. Our findings offer insights into the limitations of\nthe reasoning capabilities of current LLMs and LVLMs, and provide an effective\napproach for creating multimodal constrained tasks for future evaluations.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00043.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64efbf39b3610349e84db417",
      "avatarUrl": "/avatars/9e09a20e88f8cf5ce119efc0dadc3b7b.svg",
      "fullname": "Jiaxin Huang",
      "name": "teapot123",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.06261",
      "authors": [
        {
          "_id": "67f60df2d0df7eccaae93eb0",
          "name": "Gleb Rodionov",
          "hidden": false
        },
        {
          "_id": "67f60df2d0df7eccaae93eb1",
          "name": "Roman Garipov",
          "hidden": false
        },
        {
          "_id": "67f60df2d0df7eccaae93eb2",
          "name": "Alina Shutova",
          "hidden": false
        },
        {
          "_id": "67f60df2d0df7eccaae93eb3",
          "name": "George Yakushev",
          "hidden": false
        },
        {
          "_id": "67f60df2d0df7eccaae93eb4",
          "name": "Vage Egiazarian",
          "hidden": false
        },
        {
          "_id": "67f60df2d0df7eccaae93eb5",
          "name": "Anton Sinitsin",
          "hidden": false
        },
        {
          "_id": "67f60df2d0df7eccaae93eb6",
          "name": "Denis Kuznedelev",
          "hidden": false
        },
        {
          "_id": "67f60df2d0df7eccaae93eb7",
          "name": "Dan Alistarh",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64ef52c2718f94ae8e78a5e7/79i2ecMWxf7tpYS3hvRP4.qt"
      ],
      "publishedAt": "2025-04-08T17:59:41.000Z",
      "submittedOnDailyAt": "2025-04-09T04:36:08.129Z",
      "title": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention",
      "submittedOnDailyBy": {
        "_id": "64ef52c2718f94ae8e78a5e7",
        "avatarUrl": "/avatars/d169f4ee62786a3eb4a3fa9d1fec52e9.svg",
        "isPro": false,
        "fullname": "Alistarh",
        "user": "d-alistarh",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) have demonstrated the ability to tackle\nincreasingly complex tasks through advanced reasoning, long-form content\ngeneration, and tool use. Solving these tasks often involves long\ninference-time computations. In human problem solving, a common strategy to\nexpedite work is collaboration: by dividing the problem into sub-tasks,\nexploring different strategies concurrently, etc. Recent research has shown\nthat LLMs can also operate in parallel by implementing explicit cooperation\nframeworks, such as voting mechanisms or the explicit creation of independent\nsub-tasks that can be executed in parallel. However, each of these frameworks\nmay not be suitable for all types of tasks, which can hinder their\napplicability. In this work, we propose a different design approach: we run LLM\n\"workers\" in parallel , allowing them to synchronize via a concurrently-updated\nattention cache and prompt these workers to decide how best to collaborate. Our\napproach allows the instances to come up with their own collaboration strategy\nfor the problem at hand, all the while \"seeing\" each other's partial progress\nin the concurrent cache. We implement this approach via Hogwild! Inference: a\nparallel LLM inference engine where multiple instances of the same LLM run in\nparallel with the same attention cache, with \"instant\" access to each other's\ngenerated tokens. Hogwild! inference takes advantage of Rotary Position\nEmbeddings (RoPE) to avoid recomputation while improving parallel hardware\nutilization. We find that modern reasoning-capable LLMs can perform inference\nwith shared Key-Value cache out of the box, without additional fine-tuning.",
      "upvotes": 3,
      "discussionId": "67f60df3d0df7eccaae93eff"
    },
    "publishedAt": "2025-04-08T13:59:41.000Z",
    "title": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention",
    "summary": "Large Language Models (LLMs) have demonstrated the ability to tackle\nincreasingly complex tasks through advanced reasoning, long-form content\ngeneration, and tool use. Solving these tasks often involves long\ninference-time computations. In human problem solving, a common strategy to\nexpedite work is collaboration: by dividing the problem into sub-tasks,\nexploring different strategies concurrently, etc. Recent research has shown\nthat LLMs can also operate in parallel by implementing explicit cooperation\nframeworks, such as voting mechanisms or the explicit creation of independent\nsub-tasks that can be executed in parallel. However, each of these frameworks\nmay not be suitable for all types of tasks, which can hinder their\napplicability. In this work, we propose a different design approach: we run LLM\n\"workers\" in parallel , allowing them to synchronize via a concurrently-updated\nattention cache and prompt these workers to decide how best to collaborate. Our\napproach allows the instances to come up with their own collaboration strategy\nfor the problem at hand, all the while \"seeing\" each other's partial progress\nin the concurrent cache. We implement this approach via Hogwild! Inference: a\nparallel LLM inference engine where multiple instances of the same LLM run in\nparallel with the same attention cache, with \"instant\" access to each other's\ngenerated tokens. Hogwild! inference takes advantage of Rotary Position\nEmbeddings (RoPE) to avoid recomputation while improving parallel hardware\nutilization. We find that modern reasoning-capable LLMs can perform inference\nwith shared Key-Value cache out of the box, without additional fine-tuning.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64ef52c2718f94ae8e78a5e7/79i2ecMWxf7tpYS3hvRP4.qt"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.06261.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ef52c2718f94ae8e78a5e7",
      "avatarUrl": "/avatars/d169f4ee62786a3eb4a3fa9d1fec52e9.svg",
      "fullname": "Alistarh",
      "name": "d-alistarh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  }
]