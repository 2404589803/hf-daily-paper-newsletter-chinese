[
    {
        "paper": {
            "id": "2408.17253",
            "authors": [
                {
                    "_id": "66d58e8c35c36f266f727bf9",
                    "user": {
                        "_id": "66d58df54b87a685ccb8e4a0",
                        "avatarUrl": "/avatars/2566c20d79088ba761215b9a0197cb8e.svg",
                        "isPro": false,
                        "fullname": "Mouxiang Chen",
                        "user": "chenmouxiang",
                        "type": "user"
                    },
                    "name": "Mouxiang Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-09-03T07:47:41.241Z",
                    "hidden": false
                },
                {
                    "_id": "66d58e8c35c36f266f727bfa",
                    "user": {
                        "_id": "6609227c57b373706917a4ef",
                        "avatarUrl": "/avatars/6d9ea29db1733a7f3a5aa5e534573d7e.svg",
                        "isPro": false,
                        "fullname": "Lefei Shen",
                        "user": "HALF111",
                        "type": "user"
                    },
                    "name": "Lefei Shen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-09-03T08:28:01.595Z",
                    "hidden": false
                },
                {
                    "_id": "66d58e8c35c36f266f727bfb",
                    "name": "Zhuo Li",
                    "hidden": false
                },
                {
                    "_id": "66d58e8c35c36f266f727bfc",
                    "name": "Xiaoyun Joy Wang",
                    "hidden": false
                },
                {
                    "_id": "66d58e8c35c36f266f727bfd",
                    "name": "Jianling Sun",
                    "hidden": false
                },
                {
                    "_id": "66d58e8c35c36f266f727bfe",
                    "user": {
                        "_id": "64feae72a39388f54fe33d2d",
                        "avatarUrl": "/avatars/965691697ef2ed2abed274ca51f9d976.svg",
                        "isPro": false,
                        "fullname": "chenghao liu",
                        "user": "twinsken",
                        "type": "user"
                    },
                    "name": "Chenghao Liu",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2024-09-02T10:55:25.201Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-08-30T12:51:55.000Z",
            "title": "VisionTS: Visual Masked Autoencoders Are Free-Lunch Zero-Shot Time\n  Series Forecasters",
            "summary": "Foundation models have emerged as a promising approach in time series\nforecasting (TSF). Existing approaches either fine-tune large language models\n(LLMs) or build large-scale time-series datasets to develop TSF foundation\nmodels. However, these methods face challenges due to the severe cross-domain\ngap or in-domain heterogeneity. In this paper, we explore a new road to\nbuilding a TSF foundation model from rich and high-quality natural images,\nbased on the intrinsic similarities between images and time series. To bridge\nthe gap between the two domains, we reformulate the TSF task as an image\nreconstruction task, which is further processed by a visual masked autoencoder\n(MAE) self-supervised pre-trained on the ImageNet dataset. Surprisingly,\nwithout further adaptation in the time-series domain, the proposed VisionTS\ncould achieve superior zero-shot forecasting performance compared to existing\nTSF foundation models. With minimal fine-tuning, VisionTS could further improve\nthe forecasting and achieve state-of-the-art performance in most cases. These\nfindings suggest that visual models could be a free lunch for TSF and highlight\nthe potential for future cross-domain research between computer vision and TSF.\nOur code is publicly available at https://github.com/Keytoyze/VisionTS.",
            "upvotes": 22,
            "discussionId": "66d58e8d35c36f266f727c2e"
        },
        "publishedAt": "2024-09-03T00:53:23.921Z",
        "title": "VisionTS: Visual Masked Autoencoders Are Free-Lunch Zero-Shot Time Series Forecasters",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64feae72a39388f54fe33d2d/q-_ONCfFwK8ajba67QDZM.png",
            "https://cdn-uploads.huggingface.co/production/uploads/64feae72a39388f54fe33d2d/uCdL2KqgR_H1uv2xg67IC.png",
            "https://cdn-uploads.huggingface.co/production/uploads/64feae72a39388f54fe33d2d/f0nLcnOhfxFZxIWVPLAF4.png",
            "https://cdn-uploads.huggingface.co/production/uploads/64feae72a39388f54fe33d2d/ow-woZZlrWSLVz19WvxwS.png",
            "https://cdn-uploads.huggingface.co/production/uploads/64feae72a39388f54fe33d2d/MUbMXDecuIxZ016MVkTnZ.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.17253.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/965691697ef2ed2abed274ca51f9d976.svg",
            "fullname": "chenghao liu",
            "name": "twinsken",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2408.16725",
            "authors": [
                {
                    "_id": "66d138638a438492b07c2e7f",
                    "user": {
                        "_id": "66c5d81a4061fd5907443787",
                        "avatarUrl": "/avatars/2e107195b1ff7d06bbc6c9bd4e5620cf.svg",
                        "isPro": false,
                        "fullname": "zhifei",
                        "user": "filicos",
                        "type": "user"
                    },
                    "name": "Zhifei Xie",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-09-03T14:49:03.214Z",
                    "hidden": false
                },
                {
                    "_id": "66d138638a438492b07c2e80",
                    "user": {
                        "_id": "66cdb20c6aee4c865fe9647e",
                        "avatarUrl": "/avatars/d3637e8278c4b14d0f03ebfbfd3d3711.svg",
                        "isPro": false,
                        "fullname": "gpt-omni",
                        "user": "gpt-omni",
                        "type": "user"
                    },
                    "name": "Changqiao Wu",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2024-08-30T03:11:31.786Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-08-29T17:18:53.000Z",
            "title": "Mini-Omni: Language Models Can Hear, Talk While Thinking in Streaming",
            "summary": "Recent advances in language models have achieved significant progress.\nGPT-4o, as a new milestone, has enabled real-time conversations with humans,\ndemonstrating near-human natural fluency. Such human-computer interaction\nnecessitates models with the capability to perform reasoning directly with the\naudio modality and generate output in streaming. However, this remains beyond\nthe reach of current academic models, as they typically depend on extra TTS\nsystems for speech synthesis, resulting in undesirable latency. This paper\nintroduces the Mini-Omni, an audio-based end-to-end conversational model,\ncapable of real-time speech interaction. To achieve this capability, we propose\na text-instructed speech generation method, along with batch-parallel\nstrategies during inference to further boost the performance. Our method also\nhelps to retain the original model's language capabilities with minimal\ndegradation, enabling other works to establish real-time interaction\ncapabilities. We call this training method \"Any Model Can Talk\". We also\nintroduce the VoiceAssistant-400K dataset to fine-tune models optimized for\nspeech output. To our best knowledge, Mini-Omni is the first fully end-to-end,\nopen-source model for real-time speech interaction, offering valuable potential\nfor future research.",
            "upvotes": 11,
            "discussionId": "66d138638a438492b07c2ebc"
        },
        "publishedAt": "2024-09-03T06:16:56.314Z",
        "title": "Mini-Omni: Language Models Can Hear, Talk While Thinking in Streaming",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.16725.png",
        "numComments": 4,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6032802e1f993496bc14d9e3/w6hr-DEQot4VVkoyRIBiy.png",
            "fullname": "Omar Sanseviero",
            "name": "osanseviero",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    }
]