[
  {
    "paper": {
      "id": "2506.23044",
      "authors": [
        {
          "_id": "686347cd588cea0da970c87a",
          "name": "Guo-Hua Wang",
          "hidden": false
        },
        {
          "_id": "686347cd588cea0da970c87b",
          "name": "Shanshan Zhao",
          "hidden": false
        },
        {
          "_id": "686347cd588cea0da970c87c",
          "name": "Xinjie Zhang",
          "hidden": false
        },
        {
          "_id": "686347cd588cea0da970c87d",
          "name": "Liangfu Cao",
          "hidden": false
        },
        {
          "_id": "686347cd588cea0da970c87e",
          "name": "Pengxin Zhan",
          "hidden": false
        },
        {
          "_id": "686347cd588cea0da970c87f",
          "name": "Lunhao Duan",
          "hidden": false
        },
        {
          "_id": "686347cd588cea0da970c880",
          "name": "Shiyin Lu",
          "hidden": false
        },
        {
          "_id": "686347cd588cea0da970c881",
          "name": "Minghao Fu",
          "hidden": false
        },
        {
          "_id": "686347cd588cea0da970c882",
          "name": "Xiaohao Chen",
          "hidden": false
        },
        {
          "_id": "686347cd588cea0da970c883",
          "name": "Jianshan Zhao",
          "hidden": false
        },
        {
          "_id": "686347cd588cea0da970c884",
          "name": "Yang Li",
          "hidden": false
        },
        {
          "_id": "686347cd588cea0da970c885",
          "name": "Qing-Guo Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-29T00:40:17.000Z",
      "submittedOnDailyAt": "2025-07-01T04:30:09.856Z",
      "title": "Ovis-U1 Technical Report",
      "submittedOnDailyBy": {
        "_id": "636f4c6b5d2050767e4a1491",
        "avatarUrl": "/avatars/630c2ae12937fdb16ccd3280bc05729d.svg",
        "isPro": false,
        "fullname": "Guo-Hua Wang",
        "user": "Flourish",
        "type": "user"
      },
      "summary": "In this report, we introduce Ovis-U1, a 3-billion-parameter unified model\nthat integrates multimodal understanding, text-to-image generation, and image\nediting capabilities. Building on the foundation of the Ovis series, Ovis-U1\nincorporates a diffusion-based visual decoder paired with a bidirectional token\nrefiner, enabling image generation tasks comparable to leading models like\nGPT-4o. Unlike some previous models that use a frozen MLLM for generation\ntasks, Ovis-U1 utilizes a new unified training approach starting from a\nlanguage model. Compared to training solely on understanding or generation\ntasks, unified training yields better performance, demonstrating the\nenhancement achieved by integrating these two tasks. Ovis-U1 achieves a score\nof 69.6 on the OpenCompass Multi-modal Academic Benchmark, surpassing recent\nstate-of-the-art models such as Ristretto-3B and SAIL-VL-1.5-2B. In\ntext-to-image generation, it excels with scores of 83.72 and 0.89 on the\nDPG-Bench and GenEval benchmarks, respectively. For image editing, it achieves\n4.00 and 6.42 on the ImgEdit-Bench and GEdit-Bench-EN, respectively. As the\ninitial version of the Ovis unified model series, Ovis-U1 pushes the boundaries\nof multimodal understanding, generation, and editing.",
      "upvotes": 26,
      "discussionId": "686347cd588cea0da970c886",
      "githubRepo": "https://github.com/AIDC-AI/Ovis-U1",
      "ai_summary": "Ovis-U1, a 3-billion-parameter model, combines multimodal understanding, text-to-image generation, and image editing, achieving state-of-the-art performance in various benchmarks.",
      "ai_keywords": [
        "diffusion-based visual decoder",
        "bidirectional token refiner",
        "unified training",
        "OpenCompass",
        "DPG-Bench",
        "GenEval",
        "ImgEdit-Bench",
        "GEdit-Bench-EN"
      ],
      "githubStars": 120
    },
    "publishedAt": "2025-06-28T20:40:17.000Z",
    "title": "Ovis-U1 Technical Report",
    "summary": "In this report, we introduce Ovis-U1, a 3-billion-parameter unified model\nthat integrates multimodal understanding, text-to-image generation, and image\nediting capabilities. Building on the foundation of the Ovis series, Ovis-U1\nincorporates a diffusion-based visual decoder paired with a bidirectional token\nrefiner, enabling image generation tasks comparable to leading models like\nGPT-4o. Unlike some previous models that use a frozen MLLM for generation\ntasks, Ovis-U1 utilizes a new unified training approach starting from a\nlanguage model. Compared to training solely on understanding or generation\ntasks, unified training yields better performance, demonstrating the\nenhancement achieved by integrating these two tasks. Ovis-U1 achieves a score\nof 69.6 on the OpenCompass Multi-modal Academic Benchmark, surpassing recent\nstate-of-the-art models such as Ristretto-3B and SAIL-VL-1.5-2B. In\ntext-to-image generation, it excels with scores of 83.72 and 0.89 on the\nDPG-Bench and GenEval benchmarks, respectively. For image editing, it achieves\n4.00 and 6.42 on the ImgEdit-Bench and GEdit-Bench-EN, respectively. As the\ninitial version of the Ovis unified model series, Ovis-U1 pushes the boundaries\nof multimodal understanding, generation, and editing.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.23044.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "636f4c6b5d2050767e4a1491",
      "avatarUrl": "/avatars/630c2ae12937fdb16ccd3280bc05729d.svg",
      "fullname": "Guo-Hua Wang",
      "name": "Flourish",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.23858",
      "authors": [
        {
          "_id": "686347d3588cea0da970c888",
          "name": "Jianzong Wu",
          "hidden": false
        },
        {
          "_id": "686347d3588cea0da970c889",
          "name": "Liang Hou",
          "hidden": false
        },
        {
          "_id": "686347d3588cea0da970c88a",
          "name": "Haotian Yang",
          "hidden": false
        },
        {
          "_id": "686347d3588cea0da970c88b",
          "name": "Xin Tao",
          "hidden": false
        },
        {
          "_id": "686347d3588cea0da970c88c",
          "name": "Ye Tian",
          "hidden": false
        },
        {
          "_id": "686347d3588cea0da970c88d",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "686347d3588cea0da970c88e",
          "name": "Di Zhang",
          "hidden": false
        },
        {
          "_id": "686347d3588cea0da970c88f",
          "name": "Yunhai Tong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-30T13:52:31.000Z",
      "submittedOnDailyAt": "2025-07-01T00:59:37.837Z",
      "title": "VMoBA: Mixture-of-Block Attention for Video Diffusion Models",
      "submittedOnDailyBy": {
        "_id": "657a6eed1ccc3c2a5ea7b585",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/RIQIF-JJdNI0SwJEq_9z7.jpeg",
        "isPro": true,
        "fullname": "Jianzong Wu",
        "user": "jianzongwu",
        "type": "user"
      },
      "summary": "The quadratic complexity of full attention mechanisms poses a significant\nbottleneck for Video Diffusion Models (VDMs) aiming to generate long-duration,\nhigh-resolution videos. While various sparse attention methods have been\nproposed, many are designed as training-free inference accelerators or do not\noptimally capture the unique spatio-temporal characteristics inherent in video\ndata when trained natively. This paper introduces Video Mixture of Block\nAttention (VMoBA), a novel sparse attention mechanism specifically adapted for\nVDMs. Motivated by an in-depth analysis of attention patterns within\npre-trained video transformers, which revealed strong spatio-temporal locality,\nvarying query importance, and head-specific concentration levels, VMoBA\nenhances the original MoBA framework with three key modifications: (1) a\nlayer-wise recurrent block partition scheme (1D-2D-3D) to dynamically adapt to\ndiverse spatio-temporal attention patterns and improve efficiency; (2) global\nblock selection to prioritize the most salient query-key block interactions\nacross an entire attention head; and (3) threshold-based block selection to\ndynamically determine the number of attended blocks based on their cumulative\nsimilarity. Extensive experiments demonstrate that VMoBA significantly\naccelerates the training of VDMs on longer sequences, achieving 2.92x FLOPs and\n1.48x latency speedup, while attaining comparable or even superior generation\nquality to full attention. Furthermore, VMoBA exhibits competitive performance\nin training-free inference, offering 2.40x FLOPs and 1.35x latency speedup for\nhigh-res video generation.",
      "upvotes": 22,
      "discussionId": "686347d3588cea0da970c890",
      "projectPage": "https://github.com/KwaiVGI/VMoBA",
      "githubRepo": "https://github.com/KwaiVGI/VMoBA",
      "ai_summary": "VMoBA, a novel sparse attention mechanism for Video Diffusion Models (VDMs), accelerates training and inference by addressing the quadratic complexity of full attention mechanisms while maintaining or improving video generation quality.",
      "ai_keywords": [
        "quadartic complexity",
        "full attention mechanisms",
        "Video Diffusion Models",
        "VDMs",
        "sparse attention methods",
        "in-depth analysis",
        "attention patterns",
        "video transformers",
        "spatio-temporal locality",
        "query importance",
        "head-specific concentration",
        "layer-wise recurrent block partition scheme",
        "global block selection",
        "threshold-based block selection",
        "FLOPs",
        "latency",
        "high-res video generation"
      ],
      "githubStars": 13
    },
    "publishedAt": "2025-06-30T09:52:31.000Z",
    "title": "VMoBA: Mixture-of-Block Attention for Video Diffusion Models",
    "summary": "The quadratic complexity of full attention mechanisms poses a significant\nbottleneck for Video Diffusion Models (VDMs) aiming to generate long-duration,\nhigh-resolution videos. While various sparse attention methods have been\nproposed, many are designed as training-free inference accelerators or do not\noptimally capture the unique spatio-temporal characteristics inherent in video\ndata when trained natively. This paper introduces Video Mixture of Block\nAttention (VMoBA), a novel sparse attention mechanism specifically adapted for\nVDMs. Motivated by an in-depth analysis of attention patterns within\npre-trained video transformers, which revealed strong spatio-temporal locality,\nvarying query importance, and head-specific concentration levels, VMoBA\nenhances the original MoBA framework with three key modifications: (1) a\nlayer-wise recurrent block partition scheme (1D-2D-3D) to dynamically adapt to\ndiverse spatio-temporal attention patterns and improve efficiency; (2) global\nblock selection to prioritize the most salient query-key block interactions\nacross an entire attention head; and (3) threshold-based block selection to\ndynamically determine the number of attended blocks based on their cumulative\nsimilarity. Extensive experiments demonstrate that VMoBA significantly\naccelerates the training of VDMs on longer sequences, achieving 2.92x FLOPs and\n1.48x latency speedup, while attaining comparable or even superior generation\nquality to full attention. Furthermore, VMoBA exhibits competitive performance\nin training-free inference, offering 2.40x FLOPs and 1.35x latency speedup for\nhigh-res video generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.23858.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "657a6eed1ccc3c2a5ea7b585",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/RIQIF-JJdNI0SwJEq_9z7.jpeg",
      "fullname": "Jianzong Wu",
      "name": "jianzongwu",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.24123",
      "authors": [
        {
          "_id": "68634673588cea0da970c862",
          "name": "Yue Ma",
          "hidden": false
        },
        {
          "_id": "68634673588cea0da970c863",
          "name": "Qingyan Bai",
          "hidden": false
        },
        {
          "_id": "68634673588cea0da970c864",
          "name": "Hao Ouyang",
          "hidden": false
        },
        {
          "_id": "68634673588cea0da970c865",
          "name": "Ka Leong Cheng",
          "hidden": false
        },
        {
          "_id": "68634673588cea0da970c866",
          "name": "Qiuyu Wang",
          "hidden": false
        },
        {
          "_id": "68634673588cea0da970c867",
          "name": "Hongyu Liu",
          "hidden": false
        },
        {
          "_id": "68634673588cea0da970c868",
          "name": "Zichen Liu",
          "hidden": false
        },
        {
          "_id": "68634673588cea0da970c869",
          "name": "Haofan Wang",
          "hidden": false
        },
        {
          "_id": "68634673588cea0da970c86a",
          "name": "Jingye Chen",
          "hidden": false
        },
        {
          "_id": "68634673588cea0da970c86b",
          "name": "Yujun Shen",
          "hidden": false
        },
        {
          "_id": "68634673588cea0da970c86c",
          "name": "Qifeng Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-30T17:59:06.000Z",
      "submittedOnDailyAt": "2025-07-01T01:00:32.385Z",
      "title": "Calligrapher: Freestyle Text Image Customization",
      "submittedOnDailyBy": {
        "_id": "63f0baf66309c84d5f4a2226",
        "avatarUrl": "/avatars/a122f7d92441bd2feef7d4eda993fab7.svg",
        "isPro": false,
        "fullname": "Meme155",
        "user": "Meme145",
        "type": "user"
      },
      "summary": "We introduce Calligrapher, a novel diffusion-based framework that\ninnovatively integrates advanced text customization with artistic typography\nfor digital calligraphy and design applications. Addressing the challenges of\nprecise style control and data dependency in typographic customization, our\nframework incorporates three key technical contributions. First, we develop a\nself-distillation mechanism that leverages the pre-trained text-to-image\ngenerative model itself alongside the large language model to automatically\nconstruct a style-centric typography benchmark. Second, we introduce a\nlocalized style injection framework via a trainable style encoder, which\ncomprises both Qformer and linear layers, to extract robust style features from\nreference images. An in-context generation mechanism is also employed to\ndirectly embed reference images into the denoising process, further enhancing\nthe refined alignment of target styles. Extensive quantitative and qualitative\nevaluations across diverse fonts and design contexts confirm Calligrapher's\naccurate reproduction of intricate stylistic details and precise glyph\npositioning. By automating high-quality, visually consistent typography,\nCalligrapher surpasses traditional models, empowering creative practitioners in\ndigital art, branding, and contextual typographic design.",
      "upvotes": 17,
      "discussionId": "68634673588cea0da970c86d",
      "projectPage": "https://calligrapher2025.github.io/Calligrapher/",
      "githubRepo": "https://github.com/Calligrapher2025/Calligrapher",
      "ai_summary": "Calligrapher uses a diffusion-based framework with self-distillation and localized style injection to generate high-quality, stylistically consistent digital typography.",
      "ai_keywords": [
        "diffusion-based framework",
        "self-distillation mechanism",
        "text-to-image generative model",
        "large language model",
        "localized style injection",
        "Qformer",
        "linear layers",
        "style encoder",
        "in-context generation mechanism",
        "denoising process",
        "stylistic details",
        "glyph positioning"
      ],
      "githubStars": 21
    },
    "publishedAt": "2025-06-30T13:59:06.000Z",
    "title": "Calligrapher: Freestyle Text Image Customization",
    "summary": "We introduce Calligrapher, a novel diffusion-based framework that\ninnovatively integrates advanced text customization with artistic typography\nfor digital calligraphy and design applications. Addressing the challenges of\nprecise style control and data dependency in typographic customization, our\nframework incorporates three key technical contributions. First, we develop a\nself-distillation mechanism that leverages the pre-trained text-to-image\ngenerative model itself alongside the large language model to automatically\nconstruct a style-centric typography benchmark. Second, we introduce a\nlocalized style injection framework via a trainable style encoder, which\ncomprises both Qformer and linear layers, to extract robust style features from\nreference images. An in-context generation mechanism is also employed to\ndirectly embed reference images into the denoising process, further enhancing\nthe refined alignment of target styles. Extensive quantitative and qualitative\nevaluations across diverse fonts and design contexts confirm Calligrapher's\naccurate reproduction of intricate stylistic details and precise glyph\npositioning. By automating high-quality, visually consistent typography,\nCalligrapher surpasses traditional models, empowering creative practitioners in\ndigital art, branding, and contextual typographic design.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.24123.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63f0baf66309c84d5f4a2226",
      "avatarUrl": "/avatars/a122f7d92441bd2feef7d4eda993fab7.svg",
      "fullname": "Meme155",
      "name": "Meme145",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.24119",
      "authors": [
        {
          "_id": "68634850588cea0da970c892",
          "name": "Bo Liu",
          "hidden": false
        },
        {
          "_id": "68634850588cea0da970c893",
          "name": "Leon Guertler",
          "hidden": false
        },
        {
          "_id": "68634850588cea0da970c894",
          "name": "Simon Yu",
          "hidden": false
        },
        {
          "_id": "68634850588cea0da970c895",
          "name": "Zichen Liu",
          "hidden": false
        },
        {
          "_id": "68634850588cea0da970c896",
          "name": "Penghui Qi",
          "hidden": false
        },
        {
          "_id": "68634850588cea0da970c897",
          "name": "Daniel Balcells",
          "hidden": false
        },
        {
          "_id": "68634850588cea0da970c898",
          "name": "Mickel Liu",
          "hidden": false
        },
        {
          "_id": "68634850588cea0da970c899",
          "name": "Cheston Tan",
          "hidden": false
        },
        {
          "_id": "68634850588cea0da970c89a",
          "name": "Weiyan Shi",
          "hidden": false
        },
        {
          "_id": "68634850588cea0da970c89b",
          "name": "Min Lin",
          "hidden": false
        },
        {
          "_id": "68634850588cea0da970c89c",
          "name": "Wee Sun Lee",
          "hidden": false
        },
        {
          "_id": "68634850588cea0da970c89d",
          "name": "Natasha Jaques",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-30T17:58:13.000Z",
      "submittedOnDailyAt": "2025-07-01T01:11:49.104Z",
      "title": "SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via\n  Multi-Agent Multi-Turn Reinforcement Learning",
      "submittedOnDailyBy": {
        "_id": "635e3a76106f984574c36409",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1667120725800-635e3a76106f984574c36409.png",
        "isPro": false,
        "fullname": "Bo Liu",
        "user": "Benjamin-eecs",
        "type": "user"
      },
      "summary": "Recent advances in reinforcement learning have shown that language models can\ndevelop sophisticated reasoning through training on tasks with verifiable\nrewards, but these approaches depend on human-curated problem-answer pairs and\ndomain-specific reward engineering. We introduce SPIRAL, a self-play framework\nwhere models learn by playing multi-turn, zero-sum games against continuously\nimproving versions of themselves, eliminating the need for human supervision.\nThrough self-play, SPIRAL generates an infinite curriculum of progressively\nchallenging problems as models must constantly adapt to stronger opponents. To\nenable this self-play training at scale, We implement a fully online,\nmulti-turn, multi-agent reinforcement learning system for LLMs and propose\nrole-conditioned advantage estimation (RAE) to stabilize multi-agent training.\nUsing SPIRAL, self-play on zero-sum games produces reasoning capabilities that\ntransfer broadly. Training Qwen3-4B-Base on Kuhn Poker alone achieves 8.6%\nimprovement on math and 8.4% on general reasoning, outperforming SFT on 25,000\nexpert game trajectories. Analysis reveals that this transfer occurs through\nthree cognitive patterns: systematic decomposition, expected value calculation,\nand case-by-case analysis. Multi-game training (TicTacToe, Kuhn Poker, Simple\nNegotiation) further enhances performance as each game develops distinct\nreasoning strengths. Applying SPIRAL to a strong reasoning model\n(DeepSeek-R1-Distill-Qwen-7B) can still lead to 2.0% average improvement. These\nresults demonstrate that zero-sum games naturally develop transferable\nreasoning capabilities, highlighting a promising direction for autonomous\nreasoning development.",
      "upvotes": 10,
      "discussionId": "68634850588cea0da970c89e",
      "githubRepo": "https://github.com/spiral-rl/spiral",
      "ai_summary": "Self-play in zero-sum games using SPIRAL enhances reasoning capabilities in language models through self-improvement and transfer learning.",
      "ai_keywords": [
        "reinforcement learning",
        "language models",
        "self-play framework",
        "multi-turn games",
        "zero-sum games",
        "reward engineering",
        "role-conditioned advantage estimation",
        "Kuhn Poker",
        "TicTacToe",
        "Simple Negotiation",
        "transferable reasoning"
      ],
      "githubStars": 6
    },
    "publishedAt": "2025-06-30T13:58:13.000Z",
    "title": "SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via\n  Multi-Agent Multi-Turn Reinforcement Learning",
    "summary": "Recent advances in reinforcement learning have shown that language models can\ndevelop sophisticated reasoning through training on tasks with verifiable\nrewards, but these approaches depend on human-curated problem-answer pairs and\ndomain-specific reward engineering. We introduce SPIRAL, a self-play framework\nwhere models learn by playing multi-turn, zero-sum games against continuously\nimproving versions of themselves, eliminating the need for human supervision.\nThrough self-play, SPIRAL generates an infinite curriculum of progressively\nchallenging problems as models must constantly adapt to stronger opponents. To\nenable this self-play training at scale, We implement a fully online,\nmulti-turn, multi-agent reinforcement learning system for LLMs and propose\nrole-conditioned advantage estimation (RAE) to stabilize multi-agent training.\nUsing SPIRAL, self-play on zero-sum games produces reasoning capabilities that\ntransfer broadly. Training Qwen3-4B-Base on Kuhn Poker alone achieves 8.6%\nimprovement on math and 8.4% on general reasoning, outperforming SFT on 25,000\nexpert game trajectories. Analysis reveals that this transfer occurs through\nthree cognitive patterns: systematic decomposition, expected value calculation,\nand case-by-case analysis. Multi-game training (TicTacToe, Kuhn Poker, Simple\nNegotiation) further enhances performance as each game develops distinct\nreasoning strengths. Applying SPIRAL to a strong reasoning model\n(DeepSeek-R1-Distill-Qwen-7B) can still lead to 2.0% average improvement. These\nresults demonstrate that zero-sum games naturally develop transferable\nreasoning capabilities, highlighting a promising direction for autonomous\nreasoning development.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.24119.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "635e3a76106f984574c36409",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1667120725800-635e3a76106f984574c36409.png",
      "fullname": "Bo Liu",
      "name": "Benjamin-eecs",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 13
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.17930",
      "authors": [
        {
          "_id": "68634a4e588cea0da970c8ba",
          "name": "Jianyu Wang",
          "hidden": false
        },
        {
          "_id": "68634a4e588cea0da970c8bb",
          "name": "Zhiqiang Hu",
          "hidden": false
        },
        {
          "_id": "68634a4e588cea0da970c8bc",
          "name": "Lidong Bing",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-22T07:53:07.000Z",
      "submittedOnDailyAt": "2025-07-01T01:09:47.554Z",
      "title": "Evolving Prompts In-Context: An Open-ended, Self-replicating Perspective",
      "submittedOnDailyBy": {
        "_id": "61e09ec13a1781f66b4e9ae2",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1642110635503-noauth.jpeg",
        "isPro": false,
        "fullname": "Jianyu Wang",
        "user": "Jianyu",
        "type": "user"
      },
      "summary": "We propose a novel prompt design paradigm that challenges conventional wisdom\nin large language model (LLM) prompting. While conventional wisdom prioritizes\nwell-crafted instructions and demonstrations for in-context learning (ICL), we\nshow that pruning random demonstrations into seemingly incoherent \"gibberish\"\ncan remarkably improve performance across diverse tasks. Notably, the\n\"gibberish\" always matches or surpasses state-of-the-art automatic prompt\noptimization techniques, achieving substantial gains regardless of LLM\nalignment. Nevertheless, discovering an effective pruning strategy is\nnon-trivial, as existing attribution methods and prompt compression algorithms\nfail to deliver robust results, let alone human intuition. In terms of this, we\npropose a self-discover prompt optimization framework, PromptQuine, an\nevolutionary search framework that automatically searches for the pruning\nstrategy by itself using only low-data regimes. Much like the emergent\ncomplexity in nature--such as symbiosis and self-organization--arising in\nresponse to resource constraints, our framework evolves and refines\nunconventional yet highly effective prompts by leveraging only the tokens\npresent within the context. We demonstrate its effectiveness across\nclassification, multi-choice question answering, generation and math reasoning\ntasks across LLMs, while achieving decent runtime efficiency. We hope our\nfindings can guide mechanistic studies on in-context learning, and provide a\ncall to action, to pave the way for more open-ended search algorithms for more\neffective LLM prompting.",
      "upvotes": 8,
      "discussionId": "68634a4f588cea0da970c8bd",
      "ai_summary": "A novel prompt design paradigm, PromptQuine, shows that pruning random demonstrations into \"gibberish\" can improve large language model performance across various tasks, surpassing state-of-the-art methods.",
      "ai_keywords": [
        "prompt design paradigm",
        "in-context learning",
        "pruning",
        "demonstrations",
        "gibberish",
        "prompt optimization",
        "self-discover prompt optimization framework",
        "PromptQuine",
        "evolutionary search framework",
        "classification",
        "multi-choice question answering",
        "generation",
        "math reasoning",
        "tokens",
        "emergent complexity",
        "symbiosis",
        "self-organization"
      ]
    },
    "publishedAt": "2025-06-22T03:53:07.000Z",
    "title": "Evolving Prompts In-Context: An Open-ended, Self-replicating Perspective",
    "summary": "We propose a novel prompt design paradigm that challenges conventional wisdom\nin large language model (LLM) prompting. While conventional wisdom prioritizes\nwell-crafted instructions and demonstrations for in-context learning (ICL), we\nshow that pruning random demonstrations into seemingly incoherent \"gibberish\"\ncan remarkably improve performance across diverse tasks. Notably, the\n\"gibberish\" always matches or surpasses state-of-the-art automatic prompt\noptimization techniques, achieving substantial gains regardless of LLM\nalignment. Nevertheless, discovering an effective pruning strategy is\nnon-trivial, as existing attribution methods and prompt compression algorithms\nfail to deliver robust results, let alone human intuition. In terms of this, we\npropose a self-discover prompt optimization framework, PromptQuine, an\nevolutionary search framework that automatically searches for the pruning\nstrategy by itself using only low-data regimes. Much like the emergent\ncomplexity in nature--such as symbiosis and self-organization--arising in\nresponse to resource constraints, our framework evolves and refines\nunconventional yet highly effective prompts by leveraging only the tokens\npresent within the context. We demonstrate its effectiveness across\nclassification, multi-choice question answering, generation and math reasoning\ntasks across LLMs, while achieving decent runtime efficiency. We hope our\nfindings can guide mechanistic studies on in-context learning, and provide a\ncall to action, to pave the way for more open-ended search algorithms for more\neffective LLM prompting.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.17930.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "61e09ec13a1781f66b4e9ae2",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1642110635503-noauth.jpeg",
      "fullname": "Jianyu Wang",
      "name": "Jianyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.23542",
      "authors": [
        {
          "_id": "6863479d588cea0da970c86f",
          "name": "Weida Wang",
          "hidden": false
        },
        {
          "_id": "6863479d588cea0da970c870",
          "name": "Changyong He",
          "hidden": false
        },
        {
          "_id": "6863479d588cea0da970c871",
          "name": "Jin Zeng",
          "hidden": false
        },
        {
          "_id": "6863479d588cea0da970c872",
          "name": "Di Qiu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-30T06:29:24.000Z",
      "submittedOnDailyAt": "2025-07-01T01:05:13.898Z",
      "title": "Consistent Time-of-Flight Depth Denoising via Graph-Informed Geometric\n  Attention",
      "submittedOnDailyBy": {
        "_id": "6684b284dc7b0ae2cc67660c",
        "avatarUrl": "/avatars/54b3c0c4d808f293d78085d4d504570a.svg",
        "isPro": false,
        "fullname": "liuwanhao",
        "user": "wanhaoliu",
        "type": "user"
      },
      "summary": "Depth images captured by Time-of-Flight (ToF) sensors are prone to noise,\nrequiring denoising for reliable downstream applications. Previous works either\nfocus on single-frame processing, or perform multi-frame processing without\nconsidering depth variations at corresponding pixels across frames, leading to\nundesirable temporal inconsistency and spatial ambiguity. In this paper, we\npropose a novel ToF depth denoising network leveraging motion-invariant graph\nfusion to simultaneously enhance temporal stability and spatial sharpness.\nSpecifically, despite depth shifts across frames, graph structures exhibit\ntemporal self-similarity, enabling cross-frame geometric attention for graph\nfusion. Then, by incorporating an image smoothness prior on the fused graph and\ndata fidelity term derived from ToF noise distribution, we formulate a maximum\na posterior problem for ToF denoising. Finally, the solution is unrolled into\niterative filters whose weights are adaptively learned from the graph-informed\ngeometric attention, producing a high-performance yet interpretable network.\nExperimental results demonstrate that the proposed scheme achieves\nstate-of-the-art performance in terms of accuracy and consistency on synthetic\nDVToF dataset and exhibits robust generalization on the real Kinectv2 dataset.\nSource code will be released at\nhttps://github.com/davidweidawang/GIGA-ToF{https://github.com/davidweidawang/GIGA-ToF}.",
      "upvotes": 7,
      "discussionId": "6863479d588cea0da970c873",
      "ai_summary": "A novel ToF depth denoising network uses motion-invariant graph fusion and adaptive filters to improve temporal stability and spatial sharpness, achieving state-of-the-art performance.",
      "ai_keywords": [
        "ToF depth denoising",
        "motion-invariant graph fusion",
        "graph structures",
        "temporal self-similarity",
        "geometric attention",
        "image smoothness prior",
        "maximum a posterior problem",
        "iterative filters",
        "DVToF dataset",
        "Kinectv2 dataset"
      ]
    },
    "publishedAt": "2025-06-30T02:29:24.000Z",
    "title": "Consistent Time-of-Flight Depth Denoising via Graph-Informed Geometric\n  Attention",
    "summary": "Depth images captured by Time-of-Flight (ToF) sensors are prone to noise,\nrequiring denoising for reliable downstream applications. Previous works either\nfocus on single-frame processing, or perform multi-frame processing without\nconsidering depth variations at corresponding pixels across frames, leading to\nundesirable temporal inconsistency and spatial ambiguity. In this paper, we\npropose a novel ToF depth denoising network leveraging motion-invariant graph\nfusion to simultaneously enhance temporal stability and spatial sharpness.\nSpecifically, despite depth shifts across frames, graph structures exhibit\ntemporal self-similarity, enabling cross-frame geometric attention for graph\nfusion. Then, by incorporating an image smoothness prior on the fused graph and\ndata fidelity term derived from ToF noise distribution, we formulate a maximum\na posterior problem for ToF denoising. Finally, the solution is unrolled into\niterative filters whose weights are adaptively learned from the graph-informed\ngeometric attention, producing a high-performance yet interpretable network.\nExperimental results demonstrate that the proposed scheme achieves\nstate-of-the-art performance in terms of accuracy and consistency on synthetic\nDVToF dataset and exhibits robust generalization on the real Kinectv2 dataset.\nSource code will be released at\nhttps://github.com/davidweidawang/GIGA-ToF{https://github.com/davidweidawang/GIGA-ToF}.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.23542.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6684b284dc7b0ae2cc67660c",
      "avatarUrl": "/avatars/54b3c0c4d808f293d78085d4d504570a.svg",
      "fullname": "liuwanhao",
      "name": "wanhaoliu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.17417",
      "authors": [
        {
          "_id": "685c8635696820ba1f28f24b",
          "user": {
            "_id": "65d3b7ec8f6b98b34ee6bbe3",
            "avatarUrl": "/avatars/53c2d4e4746147fc2559435d252e8951.svg",
            "isPro": false,
            "fullname": "Mingyuan Wu",
            "user": "Mingyuan1997",
            "type": "user"
          },
          "name": "Mingyuan Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-26T09:32:22.703Z",
          "hidden": false
        },
        {
          "_id": "685c8635696820ba1f28f24c",
          "name": "Meitang Li",
          "hidden": false
        },
        {
          "_id": "685c8635696820ba1f28f24d",
          "name": "Jingcheng Yang",
          "hidden": false
        },
        {
          "_id": "685c8635696820ba1f28f24e",
          "name": "Jize Jiang",
          "hidden": false
        },
        {
          "_id": "685c8635696820ba1f28f24f",
          "name": "Kaizhuo Yan",
          "hidden": false
        },
        {
          "_id": "685c8635696820ba1f28f250",
          "name": "Zhaoheng Li",
          "hidden": false
        },
        {
          "_id": "685c8635696820ba1f28f251",
          "name": "Minjia Zhang",
          "hidden": false
        },
        {
          "_id": "685c8635696820ba1f28f252",
          "name": "Klara Nahrstedt",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-20T18:23:48.000Z",
      "submittedOnDailyAt": "2025-07-01T00:36:26.256Z",
      "title": "Aha Moment Revisited: Are VLMs Truly Capable of Self Verification in\n  Inference-time Scaling?",
      "submittedOnDailyBy": {
        "_id": "65d3b7ec8f6b98b34ee6bbe3",
        "avatarUrl": "/avatars/53c2d4e4746147fc2559435d252e8951.svg",
        "isPro": false,
        "fullname": "Mingyuan Wu",
        "user": "Mingyuan1997",
        "type": "user"
      },
      "summary": "Recent advances in large language models (LLMs) have demonstrated that\ninference-time computation techniques, such as decoding-time scaling and\nself-refinement, can significantly enhance reasoning capabilities without\nrelying on external knowledge. A key driver of this success is the emergence of\nself-correction and self-verification behaviors, often elicited through\nreinforcement learning (RL). In this paper, we investigate whether these\ninference-time techniques extend effectively to vision-language models (VLMs),\nparticularly those trained with RL. We find that while decoding strategies such\nas majority voting and best-of-N selection with self-verification all improve\nVLM reasoning performance, generation-reliant methods such as the former\nachieve significantly higher gains versus verification-reliant methods such as\nthe latter. Additionally, the self-correction behavior often associated with\nRL-tuned models, such as aha moment, does not lead to measurable gains. We show\nvia extensive experimentation within the inference-time scaling framework to\nidentify a key root cause: RL-trained VLMs still lack robust self-verification\ncapabilities across both visual and textual modalities.",
      "upvotes": 7,
      "discussionId": "685c8636696820ba1f28f253",
      "ai_summary": "Inference-time techniques like decoding-time scaling and self-refinement enhance reasoning in vision-language models, with generation-based methods providing greater improvement than verification-based methods, despite RL-trained models not showing self-correction benefits.",
      "ai_keywords": [
        "large language models",
        "inference-time computation",
        "decoding-time scaling",
        "self-refinement",
        "self-correction",
        "self-verification",
        "reinforcement learning",
        "vision-language models",
        "majority voting",
        "best-of-N selection",
        "aha moment"
      ]
    },
    "publishedAt": "2025-06-20T14:23:48.000Z",
    "title": "Aha Moment Revisited: Are VLMs Truly Capable of Self Verification in\n  Inference-time Scaling?",
    "summary": "Recent advances in large language models (LLMs) have demonstrated that\ninference-time computation techniques, such as decoding-time scaling and\nself-refinement, can significantly enhance reasoning capabilities without\nrelying on external knowledge. A key driver of this success is the emergence of\nself-correction and self-verification behaviors, often elicited through\nreinforcement learning (RL). In this paper, we investigate whether these\ninference-time techniques extend effectively to vision-language models (VLMs),\nparticularly those trained with RL. We find that while decoding strategies such\nas majority voting and best-of-N selection with self-verification all improve\nVLM reasoning performance, generation-reliant methods such as the former\nachieve significantly higher gains versus verification-reliant methods such as\nthe latter. Additionally, the self-correction behavior often associated with\nRL-tuned models, such as aha moment, does not lead to measurable gains. We show\nvia extensive experimentation within the inference-time scaling framework to\nidentify a key root cause: RL-trained VLMs still lack robust self-verification\ncapabilities across both visual and textual modalities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.17417.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65d3b7ec8f6b98b34ee6bbe3",
      "avatarUrl": "/avatars/53c2d4e4746147fc2559435d252e8951.svg",
      "fullname": "Mingyuan Wu",
      "name": "Mingyuan1997",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.16500",
      "authors": [
        {
          "_id": "6858a7f0c0c8e29df8ea3c06",
          "name": "Samir Khaki",
          "hidden": false
        },
        {
          "_id": "6858a7f0c0c8e29df8ea3c07",
          "name": "Xiuyu Li",
          "hidden": false
        },
        {
          "_id": "6858a7f0c0c8e29df8ea3c08",
          "name": "Junxian Guo",
          "hidden": false
        },
        {
          "_id": "6858a7f0c0c8e29df8ea3c09",
          "name": "Ligeng Zhu",
          "hidden": false
        },
        {
          "_id": "6858a7f0c0c8e29df8ea3c0a",
          "name": "Chenfeng Xu",
          "hidden": false
        },
        {
          "_id": "6858a7f0c0c8e29df8ea3c0b",
          "name": "Konstantinos N. Plataniotis",
          "hidden": false
        },
        {
          "_id": "6858a7f0c0c8e29df8ea3c0c",
          "name": "Amir Yazdanbakhsh",
          "hidden": false
        },
        {
          "_id": "6858a7f0c0c8e29df8ea3c0d",
          "name": "Kurt Keutzer",
          "hidden": false
        },
        {
          "_id": "6858a7f0c0c8e29df8ea3c0e",
          "name": "Song Han",
          "hidden": false
        },
        {
          "_id": "6858a7f0c0c8e29df8ea3c0f",
          "name": "Zhijian Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-19T17:53:34.000Z",
      "submittedOnDailyAt": "2025-07-01T03:16:07.666Z",
      "title": "SparseLoRA: Accelerating LLM Fine-Tuning with Contextual Sparsity",
      "submittedOnDailyBy": {
        "_id": "64b38bc2a248169796fec4fa",
        "avatarUrl": "/avatars/1371021f2ef0ce2197cc13627c9e03c9.svg",
        "isPro": false,
        "fullname": "Samir Khaki",
        "user": "Skhaki",
        "type": "user"
      },
      "summary": "Fine-tuning LLMs is both computationally and memory-intensive. While\nparameter-efficient fine-tuning methods, such as QLoRA and DoRA, reduce the\nnumber of trainable parameters and lower memory usage, they do not decrease\ncomputational cost. In some cases, they may even slow down fine-tuning. In this\npaper, we introduce SparseLoRA, a method that accelerates LLM fine-tuning\nthrough contextual sparsity. We propose a lightweight, training-free SVD\nsparsity estimator that dynamically selects a sparse subset of weights for loss\nand gradient computation. Also, we systematically analyze and address\nsensitivity across layers, tokens, and training steps. Our experimental results\nshow that SparseLoRA reduces computational cost by up to 2.2 times and a\nmeasured speedup of up to 1.6 times while maintaining accuracy across various\ndownstream tasks, including commonsense and arithmetic reasoning, code\ngeneration, and instruction following.",
      "upvotes": 6,
      "discussionId": "6858a7f0c0c8e29df8ea3c10",
      "projectPage": "https://z-lab.ai/projects/sparselora/",
      "githubRepo": "https://github.com/z-lab/sparselora",
      "ai_summary": "SparseLoRA reduces computational cost and speeds up fine-tuning of LLMs by dynamically selecting a sparse subset of weights for loss and gradient computation.",
      "ai_keywords": [
        "QLoRA",
        "DoRA",
        "parameter-efficient fine-tuning",
        "SparseLoRA",
        "contextual sparsity",
        "SVD sparsity estimator",
        "computational cost",
        "commonsense reasoning",
        "arithmetic reasoning",
        "code generation",
        "instruction following"
      ],
      "githubStars": 14
    },
    "publishedAt": "2025-06-19T13:53:34.000Z",
    "title": "SparseLoRA: Accelerating LLM Fine-Tuning with Contextual Sparsity",
    "summary": "Fine-tuning LLMs is both computationally and memory-intensive. While\nparameter-efficient fine-tuning methods, such as QLoRA and DoRA, reduce the\nnumber of trainable parameters and lower memory usage, they do not decrease\ncomputational cost. In some cases, they may even slow down fine-tuning. In this\npaper, we introduce SparseLoRA, a method that accelerates LLM fine-tuning\nthrough contextual sparsity. We propose a lightweight, training-free SVD\nsparsity estimator that dynamically selects a sparse subset of weights for loss\nand gradient computation. Also, we systematically analyze and address\nsensitivity across layers, tokens, and training steps. Our experimental results\nshow that SparseLoRA reduces computational cost by up to 2.2 times and a\nmeasured speedup of up to 1.6 times while maintaining accuracy across various\ndownstream tasks, including commonsense and arithmetic reasoning, code\ngeneration, and instruction following.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.16500.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b38bc2a248169796fec4fa",
      "avatarUrl": "/avatars/1371021f2ef0ce2197cc13627c9e03c9.svg",
      "fullname": "Samir Khaki",
      "name": "Skhaki",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.23394",
      "authors": [
        {
          "_id": "686350a9588cea0da970c8d4",
          "name": "Simeon Emanuilov",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-29T20:47:27.000Z",
      "submittedOnDailyAt": "2025-07-01T01:39:51.669Z",
      "title": "Teaching a Language Model to Speak the Language of Tools",
      "submittedOnDailyBy": {
        "_id": "645dbaa6f5760d1530d7580d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645dbaa6f5760d1530d7580d/Bqob8arLZoHIgMwNZpL9I.jpeg",
        "isPro": true,
        "fullname": "Simeon Emanuilov",
        "user": "s-emanuilov",
        "type": "user"
      },
      "summary": "External tool integration through function-calling is essential for practical\nlanguage model applications, yet most multilingual models lack reliable\ntool-use capabilities in non-English languages. Even state-of-the-art\nmultilingual models struggle with determining when to use tools and generating\nthe structured outputs required for function calls, often exhibiting language\nconfusion when prompted in lower-resource languages. This work presents a\nmethodology for adapting existing language models to enable robust tool use in\nany target language, using Bulgarian as a case study. The approach involves\ncontinued training of the BgGPT model series (2.6B, 9B, 27B parameters) on a\nnovel bilingual dataset of 10,035 function-calling examples designed to support\nstandardized protocols like MCP (Model Context Protocol). The research\nintroduces TUCAN (Tool-Using Capable Assistant Navigator), which achieves up to\n28.75% improvement in function-calling accuracy over base models while\npreserving core language understanding, as verified on established Bulgarian\nbenchmarks. Beyond accuracy gains, TUCAN models demonstrate production-ready\nresponse formatting with clean, parsable function calls, contrasting with the\nverbose and inconsistent outputs of base models. The models, evaluation\nframework, and dataset are released to enable replication for other languages.\nThis work demonstrates a practical approach for extending tool-augmented\ncapabilities beyond English-centric systems.",
      "upvotes": 2,
      "discussionId": "686350aa588cea0da970c8d5",
      "ai_summary": "A methodology is presented to adapt language models for robust tool use across languages, specifically improving function-calling accuracy in Bulgarian.",
      "ai_keywords": [
        "function-calling",
        "multilingual models",
        "tool-use capabilities",
        "language confusion",
        "BgGPT",
        "bilingual dataset",
        "MCP (Model Context Protocol)",
        "TUCAN (Tool-Using Capable Assistant Navigator)",
        "function-calling accuracy",
        "production-ready response formatting"
      ]
    },
    "publishedAt": "2025-06-29T16:47:27.000Z",
    "title": "Teaching a Language Model to Speak the Language of Tools",
    "summary": "External tool integration through function-calling is essential for practical\nlanguage model applications, yet most multilingual models lack reliable\ntool-use capabilities in non-English languages. Even state-of-the-art\nmultilingual models struggle with determining when to use tools and generating\nthe structured outputs required for function calls, often exhibiting language\nconfusion when prompted in lower-resource languages. This work presents a\nmethodology for adapting existing language models to enable robust tool use in\nany target language, using Bulgarian as a case study. The approach involves\ncontinued training of the BgGPT model series (2.6B, 9B, 27B parameters) on a\nnovel bilingual dataset of 10,035 function-calling examples designed to support\nstandardized protocols like MCP (Model Context Protocol). The research\nintroduces TUCAN (Tool-Using Capable Assistant Navigator), which achieves up to\n28.75% improvement in function-calling accuracy over base models while\npreserving core language understanding, as verified on established Bulgarian\nbenchmarks. Beyond accuracy gains, TUCAN models demonstrate production-ready\nresponse formatting with clean, parsable function calls, contrasting with the\nverbose and inconsistent outputs of base models. The models, evaluation\nframework, and dataset are released to enable replication for other languages.\nThis work demonstrates a practical approach for extending tool-augmented\ncapabilities beyond English-centric systems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.23394.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645dbaa6f5760d1530d7580d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645dbaa6f5760d1530d7580d/Bqob8arLZoHIgMwNZpL9I.jpeg",
      "fullname": "Simeon Emanuilov",
      "name": "s-emanuilov",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 39
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.23219",
      "authors": [
        {
          "_id": "686376f6588cea0da970c92b",
          "name": "Jie Feng",
          "hidden": false
        },
        {
          "_id": "686376f6588cea0da970c92c",
          "name": "Shengyuan Wang",
          "hidden": false
        },
        {
          "_id": "686376f6588cea0da970c92d",
          "name": "Tianhui Liu",
          "hidden": false
        },
        {
          "_id": "686376f6588cea0da970c92e",
          "name": "Yanxin Xi",
          "hidden": false
        },
        {
          "_id": "686376f6588cea0da970c92f",
          "name": "Yong Li",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6465d3bd63e7e09dd02e95c3/RBLtf0_JPAX9A8msse6c0.jpeg"
      ],
      "publishedAt": "2025-06-29T13:04:27.000Z",
      "submittedOnDailyAt": "2025-07-01T04:22:03.888Z",
      "title": "UrbanLLaVA: A Multi-modal Large Language Model for Urban Intelligence\n  with Spatial Reasoning and Understanding",
      "submittedOnDailyBy": {
        "_id": "6465d3bd63e7e09dd02e95c3",
        "avatarUrl": "/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg",
        "isPro": false,
        "fullname": "Jie Feng",
        "user": "JJ-TMT",
        "type": "user"
      },
      "summary": "Urban research involves a wide range of scenarios and tasks that require the\nunderstanding of multi-modal data. Current methods often focus on specific data\ntypes and lack a unified framework in urban field for processing them\ncomprehensively. The recent success of multi-modal large language models\n(MLLMs) presents a promising opportunity to overcome this limitation. In this\npaper, we introduce UrbanLLaVA, a multi-modal large language model\ndesigned to process these four types of data simultaneously and achieve strong\nperformance across diverse urban tasks compared with general MLLMs. In\nUrbanLLaVA, we first curate a diverse urban instruction dataset\nencompassing both single-modal and cross-modal urban data, spanning from\nlocation view to global view of urban environment. Additionally, we propose a\nmulti-stage training framework that decouples spatial reasoning enhancement\nfrom domain knowledge learning, thereby improving the compatibility and\ndownstream performance of UrbanLLaVA across diverse urban tasks.\nFinally, we also extend existing benchmark for urban research to assess the\nperformance of MLLMs across a wide range of urban tasks. Experimental results\nfrom three cities demonstrate that UrbanLLaVA outperforms\nopen-source and proprietary MLLMs in both single-modal tasks and complex\ncross-modal tasks and shows robust generalization abilities across cities.\nSource codes and data are openly accessible to the research community via\nhttps://github.com/tsinghua-fib-lab/UrbanLLaVA.",
      "upvotes": 2,
      "discussionId": "686376f6588cea0da970c930",
      "githubRepo": "https://github.com/tsinghua-fib-lab/UrbanLLaVA",
      "ai_summary": "UrbanLLaVA, a multi-modal large language model, effectively processes urban datasets for various tasks, outperforming existing models in both single-modal and complex cross-modal scenarios.",
      "ai_keywords": [
        "multi-modal large language models",
        "spatial reasoning",
        "domain knowledge learning",
        "urban instruction dataset",
        "single-modal",
        "cross-modal",
        "benchmark",
        "urban research"
      ],
      "githubStars": 3
    },
    "publishedAt": "2025-06-29T09:04:27.000Z",
    "title": "UrbanLLaVA: A Multi-modal Large Language Model for Urban Intelligence\n  with Spatial Reasoning and Understanding",
    "summary": "Urban research involves a wide range of scenarios and tasks that require the\nunderstanding of multi-modal data. Current methods often focus on specific data\ntypes and lack a unified framework in urban field for processing them\ncomprehensively. The recent success of multi-modal large language models\n(MLLMs) presents a promising opportunity to overcome this limitation. In this\npaper, we introduce UrbanLLaVA, a multi-modal large language model\ndesigned to process these four types of data simultaneously and achieve strong\nperformance across diverse urban tasks compared with general MLLMs. In\nUrbanLLaVA, we first curate a diverse urban instruction dataset\nencompassing both single-modal and cross-modal urban data, spanning from\nlocation view to global view of urban environment. Additionally, we propose a\nmulti-stage training framework that decouples spatial reasoning enhancement\nfrom domain knowledge learning, thereby improving the compatibility and\ndownstream performance of UrbanLLaVA across diverse urban tasks.\nFinally, we also extend existing benchmark for urban research to assess the\nperformance of MLLMs across a wide range of urban tasks. Experimental results\nfrom three cities demonstrate that UrbanLLaVA outperforms\nopen-source and proprietary MLLMs in both single-modal tasks and complex\ncross-modal tasks and shows robust generalization abilities across cities.\nSource codes and data are openly accessible to the research community via\nhttps://github.com/tsinghua-fib-lab/UrbanLLaVA.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6465d3bd63e7e09dd02e95c3/RBLtf0_JPAX9A8msse6c0.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.23219.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6465d3bd63e7e09dd02e95c3",
      "avatarUrl": "/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg",
      "fullname": "Jie Feng",
      "name": "JJ-TMT",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.22694",
      "authors": [
        {
          "_id": "68637193588cea0da970c914",
          "name": "Raghavv Goel",
          "hidden": false
        },
        {
          "_id": "68637193588cea0da970c915",
          "name": "Sudhanshu Agrawal",
          "hidden": false
        },
        {
          "_id": "68637193588cea0da970c916",
          "name": "Mukul Gagrani",
          "hidden": false
        },
        {
          "_id": "68637193588cea0da970c917",
          "name": "Junyoung Park",
          "hidden": false
        },
        {
          "_id": "68637193588cea0da970c918",
          "name": "Yifan Zao",
          "hidden": false
        },
        {
          "_id": "68637193588cea0da970c919",
          "name": "He Zhang",
          "hidden": false
        },
        {
          "_id": "68637193588cea0da970c91a",
          "name": "Tian Liu",
          "hidden": false
        },
        {
          "_id": "68637193588cea0da970c91b",
          "name": "Yiping Yang",
          "hidden": false
        },
        {
          "_id": "68637193588cea0da970c91c",
          "name": "Xin Yuan",
          "hidden": false
        },
        {
          "_id": "68637193588cea0da970c91d",
          "name": "Jiuyan Lu",
          "hidden": false
        },
        {
          "_id": "68637193588cea0da970c91e",
          "name": "Chris Lott",
          "hidden": false
        },
        {
          "_id": "68637193588cea0da970c91f",
          "name": "Mingu Lee",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-28T00:26:40.000Z",
      "submittedOnDailyAt": "2025-07-01T04:07:47.278Z",
      "title": "VOCABTRIM: Vocabulary Pruning for Efficient Speculative Decoding in LLMs",
      "submittedOnDailyBy": {
        "_id": "649b6eb2f7cc759ab756adaf",
        "avatarUrl": "/avatars/a0f7c7345dd653887122200fbe375c2b.svg",
        "isPro": false,
        "fullname": "Raghavv Goel",
        "user": "RaghavvGoel",
        "type": "user"
      },
      "summary": "In this paper, we introduce a simple training-free technique to improve the\nperformance of drafter-based speculative decoding (SpD) methods that\nincorporates language modeling head (LM head) during drafting process. A\ndrafter-based speculative decoding leverages one or more smaller language\nmodels, a.k.a. drafters or draft models, to sample a draft sequence or tree\nconsisting of multiple tokens, followed by verification by a base LLM, a target\nmodel, accepting a subset as its valid generation. As it is usually considered\nthat the speculative decoding requires one-to-one mapping between vocabularies\nof the target model and the draft model, it has been natural to share the\nvocabulary between them, or even share the LM head as in EAGLE or Medusa. We\nfirst identify that this draft token sampling scheme inherently contains an\nunnecessary inference overhead in drafting, especially for some target LLMs\nwith very large vocabularies. Then, we propose a simple technique, VocabTrim,\nto mitigate the drafting overhead to improve the generation speed in\nmemory-bound environment. VocabTrim reconstructs the drafter LM head to contain\nonly a limited set of tokens, selected by the most frequently sampled from the\nvocabulary of the target model. While limiting the vocabulary in drafting\nslightly degrades the acceptance rate, it significantly reduces the drafting\nlatency in memory-bound process which is often the case on edge devices,\nresulting in higher memory-bound speed up (MBSU). We show that our method can\nboost the memory-bound speed-up for Llama-3 models on Spec-Bench, specifically\nby 16% for Llama-3.2-3B-Instruct.",
      "upvotes": 2,
      "discussionId": "68637193588cea0da970c920",
      "ai_summary": "A technique called VocabTrim improves drafter-based speculative decoding by reducing the vocabulary of the drafter language model, thus decreasing drafting latency in memory-bound environments.",
      "ai_keywords": [
        "drafter-based speculative decoding",
        "speculative decoding",
        "LM head",
        "drafters",
        "token sampling",
        "inference overhead",
        "target LLM",
        "vocabulary sharing",
        "EAGLE",
        "Medusa",
        "VocabTrim",
        "acceptance rate",
        "drafting latency",
        "memory-bound speed up",
        "Spec-Bench",
        "Llama-3"
      ]
    },
    "publishedAt": "2025-06-27T20:26:40.000Z",
    "title": "VOCABTRIM: Vocabulary Pruning for Efficient Speculative Decoding in LLMs",
    "summary": "In this paper, we introduce a simple training-free technique to improve the\nperformance of drafter-based speculative decoding (SpD) methods that\nincorporates language modeling head (LM head) during drafting process. A\ndrafter-based speculative decoding leverages one or more smaller language\nmodels, a.k.a. drafters or draft models, to sample a draft sequence or tree\nconsisting of multiple tokens, followed by verification by a base LLM, a target\nmodel, accepting a subset as its valid generation. As it is usually considered\nthat the speculative decoding requires one-to-one mapping between vocabularies\nof the target model and the draft model, it has been natural to share the\nvocabulary between them, or even share the LM head as in EAGLE or Medusa. We\nfirst identify that this draft token sampling scheme inherently contains an\nunnecessary inference overhead in drafting, especially for some target LLMs\nwith very large vocabularies. Then, we propose a simple technique, VocabTrim,\nto mitigate the drafting overhead to improve the generation speed in\nmemory-bound environment. VocabTrim reconstructs the drafter LM head to contain\nonly a limited set of tokens, selected by the most frequently sampled from the\nvocabulary of the target model. While limiting the vocabulary in drafting\nslightly degrades the acceptance rate, it significantly reduces the drafting\nlatency in memory-bound process which is often the case on edge devices,\nresulting in higher memory-bound speed up (MBSU). We show that our method can\nboost the memory-bound speed-up for Llama-3 models on Spec-Bench, specifically\nby 16% for Llama-3.2-3B-Instruct.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.22694.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649b6eb2f7cc759ab756adaf",
      "avatarUrl": "/avatars/a0f7c7345dd653887122200fbe375c2b.svg",
      "fullname": "Raghavv Goel",
      "name": "RaghavvGoel",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.23135",
      "authors": [
        {
          "_id": "68638235588cea0da970c966",
          "name": "Yu Shang",
          "hidden": false
        },
        {
          "_id": "68638235588cea0da970c967",
          "name": "Xin Zhang",
          "hidden": false
        },
        {
          "_id": "68638235588cea0da970c968",
          "name": "Yinzhou Tang",
          "hidden": false
        },
        {
          "_id": "68638235588cea0da970c969",
          "name": "Lei Jin",
          "hidden": false
        },
        {
          "_id": "68638235588cea0da970c96a",
          "name": "Chen Gao",
          "hidden": false
        },
        {
          "_id": "68638235588cea0da970c96b",
          "name": "Wei Wu",
          "hidden": false
        },
        {
          "_id": "68638235588cea0da970c96c",
          "name": "Yong Li",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6465d3bd63e7e09dd02e95c3/oiWNtmiOwH-sWe0ewE2YQ.jpeg"
      ],
      "publishedAt": "2025-06-29T08:19:45.000Z",
      "submittedOnDailyAt": "2025-07-01T05:11:31.002Z",
      "title": "RoboScape: Physics-informed Embodied World Model",
      "submittedOnDailyBy": {
        "_id": "6465d3bd63e7e09dd02e95c3",
        "avatarUrl": "/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg",
        "isPro": false,
        "fullname": "Jie Feng",
        "user": "JJ-TMT",
        "type": "user"
      },
      "summary": "World models have become indispensable tools for embodied intelligence,\nserving as powerful simulators capable of generating realistic robotic videos\nwhile addressing critical data scarcity challenges. However, current embodied\nworld models exhibit limited physical awareness, particularly in modeling 3D\ngeometry and motion dynamics, resulting in unrealistic video generation for\ncontact-rich robotic scenarios. In this paper, we present RoboScape, a unified\nphysics-informed world model that jointly learns RGB video generation and\nphysics knowledge within an integrated framework. We introduce two key\nphysics-informed joint training tasks: temporal depth prediction that enhances\n3D geometric consistency in video rendering, and keypoint dynamics learning\nthat implicitly encodes physical properties (e.g., object shape and material\ncharacteristics) while improving complex motion modeling. Extensive experiments\ndemonstrate that RoboScape generates videos with superior visual fidelity and\nphysical plausibility across diverse robotic scenarios. We further validate its\npractical utility through downstream applications including robotic policy\ntraining with generated data and policy evaluation. Our work provides new\ninsights for building efficient physics-informed world models to advance\nembodied intelligence research. The code is available at:\nhttps://github.com/tsinghua-fib-lab/RoboScape.",
      "upvotes": 1,
      "discussionId": "68638235588cea0da970c96d",
      "ai_summary": "RoboScape is a unified physics-informed world model that enhances visual fidelity and physical plausibility in robotic video generation by integrating temporal depth prediction and keypoint dynamics learning.",
      "ai_keywords": [
        "world models",
        "embodied intelligence",
        "RGB video generation",
        "physics knowledge",
        "temporal depth prediction",
        "keypoint dynamics learning",
        "visual fidelity",
        "physical plausibility"
      ]
    },
    "publishedAt": "2025-06-29T04:19:45.000Z",
    "title": "RoboScape: Physics-informed Embodied World Model",
    "summary": "World models have become indispensable tools for embodied intelligence,\nserving as powerful simulators capable of generating realistic robotic videos\nwhile addressing critical data scarcity challenges. However, current embodied\nworld models exhibit limited physical awareness, particularly in modeling 3D\ngeometry and motion dynamics, resulting in unrealistic video generation for\ncontact-rich robotic scenarios. In this paper, we present RoboScape, a unified\nphysics-informed world model that jointly learns RGB video generation and\nphysics knowledge within an integrated framework. We introduce two key\nphysics-informed joint training tasks: temporal depth prediction that enhances\n3D geometric consistency in video rendering, and keypoint dynamics learning\nthat implicitly encodes physical properties (e.g., object shape and material\ncharacteristics) while improving complex motion modeling. Extensive experiments\ndemonstrate that RoboScape generates videos with superior visual fidelity and\nphysical plausibility across diverse robotic scenarios. We further validate its\npractical utility through downstream applications including robotic policy\ntraining with generated data and policy evaluation. Our work provides new\ninsights for building efficient physics-informed world models to advance\nembodied intelligence research. The code is available at:\nhttps://github.com/tsinghua-fib-lab/RoboScape.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6465d3bd63e7e09dd02e95c3/oiWNtmiOwH-sWe0ewE2YQ.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.23135.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6465d3bd63e7e09dd02e95c3",
      "avatarUrl": "/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg",
      "fullname": "Jie Feng",
      "name": "JJ-TMT",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.22992",
      "authors": [
        {
          "_id": "686370bd588cea0da970c90e",
          "name": "Yulun Jiang",
          "hidden": false
        },
        {
          "_id": "686370bd588cea0da970c90f",
          "name": "Yekun Chai",
          "hidden": false
        },
        {
          "_id": "686370bd588cea0da970c910",
          "name": "Maria Brbić",
          "hidden": false
        },
        {
          "_id": "686370bd588cea0da970c911",
          "name": "Michael Moor",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6438d1d843d932c462404500/FI8PgTG5g_uZ1q4y-c_Jd.png"
      ],
      "publishedAt": "2025-06-28T19:44:32.000Z",
      "submittedOnDailyAt": "2025-07-01T05:06:51.086Z",
      "title": "MARBLE: A Hard Benchmark for Multimodal Spatial Reasoning and Planning",
      "submittedOnDailyBy": {
        "_id": "6438d1d843d932c462404500",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6438d1d843d932c462404500/6UrtJbed9N5ETbImgIh-C.png",
        "isPro": false,
        "fullname": "Michael Moor",
        "user": "mdmoor",
        "type": "user"
      },
      "summary": "The ability to process information from multiple modalities and to reason\nthrough it step-by-step remains a critical challenge in advancing artificial\nintelligence. However, existing reasoning benchmarks focus on text-only\nreasoning, or employ multimodal questions that can be answered by directly\nretrieving information from a non-text modality. Thus, complex reasoning\nremains poorly understood in multimodal domains. Here, we present MARBLE, a\nchallenging multimodal reasoning benchmark that is designed to scrutinize\nmultimodal language models (MLLMs) in their ability to carefully reason\nstep-by-step through complex multimodal problems and environments. MARBLE is\ncomposed of two highly challenging tasks, M-Portal and M-Cube, that require the\ncrafting and understanding of multistep plans under spatial, visual, and\nphysical constraints. We find that current MLLMs perform poorly on MARBLE --\nall the 12 advanced models obtain near-random performance on M-Portal and 0%\naccuracy on M-Cube. Only in simplified subtasks some models outperform the\nrandom baseline, indicating that complex reasoning is still a challenge for\nexisting MLLMs. Moreover, we show that perception remains a bottleneck, where\nMLLMs occasionally fail to extract information from the visual inputs. By\nshedding a light on the limitations of MLLMs, we hope that MARBLE will spur the\ndevelopment of the next generation of models with the ability to reason and\nplan across many, multimodal reasoning steps.",
      "upvotes": 1,
      "discussionId": "686370be588cea0da970c912",
      "ai_summary": "MARBLE is a challenging multimodal reasoning benchmark that highlights the limitations of existing multimodal language models in step-by-step reasoning and plan crafting under spatial and visual constraints.",
      "ai_keywords": [
        "multimodal reasoning",
        "multimodal language models",
        "M-Portal",
        "M-Cube",
        "multistep plans",
        "spatial constraints",
        "visual constraints"
      ]
    },
    "publishedAt": "2025-06-28T15:44:32.000Z",
    "title": "MARBLE: A Hard Benchmark for Multimodal Spatial Reasoning and Planning",
    "summary": "The ability to process information from multiple modalities and to reason\nthrough it step-by-step remains a critical challenge in advancing artificial\nintelligence. However, existing reasoning benchmarks focus on text-only\nreasoning, or employ multimodal questions that can be answered by directly\nretrieving information from a non-text modality. Thus, complex reasoning\nremains poorly understood in multimodal domains. Here, we present MARBLE, a\nchallenging multimodal reasoning benchmark that is designed to scrutinize\nmultimodal language models (MLLMs) in their ability to carefully reason\nstep-by-step through complex multimodal problems and environments. MARBLE is\ncomposed of two highly challenging tasks, M-Portal and M-Cube, that require the\ncrafting and understanding of multistep plans under spatial, visual, and\nphysical constraints. We find that current MLLMs perform poorly on MARBLE --\nall the 12 advanced models obtain near-random performance on M-Portal and 0%\naccuracy on M-Cube. Only in simplified subtasks some models outperform the\nrandom baseline, indicating that complex reasoning is still a challenge for\nexisting MLLMs. Moreover, we show that perception remains a bottleneck, where\nMLLMs occasionally fail to extract information from the visual inputs. By\nshedding a light on the limitations of MLLMs, we hope that MARBLE will spur the\ndevelopment of the next generation of models with the ability to reason and\nplan across many, multimodal reasoning steps.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6438d1d843d932c462404500/FI8PgTG5g_uZ1q4y-c_Jd.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.22992.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6438d1d843d932c462404500",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6438d1d843d932c462404500/6UrtJbed9N5ETbImgIh-C.png",
      "fullname": "Michael Moor",
      "name": "mdmoor",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.21448",
      "authors": [
        {
          "_id": "68636ecd588cea0da970c905",
          "name": "Huadai Liu",
          "hidden": false
        },
        {
          "_id": "68636ecd588cea0da970c906",
          "name": "Jialei Wang",
          "hidden": false
        },
        {
          "_id": "68636ecd588cea0da970c907",
          "name": "Kaicheng Luo",
          "hidden": false
        },
        {
          "_id": "68636ecd588cea0da970c908",
          "name": "Wen Wang",
          "hidden": false
        },
        {
          "_id": "68636ecd588cea0da970c909",
          "name": "Qian Chen",
          "hidden": false
        },
        {
          "_id": "68636ecd588cea0da970c90a",
          "name": "Zhou Zhao",
          "hidden": false
        },
        {
          "_id": "68636ecd588cea0da970c90b",
          "name": "Wei Xue",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-26T16:32:06.000Z",
      "submittedOnDailyAt": "2025-07-01T03:47:20.627Z",
      "title": "ThinkSound: Chain-of-Thought Reasoning in Multimodal Large Language\n  Models for Audio Generation and Editing",
      "submittedOnDailyBy": {
        "_id": "63d8c0d3da4f72339241c7dd",
        "avatarUrl": "/avatars/c5852fa7d2b8ffb7a76f0143faa453ef.svg",
        "isPro": false,
        "fullname": "liuhuadai",
        "user": "liuhuadai",
        "type": "user"
      },
      "summary": "While end-to-end video-to-audio generation has greatly improved, producing\nhigh-fidelity audio that authentically captures the nuances of visual content\nremains challenging. Like professionals in the creative industries, such\ngeneration requires sophisticated reasoning about items such as visual\ndynamics, acoustic environments, and temporal relationships. We present\nThinkSound, a novel framework that leverages Chain-of-Thought (CoT) reasoning\nto enable stepwise, interactive audio generation and editing for videos. Our\napproach decomposes the process into three complementary stages: foundational\nfoley generation that creates semantically coherent soundscapes, interactive\nobject-centric refinement through precise user interactions, and targeted\nediting guided by natural language instructions. At each stage, a multimodal\nlarge language model generates contextually aligned CoT reasoning that guides a\nunified audio foundation model. Furthermore, we introduce AudioCoT, a\ncomprehensive dataset with structured reasoning annotations that establishes\nconnections between visual content, textual descriptions, and sound synthesis.\nExperiments demonstrate that ThinkSound achieves state-of-the-art performance\nin video-to-audio generation across both audio metrics and CoT metrics and\nexcels in out-of-distribution Movie Gen Audio benchmark. The demo page is\navailable at https://ThinkSound-Project.github.io.",
      "upvotes": 1,
      "discussionId": "68636ecd588cea0da970c90c",
      "ai_summary": "ThinkSound, a novel framework, uses Chain-of-Thought reasoning with a multimodal large language model to generate high-quality audio from videos, achieving state-of-the-art results in various benchmarks.",
      "ai_keywords": [
        "Chain-of-Thought (CoT) reasoning",
        "multimodal large language model",
        "unified audio foundation model",
        "AudioCoT",
        "video-to-audio generation",
        "foley generation",
        "object-centric refinement",
        "targeted editing",
        "Audio Metrics",
        "CoT metrics",
        "Movie Gen Audio benchmark"
      ]
    },
    "publishedAt": "2025-06-26T12:32:06.000Z",
    "title": "ThinkSound: Chain-of-Thought Reasoning in Multimodal Large Language\n  Models for Audio Generation and Editing",
    "summary": "While end-to-end video-to-audio generation has greatly improved, producing\nhigh-fidelity audio that authentically captures the nuances of visual content\nremains challenging. Like professionals in the creative industries, such\ngeneration requires sophisticated reasoning about items such as visual\ndynamics, acoustic environments, and temporal relationships. We present\nThinkSound, a novel framework that leverages Chain-of-Thought (CoT) reasoning\nto enable stepwise, interactive audio generation and editing for videos. Our\napproach decomposes the process into three complementary stages: foundational\nfoley generation that creates semantically coherent soundscapes, interactive\nobject-centric refinement through precise user interactions, and targeted\nediting guided by natural language instructions. At each stage, a multimodal\nlarge language model generates contextually aligned CoT reasoning that guides a\nunified audio foundation model. Furthermore, we introduce AudioCoT, a\ncomprehensive dataset with structured reasoning annotations that establishes\nconnections between visual content, textual descriptions, and sound synthesis.\nExperiments demonstrate that ThinkSound achieves state-of-the-art performance\nin video-to-audio generation across both audio metrics and CoT metrics and\nexcels in out-of-distribution Movie Gen Audio benchmark. The demo page is\navailable at https://ThinkSound-Project.github.io.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.21448.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63d8c0d3da4f72339241c7dd",
      "avatarUrl": "/avatars/c5852fa7d2b8ffb7a76f0143faa453ef.svg",
      "fullname": "liuhuadai",
      "name": "liuhuadai",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  }
]