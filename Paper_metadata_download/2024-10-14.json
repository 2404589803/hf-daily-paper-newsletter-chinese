[
    {
        "paper": {
            "id": "2410.08565",
            "authors": [
                {
                    "_id": "670c7526d01f57eeed9ecb02",
                    "name": "Yadong Li",
                    "hidden": false
                },
                {
                    "_id": "670c7526d01f57eeed9ecb03",
                    "name": "Haoze Sun",
                    "hidden": false
                },
                {
                    "_id": "670c7526d01f57eeed9ecb04",
                    "user": {
                        "_id": "6415947858a690df103af49f",
                        "avatarUrl": "/avatars/38aec23b869833bceb25b9250809b419.svg",
                        "isPro": false,
                        "fullname": "lma",
                        "user": "lin5547",
                        "type": "user"
                    },
                    "name": "Mingan Lin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-14T07:48:30.747Z",
                    "hidden": false
                },
                {
                    "_id": "670c7526d01f57eeed9ecb05",
                    "user": {
                        "_id": "64be4c97ef8c0e42bf448469",
                        "avatarUrl": "/avatars/a2a45b5f17d3e27c59547312bcf7bb52.svg",
                        "isPro": false,
                        "fullname": "Tianpeng Li",
                        "user": "TJU-Tianpengli",
                        "type": "user"
                    },
                    "name": "Tianpeng Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-14T07:58:36.547Z",
                    "hidden": false
                },
                {
                    "_id": "670c7526d01f57eeed9ecb06",
                    "user": {
                        "_id": "668d2ff4271bd45dab6a8e71",
                        "avatarUrl": "/avatars/4e198c35a9f09bbc6551c34148aaf560.svg",
                        "isPro": false,
                        "fullname": "guosheng dong",
                        "user": "dongguosheng",
                        "type": "user"
                    },
                    "name": "Guosheng Dong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-14T07:59:42.904Z",
                    "hidden": false
                },
                {
                    "_id": "670c7526d01f57eeed9ecb07",
                    "name": "Tao Zhang",
                    "hidden": false
                },
                {
                    "_id": "670c7526d01f57eeed9ecb08",
                    "user": {
                        "_id": "6302fe4f2a9eff961439f2bf",
                        "avatarUrl": "/avatars/ee514cdab037c8f50058b4421954d919.svg",
                        "isPro": false,
                        "fullname": "Ding Bowen",
                        "user": "dbv",
                        "type": "user"
                    },
                    "name": "Bowen Ding",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-14T08:00:06.367Z",
                    "hidden": false
                },
                {
                    "_id": "670c7526d01f57eeed9ecb09",
                    "name": "Wei Song",
                    "hidden": false
                },
                {
                    "_id": "670c7526d01f57eeed9ecb0a",
                    "user": {
                        "_id": "65028e8389707f182386588c",
                        "avatarUrl": "/avatars/d2fd7637045dc2c19f1f45a9d62d610b.svg",
                        "isPro": false,
                        "fullname": "Zhenglin Cheng",
                        "user": "kenshinn",
                        "type": "user"
                    },
                    "name": "Zhenglin Cheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-14T07:48:32.690Z",
                    "hidden": false
                },
                {
                    "_id": "670c7526d01f57eeed9ecb0b",
                    "name": "Yuqi Huo",
                    "hidden": false
                },
                {
                    "_id": "670c7526d01f57eeed9ecb0c",
                    "name": "Song Chen",
                    "hidden": false
                },
                {
                    "_id": "670c7526d01f57eeed9ecb0d",
                    "name": "Xu Li",
                    "hidden": false
                },
                {
                    "_id": "670c7526d01f57eeed9ecb0e",
                    "name": "Da Pan",
                    "hidden": false
                },
                {
                    "_id": "670c7526d01f57eeed9ecb0f",
                    "user": {
                        "_id": "64d9c9bd95cf13a381f6cc1a",
                        "avatarUrl": "/avatars/ce138298df376511044c69182b6419b9.svg",
                        "isPro": false,
                        "fullname": "zss",
                        "user": "zss9606",
                        "type": "user"
                    },
                    "name": "Shusen Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-14T07:48:23.098Z",
                    "hidden": false
                },
                {
                    "_id": "670c7526d01f57eeed9ecb10",
                    "name": "Xin Wu",
                    "hidden": false
                },
                {
                    "_id": "670c7526d01f57eeed9ecb11",
                    "name": "Zheng Liang",
                    "hidden": false
                },
                {
                    "_id": "670c7526d01f57eeed9ecb12",
                    "name": "Jun Liu",
                    "hidden": false
                },
                {
                    "_id": "670c7526d01f57eeed9ecb13",
                    "name": "Tao Zhang",
                    "hidden": false
                },
                {
                    "_id": "670c7526d01f57eeed9ecb14",
                    "name": "Keer Lu",
                    "hidden": false
                },
                {
                    "_id": "670c7526d01f57eeed9ecb15",
                    "name": "Yaqi Zhao",
                    "hidden": false
                },
                {
                    "_id": "670c7526d01f57eeed9ecb16",
                    "user": {
                        "_id": "651e0a9f60e6a3ef9a1fd38b",
                        "avatarUrl": "/avatars/50b2b5bf380fa1f754fc8e53a4d08f19.svg",
                        "isPro": false,
                        "fullname": "Yanjun Sheng",
                        "user": "Astroshiba",
                        "type": "user"
                    },
                    "name": "Yanjun Shen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-14T08:11:33.707Z",
                    "hidden": false
                },
                {
                    "_id": "670c7526d01f57eeed9ecb17",
                    "name": "Fan Yang",
                    "hidden": false
                },
                {
                    "_id": "670c7526d01f57eeed9ecb18",
                    "name": "Kaicheng Yu",
                    "hidden": false
                },
                {
                    "_id": "670c7526d01f57eeed9ecb19",
                    "name": "Tao Lin",
                    "hidden": false
                },
                {
                    "_id": "670c7526d01f57eeed9ecb1a",
                    "user": {
                        "_id": "66042414ded78d7454b9746e",
                        "avatarUrl": "/avatars/828feab7015b527b5ac5c9238fa3c7e0.svg",
                        "isPro": false,
                        "fullname": "jianhua Xu",
                        "user": "ArthurXu2020",
                        "type": "user"
                    },
                    "name": "Jianhua Xu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-14T08:01:31.350Z",
                    "hidden": false
                },
                {
                    "_id": "670c7526d01f57eeed9ecb1b",
                    "user": {
                        "_id": "668d4e50ed63008dfaa78304",
                        "avatarUrl": "/avatars/80854a3c6b4b7c70cd46694d4cf7296a.svg",
                        "isPro": false,
                        "fullname": "Zenan Zhou",
                        "user": "Zenan11",
                        "type": "user"
                    },
                    "name": "Zenan Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-14T08:01:23.922Z",
                    "hidden": false
                },
                {
                    "_id": "670c7526d01f57eeed9ecb1c",
                    "user": {
                        "_id": "6501587887b370a56ad2608e",
                        "avatarUrl": "/avatars/6779baaa8ed9032de55a2f78e1f52e20.svg",
                        "isPro": false,
                        "fullname": "Wei-Peng Chen",
                        "user": "whenfra",
                        "type": "user"
                    },
                    "name": "Weipeng Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-14T08:01:15.261Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-11T06:44:31.000Z",
            "title": "Baichuan-Omni Technical Report",
            "summary": "The salient multimodal capabilities and interactive experience of GPT-4o\nhighlight its critical role in practical applications, yet it lacks a\nhigh-performing open-source counterpart. In this paper, we introduce\nBaichuan-Omni, the first open-source 7B Multimodal Large Language Model (MLLM)\nadept at concurrently processing and analyzing modalities of image, video,\naudio, and text, while delivering an advanced multimodal interactive experience\nand strong performance. We propose an effective multimodal training schema\nstarting with 7B model and proceeding through two stages of multimodal\nalignment and multitask fine-tuning across audio, image, video, and text modal.\nThis approach equips the language model with the ability to handle visual and\naudio data effectively. Demonstrating strong performance across various\nomni-modal and multimodal benchmarks, we aim for this contribution to serve as\na competitive baseline for the open-source community in advancing multimodal\nunderstanding and real-time interaction.",
            "upvotes": 54,
            "discussionId": "670c7529d01f57eeed9ecbfa"
        },
        "publishedAt": "2024-10-14T00:29:48.304Z",
        "title": "Baichuan-Omni Technical Report",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.08565.png",
        "numComments": 6,
        "submittedBy": {
            "avatarUrl": "/avatars/d2fd7637045dc2c19f1f45a9d62d610b.svg",
            "fullname": "Zhenglin Cheng",
            "name": "kenshinn",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.08261",
            "authors": [
                {
                    "_id": "670c859d0b54b420bb46330e",
                    "user": {
                        "_id": "63fccdac93b993a4ebd7789a",
                        "avatarUrl": "/avatars/a27443f883dbde0b0d1926010efa9be4.svg",
                        "isPro": false,
                        "fullname": "Jinbin Bai",
                        "user": "BryanW",
                        "type": "user"
                    },
                    "name": "Jinbin Bai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-14T07:48:20.758Z",
                    "hidden": false
                },
                {
                    "_id": "670c859d0b54b420bb46330f",
                    "user": {
                        "_id": "66015e8aa4d296af07de538e",
                        "avatarUrl": "/avatars/a1295c631cc2646282c545859975ce4c.svg",
                        "isPro": false,
                        "fullname": "Ye",
                        "user": "Owen777",
                        "type": "user"
                    },
                    "name": "Tian Ye",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-14T07:48:17.347Z",
                    "hidden": false
                },
                {
                    "_id": "670c859d0b54b420bb463310",
                    "user": {
                        "_id": "644b71ddb2e7823a76abcf91",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644b71ddb2e7823a76abcf91/JPF7Eqeq2jx8i79nQ962K.jpeg",
                        "isPro": false,
                        "fullname": "zhou wei",
                        "user": "WeiChow",
                        "type": "user"
                    },
                    "name": "Wei Chow",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-14T08:12:54.075Z",
                    "hidden": false
                },
                {
                    "_id": "670c859d0b54b420bb463311",
                    "user": {
                        "_id": "63a7b196e27a6dbd4861e275",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a7b196e27a6dbd4861e275/QGn4hdDPNuSRgZVKxzDpq.jpeg",
                        "isPro": false,
                        "fullname": "EnxinSong",
                        "user": "Enxin",
                        "type": "user"
                    },
                    "name": "Enxin Song",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-14T08:13:06.950Z",
                    "hidden": false
                },
                {
                    "_id": "670c859d0b54b420bb463312",
                    "name": "Qing-Guo Chen",
                    "hidden": false
                },
                {
                    "_id": "670c859d0b54b420bb463313",
                    "user": {
                        "_id": "63958b4414513eaf9029ebf1",
                        "avatarUrl": "/avatars/1f5e9b9dfcc16df8e88e3dcecfcb4e10.svg",
                        "isPro": false,
                        "fullname": "Xiangtai Li",
                        "user": "LXT",
                        "type": "user"
                    },
                    "name": "Xiangtai Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-14T07:48:15.462Z",
                    "hidden": false
                },
                {
                    "_id": "670c859d0b54b420bb463314",
                    "user": {
                        "_id": "643ba2f725681c3afaa8f05e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643ba2f725681c3afaa8f05e/2RnOdmbBM8WHYhiTGG-Cd.jpeg",
                        "isPro": false,
                        "fullname": "Zhen Dong",
                        "user": "zhendongucb",
                        "type": "user"
                    },
                    "name": "Zhen Dong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-14T07:48:19.035Z",
                    "hidden": false
                },
                {
                    "_id": "670c859d0b54b420bb463315",
                    "name": "Lei Zhu",
                    "hidden": false
                },
                {
                    "_id": "670c859d0b54b420bb463316",
                    "name": "Shuicheng Yan",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-10T17:59:17.000Z",
            "title": "Meissonic: Revitalizing Masked Generative Transformers for Efficient\n  High-Resolution Text-to-Image Synthesis",
            "summary": "Diffusion models, such as Stable Diffusion, have made significant strides in\nvisual generation, yet their paradigm remains fundamentally different from\nautoregressive language models, complicating the development of unified\nlanguage-vision models. Recent efforts like LlamaGen have attempted\nautoregressive image generation using discrete VQVAE tokens, but the large\nnumber of tokens involved renders this approach inefficient and slow. In this\nwork, we present Meissonic, which elevates non-autoregressive masked image\nmodeling (MIM) text-to-image to a level comparable with state-of-the-art\ndiffusion models like SDXL. By incorporating a comprehensive suite of\narchitectural innovations, advanced positional encoding strategies, and\noptimized sampling conditions, Meissonic substantially improves MIM's\nperformance and efficiency. Additionally, we leverage high-quality training\ndata, integrate micro-conditions informed by human preference scores, and\nemploy feature compression layers to further enhance image fidelity and\nresolution. Our model not only matches but often exceeds the performance of\nexisting models like SDXL in generating high-quality, high-resolution images.\nExtensive experiments validate Meissonic's capabilities, demonstrating its\npotential as a new standard in text-to-image synthesis. We release a model\ncheckpoint capable of producing 1024 times 1024 resolution images.",
            "upvotes": 35,
            "discussionId": "670c85b10b54b420bb46411d"
        },
        "publishedAt": "2024-10-14T01:19:46.734Z",
        "title": "Meissonic: Revitalizing Masked Generative Transformers for Efficient High-Resolution Text-to-Image Synthesis",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.08261.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/1f5e9b9dfcc16df8e88e3dcecfcb4e10.svg",
            "fullname": "Xiangtai Li",
            "name": "LXT",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.06456",
            "authors": [
                {
                    "_id": "670a14c002d531812e0467a3",
                    "user": {
                        "_id": "64802423c7f87934d081bad3",
                        "avatarUrl": "/avatars/32b07a84662cbf73a0adbb325088440b.svg",
                        "isPro": false,
                        "fullname": "Yang bai",
                        "user": "yangbai123",
                        "type": "user"
                    },
                    "name": "Yang Bai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-13T20:15:39.915Z",
                    "hidden": false
                },
                {
                    "_id": "670a14c002d531812e0467a4",
                    "name": "Yang Zhou",
                    "hidden": false
                },
                {
                    "_id": "670a14c002d531812e0467a5",
                    "name": "Jun Zhou",
                    "hidden": false
                },
                {
                    "_id": "670a14c002d531812e0467a6",
                    "name": "Rick Siow Mong Goh",
                    "hidden": false
                },
                {
                    "_id": "670a14c002d531812e0467a7",
                    "name": "Daniel Shu Wei Ting",
                    "hidden": false
                },
                {
                    "_id": "670a14c002d531812e0467a8",
                    "name": "Yong Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-09T01:24:04.000Z",
            "title": "From Generalist to Specialist: Adapting Vision Language Models via\n  Task-Specific Visual Instruction Tuning",
            "summary": "Large vision language models (VLMs) combine large language models with vision\nencoders, demonstrating promise across various tasks. However, they often\nunderperform in task-specific applications due to domain gaps between\npre-training and fine-tuning. We introduce VITask, a novel framework that\nenhances task-specific adaptability of VLMs by integrating task-specific models\n(TSMs). VITask employs three key strategies: exemplar prompting (EP), response\ndistribution alignment (RDA), and contrastive response tuning (CRT) to improve\nthe task-specific performance of VLMs by adjusting their response\ndistributions. EP allows TSM features to guide VLMs, while RDA enables VLMs to\nadapt without TSMs during inference by learning from exemplar-prompted models.\nCRT further optimizes the ranking of correct image-response pairs, thereby\nreducing the risk of generating undesired responses. Experiments on 12 medical\ndiagnosis datasets across 9 imaging modalities show that VITask outperforms\nboth vanilla instruction-tuned VLMs and TSMs, showcasing its ability to\nintegrate complementary features from both models effectively. Additionally,\nVITask offers practical advantages such as flexible TSM integration and\nrobustness to incomplete instructions, making it a versatile and efficient\nsolution for task-specific VLM tuning. Our code are available at\nhttps://github.com/baiyang4/VITask.",
            "upvotes": 25,
            "discussionId": "670a14c102d531812e0467e2"
        },
        "publishedAt": "2024-10-14T02:10:18.055Z",
        "title": "From Generalist to Specialist: Adapting Vision Language Models via Task-Specific Visual Instruction Tuning",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.06456.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/32b07a84662cbf73a0adbb325088440b.svg",
            "fullname": "Yang bai",
            "name": "yangbai123",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.07133",
            "authors": [
                {
                    "_id": "67074fe901f0e95af7736253",
                    "user": {
                        "_id": "652b83b73b5997ed71a310f2",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652b83b73b5997ed71a310f2/ipCpdeHUp4-0OmRz5z8IW.png",
                        "isPro": false,
                        "fullname": "Rui Zhao",
                        "user": "ruizhaocv",
                        "type": "user"
                    },
                    "name": "Rui Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-14T07:48:43.532Z",
                    "hidden": false
                },
                {
                    "_id": "67074fe901f0e95af7736254",
                    "user": {
                        "_id": "649d54b314afbb10ce2a9eeb",
                        "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
                        "isPro": false,
                        "fullname": "Hangjie Yuan",
                        "user": "JacobYuan",
                        "type": "user"
                    },
                    "name": "Hangjie Yuan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-14T08:34:30.810Z",
                    "hidden": false
                },
                {
                    "_id": "67074fe901f0e95af7736255",
                    "user": {
                        "_id": "637f70d6fab5db9101c3dfc8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f70d6fab5db9101c3dfc8/NgkYNXWLDavLbrnCby2Fl.jpeg",
                        "isPro": false,
                        "fullname": "Yujie Wei",
                        "user": "weilllllls",
                        "type": "user"
                    },
                    "name": "Yujie Wei",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-14T12:21:30.590Z",
                    "hidden": false
                },
                {
                    "_id": "67074fe901f0e95af7736256",
                    "user": {
                        "_id": "63a1200045edac9f7508bae9",
                        "avatarUrl": "/avatars/e84f0b045f32c5b8b4da43458650b925.svg",
                        "isPro": false,
                        "fullname": "Shiwei Zhang",
                        "user": "StevenZhang",
                        "type": "user"
                    },
                    "name": "Shiwei Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-14T12:21:57.406Z",
                    "hidden": false
                },
                {
                    "_id": "67074fe901f0e95af7736257",
                    "name": "Yuchao Gu",
                    "hidden": false
                },
                {
                    "_id": "67074fe901f0e95af7736258",
                    "name": "Lingmin Ran",
                    "hidden": false
                },
                {
                    "_id": "67074fe901f0e95af7736259",
                    "user": {
                        "_id": "65fca775fa59bdf4737b1a84",
                        "avatarUrl": "/avatars/a161b510bde8f57e7686cbb0b4aa6a52.svg",
                        "isPro": false,
                        "fullname": "Xiang Wang",
                        "user": "xiangwang1223",
                        "type": "user"
                    },
                    "name": "Xiang Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-14T12:19:58.663Z",
                    "hidden": false
                },
                {
                    "_id": "67074fe901f0e95af773625a",
                    "name": "Zhangjie Wu",
                    "hidden": false
                },
                {
                    "_id": "67074fe901f0e95af773625b",
                    "name": "Junhao Zhang",
                    "hidden": false
                },
                {
                    "_id": "67074fe901f0e95af773625c",
                    "name": "Yingya Zhang",
                    "hidden": false
                },
                {
                    "_id": "67074fe901f0e95af773625d",
                    "user": {
                        "_id": "661ab3da2b14565c7acccf5c",
                        "avatarUrl": "/avatars/fa4fc03664803e02aede4d4c3d50b393.svg",
                        "isPro": false,
                        "fullname": "Mike Zheng Shou",
                        "user": "AnalMom",
                        "type": "user"
                    },
                    "name": "Mike Zheng Shou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-14T12:03:10.342Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-09T17:52:28.000Z",
            "title": "EvolveDirector: Approaching Advanced Text-to-Image Generation with Large\n  Vision-Language Models",
            "summary": "Recent advancements in generation models have showcased remarkable\ncapabilities in generating fantastic content. However, most of them are trained\non proprietary high-quality data, and some models withhold their parameters and\nonly provide accessible application programming interfaces (APIs), limiting\ntheir benefits for downstream tasks. To explore the feasibility of training a\ntext-to-image generation model comparable to advanced models using publicly\navailable resources, we introduce EvolveDirector. This framework interacts with\nadvanced models through their public APIs to obtain text-image data pairs to\ntrain a base model. Our experiments with extensive data indicate that the model\ntrained on generated data of the advanced model can approximate its generation\ncapability. However, it requires large-scale samples of 10 million or more.\nThis incurs significant expenses in time, computational resources, and\nespecially the costs associated with calling fee-based APIs. To address this\nproblem, we leverage pre-trained large vision-language models (VLMs) to guide\nthe evolution of the base model. VLM continuously evaluates the base model\nduring training and dynamically updates and refines the training dataset by the\ndiscrimination, expansion, deletion, and mutation operations. Experimental\nresults show that this paradigm significantly reduces the required data volume.\nFurthermore, when approaching multiple advanced models, EvolveDirector can\nselect the best samples generated by them to learn powerful and balanced\nabilities. The final trained model Edgen is demonstrated to outperform these\nadvanced models. The code and model weights are available at\nhttps://github.com/showlab/EvolveDirector.",
            "upvotes": 15,
            "discussionId": "67074feb01f0e95af7736308"
        },
        "publishedAt": "2024-10-14T00:17:02.232Z",
        "title": "EvolveDirector: Approaching Advanced Text-to-Image Generation with Large Vision-Language Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.07133.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652b83b73b5997ed71a310f2/ipCpdeHUp4-0OmRz5z8IW.png",
            "fullname": "Rui Zhao",
            "name": "ruizhaocv",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.08815",
            "authors": [
                {
                    "_id": "670c8c2a7603a5cfa432bfb9",
                    "user": {
                        "_id": "63664c8fa2abcdf2fd6425ed",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63664c8fa2abcdf2fd6425ed/IywpB0DXZ_twkmZmVSCCD.jpeg",
                        "isPro": false,
                        "fullname": "Li Zhuoqun",
                        "user": "lzq2021",
                        "type": "user"
                    },
                    "name": "Zhuoqun Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-14T12:16:05.106Z",
                    "hidden": false
                },
                {
                    "_id": "670c8c2a7603a5cfa432bfba",
                    "name": "Xuanang Chen",
                    "hidden": false
                },
                {
                    "_id": "670c8c2a7603a5cfa432bfbb",
                    "name": "Haiyang Yu",
                    "hidden": false
                },
                {
                    "_id": "670c8c2a7603a5cfa432bfbc",
                    "name": "Hongyu Lin",
                    "hidden": false
                },
                {
                    "_id": "670c8c2a7603a5cfa432bfbd",
                    "name": "Yaojie Lu",
                    "hidden": false
                },
                {
                    "_id": "670c8c2a7603a5cfa432bfbe",
                    "name": "Qiaoyu Tang",
                    "hidden": false
                },
                {
                    "_id": "670c8c2a7603a5cfa432bfbf",
                    "name": "Fei Huang",
                    "hidden": false
                },
                {
                    "_id": "670c8c2a7603a5cfa432bfc0",
                    "user": {
                        "_id": "65e99a77e71555ed193609cf",
                        "avatarUrl": "/avatars/38ceb127883944677665da967d17dd18.svg",
                        "isPro": false,
                        "fullname": "Xianpei Han",
                        "user": "xphan",
                        "type": "user"
                    },
                    "name": "Xianpei Han",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-14T10:47:48.178Z",
                    "hidden": false
                },
                {
                    "_id": "670c8c2a7603a5cfa432bfc1",
                    "name": "Le Sun",
                    "hidden": false
                },
                {
                    "_id": "670c8c2a7603a5cfa432bfc2",
                    "user": {
                        "_id": "66641b2fd8e1e34bc621e688",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66641b2fd8e1e34bc621e688/csPETwnx2zCIHSWi9uAi-.png",
                        "isPro": false,
                        "fullname": "Yongbin Li",
                        "user": "Robin-Lee",
                        "type": "user"
                    },
                    "name": "Yongbin Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-14T10:47:46.291Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-11T13:52:44.000Z",
            "title": "StructRAG: Boosting Knowledge Intensive Reasoning of LLMs via\n  Inference-time Hybrid Information Structurization",
            "summary": "Retrieval-augmented generation (RAG) is a key means to effectively enhance\nlarge language models (LLMs) in many knowledge-based tasks. However, existing\nRAG methods struggle with knowledge-intensive reasoning tasks, because useful\ninformation required to these tasks are badly scattered. This characteristic\nmakes it difficult for existing RAG methods to accurately identify key\ninformation and perform global reasoning with such noisy augmentation. In this\npaper, motivated by the cognitive theories that humans convert raw information\ninto various structured knowledge when tackling knowledge-intensive reasoning,\nwe proposes a new framework, StructRAG, which can identify the optimal\nstructure type for the task at hand, reconstruct original documents into this\nstructured format, and infer answers based on the resulting structure.\nExtensive experiments across various knowledge-intensive tasks show that\nStructRAG achieves state-of-the-art performance, particularly excelling in\nchallenging scenarios, demonstrating its potential as an effective solution for\nenhancing LLMs in complex real-world applications.",
            "upvotes": 11,
            "discussionId": "670c8c2b7603a5cfa432bfe6"
        },
        "publishedAt": "2024-10-14T04:14:19.698Z",
        "title": "StructRAG: Boosting Knowledge Intensive Reasoning of LLMs via Inference-time Hybrid Information Structurization",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.08815.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62e0ef42edb0462c8d51818d/3YM7DUynIWiiRFM6_enpg.jpeg",
            "fullname": "Ting-En Lin",
            "name": "tnlin",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.07035",
            "authors": [
                {
                    "_id": "6707d46de2deb345cb3bda89",
                    "name": "Zekun Wang",
                    "hidden": false
                },
                {
                    "_id": "6707d46de2deb345cb3bda8a",
                    "name": "Feiyu Duan",
                    "hidden": false
                },
                {
                    "_id": "6707d46de2deb345cb3bda8b",
                    "name": "Yibo Zhang",
                    "hidden": false
                },
                {
                    "_id": "6707d46de2deb345cb3bda8c",
                    "user": {
                        "_id": "628c8598ef14f971b698107f",
                        "avatarUrl": "/avatars/3a4ad87e6b5f9e836a1160d869df1447.svg",
                        "isPro": false,
                        "fullname": "Zhou",
                        "user": "Wangchunshu",
                        "type": "user"
                    },
                    "name": "Wangchunshu Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-14T11:16:28.376Z",
                    "hidden": false
                },
                {
                    "_id": "6707d46de2deb345cb3bda8d",
                    "name": "Ke Xu",
                    "hidden": false
                },
                {
                    "_id": "6707d46de2deb345cb3bda8e",
                    "user": {
                        "_id": "65bb11cb00a03997849e9e85",
                        "avatarUrl": "/avatars/17022b0254192a837f4fe00d84389cda.svg",
                        "isPro": false,
                        "fullname": "Wenhao Huang",
                        "user": "StephenHuang",
                        "type": "user"
                    },
                    "name": "Wenhao Huang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-13T20:16:29.003Z",
                    "hidden": false
                },
                {
                    "_id": "6707d46de2deb345cb3bda8f",
                    "name": "Jie Fu",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-09T16:15:36.000Z",
            "title": "PositionID: LLMs can Control Lengths, Copy and Paste with Explicit\n  Positional Awareness",
            "summary": "Large Language Models (LLMs) demonstrate impressive capabilities across\nvarious domains, including role-playing, creative writing, mathematical\nreasoning, and coding. Despite these advancements, LLMs still encounter\nchallenges with length control, frequently failing to adhere to specific length\nconstraints due to their token-level operations and insufficient training on\ndata with strict length limitations. We identify this issue as stemming from a\nlack of positional awareness and propose novel approaches--PositionID Prompting\nand PositionID Fine-Tuning--to address it. These methods enhance the model's\nability to continuously monitor and manage text length during generation.\nAdditionally, we introduce PositionID CP Prompting to enable LLMs to perform\ncopy and paste operations accurately. Furthermore, we develop two benchmarks\nfor evaluating length control and copy-paste abilities. Our experiments\ndemonstrate that our methods significantly improve the model's adherence to\nlength constraints and copy-paste accuracy without compromising response\nquality.",
            "upvotes": 11,
            "discussionId": "6707d46de2deb345cb3bdad4"
        },
        "publishedAt": "2024-10-14T02:03:18.722Z",
        "title": "PositionID: LLMs can Control Lengths, Copy and Paste with Explicit Positional Awareness",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.07035.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6149a9e95347647e6bb68882/Jddln1FxScCeVgTSCNBpr.png",
            "fullname": "Zekun Moore Wang",
            "name": "ZenMoore",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.09009",
            "authors": [
                {
                    "_id": "670c7992f6b3c3ad2cbe2f2b",
                    "user": {
                        "_id": "64fde4e252e82dd432b74ce9",
                        "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
                        "isPro": false,
                        "fullname": "Ling Yang",
                        "user": "Lingaaaaaaa",
                        "type": "user"
                    },
                    "name": "Ling Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-14T12:35:15.951Z",
                    "hidden": false
                },
                {
                    "_id": "670c7992f6b3c3ad2cbe2f2c",
                    "name": "Zixiang Zhang",
                    "hidden": false
                },
                {
                    "_id": "670c7992f6b3c3ad2cbe2f2d",
                    "name": "Junlin Han",
                    "hidden": false
                },
                {
                    "_id": "670c7992f6b3c3ad2cbe2f2e",
                    "user": {
                        "_id": "6671214c92412fd4640714eb",
                        "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg",
                        "isPro": false,
                        "fullname": "bohan zeng",
                        "user": "zbhpku",
                        "type": "user"
                    },
                    "name": "Bohan Zeng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-14T12:30:09.672Z",
                    "hidden": false
                },
                {
                    "_id": "670c7992f6b3c3ad2cbe2f2f",
                    "user": {
                        "_id": "638e29cf319f9c746b87ad4b",
                        "avatarUrl": "/avatars/70cac8d47847c389eb0393051a64c4a4.svg",
                        "isPro": false,
                        "fullname": "Runjia Li",
                        "user": "liguang0115",
                        "type": "user"
                    },
                    "name": "Runjia Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-14T12:29:45.711Z",
                    "hidden": false
                },
                {
                    "_id": "670c7992f6b3c3ad2cbe2f30",
                    "user": {
                        "_id": "6565ed28a5ec0231cb07225f",
                        "avatarUrl": "/avatars/7f95bba9aa7811d56eecb380827abfac.svg",
                        "isPro": false,
                        "fullname": "prof philip torr",
                        "user": "philiptorr",
                        "type": "user"
                    },
                    "name": "Philip Torr",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-14T12:23:22.107Z",
                    "hidden": false
                },
                {
                    "_id": "670c7992f6b3c3ad2cbe2f31",
                    "name": "Wentao Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-11T17:26:00.000Z",
            "title": "Semantic Score Distillation Sampling for Compositional Text-to-3D\n  Generation",
            "summary": "Generating high-quality 3D assets from textual descriptions remains a pivotal\nchallenge in computer graphics and vision research. Due to the scarcity of 3D\ndata, state-of-the-art approaches utilize pre-trained 2D diffusion priors,\noptimized through Score Distillation Sampling (SDS). Despite progress, crafting\ncomplex 3D scenes featuring multiple objects or intricate interactions is still\ndifficult. To tackle this, recent methods have incorporated box or layout\nguidance. However, these layout-guided compositional methods often struggle to\nprovide fine-grained control, as they are generally coarse and lack\nexpressiveness. To overcome these challenges, we introduce a novel SDS\napproach, Semantic Score Distillation Sampling (SemanticSDS), designed to\neffectively improve the expressiveness and accuracy of compositional text-to-3D\ngeneration. Our approach integrates new semantic embeddings that maintain\nconsistency across different rendering views and clearly differentiate between\nvarious objects and parts. These embeddings are transformed into a semantic\nmap, which directs a region-specific SDS process, enabling precise optimization\nand compositional generation. By leveraging explicit semantic guidance, our\nmethod unlocks the compositional capabilities of existing pre-trained diffusion\nmodels, thereby achieving superior quality in 3D content generation,\nparticularly for complex objects and scenes. Experimental results demonstrate\nthat our SemanticSDS framework is highly effective for generating\nstate-of-the-art complex 3D content. Code:\nhttps://github.com/YangLing0818/SemanticSDS-3D",
            "upvotes": 10,
            "discussionId": "670c7994f6b3c3ad2cbe3018"
        },
        "publishedAt": "2024-10-14T00:24:03.914Z",
        "title": "Semantic Score Distillation Sampling for Compositional Text-to-3D Generation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.09009.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
            "fullname": "Ling Yang",
            "name": "Lingaaaaaaa",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.09008",
            "authors": [
                {
                    "_id": "670c74c39f3ee99ddfb15805",
                    "user": {
                        "_id": "64fde4e252e82dd432b74ce9",
                        "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
                        "isPro": false,
                        "fullname": "Ling Yang",
                        "user": "Lingaaaaaaa",
                        "type": "user"
                    },
                    "name": "Ling Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-14T12:35:39.751Z",
                    "hidden": false
                },
                {
                    "_id": "670c74c39f3ee99ddfb15806",
                    "user": {
                        "_id": "64a131a7660cce8b86bf288d",
                        "avatarUrl": "/avatars/6c1a2475645a1a6ae3f804fe6c35a226.svg",
                        "isPro": false,
                        "fullname": "zhao chen yu",
                        "user": "chenyu01",
                        "type": "user"
                    },
                    "name": "Zhaochen Yu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-14T12:35:45.584Z",
                    "hidden": false
                },
                {
                    "_id": "670c74c39f3ee99ddfb15807",
                    "user": {
                        "_id": "6374cd6b6ea8da14f8fef8dc",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6374cd6b6ea8da14f8fef8dc/l13bg0tKDjCnUw3I895QZ.png",
                        "isPro": false,
                        "fullname": "Tianjun Zhang",
                        "user": "tianjunz",
                        "type": "user"
                    },
                    "name": "Tianjun Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-14T12:35:51.068Z",
                    "hidden": false
                },
                {
                    "_id": "670c74c39f3ee99ddfb15808",
                    "user": {
                        "_id": "64c0e950aa57599de1c75dad",
                        "avatarUrl": "/avatars/374d53317cbccc30fae70e5152ca13e0.svg",
                        "isPro": false,
                        "fullname": "Minkai Xu",
                        "user": "mkxu",
                        "type": "user"
                    },
                    "name": "Minkai Xu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-14T12:35:57.450Z",
                    "hidden": false
                },
                {
                    "_id": "670c74c39f3ee99ddfb15809",
                    "user": {
                        "_id": "645d2e8401f4eaab2a0878ce",
                        "avatarUrl": "/avatars/1273c5fb607b4b622a746a42692fa632.svg",
                        "isPro": false,
                        "fullname": "Joseph E. Gonzalez",
                        "user": "ProfJoeyG",
                        "type": "user"
                    },
                    "name": "Joseph E. Gonzalez",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-14T12:36:03.351Z",
                    "hidden": false
                },
                {
                    "_id": "670c74c39f3ee99ddfb1580a",
                    "name": "Bin Cui",
                    "hidden": false
                },
                {
                    "_id": "670c74c39f3ee99ddfb1580b",
                    "name": "Shuicheng Yan",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-11T17:25:52.000Z",
            "title": "SuperCorrect: Supervising and Correcting Language Models with\n  Error-Driven Insights",
            "summary": "Large language models (LLMs) like GPT-4, PaLM, and LLaMA have shown\nsignificant improvements in various reasoning tasks. However, smaller models\nsuch as Llama-3-8B and DeepSeekMath-Base still struggle with complex\nmathematical reasoning because they fail to effectively identify and correct\nreasoning errors. Recent reflection-based methods aim to address these issues\nby enabling self-reflection and self-correction, but they still face challenges\nin independently detecting errors in their reasoning steps. To overcome these\nlimitations, we propose SuperCorrect, a novel two-stage framework that uses a\nlarge teacher model to supervise and correct both the reasoning and reflection\nprocesses of a smaller student model. In the first stage, we extract\nhierarchical high-level and detailed thought templates from the teacher model\nto guide the student model in eliciting more fine-grained reasoning thoughts.\nIn the second stage, we introduce cross-model collaborative direct preference\noptimization (DPO) to enhance the self-correction abilities of the student\nmodel by following the teacher's correction traces during training. This\ncross-model DPO approach teaches the student model to effectively locate and\nresolve erroneous thoughts with error-driven insights from the teacher model,\nbreaking the bottleneck of its thoughts and acquiring new skills and knowledge\nto tackle challenging problems. Extensive experiments consistently demonstrate\nour superiority over previous methods. Notably, our SuperCorrect-7B model\nsignificantly surpasses powerful DeepSeekMath-7B by 7.8%/5.3% and\nQwen2.5-Math-7B by 15.1%/6.3% on MATH/GSM8K benchmarks, achieving new SOTA\nperformance among all 7B models. Code:\nhttps://github.com/YangLing0818/SuperCorrect-llm",
            "upvotes": 10,
            "discussionId": "670c74c39f3ee99ddfb1584b"
        },
        "publishedAt": "2024-10-14T00:22:21.707Z",
        "title": "SuperCorrect: Supervising and Correcting Language Models with Error-Driven Insights",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.09008.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
            "fullname": "Ling Yang",
            "name": "Lingaaaaaaa",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.07656",
            "authors": [
                {
                    "_id": "670cd93f9cf5450e15cfe12d",
                    "user": {
                        "_id": "60b364e7f88532cd79eaff7b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654185363389-60b364e7f88532cd79eaff7b.jpeg",
                        "isPro": false,
                        "fullname": "Nikita Balagansky",
                        "user": "elephantmipt",
                        "type": "user"
                    },
                    "name": "Nikita Balagansky",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-14T10:47:41.728Z",
                    "hidden": false
                },
                {
                    "_id": "670cd93f9cf5450e15cfe12e",
                    "name": "Ian Maksimov",
                    "hidden": false
                },
                {
                    "_id": "670cd93f9cf5450e15cfe12f",
                    "user": {
                        "_id": "62a9c8edc19f92ae443ab37f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669110208492-62a9c8edc19f92ae443ab37f.png",
                        "isPro": false,
                        "fullname": "Daniil Gavrilov",
                        "user": "kefirski",
                        "type": "user"
                    },
                    "name": "Daniil Gavrilov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-14T08:43:11.835Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-10T06:55:38.000Z",
            "title": "Mechanistic Permutability: Match Features Across Layers",
            "summary": "Understanding how features evolve across layers in deep neural networks is a\nfundamental challenge in mechanistic interpretability, particularly due to\npolysemanticity and feature superposition. While Sparse Autoencoders (SAEs)\nhave been used to extract interpretable features from individual layers,\naligning these features across layers has remained an open problem. In this\npaper, we introduce SAE Match, a novel, data-free method for aligning SAE\nfeatures across different layers of a neural network. Our approach involves\nmatching features by minimizing the mean squared error between the folded\nparameters of SAEs, a technique that incorporates activation thresholds into\nthe encoder and decoder weights to account for differences in feature scales.\nThrough extensive experiments on the Gemma 2 language model, we demonstrate\nthat our method effectively captures feature evolution across layers, improving\nfeature matching quality. We also show that features persist over several\nlayers and that our approach can approximate hidden states across layers. Our\nwork advances the understanding of feature dynamics in neural networks and\nprovides a new tool for mechanistic interpretability studies.",
            "upvotes": 8,
            "discussionId": "670cd9409cf5450e15cfe171"
        },
        "publishedAt": "2024-10-14T08:07:48.115Z",
        "title": "Mechanistic Permutability: Match Features Across Layers",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.07656.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669110208492-62a9c8edc19f92ae443ab37f.png",
            "fullname": "Daniil Gavrilov",
            "name": "kefirski",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.08102",
            "authors": [
                {
                    "_id": "670cab96200342b4d2c77ff5",
                    "user": {
                        "_id": "641802c31f1f3b0fa812225c",
                        "avatarUrl": "/avatars/5425c27f532fa4076493d2a7c7c84342.svg",
                        "isPro": false,
                        "fullname": "becca_bai",
                        "user": "beccabai",
                        "type": "user"
                    },
                    "name": "Tianyi Bai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-14T10:47:44.353Z",
                    "hidden": false
                },
                {
                    "_id": "670cab96200342b4d2c77ff6",
                    "user": {
                        "_id": "64fde4e252e82dd432b74ce9",
                        "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
                        "isPro": false,
                        "fullname": "Ling Yang",
                        "user": "Lingaaaaaaa",
                        "type": "user"
                    },
                    "name": "Ling Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-14T12:37:15.149Z",
                    "hidden": false
                },
                {
                    "_id": "670cab96200342b4d2c77ff7",
                    "user": {
                        "_id": "65d3149f0545ab7c11568b2b",
                        "avatarUrl": "/avatars/02f36d8320b8331849e7d7f40e509ca8.svg",
                        "isPro": false,
                        "fullname": "Zhen Hao Wong",
                        "user": "aaron1141",
                        "type": "user"
                    },
                    "name": "Zhen Hao Wong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-14T12:38:20.132Z",
                    "hidden": false
                },
                {
                    "_id": "670cab96200342b4d2c77ff8",
                    "user": {
                        "_id": "64aa3d4b329f5e6e780dce4a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/5F_eFJ8ZZOgDxrOHSFyzJ.png",
                        "isPro": false,
                        "fullname": "Jiahui Peng",
                        "user": "Jiahui2023",
                        "type": "user"
                    },
                    "name": "Jiahui Peng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-14T12:38:52.705Z",
                    "hidden": false
                },
                {
                    "_id": "670cab96200342b4d2c77ff9",
                    "user": {
                        "_id": "63567f00f44a9f0f5f885221",
                        "avatarUrl": "/avatars/bc41b72399b7a7fe9293cde62f0577a0.svg",
                        "isPro": false,
                        "fullname": "Xinlin Zhuang",
                        "user": "Mihara-bot",
                        "type": "user"
                    },
                    "name": "Xinlin Zhuang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-14T12:38:46.006Z",
                    "hidden": false
                },
                {
                    "_id": "670cab96200342b4d2c77ffa",
                    "name": "Chi Zhang",
                    "hidden": false
                },
                {
                    "_id": "670cab96200342b4d2c77ffb",
                    "name": "Lijun Wu",
                    "hidden": false
                },
                {
                    "_id": "670cab96200342b4d2c77ffc",
                    "user": {
                        "_id": "641bbd19af42e9b7dd4638ce",
                        "avatarUrl": "/avatars/8e7819fa76650cab30e60063e8d3a217.svg",
                        "isPro": false,
                        "fullname": "Qiu Jiantao",
                        "user": "qiujiantao",
                        "type": "user"
                    },
                    "name": "Qiu Jiantao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-14T12:39:34.816Z",
                    "hidden": false
                },
                {
                    "_id": "670cab96200342b4d2c77ffd",
                    "name": "Wentao Zhang",
                    "hidden": false
                },
                {
                    "_id": "670cab96200342b4d2c77ffe",
                    "user": {
                        "_id": "6307c8dda642348dc5e71c20",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661454675064-6307c8dda642348dc5e71c20.jpeg",
                        "isPro": false,
                        "fullname": "Binhang Yuan",
                        "user": "biyuan",
                        "type": "user"
                    },
                    "name": "Binhang Yuan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-14T12:40:04.373Z",
                    "hidden": false
                },
                {
                    "_id": "670cab96200342b4d2c77fff",
                    "user": {
                        "_id": "63f9fca8d4349b157a109eec",
                        "avatarUrl": "/avatars/fa1f2ae7972d7cde99dab178136ccbb0.svg",
                        "isPro": false,
                        "fullname": "Conghui He",
                        "user": "conghui",
                        "type": "user"
                    },
                    "name": "Conghui He",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-14T12:39:58.611Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-10T16:45:28.000Z",
            "title": "Multi-Agent Collaborative Data Selection for Efficient LLM Pretraining",
            "summary": "Efficient data selection is crucial to accelerate the pretraining of large\nlanguage models (LLMs). While various methods have been proposed to enhance\ndata efficiency, limited research has addressed the inherent conflicts between\nthese approaches to achieve optimal data selection for LLM pretraining. To\ntackle this problem, we propose a novel multi-agent collaborative data\nselection mechanism. In this framework, each data selection method serves as an\nindependent agent, and an agent console is designed to dynamically integrate\nthe information from all agents throughout the LLM training process. We conduct\nextensive empirical studies to evaluate our multi-agent framework. The\nexperimental results demonstrate that our approach significantly improves data\nefficiency, accelerates convergence in LLM training, and achieves an average\nperformance gain of 10.5% across multiple language model benchmarks compared to\nthe state-of-the-art methods.",
            "upvotes": 7,
            "discussionId": "670cab97200342b4d2c78068"
        },
        "publishedAt": "2024-10-14T04:56:36.539Z",
        "title": "Multi-Agent Collaborative Data Selection for Efficient LLM Pretraining",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.08102.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/28ce7388f9318b49bdd0a5594c0f6732.svg",
            "fullname": "Baichuan Zhou",
            "name": "bczhou",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.08391",
            "authors": [
                {
                    "_id": "670c99dda88de61073ac57ce",
                    "user": {
                        "_id": "647a5afc732a0cd866eb512f",
                        "avatarUrl": "/avatars/fbd0ba2a7ba70414ac903e583ce62933.svg",
                        "isPro": false,
                        "fullname": "Maxwell Horton",
                        "user": "mchorton",
                        "type": "user"
                    },
                    "name": "Maxwell Horton",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-14T08:35:23.709Z",
                    "hidden": false
                },
                {
                    "_id": "670c99dda88de61073ac57cf",
                    "user": {
                        "_id": "6619aa776e681d5e88f9d95a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6619aa776e681d5e88f9d95a/fJ_HBq87VfdtJCu6RAUEw.jpeg",
                        "isPro": false,
                        "fullname": "Qingqing Cao",
                        "user": "qicao-apple",
                        "type": "user"
                    },
                    "name": "Qingqing Cao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-14T08:42:56.210Z",
                    "hidden": false
                },
                {
                    "_id": "670c99dda88de61073ac57d0",
                    "name": "Chenfan Sun",
                    "hidden": false
                },
                {
                    "_id": "670c99dda88de61073ac57d1",
                    "user": {
                        "_id": "6627307fa39494ebb3a39194",
                        "avatarUrl": "/avatars/99290642fd6d4a6162f52cfe5d46c648.svg",
                        "isPro": false,
                        "fullname": "Yanzi Jin",
                        "user": "yjin25",
                        "type": "user"
                    },
                    "name": "Yanzi Jin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-14T08:39:07.212Z",
                    "hidden": false
                },
                {
                    "_id": "670c99dda88de61073ac57d2",
                    "name": "Sachin Mehta",
                    "hidden": false
                },
                {
                    "_id": "670c99dda88de61073ac57d3",
                    "user": {
                        "_id": "65d4c7734c2163d05eab4110",
                        "avatarUrl": "/avatars/b116a9aca7b0a2a4836a29cb4f069d42.svg",
                        "isPro": false,
                        "fullname": "Mohammad Rastegari",
                        "user": "mrastegari",
                        "type": "user"
                    },
                    "name": "Mohammad Rastegari",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-14T08:38:36.380Z",
                    "hidden": false
                },
                {
                    "_id": "670c99dda88de61073ac57d4",
                    "user": {
                        "_id": "5fe36e9297b72d62ab9b90cd",
                        "avatarUrl": "/avatars/edfea7e8c86dcf355241f5a48365b35a.svg",
                        "isPro": false,
                        "fullname": "Moin Nabi",
                        "user": "moinnabi",
                        "type": "user"
                    },
                    "name": "Moin Nabi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-14T08:35:29.716Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-10T21:55:11.000Z",
            "title": "KV Prediction for Improved Time to First Token",
            "summary": "Inference with transformer-based language models begins with a prompt\nprocessing step. In this step, the model generates the first output token and\nstores the KV cache needed for future generation steps. This prompt processing\nstep can be computationally expensive, taking 10s of seconds or more for\nbillion-parameter models on edge devices when prompt lengths or batch sizes\nrise. This degrades user experience by introducing significant latency into the\nmodel's outputs. To reduce the time spent producing the first output (known as\nthe ``time to first token'', or TTFT) of a pretrained model, we introduce a\nnovel method called KV Prediction. In our method, a small auxiliary model is\nused to process the prompt and produce an approximation of the KV cache used by\na base model. This approximated KV cache is then used with the base model for\nautoregressive generation without the need to query the auxiliary model again.\nWe demonstrate that our method produces a pareto-optimal efficiency-accuracy\ntrade-off when compared to baselines. On TriviaQA, we demonstrate relative\naccuracy improvements in the range of 15%-50% across a range of TTFT FLOPs\nbudgets. We also demonstrate accuracy improvements of up to 30% on HumanEval\npython code completion at fixed TTFT FLOPs budgets. Additionally, we benchmark\nmodels on an Apple M2 Pro CPU and demonstrate that our improvement in FLOPs\ntranslates to a TTFT speedup on hardware. We release our code at\nhttps://github.com/apple/corenet/tree/main/projects/kv-prediction .",
            "upvotes": 6,
            "discussionId": "670c99dea88de61073ac57ff"
        },
        "publishedAt": "2024-10-14T02:41:21.778Z",
        "title": "KV Prediction for Improved Time to First Token",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.08391.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.08168",
            "authors": [
                {
                    "_id": "670aa68d4bea0a5a627c6c23",
                    "user": {
                        "_id": "655dbda6668b64adf1495528",
                        "avatarUrl": "/avatars/dbf6072dcf0905873289d78209a92f87.svg",
                        "isPro": false,
                        "fullname": "Zitian Zhang",
                        "user": "zzt76",
                        "type": "user"
                    },
                    "name": "Zitian Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-14T13:17:10.791Z",
                    "hidden": false
                },
                {
                    "_id": "670aa68d4bea0a5a627c6c24",
                    "user": {
                        "_id": "64763956fb22e3b77f3e13d6",
                        "avatarUrl": "/avatars/1e6acf5d733280931ba2ce099481b2be.svg",
                        "isPro": false,
                        "fullname": "Frédéric Fortier-Chouinard",
                        "user": "lefreud",
                        "type": "user"
                    },
                    "name": "Frédéric Fortier-Chouinard",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-13T20:15:26.881Z",
                    "hidden": false
                },
                {
                    "_id": "670aa68d4bea0a5a627c6c25",
                    "user": {
                        "_id": "657df7d22bffc5568ae27a91",
                        "avatarUrl": "/avatars/a56fc5d1faaaff4b2b44f0d2c6e54ced.svg",
                        "isPro": false,
                        "fullname": "Mathieu Garon",
                        "user": "MathieuGaron",
                        "type": "user"
                    },
                    "name": "Mathieu Garon",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-14T13:17:05.387Z",
                    "hidden": false
                },
                {
                    "_id": "670aa68d4bea0a5a627c6c26",
                    "user": {
                        "_id": "6303d5924ec2dfa82a5627de",
                        "avatarUrl": "/avatars/1e433b7fb9d30b9f42dec487d658b1e3.svg",
                        "isPro": false,
                        "fullname": "Anand Bhattad",
                        "user": "anandbhattad",
                        "type": "user"
                    },
                    "name": "Anand Bhattad",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-14T13:16:30.640Z",
                    "hidden": false
                },
                {
                    "_id": "670aa68d4bea0a5a627c6c27",
                    "user": {
                        "_id": "66e9905f00d5cd69e36e0e1a",
                        "avatarUrl": "/avatars/082eb98a634ae2f46c3664c2e6d331a1.svg",
                        "isPro": false,
                        "fullname": "Jean-Francois Lalonde",
                        "user": "jflalonde",
                        "type": "user"
                    },
                    "name": "Jean-François Lalonde",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-14T13:16:36.336Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-10T17:45:12.000Z",
            "title": "ZeroComp: Zero-shot Object Compositing from Image Intrinsics via\n  Diffusion",
            "summary": "We present ZeroComp, an effective zero-shot 3D object compositing approach\nthat does not require paired composite-scene images during training. Our method\nleverages ControlNet to condition from intrinsic images and combines it with a\nStable Diffusion model to utilize its scene priors, together operating as an\neffective rendering engine. During training, ZeroComp uses intrinsic images\nbased on geometry, albedo, and masked shading, all without the need for paired\nimages of scenes with and without composite objects. Once trained, it\nseamlessly integrates virtual 3D objects into scenes, adjusting shading to\ncreate realistic composites. We developed a high-quality evaluation dataset and\ndemonstrate that ZeroComp outperforms methods using explicit lighting\nestimations and generative techniques in quantitative and human perception\nbenchmarks. Additionally, ZeroComp extends to real and outdoor image\ncompositing, even when trained solely on synthetic indoor data, showcasing its\neffectiveness in image compositing.",
            "upvotes": 3,
            "discussionId": "670aa6924bea0a5a627c6e27"
        },
        "publishedAt": "2024-10-14T11:29:03.444Z",
        "title": "ZeroComp: Zero-shot Object Compositing from Image Intrinsics via Diffusion",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.08168.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "/avatars/1e6acf5d733280931ba2ce099481b2be.svg",
            "fullname": "Frédéric Fortier-Chouinard",
            "name": "lefreud",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.09045",
            "authors": [
                {
                    "_id": "670d33df6f346132013d7214",
                    "name": "Runsheng Huang",
                    "hidden": false
                },
                {
                    "_id": "670d33df6f346132013d7215",
                    "user": {
                        "_id": "6509e1e01b3694179dee256e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/sUbok2yloLElaebkps3lT.png",
                        "isPro": false,
                        "fullname": "Liam Dugan",
                        "user": "liamdugan",
                        "type": "user"
                    },
                    "name": "Liam Dugan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-14T15:30:29.739Z",
                    "hidden": false
                },
                {
                    "_id": "670d33df6f346132013d7216",
                    "name": "Yue Yang",
                    "hidden": false
                },
                {
                    "_id": "670d33df6f346132013d7217",
                    "name": "Chris Callison-Burch",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-11T17:58:02.000Z",
            "title": "MiRAGeNews: Multimodal Realistic AI-Generated News Detection",
            "summary": "The proliferation of inflammatory or misleading \"fake\" news content has\nbecome increasingly common in recent years. Simultaneously, it has become\neasier than ever to use AI tools to generate photorealistic images depicting\nany scene imaginable. Combining these two -- AI-generated fake news content --\nis particularly potent and dangerous. To combat the spread of AI-generated fake\nnews, we propose the MiRAGeNews Dataset, a dataset of 12,500 high-quality real\nand AI-generated image-caption pairs from state-of-the-art generators. We find\nthat our dataset poses a significant challenge to humans (60% F-1) and\nstate-of-the-art multi-modal LLMs (< 24% F-1). Using our dataset we train a\nmulti-modal detector (MiRAGe) that improves by +5.1% F-1 over state-of-the-art\nbaselines on image-caption pairs from out-of-domain image generators and news\npublishers. We release our code and data to aid future work on detecting\nAI-generated content.",
            "upvotes": 2,
            "discussionId": "670d33e16f346132013d73b1"
        },
        "publishedAt": "2024-10-14T13:41:24.464Z",
        "title": "MiRAGeNews: Multimodal Realistic AI-Generated News Detection",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.09045.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/sUbok2yloLElaebkps3lT.png",
            "fullname": "Liam Dugan",
            "name": "liamdugan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.07331",
            "authors": [
                {
                    "_id": "670cdca490a771a1f03eec6c",
                    "name": "Yiming Huang",
                    "hidden": false
                },
                {
                    "_id": "670cdca490a771a1f03eec6d",
                    "user": {
                        "_id": "66adf5cc0c6056d9f4dc308f",
                        "avatarUrl": "/avatars/7689ba5538f6bbf235084d418b3065c1.svg",
                        "isPro": false,
                        "fullname": "Jianwen Luo",
                        "user": "Jianwen2003",
                        "type": "user"
                    },
                    "name": "Jianwen Luo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-14T10:51:58.359Z",
                    "hidden": false
                },
                {
                    "_id": "670cdca490a771a1f03eec6e",
                    "name": "Yan Yu",
                    "hidden": false
                },
                {
                    "_id": "670cdca490a771a1f03eec6f",
                    "name": "Yitong Zhang",
                    "hidden": false
                },
                {
                    "_id": "670cdca490a771a1f03eec70",
                    "user": {
                        "_id": "64104b467a15af878ae6695d",
                        "avatarUrl": "/avatars/407983918c12411e5ed636bf7435522b.svg",
                        "isPro": false,
                        "fullname": "Fangyu Lei",
                        "user": "FangyuLei",
                        "type": "user"
                    },
                    "name": "Fangyu Lei",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-14T10:52:29.944Z",
                    "hidden": false
                },
                {
                    "_id": "670cdca490a771a1f03eec71",
                    "user": {
                        "_id": "665c3d590e92f92b0ee233ad",
                        "avatarUrl": "/avatars/ee4bbf2872ccd5625196966e235f40f7.svg",
                        "isPro": false,
                        "fullname": "Yifan Wei",
                        "user": "bjEdward",
                        "type": "user"
                    },
                    "name": "Yifan Wei",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-14T10:52:36.300Z",
                    "hidden": false
                },
                {
                    "_id": "670cdca490a771a1f03eec72",
                    "name": "Shizhu He",
                    "hidden": false
                },
                {
                    "_id": "670cdca490a771a1f03eec73",
                    "name": "Lifu Huang",
                    "hidden": false
                },
                {
                    "_id": "670cdca490a771a1f03eec74",
                    "user": {
                        "_id": "63fb6e281b4b1bd4e7ffc5be",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677422062937-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Xiao Liu",
                        "user": "lx865712528",
                        "type": "user"
                    },
                    "name": "Xiao Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-14T10:47:40.156Z",
                    "hidden": false
                },
                {
                    "_id": "670cdca490a771a1f03eec75",
                    "name": "Jun Zhao",
                    "hidden": false
                },
                {
                    "_id": "670cdca490a771a1f03eec76",
                    "name": "Kang Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-09T18:00:05.000Z",
            "title": "DA-Code: Agent Data Science Code Generation Benchmark for Large Language\n  Models",
            "summary": "We introduce DA-Code, a code generation benchmark specifically designed to\nassess LLMs on agent-based data science tasks. This benchmark features three\ncore elements: First, the tasks within DA-Code are inherently challenging,\nsetting them apart from traditional code generation tasks and demanding\nadvanced coding skills in grounding and planning. Second, examples in DA-Code\nare all based on real and diverse data, covering a wide range of complex data\nwrangling and analytics tasks. Third, to solve the tasks, the models must\nutilize complex data science programming languages, to perform intricate data\nprocessing and derive the answers. We set up the benchmark in a controllable\nand executable environment that aligns with real-world data analysis scenarios\nand is scalable. The annotators meticulously design the evaluation suite to\nensure the accuracy and robustness of the evaluation. We develop the DA-Agent\nbaseline. Experiments show that although the baseline performs better than\nother existing frameworks, using the current best LLMs achieves only 30.5%\naccuracy, leaving ample room for improvement. We release our benchmark at\nhttps://da-code-bench.github.io.",
            "upvotes": 2,
            "discussionId": "670cdca590a771a1f03eed19"
        },
        "publishedAt": "2024-10-14T07:27:37.548Z",
        "title": "DA-Code: Agent Data Science Code Generation Benchmark for Large Language Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.07331.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677422062937-noauth.jpeg",
            "fullname": "Xiao Liu",
            "name": "lx865712528",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.07536",
            "authors": [
                {
                    "_id": "670d1ad6756dc949a69f79e5",
                    "user": {
                        "_id": "64a54586c0f13de8e7093314",
                        "avatarUrl": "/avatars/389e43e9a32cf2fc95f8f3a23b8f0508.svg",
                        "isPro": false,
                        "fullname": "Ruoyi Du",
                        "user": "RuoyiDu",
                        "type": "user"
                    },
                    "name": "Ruoyi Du",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-14T15:46:34.443Z",
                    "hidden": false
                },
                {
                    "_id": "670d1ad6756dc949a69f79e6",
                    "user": {
                        "_id": "646f1bef075e11ca78da3bb7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646f1bef075e11ca78da3bb7/gNS-ikyZXYeMrf4a7HTQE.jpeg",
                        "isPro": false,
                        "fullname": "Dongyang Liu (Chris Liu)",
                        "user": "Cxxs",
                        "type": "user"
                    },
                    "name": "Dongyang Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-14T15:46:43.133Z",
                    "hidden": false
                },
                {
                    "_id": "670d1ad6756dc949a69f79e7",
                    "name": "Le Zhuo",
                    "hidden": false
                },
                {
                    "_id": "670d1ad6756dc949a69f79e8",
                    "name": "Qin Qi",
                    "hidden": false
                },
                {
                    "_id": "670d1ad6756dc949a69f79e9",
                    "name": "Hongsheng Li",
                    "hidden": false
                },
                {
                    "_id": "670d1ad6756dc949a69f79ea",
                    "name": "Zhanyu Ma",
                    "hidden": false
                },
                {
                    "_id": "670d1ad6756dc949a69f79eb",
                    "name": "Peng Gao",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-10T02:08:23.000Z",
            "title": "I-Max: Maximize the Resolution Potential of Pre-trained Rectified Flow\n  Transformers with Projected Flow",
            "summary": "Rectified Flow Transformers (RFTs) offer superior training and inference\nefficiency, making them likely the most viable direction for scaling up\ndiffusion models. However, progress in generation resolution has been\nrelatively slow due to data quality and training costs. Tuning-free resolution\nextrapolation presents an alternative, but current methods often reduce\ngenerative stability, limiting practical application. In this paper, we review\nexisting resolution extrapolation methods and introduce the I-Max framework to\nmaximize the resolution potential of Text-to-Image RFTs. I-Max features: (i) a\nnovel Projected Flow strategy for stable extrapolation and (ii) an advanced\ninference toolkit for generalizing model knowledge to higher resolutions.\nExperiments with Lumina-Next-2K and Flux.1-dev demonstrate I-Max's ability to\nenhance stability in resolution extrapolation and show that it can bring image\ndetail emergence and artifact correction, confirming the practical value of\ntuning-free resolution extrapolation.",
            "upvotes": 1,
            "discussionId": "670d1adb756dc949a69f7b92"
        },
        "publishedAt": "2024-10-14T12:22:47.808Z",
        "title": "I-Max: Maximize the Resolution Potential of Pre-trained Rectified Flow Transformers with Projected Flow",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.07536.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/389e43e9a32cf2fc95f8f3a23b8f0508.svg",
            "fullname": "Ruoyi Du",
            "name": "RuoyiDu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.09037",
            "authors": [
                {
                    "_id": "670cefea1699355b61389dc4",
                    "user": {
                        "_id": "63609fd514657fb8cff9a8b2",
                        "avatarUrl": "/avatars/421efa9911db82c4c0d6386f0e8c43c7.svg",
                        "isPro": false,
                        "fullname": "hojae",
                        "user": "nokomon",
                        "type": "user"
                    },
                    "name": "Hojae Lee",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-14T10:47:35.460Z",
                    "hidden": false
                },
                {
                    "_id": "670cefea1699355b61389dc5",
                    "user": {
                        "_id": "669891e2bd704c8fb2de6b6d",
                        "avatarUrl": "/avatars/db492bcb1d41913c271d7fb8cd587494.svg",
                        "isPro": false,
                        "fullname": "Junho Kim",
                        "user": "monocrat23",
                        "type": "user"
                    },
                    "name": "Junho Kim",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-14T11:38:32.861Z",
                    "hidden": false
                },
                {
                    "_id": "670cefea1699355b61389dc6",
                    "user": {
                        "_id": "646ec6b5cbb7bb996513c514",
                        "avatarUrl": "/avatars/24614870dddab1da84da1ec8a5cc8f01.svg",
                        "isPro": false,
                        "fullname": "Sangkeun LEE",
                        "user": "SKyii",
                        "type": "user"
                    },
                    "name": "SangKeun Lee",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-14T11:12:58.738Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-11T17:53:27.000Z",
            "title": "Mentor-KD: Making Small Language Models Better Multi-step Reasoners",
            "summary": "Large Language Models (LLMs) have displayed remarkable performances across\nvarious complex tasks by leveraging Chain-of-Thought (CoT) prompting. Recently,\nstudies have proposed a Knowledge Distillation (KD) approach, reasoning\ndistillation, which transfers such reasoning ability of LLMs through\nfine-tuning language models of multi-step rationales generated by LLM teachers.\nHowever, they have inadequately considered two challenges regarding\ninsufficient distillation sets from the LLM teacher model, in terms of 1) data\nquality and 2) soft label provision. In this paper, we propose Mentor-KD, which\neffectively distills the multi-step reasoning capability of LLMs to smaller LMs\nwhile addressing the aforementioned challenges. Specifically, we exploit a\nmentor, intermediate-sized task-specific fine-tuned model, to augment\nadditional CoT annotations and provide soft labels for the student model during\nreasoning distillation. We conduct extensive experiments and confirm\nMentor-KD's effectiveness across various models and complex reasoning tasks.",
            "upvotes": 1,
            "discussionId": "670cefeb1699355b61389e8c"
        },
        "publishedAt": "2024-10-14T09:26:42.436Z",
        "title": "Mentor-KD: Making Small Language Models Better Multi-step Reasoners",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.09037.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/421efa9911db82c4c0d6386f0e8c43c7.svg",
            "fullname": "hojae",
            "name": "nokomon",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    }
]