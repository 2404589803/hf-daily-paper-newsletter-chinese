[
  {
    "paper": {
      "id": "2506.15675",
      "authors": [
        {
          "_id": "6853946599bf39f9665c79e0",
          "name": "Zhen Li",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79e1",
          "name": "Chuanhao Li",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79e2",
          "name": "Xiaofeng Mao",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79e3",
          "name": "Shaoheng Lin",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79e4",
          "name": "Ming Li",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79e5",
          "name": "Shitian Zhao",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79e6",
          "name": "Zhaopan Xu",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79e7",
          "name": "Xinyue Li",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79e8",
          "name": "Yukang Feng",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79e9",
          "name": "Jianwen Sun",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79ea",
          "name": "Zizhen Li",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79eb",
          "name": "Fanrui Zhang",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79ec",
          "name": "Jiaxin Ai",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79ed",
          "name": "Zhixiang Wang",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79ee",
          "name": "Yuwei Wu",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79ef",
          "name": "Tong He",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79f0",
          "name": "Jiangmiao Pang",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79f1",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79f2",
          "name": "Yunde Jia",
          "hidden": false
        },
        {
          "_id": "6853946599bf39f9665c79f3",
          "name": "Kaipeng Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/65f1713552c38a91e0a445e8/GFrjhPvZnILgeTd_w2kvc.mp4"
      ],
      "publishedAt": "2025-06-18T17:57:06.000Z",
      "submittedOnDailyAt": "2025-06-19T03:16:13.113Z",
      "title": "Sekai: A Video Dataset towards World Exploration",
      "submittedOnDailyBy": {
        "_id": "65f1713552c38a91e0a445e8",
        "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
        "isPro": false,
        "fullname": "kaipeng",
        "user": "kpzhang996",
        "type": "user"
      },
      "summary": "Video generation techniques have made remarkable progress, promising to be\nthe foundation of interactive world exploration. However, existing video\ngeneration datasets are not well-suited for world exploration training as they\nsuffer from some limitations: limited locations, short duration, static scenes,\nand a lack of annotations about exploration and the world. In this paper, we\nintroduce Sekai (meaning ``world'' in Japanese), a high-quality first-person\nview worldwide video dataset with rich annotations for world exploration. It\nconsists of over 5,000 hours of walking or drone view (FPV and UVA) videos from\nover 100 countries and regions across 750 cities. We develop an efficient and\neffective toolbox to collect, pre-process and annotate videos with location,\nscene, weather, crowd density, captions, and camera trajectories. Experiments\ndemonstrate the quality of the dataset. And, we use a subset to train an\ninteractive video world exploration model, named YUME (meaning ``dream'' in\nJapanese). We believe Sekai will benefit the area of video generation and world\nexploration, and motivate valuable applications.",
      "upvotes": 21,
      "discussionId": "6853946599bf39f9665c79f4",
      "projectPage": "https://lixsp11.github.io/sekai-project/",
      "githubRepo": "https://github.com/Lixsp11/sekai-codebase",
      "ai_summary": "Sekai, a worldwide video dataset with comprehensive annotations, is introduced to support world exploration applications, enhancing video generation models.",
      "ai_keywords": [
        "first-person view",
        "worldwide video dataset",
        "rich annotations",
        "FPV",
        "UVA",
        "video collection",
        "pre-processing",
        "camera trajectories",
        "interactive video world exploration model"
      ]
    },
    "publishedAt": "2025-06-18T13:57:06.000Z",
    "title": "Sekai: A Video Dataset towards World Exploration",
    "summary": "Video generation techniques have made remarkable progress, promising to be\nthe foundation of interactive world exploration. However, existing video\ngeneration datasets are not well-suited for world exploration training as they\nsuffer from some limitations: limited locations, short duration, static scenes,\nand a lack of annotations about exploration and the world. In this paper, we\nintroduce Sekai (meaning ``world'' in Japanese), a high-quality first-person\nview worldwide video dataset with rich annotations for world exploration. It\nconsists of over 5,000 hours of walking or drone view (FPV and UVA) videos from\nover 100 countries and regions across 750 cities. We develop an efficient and\neffective toolbox to collect, pre-process and annotate videos with location,\nscene, weather, crowd density, captions, and camera trajectories. Experiments\ndemonstrate the quality of the dataset. And, we use a subset to train an\ninteractive video world exploration model, named YUME (meaning ``dream'' in\nJapanese). We believe Sekai will benefit the area of video generation and world\nexploration, and motivate valuable applications.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/65f1713552c38a91e0a445e8/GFrjhPvZnILgeTd_w2kvc.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15675.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65f1713552c38a91e0a445e8",
      "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
      "fullname": "kaipeng",
      "name": "kpzhang996",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.15681",
      "authors": [
        {
          "_id": "68536fc899bf39f9665c7961",
          "name": "Byung-Kwan Lee",
          "hidden": false
        },
        {
          "_id": "68536fc899bf39f9665c7962",
          "user": {
            "_id": "65b33e5f7cd0069ad648c4e8",
            "avatarUrl": "/avatars/1a746ea535cffa92ea08006e05ea414a.svg",
            "isPro": false,
            "fullname": "Ryo Hachiuma",
            "user": "rhachiuma",
            "type": "user"
          },
          "name": "Ryo Hachiuma",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-19T02:02:49.546Z",
          "hidden": false
        },
        {
          "_id": "68536fc899bf39f9665c7963",
          "name": "Yong Man Ro",
          "hidden": false
        },
        {
          "_id": "68536fc899bf39f9665c7964",
          "name": "Yu-Chiang Frank Wang",
          "hidden": false
        },
        {
          "_id": "68536fc899bf39f9665c7965",
          "name": "Yueh-Hua Wu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/657152eb12f162153b50ec9d/2xNUivkkqkJWJLWtIPNvb.mp4"
      ],
      "publishedAt": "2025-06-18T17:59:49.000Z",
      "submittedOnDailyAt": "2025-06-19T00:36:10.331Z",
      "title": "GenRecal: Generation after Recalibration from Large to Small\n  Vision-Language Models",
      "submittedOnDailyBy": {
        "_id": "657152eb12f162153b50ec9d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657152eb12f162153b50ec9d/qnldHP35PclV0pDz_05q8.jpeg",
        "isPro": false,
        "fullname": "Byung-Kwan Lee",
        "user": "BK-Lee",
        "type": "user"
      },
      "summary": "Recent advancements in vision-language models (VLMs) have leveraged large\nlanguage models (LLMs) to achieve performance on par with closed-source systems\nlike GPT-4V. However, deploying these models in real-world scenarios,\nparticularly on resource-constrained devices, remains challenging due to their\nsubstantial computational demands. This has spurred interest in distilling\nknowledge from large VLMs into smaller, more efficient counterparts. A key\nchallenge arises here from the diversity of VLM architectures, which are built\non different LLMs and employ varying token types-differing in vocabulary size,\ntoken splits, and token index ordering. To address this challenge of limitation\nto a specific VLM type, we present Generation after Recalibration (GenRecal), a\nnovel, general-purpose distillation framework for VLMs. GenRecal incorporates a\nRecalibrator that aligns and adapts feature representations between\nheterogeneous VLMs, enabling effective knowledge transfer across different\ntypes of VLMs. Through extensive experiments on multiple challenging\nbenchmarks, we demonstrate that GenRecal significantly improves baseline\nperformances, eventually outperforming large-scale open- and closed-source\nVLMs.",
      "upvotes": 8,
      "discussionId": "68536fc899bf39f9665c7966",
      "projectPage": "https://byungkwanlee.github.io/GenRecal-page/",
      "ai_summary": "GenRecal, a novel distillation framework, improves performance of vision-language models by aligning feature representations across different architectures.",
      "ai_keywords": [
        "vision-language models",
        "large language models",
        "distillation framework",
        "GenerRecal",
        "recalibration",
        "feature representations",
        "heterogeneous VLMs"
      ]
    },
    "publishedAt": "2025-06-18T13:59:49.000Z",
    "title": "GenRecal: Generation after Recalibration from Large to Small\n  Vision-Language Models",
    "summary": "Recent advancements in vision-language models (VLMs) have leveraged large\nlanguage models (LLMs) to achieve performance on par with closed-source systems\nlike GPT-4V. However, deploying these models in real-world scenarios,\nparticularly on resource-constrained devices, remains challenging due to their\nsubstantial computational demands. This has spurred interest in distilling\nknowledge from large VLMs into smaller, more efficient counterparts. A key\nchallenge arises here from the diversity of VLM architectures, which are built\non different LLMs and employ varying token types-differing in vocabulary size,\ntoken splits, and token index ordering. To address this challenge of limitation\nto a specific VLM type, we present Generation after Recalibration (GenRecal), a\nnovel, general-purpose distillation framework for VLMs. GenRecal incorporates a\nRecalibrator that aligns and adapts feature representations between\nheterogeneous VLMs, enabling effective knowledge transfer across different\ntypes of VLMs. Through extensive experiments on multiple challenging\nbenchmarks, we demonstrate that GenRecal significantly improves baseline\nperformances, eventually outperforming large-scale open- and closed-source\nVLMs.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/657152eb12f162153b50ec9d/2xNUivkkqkJWJLWtIPNvb.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15681.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "657152eb12f162153b50ec9d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657152eb12f162153b50ec9d/qnldHP35PclV0pDz_05q8.jpeg",
      "fullname": "Byung-Kwan Lee",
      "name": "BK-Lee",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 56
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.15677",
      "authors": [
        {
          "_id": "68536b2399bf39f9665c794c",
          "name": "Yining Hong",
          "hidden": false
        },
        {
          "_id": "68536b2399bf39f9665c794d",
          "name": "Rui Sun",
          "hidden": false
        },
        {
          "_id": "68536b2399bf39f9665c794e",
          "name": "Bingxuan Li",
          "hidden": false
        },
        {
          "_id": "68536b2399bf39f9665c794f",
          "name": "Xingcheng Yao",
          "hidden": false
        },
        {
          "_id": "68536b2399bf39f9665c7950",
          "name": "Maxine Wu",
          "hidden": false
        },
        {
          "_id": "68536b2399bf39f9665c7951",
          "name": "Alexander Chien",
          "hidden": false
        },
        {
          "_id": "68536b2399bf39f9665c7952",
          "name": "Da Yin",
          "hidden": false
        },
        {
          "_id": "68536b2399bf39f9665c7953",
          "name": "Ying Nian Wu",
          "hidden": false
        },
        {
          "_id": "68536b2399bf39f9665c7954",
          "name": "Zhecan James Wang",
          "hidden": false
        },
        {
          "_id": "68536b2399bf39f9665c7955",
          "name": "Kai-Wei Chang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-18T17:58:17.000Z",
      "submittedOnDailyAt": "2025-06-19T00:14:22.240Z",
      "title": "Embodied Web Agents: Bridging Physical-Digital Realms for Integrated\n  Agent Intelligence",
      "submittedOnDailyBy": {
        "_id": "6431b64df76c34519e93d1ba",
        "avatarUrl": "/avatars/ea577762b6b4798f87a7a3f1d53d082c.svg",
        "isPro": true,
        "fullname": "Yining Hong",
        "user": "evelynhong",
        "type": "user"
      },
      "summary": "AI agents today are mostly siloed - they either retrieve and reason over vast\namount of digital information and knowledge obtained online; or interact with\nthe physical world through embodied perception, planning and action - but\nrarely both. This separation limits their ability to solve tasks that require\nintegrated physical and digital intelligence, such as cooking from online\nrecipes, navigating with dynamic map data, or interpreting real-world landmarks\nusing web knowledge. We introduce Embodied Web Agents, a novel paradigm for AI\nagents that fluidly bridge embodiment and web-scale reasoning. To\noperationalize this concept, we first develop the Embodied Web Agents task\nenvironments, a unified simulation platform that tightly integrates realistic\n3D indoor and outdoor environments with functional web interfaces. Building\nupon this platform, we construct and release the Embodied Web Agents Benchmark,\nwhich encompasses a diverse suite of tasks including cooking, navigation,\nshopping, tourism, and geolocation - all requiring coordinated reasoning across\nphysical and digital realms for systematic assessment of cross-domain\nintelligence. Experimental results reveal significant performance gaps between\nstate-of-the-art AI systems and human capabilities, establishing both\nchallenges and opportunities at the intersection of embodied cognition and\nweb-scale knowledge access. All datasets, codes and websites are publicly\navailable at our project page https://embodied-web-agent.github.io/.",
      "upvotes": 6,
      "discussionId": "68536b2399bf39f9665c7956",
      "ai_summary": "Embodied Web Agents integrate physical interaction and web-scale reasoning to assess cross-domain intelligence in a novel benchmark environment.",
      "ai_keywords": [
        "Embodied Web Agents",
        "task environments",
        "simulation platform",
        "3D indoor and outdoor environments",
        "functional web interfaces",
        "Embodied Web Agents Benchmark",
        "systematic assessment",
        "cross-domain intelligence",
        "embodied cognition"
      ]
    },
    "publishedAt": "2025-06-18T13:58:17.000Z",
    "title": "Embodied Web Agents: Bridging Physical-Digital Realms for Integrated\n  Agent Intelligence",
    "summary": "AI agents today are mostly siloed - they either retrieve and reason over vast\namount of digital information and knowledge obtained online; or interact with\nthe physical world through embodied perception, planning and action - but\nrarely both. This separation limits their ability to solve tasks that require\nintegrated physical and digital intelligence, such as cooking from online\nrecipes, navigating with dynamic map data, or interpreting real-world landmarks\nusing web knowledge. We introduce Embodied Web Agents, a novel paradigm for AI\nagents that fluidly bridge embodiment and web-scale reasoning. To\noperationalize this concept, we first develop the Embodied Web Agents task\nenvironments, a unified simulation platform that tightly integrates realistic\n3D indoor and outdoor environments with functional web interfaces. Building\nupon this platform, we construct and release the Embodied Web Agents Benchmark,\nwhich encompasses a diverse suite of tasks including cooking, navigation,\nshopping, tourism, and geolocation - all requiring coordinated reasoning across\nphysical and digital realms for systematic assessment of cross-domain\nintelligence. Experimental results reveal significant performance gaps between\nstate-of-the-art AI systems and human capabilities, establishing both\nchallenges and opportunities at the intersection of embodied cognition and\nweb-scale knowledge access. All datasets, codes and websites are publicly\navailable at our project page https://embodied-web-agent.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15677.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6431b64df76c34519e93d1ba",
      "avatarUrl": "/avatars/ea577762b6b4798f87a7a3f1d53d082c.svg",
      "fullname": "Yining Hong",
      "name": "evelynhong",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.15068",
      "authors": [
        {
          "_id": "68536bf399bf39f9665c7958",
          "name": "Zongxia Li",
          "hidden": false
        },
        {
          "_id": "68536bf399bf39f9665c7959",
          "name": "Yapei Chang",
          "hidden": false
        },
        {
          "_id": "68536bf399bf39f9665c795a",
          "name": "Yuhang Zhou",
          "hidden": false
        },
        {
          "_id": "68536bf399bf39f9665c795b",
          "name": "Xiyang Wu",
          "hidden": false
        },
        {
          "_id": "68536bf399bf39f9665c795c",
          "name": "Zichao Liang",
          "hidden": false
        },
        {
          "_id": "68536bf399bf39f9665c795d",
          "name": "Yoo Yeon Sung",
          "hidden": false
        },
        {
          "_id": "68536bf399bf39f9665c795e",
          "name": "Jordan Lee Boyd-Graber",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-18T02:16:53.000Z",
      "submittedOnDailyAt": "2025-06-19T04:04:25.155Z",
      "title": "Semantically-Aware Rewards for Open-Ended R1 Training in Free-Form\n  Generation",
      "submittedOnDailyBy": {
        "_id": "64ea62f918d79efd533c93fe",
        "avatarUrl": "/avatars/9985a789ce11b788de2cba12adfb72fc.svg",
        "isPro": false,
        "fullname": "Xiyang Wu",
        "user": "wuxiyang",
        "type": "user"
      },
      "summary": "Evaluating open-ended long-form generation is challenging because it is hard\nto define what clearly separates good from bad outputs. Existing methods often\nmiss key aspects like coherence, style, or relevance, or are biased by\npretraining data, making open-ended long-form evaluation an underexplored\nproblem. To address this gap, we propose PrefBERT, a scoring model for\nevaluating open-ended long-form generation in GRPO and guiding its training\nwith distinct rewards for good and bad outputs. Trained on two response\nevaluation datasets with diverse long-form styles and Likert-rated quality,\nPrefBERT effectively supports GRPO by offering better semantic reward feedback\nthan traditional metrics ROUGE-L and BERTScore do. Through comprehensive\nevaluations, including LLM-as-a-judge, human ratings, and qualitative analysis,\nwe show that PrefBERT, trained on multi-sentence and paragraph-length\nresponses, remains reliable across varied long passages and aligns well with\nthe verifiable rewards GRPO needs. Human evaluations confirm that using\nPrefBERT as the reward signal to train policy models yields responses better\naligned with human preferences than those trained with traditional metrics. Our\ncode is available at https://github.com/zli12321/long_form_rl.",
      "upvotes": 5,
      "discussionId": "68536bf399bf39f9665c795f",
      "ai_summary": "PrefBERT, a scoring model, improves open-ended long-form generation by providing better semantic reward feedback than traditional metrics.",
      "ai_keywords": [
        "PrefBERT",
        "GRPO",
        "multi-sentence responses",
        "paragraph-length responses",
        "Likert-rated quality",
        "LLM-as-a-judge",
        "human ratings",
        "qualitative analysis",
        "verifiable rewards"
      ]
    },
    "publishedAt": "2025-06-17T22:16:53.000Z",
    "title": "Semantically-Aware Rewards for Open-Ended R1 Training in Free-Form\n  Generation",
    "summary": "Evaluating open-ended long-form generation is challenging because it is hard\nto define what clearly separates good from bad outputs. Existing methods often\nmiss key aspects like coherence, style, or relevance, or are biased by\npretraining data, making open-ended long-form evaluation an underexplored\nproblem. To address this gap, we propose PrefBERT, a scoring model for\nevaluating open-ended long-form generation in GRPO and guiding its training\nwith distinct rewards for good and bad outputs. Trained on two response\nevaluation datasets with diverse long-form styles and Likert-rated quality,\nPrefBERT effectively supports GRPO by offering better semantic reward feedback\nthan traditional metrics ROUGE-L and BERTScore do. Through comprehensive\nevaluations, including LLM-as-a-judge, human ratings, and qualitative analysis,\nwe show that PrefBERT, trained on multi-sentence and paragraph-length\nresponses, remains reliable across varied long passages and aligns well with\nthe verifiable rewards GRPO needs. Human evaluations confirm that using\nPrefBERT as the reward signal to train policy models yields responses better\naligned with human preferences than those trained with traditional metrics. Our\ncode is available at https://github.com/zli12321/long_form_rl.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15068.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ea62f918d79efd533c93fe",
      "avatarUrl": "/avatars/9985a789ce11b788de2cba12adfb72fc.svg",
      "fullname": "Xiyang Wu",
      "name": "wuxiyang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.15569",
      "authors": [
        {
          "_id": "68537ca999bf39f9665c799a",
          "name": "Chengye Wang",
          "hidden": false
        },
        {
          "_id": "68537ca999bf39f9665c799b",
          "name": "Yifei Shen",
          "hidden": false
        },
        {
          "_id": "68537ca999bf39f9665c799c",
          "name": "Zexi Kuang",
          "hidden": false
        },
        {
          "_id": "68537ca999bf39f9665c799d",
          "name": "Arman Cohan",
          "hidden": false
        },
        {
          "_id": "68537ca999bf39f9665c799e",
          "name": "Yilun Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-18T15:43:26.000Z",
      "submittedOnDailyAt": "2025-06-19T01:28:40.934Z",
      "title": "SciVer: Evaluating Foundation Models for Multimodal Scientific Claim\n  Verification",
      "submittedOnDailyBy": {
        "_id": "62f662bcc58915315c4eccea",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
        "isPro": true,
        "fullname": "Yilun Zhao",
        "user": "yilunzhao",
        "type": "user"
      },
      "summary": "We introduce SciVer, the first benchmark specifically designed to evaluate\nthe ability of foundation models to verify claims within a multimodal\nscientific context. SciVer consists of 3,000 expert-annotated examples over\n1,113 scientific papers, covering four subsets, each representing a common\nreasoning type in multimodal scientific claim verification. To enable\nfine-grained evaluation, each example includes expert-annotated supporting\nevidence. We assess the performance of 21 state-of-the-art multimodal\nfoundation models, including o4-mini, Gemini-2.5-Flash, Llama-3.2-Vision, and\nQwen2.5-VL. Our experiment reveals a substantial performance gap between these\nmodels and human experts on SciVer. Through an in-depth analysis of\nretrieval-augmented generation (RAG), and human-conducted error evaluations, we\nidentify critical limitations in current open-source models, offering key\ninsights to advance models' comprehension and reasoning in multimodal\nscientific literature tasks.",
      "upvotes": 4,
      "discussionId": "68537ca999bf39f9665c799f",
      "githubRepo": "https://github.com/QDRhhhh/SciVer",
      "ai_summary": "A benchmark named SciVer evaluates multimodal foundation models' claim verification capabilities within scientific contexts, revealing performance gaps and limitations in current models.",
      "ai_keywords": [
        "retrieval-augmented generation (RAG)"
      ]
    },
    "publishedAt": "2025-06-18T11:43:26.000Z",
    "title": "SciVer: Evaluating Foundation Models for Multimodal Scientific Claim\n  Verification",
    "summary": "We introduce SciVer, the first benchmark specifically designed to evaluate\nthe ability of foundation models to verify claims within a multimodal\nscientific context. SciVer consists of 3,000 expert-annotated examples over\n1,113 scientific papers, covering four subsets, each representing a common\nreasoning type in multimodal scientific claim verification. To enable\nfine-grained evaluation, each example includes expert-annotated supporting\nevidence. We assess the performance of 21 state-of-the-art multimodal\nfoundation models, including o4-mini, Gemini-2.5-Flash, Llama-3.2-Vision, and\nQwen2.5-VL. Our experiment reveals a substantial performance gap between these\nmodels and human experts on SciVer. Through an in-depth analysis of\nretrieval-augmented generation (RAG), and human-conducted error evaluations, we\nidentify critical limitations in current open-source models, offering key\ninsights to advance models' comprehension and reasoning in multimodal\nscientific literature tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15569.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62f662bcc58915315c4eccea",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
      "fullname": "Yilun Zhao",
      "name": "yilunzhao",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 13
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.06279",
      "authors": [
        {
          "_id": "6850e0285e07650ecce890f3",
          "user": {
            "_id": "637f22d27119bd030dfd4af8",
            "avatarUrl": "/avatars/55c468c40ad3bd3218d086759fb1be3c.svg",
            "isPro": false,
            "fullname": "Shi Liu",
            "user": "CLLBJ16",
            "type": "user"
          },
          "name": "Shi Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-18T12:16:48.071Z",
          "hidden": false
        },
        {
          "_id": "6850e0285e07650ecce890f4",
          "name": "Weijie Su",
          "hidden": false
        },
        {
          "_id": "6850e0285e07650ecce890f5",
          "name": "Xizhou Zhu",
          "hidden": false
        },
        {
          "_id": "6850e0285e07650ecce890f6",
          "name": "Wenhai Wang",
          "hidden": false
        },
        {
          "_id": "6850e0285e07650ecce890f7",
          "name": "Jifeng Dai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-06T17:59:06.000Z",
      "submittedOnDailyAt": "2025-06-19T00:24:25.743Z",
      "title": "CoMemo: LVLMs Need Image Context with Image Memory",
      "submittedOnDailyBy": {
        "_id": "637f22d27119bd030dfd4af8",
        "avatarUrl": "/avatars/55c468c40ad3bd3218d086759fb1be3c.svg",
        "isPro": false,
        "fullname": "Shi Liu",
        "user": "CLLBJ16",
        "type": "user"
      },
      "summary": "Recent advancements in Large Vision-Language Models built upon Large Language\nModels have established aligning visual features with LLM representations as\nthe dominant paradigm. However, inherited LLM architectural designs introduce\nsuboptimal characteristics for multimodal processing. First, LVLMs exhibit a\nbimodal distribution in attention allocation, leading to the progressive\nneglect of middle visual content as context expands. Second, conventional\npositional encoding schemes fail to preserve vital 2D structural relationships\nwhen processing dynamic high-resolution images. To address these limitations,\nwe propose CoMemo - a dual-path architecture that combines a Context image path\nwith an image Memory path for visual processing, effectively alleviating visual\ninformation neglect. Additionally, we introduce RoPE-DHR, a novel positional\nencoding mechanism that employs thumbnail-based positional aggregation to\nmaintain 2D spatial awareness while mitigating remote decay in extended\nsequences. Evaluations across seven benchmarks,including long-context\ncomprehension, multi-image reasoning, and visual question answering,\ndemonstrate CoMemo's superior performance compared to conventional LVLM\narchitectures. Project page is available at\nhttps://lalbj.github.io/projects/CoMemo/.",
      "upvotes": 3,
      "discussionId": "6850e0295e07650ecce890f8",
      "projectPage": "https://lalbj.github.io/projects/CoMemo/",
      "githubRepo": "https://github.com/LALBJ/CoMemo",
      "ai_summary": "CoMemo addresses visual information neglect and spatial awareness in multimodal processing by using a dual-path architecture and a novel positional encoding mechanism.",
      "ai_keywords": [
        "Large Vision-Language Models",
        "Large Language Models",
        "multimodal processing",
        "bimodal distribution",
        "attention allocation",
        "middle visual content",
        "positional encoding",
        "visual processing",
        "image Memory path",
        "RoPE-DHR",
        "positional aggregation",
        "2D structural relationships",
        "spatial awareness",
        "remote decay",
        "long-context comprehension",
        "multi-image reasoning",
        "visual question answering"
      ]
    },
    "publishedAt": "2025-06-06T13:59:06.000Z",
    "title": "CoMemo: LVLMs Need Image Context with Image Memory",
    "summary": "Recent advancements in Large Vision-Language Models built upon Large Language\nModels have established aligning visual features with LLM representations as\nthe dominant paradigm. However, inherited LLM architectural designs introduce\nsuboptimal characteristics for multimodal processing. First, LVLMs exhibit a\nbimodal distribution in attention allocation, leading to the progressive\nneglect of middle visual content as context expands. Second, conventional\npositional encoding schemes fail to preserve vital 2D structural relationships\nwhen processing dynamic high-resolution images. To address these limitations,\nwe propose CoMemo - a dual-path architecture that combines a Context image path\nwith an image Memory path for visual processing, effectively alleviating visual\ninformation neglect. Additionally, we introduce RoPE-DHR, a novel positional\nencoding mechanism that employs thumbnail-based positional aggregation to\nmaintain 2D spatial awareness while mitigating remote decay in extended\nsequences. Evaluations across seven benchmarks,including long-context\ncomprehension, multi-image reasoning, and visual question answering,\ndemonstrate CoMemo's superior performance compared to conventional LVLM\narchitectures. Project page is available at\nhttps://lalbj.github.io/projects/CoMemo/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06279.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "637f22d27119bd030dfd4af8",
      "avatarUrl": "/avatars/55c468c40ad3bd3218d086759fb1be3c.svg",
      "fullname": "Shi Liu",
      "name": "CLLBJ16",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.15050",
      "authors": [
        {
          "_id": "68539c6199bf39f9665c79f6",
          "name": "Tiantian Fan",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c79f7",
          "name": "Lingjun Liu",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c79f8",
          "name": "Yu Yue",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c79f9",
          "name": "Jiaze Chen",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c79fa",
          "name": "Chengyi Wang",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c79fb",
          "name": "Qiying Yu",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c79fc",
          "name": "Chi Zhang",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c79fd",
          "name": "Zhiqi Lin",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c79fe",
          "name": "Ruofei Zhu",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c79ff",
          "name": "Yufeng Yuan",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c7a00",
          "name": "Xiaochen Zuo",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c7a01",
          "name": "Bole Ma",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c7a02",
          "name": "Mofan Zhang",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c7a03",
          "name": "Gaohong Liu",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c7a04",
          "name": "Ru Zhang",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c7a05",
          "name": "Haotian Zhou",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c7a06",
          "name": "Cong Xie",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c7a07",
          "name": "Ruidong Zhu",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c7a08",
          "name": "Zhi Zhang",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c7a09",
          "name": "Xin Liu",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c7a0a",
          "name": "Mingxuan Wang",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c7a0b",
          "name": "Lin Yan",
          "hidden": false
        },
        {
          "_id": "68539c6199bf39f9665c7a0c",
          "name": "Yonghui Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-18T01:21:38.000Z",
      "submittedOnDailyAt": "2025-06-19T03:43:49.620Z",
      "title": "Truncated Proximal Policy Optimization",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Recently, test-time scaling Large Language Models (LLMs) have demonstrated\nexceptional reasoning capabilities across scientific and professional tasks by\ngenerating long chains-of-thought (CoT). As a crucial component for developing\nthese reasoning models, reinforcement learning (RL), exemplified by Proximal\nPolicy Optimization (PPO) and its variants, allows models to learn through\ntrial and error. However, PPO can be time-consuming due to its inherent\non-policy nature, which is further exacerbated by increasing response lengths.\nIn this work, we propose Truncated Proximal Policy Optimization (T-PPO), a\nnovel extension to PPO that improves training efficiency by streamlining policy\nupdate and length-restricted response generation. T-PPO mitigates the issue of\nlow hardware utilization, an inherent drawback of fully synchronized\nlong-generation procedures, where resources often sit idle during the waiting\nperiods for complete rollouts. Our contributions are two-folds. First, we\npropose Extended Generalized Advantage Estimation (EGAE) for advantage\nestimation derived from incomplete responses while maintaining the integrity of\npolicy learning. Second, we devise a computationally optimized mechanism that\nallows for the independent optimization of the policy and value models. By\nselectively filtering prompt and truncated tokens, this mechanism reduces\nredundant computations and accelerates the training process without sacrificing\nconvergence performance. We demonstrate the effectiveness and efficacy of T-PPO\non AIME 2024 with a 32B base model. The experimental results show that T-PPO\nimproves the training efficiency of reasoning LLMs by up to 2.5x and\noutperforms its existing competitors.",
      "upvotes": 1,
      "discussionId": "68539c6199bf39f9665c7a0d",
      "ai_summary": "T-PPO, an extension of PPO, improves training efficiency for Large Language Models by optimizing policy updates and utilizing hardware resources more effectively.",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "chains-of-thought (CoT)",
        "reinforcement learning (RL)",
        "Proximal Policy Optimization (PPO)",
        "Truncated Proximal Policy Optimization (T-PPO)",
        "Extended Generalized Advantage Estimation (EGAE)",
        "advantage estimation",
        "policy and value models",
        "independent optimization",
        "prompt and truncated tokens",
        "AIME 2024",
        "base model"
      ]
    },
    "publishedAt": "2025-06-17T21:21:38.000Z",
    "title": "Truncated Proximal Policy Optimization",
    "summary": "Recently, test-time scaling Large Language Models (LLMs) have demonstrated\nexceptional reasoning capabilities across scientific and professional tasks by\ngenerating long chains-of-thought (CoT). As a crucial component for developing\nthese reasoning models, reinforcement learning (RL), exemplified by Proximal\nPolicy Optimization (PPO) and its variants, allows models to learn through\ntrial and error. However, PPO can be time-consuming due to its inherent\non-policy nature, which is further exacerbated by increasing response lengths.\nIn this work, we propose Truncated Proximal Policy Optimization (T-PPO), a\nnovel extension to PPO that improves training efficiency by streamlining policy\nupdate and length-restricted response generation. T-PPO mitigates the issue of\nlow hardware utilization, an inherent drawback of fully synchronized\nlong-generation procedures, where resources often sit idle during the waiting\nperiods for complete rollouts. Our contributions are two-folds. First, we\npropose Extended Generalized Advantage Estimation (EGAE) for advantage\nestimation derived from incomplete responses while maintaining the integrity of\npolicy learning. Second, we devise a computationally optimized mechanism that\nallows for the independent optimization of the policy and value models. By\nselectively filtering prompt and truncated tokens, this mechanism reduces\nredundant computations and accelerates the training process without sacrificing\nconvergence performance. We demonstrate the effectiveness and efficacy of T-PPO\non AIME 2024 with a 32B base model. The experimental results show that T-PPO\nimproves the training efficiency of reasoning LLMs by up to 2.5x and\noutperforms its existing competitors.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15050.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 88
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.14435",
      "authors": [
        {
          "_id": "68537b2a99bf39f9665c7990",
          "name": "Hongyu Wang",
          "hidden": false
        },
        {
          "_id": "68537b2a99bf39f9665c7991",
          "name": "Jiayu Xu",
          "hidden": false
        },
        {
          "_id": "68537b2a99bf39f9665c7992",
          "name": "Ruiping Wang",
          "hidden": false
        },
        {
          "_id": "68537b2a99bf39f9665c7993",
          "name": "Yan Feng",
          "hidden": false
        },
        {
          "_id": "68537b2a99bf39f9665c7994",
          "name": "Yitao Zhai",
          "hidden": false
        },
        {
          "_id": "68537b2a99bf39f9665c7995",
          "name": "Peng Pei",
          "hidden": false
        },
        {
          "_id": "68537b2a99bf39f9665c7996",
          "name": "Xunliang Cai",
          "hidden": false
        },
        {
          "_id": "68537b2a99bf39f9665c7997",
          "name": "Xilin Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-17T11:53:49.000Z",
      "submittedOnDailyAt": "2025-06-19T01:22:11.793Z",
      "title": "MoTE: Mixture of Ternary Experts for Memory-efficient Large Multimodal\n  Models",
      "submittedOnDailyBy": {
        "_id": "63f71771d36951307fcb4dcd",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f71771d36951307fcb4dcd/1c-GSQmSSuDXyVWjxZFy_.jpeg",
        "isPro": false,
        "fullname": "Hongyu Wang",
        "user": "hongyuw",
        "type": "user"
      },
      "summary": "Large multimodal Mixture-of-Experts (MoEs) effectively scale the model size\nto boost performance while maintaining fixed active parameters. However,\nprevious works primarily utilized full-precision experts during sparse\nup-cycling. Despite they show superior performance on end tasks, the large\namount of experts introduces higher memory footprint, which poses significant\nchallenges for the deployment on edge devices. In this work, we propose MoTE, a\nscalable and memory-efficient approach to train Mixture-of-Ternary-Experts\nmodels from dense checkpoint. Instead of training fewer high-precision experts,\nwe propose to train more low-precision experts during up-cycling. Specifically,\nwe use the pre-trained FFN as a shared expert and train ternary routed experts\nwith parameters in {-1, 0, 1}. Extensive experiments show that our approach has\npromising scaling trend along model size. MoTE achieves comparable performance\nto full-precision baseline MoE-LLaVA while offering lower memory footprint.\nFurthermore, our approach is compatible with post-training quantization methods\nand the advantage further amplifies when memory-constraint goes lower. Given\nthe same amount of expert memory footprint of 3.4GB and combined with\npost-training quantization, MoTE outperforms MoE-LLaVA by a gain of 4.3%\naverage accuracy on end tasks, demonstrating its effectiveness and potential\nfor memory-constrained devices.",
      "upvotes": 1,
      "discussionId": "68537b2b99bf39f9665c7998",
      "ai_summary": "MoTE, a scalable and memory-efficient method, improves Mixture-of-Experts models using low-precision ternary experts, enhancing performance and reducing memory footprint for deployment on edge devices.",
      "ai_keywords": [
        "Mixture-of-Experts",
        "MoEs",
        "sparse up-cycling",
        "low-precision",
        "ternary experts",
        "shared expert",
        "FFN",
        "pre-trained",
        "post-training quantization",
        "memory-constrained",
        "end tasks"
      ]
    },
    "publishedAt": "2025-06-17T07:53:49.000Z",
    "title": "MoTE: Mixture of Ternary Experts for Memory-efficient Large Multimodal\n  Models",
    "summary": "Large multimodal Mixture-of-Experts (MoEs) effectively scale the model size\nto boost performance while maintaining fixed active parameters. However,\nprevious works primarily utilized full-precision experts during sparse\nup-cycling. Despite they show superior performance on end tasks, the large\namount of experts introduces higher memory footprint, which poses significant\nchallenges for the deployment on edge devices. In this work, we propose MoTE, a\nscalable and memory-efficient approach to train Mixture-of-Ternary-Experts\nmodels from dense checkpoint. Instead of training fewer high-precision experts,\nwe propose to train more low-precision experts during up-cycling. Specifically,\nwe use the pre-trained FFN as a shared expert and train ternary routed experts\nwith parameters in {-1, 0, 1}. Extensive experiments show that our approach has\npromising scaling trend along model size. MoTE achieves comparable performance\nto full-precision baseline MoE-LLaVA while offering lower memory footprint.\nFurthermore, our approach is compatible with post-training quantization methods\nand the advantage further amplifies when memory-constraint goes lower. Given\nthe same amount of expert memory footprint of 3.4GB and combined with\npost-training quantization, MoTE outperforms MoE-LLaVA by a gain of 4.3%\naverage accuracy on end tasks, demonstrating its effectiveness and potential\nfor memory-constrained devices.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14435.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63f71771d36951307fcb4dcd",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f71771d36951307fcb4dcd/1c-GSQmSSuDXyVWjxZFy_.jpeg",
      "fullname": "Hongyu Wang",
      "name": "hongyuw",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 18
    },
    "isAuthorParticipating": false
  }
]