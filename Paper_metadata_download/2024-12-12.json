[
    "{'paper': {'id': '2412.07760', 'authors': [{'_id': '67591885536995a5bc59a89e', 'user': {'_id': '6530bf50f145530101ec03a2', 'avatarUrl': '/avatars/c61c00c314cf202b64968e51e855694d.svg', 'isPro': False, 'fullname': 'Jianhong Bai', 'user': 'jianhongbai', 'type': 'user'}, 'name': 'Jianhong Bai', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-11T09:19:50.897Z', 'hidden': False}, {'_id': '67591885536995a5bc59a89f', 'user': {'_id': '63401c89f81b9d101361f712', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1665146415483-63401c89f81b9d101361f712.png', 'isPro': False, 'fullname': 'Richard', 'user': 'menghanxia', 'type': 'user'}, 'name': 'Menghan Xia', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-12T08:24:51.075Z', 'hidden': False}, {'_id': '67591885536995a5bc59a8a0', 'user': {'_id': '60e272ca6c78a8c122b12127', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/60e272ca6c78a8c122b12127/xldEGBzGrU-bX6IwAw0Ie.jpeg', 'isPro': False, 'fullname': 'Xintao Wang', 'user': 'Xintao', 'type': 'user'}, 'name': 'Xintao Wang', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-12T08:24:43.182Z', 'hidden': False}, {'_id': '67591885536995a5bc59a8a1', 'user': {'_id': '66c15837b186e4f6a0dac80c', 'avatarUrl': '/avatars/a3b75d6945f1608e64a2fcff887a5024.svg', 'isPro': False, 'fullname': 'Ziyang Yuan', 'user': 'ziyangy', 'type': 'user'}, 'name': 'Ziyang Yuan', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-12T08:24:57.808Z', 'hidden': False}, {'_id': '67591885536995a5bc59a8a2', 'user': {'_id': '63aef2cafcca84593e6682db', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1672409763337-noauth.jpeg', 'isPro': False, 'fullname': 'Xiao Fu', 'user': 'lemonaddie', 'type': 'user'}, 'name': 'Xiao Fu', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-11T09:54:19.786Z', 'hidden': False}, {'_id': '67591885536995a5bc59a8a3', 'user': {'_id': '6458b8d0990172cd1d703715', 'avatarUrl': '/avatars/55f0695e3cb9933c3903fde5a8f740d5.svg', 'isPro': False, 'fullname': 'Zuozhu Liu', 'user': 'Zuozhu', 'type': 'user'}, 'name': 'Zuozhu Liu', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-12T08:25:03.968Z', 'hidden': False}, {'_id': '67591885536995a5bc59a8a4', 'user': {'_id': '66c46129d67297a9b93e03c5', 'avatarUrl': '/avatars/cffd8b07fa3655e240efc8e81f99d97d.svg', 'isPro': False, 'fullname': 'Haoji Hu', 'user': 'garland1979', 'type': 'user'}, 'name': 'Haoji Hu', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-12T08:25:09.626Z', 'hidden': False}, {'_id': '67591885536995a5bc59a8a5', 'name': 'Pengfei Wan', 'hidden': False}, {'_id': '67591885536995a5bc59a8a6', 'user': {'_id': '64bce15bafd1e46c5504ad38', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64bce15bafd1e46c5504ad38/bQFX1iFbXEBXcQvUNL811.png', 'isPro': False, 'fullname': 'Di Zhang', 'user': 'qq8933', 'type': 'user'}, 'name': 'Di Zhang', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-12T08:24:11.595Z', 'hidden': False}], 'publishedAt': '2024-12-10T18:55:17.000Z', 'title': 'SynCamMaster: Synchronizing Multi-Camera Video Generation from Diverse\\n  Viewpoints', 'summary': 'Recent advancements in video diffusion models have shown exceptional\\nabilities in simulating real-world dynamics and maintaining 3D consistency.\\nThis progress inspires us to investigate the potential of these models to\\nensure dynamic consistency across various viewpoints, a highly desirable\\nfeature for applications such as virtual filming. Unlike existing methods\\nfocused on multi-view generation of single objects for 4D reconstruction, our\\ninterest lies in generating open-world videos from arbitrary viewpoints,\\nincorporating 6 DoF camera poses. To achieve this, we propose a plug-and-play\\nmodule that enhances a pre-trained text-to-video model for multi-camera video\\ngeneration, ensuring consistent content across different viewpoints.\\nSpecifically, we introduce a multi-view synchronization module to maintain\\nappearance and geometry consistency across these viewpoints. Given the scarcity\\nof high-quality training data, we design a hybrid training scheme that\\nleverages multi-camera images and monocular videos to supplement Unreal\\nEngine-rendered multi-camera videos. Furthermore, our method enables intriguing\\nextensions, such as re-rendering a video from novel viewpoints. We also release\\na multi-view synchronized video dataset, named SynCamVideo-Dataset. Project\\npage: https://jianhongbai.github.io/SynCamMaster/.', 'upvotes': 36, 'discussionId': '67591887536995a5bc59a94c'}, 'publishedAt': '2024-12-11T22:40:15.170Z', 'title': 'SynCamMaster: Synchronizing Multi-Camera Video Generation from Diverse Viewpoints', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/6530bf50f145530101ec03a2/QWlp2SYpowJLtGdI0PX2U.mp4'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.07760.png', 'numComments': 3, 'submittedBy': {'_id': '6530bf50f145530101ec03a2', 'avatarUrl': '/avatars/c61c00c314cf202b64968e51e855694d.svg', 'fullname': 'Jianhong Bai', 'name': 'jianhongbai', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}}",
    "{'paper': {'id': '2412.08580', 'authors': [{'_id': '675a6094ca1f2b4cd6b2bfac', 'user': {'_id': '63a06cebe648d425374f3ea3', 'avatarUrl': '/avatars/2e15a761aac16bfdc3da46dd5eef1adf.svg', 'isPro': False, 'fullname': 'Zejian Li', 'user': 'hyllbd', 'type': 'user'}, 'name': 'Zejian Li', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-12T08:28:01.032Z', 'hidden': False}, {'_id': '675a6094ca1f2b4cd6b2bfad', 'user': {'_id': '66d1fb162e0412fa2aa4ffb0', 'avatarUrl': '/avatars/20620dd03d388f0262124f0c3523080d.svg', 'isPro': False, 'fullname': 'Chenye Meng', 'user': 'mengcy', 'type': 'user'}, 'name': 'Chenye Meng', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-12T08:27:39.200Z', 'hidden': False}, {'_id': '675a6094ca1f2b4cd6b2bfae', 'user': {'_id': '6552d3f69fa2f7205a1f94a2', 'avatarUrl': '/avatars/8b857fe89586b25b54864da91ce1b35a.svg', 'isPro': False, 'fullname': 'Yize Li', 'user': 'yeezlee', 'type': 'user'}, 'name': 'Yize Li', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-12T08:28:06.791Z', 'hidden': False}, {'_id': '675a6094ca1f2b4cd6b2bfaf', 'name': 'Ling Yang', 'hidden': False}, {'_id': '675a6094ca1f2b4cd6b2bfb0', 'user': {'_id': '63943c882b9483beb473ec25', 'avatarUrl': '/avatars/abd2aae43e68c34770159c15a01c8297.svg', 'isPro': False, 'fullname': 'Shengyuan Zhang', 'user': 'SYZhang0805', 'type': 'user'}, 'name': 'Shengyuan Zhang', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-12T08:27:07.929Z', 'hidden': False}, {'_id': '675a6094ca1f2b4cd6b2bfb1', 'user': {'_id': '6618ae26a7f8bb5a98ce4164', 'avatarUrl': '/avatars/5656997d49be6fd0f3fd793136098806.svg', 'isPro': False, 'fullname': 'JIARUI MA', 'user': 'MAJIARUI', 'type': 'user'}, 'name': 'Jiarui Ma', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-12T08:28:15.616Z', 'hidden': False}, {'_id': '675a6094ca1f2b4cd6b2bfb2', 'name': 'Jiayi Li', 'hidden': False}, {'_id': '675a6094ca1f2b4cd6b2bfb3', 'name': 'Guang Yang', 'hidden': False}, {'_id': '675a6094ca1f2b4cd6b2bfb4', 'name': 'Changyuan Yang', 'hidden': False}, {'_id': '675a6094ca1f2b4cd6b2bfb5', 'name': 'Zhiyuan Yang', 'hidden': False}, {'_id': '675a6094ca1f2b4cd6b2bfb6', 'name': 'Jinxiong Chang', 'hidden': False}, {'_id': '675a6094ca1f2b4cd6b2bfb7', 'user': {'_id': '662a5c0649e03000ed1adab4', 'avatarUrl': '/avatars/961e4b9bf923923fcd871672d156f50c.svg', 'isPro': False, 'fullname': 'Lingyun Sun', 'user': 'slysun', 'type': 'user'}, 'name': 'Lingyun Sun', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-12T08:25:33.476Z', 'hidden': False}], 'publishedAt': '2024-12-11T17:57:10.000Z', 'title': 'LAION-SG: An Enhanced Large-Scale Dataset for Training Complex\\n  Image-Text Models with Structural Annotations', 'summary': 'Recent advances in text-to-image (T2I) generation have shown remarkable\\nsuccess in producing high-quality images from text. However, existing T2I\\nmodels show decayed performance in compositional image generation involving\\nmultiple objects and intricate relationships. We attribute this problem to\\nlimitations in existing datasets of image-text pairs, which lack precise\\ninter-object relationship annotations with prompts only. To address this\\nproblem, we construct LAION-SG, a large-scale dataset with high-quality\\nstructural annotations of scene graphs (SG), which precisely describe\\nattributes and relationships of multiple objects, effectively representing the\\nsemantic structure in complex scenes. Based on LAION-SG, we train a new\\nfoundation model SDXL-SG to incorporate structural annotation information into\\nthe generation process. Extensive experiments show advanced models trained on\\nour LAION-SG boast significant performance improvements in complex scene\\ngeneration over models on existing datasets. We also introduce CompSG-Bench, a\\nbenchmark that evaluates models on compositional image generation, establishing\\na new standard for this domain.', 'upvotes': 28, 'discussionId': '675a609cca1f2b4cd6b2c12f'}, 'publishedAt': '2024-12-11T23:04:47.907Z', 'title': 'LAION-SG: An Enhanced Large-Scale Dataset for Training Complex Image-Text Models with Structural Annotations', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.08580.png', 'numComments': 3, 'submittedBy': {'_id': '63943c882b9483beb473ec25', 'avatarUrl': '/avatars/abd2aae43e68c34770159c15a01c8297.svg', 'fullname': 'Shengyuan Zhang', 'name': 'SYZhang0805', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 1}}",
    "{'paper': {'id': '2412.08443', 'authors': [{'_id': '675a62917f9a1b78132efc33', 'user': {'_id': '64d47a7a508a6313e33faedd', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64d47a7a508a6313e33faedd/8DrbXdxA-sKZb4jgNhvY1.jpeg', 'isPro': False, 'fullname': 'Yuan Liu', 'user': 'YuanLiuuuuuu', 'type': 'user'}, 'name': 'Yuan Liu', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-12T08:28:51.562Z', 'hidden': False}, {'_id': '675a62917f9a1b78132efc34', 'name': 'Le Tian', 'hidden': False}, {'_id': '675a62917f9a1b78132efc35', 'name': 'Xiao Zhou', 'hidden': False}, {'_id': '675a62917f9a1b78132efc36', 'name': 'Xinyu Gao', 'hidden': False}, {'_id': '675a62917f9a1b78132efc37', 'user': {'_id': '663f288e9cb0add00b96bf45', 'avatarUrl': '/avatars/cd90b9490c439703cb3b52ccbc491199.svg', 'isPro': False, 'fullname': 'kavioyu', 'user': 'kavio', 'type': 'user'}, 'name': 'Kavio Yu', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-12T08:29:26.282Z', 'hidden': False}, {'_id': '675a62917f9a1b78132efc38', 'user': {'_id': '646aefc235c7a57f936cfa31', 'avatarUrl': '/avatars/fc55fadf7260f8d9e164ddafb5e2b8dc.svg', 'isPro': False, 'fullname': 'Yang Yu', 'user': 'yangyu1', 'type': 'user'}, 'name': 'Yang Yu', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-12T08:29:33.015Z', 'hidden': False}, {'_id': '675a62917f9a1b78132efc39', 'name': 'Jie Zhou', 'hidden': False}], 'publishedAt': '2024-12-11T15:08:25.000Z', 'title': 'POINTS1.5: Building a Vision-Language Model towards Real World\\n  Applications', 'summary': 'Vision-language models have made significant strides recently, demonstrating\\nsuperior performance across a range of tasks, e.g. optical character\\nrecognition and complex diagram analysis. Building on this trend, we introduce\\na new vision-language model, POINTS1.5, designed to excel in various real-world\\napplications. POINTS1.5 is an enhancement of POINTS1.0 and incorporates several\\nkey innovations: i) We replace the original CLIP vision encoder, which had a\\nfixed image resolution, with a NaViT-style vision encoder that supports native\\ndynamic high resolution. This allows POINTS1.5 to process images of any\\nresolution without needing to split them into tiles. ii) We add bilingual\\nsupport to POINTS1.5, significantly enhancing its capability in Chinese. Due to\\nthe scarcity of open-source Chinese datasets for vision-language models, we\\ncollect numerous images from the Internet and annotate them using a combination\\nof manual and automatic methods. iii) We propose a set of rigorous filtering\\nmethods for visual instruction tuning datasets. We comprehensively evaluate all\\nthese filtering methods, and choose the most effective ones to obtain the final\\nvisual instruction tuning set. Thanks to these innovations, POINTS1.5\\nsignificantly outperforms POINTS1.0 and demonstrates strong performance across\\na range of real-world applications. Notably, POINTS1.5-7B is trained on fewer\\nthan 4 billion tokens and ranks first on the OpenCompass leaderboard among\\nmodels with fewer than 10 billion parameters', 'upvotes': 25, 'discussionId': '675a62927f9a1b78132efc6f'}, 'publishedAt': '2024-12-12T00:55:28.545Z', 'title': 'POINTS1.5: Building a Vision-Language Model towards Real World Applications', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.08443.png', 'numComments': 2, 'submittedBy': {'_id': '64d47a7a508a6313e33faedd', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64d47a7a508a6313e33faedd/8DrbXdxA-sKZb4jgNhvY1.jpeg', 'fullname': 'Yuan Liu', 'name': 'YuanLiuuuuuu', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 6}}",
    "{'paper': {'id': '2412.08486', 'authors': [{'_id': '675a9d12154448441ec42806', 'user': {'_id': '62e0fcfe4db2175cd2710063', 'avatarUrl': '/avatars/715841ab309b67e310755274a29d6b61.svg', 'isPro': True, 'fullname': 'Zijian Zhou', 'user': 'franciszzj', 'type': 'user'}, 'name': 'Zijian Zhou', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-12T08:26:29.544Z', 'hidden': False}, {'_id': '675a9d12154448441ec42807', 'user': {'_id': '633ef2d6905131c8f52cfb95', 'avatarUrl': '/avatars/415f61f80feaffca4e35e029b84ba5d2.svg', 'isPro': False, 'fullname': 'Shikun Liu', 'user': 'shikunl', 'type': 'user'}, 'name': 'Shikun Liu', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-12T09:03:35.757Z', 'hidden': False}, {'_id': '675a9d12154448441ec42808', 'name': 'Xiao Han', 'hidden': False}, {'_id': '675a9d12154448441ec42809', 'name': 'Haozhe Liu', 'hidden': False}, {'_id': '675a9d12154448441ec4280a', 'name': 'Kam Woh Ng', 'hidden': False}, {'_id': '675a9d12154448441ec4280b', 'name': 'Tian Xie', 'hidden': False}, {'_id': '675a9d12154448441ec4280c', 'user': {'_id': '62f8040c47d782a6e286ea8e', 'avatarUrl': '/avatars/f45224b851ae79fff9912f6e67159e52.svg', 'isPro': False, 'fullname': 'Yuren Cong', 'user': 'Yuren', 'type': 'user'}, 'name': 'Yuren Cong', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-12T09:04:50.005Z', 'hidden': False}, {'_id': '675a9d12154448441ec4280d', 'name': 'Hang Li', 'hidden': False}, {'_id': '675a9d12154448441ec4280e', 'user': {'_id': '650db809ff259b573e42e31d', 'avatarUrl': '/avatars/d6e4e4b67cb3ffa197c67197c816139b.svg', 'isPro': False, 'fullname': 'Meng Meng Xu', 'user': 'Wall-dandelion', 'type': 'user'}, 'name': 'Mengmeng Xu', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-12T09:04:56.730Z', 'hidden': False}, {'_id': '675a9d12154448441ec4280f', 'name': 'Juan-Manuel Pérez-Rúa', 'hidden': False}, {'_id': '675a9d12154448441ec42810', 'user': {'_id': '64534d44e08584d0f5088b2c', 'avatarUrl': '/avatars/c5167abd763e186050d2a2e98a1a3b2e.svg', 'isPro': False, 'fullname': 'Aditya Patel', 'user': 'AdityaPatel', 'type': 'user'}, 'name': 'Aditya Patel', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-12T09:05:15.461Z', 'hidden': False}, {'_id': '675a9d12154448441ec42811', 'name': 'Tao Xiang', 'hidden': False}, {'_id': '675a9d12154448441ec42812', 'name': 'Miaojing Shi', 'hidden': False}, {'_id': '675a9d12154448441ec42813', 'name': 'Sen He', 'hidden': False}], 'publishedAt': '2024-12-11T15:51:14.000Z', 'title': 'Learning Flow Fields in Attention for Controllable Person Image\\n  Generation', 'summary': \"Controllable person image generation aims to generate a person image\\nconditioned on reference images, allowing precise control over the person's\\nappearance or pose. However, prior methods often distort fine-grained textural\\ndetails from the reference image, despite achieving high overall image quality.\\nWe attribute these distortions to inadequate attention to corresponding regions\\nin the reference image. To address this, we thereby propose learning flow\\nfields in attention (Leffa), which explicitly guides the target query to attend\\nto the correct reference key in the attention layer during training.\\nSpecifically, it is realized via a regularization loss on top of the attention\\nmap within a diffusion-based baseline. Our extensive experiments show that\\nLeffa achieves state-of-the-art performance in controlling appearance (virtual\\ntry-on) and pose (pose transfer), significantly reducing fine-grained detail\\ndistortion while maintaining high image quality. Additionally, we show that our\\nloss is model-agnostic and can be used to improve the performance of other\\ndiffusion models.\", 'upvotes': 16, 'discussionId': '675a9d18154448441ec42997'}, 'publishedAt': '2024-12-12T03:52:03.221Z', 'title': 'Learning Flow Fields in Attention for Controllable Person Image Generation', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.08486.png', 'numComments': 3, 'submittedBy': {'_id': '62e0fcfe4db2175cd2710063', 'avatarUrl': '/avatars/715841ab309b67e310755274a29d6b61.svg', 'fullname': 'Zijian Zhou', 'name': 'franciszzj', 'type': 'user', 'isPro': True, 'isHf': False, 'isMod': False, 'followerCount': 4}}",
    "{'paper': {'id': '2412.07744', 'authors': [{'_id': '67597f9444ec15bfe4c6be7d', 'user': {'_id': '66a0b434968fb59eb2434a0a', 'avatarUrl': '/avatars/67538f97ebf1995c530563f411d3cd95.svg', 'isPro': False, 'fullname': 'Ye', 'user': 'zixuan-ye', 'type': 'user'}, 'name': 'Zixuan Ye', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-11T13:39:09.531Z', 'hidden': False}, {'_id': '67597f9444ec15bfe4c6be7e', 'name': 'Huijuan Huang', 'hidden': False}, {'_id': '67597f9444ec15bfe4c6be7f', 'user': {'_id': '60e272ca6c78a8c122b12127', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/60e272ca6c78a8c122b12127/xldEGBzGrU-bX6IwAw0Ie.jpeg', 'isPro': False, 'fullname': 'Xintao Wang', 'user': 'Xintao', 'type': 'user'}, 'name': 'Xintao Wang', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-12T09:00:35.189Z', 'hidden': False}, {'_id': '67597f9444ec15bfe4c6be80', 'name': 'Pengfei Wan', 'hidden': False}, {'_id': '67597f9444ec15bfe4c6be81', 'user': {'_id': '64bce15bafd1e46c5504ad38', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64bce15bafd1e46c5504ad38/bQFX1iFbXEBXcQvUNL811.png', 'isPro': False, 'fullname': 'Di Zhang', 'user': 'qq8933', 'type': 'user'}, 'name': 'Di Zhang', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-12T09:00:50.168Z', 'hidden': False}, {'_id': '67597f9444ec15bfe4c6be82', 'user': {'_id': '65f865cf806814aec87d45ce', 'avatarUrl': '/avatars/6b931c1efe20422a5985fce6a9e36aed.svg', 'isPro': False, 'fullname': 'Wenhan Luo', 'user': 'whluo', 'type': 'user'}, 'name': 'Wenhan Luo', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-12T09:00:55.592Z', 'hidden': False}], 'publishedAt': '2024-12-10T18:44:08.000Z', 'title': 'StyleMaster: Stylize Your Video with Artistic Generation and Translation', 'summary': 'Style control has been popular in video generation models. Existing methods\\noften generate videos far from the given style, cause content leakage, and\\nstruggle to transfer one video to the desired style. Our first observation is\\nthat the style extraction stage matters, whereas existing methods emphasize\\nglobal style but ignore local textures. In order to bring texture features\\nwhile preventing content leakage, we filter content-related patches while\\nretaining style ones based on prompt-patch similarity; for global style\\nextraction, we generate a paired style dataset through model illusion to\\nfacilitate contrastive learning, which greatly enhances the absolute style\\nconsistency. Moreover, to fill in the image-to-video gap, we train a\\nlightweight motion adapter on still videos, which implicitly enhances\\nstylization extent, and enables our image-trained model to be seamlessly\\napplied to videos. Benefited from these efforts, our approach, StyleMaster, not\\nonly achieves significant improvement in both style resemblance and temporal\\ncoherence, but also can easily generalize to video style transfer with a gray\\ntile ControlNet. Extensive experiments and visualizations demonstrate that\\nStyleMaster significantly outperforms competitors, effectively generating\\nhigh-quality stylized videos that align with textual content and closely\\nresemble the style of reference images. Our project page is at\\nhttps://zixuan-ye.github.io/stylemaster', 'upvotes': 14, 'discussionId': '67597f9944ec15bfe4c6c09a'}, 'publishedAt': '2024-12-12T02:09:01.672Z', 'title': 'StyleMaster: Stylize Your Video with Artistic Generation and Translation', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/66a0b434968fb59eb2434a0a/Utt73aTAB-LpKsuCBZtAX.mp4'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.07744.png', 'numComments': 3, 'submittedBy': {'_id': '66a0b434968fb59eb2434a0a', 'avatarUrl': '/avatars/67538f97ebf1995c530563f411d3cd95.svg', 'fullname': 'Ye', 'name': 'zixuan-ye', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}}",
    "{'paper': {'id': '2412.08646', 'authors': [{'_id': '675a9f2184252325bb60445c', 'user': {'_id': '630458fad14428368d1d0676', 'avatarUrl': '/avatars/05a0b0b75a6a9bdbc4185cc0880abf5e.svg', 'isPro': False, 'fullname': 'LIU Jihao', 'user': 'jjjjh', 'type': 'user'}, 'name': 'Jihao Liu', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-12T09:00:15.170Z', 'hidden': False}, {'_id': '675a9f2184252325bb60445d', 'user': {'_id': '66c8037c737ba92ae3fe0322', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/66c8037c737ba92ae3fe0322/WR_Yh5DWOVVh7IFlF24NM.jpeg', 'isPro': False, 'fullname': 'Zhiding Yu', 'user': 'Zhiding', 'type': 'user'}, 'name': 'Zhiding Yu', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-12T09:07:31.665Z', 'hidden': False}, {'_id': '675a9f2184252325bb60445e', 'name': 'Shiyi Lan', 'hidden': False}, {'_id': '675a9f2184252325bb60445f', 'name': 'Shihao Wang', 'hidden': False}, {'_id': '675a9f2184252325bb604460', 'user': {'_id': '65b8724123d948d884b379b1', 'avatarUrl': '/avatars/ce189d1d8d688c17912f9b869035b2d0.svg', 'isPro': False, 'fullname': 'Rongyao Fang', 'user': 'LucasFang', 'type': 'user'}, 'name': 'Rongyao Fang', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-12T09:06:48.987Z', 'hidden': False}, {'_id': '675a9f2184252325bb604461', 'name': 'Jan Kautz', 'hidden': False}, {'_id': '675a9f2184252325bb604462', 'user': {'_id': '65c04e9c27a5fdca81abcbd9', 'avatarUrl': '/avatars/12a155683c824fa23da4a9e2bed4f64e.svg', 'isPro': False, 'fullname': 'Hongsheng LI', 'user': 'hsli-cuhk', 'type': 'user'}, 'name': 'Hongsheng Li', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-12T09:06:29.840Z', 'hidden': False}, {'_id': '675a9f2184252325bb604463', 'name': 'Jose M. Alvare', 'hidden': False}], 'publishedAt': '2024-12-11T18:59:54.000Z', 'title': 'StreamChat: Chatting with Streaming Video', 'summary': 'This paper presents StreamChat, a novel approach that enhances the\\ninteraction capabilities of Large Multimodal Models (LMMs) with streaming video\\ncontent. In streaming interaction scenarios, existing methods rely solely on\\nvisual information available at the moment a question is posed, resulting in\\nsignificant delays as the model remains unaware of subsequent changes in the\\nstreaming video. StreamChat addresses this limitation by innovatively updating\\nthe visual context at each decoding step, ensuring that the model utilizes\\nup-to-date video content throughout the decoding process. Additionally, we\\nintroduce a flexible and efficient crossattention-based architecture to process\\ndynamic streaming inputs while maintaining inference efficiency for streaming\\ninteractions. Furthermore, we construct a new dense instruction dataset to\\nfacilitate the training of streaming interaction models, complemented by a\\nparallel 3D-RoPE mechanism that encodes the relative temporal information of\\nvisual and text tokens. Experimental results demonstrate that StreamChat\\nachieves competitive performance on established image and video benchmarks and\\nexhibits superior capabilities in streaming interaction scenarios compared to\\nstate-of-the-art video LMM.', 'upvotes': 12, 'discussionId': '675a9f2184252325bb6044bc'}, 'publishedAt': '2024-12-12T03:33:18.709Z', 'title': 'StreamChat: Chatting with Streaming Video', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.08646.png', 'numComments': 2, 'submittedBy': {'_id': '630458fad14428368d1d0676', 'avatarUrl': '/avatars/05a0b0b75a6a9bdbc4185cc0880abf5e.svg', 'fullname': 'LIU Jihao', 'name': 'jjjjh', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 3}}",
    "{'paper': {'id': '2412.05467', 'authors': [{'_id': '675b2a535b0522bba0448eaa', 'name': 'Thibault Le Sellier De Chezelles', 'hidden': False}, {'_id': '675b2a535b0522bba0448eab', 'name': 'Maxime Gasse', 'hidden': False}, {'_id': '675b2a535b0522bba0448eac', 'name': 'Alexandre Drouin', 'hidden': False}, {'_id': '675b2a535b0522bba0448ead', 'name': 'Massimo Caccia', 'hidden': False}, {'_id': '675b2a535b0522bba0448eae', 'name': 'Léo Boisvert', 'hidden': False}, {'_id': '675b2a535b0522bba0448eaf', 'name': 'Megh Thakkar', 'hidden': False}, {'_id': '675b2a535b0522bba0448eb0', 'name': 'Tom Marty', 'hidden': False}, {'_id': '675b2a535b0522bba0448eb1', 'name': 'Rim Assouel', 'hidden': False}, {'_id': '675b2a535b0522bba0448eb2', 'name': 'Sahar Omidi Shayegan', 'hidden': False}, {'_id': '675b2a535b0522bba0448eb3', 'name': 'Lawrence Keunho Jang', 'hidden': False}, {'_id': '675b2a535b0522bba0448eb4', 'name': 'Xing Han Lù', 'hidden': False}, {'_id': '675b2a535b0522bba0448eb5', 'name': 'Ori Yoran', 'hidden': False}, {'_id': '675b2a535b0522bba0448eb6', 'name': 'Dehan Kong', 'hidden': False}, {'_id': '675b2a535b0522bba0448eb7', 'name': 'Frank F. Xu', 'hidden': False}, {'_id': '675b2a535b0522bba0448eb8', 'name': 'Siva Reddy', 'hidden': False}, {'_id': '675b2a535b0522bba0448eb9', 'name': 'Quentin Cappart', 'hidden': False}, {'_id': '675b2a535b0522bba0448eba', 'name': 'Graham Neubig', 'hidden': False}, {'_id': '675b2a535b0522bba0448ebb', 'name': 'Ruslan Salakhutdinov', 'hidden': False}, {'_id': '675b2a535b0522bba0448ebc', 'name': 'Nicolas Chapados', 'hidden': False}, {'_id': '675b2a535b0522bba0448ebd', 'name': 'Alexandre Lacoste', 'hidden': False}], 'publishedAt': '2024-12-06T23:43:59.000Z', 'title': 'The BrowserGym Ecosystem for Web Agent Research', 'summary': \"The BrowserGym ecosystem addresses the growing need for efficient evaluation\\nand benchmarking of web agents, particularly those leveraging automation and\\nLarge Language Models (LLMs) for web interaction tasks. Many existing\\nbenchmarks suffer from fragmentation and inconsistent evaluation methodologies,\\nmaking it challenging to achieve reliable comparisons and reproducible results.\\nBrowserGym aims to solve this by providing a unified, gym-like environment with\\nwell-defined observation and action spaces, facilitating standardized\\nevaluation across diverse benchmarks. Combined with AgentLab, a complementary\\nframework that aids in agent creation, testing, and analysis, BrowserGym offers\\nflexibility for integrating new benchmarks while ensuring consistent evaluation\\nand comprehensive experiment management. This standardized approach seeks to\\nreduce the time and complexity of developing web agents, supporting more\\nreliable comparisons and facilitating in-depth analysis of agent behaviors, and\\ncould result in more adaptable, capable agents, ultimately accelerating\\ninnovation in LLM-driven automation. As a supporting evidence, we conduct the\\nfirst large-scale, multi-benchmark web agent experiment and compare the\\nperformance of 6 state-of-the-art LLMs across all benchmarks currently\\navailable in BrowserGym. Among other findings, our results highlight a large\\ndiscrepancy between OpenAI and Anthropic's latests models, with\\nClaude-3.5-Sonnet leading the way on almost all benchmarks, except on\\nvision-related tasks where GPT-4o is superior. Despite these advancements, our\\nresults emphasize that building robust and efficient web agents remains a\\nsignificant challenge, due to the inherent complexity of real-world web\\nenvironments and the limitations of current models.\", 'upvotes': 11, 'discussionId': '675b2a555b0522bba0448fa9'}, 'publishedAt': '2024-12-12T13:26:47.287Z', 'title': 'The BrowserGym Ecosystem for Web Agent Research', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/5fa9ff3ea13e063b8b2b60cb/tSkBMMNnUUlGx28hZeypw.png'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.05467.png', 'numComments': 2, 'submittedBy': {'_id': '5fa9ff3ea13e063b8b2b60cb', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1633380224986-5fa9ff3ea13e063b8b2b60cb.jpeg', 'fullname': 'Xing Han Lu', 'name': 'xhluca', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 9}}",
    "{'paper': {'id': '2412.06234', 'authors': [{'_id': '675932605456a65fb50aeed5', 'user': {'_id': '6574932cd0ed8f5761069ac2', 'avatarUrl': '/avatars/2a6b6b9e8b01179eacc92d2a9cbf5df9.svg', 'isPro': False, 'fullname': 'Seungtae', 'user': 'stnamjef', 'type': 'user'}, 'name': 'Seungtae Nam', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-12T08:22:38.514Z', 'hidden': False}, {'_id': '675932605456a65fb50aeed6', 'user': {'_id': '634d10d4a252e0c53f99534c', 'avatarUrl': '/avatars/d8a486e105769ffee59cf996a4f51686.svg', 'isPro': False, 'fullname': 'Sun Xiang yu', 'user': 'xysun', 'type': 'user'}, 'name': 'Xiangyu Sun', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-12T09:01:38.502Z', 'hidden': False}, {'_id': '675932605456a65fb50aeed7', 'user': {'_id': '64aa93e799639cc312795ea8', 'avatarUrl': '/avatars/904453c39c09aa51d3f6aa9fafb4a47d.svg', 'isPro': False, 'fullname': 'Gyeongjin Kang', 'user': 'lelady', 'type': 'user'}, 'name': 'Gyeongjin Kang', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-11T09:19:29.174Z', 'hidden': False}, {'_id': '675932605456a65fb50aeed8', 'user': {'_id': '66a4a1a7d8e85b03deddfa59', 'avatarUrl': '/avatars/56dbec2101717ad9471e08a03ae51f0c.svg', 'isPro': False, 'fullname': 'Young geun Lee', 'user': 'LeeYG', 'type': 'user'}, 'name': 'Younggeun Lee', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-12T09:01:44.417Z', 'hidden': False}, {'_id': '675932605456a65fb50aeed9', 'user': {'_id': '659b83ff2e1d57d594f714e7', 'avatarUrl': '/avatars/4072dc79d87eb3634392b6de3bc81a21.svg', 'isPro': False, 'fullname': 'SeungJun Oh', 'user': 'JustinOh', 'type': 'user'}, 'name': 'Seungjun Oh', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-12T09:01:49.993Z', 'hidden': False}, {'_id': '675932605456a65fb50aeeda', 'user': {'_id': '655e0141d36a195f663ee4b0', 'avatarUrl': '/avatars/64609d6fd1581e6cf3e057ee569d69a1.svg', 'isPro': False, 'fullname': 'Eunbyung Park', 'user': 'epark', 'type': 'user'}, 'name': 'Eunbyung Park', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-12T09:01:55.354Z', 'hidden': False}], 'publishedAt': '2024-12-09T06:20:51.000Z', 'title': 'Generative Densification: Learning to Densify Gaussians for\\n  High-Fidelity Generalizable 3D Reconstruction', 'summary': 'Generalized feed-forward Gaussian models have achieved significant progress\\nin sparse-view 3D reconstruction by leveraging prior knowledge from large\\nmulti-view datasets. However, these models often struggle to represent\\nhigh-frequency details due to the limited number of Gaussians. While the\\ndensification strategy used in per-scene 3D Gaussian splatting (3D-GS)\\noptimization can be adapted to the feed-forward models, it may not be ideally\\nsuited for generalized scenarios. In this paper, we propose Generative\\nDensification, an efficient and generalizable method to densify Gaussians\\ngenerated by feed-forward models. Unlike the 3D-GS densification strategy,\\nwhich iteratively splits and clones raw Gaussian parameters, our method\\nup-samples feature representations from the feed-forward models and generates\\ntheir corresponding fine Gaussians in a single forward pass, leveraging the\\nembedded prior knowledge for enhanced generalization. Experimental results on\\nboth object-level and scene-level reconstruction tasks demonstrate that our\\nmethod outperforms state-of-the-art approaches with comparable or smaller model\\nsizes, achieving notable improvements in representing fine details.', 'upvotes': 11, 'discussionId': '675932625456a65fb50aef52'}, 'publishedAt': '2024-12-12T00:51:03.082Z', 'title': 'Generative Densification: Learning to Densify Gaussians for High-Fidelity Generalizable 3D Reconstruction', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.06234.png', 'numComments': 2, 'submittedBy': {'_id': '64aa93e799639cc312795ea8', 'avatarUrl': '/avatars/904453c39c09aa51d3f6aa9fafb4a47d.svg', 'fullname': 'Gyeongjin Kang', 'name': 'lelady', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}}",
    "{'paper': {'id': '2412.07825', 'authors': [{'_id': '675a65ea8e18d279de5b6a35', 'user': {'_id': '625f81afe1994410eef1c36a', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1650426282769-noauth.jpeg', 'isPro': False, 'fullname': 'Wufei Ma', 'user': 'wufeim', 'type': 'user'}, 'name': 'Wufei Ma', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-12T09:03:03.299Z', 'hidden': False}, {'_id': '675a65ea8e18d279de5b6a36', 'name': 'Haoyu Chen', 'hidden': False}, {'_id': '675a65ea8e18d279de5b6a37', 'user': {'_id': '661434dd0dcdede49138cd7c', 'avatarUrl': '/avatars/35f08bf08c5e9edfb3c78e280af718cb.svg', 'isPro': False, 'fullname': 'Guofeng Zhang', 'user': 'guofeng1123', 'type': 'user'}, 'name': 'Guofeng Zhang', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-12T09:02:35.026Z', 'hidden': False}, {'_id': '675a65ea8e18d279de5b6a38', 'user': {'_id': '6504a2629310ce8c40f083c7', 'avatarUrl': '/avatars/e6c6bdd868729d9f66240b6957f6215c.svg', 'isPro': False, 'fullname': 'Celso M de Melo', 'user': 'cdemelo', 'type': 'user'}, 'name': 'Celso M de Melo', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-12T09:02:28.868Z', 'hidden': False}, {'_id': '675a65ea8e18d279de5b6a39', 'name': 'Alan Yuille', 'hidden': False}, {'_id': '675a65ea8e18d279de5b6a3a', 'user': {'_id': '660c9ac4b202fcf3892f62fa', 'avatarUrl': '/avatars/7314fd5f3f642096d0e37d3194f1aa7e.svg', 'isPro': False, 'fullname': 'Jieneng Chen', 'user': 'jienengchen', 'type': 'user'}, 'name': 'Jieneng Chen', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-12T09:02:17.141Z', 'hidden': False}], 'publishedAt': '2024-12-10T18:55:23.000Z', 'title': '3DSRBench: A Comprehensive 3D Spatial Reasoning Benchmark', 'summary': '3D spatial reasoning is the ability to analyze and interpret the positions,\\norientations, and spatial relationships of objects within the 3D space. This\\nallows models to develop a comprehensive understanding of the 3D scene,\\nenabling their applicability to a broader range of areas, such as autonomous\\nnavigation, robotics, and AR/VR. While large multi-modal models (LMMs) have\\nachieved remarkable progress in a wide range of image and video understanding\\ntasks, their capabilities to perform 3D spatial reasoning on diverse natural\\nimages are less studied. In this work we present the first comprehensive 3D\\nspatial reasoning benchmark, 3DSRBench, with 2,772 manually annotated visual\\nquestion-answer pairs across 12 question types. We conduct robust and thorough\\nevaluation of 3D spatial reasoning capabilities by balancing the data\\ndistribution and adopting a novel FlipEval strategy. To further study the\\nrobustness of 3D spatial reasoning w.r.t. camera 3D viewpoints, our 3DSRBench\\nincludes two subsets with 3D spatial reasoning questions on paired images with\\ncommon and uncommon viewpoints. We benchmark a wide range of open-sourced and\\nproprietary LMMs, uncovering their limitations in various aspects of 3D\\nawareness, such as height, orientation, location, and multi-object reasoning,\\nas well as their degraded performance on images with uncommon camera\\nviewpoints. Our 3DSRBench provide valuable findings and insights about the\\nfuture development of LMMs with strong 3D reasoning capabilities. Our project\\npage and dataset is available https://3dsrbench.github.io.', 'upvotes': 11, 'discussionId': '675a65f08e18d279de5b6bdc'}, 'publishedAt': '2024-12-11T23:28:12.381Z', 'title': '3DSRBench: A Comprehensive 3D Spatial Reasoning Benchmark', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.07825.png', 'numComments': 2, 'submittedBy': {'_id': '625f81afe1994410eef1c36a', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1650426282769-noauth.jpeg', 'fullname': 'Wufei Ma', 'name': 'wufeim', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 2}}",
    "{'paper': {'id': '2412.08629', 'authors': [{'_id': '675a8cf2c67a008f39d94c29', 'name': 'Vladimir Kulikov', 'hidden': False}, {'_id': '675a8cf2c67a008f39d94c2a', 'name': 'Matan Kleiner', 'hidden': False}, {'_id': '675a8cf2c67a008f39d94c2b', 'name': 'Inbar Huberman-Spiegelglas', 'hidden': False}, {'_id': '675a8cf2c67a008f39d94c2c', 'name': 'Tomer Michaeli', 'hidden': False}], 'publishedAt': '2024-12-11T18:50:29.000Z', 'title': 'FlowEdit: Inversion-Free Text-Based Editing Using Pre-Trained Flow\\n  Models', 'summary': \"Editing real images using a pre-trained text-to-image (T2I) diffusion/flow\\nmodel often involves inverting the image into its corresponding noise map.\\nHowever, inversion by itself is typically insufficient for obtaining\\nsatisfactory results, and therefore many methods additionally intervene in the\\nsampling process. Such methods achieve improved results but are not seamlessly\\ntransferable between model architectures. Here, we introduce FlowEdit, a\\ntext-based editing method for pre-trained T2I flow models, which is\\ninversion-free, optimization-free and model agnostic. Our method constructs an\\nODE that directly maps between the source and target distributions\\n(corresponding to the source and target text prompts) and achieves a lower\\ntransport cost than the inversion approach. This leads to state-of-the-art\\nresults, as we illustrate with Stable Diffusion 3 and FLUX. Code and examples\\nare available on the project's webpage.\", 'upvotes': 8, 'discussionId': '675a8cf8c67a008f39d94dfa'}, 'publishedAt': '2024-12-12T05:15:32.102Z', 'title': 'FlowEdit: Inversion-Free Text-Based Editing Using Pre-Trained Flow Models', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.08629.png', 'numComments': 4, 'submittedBy': {'_id': '637f42eb069b3a40ffb24f2e', 'avatarUrl': '/avatars/f9081fda9e2fb46884bc0e34027efb90.svg', 'fullname': 'Inbar Huberman-Spiegelglas', 'name': 'Inbarhub', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 4}}",
    "{'paper': {'id': '2412.07797', 'authors': [{'_id': '675a5805e32fafc3f302070a', 'user': {'_id': '647477a1e0b188d3cb20a910', 'avatarUrl': '/avatars/5da1cf4f7d727afc9320ed162bd38404.svg', 'isPro': False, 'fullname': 'Fragile Merak', 'user': 'Frag1le', 'type': 'user'}, 'name': 'Dongjie Fu', 'status': 'extracted_pending', 'statusLastChangedAt': '2024-12-12T03:27:03.907Z', 'hidden': False}], 'publishedAt': '2024-12-05T08:30:43.000Z', 'title': 'Mogo: RQ Hierarchical Causal Transformer for High-Quality 3D Human\\n  Motion Generation', 'summary': 'In the field of text-to-motion generation, Bert-type Masked Models (MoMask,\\nMMM) currently produce higher-quality outputs compared to GPT-type\\nautoregressive models (T2M-GPT). However, these Bert-type models often lack the\\nstreaming output capability required for applications in video game and\\nmultimedia environments, a feature inherent to GPT-type models. Additionally,\\nthey demonstrate weaker performance in out-of-distribution generation. To\\nsurpass the quality of BERT-type models while leveraging a GPT-type structure,\\nwithout adding extra refinement models that complicate scaling data, we propose\\na novel architecture, Mogo (Motion Only Generate Once), which generates\\nhigh-quality lifelike 3D human motions by training a single transformer model.\\nMogo consists of only two main components: 1) RVQ-VAE, a hierarchical residual\\nvector quantization variational autoencoder, which discretizes continuous\\nmotion sequences with high precision; 2) Hierarchical Causal Transformer,\\nresponsible for generating the base motion sequences in an autoregressive\\nmanner while simultaneously inferring residuals across different layers.\\nExperimental results demonstrate that Mogo can generate continuous and cyclic\\nmotion sequences up to 260 frames (13 seconds), surpassing the 196 frames (10\\nseconds) length limitation of existing datasets like HumanML3D. On the\\nHumanML3D test set, Mogo achieves a FID score of 0.079, outperforming both the\\nGPT-type model T2M-GPT (FID = 0.116), AttT2M (FID = 0.112) and the BERT-type\\nmodel MMM (FID = 0.080). Furthermore, our model achieves the best quantitative\\nperformance in out-of-distribution generation.', 'upvotes': 7, 'discussionId': '675a5807e32fafc3f3020808'}, 'publishedAt': '2024-12-11T22:29:17.604Z', 'title': 'Mogo: RQ Hierarchical Causal Transformer for High-Quality 3D Human Motion Generation', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.07797.png', 'numComments': 2, 'submittedBy': {'_id': '647477a1e0b188d3cb20a910', 'avatarUrl': '/avatars/5da1cf4f7d727afc9320ed162bd38404.svg', 'fullname': 'Fragile Merak', 'name': 'Frag1le', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 2}}",
    "{'paper': {'id': '2412.06016', 'authors': [{'_id': '675b3e766b4e158cc439a67c', 'name': 'Hyeonho Jeong', 'hidden': False}, {'_id': '675b3e766b4e158cc439a67d', 'name': 'Chun-Hao Paul Huang', 'hidden': False}, {'_id': '675b3e766b4e158cc439a67e', 'name': 'Jong Chul Ye', 'hidden': False}, {'_id': '675b3e766b4e158cc439a67f', 'name': 'Niloy Mitra', 'hidden': False}, {'_id': '675b3e766b4e158cc439a680', 'name': 'Duygu Ceylan', 'hidden': False}], 'publishedAt': '2024-12-08T18:21:00.000Z', 'title': 'Track4Gen: Teaching Video Diffusion Models to Track Points Improves\\n  Video Generation', 'summary': 'While recent foundational video generators produce visually rich output, they\\nstill struggle with appearance drift, where objects gradually degrade or change\\ninconsistently across frames, breaking visual coherence. We hypothesize that\\nthis is because there is no explicit supervision in terms of spatial tracking\\nat the feature level. We propose Track4Gen, a spatially aware video generator\\nthat combines video diffusion loss with point tracking across frames, providing\\nenhanced spatial supervision on the diffusion features. Track4Gen merges the\\nvideo generation and point tracking tasks into a single network by making\\nminimal changes to existing video generation architectures. Using Stable Video\\nDiffusion as a backbone, Track4Gen demonstrates that it is possible to unify\\nvideo generation and point tracking, which are typically handled as separate\\ntasks. Our extensive evaluations show that Track4Gen effectively reduces\\nappearance drift, resulting in temporally stable and visually coherent video\\ngeneration. Project page: hyeonho99.github.io/track4gen', 'upvotes': 6, 'discussionId': '675b3e7a6b4e158cc439a786'}, 'publishedAt': '2024-12-12T14:51:09.294Z', 'title': 'Track4Gen: Teaching Video Diffusion Models to Track Points Improves Video Generation', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.06016.png', 'numComments': 2, 'submittedBy': {'_id': '656ee8008bb9f4f8d95bd8f7', 'avatarUrl': '/avatars/4069d70f1279d928da521211c495d638.svg', 'fullname': 'Hyeonho Jeong', 'name': 'hyeonho-jeong-video', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 6}}",
    "{'paper': {'id': '2412.06071', 'authors': [{'_id': '675ad3a39d2eead3eb196816', 'name': 'Fan Wang', 'hidden': False}, {'_id': '675ad3a39d2eead3eb196817', 'name': 'Juyong Jiang', 'hidden': False}, {'_id': '675ad3a39d2eead3eb196818', 'user': {'_id': '60d3b57ad7b174177faabd6e', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1659971187637-60d3b57ad7b174177faabd6e.jpeg', 'isPro': True, 'fullname': 'chansung park', 'user': 'chansung', 'type': 'user'}, 'name': 'Chansung Park', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-12T13:40:54.230Z', 'hidden': False}, {'_id': '675ad3a39d2eead3eb196819', 'name': 'Sunghun Kim', 'hidden': False}, {'_id': '675ad3a39d2eead3eb19681a', 'name': 'Jing Tang', 'hidden': False}], 'publishedAt': '2024-12-08T21:26:22.000Z', 'title': 'KaSA: Knowledge-Aware Singular-Value Adaptation of Large Language Models', 'summary': \"The increasing sizes of large language models (LLMs) result in significant\\ncomputational overhead and memory usage when adapting these models to specific\\ntasks or domains. Various parameter-efficient fine-tuning (PEFT) methods have\\nbeen devised to mitigate these challenges by training a small set of parameters\\nfor the task-specific updates of the model weights. Among PEFT methods, LoRA\\nstands out for its simplicity and efficiency, inspiring the development of a\\nseries of variants. However, LoRA and its successors disregard the knowledge\\nthat is noisy or irrelevant to the targeted task, detrimentally impacting model\\nperformance and leading to suboptimality. To address this limitation, we\\nintroduce Knowledge-aware Singular-value Adaptation (KaSA), a PEFT method that\\nleverages singular value decomposition (SVD) with knowledge-aware singular\\nvalues to dynamically activate knowledge based on its relevance to the task at\\nhand. We conduct extensive experiments across a range of LLMs on tasks spanning\\nnatural language understanding (NLU), generation (NLG), instruction following,\\nand commonsense reasoning. The experimental results demonstrate that KaSA\\nconsistently outperforms FFT and 14 popular PEFT baselines across 16 benchmarks\\nand 4 synthetic datasets, underscoring our method's efficacy and adaptability.\\nThe source code of our method is available at\\nhttps://github.com/juyongjiang/KaSA.\", 'upvotes': 4, 'discussionId': '675ad3a59d2eead3eb196893'}, 'publishedAt': '2024-12-12T07:16:42.029Z', 'title': 'KaSA: Knowledge-Aware Singular-Value Adaptation of Large Language Models', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/60d3b57ad7b174177faabd6e/--wVv9i2qIGeInGmsBiMg.png'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.06071.png', 'numComments': 2, 'submittedBy': {'_id': '60d3b57ad7b174177faabd6e', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1659971187637-60d3b57ad7b174177faabd6e.jpeg', 'fullname': 'chansung park', 'name': 'chansung', 'type': 'user', 'isPro': True, 'isHf': False, 'isMod': False, 'followerCount': 2750}}",
    "{'paper': {'id': '2412.09551', 'authors': [{'_id': '675b963adbb389af011b5a01', 'name': 'Yihong Sun', 'hidden': False}, {'_id': '675b963adbb389af011b5a02', 'name': 'Hao Zhou', 'hidden': False}, {'_id': '675b963adbb389af011b5a03', 'name': 'Liangzhe Yuan', 'hidden': False}, {'_id': '675b963adbb389af011b5a04', 'name': 'Jennifer J. Sun', 'hidden': False}, {'_id': '675b963adbb389af011b5a05', 'name': 'Yandong Li', 'hidden': False}, {'_id': '675b963adbb389af011b5a06', 'name': 'Xuhui Jia', 'hidden': False}, {'_id': '675b963adbb389af011b5a07', 'name': 'Hartwig Adam', 'hidden': False}, {'_id': '675b963adbb389af011b5a08', 'name': 'Bharath Hariharan', 'hidden': False}, {'_id': '675b963adbb389af011b5a09', 'user': {'_id': '650c249887dcda6616baa040', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/tIVfxoAHAJ0sWyDMkaarA.jpeg', 'isPro': False, 'fullname': 'Long Zhao', 'user': 'garyzhao9012', 'type': 'user'}, 'name': 'Long Zhao', 'status': 'extracted_confirmed', 'statusLastChangedAt': '2024-12-13T02:13:07.970Z', 'hidden': False}, {'_id': '675b963adbb389af011b5a0a', 'name': 'Ting Liu', 'hidden': False}], 'publishedAt': '2024-12-12T18:41:20.000Z', 'title': 'Video Creation by Demonstration', 'summary': 'We explore a novel video creation experience, namely Video Creation by\\nDemonstration. Given a demonstration video and a context image from a different\\nscene, we generate a physically plausible video that continues naturally from\\nthe context image and carries out the action concepts from the demonstration.\\nTo enable this capability, we present delta-Diffusion, a self-supervised\\ntraining approach that learns from unlabeled videos by conditional future frame\\nprediction. Unlike most existing video generation controls that are based on\\nexplicit signals, we adopts the form of implicit latent control for maximal\\nflexibility and expressiveness required by general videos. By leveraging a\\nvideo foundation model with an appearance bottleneck design on top, we extract\\naction latents from demonstration videos for conditioning the generation\\nprocess with minimal appearance leakage. Empirically, delta-Diffusion\\noutperforms related baselines in terms of both human preference and large-scale\\nmachine evaluations, and demonstrates potentials towards interactive world\\nsimulation. Sampled video generation results are available at\\nhttps://delta-diffusion.github.io/.', 'upvotes': 3, 'discussionId': '675b963bdbb389af011b5a75'}, 'publishedAt': '2024-12-12T21:08:12.074Z', 'title': 'Video Creation by Demonstration', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.09551.png', 'numComments': 1, 'submittedBy': {'_id': '650c249887dcda6616baa040', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/tIVfxoAHAJ0sWyDMkaarA.jpeg', 'fullname': 'Long Zhao', 'name': 'garyzhao9012', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 6}}",
    "{'paper': {'id': '2412.06676', 'authors': [{'_id': '6759fcc41f49e61f776bb7ce', 'user': {'_id': '65b7afaeb4ec059b974e9189', 'avatarUrl': '/avatars/08059754f2d299b76ab4649e8d768253.svg', 'isPro': False, 'fullname': 'Roi Cohen', 'user': 'roicohen9', 'type': 'user'}, 'name': 'Roi Cohen', 'status': 'extracted_confirmed', 'statusLastChangedAt': '2024-12-11T21:01:39.374Z', 'hidden': False}, {'_id': '6759fcc41f49e61f776bb7cf', 'name': 'Konstantin Dobler', 'hidden': False}, {'_id': '6759fcc41f49e61f776bb7d0', 'name': 'Eden Biran', 'hidden': False}, {'_id': '6759fcc41f49e61f776bb7d1', 'name': 'Gerard de Melo', 'hidden': False}], 'publishedAt': '2024-12-09T17:13:20.000Z', 'title': \"I Don't Know: Explicit Modeling of Uncertainty with an [IDK] Token\", 'summary': 'Large Language Models are known to capture real-world knowledge, allowing\\nthem to excel in many downstream tasks. Despite recent advances, these models\\nare still prone to what are commonly known as hallucinations, causing them to\\nemit unwanted and factually incorrect text. In this work, we propose a novel\\ncalibration method that can be used to combat hallucinations. We add a special\\n[IDK] (\"I don\\'t know\") token to the model\\'s vocabulary and introduce an\\nobjective function that shifts probability mass to the [IDK] token for\\nincorrect predictions. This approach allows the model to express uncertainty in\\nits output explicitly. We evaluate our proposed method across multiple model\\narchitectures and factual downstream tasks. We find that models trained with\\nour method are able to express uncertainty in places where they would\\npreviously make mistakes while suffering only a small loss of encoded\\nknowledge. We further perform extensive ablation studies of multiple variations\\nof our approach and provide a detailed analysis of the precision-recall\\ntradeoff of our method.', 'upvotes': 2, 'discussionId': '6759fcc51f49e61f776bb814'}, 'publishedAt': '2024-12-12T13:17:10.236Z', 'title': \"I Don't Know: Explicit Modeling of Uncertainty with an [IDK] Token\", 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/65b7afaeb4ec059b974e9189/ddjPgBxu4TcdhZxieEyZ7.png', 'https://cdn-uploads.huggingface.co/production/uploads/65b7afaeb4ec059b974e9189/uKIkqfjeiUrzb99Ti0qSm.png'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.06676.png', 'numComments': 2, 'submittedBy': {'_id': '65b7afaeb4ec059b974e9189', 'avatarUrl': '/avatars/08059754f2d299b76ab4649e8d768253.svg', 'fullname': 'Roi Cohen', 'name': 'roicohen9', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 1}}",
    "{'paper': {'id': '2412.08503', 'authors': [{'_id': '675a91605f94292180d7856a', 'name': 'Mingkun Lei', 'hidden': False}, {'_id': '675a91605f94292180d7856b', 'name': 'Xue Song', 'hidden': False}, {'_id': '675a91605f94292180d7856c', 'name': 'Beier Zhu', 'hidden': False}, {'_id': '675a91605f94292180d7856d', 'name': 'Hao Wang', 'hidden': False}, {'_id': '675a91605f94292180d7856e', 'name': 'Chi Zhang', 'hidden': False}], 'publishedAt': '2024-12-11T16:13:23.000Z', 'title': 'StyleStudio: Text-Driven Style Transfer with Selective Control of Style\\n  Elements', 'summary': 'Text-driven style transfer aims to merge the style of a reference image with\\ncontent described by a text prompt. Recent advancements in text-to-image models\\nhave improved the nuance of style transformations, yet significant challenges\\nremain, particularly with overfitting to reference styles, limiting stylistic\\ncontrol, and misaligning with textual content. In this paper, we propose three\\ncomplementary strategies to address these issues. First, we introduce a\\ncross-modal Adaptive Instance Normalization (AdaIN) mechanism for better\\nintegration of style and text features, enhancing alignment. Second, we develop\\na Style-based Classifier-Free Guidance (SCFG) approach that enables selective\\ncontrol over stylistic elements, reducing irrelevant influences. Finally, we\\nincorporate a teacher model during early generation stages to stabilize spatial\\nlayouts and mitigate artifacts. Our extensive evaluations demonstrate\\nsignificant improvements in style transfer quality and alignment with textual\\nprompts. Furthermore, our approach can be integrated into existing style\\ntransfer frameworks without fine-tuning.', 'upvotes': 2, 'discussionId': '675a91655f94292180d786e0'}, 'publishedAt': '2024-12-12T06:06:22.828Z', 'title': 'StyleStudio: Text-Driven Style Transfer with Selective Control of Style Elements', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/64196320ed725fef64419c2a/INlUiHbYMl9OISmXBooPm.png', 'https://cdn-uploads.huggingface.co/production/uploads/64196320ed725fef64419c2a/UHoMxj_qsMnGgW2YQzlh5.png'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.08503.png', 'numComments': 2, 'submittedBy': {'_id': '64196320ed725fef64419c2a', 'avatarUrl': '/avatars/96feb22fb5e8931d6c9e0ea06148266f.svg', 'fullname': 'Chi Zhang', 'name': 'DrChiZhang', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 3}}",
    "{'paper': {'id': '2412.07147', 'authors': [{'_id': '6758f7bfcbf493892e08e882', 'user': {'_id': '6582c482f3006507ea10302a', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6582c482f3006507ea10302a/KbgSsq0FnbMngBcWPhIXi.jpeg', 'isPro': False, 'fullname': 'Bo Li', 'user': 'liboaccn', 'type': 'user'}, 'name': 'Bo Li', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-11T09:20:03.698Z', 'hidden': False}, {'_id': '6758f7bfcbf493892e08e883', 'name': 'Shaolin Zhu', 'hidden': False}, {'_id': '6758f7bfcbf493892e08e884', 'name': 'Lijie Wen', 'hidden': False}], 'publishedAt': '2024-12-10T03:12:35.000Z', 'title': 'MIT-10M: A Large Scale Parallel Corpus of Multilingual Image Translation', 'summary': 'Image Translation (IT) holds immense potential across diverse domains,\\nenabling the translation of textual content within images into various\\nlanguages. However, existing datasets often suffer from limitations in scale,\\ndiversity, and quality, hindering the development and evaluation of IT models.\\nTo address this issue, we introduce MIT-10M, a large-scale parallel corpus of\\nmultilingual image translation with over 10M image-text pairs derived from\\nreal-world data, which has undergone extensive data cleaning and multilingual\\ntranslation validation. It contains 840K images in three sizes, 28 categories,\\ntasks with three levels of difficulty and 14 languages image-text pairs, which\\nis a considerable improvement on existing datasets. We conduct extensive\\nexperiments to evaluate and train models on MIT-10M. The experimental results\\nclearly indicate that our dataset has higher adaptability when it comes to\\nevaluating the performance of the models in tackling challenging and complex\\nimage translation tasks in the real world. Moreover, the performance of the\\nmodel fine-tuned with MIT-10M has tripled compared to the baseline model,\\nfurther confirming its superiority.', 'upvotes': 1, 'discussionId': '6758f7c2cbf493892e08e908'}, 'publishedAt': '2024-12-12T04:40:46.247Z', 'title': 'MIT-10M: A Large Scale Parallel Corpus of Multilingual Image Translation', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.07147.png', 'numComments': 2, 'submittedBy': {'_id': '6582c482f3006507ea10302a', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6582c482f3006507ea10302a/KbgSsq0FnbMngBcWPhIXi.jpeg', 'fullname': 'Bo Li', 'name': 'liboaccn', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}}",
    "{'paper': {'id': '2412.08467', 'authors': [{'_id': '675ad589659e5553260b6610', 'name': 'Zun Wang', 'hidden': False}, {'_id': '675ad589659e5553260b6611', 'name': 'Jialu Li', 'hidden': False}, {'_id': '675ad589659e5553260b6612', 'name': 'Yicong Hong', 'hidden': False}, {'_id': '675ad589659e5553260b6613', 'name': 'Songze Li', 'hidden': False}, {'_id': '675ad589659e5553260b6614', 'name': 'Kunchang Li', 'hidden': False}, {'_id': '675ad589659e5553260b6615', 'name': 'Shoubin Yu', 'hidden': False}, {'_id': '675ad589659e5553260b6616', 'name': 'Yi Wang', 'hidden': False}, {'_id': '675ad589659e5553260b6617', 'name': 'Yu Qiao', 'hidden': False}, {'_id': '675ad589659e5553260b6618', 'name': 'Yali Wang', 'hidden': False}, {'_id': '675ad589659e5553260b6619', 'name': 'Mohit Bansal', 'hidden': False}, {'_id': '675ad589659e5553260b661a', 'name': 'Limin Wang', 'hidden': False}], 'publishedAt': '2024-12-11T15:32:24.000Z', 'title': 'Bootstrapping Language-Guided Navigation Learning with Self-Refining\\n  Data Flywheel', 'summary': 'Creating high-quality data for training robust language-instructed agents is\\na long-lasting challenge in embodied AI. In this paper, we introduce a\\nSelf-Refining Data Flywheel (SRDF) that generates high-quality and large-scale\\nnavigational instruction-trajectory pairs by iteratively refining the data pool\\nthrough the collaboration between two models, the instruction generator and the\\nnavigator, without any human-in-the-loop annotation. Specifically, SRDF starts\\nwith using a base generator to create an initial data pool for training a base\\nnavigator, followed by applying the trained navigator to filter the data pool.\\nThis leads to higher-fidelity data to train a better generator, which can, in\\nturn, produce higher-quality data for training the next-round navigator. Such a\\nflywheel establishes a data self-refining process, yielding a continuously\\nimproved and highly effective dataset for large-scale language-guided\\nnavigation learning. Our experiments demonstrate that after several flywheel\\nrounds, the navigator elevates the performance boundary from 70% to 78% SPL on\\nthe classic R2R test set, surpassing human performance (76%) for the first\\ntime. Meanwhile, this process results in a superior generator, evidenced by a\\nSPICE increase from 23.5 to 26.2, better than all previous VLN instruction\\ngeneration methods. Finally, we demonstrate the scalability of our method\\nthrough increasing environment and instruction diversity, and the\\ngeneralization ability of our pre-trained navigator across various downstream\\nnavigation tasks, surpassing state-of-the-art methods by a large margin in all\\ncases.', 'upvotes': 0, 'discussionId': '675ad58b659e5553260b667a'}, 'publishedAt': '2024-12-12T07:22:35.320Z', 'title': 'Bootstrapping Language-Guided Navigation Learning with Self-Refining Data Flywheel', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.08467.png', 'numComments': 2, 'submittedBy': {'_id': '6622c710b0e5c5e3de8311c1', 'avatarUrl': '/avatars/a824c150040731679bbd77762ca9d4eb.svg', 'fullname': 'Zun Wang', 'name': 'ZunWang', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 1}}"
]