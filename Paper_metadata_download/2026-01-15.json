[
  {
    "paper": {
      "id": "2601.07348",
      "authors": [
        {
          "_id": "696855610ac10a06522f69cf",
          "name": "Tu Hu",
          "hidden": false
        },
        {
          "_id": "696855610ac10a06522f69d0",
          "name": "Ronghao Chen",
          "hidden": false
        },
        {
          "_id": "696855610ac10a06522f69d1",
          "name": "Shuo Zhang",
          "hidden": false
        },
        {
          "_id": "696855610ac10a06522f69d2",
          "name": "Jianghao Yin",
          "hidden": false
        },
        {
          "_id": "696855610ac10a06522f69d3",
          "name": "Mou Xiao Feng",
          "hidden": false
        },
        {
          "_id": "696855610ac10a06522f69d4",
          "name": "Jingping Liu",
          "hidden": false
        },
        {
          "_id": "696855610ac10a06522f69d5",
          "name": "Shaolei Zhang",
          "hidden": false
        },
        {
          "_id": "696855610ac10a06522f69d6",
          "name": "Wenqi Jiang",
          "hidden": false
        },
        {
          "_id": "696855610ac10a06522f69d7",
          "name": "Yuqi Fang",
          "hidden": false
        },
        {
          "_id": "696855610ac10a06522f69d8",
          "name": "Sen Hu",
          "hidden": false
        },
        {
          "_id": "696855610ac10a06522f69d9",
          "name": "Yi Xu",
          "hidden": false
        },
        {
          "_id": "696855610ac10a06522f69da",
          "name": "Huacan Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-12T09:23:13.000Z",
      "submittedOnDailyAt": "2026-01-15T00:23:14.421Z",
      "title": "Controlled Self-Evolution for Algorithmic Code Optimization",
      "submittedOnDailyBy": {
        "_id": "6603d56ab4344a2b07cd6d21",
        "avatarUrl": "/avatars/1569bb60166532317c85e80da722ba1c.svg",
        "isPro": false,
        "fullname": "Huacan Wang",
        "user": "Huacan-Wang",
        "type": "user"
      },
      "summary": "Self-evolution methods enhance code generation through iterative \"generate-verify-refine\" cycles, yet existing approaches suffer from low exploration efficiency, failing to discover solutions with superior complexity within limited budgets. This inefficiency stems from initialization bias trapping evolution in poor solution regions, uncontrolled stochastic operations lacking feedback guidance, and insufficient experience utilization across tasks. To address these bottlenecks, we propose Controlled Self-Evolution (CSE), which consists of three key components. Diversified Planning Initialization generates structurally distinct algorithmic strategies for broad solution space coverage. Genetic Evolution replaces stochastic operations with feedback-guided mechanisms, enabling targeted mutation and compositional crossover. Hierarchical Evolution Memory captures both successful and failed experiences at inter-task and intra-task levels. Experiments on EffiBench-X demonstrate that CSE consistently outperforms all baselines across various LLM backbones. Furthermore, CSE achieves higher efficiency from early generations and maintains continuous improvement throughout evolution. Our code is publicly available at https://github.com/QuantaAlpha/EvoControl.",
      "upvotes": 55,
      "discussionId": "696855610ac10a06522f69db",
      "githubRepo": "https://github.com/QuantaAlpha/EvoControl",
      "githubRepoAddedBy": "user",
      "ai_summary": "Controlled Self-Evolution method improves code generation through diversified initialization, feedback-guided genetic evolution, and hierarchical memory to enhance exploration efficiency and solution quality.",
      "ai_keywords": [
        "self-evolution methods",
        "generate-verify-refine cycles",
        "exploration efficiency",
        "initialization bias",
        "stochastic operations",
        "feedback guidance",
        "genetic evolution",
        "targeted mutation",
        "compositional crossover",
        "hierarchical evolution memory",
        "LLM backbones",
        "EffiBench-X"
      ],
      "githubStars": 22,
      "organization": {
        "_id": "68b33ab6a9ed99140481cf44",
        "name": "QuantaAlpha",
        "fullname": "QuantaAlpha",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f7767fbd28622c9b9915e9/DRN8PvmnpKmn2MSLQ7qhF.jpeg"
      }
    },
    "publishedAt": "2026-01-12T04:23:13.000Z",
    "title": "Controlled Self-Evolution for Algorithmic Code Optimization",
    "summary": "Self-evolution methods enhance code generation through iterative \"generate-verify-refine\" cycles, yet existing approaches suffer from low exploration efficiency, failing to discover solutions with superior complexity within limited budgets. This inefficiency stems from initialization bias trapping evolution in poor solution regions, uncontrolled stochastic operations lacking feedback guidance, and insufficient experience utilization across tasks. To address these bottlenecks, we propose Controlled Self-Evolution (CSE), which consists of three key components. Diversified Planning Initialization generates structurally distinct algorithmic strategies for broad solution space coverage. Genetic Evolution replaces stochastic operations with feedback-guided mechanisms, enabling targeted mutation and compositional crossover. Hierarchical Evolution Memory captures both successful and failed experiences at inter-task and intra-task levels. Experiments on EffiBench-X demonstrate that CSE consistently outperforms all baselines across various LLM backbones. Furthermore, CSE achieves higher efficiency from early generations and maintains continuous improvement throughout evolution. Our code is publicly available at https://github.com/QuantaAlpha/EvoControl.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.07348.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6603d56ab4344a2b07cd6d21",
      "avatarUrl": "/avatars/1569bb60166532317c85e80da722ba1c.svg",
      "fullname": "Huacan Wang",
      "name": "Huacan-Wang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "68b33ab6a9ed99140481cf44",
      "name": "QuantaAlpha",
      "fullname": "QuantaAlpha",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f7767fbd28622c9b9915e9/DRN8PvmnpKmn2MSLQ7qhF.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.09688",
      "authors": [
        {
          "_id": "696864c90ac10a06522f6a4a",
          "name": "Yibo Wang",
          "hidden": false
        },
        {
          "_id": "696864c90ac10a06522f6a4b",
          "name": "Lei Wang",
          "hidden": false
        },
        {
          "_id": "696864c90ac10a06522f6a4c",
          "name": "Yue Deng",
          "hidden": false
        },
        {
          "_id": "696864c90ac10a06522f6a4d",
          "name": "Keming Wu",
          "hidden": false
        },
        {
          "_id": "696864c90ac10a06522f6a4e",
          "name": "Yao Xiao",
          "hidden": false
        },
        {
          "_id": "696864c90ac10a06522f6a4f",
          "name": "Huanjin Yao",
          "hidden": false
        },
        {
          "_id": "696864c90ac10a06522f6a50",
          "name": "Liwei Kang",
          "hidden": false
        },
        {
          "_id": "696864c90ac10a06522f6a51",
          "name": "Hai Ye",
          "hidden": false
        },
        {
          "_id": "696864c90ac10a06522f6a52",
          "name": "Yongcheng Jing",
          "hidden": false
        },
        {
          "_id": "696864c90ac10a06522f6a53",
          "name": "Lidong Bing",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-14T18:38:31.000Z",
      "submittedOnDailyAt": "2026-01-15T01:33:59.520Z",
      "title": "DeepResearchEval: An Automated Framework for Deep Research Task Construction and Agentic Evaluation",
      "submittedOnDailyBy": {
        "_id": "66bf00ca5b4e241fe266059d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66bf00ca5b4e241fe266059d/VoWPC_C4zoeT6dS699t7L.png",
        "isPro": false,
        "fullname": "Keming Wu",
        "user": "wukeming11",
        "type": "user"
      },
      "summary": "Deep research systems are widely used for multi-step web research, analysis, and cross-source synthesis, yet their evaluation remains challenging. Existing benchmarks often require annotation-intensive task construction, rely on static evaluation dimensions, or fail to reliably verify facts when citations are missing. To bridge these gaps, we introduce DeepResearchEval, an automated framework for deep research task construction and agentic evaluation. For task construction, we propose a persona-driven pipeline generating realistic, complex research tasks anchored in diverse user profiles, applying a two-stage filter Task Qualification and Search Necessity to retain only tasks requiring multi-source evidence integration and external retrieval. For evaluation, we propose an agentic pipeline with two components: an Adaptive Point-wise Quality Evaluation that dynamically derives task-specific evaluation dimensions, criteria, and weights conditioned on each generated task, and an Active Fact-Checking that autonomously extracts and verifies report statements via web search, even when citations are missing.",
      "upvotes": 49,
      "discussionId": "696864c90ac10a06522f6a54",
      "githubRepo": "https://github.com/Infinity-AILab/DeepResearchEval",
      "githubRepoAddedBy": "user",
      "ai_summary": "DeepResearchEval presents an automated framework for creating complex research tasks and evaluating them through agent-based methods that adapt to task specifics and verify facts without relying on citations.",
      "ai_keywords": [
        "automated framework",
        "deep research task construction",
        "agentic evaluation",
        "persona-driven pipeline",
        "task qualification",
        "search necessity",
        "adaptive point-wise quality evaluation",
        "active fact-checking",
        "web search",
        "multi-source evidence integration"
      ],
      "githubStars": 24,
      "organization": {
        "_id": "6948e6c46d88786b0ec9cf9d",
        "name": "Infinity-AILab",
        "fullname": "Infinity Lab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6362a77dd3be91534c2e9213/-zILHmHPjnq27MzoESFsG.png"
      }
    },
    "publishedAt": "2026-01-14T13:38:31.000Z",
    "title": "DeepResearchEval: An Automated Framework for Deep Research Task Construction and Agentic Evaluation",
    "summary": "Deep research systems are widely used for multi-step web research, analysis, and cross-source synthesis, yet their evaluation remains challenging. Existing benchmarks often require annotation-intensive task construction, rely on static evaluation dimensions, or fail to reliably verify facts when citations are missing. To bridge these gaps, we introduce DeepResearchEval, an automated framework for deep research task construction and agentic evaluation. For task construction, we propose a persona-driven pipeline generating realistic, complex research tasks anchored in diverse user profiles, applying a two-stage filter Task Qualification and Search Necessity to retain only tasks requiring multi-source evidence integration and external retrieval. For evaluation, we propose an agentic pipeline with two components: an Adaptive Point-wise Quality Evaluation that dynamically derives task-specific evaluation dimensions, criteria, and weights conditioned on each generated task, and an Active Fact-Checking that autonomously extracts and verifies report statements via web search, even when citations are missing.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09688.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66bf00ca5b4e241fe266059d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66bf00ca5b4e241fe266059d/VoWPC_C4zoeT6dS699t7L.png",
      "fullname": "Keming Wu",
      "name": "wukeming11",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "6948e6c46d88786b0ec9cf9d",
      "name": "Infinity-AILab",
      "fullname": "Infinity Lab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6362a77dd3be91534c2e9213/-zILHmHPjnq27MzoESFsG.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.09259",
      "authors": [
        {
          "_id": "696856230ac10a06522f69dd",
          "name": "Jian Zhang",
          "hidden": false
        },
        {
          "_id": "696856230ac10a06522f69de",
          "name": "Zhiyuan Wang",
          "hidden": false
        },
        {
          "_id": "696856230ac10a06522f69df",
          "name": "Zhangqi Wang",
          "hidden": false
        },
        {
          "_id": "696856230ac10a06522f69e0",
          "name": "Yu He",
          "hidden": false
        },
        {
          "_id": "696856230ac10a06522f69e1",
          "name": "Haoran Luo",
          "hidden": false
        },
        {
          "_id": "696856230ac10a06522f69e2",
          "name": "li yuan",
          "hidden": false
        },
        {
          "_id": "696856230ac10a06522f69e3",
          "name": "Lingling Zhang",
          "hidden": false
        },
        {
          "_id": "696856230ac10a06522f69e4",
          "name": "Rui Mao",
          "hidden": false
        },
        {
          "_id": "696856230ac10a06522f69e5",
          "name": "Qika Lin",
          "hidden": false
        },
        {
          "_id": "696856230ac10a06522f69e6",
          "name": "Jun Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-14T07:48:00.000Z",
      "submittedOnDailyAt": "2026-01-15T00:22:01.292Z",
      "title": "MAXS: Meta-Adaptive Exploration with LLM Agents",
      "submittedOnDailyBy": {
        "_id": "658be7fe135580745c510323",
        "avatarUrl": "/avatars/830e5cec4565efdc23226a86a0fcef0e.svg",
        "isPro": false,
        "fullname": "Jian Zhang",
        "user": "VentureZJ",
        "type": "user"
      },
      "summary": "Large Language Model (LLM) Agents exhibit inherent reasoning abilities through the collaboration of multiple tools. However, during agent inference, existing methods often suffer from (i) locally myopic generation, due to the absence of lookahead, and (ii) trajectory instability, where minor early errors can escalate into divergent reasoning paths. These issues make it difficult to balance global effectiveness and computational efficiency. To address these two issues, we propose meta-adaptive exploration with LLM agents https://github.com/exoskeletonzj/MAXS, a meta-adaptive reasoning framework based on LLM Agents that flexibly integrates tool execution and reasoning planning. MAXS employs a lookahead strategy to extend reasoning paths a few steps ahead, estimating the advantage value of tool usage, and combines step consistency variance and inter-step trend slopes to jointly select stable, consistent, and high-value reasoning steps. Additionally, we introduce a trajectory convergence mechanism that controls computational cost by halting further rollouts once path consistency is achieved, enabling a balance between resource efficiency and global effectiveness in multi-tool reasoning. We conduct extensive empirical studies across three base models (MiMo-VL-7B, Qwen2.5-VL-7B, Qwen2.5-VL-32B) and five datasets, demonstrating that MAXS consistently outperforms existing methods in both performance and inference efficiency. Further analysis confirms the effectiveness of our lookahead strategy and tool usage.",
      "upvotes": 40,
      "discussionId": "696856230ac10a06522f69e7",
      "ai_summary": "MAXS is a meta-adaptive reasoning framework for LLM agents that improves multi-tool reasoning through lookahead strategies and trajectory convergence mechanisms, balancing global effectiveness and computational efficiency.",
      "ai_keywords": [
        "LLM agents",
        "tool execution",
        "reasoning planning",
        "lookahead strategy",
        "advantage value",
        "step consistency variance",
        "inter-step trend slopes",
        "trajectory convergence",
        "multi-tool reasoning",
        "inference efficiency"
      ]
    },
    "publishedAt": "2026-01-14T02:48:00.000Z",
    "title": "MAXS: Meta-Adaptive Exploration with LLM Agents",
    "summary": "Large Language Model (LLM) Agents exhibit inherent reasoning abilities through the collaboration of multiple tools. However, during agent inference, existing methods often suffer from (i) locally myopic generation, due to the absence of lookahead, and (ii) trajectory instability, where minor early errors can escalate into divergent reasoning paths. These issues make it difficult to balance global effectiveness and computational efficiency. To address these two issues, we propose meta-adaptive exploration with LLM agents https://github.com/exoskeletonzj/MAXS, a meta-adaptive reasoning framework based on LLM Agents that flexibly integrates tool execution and reasoning planning. MAXS employs a lookahead strategy to extend reasoning paths a few steps ahead, estimating the advantage value of tool usage, and combines step consistency variance and inter-step trend slopes to jointly select stable, consistent, and high-value reasoning steps. Additionally, we introduce a trajectory convergence mechanism that controls computational cost by halting further rollouts once path consistency is achieved, enabling a balance between resource efficiency and global effectiveness in multi-tool reasoning. We conduct extensive empirical studies across three base models (MiMo-VL-7B, Qwen2.5-VL-7B, Qwen2.5-VL-32B) and five datasets, demonstrating that MAXS consistently outperforms existing methods in both performance and inference efficiency. Further analysis confirms the effectiveness of our lookahead strategy and tool usage.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09259.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "658be7fe135580745c510323",
      "avatarUrl": "/avatars/830e5cec4565efdc23226a86a0fcef0e.svg",
      "fullname": "Jian Zhang",
      "name": "VentureZJ",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.09274",
      "authors": [
        {
          "_id": "6968568f0ac10a06522f69e9",
          "name": "Jian Zhang",
          "hidden": false
        },
        {
          "_id": "6968568f0ac10a06522f69ea",
          "name": "Yu He",
          "hidden": false
        },
        {
          "_id": "6968568f0ac10a06522f69eb",
          "name": "Zhiyuan Wang",
          "hidden": false
        },
        {
          "_id": "6968568f0ac10a06522f69ec",
          "name": "Zhangqi Wang",
          "hidden": false
        },
        {
          "_id": "6968568f0ac10a06522f69ed",
          "name": "Kai He",
          "hidden": false
        },
        {
          "_id": "6968568f0ac10a06522f69ee",
          "name": "Fangzhi Xu",
          "hidden": false
        },
        {
          "_id": "6968568f0ac10a06522f69ef",
          "name": "Qika Lin",
          "hidden": false
        },
        {
          "_id": "6968568f0ac10a06522f69f0",
          "name": "Jun Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-14T08:17:41.000Z",
      "submittedOnDailyAt": "2026-01-15T00:23:45.077Z",
      "title": "A^3-Bench: Benchmarking Memory-Driven Scientific Reasoning via Anchor and Attractor Activation",
      "submittedOnDailyBy": {
        "_id": "658be7fe135580745c510323",
        "avatarUrl": "/avatars/830e5cec4565efdc23226a86a0fcef0e.svg",
        "isPro": false,
        "fullname": "Jian Zhang",
        "user": "VentureZJ",
        "type": "user"
      },
      "summary": "Scientific reasoning relies not only on logical inference but also on activating prior knowledge and experiential structures. Memory can efficiently reuse knowledge and enhance reasoning consistency and stability. However, existing benchmarks mainly evaluate final answers or step-by-step coherence, overlooking the memory-driven mechanisms that underlie human reasoning, which involves activating anchors and attractors, then integrating them into multi-step inference. To address this gap, we propose A^3-Bench~ https://a3-bench.github.io, a benchmark designed to evaluate scientific reasoning through dual-scale memory-driven activation, grounded in Anchor and Attractor Activation. First, we annotate 2,198 science reasoning problems across domains using the SAPM process(subject, anchor & attractor, problem, and memory developing). Second, we introduce a dual-scale memory evaluation framework utilizing anchors and attractors, along with the AAUI(Anchor--Attractor Utilization Index) metric to measure memory activation rates. Finally, through experiments with various base models and paradigms, we validate A^3-Bench and analyze how memory activation impacts reasoning performance, providing insights into memory-driven scientific reasoning.",
      "upvotes": 39,
      "discussionId": "6968568f0ac10a06522f69f1"
    },
    "publishedAt": "2026-01-14T03:17:41.000Z",
    "title": "A^3-Bench: Benchmarking Memory-Driven Scientific Reasoning via Anchor and Attractor Activation",
    "summary": "Scientific reasoning relies not only on logical inference but also on activating prior knowledge and experiential structures. Memory can efficiently reuse knowledge and enhance reasoning consistency and stability. However, existing benchmarks mainly evaluate final answers or step-by-step coherence, overlooking the memory-driven mechanisms that underlie human reasoning, which involves activating anchors and attractors, then integrating them into multi-step inference. To address this gap, we propose A^3-Bench~ https://a3-bench.github.io, a benchmark designed to evaluate scientific reasoning through dual-scale memory-driven activation, grounded in Anchor and Attractor Activation. First, we annotate 2,198 science reasoning problems across domains using the SAPM process(subject, anchor & attractor, problem, and memory developing). Second, we introduce a dual-scale memory evaluation framework utilizing anchors and attractors, along with the AAUI(Anchor--Attractor Utilization Index) metric to measure memory activation rates. Finally, through experiments with various base models and paradigms, we validate A^3-Bench and analyze how memory activation impacts reasoning performance, providing insights into memory-driven scientific reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09274.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "658be7fe135580745c510323",
      "avatarUrl": "/avatars/830e5cec4565efdc23226a86a0fcef0e.svg",
      "fullname": "Jian Zhang",
      "name": "VentureZJ",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.09708",
      "authors": [
        {
          "_id": "69684f740ac10a06522f69ba",
          "name": "Chi-Pin Huang",
          "hidden": false
        },
        {
          "_id": "69684f740ac10a06522f69bb",
          "name": "Yunze Man",
          "hidden": false
        },
        {
          "_id": "69684f740ac10a06522f69bc",
          "name": "Zhiding Yu",
          "hidden": false
        },
        {
          "_id": "69684f740ac10a06522f69bd",
          "name": "Min-Hung Chen",
          "hidden": false
        },
        {
          "_id": "69684f740ac10a06522f69be",
          "name": "Jan Kautz",
          "hidden": false
        },
        {
          "_id": "69684f740ac10a06522f69bf",
          "name": "Yu-Chiang Frank Wang",
          "hidden": false
        },
        {
          "_id": "69684f740ac10a06522f69c0",
          "name": "Fu-En Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-14T18:59:59.000Z",
      "submittedOnDailyAt": "2026-01-15T00:10:27.528Z",
      "title": "Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning",
      "submittedOnDailyBy": {
        "_id": "64705d224be5cf1f3348d6bc",
        "avatarUrl": "/avatars/270bff7c7cb326528dc192fc38561a8b.svg",
        "isPro": false,
        "fullname": "Chi-Pin Huang",
        "user": "jasper0314-huang",
        "type": "user"
      },
      "summary": "Vision-Language-Action (VLA) tasks require reasoning over complex visual scenes and executing adaptive actions in dynamic environments. While recent studies on reasoning VLAs show that explicit chain-of-thought (CoT) can improve generalization, they suffer from high inference latency due to lengthy reasoning traces. We propose Fast-ThinkAct, an efficient reasoning framework that achieves compact yet performant planning through verbalizable latent reasoning. Fast-ThinkAct learns to reason efficiently with latent CoTs by distilling from a teacher, driven by a preference-guided objective to align manipulation trajectories that transfers both linguistic and visual planning capabilities for embodied control. This enables reasoning-enhanced policy learning that effectively connects compact reasoning to action execution. Extensive experiments across diverse embodied manipulation and reasoning benchmarks demonstrate that Fast-ThinkAct achieves strong performance with up to 89.3\\% reduced inference latency over state-of-the-art reasoning VLAs, while maintaining effective long-horizon planning, few-shot adaptation, and failure recovery.",
      "upvotes": 30,
      "discussionId": "69684f740ac10a06522f69c1",
      "projectPage": "https://jasper0314-huang.github.io/fast-thinkact/",
      "ai_summary": "Fast-ThinkAct is an efficient vision-language-action framework that reduces inference latency by 89.3% through compact latent reasoning while maintaining long-horizon planning and few-shot adaptation capabilities.",
      "ai_keywords": [
        "chain-of-thought",
        "latent reasoning",
        "preference-guided objective",
        "embodied control",
        "policy learning",
        "inference latency",
        "vision-language-action"
      ],
      "organization": {
        "_id": "60262b67268c201cdc8b7d43",
        "name": "nvidia",
        "fullname": "NVIDIA",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
      }
    },
    "publishedAt": "2026-01-14T13:59:59.000Z",
    "title": "Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning",
    "summary": "Vision-Language-Action (VLA) tasks require reasoning over complex visual scenes and executing adaptive actions in dynamic environments. While recent studies on reasoning VLAs show that explicit chain-of-thought (CoT) can improve generalization, they suffer from high inference latency due to lengthy reasoning traces. We propose Fast-ThinkAct, an efficient reasoning framework that achieves compact yet performant planning through verbalizable latent reasoning. Fast-ThinkAct learns to reason efficiently with latent CoTs by distilling from a teacher, driven by a preference-guided objective to align manipulation trajectories that transfers both linguistic and visual planning capabilities for embodied control. This enables reasoning-enhanced policy learning that effectively connects compact reasoning to action execution. Extensive experiments across diverse embodied manipulation and reasoning benchmarks demonstrate that Fast-ThinkAct achieves strong performance with up to 89.3\\% reduced inference latency over state-of-the-art reasoning VLAs, while maintaining effective long-horizon planning, few-shot adaptation, and failure recovery.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09708.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64705d224be5cf1f3348d6bc",
      "avatarUrl": "/avatars/270bff7c7cb326528dc192fc38561a8b.svg",
      "fullname": "Chi-Pin Huang",
      "name": "jasper0314-huang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "60262b67268c201cdc8b7d43",
      "name": "nvidia",
      "fullname": "NVIDIA",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.09136",
      "authors": [
        {
          "_id": "6968555c0ac10a06522f69c3",
          "name": "Lijun Liu",
          "hidden": false
        },
        {
          "_id": "6968555c0ac10a06522f69c4",
          "name": "Linwei Chen",
          "hidden": false
        },
        {
          "_id": "6968555c0ac10a06522f69c5",
          "name": "Zhishou Zhang",
          "hidden": false
        },
        {
          "_id": "6968555c0ac10a06522f69c6",
          "name": "Meng Tian",
          "hidden": false
        },
        {
          "_id": "6968555c0ac10a06522f69c7",
          "name": "Hengfu Cui",
          "hidden": false
        },
        {
          "_id": "6968555c0ac10a06522f69c8",
          "name": "Ruiyang Li",
          "hidden": false
        },
        {
          "_id": "6968555c0ac10a06522f69c9",
          "name": "Zhaocheng Liu",
          "hidden": false
        },
        {
          "_id": "6968555c0ac10a06522f69ca",
          "name": "Qiang Ju",
          "hidden": false
        },
        {
          "_id": "6968555c0ac10a06522f69cb",
          "name": "Qianxi Li",
          "hidden": false
        },
        {
          "_id": "6968555c0ac10a06522f69cc",
          "name": "Hong-Yu Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-14T04:21:07.000Z",
      "submittedOnDailyAt": "2026-01-15T00:59:44.722Z",
      "title": "SkinFlow: Efficient Information Transmission for Open Dermatological Diagnosis via Dynamic Visual Encoding and Staged RL",
      "submittedOnDailyBy": {
        "_id": "642438eaa3adbc7142c3ca0f",
        "avatarUrl": "/avatars/8deff70e0c93d259a42ee47f00a31e3e.svg",
        "isPro": false,
        "fullname": "CharlesChen",
        "user": "CharlesChen2023",
        "type": "user"
      },
      "summary": "General-purpose Large Vision-Language Models (LVLMs), despite their massive scale, often falter in dermatology due to \"diffuse attention\" - the inability to disentangle subtle pathological lesions from background noise. In this paper, we challenge the assumption that parameter scaling is the only path to medical precision. We introduce SkinFlow, a framework that treats diagnosis as an optimization of visual information transmission efficiency. Our approach utilizes a Virtual-Width Dynamic Vision Encoder (DVE) to \"unfold\" complex pathological manifolds without physical parameter expansion, coupled with a two-stage Reinforcement Learning strategy. This strategy sequentially aligns explicit medical descriptions (Stage I) and reconstructs implicit diagnostic textures (Stage II) within a constrained semantic space. Furthermore, we propose a clinically grounded evaluation protocol that prioritizes diagnostic safety and hierarchical relevance over rigid label matching. Empirical results are compelling: our 7B model establishes a new state-of-the-art on the Fitzpatrick17k benchmark, achieving a +12.06% gain in Top-1 accuracy and a +28.57% boost in Top-6 accuracy over the massive general-purpose models (e.g., Qwen3VL-235B and GPT-5.2). These findings demonstrate that optimizing geometric capacity and information flow yields superior diagnostic reasoning compared to raw parameter scaling.",
      "upvotes": 26,
      "discussionId": "6968555c0ac10a06522f69cd",
      "ai_summary": "SkinFlow introduces a novel framework for dermatological vision-language modeling that improves diagnostic accuracy through optimized visual information transmission efficiency rather than parameter scaling alone.",
      "ai_keywords": [
        "Large Vision-Language Models",
        "diffuse attention",
        "Virtual-Width Dynamic Vision Encoder",
        "reinforcement learning",
        "visual information transmission efficiency",
        "diagnostic reasoning",
        "Fitzpatrick17k benchmark",
        "Top-1 accuracy",
        "Top-6 accuracy"
      ],
      "organization": {
        "_id": "648457d38cf0b32b0ba0a913",
        "name": "baichuan-inc",
        "fullname": "Baichuan Intelligent Technology",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/640dd3e0c364a086c6322ad2/acwcllU0PQz4Bg3gchhYo.png"
      }
    },
    "publishedAt": "2026-01-13T23:21:07.000Z",
    "title": "SkinFlow: Efficient Information Transmission for Open Dermatological Diagnosis via Dynamic Visual Encoding and Staged RL",
    "summary": "General-purpose Large Vision-Language Models (LVLMs), despite their massive scale, often falter in dermatology due to \"diffuse attention\" - the inability to disentangle subtle pathological lesions from background noise. In this paper, we challenge the assumption that parameter scaling is the only path to medical precision. We introduce SkinFlow, a framework that treats diagnosis as an optimization of visual information transmission efficiency. Our approach utilizes a Virtual-Width Dynamic Vision Encoder (DVE) to \"unfold\" complex pathological manifolds without physical parameter expansion, coupled with a two-stage Reinforcement Learning strategy. This strategy sequentially aligns explicit medical descriptions (Stage I) and reconstructs implicit diagnostic textures (Stage II) within a constrained semantic space. Furthermore, we propose a clinically grounded evaluation protocol that prioritizes diagnostic safety and hierarchical relevance over rigid label matching. Empirical results are compelling: our 7B model establishes a new state-of-the-art on the Fitzpatrick17k benchmark, achieving a +12.06% gain in Top-1 accuracy and a +28.57% boost in Top-6 accuracy over the massive general-purpose models (e.g., Qwen3VL-235B and GPT-5.2). These findings demonstrate that optimizing geometric capacity and information flow yields superior diagnostic reasoning compared to raw parameter scaling.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09136.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "642438eaa3adbc7142c3ca0f",
      "avatarUrl": "/avatars/8deff70e0c93d259a42ee47f00a31e3e.svg",
      "fullname": "CharlesChen",
      "name": "CharlesChen2023",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "648457d38cf0b32b0ba0a913",
      "name": "baichuan-inc",
      "fullname": "Baichuan Intelligent Technology",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/640dd3e0c364a086c6322ad2/acwcllU0PQz4Bg3gchhYo.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.09575",
      "authors": [
        {
          "_id": "696869800ac10a06522f6a92",
          "name": "Sheng-Yu Huang",
          "hidden": false
        },
        {
          "_id": "696869800ac10a06522f6a93",
          "name": "Jaesung Choe",
          "hidden": false
        },
        {
          "_id": "696869800ac10a06522f6a94",
          "name": "Yu-Chiang Frank Wang",
          "hidden": false
        },
        {
          "_id": "696869800ac10a06522f6a95",
          "name": "Cheng Sun",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/Mf0UQtcxycDL3jlgJpk6d.jpeg"
      ],
      "publishedAt": "2026-01-14T15:45:57.000Z",
      "submittedOnDailyAt": "2026-01-15T01:44:30.828Z",
      "title": "OpenVoxel: Training-Free Grouping and Captioning Voxels for Open-Vocabulary 3D Scene Understanding",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We propose OpenVoxel, a training-free algorithm for grouping and captioning sparse voxels for the open-vocabulary 3D scene understanding tasks. Given the sparse voxel rasterization (SVR) model obtained from multi-view images of a 3D scene, our OpenVoxel is able to produce meaningful groups that describe different objects in the scene. Also, by leveraging powerful Vision Language Models (VLMs) and Multi-modal Large Language Models (MLLMs), our OpenVoxel successfully build an informative scene map by captioning each group, enabling further 3D scene understanding tasks such as open-vocabulary segmentation (OVS) or referring expression segmentation (RES). Unlike previous methods, our method is training-free and does not introduce embeddings from a CLIP/BERT text encoder. Instead, we directly proceed with text-to-text search using MLLMs. Through extensive experiments, our method demonstrates superior performance compared to recent studies, particularly in complex referring expression segmentation (RES) tasks. The code will be open.",
      "upvotes": 18,
      "discussionId": "696869810ac10a06522f6a96",
      "projectPage": "https://peterjohnsonhuang.github.io/openvoxel-pages/",
      "ai_summary": "OpenVoxel enables open-vocabulary 3D scene understanding through training-free grouping and captioning of sparse voxels using Vision Language Models and Multi-modal Large Language Models.",
      "ai_keywords": [
        "open-vocabulary 3D scene understanding",
        "sparse voxels",
        "sparse voxel rasterization",
        "Vision Language Models",
        "Multi-modal Large Language Models",
        "open-vocabulary segmentation",
        "referring expression segmentation",
        "training-free algorithm"
      ],
      "organization": {
        "_id": "60262b67268c201cdc8b7d43",
        "name": "nvidia",
        "fullname": "NVIDIA",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
      }
    },
    "publishedAt": "2026-01-14T10:45:57.000Z",
    "title": "OpenVoxel: Training-Free Grouping and Captioning Voxels for Open-Vocabulary 3D Scene Understanding",
    "summary": "We propose OpenVoxel, a training-free algorithm for grouping and captioning sparse voxels for the open-vocabulary 3D scene understanding tasks. Given the sparse voxel rasterization (SVR) model obtained from multi-view images of a 3D scene, our OpenVoxel is able to produce meaningful groups that describe different objects in the scene. Also, by leveraging powerful Vision Language Models (VLMs) and Multi-modal Large Language Models (MLLMs), our OpenVoxel successfully build an informative scene map by captioning each group, enabling further 3D scene understanding tasks such as open-vocabulary segmentation (OVS) or referring expression segmentation (RES). Unlike previous methods, our method is training-free and does not introduce embeddings from a CLIP/BERT text encoder. Instead, we directly proceed with text-to-text search using MLLMs. Through extensive experiments, our method demonstrates superior performance compared to recent studies, particularly in complex referring expression segmentation (RES) tasks. The code will be open.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/Mf0UQtcxycDL3jlgJpk6d.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09575.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 208,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "60262b67268c201cdc8b7d43",
      "name": "nvidia",
      "fullname": "NVIDIA",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.08605",
      "authors": [
        {
          "_id": "69670413c5e371f6b235d0ea",
          "name": "Wenyuan Zhang",
          "hidden": false
        },
        {
          "_id": "69670413c5e371f6b235d0eb",
          "name": "Xinghua Zhang",
          "hidden": false
        },
        {
          "_id": "69670413c5e371f6b235d0ec",
          "name": "Haiyang Yu",
          "hidden": false
        },
        {
          "_id": "69670413c5e371f6b235d0ed",
          "name": "Shuaiyi Nie",
          "hidden": false
        },
        {
          "_id": "69670413c5e371f6b235d0ee",
          "name": "Bingli Wu",
          "hidden": false
        },
        {
          "_id": "69670413c5e371f6b235d0ef",
          "name": "Juwei Yue",
          "hidden": false
        },
        {
          "_id": "69670413c5e371f6b235d0f0",
          "name": "Tingwen Liu",
          "hidden": false
        },
        {
          "_id": "69670413c5e371f6b235d0f1",
          "name": "Yongbin Li",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-13T14:48:34.000Z",
      "submittedOnDailyAt": "2026-01-15T02:32:45.902Z",
      "title": "ExpSeek: Self-Triggered Experience Seeking for Web Agents",
      "submittedOnDailyBy": {
        "_id": "6617c98901ad3a0642a2a08f",
        "avatarUrl": "/avatars/cf52fb511f2f31de7940f9c13d19b8e7.svg",
        "isPro": false,
        "fullname": "Wenyuan Zhang",
        "user": "WYRipple",
        "type": "user"
      },
      "summary": "Experience intervention in web agents emerges as a promising technical paradigm, enhancing agent interaction capabilities by providing valuable insights from accumulated experiences. However, existing methods predominantly inject experience passively as global context before task execution, struggling to adapt to dynamically changing contextual observations during agent-environment interaction. We propose ExpSeek, which shifts experience toward step-level proactive seeking: (1) estimating step-level entropy thresholds to determine intervention timing using the model's intrinsic signals; (2) designing step-level tailor-designed experience content. Experiments on Qwen3-8B and 32B models across four challenging web agent benchmarks demonstrate that ExpSeek achieves absolute improvements of 9.3% and 7.5%, respectively. Our experiments validate the feasibility and advantages of entropy as a self-triggering signal, reveal that even a 4B small-scale experience model can significantly boost the performance of larger agent models.",
      "upvotes": 15,
      "discussionId": "69670413c5e371f6b235d0f2",
      "ai_summary": "ExpSeek enables web agents to proactively seek experience during interaction by using entropy-based timing and tailored content, achieving significant performance improvements across multiple benchmarks.",
      "ai_keywords": [
        "experience intervention",
        "web agents",
        "step-level proactive seeking",
        "entropy thresholds",
        "intrinsic signals",
        "experience content design",
        "Qwen3-8B",
        "Qwen3-32B",
        "web agent benchmarks"
      ],
      "organization": {
        "_id": "67d15cca6e2cf0e062dbfb54",
        "name": "AlibabaTongyiLab",
        "fullname": "TongyiLab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
      }
    },
    "publishedAt": "2026-01-13T09:48:34.000Z",
    "title": "ExpSeek: Self-Triggered Experience Seeking for Web Agents",
    "summary": "Experience intervention in web agents emerges as a promising technical paradigm, enhancing agent interaction capabilities by providing valuable insights from accumulated experiences. However, existing methods predominantly inject experience passively as global context before task execution, struggling to adapt to dynamically changing contextual observations during agent-environment interaction. We propose ExpSeek, which shifts experience toward step-level proactive seeking: (1) estimating step-level entropy thresholds to determine intervention timing using the model's intrinsic signals; (2) designing step-level tailor-designed experience content. Experiments on Qwen3-8B and 32B models across four challenging web agent benchmarks demonstrate that ExpSeek achieves absolute improvements of 9.3% and 7.5%, respectively. Our experiments validate the feasibility and advantages of entropy as a self-triggering signal, reveal that even a 4B small-scale experience model can significantly boost the performance of larger agent models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.08605.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6617c98901ad3a0642a2a08f",
      "avatarUrl": "/avatars/cf52fb511f2f31de7940f9c13d19b8e7.svg",
      "fullname": "Wenyuan Zhang",
      "name": "WYRipple",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "67d15cca6e2cf0e062dbfb54",
      "name": "AlibabaTongyiLab",
      "fullname": "TongyiLab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.06596",
      "authors": [
        {
          "_id": "69685b660ac10a06522f6a09",
          "name": "Hongjun An",
          "hidden": false
        },
        {
          "_id": "69685b660ac10a06522f6a0a",
          "name": "Yiliang Song",
          "hidden": false
        },
        {
          "_id": "69685b660ac10a06522f6a0b",
          "name": "Jiangan Chen",
          "hidden": false
        },
        {
          "_id": "69685b660ac10a06522f6a0c",
          "name": "Jiawei Shao",
          "hidden": false
        },
        {
          "_id": "69685b660ac10a06522f6a0d",
          "name": "Chi Zhang",
          "hidden": false
        },
        {
          "_id": "69685b660ac10a06522f6a0e",
          "name": "Xuelong Li",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-10T15:16:23.000Z",
      "submittedOnDailyAt": "2026-01-15T00:58:08.622Z",
      "title": "Are LLMs Vulnerable to Preference-Undermining Attacks (PUA)? A Factorial Analysis Methodology for Diagnosing the Trade-off between Preference Alignment and Real-World Validity",
      "submittedOnDailyBy": {
        "_id": "65bc71766db364adbb0ee817",
        "avatarUrl": "/avatars/4f889e82fed3aaff058bde7299b8d585.svg",
        "isPro": false,
        "fullname": "Hongjun An",
        "user": "Coder-AN",
        "type": "user"
      },
      "summary": "Large Language Model (LLM) training often optimizes for preference alignment, rewarding outputs that are perceived as helpful and interaction-friendly. However, this preference-oriented objective can be exploited: manipulative prompts can steer responses toward user-appeasing agreement and away from truth-oriented correction. In this work, we investigate whether aligned models are vulnerable to Preference-Undermining Attacks (PUA), a class of manipulative prompting strategies designed to exploit the model's desire to please user preferences at the expense of truthfulness. We propose a diagnostic methodology that provides a finer-grained and more directive analysis than aggregate benchmark scores, using a factorial evaluation framework to decompose prompt-induced shifts into interpretable effects of system objectives (truth- vs. preference-oriented) and PUA-style dialogue factors (directive control, personal derogation, conditional approval, reality denial) within a controlled 2 times 2^4 design. Surprisingly, more advanced models are sometimes more susceptible to manipulative prompts. Beyond the dominant reality-denial factor, we observe model-specific sign reversals and interactions with PUA-style factors, suggesting tailored defenses rather than uniform robustness. These findings offer a novel, reproducible factorial evaluation methodology that provides finer-grained diagnostics for post-training processes like RLHF, enabling better trade-offs in the product iteration of LLMs by offering a more nuanced understanding of preference alignment risks and the impact of manipulative prompts.",
      "upvotes": 8,
      "discussionId": "69685b670ac10a06522f6a0f",
      "ai_summary": "Research examines how large language models can be manipulated through preference-undermining attacks that exploit alignment objectives, revealing model vulnerabilities and proposing a factorial evaluation method for diagnosing alignment risks.",
      "ai_keywords": [
        "large language model",
        "preference alignment",
        "preference-oriented objective",
        "Preference-Undermining Attacks",
        "RLHF",
        "factorial evaluation framework",
        "manipulative prompting",
        "truth-oriented correction",
        "post-training processes"
      ]
    },
    "publishedAt": "2026-01-10T10:16:23.000Z",
    "title": "Are LLMs Vulnerable to Preference-Undermining Attacks (PUA)? A Factorial Analysis Methodology for Diagnosing the Trade-off between Preference Alignment and Real-World Validity",
    "summary": "Large Language Model (LLM) training often optimizes for preference alignment, rewarding outputs that are perceived as helpful and interaction-friendly. However, this preference-oriented objective can be exploited: manipulative prompts can steer responses toward user-appeasing agreement and away from truth-oriented correction. In this work, we investigate whether aligned models are vulnerable to Preference-Undermining Attacks (PUA), a class of manipulative prompting strategies designed to exploit the model's desire to please user preferences at the expense of truthfulness. We propose a diagnostic methodology that provides a finer-grained and more directive analysis than aggregate benchmark scores, using a factorial evaluation framework to decompose prompt-induced shifts into interpretable effects of system objectives (truth- vs. preference-oriented) and PUA-style dialogue factors (directive control, personal derogation, conditional approval, reality denial) within a controlled 2 times 2^4 design. Surprisingly, more advanced models are sometimes more susceptible to manipulative prompts. Beyond the dominant reality-denial factor, we observe model-specific sign reversals and interactions with PUA-style factors, suggesting tailored defenses rather than uniform robustness. These findings offer a novel, reproducible factorial evaluation methodology that provides finer-grained diagnostics for post-training processes like RLHF, enabling better trade-offs in the product iteration of LLMs by offering a more nuanced understanding of preference alignment risks and the impact of manipulative prompts.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06596.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "65bc71766db364adbb0ee817",
      "avatarUrl": "/avatars/4f889e82fed3aaff058bde7299b8d585.svg",
      "fullname": "Hongjun An",
      "name": "Coder-AN",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.08955",
      "authors": [
        {
          "_id": "696877d40ac10a06522f6abf",
          "name": "Youwei Liu",
          "hidden": false
        },
        {
          "_id": "696877d40ac10a06522f6ac0",
          "name": "Jian Wang",
          "hidden": false
        },
        {
          "_id": "696877d40ac10a06522f6ac1",
          "name": "Hanlin Wang",
          "hidden": false
        },
        {
          "_id": "696877d40ac10a06522f6ac2",
          "name": "Beichen Guo",
          "hidden": false
        },
        {
          "_id": "696877d40ac10a06522f6ac3",
          "name": "Wenjie Li",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-13T19:49:58.000Z",
      "submittedOnDailyAt": "2026-01-15T03:31:06.215Z",
      "title": "Imagine-then-Plan: Agent Learning from Adaptive Lookahead with World Models",
      "submittedOnDailyBy": {
        "_id": "63a4117984a6a25c65bc2fff",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675936454785-63a4117984a6a25c65bc2fff.jpeg",
        "isPro": false,
        "fullname": "Jian Wang",
        "user": "jwanglvy",
        "type": "user"
      },
      "summary": "Recent advances in world models have shown promise for modeling future dynamics of environmental states, enabling agents to reason and act without accessing real environments. Current methods mainly perform single-step or fixed-horizon rollouts, leaving their potential for complex task planning under-exploited. We propose Imagine-then-Plan (ITP), a unified framework for agent learning via lookahead imagination, where an agent's policy model interacts with the learned world model, yielding multi-step ``imagined'' trajectories. Since the imagination horizon may vary by tasks and stages, we introduce a novel adaptive lookahead mechanism by trading off the ultimate goal and task progress. The resulting imagined trajectories provide rich signals about future consequences, such as achieved progress and potential conflicts, which are fused with current observations, formulating a partially observable and imaginable Markov decision process to guide policy learning. We instantiate ITP with both training-free and reinforcement-trained variants. Extensive experiments across representative agent benchmarks demonstrate that ITP significantly outperforms competitive baselines. Further analyses validate that our adaptive lookahead largely enhances agents' reasoning capability, providing valuable insights into addressing broader, complex tasks.",
      "upvotes": 7,
      "discussionId": "696877d50ac10a06522f6ac4",
      "ai_summary": "Imagine-then-Plan framework enables agent learning through adaptive lookahead imagination, combining imagined trajectories with current observations to guide policy learning in complex task scenarios.",
      "ai_keywords": [
        "world models",
        "lookahead imagination",
        "multi-step trajectories",
        "adaptive lookahead mechanism",
        "partially observable Markov decision process",
        "policy learning",
        "reinforcement training",
        "agent benchmarks"
      ]
    },
    "publishedAt": "2026-01-13T14:49:58.000Z",
    "title": "Imagine-then-Plan: Agent Learning from Adaptive Lookahead with World Models",
    "summary": "Recent advances in world models have shown promise for modeling future dynamics of environmental states, enabling agents to reason and act without accessing real environments. Current methods mainly perform single-step or fixed-horizon rollouts, leaving their potential for complex task planning under-exploited. We propose Imagine-then-Plan (ITP), a unified framework for agent learning via lookahead imagination, where an agent's policy model interacts with the learned world model, yielding multi-step ``imagined'' trajectories. Since the imagination horizon may vary by tasks and stages, we introduce a novel adaptive lookahead mechanism by trading off the ultimate goal and task progress. The resulting imagined trajectories provide rich signals about future consequences, such as achieved progress and potential conflicts, which are fused with current observations, formulating a partially observable and imaginable Markov decision process to guide policy learning. We instantiate ITP with both training-free and reinforcement-trained variants. Extensive experiments across representative agent benchmarks demonstrate that ITP significantly outperforms competitive baselines. Further analyses validate that our adaptive lookahead largely enhances agents' reasoning capability, providing valuable insights into addressing broader, complex tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.08955.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a4117984a6a25c65bc2fff",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675936454785-63a4117984a6a25c65bc2fff.jpeg",
      "fullname": "Jian Wang",
      "name": "jwanglvy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.09465",
      "authors": [
        {
          "_id": "696867a90ac10a06522f6a82",
          "name": "Shuo Zhang",
          "hidden": false
        },
        {
          "_id": "696867a90ac10a06522f6a83",
          "name": "Chaofa Yuan",
          "hidden": false
        },
        {
          "_id": "696867a90ac10a06522f6a84",
          "name": "Ryan Guo",
          "hidden": false
        },
        {
          "_id": "696867a90ac10a06522f6a85",
          "name": "Xiaomin Yu",
          "hidden": false
        },
        {
          "_id": "696867a90ac10a06522f6a86",
          "name": "Rui Xu",
          "hidden": false
        },
        {
          "_id": "696867a90ac10a06522f6a87",
          "name": "Zhangquan Chen",
          "hidden": false
        },
        {
          "_id": "696867a90ac10a06522f6a88",
          "name": "Zinuo Li",
          "hidden": false
        },
        {
          "_id": "696867a90ac10a06522f6a89",
          "name": "Zhi Yang",
          "hidden": false
        },
        {
          "_id": "696867a90ac10a06522f6a8a",
          "name": "Shuhao Guan",
          "hidden": false
        },
        {
          "_id": "696867a90ac10a06522f6a8b",
          "name": "Zhenheng Tang",
          "hidden": false
        },
        {
          "_id": "696867a90ac10a06522f6a8c",
          "name": "Sen Hu",
          "hidden": false
        },
        {
          "_id": "696867a90ac10a06522f6a8d",
          "name": "Liwen Zhang",
          "hidden": false
        },
        {
          "_id": "696867a90ac10a06522f6a8e",
          "name": "Ronghao Chen",
          "hidden": false
        },
        {
          "_id": "696867a90ac10a06522f6a8f",
          "name": "Huacan Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-14T13:19:13.000Z",
      "submittedOnDailyAt": "2026-01-15T01:36:08.471Z",
      "title": "EvoFSM: Controllable Self-Evolution for Deep Research with Finite State Machines",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "While LLM-based agents have shown promise for deep research, most existing approaches rely on fixed workflows that struggle to adapt to real-world, open-ended queries. Recent work therefore explores self-evolution by allowing agents to rewrite their own code or prompts to improve problem-solving ability, but unconstrained optimization often triggers instability, hallucinations, and instruction drift. We propose EvoFSM, a structured self-evolving framework that achieves both adaptability and control by evolving an explicit Finite State Machine (FSM) instead of relying on free-form rewriting. EvoFSM decouples the optimization space into macroscopic Flow (state-transition logic) and microscopic Skill (state-specific behaviors), enabling targeted improvements under clear behavioral boundaries. Guided by a critic mechanism, EvoFSM refines the FSM through a small set of constrained operations, and further incorporates a self-evolving memory that distills successful trajectories as reusable priors and failure patterns as constraints for future queries. Extensive evaluations on five multi-hop QA benchmarks demonstrate the effectiveness of EvoFSM. In particular, EvoFSM reaches 58.0% accuracy on the DeepSearch benchmark. Additional results on interactive decision-making tasks further validate its generalization.",
      "upvotes": 6,
      "discussionId": "696867a90ac10a06522f6a90",
      "ai_summary": "EvoFSM is a structured self-evolving framework for LLM agents that uses finite state machines to improve adaptability while maintaining control through constrained optimization and memory mechanisms.",
      "ai_keywords": [
        "LLM-based agents",
        "self-evolution",
        "finite state machine",
        "workflow adaptation",
        "constrained optimization",
        "critic mechanism",
        "self-evolving memory",
        "multi-hop QA",
        "DeepSearch benchmark",
        "interactive decision-making"
      ]
    },
    "publishedAt": "2026-01-14T08:19:13.000Z",
    "title": "EvoFSM: Controllable Self-Evolution for Deep Research with Finite State Machines",
    "summary": "While LLM-based agents have shown promise for deep research, most existing approaches rely on fixed workflows that struggle to adapt to real-world, open-ended queries. Recent work therefore explores self-evolution by allowing agents to rewrite their own code or prompts to improve problem-solving ability, but unconstrained optimization often triggers instability, hallucinations, and instruction drift. We propose EvoFSM, a structured self-evolving framework that achieves both adaptability and control by evolving an explicit Finite State Machine (FSM) instead of relying on free-form rewriting. EvoFSM decouples the optimization space into macroscopic Flow (state-transition logic) and microscopic Skill (state-specific behaviors), enabling targeted improvements under clear behavioral boundaries. Guided by a critic mechanism, EvoFSM refines the FSM through a small set of constrained operations, and further incorporates a self-evolving memory that distills successful trajectories as reusable priors and failure patterns as constraints for future queries. Extensive evaluations on five multi-hop QA benchmarks demonstrate the effectiveness of EvoFSM. In particular, EvoFSM reaches 58.0% accuracy on the DeepSearch benchmark. Additional results on interactive decision-making tasks further validate its generalization.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09465.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 208,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.09012",
      "authors": [
        {
          "_id": "696869da0ac10a06522f6a98",
          "name": "Mara Finkelstein",
          "hidden": false
        },
        {
          "_id": "696869da0ac10a06522f6a99",
          "name": "Isaac Caswell",
          "hidden": false
        },
        {
          "_id": "696869da0ac10a06522f6a9a",
          "name": "Tobias Domhan",
          "hidden": false
        },
        {
          "_id": "696869da0ac10a06522f6a9b",
          "name": "Jan-Thorsten Peter",
          "hidden": false
        },
        {
          "_id": "696869da0ac10a06522f6a9c",
          "name": "Juraj Juraska",
          "hidden": false
        },
        {
          "_id": "696869da0ac10a06522f6a9d",
          "name": "Parker Riley",
          "hidden": false
        },
        {
          "_id": "696869da0ac10a06522f6a9e",
          "name": "Daniel Deutsch",
          "hidden": false
        },
        {
          "_id": "696869da0ac10a06522f6a9f",
          "name": "Cole Dilanni",
          "hidden": false
        },
        {
          "_id": "696869da0ac10a06522f6aa0",
          "name": "Colin Cherry",
          "hidden": false
        },
        {
          "_id": "696869da0ac10a06522f6aa1",
          "name": "Eleftheria Briakou",
          "hidden": false
        },
        {
          "_id": "696869da0ac10a06522f6aa2",
          "name": "Elizabeth Nielsen",
          "hidden": false
        },
        {
          "_id": "696869da0ac10a06522f6aa3",
          "name": "Jiaming Luo",
          "hidden": false
        },
        {
          "_id": "696869da0ac10a06522f6aa4",
          "name": "Kat Black",
          "hidden": false
        },
        {
          "_id": "696869da0ac10a06522f6aa5",
          "name": "Ryan Mullins",
          "hidden": false
        },
        {
          "_id": "696869da0ac10a06522f6aa6",
          "name": "Sweta Agrawal",
          "hidden": false
        },
        {
          "_id": "696869da0ac10a06522f6aa7",
          "name": "Wenda Xu",
          "hidden": false
        },
        {
          "_id": "696869da0ac10a06522f6aa8",
          "name": "Erin Kats",
          "hidden": false
        },
        {
          "_id": "696869da0ac10a06522f6aa9",
          "name": "Stephane Jaskiewicz",
          "hidden": false
        },
        {
          "_id": "696869da0ac10a06522f6aaa",
          "name": "Markus Freitag",
          "hidden": false
        },
        {
          "_id": "696869da0ac10a06522f6aab",
          "name": "David Vilar",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-13T22:23:24.000Z",
      "submittedOnDailyAt": "2026-01-15T01:45:34.692Z",
      "title": "TranslateGemma Technical Report",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We present TranslateGemma, a suite of open machine translation models based on the Gemma 3 foundation models. To enhance the inherent multilingual capabilities of Gemma 3 for the translation task, we employ a two-stage fine-tuning process. First, supervised fine-tuning is performed using a rich mixture of high-quality large-scale synthetic parallel data generated via state-of-the-art models and human-translated parallel data. This is followed by a reinforcement learning phase, where we optimize translation quality using an ensemble of reward models, including MetricX-QE and AutoMQM, targeting translation quality. We demonstrate the effectiveness of TranslateGemma with human evaluation on the WMT25 test set across 10 language pairs and with automatic evaluation on the WMT24++ benchmark across 55 language pairs. Automatic metrics show consistent and substantial gains over the baseline Gemma 3 models across all sizes. Notably, smaller TranslateGemma models often achieve performance comparable to larger baseline models, offering improved efficiency. We also show that TranslateGemma models retain strong multimodal capabilities, with enhanced performance on the Vistra image translation benchmark. The release of the open TranslateGemma models aims to provide the research community with powerful and adaptable tools for machine translation.",
      "upvotes": 4,
      "discussionId": "696869da0ac10a06522f6aac",
      "ai_summary": "TranslateGemma enhances Gemma 3's multilingual capabilities through two-stage fine-tuning with synthetic and human-translated data, achieving superior translation quality with improved efficiency.",
      "ai_keywords": [
        "machine translation",
        "Gemma 3",
        "two-stage fine-tuning",
        "supervised fine-tuning",
        "reinforcement learning",
        "reward models",
        "MetricX-QE",
        "AutoMQM",
        "WMT25",
        "WMT24++",
        "Vistra"
      ],
      "organization": {
        "_id": "5e6aca39878b8b2bf9806447",
        "name": "google",
        "fullname": "Google",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"
      }
    },
    "publishedAt": "2026-01-13T17:23:24.000Z",
    "title": "TranslateGemma Technical Report",
    "summary": "We present TranslateGemma, a suite of open machine translation models based on the Gemma 3 foundation models. To enhance the inherent multilingual capabilities of Gemma 3 for the translation task, we employ a two-stage fine-tuning process. First, supervised fine-tuning is performed using a rich mixture of high-quality large-scale synthetic parallel data generated via state-of-the-art models and human-translated parallel data. This is followed by a reinforcement learning phase, where we optimize translation quality using an ensemble of reward models, including MetricX-QE and AutoMQM, targeting translation quality. We demonstrate the effectiveness of TranslateGemma with human evaluation on the WMT25 test set across 10 language pairs and with automatic evaluation on the WMT24++ benchmark across 55 language pairs. Automatic metrics show consistent and substantial gains over the baseline Gemma 3 models across all sizes. Notably, smaller TranslateGemma models often achieve performance comparable to larger baseline models, offering improved efficiency. We also show that TranslateGemma models retain strong multimodal capabilities, with enhanced performance on the Vistra image translation benchmark. The release of the open TranslateGemma models aims to provide the research community with powerful and adaptable tools for machine translation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09012.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 208,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "5e6aca39878b8b2bf9806447",
      "name": "google",
      "fullname": "Google",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.09113",
      "authors": [
        {
          "_id": "696867330ac10a06522f6a71",
          "name": "Zixia Jia",
          "hidden": false
        },
        {
          "_id": "696867330ac10a06522f6a72",
          "name": "Jiaqi Li",
          "hidden": false
        },
        {
          "_id": "696867330ac10a06522f6a73",
          "name": "Yipeng Kang",
          "hidden": false
        },
        {
          "_id": "696867330ac10a06522f6a74",
          "name": "Yuxuan Wang",
          "hidden": false
        },
        {
          "_id": "696867330ac10a06522f6a75",
          "name": "Tong Wu",
          "hidden": false
        },
        {
          "_id": "696867330ac10a06522f6a76",
          "name": "Quansen Wang",
          "hidden": false
        },
        {
          "_id": "696867330ac10a06522f6a77",
          "name": "Xiaobo Wang",
          "hidden": false
        },
        {
          "_id": "696867330ac10a06522f6a78",
          "name": "Shuyi Zhang",
          "hidden": false
        },
        {
          "_id": "696867330ac10a06522f6a79",
          "name": "Junzhe Shen",
          "hidden": false
        },
        {
          "_id": "696867330ac10a06522f6a7a",
          "name": "Qing Li",
          "hidden": false
        },
        {
          "_id": "696867330ac10a06522f6a7b",
          "name": "Siyuan Qi",
          "hidden": false
        },
        {
          "_id": "696867330ac10a06522f6a7c",
          "name": "Yitao Liang",
          "hidden": false
        },
        {
          "_id": "696867330ac10a06522f6a7d",
          "name": "Di He",
          "hidden": false
        },
        {
          "_id": "696867330ac10a06522f6a7e",
          "name": "Zilong Zheng",
          "hidden": false
        },
        {
          "_id": "696867330ac10a06522f6a7f",
          "name": "Song-Chun Zhu",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-14T03:24:08.000Z",
      "submittedOnDailyAt": "2026-01-15T01:34:09.547Z",
      "title": "The AI Hippocampus: How Far are We From Human Memory?",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Memory plays a foundational role in augmenting the reasoning, adaptability, and contextual fidelity of modern Large Language Models and Multi-Modal LLMs. As these models transition from static predictors to interactive systems capable of continual learning and personalized inference, the incorporation of memory mechanisms has emerged as a central theme in their architectural and functional evolution. This survey presents a comprehensive and structured synthesis of memory in LLMs and MLLMs, organizing the literature into a cohesive taxonomy comprising implicit, explicit, and agentic memory paradigms. Specifically, the survey delineates three primary memory frameworks. Implicit memory refers to the knowledge embedded within the internal parameters of pre-trained transformers, encompassing their capacity for memorization, associative retrieval, and contextual reasoning. Recent work has explored methods to interpret, manipulate, and reconfigure this latent memory. Explicit memory involves external storage and retrieval components designed to augment model outputs with dynamic, queryable knowledge representations, such as textual corpora, dense vectors, and graph-based structures, thereby enabling scalable and updatable interaction with information sources. Agentic memory introduces persistent, temporally extended memory structures within autonomous agents, facilitating long-term planning, self-consistency, and collaborative behavior in multi-agent systems, with relevance to embodied and interactive AI. Extending beyond text, the survey examines the integration of memory within multi-modal settings, where coherence across vision, language, audio, and action modalities is essential. Key architectural advances, benchmark tasks, and open challenges are discussed, including issues related to memory capacity, alignment, factual consistency, and cross-system interoperability.",
      "upvotes": 3,
      "discussionId": "696867330ac10a06522f6a80",
      "ai_summary": "Memory mechanisms in large language models and multi-modal language models are categorized into implicit, explicit, and agentic paradigms, supporting enhanced reasoning, adaptability, and contextual fidelity through internal parameters, external knowledge storage, and persistent agent memory structures.",
      "ai_keywords": [
        "Large Language Models",
        "Multi-Modal LLMs",
        "memory mechanisms",
        "implicit memory",
        "explicit memory",
        "agentic memory",
        "transformers",
        "memorization",
        "associative retrieval",
        "contextual reasoning",
        "external storage",
        "dynamic knowledge representations",
        "persistent memory structures",
        "long-term planning",
        "self-consistency",
        "multi-agent systems",
        "embodied AI",
        "interactive AI",
        "memory capacity",
        "factual consistency",
        "cross-system interoperability"
      ]
    },
    "publishedAt": "2026-01-13T22:24:08.000Z",
    "title": "The AI Hippocampus: How Far are We From Human Memory?",
    "summary": "Memory plays a foundational role in augmenting the reasoning, adaptability, and contextual fidelity of modern Large Language Models and Multi-Modal LLMs. As these models transition from static predictors to interactive systems capable of continual learning and personalized inference, the incorporation of memory mechanisms has emerged as a central theme in their architectural and functional evolution. This survey presents a comprehensive and structured synthesis of memory in LLMs and MLLMs, organizing the literature into a cohesive taxonomy comprising implicit, explicit, and agentic memory paradigms. Specifically, the survey delineates three primary memory frameworks. Implicit memory refers to the knowledge embedded within the internal parameters of pre-trained transformers, encompassing their capacity for memorization, associative retrieval, and contextual reasoning. Recent work has explored methods to interpret, manipulate, and reconfigure this latent memory. Explicit memory involves external storage and retrieval components designed to augment model outputs with dynamic, queryable knowledge representations, such as textual corpora, dense vectors, and graph-based structures, thereby enabling scalable and updatable interaction with information sources. Agentic memory introduces persistent, temporally extended memory structures within autonomous agents, facilitating long-term planning, self-consistency, and collaborative behavior in multi-agent systems, with relevance to embodied and interactive AI. Extending beyond text, the survey examines the integration of memory within multi-modal settings, where coherence across vision, language, audio, and action modalities is essential. Key architectural advances, benchmark tasks, and open challenges are discussed, including issues related to memory capacity, alignment, factual consistency, and cross-system interoperability.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09113.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 208,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.09697",
      "authors": [
        {
          "_id": "6968654c0ac10a06522f6a56",
          "name": "Jieying Chen",
          "hidden": false
        },
        {
          "_id": "6968654c0ac10a06522f6a57",
          "name": "Jeffrey Hu",
          "hidden": false
        },
        {
          "_id": "6968654c0ac10a06522f6a58",
          "name": "Joan Lasenby",
          "hidden": false
        },
        {
          "_id": "6968654c0ac10a06522f6a59",
          "name": "Ayush Tewari",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/eOKN4AvNtQiQsBxP-sroP.mp4"
      ],
      "publishedAt": "2026-01-14T18:50:06.000Z",
      "submittedOnDailyAt": "2026-01-15T01:26:04.275Z",
      "title": "Efficient Camera-Controlled Video Generation of Static Scenes via Sparse Diffusion and 3D Rendering",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Modern video generative models based on diffusion models can produce very realistic clips, but they are computationally inefficient, often requiring minutes of GPU time for just a few seconds of video. This inefficiency poses a critical barrier to deploying generative video in applications that require real-time interactions, such as embodied AI and VR/AR. This paper explores a new strategy for camera-conditioned video generation of static scenes: using diffusion-based generative models to generate a sparse set of keyframes, and then synthesizing the full video through 3D reconstruction and rendering. By lifting keyframes into a 3D representation and rendering intermediate views, our approach amortizes the generation cost across hundreds of frames while enforcing geometric consistency. We further introduce a model that predicts the optimal number of keyframes for a given camera trajectory, allowing the system to adaptively allocate computation. Our final method, SRENDER, uses very sparse keyframes for simple trajectories and denser ones for complex camera motion. This results in video generation that is more than 40 times faster than the diffusion-based baseline in generating 20 seconds of video, while maintaining high visual fidelity and temporal stability, offering a practical path toward efficient and controllable video synthesis.",
      "upvotes": 2,
      "discussionId": "6968654c0ac10a06522f6a5a",
      "ai_summary": "Diffusion-based video generation is made more efficient through keyframe-based 3D reconstruction and rendering, enabling faster synthesis with maintained visual quality.",
      "ai_keywords": [
        "diffusion models",
        "video generation",
        "keyframes",
        "3D reconstruction",
        "3D rendering",
        "camera trajectory",
        "geometric consistency",
        "temporal stability"
      ]
    },
    "publishedAt": "2026-01-14T13:50:06.000Z",
    "title": "Efficient Camera-Controlled Video Generation of Static Scenes via Sparse Diffusion and 3D Rendering",
    "summary": "Modern video generative models based on diffusion models can produce very realistic clips, but they are computationally inefficient, often requiring minutes of GPU time for just a few seconds of video. This inefficiency poses a critical barrier to deploying generative video in applications that require real-time interactions, such as embodied AI and VR/AR. This paper explores a new strategy for camera-conditioned video generation of static scenes: using diffusion-based generative models to generate a sparse set of keyframes, and then synthesizing the full video through 3D reconstruction and rendering. By lifting keyframes into a 3D representation and rendering intermediate views, our approach amortizes the generation cost across hundreds of frames while enforcing geometric consistency. We further introduce a model that predicts the optimal number of keyframes for a given camera trajectory, allowing the system to adaptively allocate computation. Our final method, SRENDER, uses very sparse keyframes for simple trajectories and denser ones for complex camera motion. This results in video generation that is more than 40 times faster than the diffusion-based baseline in generating 20 seconds of video, while maintaining high visual fidelity and temporal stability, offering a practical path toward efficient and controllable video synthesis.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/eOKN4AvNtQiQsBxP-sroP.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09697.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 208,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.03928",
      "authors": [
        {
          "_id": "69681d640ac10a06522f6988",
          "name": "Mingyu Ouyang",
          "hidden": false
        },
        {
          "_id": "69681d640ac10a06522f6989",
          "name": "Kevin Qinghong Lin",
          "hidden": false
        },
        {
          "_id": "69681d640ac10a06522f698a",
          "name": "Mike Zheng Shou",
          "hidden": false
        },
        {
          "_id": "69681d640ac10a06522f698b",
          "name": "Hwee Tou Ng",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/634e2217c1ce28f1de921708/AWCt2drJyoFbFk0FEwlwd.mp4"
      ],
      "publishedAt": "2026-01-07T13:48:12.000Z",
      "submittedOnDailyAt": "2026-01-15T01:14:34.735Z",
      "title": "FocusUI: Efficient UI Grounding via Position-Preserving Visual Token Selection",
      "submittedOnDailyBy": {
        "_id": "634e2217c1ce28f1de921708",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634e2217c1ce28f1de921708/XTMB6alYUM0KAUptM98kP.jpeg",
        "isPro": false,
        "fullname": "Yang",
        "user": "yyyang",
        "type": "user"
      },
      "summary": "Vision-Language Models (VLMs) have shown remarkable performance in User Interface (UI) grounding tasks, driven by their ability to process increasingly high-resolution screenshots. However, screenshots are tokenized into thousands of visual tokens (e.g., about 4700 for 2K resolution), incurring significant computational overhead and diluting attention. In contrast, humans typically focus on regions of interest when interacting with UI. In this work, we pioneer the task of efficient UI grounding. Guided by practical analysis of the task's characteristics and challenges, we propose FocusUI, an efficient UI grounding framework that selects patches most relevant to the instruction while preserving positional continuity for precise grounding. FocusUI addresses two key challenges: (1) Eliminating redundant tokens in visual encoding. We construct patch-level supervision by fusing an instruction-conditioned score with a rule-based UI-graph score that down-weights large homogeneous regions to select distinct and instruction-relevant visual tokens. (2) Preserving positional continuity during visual token selection. We find that general visual token pruning methods suffer from severe accuracy degradation on UI grounding tasks due to broken positional information. We introduce a novel PosPad strategy, which compresses each contiguous sequence of dropped visual tokens into a single special marker placed at the sequence's last index to preserve positional continuity. Comprehensive experiments on four grounding benchmarks demonstrate that FocusUI surpasses GUI-specific baselines. On the ScreenSpot-Pro benchmark, FocusUI-7B achieves a performance improvement of 3.7% over GUI-Actor-7B. Even with only 30% visual token retention, FocusUI-7B drops by only 3.2% while achieving up to 1.44x faster inference and 17% lower peak GPU memory.",
      "upvotes": 2,
      "discussionId": "69681d640ac10a06522f698c",
      "ai_summary": "FocusUI is an efficient UI grounding framework that reduces computational overhead by selecting relevant visual tokens while preserving positional continuity through a novel PosPad strategy.",
      "ai_keywords": [
        "Vision-Language Models",
        "UI grounding",
        "visual tokens",
        "patch-level supervision",
        "instruction-conditioned score",
        "UI-graph score",
        "visual token pruning",
        "positional continuity",
        "PosPad strategy",
        "ScreenSpot-Pro benchmark"
      ],
      "organization": {
        "_id": "63a553c4ce5763e06f78669c",
        "name": "showlab",
        "fullname": "Show Lab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1671779505215-63a55320ce5763e06f78519c.png"
      }
    },
    "publishedAt": "2026-01-07T08:48:12.000Z",
    "title": "FocusUI: Efficient UI Grounding via Position-Preserving Visual Token Selection",
    "summary": "Vision-Language Models (VLMs) have shown remarkable performance in User Interface (UI) grounding tasks, driven by their ability to process increasingly high-resolution screenshots. However, screenshots are tokenized into thousands of visual tokens (e.g., about 4700 for 2K resolution), incurring significant computational overhead and diluting attention. In contrast, humans typically focus on regions of interest when interacting with UI. In this work, we pioneer the task of efficient UI grounding. Guided by practical analysis of the task's characteristics and challenges, we propose FocusUI, an efficient UI grounding framework that selects patches most relevant to the instruction while preserving positional continuity for precise grounding. FocusUI addresses two key challenges: (1) Eliminating redundant tokens in visual encoding. We construct patch-level supervision by fusing an instruction-conditioned score with a rule-based UI-graph score that down-weights large homogeneous regions to select distinct and instruction-relevant visual tokens. (2) Preserving positional continuity during visual token selection. We find that general visual token pruning methods suffer from severe accuracy degradation on UI grounding tasks due to broken positional information. We introduce a novel PosPad strategy, which compresses each contiguous sequence of dropped visual tokens into a single special marker placed at the sequence's last index to preserve positional continuity. Comprehensive experiments on four grounding benchmarks demonstrate that FocusUI surpasses GUI-specific baselines. On the ScreenSpot-Pro benchmark, FocusUI-7B achieves a performance improvement of 3.7% over GUI-Actor-7B. Even with only 30% visual token retention, FocusUI-7B drops by only 3.2% while achieving up to 1.44x faster inference and 17% lower peak GPU memory.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/634e2217c1ce28f1de921708/AWCt2drJyoFbFk0FEwlwd.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03928.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "634e2217c1ce28f1de921708",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634e2217c1ce28f1de921708/XTMB6alYUM0KAUptM98kP.jpeg",
      "fullname": "Yang",
      "name": "yyyang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "63a553c4ce5763e06f78669c",
      "name": "showlab",
      "fullname": "Show Lab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1671779505215-63a55320ce5763e06f78519c.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.07287",
      "authors": [
        {
          "_id": "69671bdac5e371f6b235d1c4",
          "name": "Yuanyang Yin",
          "hidden": false
        },
        {
          "_id": "69671bdac5e371f6b235d1c5",
          "user": {
            "_id": "68fce03ed1d0efce7ca87075",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68fce03ed1d0efce7ca87075/GRKTeVIaLZD_M-KoJE8YF.png",
            "isPro": false,
            "fullname": "yfdeng",
            "user": "yfdeng10",
            "type": "user"
          },
          "name": "Yufan Deng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-14T09:51:08.659Z",
          "hidden": false
        },
        {
          "_id": "69671bdac5e371f6b235d1c6",
          "name": "Shenghai Yuan",
          "hidden": false
        },
        {
          "_id": "69671bdac5e371f6b235d1c7",
          "name": "Kaipeng Zhang",
          "hidden": false
        },
        {
          "_id": "69671bdac5e371f6b235d1c8",
          "name": "Xiao Yang",
          "hidden": false
        },
        {
          "_id": "69671bdac5e371f6b235d1c9",
          "name": "Feng Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-12T07:48:26.000Z",
      "submittedOnDailyAt": "2026-01-15T03:35:54.897Z",
      "title": "Focal Guidance: Unlocking Controllability from Semantic-Weak Layers in Video Diffusion Models",
      "submittedOnDailyBy": {
        "_id": "68fce03ed1d0efce7ca87075",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68fce03ed1d0efce7ca87075/GRKTeVIaLZD_M-KoJE8YF.png",
        "isPro": false,
        "fullname": "yfdeng",
        "user": "yfdeng10",
        "type": "user"
      },
      "summary": "The task of Image-to-Video (I2V) generation aims to synthesize a video from a reference image and a text prompt. This requires diffusion models to reconcile high-frequency visual constraints and low-frequency textual guidance during the denoising process. However, while existing I2V models prioritize visual consistency, how to effectively couple this dual guidance to ensure strong adherence to the text prompt remains underexplored. In this work, we observe that in Diffusion Transformer (DiT)-based I2V models, certain intermediate layers exhibit weak semantic responses (termed Semantic-Weak Layers), as indicated by a measurable drop in text-visual similarity. We attribute this to a phenomenon called Condition Isolation, where attention to visual features becomes partially detached from text guidance and overly relies on learned visual priors. To address this, we propose Focal Guidance (FG), which enhances the controllability from Semantic-Weak Layers. FG comprises two mechanisms: (1) Fine-grained Semantic Guidance (FSG) leverages CLIP to identify key regions in the reference frame and uses them as anchors to guide Semantic-Weak Layers. (2) Attention Cache transfers attention maps from semantically responsive layers to Semantic-Weak Layers, injecting explicit semantic signals and alleviating their over-reliance on the model's learned visual priors, thereby enhancing adherence to textual instructions. To further validate our approach and address the lack of evaluation in this direction, we introduce a benchmark for assessing instruction following in I2V models. On this benchmark, Focal Guidance proves its effectiveness and generalizability, raising the total score on Wan2.1-I2V to 0.7250 (+3.97\\%) and boosting the MMDiT-based HunyuanVideo-I2V to 0.5571 (+7.44\\%).",
      "upvotes": 1,
      "discussionId": "69671bdac5e371f6b235d1ca",
      "ai_summary": "Diffusion Transformer-based image-to-video models suffer from condition isolation where visual attention becomes detached from text guidance; focal guidance addresses this through fine-grained semantic guidance and attention cache mechanisms.",
      "ai_keywords": [
        "diffusion models",
        "diffusion transformer",
        "image-to-video generation",
        "text prompt",
        "denoising process",
        "semantic responses",
        "condition isolation",
        "attention mechanisms",
        "clip",
        "fine-grained semantic guidance",
        "attention cache"
      ]
    },
    "publishedAt": "2026-01-12T02:48:26.000Z",
    "title": "Focal Guidance: Unlocking Controllability from Semantic-Weak Layers in Video Diffusion Models",
    "summary": "The task of Image-to-Video (I2V) generation aims to synthesize a video from a reference image and a text prompt. This requires diffusion models to reconcile high-frequency visual constraints and low-frequency textual guidance during the denoising process. However, while existing I2V models prioritize visual consistency, how to effectively couple this dual guidance to ensure strong adherence to the text prompt remains underexplored. In this work, we observe that in Diffusion Transformer (DiT)-based I2V models, certain intermediate layers exhibit weak semantic responses (termed Semantic-Weak Layers), as indicated by a measurable drop in text-visual similarity. We attribute this to a phenomenon called Condition Isolation, where attention to visual features becomes partially detached from text guidance and overly relies on learned visual priors. To address this, we propose Focal Guidance (FG), which enhances the controllability from Semantic-Weak Layers. FG comprises two mechanisms: (1) Fine-grained Semantic Guidance (FSG) leverages CLIP to identify key regions in the reference frame and uses them as anchors to guide Semantic-Weak Layers. (2) Attention Cache transfers attention maps from semantically responsive layers to Semantic-Weak Layers, injecting explicit semantic signals and alleviating their over-reliance on the model's learned visual priors, thereby enhancing adherence to textual instructions. To further validate our approach and address the lack of evaluation in this direction, we introduce a benchmark for assessing instruction following in I2V models. On this benchmark, Focal Guidance proves its effectiveness and generalizability, raising the total score on Wan2.1-I2V to 0.7250 (+3.97\\%) and boosting the MMDiT-based HunyuanVideo-I2V to 0.5571 (+7.44\\%).",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.07287.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "68fce03ed1d0efce7ca87075",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68fce03ed1d0efce7ca87075/GRKTeVIaLZD_M-KoJE8YF.png",
      "fullname": "yfdeng",
      "name": "yfdeng10",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "isAuthorParticipating": true
  }
]