[
  {
    "paper": {
      "id": "2502.07870",
      "authors": [
        {
          "_id": "67ad79cb60ec3f444b21cbcb",
          "name": "Alex Jinpeng Wang",
          "hidden": false
        },
        {
          "_id": "67ad79cb60ec3f444b21cbcc",
          "name": "Dongxing Mao",
          "hidden": false
        },
        {
          "_id": "67ad79cb60ec3f444b21cbcd",
          "name": "Jiawei Zhang",
          "hidden": false
        },
        {
          "_id": "67ad79cb60ec3f444b21cbce",
          "name": "Weiming Han",
          "hidden": false
        },
        {
          "_id": "67ad79cb60ec3f444b21cbcf",
          "name": "Zhuobai Dong",
          "hidden": false
        },
        {
          "_id": "67ad79cb60ec3f444b21cbd0",
          "name": "Linjie Li",
          "hidden": false
        },
        {
          "_id": "67ad79cb60ec3f444b21cbd1",
          "name": "Yiqi Lin",
          "hidden": false
        },
        {
          "_id": "67ad79cb60ec3f444b21cbd2",
          "name": "Zhengyuan Yang",
          "hidden": false
        },
        {
          "_id": "67ad79cb60ec3f444b21cbd3",
          "name": "Libo Qin",
          "hidden": false
        },
        {
          "_id": "67ad79cb60ec3f444b21cbd4",
          "name": "Fuwei Zhang",
          "hidden": false
        },
        {
          "_id": "67ad79cb60ec3f444b21cbd5",
          "name": "Lijuan Wang",
          "hidden": false
        },
        {
          "_id": "67ad79cb60ec3f444b21cbd6",
          "name": "Min Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T18:59:19.000Z",
      "title": "TextAtlas5M: A Large-scale Dataset for Dense Text Image Generation",
      "summary": "Text-conditioned image generation has gained significant attention in recent\nyears and are processing increasingly longer and comprehensive text prompt. In\neveryday life, dense and intricate text appears in contexts like\nadvertisements, infographics, and signage, where the integration of both text\nand visuals is essential for conveying complex information. However, despite\nthese advances, the generation of images containing long-form text remains a\npersistent challenge, largely due to the limitations of existing datasets,\nwhich often focus on shorter and simpler text. To address this gap, we\nintroduce TextAtlas5M, a novel dataset specifically designed to evaluate\nlong-text rendering in text-conditioned image generation. Our dataset consists\nof 5 million long-text generated and collected images across diverse data\ntypes, enabling comprehensive evaluation of large-scale generative models on\nlong-text image generation. We further curate 3000 human-improved test set\nTextAtlasEval across 3 data domains, establishing one of the most extensive\nbenchmarks for text-conditioned generation. Evaluations suggest that the\nTextAtlasEval benchmarks present significant challenges even for the most\nadvanced proprietary models (e.g. GPT4o with DallE-3), while their open-source\ncounterparts show an even larger performance gap. These evidences position\nTextAtlas5M as a valuable dataset for training and evaluating future-generation\ntext-conditioned image generation models.",
      "upvotes": 22,
      "discussionId": "67ad79d260ec3f444b21cd1f"
    },
    "publishedAt": "2025-02-12T23:50:07.130Z",
    "title": "TextAtlas5M: A Large-scale Dataset for Dense Text Image Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07870.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62333a88fd7bb4a39b92d387",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62333a88fd7bb4a39b92d387/e21AhpcXq37Ak_7rZ-Ca9.png",
      "fullname": "Alex Jinpeng Wang",
      "name": "Awiny",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.08590",
      "authors": [
        {
          "_id": "67ad79552fdac6537b43f120",
          "name": "Yujie Zhou",
          "hidden": false
        },
        {
          "_id": "67ad79552fdac6537b43f121",
          "name": "Jiazi Bu",
          "hidden": false
        },
        {
          "_id": "67ad79552fdac6537b43f122",
          "name": "Pengyang Ling",
          "hidden": false
        },
        {
          "_id": "67ad79552fdac6537b43f123",
          "name": "Pan Zhang",
          "hidden": false
        },
        {
          "_id": "67ad79552fdac6537b43f124",
          "name": "Tong Wu",
          "hidden": false
        },
        {
          "_id": "67ad79552fdac6537b43f125",
          "name": "Qidong Huang",
          "hidden": false
        },
        {
          "_id": "67ad79552fdac6537b43f126",
          "name": "Jinsong Li",
          "hidden": false
        },
        {
          "_id": "67ad79552fdac6537b43f127",
          "name": "Xiaoyi Dong",
          "hidden": false
        },
        {
          "_id": "67ad79552fdac6537b43f128",
          "name": "Yuhang Zang",
          "hidden": false
        },
        {
          "_id": "67ad79552fdac6537b43f129",
          "name": "Yuhang Cao",
          "hidden": false
        },
        {
          "_id": "67ad79552fdac6537b43f12a",
          "name": "Anyi Rao",
          "hidden": false
        },
        {
          "_id": "67ad79552fdac6537b43f12b",
          "name": "Jiaqi Wang",
          "hidden": false
        },
        {
          "_id": "67ad79552fdac6537b43f12c",
          "name": "Li Niu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-12T17:24:19.000Z",
      "title": "Light-A-Video: Training-free Video Relighting via Progressive Light\n  Fusion",
      "summary": "Recent advancements in image relighting models, driven by large-scale\ndatasets and pre-trained diffusion models, have enabled the imposition of\nconsistent lighting. However, video relighting still lags, primarily due to the\nexcessive training costs and the scarcity of diverse, high-quality video\nrelighting datasets. A simple application of image relighting models on a\nframe-by-frame basis leads to several issues: lighting source inconsistency and\nrelighted appearance inconsistency, resulting in flickers in the generated\nvideos. In this work, we propose Light-A-Video, a training-free approach to\nachieve temporally smooth video relighting. Adapted from image relighting\nmodels, Light-A-Video introduces two key techniques to enhance lighting\nconsistency. First, we design a Consistent Light Attention (CLA) module, which\nenhances cross-frame interactions within the self-attention layers to stabilize\nthe generation of the background lighting source. Second, leveraging the\nphysical principle of light transport independence, we apply linear blending\nbetween the source video's appearance and the relighted appearance, using a\nProgressive Light Fusion (PLF) strategy to ensure smooth temporal transitions\nin illumination. Experiments show that Light-A-Video improves the temporal\nconsistency of relighted video while maintaining the image quality, ensuring\ncoherent lighting transitions across frames. Project page:\nhttps://bujiazi.github.io/light-a-video.github.io/.",
      "upvotes": 22,
      "discussionId": "67ad79572fdac6537b43f189"
    },
    "publishedAt": "2025-02-12T23:47:56.223Z",
    "title": "Light-A-Video: Training-free Video Relighting via Progressive Light Fusion",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08590.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b4eec4faa3181a5eab9c46",
      "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
      "fullname": "Jiaqi Wang",
      "name": "myownskyW7",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 16
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.08639",
      "authors": [
        {
          "_id": "67ad5f25cad644864b436186",
          "name": "Qinghe Wang",
          "hidden": false
        },
        {
          "_id": "67ad5f25cad644864b436187",
          "name": "Yawen Luo",
          "hidden": false
        },
        {
          "_id": "67ad5f25cad644864b436188",
          "name": "Xiaoyu Shi",
          "hidden": false
        },
        {
          "_id": "67ad5f25cad644864b436189",
          "name": "Xu Jia",
          "hidden": false
        },
        {
          "_id": "67ad5f25cad644864b43618a",
          "name": "Huchuan Lu",
          "hidden": false
        },
        {
          "_id": "67ad5f25cad644864b43618b",
          "name": "Tianfan Xue",
          "hidden": false
        },
        {
          "_id": "67ad5f25cad644864b43618c",
          "name": "Xintao Wang",
          "hidden": false
        },
        {
          "_id": "67ad5f25cad644864b43618d",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "67ad5f25cad644864b43618e",
          "name": "Di Zhang",
          "hidden": false
        },
        {
          "_id": "67ad5f25cad644864b43618f",
          "name": "Kun Gai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-12T18:55:36.000Z",
      "title": "CineMaster: A 3D-Aware and Controllable Framework for Cinematic\n  Text-to-Video Generation",
      "summary": "In this work, we present CineMaster, a novel framework for 3D-aware and\ncontrollable text-to-video generation. Our goal is to empower users with\ncomparable controllability as professional film directors: precise placement of\nobjects within the scene, flexible manipulation of both objects and camera in\n3D space, and intuitive layout control over the rendered frames. To achieve\nthis, CineMaster operates in two stages. In the first stage, we design an\ninteractive workflow that allows users to intuitively construct 3D-aware\nconditional signals by positioning object bounding boxes and defining camera\nmovements within the 3D space. In the second stage, these control\nsignals--comprising rendered depth maps, camera trajectories and object class\nlabels--serve as the guidance for a text-to-video diffusion model, ensuring to\ngenerate the user-intended video content. Furthermore, to overcome the scarcity\nof in-the-wild datasets with 3D object motion and camera pose annotations, we\ncarefully establish an automated data annotation pipeline that extracts 3D\nbounding boxes and camera trajectories from large-scale video data. Extensive\nqualitative and quantitative experiments demonstrate that CineMaster\nsignificantly outperforms existing methods and implements prominent 3D-aware\ntext-to-video generation. Project page: https://cinemaster-dev.github.io/.",
      "upvotes": 20,
      "discussionId": "67ad5f26cad644864b4361cf"
    },
    "publishedAt": "2025-02-12T21:55:44.479Z",
    "title": "CineMaster: A 3D-Aware and Controllable Framework for Cinematic Text-to-Video Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08639.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6061
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.08047",
      "authors": [
        {
          "_id": "67ad92bfbbf3810ab20595c2",
          "name": "Henry Hengyuan Zhao",
          "hidden": false
        },
        {
          "_id": "67ad92bfbbf3810ab20595c3",
          "name": "Difei Gao",
          "hidden": false
        },
        {
          "_id": "67ad92bfbbf3810ab20595c4",
          "name": "Mike Zheng Shou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-12T01:06:10.000Z",
      "title": "WorldGUI: Dynamic Testing for Comprehensive Desktop GUI Automation",
      "summary": "Current GUI agents have achieved outstanding performance in GUI element\ngrounding. However, planning remains highly challenging, especially due to\nsensitivity to the initial state of the environment. Specifically, slight\ndifferences in the initial state-such as the target software not being open or\nthe interface not being in its default state-often lead to planning errors.\nThis issue is widespread in real user scenarios, but existing benchmarks fail\nto evaluate it. In this paper, we present WorldGUI, a novel GUI benchmark that\ndesigns GUI tasks with various initial states to simulate real computer-user\ninteractions. The benchmark spans a wide range of tasks across 10 popular\nsoftware applications, including PowerPoint, VSCode, and Adobe Acrobat. In\naddition, to address the challenges of dynamic GUI automation tasks, we propose\nGUI-Thinker, a holistic framework, leveraging a critique mechanism, that\neffectively manages the unpredictability and complexity of GUI interactions.\nExperimental results demonstrate that GUI-Thinker significantly outperforms\nClaude-3.5 (Computer Use) by 14.9% in success rate on WorldGUI tasks. This\nimprovement underscores the effectiveness of our critical-thinking-based\nframework in enhancing GUI automation.",
      "upvotes": 16,
      "discussionId": "67ad92c1bbf3810ab205961c"
    },
    "publishedAt": "2025-02-13T01:39:08.775Z",
    "title": "WorldGUI: Dynamic Testing for Comprehensive Desktop GUI Automation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08047.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "647d7eb9770c299e56f5b39b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647d7eb9770c299e56f5b39b/CC5JJgkyLkXOxw-BeT4G5.jpeg",
      "fullname": "Hengyuan Zhao",
      "name": "hhenryz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.07563",
      "authors": [
        {
          "_id": "67ad7929dc2968691c241147",
          "name": "Weigao Sun",
          "hidden": false
        },
        {
          "_id": "67ad7929dc2968691c241148",
          "name": "Disen Lan",
          "hidden": false
        },
        {
          "_id": "67ad7929dc2968691c241149",
          "name": "Yiran Zhong",
          "hidden": false
        },
        {
          "_id": "67ad7929dc2968691c24114a",
          "name": "Xiaoye Qu",
          "hidden": false
        },
        {
          "_id": "67ad7929dc2968691c24114b",
          "name": "Yu Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T14:01:39.000Z",
      "title": "LASP-2: Rethinking Sequence Parallelism for Linear Attention and Its\n  Hybrid",
      "summary": "Linear sequence modeling approaches, such as linear attention, provide\nadvantages like linear-time training and constant-memory inference over\nsequence lengths. However, existing sequence parallelism (SP) methods are\neither not optimized for the right-product-first feature of linear attention or\nuse a ring-style communication strategy, which results in lower computation\nparallelism, limits their scalability for longer sequences in distributed\nsystems. In this paper, we introduce LASP-2, a new SP method to enhance both\ncommunication and computation parallelism when training linear attention\ntransformer models with very-long input sequences. Compared to previous work\nLASP, LASP-2 rethinks the minimal communication requirement for SP on linear\nattention layers, reorganizes the whole communication-computation workflow of\nLASP. In this way, only one single AllGather collective communication is needed\non intermediate memory states, whose sizes are independent of the sequence\nlength, leading to significant improvements of both communication and\ncomputation parallelism, as well as their overlap. Additionally, we extend\nLASP-2 to LASP-2H by applying similar communication redesign to standard\nattention modules, offering an efficient SP solution for hybrid models that\nblend linear and standard attention layers. Our evaluation on a Linear-Llama3\nmodel, a variant of Llama3 with linear attention replacing standard attention,\ndemonstrates the effectiveness of LASP-2 and LASP-2H. Specifically, LASP-2\nachieves training speed improvements of 15.2% over LASP and 36.6% over Ring\nAttention, with a sequence length of 2048K across 64 GPUs. The Code is released\nas a part of: https://github.com/OpenSparseLLMs/Linear-MoE.",
      "upvotes": 16,
      "discussionId": "67ad792adc2968691c241173"
    },
    "publishedAt": "2025-02-12T23:47:31.651Z",
    "title": "LASP-2: Rethinking Sequence Parallelism for Linear Attention and Its Hybrid",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07563.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6246bb33da617c00b48e4d92",
      "avatarUrl": "/avatars/0304a9f6eb7f5dee4d933d03222f94e9.svg",
      "fullname": "Weigao Sun",
      "name": "weigao266",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.08127",
      "authors": [
        {
          "_id": "67ad5ca29109885ce9b859e4",
          "name": "Lingfei Qian",
          "hidden": false
        },
        {
          "_id": "67ad5ca29109885ce9b859e5",
          "name": "Weipeng Zhou",
          "hidden": false
        },
        {
          "_id": "67ad5ca29109885ce9b859e6",
          "name": "Yan Wang",
          "hidden": false
        },
        {
          "_id": "67ad5ca29109885ce9b859e7",
          "name": "Xueqing Peng",
          "hidden": false
        },
        {
          "_id": "67ad5ca29109885ce9b859e8",
          "user": {
            "_id": "63b58ed5889aa6707f0bb0f4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b58ed5889aa6707f0bb0f4/9-6SJBOLdqUoc2LrKsI6y.jpeg",
            "isPro": true,
            "fullname": "Jimin Huang",
            "user": "jiminHuang",
            "type": "user"
          },
          "name": "Jimin Huang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-13T02:44:52.979Z",
          "hidden": false
        },
        {
          "_id": "67ad5ca29109885ce9b859e9",
          "name": "Qianqian Xie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-12T05:13:04.000Z",
      "title": "Fino1: On the Transferability of Reasoning Enhanced LLMs to Finance",
      "summary": "Recent advancements in large language models (LLMs) have shown strong general\nreasoning abilities, yet their effectiveness in financial reasoning remains\nunderexplored. In this study, we comprehensively evaluate 16 powerful reasoning\nand general LLMs on three complex financial tasks involving financial text,\ntabular data, and equations, assessing numerical reasoning, tabular\ninterpretation, financial terminology comprehension, long-context processing,\nand equation-based problem solving. Our results show that while better datasets\nand pretraining improve financial reasoning, general enhancements like CoT\nfine-tuning do not always yield consistent gains. Moreover, all reasoning\nstrategies face challenges in improving performance on long-context and\nmulti-table tasks. To address these limitations, we develop a financial\nreasoning-enhanced model based on Llama-3.1-8B-Instruct, by CoT fine-tuning and\nreinforcement learning with domain-specific reasoning paths. Even with simple\nfine-tuning with one financial dataset, our model achieves a consistent 10%\nperformance improvement across tasks, surpassing all 8B models and even\nLlama3-70B-Instruct and Llama3.1-70B-Instruct on average. Our results highlight\nthe need for domain-specific adaptations in financial tasks, emphasizing future\ndirections such as multi-table reasoning, long-context processing, and\nfinancial terminology comprehension. All our datasets, models, and codes are\npublicly available. Furthermore, we introduce a leaderboard for benchmarking\nfuture datasets and models.",
      "upvotes": 13,
      "discussionId": "67ad5ca59109885ce9b85a5b"
    },
    "publishedAt": "2025-02-12T21:45:28.944Z",
    "title": "Fino1: On the Transferability of Reasoning Enhanced LLMs to Finance",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08127.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63b58ed5889aa6707f0bb0f4",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b58ed5889aa6707f0bb0f4/9-6SJBOLdqUoc2LrKsI6y.jpeg",
      "fullname": "Jimin Huang",
      "name": "jiminHuang",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 13
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.07864",
      "authors": [
        {
          "_id": "67ad5b3a007d78b391946a57",
          "name": "Fanxu Meng",
          "hidden": false
        },
        {
          "_id": "67ad5b3a007d78b391946a58",
          "name": "Zengwei Yao",
          "hidden": false
        },
        {
          "_id": "67ad5b3a007d78b391946a59",
          "name": "Muhan Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T18:20:18.000Z",
      "title": "TransMLA: Multi-head Latent Attention Is All You Need",
      "summary": "Modern large language models (LLMs) often encounter communication bottlenecks\non current hardware, rather than purely computational constraints. Multi-head\nLatent Attention (MLA) tackles this challenge by using low-rank matrices in the\nkey-value (KV) layers, thereby allowing compressed latent KV states to be\ncached. This approach significantly reduces the KV cache size relative to\ntraditional multi-head attention, leading to faster inference. Moreover, MLA\nemploys an up-projection matrix to increase expressiveness, trading additional\ncomputation for reduced communication overhead. Although MLA has demonstrated\nefficiency and effectiveness in Deepseek V2/V3/R1, many major model providers\nstill rely on Group Query Attention (GQA) and have not announced any plans to\nadopt MLA. In this paper, we show that GQA can always be represented by MLA\nwhile maintaining the same KV cache overhead, but the converse does not hold.\nTo encourage broader use of MLA, we introduce **TransMLA**, a post-training\nmethod that converts widely used GQA-based pre-trained models (e.g., LLaMA,\nQwen, Mixtral) into MLA-based models. After conversion, the model can undergo\nadditional training to boost expressiveness without increasing the KV cache\nsize. Furthermore, we plan to develop MLA-specific inference acceleration\ntechniques to preserve low latency in transformed models, thus enabling more\nefficient distillation of Deepseek R1.",
      "upvotes": 11,
      "discussionId": "67ad5b3b007d78b391946a79"
    },
    "publishedAt": "2025-02-12T21:41:19.791Z",
    "title": "TransMLA: Multi-head Latent Attention Is All You Need",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07864.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643f55d4ec817b766686438a",
      "avatarUrl": "/avatars/0feb460432c92ab9ada0d417a7a38f6a.svg",
      "fullname": "mengfanxu",
      "name": "fxmeng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.08168",
      "authors": [
        {
          "_id": "67ad5f32d1a5243cc4fa38ad",
          "name": "Zhiming Ma",
          "hidden": false
        },
        {
          "_id": "67ad5f32d1a5243cc4fa38ae",
          "name": "Xiayang Xiao",
          "hidden": false
        },
        {
          "_id": "67ad5f32d1a5243cc4fa38af",
          "name": "Sihao Dong",
          "hidden": false
        },
        {
          "_id": "67ad5f32d1a5243cc4fa38b0",
          "name": "Peidong Wang",
          "hidden": false
        },
        {
          "_id": "67ad5f32d1a5243cc4fa38b1",
          "name": "HaiPeng Wang",
          "hidden": false
        },
        {
          "_id": "67ad5f32d1a5243cc4fa38b2",
          "name": "Qingyun Pan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-12T07:19:36.000Z",
      "title": "SARChat-Bench-2M: A Multi-Task Vision-Language Benchmark for SAR Image\n  Interpretation",
      "summary": "In the field of synthetic aperture radar (SAR) remote sensing image\ninterpretation, although Vision language models (VLMs) have made remarkable\nprogress in natural language processing and image understanding, their\napplications remain limited in professional domains due to insufficient domain\nexpertise. This paper innovatively proposes the first large-scale multimodal\ndialogue dataset for SAR images, named SARChat-2M, which contains approximately\n2 million high-quality image-text pairs, encompasses diverse scenarios with\ndetailed target annotations. This dataset not only supports several key tasks\nsuch as visual understanding and object detection tasks, but also has unique\ninnovative aspects: this study develop a visual-language dataset and benchmark\nfor the SAR domain, enabling and evaluating VLMs' capabilities in SAR image\ninterpretation, which provides a paradigmatic framework for constructing\nmultimodal datasets across various remote sensing vertical domains. Through\nexperiments on 16 mainstream VLMs, the effectiveness of the dataset has been\nfully verified, and the first multi-task dialogue benchmark in the SAR field\nhas been successfully established. The project will be released at\nhttps://github.com/JimmyMa99/SARChat, aiming to promote the in-depth\ndevelopment and wide application of SAR visual language models.",
      "upvotes": 7,
      "discussionId": "67ad5f37d1a5243cc4fa399c"
    },
    "publishedAt": "2025-02-12T21:57:30.420Z",
    "title": "SARChat-Bench-2M: A Multi-Task Vision-Language Benchmark for SAR Image Interpretation",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64a0ed5ed5374ca472cfb0ac/LvHzRQCttMAvKS-LM0ZDH.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08168.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "64a0ed5ed5374ca472cfb0ac",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a0ed5ed5374ca472cfb0ac/n_wXamXfR_PPn0hRbnR1X.jpeg",
      "fullname": "ZhimingMa",
      "name": "JimmyMa99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.08524",
      "authors": [
        {
          "_id": "67ad783da2808b57a3cd3316",
          "name": "Jihoon Tack",
          "hidden": false
        },
        {
          "_id": "67ad783da2808b57a3cd3317",
          "name": "Jack Lanchantin",
          "hidden": false
        },
        {
          "_id": "67ad783da2808b57a3cd3318",
          "name": "Jane Yu",
          "hidden": false
        },
        {
          "_id": "67ad783da2808b57a3cd3319",
          "name": "Andrew Cohen",
          "hidden": false
        },
        {
          "_id": "67ad783da2808b57a3cd331a",
          "name": "Ilia Kulikov",
          "hidden": false
        },
        {
          "_id": "67ad783da2808b57a3cd331b",
          "name": "Janice Lan",
          "hidden": false
        },
        {
          "_id": "67ad783da2808b57a3cd331c",
          "name": "Shibo Hao",
          "hidden": false
        },
        {
          "_id": "67ad783da2808b57a3cd331d",
          "name": "Yuandong Tian",
          "hidden": false
        },
        {
          "_id": "67ad783da2808b57a3cd331e",
          "name": "Jason Weston",
          "hidden": false
        },
        {
          "_id": "67ad783da2808b57a3cd331f",
          "user": {
            "_id": "659a395421a7431643caedda",
            "avatarUrl": "/avatars/c1e0bbcedce68fe3b4fe39e0cf01c65c.svg",
            "isPro": false,
            "fullname": "Xian Li",
            "user": "xlxxl",
            "type": "user"
          },
          "name": "Xian Li",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-13T04:42:38.302Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-12T16:00:11.000Z",
      "title": "LLM Pretraining with Continuous Concepts",
      "summary": "Next token prediction has been the standard training objective used in large\nlanguage model pretraining. Representations are learned as a result of\noptimizing for token-level perplexity. We propose Continuous Concept Mixing\n(CoCoMix), a novel pretraining framework that combines discrete next token\nprediction with continuous concepts. Specifically, CoCoMix predicts continuous\nconcepts learned from a pretrained sparse autoencoder and mixes them into the\nmodel's hidden state by interleaving with token hidden representations. Through\nexperiments on multiple benchmarks, including language modeling and downstream\nreasoning tasks, we show that CoCoMix is more sample efficient and consistently\noutperforms standard next token prediction, knowledge distillation and\ninserting pause tokens. We find that combining both concept learning and\ninterleaving in an end-to-end framework is critical to performance gains.\nFurthermore, CoCoMix enhances interpretability and steerability by allowing\ndirect inspection and modification of the predicted concept, offering a\ntransparent way to guide the model's internal reasoning process.",
      "upvotes": 5,
      "discussionId": "67ad783ea2808b57a3cd3361"
    },
    "publishedAt": "2025-02-12T23:42:44.287Z",
    "title": "LLM Pretraining with Continuous Concepts",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08524.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6061
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.08606",
      "authors": [
        {
          "_id": "67ad77f9cd8de299e5049c05",
          "name": "Dan Busbridge",
          "hidden": false
        },
        {
          "_id": "67ad77f9cd8de299e5049c06",
          "name": "Amitis Shidani",
          "hidden": false
        },
        {
          "_id": "67ad77f9cd8de299e5049c07",
          "name": "Floris Weers",
          "hidden": false
        },
        {
          "_id": "67ad77f9cd8de299e5049c08",
          "name": "Jason Ramapuram",
          "hidden": false
        },
        {
          "_id": "67ad77f9cd8de299e5049c09",
          "name": "Etai Littwin",
          "hidden": false
        },
        {
          "_id": "67ad77f9cd8de299e5049c0a",
          "name": "Russ Webb",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-12T17:52:47.000Z",
      "title": "Distillation Scaling Laws",
      "summary": "We provide a distillation scaling law that estimates distilled model\nperformance based on a compute budget and its allocation between the student\nand teacher. Our findings reduce the risks associated with using distillation\nat scale; compute allocation for both the teacher and student models can now be\ndone to maximize student performance. We provide compute optimal distillation\nrecipes for when 1) a teacher exists, or 2) a teacher needs training. If many\nstudents are to be distilled, or a teacher already exists, distillation\noutperforms supervised pretraining until a compute level which grows\npredictably with student size. If one student is to be distilled and a teacher\nalso needs training, supervised learning should be done instead. Additionally,\nwe provide insights across our large scale study of distillation, which\nincrease our understanding of distillation and inform experimental design.",
      "upvotes": 5,
      "discussionId": "67ad77fccd8de299e5049d06"
    },
    "publishedAt": "2025-02-12T23:41:41.281Z",
    "title": "Distillation Scaling Laws",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08606.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6061
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.06872",
      "authors": [
        {
          "_id": "67ad7da995ff670869168209",
          "name": "Bo Ni",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff67086916820a",
          "name": "Zheyuan Liu",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff67086916820b",
          "name": "Leyao Wang",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff67086916820c",
          "name": "Yongjia Lei",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff67086916820d",
          "name": "Yuying Zhao",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff67086916820e",
          "name": "Xueqi Cheng",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff67086916820f",
          "name": "Qingkai Zeng",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff670869168210",
          "name": "Luna Dong",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff670869168211",
          "name": "Yinglong Xia",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff670869168212",
          "name": "Krishnaram Kenthapadi",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff670869168213",
          "name": "Ryan Rossi",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff670869168214",
          "name": "Franck Dernoncourt",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff670869168215",
          "name": "Md Mehrab Tanjim",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff670869168216",
          "name": "Nesreen Ahmed",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff670869168217",
          "name": "Xiaorui Liu",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff670869168218",
          "name": "Wenqi Fan",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff670869168219",
          "name": "Erik Blasch",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff67086916821a",
          "name": "Yu Wang",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff67086916821b",
          "name": "Meng Jiang",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff67086916821c",
          "name": "Tyler Derr",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-08T06:50:47.000Z",
      "title": "Towards Trustworthy Retrieval Augmented Generation for Large Language\n  Models: A Survey",
      "summary": "Retrieval-Augmented Generation (RAG) is an advanced technique designed to\naddress the challenges of Artificial Intelligence-Generated Content (AIGC). By\nintegrating context retrieval into content generation, RAG provides reliable\nand up-to-date external knowledge, reduces hallucinations, and ensures relevant\ncontext across a wide range of tasks. However, despite RAG's success and\npotential, recent studies have shown that the RAG paradigm also introduces new\nrisks, including robustness issues, privacy concerns, adversarial attacks, and\naccountability issues. Addressing these risks is critical for future\napplications of RAG systems, as they directly impact their trustworthiness.\nAlthough various methods have been developed to improve the trustworthiness of\nRAG methods, there is a lack of a unified perspective and framework for\nresearch in this topic. Thus, in this paper, we aim to address this gap by\nproviding a comprehensive roadmap for developing trustworthy RAG systems. We\nplace our discussion around five key perspectives: reliability, privacy,\nsafety, fairness, explainability, and accountability. For each perspective, we\npresent a general framework and taxonomy, offering a structured approach to\nunderstanding the current challenges, evaluating existing solutions, and\nidentifying promising future research directions. To encourage broader adoption\nand innovation, we also highlight the downstream applications where trustworthy\nRAG systems have a significant impact.",
      "upvotes": 2,
      "discussionId": "67ad7daa95ff670869168251"
    },
    "publishedAt": "2025-02-13T00:06:04.056Z",
    "title": "Towards Trustworthy Retrieval Augmented Generation for Large Language Models: A Survey",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06872.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c5947524171688a9feb992",
      "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
      "fullname": "Franck Dernoncourt",
      "name": "Franck-Dernoncourt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.05167",
      "authors": [
        {
          "_id": "67aa583c3a878652daeae02e",
          "user": {
            "_id": "60e4738a8c0ddd18fc27ff88",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60e4738a8c0ddd18fc27ff88/lpLeeIW8r85RTY4fGZTva.jpeg",
            "isPro": false,
            "fullname": "Ali Modarressi",
            "user": "amodaresi",
            "type": "user"
          },
          "name": "Ali Modarressi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T07:55:42.560Z",
          "hidden": false
        },
        {
          "_id": "67aa583c3a878652daeae02f",
          "name": "Hanieh Deilamsalehy",
          "hidden": false
        },
        {
          "_id": "67aa583c3a878652daeae030",
          "name": "Franck Dernoncourt",
          "hidden": false
        },
        {
          "_id": "67aa583c3a878652daeae031",
          "name": "Trung Bui",
          "hidden": false
        },
        {
          "_id": "67aa583c3a878652daeae032",
          "name": "Ryan A. Rossi",
          "hidden": false
        },
        {
          "_id": "67aa583c3a878652daeae033",
          "name": "Seunghyun Yoon",
          "hidden": false
        },
        {
          "_id": "67aa583c3a878652daeae034",
          "name": "Hinrich Schütze",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-07T18:49:46.000Z",
      "title": "NoLiMa: Long-Context Evaluation Beyond Literal Matching",
      "summary": "Recent large language models (LLMs) support long contexts ranging from 128K\nto 1M tokens. A popular method for evaluating these capabilities is the\nneedle-in-a-haystack (NIAH) test, which involves retrieving a \"needle\"\n(relevant information) from a \"haystack\" (long irrelevant context). Extensions\nof this approach include increasing distractors, fact chaining, and in-context\nreasoning. However, in these benchmarks, models can exploit existing literal\nmatches between the needle and haystack to simplify the task. To address this,\nwe introduce NoLiMa, a benchmark extending NIAH with a carefully designed\nneedle set, where questions and needles have minimal lexical overlap, requiring\nmodels to infer latent associations to locate the needle within the haystack.\nWe evaluate 12 popular LLMs that claim to support contexts of at least 128K\ntokens. While they perform well in short contexts (<1K), performance degrades\nsignificantly as context length increases. At 32K, for instance, 10 models drop\nbelow 50% of their strong short-length baselines. Even GPT-4o, one of the\ntop-performing exceptions, experiences a reduction from an almost-perfect\nbaseline of 99.3% to 69.7%. Our analysis suggests these declines stem from the\nincreased difficulty the attention mechanism faces in longer contexts when\nliteral matches are absent, making it harder to retrieve relevant information.",
      "upvotes": 2,
      "discussionId": "67aa583d3a878652daeae06c"
    },
    "publishedAt": "2025-02-13T00:04:29.194Z",
    "title": "NoLiMa: Long-Context Evaluation Beyond Literal Matching",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05167.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c5947524171688a9feb992",
      "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
      "fullname": "Franck Dernoncourt",
      "name": "Franck-Dernoncourt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.07737",
      "authors": [
        {
          "_id": "67ad5d2f8436e8ea7abb7a15",
          "name": "Shuhuai Ren",
          "hidden": false
        },
        {
          "_id": "67ad5d2f8436e8ea7abb7a16",
          "name": "Shuming Ma",
          "hidden": false
        },
        {
          "_id": "67ad5d2f8436e8ea7abb7a17",
          "name": "Xu Sun",
          "hidden": false
        },
        {
          "_id": "67ad5d2f8436e8ea7abb7a18",
          "name": "Furu Wei",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T17:57:53.000Z",
      "title": "Next Block Prediction: Video Generation via Semi-Autoregressive Modeling",
      "summary": "Next-Token Prediction (NTP) is a de facto approach for autoregressive (AR)\nvideo generation, but it suffers from suboptimal unidirectional dependencies\nand slow inference speed. In this work, we propose a semi-autoregressive\n(semi-AR) framework, called Next-Block Prediction (NBP), for video generation.\nBy uniformly decomposing video content into equal-sized blocks (e.g., rows or\nframes), we shift the generation unit from individual tokens to blocks,\nallowing each token in the current block to simultaneously predict the\ncorresponding token in the next block. Unlike traditional AR modeling, our\nframework employs bidirectional attention within each block, enabling tokens to\ncapture more robust spatial dependencies. By predicting multiple tokens in\nparallel, NBP models significantly reduce the number of generation steps,\nleading to faster and more efficient inference. Our model achieves FVD scores\nof 103.3 on UCF101 and 25.5 on K600, outperforming the vanilla NTP model by an\naverage of 4.4. Furthermore, thanks to the reduced number of inference steps,\nthe NBP model generates 8.89 frames (128x128 resolution) per second, achieving\nan 11x speedup. We also explored model scales ranging from 700M to 3B\nparameters, observing significant improvements in generation quality, with FVD\nscores dropping from 103.3 to 55.3 on UCF101 and from 25.5 to 19.5 on K600,\ndemonstrating the scalability of our approach.",
      "upvotes": 2,
      "discussionId": "67ad5d308436e8ea7abb7a3d"
    },
    "publishedAt": "2025-02-12T21:48:00.325Z",
    "title": "Next Block Prediction: Video Generation via Semi-Autoregressive Modeling",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07737.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60d2e681b8448e1785bbda06",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1624434302056-noauth.jpeg",
      "fullname": "Shuhuai Ren",
      "name": "ShuhuaiRen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.07599",
      "authors": [
        {
          "_id": "67ad5bd2ac32a8e230fc8996",
          "name": "Xiliang Yang",
          "hidden": false
        },
        {
          "_id": "67ad5bd2ac32a8e230fc8997",
          "name": "Feng Jiang",
          "hidden": false
        },
        {
          "_id": "67ad5bd2ac32a8e230fc8998",
          "name": "Qianen Zhang",
          "hidden": false
        },
        {
          "_id": "67ad5bd2ac32a8e230fc8999",
          "name": "Lei Zhao",
          "hidden": false
        },
        {
          "_id": "67ad5bd2ac32a8e230fc899a",
          "name": "Xiao Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T14:49:44.000Z",
      "title": "DPO-Shift: Shifting the Distribution of Direct Preference Optimization",
      "summary": "Direct Preference Optimization (DPO) and its variants have become\nincreasingly popular for aligning language models with human preferences. These\nmethods aim to teach models to better distinguish between chosen (or preferred)\nand rejected (or dispreferred) responses. However, prior research has\nidentified that the probability of chosen responses often decreases during\ntraining, and this phenomenon is known as likelihood displacement. To tackle\nthis challenge, in this work we introduce \\method to controllably shift the\ndistribution of the chosen probability. Then, we show that \\method exhibits a\nfundamental trade-off between improving the chosen probability and sacrificing\nthe reward margin, as supported by both theoretical analysis and experimental\nvalidation. Furthermore, we demonstrate the superiority of \\method over DPO on\ndownstream tasks such as MT-Bench and a designed win rate experiment. We\nbelieve this study shows that the likelihood displacement issue of DPO can be\neffectively mitigated with a simple, theoretically grounded solution. Our code\nis available at https://github.com/Meaquadddd/DPO-Shift.",
      "upvotes": 2,
      "discussionId": "67ad5bd3ac32a8e230fc89a7"
    },
    "publishedAt": "2025-02-12T21:43:42.404Z",
    "title": "DPO-Shift: Shifting the Distribution of Direct Preference Optimization",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07599.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66270fcef7cf69d4223a8a3f",
      "avatarUrl": "/avatars/115db0326737e65318c92a7b8dc5ed6a.svg",
      "fullname": "Xiao Li",
      "name": "xli0982",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.07985",
      "authors": [
        {
          "_id": "67ad9577b469222e0df18134",
          "user": {
            "_id": "5fad8602b8423e1d80b8a965",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fad8602b8423e1d80b8a965/tRqTwcZmrGka8c1vFq2wX.jpeg",
            "isPro": false,
            "fullname": "Victor Gallego",
            "user": "vicgalle",
            "type": "user"
          },
          "name": "Víctor Gallego",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-13T06:47:20.731Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T22:06:25.000Z",
      "title": "MetaSC: Test-Time Safety Specification Optimization for Language Models",
      "summary": "We propose a novel dynamic safety framework that optimizes language model\n(LM) safety reasoning at inference time without modifying model weights.\nBuilding on recent advances in self-critique methods, our approach leverages a\nmeta-critique mechanism that iteratively updates safety prompts-termed\nspecifications-to drive the critique and revision process adaptively. This\ntest-time optimization not only improves performance against adversarial\njailbreak requests but also in diverse general safety-related tasks, such as\navoiding moral harm or pursuing honest responses. Our empirical evaluations\nacross several language models demonstrate that dynamically optimized safety\nprompts yield significantly higher safety scores compared to fixed system\nprompts and static self-critique defenses. Code to be released at\nhttps://github.com/vicgalle/meta-self-critique.git .",
      "upvotes": 1,
      "discussionId": "67ad9578b469222e0df18162"
    },
    "publishedAt": "2025-02-13T01:47:30.377Z",
    "title": "MetaSC: Test-Time Safety Specification Optimization for Language Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07985.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5fad8602b8423e1d80b8a965",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fad8602b8423e1d80b8a965/tRqTwcZmrGka8c1vFq2wX.jpeg",
      "fullname": "Victor Gallego",
      "name": "vicgalle",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 117
    },
    "isAuthorParticipating": true
  }
]