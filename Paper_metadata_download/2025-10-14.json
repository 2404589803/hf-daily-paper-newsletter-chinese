[
  {
    "paper": {
      "id": "2510.11696",
      "authors": [
        {
          "_id": "68edb7abde1fee572713a7df",
          "name": "Wei Huang",
          "hidden": false
        },
        {
          "_id": "68edb7abde1fee572713a7e0",
          "name": "Yi Ge",
          "hidden": false
        },
        {
          "_id": "68edb7abde1fee572713a7e1",
          "name": "Shuai Yang",
          "hidden": false
        },
        {
          "_id": "68edb7abde1fee572713a7e2",
          "name": "Yicheng Xiao",
          "hidden": false
        },
        {
          "_id": "68edb7abde1fee572713a7e3",
          "name": "Huizi Mao",
          "hidden": false
        },
        {
          "_id": "68edb7abde1fee572713a7e4",
          "name": "Yujun Lin",
          "hidden": false
        },
        {
          "_id": "68edb7abde1fee572713a7e5",
          "name": "Hanrong Ye",
          "hidden": false
        },
        {
          "_id": "68edb7abde1fee572713a7e6",
          "name": "Sifei Liu",
          "hidden": false
        },
        {
          "_id": "68edb7abde1fee572713a7e7",
          "name": "Ka Chun Cheung",
          "hidden": false
        },
        {
          "_id": "68edb7abde1fee572713a7e8",
          "name": "Hongxu Yin",
          "hidden": false
        },
        {
          "_id": "68edb7abde1fee572713a7e9",
          "name": "Yao Lu",
          "hidden": false
        },
        {
          "_id": "68edb7abde1fee572713a7ea",
          "name": "Xiaojuan Qi",
          "hidden": false
        },
        {
          "_id": "68edb7abde1fee572713a7eb",
          "name": "Song Han",
          "hidden": false
        },
        {
          "_id": "68edb7abde1fee572713a7ec",
          "name": "Yukang Chen",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/656db3f53dc1d277e5a64410/M3Wu6kYubMte0JBARLogc.qt"
      ],
      "publishedAt": "2025-10-13T17:55:09.000Z",
      "submittedOnDailyAt": "2025-10-14T01:17:25.740Z",
      "title": "QeRL: Beyond Efficiency -- Quantization-enhanced Reinforcement Learning\n  for LLMs",
      "submittedOnDailyBy": {
        "_id": "656db3f53dc1d277e5a64410",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656db3f53dc1d277e5a64410/9kiY2K3MCRcBDk7MrkTBK.png",
        "isPro": false,
        "fullname": "Wei Huang",
        "user": "AaronHuangWei",
        "type": "user"
      },
      "summary": "We propose QeRL, a Quantization-enhanced Reinforcement Learning framework for\nlarge language models (LLMs). While RL is essential for LLMs' reasoning\ncapabilities, it is resource-intensive, requiring substantial GPU memory and\nlong rollout durations. QeRL addresses these issues by combining NVFP4\nquantization with Low-Rank Adaptation (LoRA), accelerating rollout phase of RL\nwhile reducing memory overhead. Beyond efficiency, our findings show that\nquantization noise increases policy entropy, enhancing exploration, and\nenabling the discovery of better strategies during RL. To further optimize\nexploration, QeRL introduces an Adaptive Quantization Noise (AQN) mechanism,\nwhich dynamically adjusts noise during training. Experiments demonstrate that\nQeRL delivers over 1.5 times speedup in the rollout phase. Moreover, this is\nthe first framework to enable RL training of a 32B LLM on a single H100 80GB\nGPU, while delivering overall speedups for RL training. It also achieves faster\nreward growth and higher final accuracy than 16-bit LoRA and QLoRA, while\nmatching the performance of full-parameter fine-tuning on mathematical\nbenchmarks such as GSM8K (90.8%) and MATH 500 (77.4%) in the 7B model. These\nresults establish QeRL as an efficient and effective framework for RL training\nin LLMs.",
      "upvotes": 75,
      "discussionId": "68edb7abde1fee572713a7ed",
      "projectPage": "https://github.com/NVlabs/QeRL",
      "githubRepo": "https://github.com/NVlabs/QeRL",
      "ai_summary": "QeRL, a quantization-enhanced reinforcement learning framework, accelerates RL training for large language models by combining NVFP4 quantization with Low-Rank Adaptation and an Adaptive Quantization Noise mechanism, achieving significant speedups and improved performance.",
      "ai_keywords": [
        "NVFP4 quantization",
        "Low-Rank Adaptation (LoRA)",
        "Adaptive Quantization Noise (AQN)",
        "reinforcement learning",
        "large language models (LLMs)",
        "rollout phase",
        "policy entropy",
        "exploration",
        "reward growth",
        "GSM8K",
        "MATH 500"
      ],
      "githubStars": 103,
      "organization": {
        "_id": "60262b67268c201cdc8b7d43",
        "name": "nvidia",
        "fullname": "NVIDIA",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
      }
    },
    "publishedAt": "2025-10-13T13:55:09.000Z",
    "title": "QeRL: Beyond Efficiency -- Quantization-enhanced Reinforcement Learning\n  for LLMs",
    "summary": "We propose QeRL, a Quantization-enhanced Reinforcement Learning framework for\nlarge language models (LLMs). While RL is essential for LLMs' reasoning\ncapabilities, it is resource-intensive, requiring substantial GPU memory and\nlong rollout durations. QeRL addresses these issues by combining NVFP4\nquantization with Low-Rank Adaptation (LoRA), accelerating rollout phase of RL\nwhile reducing memory overhead. Beyond efficiency, our findings show that\nquantization noise increases policy entropy, enhancing exploration, and\nenabling the discovery of better strategies during RL. To further optimize\nexploration, QeRL introduces an Adaptive Quantization Noise (AQN) mechanism,\nwhich dynamically adjusts noise during training. Experiments demonstrate that\nQeRL delivers over 1.5 times speedup in the rollout phase. Moreover, this is\nthe first framework to enable RL training of a 32B LLM on a single H100 80GB\nGPU, while delivering overall speedups for RL training. It also achieves faster\nreward growth and higher final accuracy than 16-bit LoRA and QLoRA, while\nmatching the performance of full-parameter fine-tuning on mathematical\nbenchmarks such as GSM8K (90.8%) and MATH 500 (77.4%) in the 7B model. These\nresults establish QeRL as an efficient and effective framework for RL training\nin LLMs.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/656db3f53dc1d277e5a64410/M3Wu6kYubMte0JBARLogc.qt"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.11696.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "656db3f53dc1d277e5a64410",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656db3f53dc1d277e5a64410/9kiY2K3MCRcBDk7MrkTBK.png",
      "fullname": "Wei Huang",
      "name": "AaronHuangWei",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "organization": {
      "_id": "60262b67268c201cdc8b7d43",
      "name": "nvidia",
      "fullname": "NVIDIA",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.11690",
      "authors": [
        {
          "_id": "68edbbccde1fee572713a848",
          "name": "Boyang Zheng",
          "hidden": false
        },
        {
          "_id": "68edbbccde1fee572713a849",
          "name": "Nanye Ma",
          "hidden": false
        },
        {
          "_id": "68edbbccde1fee572713a84a",
          "name": "Shengbang Tong",
          "hidden": false
        },
        {
          "_id": "68edbbccde1fee572713a84b",
          "name": "Saining Xie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-13T17:51:39.000Z",
      "submittedOnDailyAt": "2025-10-14T01:43:32.670Z",
      "title": "Diffusion Transformers with Representation Autoencoders",
      "submittedOnDailyBy": {
        "_id": "6374cbb7255276f3a22b4b35",
        "avatarUrl": "/avatars/7cf1bbb83447441e5fa2e1e4fcf7617b.svg",
        "isPro": true,
        "fullname": "Peter Tong",
        "user": "tsbpp",
        "type": "user"
      },
      "summary": "Latent generative modeling, where a pretrained autoencoder maps pixels into a\nlatent space for the diffusion process, has become the standard strategy for\nDiffusion Transformers (DiT); however, the autoencoder component has barely\nevolved. Most DiTs continue to rely on the original VAE encoder, which\nintroduces several limitations: outdated backbones that compromise\narchitectural simplicity, low-dimensional latent spaces that restrict\ninformation capacity, and weak representations that result from purely\nreconstruction-based training and ultimately limit generative quality. In this\nwork, we explore replacing the VAE with pretrained representation encoders\n(e.g., DINO, SigLIP, MAE) paired with trained decoders, forming what we term\nRepresentation Autoencoders (RAEs). These models provide both high-quality\nreconstructions and semantically rich latent spaces, while allowing for a\nscalable transformer-based architecture. Since these latent spaces are\ntypically high-dimensional, a key challenge is enabling diffusion transformers\nto operate effectively within them. We analyze the sources of this difficulty,\npropose theoretically motivated solutions, and validate them empirically. Our\napproach achieves faster convergence without auxiliary representation alignment\nlosses. Using a DiT variant equipped with a lightweight, wide DDT head, we\nachieve strong image generation results on ImageNet: 1.51 FID at 256x256 (no\nguidance) and 1.13 at both 256x256 and 512x512 (with guidance). RAE offers\nclear advantages and should be the new default for diffusion transformer\ntraining.",
      "upvotes": 39,
      "discussionId": "68edbbccde1fee572713a84c",
      "projectPage": "https://rae-dit.github.io/",
      "githubRepo": "https://github.com/bytetriper/RAE",
      "ai_summary": "Replacing VAEs with pretrained representation encoders in Diffusion Transformers enhances generative quality and convergence speed without auxiliary losses.",
      "ai_keywords": [
        "Latent generative modeling",
        "Diffusion Transformers (DiT)",
        "VAE encoder",
        "Representation Autoencoders (RAEs)",
        "pretrained representation encoders",
        "DINO",
        "SigLIP",
        "MAE",
        "high-dimensional latent spaces",
        "transformer-based architecture",
        "diffusion transformers",
        "image generation",
        "ImageNet",
        "FID"
      ],
      "organization": {
        "_id": "662741612ada5b77e310d171",
        "name": "nyu-visionx",
        "fullname": "NYU VisionX",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/626dc5105f7327906f0b2a4e/Kn-QtZjE6TJE-syTndXIW.jpeg"
      }
    },
    "publishedAt": "2025-10-13T13:51:39.000Z",
    "title": "Diffusion Transformers with Representation Autoencoders",
    "summary": "Latent generative modeling, where a pretrained autoencoder maps pixels into a\nlatent space for the diffusion process, has become the standard strategy for\nDiffusion Transformers (DiT); however, the autoencoder component has barely\nevolved. Most DiTs continue to rely on the original VAE encoder, which\nintroduces several limitations: outdated backbones that compromise\narchitectural simplicity, low-dimensional latent spaces that restrict\ninformation capacity, and weak representations that result from purely\nreconstruction-based training and ultimately limit generative quality. In this\nwork, we explore replacing the VAE with pretrained representation encoders\n(e.g., DINO, SigLIP, MAE) paired with trained decoders, forming what we term\nRepresentation Autoencoders (RAEs). These models provide both high-quality\nreconstructions and semantically rich latent spaces, while allowing for a\nscalable transformer-based architecture. Since these latent spaces are\ntypically high-dimensional, a key challenge is enabling diffusion transformers\nto operate effectively within them. We analyze the sources of this difficulty,\npropose theoretically motivated solutions, and validate them empirically. Our\napproach achieves faster convergence without auxiliary representation alignment\nlosses. Using a DiT variant equipped with a lightweight, wide DDT head, we\nachieve strong image generation results on ImageNet: 1.51 FID at 256x256 (no\nguidance) and 1.13 at both 256x256 and 512x512 (with guidance). RAE offers\nclear advantages and should be the new default for diffusion transformer\ntraining.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.11690.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6374cbb7255276f3a22b4b35",
      "avatarUrl": "/avatars/7cf1bbb83447441e5fa2e1e4fcf7617b.svg",
      "fullname": "Peter Tong",
      "name": "tsbpp",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12
    },
    "organization": {
      "_id": "662741612ada5b77e310d171",
      "name": "nyu-visionx",
      "fullname": "NYU VisionX",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/626dc5105f7327906f0b2a4e/Kn-QtZjE6TJE-syTndXIW.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.10689",
      "authors": [
        {
          "_id": "68edc3cede1fee572713a8c7",
          "name": "Caorui Li",
          "hidden": false
        },
        {
          "_id": "68edc3cede1fee572713a8c8",
          "name": "Yu Chen",
          "hidden": false
        },
        {
          "_id": "68edc3cede1fee572713a8c9",
          "name": "Yiyan Ji",
          "hidden": false
        },
        {
          "_id": "68edc3cede1fee572713a8ca",
          "name": "Jin Xu",
          "hidden": false
        },
        {
          "_id": "68edc3cede1fee572713a8cb",
          "name": "Zhenyu Cui",
          "hidden": false
        },
        {
          "_id": "68edc3cede1fee572713a8cc",
          "name": "Shihao Li",
          "hidden": false
        },
        {
          "_id": "68edc3cede1fee572713a8cd",
          "name": "Yuanxing Zhang",
          "hidden": false
        },
        {
          "_id": "68edc3cede1fee572713a8ce",
          "name": "Jiafu Tang",
          "hidden": false
        },
        {
          "_id": "68edc3cede1fee572713a8cf",
          "name": "Zhenghao Song",
          "hidden": false
        },
        {
          "_id": "68edc3cede1fee572713a8d0",
          "name": "Dingling Zhang",
          "hidden": false
        },
        {
          "_id": "68edc3cede1fee572713a8d1",
          "name": "Ying He",
          "hidden": false
        },
        {
          "_id": "68edc3cede1fee572713a8d2",
          "name": "Haoxiang Liu",
          "hidden": false
        },
        {
          "_id": "68edc3cede1fee572713a8d3",
          "name": "Yuxuan Wang",
          "hidden": false
        },
        {
          "_id": "68edc3cede1fee572713a8d4",
          "name": "Qiufeng Wang",
          "hidden": false
        },
        {
          "_id": "68edc3cede1fee572713a8d5",
          "name": "Zhenhe Wu",
          "hidden": false
        },
        {
          "_id": "68edc3cede1fee572713a8d6",
          "name": "Jiehui Luo",
          "hidden": false
        },
        {
          "_id": "68edc3cede1fee572713a8d7",
          "name": "Zhiyu Pan",
          "hidden": false
        },
        {
          "_id": "68edc3cede1fee572713a8d8",
          "name": "Weihao Xie",
          "hidden": false
        },
        {
          "_id": "68edc3cede1fee572713a8d9",
          "user": {
            "_id": "64b74b906ab5d14ca7f289cd",
            "avatarUrl": "/avatars/b131b7c4ce5216708ca4a678f35ead0a.svg",
            "isPro": false,
            "fullname": "Chenchen Zhang",
            "user": "xxzcc",
            "type": "user"
          },
          "name": "Chenchen Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-14T07:30:15.133Z",
          "hidden": false
        },
        {
          "_id": "68edc3cede1fee572713a8da",
          "name": "Zhaohui Wang",
          "hidden": false
        },
        {
          "_id": "68edc3cede1fee572713a8db",
          "name": "Jiayi Tian",
          "hidden": false
        },
        {
          "_id": "68edc3cede1fee572713a8dc",
          "name": "Yanghai Wang",
          "hidden": false
        },
        {
          "_id": "68edc3cede1fee572713a8dd",
          "name": "Zhe Cao",
          "hidden": false
        },
        {
          "_id": "68edc3cede1fee572713a8de",
          "name": "Minxin Dai",
          "hidden": false
        },
        {
          "_id": "68edc3cede1fee572713a8df",
          "name": "Ke Wang",
          "hidden": false
        },
        {
          "_id": "68edc3cede1fee572713a8e0",
          "name": "Runzhe Wen",
          "hidden": false
        },
        {
          "_id": "68edc3cede1fee572713a8e1",
          "name": "Yinghao Ma",
          "hidden": false
        },
        {
          "_id": "68edc3cede1fee572713a8e2",
          "name": "Yaning Pan",
          "hidden": false
        },
        {
          "_id": "68edc3cede1fee572713a8e3",
          "name": "Sungkyun Chang",
          "hidden": false
        },
        {
          "_id": "68edc3cede1fee572713a8e4",
          "name": "Termeh Taheri",
          "hidden": false
        },
        {
          "_id": "68edc3cede1fee572713a8e5",
          "name": "Haiwen Xia",
          "hidden": false
        },
        {
          "_id": "68edc3cede1fee572713a8e6",
          "name": "Christos Plachouras",
          "hidden": false
        },
        {
          "_id": "68edc3cede1fee572713a8e7",
          "name": "Emmanouil Benetos",
          "hidden": false
        },
        {
          "_id": "68edc3cede1fee572713a8e8",
          "name": "Yizhi Li",
          "hidden": false
        },
        {
          "_id": "68edc3cede1fee572713a8e9",
          "name": "Ge Zhang",
          "hidden": false
        },
        {
          "_id": "68edc3cede1fee572713a8ea",
          "name": "Jian Yang",
          "hidden": false
        },
        {
          "_id": "68edc3cede1fee572713a8eb",
          "name": "Tianhao Peng",
          "hidden": false
        },
        {
          "_id": "68edc3cede1fee572713a8ec",
          "name": "Zili Wang",
          "hidden": false
        },
        {
          "_id": "68edc3cede1fee572713a8ed",
          "name": "Minghao Liu",
          "hidden": false
        },
        {
          "_id": "68edc3cede1fee572713a8ee",
          "name": "Junran Peng",
          "hidden": false
        },
        {
          "_id": "68edc3cede1fee572713a8ef",
          "name": "Zhaoxiang Zhang",
          "hidden": false
        },
        {
          "_id": "68edc3cede1fee572713a8f0",
          "name": "Jiaheng Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-12T16:34:00.000Z",
      "submittedOnDailyAt": "2025-10-14T02:01:14.736Z",
      "title": "OmniVideoBench: Towards Audio-Visual Understanding Evaluation for Omni\n  MLLMs",
      "submittedOnDailyBy": {
        "_id": "65377c30e48353201e6fdda0",
        "avatarUrl": "/avatars/a8f803b6f2e598eaee9c52c0d2ddfc16.svg",
        "isPro": false,
        "fullname": "Jiaheng Liu",
        "user": "CheeryLJH",
        "type": "user"
      },
      "summary": "Recent advances in multimodal large language models (MLLMs) have demonstrated\nsubstantial potential in video understanding. However, existing benchmarks fail\nto comprehensively evaluate synergistic reasoning capabilities across audio and\nvisual modalities, often neglecting either one of the modalities or integrating\nthem in a logically inconsistent manner. To bridge this gap, we introduce\nOmniVideoBench, a large-scale and rigorously designed benchmark dedicated to\nassessing synergistic audio-visual understanding, with a strong emphasis on\nmodality complementarity and logical consistency. Specifically, OmniVideoBench\ncomprises 1000 high-quality question-answer(QA) pairs, each annotated with\nstep-by-step reasoning traces, derived from 628 diverse videos ranging from\nseveral seconds to 30 minutes, and manually verified to guarantee complete\ncorrectness and uniqueness. Moreover, OmniVideoBench encompasses 13 carefully\ndesigned question types, covering temporal reasoning, spatial localization,\ncounting, causal inference, summarization, and beyond, thereby capturing the\nessential challenges of video understanding. Evaluation of multiple MLLMs on\nOmniVideoBench reveals a pronounced gap between model performance and human\nreasoning, with open-source models lagging significantly behind their\nclosed-source counterparts, underscoring the inherent difficulty of genuine\naudio-visual reasoning. We will release OmniVideoBench to foster the\ndevelopment of MLLMs with stronger and more generalizable reasoning\ncapabilities.",
      "upvotes": 30,
      "discussionId": "68edc3cede1fee572713a8f1",
      "projectPage": "https://omnivideobench.github.io/omnivideobench_home/",
      "githubRepo": "https://github.com/NJU-LINK/OmniVideoBench",
      "ai_summary": "OmniVideoBench is a comprehensive benchmark for evaluating audio-visual reasoning in multimodal large language models, addressing modality complementarity and logical consistency.",
      "ai_keywords": [
        "multimodal large language models",
        "video understanding",
        "audio-visual understanding",
        "modality complementarity",
        "logical consistency",
        "question-answer pairs",
        "reasoning traces",
        "temporal reasoning",
        "spatial localization",
        "counting",
        "causal inference",
        "summarization"
      ],
      "githubStars": 4,
      "organization": {
        "_id": "68edc767abe005ac1b354573",
        "name": "NJU-LINK",
        "fullname": "Large-scale Intelligence and Knowledge Lab, Nanjing University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67f9d060395fb1a0d7e4ae21/O3V4UZjcSGnOivcQqTcXW.png"
      }
    },
    "publishedAt": "2025-10-12T12:34:00.000Z",
    "title": "OmniVideoBench: Towards Audio-Visual Understanding Evaluation for Omni\n  MLLMs",
    "summary": "Recent advances in multimodal large language models (MLLMs) have demonstrated\nsubstantial potential in video understanding. However, existing benchmarks fail\nto comprehensively evaluate synergistic reasoning capabilities across audio and\nvisual modalities, often neglecting either one of the modalities or integrating\nthem in a logically inconsistent manner. To bridge this gap, we introduce\nOmniVideoBench, a large-scale and rigorously designed benchmark dedicated to\nassessing synergistic audio-visual understanding, with a strong emphasis on\nmodality complementarity and logical consistency. Specifically, OmniVideoBench\ncomprises 1000 high-quality question-answer(QA) pairs, each annotated with\nstep-by-step reasoning traces, derived from 628 diverse videos ranging from\nseveral seconds to 30 minutes, and manually verified to guarantee complete\ncorrectness and uniqueness. Moreover, OmniVideoBench encompasses 13 carefully\ndesigned question types, covering temporal reasoning, spatial localization,\ncounting, causal inference, summarization, and beyond, thereby capturing the\nessential challenges of video understanding. Evaluation of multiple MLLMs on\nOmniVideoBench reveals a pronounced gap between model performance and human\nreasoning, with open-source models lagging significantly behind their\nclosed-source counterparts, underscoring the inherent difficulty of genuine\naudio-visual reasoning. We will release OmniVideoBench to foster the\ndevelopment of MLLMs with stronger and more generalizable reasoning\ncapabilities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.10689.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65377c30e48353201e6fdda0",
      "avatarUrl": "/avatars/a8f803b6f2e598eaee9c52c0d2ddfc16.svg",
      "fullname": "Jiaheng Liu",
      "name": "CheeryLJH",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 18
    },
    "organization": {
      "_id": "68edc767abe005ac1b354573",
      "name": "NJU-LINK",
      "fullname": "Large-scale Intelligence and Knowledge Lab, Nanjing University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67f9d060395fb1a0d7e4ae21/O3V4UZjcSGnOivcQqTcXW.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.09285",
      "authors": [
        {
          "_id": "68ec8ab6cd07fb414898ca7e",
          "name": "Siyuan Huang",
          "hidden": false
        },
        {
          "_id": "68ec8ab6cd07fb414898ca7f",
          "name": "Xiaoye Qu",
          "hidden": false
        },
        {
          "_id": "68ec8ab6cd07fb414898ca80",
          "name": "Yafu Li",
          "hidden": false
        },
        {
          "_id": "68ec8ab6cd07fb414898ca81",
          "name": "Yun Luo",
          "hidden": false
        },
        {
          "_id": "68ec8ab6cd07fb414898ca82",
          "name": "Zefeng He",
          "hidden": false
        },
        {
          "_id": "68ec8ab6cd07fb414898ca83",
          "name": "Daizong Liu",
          "hidden": false
        },
        {
          "_id": "68ec8ab6cd07fb414898ca84",
          "name": "Yu Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-10T11:25:33.000Z",
      "submittedOnDailyAt": "2025-10-14T04:40:24.081Z",
      "title": "Spotlight on Token Perception for Multimodal Reinforcement Learning",
      "submittedOnDailyBy": {
        "_id": "64cb54da1af278541d663708",
        "avatarUrl": "/avatars/c44507cc92bb2e83154bad31b90ce6dd.svg",
        "isPro": false,
        "fullname": "Xiaoye Qu",
        "user": "Xiaoye08",
        "type": "user"
      },
      "summary": "While Reinforcement Learning with Verifiable Rewards (RLVR) has advanced the\nreasoning capabilities of Large Vision-Language Models (LVLMs), most existing\nmethods in multimodal reasoning neglect the critical role of visual perception\nwithin the RLVR optimization process. In this paper, we undertake a pioneering\nexploration of multimodal RLVR through the novel perspective of token\nperception, which measures the visual dependency of each generated token. With\na granular analysis of Chain-of-Thought (CoT) processes, we uncover two key\ninsights: first, token perception in a rollout trajectory is sparsely\ndistributed, where only a small fraction of tokens have high visual dependency\nfor visually-grounded reasoning; second, different trajectories exhibit\nsignificant divergence in their overall visual dependency. Based on these\nobservations, we propose Visually-Perceptive Policy Optimization (VPPO), a\nnovel policy gradient algorithm that explicitly leverages token perception to\nrefine the learning signal. Specifically, VPPO achieves this through a dual\nmechanism: it reweights a trajectory's advantage by its overall visual\ndependency, and focuses policy updates exclusively on perceptually pivotal\ntokens. On a comprehensive suite of eight perception and reasoning benchmarks,\nVPPO demonstrates substantial gains over leading open-source RL-tuned models,\nwith its effectiveness consistently validated across 7B and 32B model scales.\nOur findings not only establish a new token-level perceptual perspective for\nanalyzing multimodal RLVR but also present a novel and effective optimization\nstrategy to significantly enhance the multimodal reasoning capabilities of\nLVLMs.",
      "upvotes": 26,
      "discussionId": "68ec8ab6cd07fb414898ca85",
      "projectPage": "https://github.com/huaixuheqing/VPPO-RL",
      "githubRepo": "https://github.com/huaixuheqing/VPPO-RL",
      "ai_summary": "VPPO, a novel policy gradient algorithm, enhances multimodal RLVR by leveraging token perception to refine learning signals and improve reasoning capabilities in Large Vision-Language Models.",
      "ai_keywords": [
        "Reinforcement Learning with Verifiable Rewards",
        "Large Vision-Language Models",
        "multimodal reasoning",
        "token perception",
        "Chain-of-Thought",
        "rollout trajectory",
        "visual dependency",
        "Visually-Perceptive Policy Optimization",
        "policy gradient algorithm",
        "perceptually pivotal tokens"
      ],
      "githubStars": 14
    },
    "publishedAt": "2025-10-10T07:25:33.000Z",
    "title": "Spotlight on Token Perception for Multimodal Reinforcement Learning",
    "summary": "While Reinforcement Learning with Verifiable Rewards (RLVR) has advanced the\nreasoning capabilities of Large Vision-Language Models (LVLMs), most existing\nmethods in multimodal reasoning neglect the critical role of visual perception\nwithin the RLVR optimization process. In this paper, we undertake a pioneering\nexploration of multimodal RLVR through the novel perspective of token\nperception, which measures the visual dependency of each generated token. With\na granular analysis of Chain-of-Thought (CoT) processes, we uncover two key\ninsights: first, token perception in a rollout trajectory is sparsely\ndistributed, where only a small fraction of tokens have high visual dependency\nfor visually-grounded reasoning; second, different trajectories exhibit\nsignificant divergence in their overall visual dependency. Based on these\nobservations, we propose Visually-Perceptive Policy Optimization (VPPO), a\nnovel policy gradient algorithm that explicitly leverages token perception to\nrefine the learning signal. Specifically, VPPO achieves this through a dual\nmechanism: it reweights a trajectory's advantage by its overall visual\ndependency, and focuses policy updates exclusively on perceptually pivotal\ntokens. On a comprehensive suite of eight perception and reasoning benchmarks,\nVPPO demonstrates substantial gains over leading open-source RL-tuned models,\nwith its effectiveness consistently validated across 7B and 32B model scales.\nOur findings not only establish a new token-level perceptual perspective for\nanalyzing multimodal RLVR but also present a novel and effective optimization\nstrategy to significantly enhance the multimodal reasoning capabilities of\nLVLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.09285.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64cb54da1af278541d663708",
      "avatarUrl": "/avatars/c44507cc92bb2e83154bad31b90ce6dd.svg",
      "fullname": "Xiaoye Qu",
      "name": "Xiaoye08",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.10395",
      "authors": [
        {
          "_id": "68eda8c8de1fee572713a6b2",
          "user": {
            "_id": "66650d38b52f0890724f3b07",
            "avatarUrl": "/avatars/c25a365bff4985ebb71c96dd097b804f.svg",
            "isPro": false,
            "fullname": "Xinlong Chen",
            "user": "XinlongChen",
            "type": "user"
          },
          "name": "Xinlong Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-14T07:31:15.388Z",
          "hidden": false
        },
        {
          "_id": "68eda8c8de1fee572713a6b3",
          "name": "Yue Ding",
          "hidden": false
        },
        {
          "_id": "68eda8c8de1fee572713a6b4",
          "name": "Weihong Lin",
          "hidden": false
        },
        {
          "_id": "68eda8c8de1fee572713a6b5",
          "user": {
            "_id": "61540338e5b9ae6774201e58",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61540338e5b9ae6774201e58/h_159VrXOlIgu0N0pNgXj.png",
            "isPro": false,
            "fullname": "jingyun",
            "user": "hjy",
            "type": "user"
          },
          "name": "Jingyun Hua",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-14T07:31:10.887Z",
          "hidden": false
        },
        {
          "_id": "68eda8c8de1fee572713a6b6",
          "name": "Linli Yao",
          "hidden": false
        },
        {
          "_id": "68eda8c8de1fee572713a6b7",
          "user": {
            "_id": "673c7319d11b1c2e246ead9c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/673c7319d11b1c2e246ead9c/IjFIO--N7Hm_BOEafhEQv.jpeg",
            "isPro": false,
            "fullname": "Yang Shi",
            "user": "DogNeverSleep",
            "type": "user"
          },
          "name": "Yang Shi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-14T07:31:13.095Z",
          "hidden": false
        },
        {
          "_id": "68eda8c8de1fee572713a6b8",
          "name": "Bozhou Li",
          "hidden": false
        },
        {
          "_id": "68eda8c8de1fee572713a6b9",
          "name": "Yuanxing Zhang",
          "hidden": false
        },
        {
          "_id": "68eda8c8de1fee572713a6ba",
          "name": "Qiang Liu",
          "hidden": false
        },
        {
          "_id": "68eda8c8de1fee572713a6bb",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "68eda8c8de1fee572713a6bc",
          "name": "Liang Wang",
          "hidden": false
        },
        {
          "_id": "68eda8c8de1fee572713a6bd",
          "name": "Tieniu Tan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-12T01:20:22.000Z",
      "submittedOnDailyAt": "2025-10-14T01:50:38.444Z",
      "title": "AVoCaDO: An Audiovisual Video Captioner Driven by Temporal Orchestration",
      "submittedOnDailyBy": {
        "_id": "673c7319d11b1c2e246ead9c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/673c7319d11b1c2e246ead9c/IjFIO--N7Hm_BOEafhEQv.jpeg",
        "isPro": false,
        "fullname": "Yang Shi",
        "user": "DogNeverSleep",
        "type": "user"
      },
      "summary": "Audiovisual video captioning aims to generate semantically rich descriptions\nwith temporal alignment between visual and auditory events, thereby benefiting\nboth video understanding and generation. In this paper, we present AVoCaDO, a\npowerful audiovisual video captioner driven by the temporal orchestration\nbetween audio and visual modalities. We propose a two-stage post-training\npipeline: (1) AVoCaDO SFT, which fine-tunes the model on a newly curated\ndataset of 107K high-quality, temporally-aligned audiovisual captions; and (2)\nAVoCaDO GRPO, which leverages tailored reward functions to further enhance\ntemporal coherence and dialogue accuracy while regularizing caption length and\nreducing collapse. Experimental results demonstrate that AVoCaDO significantly\noutperforms existing open-source models across four audiovisual video\ncaptioning benchmarks, and also achieves competitive performance on the VDC and\nDREAM-1K benchmark under visual-only settings.",
      "upvotes": 25,
      "discussionId": "68eda8c8de1fee572713a6be",
      "projectPage": "https://avocado-captioner.github.io/",
      "githubRepo": "https://github.com/AVoCaDO-Captioner/AVoCaDO",
      "ai_summary": "AVoCaDO, an audiovisual video captioner, enhances temporal coherence and dialogue accuracy through a two-stage post-training pipeline, outperforming existing models across multiple benchmarks.",
      "ai_keywords": [
        "audiovisual video captioning",
        "temporal orchestration",
        "AVoCaDO",
        "AVoCaDO SFT",
        "AVoCaDO GRPO",
        "reward functions",
        "temporal coherence",
        "dialogue accuracy",
        "caption length",
        "collapse",
        "VDC",
        "DREAM-1K"
      ],
      "githubStars": 1
    },
    "publishedAt": "2025-10-11T21:20:22.000Z",
    "title": "AVoCaDO: An Audiovisual Video Captioner Driven by Temporal Orchestration",
    "summary": "Audiovisual video captioning aims to generate semantically rich descriptions\nwith temporal alignment between visual and auditory events, thereby benefiting\nboth video understanding and generation. In this paper, we present AVoCaDO, a\npowerful audiovisual video captioner driven by the temporal orchestration\nbetween audio and visual modalities. We propose a two-stage post-training\npipeline: (1) AVoCaDO SFT, which fine-tunes the model on a newly curated\ndataset of 107K high-quality, temporally-aligned audiovisual captions; and (2)\nAVoCaDO GRPO, which leverages tailored reward functions to further enhance\ntemporal coherence and dialogue accuracy while regularizing caption length and\nreducing collapse. Experimental results demonstrate that AVoCaDO significantly\noutperforms existing open-source models across four audiovisual video\ncaptioning benchmarks, and also achieves competitive performance on the VDC and\nDREAM-1K benchmark under visual-only settings.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.10395.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "673c7319d11b1c2e246ead9c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/673c7319d11b1c2e246ead9c/IjFIO--N7Hm_BOEafhEQv.jpeg",
      "fullname": "Yang Shi",
      "name": "DogNeverSleep",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.11712",
      "authors": [
        {
          "_id": "68edbf46de1fee572713a874",
          "user": {
            "_id": "65240d0ca801972b6eb12ed8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65240d0ca801972b6eb12ed8/hl2RAssBperb5JlgOIDvw.jpeg",
            "isPro": false,
            "fullname": "Haoran Feng",
            "user": "fenghora",
            "type": "user"
          },
          "name": "Haoran Feng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-14T07:30:40.094Z",
          "hidden": false
        },
        {
          "_id": "68edbf46de1fee572713a875",
          "name": "Dizhe Zhang",
          "hidden": false
        },
        {
          "_id": "68edbf46de1fee572713a876",
          "name": "Xiangtai Li",
          "hidden": false
        },
        {
          "_id": "68edbf46de1fee572713a877",
          "name": "Bo Du",
          "hidden": false
        },
        {
          "_id": "68edbf46de1fee572713a878",
          "name": "Lu Qi",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/65240d0ca801972b6eb12ed8/zv8-Q8Pb9lHDorRizvX94.mp4"
      ],
      "publishedAt": "2025-10-13T17:59:15.000Z",
      "submittedOnDailyAt": "2025-10-14T01:58:07.411Z",
      "title": "DiT360: High-Fidelity Panoramic Image Generation via Hybrid Training",
      "submittedOnDailyBy": {
        "_id": "65240d0ca801972b6eb12ed8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65240d0ca801972b6eb12ed8/hl2RAssBperb5JlgOIDvw.jpeg",
        "isPro": false,
        "fullname": "Haoran Feng",
        "user": "fenghora",
        "type": "user"
      },
      "summary": "In this work, we propose DiT360, a DiT-based framework that performs hybrid\ntraining on perspective and panoramic data for panoramic image generation. For\nthe issues of maintaining geometric fidelity and photorealism in generation\nquality, we attribute the main reason to the lack of large-scale, high-quality,\nreal-world panoramic data, where such a data-centric view differs from prior\nmethods that focus on model design. Basically, DiT360 has several key modules\nfor inter-domain transformation and intra-domain augmentation, applied at both\nthe pre-VAE image level and the post-VAE token level. At the image level, we\nincorporate cross-domain knowledge through perspective image guidance and\npanoramic refinement, which enhance perceptual quality while regularizing\ndiversity and photorealism. At the token level, hybrid supervision is applied\nacross multiple modules, which include circular padding for boundary\ncontinuity, yaw loss for rotational robustness, and cube loss for distortion\nawareness. Extensive experiments on text-to-panorama, inpainting, and\noutpainting tasks demonstrate that our method achieves better boundary\nconsistency and image fidelity across eleven quantitative metrics. Our code is\navailable at https://github.com/Insta360-Research-Team/DiT360.",
      "upvotes": 21,
      "discussionId": "68edbf46de1fee572713a879",
      "projectPage": "https://fenghora.github.io/DiT360-Page/",
      "githubRepo": "https://github.com/Insta360-Research-Team/DiT360",
      "ai_summary": "DiT360 framework enhances panoramic image generation by hybrid training on perspective and panoramic data, incorporating cross-domain knowledge and hybrid supervision to improve boundary consistency and image fidelity.",
      "ai_keywords": [
        "DiT",
        "hybrid training",
        "perspective data",
        "panoramic data",
        "geometric fidelity",
        "photorealism",
        "pre-VAE",
        "post-VAE",
        "cross-domain knowledge",
        "perspective image guidance",
        "panoramic refinement",
        "perceptual quality",
        "diversity",
        "circular padding",
        "yaw loss",
        "cube loss",
        "text-to-panorama",
        "inpainting",
        "outpainting"
      ],
      "githubStars": 37
    },
    "publishedAt": "2025-10-13T13:59:15.000Z",
    "title": "DiT360: High-Fidelity Panoramic Image Generation via Hybrid Training",
    "summary": "In this work, we propose DiT360, a DiT-based framework that performs hybrid\ntraining on perspective and panoramic data for panoramic image generation. For\nthe issues of maintaining geometric fidelity and photorealism in generation\nquality, we attribute the main reason to the lack of large-scale, high-quality,\nreal-world panoramic data, where such a data-centric view differs from prior\nmethods that focus on model design. Basically, DiT360 has several key modules\nfor inter-domain transformation and intra-domain augmentation, applied at both\nthe pre-VAE image level and the post-VAE token level. At the image level, we\nincorporate cross-domain knowledge through perspective image guidance and\npanoramic refinement, which enhance perceptual quality while regularizing\ndiversity and photorealism. At the token level, hybrid supervision is applied\nacross multiple modules, which include circular padding for boundary\ncontinuity, yaw loss for rotational robustness, and cube loss for distortion\nawareness. Extensive experiments on text-to-panorama, inpainting, and\noutpainting tasks demonstrate that our method achieves better boundary\nconsistency and image fidelity across eleven quantitative metrics. Our code is\navailable at https://github.com/Insta360-Research-Team/DiT360.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/65240d0ca801972b6eb12ed8/zv8-Q8Pb9lHDorRizvX94.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.11712.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65240d0ca801972b6eb12ed8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65240d0ca801972b6eb12ed8/hl2RAssBperb5JlgOIDvw.jpeg",
      "fullname": "Haoran Feng",
      "name": "fenghora",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.10201",
      "authors": [
        {
          "_id": "68edab32de1fee572713a6eb",
          "user": {
            "_id": "64673258fc6f6da8b119cab8",
            "avatarUrl": "/avatars/36e025862984c7a86b97cee750ee2d04.svg",
            "isPro": false,
            "fullname": "SII-Jhao Zhang",
            "user": "JingHaoZ",
            "type": "user"
          },
          "name": "Jinghao Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-14T07:31:05.248Z",
          "hidden": false
        },
        {
          "_id": "68edab32de1fee572713a6ec",
          "name": "Naishan Zheng",
          "hidden": false
        },
        {
          "_id": "68edab32de1fee572713a6ed",
          "name": "Ruilin Li",
          "hidden": false
        },
        {
          "_id": "68edab32de1fee572713a6ee",
          "name": "Dongzhou Cheng",
          "hidden": false
        },
        {
          "_id": "68edab32de1fee572713a6ef",
          "name": "Zheming Liang",
          "hidden": false
        },
        {
          "_id": "68edab32de1fee572713a6f0",
          "name": "Feng Zhao",
          "hidden": false
        },
        {
          "_id": "68edab32de1fee572713a6f1",
          "name": "Jiaqi Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-11T13:00:25.000Z",
      "submittedOnDailyAt": "2025-10-14T02:17:56.665Z",
      "title": "RLFR: Extending Reinforcement Learning for LLMs with Flow Environment",
      "submittedOnDailyBy": {
        "_id": "64673258fc6f6da8b119cab8",
        "avatarUrl": "/avatars/36e025862984c7a86b97cee750ee2d04.svg",
        "isPro": false,
        "fullname": "SII-Jhao Zhang",
        "user": "JingHaoZ",
        "type": "user"
      },
      "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as\na promising framework for improving reasoning abilities in Large Language\nModels (LLMs). However, policy optimized with binary verification prone to\noverlook potential valuable exploration in reasoning trajectory. In view of\nheavy annotation cost of golden Process Reward Models (PRMs), recent works\nattempt using auxiliary signals for reward shaping of process tokens, involving\nentropy and likelihood collected from logit space. In this work, we offer a\nnovel perspective on shaping RLVR with flow rewards derived from latent space,\nand propose RLFR, where the flow fields of model latents are constructed from\neither off-policy high-quality data and on-policy rejection sampling data, and\nthe velocity deviations of policy latents within it are quantified to serve as\na reward signal. RLFR first demonstrates that a well-established flow field can\nbe a sound environment for reward signal collection, highlighting the\nexpressive latent space is much underexplored. Moreover, RLFR is able to\ncompress any off-policy expert data as reference for constituting reward\nsignals, and we show that the efficient context dependence compressed within\nthe hidden states are utilized, rather than individual token-level denotation\nfor context comprehending. Experiments on both language and multimodal\nreasoning benchmarks demonstrate the reliability of flow rewards, and\nsuggesting a promising paradigm for reward shaping with auxiliary signals.",
      "upvotes": 21,
      "discussionId": "68edab32de1fee572713a6f2",
      "projectPage": "https://jinghaoleven.github.io/RLFR/",
      "githubRepo": "https://github.com/Jinghaoleven/RLFR",
      "ai_summary": "RLFR uses flow rewards derived from latent space to improve reinforcement learning with verifiable rewards, demonstrating reliable reward shaping and efficient context comprehension.",
      "ai_keywords": [
        "Reinforcement Learning with Verifiable Rewards",
        "RLVR",
        "Large Language Models",
        "LLMs",
        "Process Reward Models",
        "PRMs",
        "reward shaping",
        "process tokens",
        "logit space",
        "flow rewards",
        "latent space",
        "flow fields",
        "off-policy data",
        "on-policy rejection sampling",
        "velocity deviations",
        "hidden states",
        "context comprehension",
        "language benchmarks",
        "multimodal reasoning benchmarks"
      ],
      "githubStars": 25
    },
    "publishedAt": "2025-10-11T09:00:25.000Z",
    "title": "RLFR: Extending Reinforcement Learning for LLMs with Flow Environment",
    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as\na promising framework for improving reasoning abilities in Large Language\nModels (LLMs). However, policy optimized with binary verification prone to\noverlook potential valuable exploration in reasoning trajectory. In view of\nheavy annotation cost of golden Process Reward Models (PRMs), recent works\nattempt using auxiliary signals for reward shaping of process tokens, involving\nentropy and likelihood collected from logit space. In this work, we offer a\nnovel perspective on shaping RLVR with flow rewards derived from latent space,\nand propose RLFR, where the flow fields of model latents are constructed from\neither off-policy high-quality data and on-policy rejection sampling data, and\nthe velocity deviations of policy latents within it are quantified to serve as\na reward signal. RLFR first demonstrates that a well-established flow field can\nbe a sound environment for reward signal collection, highlighting the\nexpressive latent space is much underexplored. Moreover, RLFR is able to\ncompress any off-policy expert data as reference for constituting reward\nsignals, and we show that the efficient context dependence compressed within\nthe hidden states are utilized, rather than individual token-level denotation\nfor context comprehending. Experiments on both language and multimodal\nreasoning benchmarks demonstrate the reliability of flow rewards, and\nsuggesting a promising paradigm for reward shaping with auxiliary signals.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.10201.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64673258fc6f6da8b119cab8",
      "avatarUrl": "/avatars/36e025862984c7a86b97cee750ee2d04.svg",
      "fullname": "SII-Jhao Zhang",
      "name": "JingHaoZ",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.04617",
      "authors": [
        {
          "_id": "68e5159bf9af2f6567eab906",
          "user": {
            "_id": "643525ea0b30bd434ea15363",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643525ea0b30bd434ea15363/7sAzllfWUPtt68NY1gDLj.png",
            "isPro": false,
            "fullname": "Jackie Lai",
            "user": "DreamW1ngs",
            "type": "user"
          },
          "name": "Zhejian Lai",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-08T08:01:46.085Z",
          "hidden": false
        },
        {
          "_id": "68e5159bf9af2f6567eab907",
          "name": "Xiang Geng",
          "hidden": false
        },
        {
          "_id": "68e5159bf9af2f6567eab908",
          "name": "Zhijun Wang",
          "hidden": false
        },
        {
          "_id": "68e5159bf9af2f6567eab909",
          "name": "Yang Bai",
          "hidden": false
        },
        {
          "_id": "68e5159bf9af2f6567eab90a",
          "name": "Jiahuan Li",
          "hidden": false
        },
        {
          "_id": "68e5159bf9af2f6567eab90b",
          "name": "Rongxiang Weng",
          "hidden": false
        },
        {
          "_id": "68e5159bf9af2f6567eab90c",
          "name": "Jingang Wang",
          "hidden": false
        },
        {
          "_id": "68e5159bf9af2f6567eab90d",
          "name": "Xuezhi Cao",
          "hidden": false
        },
        {
          "_id": "68e5159bf9af2f6567eab90e",
          "name": "Xunliang Cai",
          "hidden": false
        },
        {
          "_id": "68e5159bf9af2f6567eab90f",
          "name": "Shujian Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-06T09:30:05.000Z",
      "submittedOnDailyAt": "2025-10-14T00:44:28.428Z",
      "title": "Making Mathematical Reasoning Adaptive",
      "submittedOnDailyBy": {
        "_id": "643525ea0b30bd434ea15363",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643525ea0b30bd434ea15363/7sAzllfWUPtt68NY1gDLj.png",
        "isPro": false,
        "fullname": "Jackie Lai",
        "user": "DreamW1ngs",
        "type": "user"
      },
      "summary": "Mathematical reasoning is a primary indicator of large language models (LLMs)\nintelligence. However, existing LLMs exhibit failures of robustness and\ngeneralization. This paper attributes these deficiencies to spurious reasoning,\ni.e., producing answers from superficial features. To address this challenge,\nwe propose the AdaR framework to enable adaptive reasoning, wherein models rely\non problem-solving logic to produce answers. AdaR synthesizes logically\nequivalent queries by varying variable values, and trains models with RLVR on\nthese data to penalize spurious logic while encouraging adaptive logic. To\nimprove data quality, we extract the problem-solving logic from the original\nquery and generate the corresponding answer by code execution, then apply a\nsanity check. Experimental results demonstrate that AdaR improves robustness\nand generalization, achieving substantial improvement in mathematical reasoning\nwhile maintaining high data efficiency. Analysis indicates that data synthesis\nand RLVR function in a coordinated manner to enable adaptive reasoning in LLMs.\nSubsequent analyses derive key design insights into the effect of critical\nfactors and the applicability to instruct LLMs. Our project is available at\nhttps://github.com/LaiZhejian/AdaR",
      "upvotes": 19,
      "discussionId": "68e5163ff9af2f6567eab910",
      "githubRepo": "https://github.com/LaiZhejian/AdaR",
      "ai_summary": "AdaR framework enhances LLMs' robustness and generalization in mathematical reasoning by synthesizing logically equivalent queries and using RLVR to penalize spurious logic.",
      "ai_keywords": [
        "AdaR",
        "adaptive reasoning",
        "spurious reasoning",
        "logically equivalent queries",
        "RLVR",
        "data synthesis",
        "mathematical reasoning",
        "data efficiency"
      ],
      "githubStars": 10
    },
    "publishedAt": "2025-10-06T05:30:05.000Z",
    "title": "Making Mathematical Reasoning Adaptive",
    "summary": "Mathematical reasoning is a primary indicator of large language models (LLMs)\nintelligence. However, existing LLMs exhibit failures of robustness and\ngeneralization. This paper attributes these deficiencies to spurious reasoning,\ni.e., producing answers from superficial features. To address this challenge,\nwe propose the AdaR framework to enable adaptive reasoning, wherein models rely\non problem-solving logic to produce answers. AdaR synthesizes logically\nequivalent queries by varying variable values, and trains models with RLVR on\nthese data to penalize spurious logic while encouraging adaptive logic. To\nimprove data quality, we extract the problem-solving logic from the original\nquery and generate the corresponding answer by code execution, then apply a\nsanity check. Experimental results demonstrate that AdaR improves robustness\nand generalization, achieving substantial improvement in mathematical reasoning\nwhile maintaining high data efficiency. Analysis indicates that data synthesis\nand RLVR function in a coordinated manner to enable adaptive reasoning in LLMs.\nSubsequent analyses derive key design insights into the effect of critical\nfactors and the applicability to instruct LLMs. Our project is available at\nhttps://github.com/LaiZhejian/AdaR",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.04617.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643525ea0b30bd434ea15363",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643525ea0b30bd434ea15363/7sAzllfWUPtt68NY1gDLj.png",
      "fullname": "Jackie Lai",
      "name": "DreamW1ngs",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.11701",
      "authors": [
        {
          "_id": "68edb4e8de1fee572713a782",
          "name": "Zhaochen Yu",
          "hidden": false
        },
        {
          "_id": "68edb4e8de1fee572713a783",
          "name": "Ling Yang",
          "hidden": false
        },
        {
          "_id": "68edb4e8de1fee572713a784",
          "name": "Jiaru Zou",
          "hidden": false
        },
        {
          "_id": "68edb4e8de1fee572713a785",
          "name": "Shuicheng Yan",
          "hidden": false
        },
        {
          "_id": "68edb4e8de1fee572713a786",
          "name": "Mengdi Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-13T17:57:15.000Z",
      "submittedOnDailyAt": "2025-10-14T01:06:40.588Z",
      "title": "Demystifying Reinforcement Learning in Agentic Reasoning",
      "submittedOnDailyBy": {
        "_id": "64fde4e252e82dd432b74ce9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64fde4e252e82dd432b74ce9/-CQZbBP7FsPPyawYrsi4z.jpeg",
        "isPro": false,
        "fullname": "Ling Yang",
        "user": "Lingaaaaaaa",
        "type": "user"
      },
      "summary": "Recently, the emergence of agentic RL has showcased that RL could also\neffectively improve the agentic reasoning ability of LLMs, yet the key design\nprinciples and optimal practices remain unclear. In this work, we conduct a\ncomprehensive and systematic investigation to demystify reinforcement learning\nin agentic reasoning from three key perspectives: data, algorithm, and\nreasoning mode. We highlight our key insights: (i) Replacing stitched synthetic\ntrajectories with real end-to-end tool-use trajectories yields a far stronger\nSFT initialization; high-diversity, model-aware datasets sustain exploration\nand markedly improve RL performance. (ii) Exploration-friendly techniques are\ncrucial for agentic RL, such as clip higher, overlong reward shaping, and\nmaintaining adequate policy entropy could improve the training efficiency.\n(iii) A deliberative strategy with fewer tool calls outperforms frequent tool\ncalls or verbose self-reasoning, improving tool efficiency and final accuracy.\nTogether, these simple practices consistently enhance agentic reasoning and\ntraining efficiency, achieving strong results on challenging benchmarks with\nsmaller models, and establishing a practical baseline for future agentic RL\nresearch. Beyond these empirical insights, we further contribute a\nhigh-quality, real end-to-end agentic SFT dataset along with a high-quality RL\ndataset, and demonstrate the effectiveness of our insights in boosting the\nagentic reasoning ability of LLMs across four challenging benchmarks, including\nAIME2024/AIME2025, GPQA-Diamond, and LiveCodeBench-v6. With our recipes,\n4B-sized models could also achieve superior agentic reasoning performance\ncompared to 32B-sized models. Code and models:\nhttps://github.com/Gen-Verse/Open-AgentRL",
      "upvotes": 18,
      "discussionId": "68edb4e8de1fee572713a787",
      "projectPage": "https://huggingface.co/collections/Gen-Verse/open-agentrl-68eda4c05755ca5a8c663656",
      "githubRepo": "https://github.com/Gen-Verse/Open-AgentRL",
      "ai_summary": "Agentic reinforcement learning enhances LLMs' reasoning ability through real datasets, exploration techniques, and a deliberative strategy, achieving strong performance with smaller models.",
      "ai_keywords": [
        "agentic RL",
        "SFT initialization",
        "real end-to-end tool-use trajectories",
        "high-diversity datasets",
        "model-aware datasets",
        "exploration-friendly techniques",
        "clip higher",
        "overlong reward shaping",
        "policy entropy",
        "deliberative strategy",
        "agentic SFT dataset",
        "RL dataset",
        "AIME2024/AIME2025",
        "GPQA-Diamond",
        "LiveCodeBench-v6"
      ],
      "githubStars": 6
    },
    "publishedAt": "2025-10-13T13:57:15.000Z",
    "title": "Demystifying Reinforcement Learning in Agentic Reasoning",
    "summary": "Recently, the emergence of agentic RL has showcased that RL could also\neffectively improve the agentic reasoning ability of LLMs, yet the key design\nprinciples and optimal practices remain unclear. In this work, we conduct a\ncomprehensive and systematic investigation to demystify reinforcement learning\nin agentic reasoning from three key perspectives: data, algorithm, and\nreasoning mode. We highlight our key insights: (i) Replacing stitched synthetic\ntrajectories with real end-to-end tool-use trajectories yields a far stronger\nSFT initialization; high-diversity, model-aware datasets sustain exploration\nand markedly improve RL performance. (ii) Exploration-friendly techniques are\ncrucial for agentic RL, such as clip higher, overlong reward shaping, and\nmaintaining adequate policy entropy could improve the training efficiency.\n(iii) A deliberative strategy with fewer tool calls outperforms frequent tool\ncalls or verbose self-reasoning, improving tool efficiency and final accuracy.\nTogether, these simple practices consistently enhance agentic reasoning and\ntraining efficiency, achieving strong results on challenging benchmarks with\nsmaller models, and establishing a practical baseline for future agentic RL\nresearch. Beyond these empirical insights, we further contribute a\nhigh-quality, real end-to-end agentic SFT dataset along with a high-quality RL\ndataset, and demonstrate the effectiveness of our insights in boosting the\nagentic reasoning ability of LLMs across four challenging benchmarks, including\nAIME2024/AIME2025, GPQA-Diamond, and LiveCodeBench-v6. With our recipes,\n4B-sized models could also achieve superior agentic reasoning performance\ncompared to 32B-sized models. Code and models:\nhttps://github.com/Gen-Verse/Open-AgentRL",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.11701.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64fde4e252e82dd432b74ce9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64fde4e252e82dd432b74ce9/-CQZbBP7FsPPyawYrsi4z.jpeg",
      "fullname": "Ling Yang",
      "name": "Lingaaaaaaa",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.09781",
      "authors": [
        {
          "_id": "68edcbffde1fee572713a927",
          "name": "Yue Huang",
          "hidden": false
        },
        {
          "_id": "68edcbffde1fee572713a928",
          "user": {
            "_id": "639f8277beb95d698de007dd",
            "avatarUrl": "/avatars/57f223ccd9d3cb03166ccf0e41361c58.svg",
            "isPro": false,
            "fullname": "HangHua",
            "user": "hhua2",
            "type": "user"
          },
          "name": "Hang Hua",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-14T07:29:51.627Z",
          "hidden": false
        },
        {
          "_id": "68edcbffde1fee572713a929",
          "name": "Yujun Zhou",
          "hidden": false
        },
        {
          "_id": "68edcbffde1fee572713a92a",
          "name": "Pengcheng Jing",
          "hidden": false
        },
        {
          "_id": "68edcbffde1fee572713a92b",
          "name": "Manish Nagireddy",
          "hidden": false
        },
        {
          "_id": "68edcbffde1fee572713a92c",
          "name": "Inkit Padhi",
          "hidden": false
        },
        {
          "_id": "68edcbffde1fee572713a92d",
          "name": "Greta Dolcetti",
          "hidden": false
        },
        {
          "_id": "68edcbffde1fee572713a92e",
          "name": "Zhangchen Xu",
          "hidden": false
        },
        {
          "_id": "68edcbffde1fee572713a92f",
          "name": "Subhajit Chaudhury",
          "hidden": false
        },
        {
          "_id": "68edcbffde1fee572713a930",
          "name": "Ambrish Rawat",
          "hidden": false
        },
        {
          "_id": "68edcbffde1fee572713a931",
          "name": "Liubov Nedoshivina",
          "hidden": false
        },
        {
          "_id": "68edcbffde1fee572713a932",
          "name": "Pin-Yu Chen",
          "hidden": false
        },
        {
          "_id": "68edcbffde1fee572713a933",
          "name": "Prasanna Sattigeri",
          "hidden": false
        },
        {
          "_id": "68edcbffde1fee572713a934",
          "name": "Xiangliang Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-10T18:42:32.000Z",
      "submittedOnDailyAt": "2025-10-14T02:36:39.228Z",
      "title": "Building a Foundational Guardrail for General Agentic Systems via\n  Synthetic Data",
      "submittedOnDailyBy": {
        "_id": "639d94ab7145123e0d44e48a",
        "avatarUrl": "/avatars/5bb6a65b306d1383c4a8bcd9334b470a.svg",
        "isPro": false,
        "fullname": "Yue Huang",
        "user": "HowieHwong",
        "type": "user"
      },
      "summary": "While LLM agents can plan multi-step tasks, intervening at the planning\nstage-before any action is executed-is often the safest way to prevent harm,\nsince certain risks can lead to severe consequences once carried out. However,\nexisting guardrails mostly operate post-execution, which is difficult to scale\nand leaves little room for controllable supervision at the plan level. To\naddress this challenge, we highlight three critical gaps in current research:\ndata gap, model gap, and evaluation gap. To close the data gap, we introduce\nAuraGen, a controllable engine that (i) synthesizes benign trajectories, (ii)\ninjects category-labeled risks with calibrated difficulty, and (iii) filters\noutputs via an automated reward model, producing large and reliable corpora for\npre-execution safety. To close the guardian model gap, we propose a\nfoundational guardrail Safiron, combining a cross-planner adapter with a\ncompact guardian model. The adapter unifies different input formats, while\nSafiron flags risky cases, assigns risk types, and generates rationales;\ntrained in two stages with a broadly explored data recipe, Safiron achieves\nrobust transfer across settings. To close the evaluation gap, we release\nPre-Exec Bench, a realistic benchmark covering diverse tools and branching\ntrajectories, which measures detection, fine-grained categorization,\nexplanation, and cross-planner generalization in human-verified scenarios.\nExtensive experiments demonstrate consistent gains of the proposed guardrail\nover strong baselines on Pre-Exec Bench, and ablations further distill\nactionable practices, providing a practical template for safer agentic systems.",
      "upvotes": 16,
      "discussionId": "68edcbffde1fee572713a935",
      "githubRepo": "https://github.com/HowieHwong/Agentic-Guardian",
      "ai_summary": "AuraGen and Safiron address pre-execution safety gaps in LLM agents by synthesizing benign trajectories, injecting risks, and using a cross-planner adapter for robust risk detection and explanation.",
      "ai_keywords": [
        "AuraGen",
        "Safiron",
        "cross-planner adapter",
        "compact guardian model",
        "automated reward model",
        "Pre-Exec Bench",
        "risk detection",
        "risk categorization",
        "explanation",
        "cross-planner generalization"
      ],
      "githubStars": 20
    },
    "publishedAt": "2025-10-10T14:42:32.000Z",
    "title": "Building a Foundational Guardrail for General Agentic Systems via\n  Synthetic Data",
    "summary": "While LLM agents can plan multi-step tasks, intervening at the planning\nstage-before any action is executed-is often the safest way to prevent harm,\nsince certain risks can lead to severe consequences once carried out. However,\nexisting guardrails mostly operate post-execution, which is difficult to scale\nand leaves little room for controllable supervision at the plan level. To\naddress this challenge, we highlight three critical gaps in current research:\ndata gap, model gap, and evaluation gap. To close the data gap, we introduce\nAuraGen, a controllable engine that (i) synthesizes benign trajectories, (ii)\ninjects category-labeled risks with calibrated difficulty, and (iii) filters\noutputs via an automated reward model, producing large and reliable corpora for\npre-execution safety. To close the guardian model gap, we propose a\nfoundational guardrail Safiron, combining a cross-planner adapter with a\ncompact guardian model. The adapter unifies different input formats, while\nSafiron flags risky cases, assigns risk types, and generates rationales;\ntrained in two stages with a broadly explored data recipe, Safiron achieves\nrobust transfer across settings. To close the evaluation gap, we release\nPre-Exec Bench, a realistic benchmark covering diverse tools and branching\ntrajectories, which measures detection, fine-grained categorization,\nexplanation, and cross-planner generalization in human-verified scenarios.\nExtensive experiments demonstrate consistent gains of the proposed guardrail\nover strong baselines on Pre-Exec Bench, and ablations further distill\nactionable practices, providing a practical template for safer agentic systems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.09781.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "639d94ab7145123e0d44e48a",
      "avatarUrl": "/avatars/5bb6a65b306d1383c4a8bcd9334b470a.svg",
      "fullname": "Yue Huang",
      "name": "HowieHwong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.11652",
      "authors": [
        {
          "_id": "68edd775de1fee572713a97f",
          "user": {
            "_id": "67dbf07f9d821d38905d145d",
            "avatarUrl": "/avatars/e806467d2c0f4f642c8d4906b0855817.svg",
            "isPro": false,
            "fullname": "guixin",
            "user": "Ross12",
            "type": "user"
          },
          "name": "Xin Gui",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-14T07:29:37.509Z",
          "hidden": false
        },
        {
          "_id": "68edd775de1fee572713a980",
          "user": {
            "_id": "6578265ddea7e2122d02f6ba",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6578265ddea7e2122d02f6ba/Bh6JjoVF5ceLSjV7Z7nTk.jpeg",
            "isPro": false,
            "fullname": "king zhu",
            "user": "kangz",
            "type": "user"
          },
          "name": "King Zhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-14T07:29:32.982Z",
          "hidden": false
        },
        {
          "_id": "68edd775de1fee572713a981",
          "user": {
            "_id": "6704ee27386892c420db1938",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6704ee27386892c420db1938/lb5mtEwYhn47RawkynYPs.jpeg",
            "isPro": false,
            "fullname": "JinCheng Ren",
            "user": "JinChengRen",
            "type": "user"
          },
          "name": "JinCheng Ren",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-14T07:29:35.249Z",
          "hidden": false
        },
        {
          "_id": "68edd775de1fee572713a982",
          "name": "Qianben Chen",
          "hidden": false
        },
        {
          "_id": "68edd775de1fee572713a983",
          "name": "Zekun Moore Wang",
          "hidden": false
        },
        {
          "_id": "68edd775de1fee572713a984",
          "name": "Yizhi LI",
          "hidden": false
        },
        {
          "_id": "68edd775de1fee572713a985",
          "name": "Xinpeng Liu",
          "hidden": false
        },
        {
          "_id": "68edd775de1fee572713a986",
          "name": "Xiaowan Li",
          "hidden": false
        },
        {
          "_id": "68edd775de1fee572713a987",
          "name": "Wenli Ren",
          "hidden": false
        },
        {
          "_id": "68edd775de1fee572713a988",
          "name": "Linyu Miao",
          "hidden": false
        },
        {
          "_id": "68edd775de1fee572713a989",
          "name": "Tianrui Qin",
          "hidden": false
        },
        {
          "_id": "68edd775de1fee572713a98a",
          "name": "Ziqi Shu",
          "hidden": false
        },
        {
          "_id": "68edd775de1fee572713a98b",
          "name": "He Zhu",
          "hidden": false
        },
        {
          "_id": "68edd775de1fee572713a98c",
          "name": "Xiangru Tang",
          "hidden": false
        },
        {
          "_id": "68edd775de1fee572713a98d",
          "name": "Dingfeng Shi",
          "hidden": false
        },
        {
          "_id": "68edd775de1fee572713a98e",
          "name": "Jiaheng Liu",
          "hidden": false
        },
        {
          "_id": "68edd775de1fee572713a98f",
          "name": "Yuchen Eleanor Jiang",
          "hidden": false
        },
        {
          "_id": "68edd775de1fee572713a990",
          "name": "Minghao Liu",
          "hidden": false
        },
        {
          "_id": "68edd775de1fee572713a991",
          "name": "Ge Zhang",
          "hidden": false
        },
        {
          "_id": "68edd775de1fee572713a992",
          "name": "Wangchunshu Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-13T17:30:36.000Z",
      "submittedOnDailyAt": "2025-10-14T04:51:41.289Z",
      "title": "ACADREASON: Exploring the Limits of Reasoning Models with Academic\n  Research Problems",
      "submittedOnDailyBy": {
        "_id": "6704ee27386892c420db1938",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6704ee27386892c420db1938/lb5mtEwYhn47RawkynYPs.jpeg",
        "isPro": false,
        "fullname": "JinCheng Ren",
        "user": "JinChengRen",
        "type": "user"
      },
      "summary": "In recent years, the research focus of large language models (LLMs) and\nagents has shifted increasingly from demonstrating novel capabilities to\ncomplex reasoning and tackling challenging tasks. However, existing evaluations\nfocus mainly on math/code contests or general tasks, while existing\nmulti-domain academic benchmarks lack sufficient reasoning depth, leaving the\nfield without a rigorous benchmark for high-level reasoning. To fill this gap,\nwe introduce the Acadreason benchmark, designed to evaluate the ability of LLMs\nand agents to acquire and reason over academic knowledge. It consists of 50\nexpert-annotated academic problems across five high-reasoning domains,\nincluding computer science, economics, law, mathematics, and philosophy. All\nquestions are sourced from top-tier publications in recent years and undergo\nrigorous annotation and quality control to ensure they are both challenging and\nanswerable. We conduct systematic evaluations of over 10 mainstream LLMs and\nagents. The results show that most LLMs scored below 20 points, with even the\ncutting-edge GPT-5 achieving only 16 points. While agents achieved higher\nscores, none exceeded 40 points. This demonstrates the current capability gap\nbetween LLMs and agents in super-intelligent academic research tasks and\nhighlights the challenges of Acadreason.",
      "upvotes": 15,
      "discussionId": "68edd776de1fee572713a993",
      "ai_summary": "The Acadreason benchmark evaluates LLMs and agents on high-level academic reasoning across multiple domains, revealing significant capability gaps.",
      "ai_keywords": [
        "large language models",
        "agents",
        "Acadreason benchmark",
        "academic knowledge",
        "computer science",
        "economics",
        "law",
        "mathematics",
        "philosophy",
        "top-tier publications",
        "systematic evaluations",
        "GPT-5"
      ],
      "organization": {
        "_id": "684a463d17db6e9f271a0b66",
        "name": "PersonalAILab",
        "fullname": "OPPO-Personal-AI-Lab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/632bfaebea6e62428ab0e9c2/P5L4TzWg2d4NXntonI-fK.png"
      }
    },
    "publishedAt": "2025-10-13T13:30:36.000Z",
    "title": "ACADREASON: Exploring the Limits of Reasoning Models with Academic\n  Research Problems",
    "summary": "In recent years, the research focus of large language models (LLMs) and\nagents has shifted increasingly from demonstrating novel capabilities to\ncomplex reasoning and tackling challenging tasks. However, existing evaluations\nfocus mainly on math/code contests or general tasks, while existing\nmulti-domain academic benchmarks lack sufficient reasoning depth, leaving the\nfield without a rigorous benchmark for high-level reasoning. To fill this gap,\nwe introduce the Acadreason benchmark, designed to evaluate the ability of LLMs\nand agents to acquire and reason over academic knowledge. It consists of 50\nexpert-annotated academic problems across five high-reasoning domains,\nincluding computer science, economics, law, mathematics, and philosophy. All\nquestions are sourced from top-tier publications in recent years and undergo\nrigorous annotation and quality control to ensure they are both challenging and\nanswerable. We conduct systematic evaluations of over 10 mainstream LLMs and\nagents. The results show that most LLMs scored below 20 points, with even the\ncutting-edge GPT-5 achieving only 16 points. While agents achieved higher\nscores, none exceeded 40 points. This demonstrates the current capability gap\nbetween LLMs and agents in super-intelligent academic research tasks and\nhighlights the challenges of Acadreason.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.11652.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6704ee27386892c420db1938",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6704ee27386892c420db1938/lb5mtEwYhn47RawkynYPs.jpeg",
      "fullname": "JinCheng Ren",
      "name": "JinChengRen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "organization": {
      "_id": "684a463d17db6e9f271a0b66",
      "name": "PersonalAILab",
      "fullname": "OPPO-Personal-AI-Lab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/632bfaebea6e62428ab0e9c2/P5L4TzWg2d4NXntonI-fK.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.10670",
      "authors": [
        {
          "_id": "68edaa98de1fee572713a6d8",
          "name": "Yu Li",
          "hidden": false
        },
        {
          "_id": "68edaa98de1fee572713a6d9",
          "name": "Menghan Xia",
          "hidden": false
        },
        {
          "_id": "68edaa98de1fee572713a6da",
          "name": "Gongye Liu",
          "hidden": false
        },
        {
          "_id": "68edaa98de1fee572713a6db",
          "name": "Jianhong Bai",
          "hidden": false
        },
        {
          "_id": "68edaa98de1fee572713a6dc",
          "name": "Xintao Wang",
          "hidden": false
        },
        {
          "_id": "68edaa98de1fee572713a6dd",
          "name": "Conglang Zhang",
          "hidden": false
        },
        {
          "_id": "68edaa98de1fee572713a6de",
          "name": "Yuxuan Lin",
          "hidden": false
        },
        {
          "_id": "68edaa98de1fee572713a6df",
          "name": "Ruihang Chu",
          "hidden": false
        },
        {
          "_id": "68edaa98de1fee572713a6e0",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "68edaa98de1fee572713a6e1",
          "name": "Yujiu Yang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/651ed7ef755e92f7f12742e6/E-2LhC-CbFdLdojYWTqLg.png"
      ],
      "publishedAt": "2025-10-12T15:55:44.000Z",
      "submittedOnDailyAt": "2025-10-14T00:20:52.273Z",
      "title": "AdaViewPlanner: Adapting Video Diffusion Models for Viewpoint Planning\n  in 4D Scenes",
      "submittedOnDailyBy": {
        "_id": "651ed7ef755e92f7f12742e6",
        "avatarUrl": "/avatars/57a9cc189b4a59299aad6c96191b18d8.svg",
        "isPro": false,
        "fullname": "yu li",
        "user": "lyabc",
        "type": "user"
      },
      "summary": "Recent Text-to-Video (T2V) models have demonstrated powerful capability in\nvisual simulation of real-world geometry and physical laws, indicating its\npotential as implicit world models. Inspired by this, we explore the\nfeasibility of leveraging the video generation prior for viewpoint planning\nfrom given 4D scenes, since videos internally accompany dynamic scenes with\nnatural viewpoints. To this end, we propose a two-stage paradigm to adapt\npre-trained T2V models for viewpoint prediction, in a compatible manner. First,\nwe inject the 4D scene representation into the pre-trained T2V model via an\nadaptive learning branch, where the 4D scene is viewpoint-agnostic and the\nconditional generated video embeds the viewpoints visually. Then, we formulate\nviewpoint extraction as a hybrid-condition guided camera extrinsic denoising\nprocess. Specifically, a camera extrinsic diffusion branch is further\nintroduced onto the pre-trained T2V model, by taking the generated video and 4D\nscene as input. Experimental results show the superiority of our proposed\nmethod over existing competitors, and ablation studies validate the\neffectiveness of our key technical designs. To some extent, this work proves\nthe potential of video generation models toward 4D interaction in real world.",
      "upvotes": 15,
      "discussionId": "68edaa98de1fee572713a6e2",
      "projectPage": "https://yuli0103.github.io/AdaViewPlanner/",
      "ai_summary": "A two-stage paradigm adapts pre-trained Text-to-Video models for viewpoint prediction in 4D scenes by integrating an adaptive learning branch and a camera extrinsic diffusion branch.",
      "ai_keywords": [
        "Text-to-Video models",
        "viewpoint planning",
        "4D scenes",
        "adaptive learning branch",
        "viewpoint extraction",
        "hybrid-condition guided camera extrinsic denoising",
        "camera extrinsic diffusion branch"
      ],
      "organization": {
        "_id": "662c559b322afcbae51b3c8b",
        "name": "KwaiVGI",
        "fullname": "Kuaishou Visual Generation and Interaction Center",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"
      }
    },
    "publishedAt": "2025-10-12T11:55:44.000Z",
    "title": "AdaViewPlanner: Adapting Video Diffusion Models for Viewpoint Planning\n  in 4D Scenes",
    "summary": "Recent Text-to-Video (T2V) models have demonstrated powerful capability in\nvisual simulation of real-world geometry and physical laws, indicating its\npotential as implicit world models. Inspired by this, we explore the\nfeasibility of leveraging the video generation prior for viewpoint planning\nfrom given 4D scenes, since videos internally accompany dynamic scenes with\nnatural viewpoints. To this end, we propose a two-stage paradigm to adapt\npre-trained T2V models for viewpoint prediction, in a compatible manner. First,\nwe inject the 4D scene representation into the pre-trained T2V model via an\nadaptive learning branch, where the 4D scene is viewpoint-agnostic and the\nconditional generated video embeds the viewpoints visually. Then, we formulate\nviewpoint extraction as a hybrid-condition guided camera extrinsic denoising\nprocess. Specifically, a camera extrinsic diffusion branch is further\nintroduced onto the pre-trained T2V model, by taking the generated video and 4D\nscene as input. Experimental results show the superiority of our proposed\nmethod over existing competitors, and ablation studies validate the\neffectiveness of our key technical designs. To some extent, this work proves\nthe potential of video generation models toward 4D interaction in real world.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/651ed7ef755e92f7f12742e6/E-2LhC-CbFdLdojYWTqLg.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.10670.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "651ed7ef755e92f7f12742e6",
      "avatarUrl": "/avatars/57a9cc189b4a59299aad6c96191b18d8.svg",
      "fullname": "yu li",
      "name": "lyabc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "662c559b322afcbae51b3c8b",
      "name": "KwaiVGI",
      "fullname": "Kuaishou Visual Generation and Interaction Center",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.08886",
      "authors": [
        {
          "_id": "68edac0dde1fee572713a705",
          "user": {
            "_id": "65d76cc5b9b7b8bf88faa916",
            "avatarUrl": "/avatars/d95232cd0c307efab6197ade1a66190b.svg",
            "isPro": true,
            "fullname": "Yan Wang",
            "user": "YanAdjeNole",
            "type": "user"
          },
          "name": "Yan Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-14T07:30:56.790Z",
          "hidden": false
        },
        {
          "_id": "68edac0dde1fee572713a706",
          "name": "Keyi Wang",
          "hidden": false
        },
        {
          "_id": "68edac0dde1fee572713a707",
          "name": "Shanshan Yang",
          "hidden": false
        },
        {
          "_id": "68edac0dde1fee572713a708",
          "name": "Jaisal Patel",
          "hidden": false
        },
        {
          "_id": "68edac0dde1fee572713a709",
          "name": "Jeff Zhao",
          "hidden": false
        },
        {
          "_id": "68edac0dde1fee572713a70a",
          "name": "Fengran Mo",
          "hidden": false
        },
        {
          "_id": "68edac0dde1fee572713a70b",
          "name": "Xueqing Peng",
          "hidden": false
        },
        {
          "_id": "68edac0dde1fee572713a70c",
          "name": "Lingfei Qian",
          "hidden": false
        },
        {
          "_id": "68edac0dde1fee572713a70d",
          "user": {
            "_id": "63b58ed5889aa6707f0bb0f4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b58ed5889aa6707f0bb0f4/znl74_aMswlV8VtHrfj3G.jpeg",
            "isPro": true,
            "fullname": "Jimin Huang",
            "user": "jiminHuang",
            "type": "user"
          },
          "name": "Jimin Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-14T07:30:58.956Z",
          "hidden": false
        },
        {
          "_id": "68edac0dde1fee572713a70e",
          "name": "Guojun Xiong",
          "hidden": false
        },
        {
          "_id": "68edac0dde1fee572713a70f",
          "name": "Xiao-Yang Liu",
          "hidden": false
        },
        {
          "_id": "68edac0dde1fee572713a710",
          "name": "Jian-Yun Nie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-10T00:41:55.000Z",
      "submittedOnDailyAt": "2025-10-14T00:23:32.419Z",
      "title": "FinAuditing: A Financial Taxonomy-Structured Multi-Document Benchmark\n  for Evaluating LLMs",
      "submittedOnDailyBy": {
        "_id": "65d76cc5b9b7b8bf88faa916",
        "avatarUrl": "/avatars/d95232cd0c307efab6197ade1a66190b.svg",
        "isPro": true,
        "fullname": "Yan Wang",
        "user": "YanAdjeNole",
        "type": "user"
      },
      "summary": "The complexity of the Generally Accepted Accounting Principles (GAAP) and the\nhierarchical structure of eXtensible Business Reporting Language (XBRL) filings\nmake financial auditing increasingly difficult to automate and verify. While\nlarge language models (LLMs) have demonstrated strong capabilities in\nunstructured text understanding, their ability to reason over structured,\ninterdependent, and taxonomy-driven financial documents remains largely\nunexplored. To fill this gap, we introduce FinAuditing, the first\ntaxonomy-aligned, structure-aware, multi-document benchmark for evaluating LLMs\non financial auditing tasks. Built from real US-GAAP-compliant XBRL filings,\nFinAuditing defines three complementary subtasks, FinSM for semantic\nconsistency, FinRE for relational consistency, and FinMR for numerical\nconsistency, each targeting a distinct aspect of structured auditing reasoning.\nWe further propose a unified evaluation framework integrating retrieval,\nclassification, and reasoning metrics across these subtasks. Extensive\nzero-shot experiments on 13 state-of-the-art LLMs reveal that current models\nperform inconsistently across semantic, relational, and mathematical\ndimensions, with accuracy drops of up to 60-90% when reasoning over\nhierarchical multi-document structures. Our findings expose the systematic\nlimitations of modern LLMs in taxonomy-grounded financial reasoning and\nestablish FinAuditing as a foundation for developing trustworthy,\nstructure-aware, and regulation-aligned financial intelligence systems. The\nbenchmark dataset is available at Hugging Face.",
      "upvotes": 15,
      "discussionId": "68edac0ede1fee572713a711",
      "githubRepo": "https://github.com/The-FinAI/FinAuditing.git",
      "ai_summary": "FinAuditing is a benchmark for evaluating LLMs on structured financial auditing tasks, revealing their limitations in handling taxonomy-driven, hierarchical financial documents.",
      "ai_keywords": [
        "LLMs",
        "FinAuditing",
        "taxonomy-aligned",
        "structure-aware",
        "multi-document benchmark",
        "FinSM",
        "FinRE",
        "FinMR",
        "semantic consistency",
        "relational consistency",
        "numerical consistency",
        "retrieval",
        "classification",
        "reasoning metrics",
        "zero-shot experiments",
        "US-GAAP",
        "XBRL filings"
      ],
      "githubStars": 0,
      "organization": {
        "_id": "658f4413674349122c0708e9",
        "name": "TheFinAI",
        "fullname": "The Fin AI",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63b58ed5889aa6707f0bb0f4/ZK5nQKw34W3-eH3p4NAYc.jpeg"
      }
    },
    "publishedAt": "2025-10-09T20:41:55.000Z",
    "title": "FinAuditing: A Financial Taxonomy-Structured Multi-Document Benchmark\n  for Evaluating LLMs",
    "summary": "The complexity of the Generally Accepted Accounting Principles (GAAP) and the\nhierarchical structure of eXtensible Business Reporting Language (XBRL) filings\nmake financial auditing increasingly difficult to automate and verify. While\nlarge language models (LLMs) have demonstrated strong capabilities in\nunstructured text understanding, their ability to reason over structured,\ninterdependent, and taxonomy-driven financial documents remains largely\nunexplored. To fill this gap, we introduce FinAuditing, the first\ntaxonomy-aligned, structure-aware, multi-document benchmark for evaluating LLMs\non financial auditing tasks. Built from real US-GAAP-compliant XBRL filings,\nFinAuditing defines three complementary subtasks, FinSM for semantic\nconsistency, FinRE for relational consistency, and FinMR for numerical\nconsistency, each targeting a distinct aspect of structured auditing reasoning.\nWe further propose a unified evaluation framework integrating retrieval,\nclassification, and reasoning metrics across these subtasks. Extensive\nzero-shot experiments on 13 state-of-the-art LLMs reveal that current models\nperform inconsistently across semantic, relational, and mathematical\ndimensions, with accuracy drops of up to 60-90% when reasoning over\nhierarchical multi-document structures. Our findings expose the systematic\nlimitations of modern LLMs in taxonomy-grounded financial reasoning and\nestablish FinAuditing as a foundation for developing trustworthy,\nstructure-aware, and regulation-aligned financial intelligence systems. The\nbenchmark dataset is available at Hugging Face.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.08886.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65d76cc5b9b7b8bf88faa916",
      "avatarUrl": "/avatars/d95232cd0c307efab6197ade1a66190b.svg",
      "fullname": "Yan Wang",
      "name": "YanAdjeNole",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "organization": {
      "_id": "658f4413674349122c0708e9",
      "name": "TheFinAI",
      "fullname": "The Fin AI",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63b58ed5889aa6707f0bb0f4/ZK5nQKw34W3-eH3p4NAYc.jpeg"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.11026",
      "authors": [
        {
          "_id": "68edc362de1fee572713a8bb",
          "user": {
            "_id": "675b94e3a301c0203a4131bc",
            "avatarUrl": "/avatars/4b7b74a3efb1ed1649114c644389b8c9.svg",
            "isPro": false,
            "fullname": "Hongxiang Li",
            "user": "lihxxx",
            "type": "user"
          },
          "name": "Hongxiang Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-14T07:30:17.520Z",
          "hidden": false
        },
        {
          "_id": "68edc362de1fee572713a8bc",
          "name": "Yaowei Li",
          "hidden": false
        },
        {
          "_id": "68edc362de1fee572713a8bd",
          "name": "Bin Lin",
          "hidden": false
        },
        {
          "_id": "68edc362de1fee572713a8be",
          "name": "Yuwei Niu",
          "hidden": false
        },
        {
          "_id": "68edc362de1fee572713a8bf",
          "name": "Yuhang Yang",
          "hidden": false
        },
        {
          "_id": "68edc362de1fee572713a8c0",
          "name": "Xiaoshuang Huang",
          "hidden": false
        },
        {
          "_id": "68edc362de1fee572713a8c1",
          "name": "Jiayin Cai",
          "hidden": false
        },
        {
          "_id": "68edc362de1fee572713a8c2",
          "name": "Xiaolong Jiang",
          "hidden": false
        },
        {
          "_id": "68edc362de1fee572713a8c3",
          "name": "Yao Hu",
          "hidden": false
        },
        {
          "_id": "68edc362de1fee572713a8c4",
          "name": "Long Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-13T05:50:44.000Z",
      "submittedOnDailyAt": "2025-10-14T02:02:47.558Z",
      "title": "GIR-Bench: Versatile Benchmark for Generating Images with Reasoning",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Unified multimodal models integrate the reasoning capacity of large language\nmodels with both image understanding and generation, showing great promise for\nadvanced multimodal intelligence. However, the community still lacks a rigorous\nreasoning-centric benchmark to systematically evaluate the alignment between\nunderstanding and generation, and their generalization potential in complex\nvisual tasks. To this end, we introduce GIR-Bench, a comprehensive\nbenchmark that evaluates unified models across three complementary\nperspectives. Firstly, we investigate understanding-generation consistency\n(GIR-Bench-UGC), asking whether models can consistently leverage the same\nknowledge in both understanding and generation tasks. Secondly, we investigate\nwhether models can perform reasoning-centric text-to-image generation that\nrequires applying logical constraints and implicit knowledge to generate\nfaithful visual content (GIR-Bench-T2I). Thirdly, we evaluate whether models\ncan handle multi-step reasoning in editing (GIR-Bench-Edit). For each subset,\nwe carefully design different task-specific evaluation pipelines tailored for\neach task. This enables fine-grained and interpretable evaluation while\nmitigating biases from the prevalent MLLM-as-a-Judge paradigm. Extensive\nablations over various unified models and generation-only systems have shown\nthat: Although unified models are more capable of reasoning-driven visual\ntasks, they still exhibit a persistent gap between understanding and\ngeneration. The data and code for GIR-Bench are available at\nhttps://hkust-longgroup.github.io/GIR-Bench{https://hkust-longgroup.github.io/GIR-Bench}.",
      "upvotes": 14,
      "discussionId": "68edc362de1fee572713a8c5",
      "projectPage": "https://hkust-longgroup.github.io/GIR-Bench/",
      "githubRepo": "https://github.com/HKUST-LongGroup/GIR-Bench",
      "ai_summary": "GIR-Bench evaluates unified multimodal models across understanding-generation consistency, reasoning-centric text-to-image generation, and multi-step reasoning in editing, highlighting gaps in their capabilities.",
      "ai_keywords": [
        "unified multimodal models",
        "large language models",
        "image understanding",
        "image generation",
        "multimodal intelligence",
        "reasoning-centric benchmark",
        "understanding-generation consistency",
        "text-to-image generation",
        "multi-step reasoning",
        "evaluation pipelines",
        "MLLM-as-a-Judge paradigm"
      ],
      "githubStars": 19,
      "organization": {
        "_id": "63355133edc1a61aecf74b0e",
        "name": "HKUST",
        "fullname": "HKUST"
      }
    },
    "publishedAt": "2025-10-13T01:50:44.000Z",
    "title": "GIR-Bench: Versatile Benchmark for Generating Images with Reasoning",
    "summary": "Unified multimodal models integrate the reasoning capacity of large language\nmodels with both image understanding and generation, showing great promise for\nadvanced multimodal intelligence. However, the community still lacks a rigorous\nreasoning-centric benchmark to systematically evaluate the alignment between\nunderstanding and generation, and their generalization potential in complex\nvisual tasks. To this end, we introduce GIR-Bench, a comprehensive\nbenchmark that evaluates unified models across three complementary\nperspectives. Firstly, we investigate understanding-generation consistency\n(GIR-Bench-UGC), asking whether models can consistently leverage the same\nknowledge in both understanding and generation tasks. Secondly, we investigate\nwhether models can perform reasoning-centric text-to-image generation that\nrequires applying logical constraints and implicit knowledge to generate\nfaithful visual content (GIR-Bench-T2I). Thirdly, we evaluate whether models\ncan handle multi-step reasoning in editing (GIR-Bench-Edit). For each subset,\nwe carefully design different task-specific evaluation pipelines tailored for\neach task. This enables fine-grained and interpretable evaluation while\nmitigating biases from the prevalent MLLM-as-a-Judge paradigm. Extensive\nablations over various unified models and generation-only systems have shown\nthat: Although unified models are more capable of reasoning-driven visual\ntasks, they still exhibit a persistent gap between understanding and\ngeneration. The data and code for GIR-Bench are available at\nhttps://hkust-longgroup.github.io/GIR-Bench{https://hkust-longgroup.github.io/GIR-Bench}.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.11026.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 126
    },
    "organization": {
      "_id": "63355133edc1a61aecf74b0e",
      "name": "HKUST",
      "fullname": "HKUST"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.09541",
      "authors": [
        {
          "_id": "68edb2ecde1fee572713a753",
          "name": "Chenyu Wang",
          "hidden": false
        },
        {
          "_id": "68edb2ecde1fee572713a754",
          "name": "Paria Rashidinejad",
          "hidden": false
        },
        {
          "_id": "68edb2ecde1fee572713a755",
          "name": "DiJia Su",
          "hidden": false
        },
        {
          "_id": "68edb2ecde1fee572713a756",
          "name": "Song Jiang",
          "hidden": false
        },
        {
          "_id": "68edb2ecde1fee572713a757",
          "name": "Sid Wang",
          "hidden": false
        },
        {
          "_id": "68edb2ecde1fee572713a758",
          "name": "Siyan Zhao",
          "hidden": false
        },
        {
          "_id": "68edb2ecde1fee572713a759",
          "name": "Cai Zhou",
          "hidden": false
        },
        {
          "_id": "68edb2ecde1fee572713a75a",
          "name": "Shannon Zejiang Shen",
          "hidden": false
        },
        {
          "_id": "68edb2ecde1fee572713a75b",
          "name": "Feiyu Chen",
          "hidden": false
        },
        {
          "_id": "68edb2ecde1fee572713a75c",
          "name": "Tommi Jaakkola",
          "hidden": false
        },
        {
          "_id": "68edb2ecde1fee572713a75d",
          "name": "Yuandong Tian",
          "hidden": false
        },
        {
          "_id": "68edb2ecde1fee572713a75e",
          "name": "Bo Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-10T16:52:25.000Z",
      "submittedOnDailyAt": "2025-10-14T00:56:45.124Z",
      "title": "SPG: Sandwiched Policy Gradient for Masked Diffusion Language Models",
      "submittedOnDailyBy": {
        "_id": "6632b788ec8aa01ab2f4dc36",
        "avatarUrl": "/avatars/7a80f946dfd0e105007d84623cd55e90.svg",
        "isPro": false,
        "fullname": "Chenyu Wang",
        "user": "wangchy",
        "type": "user"
      },
      "summary": "Diffusion large language models (dLLMs) are emerging as an efficient\nalternative to autoregressive models due to their ability to decode multiple\ntokens in parallel. However, aligning dLLMs with human preferences or\ntask-specific rewards via reinforcement learning (RL) is challenging because\ntheir intractable log-likelihood precludes the direct application of standard\npolicy gradient methods. While prior work uses surrogates like the evidence\nlower bound (ELBO), these one-sided approximations can introduce significant\npolicy gradient bias. To address this, we propose the Sandwiched Policy\nGradient (SPG) that leverages both an upper and a lower bound of the true\nlog-likelihood. Experiments show that SPG significantly outperforms baselines\nbased on ELBO or one-step estimation. Specifically, SPG improves the accuracy\nover state-of-the-art RL methods for dLLMs by 3.6% in GSM8K, 2.6% in MATH500,\n18.4% in Countdown and 27.0% in Sudoku.",
      "upvotes": 13,
      "discussionId": "68edb2ecde1fee572713a75f",
      "githubRepo": "https://github.com/facebookresearch/SPG",
      "ai_summary": "The Sandwiched Policy Gradient method improves reinforcement learning for diffusion large language models by using both upper and lower bounds of log-likelihood, outperforming ELBO-based methods.",
      "ai_keywords": [
        "diffusion large language models",
        "dLLMs",
        "autoregressive models",
        "reinforcement learning",
        "RL",
        "log-likelihood",
        "policy gradient methods",
        "evidence lower bound",
        "ELBO",
        "Sandwiched Policy Gradient",
        "SPG",
        "GSM8K",
        "MATH500",
        "Countdown",
        "Sudoku"
      ],
      "githubStars": 7,
      "organization": {
        "_id": "66b54027408752ae16404b05",
        "name": "metaresearch",
        "fullname": "Meta Research",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66b25f3f58babfaeb76112dc/2GmiaF075AZ7BcE538oPk.png"
      }
    },
    "publishedAt": "2025-10-10T12:52:25.000Z",
    "title": "SPG: Sandwiched Policy Gradient for Masked Diffusion Language Models",
    "summary": "Diffusion large language models (dLLMs) are emerging as an efficient\nalternative to autoregressive models due to their ability to decode multiple\ntokens in parallel. However, aligning dLLMs with human preferences or\ntask-specific rewards via reinforcement learning (RL) is challenging because\ntheir intractable log-likelihood precludes the direct application of standard\npolicy gradient methods. While prior work uses surrogates like the evidence\nlower bound (ELBO), these one-sided approximations can introduce significant\npolicy gradient bias. To address this, we propose the Sandwiched Policy\nGradient (SPG) that leverages both an upper and a lower bound of the true\nlog-likelihood. Experiments show that SPG significantly outperforms baselines\nbased on ELBO or one-step estimation. Specifically, SPG improves the accuracy\nover state-of-the-art RL methods for dLLMs by 3.6% in GSM8K, 2.6% in MATH500,\n18.4% in Countdown and 27.0% in Sudoku.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.09541.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6632b788ec8aa01ab2f4dc36",
      "avatarUrl": "/avatars/7a80f946dfd0e105007d84623cd55e90.svg",
      "fullname": "Chenyu Wang",
      "name": "wangchy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "66b54027408752ae16404b05",
      "name": "metaresearch",
      "fullname": "Meta Research",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66b25f3f58babfaeb76112dc/2GmiaF075AZ7BcE538oPk.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.11027",
      "authors": [
        {
          "_id": "68edc292de1fee572713a8a7",
          "user": {
            "_id": "6565d7149afd51867e55520b",
            "avatarUrl": "/avatars/027b17651e61df598af53f69b92e7771.svg",
            "isPro": false,
            "fullname": "Ganlin Yang",
            "user": "ganlinyang",
            "type": "user"
          },
          "name": "Ganlin Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-14T07:30:25.854Z",
          "hidden": false
        },
        {
          "_id": "68edc292de1fee572713a8a8",
          "name": "Tianyi Zhang",
          "hidden": false
        },
        {
          "_id": "68edc292de1fee572713a8a9",
          "user": {
            "_id": "669e221425f2081c6e3d8b61",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/669e221425f2081c6e3d8b61/OearXzKrvoIZtDKTH4kje.jpeg",
            "isPro": false,
            "fullname": "Haoran Hao",
            "user": "Hoar012",
            "type": "user"
          },
          "name": "Haoran Hao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-14T07:30:20.398Z",
          "hidden": false
        },
        {
          "_id": "68edc292de1fee572713a8aa",
          "name": "Weiyun Wang",
          "hidden": false
        },
        {
          "_id": "68edc292de1fee572713a8ab",
          "name": "Yibin Liu",
          "hidden": false
        },
        {
          "_id": "68edc292de1fee572713a8ac",
          "name": "Dehui Wang",
          "hidden": false
        },
        {
          "_id": "68edc292de1fee572713a8ad",
          "user": {
            "_id": "6624ba6d79d897d7ddee24b5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6624ba6d79d897d7ddee24b5/eHkAquXvHBlCNNgaRDYgG.jpeg",
            "isPro": false,
            "fullname": "Guanzhou Chen",
            "user": "Rayment",
            "type": "user"
          },
          "name": "Guanzhou Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-14T07:30:22.944Z",
          "hidden": false
        },
        {
          "_id": "68edc292de1fee572713a8ae",
          "name": "Zijian Cai",
          "hidden": false
        },
        {
          "_id": "68edc292de1fee572713a8af",
          "name": "Junting Chen",
          "hidden": false
        },
        {
          "_id": "68edc292de1fee572713a8b0",
          "name": "Weijie Su",
          "hidden": false
        },
        {
          "_id": "68edc292de1fee572713a8b1",
          "name": "Wengang Zhou",
          "hidden": false
        },
        {
          "_id": "68edc292de1fee572713a8b2",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "68edc292de1fee572713a8b3",
          "name": "Jifeng Dai",
          "hidden": false
        },
        {
          "_id": "68edc292de1fee572713a8b4",
          "name": "Jiangmiao Pang",
          "hidden": false
        },
        {
          "_id": "68edc292de1fee572713a8b5",
          "name": "Gen Luo",
          "hidden": false
        },
        {
          "_id": "68edc292de1fee572713a8b6",
          "name": "Wenhai Wang",
          "hidden": false
        },
        {
          "_id": "68edc292de1fee572713a8b7",
          "name": "Yao Mu",
          "hidden": false
        },
        {
          "_id": "68edc292de1fee572713a8b8",
          "name": "Zhi Hou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-13T05:51:22.000Z",
      "submittedOnDailyAt": "2025-10-14T02:00:35.070Z",
      "title": "Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning",
      "submittedOnDailyBy": {
        "_id": "6565d7149afd51867e55520b",
        "avatarUrl": "/avatars/027b17651e61df598af53f69b92e7771.svg",
        "isPro": false,
        "fullname": "Ganlin Yang",
        "user": "ganlinyang",
        "type": "user"
      },
      "summary": "While significant research has focused on developing embodied reasoning\ncapabilities using Vision-Language Models (VLMs) or integrating advanced VLMs\ninto Vision-Language-Action (VLA) models for end-to-end robot control, few\nstudies directly address the critical gap between upstream VLM-based reasoning\nand downstream VLA policy learning. In this work, we take an initial step\ntoward bridging embodied reasoning with VLA policy learning by introducing\nVlaser - a Vision-Language-Action Model with synergistic embodied reasoning\ncapability, which is a foundational vision-language model designed to integrate\nhigh-level reasoning with low-level control for embodied agents. Built upon the\nhigh-quality Vlaser-6M dataset, Vlaser achieves state-of-the-art performance\nacross a range of embodied reasoning benchmarks - including spatial reasoning,\nembodied grounding, embodied QA, and task planning. Furthermore, we\nsystematically examine how different VLM initializations affect supervised VLA\nfine-tuning, offering novel insights into mitigating the domain shift between\ninternet-scale pre-training data and embodied-specific policy learning data.\nBased on these insights, our approach achieves state-of-the-art results on the\nWidowX benchmark and competitive performance on the Google Robot benchmark.",
      "upvotes": 11,
      "discussionId": "68edc293de1fee572713a8b9",
      "projectPage": "https://internvl.github.io/blog/2025-10-11-Vlaser/",
      "githubRepo": "https://github.com/OpenGVLab/Vlaser/",
      "ai_summary": "Vlaser, a Vision-Language-Action Model, integrates high-level reasoning with low-level control for embodied agents, achieving state-of-the-art performance in embodied reasoning tasks and competitive results in robot benchmarks.",
      "ai_keywords": [
        "Vision-Language Models",
        "Vision-Language-Action models",
        "embodied reasoning",
        "Vlaser",
        "Vlaser-6M dataset",
        "spatial reasoning",
        "embodied grounding",
        "embodied QA",
        "task planning",
        "supervised VLA fine-tuning",
        "domain shift",
        "WidowX benchmark",
        "Google Robot benchmark"
      ],
      "githubStars": 12,
      "organization": {
        "_id": "6747ee5decec679eafb90450",
        "name": "ShanghaiAiLab",
        "fullname": "shanghai ailab "
      }
    },
    "publishedAt": "2025-10-13T01:51:22.000Z",
    "title": "Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning",
    "summary": "While significant research has focused on developing embodied reasoning\ncapabilities using Vision-Language Models (VLMs) or integrating advanced VLMs\ninto Vision-Language-Action (VLA) models for end-to-end robot control, few\nstudies directly address the critical gap between upstream VLM-based reasoning\nand downstream VLA policy learning. In this work, we take an initial step\ntoward bridging embodied reasoning with VLA policy learning by introducing\nVlaser - a Vision-Language-Action Model with synergistic embodied reasoning\ncapability, which is a foundational vision-language model designed to integrate\nhigh-level reasoning with low-level control for embodied agents. Built upon the\nhigh-quality Vlaser-6M dataset, Vlaser achieves state-of-the-art performance\nacross a range of embodied reasoning benchmarks - including spatial reasoning,\nembodied grounding, embodied QA, and task planning. Furthermore, we\nsystematically examine how different VLM initializations affect supervised VLA\nfine-tuning, offering novel insights into mitigating the domain shift between\ninternet-scale pre-training data and embodied-specific policy learning data.\nBased on these insights, our approach achieves state-of-the-art results on the\nWidowX benchmark and competitive performance on the Google Robot benchmark.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.11027.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6565d7149afd51867e55520b",
      "avatarUrl": "/avatars/027b17651e61df598af53f69b92e7771.svg",
      "fullname": "Ganlin Yang",
      "name": "ganlinyang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "organization": {
      "_id": "6747ee5decec679eafb90450",
      "name": "ShanghaiAiLab",
      "fullname": "shanghai ailab "
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.10197",
      "authors": [
        {
          "_id": "68edd8cede1fee572713a995",
          "user": {
            "_id": "6696954da7fd582ae70db39d",
            "avatarUrl": "/avatars/92bf42710b206a3164b3ace8090443fa.svg",
            "isPro": false,
            "fullname": "Siyuan Lu (SII)",
            "user": "IcyFish",
            "type": "user"
          },
          "name": "Siyuan Lu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-14T07:29:30.092Z",
          "hidden": false
        },
        {
          "_id": "68edd8cede1fee572713a996",
          "name": "Zechuan Wang",
          "hidden": false
        },
        {
          "_id": "68edd8cede1fee572713a997",
          "name": "Hongxuan Zhang",
          "hidden": false
        },
        {
          "_id": "68edd8cede1fee572713a998",
          "user": {
            "_id": "68243db9d08d8e01109dd4f4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/iZKRzoFHewLgAyNVanWf_.png",
            "isPro": false,
            "fullname": "Qing Wu",
            "user": "qingw-dev",
            "type": "user"
          },
          "name": "Qintong Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-14T07:29:27.348Z",
          "hidden": false
        },
        {
          "_id": "68edd8cede1fee572713a999",
          "name": "Leilei Gan",
          "hidden": false
        },
        {
          "_id": "68edd8cede1fee572713a99a",
          "user": {
            "_id": "64e847ab5ddcace745b8f5b1",
            "avatarUrl": "/avatars/89525e66ff2900f86f66a11043a298f9.svg",
            "isPro": true,
            "fullname": "chenyi zhuang",
            "user": "chengle",
            "type": "user"
          },
          "name": "Chenyi Zhuang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-14T07:29:25.180Z",
          "hidden": false
        },
        {
          "_id": "68edd8cede1fee572713a99b",
          "name": "Jinjie Gu",
          "hidden": false
        },
        {
          "_id": "68edd8cede1fee572713a99c",
          "name": "Tao Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-11T12:35:15.000Z",
      "submittedOnDailyAt": "2025-10-14T03:51:08.492Z",
      "title": "Don't Just Fine-tune the Agent, Tune the Environment",
      "submittedOnDailyBy": {
        "_id": "6696954da7fd582ae70db39d",
        "avatarUrl": "/avatars/92bf42710b206a3164b3ace8090443fa.svg",
        "isPro": false,
        "fullname": "Siyuan Lu (SII)",
        "user": "IcyFish",
        "type": "user"
      },
      "summary": "Large Language Model (LLM) agents show great promise for complex, multi-turn\ntool-use tasks, but their development is often hampered by the extreme scarcity\nof high-quality training data. Supervised fine-tuning (SFT) on synthetic data\nleads to overfitting, whereas standard reinforcement learning (RL) struggles\nwith a critical cold-start problem and training instability. To address these\nchallenges, we introduce Environment Tuning, a novel training\nparadigm that enables agents to learn complex behaviors directly from problem\ninstances without relying on pre-collected expert trajectories.\nEnvironment Tuning orchestrates this learning process through a\nstructured curriculum, actionable environment augmentation that provides\ncorrective feedback, and fine-grained progress rewards to ensure stable and\nefficient exploration. Using only 400 problem instances from Berkeley\nFunction-Calling Leaderboard (BFCL) benchmark, our method not only achieves\ncompetitive in-distribution performance against strong baselines but also\ndemonstrates superior out-of-distribution generalization, overcoming the\nperformance collapse common to SFT-based approaches. Our work presents a\nparadigm shift from supervised fine-tuning on static trajectories to dynamic,\nenvironment-based exploration, paving the way for training more robust and\ndata-efficient agents.",
      "upvotes": 11,
      "discussionId": "68edd8cfde1fee572713a99d",
      "ai_summary": "Environment Tuning enables LLM agents to learn complex behaviors from problem instances using a structured curriculum, environment augmentation, and progress rewards, achieving competitive in-distribution performance and superior out-of-distribution generalization.",
      "ai_keywords": [
        "Environment Tuning",
        "structured curriculum",
        "environment augmentation",
        "progress rewards",
        "reinforcement learning",
        "supervised fine-tuning",
        "LLM agents",
        "Berkeley Function-Calling Leaderboard",
        "in-distribution performance",
        "out-of-distribution generalization"
      ],
      "organization": {
        "_id": "67aea5c8f086ab0f70ed97c9",
        "name": "inclusionAI",
        "fullname": "inclusionAI",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/fyKuazRifqiaIO34xrhhm.jpeg"
      }
    },
    "publishedAt": "2025-10-11T08:35:15.000Z",
    "title": "Don't Just Fine-tune the Agent, Tune the Environment",
    "summary": "Large Language Model (LLM) agents show great promise for complex, multi-turn\ntool-use tasks, but their development is often hampered by the extreme scarcity\nof high-quality training data. Supervised fine-tuning (SFT) on synthetic data\nleads to overfitting, whereas standard reinforcement learning (RL) struggles\nwith a critical cold-start problem and training instability. To address these\nchallenges, we introduce Environment Tuning, a novel training\nparadigm that enables agents to learn complex behaviors directly from problem\ninstances without relying on pre-collected expert trajectories.\nEnvironment Tuning orchestrates this learning process through a\nstructured curriculum, actionable environment augmentation that provides\ncorrective feedback, and fine-grained progress rewards to ensure stable and\nefficient exploration. Using only 400 problem instances from Berkeley\nFunction-Calling Leaderboard (BFCL) benchmark, our method not only achieves\ncompetitive in-distribution performance against strong baselines but also\ndemonstrates superior out-of-distribution generalization, overcoming the\nperformance collapse common to SFT-based approaches. Our work presents a\nparadigm shift from supervised fine-tuning on static trajectories to dynamic,\nenvironment-based exploration, paving the way for training more robust and\ndata-efficient agents.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.10197.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6696954da7fd582ae70db39d",
      "avatarUrl": "/avatars/92bf42710b206a3164b3ace8090443fa.svg",
      "fullname": "Siyuan Lu (SII)",
      "name": "IcyFish",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "67aea5c8f086ab0f70ed97c9",
      "name": "inclusionAI",
      "fullname": "inclusionAI",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/fyKuazRifqiaIO34xrhhm.jpeg"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.09008",
      "authors": [
        {
          "_id": "68edc6a9de1fee572713a8fe",
          "user": {
            "_id": "633e6f07309a99325095dd42",
            "avatarUrl": "/avatars/57b91a488ac1745b3c0509c04eb6ad93.svg",
            "isPro": false,
            "fullname": "Hoigi Seo",
            "user": "Agorium",
            "type": "user"
          },
          "name": "Hoigi Seo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-14T07:30:10.592Z",
          "hidden": false
        },
        {
          "_id": "68edc6a9de1fee572713a8ff",
          "name": "Dong Un Kang",
          "hidden": false
        },
        {
          "_id": "68edc6a9de1fee572713a900",
          "user": {
            "_id": "66b46b576badc45885923979",
            "avatarUrl": "/avatars/622e95c50876d48fc2dcb9b0dbc74607.svg",
            "isPro": false,
            "fullname": "Hyunjin Cho",
            "user": "jfdkjjs",
            "type": "user"
          },
          "name": "Hyunjin Cho",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-14T07:30:08.146Z",
          "hidden": false
        },
        {
          "_id": "68edc6a9de1fee572713a901",
          "name": "Joohoon Lee",
          "hidden": false
        },
        {
          "_id": "68edc6a9de1fee572713a902",
          "name": "Se Young Chun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-10T05:12:52.000Z",
      "submittedOnDailyAt": "2025-10-14T02:13:55.551Z",
      "title": "On Epistemic Uncertainty of Visual Tokens for Object Hallucinations in\n  Large Vision-Language Models",
      "submittedOnDailyBy": {
        "_id": "633e6f07309a99325095dd42",
        "avatarUrl": "/avatars/57b91a488ac1745b3c0509c04eb6ad93.svg",
        "isPro": false,
        "fullname": "Hoigi Seo",
        "user": "Agorium",
        "type": "user"
      },
      "summary": "Large vision-language models (LVLMs), which integrate a vision encoder (VE)\nwith a large language model, have achieved remarkable success across various\ntasks. However, there are still crucial challenges in LVLMs such as object\nhallucination, generating descriptions of objects that are not in the input\nimage. Here, we argue that uncertain visual tokens within the VE is a key\nfactor that contributes to object hallucination. Our statistical analysis found\nthat there are positive correlations between visual tokens with high epistemic\nuncertainty and the occurrence of hallucinations. Furthermore, we show\ntheoretically and empirically that visual tokens in early VE layers that\nexhibit large representation deviations under small adversarial perturbations\nindicate high epistemic uncertainty. Based on these findings, we propose a\nsimple yet effective strategy to mitigate object hallucination by modifying the\nVE only. Our method comprises a proxy method with adversarial perturbations for\nidentifying uncertain visual tokens efficiently and a method to mask these\nuncertain visual tokens during the self-attention process in the middle layers\nof the VE, suppressing their influence on visual encoding and thus alleviating\nhallucinations. Extensive experiments show that our method significantly\nreduces object hallucinations in LVLMs and can synergistically work with other\nprior arts.",
      "upvotes": 11,
      "discussionId": "68edc6a9de1fee572713a903",
      "projectPage": "https://keenyjin.github.io/epistemic/",
      "githubRepo": "https://github.com/joohoonlee/Epistemic",
      "ai_summary": "A method to reduce object hallucinations in large vision-language models by identifying and masking uncertain visual tokens in the vision encoder.",
      "ai_keywords": [
        "vision-language models",
        "vision encoder",
        "object hallucination",
        "visual tokens",
        "epistemic uncertainty",
        "adversarial perturbations",
        "self-attention",
        "visual encoding"
      ],
      "githubStars": 3,
      "organization": {
        "_id": "66d54dc8033492801db2bf5a",
        "name": "SeoulNatlUniv",
        "fullname": "Seoul National University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/659ccc9d18897eb6594e897f/_-0BM-1UyM-d-lRiahFnf.png"
      }
    },
    "publishedAt": "2025-10-10T01:12:52.000Z",
    "title": "On Epistemic Uncertainty of Visual Tokens for Object Hallucinations in\n  Large Vision-Language Models",
    "summary": "Large vision-language models (LVLMs), which integrate a vision encoder (VE)\nwith a large language model, have achieved remarkable success across various\ntasks. However, there are still crucial challenges in LVLMs such as object\nhallucination, generating descriptions of objects that are not in the input\nimage. Here, we argue that uncertain visual tokens within the VE is a key\nfactor that contributes to object hallucination. Our statistical analysis found\nthat there are positive correlations between visual tokens with high epistemic\nuncertainty and the occurrence of hallucinations. Furthermore, we show\ntheoretically and empirically that visual tokens in early VE layers that\nexhibit large representation deviations under small adversarial perturbations\nindicate high epistemic uncertainty. Based on these findings, we propose a\nsimple yet effective strategy to mitigate object hallucination by modifying the\nVE only. Our method comprises a proxy method with adversarial perturbations for\nidentifying uncertain visual tokens efficiently and a method to mask these\nuncertain visual tokens during the self-attention process in the middle layers\nof the VE, suppressing their influence on visual encoding and thus alleviating\nhallucinations. Extensive experiments show that our method significantly\nreduces object hallucinations in LVLMs and can synergistically work with other\nprior arts.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.09008.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "633e6f07309a99325095dd42",
      "avatarUrl": "/avatars/57b91a488ac1745b3c0509c04eb6ad93.svg",
      "fullname": "Hoigi Seo",
      "name": "Agorium",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "organization": {
      "_id": "66d54dc8033492801db2bf5a",
      "name": "SeoulNatlUniv",
      "fullname": "Seoul National University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/659ccc9d18897eb6594e897f/_-0BM-1UyM-d-lRiahFnf.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.11718",
      "authors": [
        {
          "_id": "68edb609de1fee572713a78f",
          "name": "Chengqi Duan",
          "hidden": false
        },
        {
          "_id": "68edb609de1fee572713a790",
          "name": "Kaiyue Sun",
          "hidden": false
        },
        {
          "_id": "68edb609de1fee572713a791",
          "name": "Rongyao Fang",
          "hidden": false
        },
        {
          "_id": "68edb609de1fee572713a792",
          "name": "Manyuan Zhang",
          "hidden": false
        },
        {
          "_id": "68edb609de1fee572713a793",
          "name": "Yan Feng",
          "hidden": false
        },
        {
          "_id": "68edb609de1fee572713a794",
          "name": "Ying Luo",
          "hidden": false
        },
        {
          "_id": "68edb609de1fee572713a795",
          "name": "Yufang Liu",
          "hidden": false
        },
        {
          "_id": "68edb609de1fee572713a796",
          "name": "Ke Wang",
          "hidden": false
        },
        {
          "_id": "68edb609de1fee572713a797",
          "name": "Peng Pei",
          "hidden": false
        },
        {
          "_id": "68edb609de1fee572713a798",
          "name": "Xunliang Cai",
          "hidden": false
        },
        {
          "_id": "68edb609de1fee572713a799",
          "name": "Hongsheng Li",
          "hidden": false
        },
        {
          "_id": "68edb609de1fee572713a79a",
          "name": "Yi Ma",
          "hidden": false
        },
        {
          "_id": "68edb609de1fee572713a79b",
          "user": {
            "_id": "65d5ec74cd05bc1eaa125040",
            "avatarUrl": "/avatars/2de1b1539a86452c2c89570eeb02f5ab.svg",
            "isPro": false,
            "fullname": "Xihui Liu",
            "user": "XihuiLiu",
            "type": "user"
          },
          "name": "Xihui Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-14T07:30:42.570Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-13T17:59:55.000Z",
      "submittedOnDailyAt": "2025-10-14T01:40:32.364Z",
      "title": "CodePlot-CoT: Mathematical Visual Reasoning by Thinking with Code-Driven\n  Images",
      "submittedOnDailyBy": {
        "_id": "65b8724123d948d884b379b1",
        "avatarUrl": "/avatars/ce189d1d8d688c17912f9b869035b2d0.svg",
        "isPro": false,
        "fullname": "Rongyao Fang",
        "user": "LucasFang",
        "type": "user"
      },
      "summary": "Recent advances in Large Language Models (LLMs) and Vision Language Models\n(VLMs) have shown significant progress in mathematical reasoning, yet they\nstill face a critical bottleneck with problems requiring visual assistance,\nsuch as drawing auxiliary lines or plotting functions to solve the problems.\nMost LLMs and VLMs are constrained to text-only reasoning chains, while\nmultimodal unified models that can generate interleaved text and images lack\nthe necessary precision and controllability for such tasks. To address this, we\npropose CodePlot-CoT, a code-driven Chain-of-Thought paradigm for \"thinking\nwith images\" in mathematics. Our approach leverages the VLM to generate text\nreasoning as well as executable plotting code, which is then rendered into\nimages as \"visual thought\", to solve mathematical problems. To achieve this, we\nfirst construct Math-VR, the first large-scale, bilingual dataset and benchmark\nfor Mathematics problems with Visual Reasoning, comprising 178K samples.\nSecond, to create high-quality training data, we develop a state-of-the-art\nimage-to-code converter specialized for parsing complex mathematical figures\ninto codes. Finally, using these training data, we train the CodePlot-CoT model\nfor solving mathematical problems. Experimental results show that our model\nachieves up to 21% increase over base model on our new benchmark, fully\nvalidating the efficacy of our proposed code-driven reasoning paradigm. Our\nwork opens a new direction for multimodal mathematical reasoning and provides\nthe community with the first large-scale dataset, comprehensive benchmark, and\nstrong approach for such problems. To facilitate future research, we make our\ndatasets, code, and pretrained models publicly available at\nhttps://github.com/HKU-MMLab/Math-VR-CodePlot-CoT.",
      "upvotes": 9,
      "discussionId": "68edb609de1fee572713a79c",
      "projectPage": "https://math-vr.github.io/",
      "githubRepo": "https://github.com/HKU-MMLab/Math-VR-CodePlot-CoT",
      "ai_summary": "CodePlot-CoT, a code-driven Chain-of-Thought model, enhances multimodal mathematical reasoning by generating both text and executable plotting code to solve problems requiring visual assistance.",
      "ai_keywords": [
        "Large Language Models",
        "Vision Language Models",
        "mathematical reasoning",
        "visual assistance",
        "multimodal unified models",
        "Chain-of-Thought",
        "Math-VR",
        "image-to-code converter",
        "visual reasoning",
        "mathematical problems"
      ],
      "githubStars": 18,
      "organization": {
        "_id": "67ea9ecfc234715db8dbf339",
        "name": "hkuhk",
        "fullname": "The University of Hong Kong",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67ea9e8d2d95c10a0da11b0c/FNnR4M7YqKRuG43N5771B.png"
      }
    },
    "publishedAt": "2025-10-13T13:59:55.000Z",
    "title": "CodePlot-CoT: Mathematical Visual Reasoning by Thinking with Code-Driven\n  Images",
    "summary": "Recent advances in Large Language Models (LLMs) and Vision Language Models\n(VLMs) have shown significant progress in mathematical reasoning, yet they\nstill face a critical bottleneck with problems requiring visual assistance,\nsuch as drawing auxiliary lines or plotting functions to solve the problems.\nMost LLMs and VLMs are constrained to text-only reasoning chains, while\nmultimodal unified models that can generate interleaved text and images lack\nthe necessary precision and controllability for such tasks. To address this, we\npropose CodePlot-CoT, a code-driven Chain-of-Thought paradigm for \"thinking\nwith images\" in mathematics. Our approach leverages the VLM to generate text\nreasoning as well as executable plotting code, which is then rendered into\nimages as \"visual thought\", to solve mathematical problems. To achieve this, we\nfirst construct Math-VR, the first large-scale, bilingual dataset and benchmark\nfor Mathematics problems with Visual Reasoning, comprising 178K samples.\nSecond, to create high-quality training data, we develop a state-of-the-art\nimage-to-code converter specialized for parsing complex mathematical figures\ninto codes. Finally, using these training data, we train the CodePlot-CoT model\nfor solving mathematical problems. Experimental results show that our model\nachieves up to 21% increase over base model on our new benchmark, fully\nvalidating the efficacy of our proposed code-driven reasoning paradigm. Our\nwork opens a new direction for multimodal mathematical reasoning and provides\nthe community with the first large-scale dataset, comprehensive benchmark, and\nstrong approach for such problems. To facilitate future research, we make our\ndatasets, code, and pretrained models publicly available at\nhttps://github.com/HKU-MMLab/Math-VR-CodePlot-CoT.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.11718.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65b8724123d948d884b379b1",
      "avatarUrl": "/avatars/ce189d1d8d688c17912f9b869035b2d0.svg",
      "fullname": "Rongyao Fang",
      "name": "LucasFang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "organization": {
      "_id": "67ea9ecfc234715db8dbf339",
      "name": "hkuhk",
      "fullname": "The University of Hong Kong",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67ea9e8d2d95c10a0da11b0c/FNnR4M7YqKRuG43N5771B.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.11391",
      "authors": [
        {
          "_id": "68edb30ade1fee572713a76d",
          "user": {
            "_id": "6512a2e284bedffb5ac5a511",
            "avatarUrl": "/avatars/e798f2c8633d77aea41fce684d749390.svg",
            "isPro": false,
            "fullname": "Junpeng Liu",
            "user": "jeepliu",
            "type": "user"
          },
          "name": "Junpeng Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-14T07:30:44.805Z",
          "hidden": false
        },
        {
          "_id": "68edb30ade1fee572713a76e",
          "name": "Yuzhong Zhao",
          "hidden": false
        },
        {
          "_id": "68edb30ade1fee572713a76f",
          "name": "Bowen Cao",
          "hidden": false
        },
        {
          "_id": "68edb30ade1fee572713a770",
          "name": "Jiayu Ding",
          "hidden": false
        },
        {
          "_id": "68edb30ade1fee572713a771",
          "name": "Yilin Jia",
          "hidden": false
        },
        {
          "_id": "68edb30ade1fee572713a772",
          "name": "Tengchao Lv",
          "hidden": false
        },
        {
          "_id": "68edb30ade1fee572713a773",
          "name": "Yupan Huang",
          "hidden": false
        },
        {
          "_id": "68edb30ade1fee572713a774",
          "name": "Shaohan Huang",
          "hidden": false
        },
        {
          "_id": "68edb30ade1fee572713a775",
          "name": "Nan Yang",
          "hidden": false
        },
        {
          "_id": "68edb30ade1fee572713a776",
          "name": "Li Dong",
          "hidden": false
        },
        {
          "_id": "68edb30ade1fee572713a777",
          "name": "Lei Cui",
          "hidden": false
        },
        {
          "_id": "68edb30ade1fee572713a778",
          "name": "Tao Ge",
          "hidden": false
        },
        {
          "_id": "68edb30ade1fee572713a779",
          "name": "Xun Wang",
          "hidden": false
        },
        {
          "_id": "68edb30ade1fee572713a77a",
          "name": "Huitian Jiao",
          "hidden": false
        },
        {
          "_id": "68edb30ade1fee572713a77b",
          "name": "Sun Mao",
          "hidden": false
        },
        {
          "_id": "68edb30ade1fee572713a77c",
          "name": "FNU Kartik",
          "hidden": false
        },
        {
          "_id": "68edb30ade1fee572713a77d",
          "name": "Si-Qing Chen",
          "hidden": false
        },
        {
          "_id": "68edb30ade1fee572713a77e",
          "name": "Wai Lam",
          "hidden": false
        },
        {
          "_id": "68edb30ade1fee572713a77f",
          "name": "Furu Wei",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-13T13:36:32.000Z",
      "submittedOnDailyAt": "2025-10-14T00:51:14.681Z",
      "title": "DocReward: A Document Reward Model for Structuring and Stylizing",
      "submittedOnDailyBy": {
        "_id": "6512a2e284bedffb5ac5a511",
        "avatarUrl": "/avatars/e798f2c8633d77aea41fce684d749390.svg",
        "isPro": false,
        "fullname": "Junpeng Liu",
        "user": "jeepliu",
        "type": "user"
      },
      "summary": "Recent advances in agentic workflows have enabled the automation of tasks\nsuch as professional document generation. However, they primarily focus on\ntextual quality, neglecting visual structure and style, which are crucial for\nreadability and engagement. This gap arises mainly from the absence of suitable\nreward models to guide agentic workflows toward producing documents with\nstronger structural and stylistic quality. To address this, we propose\nDocReward, a document reward model that evaluates documents based on their\nstructure and style. We construct a multi-domain dataset DocPair of 117K paired\ndocuments, covering 32 domains and 267 document types, each including a high-\nand low-professionalism document with identical content but different structure\nand style. This enables the model to evaluate professionalism comprehensively,\nand in a textual-quality-agnostic way. DocReward is trained using the\nBradley-Terry loss to score documents, penalizing predictions that contradict\nthe annotated ranking. To assess the performance of reward models, we create a\ntest dataset containing document bundles ranked by well-educated human\nevaluators. Notably, DocReward outperforms GPT-4o and GPT-5 in accuracy by 30.6\nand 19.4 percentage points, respectively, demonstrating its superiority over\nbaselines. In an extrinsic evaluation of document generation, DocReward\nachieves a significantly higher win rate of 60.8%, compared to GPT-5's 37.7%\nwin rate, demonstrating its utility in guiding generation agents toward\nproducing human-preferred documents.",
      "upvotes": 8,
      "discussionId": "68edb30ade1fee572713a780",
      "ai_summary": "DocReward, a document reward model, evaluates and enhances the structural and stylistic quality of generated documents, outperforming GPT-4o and GPT-5 in both accuracy and human-preferred document generation.",
      "ai_keywords": [
        "DocReward",
        "Bradley-Terry loss",
        "document reward model",
        "document generation",
        "extrinsic evaluation"
      ]
    },
    "publishedAt": "2025-10-13T09:36:32.000Z",
    "title": "DocReward: A Document Reward Model for Structuring and Stylizing",
    "summary": "Recent advances in agentic workflows have enabled the automation of tasks\nsuch as professional document generation. However, they primarily focus on\ntextual quality, neglecting visual structure and style, which are crucial for\nreadability and engagement. This gap arises mainly from the absence of suitable\nreward models to guide agentic workflows toward producing documents with\nstronger structural and stylistic quality. To address this, we propose\nDocReward, a document reward model that evaluates documents based on their\nstructure and style. We construct a multi-domain dataset DocPair of 117K paired\ndocuments, covering 32 domains and 267 document types, each including a high-\nand low-professionalism document with identical content but different structure\nand style. This enables the model to evaluate professionalism comprehensively,\nand in a textual-quality-agnostic way. DocReward is trained using the\nBradley-Terry loss to score documents, penalizing predictions that contradict\nthe annotated ranking. To assess the performance of reward models, we create a\ntest dataset containing document bundles ranked by well-educated human\nevaluators. Notably, DocReward outperforms GPT-4o and GPT-5 in accuracy by 30.6\nand 19.4 percentage points, respectively, demonstrating its superiority over\nbaselines. In an extrinsic evaluation of document generation, DocReward\nachieves a significantly higher win rate of 60.8%, compared to GPT-5's 37.7%\nwin rate, demonstrating its utility in guiding generation agents toward\nproducing human-preferred documents.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.11391.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6512a2e284bedffb5ac5a511",
      "avatarUrl": "/avatars/e798f2c8633d77aea41fce684d749390.svg",
      "fullname": "Junpeng Liu",
      "name": "jeepliu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.10637",
      "authors": [
        {
          "_id": "68edb788de1fee572713a7ac",
          "name": "Haoyu Zhao",
          "hidden": false
        },
        {
          "_id": "68edb788de1fee572713a7ad",
          "name": "Cheng Zeng",
          "hidden": false
        },
        {
          "_id": "68edb788de1fee572713a7ae",
          "name": "Linghao Zhuang",
          "hidden": false
        },
        {
          "_id": "68edb788de1fee572713a7af",
          "name": "Yaxi Zhao",
          "hidden": false
        },
        {
          "_id": "68edb788de1fee572713a7b0",
          "name": "Shengke Xue",
          "hidden": false
        },
        {
          "_id": "68edb788de1fee572713a7b1",
          "name": "Hao Wang",
          "hidden": false
        },
        {
          "_id": "68edb788de1fee572713a7b2",
          "name": "Xingyue Zhao",
          "hidden": false
        },
        {
          "_id": "68edb788de1fee572713a7b3",
          "name": "Zhongyu Li",
          "hidden": false
        },
        {
          "_id": "68edb788de1fee572713a7b4",
          "name": "Kehan Li",
          "hidden": false
        },
        {
          "_id": "68edb788de1fee572713a7b5",
          "name": "Siteng Huang",
          "hidden": false
        },
        {
          "_id": "68edb788de1fee572713a7b6",
          "name": "Mingxiu Chen",
          "hidden": false
        },
        {
          "_id": "68edb788de1fee572713a7b7",
          "name": "Xin Li",
          "hidden": false
        },
        {
          "_id": "68edb788de1fee572713a7b8",
          "name": "Deli Zhao",
          "hidden": false
        },
        {
          "_id": "68edb788de1fee572713a7b9",
          "name": "Hua Zou",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/65fd82762bf2cd20ddaa193f/APUgtP0dJ9AuLFpwvc-4i.mp4"
      ],
      "publishedAt": "2025-10-12T14:42:07.000Z",
      "submittedOnDailyAt": "2025-10-14T03:11:32.748Z",
      "title": "High-Fidelity Simulated Data Generation for Real-World Zero-Shot Robotic\n  Manipulation Learning with Gaussian Splatting",
      "submittedOnDailyBy": {
        "_id": "65fd82762bf2cd20ddaa193f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yBYbWp_mT7UusYdkqtAvw.png",
        "isPro": false,
        "fullname": "Siteng Huang",
        "user": "huangsiteng",
        "type": "user"
      },
      "summary": "The scalability of robotic learning is fundamentally bottlenecked by the\nsignificant cost and labor of real-world data collection. While simulated data\noffers a scalable alternative, it often fails to generalize to the real world\ndue to significant gaps in visual appearance, physical properties, and object\ninteractions. To address this, we propose RoboSimGS, a novel Real2Sim2Real\nframework that converts multi-view real-world images into scalable,\nhigh-fidelity, and physically interactive simulation environments for robotic\nmanipulation. Our approach reconstructs scenes using a hybrid representation:\n3D Gaussian Splatting (3DGS) captures the photorealistic appearance of the\nenvironment, while mesh primitives for interactive objects ensure accurate\nphysics simulation. Crucially, we pioneer the use of a Multi-modal Large\nLanguage Model (MLLM) to automate the creation of physically plausible,\narticulated assets. The MLLM analyzes visual data to infer not only physical\nproperties (e.g., density, stiffness) but also complex kinematic structures\n(e.g., hinges, sliding rails) of objects. We demonstrate that policies trained\nentirely on data generated by RoboSimGS achieve successful zero-shot\nsim-to-real transfer across a diverse set of real-world manipulation tasks.\nFurthermore, data from RoboSimGS significantly enhances the performance and\ngeneralization capabilities of SOTA methods. Our results validate RoboSimGS as\na powerful and scalable solution for bridging the sim-to-real gap.",
      "upvotes": 8,
      "discussionId": "68edb788de1fee572713a7ba",
      "projectPage": "https://robosimgs.github.io/",
      "ai_summary": "RoboSimGS, a Real2Sim2Real framework, uses 3D Gaussian Splatting and mesh primitives to create scalable, high-fidelity, and physically interactive simulation environments, enabling successful zero-shot sim-to-real transfer for robotic manipulation tasks.",
      "ai_keywords": [
        "3D Gaussian Splatting",
        "mesh primitives",
        "Multi-modal Large Language Model",
        "physically plausible",
        "articulated assets",
        "zero-shot sim-to-real transfer",
        "robotic manipulation"
      ],
      "organization": {
        "_id": "6808e7522a4d69d5111da55f",
        "name": "Alibaba-DAMO-Academy",
        "fullname": "DAMO Academy",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6808e64de5dd22427c006e10/9J3vdB62CdeTOd_YrGh9w.jpeg"
      }
    },
    "publishedAt": "2025-10-12T10:42:07.000Z",
    "title": "High-Fidelity Simulated Data Generation for Real-World Zero-Shot Robotic\n  Manipulation Learning with Gaussian Splatting",
    "summary": "The scalability of robotic learning is fundamentally bottlenecked by the\nsignificant cost and labor of real-world data collection. While simulated data\noffers a scalable alternative, it often fails to generalize to the real world\ndue to significant gaps in visual appearance, physical properties, and object\ninteractions. To address this, we propose RoboSimGS, a novel Real2Sim2Real\nframework that converts multi-view real-world images into scalable,\nhigh-fidelity, and physically interactive simulation environments for robotic\nmanipulation. Our approach reconstructs scenes using a hybrid representation:\n3D Gaussian Splatting (3DGS) captures the photorealistic appearance of the\nenvironment, while mesh primitives for interactive objects ensure accurate\nphysics simulation. Crucially, we pioneer the use of a Multi-modal Large\nLanguage Model (MLLM) to automate the creation of physically plausible,\narticulated assets. The MLLM analyzes visual data to infer not only physical\nproperties (e.g., density, stiffness) but also complex kinematic structures\n(e.g., hinges, sliding rails) of objects. We demonstrate that policies trained\nentirely on data generated by RoboSimGS achieve successful zero-shot\nsim-to-real transfer across a diverse set of real-world manipulation tasks.\nFurthermore, data from RoboSimGS significantly enhances the performance and\ngeneralization capabilities of SOTA methods. Our results validate RoboSimGS as\na powerful and scalable solution for bridging the sim-to-real gap.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/65fd82762bf2cd20ddaa193f/APUgtP0dJ9AuLFpwvc-4i.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.10637.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65fd82762bf2cd20ddaa193f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yBYbWp_mT7UusYdkqtAvw.png",
      "fullname": "Siteng Huang",
      "name": "huangsiteng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "organization": {
      "_id": "6808e7522a4d69d5111da55f",
      "name": "Alibaba-DAMO-Academy",
      "fullname": "DAMO Academy",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6808e64de5dd22427c006e10/9J3vdB62CdeTOd_YrGh9w.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.10023",
      "authors": [
        {
          "_id": "68edb508de1fee572713a789",
          "name": "Yinghui He",
          "hidden": false
        },
        {
          "_id": "68edb508de1fee572713a78a",
          "name": "Abhishek Panigrahi",
          "hidden": false
        },
        {
          "_id": "68edb508de1fee572713a78b",
          "name": "Yong Lin",
          "hidden": false
        },
        {
          "_id": "68edb508de1fee572713a78c",
          "name": "Sanjeev Arora",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/652abf5360e706730596e8f4/RtrdHBbrD006CusTGFnQ1.jpeg"
      ],
      "publishedAt": "2025-10-11T05:02:36.000Z",
      "submittedOnDailyAt": "2025-10-14T01:04:54.468Z",
      "title": "Skill-Targeted Adaptive Training",
      "submittedOnDailyBy": {
        "_id": "652abf5360e706730596e8f4",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/zRFJ4FjZJZGkz2wH8PPmU.jpeg",
        "isPro": false,
        "fullname": "Yinghui He",
        "user": "yinghuihe",
        "type": "user"
      },
      "summary": "Language models often show little to no improvement (i.e., \"saturation\") when\ntrained via vanilla supervised fine-tuning (SFT) on data similar to what they\nsaw in their training set (e.g., MATH). We introduce a new fine-tuning\nstrategy, STAT, to train such a student model by using the metacognition\nability of a stronger large language model (LLM) as the teacher. The teacher\nuses the task dataset to create a list of skills needed for the task, and then\nlabels each data point with its required skills (Didolkar et al., 2024). By\nmonitoring the student's answers, the teacher creates a Missing-Skill-Profile\nfor the student, tracking how often they failed to apply each skill in their\nresponses. We use this idea to build a modified training set in one of two\nways. In STAT-Sel, the teacher uses an existing set of training examples but\nadaptively reweights them according to the Missing-Skill-Profile. In STAT-Syn,\nthe teacher synthesizes additional examples involving missing skills. Across\nextensive experiments on Llama and Qwen models, our methods yield improvements\nof up to 7.5% on MATH, whereas SFT provides only limited gains. Furthermore,\nSTAT enhances performance on out-of-distribution benchmarks (e.g., AIME24/25,\nAMC23, etc.) by an average of 4.6%. Crucially, we find that STAT is\ncomplementary to RL via GRPO (Shao et al., 2024): after the model is improved\nusing STAT to address skill gaps, GRPO continues to add further gains. We\nconclude that skill-targeted adaptive training should broadly improve current\ntraining pipelines. Our code is available at:\nhttps://github.com/princeton-pli/STAT.",
      "upvotes": 8,
      "discussionId": "68edb508de1fee572713a78d",
      "ai_summary": "A new fine-tuning strategy, STAT, uses a teacher model's metacognition to identify and address skill gaps in a student model, leading to improved performance on both in-distribution and out-of-distribution benchmarks.",
      "ai_keywords": [
        "vanilla supervised fine-tuning",
        "SFT",
        "STAT",
        "metacognition",
        "large language model",
        "LLM",
        "task dataset",
        "skills",
        "Missing-Skill-Profile",
        "STAT-Sel",
        "STAT-Syn",
        "Llama",
        "Qwen",
        "MATH",
        "AIME24/25",
        "AMC23",
        "RL",
        "GRPO"
      ],
      "organization": {
        "_id": "6735d51c08a190b1caea1f29",
        "name": "PrincetonUniversity",
        "fullname": "Princeton University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6735d43222d14a01ae63fb6d/N7pHYiJcM_zqOnp5MWfo3.png"
      }
    },
    "publishedAt": "2025-10-11T01:02:36.000Z",
    "title": "Skill-Targeted Adaptive Training",
    "summary": "Language models often show little to no improvement (i.e., \"saturation\") when\ntrained via vanilla supervised fine-tuning (SFT) on data similar to what they\nsaw in their training set (e.g., MATH). We introduce a new fine-tuning\nstrategy, STAT, to train such a student model by using the metacognition\nability of a stronger large language model (LLM) as the teacher. The teacher\nuses the task dataset to create a list of skills needed for the task, and then\nlabels each data point with its required skills (Didolkar et al., 2024). By\nmonitoring the student's answers, the teacher creates a Missing-Skill-Profile\nfor the student, tracking how often they failed to apply each skill in their\nresponses. We use this idea to build a modified training set in one of two\nways. In STAT-Sel, the teacher uses an existing set of training examples but\nadaptively reweights them according to the Missing-Skill-Profile. In STAT-Syn,\nthe teacher synthesizes additional examples involving missing skills. Across\nextensive experiments on Llama and Qwen models, our methods yield improvements\nof up to 7.5% on MATH, whereas SFT provides only limited gains. Furthermore,\nSTAT enhances performance on out-of-distribution benchmarks (e.g., AIME24/25,\nAMC23, etc.) by an average of 4.6%. Crucially, we find that STAT is\ncomplementary to RL via GRPO (Shao et al., 2024): after the model is improved\nusing STAT to address skill gaps, GRPO continues to add further gains. We\nconclude that skill-targeted adaptive training should broadly improve current\ntraining pipelines. Our code is available at:\nhttps://github.com/princeton-pli/STAT.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/652abf5360e706730596e8f4/RtrdHBbrD006CusTGFnQ1.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.10023.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "652abf5360e706730596e8f4",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/zRFJ4FjZJZGkz2wH8PPmU.jpeg",
      "fullname": "Yinghui He",
      "name": "yinghuihe",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "6735d51c08a190b1caea1f29",
      "name": "PrincetonUniversity",
      "fullname": "Princeton University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6735d43222d14a01ae63fb6d/N7pHYiJcM_zqOnp5MWfo3.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.11498",
      "authors": [
        {
          "_id": "68edc640de1fee572713a8f3",
          "name": "Yuhang Li",
          "hidden": false
        },
        {
          "_id": "68edc640de1fee572713a8f4",
          "user": {
            "_id": "64b74b906ab5d14ca7f289cd",
            "avatarUrl": "/avatars/b131b7c4ce5216708ca4a678f35ead0a.svg",
            "isPro": false,
            "fullname": "Chenchen Zhang",
            "user": "xxzcc",
            "type": "user"
          },
          "name": "Chenchen Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-14T07:30:13.082Z",
          "hidden": false
        },
        {
          "_id": "68edc640de1fee572713a8f5",
          "name": "Ruilin Lv",
          "hidden": false
        },
        {
          "_id": "68edc640de1fee572713a8f6",
          "name": "Ao Liu",
          "hidden": false
        },
        {
          "_id": "68edc640de1fee572713a8f7",
          "name": "Ken Deng",
          "hidden": false
        },
        {
          "_id": "68edc640de1fee572713a8f8",
          "name": "Yuanxing Zhang",
          "hidden": false
        },
        {
          "_id": "68edc640de1fee572713a8f9",
          "name": "Jiaheng Liu",
          "hidden": false
        },
        {
          "_id": "68edc640de1fee572713a8fa",
          "name": "Wiggin Zhou",
          "hidden": false
        },
        {
          "_id": "68edc640de1fee572713a8fb",
          "name": "Bo Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-13T15:05:50.000Z",
      "submittedOnDailyAt": "2025-10-14T02:12:08.156Z",
      "title": "ReLook: Vision-Grounded RL with a Multimodal LLM Critic for Agentic Web\n  Coding",
      "submittedOnDailyBy": {
        "_id": "64b74b906ab5d14ca7f289cd",
        "avatarUrl": "/avatars/b131b7c4ce5216708ca4a678f35ead0a.svg",
        "isPro": false,
        "fullname": "Chenchen Zhang",
        "user": "xxzcc",
        "type": "user"
      },
      "summary": "While Large Language Models (LLMs) excel at algorithmic code generation, they\nstruggle with front-end development, where correctness is judged on rendered\npixels and interaction. We present ReLook, an agentic, vision-grounded\nreinforcement learning framework that empowers an agent to close a robust\ngenerate--diagnose--refine loop by invoking a multimodal LLM (MLLM) as a tool.\nDuring training, the agent uses the MLLM-in-the-loop both as a visual\ncritic--scoring code with screenshots--and as a source of actionable,\nvision-grounded feedback; a strict zero-reward rule for invalid renders anchors\nrenderability and prevents reward hacking. To prevent behavioral collapse, we\nintroduce Forced Optimization, a strict acceptance rule that admits only\nimproving revisions, yielding monotonically better trajectories. At inference,\nwe decouple the critic and run a lightweight, critic-free self-edit cycle,\nkeeping latency comparable to base decoding while retaining most of the gains.\nAcross three widely used benchmarks, ReLook consistently outperforms strong\nbaselines in vision-grounded front-end code generation, highlighting the\nbenefits of agentic perception, visual rewards, and training-inference\ndecoupling.",
      "upvotes": 7,
      "discussionId": "68edc641de1fee572713a8fc",
      "projectPage": "https://artifactsbenchmark.github.io/",
      "githubRepo": "https://github.com/Tencent-Hunyuan/ArtifactsBenchmark",
      "ai_summary": "ReLook, a vision-grounded reinforcement learning framework, enhances front-end code generation by integrating a multimodal LLM for visual feedback and forced optimization, outperforming existing methods.",
      "ai_keywords": [
        "Large Language Models",
        "LLMs",
        "front-end development",
        "reinforcement learning",
        "agentic",
        "vision-grounded",
        "multimodal LLM",
        "MLLM",
        "visual critic",
        "vision-grounded feedback",
        "Forced Optimization",
        "behavioral collapse",
        "self-edit cycle",
        "vision-grounded front-end code generation"
      ],
      "githubStars": 231,
      "organization": {
        "_id": "66543b6e420092799d2f625c",
        "name": "tencent",
        "fullname": "Tencent",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
      }
    },
    "publishedAt": "2025-10-13T11:05:50.000Z",
    "title": "ReLook: Vision-Grounded RL with a Multimodal LLM Critic for Agentic Web\n  Coding",
    "summary": "While Large Language Models (LLMs) excel at algorithmic code generation, they\nstruggle with front-end development, where correctness is judged on rendered\npixels and interaction. We present ReLook, an agentic, vision-grounded\nreinforcement learning framework that empowers an agent to close a robust\ngenerate--diagnose--refine loop by invoking a multimodal LLM (MLLM) as a tool.\nDuring training, the agent uses the MLLM-in-the-loop both as a visual\ncritic--scoring code with screenshots--and as a source of actionable,\nvision-grounded feedback; a strict zero-reward rule for invalid renders anchors\nrenderability and prevents reward hacking. To prevent behavioral collapse, we\nintroduce Forced Optimization, a strict acceptance rule that admits only\nimproving revisions, yielding monotonically better trajectories. At inference,\nwe decouple the critic and run a lightweight, critic-free self-edit cycle,\nkeeping latency comparable to base decoding while retaining most of the gains.\nAcross three widely used benchmarks, ReLook consistently outperforms strong\nbaselines in vision-grounded front-end code generation, highlighting the\nbenefits of agentic perception, visual rewards, and training-inference\ndecoupling.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.11498.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b74b906ab5d14ca7f289cd",
      "avatarUrl": "/avatars/b131b7c4ce5216708ca4a678f35ead0a.svg",
      "fullname": "Chenchen Zhang",
      "name": "xxzcc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "organization": {
      "_id": "66543b6e420092799d2f625c",
      "name": "tencent",
      "fullname": "Tencent",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.10666",
      "authors": [
        {
          "_id": "68edce77de1fee572713a943",
          "name": "Zhengbo Zhang",
          "hidden": false
        },
        {
          "_id": "68edce77de1fee572713a944",
          "name": "Zhiheng Lyu",
          "hidden": false
        },
        {
          "_id": "68edce77de1fee572713a945",
          "name": "Junhao Gong",
          "hidden": false
        },
        {
          "_id": "68edce77de1fee572713a946",
          "name": "Hongzhu Yi",
          "hidden": false
        },
        {
          "_id": "68edce77de1fee572713a947",
          "name": "Xinming Wang",
          "hidden": false
        },
        {
          "_id": "68edce77de1fee572713a948",
          "name": "Yuxuan Zhou",
          "hidden": false
        },
        {
          "_id": "68edce77de1fee572713a949",
          "name": "Jiabing Yang",
          "hidden": false
        },
        {
          "_id": "68edce77de1fee572713a94a",
          "name": "Ping Nie",
          "hidden": false
        },
        {
          "_id": "68edce77de1fee572713a94b",
          "name": "Yan Huang",
          "hidden": false
        },
        {
          "_id": "68edce77de1fee572713a94c",
          "name": "Wenhu Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-12T15:43:37.000Z",
      "submittedOnDailyAt": "2025-10-14T02:48:04.607Z",
      "title": "BrowserAgent: Building Web Agents with Human-Inspired Web Browsing\n  Actions",
      "submittedOnDailyBy": {
        "_id": "6313a86154e6e5d9f0f94e04",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
        "isPro": false,
        "fullname": "Wenhu Chen",
        "user": "wenhu",
        "type": "user"
      },
      "summary": "Efficiently solving real-world problems with LLMs increasingly hinges on\ntheir ability to interact with dynamic web environments and autonomously\nacquire external information. While recent research like Search-R1 and\nWebDancer demonstrates strong performance in solving web tasks, they heavily\nrely on additional tools to convert the interactive web environment into static\ntext content. This is in contrast to human browsing behaviors, which involve\ndiverse interactions with the browser, such as scrolling, clicking, and typing.\nIn this paper, we propose BrowserAgent, a more interactive agent that solves\ncomplex tasks through human-inspired browser actions. BrowserAgent operates\ndirectly on raw web pages via Playwright through a set of predefined browser\nactions. We adopt a two-stage training (Supervised Fine-Tuning (SFT) and\nRejection Fine-Tuning (RFT)) to improve the model's generalization abilities.\nDespite using significantly less training data than Search-R1, BrowserAgent\nachieves more competitive results across different Open-QA tasks. Additionally,\nwe introduce an explicit memory mechanism to store key conclusions across\nsteps, further enhancing the model's reasoning capabilities for long-horizon\ntasks. Notably, BrowserAgent-7B can achieve around 20\\% improvement over\nSearch-R1 on multi-hop QA tasks like HotpotQA, 2Wiki, and Bamboogle. These\nresults indicate that BrowserAgent can serve as a more advanced framework for\nmore interactive and scalable web agents.",
      "upvotes": 7,
      "discussionId": "68edce77de1fee572713a94d",
      "projectPage": "https://tiger-ai-lab.github.io/BrowserAgent/",
      "githubRepo": "https://github.com/TIGER-AI-Lab/BrowserAgent",
      "ai_summary": "BrowserAgent, an interactive web agent using human-like browser actions and a two-stage training process, achieves competitive results in Open-QA tasks with less training data and improved reasoning for multi-hop QA.",
      "ai_keywords": [
        "LLMs",
        "BrowserAgent",
        "Playwright",
        "Supervised Fine-Tuning",
        "Rejection Fine-Tuning",
        "explicit memory mechanism",
        "multi-hop QA",
        "HotpotQA",
        "2Wiki",
        "Bamboogle"
      ],
      "githubStars": 2,
      "organization": {
        "_id": "6313a90017838d05194fd282",
        "name": "TIGER-Lab",
        "fullname": "TIGER-Lab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6313a86154e6e5d9f0f94e04/Noi3Qq3RYz8Jdq6BaFteq.png"
      }
    },
    "publishedAt": "2025-10-12T11:43:37.000Z",
    "title": "BrowserAgent: Building Web Agents with Human-Inspired Web Browsing\n  Actions",
    "summary": "Efficiently solving real-world problems with LLMs increasingly hinges on\ntheir ability to interact with dynamic web environments and autonomously\nacquire external information. While recent research like Search-R1 and\nWebDancer demonstrates strong performance in solving web tasks, they heavily\nrely on additional tools to convert the interactive web environment into static\ntext content. This is in contrast to human browsing behaviors, which involve\ndiverse interactions with the browser, such as scrolling, clicking, and typing.\nIn this paper, we propose BrowserAgent, a more interactive agent that solves\ncomplex tasks through human-inspired browser actions. BrowserAgent operates\ndirectly on raw web pages via Playwright through a set of predefined browser\nactions. We adopt a two-stage training (Supervised Fine-Tuning (SFT) and\nRejection Fine-Tuning (RFT)) to improve the model's generalization abilities.\nDespite using significantly less training data than Search-R1, BrowserAgent\nachieves more competitive results across different Open-QA tasks. Additionally,\nwe introduce an explicit memory mechanism to store key conclusions across\nsteps, further enhancing the model's reasoning capabilities for long-horizon\ntasks. Notably, BrowserAgent-7B can achieve around 20\\% improvement over\nSearch-R1 on multi-hop QA tasks like HotpotQA, 2Wiki, and Bamboogle. These\nresults indicate that BrowserAgent can serve as a more advanced framework for\nmore interactive and scalable web agents.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.10666.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6313a86154e6e5d9f0f94e04",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
      "fullname": "Wenhu Chen",
      "name": "wenhu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 44
    },
    "organization": {
      "_id": "6313a90017838d05194fd282",
      "name": "TIGER-Lab",
      "fullname": "TIGER-Lab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6313a86154e6e5d9f0f94e04/Noi3Qq3RYz8Jdq6BaFteq.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.08026",
      "authors": [
        {
          "_id": "68eda813de1fee572713a6ad",
          "user": {
            "_id": "65d7b983baa72790a1151923",
            "avatarUrl": "/avatars/938531e84ca01a0c5a2a174057e3e9c5.svg",
            "isPro": false,
            "fullname": "Chen Huang",
            "user": "Albus-Chen",
            "type": "user"
          },
          "name": "Chen Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-14T07:31:17.491Z",
          "hidden": false
        },
        {
          "_id": "68eda813de1fee572713a6ae",
          "name": "Wei Lu",
          "hidden": false
        },
        {
          "_id": "68eda813de1fee572713a6af",
          "name": "Wenxuan Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-09T10:04:31.000Z",
      "submittedOnDailyAt": "2025-10-14T00:03:21.288Z",
      "title": "PEAR: Phase Entropy Aware Reward for Efficient Reasoning",
      "submittedOnDailyBy": {
        "_id": "65d7b983baa72790a1151923",
        "avatarUrl": "/avatars/938531e84ca01a0c5a2a174057e3e9c5.svg",
        "isPro": false,
        "fullname": "Chen Huang",
        "user": "Albus-Chen",
        "type": "user"
      },
      "summary": "Large Reasoning Models (LRMs) have achieved impressive performance on complex\nreasoning tasks by generating detailed chain-of-thought (CoT) explanations.\nHowever, these responses are often excessively long, containing redundant\nreasoning steps that inflate inference cost and reduce usability. Controlling\nthe length of generated reasoning without sacrificing accuracy remains an open\nchallenge. Through a systematic empirical analysis, we reveal a consistent\npositive correlation between model entropy and response length at different\nreasoning stages across diverse LRMs: the thinking phase exhibits higher\nentropy, reflecting exploratory behavior of longer responses, while the final\nanswer phase shows lower entropy, indicating a more deterministic solution.\nThis observation suggests that entropy at different reasoning stages can serve\nas a control knob for balancing conciseness and performance. Based on this\ninsight, this paper introduces Phase Entropy Aware Reward (PEAR), a reward\nmechanism that incorporating phase-dependent entropy into the reward design.\nInstead of treating all tokens uniformly, PEAR penalize excessive entropy\nduring the thinking phase and allowing moderate exploration at the final answer\nphase, which encourages models to generate concise reasoning traces that retain\nsufficient flexibility to solve the task correctly. This enables adaptive\ncontrol of response length without relying on explicit length targets or rigid\ntruncation rules. Extensive experiments across four benchmarks demonstrate that\nPEAR consistently reduces response length while sustaining competitive accuracy\nacross model scales. In addition, PEAR demonstrates strong out-of-distribution\n(OOD) robustness beyond the training distribution. Our code is available at:\nhttps://github.com/iNLP-Lab/PEAR.",
      "upvotes": 7,
      "discussionId": "68eda813de1fee572713a6b0",
      "githubRepo": "https://github.com/iNLP-Lab/PEAR",
      "ai_summary": "A reward mechanism called Phase Entropy Aware Reward (PEAR) controls the length of reasoning in large models by adjusting entropy at different stages, balancing conciseness and accuracy.",
      "ai_keywords": [
        "Large Reasoning Models",
        "chain-of-thought",
        "response length",
        "model entropy",
        "thinking phase",
        "final answer phase",
        "Phase Entropy Aware Reward",
        "PEAR",
        "out-of-distribution robustness"
      ],
      "githubStars": 0,
      "organization": {
        "_id": "68b82daee976083ccd80824b",
        "name": "iNLP-Lab",
        "fullname": "iNLP Lab @ SUTD",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60dff6ae19a362a8c27862aa/3Ukf0b4f546tJM84zynTz.png"
      }
    },
    "publishedAt": "2025-10-09T06:04:31.000Z",
    "title": "PEAR: Phase Entropy Aware Reward for Efficient Reasoning",
    "summary": "Large Reasoning Models (LRMs) have achieved impressive performance on complex\nreasoning tasks by generating detailed chain-of-thought (CoT) explanations.\nHowever, these responses are often excessively long, containing redundant\nreasoning steps that inflate inference cost and reduce usability. Controlling\nthe length of generated reasoning without sacrificing accuracy remains an open\nchallenge. Through a systematic empirical analysis, we reveal a consistent\npositive correlation between model entropy and response length at different\nreasoning stages across diverse LRMs: the thinking phase exhibits higher\nentropy, reflecting exploratory behavior of longer responses, while the final\nanswer phase shows lower entropy, indicating a more deterministic solution.\nThis observation suggests that entropy at different reasoning stages can serve\nas a control knob for balancing conciseness and performance. Based on this\ninsight, this paper introduces Phase Entropy Aware Reward (PEAR), a reward\nmechanism that incorporating phase-dependent entropy into the reward design.\nInstead of treating all tokens uniformly, PEAR penalize excessive entropy\nduring the thinking phase and allowing moderate exploration at the final answer\nphase, which encourages models to generate concise reasoning traces that retain\nsufficient flexibility to solve the task correctly. This enables adaptive\ncontrol of response length without relying on explicit length targets or rigid\ntruncation rules. Extensive experiments across four benchmarks demonstrate that\nPEAR consistently reduces response length while sustaining competitive accuracy\nacross model scales. In addition, PEAR demonstrates strong out-of-distribution\n(OOD) robustness beyond the training distribution. Our code is available at:\nhttps://github.com/iNLP-Lab/PEAR.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.08026.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65d7b983baa72790a1151923",
      "avatarUrl": "/avatars/938531e84ca01a0c5a2a174057e3e9c5.svg",
      "fullname": "Chen Huang",
      "name": "Albus-Chen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "68b82daee976083ccd80824b",
      "name": "iNLP-Lab",
      "fullname": "iNLP Lab @ SUTD",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60dff6ae19a362a8c27862aa/3Ukf0b4f546tJM84zynTz.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.07841",
      "authors": [
        {
          "_id": "68edaaf5de1fee572713a6e4",
          "user": {
            "_id": "63888d3fd68e37abd599f428",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63888d3fd68e37abd599f428/YaNyxG_oM6IgrHTkFZ6Eq.jpeg",
            "isPro": true,
            "fullname": "emre can",
            "user": "emrecanacikgoz",
            "type": "user"
          },
          "name": "Emre Can Acikgoz",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-14T07:31:08.327Z",
          "hidden": false
        },
        {
          "_id": "68edaaf5de1fee572713a6e5",
          "name": "Cheng Qian",
          "hidden": false
        },
        {
          "_id": "68edaaf5de1fee572713a6e6",
          "name": "Heng Ji",
          "hidden": false
        },
        {
          "_id": "68edaaf5de1fee572713a6e7",
          "name": "Dilek Hakkani-Tr",
          "hidden": false
        },
        {
          "_id": "68edaaf5de1fee572713a6e8",
          "name": "Gokhan Tur",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-09T06:37:35.000Z",
      "submittedOnDailyAt": "2025-10-14T00:16:59.509Z",
      "title": "Self-Improving LLM Agents at Test-Time",
      "submittedOnDailyBy": {
        "_id": "63888d3fd68e37abd599f428",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63888d3fd68e37abd599f428/YaNyxG_oM6IgrHTkFZ6Eq.jpeg",
        "isPro": true,
        "fullname": "emre can",
        "user": "emrecanacikgoz",
        "type": "user"
      },
      "summary": "One paradigm of language model (LM) fine-tuning relies on creating large\ntraining datasets, under the assumption that high quantity and diversity will\nenable models to generalize to novel tasks after post-training. In practice,\ngathering large sets of data is inefficient, and training on them is\nprohibitively expensive; worse, there is no guarantee that the resulting model\nwill handle complex scenarios or generalize better. Moreover, existing\ntechniques rarely assess whether a training sample provides novel information\nor is redundant with the knowledge already acquired by the model, resulting in\nunnecessary costs. In this work, we explore a new test-time self-improvement\nmethod to create more effective and generalizable agentic LMs on-the-fly. The\nproposed algorithm can be summarized in three steps: (i) first it identifies\nthe samples that model struggles with (self-awareness), (ii) then generates\nsimilar examples from detected uncertain samples (self-data augmentation), and\n(iii) uses these newly generated samples at test-time fine-tuning\n(self-improvement). We study two variants of this approach: Test-Time\nSelf-Improvement (TT-SI), where the same model generates additional training\nexamples from its own uncertain cases and then learns from them, and contrast\nthis approach with Test-Time Distillation (TT-D), where a stronger model\ngenerates similar examples for uncertain cases, enabling student to adapt using\ndistilled supervision. Empirical evaluations across different agent benchmarks\ndemonstrate that TT-SI improves the performance with +5.48% absolute accuracy\ngain on average across all benchmarks and surpasses other standard learning\nmethods, yet using 68x less training samples. Our findings highlight the\npromise of TT-SI, demonstrating the potential of self-improvement algorithms at\ntest-time as a new paradigm for building more capable agents toward\nself-evolution.",
      "upvotes": 6,
      "discussionId": "68edaaf5de1fee572713a6e9",
      "ai_summary": "A test-time self-improvement method enhances language models by generating additional training examples from uncertain cases, leading to better performance with fewer samples.",
      "ai_keywords": [
        "language model fine-tuning",
        "self-awareness",
        "self-data augmentation",
        "test-time fine-tuning",
        "Test-Time Self-Improvement (TT-SI)",
        "Test-Time Distillation (TT-D)",
        "self-improvement algorithms",
        "self-evolution"
      ]
    },
    "publishedAt": "2025-10-09T02:37:35.000Z",
    "title": "Self-Improving LLM Agents at Test-Time",
    "summary": "One paradigm of language model (LM) fine-tuning relies on creating large\ntraining datasets, under the assumption that high quantity and diversity will\nenable models to generalize to novel tasks after post-training. In practice,\ngathering large sets of data is inefficient, and training on them is\nprohibitively expensive; worse, there is no guarantee that the resulting model\nwill handle complex scenarios or generalize better. Moreover, existing\ntechniques rarely assess whether a training sample provides novel information\nor is redundant with the knowledge already acquired by the model, resulting in\nunnecessary costs. In this work, we explore a new test-time self-improvement\nmethod to create more effective and generalizable agentic LMs on-the-fly. The\nproposed algorithm can be summarized in three steps: (i) first it identifies\nthe samples that model struggles with (self-awareness), (ii) then generates\nsimilar examples from detected uncertain samples (self-data augmentation), and\n(iii) uses these newly generated samples at test-time fine-tuning\n(self-improvement). We study two variants of this approach: Test-Time\nSelf-Improvement (TT-SI), where the same model generates additional training\nexamples from its own uncertain cases and then learns from them, and contrast\nthis approach with Test-Time Distillation (TT-D), where a stronger model\ngenerates similar examples for uncertain cases, enabling student to adapt using\ndistilled supervision. Empirical evaluations across different agent benchmarks\ndemonstrate that TT-SI improves the performance with +5.48% absolute accuracy\ngain on average across all benchmarks and surpasses other standard learning\nmethods, yet using 68x less training samples. Our findings highlight the\npromise of TT-SI, demonstrating the potential of self-improvement algorithms at\ntest-time as a new paradigm for building more capable agents toward\nself-evolution.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.07841.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63888d3fd68e37abd599f428",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63888d3fd68e37abd599f428/YaNyxG_oM6IgrHTkFZ6Eq.jpeg",
      "fullname": "emre can",
      "name": "emrecanacikgoz",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 22
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.10868",
      "authors": [
        {
          "_id": "68edbe4bde1fee572713a86f",
          "name": "Soroush Mehraban",
          "hidden": false
        },
        {
          "_id": "68edbe4bde1fee572713a870",
          "name": "Andrea Iaboni",
          "hidden": false
        },
        {
          "_id": "68edbe4bde1fee572713a871",
          "name": "Babak Taati",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6375965008eebfdd0a399891/0v5aDIZjucnu_Fhy2ADBt.mp4"
      ],
      "publishedAt": "2025-10-13T00:23:17.000Z",
      "submittedOnDailyAt": "2025-10-14T01:37:51.320Z",
      "title": "FastHMR: Accelerating Human Mesh Recovery via Token and Layer Merging\n  with Diffusion Decoding",
      "submittedOnDailyBy": {
        "_id": "6375965008eebfdd0a399891",
        "avatarUrl": "/avatars/946768f40a18793ced82f09a1de47952.svg",
        "isPro": false,
        "fullname": "Soroush Mehraban",
        "user": "SoroushMehraban",
        "type": "user"
      },
      "summary": "Recent transformer-based models for 3D Human Mesh Recovery (HMR) have\nachieved strong performance but often suffer from high computational cost and\ncomplexity due to deep transformer architectures and redundant tokens. In this\npaper, we introduce two HMR-specific merging strategies: Error-Constrained\nLayer Merging (ECLM) and Mask-guided Token Merging (Mask-ToMe). ECLM\nselectively merges transformer layers that have minimal impact on the Mean Per\nJoint Position Error (MPJPE), while Mask-ToMe focuses on merging background\ntokens that contribute little to the final prediction. To further address the\npotential performance drop caused by merging, we propose a diffusion-based\ndecoder that incorporates temporal context and leverages pose priors learned\nfrom large-scale motion capture datasets. Experiments across multiple\nbenchmarks demonstrate that our method achieves up to 2.3x speed-up while\nslightly improving performance over the baseline.",
      "upvotes": 5,
      "discussionId": "68edbe4bde1fee572713a872",
      "projectPage": "https://soroushmehraban.github.io/FastHMR/",
      "ai_summary": "Two merging strategies and a diffusion-based decoder improve 3D Human Mesh Recovery by reducing computational cost and slightly enhancing performance.",
      "ai_keywords": [
        "transformer-based models",
        "3D Human Mesh Recovery",
        "Error-Constrained Layer Merging",
        "Mask-guided Token Merging",
        "Mean Per Joint Position Error",
        "diffusion-based decoder",
        "temporal context",
        "pose priors",
        "motion capture datasets"
      ],
      "organization": {
        "_id": "65b2b60eade89bc3fac3108a",
        "name": "vector-institute",
        "fullname": "Vector Institute",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62f2d5d052ad88c930b7ba5e/_GqWuc2CjKyGsNtHDYKF7.png"
      }
    },
    "publishedAt": "2025-10-12T20:23:17.000Z",
    "title": "FastHMR: Accelerating Human Mesh Recovery via Token and Layer Merging\n  with Diffusion Decoding",
    "summary": "Recent transformer-based models for 3D Human Mesh Recovery (HMR) have\nachieved strong performance but often suffer from high computational cost and\ncomplexity due to deep transformer architectures and redundant tokens. In this\npaper, we introduce two HMR-specific merging strategies: Error-Constrained\nLayer Merging (ECLM) and Mask-guided Token Merging (Mask-ToMe). ECLM\nselectively merges transformer layers that have minimal impact on the Mean Per\nJoint Position Error (MPJPE), while Mask-ToMe focuses on merging background\ntokens that contribute little to the final prediction. To further address the\npotential performance drop caused by merging, we propose a diffusion-based\ndecoder that incorporates temporal context and leverages pose priors learned\nfrom large-scale motion capture datasets. Experiments across multiple\nbenchmarks demonstrate that our method achieves up to 2.3x speed-up while\nslightly improving performance over the baseline.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6375965008eebfdd0a399891/0v5aDIZjucnu_Fhy2ADBt.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.10868.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6375965008eebfdd0a399891",
      "avatarUrl": "/avatars/946768f40a18793ced82f09a1de47952.svg",
      "fullname": "Soroush Mehraban",
      "name": "SoroushMehraban",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "65b2b60eade89bc3fac3108a",
      "name": "vector-institute",
      "fullname": "Vector Institute",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62f2d5d052ad88c930b7ba5e/_GqWuc2CjKyGsNtHDYKF7.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.09905",
      "authors": [
        {
          "_id": "68edba56de1fee572713a804",
          "name": "Xi Fang",
          "hidden": false
        },
        {
          "_id": "68edba56de1fee572713a805",
          "name": "Weijie Xu",
          "hidden": false
        },
        {
          "_id": "68edba56de1fee572713a806",
          "name": "Yuchong Zhang",
          "hidden": false
        },
        {
          "_id": "68edba56de1fee572713a807",
          "name": "Stephanie Eckman",
          "hidden": false
        },
        {
          "_id": "68edba56de1fee572713a808",
          "name": "Scott Nickleach",
          "hidden": false
        },
        {
          "_id": "68edba56de1fee572713a809",
          "name": "Chandan K. Reddy",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-10T22:39:37.000Z",
      "submittedOnDailyAt": "2025-10-14T01:24:10.792Z",
      "title": "The Personalization Trap: How User Memory Alters Emotional Reasoning in\n  LLMs",
      "submittedOnDailyBy": {
        "_id": "63e3f57754f51ea342ce26be",
        "avatarUrl": "/avatars/df9d52c376bed6868d341bb006bec212.svg",
        "isPro": false,
        "fullname": "Weijie Xu",
        "user": "xwjzds",
        "type": "user"
      },
      "summary": "When an AI assistant remembers that Sarah is a single mother working two\njobs, does it interpret her stress differently than if she were a wealthy\nexecutive? As personalized AI systems increasingly incorporate long-term user\nmemory, understanding how this memory shapes emotional reasoning is critical.\nWe investigate how user memory affects emotional intelligence in large language\nmodels (LLMs) by evaluating 15 models on human validated emotional intelligence\ntests. We find that identical scenarios paired with different user profiles\nproduce systematically divergent emotional interpretations. Across validated\nuser independent emotional scenarios and diverse user profiles, systematic\nbiases emerged in several high-performing LLMs where advantaged profiles\nreceived more accurate emotional interpretations. Moreover, LLMs demonstrate\nsignificant disparities across demographic factors in emotion understanding and\nsupportive recommendations tasks, indicating that personalization mechanisms\ncan embed social hierarchies into models emotional reasoning. These results\nhighlight a key challenge for memory enhanced AI: systems designed for\npersonalization may inadvertently reinforce social inequalities.",
      "upvotes": 5,
      "discussionId": "68edba56de1fee572713a80a",
      "ai_summary": "LLMs exhibit systematic biases in emotional interpretation and support based on user profiles, potentially reinforcing social hierarchies.",
      "ai_keywords": [
        "large language models",
        "emotional intelligence",
        "user memory",
        "emotional scenarios",
        "user profiles",
        "systematic biases",
        "demographic factors",
        "emotion understanding",
        "supportive recommendations",
        "social hierarchies"
      ],
      "organization": {
        "_id": "5ffdfbadbba2ae614d771970",
        "name": "amazon",
        "fullname": "Amazon",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66f19ed428ae41c20c470792/8y7msN6A6W82LdQhQd85a.png"
      }
    },
    "publishedAt": "2025-10-10T18:39:37.000Z",
    "title": "The Personalization Trap: How User Memory Alters Emotional Reasoning in\n  LLMs",
    "summary": "When an AI assistant remembers that Sarah is a single mother working two\njobs, does it interpret her stress differently than if she were a wealthy\nexecutive? As personalized AI systems increasingly incorporate long-term user\nmemory, understanding how this memory shapes emotional reasoning is critical.\nWe investigate how user memory affects emotional intelligence in large language\nmodels (LLMs) by evaluating 15 models on human validated emotional intelligence\ntests. We find that identical scenarios paired with different user profiles\nproduce systematically divergent emotional interpretations. Across validated\nuser independent emotional scenarios and diverse user profiles, systematic\nbiases emerged in several high-performing LLMs where advantaged profiles\nreceived more accurate emotional interpretations. Moreover, LLMs demonstrate\nsignificant disparities across demographic factors in emotion understanding and\nsupportive recommendations tasks, indicating that personalization mechanisms\ncan embed social hierarchies into models emotional reasoning. These results\nhighlight a key challenge for memory enhanced AI: systems designed for\npersonalization may inadvertently reinforce social inequalities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.09905.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "63e3f57754f51ea342ce26be",
      "avatarUrl": "/avatars/df9d52c376bed6868d341bb006bec212.svg",
      "fullname": "Weijie Xu",
      "name": "xwjzds",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 16
    },
    "organization": {
      "_id": "5ffdfbadbba2ae614d771970",
      "name": "amazon",
      "fullname": "Amazon",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66f19ed428ae41c20c470792/8y7msN6A6W82LdQhQd85a.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.11650",
      "authors": [
        {
          "_id": "68edc0bbde1fee572713a884",
          "name": "Yuxuan Xue",
          "hidden": false
        },
        {
          "_id": "68edc0bbde1fee572713a885",
          "name": "Xianghui Xie",
          "hidden": false
        },
        {
          "_id": "68edc0bbde1fee572713a886",
          "name": "Margaret Kostyrko",
          "hidden": false
        },
        {
          "_id": "68edc0bbde1fee572713a887",
          "name": "Gerard Pons-Moll",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-13T17:29:55.000Z",
      "submittedOnDailyAt": "2025-10-14T01:47:25.488Z",
      "title": "InfiniHuman: Infinite 3D Human Creation with Precise Control",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Generating realistic and controllable 3D human avatars is a long-standing\nchallenge, particularly when covering broad attribute ranges such as ethnicity,\nage, clothing styles, and detailed body shapes. Capturing and annotating\nlarge-scale human datasets for training generative models is prohibitively\nexpensive and limited in scale and diversity. The central question we address\nin this paper is: Can existing foundation models be distilled to generate\ntheoretically unbounded, richly annotated 3D human data? We introduce\nInfiniHuman, a framework that synergistically distills these models to produce\nrichly annotated human data at minimal cost and with theoretically unlimited\nscalability. We propose InfiniHumanData, a fully automatic pipeline that\nleverages vision-language and image generation models to create a large-scale\nmulti-modal dataset. User study shows our automatically generated identities\nare undistinguishable from scan renderings. InfiniHumanData contains 111K\nidentities spanning unprecedented diversity. Each identity is annotated with\nmulti-granularity text descriptions, multi-view RGB images, detailed clothing\nimages, and SMPL body-shape parameters. Building on this dataset, we propose\nInfiniHumanGen, a diffusion-based generative pipeline conditioned on text, body\nshape, and clothing assets. InfiniHumanGen enables fast, realistic, and\nprecisely controllable avatar generation. Extensive experiments demonstrate\nsignificant improvements over state-of-the-art methods in visual quality,\ngeneration speed, and controllability. Our approach enables high-quality avatar\ngeneration with fine-grained control at effectively unbounded scale through a\npractical and affordable solution. We will publicly release the automatic data\ngeneration pipeline, the comprehensive InfiniHumanData dataset, and the\nInfiniHumanGen models at https://yuxuan-xue.com/infini-human.",
      "upvotes": 3,
      "discussionId": "68edc0bcde1fee572713a888",
      "projectPage": "https://yuxuan-xue.com/infini-human",
      "ai_summary": "InfiniHuman framework distills existing models to generate large-scale, richly annotated 3D human data using a diffusion-based generative pipeline, achieving high visual quality, speed, and controllability.",
      "ai_keywords": [
        "vision-language models",
        "image generation models",
        "diffusion-based generative pipeline",
        "SMPL body-shape parameters"
      ]
    },
    "publishedAt": "2025-10-13T13:29:55.000Z",
    "title": "InfiniHuman: Infinite 3D Human Creation with Precise Control",
    "summary": "Generating realistic and controllable 3D human avatars is a long-standing\nchallenge, particularly when covering broad attribute ranges such as ethnicity,\nage, clothing styles, and detailed body shapes. Capturing and annotating\nlarge-scale human datasets for training generative models is prohibitively\nexpensive and limited in scale and diversity. The central question we address\nin this paper is: Can existing foundation models be distilled to generate\ntheoretically unbounded, richly annotated 3D human data? We introduce\nInfiniHuman, a framework that synergistically distills these models to produce\nrichly annotated human data at minimal cost and with theoretically unlimited\nscalability. We propose InfiniHumanData, a fully automatic pipeline that\nleverages vision-language and image generation models to create a large-scale\nmulti-modal dataset. User study shows our automatically generated identities\nare undistinguishable from scan renderings. InfiniHumanData contains 111K\nidentities spanning unprecedented diversity. Each identity is annotated with\nmulti-granularity text descriptions, multi-view RGB images, detailed clothing\nimages, and SMPL body-shape parameters. Building on this dataset, we propose\nInfiniHumanGen, a diffusion-based generative pipeline conditioned on text, body\nshape, and clothing assets. InfiniHumanGen enables fast, realistic, and\nprecisely controllable avatar generation. Extensive experiments demonstrate\nsignificant improvements over state-of-the-art methods in visual quality,\ngeneration speed, and controllability. Our approach enables high-quality avatar\ngeneration with fine-grained control at effectively unbounded scale through a\npractical and affordable solution. We will publicly release the automatic data\ngeneration pipeline, the comprehensive InfiniHumanData dataset, and the\nInfiniHumanGen models at https://yuxuan-xue.com/infini-human.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.11650.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 126
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.09189",
      "authors": [
        {
          "_id": "68ec701fcd07fb414898c9d2",
          "name": "Changjiang Gao",
          "hidden": false
        },
        {
          "_id": "68ec701fcd07fb414898c9d3",
          "name": "Zixian Huang",
          "hidden": false
        },
        {
          "_id": "68ec701fcd07fb414898c9d4",
          "name": "Jingyang Gong",
          "hidden": false
        },
        {
          "_id": "68ec701fcd07fb414898c9d5",
          "name": "Shujian Huang",
          "hidden": false
        },
        {
          "_id": "68ec701fcd07fb414898c9d6",
          "name": "Lei Li",
          "hidden": false
        },
        {
          "_id": "68ec701fcd07fb414898c9d7",
          "name": "Fei Yuan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-10T09:33:28.000Z",
      "submittedOnDailyAt": "2025-10-14T01:15:23.444Z",
      "title": "LLaMAX2: Your Translation-Enhanced Model also Performs Well in Reasoning",
      "submittedOnDailyBy": {
        "_id": "65fed45b08d35929362dd651",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65fed45b08d35929362dd651/KLMxsyRN6_HhCZP1iDw6K.png",
        "isPro": false,
        "fullname": "FeiYuan",
        "user": "FeYuan",
        "type": "user"
      },
      "summary": "General Large Language Models (LLMs) excel in reasoning, but those enhanced\nfor translation struggle with reasoning tasks. To address this, we propose a\nnovel translationenhanced recipe that begins with instruct models and applies\nlayer-selective tuning only on parallel data. Following this pipeline, we\nintroduce the Qwen3-XPlus models, which demonstrate significant improvements in\ntranslation performance across both high- and lowresource languages, achieving\n15+ spBLEU and 40+ xComet in low-resource languages, like Swahili.\nInterestingly, training only with small parallel datasets, Qwen3-XPlus achieves\nan average improvement of 1+ points on 7 multilingual tasks while maintaining\nproficiency comparable to the Qwen3 instruct model in 15 popular reasoning\ndatasets. This work offers a promising approach to multilingual enhancement,\nsignificantly reducing complexity and enhancing accessibility for a wider range\nof languages. The code and model are publicly available.",
      "upvotes": 3,
      "discussionId": "68ec701fcd07fb414898c9d8",
      "ai_summary": "A novel translation-enhanced recipe using layer-selective tuning on parallel data improves translation performance in both high- and low-resource languages while maintaining reasoning proficiency.",
      "ai_keywords": [
        "instruct models",
        "layer-selective tuning",
        "parallel data",
        "Qwen3-XPlus models",
        "spBLEU",
        "xComet",
        "multilingual tasks",
        "reasoning datasets"
      ]
    },
    "publishedAt": "2025-10-10T05:33:28.000Z",
    "title": "LLaMAX2: Your Translation-Enhanced Model also Performs Well in Reasoning",
    "summary": "General Large Language Models (LLMs) excel in reasoning, but those enhanced\nfor translation struggle with reasoning tasks. To address this, we propose a\nnovel translationenhanced recipe that begins with instruct models and applies\nlayer-selective tuning only on parallel data. Following this pipeline, we\nintroduce the Qwen3-XPlus models, which demonstrate significant improvements in\ntranslation performance across both high- and lowresource languages, achieving\n15+ spBLEU and 40+ xComet in low-resource languages, like Swahili.\nInterestingly, training only with small parallel datasets, Qwen3-XPlus achieves\nan average improvement of 1+ points on 7 multilingual tasks while maintaining\nproficiency comparable to the Qwen3 instruct model in 15 popular reasoning\ndatasets. This work offers a promising approach to multilingual enhancement,\nsignificantly reducing complexity and enhancing accessibility for a wider range\nof languages. The code and model are publicly available.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.09189.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65fed45b08d35929362dd651",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65fed45b08d35929362dd651/KLMxsyRN6_HhCZP1iDw6K.png",
      "fullname": "FeiYuan",
      "name": "FeYuan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 24
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.11713",
      "authors": [
        {
          "_id": "68edc9b7de1fee572713a919",
          "user": {
            "_id": "644a767044b75fd95805d232",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644a767044b75fd95805d232/vHA2vI_B3CpXapdBEwspB.jpeg",
            "isPro": false,
            "fullname": "Patrick (Tsung-Han) Wu",
            "user": "tsunghanwu",
            "type": "user"
          },
          "name": "Tsung-Han Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-14T07:29:59.528Z",
          "hidden": false
        },
        {
          "_id": "68edc9b7de1fee572713a91a",
          "name": "Mihran Miroyan",
          "hidden": false
        },
        {
          "_id": "68edc9b7de1fee572713a91b",
          "name": "David M. Chan",
          "hidden": false
        },
        {
          "_id": "68edc9b7de1fee572713a91c",
          "name": "Trevor Darrell",
          "hidden": false
        },
        {
          "_id": "68edc9b7de1fee572713a91d",
          "name": "Narges Norouzi",
          "hidden": false
        },
        {
          "_id": "68edc9b7de1fee572713a91e",
          "name": "Joseph E. Gonzalez",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-13T17:59:35.000Z",
      "submittedOnDailyAt": "2025-10-14T02:25:47.769Z",
      "title": "Are Large Reasoning Models Interruptible?",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Large Reasoning Models (LRMs) excel at complex reasoning but are\ntraditionally evaluated in static, \"frozen world\" settings: model responses are\nassumed to be instantaneous, and the context of a request is presumed to be\nimmutable over the duration of the response. While generally true for\nshort-term tasks, the \"frozen world\" assumption breaks down in modern reasoning\ntasks such as assistive programming, where models may take hours to think\nthrough problems and code may change dramatically from the time the model\nstarts thinking to the model's final output. In this work, we challenge the\nfrozen world assumption and evaluate LRM robustness under two realistic dynamic\nscenarios: interruptions, which test the quality of the model's partial outputs\non a limited budget, and dynamic context, which tests model adaptation to\nin-flight changes. Across mathematics and programming benchmarks that require\nlong-form reasoning, static evaluations consistently overestimate robustness:\neven state-of-the-art LRMs, which achieve high accuracy in static settings, can\nfail unpredictably when interrupted or exposed to changing context, with\nperformance dropping by up to 60% when updates are introduced late in the\nreasoning process. Our analysis further reveals several novel failure modes,\nincluding reasoning leakage, where models fold the reasoning into their final\nanswer when interrupted; panic, where under time pressure models abandon\nreasoning entirely and return incorrect answers; and self-doubt, where\nperformance degrades while incorporating updated information.",
      "upvotes": 2,
      "discussionId": "68edc9b7de1fee572713a91f",
      "projectPage": "https://dynamic-lm.github.io/",
      "ai_summary": "Large Reasoning Models evaluated in dynamic scenarios with interruptions and changing context show significant performance drops compared to static evaluations.",
      "ai_keywords": [
        "Large Reasoning Models",
        "frozen world assumption",
        "interruptions",
        "dynamic context",
        "long-form reasoning",
        "reasoning leakage",
        "panic",
        "self-doubt"
      ]
    },
    "publishedAt": "2025-10-13T13:59:35.000Z",
    "title": "Are Large Reasoning Models Interruptible?",
    "summary": "Large Reasoning Models (LRMs) excel at complex reasoning but are\ntraditionally evaluated in static, \"frozen world\" settings: model responses are\nassumed to be instantaneous, and the context of a request is presumed to be\nimmutable over the duration of the response. While generally true for\nshort-term tasks, the \"frozen world\" assumption breaks down in modern reasoning\ntasks such as assistive programming, where models may take hours to think\nthrough problems and code may change dramatically from the time the model\nstarts thinking to the model's final output. In this work, we challenge the\nfrozen world assumption and evaluate LRM robustness under two realistic dynamic\nscenarios: interruptions, which test the quality of the model's partial outputs\non a limited budget, and dynamic context, which tests model adaptation to\nin-flight changes. Across mathematics and programming benchmarks that require\nlong-form reasoning, static evaluations consistently overestimate robustness:\neven state-of-the-art LRMs, which achieve high accuracy in static settings, can\nfail unpredictably when interrupted or exposed to changing context, with\nperformance dropping by up to 60% when updates are introduced late in the\nreasoning process. Our analysis further reveals several novel failure modes,\nincluding reasoning leakage, where models fold the reasoning into their final\nanswer when interrupted; panic, where under time pressure models abandon\nreasoning entirely and return incorrect answers; and self-doubt, where\nperformance degrades while incorporating updated information.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.11713.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 126
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.11512",
      "authors": [
        {
          "_id": "68edc22cde1fee572713a89d",
          "name": "Jianhao Yuan",
          "hidden": false
        },
        {
          "_id": "68edc22cde1fee572713a89e",
          "name": "Fabio Pizzati",
          "hidden": false
        },
        {
          "_id": "68edc22cde1fee572713a89f",
          "name": "Francesco Pinto",
          "hidden": false
        },
        {
          "_id": "68edc22cde1fee572713a8a0",
          "name": "Lars Kunze",
          "hidden": false
        },
        {
          "_id": "68edc22cde1fee572713a8a1",
          "name": "Ivan Laptev",
          "hidden": false
        },
        {
          "_id": "68edc22cde1fee572713a8a2",
          "name": "Paul Newman",
          "hidden": false
        },
        {
          "_id": "68edc22cde1fee572713a8a3",
          "name": "Philip Torr",
          "hidden": false
        },
        {
          "_id": "68edc22cde1fee572713a8a4",
          "name": "Daniele De Martini",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-13T15:19:07.000Z",
      "submittedOnDailyAt": "2025-10-14T01:53:33.713Z",
      "title": "LikePhys: Evaluating Intuitive Physics Understanding in Video Diffusion\n  Models via Likelihood Preference",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Intuitive physics understanding in video diffusion models plays an essential\nrole in building general-purpose physically plausible world simulators, yet\naccurately evaluating such capacity remains a challenging task due to the\ndifficulty in disentangling physics correctness from visual appearance in\ngeneration. To the end, we introduce LikePhys, a training-free method that\nevaluates intuitive physics in video diffusion models by distinguishing\nphysically valid and impossible videos using the denoising objective as an\nELBO-based likelihood surrogate on a curated dataset of valid-invalid pairs. By\ntesting on our constructed benchmark of twelve scenarios spanning over four\nphysics domains, we show that our evaluation metric, Plausibility Preference\nError (PPE), demonstrates strong alignment with human preference, outperforming\nstate-of-the-art evaluator baselines. We then systematically benchmark\nintuitive physics understanding in current video diffusion models. Our study\nfurther analyses how model design and inference settings affect intuitive\nphysics understanding and highlights domain-specific capacity variations across\nphysical laws. Empirical results show that, despite current models struggling\nwith complex and chaotic dynamics, there is a clear trend of improvement in\nphysics understanding as model capacity and inference settings scale.",
      "upvotes": 2,
      "discussionId": "68edc22cde1fee572713a8a5",
      "ai_summary": "LikePhys evaluates intuitive physics in video diffusion models using a denoising objective-based metric, demonstrating better alignment with human preference than existing methods.",
      "ai_keywords": [
        "video diffusion models",
        "intuitive physics",
        "LikePhys",
        "denoising objective",
        "ELBO-based likelihood",
        "Plausibility Preference Error",
        "benchmark",
        "human preference",
        "model design",
        "inference settings",
        "domain-specific capacity variations"
      ]
    },
    "publishedAt": "2025-10-13T11:19:07.000Z",
    "title": "LikePhys: Evaluating Intuitive Physics Understanding in Video Diffusion\n  Models via Likelihood Preference",
    "summary": "Intuitive physics understanding in video diffusion models plays an essential\nrole in building general-purpose physically plausible world simulators, yet\naccurately evaluating such capacity remains a challenging task due to the\ndifficulty in disentangling physics correctness from visual appearance in\ngeneration. To the end, we introduce LikePhys, a training-free method that\nevaluates intuitive physics in video diffusion models by distinguishing\nphysically valid and impossible videos using the denoising objective as an\nELBO-based likelihood surrogate on a curated dataset of valid-invalid pairs. By\ntesting on our constructed benchmark of twelve scenarios spanning over four\nphysics domains, we show that our evaluation metric, Plausibility Preference\nError (PPE), demonstrates strong alignment with human preference, outperforming\nstate-of-the-art evaluator baselines. We then systematically benchmark\nintuitive physics understanding in current video diffusion models. Our study\nfurther analyses how model design and inference settings affect intuitive\nphysics understanding and highlights domain-specific capacity variations across\nphysical laws. Empirical results show that, despite current models struggling\nwith complex and chaotic dynamics, there is a clear trend of improvement in\nphysics understanding as model capacity and inference settings scale.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.11512.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 126
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.10681",
      "authors": [
        {
          "_id": "68edc950de1fee572713a915",
          "name": "Zichun Yu",
          "hidden": false
        },
        {
          "_id": "68edc950de1fee572713a916",
          "name": "Chenyan Xiong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-12T16:08:38.000Z",
      "submittedOnDailyAt": "2025-10-14T02:24:39.497Z",
      "title": "RePro: Training Language Models to Faithfully Recycle the Web for\n  Pretraining",
      "submittedOnDailyBy": {
        "_id": "649a943f575e60d3a87cfcdf",
        "avatarUrl": "/avatars/879f79628a30dddeab6e36e7d82bdbaa.svg",
        "isPro": false,
        "fullname": " Zichun Yu",
        "user": "yuzc19",
        "type": "user"
      },
      "summary": "High-quality pretraining data is the fossil fuel of large language models\n(LLMs), yet its reserves are running low for frontier models. In this paper, we\nintroduce RePro, a novel web recycling method that trains a relatively small LM\nwith reinforcement learning to generate effective and faithful rephrasings of\npretraining data. Specifically, we design one quality reward and three\nfaithfulness rewards, optimizing the LM rephraser to convert organic data into\nhigh-quality rephrasings while maintaining its core semantics and structure. In\nour experiment, we train a 4B rephraser to recycle 72B tokens sampled from\nDCLM-RefinedWeb. Pretraining results on 400M and 1.4B models demonstrate that\nRePro delivers 4.7%-14.0% relative accuracy gains over organic-only baseline on\n22 downstream tasks. RePro also outperforms ReWire, the state-of-the-art web\nrecycling method that prompts a 70B rephraser, as well as the organic baseline\nwith a 4x larger data pool. Experiments with different amounts of recycled data\nhighlight that RePro improves organic data efficiency by 2-3x. Individual and\ndistributional analyses validate that RePro preserves more critical information\nand faithfully reflects the characteristics of organic data compared to\nprompting-based methods. Together, these results show that RePro provides an\nefficient and controllable path to effectively harness the fossil fuel of LLM\npretraining. We open-source our code, rephraser, and recycled data at\nhttps://github.com/cxcscmu/RePro.",
      "upvotes": 2,
      "discussionId": "68edc950de1fee572713a917",
      "githubRepo": "https://github.com/cxcscmu/RePro",
      "ai_summary": "RePro, a reinforcement learning-based method, generates high-quality rephrasings of pretraining data to enhance the efficiency and accuracy of large language models.",
      "ai_keywords": [
        "reinforcement learning",
        "rephraser",
        "quality reward",
        "faithfulness rewards",
        "pretraining data",
        "DCLM-RefinedWeb",
        "downstream tasks",
        "ReWire",
        "data efficiency",
        "critical information preservation"
      ],
      "githubStars": 1,
      "organization": {
        "_id": "67bf7cb85baf57a6b4009545",
        "name": "cx-cmu",
        "fullname": "Chenyan Xiong Research Group at CMU"
      }
    },
    "publishedAt": "2025-10-12T12:08:38.000Z",
    "title": "RePro: Training Language Models to Faithfully Recycle the Web for\n  Pretraining",
    "summary": "High-quality pretraining data is the fossil fuel of large language models\n(LLMs), yet its reserves are running low for frontier models. In this paper, we\nintroduce RePro, a novel web recycling method that trains a relatively small LM\nwith reinforcement learning to generate effective and faithful rephrasings of\npretraining data. Specifically, we design one quality reward and three\nfaithfulness rewards, optimizing the LM rephraser to convert organic data into\nhigh-quality rephrasings while maintaining its core semantics and structure. In\nour experiment, we train a 4B rephraser to recycle 72B tokens sampled from\nDCLM-RefinedWeb. Pretraining results on 400M and 1.4B models demonstrate that\nRePro delivers 4.7%-14.0% relative accuracy gains over organic-only baseline on\n22 downstream tasks. RePro also outperforms ReWire, the state-of-the-art web\nrecycling method that prompts a 70B rephraser, as well as the organic baseline\nwith a 4x larger data pool. Experiments with different amounts of recycled data\nhighlight that RePro improves organic data efficiency by 2-3x. Individual and\ndistributional analyses validate that RePro preserves more critical information\nand faithfully reflects the characteristics of organic data compared to\nprompting-based methods. Together, these results show that RePro provides an\nefficient and controllable path to effectively harness the fossil fuel of LLM\npretraining. We open-source our code, rephraser, and recycled data at\nhttps://github.com/cxcscmu/RePro.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.10681.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649a943f575e60d3a87cfcdf",
      "avatarUrl": "/avatars/879f79628a30dddeab6e36e7d82bdbaa.svg",
      "fullname": " Zichun Yu",
      "name": "yuzc19",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "organization": {
      "_id": "67bf7cb85baf57a6b4009545",
      "name": "cx-cmu",
      "fullname": "Chenyan Xiong Research Group at CMU"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.10062",
      "authors": [
        {
          "_id": "68ede08ede1fee572713a9af",
          "name": "Adnan El Assadi",
          "hidden": false
        },
        {
          "_id": "68ede08ede1fee572713a9b0",
          "user": {
            "_id": "64cc0e80a257a3212c0c4b24",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64cc0e80a257a3212c0c4b24/wqs6WZN8-3OQthcnQXgN7.png",
            "isPro": false,
            "fullname": "Isaac Chung",
            "user": "isaacchung",
            "type": "user"
          },
          "name": "Isaac Chung",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-14T07:29:22.077Z",
          "hidden": false
        },
        {
          "_id": "68ede08ede1fee572713a9b1",
          "user": {
            "_id": "61af4544d691b3aadd1f62b6",
            "avatarUrl": "/avatars/7a4067accdd1005f78c3c4adad3ee0a5.svg",
            "isPro": false,
            "fullname": "Solomatin Roman",
            "user": "Samoed",
            "type": "user"
          },
          "name": "Roman Solomatin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-14T07:29:19.917Z",
          "hidden": false
        },
        {
          "_id": "68ede08ede1fee572713a9b2",
          "name": "Niklas Muennighoff",
          "hidden": false
        },
        {
          "_id": "68ede08ede1fee572713a9b3",
          "name": "Kenneth Enevoldsen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-11T06:56:53.000Z",
      "submittedOnDailyAt": "2025-10-14T04:06:31.362Z",
      "title": "HUME: Measuring the Human-Model Performance Gap in Text Embedding Task",
      "submittedOnDailyBy": {
        "_id": "64cc0e80a257a3212c0c4b24",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64cc0e80a257a3212c0c4b24/wqs6WZN8-3OQthcnQXgN7.png",
        "isPro": false,
        "fullname": "Isaac Chung",
        "user": "isaacchung",
        "type": "user"
      },
      "summary": "Comparing human and model performance offers a valuable perspective for\nunderstanding the strengths and limitations of embedding models, highlighting\nwhere they succeed and where they fail to capture meaning and nuance. However,\nsuch comparisons are rarely made, as human performance on embedding tasks is\ndifficult to measure. To fill this gap, we introduce HUME: Human Evaluation\nFramework for Text Embeddings. While frameworks like MTEB provide broad model\nevaluation, they lack reliable estimates of human performance, limiting the\ninterpretability of model scores. We measure human performance across 16 MTEB\ndatasets spanning reranking, classification, clustering, and semantic textual\nsimilarity across linguistically diverse high- and low-resource languages.\nHumans achieve an average performance of 77.6% compared to 80.1% for the best\nembedding model, although variation is substantial: models reach near-ceiling\nperformance on some datasets while struggling on others, suggesting dataset\nissues and revealing shortcomings in low-resource languages. We provide human\nperformance baselines, insight into task difficulty patterns, and an extensible\nevaluation framework that enables a more meaningful interpretation of the model\nand informs the development of both models and benchmarks. Our code, dataset,\nand leaderboard are publicly available at\nhttps://github.com/embeddings-benchmark/mteb.",
      "upvotes": 2,
      "discussionId": "68ede08ede1fee572713a9b4",
      "ai_summary": "HUME provides human performance baselines for text embedding tasks, enhancing the interpretability of model evaluations and revealing dataset and language-specific challenges.",
      "ai_keywords": [
        "embedding models",
        "HUME",
        "Human Evaluation Framework for Text Embeddings",
        "MTEB",
        "reranking",
        "classification",
        "clustering",
        "semantic textual similarity",
        "low-resource languages",
        "dataset issues",
        "model development",
        "benchmarks"
      ],
      "organization": {
        "_id": "624bfda5459c48438cc39f80",
        "name": "mteb",
        "fullname": "Massive Text Embedding Benchmark",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664267264786-5f1eb362eec0ad2a071ad6e2.png"
      }
    },
    "publishedAt": "2025-10-11T02:56:53.000Z",
    "title": "HUME: Measuring the Human-Model Performance Gap in Text Embedding Task",
    "summary": "Comparing human and model performance offers a valuable perspective for\nunderstanding the strengths and limitations of embedding models, highlighting\nwhere they succeed and where they fail to capture meaning and nuance. However,\nsuch comparisons are rarely made, as human performance on embedding tasks is\ndifficult to measure. To fill this gap, we introduce HUME: Human Evaluation\nFramework for Text Embeddings. While frameworks like MTEB provide broad model\nevaluation, they lack reliable estimates of human performance, limiting the\ninterpretability of model scores. We measure human performance across 16 MTEB\ndatasets spanning reranking, classification, clustering, and semantic textual\nsimilarity across linguistically diverse high- and low-resource languages.\nHumans achieve an average performance of 77.6% compared to 80.1% for the best\nembedding model, although variation is substantial: models reach near-ceiling\nperformance on some datasets while struggling on others, suggesting dataset\nissues and revealing shortcomings in low-resource languages. We provide human\nperformance baselines, insight into task difficulty patterns, and an extensible\nevaluation framework that enables a more meaningful interpretation of the model\nand informs the development of both models and benchmarks. Our code, dataset,\nand leaderboard are publicly available at\nhttps://github.com/embeddings-benchmark/mteb.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.10062.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64cc0e80a257a3212c0c4b24",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64cc0e80a257a3212c0c4b24/wqs6WZN8-3OQthcnQXgN7.png",
      "fullname": "Isaac Chung",
      "name": "isaacchung",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "organization": {
      "_id": "624bfda5459c48438cc39f80",
      "name": "mteb",
      "fullname": "Massive Text Embedding Benchmark",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664267264786-5f1eb362eec0ad2a071ad6e2.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.08744",
      "authors": [
        {
          "_id": "68edad48de1fee572713a720",
          "name": "Gang Liu",
          "hidden": false
        },
        {
          "_id": "68edad48de1fee572713a721",
          "name": "Jie Chen",
          "hidden": false
        },
        {
          "_id": "68edad48de1fee572713a722",
          "name": "Yihan Zhu",
          "hidden": false
        },
        {
          "_id": "68edad48de1fee572713a723",
          "name": "Michael Sun",
          "hidden": false
        },
        {
          "_id": "68edad48de1fee572713a724",
          "name": "Tengfei Luo",
          "hidden": false
        },
        {
          "_id": "68edad48de1fee572713a725",
          "name": "Nitesh V Chawla",
          "hidden": false
        },
        {
          "_id": "68edad48de1fee572713a726",
          "name": "Meng Jiang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-09T18:56:57.000Z",
      "submittedOnDailyAt": "2025-10-14T01:04:16.487Z",
      "title": "Graph Diffusion Transformers are In-Context Molecular Designers",
      "submittedOnDailyBy": {
        "_id": "63bf1afd4a2beec65565ee90",
        "avatarUrl": "/avatars/134ac63b443e1360be7c91f84f9d5ec7.svg",
        "isPro": false,
        "fullname": "Gang Liu",
        "user": "liuganghuggingface",
        "type": "user"
      },
      "summary": "In-context learning allows large models to adapt to new tasks from a few\ndemonstrations, but it has shown limited success in molecular design. Existing\ndatabases such as ChEMBL contain molecular properties spanning millions of\nbiological assays, yet labeled data for each property remain scarce. To address\nthis limitation, we introduce demonstration-conditioned diffusion models\n(DemoDiff), which define task contexts using a small set of molecule-score\nexamples instead of text descriptions. These demonstrations guide a denoising\nTransformer to generate molecules aligned with target properties. For scalable\npretraining, we develop a new molecular tokenizer with Node Pair Encoding that\nrepresents molecules at the motif level, requiring 5.5times fewer nodes. We\ncurate a dataset containing millions of context tasks from multiple sources\ncovering both drugs and materials, and pretrain a 0.7-billion-parameter model\non it. Across 33 design tasks in six categories, DemoDiff matches or surpasses\nlanguage models 100-1000times larger and achieves an average rank of 3.63\ncompared to 5.25-10.20 for domain-specific approaches. These results position\nDemoDiff as a molecular foundation model for in-context molecular design. Our\ncode is available at https://github.com/liugangcode/DemoDiff.",
      "upvotes": 2,
      "discussionId": "68edad48de1fee572713a727",
      "githubRepo": "https://github.com/liugangcode/DemoDiff",
      "ai_summary": "DemoDiff, a demonstration-conditioned diffusion model, uses molecule-score examples to guide a denoising Transformer for molecular design, outperforming larger language models and domain-specific approaches.",
      "ai_keywords": [
        "in-context learning",
        "diffusion models",
        "denoising Transformer",
        "molecular tokenizer",
        "Node Pair Encoding",
        "molecular design",
        "pretraining",
        "foundation model"
      ],
      "githubStars": 4
    },
    "publishedAt": "2025-10-09T14:56:57.000Z",
    "title": "Graph Diffusion Transformers are In-Context Molecular Designers",
    "summary": "In-context learning allows large models to adapt to new tasks from a few\ndemonstrations, but it has shown limited success in molecular design. Existing\ndatabases such as ChEMBL contain molecular properties spanning millions of\nbiological assays, yet labeled data for each property remain scarce. To address\nthis limitation, we introduce demonstration-conditioned diffusion models\n(DemoDiff), which define task contexts using a small set of molecule-score\nexamples instead of text descriptions. These demonstrations guide a denoising\nTransformer to generate molecules aligned with target properties. For scalable\npretraining, we develop a new molecular tokenizer with Node Pair Encoding that\nrepresents molecules at the motif level, requiring 5.5times fewer nodes. We\ncurate a dataset containing millions of context tasks from multiple sources\ncovering both drugs and materials, and pretrain a 0.7-billion-parameter model\non it. Across 33 design tasks in six categories, DemoDiff matches or surpasses\nlanguage models 100-1000times larger and achieves an average rank of 3.63\ncompared to 5.25-10.20 for domain-specific approaches. These results position\nDemoDiff as a molecular foundation model for in-context molecular design. Our\ncode is available at https://github.com/liugangcode/DemoDiff.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.08744.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63bf1afd4a2beec65565ee90",
      "avatarUrl": "/avatars/134ac63b443e1360be7c91f84f9d5ec7.svg",
      "fullname": "Gang Liu",
      "name": "liuganghuggingface",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.05213",
      "authors": [
        {
          "_id": "68ede69ede1fee572713a9cc",
          "name": "Yixiao Wang",
          "hidden": false
        },
        {
          "_id": "68ede69ede1fee572713a9cd",
          "name": "Mingxiao Huo",
          "hidden": false
        },
        {
          "_id": "68ede69ede1fee572713a9ce",
          "name": "Zhixuan Liang",
          "hidden": false
        },
        {
          "_id": "68ede69ede1fee572713a9cf",
          "name": "Yushi Du",
          "hidden": false
        },
        {
          "_id": "68ede69ede1fee572713a9d0",
          "name": "Lingfeng Sun",
          "hidden": false
        },
        {
          "_id": "68ede69ede1fee572713a9d1",
          "name": "Haotian Lin",
          "hidden": false
        },
        {
          "_id": "68ede69ede1fee572713a9d2",
          "name": "Jinghuan Shang",
          "hidden": false
        },
        {
          "_id": "68ede69ede1fee572713a9d3",
          "name": "Chensheng Peng",
          "hidden": false
        },
        {
          "_id": "68ede69ede1fee572713a9d4",
          "name": "Mohit Bansal",
          "hidden": false
        },
        {
          "_id": "68ede69ede1fee572713a9d5",
          "name": "Mingyu Ding",
          "hidden": false
        },
        {
          "_id": "68ede69ede1fee572713a9d6",
          "name": "Masayoshi Tomizuka",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-06T18:00:43.000Z",
      "submittedOnDailyAt": "2025-10-14T04:30:03.749Z",
      "title": "VER: Vision Expert Transformer for Robot Learning via Foundation\n  Distillation and Dynamic Routing",
      "submittedOnDailyBy": {
        "_id": "662a471e94baa018b00c0f5c",
        "avatarUrl": "/avatars/62a67a2ee6e4b9a7124f8b02b9b3f280.svg",
        "isPro": false,
        "fullname": "Zhixuan Liang",
        "user": "Liang-ZX",
        "type": "user"
      },
      "summary": "Pretrained vision foundation models (VFMs) advance robotic learning via rich\nvisual representations, yet individual VFMs typically excel only in specific\ndomains, limiting generality across tasks. Distilling multiple VFMs into a\nunified representation for policy can mitigate this limitation but often yields\ninflexible task-specific feature selection and requires costly full re-training\nto incorporate robot-domain knowledge. We propose VER, a Vision Expert\ntransformer for Robot learning. During pretraining, VER distills multiple VFMs\ninto a vision expert library. It then fine-tunes only a lightweight routing\nnetwork (fewer than 0.4% of parameters) to dynamically select task-relevant\nexperts from the pretrained library for downstream robot tasks. We further\nintroduce Patchwise Expert Routing with Curriculum Top-K Annealing to improve\nboth flexibility and precision of dynamic expert selection. Moreover, VER\nsupports parameter-efficient finetuning for scalable expert utilization and\nadaptive robot-domain knowledge integration. Across 17 diverse robotic tasks\nand multiple policy heads, VER achieves state-of-the-art performance. We find\nthat VER reduces large-norm outliers in task-irrelevant regions (e.g.,\nbackground) and concentrates on task-critical regions. Visualizations and codes\ncan be found in https://yixiaowang7.github.io/ver_page/.",
      "upvotes": 2,
      "discussionId": "68ede69fde1fee572713a9d7",
      "ai_summary": "VER, a Vision Expert Transformer, dynamically selects task-relevant experts from a pretrained vision expert library, achieving state-of-the-art performance across diverse robotic tasks with parameter-efficient fine-tuning.",
      "ai_keywords": [
        "pretrained vision foundation models",
        "VFMs",
        "unified representation",
        "policy",
        "routing network",
        "Patchwise Expert Routing",
        "Curriculum Top-K Annealing",
        "parameter-efficient fine-tuning",
        "expert utilization",
        "adaptive robot-domain knowledge integration"
      ],
      "organization": {
        "_id": "61f20a9ce108f2cba2dc0730",
        "name": "Berkeley",
        "fullname": "UC Berkeley",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/0FjsTg2txEZZ4dEgmMnQL.png"
      }
    },
    "publishedAt": "2025-10-06T14:00:43.000Z",
    "title": "VER: Vision Expert Transformer for Robot Learning via Foundation\n  Distillation and Dynamic Routing",
    "summary": "Pretrained vision foundation models (VFMs) advance robotic learning via rich\nvisual representations, yet individual VFMs typically excel only in specific\ndomains, limiting generality across tasks. Distilling multiple VFMs into a\nunified representation for policy can mitigate this limitation but often yields\ninflexible task-specific feature selection and requires costly full re-training\nto incorporate robot-domain knowledge. We propose VER, a Vision Expert\ntransformer for Robot learning. During pretraining, VER distills multiple VFMs\ninto a vision expert library. It then fine-tunes only a lightweight routing\nnetwork (fewer than 0.4% of parameters) to dynamically select task-relevant\nexperts from the pretrained library for downstream robot tasks. We further\nintroduce Patchwise Expert Routing with Curriculum Top-K Annealing to improve\nboth flexibility and precision of dynamic expert selection. Moreover, VER\nsupports parameter-efficient finetuning for scalable expert utilization and\nadaptive robot-domain knowledge integration. Across 17 diverse robotic tasks\nand multiple policy heads, VER achieves state-of-the-art performance. We find\nthat VER reduces large-norm outliers in task-irrelevant regions (e.g.,\nbackground) and concentrates on task-critical regions. Visualizations and codes\ncan be found in https://yixiaowang7.github.io/ver_page/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.05213.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "662a471e94baa018b00c0f5c",
      "avatarUrl": "/avatars/62a67a2ee6e4b9a7124f8b02b9b3f280.svg",
      "fullname": "Zhixuan Liang",
      "name": "Liang-ZX",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "organization": {
      "_id": "61f20a9ce108f2cba2dc0730",
      "name": "Berkeley",
      "fullname": "UC Berkeley",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/0FjsTg2txEZZ4dEgmMnQL.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.11647",
      "authors": [
        {
          "_id": "68edc14ade1fee572713a891",
          "name": "Yinan Chen",
          "hidden": false
        },
        {
          "_id": "68edc14ade1fee572713a892",
          "name": "Jiangning Zhang",
          "hidden": false
        },
        {
          "_id": "68edc14ade1fee572713a893",
          "name": "Teng Hu",
          "hidden": false
        },
        {
          "_id": "68edc14ade1fee572713a894",
          "name": "Yuxiang Zeng",
          "hidden": false
        },
        {
          "_id": "68edc14ade1fee572713a895",
          "name": "Zhucun Xue",
          "hidden": false
        },
        {
          "_id": "68edc14ade1fee572713a896",
          "name": "Qingdong He",
          "hidden": false
        },
        {
          "_id": "68edc14ade1fee572713a897",
          "name": "Chengjie Wang",
          "hidden": false
        },
        {
          "_id": "68edc14ade1fee572713a898",
          "name": "Yong Liu",
          "hidden": false
        },
        {
          "_id": "68edc14ade1fee572713a899",
          "name": "Xiaobin Hu",
          "hidden": false
        },
        {
          "_id": "68edc14ade1fee572713a89a",
          "name": "Shuicheng Yan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-13T17:27:08.000Z",
      "submittedOnDailyAt": "2025-10-14T01:49:51.520Z",
      "title": "IVEBench: Modern Benchmark Suite for Instruction-Guided Video Editing\n  Assessment",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Instruction-guided video editing has emerged as a rapidly advancing research\ndirection, offering new opportunities for intuitive content transformation\nwhile also posing significant challenges for systematic evaluation. Existing\nvideo editing benchmarks fail to support the evaluation of instruction-guided\nvideo editing adequately and further suffer from limited source diversity,\nnarrow task coverage and incomplete evaluation metrics. To address the above\nlimitations, we introduce IVEBench, a modern benchmark suite specifically\ndesigned for instruction-guided video editing assessment. IVEBench comprises a\ndiverse database of 600 high-quality source videos, spanning seven semantic\ndimensions, and covering video lengths ranging from 32 to 1,024 frames. It\nfurther includes 8 categories of editing tasks with 35 subcategories, whose\nprompts are generated and refined through large language models and expert\nreview. Crucially, IVEBench establishes a three-dimensional evaluation protocol\nencompassing video quality, instruction compliance and video fidelity,\nintegrating both traditional metrics and multimodal large language model-based\nassessments. Extensive experiments demonstrate the effectiveness of IVEBench in\nbenchmarking state-of-the-art instruction-guided video editing methods, showing\nits ability to provide comprehensive and human-aligned evaluation outcomes.",
      "upvotes": 1,
      "discussionId": "68edc14bde1fee572713a89b",
      "projectPage": "https://ryanchenyn.github.io/projects/IVEBench/",
      "ai_summary": "IVEBench is a benchmark suite for instruction-guided video editing that addresses limitations in existing benchmarks through diverse video sources, comprehensive task coverage, and a multi-dimensional evaluation protocol.",
      "ai_keywords": [
        "instruction-guided video editing",
        "benchmark suite",
        "source diversity",
        "task coverage",
        "evaluation metrics",
        "large language models",
        "video quality",
        "instruction compliance",
        "video fidelity",
        "multimodal assessments"
      ]
    },
    "publishedAt": "2025-10-13T13:27:08.000Z",
    "title": "IVEBench: Modern Benchmark Suite for Instruction-Guided Video Editing\n  Assessment",
    "summary": "Instruction-guided video editing has emerged as a rapidly advancing research\ndirection, offering new opportunities for intuitive content transformation\nwhile also posing significant challenges for systematic evaluation. Existing\nvideo editing benchmarks fail to support the evaluation of instruction-guided\nvideo editing adequately and further suffer from limited source diversity,\nnarrow task coverage and incomplete evaluation metrics. To address the above\nlimitations, we introduce IVEBench, a modern benchmark suite specifically\ndesigned for instruction-guided video editing assessment. IVEBench comprises a\ndiverse database of 600 high-quality source videos, spanning seven semantic\ndimensions, and covering video lengths ranging from 32 to 1,024 frames. It\nfurther includes 8 categories of editing tasks with 35 subcategories, whose\nprompts are generated and refined through large language models and expert\nreview. Crucially, IVEBench establishes a three-dimensional evaluation protocol\nencompassing video quality, instruction compliance and video fidelity,\nintegrating both traditional metrics and multimodal large language model-based\nassessments. Extensive experiments demonstrate the effectiveness of IVEBench in\nbenchmarking state-of-the-art instruction-guided video editing methods, showing\nits ability to provide comprehensive and human-aligned evaluation outcomes.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.11647.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 126
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.10493",
      "authors": [
        {
          "_id": "68edd1ccde1fee572713a95c",
          "name": "Norbert Tihanyi",
          "hidden": false
        },
        {
          "_id": "68edd1ccde1fee572713a95d",
          "user": {
            "_id": "64d3db80aea0ccb1b4975d95",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Mi0eKzNp6wKFrqketK-DN.png",
            "isPro": false,
            "fullname": "Bilel Cherif",
            "user": "Neo111x",
            "type": "user"
          },
          "name": "Bilel Cherif",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-14T07:29:49.566Z",
          "hidden": false
        },
        {
          "_id": "68edd1ccde1fee572713a95e",
          "name": "Richard A. Dubniczky",
          "hidden": false
        },
        {
          "_id": "68edd1ccde1fee572713a95f",
          "name": "Mohamed Amine Ferrag",
          "hidden": false
        },
        {
          "_id": "68edd1ccde1fee572713a960",
          "name": "Tams Bisztray",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-12T07:51:03.000Z",
      "submittedOnDailyAt": "2025-10-14T03:01:46.544Z",
      "title": "The Hidden DNA of LLM-Generated JavaScript: Structural Patterns Enable\n  High-Accuracy Authorship Attribution",
      "submittedOnDailyBy": {
        "_id": "64d3db80aea0ccb1b4975d95",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Mi0eKzNp6wKFrqketK-DN.png",
        "isPro": false,
        "fullname": "Bilel Cherif",
        "user": "Neo111x",
        "type": "user"
      },
      "summary": "In this paper, we present the first large-scale study exploring whether\nJavaScript code generated by Large Language Models (LLMs) can reveal which\nmodel produced it, enabling reliable authorship attribution and model\nfingerprinting. With the rapid rise of AI-generated code, attribution is\nplaying a critical role in detecting vulnerabilities, flagging malicious\ncontent, and ensuring accountability. While AI-vs-human detection usually\ntreats AI as a single category we show that individual LLMs leave unique\nstylistic signatures, even among models belonging to the same family or\nparameter size. To this end, we introduce LLM-NodeJS, a dataset of 50,000\nNode.js back-end programs from 20 large language models. Each has four\ntransformed variants, yielding 250,000 unique JavaScript samples and two\nadditional representations (JSIR and AST) for diverse research applications.\nUsing this dataset, we benchmark traditional machine learning classifiers\nagainst fine-tuned Transformer encoders and introduce CodeT5-JSA, a custom\narchitecture derived from the 770M-parameter CodeT5 model with its decoder\nremoved and a modified classification head. It achieves 95.8% accuracy on\nfive-class attribution, 94.6% on ten-class, and 88.5% on twenty-class tasks,\nsurpassing other tested models such as BERT, CodeBERT, and Longformer. We\ndemonstrate that classifiers capture deeper stylistic regularities in program\ndataflow and structure, rather than relying on surface-level features. As a\nresult, attribution remains effective even after mangling, comment removal, and\nheavy code transformations. To support open science and reproducibility, we\nrelease the LLM-NodeJS dataset, Google Colab training scripts, and all related\nmaterials on GitHub: https://github.com/LLM-NodeJS-dataset.",
      "upvotes": 1,
      "discussionId": "68edd1ccde1fee572713a961",
      "projectPage": "https://github.com/LLM-NodeJS-dataset",
      "githubRepo": "https://github.com/LLM-NodeJS-dataset/LLM-NodeJS-dataset",
      "ai_summary": "A study on authorship attribution of JavaScript code generated by large language models using a custom dataset and advanced machine learning classifiers demonstrates high accuracy even after code transformations.",
      "ai_keywords": [
        "Large Language Models",
        "LLM-NodeJS",
        "Node.js",
        "JavaScript",
        "machine learning classifiers",
        "Transformer encoders",
        "CodeT5-JSA",
        "CodeT5",
        "BERT",
        "CodeBERT",
        "Longformer",
        "authorship attribution",
        "model fingerprinting",
        "dataflow",
        "structure",
        "code transformations"
      ],
      "githubStars": 0
    },
    "publishedAt": "2025-10-12T03:51:03.000Z",
    "title": "The Hidden DNA of LLM-Generated JavaScript: Structural Patterns Enable\n  High-Accuracy Authorship Attribution",
    "summary": "In this paper, we present the first large-scale study exploring whether\nJavaScript code generated by Large Language Models (LLMs) can reveal which\nmodel produced it, enabling reliable authorship attribution and model\nfingerprinting. With the rapid rise of AI-generated code, attribution is\nplaying a critical role in detecting vulnerabilities, flagging malicious\ncontent, and ensuring accountability. While AI-vs-human detection usually\ntreats AI as a single category we show that individual LLMs leave unique\nstylistic signatures, even among models belonging to the same family or\nparameter size. To this end, we introduce LLM-NodeJS, a dataset of 50,000\nNode.js back-end programs from 20 large language models. Each has four\ntransformed variants, yielding 250,000 unique JavaScript samples and two\nadditional representations (JSIR and AST) for diverse research applications.\nUsing this dataset, we benchmark traditional machine learning classifiers\nagainst fine-tuned Transformer encoders and introduce CodeT5-JSA, a custom\narchitecture derived from the 770M-parameter CodeT5 model with its decoder\nremoved and a modified classification head. It achieves 95.8% accuracy on\nfive-class attribution, 94.6% on ten-class, and 88.5% on twenty-class tasks,\nsurpassing other tested models such as BERT, CodeBERT, and Longformer. We\ndemonstrate that classifiers capture deeper stylistic regularities in program\ndataflow and structure, rather than relying on surface-level features. As a\nresult, attribution remains effective even after mangling, comment removal, and\nheavy code transformations. To support open science and reproducibility, we\nrelease the LLM-NodeJS dataset, Google Colab training scripts, and all related\nmaterials on GitHub: https://github.com/LLM-NodeJS-dataset.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.10493.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64d3db80aea0ccb1b4975d95",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Mi0eKzNp6wKFrqketK-DN.png",
      "fullname": "Bilel Cherif",
      "name": "Neo111x",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.04587",
      "authors": [
        {
          "_id": "68edadefde1fee572713a739",
          "name": "Sheng Wang",
          "hidden": false
        },
        {
          "_id": "68edadefde1fee572713a73a",
          "name": "Ruiming Wu",
          "hidden": false
        },
        {
          "_id": "68edadefde1fee572713a73b",
          "name": "Charles Herndon",
          "hidden": false
        },
        {
          "_id": "68edadefde1fee572713a73c",
          "name": "Yihang Liu",
          "hidden": false
        },
        {
          "_id": "68edadefde1fee572713a73d",
          "name": "Shunsuke Koga",
          "hidden": false
        },
        {
          "_id": "68edadefde1fee572713a73e",
          "name": "Jeanne Shen",
          "hidden": false
        },
        {
          "_id": "68edadefde1fee572713a73f",
          "name": "Zhi Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-06T08:44:04.000Z",
      "submittedOnDailyAt": "2025-10-14T00:28:21.760Z",
      "title": "Pathology-CoT: Learning Visual Chain-of-Thought Agent from Expert Whole\n  Slide Image Diagnosis Behavior",
      "submittedOnDailyBy": {
        "_id": "634b90db3a0cd2d498640479",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634b90db3a0cd2d498640479/hsM58SlJHK_PnLN2f2esf.jpeg",
        "isPro": false,
        "fullname": "Zhi Huang",
        "user": "zhihuang",
        "type": "user"
      },
      "summary": "Diagnosing a whole-slide image is an interactive, multi-stage process\ninvolving changes in magnification and movement between fields. Although recent\npathology foundation models are strong, practical agentic systems that decide\nwhat field to examine next, adjust magnification, and deliver explainable\ndiagnoses are still lacking. The blocker is data: scalable, clinically aligned\nsupervision of expert viewing behavior that is tacit and experience-based, not\nwritten in textbooks or online, and therefore absent from large language model\ntraining. We introduce the AI Session Recorder, which works with standard WSI\nviewers to unobtrusively record routine navigation and convert the viewer logs\ninto standardized behavioral commands (inspect or peek at discrete\nmagnifications) and bounding boxes. A lightweight human-in-the-loop review\nturns AI-drafted rationales into the Pathology-CoT dataset, a form of paired\n\"where to look\" and \"why it matters\" supervision produced at roughly six times\nlower labeling time. Using this behavioral data, we build Pathologist-o3, a\ntwo-stage agent that first proposes regions of interest and then performs\nbehavior-guided reasoning. On gastrointestinal lymph-node metastasis detection,\nit achieved 84.5% precision, 100.0% recall, and 75.4% accuracy, exceeding the\nstate-of-the-art OpenAI o3 model and generalizing across backbones. To our\nknowledge, this constitutes one of the first behavior-grounded agentic systems\nin pathology. Turning everyday viewer logs into scalable, expert-validated\nsupervision, our framework makes agentic pathology practical and establishes a\npath to human-aligned, upgradeable clinical AI.",
      "upvotes": 0,
      "discussionId": "68edadefde1fee572713a740",
      "ai_summary": "A framework records and utilizes expert navigation behavior in whole-slide imaging to build an agentic system for pathology diagnosis, achieving high precision and recall in metastasis detection.",
      "ai_keywords": [
        "whole-slide image",
        "AI Session Recorder",
        "viewer logs",
        "behavioral commands",
        "bounding boxes",
        "Pathology-CoT dataset",
        "Pathologist-o3",
        "regions of interest",
        "behavior-guided reasoning",
        "gastrointestinal lymph-node metastasis detection",
        "OpenAI o3 model"
      ],
      "organization": {
        "_id": "660222ec481b407776cf7eae",
        "name": "zhihuanglab",
        "fullname": "Zhi Huang Lab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/634b90db3a0cd2d498640479/zNqIiWA6p_0yin3Sp7eXb.png"
      }
    },
    "publishedAt": "2025-10-06T04:44:04.000Z",
    "title": "Pathology-CoT: Learning Visual Chain-of-Thought Agent from Expert Whole\n  Slide Image Diagnosis Behavior",
    "summary": "Diagnosing a whole-slide image is an interactive, multi-stage process\ninvolving changes in magnification and movement between fields. Although recent\npathology foundation models are strong, practical agentic systems that decide\nwhat field to examine next, adjust magnification, and deliver explainable\ndiagnoses are still lacking. The blocker is data: scalable, clinically aligned\nsupervision of expert viewing behavior that is tacit and experience-based, not\nwritten in textbooks or online, and therefore absent from large language model\ntraining. We introduce the AI Session Recorder, which works with standard WSI\nviewers to unobtrusively record routine navigation and convert the viewer logs\ninto standardized behavioral commands (inspect or peek at discrete\nmagnifications) and bounding boxes. A lightweight human-in-the-loop review\nturns AI-drafted rationales into the Pathology-CoT dataset, a form of paired\n\"where to look\" and \"why it matters\" supervision produced at roughly six times\nlower labeling time. Using this behavioral data, we build Pathologist-o3, a\ntwo-stage agent that first proposes regions of interest and then performs\nbehavior-guided reasoning. On gastrointestinal lymph-node metastasis detection,\nit achieved 84.5% precision, 100.0% recall, and 75.4% accuracy, exceeding the\nstate-of-the-art OpenAI o3 model and generalizing across backbones. To our\nknowledge, this constitutes one of the first behavior-grounded agentic systems\nin pathology. Turning everyday viewer logs into scalable, expert-validated\nsupervision, our framework makes agentic pathology practical and establishes a\npath to human-aligned, upgradeable clinical AI.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.04587.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "634b90db3a0cd2d498640479",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634b90db3a0cd2d498640479/hsM58SlJHK_PnLN2f2esf.jpeg",
      "fullname": "Zhi Huang",
      "name": "zhihuang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "organization": {
      "_id": "660222ec481b407776cf7eae",
      "name": "zhihuanglab",
      "fullname": "Zhi Huang Lab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/634b90db3a0cd2d498640479/zNqIiWA6p_0yin3Sp7eXb.png"
    },
    "isAuthorParticipating": false
  }
]