[
  {
    "paper": {
      "id": "2505.22617",
      "authors": [
        {
          "_id": "6837cd8fc537d91527323667",
          "user": {
            "_id": "650eba9555dc1e841746f132",
            "avatarUrl": "/avatars/af6f5ee78f161d25ec0afc45d2def8eb.svg",
            "isPro": false,
            "fullname": "Ganqu Cui",
            "user": "ganqu",
            "type": "user"
          },
          "name": "Ganqu Cui",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:09.467Z",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d91527323668",
          "user": {
            "_id": "66e3f8fb5d97b5bb46923444",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/DW806I00-00oQAYvD4ocQ.png",
            "isPro": false,
            "fullname": "Yuchen Zhang",
            "user": "YucZhang2003",
            "type": "user"
          },
          "name": "Yuchen Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:14.207Z",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d91527323669",
          "name": "Jiacheng Chen",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d9152732366a",
          "name": "Lifan Yuan",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d9152732366b",
          "name": "Zhi Wang",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d9152732366c",
          "user": {
            "_id": "622474f38dc6b0b64f5e903d",
            "avatarUrl": "/avatars/d6b60a014277a8ec7d564163c5f644aa.svg",
            "isPro": false,
            "fullname": "Yuxin Zuo",
            "user": "yuxinzuo",
            "type": "user"
          },
          "name": "Yuxin Zuo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:12.022Z",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d9152732366d",
          "name": "Haozhan Li",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d9152732366e",
          "name": "Yuchen Fan",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d9152732366f",
          "name": "Huayu Chen",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d91527323670",
          "name": "Weize Chen",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d91527323671",
          "name": "Zhiyuan Liu",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d91527323672",
          "name": "Hao Peng",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d91527323673",
          "name": "Lei Bai",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d91527323674",
          "name": "Wanli Ouyang",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d91527323675",
          "name": "Yu Cheng",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d91527323676",
          "name": "Bowen Zhou",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d91527323677",
          "user": {
            "_id": "60cf4bcb1ce3775ebb86e5d5",
            "avatarUrl": "/avatars/12bcd18d215abf91f297f93007733148.svg",
            "isPro": false,
            "fullname": "Ning Ding",
            "user": "stingning",
            "type": "user"
          },
          "name": "Ning Ding",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:16.809Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T17:38:45.000Z",
      "submittedOnDailyAt": "2025-05-29T01:38:57.501Z",
      "title": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language\n  Models",
      "submittedOnDailyBy": {
        "_id": "650eba9555dc1e841746f132",
        "avatarUrl": "/avatars/af6f5ee78f161d25ec0afc45d2def8eb.svg",
        "isPro": false,
        "fullname": "Ganqu Cui",
        "user": "ganqu",
        "type": "user"
      },
      "summary": "This paper aims to overcome a major obstacle in scaling RL for reasoning with\nLLMs, namely the collapse of policy entropy. Such phenomenon is consistently\nobserved across vast RL runs without entropy intervention, where the policy\nentropy dropped sharply at the early training stage, this diminished\nexploratory ability is always accompanied with the saturation of policy\nperformance. In practice, we establish a transformation equation R=-a*e^H+b\nbetween entropy H and downstream performance R. This empirical law strongly\nindicates that, the policy performance is traded from policy entropy, thus\nbottlenecked by its exhaustion, and the ceiling is fully predictable H=0,\nR=-a+b. Our finding necessitates entropy management for continuous exploration\ntoward scaling compute for RL. To this end, we investigate entropy dynamics\nboth theoretically and empirically. Our derivation highlights that, the change\nin policy entropy is driven by the covariance between action probability and\nthe change in logits, which is proportional to its advantage when using Policy\nGradient-like algorithms. Empirical study shows that, the values of covariance\nterm and entropy differences matched exactly, supporting the theoretical\nconclusion. Moreover, the covariance term stays mostly positive throughout\ntraining, further explaining why policy entropy would decrease monotonically.\nThrough understanding the mechanism behind entropy dynamics, we motivate to\ncontrol entropy by restricting the update of high-covariance tokens.\nSpecifically, we propose two simple yet effective techniques, namely Clip-Cov\nand KL-Cov, which clip and apply KL penalty to tokens with high covariances\nrespectively. Experiments show that these methods encourage exploration, thus\nhelping policy escape entropy collapse and achieve better downstream\nperformance.",
      "upvotes": 44,
      "discussionId": "6837cd90c537d9152732369d",
      "ai_summary": "Entropy dynamics in reinforcement learning with large language models are investigated to prevent policy entropy collapse and improve exploration.",
      "ai_keywords": [
        "policy entropy",
        "reinforcement learning",
        "LLMs",
        "entropy intervention",
        "transformation equation",
        "policy performance",
        "entropy dynamics",
        "covariance",
        "action probability",
        "logits",
        "advantage",
        "Policy Gradient",
        "Clip-Cov",
        "KL-Cov"
      ]
    },
    "publishedAt": "2025-05-28T13:38:45.000Z",
    "title": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language\n  Models",
    "summary": "This paper aims to overcome a major obstacle in scaling RL for reasoning with\nLLMs, namely the collapse of policy entropy. Such phenomenon is consistently\nobserved across vast RL runs without entropy intervention, where the policy\nentropy dropped sharply at the early training stage, this diminished\nexploratory ability is always accompanied with the saturation of policy\nperformance. In practice, we establish a transformation equation R=-a*e^H+b\nbetween entropy H and downstream performance R. This empirical law strongly\nindicates that, the policy performance is traded from policy entropy, thus\nbottlenecked by its exhaustion, and the ceiling is fully predictable H=0,\nR=-a+b. Our finding necessitates entropy management for continuous exploration\ntoward scaling compute for RL. To this end, we investigate entropy dynamics\nboth theoretically and empirically. Our derivation highlights that, the change\nin policy entropy is driven by the covariance between action probability and\nthe change in logits, which is proportional to its advantage when using Policy\nGradient-like algorithms. Empirical study shows that, the values of covariance\nterm and entropy differences matched exactly, supporting the theoretical\nconclusion. Moreover, the covariance term stays mostly positive throughout\ntraining, further explaining why policy entropy would decrease monotonically.\nThrough understanding the mechanism behind entropy dynamics, we motivate to\ncontrol entropy by restricting the update of high-covariance tokens.\nSpecifically, we propose two simple yet effective techniques, namely Clip-Cov\nand KL-Cov, which clip and apply KL penalty to tokens with high covariances\nrespectively. Experiments show that these methods encourage exploration, thus\nhelping policy escape entropy collapse and achieve better downstream\nperformance.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22617.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "650eba9555dc1e841746f132",
      "avatarUrl": "/avatars/af6f5ee78f161d25ec0afc45d2def8eb.svg",
      "fullname": "Ganqu Cui",
      "name": "ganqu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 16
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.21600",
      "authors": [
        {
          "_id": "6837bc9c9937bcb69885799c",
          "user": {
            "_id": "6445fd9ba56444c355dcbcba",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6445fd9ba56444c355dcbcba/NCyRCD-MK-yA0_qY6I2y0.png",
            "isPro": false,
            "fullname": "Tianyu Fu",
            "user": "fuvty",
            "type": "user"
          },
          "name": "Tianyu Fu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:43.123Z",
          "hidden": false
        },
        {
          "_id": "6837bc9c9937bcb69885799d",
          "name": "Yi Ge",
          "hidden": false
        },
        {
          "_id": "6837bc9c9937bcb69885799e",
          "user": {
            "_id": "66954ebfbcd81f395e9dca37",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66954ebfbcd81f395e9dca37/0C3m5YdxyXuK7dJBu4AdL.png",
            "isPro": false,
            "fullname": "yichen you",
            "user": "youyc22",
            "type": "user"
          },
          "name": "Yichen You",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:40.790Z",
          "hidden": false
        },
        {
          "_id": "6837bc9c9937bcb69885799f",
          "name": "Enshu Liu",
          "hidden": false
        },
        {
          "_id": "6837bc9c9937bcb6988579a0",
          "name": "Zhihang Yuan",
          "hidden": false
        },
        {
          "_id": "6837bc9c9937bcb6988579a1",
          "name": "Guohao Dai",
          "hidden": false
        },
        {
          "_id": "6837bc9c9937bcb6988579a2",
          "name": "Shengen Yan",
          "hidden": false
        },
        {
          "_id": "6837bc9c9937bcb6988579a3",
          "name": "Huazhong Yang",
          "hidden": false
        },
        {
          "_id": "6837bc9c9937bcb6988579a4",
          "name": "Yu Wang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6445fd9ba56444c355dcbcba/edz42EmKaJTdax2mDUmhh.mp4"
      ],
      "publishedAt": "2025-05-27T16:57:20.000Z",
      "submittedOnDailyAt": "2025-05-29T00:18:54.118Z",
      "title": "R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large\n  Model Token Routing",
      "submittedOnDailyBy": {
        "_id": "6445fd9ba56444c355dcbcba",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6445fd9ba56444c355dcbcba/NCyRCD-MK-yA0_qY6I2y0.png",
        "isPro": false,
        "fullname": "Tianyu Fu",
        "user": "fuvty",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) achieve impressive reasoning capabilities at the\ncost of substantial inference overhead, posing substantial deployment\nchallenges. Although distilled Small Language Models (SLMs) significantly\nenhance efficiency, their performance suffers as they fail to follow LLMs'\nreasoning paths. Luckily, we reveal that only a small fraction of tokens\ngenuinely diverge reasoning paths between LLMs and SLMs. Most generated tokens\nare either identical or exhibit neutral differences, such as minor variations\nin abbreviations or expressions. Leveraging this insight, we introduce **Roads\nto Rome (R2R)**, a neural token routing method that selectively utilizes LLMs\nonly for these critical, path-divergent tokens, while leaving the majority of\ntoken generation to the SLM. We also develop an automatic data generation\npipeline that identifies divergent tokens and generates token-level routing\nlabels to train the lightweight router. We apply R2R to combine R1-1.5B and\nR1-32B models from the DeepSeek family, and evaluate on challenging math,\ncoding, and QA benchmarks. With an average activated parameter size of 5.6B,\nR2R surpasses the average accuracy of R1-7B by 1.6x, outperforming even the\nR1-14B model. Compared to R1-32B, it delivers a 2.8x wall-clock speedup with\ncomparable performance, advancing the Pareto frontier of test-time scaling\nefficiency. Our code is available at https://github.com/thu-nics/R2R.",
      "upvotes": 41,
      "discussionId": "6837bc9d9937bcb6988579d1",
      "projectPage": "https://fuvty.github.io/R2R_Project_Page/",
      "githubRepo": "https://github.com/thu-nics/R2R",
      "ai_summary": "Roads to Rome (R2R) selectively utilizes large language models for critical reasoning tasks to enhance efficiency and performance in lightweight models.",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "Small Language Models (SLMs)",
        "token routing",
        "neural token routing",
        "token divergence",
        "token generation",
        "automatic data generation pipeline",
        "token-level routing labels",
        "R1-1.5B",
        "R1-32B",
        "math benchmarks",
        "coding benchmarks",
        "QA benchmarks",
        "parameter size",
        "activated parameters",
        "test-time scaling efficiency",
        "pareto frontier"
      ]
    },
    "publishedAt": "2025-05-27T12:57:20.000Z",
    "title": "R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large\n  Model Token Routing",
    "summary": "Large Language Models (LLMs) achieve impressive reasoning capabilities at the\ncost of substantial inference overhead, posing substantial deployment\nchallenges. Although distilled Small Language Models (SLMs) significantly\nenhance efficiency, their performance suffers as they fail to follow LLMs'\nreasoning paths. Luckily, we reveal that only a small fraction of tokens\ngenuinely diverge reasoning paths between LLMs and SLMs. Most generated tokens\nare either identical or exhibit neutral differences, such as minor variations\nin abbreviations or expressions. Leveraging this insight, we introduce **Roads\nto Rome (R2R)**, a neural token routing method that selectively utilizes LLMs\nonly for these critical, path-divergent tokens, while leaving the majority of\ntoken generation to the SLM. We also develop an automatic data generation\npipeline that identifies divergent tokens and generates token-level routing\nlabels to train the lightweight router. We apply R2R to combine R1-1.5B and\nR1-32B models from the DeepSeek family, and evaluate on challenging math,\ncoding, and QA benchmarks. With an average activated parameter size of 5.6B,\nR2R surpasses the average accuracy of R1-7B by 1.6x, outperforming even the\nR1-14B model. Compared to R1-32B, it delivers a 2.8x wall-clock speedup with\ncomparable performance, advancing the Pareto frontier of test-time scaling\nefficiency. Our code is available at https://github.com/thu-nics/R2R.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6445fd9ba56444c355dcbcba/edz42EmKaJTdax2mDUmhh.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21600.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6445fd9ba56444c355dcbcba",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6445fd9ba56444c355dcbcba/NCyRCD-MK-yA0_qY6I2y0.png",
      "fullname": "Tianyu Fu",
      "name": "fuvty",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22312",
      "authors": [
        {
          "_id": "6837c342cd1601f5bd670255",
          "name": "Jujie He",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd670256",
          "user": {
            "_id": "65643645b09c0b9ece1b8f0e",
            "avatarUrl": "/avatars/d5197103b6e92f765bfda7ed2cc8d53e.svg",
            "isPro": false,
            "fullname": "Jiacai Liu",
            "user": "skydownacai",
            "type": "user"
          },
          "name": "Jiacai Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:28.696Z",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd670257",
          "user": {
            "_id": "658229ef5f6d83438257fce5",
            "avatarUrl": "/avatars/b4417de9a338e95dc69cc547a46348e8.svg",
            "isPro": false,
            "fullname": "Chris (Yuhao) Liu",
            "user": "chrisliu298",
            "type": "user"
          },
          "name": "Chris Yuhao Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:32.117Z",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd670258",
          "name": "Rui Yan",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd670259",
          "name": "Chaojie Wang",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd67025a",
          "name": "Peng Cheng",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd67025b",
          "name": "Xiaoyu Zhang",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd67025c",
          "name": "Fuxiang Zhang",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd67025d",
          "name": "Jiacheng Xu",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd67025e",
          "name": "Wei Shen",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd67025f",
          "name": "Siyuan Li",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd670260",
          "name": "Liang Zeng",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd670261",
          "name": "Tianwen Wei",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd670262",
          "name": "Cheng Cheng",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd670263",
          "name": "Bo An",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd670264",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd670265",
          "name": "Yahui Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T12:56:04.000Z",
      "submittedOnDailyAt": "2025-05-29T00:48:05.741Z",
      "title": "Skywork Open Reasoner 1 Technical Report",
      "submittedOnDailyBy": {
        "_id": "658229ef5f6d83438257fce5",
        "avatarUrl": "/avatars/b4417de9a338e95dc69cc547a46348e8.svg",
        "isPro": false,
        "fullname": "Chris (Yuhao) Liu",
        "user": "chrisliu298",
        "type": "user"
      },
      "summary": "The success of DeepSeek-R1 underscores the significant role of reinforcement\nlearning (RL) in enhancing the reasoning capabilities of large language models\n(LLMs). In this work, we present Skywork-OR1, an effective and scalable RL\nimplementation for long Chain-of-Thought (CoT) models. Building on the\nDeepSeek-R1-Distill model series, our RL approach achieves notable performance\ngains, increasing average accuracy across AIME24, AIME25, and LiveCodeBench\nfrom 57.8% to 72.8% (+15.0%) for the 32B model and from 43.6% to 57.5% (+13.9%)\nfor the 7B model. Our Skywork-OR1-32B model surpasses both DeepSeek-R1 and\nQwen3-32B on the AIME24 and AIME25 benchmarks, while achieving comparable\nresults on LiveCodeBench. The Skywork-OR1-7B and Skywork-OR1-Math-7B models\ndemonstrate competitive reasoning capabilities among models of similar size. We\nperform comprehensive ablation studies on the core components of our training\npipeline to validate their effectiveness. Additionally, we thoroughly\ninvestigate the phenomenon of entropy collapse, identify key factors affecting\nentropy dynamics, and demonstrate that mitigating premature entropy collapse is\ncritical for improved test performance. To support community research, we fully\nopen-source our model weights, training code, and training datasets.",
      "upvotes": 35,
      "discussionId": "6837c344cd1601f5bd6702dd",
      "githubRepo": "https://github.com/SkyworkAI/Skywork-OR1",
      "ai_summary": "Skywork-OR1 is a reinforcement learning approach for long Chain-of-Thought models that improves accuracy over DeepSeek-R1 across various benchmarks by addressing entropy collapse.",
      "ai_keywords": [
        "reinforcement learning",
        "LLMs",
        "Chain-of-Thought",
        "Skywork-OR1",
        "DeepSeek-R1-Distill",
        "AIME24",
        "AIME25",
        "LiveCodeBench",
        "entropy collapse",
        "ablation studies"
      ]
    },
    "publishedAt": "2025-05-28T08:56:04.000Z",
    "title": "Skywork Open Reasoner 1 Technical Report",
    "summary": "The success of DeepSeek-R1 underscores the significant role of reinforcement\nlearning (RL) in enhancing the reasoning capabilities of large language models\n(LLMs). In this work, we present Skywork-OR1, an effective and scalable RL\nimplementation for long Chain-of-Thought (CoT) models. Building on the\nDeepSeek-R1-Distill model series, our RL approach achieves notable performance\ngains, increasing average accuracy across AIME24, AIME25, and LiveCodeBench\nfrom 57.8% to 72.8% (+15.0%) for the 32B model and from 43.6% to 57.5% (+13.9%)\nfor the 7B model. Our Skywork-OR1-32B model surpasses both DeepSeek-R1 and\nQwen3-32B on the AIME24 and AIME25 benchmarks, while achieving comparable\nresults on LiveCodeBench. The Skywork-OR1-7B and Skywork-OR1-Math-7B models\ndemonstrate competitive reasoning capabilities among models of similar size. We\nperform comprehensive ablation studies on the core components of our training\npipeline to validate their effectiveness. Additionally, we thoroughly\ninvestigate the phenomenon of entropy collapse, identify key factors affecting\nentropy dynamics, and demonstrate that mitigating premature entropy collapse is\ncritical for improved test performance. To support community research, we fully\nopen-source our model weights, training code, and training datasets.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22312.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "658229ef5f6d83438257fce5",
      "avatarUrl": "/avatars/b4417de9a338e95dc69cc547a46348e8.svg",
      "fullname": "Chris (Yuhao) Liu",
      "name": "chrisliu298",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22453",
      "authors": [
        {
          "_id": "6837c318a4e378954486e45d",
          "user": {
            "_id": "64a16b1aeacb4b50ba1c889d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a16b1aeacb4b50ba1c889d/EhWoBGL6LlFGIVTUsp2F4.jpeg",
            "isPro": false,
            "fullname": "Lai Wei",
            "user": "WaltonFuture",
            "type": "user"
          },
          "name": "Lai Wei",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:35.145Z",
          "hidden": false
        },
        {
          "_id": "6837c318a4e378954486e45e",
          "name": "Yuting Li",
          "hidden": false
        },
        {
          "_id": "6837c318a4e378954486e45f",
          "name": "Chen Wang",
          "hidden": false
        },
        {
          "_id": "6837c318a4e378954486e460",
          "user": {
            "_id": "65e095da35ad8b2fe8c80e71",
            "avatarUrl": "/avatars/225d4be6411dcec2460047ea1a88a5c3.svg",
            "isPro": false,
            "fullname": "Weiran Huang",
            "user": "weiranhuang",
            "type": "user"
          },
          "name": "Yue Wang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-29T02:14:48.838Z",
          "hidden": false
        },
        {
          "_id": "6837c318a4e378954486e461",
          "name": "Linghe Kong",
          "hidden": false
        },
        {
          "_id": "6837c318a4e378954486e462",
          "user": {
            "_id": "65e095da35ad8b2fe8c80e71",
            "avatarUrl": "/avatars/225d4be6411dcec2460047ea1a88a5c3.svg",
            "isPro": false,
            "fullname": "Weiran Huang",
            "user": "weiranhuang",
            "type": "user"
          },
          "name": "Weiran Huang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-29T02:14:48.838Z",
          "hidden": false
        },
        {
          "_id": "6837c318a4e378954486e463",
          "name": "Lichao Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T15:11:16.000Z",
      "submittedOnDailyAt": "2025-05-29T00:45:28.570Z",
      "title": "Unsupervised Post-Training for Multi-Modal LLM Reasoning via GRPO",
      "submittedOnDailyBy": {
        "_id": "64a16b1aeacb4b50ba1c889d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a16b1aeacb4b50ba1c889d/EhWoBGL6LlFGIVTUsp2F4.jpeg",
        "isPro": false,
        "fullname": "Lai Wei",
        "user": "WaltonFuture",
        "type": "user"
      },
      "summary": "Improving Multi-modal Large Language Models (MLLMs) in the post-training\nstage typically relies on supervised fine-tuning (SFT) or reinforcement\nlearning (RL). However, these supervised methods require expensive and manually\nannotated multi-modal data--an ultimately unsustainable resource. While recent\nefforts have explored unsupervised post-training, their methods are complex and\ndifficult to iterate. In this work, we are the first to investigate the use of\nGRPO, a stable and scalable online RL algorithm, for enabling continual\nself-improvement without any external supervision. We propose MM-UPT, a simple\nyet effective framework for unsupervised post-training of MLLMs. MM-UPT builds\nupon GRPO, replacing traditional reward signals with a self-rewarding mechanism\nbased on majority voting over multiple sampled responses. Our experiments\ndemonstrate that MM-UPT significantly improves the reasoning ability of\nQwen2.5-VL-7B (e.g., 66.3 %rightarrow72.9 % on MathVista, 62.9\n%rightarrow68.7 % on We-Math), using standard dataset without ground truth\nlabels. MM-UPT also outperforms prior unsupervised baselines and even\napproaches the results of supervised GRPO. Furthermore, we show that\nincorporating synthetic questions, generated solely by MLLM itself, can boost\nperformance as well, highlighting a promising approach for scalable\nself-improvement. Overall, MM-UPT offers a new paradigm for continual,\nautonomous enhancement of MLLMs in the absence of external supervision. Our\ncode is available at https://github.com/waltonfuture/MM-UPT.",
      "upvotes": 28,
      "discussionId": "6837c318a4e378954486e48a",
      "projectPage": "https://github.com/waltonfuture/MM-UPT",
      "githubRepo": "https://github.com/waltonfuture/MM-UPT",
      "ai_summary": "MM-UPT, a framework employing GRPO and self-rewarding, enhances multi-modal LLMs through unsupervised continual learning, showing performance improvements without manual annotations.",
      "ai_keywords": [
        "GRPO",
        "MM-UPT",
        "reinforcement learning",
        "unsupervised post-training",
        "multi-modal large language models",
        "self-rewarding mechanism",
        "majority voting",
        "synthetic questions",
        "MathVista",
        "We-Math"
      ]
    },
    "publishedAt": "2025-05-28T11:11:16.000Z",
    "title": "Unsupervised Post-Training for Multi-Modal LLM Reasoning via GRPO",
    "summary": "Improving Multi-modal Large Language Models (MLLMs) in the post-training\nstage typically relies on supervised fine-tuning (SFT) or reinforcement\nlearning (RL). However, these supervised methods require expensive and manually\nannotated multi-modal data--an ultimately unsustainable resource. While recent\nefforts have explored unsupervised post-training, their methods are complex and\ndifficult to iterate. In this work, we are the first to investigate the use of\nGRPO, a stable and scalable online RL algorithm, for enabling continual\nself-improvement without any external supervision. We propose MM-UPT, a simple\nyet effective framework for unsupervised post-training of MLLMs. MM-UPT builds\nupon GRPO, replacing traditional reward signals with a self-rewarding mechanism\nbased on majority voting over multiple sampled responses. Our experiments\ndemonstrate that MM-UPT significantly improves the reasoning ability of\nQwen2.5-VL-7B (e.g., 66.3 %rightarrow72.9 % on MathVista, 62.9\n%rightarrow68.7 % on We-Math), using standard dataset without ground truth\nlabels. MM-UPT also outperforms prior unsupervised baselines and even\napproaches the results of supervised GRPO. Furthermore, we show that\nincorporating synthetic questions, generated solely by MLLM itself, can boost\nperformance as well, highlighting a promising approach for scalable\nself-improvement. Overall, MM-UPT offers a new paradigm for continual,\nautonomous enhancement of MLLMs in the absence of external supervision. Our\ncode is available at https://github.com/waltonfuture/MM-UPT.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22453.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a16b1aeacb4b50ba1c889d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a16b1aeacb4b50ba1c889d/EhWoBGL6LlFGIVTUsp2F4.jpeg",
      "fullname": "Lai Wei",
      "name": "WaltonFuture",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.21136",
      "authors": [
        {
          "_id": "6837c91ec790885f338b8f27",
          "user": {
            "_id": "66c0a08bac74db25de8427ec",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg",
            "isPro": false,
            "fullname": "Jintao Zhang",
            "user": "jt-zhang",
            "type": "user"
          },
          "name": "Jintao Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:22.525Z",
          "hidden": false
        },
        {
          "_id": "6837c91ec790885f338b8f28",
          "name": "Xiaoming Xu",
          "hidden": false
        },
        {
          "_id": "6837c91ec790885f338b8f29",
          "name": "Jia Wei",
          "hidden": false
        },
        {
          "_id": "6837c91ec790885f338b8f2a",
          "name": "Haofeng Huang",
          "hidden": false
        },
        {
          "_id": "6837c91ec790885f338b8f2b",
          "name": "Pengle Zhang",
          "hidden": false
        },
        {
          "_id": "6837c91ec790885f338b8f2c",
          "name": "Chendong Xiang",
          "hidden": false
        },
        {
          "_id": "6837c91ec790885f338b8f2d",
          "name": "Jun Zhu",
          "hidden": false
        },
        {
          "_id": "6837c91ec790885f338b8f2e",
          "name": "Jianfei Chen",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/66c0a08bac74db25de8427ec/e7t1XOditivMjvbyyouoc.png"
      ],
      "publishedAt": "2025-05-27T12:50:36.000Z",
      "submittedOnDailyAt": "2025-05-29T01:12:49.349Z",
      "title": "SageAttention2++: A More Efficient Implementation of SageAttention2",
      "submittedOnDailyBy": {
        "_id": "66c0a08bac74db25de8427ec",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg",
        "isPro": false,
        "fullname": "Jintao Zhang",
        "user": "jt-zhang",
        "type": "user"
      },
      "summary": "The efficiency of attention is critical because its time complexity grows\nquadratically with sequence length. SageAttention2 addresses this by utilizing\nquantization to accelerate matrix multiplications (Matmul) in attention. To\nfurther accelerate SageAttention2, we propose to utilize the faster instruction\nof FP8 Matmul accumulated in FP16. The instruction is 2x faster than the FP8\nMatmul used in SageAttention2. Our experiments show that SageAttention2++\nachieves a 3.9x speedup over FlashAttention while maintaining the same\nattention accuracy as SageAttention2. This means SageAttention2++ effectively\naccelerates various models, including those for language, image, and video\ngeneration, with negligible end-to-end metrics loss. The code will be available\nat https://github.com/thu-ml/SageAttention.",
      "upvotes": 28,
      "discussionId": "6837c923c790885f338b90e5",
      "projectPage": "https://github.com/thu-ml/SageAttention",
      "githubRepo": "https://github.com/thu-ml/SageAttention",
      "ai_summary": "SageAttention2++ improves attention efficiency by using FP8 Matmul in FP16, achieving a 3.9x speedup over FlashAttention without losing accuracy.",
      "ai_keywords": [
        "attention",
        "time complexity",
        "sequence length",
        "quantization",
        "matrix multiplications",
        "Matmul",
        "FP8",
        "FP16",
        "SageAttention2",
        "SageAttention2++",
        "FlashAttention",
        "image generation",
        "video generation"
      ]
    },
    "publishedAt": "2025-05-27T08:50:36.000Z",
    "title": "SageAttention2++: A More Efficient Implementation of SageAttention2",
    "summary": "The efficiency of attention is critical because its time complexity grows\nquadratically with sequence length. SageAttention2 addresses this by utilizing\nquantization to accelerate matrix multiplications (Matmul) in attention. To\nfurther accelerate SageAttention2, we propose to utilize the faster instruction\nof FP8 Matmul accumulated in FP16. The instruction is 2x faster than the FP8\nMatmul used in SageAttention2. Our experiments show that SageAttention2++\nachieves a 3.9x speedup over FlashAttention while maintaining the same\nattention accuracy as SageAttention2. This means SageAttention2++ effectively\naccelerates various models, including those for language, image, and video\ngeneration, with negligible end-to-end metrics loss. The code will be available\nat https://github.com/thu-ml/SageAttention.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/66c0a08bac74db25de8427ec/e7t1XOditivMjvbyyouoc.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21136.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66c0a08bac74db25de8427ec",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg",
      "fullname": "Jintao Zhang",
      "name": "jt-zhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22334",
      "authors": [
        {
          "_id": "6837c360b127cae8a0b36e85",
          "user": {
            "_id": "64a16b1aeacb4b50ba1c889d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a16b1aeacb4b50ba1c889d/EhWoBGL6LlFGIVTUsp2F4.jpeg",
            "isPro": false,
            "fullname": "Lai Wei",
            "user": "WaltonFuture",
            "type": "user"
          },
          "name": "Lai Wei",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:26.705Z",
          "hidden": false
        },
        {
          "_id": "6837c360b127cae8a0b36e86",
          "name": "Yuting Li",
          "hidden": false
        },
        {
          "_id": "6837c360b127cae8a0b36e87",
          "name": "Kaipeng Zheng",
          "hidden": false
        },
        {
          "_id": "6837c360b127cae8a0b36e88",
          "name": "Chen Wang",
          "hidden": false
        },
        {
          "_id": "6837c360b127cae8a0b36e89",
          "user": {
            "_id": "65e095da35ad8b2fe8c80e71",
            "avatarUrl": "/avatars/225d4be6411dcec2460047ea1a88a5c3.svg",
            "isPro": false,
            "fullname": "Weiran Huang",
            "user": "weiranhuang",
            "type": "user"
          },
          "name": "Yue Wang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-29T02:16:03.908Z",
          "hidden": false
        },
        {
          "_id": "6837c360b127cae8a0b36e8a",
          "name": "Linghe Kong",
          "hidden": false
        },
        {
          "_id": "6837c360b127cae8a0b36e8b",
          "name": "Lichao Sun",
          "hidden": false
        },
        {
          "_id": "6837c360b127cae8a0b36e8c",
          "user": {
            "_id": "65e095da35ad8b2fe8c80e71",
            "avatarUrl": "/avatars/225d4be6411dcec2460047ea1a88a5c3.svg",
            "isPro": false,
            "fullname": "Weiran Huang",
            "user": "weiranhuang",
            "type": "user"
          },
          "name": "Weiran Huang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-29T02:16:03.908Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T13:21:38.000Z",
      "submittedOnDailyAt": "2025-05-29T00:46:20.111Z",
      "title": "Advancing Multimodal Reasoning via Reinforcement Learning with Cold\n  Start",
      "submittedOnDailyBy": {
        "_id": "64a16b1aeacb4b50ba1c889d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a16b1aeacb4b50ba1c889d/EhWoBGL6LlFGIVTUsp2F4.jpeg",
        "isPro": false,
        "fullname": "Lai Wei",
        "user": "WaltonFuture",
        "type": "user"
      },
      "summary": "Recent advancements in large language models (LLMs) have demonstrated\nimpressive chain-of-thought reasoning capabilities, with reinforcement learning\n(RL) playing a crucial role in this progress. While \"aha moment\"\npatterns--where models exhibit self-correction through reflection--are often\nattributed to emergent properties from RL, we first demonstrate that these\npatterns exist in multimodal LLMs (MLLMs) prior to RL training but may not\nnecessarily correlate with improved reasoning performance. Building on these\ninsights, we present a comprehensive study on enhancing multimodal reasoning\nthrough a two-stage approach: (1) supervised fine-tuning (SFT) as a cold start\nwith structured chain-of-thought reasoning patterns, followed by (2)\nreinforcement learning via GRPO to further refine these capabilities. Our\nextensive experiments show that this combined approach consistently outperforms\nboth SFT-only and RL-only methods across challenging multimodal reasoning\nbenchmarks. The resulting models achieve state-of-the-art performance among\nopen-source MLLMs at both 3B and 7B scales, with our 7B model showing\nsubstantial improvements over base models (e.g., 66.3 %rightarrow73.4 % on\nMathVista, 62.9 %rightarrow70.4 % on We-Math) and our 3B model achieving\nperformance competitive with several 7B models. Overall, this work provides\npractical guidance for building advanced multimodal reasoning models. Our code\nis available at https://github.com/waltonfuture/RL-with-Cold-Start.",
      "upvotes": 23,
      "discussionId": "6837c363b127cae8a0b36f6f",
      "projectPage": "https://github.com/waltonfuture/RL-with-Cold-Start",
      "githubRepo": "https://github.com/waltonfuture/RL-with-Cold-Start",
      "ai_summary": "A two-stage approach combining supervised fine-tuning and reinforcement learning enhances multimodal reasoning in large language models, achieving state-of-the-art performance on benchmarks.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "multimodal LLMs",
        "MLLMs",
        "reinforcement learning",
        "RL",
        "chain-of-thought reasoning",
        "GRPO",
        "supervised fine-tuning",
        "SFT",
        "multimodal reasoning",
        "MathVista",
        "We-Math"
      ]
    },
    "publishedAt": "2025-05-28T09:21:38.000Z",
    "title": "Advancing Multimodal Reasoning via Reinforcement Learning with Cold\n  Start",
    "summary": "Recent advancements in large language models (LLMs) have demonstrated\nimpressive chain-of-thought reasoning capabilities, with reinforcement learning\n(RL) playing a crucial role in this progress. While \"aha moment\"\npatterns--where models exhibit self-correction through reflection--are often\nattributed to emergent properties from RL, we first demonstrate that these\npatterns exist in multimodal LLMs (MLLMs) prior to RL training but may not\nnecessarily correlate with improved reasoning performance. Building on these\ninsights, we present a comprehensive study on enhancing multimodal reasoning\nthrough a two-stage approach: (1) supervised fine-tuning (SFT) as a cold start\nwith structured chain-of-thought reasoning patterns, followed by (2)\nreinforcement learning via GRPO to further refine these capabilities. Our\nextensive experiments show that this combined approach consistently outperforms\nboth SFT-only and RL-only methods across challenging multimodal reasoning\nbenchmarks. The resulting models achieve state-of-the-art performance among\nopen-source MLLMs at both 3B and 7B scales, with our 7B model showing\nsubstantial improvements over base models (e.g., 66.3 %rightarrow73.4 % on\nMathVista, 62.9 %rightarrow70.4 % on We-Math) and our 3B model achieving\nperformance competitive with several 7B models. Overall, this work provides\npractical guidance for building advanced multimodal reasoning models. Our code\nis available at https://github.com/waltonfuture/RL-with-Cold-Start.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22334.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a16b1aeacb4b50ba1c889d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a16b1aeacb4b50ba1c889d/EhWoBGL6LlFGIVTUsp2F4.jpeg",
      "fullname": "Lai Wei",
      "name": "WaltonFuture",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.19253",
      "authors": [
        {
          "_id": "6837bc8d0b39c9653de4d06f",
          "user": {
            "_id": "6244451c9fdefb55a0b900cc",
            "avatarUrl": "/avatars/ca2b46ddb5d905501d827920582b5438.svg",
            "isPro": false,
            "fullname": "Joao Coelho",
            "user": "jmvcoelho",
            "type": "user"
          },
          "name": "João Coelho",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:45.049Z",
          "hidden": false
        },
        {
          "_id": "6837bc8d0b39c9653de4d070",
          "name": "Jingjie Ning",
          "hidden": false
        },
        {
          "_id": "6837bc8d0b39c9653de4d071",
          "name": "Jingyuan He",
          "hidden": false
        },
        {
          "_id": "6837bc8d0b39c9653de4d072",
          "name": "Kangrui Mao",
          "hidden": false
        },
        {
          "_id": "6837bc8d0b39c9653de4d073",
          "name": "Abhijay Paladugu",
          "hidden": false
        },
        {
          "_id": "6837bc8d0b39c9653de4d074",
          "name": "Pranav Setlur",
          "hidden": false
        },
        {
          "_id": "6837bc8d0b39c9653de4d075",
          "name": "Jiahe Jin",
          "hidden": false
        },
        {
          "_id": "6837bc8d0b39c9653de4d076",
          "name": "Jamie Callan",
          "hidden": false
        },
        {
          "_id": "6837bc8d0b39c9653de4d077",
          "name": "João Magalhães",
          "hidden": false
        },
        {
          "_id": "6837bc8d0b39c9653de4d078",
          "name": "Bruno Martins",
          "hidden": false
        },
        {
          "_id": "6837bc8d0b39c9653de4d079",
          "name": "Chenyan Xiong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-25T18:16:13.000Z",
      "submittedOnDailyAt": "2025-05-29T00:28:31.472Z",
      "title": "DeepResearchGym: A Free, Transparent, and Reproducible Evaluation\n  Sandbox for Deep Research",
      "submittedOnDailyBy": {
        "_id": "6135eeeb5bc6ecdf86b60f0d",
        "avatarUrl": "/avatars/43cedcf20ab6b0801a662787400e1384.svg",
        "isPro": false,
        "fullname": "Shi Yu",
        "user": "yushi",
        "type": "user"
      },
      "summary": "Deep research systems represent an emerging class of agentic information\nretrieval methods that generate comprehensive and well-supported reports to\ncomplex queries. However, most existing frameworks rely on dynamic commercial\nsearch APIs, which pose reproducibility and transparency challenges in addition\nto their cost. To address these limitations, we introduce DeepResearchGym, an\nopen-source sandbox that combines a reproducible search API with a rigorous\nevaluation protocol for benchmarking deep research systems. The API indexes\nlarge-scale public web corpora, namely ClueWeb22 and FineWeb, using a\nstate-of-the-art dense retriever and approximate nearest neighbor search via\nDiskANN. It achieves lower latency than popular commercial APIs while ensuring\nstable document rankings across runs, and is freely available for research use.\nTo evaluate deep research systems' outputs, we extend the Researchy Questions\nbenchmark with automatic metrics through LLM-as-a-judge assessments to measure\nalignment with users' information needs, retrieval faithfulness, and report\nquality. Experimental results show that systems integrated with DeepResearchGym\nachieve performance comparable to those using commercial APIs, with performance\nrankings remaining consistent across evaluation metrics. A human evaluation\nstudy further confirms that our automatic protocol aligns with human\npreferences, validating the framework's ability to help support controlled\nassessment of deep research systems. Our code and API documentation are\navailable at https://www.deepresearchgym.ai.",
      "upvotes": 16,
      "discussionId": "6837bc8d0b39c9653de4d0a6",
      "projectPage": "https://www.deepresearchgym.ai",
      "ai_summary": "DeepResearchGym provides an open-source evaluation framework for deep research systems using a reproducible search API and LLM-as-a-judge assessments.",
      "ai_keywords": [
        "agentic information retrieval",
        "deep research systems",
        "search API",
        "reproducibility",
        "transparency",
        "open-source sandbox",
        "ClueWeb22",
        "FineWeb",
        "dense retriever",
        "approximate nearest neighbor search",
        "DiskANN",
        "Researchy Questions benchmark",
        "LLM-as-a-judge",
        "retrieval faithfulness",
        "report quality",
        "human evaluation"
      ]
    },
    "publishedAt": "2025-05-25T14:16:13.000Z",
    "title": "DeepResearchGym: A Free, Transparent, and Reproducible Evaluation\n  Sandbox for Deep Research",
    "summary": "Deep research systems represent an emerging class of agentic information\nretrieval methods that generate comprehensive and well-supported reports to\ncomplex queries. However, most existing frameworks rely on dynamic commercial\nsearch APIs, which pose reproducibility and transparency challenges in addition\nto their cost. To address these limitations, we introduce DeepResearchGym, an\nopen-source sandbox that combines a reproducible search API with a rigorous\nevaluation protocol for benchmarking deep research systems. The API indexes\nlarge-scale public web corpora, namely ClueWeb22 and FineWeb, using a\nstate-of-the-art dense retriever and approximate nearest neighbor search via\nDiskANN. It achieves lower latency than popular commercial APIs while ensuring\nstable document rankings across runs, and is freely available for research use.\nTo evaluate deep research systems' outputs, we extend the Researchy Questions\nbenchmark with automatic metrics through LLM-as-a-judge assessments to measure\nalignment with users' information needs, retrieval faithfulness, and report\nquality. Experimental results show that systems integrated with DeepResearchGym\nachieve performance comparable to those using commercial APIs, with performance\nrankings remaining consistent across evaluation metrics. A human evaluation\nstudy further confirms that our automatic protocol aligns with human\npreferences, validating the framework's ability to help support controlled\nassessment of deep research systems. Our code and API documentation are\navailable at https://www.deepresearchgym.ai.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19253.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6135eeeb5bc6ecdf86b60f0d",
      "avatarUrl": "/avatars/43cedcf20ab6b0801a662787400e1384.svg",
      "fullname": "Shi Yu",
      "name": "yushi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.22457",
      "authors": [
        {
          "_id": "68380912e9c1608de91e23f3",
          "name": "Haonan Wang",
          "hidden": false
        },
        {
          "_id": "68380912e9c1608de91e23f4",
          "name": "Hongfu Liu",
          "hidden": false
        },
        {
          "_id": "68380912e9c1608de91e23f5",
          "name": "Xiangyan Liu",
          "hidden": false
        },
        {
          "_id": "68380912e9c1608de91e23f6",
          "name": "Chao Du",
          "hidden": false
        },
        {
          "_id": "68380912e9c1608de91e23f7",
          "name": "Kenji Kawaguchi",
          "hidden": false
        },
        {
          "_id": "68380912e9c1608de91e23f8",
          "name": "Ye Wang",
          "hidden": false
        },
        {
          "_id": "68380912e9c1608de91e23f9",
          "name": "Tianyu Pang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T15:13:34.000Z",
      "submittedOnDailyAt": "2025-05-29T05:44:17.072Z",
      "title": "Fostering Video Reasoning via Next-Event Prediction",
      "submittedOnDailyBy": {
        "_id": "63d91b6d255ef6add20e1b38",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675921369867-63d91b6d255ef6add20e1b38.jpeg",
        "isPro": false,
        "fullname": "Tianyu Pang",
        "user": "P2333",
        "type": "user"
      },
      "summary": "Next-token prediction serves as the foundational learning task enabling\nreasoning in LLMs. But what should the learning task be when aiming to equip\nMLLMs with temporal reasoning capabilities over video inputs? Existing tasks\nsuch as video question answering often rely on annotations from humans or much\nstronger MLLMs, while video captioning tends to entangle temporal reasoning\nwith spatial information. To address this gap, we propose next-event prediction\n(NEP), a learning task that harnesses future video segments as a rich,\nself-supervised signal to foster temporal reasoning. We segment each video into\npast and future frames: the MLLM takes the past frames as input and predicts a\nsummary of events derived from the future frames, thereby encouraging the model\nto reason temporally in order to complete the task. To support this task, we\ncurate V1-33K, a dataset comprising 33,000 automatically extracted video\nsegments spanning diverse real-world scenarios. We further explore a range of\nvideo instruction-tuning strategies to study their effects on temporal\nreasoning. To evaluate progress, we introduce FutureBench to assess coherence\nin predicting unseen future events. Experiments validate that NEP offers a\nscalable and effective training paradigm for fostering temporal reasoning in\nMLLMs.",
      "upvotes": 15,
      "discussionId": "68380913e9c1608de91e2430",
      "ai_summary": "Next-event prediction (NEP) is proposed as a learning task to enable MLLMs to reason temporally over video inputs, using future video segments as a self-supervised signal.",
      "ai_keywords": [
        "next-token prediction",
        "next-event prediction (NEP)",
        "LLMs",
        "MLLMs",
        "video question answering",
        "video captioning",
        "temporal reasoning",
        "future video segments",
        "past frames",
        "video segments",
        "V1-33K",
        "video instruction-tuning strategies",
        "FutureBench"
      ]
    },
    "publishedAt": "2025-05-28T11:13:34.000Z",
    "title": "Fostering Video Reasoning via Next-Event Prediction",
    "summary": "Next-token prediction serves as the foundational learning task enabling\nreasoning in LLMs. But what should the learning task be when aiming to equip\nMLLMs with temporal reasoning capabilities over video inputs? Existing tasks\nsuch as video question answering often rely on annotations from humans or much\nstronger MLLMs, while video captioning tends to entangle temporal reasoning\nwith spatial information. To address this gap, we propose next-event prediction\n(NEP), a learning task that harnesses future video segments as a rich,\nself-supervised signal to foster temporal reasoning. We segment each video into\npast and future frames: the MLLM takes the past frames as input and predicts a\nsummary of events derived from the future frames, thereby encouraging the model\nto reason temporally in order to complete the task. To support this task, we\ncurate V1-33K, a dataset comprising 33,000 automatically extracted video\nsegments spanning diverse real-world scenarios. We further explore a range of\nvideo instruction-tuning strategies to study their effects on temporal\nreasoning. To evaluate progress, we introduce FutureBench to assess coherence\nin predicting unseen future events. Experiments validate that NEP offers a\nscalable and effective training paradigm for fostering temporal reasoning in\nMLLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22457.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63d91b6d255ef6add20e1b38",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675921369867-63d91b6d255ef6add20e1b38.jpeg",
      "fullname": "Tianyu Pang",
      "name": "P2333",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.22129",
      "authors": [
        {
          "_id": "6837dae6e9b21653755a05d4",
          "user": {
            "_id": "64c71a5647418a0a59e5c7cb",
            "avatarUrl": "/avatars/a99ab24c0c19b1399d2e6795fb9d7000.svg",
            "isPro": false,
            "fullname": "Jinhong Ni",
            "user": "mcleanie",
            "type": "user"
          },
          "name": "Jinhong Ni",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:03.476Z",
          "hidden": false
        },
        {
          "_id": "6837dae6e9b21653755a05d5",
          "user": {
            "_id": "65434daa5a36a8774d0e2271",
            "avatarUrl": "/avatars/abc3ddec72072121130d581e32cd9045.svg",
            "isPro": false,
            "fullname": "Allen Zhang",
            "user": "allencbzhang",
            "type": "user"
          },
          "name": "Chang-Bin Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:41:59.930Z",
          "hidden": false
        },
        {
          "_id": "6837dae6e9b21653755a05d6",
          "name": "Qiang Zhang",
          "hidden": false
        },
        {
          "_id": "6837dae6e9b21653755a05d7",
          "name": "Jing Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T08:54:04.000Z",
      "submittedOnDailyAt": "2025-05-29T02:38:27.338Z",
      "title": "What Makes for Text to 360-degree Panorama Generation with Stable\n  Diffusion?",
      "submittedOnDailyBy": {
        "_id": "65434daa5a36a8774d0e2271",
        "avatarUrl": "/avatars/abc3ddec72072121130d581e32cd9045.svg",
        "isPro": false,
        "fullname": "Allen Zhang",
        "user": "allencbzhang",
        "type": "user"
      },
      "summary": "Recent prosperity of text-to-image diffusion models, e.g. Stable Diffusion,\nhas stimulated research to adapt them to 360-degree panorama generation. Prior\nwork has demonstrated the feasibility of using conventional low-rank adaptation\ntechniques on pre-trained diffusion models to generate panoramic images.\nHowever, the substantial domain gap between perspective and panoramic images\nraises questions about the underlying mechanisms enabling this empirical\nsuccess. We hypothesize and examine that the trainable counterparts exhibit\ndistinct behaviors when fine-tuned on panoramic data, and such an adaptation\nconceals some intrinsic mechanism to leverage the prior knowledge within the\npre-trained diffusion models. Our analysis reveals the following: 1) the query\nand key matrices in the attention modules are responsible for common\ninformation that can be shared between the panoramic and perspective domains,\nthus are less relevant to panorama generation; and 2) the value and output\nweight matrices specialize in adapting pre-trained knowledge to the panoramic\ndomain, playing a more critical role during fine-tuning for panorama\ngeneration. We empirically verify these insights by introducing a simple\nframework called UniPano, with the objective of establishing an elegant\nbaseline for future research. UniPano not only outperforms existing methods but\nalso significantly reduces memory usage and training time compared to prior\ndual-branch approaches, making it scalable for end-to-end panorama generation\nwith higher resolution. The code will be released.",
      "upvotes": 12,
      "discussionId": "6837daece9b21653755a0791",
      "ai_summary": "Analysis of fine-tuning diffusion models for panoramic image generation reveals distinct roles of attention module matrices and introduces UniPano, a memory-efficient and speed-enhanced baseline framework.",
      "ai_keywords": [
        "text-to-image diffusion models",
        "Stable Diffusion",
        "low-rank adaptation",
        "pre-trained diffusion models",
        "attention modules",
        "query matrices",
        "key matrices",
        "value matrices",
        "output weight matrices",
        "panoramic image generation",
        "domain gap",
        "common information",
        "pre-trained knowledge",
        "UniPano",
        "end-to-end panorama generation",
        "memory usage",
        "training time"
      ]
    },
    "publishedAt": "2025-05-28T04:54:04.000Z",
    "title": "What Makes for Text to 360-degree Panorama Generation with Stable\n  Diffusion?",
    "summary": "Recent prosperity of text-to-image diffusion models, e.g. Stable Diffusion,\nhas stimulated research to adapt them to 360-degree panorama generation. Prior\nwork has demonstrated the feasibility of using conventional low-rank adaptation\ntechniques on pre-trained diffusion models to generate panoramic images.\nHowever, the substantial domain gap between perspective and panoramic images\nraises questions about the underlying mechanisms enabling this empirical\nsuccess. We hypothesize and examine that the trainable counterparts exhibit\ndistinct behaviors when fine-tuned on panoramic data, and such an adaptation\nconceals some intrinsic mechanism to leverage the prior knowledge within the\npre-trained diffusion models. Our analysis reveals the following: 1) the query\nand key matrices in the attention modules are responsible for common\ninformation that can be shared between the panoramic and perspective domains,\nthus are less relevant to panorama generation; and 2) the value and output\nweight matrices specialize in adapting pre-trained knowledge to the panoramic\ndomain, playing a more critical role during fine-tuning for panorama\ngeneration. We empirically verify these insights by introducing a simple\nframework called UniPano, with the objective of establishing an elegant\nbaseline for future research. UniPano not only outperforms existing methods but\nalso significantly reduces memory usage and training time compared to prior\ndual-branch approaches, making it scalable for end-to-end panorama generation\nwith higher resolution. The code will be released.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22129.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65434daa5a36a8774d0e2271",
      "avatarUrl": "/avatars/abc3ddec72072121130d581e32cd9045.svg",
      "fullname": "Allen Zhang",
      "name": "allencbzhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.18600",
      "authors": [
        {
          "_id": "6837fe7664391bba7e477747",
          "name": "Bryan Sangwoo Kim",
          "hidden": false
        },
        {
          "_id": "6837fe7664391bba7e477748",
          "name": "Jeongsol Kim",
          "hidden": false
        },
        {
          "_id": "6837fe7664391bba7e477749",
          "name": "Jong Chul Ye",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-24T08:50:08.000Z",
      "submittedOnDailyAt": "2025-05-29T05:07:53.112Z",
      "title": "Chain-of-Zoom: Extreme Super-Resolution via Scale Autoregression and\n  Preference Alignment",
      "submittedOnDailyBy": {
        "_id": "6628efe14e1fa854f48d3a28",
        "avatarUrl": "/avatars/aa5421149a07a82b5c2a25978f9b6926.svg",
        "isPro": false,
        "fullname": "Sangwoo Kim",
        "user": "bryanswkim",
        "type": "user"
      },
      "summary": "Modern single-image super-resolution (SISR) models deliver photo-realistic\nresults at the scale factors on which they are trained, but collapse when asked\nto magnify far beyond that regime. We address this scalability bottleneck with\nChain-of-Zoom (CoZ), a model-agnostic framework that factorizes SISR into an\nautoregressive chain of intermediate scale-states with multi-scale-aware\nprompts. CoZ repeatedly re-uses a backbone SR model, decomposing the\nconditional probability into tractable sub-problems to achieve extreme\nresolutions without additional training. Because visual cues diminish at high\nmagnifications, we augment each zoom step with multi-scale-aware text prompts\ngenerated by a vision-language model (VLM). The prompt extractor itself is\nfine-tuned using Generalized Reward Policy Optimization (GRPO) with a critic\nVLM, aligning text guidance towards human preference. Experiments show that a\nstandard 4x diffusion SR model wrapped in CoZ attains beyond 256x enlargement\nwith high perceptual quality and fidelity. Project Page:\nhttps://bryanswkim.github.io/chain-of-zoom/ .",
      "upvotes": 12,
      "discussionId": "6837fe7864391bba7e47779b",
      "ai_summary": "Chain-of-Zoom (CoZ) enhances single-image super-resolution models by using an autoregressive chain of intermediate scale-states and multi-scale-aware prompts to achieve extreme magnifications with high quality.",
      "ai_keywords": [
        "single-image super-resolution",
        "Chain-of-Zoom",
        "autoregressive chain",
        "multi-scale-aware prompts",
        "backbone SR model",
        "diffusion SR model",
        "prompt extractor",
        "Generalized Reward Policy Optimization",
        "critic VLM"
      ]
    },
    "publishedAt": "2025-05-24T04:50:08.000Z",
    "title": "Chain-of-Zoom: Extreme Super-Resolution via Scale Autoregression and\n  Preference Alignment",
    "summary": "Modern single-image super-resolution (SISR) models deliver photo-realistic\nresults at the scale factors on which they are trained, but collapse when asked\nto magnify far beyond that regime. We address this scalability bottleneck with\nChain-of-Zoom (CoZ), a model-agnostic framework that factorizes SISR into an\nautoregressive chain of intermediate scale-states with multi-scale-aware\nprompts. CoZ repeatedly re-uses a backbone SR model, decomposing the\nconditional probability into tractable sub-problems to achieve extreme\nresolutions without additional training. Because visual cues diminish at high\nmagnifications, we augment each zoom step with multi-scale-aware text prompts\ngenerated by a vision-language model (VLM). The prompt extractor itself is\nfine-tuned using Generalized Reward Policy Optimization (GRPO) with a critic\nVLM, aligning text guidance towards human preference. Experiments show that a\nstandard 4x diffusion SR model wrapped in CoZ attains beyond 256x enlargement\nwith high perceptual quality and fidelity. Project Page:\nhttps://bryanswkim.github.io/chain-of-zoom/ .",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18600.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6628efe14e1fa854f48d3a28",
      "avatarUrl": "/avatars/aa5421149a07a82b5c2a25978f9b6926.svg",
      "fullname": "Sangwoo Kim",
      "name": "bryanswkim",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.21887",
      "authors": [
        {
          "_id": "6837e3400aa18c6f96fe4876",
          "user": {
            "_id": "656864e12d73834278a8dea7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
            "isPro": true,
            "fullname": "Ahmed Heakl",
            "user": "ahmedheakl",
            "type": "user"
          },
          "name": "Ahmed Heakl",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:41:49.456Z",
          "hidden": false
        },
        {
          "_id": "6837e3400aa18c6f96fe4877",
          "name": "Yahia Salaheldin Shaaban",
          "hidden": false
        },
        {
          "_id": "6837e3400aa18c6f96fe4878",
          "name": "Martin Takac",
          "hidden": false
        },
        {
          "_id": "6837e3400aa18c6f96fe4879",
          "name": "Salem Lahlou",
          "hidden": false
        },
        {
          "_id": "6837e3400aa18c6f96fe487a",
          "name": "Zangir Iklassov",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/Xa51WJrOIV-xl7A-Ry4t2.png",
        "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/eS_bEDm2o2WAbn3YPS2XR.png",
        "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/jaG4WahYyrQGaalZObNew.png",
        "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/mMKY5jn0SCJPxxR8_mssx.png"
      ],
      "publishedAt": "2025-05-28T02:03:31.000Z",
      "submittedOnDailyAt": "2025-05-29T03:02:06.399Z",
      "title": "SVRPBench: A Realistic Benchmark for Stochastic Vehicle Routing Problem",
      "submittedOnDailyBy": {
        "_id": "656864e12d73834278a8dea7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
        "isPro": true,
        "fullname": "Ahmed Heakl",
        "user": "ahmedheakl",
        "type": "user"
      },
      "summary": "Robust routing under uncertainty is central to real-world logistics, yet most\nbenchmarks assume static, idealized settings. We present SVRPBench, the first\nopen benchmark to capture high-fidelity stochastic dynamics in vehicle routing\nat urban scale. Spanning more than 500 instances with up to 1000 customers, it\nsimulates realistic delivery conditions: time-dependent congestion, log-normal\ndelays, probabilistic accidents, and empirically grounded time windows for\nresidential and commercial clients. Our pipeline generates diverse,\nconstraint-rich scenarios, including multi-depot and multi-vehicle setups.\nBenchmarking reveals that state-of-the-art RL solvers like POMO and AM degrade\nby over 20% under distributional shift, while classical and metaheuristic\nmethods remain robust. To enable reproducible research, we release the dataset\nand evaluation suite. SVRPBench challenges the community to design solvers that\ngeneralize beyond synthetic assumptions and adapt to real-world uncertainty.",
      "upvotes": 11,
      "discussionId": "6837e3410aa18c6f96fe48b8",
      "githubRepo": "https://github.com/yehias21/vrp-benchmarks",
      "ai_summary": "SVRPBench introduces a new benchmark for vehicle routing under uncertainty, simulating realistic urban conditions and highlighting the limitations of state-of-the-art RL solvers.",
      "ai_keywords": [
        "vehicle routing",
        "SVRPBench",
        "time-dependent congestion",
        "log-normal delays",
        "probabilistic accidents",
        "multi-depot",
        "multi-vehicle",
        "state-of-the-art RL solvers",
        "POMO",
        "AM",
        "distributional shift",
        "classical methods",
        "metaheuristic methods"
      ]
    },
    "publishedAt": "2025-05-27T22:03:31.000Z",
    "title": "SVRPBench: A Realistic Benchmark for Stochastic Vehicle Routing Problem",
    "summary": "Robust routing under uncertainty is central to real-world logistics, yet most\nbenchmarks assume static, idealized settings. We present SVRPBench, the first\nopen benchmark to capture high-fidelity stochastic dynamics in vehicle routing\nat urban scale. Spanning more than 500 instances with up to 1000 customers, it\nsimulates realistic delivery conditions: time-dependent congestion, log-normal\ndelays, probabilistic accidents, and empirically grounded time windows for\nresidential and commercial clients. Our pipeline generates diverse,\nconstraint-rich scenarios, including multi-depot and multi-vehicle setups.\nBenchmarking reveals that state-of-the-art RL solvers like POMO and AM degrade\nby over 20% under distributional shift, while classical and metaheuristic\nmethods remain robust. To enable reproducible research, we release the dataset\nand evaluation suite. SVRPBench challenges the community to design solvers that\ngeneralize beyond synthetic assumptions and adapt to real-world uncertainty.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/Xa51WJrOIV-xl7A-Ry4t2.png",
      "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/eS_bEDm2o2WAbn3YPS2XR.png",
      "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/jaG4WahYyrQGaalZObNew.png",
      "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/mMKY5jn0SCJPxxR8_mssx.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21887.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "656864e12d73834278a8dea7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
      "fullname": "Ahmed Heakl",
      "name": "ahmedheakl",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 39
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22648",
      "authors": [
        {
          "_id": "6837c03cbbee677da73e6034",
          "user": {
            "_id": "644a4fbc2166258fccc664bc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
            "isPro": false,
            "fullname": "Jialong Wu",
            "user": "callanwu",
            "type": "user"
          },
          "name": "Jialong Wu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-29T02:02:37.069Z",
          "hidden": false
        },
        {
          "_id": "6837c03cbbee677da73e6035",
          "name": "Baixuan Li",
          "hidden": false
        },
        {
          "_id": "6837c03cbbee677da73e6036",
          "name": "Runnan Fang",
          "hidden": false
        },
        {
          "_id": "6837c03cbbee677da73e6037",
          "name": "Wenbiao Yin",
          "hidden": false
        },
        {
          "_id": "6837c03cbbee677da73e6038",
          "name": "Liwen Zhang",
          "hidden": false
        },
        {
          "_id": "6837c03cbbee677da73e6039",
          "name": "Zhengwei Tao",
          "hidden": false
        },
        {
          "_id": "6837c03cbbee677da73e603a",
          "name": "Dingchu Zhang",
          "hidden": false
        },
        {
          "_id": "6837c03cbbee677da73e603b",
          "name": "Zekun Xi",
          "hidden": false
        },
        {
          "_id": "6837c03cbbee677da73e603c",
          "name": "Yong Jiang",
          "hidden": false
        },
        {
          "_id": "6837c03cbbee677da73e603d",
          "name": "Pengjun Xie",
          "hidden": false
        },
        {
          "_id": "6837c03cbbee677da73e603e",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "6837c03cbbee677da73e603f",
          "name": "Jingren Zhou",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/644a4fbc2166258fccc664bc/vhAmZAlJqekE6vLcVjWtO.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/644a4fbc2166258fccc664bc/RXQVwE9PRmBzxCURRiKAU.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/644a4fbc2166258fccc664bc/woFRNvRqdUXKnAHpRH2eM.mp4"
      ],
      "publishedAt": "2025-05-28T17:57:07.000Z",
      "submittedOnDailyAt": "2025-05-29T00:34:30.750Z",
      "title": "WebDancer: Towards Autonomous Information Seeking Agency",
      "submittedOnDailyBy": {
        "_id": "644a4fbc2166258fccc664bc",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
        "isPro": false,
        "fullname": "Jialong Wu",
        "user": "callanwu",
        "type": "user"
      },
      "summary": "Addressing intricate real-world problems necessitates in-depth information\nseeking and multi-step reasoning. Recent progress in agentic systems,\nexemplified by Deep Research, underscores the potential for autonomous\nmulti-step research. In this work, we present a cohesive paradigm for building\nend-to-end agentic information seeking agents from a data-centric and\ntraining-stage perspective. Our approach consists of four key stages: (1)\nbrowsing data construction, (2) trajectories sampling, (3) supervised\nfine-tuning for effective cold start, and (4) reinforcement learning for\nenhanced generalisation. We instantiate this framework in a web agent based on\nthe ReAct, WebDancer. Empirical evaluations on the challenging information\nseeking benchmarks, GAIA and WebWalkerQA, demonstrate the strong performance of\nWebDancer, achieving considerable results and highlighting the efficacy of our\ntraining paradigm. Further analysis of agent training provides valuable\ninsights and actionable, systematic pathways for developing more capable\nagentic models. The codes and demo will be released in\nhttps://github.com/Alibaba-NLP/WebAgent.",
      "upvotes": 10,
      "discussionId": "6837c03dbbee677da73e607f",
      "githubRepo": "https://github.com/Alibaba-NLP/WebAgent",
      "ai_summary": "The paper proposes a framework for building end-to-end agentic information seeking agents through a combination of data construction, trajectory sampling, supervised fine-tuning, and reinforcement learning, showcasing its effectiveness on information seeking benchmarks.",
      "ai_keywords": [
        "browsing data construction",
        "trajectories sampling",
        "supervised fine-tuning",
        "reinforcement learning",
        "WebDancer",
        "GAIA",
        "WebWalkerQA"
      ]
    },
    "publishedAt": "2025-05-28T13:57:07.000Z",
    "title": "WebDancer: Towards Autonomous Information Seeking Agency",
    "summary": "Addressing intricate real-world problems necessitates in-depth information\nseeking and multi-step reasoning. Recent progress in agentic systems,\nexemplified by Deep Research, underscores the potential for autonomous\nmulti-step research. In this work, we present a cohesive paradigm for building\nend-to-end agentic information seeking agents from a data-centric and\ntraining-stage perspective. Our approach consists of four key stages: (1)\nbrowsing data construction, (2) trajectories sampling, (3) supervised\nfine-tuning for effective cold start, and (4) reinforcement learning for\nenhanced generalisation. We instantiate this framework in a web agent based on\nthe ReAct, WebDancer. Empirical evaluations on the challenging information\nseeking benchmarks, GAIA and WebWalkerQA, demonstrate the strong performance of\nWebDancer, achieving considerable results and highlighting the efficacy of our\ntraining paradigm. Further analysis of agent training provides valuable\ninsights and actionable, systematic pathways for developing more capable\nagentic models. The codes and demo will be released in\nhttps://github.com/Alibaba-NLP/WebAgent.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/644a4fbc2166258fccc664bc/vhAmZAlJqekE6vLcVjWtO.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/644a4fbc2166258fccc664bc/RXQVwE9PRmBzxCURRiKAU.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/644a4fbc2166258fccc664bc/woFRNvRqdUXKnAHpRH2eM.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22648.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "644a4fbc2166258fccc664bc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
      "fullname": "Jialong Wu",
      "name": "callanwu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.19075",
      "authors": [
        {
          "_id": "6837fc484d14d7c8800e8b9c",
          "user": {
            "_id": "64c3732de6c3860fba66ceb0",
            "avatarUrl": "/avatars/785783ca08687923053eac641326281f.svg",
            "isPro": false,
            "fullname": "JaeminKim",
            "user": "kjm981995",
            "type": "user"
          },
          "name": "Jaemin Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:41:35.589Z",
          "hidden": false
        },
        {
          "_id": "6837fc484d14d7c8800e8b9d",
          "name": "Hangeol Chang",
          "hidden": false
        },
        {
          "_id": "6837fc484d14d7c8800e8b9e",
          "name": "Hyunmin Hwang",
          "hidden": false
        },
        {
          "_id": "6837fc484d14d7c8800e8b9f",
          "name": "Choonghan Kim",
          "hidden": false
        },
        {
          "_id": "6837fc484d14d7c8800e8ba0",
          "name": "Jong Chul Ye",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-25T10:19:10.000Z",
      "submittedOnDailyAt": "2025-05-29T04:49:39.731Z",
      "title": "Universal Reasoner: A Single, Composable Plug-and-Play Reasoner for\n  Frozen LLMs",
      "submittedOnDailyBy": {
        "_id": "64c3732de6c3860fba66ceb0",
        "avatarUrl": "/avatars/785783ca08687923053eac641326281f.svg",
        "isPro": false,
        "fullname": "JaeminKim",
        "user": "kjm981995",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) have demonstrated remarkable general\ncapabilities, but enhancing skills such as reasoning often demands substantial\ncomputational resources and may compromise their generalization. While\nParameter-Efficient Fine-Tuning (PEFT) methods offer a more resource-conscious\nalternative, they typically requires retraining for each LLM backbone due to\narchitectural dependencies. To address these challenges, here we propose\nUniversal Reasoner (UniR) - a single, lightweight, composable, and\nplug-and-play reasoning module that can be used with any frozen LLM to endow it\nwith specialized reasoning capabilities. Specifically, UniR decomposes the\nreward into a standalone reasoning module that is trained independently using\npredefined rewards, effectively translating trajectory-level signals into\ntoken-level guidance. Once trained, UniR can be combined with any frozen LLM at\ninference time by simply adding its output logits to those of the LLM backbone.\nThis additive structure naturally enables modular composition: multiple UniR\nmodules trained for different tasks can be jointly applied by summing their\nlogits, enabling complex reasoning via composition. Experimental results on\nmathematical reasoning and machine translation tasks show that UniR\nsignificantly outperforms existing baseline fine-tuning methods using the\nLlama3.2 model. Furthermore, UniR demonstrates strong weak-to-strong\ngeneralization: reasoning modules trained on smaller models effectively guide\nmuch larger LLMs. This makes UniR a cost-efficient, adaptable, and robust\nsolution for enhancing reasoning in LLMs without compromising their core\ncapabilities. Code is open-sourced at https://github.com/hangeol/UniR",
      "upvotes": 10,
      "discussionId": "6837fc494d14d7c8800e8be6",
      "ai_summary": "UniR, a lightweight reasoning module, enhances Large Language Models with specialized reasoning abilities through modular composition, improving performance and generalization at lower computational costs.",
      "ai_keywords": [
        "Large Language Models",
        "Parameter-Efficient Fine-Tuning",
        "Universal Reasoner",
        "trajectory-level signals",
        "token-level guidance",
        "additive structure",
        "modular composition",
        "Llama3.2",
        "mathematical reasoning",
        "machine translation",
        "cost-efficient",
        "adaptable",
        "robust"
      ]
    },
    "publishedAt": "2025-05-25T06:19:10.000Z",
    "title": "Universal Reasoner: A Single, Composable Plug-and-Play Reasoner for\n  Frozen LLMs",
    "summary": "Large Language Models (LLMs) have demonstrated remarkable general\ncapabilities, but enhancing skills such as reasoning often demands substantial\ncomputational resources and may compromise their generalization. While\nParameter-Efficient Fine-Tuning (PEFT) methods offer a more resource-conscious\nalternative, they typically requires retraining for each LLM backbone due to\narchitectural dependencies. To address these challenges, here we propose\nUniversal Reasoner (UniR) - a single, lightweight, composable, and\nplug-and-play reasoning module that can be used with any frozen LLM to endow it\nwith specialized reasoning capabilities. Specifically, UniR decomposes the\nreward into a standalone reasoning module that is trained independently using\npredefined rewards, effectively translating trajectory-level signals into\ntoken-level guidance. Once trained, UniR can be combined with any frozen LLM at\ninference time by simply adding its output logits to those of the LLM backbone.\nThis additive structure naturally enables modular composition: multiple UniR\nmodules trained for different tasks can be jointly applied by summing their\nlogits, enabling complex reasoning via composition. Experimental results on\nmathematical reasoning and machine translation tasks show that UniR\nsignificantly outperforms existing baseline fine-tuning methods using the\nLlama3.2 model. Furthermore, UniR demonstrates strong weak-to-strong\ngeneralization: reasoning modules trained on smaller models effectively guide\nmuch larger LLMs. This makes UniR a cost-efficient, adaptable, and robust\nsolution for enhancing reasoning in LLMs without compromising their core\ncapabilities. Code is open-sourced at https://github.com/hangeol/UniR",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19075.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c3732de6c3860fba66ceb0",
      "avatarUrl": "/avatars/785783ca08687923053eac641326281f.svg",
      "fullname": "JaeminKim",
      "name": "kjm981995",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.17663",
      "authors": [
        {
          "_id": "6833c6ff97966d18e7b995b0",
          "user": {
            "_id": "6002c316698168af3bb9f4a6",
            "avatarUrl": "/avatars/1028860b59951be04e94b126b0985dc0.svg",
            "isPro": false,
            "fullname": "yangxiao",
            "user": "YangXiao-nlp",
            "type": "user"
          },
          "name": "Yang Xiao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-28T19:14:41.639Z",
          "hidden": false
        },
        {
          "_id": "6833c6ff97966d18e7b995b1",
          "name": "Jiashuo Wang",
          "hidden": false
        },
        {
          "_id": "6833c6ff97966d18e7b995b2",
          "name": "Qiancheng Xu",
          "hidden": false
        },
        {
          "_id": "6833c6ff97966d18e7b995b3",
          "name": "Changhe Song",
          "hidden": false
        },
        {
          "_id": "6833c6ff97966d18e7b995b4",
          "name": "Chunpu Xu",
          "hidden": false
        },
        {
          "_id": "6833c6ff97966d18e7b995b5",
          "name": "Yi Cheng",
          "hidden": false
        },
        {
          "_id": "6833c6ff97966d18e7b995b6",
          "name": "Wenjie Li",
          "hidden": false
        },
        {
          "_id": "6833c6ff97966d18e7b995b7",
          "name": "Pengfei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T09:27:40.000Z",
      "submittedOnDailyAt": "2025-05-29T01:05:57.079Z",
      "title": "Towards Dynamic Theory of Mind: Evaluating LLM Adaptation to Temporal\n  Evolution of Human States",
      "submittedOnDailyBy": {
        "_id": "6002c316698168af3bb9f4a6",
        "avatarUrl": "/avatars/1028860b59951be04e94b126b0985dc0.svg",
        "isPro": false,
        "fullname": "yangxiao",
        "user": "YangXiao-nlp",
        "type": "user"
      },
      "summary": "As Large Language Models (LLMs) increasingly participate in human-AI\ninteractions, evaluating their Theory of Mind (ToM) capabilities - particularly\ntheir ability to track dynamic mental states - becomes crucial. While existing\nbenchmarks assess basic ToM abilities, they predominantly focus on static\nsnapshots of mental states, overlooking the temporal evolution that\ncharacterizes real-world social interactions. We present DynToM, a\nnovel benchmark specifically designed to evaluate LLMs' ability to understand\nand track the temporal progression of mental states across interconnected\nscenarios. Through a systematic four-step framework, we generate 1,100 social\ncontexts encompassing 5,500 scenarios and 78,100 questions, each validated for\nrealism and quality. Our comprehensive evaluation of ten state-of-the-art LLMs\nreveals that their average performance underperforms humans by 44.7\\%, with\nperformance degrading significantly when tracking and reasoning about the shift\nof mental states. This performance gap highlights fundamental limitations in\ncurrent LLMs' ability to model the dynamic nature of human mental states.",
      "upvotes": 10,
      "discussionId": "6833c70097966d18e7b99616",
      "ai_summary": "The DynToM benchmark evaluates LLMs' ability to track and understand the temporal progression of mental states, revealing significant gaps compared to human performance.",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "Theory of Mind (ToM)",
        "DynToM",
        "social contexts",
        "mental states",
        "temporal progression",
        "evaluation framework",
        "state-of-the-art LLMs",
        "performance gap",
        "dynamic mental states"
      ]
    },
    "publishedAt": "2025-05-23T05:27:40.000Z",
    "title": "Towards Dynamic Theory of Mind: Evaluating LLM Adaptation to Temporal\n  Evolution of Human States",
    "summary": "As Large Language Models (LLMs) increasingly participate in human-AI\ninteractions, evaluating their Theory of Mind (ToM) capabilities - particularly\ntheir ability to track dynamic mental states - becomes crucial. While existing\nbenchmarks assess basic ToM abilities, they predominantly focus on static\nsnapshots of mental states, overlooking the temporal evolution that\ncharacterizes real-world social interactions. We present DynToM, a\nnovel benchmark specifically designed to evaluate LLMs' ability to understand\nand track the temporal progression of mental states across interconnected\nscenarios. Through a systematic four-step framework, we generate 1,100 social\ncontexts encompassing 5,500 scenarios and 78,100 questions, each validated for\nrealism and quality. Our comprehensive evaluation of ten state-of-the-art LLMs\nreveals that their average performance underperforms humans by 44.7\\%, with\nperformance degrading significantly when tracking and reasoning about the shift\nof mental states. This performance gap highlights fundamental limitations in\ncurrent LLMs' ability to model the dynamic nature of human mental states.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17663.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6002c316698168af3bb9f4a6",
      "avatarUrl": "/avatars/1028860b59951be04e94b126b0985dc0.svg",
      "fullname": "yangxiao",
      "name": "YangXiao-nlp",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.19187",
      "authors": [
        {
          "_id": "6837210455e9bab4e9c302b1",
          "user": {
            "_id": "6002c316698168af3bb9f4a6",
            "avatarUrl": "/avatars/1028860b59951be04e94b126b0985dc0.svg",
            "isPro": false,
            "fullname": "yangxiao",
            "user": "YangXiao-nlp",
            "type": "user"
          },
          "name": "Yang Xiao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-28T19:13:52.090Z",
          "hidden": false
        },
        {
          "_id": "6837210455e9bab4e9c302b2",
          "name": "Jiashuo Wang",
          "hidden": false
        },
        {
          "_id": "6837210455e9bab4e9c302b3",
          "name": "Ruifeng Yuan",
          "hidden": false
        },
        {
          "_id": "6837210455e9bab4e9c302b4",
          "name": "Chunpu Xu",
          "hidden": false
        },
        {
          "_id": "6837210455e9bab4e9c302b5",
          "name": "Kaishuai Xu",
          "hidden": false
        },
        {
          "_id": "6837210455e9bab4e9c302b6",
          "name": "Wenjie Li",
          "hidden": false
        },
        {
          "_id": "6837210455e9bab4e9c302b7",
          "name": "Pengfei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-25T15:17:57.000Z",
      "submittedOnDailyAt": "2025-05-29T01:05:08.605Z",
      "title": "LIMOPro: Reasoning Refinement for Efficient and Effective Test-time\n  Scaling",
      "submittedOnDailyBy": {
        "_id": "6002c316698168af3bb9f4a6",
        "avatarUrl": "/avatars/1028860b59951be04e94b126b0985dc0.svg",
        "isPro": false,
        "fullname": "yangxiao",
        "user": "YangXiao-nlp",
        "type": "user"
      },
      "summary": "Large language models (LLMs) have demonstrated remarkable reasoning\ncapabilities through test-time scaling approaches, particularly when fine-tuned\nwith chain-of-thought (CoT) data distilled from more powerful large reasoning\nmodels (LRMs). However, these reasoning chains often contain verbose elements\nthat mirror human problem-solving, categorized as progressive reasoning (the\nessential solution development path) and functional elements (verification\nprocesses, alternative solution approaches, and error corrections). While\nprogressive reasoning is crucial, the functional elements significantly\nincrease computational demands during test-time inference. We introduce PIR\n(Perplexity-based Importance Refinement), a principled framework that\nquantitatively evaluates the importance of each reasoning step based on its\nimpact on answer prediction confidence. PIR systematically identifies and\nselectively prunes only low-importance functional steps while preserving\nprogressive reasoning components, creating optimized training data that\nmaintains the integrity of the core solution path while reducing verbosity.\nModels fine-tuned on PIR-optimized data exhibit superior test-time scaling\nproperties, generating more concise reasoning chains while achieving improved\naccuracy (+0.9\\% to +6.6\\%) with significantly reduced token usage (-3\\% to\n-41\\%) across challenging reasoning benchmarks (AIME, AMC, and GPQA Diamond).\nOur approach demonstrates strong generalizability across different model sizes,\ndata sources, and token budgets, offering a practical solution for deploying\nreasoning-capable LLMs in scenarios where efficient test-time scaling, response\ntime, and computational efficiency are valuable constraints.",
      "upvotes": 9,
      "discussionId": "6837210555e9bab4e9c302f2",
      "ai_summary": "A framework called PIR refines the importance of reasoning steps in large language models by pruning low-importance functional elements, leading to more concise reasoning chains with improved accuracy and reduced computational demands.",
      "ai_keywords": [
        "large language models",
        "chain-of-thought",
        "large reasoning models",
        "progressive reasoning",
        "functional elements",
        "perplexity-based importance refinement",
        "token usage",
        "reasoning benchmarks",
        "AIME",
        "AMC",
        "GPQA Diamond"
      ]
    },
    "publishedAt": "2025-05-25T11:17:57.000Z",
    "title": "LIMOPro: Reasoning Refinement for Efficient and Effective Test-time\n  Scaling",
    "summary": "Large language models (LLMs) have demonstrated remarkable reasoning\ncapabilities through test-time scaling approaches, particularly when fine-tuned\nwith chain-of-thought (CoT) data distilled from more powerful large reasoning\nmodels (LRMs). However, these reasoning chains often contain verbose elements\nthat mirror human problem-solving, categorized as progressive reasoning (the\nessential solution development path) and functional elements (verification\nprocesses, alternative solution approaches, and error corrections). While\nprogressive reasoning is crucial, the functional elements significantly\nincrease computational demands during test-time inference. We introduce PIR\n(Perplexity-based Importance Refinement), a principled framework that\nquantitatively evaluates the importance of each reasoning step based on its\nimpact on answer prediction confidence. PIR systematically identifies and\nselectively prunes only low-importance functional steps while preserving\nprogressive reasoning components, creating optimized training data that\nmaintains the integrity of the core solution path while reducing verbosity.\nModels fine-tuned on PIR-optimized data exhibit superior test-time scaling\nproperties, generating more concise reasoning chains while achieving improved\naccuracy (+0.9\\% to +6.6\\%) with significantly reduced token usage (-3\\% to\n-41\\%) across challenging reasoning benchmarks (AIME, AMC, and GPQA Diamond).\nOur approach demonstrates strong generalizability across different model sizes,\ndata sources, and token budgets, offering a practical solution for deploying\nreasoning-capable LLMs in scenarios where efficient test-time scaling, response\ntime, and computational efficiency are valuable constraints.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19187.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6002c316698168af3bb9f4a6",
      "avatarUrl": "/avatars/1028860b59951be04e94b126b0985dc0.svg",
      "fullname": "yangxiao",
      "name": "YangXiao-nlp",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.21925",
      "authors": [
        {
          "_id": "6837c23acce400abe6f18790",
          "user": {
            "_id": "60747cbf3ea03830676542b5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60747cbf3ea03830676542b5/wGr1Jzz520JM9nZ-UcLyb.png",
            "isPro": false,
            "fullname": "Chong Zeng",
            "user": "NCJ",
            "type": "user"
          },
          "name": "Chong Zeng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:38.730Z",
          "hidden": false
        },
        {
          "_id": "6837c23acce400abe6f18791",
          "user": {
            "_id": "63299011bdb6242b42b77f57",
            "avatarUrl": "/avatars/056aec97eb3c10d3b63eb13238e1d2a4.svg",
            "isPro": false,
            "fullname": "doyleconan",
            "user": "doyleconan",
            "type": "user"
          },
          "name": "Yue Dong",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-29T02:11:08.981Z",
          "hidden": false
        },
        {
          "_id": "6837c23acce400abe6f18792",
          "name": "Pieter Peers",
          "hidden": false
        },
        {
          "_id": "6837c23acce400abe6f18793",
          "name": "Hongzhi Wu",
          "hidden": false
        },
        {
          "_id": "6837c23acce400abe6f18794",
          "name": "Xin Tong",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/60747cbf3ea03830676542b5/DuGYODs8k7DhVHb78BmCZ.jpeg"
      ],
      "publishedAt": "2025-05-28T03:20:46.000Z",
      "submittedOnDailyAt": "2025-05-29T00:45:14.960Z",
      "title": "RenderFormer: Transformer-based Neural Rendering of Triangle Meshes with\n  Global Illumination",
      "submittedOnDailyBy": {
        "_id": "60747cbf3ea03830676542b5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60747cbf3ea03830676542b5/wGr1Jzz520JM9nZ-UcLyb.png",
        "isPro": false,
        "fullname": "Chong Zeng",
        "user": "NCJ",
        "type": "user"
      },
      "summary": "We present RenderFormer, a neural rendering pipeline that directly renders an\nimage from a triangle-based representation of a scene with full global\nillumination effects and that does not require per-scene training or\nfine-tuning. Instead of taking a physics-centric approach to rendering, we\nformulate rendering as a sequence-to-sequence transformation where a sequence\nof tokens representing triangles with reflectance properties is converted to a\nsequence of output tokens representing small patches of pixels. RenderFormer\nfollows a two stage pipeline: a view-independent stage that models\ntriangle-to-triangle light transport, and a view-dependent stage that\ntransforms a token representing a bundle of rays to the corresponding pixel\nvalues guided by the triangle-sequence from the view-independent stage. Both\nstages are based on the transformer architecture and are learned with minimal\nprior constraints. We demonstrate and evaluate RenderFormer on scenes with\nvarying complexity in shape and light transport.",
      "upvotes": 6,
      "discussionId": "6837c23ccce400abe6f18812",
      "projectPage": "https://microsoft.github.io/renderformer/",
      "githubRepo": "https://github.com/microsoft/renderformer",
      "ai_summary": "RenderFormer is a transformer-based neural rendering pipeline that renders images from triangle representations without per-scene training and with full global illumination effects.",
      "ai_keywords": [
        "neural rendering pipeline",
        "global illumination effects",
        "sequence-to-sequence transformation",
        "tokens",
        "reflectance properties",
        "pixel patches",
        "transformer architecture",
        "view-independent stage",
        "view-dependent stage",
        "triangle-to-triangle light transport",
        "ray bundles"
      ]
    },
    "publishedAt": "2025-05-27T23:20:46.000Z",
    "title": "RenderFormer: Transformer-based Neural Rendering of Triangle Meshes with\n  Global Illumination",
    "summary": "We present RenderFormer, a neural rendering pipeline that directly renders an\nimage from a triangle-based representation of a scene with full global\nillumination effects and that does not require per-scene training or\nfine-tuning. Instead of taking a physics-centric approach to rendering, we\nformulate rendering as a sequence-to-sequence transformation where a sequence\nof tokens representing triangles with reflectance properties is converted to a\nsequence of output tokens representing small patches of pixels. RenderFormer\nfollows a two stage pipeline: a view-independent stage that models\ntriangle-to-triangle light transport, and a view-dependent stage that\ntransforms a token representing a bundle of rays to the corresponding pixel\nvalues guided by the triangle-sequence from the view-independent stage. Both\nstages are based on the transformer architecture and are learned with minimal\nprior constraints. We demonstrate and evaluate RenderFormer on scenes with\nvarying complexity in shape and light transport.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/60747cbf3ea03830676542b5/DuGYODs8k7DhVHb78BmCZ.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21925.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60747cbf3ea03830676542b5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60747cbf3ea03830676542b5/wGr1Jzz520JM9nZ-UcLyb.png",
      "fullname": "Chong Zeng",
      "name": "NCJ",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22613",
      "authors": [
        {
          "_id": "6837d79d4d9866c160d8f43b",
          "name": "Yuchi Wang",
          "hidden": false
        },
        {
          "_id": "6837d79d4d9866c160d8f43c",
          "name": "Yishuo Cai",
          "hidden": false
        },
        {
          "_id": "6837d79d4d9866c160d8f43d",
          "name": "Shuhuai Ren",
          "hidden": false
        },
        {
          "_id": "6837d79d4d9866c160d8f43e",
          "name": "Sihan Yang",
          "hidden": false
        },
        {
          "_id": "6837d79d4d9866c160d8f43f",
          "name": "Linli Yao",
          "hidden": false
        },
        {
          "_id": "6837d79d4d9866c160d8f440",
          "name": "Yuanxin Liu",
          "hidden": false
        },
        {
          "_id": "6837d79d4d9866c160d8f441",
          "name": "Yuanxing Zhang",
          "hidden": false
        },
        {
          "_id": "6837d79d4d9866c160d8f442",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "6837d79d4d9866c160d8f443",
          "name": "Xu Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T17:29:34.000Z",
      "submittedOnDailyAt": "2025-05-29T02:20:00.209Z",
      "title": "RICO: Improving Accuracy and Completeness in Image Recaptioning via\n  Visual Reconstruction",
      "submittedOnDailyBy": {
        "_id": "622842e296588dd1a2594746",
        "avatarUrl": "/avatars/b96d0b49e4cff25d83fefc67b7cd1076.svg",
        "isPro": false,
        "fullname": "wangyuchi",
        "user": "YuchiWang",
        "type": "user"
      },
      "summary": "Image recaptioning is widely used to generate training datasets with enhanced\nquality for various multimodal tasks. Existing recaptioning methods typically\nrely on powerful multimodal large language models (MLLMs) to enhance textual\ndescriptions, but often suffer from inaccuracies due to hallucinations and\nincompleteness caused by missing fine-grained details. To address these\nlimitations, we propose RICO, a novel framework that refines captions through\nvisual reconstruction. Specifically, we leverage a text-to-image model to\nreconstruct a caption into a reference image, and prompt an MLLM to identify\ndiscrepancies between the original and reconstructed images to refine the\ncaption. This process is performed iteratively, further progressively promoting\nthe generation of more faithful and comprehensive descriptions. To mitigate the\nadditional computational cost induced by the iterative process, we introduce\nRICO-Flash, which learns to generate captions like RICO using DPO. Extensive\nexperiments demonstrate that our approach significantly improves caption\naccuracy and completeness, outperforms most baselines by approximately 10% on\nboth CapsBench and CompreCap. Code released at\nhttps://github.com/wangyuchi369/RICO.",
      "upvotes": 5,
      "discussionId": "6837d79e4d9866c160d8f471",
      "githubRepo": "https://github.com/wangyuchi369/RICO",
      "ai_summary": "A novel iterative framework, RICO, improves image caption accuracy by using visual reconstruction and a text-to-image model to refine discrepancies, while RICO-Flash enhances efficiency using DPO.",
      "ai_keywords": [
        "multimodal large language models",
        "text-to-image model",
        "DPO",
        "CapsBench",
        "CompreCap"
      ]
    },
    "publishedAt": "2025-05-28T13:29:34.000Z",
    "title": "RICO: Improving Accuracy and Completeness in Image Recaptioning via\n  Visual Reconstruction",
    "summary": "Image recaptioning is widely used to generate training datasets with enhanced\nquality for various multimodal tasks. Existing recaptioning methods typically\nrely on powerful multimodal large language models (MLLMs) to enhance textual\ndescriptions, but often suffer from inaccuracies due to hallucinations and\nincompleteness caused by missing fine-grained details. To address these\nlimitations, we propose RICO, a novel framework that refines captions through\nvisual reconstruction. Specifically, we leverage a text-to-image model to\nreconstruct a caption into a reference image, and prompt an MLLM to identify\ndiscrepancies between the original and reconstructed images to refine the\ncaption. This process is performed iteratively, further progressively promoting\nthe generation of more faithful and comprehensive descriptions. To mitigate the\nadditional computational cost induced by the iterative process, we introduce\nRICO-Flash, which learns to generate captions like RICO using DPO. Extensive\nexperiments demonstrate that our approach significantly improves caption\naccuracy and completeness, outperforms most baselines by approximately 10% on\nboth CapsBench and CompreCap. Code released at\nhttps://github.com/wangyuchi369/RICO.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22613.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "622842e296588dd1a2594746",
      "avatarUrl": "/avatars/b96d0b49e4cff25d83fefc67b7cd1076.svg",
      "fullname": "wangyuchi",
      "name": "YuchiWang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.22525",
      "authors": [
        {
          "_id": "6837da438f680552f7b86b28",
          "user": {
            "_id": "64bb5f9d8e051085bace4d1e",
            "avatarUrl": "/avatars/15ccbb78c6131dfe46b7a9d8e7d1a31f.svg",
            "isPro": true,
            "fullname": "Ethan Chern",
            "user": "ethanchern",
            "type": "user"
          },
          "name": "Ethan Chern",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:05.613Z",
          "hidden": false
        },
        {
          "_id": "6837da438f680552f7b86b29",
          "name": "Zhulin Hu",
          "hidden": false
        },
        {
          "_id": "6837da438f680552f7b86b2a",
          "name": "Steffi Chern",
          "hidden": false
        },
        {
          "_id": "6837da438f680552f7b86b2b",
          "name": "Siqi Kou",
          "hidden": false
        },
        {
          "_id": "6837da438f680552f7b86b2c",
          "name": "Jiadi Su",
          "hidden": false
        },
        {
          "_id": "6837da438f680552f7b86b2d",
          "name": "Yan Ma",
          "hidden": false
        },
        {
          "_id": "6837da438f680552f7b86b2e",
          "name": "Zhijie Deng",
          "hidden": false
        },
        {
          "_id": "6837da438f680552f7b86b2f",
          "name": "Pengfei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T16:12:45.000Z",
      "submittedOnDailyAt": "2025-05-29T02:24:51.876Z",
      "title": "Thinking with Generated Images",
      "submittedOnDailyBy": {
        "_id": "64bb5f9d8e051085bace4d1e",
        "avatarUrl": "/avatars/15ccbb78c6131dfe46b7a9d8e7d1a31f.svg",
        "isPro": true,
        "fullname": "Ethan Chern",
        "user": "ethanchern",
        "type": "user"
      },
      "summary": "We present Thinking with Generated Images, a novel paradigm that\nfundamentally transforms how large multimodal models (LMMs) engage with visual\nreasoning by enabling them to natively think across text and vision modalities\nthrough spontaneous generation of intermediate visual thinking steps. Current\nvisual reasoning with LMMs is constrained to either processing fixed\nuser-provided images or reasoning solely through text-based chain-of-thought\n(CoT). Thinking with Generated Images unlocks a new dimension of cognitive\ncapability where models can actively construct intermediate visual thoughts,\ncritique their own visual hypotheses, and refine them as integral components of\ntheir reasoning process. We demonstrate the effectiveness of our approach\nthrough two complementary mechanisms: (1) vision generation with intermediate\nvisual subgoals, where models decompose complex visual tasks into manageable\ncomponents that are generated and integrated progressively, and (2) vision\ngeneration with self-critique, where models generate an initial visual\nhypothesis, analyze its shortcomings through textual reasoning, and produce\nrefined outputs based on their own critiques. Our experiments on vision\ngeneration benchmarks show substantial improvements over baseline approaches,\nwith our models achieving up to 50% (from 38% to 57%) relative improvement in\nhandling complex multi-object scenarios. From biochemists exploring novel\nprotein structures, and architects iterating on spatial designs, to forensic\nanalysts reconstructing crime scenes, and basketball players envisioning\nstrategic plays, our approach enables AI models to engage in the kind of visual\nimagination and iterative refinement that characterizes human creative,\nanalytical, and strategic thinking. We release our open-source suite at\nhttps://github.com/GAIR-NLP/thinking-with-generated-images.",
      "upvotes": 5,
      "discussionId": "6837da468f680552f7b86bb2",
      "githubRepo": "https://github.com/GAIR-NLP/thinking-with-generated-images",
      "ai_summary": "Thinking with Generated Images allows large multimodal models to generate and critique intermediate visual steps, enhancing visual reasoning capabilities and achieving significant improvements in complex scenarios.",
      "ai_keywords": [
        "LMMs",
        "visual reasoning",
        "chain-of-thought",
        "vision generation",
        "intermediate visual subgoals",
        "self-critique",
        "multis-object scenarios",
        "biochemists",
        "architects",
        "forensic analysts",
        "basketball players"
      ]
    },
    "publishedAt": "2025-05-28T12:12:45.000Z",
    "title": "Thinking with Generated Images",
    "summary": "We present Thinking with Generated Images, a novel paradigm that\nfundamentally transforms how large multimodal models (LMMs) engage with visual\nreasoning by enabling them to natively think across text and vision modalities\nthrough spontaneous generation of intermediate visual thinking steps. Current\nvisual reasoning with LMMs is constrained to either processing fixed\nuser-provided images or reasoning solely through text-based chain-of-thought\n(CoT). Thinking with Generated Images unlocks a new dimension of cognitive\ncapability where models can actively construct intermediate visual thoughts,\ncritique their own visual hypotheses, and refine them as integral components of\ntheir reasoning process. We demonstrate the effectiveness of our approach\nthrough two complementary mechanisms: (1) vision generation with intermediate\nvisual subgoals, where models decompose complex visual tasks into manageable\ncomponents that are generated and integrated progressively, and (2) vision\ngeneration with self-critique, where models generate an initial visual\nhypothesis, analyze its shortcomings through textual reasoning, and produce\nrefined outputs based on their own critiques. Our experiments on vision\ngeneration benchmarks show substantial improvements over baseline approaches,\nwith our models achieving up to 50% (from 38% to 57%) relative improvement in\nhandling complex multi-object scenarios. From biochemists exploring novel\nprotein structures, and architects iterating on spatial designs, to forensic\nanalysts reconstructing crime scenes, and basketball players envisioning\nstrategic plays, our approach enables AI models to engage in the kind of visual\nimagination and iterative refinement that characterizes human creative,\nanalytical, and strategic thinking. We release our open-source suite at\nhttps://github.com/GAIR-NLP/thinking-with-generated-images.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22525.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64bb5f9d8e051085bace4d1e",
      "avatarUrl": "/avatars/15ccbb78c6131dfe46b7a9d8e7d1a31f.svg",
      "fullname": "Ethan Chern",
      "name": "ethanchern",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22019",
      "authors": [
        {
          "_id": "6837ed297d00cf0a04677bc1",
          "name": "Qiuchen Wang",
          "hidden": false
        },
        {
          "_id": "6837ed297d00cf0a04677bc2",
          "name": "Ruixue Ding",
          "hidden": false
        },
        {
          "_id": "6837ed297d00cf0a04677bc3",
          "user": {
            "_id": "665d652e0f35c005de892108",
            "avatarUrl": "/avatars/240bebdc7fdc6d50719c65de0e3cf1cd.svg",
            "isPro": false,
            "fullname": "Yu Zeng",
            "user": "YuZeng260",
            "type": "user"
          },
          "name": "Yu Zeng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:41:42.906Z",
          "hidden": false
        },
        {
          "_id": "6837ed297d00cf0a04677bc4",
          "name": "Zehui Chen",
          "hidden": false
        },
        {
          "_id": "6837ed297d00cf0a04677bc5",
          "user": {
            "_id": "64b02ec0e5000ae8a572ced5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b02ec0e5000ae8a572ced5/6ifLntBU2ICQK7SW8WxKU.png",
            "isPro": false,
            "fullname": "Lin Chen",
            "user": "Lin-Chen",
            "type": "user"
          },
          "name": "Lin Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:41:44.983Z",
          "hidden": false
        },
        {
          "_id": "6837ed297d00cf0a04677bc6",
          "name": "Shihang Wang",
          "hidden": false
        },
        {
          "_id": "6837ed297d00cf0a04677bc7",
          "name": "Pengjun Xie",
          "hidden": false
        },
        {
          "_id": "6837ed297d00cf0a04677bc8",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "6837ed297d00cf0a04677bc9",
          "name": "Feng Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T06:30:51.000Z",
      "submittedOnDailyAt": "2025-05-29T03:46:05.539Z",
      "title": "VRAG-RL: Empower Vision-Perception-Based RAG for Visually Rich\n  Information Understanding via Iterative Reasoning with Reinforcement Learning",
      "submittedOnDailyBy": {
        "_id": "64b02ec0e5000ae8a572ced5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b02ec0e5000ae8a572ced5/6ifLntBU2ICQK7SW8WxKU.png",
        "isPro": false,
        "fullname": "Lin Chen",
        "user": "Lin-Chen",
        "type": "user"
      },
      "summary": "Effectively retrieving, reasoning and understanding visually rich information\nremains a challenge for RAG methods. Traditional text-based methods cannot\nhandle visual-related information. On the other hand, current vision-based RAG\napproaches are often limited by fixed pipelines and frequently struggle to\nreason effectively due to the insufficient activation of the fundamental\ncapabilities of models. As RL has been proven to be beneficial for model\nreasoning, we introduce VRAG-RL, a novel RL framework tailored for complex\nreasoning across visually rich information. With this framework, VLMs interact\nwith search engines, autonomously sampling single-turn or multi-turn reasoning\ntrajectories with the help of visual perception tokens and undergoing continual\noptimization based on these samples. Our approach highlights key limitations of\nRL in RAG domains: (i) Prior Multi-modal RAG approaches tend to merely\nincorporate images into the context, leading to insufficient reasoning token\nallocation and neglecting visual-specific perception; and (ii) When models\ninteract with search engines, their queries often fail to retrieve relevant\ninformation due to the inability to articulate requirements, thereby leading to\nsuboptimal performance. To address these challenges, we define an action space\ntailored for visually rich inputs, with actions including cropping and scaling,\nallowing the model to gather information from a coarse-to-fine perspective.\nFurthermore, to bridge the gap between users' original inquiries and the\nretriever, we employ a simple yet effective reward that integrates query\nrewriting and retrieval performance with a model-based reward. Our VRAG-RL\noptimizes VLMs for RAG tasks using specially designed RL strategies, aligning\nthe model with real-world applications. The code is available at\nhttps://github.com/Alibaba-NLP/VRAG{https://github.com/Alibaba-NLP/VRAG}.",
      "upvotes": 5,
      "discussionId": "6837ed297d00cf0a04677bf5",
      "ai_summary": "VRAG-RL, a reinforcement learning framework, enhances reasoning and visual information handling in RAG methods by integrating visual perception tokens and employing specialized action spaces and rewards.",
      "ai_keywords": [
        "reinforcement learning",
        "VRAG-RL",
        "VLMs",
        "search engines",
        "visually rich information",
        "reasoning trajectories",
        "visual perception tokens",
        "action space",
        "query rewriting",
        "retrieval performance",
        "model-based reward"
      ]
    },
    "publishedAt": "2025-05-28T02:30:51.000Z",
    "title": "VRAG-RL: Empower Vision-Perception-Based RAG for Visually Rich\n  Information Understanding via Iterative Reasoning with Reinforcement Learning",
    "summary": "Effectively retrieving, reasoning and understanding visually rich information\nremains a challenge for RAG methods. Traditional text-based methods cannot\nhandle visual-related information. On the other hand, current vision-based RAG\napproaches are often limited by fixed pipelines and frequently struggle to\nreason effectively due to the insufficient activation of the fundamental\ncapabilities of models. As RL has been proven to be beneficial for model\nreasoning, we introduce VRAG-RL, a novel RL framework tailored for complex\nreasoning across visually rich information. With this framework, VLMs interact\nwith search engines, autonomously sampling single-turn or multi-turn reasoning\ntrajectories with the help of visual perception tokens and undergoing continual\noptimization based on these samples. Our approach highlights key limitations of\nRL in RAG domains: (i) Prior Multi-modal RAG approaches tend to merely\nincorporate images into the context, leading to insufficient reasoning token\nallocation and neglecting visual-specific perception; and (ii) When models\ninteract with search engines, their queries often fail to retrieve relevant\ninformation due to the inability to articulate requirements, thereby leading to\nsuboptimal performance. To address these challenges, we define an action space\ntailored for visually rich inputs, with actions including cropping and scaling,\nallowing the model to gather information from a coarse-to-fine perspective.\nFurthermore, to bridge the gap between users' original inquiries and the\nretriever, we employ a simple yet effective reward that integrates query\nrewriting and retrieval performance with a model-based reward. Our VRAG-RL\noptimizes VLMs for RAG tasks using specially designed RL strategies, aligning\nthe model with real-world applications. The code is available at\nhttps://github.com/Alibaba-NLP/VRAG{https://github.com/Alibaba-NLP/VRAG}.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22019.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b02ec0e5000ae8a572ced5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b02ec0e5000ae8a572ced5/6ifLntBU2ICQK7SW8WxKU.png",
      "fullname": "Lin Chen",
      "name": "Lin-Chen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 89
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22651",
      "authors": [
        {
          "_id": "6837ffdd1bfb4a669ad6de09",
          "user": {
            "_id": "662678dfdd43e904ef1dcd03",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662678dfdd43e904ef1dcd03/9bbnrnsRV3ylSuZXwcSNv.jpeg",
            "isPro": false,
            "fullname": "Yi Ding",
            "user": "Tuwhy",
            "type": "user"
          },
          "name": "Yi Ding",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:41:19.811Z",
          "hidden": false
        },
        {
          "_id": "6837ffdd1bfb4a669ad6de0a",
          "name": "Ruqi Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T17:58:03.000Z",
      "submittedOnDailyAt": "2025-05-29T06:18:44.515Z",
      "title": "Sherlock: Self-Correcting Reasoning in Vision-Language Models",
      "submittedOnDailyBy": {
        "_id": "662678dfdd43e904ef1dcd03",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662678dfdd43e904ef1dcd03/9bbnrnsRV3ylSuZXwcSNv.jpeg",
        "isPro": false,
        "fullname": "Yi Ding",
        "user": "Tuwhy",
        "type": "user"
      },
      "summary": "Reasoning Vision-Language Models (VLMs) have shown promising performance on\ncomplex multimodal tasks. However, they still face significant challenges: they\nare highly sensitive to reasoning errors, require large volumes of annotated\ndata or accurate verifiers, and struggle to generalize beyond specific domains.\nTo address these limitations, we explore self-correction as a strategy to\nenhance reasoning VLMs. We first conduct an in-depth analysis of reasoning\nVLMs' self-correction abilities and identify key gaps. Based on our findings,\nwe introduce Sherlock, a self-correction and self-improvement training\nframework. Sherlock introduces a trajectory-level self-correction objective, a\npreference data construction method based on visual perturbation, and a dynamic\nbeta for preference tuning. Once the model acquires self-correction\ncapabilities using only 20k randomly sampled annotated data, it continues to\nself-improve without external supervision. Built on the Llama3.2-Vision-11B\nmodel, Sherlock achieves remarkable results across eight benchmarks, reaching\nan average accuracy of 64.1 with direct generation and 65.4 after\nself-correction. It outperforms LLaVA-CoT (63.2), Mulberry (63.9), and\nLlamaV-o1 (63.4) while using less than 20% of the annotated data.",
      "upvotes": 4,
      "discussionId": "6837ffdf1bfb4a669ad6de71",
      "projectPage": "https://dripnowhy.github.io/Sherlock/",
      "githubRepo": "https://github.com/DripNowhy/Sherlock",
      "ai_summary": "Sherlock, a self-correction and self-improvement framework for reasoning vision-language models, enhances accuracy across benchmarks using limited annotated data.",
      "ai_keywords": [
        "vision-language models",
        "self-correction",
        "trajectory-level self-correction",
        "preference data",
        "visual perturbation",
        "dynamic beta",
        "Llama3.2-Vision-11B",
        "LLaVA-CoT",
        "Mulberry",
        "LlamaV-o1"
      ]
    },
    "publishedAt": "2025-05-28T13:58:03.000Z",
    "title": "Sherlock: Self-Correcting Reasoning in Vision-Language Models",
    "summary": "Reasoning Vision-Language Models (VLMs) have shown promising performance on\ncomplex multimodal tasks. However, they still face significant challenges: they\nare highly sensitive to reasoning errors, require large volumes of annotated\ndata or accurate verifiers, and struggle to generalize beyond specific domains.\nTo address these limitations, we explore self-correction as a strategy to\nenhance reasoning VLMs. We first conduct an in-depth analysis of reasoning\nVLMs' self-correction abilities and identify key gaps. Based on our findings,\nwe introduce Sherlock, a self-correction and self-improvement training\nframework. Sherlock introduces a trajectory-level self-correction objective, a\npreference data construction method based on visual perturbation, and a dynamic\nbeta for preference tuning. Once the model acquires self-correction\ncapabilities using only 20k randomly sampled annotated data, it continues to\nself-improve without external supervision. Built on the Llama3.2-Vision-11B\nmodel, Sherlock achieves remarkable results across eight benchmarks, reaching\nan average accuracy of 64.1 with direct generation and 65.4 after\nself-correction. It outperforms LLaVA-CoT (63.2), Mulberry (63.9), and\nLlamaV-o1 (63.4) while using less than 20% of the annotated data.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22651.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "662678dfdd43e904ef1dcd03",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662678dfdd43e904ef1dcd03/9bbnrnsRV3ylSuZXwcSNv.jpeg",
      "fullname": "Yi Ding",
      "name": "Tuwhy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22523",
      "authors": [
        {
          "_id": "6837c1cb80fc90ca2d9e8153",
          "name": "Junwen Chen",
          "hidden": false
        },
        {
          "_id": "6837c1cb80fc90ca2d9e8154",
          "name": "Heyang Jiang",
          "hidden": false
        },
        {
          "_id": "6837c1cb80fc90ca2d9e8155",
          "name": "Yanbin Wang",
          "hidden": false
        },
        {
          "_id": "6837c1cb80fc90ca2d9e8156",
          "name": "Keming Wu",
          "hidden": false
        },
        {
          "_id": "6837c1cb80fc90ca2d9e8157",
          "name": "Ji Li",
          "hidden": false
        },
        {
          "_id": "6837c1cb80fc90ca2d9e8158",
          "name": "Chao Zhang",
          "hidden": false
        },
        {
          "_id": "6837c1cb80fc90ca2d9e8159",
          "name": "Keiji Yanai",
          "hidden": false
        },
        {
          "_id": "6837c1cb80fc90ca2d9e815a",
          "name": "Dong Chen",
          "hidden": false
        },
        {
          "_id": "6837c1cb80fc90ca2d9e815b",
          "name": "Yuhui Yuan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T16:09:33.000Z",
      "submittedOnDailyAt": "2025-05-29T00:40:52.849Z",
      "title": "PrismLayers: Open Data for High-Quality Multi-Layer Transparent Image\n  Generative Models",
      "submittedOnDailyBy": {
        "_id": "631f108bb45367a05fe74260",
        "avatarUrl": "/avatars/c20da6800b643728062712d9a2771648.svg",
        "isPro": false,
        "fullname": "Researcher",
        "user": "YuanYuhui",
        "type": "user"
      },
      "summary": "Generating high-quality, multi-layer transparent images from text prompts can\nunlock a new level of creative control, allowing users to edit each layer as\neffortlessly as editing text outputs from LLMs. However, the development of\nmulti-layer generative models lags behind that of conventional text-to-image\nmodels due to the absence of a large, high-quality corpus of multi-layer\ntransparent data. In this paper, we address this fundamental challenge by: (i)\nreleasing the first open, ultra-high-fidelity PrismLayers (PrismLayersPro)\ndataset of 200K (20K) multilayer transparent images with accurate alpha mattes,\n(ii) introducing a trainingfree synthesis pipeline that generates such data on\ndemand using off-the-shelf diffusion models, and (iii) delivering a strong,\nopen-source multi-layer generation model, ART+, which matches the aesthetics of\nmodern text-to-image generation models. The key technical contributions\ninclude: LayerFLUX, which excels at generating high-quality single transparent\nlayers with accurate alpha mattes, and MultiLayerFLUX, which composes multiple\nLayerFLUX outputs into complete images, guided by human-annotated semantic\nlayout. To ensure higher quality, we apply a rigorous filtering stage to remove\nartifacts and semantic mismatches, followed by human selection. Fine-tuning the\nstate-of-the-art ART model on our synthetic PrismLayersPro yields ART+, which\noutperforms the original ART in 60% of head-to-head user study comparisons and\neven matches the visual quality of images generated by the FLUX.1-[dev] model.\nWe anticipate that our work will establish a solid dataset foundation for the\nmulti-layer transparent image generation task, enabling research and\napplications that require precise, editable, and visually compelling layered\nimagery.",
      "upvotes": 4,
      "discussionId": "6837c1d180fc90ca2d9e82bc",
      "ai_summary": "The work introduces a dataset and model for generating high-quality, multi-layer transparent images using diffusion models and a novel synthesis pipeline.",
      "ai_keywords": [
        "PrismLayers",
        "diffusion models",
        "LayerFLUX",
        "MultiLayerFLUX",
        "alpha mattes",
        "semantic layout",
        "user study",
        "ART model",
        "FLUX.1-[dev]"
      ]
    },
    "publishedAt": "2025-05-28T12:09:33.000Z",
    "title": "PrismLayers: Open Data for High-Quality Multi-Layer Transparent Image\n  Generative Models",
    "summary": "Generating high-quality, multi-layer transparent images from text prompts can\nunlock a new level of creative control, allowing users to edit each layer as\neffortlessly as editing text outputs from LLMs. However, the development of\nmulti-layer generative models lags behind that of conventional text-to-image\nmodels due to the absence of a large, high-quality corpus of multi-layer\ntransparent data. In this paper, we address this fundamental challenge by: (i)\nreleasing the first open, ultra-high-fidelity PrismLayers (PrismLayersPro)\ndataset of 200K (20K) multilayer transparent images with accurate alpha mattes,\n(ii) introducing a trainingfree synthesis pipeline that generates such data on\ndemand using off-the-shelf diffusion models, and (iii) delivering a strong,\nopen-source multi-layer generation model, ART+, which matches the aesthetics of\nmodern text-to-image generation models. The key technical contributions\ninclude: LayerFLUX, which excels at generating high-quality single transparent\nlayers with accurate alpha mattes, and MultiLayerFLUX, which composes multiple\nLayerFLUX outputs into complete images, guided by human-annotated semantic\nlayout. To ensure higher quality, we apply a rigorous filtering stage to remove\nartifacts and semantic mismatches, followed by human selection. Fine-tuning the\nstate-of-the-art ART model on our synthetic PrismLayersPro yields ART+, which\noutperforms the original ART in 60% of head-to-head user study comparisons and\neven matches the visual quality of images generated by the FLUX.1-[dev] model.\nWe anticipate that our work will establish a solid dataset foundation for the\nmulti-layer transparent image generation task, enabling research and\napplications that require precise, editable, and visually compelling layered\nimagery.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22523.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "631f108bb45367a05fe74260",
      "avatarUrl": "/avatars/c20da6800b643728062712d9a2771648.svg",
      "fullname": "Researcher",
      "name": "YuanYuhui",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.22338",
      "authors": [
        {
          "_id": "6837c79576eac3fa930de19b",
          "name": "Hanyang Wang",
          "hidden": false
        },
        {
          "_id": "6837c79576eac3fa930de19c",
          "name": "Lu Wang",
          "hidden": false
        },
        {
          "_id": "6837c79576eac3fa930de19d",
          "name": "Chaoyun Zhang",
          "hidden": false
        },
        {
          "_id": "6837c79576eac3fa930de19e",
          "name": "Tianjun Mao",
          "hidden": false
        },
        {
          "_id": "6837c79576eac3fa930de19f",
          "name": "Si Qin",
          "hidden": false
        },
        {
          "_id": "6837c79576eac3fa930de1a0",
          "name": "Qingwei Lin",
          "hidden": false
        },
        {
          "_id": "6837c79576eac3fa930de1a1",
          "name": "Saravan Rajmohan",
          "hidden": false
        },
        {
          "_id": "6837c79576eac3fa930de1a2",
          "name": "Dongmei Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T13:23:49.000Z",
      "submittedOnDailyAt": "2025-05-29T01:04:31.053Z",
      "title": "Text2Grad: Reinforcement Learning from Natural Language Feedback",
      "submittedOnDailyBy": {
        "_id": "654dbac9938fbf1e696be8aa",
        "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
        "isPro": false,
        "fullname": "Chaoyun Zhang",
        "user": "vyokky",
        "type": "user"
      },
      "summary": "Traditional RLHF optimizes language models with coarse, scalar rewards that\nmask the fine-grained reasons behind success or failure, leading to slow and\nopaque learning. Recent work augments RL with textual critiques through\nprompting or reflection, improving interpretability but leaving model\nparameters untouched. We introduce Text2Grad, a reinforcement-learning paradigm\nthat turns free-form textual feedback into span-level gradients. Given human\n(or programmatic) critiques, Text2Grad aligns each feedback phrase with the\nrelevant token spans, converts these alignments into differentiable reward\nsignals, and performs gradient updates that directly refine the offending\nportions of the model's policy. This yields precise, feedback-conditioned\nadjustments instead of global nudges. Text2Grad is realized through three\ncomponents: (1) a high-quality feedback-annotation pipeline that pairs\ncritiques with token spans; (2) a fine-grained reward model that predicts\nspan-level reward on answer while generating explanatory critiques; and (3) a\nspan-level policy optimizer that back-propagates natural-language gradients.\nAcross summarization, code generation, and question answering, Text2Grad\nconsistently surpasses scalar-reward RL and prompt-only baselines, providing\nboth higher task metrics and richer interpretability. Our results demonstrate\nthat natural-language feedback, when converted to gradients, is a powerful\nsignal for fine-grained policy optimization. The code for our method is\navailable at https://github.com/microsoft/Text2Grad",
      "upvotes": 4,
      "discussionId": "6837c79576eac3fa930de1dd",
      "ai_summary": "Text2Grad converts human textual feedback into span-level gradients to optimize language models precisely and efficiently.",
      "ai_keywords": [
        "RLHF",
        "reinforcement-learning",
        "free-form textual feedback",
        "span-level gradients",
        "token spans",
        "differentiable reward signals",
        "gradient updates",
        "span-level policy optimizer",
        "fine-grained reward model",
        "feedback-annotation pipeline"
      ]
    },
    "publishedAt": "2025-05-28T09:23:49.000Z",
    "title": "Text2Grad: Reinforcement Learning from Natural Language Feedback",
    "summary": "Traditional RLHF optimizes language models with coarse, scalar rewards that\nmask the fine-grained reasons behind success or failure, leading to slow and\nopaque learning. Recent work augments RL with textual critiques through\nprompting or reflection, improving interpretability but leaving model\nparameters untouched. We introduce Text2Grad, a reinforcement-learning paradigm\nthat turns free-form textual feedback into span-level gradients. Given human\n(or programmatic) critiques, Text2Grad aligns each feedback phrase with the\nrelevant token spans, converts these alignments into differentiable reward\nsignals, and performs gradient updates that directly refine the offending\nportions of the model's policy. This yields precise, feedback-conditioned\nadjustments instead of global nudges. Text2Grad is realized through three\ncomponents: (1) a high-quality feedback-annotation pipeline that pairs\ncritiques with token spans; (2) a fine-grained reward model that predicts\nspan-level reward on answer while generating explanatory critiques; and (3) a\nspan-level policy optimizer that back-propagates natural-language gradients.\nAcross summarization, code generation, and question answering, Text2Grad\nconsistently surpasses scalar-reward RL and prompt-only baselines, providing\nboth higher task metrics and richer interpretability. Our results demonstrate\nthat natural-language feedback, when converted to gradients, is a powerful\nsignal for fine-grained policy optimization. The code for our method is\navailable at https://github.com/microsoft/Text2Grad",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22338.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "654dbac9938fbf1e696be8aa",
      "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
      "fullname": "Chaoyun Zhang",
      "name": "vyokky",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.22203",
      "authors": [
        {
          "_id": "6837dcc41448b8bf0c91fa30",
          "user": {
            "_id": "6462def82a83863b97c0611e",
            "avatarUrl": "/avatars/c03e9cc7d75b0266fcc56ecb6ee62148.svg",
            "isPro": false,
            "fullname": "Yuzhen Huang",
            "user": "yuzhen17",
            "type": "user"
          },
          "name": "Yuzhen Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:41:53.737Z",
          "hidden": false
        },
        {
          "_id": "6837dcc41448b8bf0c91fa31",
          "name": "Weihao Zeng",
          "hidden": false
        },
        {
          "_id": "6837dcc41448b8bf0c91fa32",
          "name": "Xingshan Zeng",
          "hidden": false
        },
        {
          "_id": "6837dcc41448b8bf0c91fa33",
          "name": "Qi Zhu",
          "hidden": false
        },
        {
          "_id": "6837dcc41448b8bf0c91fa34",
          "name": "Junxian He",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T10:28:41.000Z",
      "submittedOnDailyAt": "2025-05-29T02:37:12.694Z",
      "title": "Pitfalls of Rule- and Model-based Verifiers -- A Case Study on\n  Mathematical Reasoning",
      "submittedOnDailyBy": {
        "_id": "6462def82a83863b97c0611e",
        "avatarUrl": "/avatars/c03e9cc7d75b0266fcc56ecb6ee62148.svg",
        "isPro": false,
        "fullname": "Yuzhen Huang",
        "user": "yuzhen17",
        "type": "user"
      },
      "summary": "Trustworthy verifiers are essential for the success of reinforcement learning\nwith verifiable reward (RLVR), which is the core methodology behind various\nlarge reasoning models such as DeepSeek-R1. In complex domains like\nmathematical reasoning, rule-based verifiers have been widely adopted in\nprevious works to train strong reasoning models. However, the reliability of\nthese verifiers and their impact on the RL training process remain poorly\nunderstood. In this work, we take mathematical reasoning as a case study and\nconduct a comprehensive analysis of various verifiers in both static evaluation\nand RL training scenarios. First, we find that current open-source rule-based\nverifiers often fail to recognize equivalent answers presented in different\nformats across multiple commonly used mathematical datasets, resulting in\nnon-negligible false negative rates. This limitation adversely affects RL\ntraining performance and becomes more pronounced as the policy model gets\nstronger. Subsequently, we investigate model-based verifiers as a potential\nsolution to address these limitations. While the static evaluation shows that\nmodel-based verifiers achieve significantly higher verification accuracy,\nfurther analysis and RL training results imply that they are highly susceptible\nto hacking, where they misclassify certain patterns in responses as correct\n(i.e., false positives). This vulnerability is exploited during policy model\noptimization, leading to artificially inflated rewards. Our findings underscore\nthe unique risks inherent to both rule-based and model-based verifiers, aiming\nto offer valuable insights to develop more robust reward systems in\nreinforcement learning.",
      "upvotes": 4,
      "discussionId": "6837dcc51448b8bf0c91fa54",
      "ai_summary": "The study examines the effectiveness and reliability of rule-based and model-based verifiers in reinforcement learning with verifiable reward, highlighting limitations and vulnerabilities in their use for mathematical reasoning tasks.",
      "ai_keywords": [
        "reinforcement learning",
        "verifiable reward",
        "DeepSeek-R1",
        "mathematical reasoning",
        "rule-based verifiers",
        "reward systems",
        "model-based verifiers",
        "false negatives",
        "false positives"
      ]
    },
    "publishedAt": "2025-05-28T06:28:41.000Z",
    "title": "Pitfalls of Rule- and Model-based Verifiers -- A Case Study on\n  Mathematical Reasoning",
    "summary": "Trustworthy verifiers are essential for the success of reinforcement learning\nwith verifiable reward (RLVR), which is the core methodology behind various\nlarge reasoning models such as DeepSeek-R1. In complex domains like\nmathematical reasoning, rule-based verifiers have been widely adopted in\nprevious works to train strong reasoning models. However, the reliability of\nthese verifiers and their impact on the RL training process remain poorly\nunderstood. In this work, we take mathematical reasoning as a case study and\nconduct a comprehensive analysis of various verifiers in both static evaluation\nand RL training scenarios. First, we find that current open-source rule-based\nverifiers often fail to recognize equivalent answers presented in different\nformats across multiple commonly used mathematical datasets, resulting in\nnon-negligible false negative rates. This limitation adversely affects RL\ntraining performance and becomes more pronounced as the policy model gets\nstronger. Subsequently, we investigate model-based verifiers as a potential\nsolution to address these limitations. While the static evaluation shows that\nmodel-based verifiers achieve significantly higher verification accuracy,\nfurther analysis and RL training results imply that they are highly susceptible\nto hacking, where they misclassify certain patterns in responses as correct\n(i.e., false positives). This vulnerability is exploited during policy model\noptimization, leading to artificially inflated rewards. Our findings underscore\nthe unique risks inherent to both rule-based and model-based verifiers, aiming\nto offer valuable insights to develop more robust reward systems in\nreinforcement learning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22203.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6462def82a83863b97c0611e",
      "avatarUrl": "/avatars/c03e9cc7d75b0266fcc56ecb6ee62148.svg",
      "fullname": "Yuzhen Huang",
      "name": "yuzhen17",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.21876",
      "authors": [
        {
          "_id": "6837d80bf42b2aacfc26c460",
          "name": "Zun Wang",
          "hidden": false
        },
        {
          "_id": "6837d80bf42b2aacfc26c461",
          "name": "Jaemin Cho",
          "hidden": false
        },
        {
          "_id": "6837d80bf42b2aacfc26c462",
          "name": "Jialu Li",
          "hidden": false
        },
        {
          "_id": "6837d80bf42b2aacfc26c463",
          "name": "Han Lin",
          "hidden": false
        },
        {
          "_id": "6837d80bf42b2aacfc26c464",
          "user": {
            "_id": "652066649004117947e46ed6",
            "avatarUrl": "/avatars/972c97df6f26d2c3d6ce71ec579984bb.svg",
            "isPro": false,
            "fullname": "Jaehong Yoon",
            "user": "jaehong31",
            "type": "user"
          },
          "name": "Jaehong Yoon",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:07.590Z",
          "hidden": false
        },
        {
          "_id": "6837d80bf42b2aacfc26c465",
          "name": "Yue Zhang",
          "hidden": false
        },
        {
          "_id": "6837d80bf42b2aacfc26c466",
          "name": "Mohit Bansal",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T01:45:26.000Z",
      "submittedOnDailyAt": "2025-05-29T02:15:48.194Z",
      "title": "EPiC: Efficient Video Camera Control Learning with Precise Anchor-Video\n  Guidance",
      "submittedOnDailyBy": {
        "_id": "5ffe32d8942cf3533d364449",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654821969191-5ffe32d8942cf3533d364449.jpeg",
        "isPro": false,
        "fullname": "Jaemin Cho",
        "user": "j-min",
        "type": "user"
      },
      "summary": "Recent approaches on 3D camera control in video diffusion models (VDMs) often\ncreate anchor videos to guide diffusion models as a structured prior by\nrendering from estimated point clouds following annotated camera trajectories.\nHowever, errors inherent in point cloud estimation often lead to inaccurate\nanchor videos. Moreover, the requirement for extensive camera trajectory\nannotations further increases resource demands. To address these limitations,\nwe introduce EPiC, an efficient and precise camera control learning framework\nthat automatically constructs high-quality anchor videos without expensive\ncamera trajectory annotations. Concretely, we create highly precise anchor\nvideos for training by masking source videos based on first-frame visibility.\nThis approach ensures high alignment, eliminates the need for camera trajectory\nannotations, and thus can be readily applied to any in-the-wild video to\ngenerate image-to-video (I2V) training pairs. Furthermore, we introduce\nAnchor-ControlNet, a lightweight conditioning module that integrates anchor\nvideo guidance in visible regions to pretrained VDMs, with less than 1% of\nbackbone model parameters. By combining the proposed anchor video data and\nControlNet module, EPiC achieves efficient training with substantially fewer\nparameters, training steps, and less data, without requiring modifications to\nthe diffusion model backbone typically needed to mitigate rendering\nmisalignments. Although being trained on masking-based anchor videos, our\nmethod generalizes robustly to anchor videos made with point clouds during\ninference, enabling precise 3D-informed camera control. EPiC achieves SOTA\nperformance on RealEstate10K and MiraData for I2V camera control task,\ndemonstrating precise and robust camera control ability both quantitatively and\nqualitatively. Notably, EPiC also exhibits strong zero-shot generalization to\nvideo-to-video scenarios.",
      "upvotes": 3,
      "discussionId": "6837d810f42b2aacfc26c5ec",
      "projectPage": "https://zunwang1.github.io/Epic",
      "githubRepo": "https://github.com/wz0919/EPiC",
      "ai_summary": "EPiC is a framework for efficient 3D camera control in video diffusion models that constructs high-quality anchor videos through first-frame visibility masking, integrates them using a lightweight ControlNet module, and achieves state-of-the-art performance on I2V tasks with minimal resources.",
      "ai_keywords": [
        "anchor videos",
        "point cloud estimation",
        "camera trajectories",
        "diffusion models",
        "first-frame visibility",
        "EPiC",
        "ControlNet",
        "I2V training pairs",
        "rendering misalignments",
        "RealEstate10K",
        "MiraData"
      ]
    },
    "publishedAt": "2025-05-27T21:45:26.000Z",
    "title": "EPiC: Efficient Video Camera Control Learning with Precise Anchor-Video\n  Guidance",
    "summary": "Recent approaches on 3D camera control in video diffusion models (VDMs) often\ncreate anchor videos to guide diffusion models as a structured prior by\nrendering from estimated point clouds following annotated camera trajectories.\nHowever, errors inherent in point cloud estimation often lead to inaccurate\nanchor videos. Moreover, the requirement for extensive camera trajectory\nannotations further increases resource demands. To address these limitations,\nwe introduce EPiC, an efficient and precise camera control learning framework\nthat automatically constructs high-quality anchor videos without expensive\ncamera trajectory annotations. Concretely, we create highly precise anchor\nvideos for training by masking source videos based on first-frame visibility.\nThis approach ensures high alignment, eliminates the need for camera trajectory\nannotations, and thus can be readily applied to any in-the-wild video to\ngenerate image-to-video (I2V) training pairs. Furthermore, we introduce\nAnchor-ControlNet, a lightweight conditioning module that integrates anchor\nvideo guidance in visible regions to pretrained VDMs, with less than 1% of\nbackbone model parameters. By combining the proposed anchor video data and\nControlNet module, EPiC achieves efficient training with substantially fewer\nparameters, training steps, and less data, without requiring modifications to\nthe diffusion model backbone typically needed to mitigate rendering\nmisalignments. Although being trained on masking-based anchor videos, our\nmethod generalizes robustly to anchor videos made with point clouds during\ninference, enabling precise 3D-informed camera control. EPiC achieves SOTA\nperformance on RealEstate10K and MiraData for I2V camera control task,\ndemonstrating precise and robust camera control ability both quantitatively and\nqualitatively. Notably, EPiC also exhibits strong zero-shot generalization to\nvideo-to-video scenarios.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21876.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5ffe32d8942cf3533d364449",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654821969191-5ffe32d8942cf3533d364449.jpeg",
      "fullname": "Jaemin Cho",
      "name": "j-min",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.18700",
      "authors": [
        {
          "_id": "6837cc29bbee677da741aba7",
          "name": "Chun Wang",
          "hidden": false
        },
        {
          "_id": "6837cc29bbee677da741aba8",
          "name": "Xiaoran Pan",
          "hidden": false
        },
        {
          "_id": "6837cc29bbee677da741aba9",
          "name": "Zihao Pan",
          "hidden": false
        },
        {
          "_id": "6837cc29bbee677da741abaa",
          "name": "Haofan Wang",
          "hidden": false
        },
        {
          "_id": "6837cc29bbee677da741abab",
          "name": "Yiren Song",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-24T13:48:57.000Z",
      "submittedOnDailyAt": "2025-05-29T01:25:51.194Z",
      "title": "GRE Suite: Geo-localization Inference via Fine-Tuned Vision-Language\n  Models and Enhanced Reasoning Chains",
      "submittedOnDailyBy": {
        "_id": "64311a95034ecbefddd141ef",
        "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
        "isPro": true,
        "fullname": "Yiren Song",
        "user": "yiren98",
        "type": "user"
      },
      "summary": "Recent advances in Visual Language Models (VLMs) have demonstrated\nexceptional performance in visual reasoning tasks. However, geo-localization\npresents unique challenges, requiring the extraction of multigranular visual\ncues from images and their integration with external world knowledge for\nsystematic reasoning. Current approaches to geo-localization tasks often lack\nrobust reasoning mechanisms and explainability, limiting their effectiveness.\nTo address these limitations, we propose the Geo Reason Enhancement (GRE)\nSuite, a novel framework that augments VLMs with structured reasoning chains\nfor accurate and interpretable location inference. The GRE Suite is\nsystematically developed across three key dimensions: dataset, model, and\nbenchmark. First, we introduce GRE30K, a high-quality geo-localization\nreasoning dataset designed to facilitate fine-grained visual and contextual\nanalysis. Next, we present the GRE model, which employs a multi-stage reasoning\nstrategy to progressively infer scene attributes, local details, and semantic\nfeatures, thereby narrowing down potential geographic regions with enhanced\nprecision. Finally, we construct the Geo Reason Evaluation Benchmark\n(GREval-Bench), a comprehensive evaluation framework that assesses VLMs across\ndiverse urban, natural, and landmark scenes to measure both coarse-grained\n(e.g., country, continent) and fine-grained (e.g., city, street) localization\nperformance. Experimental results demonstrate that GRE significantly\noutperforms existing methods across all granularities of geo-localization\ntasks, underscoring the efficacy of reasoning-augmented VLMs in complex\ngeographic inference. Code and data will be released at\nhttps://github.com/Thorin215/GRE.",
      "upvotes": 3,
      "discussionId": "6837cc2abbee677da741abf5",
      "ai_summary": "The GRE Suite enhances Visual Language Models with structured reasoning chains, improving geo-localization tasks through a multi-stage strategy and comprehensive evaluation benchmark.",
      "ai_keywords": [
        "Visual Language Models",
        "geo-localization",
        "reasoning chains",
        "GRE30K",
        "GRE model",
        "GREval-Bench",
        "multi-stage reasoning",
        "scene attributes",
        "local details",
        "semantic features",
        "coarse-grained localization",
        "fine-grained localization"
      ]
    },
    "publishedAt": "2025-05-24T09:48:57.000Z",
    "title": "GRE Suite: Geo-localization Inference via Fine-Tuned Vision-Language\n  Models and Enhanced Reasoning Chains",
    "summary": "Recent advances in Visual Language Models (VLMs) have demonstrated\nexceptional performance in visual reasoning tasks. However, geo-localization\npresents unique challenges, requiring the extraction of multigranular visual\ncues from images and their integration with external world knowledge for\nsystematic reasoning. Current approaches to geo-localization tasks often lack\nrobust reasoning mechanisms and explainability, limiting their effectiveness.\nTo address these limitations, we propose the Geo Reason Enhancement (GRE)\nSuite, a novel framework that augments VLMs with structured reasoning chains\nfor accurate and interpretable location inference. The GRE Suite is\nsystematically developed across three key dimensions: dataset, model, and\nbenchmark. First, we introduce GRE30K, a high-quality geo-localization\nreasoning dataset designed to facilitate fine-grained visual and contextual\nanalysis. Next, we present the GRE model, which employs a multi-stage reasoning\nstrategy to progressively infer scene attributes, local details, and semantic\nfeatures, thereby narrowing down potential geographic regions with enhanced\nprecision. Finally, we construct the Geo Reason Evaluation Benchmark\n(GREval-Bench), a comprehensive evaluation framework that assesses VLMs across\ndiverse urban, natural, and landmark scenes to measure both coarse-grained\n(e.g., country, continent) and fine-grained (e.g., city, street) localization\nperformance. Experimental results demonstrate that GRE significantly\noutperforms existing methods across all granularities of geo-localization\ntasks, underscoring the efficacy of reasoning-augmented VLMs in complex\ngeographic inference. Code and data will be released at\nhttps://github.com/Thorin215/GRE.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18700.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64311a95034ecbefddd141ef",
      "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
      "fullname": "Yiren Song",
      "name": "yiren98",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.17870",
      "authors": [
        {
          "_id": "6837f049023315653be65a88",
          "user": {
            "_id": "640f32f6ef5c6dcac8b094bd",
            "avatarUrl": "/avatars/89b95837666ad696fe1f10808e4619b0.svg",
            "isPro": false,
            "fullname": "Shaina Raza",
            "user": "Shainarazavi",
            "type": "user"
          },
          "name": "Shaina Raza",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-29T05:27:38.248Z",
          "hidden": false
        },
        {
          "_id": "6837f049023315653be65a89",
          "name": "Rizwan Qureshi",
          "hidden": false
        },
        {
          "_id": "6837f049023315653be65a8a",
          "name": "Marcelo Lotif",
          "hidden": false
        },
        {
          "_id": "6837f049023315653be65a8b",
          "user": {
            "_id": "63a4754927f1f64ed7238dac",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
            "isPro": false,
            "fullname": "Aman Chadha",
            "user": "amanchadha",
            "type": "user"
          },
          "name": "Aman Chadha",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:41:40.928Z",
          "hidden": false
        },
        {
          "_id": "6837f049023315653be65a8c",
          "name": "Deval Pandya",
          "hidden": false
        },
        {
          "_id": "6837f049023315653be65a8d",
          "name": "Christos Emmanouilidis",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T13:20:23.000Z",
      "submittedOnDailyAt": "2025-05-29T04:08:04.627Z",
      "title": "Just as Humans Need Vaccines, So Do Models: Model Immunization to Combat\n  Falsehoods",
      "submittedOnDailyBy": {
        "_id": "63a4754927f1f64ed7238dac",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
        "isPro": false,
        "fullname": "Aman Chadha",
        "user": "amanchadha",
        "type": "user"
      },
      "summary": "Generative AI models often learn and reproduce false information present in\ntheir training corpora. This position paper argues that, analogous to\nbiological immunization, where controlled exposure to a weakened pathogen\nbuilds immunity, AI models should be fine tuned on small, quarantined sets of\nexplicitly labeled falsehoods as a \"vaccine\" against misinformation. These\ncurated false examples are periodically injected during finetuning,\nstrengthening the model ability to recognize and reject misleading claims while\npreserving accuracy on truthful inputs. An illustrative case study shows that\nimmunized models generate substantially less misinformation than baselines. To\nour knowledge, this is the first training framework that treats fact checked\nfalsehoods themselves as a supervised vaccine, rather than relying on input\nperturbations or generic human feedback signals, to harden models against\nfuture misinformation. We also outline ethical safeguards and governance\ncontrols to ensure the safe use of false data. Model immunization offers a\nproactive paradigm for aligning AI systems with factuality.",
      "upvotes": 3,
      "discussionId": "6837f04a023315653be65ac6",
      "ai_summary": "A generative AI model is fine-tuned with labeled falsehoods to reduce misinformation generation, analogous to biological immunization.",
      "ai_keywords": [
        "generative AI models",
        "misinformation",
        "fine-tuning",
        "labeled falsehoods",
        "immunization",
        "fact-checked falsehoods",
        "supervised vaccine"
      ]
    },
    "publishedAt": "2025-05-23T09:20:23.000Z",
    "title": "Just as Humans Need Vaccines, So Do Models: Model Immunization to Combat\n  Falsehoods",
    "summary": "Generative AI models often learn and reproduce false information present in\ntheir training corpora. This position paper argues that, analogous to\nbiological immunization, where controlled exposure to a weakened pathogen\nbuilds immunity, AI models should be fine tuned on small, quarantined sets of\nexplicitly labeled falsehoods as a \"vaccine\" against misinformation. These\ncurated false examples are periodically injected during finetuning,\nstrengthening the model ability to recognize and reject misleading claims while\npreserving accuracy on truthful inputs. An illustrative case study shows that\nimmunized models generate substantially less misinformation than baselines. To\nour knowledge, this is the first training framework that treats fact checked\nfalsehoods themselves as a supervised vaccine, rather than relying on input\nperturbations or generic human feedback signals, to harden models against\nfuture misinformation. We also outline ethical safeguards and governance\ncontrols to ensure the safe use of false data. Model immunization offers a\nproactive paradigm for aligning AI systems with factuality.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17870.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a4754927f1f64ed7238dac",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
      "fullname": "Aman Chadha",
      "name": "amanchadha",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22202",
      "authors": [
        {
          "_id": "6837eff312d1f7a138bd09b3",
          "name": "Hyeonbin Hwang",
          "hidden": false
        },
        {
          "_id": "6837eff312d1f7a138bd09b4",
          "name": "Byeongguk Jeon",
          "hidden": false
        },
        {
          "_id": "6837eff312d1f7a138bd09b5",
          "name": "Seungone Kim",
          "hidden": false
        },
        {
          "_id": "6837eff312d1f7a138bd09b6",
          "name": "Jiyeon Kim",
          "hidden": false
        },
        {
          "_id": "6837eff312d1f7a138bd09b7",
          "name": "Hoyeon Chang",
          "hidden": false
        },
        {
          "_id": "6837eff312d1f7a138bd09b8",
          "name": "Sohee Yang",
          "hidden": false
        },
        {
          "_id": "6837eff312d1f7a138bd09b9",
          "name": "Seungpil Won",
          "hidden": false
        },
        {
          "_id": "6837eff312d1f7a138bd09ba",
          "name": "Dohaeng Lee",
          "hidden": false
        },
        {
          "_id": "6837eff312d1f7a138bd09bb",
          "name": "Youbin Ahn",
          "hidden": false
        },
        {
          "_id": "6837eff312d1f7a138bd09bc",
          "name": "Minjoon Seo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T10:28:35.000Z",
      "submittedOnDailyAt": "2025-05-29T04:00:37.847Z",
      "title": "Let's Predict Sentence by Sentence",
      "submittedOnDailyBy": {
        "_id": "647eaaf61a1fcad2fdc5d1ef",
        "avatarUrl": "/avatars/596d7a61d6e6227d38b661210a32fed0.svg",
        "isPro": false,
        "fullname": "Hyeonbin Hwang ",
        "user": "hbin0701",
        "type": "user"
      },
      "summary": "Autoregressive language models (LMs) generate one token at a time, yet human\nreasoning operates over higher-level abstractions - sentences, propositions,\nand concepts. This contrast raises a central question- Can LMs likewise learn\nto reason over structured semantic units rather than raw token sequences? In\nthis work, we investigate whether pretrained LMs can be lifted into such\nabstract reasoning spaces by building on their learned representations. We\npresent a framework that adapts a pretrained token-level LM to operate in\nsentence space by autoregressively predicting continuous embeddings of next\nsentences. We explore two embedding paradigms inspired by classical\nrepresentation learning: 1) semantic embeddings, learned via autoencoding to\npreserve surface meaning; and 2) contextual embeddings, trained via\nnext-sentence prediction to encode anticipatory structure. We evaluate both\nunder two inference regimes: Discretized, which decodes each predicted\nembedding into text before re-encoding; and Continuous, which reasons entirely\nin embedding space for improved efficiency. Across four domains - mathematics,\nlogic, commonsense, and planning - contextual embeddings under continuous\ninference show competitive performance with Chain-of-Thought (CoT) while\nreducing inference-time FLOPs on average by half. We also present early signs\nof scalability and modular adaptation. Finally, to visualize latent\ntrajectories, we introduce SentenceLens, a diagnostic tool that decodes\nintermediate model states into interpretable sentences. Together, our results\nindicate that pretrained LMs can effectively transition to abstract, structured\nreasoning within latent embedding spaces.",
      "upvotes": 2,
      "discussionId": "6837eff412d1f7a138bd0a3e",
      "ai_summary": "Pretrained language models can adapt to operate in sentence space, reason over structured semantic units, and achieve competitive performance with Chain-of-Thought while reducing inference-time FLOPs.",
      "ai_keywords": [
        "autoregressive language models",
        "semantic embeddings",
        "contextual embeddings",
        "next-sentence prediction",
        "Chain-of-Thought",
        "SentenceLens"
      ]
    },
    "publishedAt": "2025-05-28T06:28:35.000Z",
    "title": "Let's Predict Sentence by Sentence",
    "summary": "Autoregressive language models (LMs) generate one token at a time, yet human\nreasoning operates over higher-level abstractions - sentences, propositions,\nand concepts. This contrast raises a central question- Can LMs likewise learn\nto reason over structured semantic units rather than raw token sequences? In\nthis work, we investigate whether pretrained LMs can be lifted into such\nabstract reasoning spaces by building on their learned representations. We\npresent a framework that adapts a pretrained token-level LM to operate in\nsentence space by autoregressively predicting continuous embeddings of next\nsentences. We explore two embedding paradigms inspired by classical\nrepresentation learning: 1) semantic embeddings, learned via autoencoding to\npreserve surface meaning; and 2) contextual embeddings, trained via\nnext-sentence prediction to encode anticipatory structure. We evaluate both\nunder two inference regimes: Discretized, which decodes each predicted\nembedding into text before re-encoding; and Continuous, which reasons entirely\nin embedding space for improved efficiency. Across four domains - mathematics,\nlogic, commonsense, and planning - contextual embeddings under continuous\ninference show competitive performance with Chain-of-Thought (CoT) while\nreducing inference-time FLOPs on average by half. We also present early signs\nof scalability and modular adaptation. Finally, to visualize latent\ntrajectories, we introduce SentenceLens, a diagnostic tool that decodes\nintermediate model states into interpretable sentences. Together, our results\nindicate that pretrained LMs can effectively transition to abstract, structured\nreasoning within latent embedding spaces.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22202.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647eaaf61a1fcad2fdc5d1ef",
      "avatarUrl": "/avatars/596d7a61d6e6227d38b661210a32fed0.svg",
      "fullname": "Hyeonbin Hwang ",
      "name": "hbin0701",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.21191",
      "authors": [
        {
          "_id": "6837eb38f09a146728a4b80f",
          "name": "Junyan Zhang",
          "hidden": false
        },
        {
          "_id": "6837eb38f09a146728a4b810",
          "name": "Yubo Gao",
          "hidden": false
        },
        {
          "_id": "6837eb38f09a146728a4b811",
          "name": "Yibo Yan",
          "hidden": false
        },
        {
          "_id": "6837eb38f09a146728a4b812",
          "name": "Jungang Li",
          "hidden": false
        },
        {
          "_id": "6837eb38f09a146728a4b813",
          "name": "Zhaorui Hou",
          "hidden": false
        },
        {
          "_id": "6837eb38f09a146728a4b814",
          "name": "Sicheng Tao",
          "hidden": false
        },
        {
          "_id": "6837eb38f09a146728a4b815",
          "name": "Shuliang Liu",
          "hidden": false
        },
        {
          "_id": "6837eb38f09a146728a4b816",
          "name": "Song Dai",
          "hidden": false
        },
        {
          "_id": "6837eb38f09a146728a4b817",
          "name": "Yonghua Hei",
          "hidden": false
        },
        {
          "_id": "6837eb38f09a146728a4b818",
          "name": "Junzhuo Li",
          "hidden": false
        },
        {
          "_id": "6837eb38f09a146728a4b819",
          "name": "Xuming Hu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-27T13:40:28.000Z",
      "submittedOnDailyAt": "2025-05-29T03:36:16.801Z",
      "title": "Unveiling Instruction-Specific Neurons & Experts: An Analytical\n  Framework for LLM's Instruction-Following Capabilities",
      "submittedOnDailyBy": {
        "_id": "64b76528fdb702b3d8641514",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b76528fdb702b3d8641514/wCbEtpPEHRjS4sZSVK1gK.png",
        "isPro": false,
        "fullname": "Jungang Li",
        "user": "Jungang",
        "type": "user"
      },
      "summary": "The finetuning of Large Language Models (LLMs) has significantly advanced\ntheir instruction-following capabilities, yet the underlying computational\nmechanisms driving these improvements remain poorly understood. This study\nsystematically examines how fine-tuning reconfigures LLM computations by\nisolating and analyzing instruction-specific sparse components, i.e., neurons\nin dense models and both neurons and experts in Mixture-of-Experts (MoE)\narchitectures. In particular, we introduce HexaInst, a carefully curated and\nbalanced instructional dataset spanning six distinct categories, and propose\nSPARCOM, a novel analytical framework comprising three key contributions: (1) a\nmethod for identifying these sparse components, (2) an evaluation of their\nfunctional generality and uniqueness, and (3) a systematic comparison of their\nalterations. Through experiments, we demonstrate functional generality,\nuniqueness, and the critical role of these components in instruction execution.\nBy elucidating the relationship between fine-tuning-induced adaptations and\nsparse computational substrates, this work provides deeper insights into how\nLLMs internalize instruction-following behavior for the trustworthy LLM\ncommunity.",
      "upvotes": 2,
      "discussionId": "6837eb39f09a146728a4b872",
      "ai_summary": "The study investigates the role of sparse computational components in the instruction-following capabilities of Large Language Models through systematic analysis and introduces HexaInst and SPARCOM for better understanding.",
      "ai_keywords": [
        "Large Language Models",
        "fine-tuning",
        "instruction-following",
        "HexaInst",
        "SPARCOM",
        "sparse components",
        "neurons",
        "Mixture-of-Experts",
        "instruction execution",
        "computational adaptations"
      ]
    },
    "publishedAt": "2025-05-27T09:40:28.000Z",
    "title": "Unveiling Instruction-Specific Neurons & Experts: An Analytical\n  Framework for LLM's Instruction-Following Capabilities",
    "summary": "The finetuning of Large Language Models (LLMs) has significantly advanced\ntheir instruction-following capabilities, yet the underlying computational\nmechanisms driving these improvements remain poorly understood. This study\nsystematically examines how fine-tuning reconfigures LLM computations by\nisolating and analyzing instruction-specific sparse components, i.e., neurons\nin dense models and both neurons and experts in Mixture-of-Experts (MoE)\narchitectures. In particular, we introduce HexaInst, a carefully curated and\nbalanced instructional dataset spanning six distinct categories, and propose\nSPARCOM, a novel analytical framework comprising three key contributions: (1) a\nmethod for identifying these sparse components, (2) an evaluation of their\nfunctional generality and uniqueness, and (3) a systematic comparison of their\nalterations. Through experiments, we demonstrate functional generality,\nuniqueness, and the critical role of these components in instruction execution.\nBy elucidating the relationship between fine-tuning-induced adaptations and\nsparse computational substrates, this work provides deeper insights into how\nLLMs internalize instruction-following behavior for the trustworthy LLM\ncommunity.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21191.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "64b76528fdb702b3d8641514",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b76528fdb702b3d8641514/wCbEtpPEHRjS4sZSVK1gK.png",
      "fullname": "Jungang Li",
      "name": "Jungang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.17507",
      "authors": [
        {
          "_id": "6833f24ed5c438959f7decf9",
          "user": {
            "_id": "619ef3f253061ce00477b09e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/619ef3f253061ce00477b09e/FknZhgQhV2_3aTqIKVsTo.jpeg",
            "isPro": false,
            "fullname": "Qiaosheng Chen",
            "user": "cqsss",
            "type": "user"
          },
          "name": "Qiaosheng Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-28T09:01:00.198Z",
          "hidden": false
        },
        {
          "_id": "6833f24ed5c438959f7decfa",
          "name": "Kaijia Huang",
          "hidden": false
        },
        {
          "_id": "6833f24ed5c438959f7decfb",
          "name": "Xiao Zhou",
          "hidden": false
        },
        {
          "_id": "6833f24ed5c438959f7decfc",
          "name": "Weiqing Luo",
          "hidden": false
        },
        {
          "_id": "6833f24ed5c438959f7decfd",
          "name": "Yuanning Cui",
          "hidden": false
        },
        {
          "_id": "6833f24ed5c438959f7decfe",
          "name": "Gong Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T06:00:20.000Z",
      "submittedOnDailyAt": "2025-05-29T00:53:38.693Z",
      "title": "Benchmarking Recommendation, Classification, and Tracing Based on\n  Hugging Face Knowledge Graph",
      "submittedOnDailyBy": {
        "_id": "619ef3f253061ce00477b09e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/619ef3f253061ce00477b09e/FknZhgQhV2_3aTqIKVsTo.jpeg",
        "isPro": false,
        "fullname": "Qiaosheng Chen",
        "user": "cqsss",
        "type": "user"
      },
      "summary": "The rapid growth of open source machine learning (ML) resources, such as\nmodels and datasets, has accelerated IR research. However, existing platforms\nlike Hugging Face do not explicitly utilize structured representations,\nlimiting advanced queries and analyses such as tracing model evolution and\nrecommending relevant datasets. To fill the gap, we construct HuggingKG, the\nfirst large-scale knowledge graph built from the Hugging Face community for ML\nresource management. With 2.6 million nodes and 6.2 million edges, HuggingKG\ncaptures domain-specific relations and rich textual attributes. It enables us\nto further present HuggingBench, a multi-task benchmark with three novel test\ncollections for IR tasks including resource recommendation, classification, and\ntracing. Our experiments reveal unique characteristics of HuggingKG and the\nderived tasks. Both resources are publicly available, expected to advance\nresearch in open source resource sharing and management.",
      "upvotes": 2,
      "discussionId": "6833f24ed5c438959f7ded31",
      "githubRepo": "https://github.com/nju-websoft/HuggingBench",
      "ai_summary": "HuggingKG, a large-scale knowledge graph, enhances open source ML resource management by enabling advanced queries and analyses via HuggingBench.",
      "ai_keywords": [
        "knowledge graph",
        "resource recommendation",
        "classification",
        "tracing",
        "multi-task benchmark"
      ]
    },
    "publishedAt": "2025-05-23T02:00:20.000Z",
    "title": "Benchmarking Recommendation, Classification, and Tracing Based on\n  Hugging Face Knowledge Graph",
    "summary": "The rapid growth of open source machine learning (ML) resources, such as\nmodels and datasets, has accelerated IR research. However, existing platforms\nlike Hugging Face do not explicitly utilize structured representations,\nlimiting advanced queries and analyses such as tracing model evolution and\nrecommending relevant datasets. To fill the gap, we construct HuggingKG, the\nfirst large-scale knowledge graph built from the Hugging Face community for ML\nresource management. With 2.6 million nodes and 6.2 million edges, HuggingKG\ncaptures domain-specific relations and rich textual attributes. It enables us\nto further present HuggingBench, a multi-task benchmark with three novel test\ncollections for IR tasks including resource recommendation, classification, and\ntracing. Our experiments reveal unique characteristics of HuggingKG and the\nderived tasks. Both resources are publicly available, expected to advance\nresearch in open source resource sharing and management.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17507.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "619ef3f253061ce00477b09e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/619ef3f253061ce00477b09e/FknZhgQhV2_3aTqIKVsTo.jpeg",
      "fullname": "Qiaosheng Chen",
      "name": "cqsss",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.12667",
      "authors": [
        {
          "_id": "682dd41740c6417d995087de",
          "user": {
            "_id": "648dca31385b84261811505d",
            "avatarUrl": "/avatars/dfd124e3b5ffed8b0d3f429be0f7bdc0.svg",
            "isPro": false,
            "fullname": "Zihan Su",
            "user": "Sugewud",
            "type": "user"
          },
          "name": "Zihan Su",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-28T16:17:54.970Z",
          "hidden": false
        },
        {
          "_id": "682dd41740c6417d995087df",
          "name": "Xuerui Qiu",
          "hidden": false
        },
        {
          "_id": "682dd41740c6417d995087e0",
          "name": "Hongbin Xu",
          "hidden": false
        },
        {
          "_id": "682dd41740c6417d995087e1",
          "name": "Tangyu Jiang",
          "hidden": false
        },
        {
          "_id": "682dd41740c6417d995087e2",
          "name": "Junhao Zhuang",
          "hidden": false
        },
        {
          "_id": "682dd41740c6417d995087e3",
          "name": "Chun Yuan",
          "hidden": false
        },
        {
          "_id": "682dd41740c6417d995087e4",
          "name": "Ming Li",
          "hidden": false
        },
        {
          "_id": "682dd41740c6417d995087e5",
          "name": "Shengfeng He",
          "hidden": false
        },
        {
          "_id": "682dd41740c6417d995087e6",
          "name": "Fei Richard Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-19T03:31:31.000Z",
      "submittedOnDailyAt": "2025-05-29T01:30:41.124Z",
      "title": "Safe-Sora: Safe Text-to-Video Generation via Graphical Watermarking",
      "submittedOnDailyBy": {
        "_id": "648dca31385b84261811505d",
        "avatarUrl": "/avatars/dfd124e3b5ffed8b0d3f429be0f7bdc0.svg",
        "isPro": false,
        "fullname": "Zihan Su",
        "user": "Sugewud",
        "type": "user"
      },
      "summary": "The explosive growth of generative video models has amplified the demand for\nreliable copyright preservation of AI-generated content. Despite its popularity\nin image synthesis, invisible generative watermarking remains largely\nunderexplored in video generation. To address this gap, we propose Safe-Sora,\nthe first framework to embed graphical watermarks directly into the video\ngeneration process. Motivated by the observation that watermarking performance\nis closely tied to the visual similarity between the watermark and cover\ncontent, we introduce a hierarchical coarse-to-fine adaptive matching\nmechanism. Specifically, the watermark image is divided into patches, each\nassigned to the most visually similar video frame, and further localized to the\noptimal spatial region for seamless embedding. To enable spatiotemporal fusion\nof watermark patches across video frames, we develop a 3D wavelet\ntransform-enhanced Mamba architecture with a novel spatiotemporal local\nscanning strategy, effectively modeling long-range dependencies during\nwatermark embedding and retrieval. To the best of our knowledge, this is the\nfirst attempt to apply state space models to watermarking, opening new avenues\nfor efficient and robust watermark protection. Extensive experiments\ndemonstrate that Safe-Sora achieves state-of-the-art performance in terms of\nvideo quality, watermark fidelity, and robustness, which is largely attributed\nto our proposals. We will release our code upon publication.",
      "upvotes": 2,
      "discussionId": "682dd41840c6417d99508847",
      "projectPage": "https://sugewud.github.io/Safe-Sora-project/",
      "githubRepo": "https://github.com/Sugewud/Safe-Sora",
      "ai_summary": "Safe-Sora embeds invisible watermarks into AI-generated videos using a hierarchical adaptive matching mechanism and a 3D wavelet transform-enhanced Mamba architecture, achieving top performance in video quality, watermark fidelity, and robustness.",
      "ai_keywords": [
        "generative watermarking",
        "hierarchical coarse-to-fine adaptive matching",
        "3D wavelet transform",
        "Mamba architecture",
        "spatiotemporal local scanning",
        "state space models",
        "watermark embedding",
        "watermark retrieval"
      ]
    },
    "publishedAt": "2025-05-18T23:31:31.000Z",
    "title": "Safe-Sora: Safe Text-to-Video Generation via Graphical Watermarking",
    "summary": "The explosive growth of generative video models has amplified the demand for\nreliable copyright preservation of AI-generated content. Despite its popularity\nin image synthesis, invisible generative watermarking remains largely\nunderexplored in video generation. To address this gap, we propose Safe-Sora,\nthe first framework to embed graphical watermarks directly into the video\ngeneration process. Motivated by the observation that watermarking performance\nis closely tied to the visual similarity between the watermark and cover\ncontent, we introduce a hierarchical coarse-to-fine adaptive matching\nmechanism. Specifically, the watermark image is divided into patches, each\nassigned to the most visually similar video frame, and further localized to the\noptimal spatial region for seamless embedding. To enable spatiotemporal fusion\nof watermark patches across video frames, we develop a 3D wavelet\ntransform-enhanced Mamba architecture with a novel spatiotemporal local\nscanning strategy, effectively modeling long-range dependencies during\nwatermark embedding and retrieval. To the best of our knowledge, this is the\nfirst attempt to apply state space models to watermarking, opening new avenues\nfor efficient and robust watermark protection. Extensive experiments\ndemonstrate that Safe-Sora achieves state-of-the-art performance in terms of\nvideo quality, watermark fidelity, and robustness, which is largely attributed\nto our proposals. We will release our code upon publication.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.12667.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648dca31385b84261811505d",
      "avatarUrl": "/avatars/dfd124e3b5ffed8b0d3f429be0f7bdc0.svg",
      "fullname": "Zihan Su",
      "name": "Sugewud",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22645",
      "authors": [
        {
          "_id": "6837dbed1233747046da00f5",
          "name": "Hanjia Lyu",
          "hidden": false
        },
        {
          "_id": "6837dbed1233747046da00f6",
          "name": "Jiebo Luo",
          "hidden": false
        },
        {
          "_id": "6837dbed1233747046da00f7",
          "name": "Jian Kang",
          "hidden": false
        },
        {
          "_id": "6837dbed1233747046da00f8",
          "name": "Allison Koenecke",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T17:56:49.000Z",
      "submittedOnDailyAt": "2025-05-29T02:33:57.876Z",
      "title": "Characterizing Bias: Benchmarking Large Language Models in Simplified\n  versus Traditional Chinese",
      "submittedOnDailyBy": {
        "_id": "64c939307dba66c3a7e4d215",
        "avatarUrl": "/avatars/0b662cc1799525188476f3e6e1f97d29.svg",
        "isPro": false,
        "fullname": "BruceLyu",
        "user": "brucelyu",
        "type": "user"
      },
      "summary": "While the capabilities of Large Language Models (LLMs) have been studied in\nboth Simplified and Traditional Chinese, it is yet unclear whether LLMs exhibit\ndifferential performance when prompted in these two variants of written\nChinese. This understanding is critical, as disparities in the quality of LLM\nresponses can perpetuate representational harms by ignoring the different\ncultural contexts underlying Simplified versus Traditional Chinese, and can\nexacerbate downstream harms in LLM-facilitated decision-making in domains such\nas education or hiring. To investigate potential LLM performance disparities,\nwe design two benchmark tasks that reflect real-world scenarios: regional term\nchoice (prompting the LLM to name a described item which is referred to\ndifferently in Mainland China and Taiwan), and regional name choice (prompting\nthe LLM to choose who to hire from a list of names in both Simplified and\nTraditional Chinese). For both tasks, we audit the performance of 11 leading\ncommercial LLM services and open-sourced models -- spanning those primarily\ntrained on English, Simplified Chinese, or Traditional Chinese. Our analyses\nindicate that biases in LLM responses are dependent on both the task and\nprompting language: while most LLMs disproportionately favored Simplified\nChinese responses in the regional term choice task, they surprisingly favored\nTraditional Chinese names in the regional name choice task. We find that these\ndisparities may arise from differences in training data representation, written\ncharacter preferences, and tokenization of Simplified and Traditional Chinese.\nThese findings highlight the need for further analysis of LLM biases; as such,\nwe provide an open-sourced benchmark dataset to foster reproducible evaluations\nof future LLM behavior across Chinese language variants\n(https://github.com/brucelyu17/SC-TC-Bench).",
      "upvotes": 1,
      "discussionId": "6837dbee1233747046da0125",
      "githubRepo": "https://github.com/brucelyu17/SC-TC-Bench",
      "ai_summary": "Research examines LLM performance biases between Simplified and Traditional Chinese in regional term and name choice tasks, attributing differences to training data and tokenization.",
      "ai_keywords": [
        "Large Language Models",
        "LLMs",
        "LLM-facilitated decision-making",
        "regional term choice",
        "regional name choice",
        "open-sourced benchmark dataset",
        "SC-TC-Bench"
      ]
    },
    "publishedAt": "2025-05-28T13:56:49.000Z",
    "title": "Characterizing Bias: Benchmarking Large Language Models in Simplified\n  versus Traditional Chinese",
    "summary": "While the capabilities of Large Language Models (LLMs) have been studied in\nboth Simplified and Traditional Chinese, it is yet unclear whether LLMs exhibit\ndifferential performance when prompted in these two variants of written\nChinese. This understanding is critical, as disparities in the quality of LLM\nresponses can perpetuate representational harms by ignoring the different\ncultural contexts underlying Simplified versus Traditional Chinese, and can\nexacerbate downstream harms in LLM-facilitated decision-making in domains such\nas education or hiring. To investigate potential LLM performance disparities,\nwe design two benchmark tasks that reflect real-world scenarios: regional term\nchoice (prompting the LLM to name a described item which is referred to\ndifferently in Mainland China and Taiwan), and regional name choice (prompting\nthe LLM to choose who to hire from a list of names in both Simplified and\nTraditional Chinese). For both tasks, we audit the performance of 11 leading\ncommercial LLM services and open-sourced models -- spanning those primarily\ntrained on English, Simplified Chinese, or Traditional Chinese. Our analyses\nindicate that biases in LLM responses are dependent on both the task and\nprompting language: while most LLMs disproportionately favored Simplified\nChinese responses in the regional term choice task, they surprisingly favored\nTraditional Chinese names in the regional name choice task. We find that these\ndisparities may arise from differences in training data representation, written\ncharacter preferences, and tokenization of Simplified and Traditional Chinese.\nThese findings highlight the need for further analysis of LLM biases; as such,\nwe provide an open-sourced benchmark dataset to foster reproducible evaluations\nof future LLM behavior across Chinese language variants\n(https://github.com/brucelyu17/SC-TC-Bench).",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22645.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c939307dba66c3a7e4d215",
      "avatarUrl": "/avatars/0b662cc1799525188476f3e6e1f97d29.svg",
      "fullname": "BruceLyu",
      "name": "brucelyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.21960",
      "authors": [
        {
          "_id": "6837dd74ec10479b9605da15",
          "user": {
            "_id": "637e1cf4f09bf2498c543a73",
            "avatarUrl": "/avatars/3c0e4e15602a384839bee0c23029f58a.svg",
            "isPro": false,
            "fullname": "Senmao Li",
            "user": "senmaonk",
            "type": "user"
          },
          "name": "Senmao Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:41:51.659Z",
          "hidden": false
        },
        {
          "_id": "6837dd74ec10479b9605da16",
          "name": "Lei Wang",
          "hidden": false
        },
        {
          "_id": "6837dd74ec10479b9605da17",
          "name": "Kai Wang",
          "hidden": false
        },
        {
          "_id": "6837dd74ec10479b9605da18",
          "name": "Tao Liu",
          "hidden": false
        },
        {
          "_id": "6837dd74ec10479b9605da19",
          "name": "Jiehang Xie",
          "hidden": false
        },
        {
          "_id": "6837dd74ec10479b9605da1a",
          "name": "Joost van de Weijer",
          "hidden": false
        },
        {
          "_id": "6837dd74ec10479b9605da1b",
          "name": "Fahad Shahbaz Khan",
          "hidden": false
        },
        {
          "_id": "6837dd74ec10479b9605da1c",
          "name": "Shiqi Yang",
          "hidden": false
        },
        {
          "_id": "6837dd74ec10479b9605da1d",
          "name": "Yaxing Wang",
          "hidden": false
        },
        {
          "_id": "6837dd74ec10479b9605da1e",
          "name": "Jian Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T04:23:22.000Z",
      "submittedOnDailyAt": "2025-05-29T02:44:09.323Z",
      "title": "One-Way Ticket:Time-Independent Unified Encoder for Distilling\n  Text-to-Image Diffusion Models",
      "submittedOnDailyBy": {
        "_id": "637e1cf4f09bf2498c543a73",
        "avatarUrl": "/avatars/3c0e4e15602a384839bee0c23029f58a.svg",
        "isPro": false,
        "fullname": "Senmao Li",
        "user": "senmaonk",
        "type": "user"
      },
      "summary": "Text-to-Image (T2I) diffusion models have made remarkable advancements in\ngenerative modeling; however, they face a trade-off between inference speed and\nimage quality, posing challenges for efficient deployment. Existing distilled\nT2I models can generate high-fidelity images with fewer sampling steps, but\noften struggle with diversity and quality, especially in one-step models. From\nour analysis, we observe redundant computations in the UNet encoders. Our\nfindings suggest that, for T2I diffusion models, decoders are more adept at\ncapturing richer and more explicit semantic information, while encoders can be\neffectively shared across decoders from diverse time steps. Based on these\nobservations, we introduce the first Time-independent Unified Encoder TiUE for\nthe student model UNet architecture, which is a loop-free image generation\napproach for distilling T2I diffusion models. Using a one-pass scheme, TiUE\nshares encoder features across multiple decoder time steps, enabling parallel\nsampling and significantly reducing inference time complexity. In addition, we\nincorporate a KL divergence term to regularize noise prediction, which enhances\nthe perceptual realism and diversity of the generated images. Experimental\nresults demonstrate that TiUE outperforms state-of-the-art methods, including\nLCM, SD-Turbo, and SwiftBrushv2, producing more diverse and realistic results\nwhile maintaining the computational efficiency.",
      "upvotes": 1,
      "discussionId": "6837dd78ec10479b9605db06",
      "githubRepo": "https://github.com/sen-mao/Loopfree",
      "ai_summary": "Time-independent Unified Encoder TiUE reduces inference time and improves diversity and quality in Text-to-Image diffusion models by sharing encoder features across decoder time steps.",
      "ai_keywords": [
        "Text-to-Image diffusion models",
        "inference speed",
        "image quality",
        "distilled T2I models",
        "UNet encoders",
        "decoders",
        "semantic information",
        "Time-independent Unified Encoder TiUE",
        "loop-free image generation",
        "one-pass scheme",
        "parallel sampling",
        "KL divergence",
        "perceptual realism",
        "LCM",
        "SD-Turbo",
        "SwiftBrushv2"
      ]
    },
    "publishedAt": "2025-05-28T00:23:22.000Z",
    "title": "One-Way Ticket:Time-Independent Unified Encoder for Distilling\n  Text-to-Image Diffusion Models",
    "summary": "Text-to-Image (T2I) diffusion models have made remarkable advancements in\ngenerative modeling; however, they face a trade-off between inference speed and\nimage quality, posing challenges for efficient deployment. Existing distilled\nT2I models can generate high-fidelity images with fewer sampling steps, but\noften struggle with diversity and quality, especially in one-step models. From\nour analysis, we observe redundant computations in the UNet encoders. Our\nfindings suggest that, for T2I diffusion models, decoders are more adept at\ncapturing richer and more explicit semantic information, while encoders can be\neffectively shared across decoders from diverse time steps. Based on these\nobservations, we introduce the first Time-independent Unified Encoder TiUE for\nthe student model UNet architecture, which is a loop-free image generation\napproach for distilling T2I diffusion models. Using a one-pass scheme, TiUE\nshares encoder features across multiple decoder time steps, enabling parallel\nsampling and significantly reducing inference time complexity. In addition, we\nincorporate a KL divergence term to regularize noise prediction, which enhances\nthe perceptual realism and diversity of the generated images. Experimental\nresults demonstrate that TiUE outperforms state-of-the-art methods, including\nLCM, SD-Turbo, and SwiftBrushv2, producing more diverse and realistic results\nwhile maintaining the computational efficiency.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21960.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "637e1cf4f09bf2498c543a73",
      "avatarUrl": "/avatars/3c0e4e15602a384839bee0c23029f58a.svg",
      "fullname": "Senmao Li",
      "name": "senmaonk",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.20715",
      "authors": [
        {
          "_id": "6837eef0e237f02cd3c87963",
          "name": "Fuwen Luo",
          "hidden": false
        },
        {
          "_id": "6837eef0e237f02cd3c87964",
          "name": "Shengfeng Lou",
          "hidden": false
        },
        {
          "_id": "6837eef0e237f02cd3c87965",
          "name": "Chi Chen",
          "hidden": false
        },
        {
          "_id": "6837eef0e237f02cd3c87966",
          "name": "Ziyue Wang",
          "hidden": false
        },
        {
          "_id": "6837eef0e237f02cd3c87967",
          "name": "Chenliang Li",
          "hidden": false
        },
        {
          "_id": "6837eef0e237f02cd3c87968",
          "name": "Weizhou Shen",
          "hidden": false
        },
        {
          "_id": "6837eef0e237f02cd3c87969",
          "name": "Jiyue Guo",
          "hidden": false
        },
        {
          "_id": "6837eef0e237f02cd3c8796a",
          "name": "Peng Li",
          "hidden": false
        },
        {
          "_id": "6837eef0e237f02cd3c8796b",
          "name": "Ming Yan",
          "hidden": false
        },
        {
          "_id": "6837eef0e237f02cd3c8796c",
          "name": "Ji Zhang",
          "hidden": false
        },
        {
          "_id": "6837eef0e237f02cd3c8796d",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "6837eef0e237f02cd3c8796e",
          "name": "Yang Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-27T04:50:07.000Z",
      "submittedOnDailyAt": "2025-05-29T03:57:26.118Z",
      "title": "MUSEG: Reinforcing Video Temporal Understanding via Timestamp-Aware\n  Multi-Segment Grounding",
      "submittedOnDailyBy": {
        "_id": "642086ed290342c5df85662d",
        "avatarUrl": "/avatars/915a4d7b89455ae97b8544c79286ddf8.svg",
        "isPro": false,
        "fullname": "Chi Chen",
        "user": "carboncoo",
        "type": "user"
      },
      "summary": "Video temporal understanding is crucial for multimodal large language models\n(MLLMs) to reason over events in videos. Despite recent advances in general\nvideo understanding, current MLLMs still struggle with fine-grained temporal\nreasoning. While reinforcement learning (RL) has been explored to address this\nissue recently, existing RL approaches remain limited in effectiveness. In this\nwork, we propose MUSEG, a novel RL-based method that enhances temporal\nunderstanding by introducing timestamp-aware multi-segment grounding. MUSEG\nenables MLLMs to align queries with multiple relevant video segments, promoting\nmore comprehensive temporal reasoning. To facilitate effective learning, we\ndesign a customized RL training recipe with phased rewards that progressively\nguides the model toward temporally grounded reasoning. Extensive experiments on\ntemporal grounding and time-sensitive video QA tasks demonstrate that MUSEG\nsignificantly outperforms existing methods and generalizes well across diverse\ntemporal understanding scenarios. View our project at\nhttps://github.com/THUNLP-MT/MUSEG.",
      "upvotes": 1,
      "discussionId": "6837eef1e237f02cd3c8798d",
      "githubRepo": "https://github.com/THUNLP-MT/MUSEG",
      "ai_summary": "MUSEG, an RL-based method with timestamp-aware multi-segment grounding, significantly enhances the temporal understanding of large language models by improving alignment with video segments and demonstrating superior performance in temporal reasoning tasks.",
      "ai_keywords": [
        "reinforcement learning",
        "timestamp-aware multi-segment grounding",
        "temporal understanding",
        "MUSEG",
        "phased rewards",
        "temporal grounding",
        "time-sensitive video QA"
      ]
    },
    "publishedAt": "2025-05-27T00:50:07.000Z",
    "title": "MUSEG: Reinforcing Video Temporal Understanding via Timestamp-Aware\n  Multi-Segment Grounding",
    "summary": "Video temporal understanding is crucial for multimodal large language models\n(MLLMs) to reason over events in videos. Despite recent advances in general\nvideo understanding, current MLLMs still struggle with fine-grained temporal\nreasoning. While reinforcement learning (RL) has been explored to address this\nissue recently, existing RL approaches remain limited in effectiveness. In this\nwork, we propose MUSEG, a novel RL-based method that enhances temporal\nunderstanding by introducing timestamp-aware multi-segment grounding. MUSEG\nenables MLLMs to align queries with multiple relevant video segments, promoting\nmore comprehensive temporal reasoning. To facilitate effective learning, we\ndesign a customized RL training recipe with phased rewards that progressively\nguides the model toward temporally grounded reasoning. Extensive experiments on\ntemporal grounding and time-sensitive video QA tasks demonstrate that MUSEG\nsignificantly outperforms existing methods and generalizes well across diverse\ntemporal understanding scenarios. View our project at\nhttps://github.com/THUNLP-MT/MUSEG.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20715.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642086ed290342c5df85662d",
      "avatarUrl": "/avatars/915a4d7b89455ae97b8544c79286ddf8.svg",
      "fullname": "Chi Chen",
      "name": "carboncoo",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15813",
      "authors": [
        {
          "_id": "68380974717461677df17514",
          "name": "Muquan Yu",
          "hidden": false
        },
        {
          "_id": "68380974717461677df17515",
          "name": "Mu Nan",
          "hidden": false
        },
        {
          "_id": "68380974717461677df17516",
          "name": "Hossein Adeli",
          "hidden": false
        },
        {
          "_id": "68380974717461677df17517",
          "name": "Jacob S. Prince",
          "hidden": false
        },
        {
          "_id": "68380974717461677df17518",
          "name": "John A. Pyles",
          "hidden": false
        },
        {
          "_id": "68380974717461677df17519",
          "name": "Leila Wehbe",
          "hidden": false
        },
        {
          "_id": "68380974717461677df1751a",
          "name": "Margaret M. Henderson",
          "hidden": false
        },
        {
          "_id": "68380974717461677df1751b",
          "name": "Michael J. Tarr",
          "hidden": false
        },
        {
          "_id": "68380974717461677df1751c",
          "user": {
            "_id": "64b6ce23dbbd1f2cdb624d56",
            "avatarUrl": "/avatars/022738ce8f76fa9545b3e363d7264b53.svg",
            "isPro": false,
            "fullname": "Andrew Luo",
            "user": "aluo-x",
            "type": "user"
          },
          "name": "Andrew F. Luo",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-29T07:16:44.495Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64b6ce23dbbd1f2cdb624d56/vCXz4Xv4AgHNNIC1S8aYf.png"
      ],
      "publishedAt": "2025-05-21T17:59:41.000Z",
      "submittedOnDailyAt": "2025-05-29T05:45:43.922Z",
      "title": "Meta-Learning an In-Context Transformer Model of Human Higher Visual\n  Cortex",
      "submittedOnDailyBy": {
        "_id": "64b6ce23dbbd1f2cdb624d56",
        "avatarUrl": "/avatars/022738ce8f76fa9545b3e363d7264b53.svg",
        "isPro": false,
        "fullname": "Andrew Luo",
        "user": "aluo-x",
        "type": "user"
      },
      "summary": "Understanding functional representations within higher visual cortex is a\nfundamental question in computational neuroscience. While artificial neural\nnetworks pretrained on large-scale datasets exhibit striking representational\nalignment with human neural responses, learning image-computable models of\nvisual cortex relies on individual-level, large-scale fMRI datasets. The\nnecessity for expensive, time-intensive, and often impractical data acquisition\nlimits the generalizability of encoders to new subjects and stimuli. BraInCoRL\nuses in-context learning to predict voxelwise neural responses from few-shot\nexamples without any additional finetuning for novel subjects and stimuli. We\nleverage a transformer architecture that can flexibly condition on a variable\nnumber of in-context image stimuli, learning an inductive bias over multiple\nsubjects. During training, we explicitly optimize the model for in-context\nlearning. By jointly conditioning on image features and voxel activations, our\nmodel learns to directly generate better performing voxelwise models of higher\nvisual cortex. We demonstrate that BraInCoRL consistently outperforms existing\nvoxelwise encoder designs in a low-data regime when evaluated on entirely novel\nimages, while also exhibiting strong test-time scaling behavior. The model also\ngeneralizes to an entirely new visual fMRI dataset, which uses different\nsubjects and fMRI data acquisition parameters. Further, BraInCoRL facilitates\nbetter interpretability of neural signals in higher visual cortex by attending\nto semantically relevant stimuli. Finally, we show that our framework enables\ninterpretable mappings from natural language queries to voxel selectivity.",
      "upvotes": 1,
      "discussionId": "68380977717461677df17638",
      "ai_summary": "BraInCoRL employs a transformer-based in-context learning approach to model higher visual cortex neural responses with few-shot examples, demonstrating superior performance and generalizability across new subjects, stimuli, and datasets.",
      "ai_keywords": [
        "functional representations",
        "higher visual cortex",
        "artificial neural networks",
        "fMRI datasets",
        "in-context learning",
        "transformer architecture",
        "inductive bias",
        "voxelwise neural responses",
        "image features",
        "voxel activations",
        "test-time scaling",
        "interpretability",
        "natural language queries",
        "voxel selectivity"
      ]
    },
    "publishedAt": "2025-05-21T13:59:41.000Z",
    "title": "Meta-Learning an In-Context Transformer Model of Human Higher Visual\n  Cortex",
    "summary": "Understanding functional representations within higher visual cortex is a\nfundamental question in computational neuroscience. While artificial neural\nnetworks pretrained on large-scale datasets exhibit striking representational\nalignment with human neural responses, learning image-computable models of\nvisual cortex relies on individual-level, large-scale fMRI datasets. The\nnecessity for expensive, time-intensive, and often impractical data acquisition\nlimits the generalizability of encoders to new subjects and stimuli. BraInCoRL\nuses in-context learning to predict voxelwise neural responses from few-shot\nexamples without any additional finetuning for novel subjects and stimuli. We\nleverage a transformer architecture that can flexibly condition on a variable\nnumber of in-context image stimuli, learning an inductive bias over multiple\nsubjects. During training, we explicitly optimize the model for in-context\nlearning. By jointly conditioning on image features and voxel activations, our\nmodel learns to directly generate better performing voxelwise models of higher\nvisual cortex. We demonstrate that BraInCoRL consistently outperforms existing\nvoxelwise encoder designs in a low-data regime when evaluated on entirely novel\nimages, while also exhibiting strong test-time scaling behavior. The model also\ngeneralizes to an entirely new visual fMRI dataset, which uses different\nsubjects and fMRI data acquisition parameters. Further, BraInCoRL facilitates\nbetter interpretability of neural signals in higher visual cortex by attending\nto semantically relevant stimuli. Finally, we show that our framework enables\ninterpretable mappings from natural language queries to voxel selectivity.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64b6ce23dbbd1f2cdb624d56/vCXz4Xv4AgHNNIC1S8aYf.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15813.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b6ce23dbbd1f2cdb624d56",
      "avatarUrl": "/avatars/022738ce8f76fa9545b3e363d7264b53.svg",
      "fullname": "Andrew Luo",
      "name": "aluo-x",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.21582",
      "authors": [
        {
          "_id": "68380c860fb1ddbe91ba0bf9",
          "user": {
            "_id": "631f5035c6b20f03c823c4ba",
            "avatarUrl": "/avatars/5557b07bc7bd76f5752b5cf3d8e8f22f.svg",
            "isPro": false,
            "fullname": "Christopher Knievel",
            "user": "CKnievel",
            "type": "user"
          },
          "name": "Christopher Knievel",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:41:12.986Z",
          "hidden": false
        },
        {
          "_id": "68380c860fb1ddbe91ba0bfa",
          "name": "Alexander Bernhardt",
          "hidden": false
        },
        {
          "_id": "68380c860fb1ddbe91ba0bfb",
          "name": "Christian Bernhardt",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-27T10:07:05.000Z",
      "submittedOnDailyAt": "2025-05-29T06:18:15.842Z",
      "title": "AITEE -- Agentic Tutor for Electrical Engineering",
      "submittedOnDailyBy": {
        "_id": "631f5035c6b20f03c823c4ba",
        "avatarUrl": "/avatars/5557b07bc7bd76f5752b5cf3d8e8f22f.svg",
        "isPro": false,
        "fullname": "Christopher Knievel",
        "user": "CKnievel",
        "type": "user"
      },
      "summary": "Intelligent tutoring systems combined with large language models offer a\npromising approach to address students' diverse needs and promote\nself-efficacious learning. While large language models possess good\nfoundational knowledge of electrical engineering basics, they remain\ninsufficiently capable of addressing specific questions about electrical\ncircuits. In this paper, we present AITEE, an agent-based tutoring system for\nelectrical engineering designed to accompany students throughout their learning\nprocess, offer individualized support, and promote self-directed learning.\nAITEE supports both hand-drawn and digital circuits through an adapted circuit\nreconstruction process, enabling natural interaction with students. Our novel\ngraph-based similarity measure identifies relevant context from lecture\nmaterials through a retrieval augmented generation approach, while parallel\nSpice simulation further enhances accuracy in applying solution methodologies.\nThe system implements a Socratic dialogue to foster learner autonomy through\nguided questioning. Experimental evaluations demonstrate that AITEE\nsignificantly outperforms baseline approaches in domain-specific knowledge\napplication, with even medium-sized LLM models showing acceptable performance.\nOur results highlight the potential of agentic tutors to deliver scalable,\npersonalized, and effective learning environments for electrical engineering\neducation.",
      "upvotes": 0,
      "discussionId": "68380c870fb1ddbe91ba0c55",
      "ai_summary": "An agent-based tutoring system for electrical engineering enhances learning through natural circuit interaction, context-retrieving generation, and guided questioning, demonstrating superior performance compared to baseline methods.",
      "ai_keywords": [
        "agent-based tutoring system",
        "large language models",
        "electrical circuits",
        "adapted circuit reconstruction",
        "graph-based similarity measure",
        "retrieval augmented generation",
        "parallel Spice simulation",
        "Socratic dialogue"
      ]
    },
    "publishedAt": "2025-05-27T06:07:05.000Z",
    "title": "AITEE -- Agentic Tutor for Electrical Engineering",
    "summary": "Intelligent tutoring systems combined with large language models offer a\npromising approach to address students' diverse needs and promote\nself-efficacious learning. While large language models possess good\nfoundational knowledge of electrical engineering basics, they remain\ninsufficiently capable of addressing specific questions about electrical\ncircuits. In this paper, we present AITEE, an agent-based tutoring system for\nelectrical engineering designed to accompany students throughout their learning\nprocess, offer individualized support, and promote self-directed learning.\nAITEE supports both hand-drawn and digital circuits through an adapted circuit\nreconstruction process, enabling natural interaction with students. Our novel\ngraph-based similarity measure identifies relevant context from lecture\nmaterials through a retrieval augmented generation approach, while parallel\nSpice simulation further enhances accuracy in applying solution methodologies.\nThe system implements a Socratic dialogue to foster learner autonomy through\nguided questioning. Experimental evaluations demonstrate that AITEE\nsignificantly outperforms baseline approaches in domain-specific knowledge\napplication, with even medium-sized LLM models showing acceptable performance.\nOur results highlight the potential of agentic tutors to deliver scalable,\npersonalized, and effective learning environments for electrical engineering\neducation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21582.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "631f5035c6b20f03c823c4ba",
      "avatarUrl": "/avatars/5557b07bc7bd76f5752b5cf3d8e8f22f.svg",
      "fullname": "Christopher Knievel",
      "name": "CKnievel",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.18149",
      "authors": [
        {
          "_id": "6837e51d05c81fd7d7d1962e",
          "user": {
            "_id": "63ca499104c97982831127ec",
            "avatarUrl": "/avatars/816a962c62c805adf7789fa8e398b01e.svg",
            "isPro": false,
            "fullname": "Aradhye Agarwal",
            "user": "aradhye",
            "type": "user"
          },
          "name": "Aradhye Agarwal",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:41:47.337Z",
          "hidden": false
        },
        {
          "_id": "6837e51d05c81fd7d7d1962f",
          "name": "Ayan Sengupta",
          "hidden": false
        },
        {
          "_id": "6837e51d05c81fd7d7d19630",
          "name": "Tanmoy Chakraborty",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T17:57:43.000Z",
      "submittedOnDailyAt": "2025-05-29T06:15:54.434Z",
      "title": "First Finish Search: Efficient Test-Time Scaling in Large Language\n  Models",
      "submittedOnDailyBy": {
        "_id": "63ca499104c97982831127ec",
        "avatarUrl": "/avatars/816a962c62c805adf7789fa8e398b01e.svg",
        "isPro": false,
        "fullname": "Aradhye Agarwal",
        "user": "aradhye",
        "type": "user"
      },
      "summary": "Test-time scaling (TTS), which involves dynamic allocation of compute during\ninference, offers a promising way to improve reasoning in large language\nmodels. While existing TTS methods work well, they often rely on long decoding\npaths or require a large number of samples to be generated, increasing the\ntoken usage and inference latency. We observe the surprising fact that for\nreasoning tasks, shorter traces are much more likely to be correct than longer\nones. Motivated by this, we introduce First Finish Search (FFS), a\ntraining-free parallel decoding strategy that launches n independent samples\nand returns as soon as any one completes. We evaluate FFS alongside simple\ndecoding, beam search, majority voting, and budget forcing on four reasoning\nmodels (DeepSeek-R1, R1-Distill-Qwen-32B, QwQ-32B and Phi-4-Reasoning-Plus) and\nacross four datasets (AIME24, AIME25-I, AIME25-II and GPQA Diamond). With\nDeepSeek-R1, FFS achieves 82.23% accuracy on the AIME datasets, a 15%\nimprovement over DeepSeek-R1's standalone accuracy, nearly matching OpenAI's\no4-mini performance. Our theoretical analysis explains why stopping at the\nshortest trace is likely to yield a correct answer and identifies the\nconditions under which early stopping may be suboptimal. The elegance and\nsimplicity of FFS demonstrate that straightforward TTS strategies can perform\nremarkably well, revealing the untapped potential of simple approaches at\ninference time.",
      "upvotes": 0,
      "discussionId": "6837e51d05c81fd7d7d19659",
      "ai_summary": "First Finish Search improves accuracy in large language models by stopping inference at the first completed sample, significantly outperforming other decoding strategies in reasoning tasks.",
      "ai_keywords": [
        "Test-time scaling",
        "TTS",
        "dynamic allocation",
        "inference",
        "reasoning tasks",
        "First Finish Search",
        "FFS",
        "parallel decoding",
        "decoding strategies",
        "beam search",
        "majority voting",
        "budget forcing",
        "accuracy",
        "performance",
        "inference latency",
        "early stopping"
      ]
    },
    "publishedAt": "2025-05-23T13:57:43.000Z",
    "title": "First Finish Search: Efficient Test-Time Scaling in Large Language\n  Models",
    "summary": "Test-time scaling (TTS), which involves dynamic allocation of compute during\ninference, offers a promising way to improve reasoning in large language\nmodels. While existing TTS methods work well, they often rely on long decoding\npaths or require a large number of samples to be generated, increasing the\ntoken usage and inference latency. We observe the surprising fact that for\nreasoning tasks, shorter traces are much more likely to be correct than longer\nones. Motivated by this, we introduce First Finish Search (FFS), a\ntraining-free parallel decoding strategy that launches n independent samples\nand returns as soon as any one completes. We evaluate FFS alongside simple\ndecoding, beam search, majority voting, and budget forcing on four reasoning\nmodels (DeepSeek-R1, R1-Distill-Qwen-32B, QwQ-32B and Phi-4-Reasoning-Plus) and\nacross four datasets (AIME24, AIME25-I, AIME25-II and GPQA Diamond). With\nDeepSeek-R1, FFS achieves 82.23% accuracy on the AIME datasets, a 15%\nimprovement over DeepSeek-R1's standalone accuracy, nearly matching OpenAI's\no4-mini performance. Our theoretical analysis explains why stopping at the\nshortest trace is likely to yield a correct answer and identifies the\nconditions under which early stopping may be suboptimal. The elegance and\nsimplicity of FFS demonstrate that straightforward TTS strategies can perform\nremarkably well, revealing the untapped potential of simple approaches at\ninference time.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18149.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63ca499104c97982831127ec",
      "avatarUrl": "/avatars/816a962c62c805adf7789fa8e398b01e.svg",
      "fullname": "Aradhye Agarwal",
      "name": "aradhye",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  }
]