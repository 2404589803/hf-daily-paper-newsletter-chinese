[
  {
    "paper": {
      "id": "2509.04664",
      "authors": [
        {
          "_id": "68be5810c123124955ef60ac",
          "name": "Adam Tauman Kalai",
          "hidden": false
        },
        {
          "_id": "68be5810c123124955ef60ad",
          "name": "Ofir Nachum",
          "hidden": false
        },
        {
          "_id": "68be5810c123124955ef60ae",
          "name": "Santosh S. Vempala",
          "hidden": false
        },
        {
          "_id": "68be5810c123124955ef60af",
          "name": "Edwin Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-04T21:26:31.000Z",
      "submittedOnDailyAt": "2025-09-08T02:44:20.973Z",
      "title": "Why Language Models Hallucinate",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Like students facing hard exam questions, large language models sometimes\nguess when uncertain, producing plausible yet incorrect statements instead of\nadmitting uncertainty. Such \"hallucinations\" persist even in state-of-the-art\nsystems and undermine trust. We argue that language models hallucinate because\nthe training and evaluation procedures reward guessing over acknowledging\nuncertainty, and we analyze the statistical causes of hallucinations in the\nmodern training pipeline. Hallucinations need not be mysterious -- they\noriginate simply as errors in binary classification. If incorrect statements\ncannot be distinguished from facts, then hallucinations in pretrained language\nmodels will arise through natural statistical pressures. We then argue that\nhallucinations persist due to the way most evaluations are graded -- language\nmodels are optimized to be good test-takers, and guessing when uncertain\nimproves test performance. This \"epidemic\" of penalizing uncertain responses\ncan only be addressed through a socio-technical mitigation: modifying the\nscoring of existing benchmarks that are misaligned but dominate leaderboards,\nrather than introducing additional hallucination evaluations. This change may\nsteer the field toward more trustworthy AI systems.",
      "upvotes": 27,
      "discussionId": "68be5810c123124955ef60b0",
      "ai_summary": "Language models produce incorrect statements due to training and evaluation procedures that reward guessing over acknowledging uncertainty, leading to a need for socio-technical changes in benchmark scoring.",
      "ai_keywords": [
        "hallucinations",
        "binary classification",
        "uncertain responses",
        "trustworthy AI systems"
      ]
    },
    "publishedAt": "2025-09-04T17:26:31.000Z",
    "title": "Why Language Models Hallucinate",
    "summary": "Like students facing hard exam questions, large language models sometimes\nguess when uncertain, producing plausible yet incorrect statements instead of\nadmitting uncertainty. Such \"hallucinations\" persist even in state-of-the-art\nsystems and undermine trust. We argue that language models hallucinate because\nthe training and evaluation procedures reward guessing over acknowledging\nuncertainty, and we analyze the statistical causes of hallucinations in the\nmodern training pipeline. Hallucinations need not be mysterious -- they\noriginate simply as errors in binary classification. If incorrect statements\ncannot be distinguished from facts, then hallucinations in pretrained language\nmodels will arise through natural statistical pressures. We then argue that\nhallucinations persist due to the way most evaluations are graded -- language\nmodels are optimized to be good test-takers, and guessing when uncertain\nimproves test performance. This \"epidemic\" of penalizing uncertain responses\ncan only be addressed through a socio-technical mitigation: modifying the\nscoring of existing benchmarks that are misaligned but dominate leaderboards,\nrather than introducing additional hallucination evaluations. This change may\nsteer the field toward more trustworthy AI systems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.04664.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 98
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.04744",
      "authors": [
        {
          "_id": "68be3e13c123124955ef5f99",
          "name": "Gagan Mundada",
          "hidden": false
        },
        {
          "_id": "68be3e13c123124955ef5f9a",
          "name": "Yash Vishe",
          "hidden": false
        },
        {
          "_id": "68be3e13c123124955ef5f9b",
          "name": "Amit Namburi",
          "hidden": false
        },
        {
          "_id": "68be3e13c123124955ef5f9c",
          "user": {
            "_id": "6190ab805ca89a28e9f66873",
            "avatarUrl": "/avatars/3c7ecc398fbf851acd2a132e947a92be.svg",
            "isPro": false,
            "fullname": "Xin Xu",
            "user": "XinXuNLPer",
            "type": "user"
          },
          "name": "Xin Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-08T07:01:58.002Z",
          "hidden": false
        },
        {
          "_id": "68be3e13c123124955ef5f9d",
          "user": {
            "_id": "643060c6cb3fe707b24c53a2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643060c6cb3fe707b24c53a2/MIoM9hrX0vV4XRyrm-4Kz.jpeg",
            "isPro": false,
            "fullname": "Zachary Novack",
            "user": "ZacharyNovack",
            "type": "user"
          },
          "name": "Zachary Novack",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-08T07:02:00.032Z",
          "hidden": false
        },
        {
          "_id": "68be3e13c123124955ef5f9e",
          "name": "Julian McAuley",
          "hidden": false
        },
        {
          "_id": "68be3e13c123124955ef5f9f",
          "name": "Junda Wu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/643060c6cb3fe707b24c53a2/i7xvFhvJz98DfSA92HH8z.png",
        "https://cdn-uploads.huggingface.co/production/uploads/643060c6cb3fe707b24c53a2/jb9rmpc4sKJdwj48aO16M.png"
      ],
      "publishedAt": "2025-09-05T01:54:50.000Z",
      "submittedOnDailyAt": "2025-09-08T00:59:18.510Z",
      "title": "WildScore: Benchmarking MLLMs in-the-Wild Symbolic Music Reasoning",
      "submittedOnDailyBy": {
        "_id": "643060c6cb3fe707b24c53a2",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643060c6cb3fe707b24c53a2/MIoM9hrX0vV4XRyrm-4Kz.jpeg",
        "isPro": false,
        "fullname": "Zachary Novack",
        "user": "ZacharyNovack",
        "type": "user"
      },
      "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have demonstrated\nimpressive capabilities across various vision-language tasks. However, their\nreasoning abilities in the multimodal symbolic music domain remain largely\nunexplored. We introduce WildScore, the first in-the-wild multimodal symbolic\nmusic reasoning and analysis benchmark, designed to evaluate MLLMs' capacity to\ninterpret real-world music scores and answer complex musicological queries.\nEach instance in WildScore is sourced from genuine musical compositions and\naccompanied by authentic user-generated questions and discussions, capturing\nthe intricacies of practical music analysis. To facilitate systematic\nevaluation, we propose a systematic taxonomy, comprising both high-level and\nfine-grained musicological ontologies. Furthermore, we frame complex music\nreasoning as multiple-choice question answering, enabling controlled and\nscalable assessment of MLLMs' symbolic music understanding. Empirical\nbenchmarking of state-of-the-art MLLMs on WildScore reveals intriguing patterns\nin their visual-symbolic reasoning, uncovering both promising directions and\npersistent challenges for MLLMs in symbolic music reasoning and analysis. We\nrelease the dataset and code.",
      "upvotes": 6,
      "discussionId": "68be3e13c123124955ef5fa0",
      "ai_summary": "WildScore evaluates MLLMs' symbolic music reasoning through a benchmark of real-world music scores and user-generated queries, revealing both strengths and challenges.",
      "ai_keywords": [
        "Multimodal Large Language Models",
        "MLLMs",
        "multimodal symbolic music reasoning",
        "WildScore",
        "musicological queries",
        "musicological ontologies",
        "multiple-choice question answering",
        "visual-symbolic reasoning"
      ]
    },
    "publishedAt": "2025-09-04T21:54:50.000Z",
    "title": "WildScore: Benchmarking MLLMs in-the-Wild Symbolic Music Reasoning",
    "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have demonstrated\nimpressive capabilities across various vision-language tasks. However, their\nreasoning abilities in the multimodal symbolic music domain remain largely\nunexplored. We introduce WildScore, the first in-the-wild multimodal symbolic\nmusic reasoning and analysis benchmark, designed to evaluate MLLMs' capacity to\ninterpret real-world music scores and answer complex musicological queries.\nEach instance in WildScore is sourced from genuine musical compositions and\naccompanied by authentic user-generated questions and discussions, capturing\nthe intricacies of practical music analysis. To facilitate systematic\nevaluation, we propose a systematic taxonomy, comprising both high-level and\nfine-grained musicological ontologies. Furthermore, we frame complex music\nreasoning as multiple-choice question answering, enabling controlled and\nscalable assessment of MLLMs' symbolic music understanding. Empirical\nbenchmarking of state-of-the-art MLLMs on WildScore reveals intriguing patterns\nin their visual-symbolic reasoning, uncovering both promising directions and\npersistent challenges for MLLMs in symbolic music reasoning and analysis. We\nrelease the dataset and code.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/643060c6cb3fe707b24c53a2/i7xvFhvJz98DfSA92HH8z.png",
      "https://cdn-uploads.huggingface.co/production/uploads/643060c6cb3fe707b24c53a2/jb9rmpc4sKJdwj48aO16M.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.04744.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643060c6cb3fe707b24c53a2",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643060c6cb3fe707b24c53a2/MIoM9hrX0vV4XRyrm-4Kz.jpeg",
      "fullname": "Zachary Novack",
      "name": "ZacharyNovack",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2509.05263",
      "authors": [
        {
          "_id": "68be4a9dc123124955ef5fd9",
          "name": "Yinglin Duan",
          "hidden": false
        },
        {
          "_id": "68be4a9dc123124955ef5fda",
          "name": "Zhengxia Zou",
          "hidden": false
        },
        {
          "_id": "68be4a9dc123124955ef5fdb",
          "name": "Tongwei Gu",
          "hidden": false
        },
        {
          "_id": "68be4a9dc123124955ef5fdc",
          "name": "Wei Jia",
          "hidden": false
        },
        {
          "_id": "68be4a9dc123124955ef5fdd",
          "name": "Zhan Zhao",
          "hidden": false
        },
        {
          "_id": "68be4a9dc123124955ef5fde",
          "name": "Luyi Xu",
          "hidden": false
        },
        {
          "_id": "68be4a9dc123124955ef5fdf",
          "name": "Xinzhu Liu",
          "hidden": false
        },
        {
          "_id": "68be4a9dc123124955ef5fe0",
          "name": "Hao Jiang",
          "hidden": false
        },
        {
          "_id": "68be4a9dc123124955ef5fe1",
          "name": "Kang Chen",
          "hidden": false
        },
        {
          "_id": "68be4a9dc123124955ef5fe2",
          "name": "Shuang Qiu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-05T17:22:33.000Z",
      "submittedOnDailyAt": "2025-09-08T01:46:57.372Z",
      "title": "LatticeWorld: A Multimodal Large Language Model-Empowered Framework for\n  Interactive Complex World Generation",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Recent research has been increasingly focusing on developing 3D world models\nthat simulate complex real-world scenarios. World models have found broad\napplications across various domains, including embodied AI, autonomous driving,\nentertainment, etc. A more realistic simulation with accurate physics will\neffectively narrow the sim-to-real gap and allow us to gather rich information\nabout the real world conveniently. While traditional manual modeling has\nenabled the creation of virtual 3D scenes, modern approaches have leveraged\nadvanced machine learning algorithms for 3D world generation, with most recent\nadvances focusing on generative methods that can create virtual worlds based on\nuser instructions. This work explores such a research direction by proposing\nLatticeWorld, a simple yet effective 3D world generation framework that\nstreamlines the industrial production pipeline of 3D environments. LatticeWorld\nleverages lightweight LLMs (LLaMA-2-7B) alongside the industry-grade rendering\nengine (e.g., Unreal Engine 5) to generate a dynamic environment. Our proposed\nframework accepts textual descriptions and visual instructions as multimodal\ninputs and creates large-scale 3D interactive worlds with dynamic agents,\nfeaturing competitive multi-agent interaction, high-fidelity physics\nsimulation, and real-time rendering. We conduct comprehensive experiments to\nevaluate LatticeWorld, showing that it achieves superior accuracy in scene\nlayout generation and visual fidelity. Moreover, LatticeWorld achieves over a\n90times increase in industrial production efficiency while maintaining high\ncreative quality compared with traditional manual production methods. Our demo\nvideo is available at https://youtu.be/8VWZXpERR18",
      "upvotes": 2,
      "discussionId": "68be4a9dc123124955ef5fe3",
      "ai_summary": "LatticeWorld, a 3D world generation framework using lightweight LLMs and Unreal Engine 5, creates dynamic, interactive environments from textual and visual inputs, achieving high accuracy and efficiency.",
      "ai_keywords": [
        "LLMs",
        "LLaMA-2-7B",
        "Unreal Engine 5",
        "multimodal inputs",
        "dynamic agents",
        "multi-agent interaction",
        "high-fidelity physics simulation",
        "real-time rendering",
        "scene layout generation",
        "visual fidelity"
      ]
    },
    "publishedAt": "2025-09-05T13:22:33.000Z",
    "title": "LatticeWorld: A Multimodal Large Language Model-Empowered Framework for\n  Interactive Complex World Generation",
    "summary": "Recent research has been increasingly focusing on developing 3D world models\nthat simulate complex real-world scenarios. World models have found broad\napplications across various domains, including embodied AI, autonomous driving,\nentertainment, etc. A more realistic simulation with accurate physics will\neffectively narrow the sim-to-real gap and allow us to gather rich information\nabout the real world conveniently. While traditional manual modeling has\nenabled the creation of virtual 3D scenes, modern approaches have leveraged\nadvanced machine learning algorithms for 3D world generation, with most recent\nadvances focusing on generative methods that can create virtual worlds based on\nuser instructions. This work explores such a research direction by proposing\nLatticeWorld, a simple yet effective 3D world generation framework that\nstreamlines the industrial production pipeline of 3D environments. LatticeWorld\nleverages lightweight LLMs (LLaMA-2-7B) alongside the industry-grade rendering\nengine (e.g., Unreal Engine 5) to generate a dynamic environment. Our proposed\nframework accepts textual descriptions and visual instructions as multimodal\ninputs and creates large-scale 3D interactive worlds with dynamic agents,\nfeaturing competitive multi-agent interaction, high-fidelity physics\nsimulation, and real-time rendering. We conduct comprehensive experiments to\nevaluate LatticeWorld, showing that it achieves superior accuracy in scene\nlayout generation and visual fidelity. Moreover, LatticeWorld achieves over a\n90times increase in industrial production efficiency while maintaining high\ncreative quality compared with traditional manual production methods. Our demo\nvideo is available at https://youtu.be/8VWZXpERR18",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.05263.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 98
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.04013",
      "authors": [
        {
          "_id": "68be6ac4c123124955ef60cb",
          "name": "Riccardo Lunardi",
          "hidden": false
        },
        {
          "_id": "68be6ac4c123124955ef60cc",
          "name": "Vincenzo Della Mea",
          "hidden": false
        },
        {
          "_id": "68be6ac4c123124955ef60cd",
          "name": "Stefano Mizzaro",
          "hidden": false
        },
        {
          "_id": "68be6ac4c123124955ef60ce",
          "name": "Kevin Roitero",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-04T08:43:27.000Z",
      "submittedOnDailyAt": "2025-09-08T04:05:06.987Z",
      "title": "On Robustness and Reliability of Benchmark-Based Evaluation of LLMs",
      "submittedOnDailyBy": {
        "_id": "620cca6f06a4320dbf3b50d8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654011487582-620cca6f06a4320dbf3b50d8.png",
        "isPro": false,
        "fullname": "Kevin Roitero",
        "user": "kevinr",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) effectiveness is usually evaluated by means of\nbenchmarks such as MMLU, ARC-C, or HellaSwag, where questions are presented in\ntheir original wording, thus in a fixed, standardized format. However,\nreal-world applications involve linguistic variability, requiring models to\nmaintain their effectiveness across diverse rewordings of the same question or\nquery. In this study, we systematically assess the robustness of LLMs to\nparaphrased benchmark questions and investigate whether benchmark-based\nevaluations provide a reliable measure of model capabilities. We systematically\ngenerate various paraphrases of all the questions across six different common\nbenchmarks, and measure the resulting variations in effectiveness of 34\nstate-of-the-art LLMs, of different size and effectiveness. Our findings reveal\nthat while LLM rankings remain relatively stable across paraphrased inputs,\nabsolute effectiveness scores change, and decline significantly. This suggests\nthat LLMs struggle with linguistic variability, raising concerns about their\ngeneralization abilities and evaluation methodologies. Furthermore, the\nobserved performance drop challenges the reliability of benchmark-based\nevaluations, indicating that high benchmark scores may not fully capture a\nmodel's robustness to real-world input variations. We discuss the implications\nof these findings for LLM evaluation methodologies, emphasizing the need for\nrobustness-aware benchmarks that better reflect practical deployment scenarios.",
      "upvotes": 2,
      "discussionId": "68be6ac4c123124955ef60cf",
      "ai_summary": "LLMs show reduced effectiveness on paraphrased benchmark questions, indicating limitations in handling linguistic variability and suggesting the need for more robust evaluation methods.",
      "ai_keywords": [
        "Large Language Models",
        "LLMs",
        "MMLU",
        "ARC-C",
        "HellaSwag",
        "paraphrased benchmark questions",
        "linguistic variability",
        "generalization abilities",
        "benchmark-based evaluations",
        "robustness-aware benchmarks"
      ]
    },
    "publishedAt": "2025-09-04T04:43:27.000Z",
    "title": "On Robustness and Reliability of Benchmark-Based Evaluation of LLMs",
    "summary": "Large Language Models (LLMs) effectiveness is usually evaluated by means of\nbenchmarks such as MMLU, ARC-C, or HellaSwag, where questions are presented in\ntheir original wording, thus in a fixed, standardized format. However,\nreal-world applications involve linguistic variability, requiring models to\nmaintain their effectiveness across diverse rewordings of the same question or\nquery. In this study, we systematically assess the robustness of LLMs to\nparaphrased benchmark questions and investigate whether benchmark-based\nevaluations provide a reliable measure of model capabilities. We systematically\ngenerate various paraphrases of all the questions across six different common\nbenchmarks, and measure the resulting variations in effectiveness of 34\nstate-of-the-art LLMs, of different size and effectiveness. Our findings reveal\nthat while LLM rankings remain relatively stable across paraphrased inputs,\nabsolute effectiveness scores change, and decline significantly. This suggests\nthat LLMs struggle with linguistic variability, raising concerns about their\ngeneralization abilities and evaluation methodologies. Furthermore, the\nobserved performance drop challenges the reliability of benchmark-based\nevaluations, indicating that high benchmark scores may not fully capture a\nmodel's robustness to real-world input variations. We discuss the implications\nof these findings for LLM evaluation methodologies, emphasizing the need for\nrobustness-aware benchmarks that better reflect practical deployment scenarios.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.04013.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620cca6f06a4320dbf3b50d8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654011487582-620cca6f06a4320dbf3b50d8.png",
      "fullname": "Kevin Roitero",
      "name": "kevinr",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.05296",
      "authors": [
        {
          "_id": "68be737bc123124955ef60d8",
          "name": "Zizun Li",
          "hidden": false
        },
        {
          "_id": "68be737bc123124955ef60d9",
          "name": "Jianjun Zhou",
          "hidden": false
        },
        {
          "_id": "68be737bc123124955ef60da",
          "name": "Yifan Wang",
          "hidden": false
        },
        {
          "_id": "68be737bc123124955ef60db",
          "user": {
            "_id": "652ce0d4c543a08aa92e010f",
            "avatarUrl": "/avatars/7978304e3fe99b0d4d0712441c6a24f3.svg",
            "isPro": false,
            "fullname": "Haoyu Guo",
            "user": "ghy0324",
            "type": "user"
          },
          "name": "Haoyu Guo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-08T07:01:51.542Z",
          "hidden": false
        },
        {
          "_id": "68be737bc123124955ef60dc",
          "name": "Wenzheng Chang",
          "hidden": false
        },
        {
          "_id": "68be737bc123124955ef60dd",
          "name": "Yang Zhou",
          "hidden": false
        },
        {
          "_id": "68be737bc123124955ef60de",
          "name": "Haoyi Zhu",
          "hidden": false
        },
        {
          "_id": "68be737bc123124955ef60df",
          "name": "Junyi Chen",
          "hidden": false
        },
        {
          "_id": "68be737bc123124955ef60e0",
          "name": "Chunhua Shen",
          "hidden": false
        },
        {
          "_id": "68be737bc123124955ef60e1",
          "name": "Tong He",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/65e7eb86c7a0617cc71d3df4/FbisPQymZ9xE9u1ePHpSq.mp4"
      ],
      "publishedAt": "2025-09-05T17:59:47.000Z",
      "submittedOnDailyAt": "2025-09-08T04:43:48.454Z",
      "title": "WinT3R: Window-Based Streaming Reconstruction with Camera Token Pool",
      "submittedOnDailyBy": {
        "_id": "65e7eb86c7a0617cc71d3df4",
        "avatarUrl": "/avatars/01020b6b5ccb08bf8aa10fd5f8b2701d.svg",
        "isPro": false,
        "fullname": "lizizun",
        "user": "lizizun",
        "type": "user"
      },
      "summary": "We present WinT3R, a feed-forward reconstruction model capable of online\nprediction of precise camera poses and high-quality point maps. Previous\nmethods suffer from a trade-off between reconstruction quality and real-time\nperformance. To address this, we first introduce a sliding window mechanism\nthat ensures sufficient information exchange among frames within the window,\nthereby improving the quality of geometric predictions without large\ncomputation. In addition, we leverage a compact representation of cameras and\nmaintain a global camera token pool, which enhances the reliability of camera\npose estimation without sacrificing efficiency. These designs enable WinT3R to\nachieve state-of-the-art performance in terms of online reconstruction quality,\ncamera pose estimation, and reconstruction speed, as validated by extensive\nexperiments on diverse datasets. Code and model are publicly available at\nhttps://github.com/LiZizun/WinT3R.",
      "upvotes": 1,
      "discussionId": "68be737bc123124955ef60e2",
      "ai_summary": "WinT3R, a feed-forward reconstruction model, achieves high-quality camera pose estimation and real-time performance using a sliding window mechanism and a global camera token pool.",
      "ai_keywords": [
        "feed-forward reconstruction model",
        "sliding window mechanism",
        "geometric predictions",
        "compact representation",
        "global camera token pool",
        "camera pose estimation",
        "real-time performance"
      ]
    },
    "publishedAt": "2025-09-05T13:59:47.000Z",
    "title": "WinT3R: Window-Based Streaming Reconstruction with Camera Token Pool",
    "summary": "We present WinT3R, a feed-forward reconstruction model capable of online\nprediction of precise camera poses and high-quality point maps. Previous\nmethods suffer from a trade-off between reconstruction quality and real-time\nperformance. To address this, we first introduce a sliding window mechanism\nthat ensures sufficient information exchange among frames within the window,\nthereby improving the quality of geometric predictions without large\ncomputation. In addition, we leverage a compact representation of cameras and\nmaintain a global camera token pool, which enhances the reliability of camera\npose estimation without sacrificing efficiency. These designs enable WinT3R to\nachieve state-of-the-art performance in terms of online reconstruction quality,\ncamera pose estimation, and reconstruction speed, as validated by extensive\nexperiments on diverse datasets. Code and model are publicly available at\nhttps://github.com/LiZizun/WinT3R.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/65e7eb86c7a0617cc71d3df4/FbisPQymZ9xE9u1ePHpSq.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.05296.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65e7eb86c7a0617cc71d3df4",
      "avatarUrl": "/avatars/01020b6b5ccb08bf8aa10fd5f8b2701d.svg",
      "fullname": "lizizun",
      "name": "lizizun",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.03800",
      "authors": [
        {
          "_id": "68be5223c123124955ef5fec",
          "name": "Yuheng Li",
          "hidden": false
        },
        {
          "_id": "68be5223c123124955ef5fed",
          "name": "Yenho Chen",
          "hidden": false
        },
        {
          "_id": "68be5223c123124955ef5fee",
          "name": "Yuxiang Lai",
          "hidden": false
        },
        {
          "_id": "68be5223c123124955ef5fef",
          "name": "Jike Zhong",
          "hidden": false
        },
        {
          "_id": "68be5223c123124955ef5ff0",
          "name": "Vanessa Wildman",
          "hidden": false
        },
        {
          "_id": "68be5223c123124955ef5ff1",
          "name": "Xiaofeng Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-04T01:28:44.000Z",
      "submittedOnDailyAt": "2025-09-08T02:19:21.951Z",
      "title": "MedVista3D: Vision-Language Modeling for Reducing Diagnostic Errors in\n  3D CT Disease Detection, Understanding and Reporting",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Radiologic diagnostic errors-under-reading errors, inattentional blindness,\nand communication failures-remain prevalent in clinical practice. These issues\noften stem from missed localized abnormalities, limited global context, and\nvariability in report language. These challenges are amplified in 3D imaging,\nwhere clinicians must examine hundreds of slices per scan. Addressing them\nrequires systems with precise localized detection, global volume-level\nreasoning, and semantically consistent natural language reporting. However,\nexisting 3D vision-language models are unable to meet all three needs jointly,\nlacking local-global understanding for spatial reasoning and struggling with\nthe variability and noise of uncurated radiology reports. We present\nMedVista3D, a multi-scale semantic-enriched vision-language pretraining\nframework for 3D CT analysis. To enable joint disease detection and holistic\ninterpretation, MedVista3D performs local and global image-text alignment for\nfine-grained representation learning within full-volume context. To address\nreport variability, we apply language model rewrites and introduce a Radiology\nSemantic Matching Bank for semantics-aware alignment. MedVista3D achieves\nstate-of-the-art performance on zero-shot disease classification, report\nretrieval, and medical visual question answering, while transferring well to\norgan segmentation and prognosis prediction. Code and datasets will be\nreleased.",
      "upvotes": 1,
      "discussionId": "68be5224c123124955ef5ff2",
      "ai_summary": "MedVista3D is a multi-scale semantic-enriched vision-language pretraining framework for 3D CT analysis that addresses local-global understanding, report variability, and achieves state-of-the-art performance in disease classification, report retrieval, and medical visual question answering.",
      "ai_keywords": [
        "3D vision-language models",
        "local-global understanding",
        "spatial reasoning",
        "language model rewrites",
        "Radiology Semantic Matching Bank",
        "zero-shot disease classification",
        "report retrieval",
        "medical visual question answering",
        "organ segmentation",
        "prognosis prediction"
      ]
    },
    "publishedAt": "2025-09-03T21:28:44.000Z",
    "title": "MedVista3D: Vision-Language Modeling for Reducing Diagnostic Errors in\n  3D CT Disease Detection, Understanding and Reporting",
    "summary": "Radiologic diagnostic errors-under-reading errors, inattentional blindness,\nand communication failures-remain prevalent in clinical practice. These issues\noften stem from missed localized abnormalities, limited global context, and\nvariability in report language. These challenges are amplified in 3D imaging,\nwhere clinicians must examine hundreds of slices per scan. Addressing them\nrequires systems with precise localized detection, global volume-level\nreasoning, and semantically consistent natural language reporting. However,\nexisting 3D vision-language models are unable to meet all three needs jointly,\nlacking local-global understanding for spatial reasoning and struggling with\nthe variability and noise of uncurated radiology reports. We present\nMedVista3D, a multi-scale semantic-enriched vision-language pretraining\nframework for 3D CT analysis. To enable joint disease detection and holistic\ninterpretation, MedVista3D performs local and global image-text alignment for\nfine-grained representation learning within full-volume context. To address\nreport variability, we apply language model rewrites and introduce a Radiology\nSemantic Matching Bank for semantics-aware alignment. MedVista3D achieves\nstate-of-the-art performance on zero-shot disease classification, report\nretrieval, and medical visual question answering, while transferring well to\norgan segmentation and prognosis prediction. Code and datasets will be\nreleased.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.03800.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 98
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.04504",
      "authors": [
        {
          "_id": "68be5b08c123124955ef60bb",
          "name": "Zehua Pei",
          "hidden": false
        },
        {
          "_id": "68be5b08c123124955ef60bc",
          "name": "Hui-Ling Zhen",
          "hidden": false
        },
        {
          "_id": "68be5b08c123124955ef60bd",
          "name": "Ying Zhang",
          "hidden": false
        },
        {
          "_id": "68be5b08c123124955ef60be",
          "name": "Zhiyuan Yang",
          "hidden": false
        },
        {
          "_id": "68be5b08c123124955ef60bf",
          "name": "Xing Li",
          "hidden": false
        },
        {
          "_id": "68be5b08c123124955ef60c0",
          "name": "Xianzhi Yu",
          "hidden": false
        },
        {
          "_id": "68be5b08c123124955ef60c1",
          "name": "Mingxuan Yuan",
          "hidden": false
        },
        {
          "_id": "68be5b08c123124955ef60c2",
          "name": "Bei Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-02T07:03:20.000Z",
      "submittedOnDailyAt": "2025-09-08T02:57:59.995Z",
      "title": "Behavioral Fingerprinting of Large Language Models",
      "submittedOnDailyBy": {
        "_id": "6527c063e86758eb6ca800a1",
        "avatarUrl": "/avatars/9091be87eea518209c1de9eebfa663c0.svg",
        "isPro": false,
        "fullname": "JarvisPei",
        "user": "Eleven-P",
        "type": "user"
      },
      "summary": "Current benchmarks for Large Language Models (LLMs) primarily focus on\nperformance metrics, often failing to capture the nuanced behavioral\ncharacteristics that differentiate them. This paper introduces a novel\n``Behavioral Fingerprinting'' framework designed to move beyond traditional\nevaluation by creating a multi-faceted profile of a model's intrinsic cognitive\nand interactive styles. Using a curated Diagnostic Prompt Suite and an\ninnovative, automated evaluation pipeline where a powerful LLM acts as an\nimpartial judge, we analyze eighteen models across capability tiers. Our\nresults reveal a critical divergence in the LLM landscape: while core\ncapabilities like abstract and causal reasoning are converging among top\nmodels, alignment-related behaviors such as sycophancy and semantic robustness\nvary dramatically. We further document a cross-model default persona clustering\n(ISTJ/ESTJ) that likely reflects common alignment incentives. Taken together,\nthis suggests that a model's interactive nature is not an emergent property of\nits scale or reasoning power, but a direct consequence of specific, and highly\nvariable, developer alignment strategies. Our framework provides a reproducible\nand scalable methodology for uncovering these deep behavioral differences.\nProject: https://github.com/JarvisPei/Behavioral-Fingerprinting",
      "upvotes": 1,
      "discussionId": "68be5b08c123124955ef60c3",
      "projectPage": "https://github.com/JarvisPei/Behavioral-Fingerprinting",
      "githubRepo": "https://github.com/JarvisPei/Behavioral-Fingerprinting",
      "ai_summary": "A Behavioral Fingerprinting framework evaluates Large Language Models using a Diagnostic Prompt Suite and automated pipeline, revealing divergent alignment behaviors and clustering patterns.",
      "ai_keywords": [
        "Behavioral Fingerprinting",
        "Diagnostic Prompt Suite",
        "automated evaluation pipeline",
        "Large Language Models",
        "abstract reasoning",
        "causal reasoning",
        "sycophancy",
        "semantic robustness",
        "default persona clustering",
        "ISTJ/ESTJ",
        "alignment strategies"
      ],
      "githubStars": 0
    },
    "publishedAt": "2025-09-02T03:03:20.000Z",
    "title": "Behavioral Fingerprinting of Large Language Models",
    "summary": "Current benchmarks for Large Language Models (LLMs) primarily focus on\nperformance metrics, often failing to capture the nuanced behavioral\ncharacteristics that differentiate them. This paper introduces a novel\n``Behavioral Fingerprinting'' framework designed to move beyond traditional\nevaluation by creating a multi-faceted profile of a model's intrinsic cognitive\nand interactive styles. Using a curated Diagnostic Prompt Suite and an\ninnovative, automated evaluation pipeline where a powerful LLM acts as an\nimpartial judge, we analyze eighteen models across capability tiers. Our\nresults reveal a critical divergence in the LLM landscape: while core\ncapabilities like abstract and causal reasoning are converging among top\nmodels, alignment-related behaviors such as sycophancy and semantic robustness\nvary dramatically. We further document a cross-model default persona clustering\n(ISTJ/ESTJ) that likely reflects common alignment incentives. Taken together,\nthis suggests that a model's interactive nature is not an emergent property of\nits scale or reasoning power, but a direct consequence of specific, and highly\nvariable, developer alignment strategies. Our framework provides a reproducible\nand scalable methodology for uncovering these deep behavioral differences.\nProject: https://github.com/JarvisPei/Behavioral-Fingerprinting",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.04504.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6527c063e86758eb6ca800a1",
      "avatarUrl": "/avatars/9091be87eea518209c1de9eebfa663c0.svg",
      "fullname": "JarvisPei",
      "name": "Eleven-P",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]