[
  {
    "paper": {
      "id": "2503.04625",
      "authors": [
        {
          "_id": "67ca670d3e81e3344dc4c2d9",
          "name": "Chengpeng Li",
          "hidden": false
        },
        {
          "_id": "67ca670d3e81e3344dc4c2da",
          "name": "Mingfeng Xue",
          "hidden": false
        },
        {
          "_id": "67ca670d3e81e3344dc4c2db",
          "name": "Zhenru Zhang",
          "hidden": false
        },
        {
          "_id": "67ca670d3e81e3344dc4c2dc",
          "name": "Jiaxi Yang",
          "hidden": false
        },
        {
          "_id": "67ca670d3e81e3344dc4c2dd",
          "name": "Beichen Zhang",
          "hidden": false
        },
        {
          "_id": "67ca670d3e81e3344dc4c2de",
          "name": "Xiang Wang",
          "hidden": false
        },
        {
          "_id": "67ca670d3e81e3344dc4c2df",
          "name": "Bowen Yu",
          "hidden": false
        },
        {
          "_id": "67ca670d3e81e3344dc4c2e0",
          "name": "Binyuan Hui",
          "hidden": false
        },
        {
          "_id": "67ca670d3e81e3344dc4c2e1",
          "name": "Junyang Lin",
          "hidden": false
        },
        {
          "_id": "67ca670d3e81e3344dc4c2e2",
          "name": "Dayiheng Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-06T17:11:51.000Z",
      "title": "START: Self-taught Reasoner with Tools",
      "summary": "Large reasoning models (LRMs) like OpenAI-o1 and DeepSeek-R1 have\ndemonstrated remarkable capabilities in complex reasoning tasks through the\nutilization of long Chain-of-thought (CoT). However, these models often suffer\nfrom hallucinations and inefficiencies due to their reliance solely on internal\nreasoning processes. In this paper, we introduce START (Self-Taught Reasoner\nwith Tools), a novel tool-integrated long CoT reasoning LLM that significantly\nenhances reasoning capabilities by leveraging external tools. Through code\nexecution, START is capable of performing complex computations, self-checking,\nexploring diverse methods, and self-debugging, thereby addressing the\nlimitations of LRMs. The core innovation of START lies in its self-learning\nframework, which comprises two key techniques: 1) Hint-infer: We demonstrate\nthat inserting artificially designed hints (e.g., ``Wait, maybe using Python\nhere is a good idea.'') during the inference process of a LRM effectively\nstimulates its ability to utilize external tools without the need for any\ndemonstration data. Hint-infer can also serve as a simple and effective\nsequential test-time scaling method; 2) Hint Rejection Sampling Fine-Tuning\n(Hint-RFT): Hint-RFT combines Hint-infer and RFT by scoring, filtering, and\nmodifying the reasoning trajectories with tool invocation generated by a LRM\nvia Hint-infer, followed by fine-tuning the LRM. Through this framework, we\nhave fine-tuned the QwQ-32B model to achieve START. On PhD-level science QA\n(GPQA), competition-level math benchmarks (AMC23, AIME24, AIME25), and the\ncompetition-level code benchmark (LiveCodeBench), START achieves accuracy rates\nof 63.6%, 95.0%, 66.7%, 47.1%, and 47.3%, respectively. It significantly\noutperforms the base QwQ-32B and achieves performance comparable to the\nstate-of-the-art open-weight model R1-Distill-Qwen-32B and the proprietary\nmodel o1-Preview.",
      "upvotes": 29,
      "discussionId": "67ca67103e81e3344dc4c366"
    },
    "publishedAt": "2025-03-06T22:35:47.725Z",
    "title": "START: Self-taught Reasoner with Tools",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04625.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6299
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.03803",
      "authors": [
        {
          "_id": "67ca874c3ac187dbbed924d6",
          "name": "Jingkang Yang",
          "hidden": false
        },
        {
          "_id": "67ca874c3ac187dbbed924d7",
          "name": "Shuai Liu",
          "hidden": false
        },
        {
          "_id": "67ca874c3ac187dbbed924d8",
          "name": "Hongming Guo",
          "hidden": false
        },
        {
          "_id": "67ca874c3ac187dbbed924d9",
          "name": "Yuhao Dong",
          "hidden": false
        },
        {
          "_id": "67ca874c3ac187dbbed924da",
          "name": "Xiamengwei Zhang",
          "hidden": false
        },
        {
          "_id": "67ca874c3ac187dbbed924db",
          "name": "Sicheng Zhang",
          "hidden": false
        },
        {
          "_id": "67ca874c3ac187dbbed924dc",
          "name": "Pengyun Wang",
          "hidden": false
        },
        {
          "_id": "67ca874c3ac187dbbed924dd",
          "name": "Zitang Zhou",
          "hidden": false
        },
        {
          "_id": "67ca874c3ac187dbbed924de",
          "name": "Binzhu Xie",
          "hidden": false
        },
        {
          "_id": "67ca874c3ac187dbbed924df",
          "name": "Ziyue Wang",
          "hidden": false
        },
        {
          "_id": "67ca874c3ac187dbbed924e0",
          "name": "Bei Ouyang",
          "hidden": false
        },
        {
          "_id": "67ca874c3ac187dbbed924e1",
          "name": "Zhengyu Lin",
          "hidden": false
        },
        {
          "_id": "67ca874c3ac187dbbed924e2",
          "name": "Marco Cominelli",
          "hidden": false
        },
        {
          "_id": "67ca874c3ac187dbbed924e3",
          "name": "Zhongang Cai",
          "hidden": false
        },
        {
          "_id": "67ca874c3ac187dbbed924e4",
          "name": "Yuanhan Zhang",
          "hidden": false
        },
        {
          "_id": "67ca874c3ac187dbbed924e5",
          "name": "Peiyuan Zhang",
          "hidden": false
        },
        {
          "_id": "67ca874c3ac187dbbed924e6",
          "name": "Fangzhou Hong",
          "hidden": false
        },
        {
          "_id": "67ca874c3ac187dbbed924e7",
          "name": "Joerg Widmer",
          "hidden": false
        },
        {
          "_id": "67ca874c3ac187dbbed924e8",
          "name": "Francesco Gringoli",
          "hidden": false
        },
        {
          "_id": "67ca874c3ac187dbbed924e9",
          "name": "Lei Yang",
          "hidden": false
        },
        {
          "_id": "67ca874c3ac187dbbed924ea",
          "name": "Bo Li",
          "hidden": false
        },
        {
          "_id": "67ca874c3ac187dbbed924eb",
          "name": "Ziwei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-05T18:54:16.000Z",
      "title": "EgoLife: Towards Egocentric Life Assistant",
      "summary": "We introduce EgoLife, a project to develop an egocentric life assistant that\naccompanies and enhances personal efficiency through AI-powered wearable\nglasses. To lay the foundation for this assistant, we conducted a comprehensive\ndata collection study where six participants lived together for one week,\ncontinuously recording their daily activities - including discussions,\nshopping, cooking, socializing, and entertainment - using AI glasses for\nmultimodal egocentric video capture, along with synchronized third-person-view\nvideo references. This effort resulted in the EgoLife Dataset, a comprehensive\n300-hour egocentric, interpersonal, multiview, and multimodal daily life\ndataset with intensive annotation. Leveraging this dataset, we introduce\nEgoLifeQA, a suite of long-context, life-oriented question-answering tasks\ndesigned to provide meaningful assistance in daily life by addressing practical\nquestions such as recalling past relevant events, monitoring health habits, and\noffering personalized recommendations. To address the key technical challenges\nof (1) developing robust visual-audio models for egocentric data, (2) enabling\nidentity recognition, and (3) facilitating long-context question answering over\nextensive temporal information, we introduce EgoButler, an integrated system\ncomprising EgoGPT and EgoRAG. EgoGPT is an omni-modal model trained on\negocentric datasets, achieving state-of-the-art performance on egocentric video\nunderstanding. EgoRAG is a retrieval-based component that supports answering\nultra-long-context questions. Our experimental studies verify their working\nmechanisms and reveal critical factors and bottlenecks, guiding future\nimprovements. By releasing our datasets, models, and benchmarks, we aim to\nstimulate further research in egocentric AI assistants.",
      "upvotes": 8,
      "discussionId": "67ca874f3ac187dbbed925cc",
      "projectPage": "https://egolife-ai.github.io/",
      "githubRepo": "https://github.com/EvolvingLMMs-Lab/EgoLife"
    },
    "publishedAt": "2025-03-07T00:44:13.546Z",
    "title": "EgoLife: Towards Egocentric Life Assistant",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.03803.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62b5777f593a2c49da69dc02",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1658152070753-62b5777f593a2c49da69dc02.jpeg",
      "fullname": "Jingkang Yang",
      "name": "Jingkang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.04598",
      "authors": [
        {
          "_id": "67ca69063a6e3e8656bcc1d2",
          "name": "Zhijian Zhuo",
          "hidden": false
        },
        {
          "_id": "67ca69063a6e3e8656bcc1d3",
          "name": "Yutao Zeng",
          "hidden": false
        },
        {
          "_id": "67ca69063a6e3e8656bcc1d4",
          "name": "Ya Wang",
          "hidden": false
        },
        {
          "_id": "67ca69063a6e3e8656bcc1d5",
          "name": "Sijun Zhang",
          "hidden": false
        },
        {
          "_id": "67ca69063a6e3e8656bcc1d6",
          "name": "Jian Yang",
          "hidden": false
        },
        {
          "_id": "67ca69063a6e3e8656bcc1d7",
          "name": "Xiaoqing Li",
          "hidden": false
        },
        {
          "_id": "67ca69063a6e3e8656bcc1d8",
          "name": "Xun Zhou",
          "hidden": false
        },
        {
          "_id": "67ca69063a6e3e8656bcc1d9",
          "name": "Jinwen Ma",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-06T16:40:48.000Z",
      "title": "HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid\n  Normalization",
      "summary": "Transformers have become the de facto architecture for a wide range of\nmachine learning tasks, particularly in large language models (LLMs). Despite\ntheir remarkable performance, challenges remain in training deep transformer\nnetworks, especially regarding the location of layer normalization. While\nPre-Norm structures facilitate easier training due to their more prominent\nidentity path, they often yield suboptimal performance compared to Post-Norm.\nIn this paper, we propose HybridNorm, a straightforward yet\neffective hybrid normalization strategy that integrates the advantages of both\nPre-Norm and Post-Norm approaches. Specifically, HybridNorm employs QKV\nnormalization within the attention mechanism and Post-Norm in the feed-forward\nnetwork (FFN) of each transformer block. This design not only stabilizes\ntraining but also enhances performance, particularly in the context of LLMs.\nComprehensive experiments in both dense and sparse architectures show that\nHybridNorm consistently outperforms both Pre-Norm and Post-Norm approaches,\nachieving state-of-the-art results across various benchmarks. These findings\nhighlight the potential of HybridNorm as a more stable and effective technique\nfor improving the training and performance of deep transformer models. %Code\nwill be made publicly available. Code is available at\nhttps://github.com/BryceZhuo/HybridNorm.",
      "upvotes": 6,
      "discussionId": "67ca69073a6e3e8656bcc244",
      "projectPage": "https://github.com/BryceZhuo/HybridNorm",
      "githubRepo": "https://github.com/BryceZhuo/HybridNorm"
    },
    "publishedAt": "2025-03-06T23:04:06.421Z",
    "title": "HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid Normalization",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6371128eafbe42caa5a5222b/DB_sfuRG7M-k8w6UVTgXy.png",
      "https://cdn-uploads.huggingface.co/production/uploads/6371128eafbe42caa5a5222b/F0lAhIiju8M-0fKBaPATA.png",
      "https://cdn-uploads.huggingface.co/production/uploads/6371128eafbe42caa5a5222b/g_741Ez-YVcMK69EqCsPa.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04598.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "6371128eafbe42caa5a5222b",
      "avatarUrl": "/avatars/c3b2ab35949c38aa3dfb2657a1300aac.svg",
      "fullname": "Yutao Zeng",
      "name": "Taoer",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.04222",
      "authors": [
        {
          "_id": "67ca64cdd153739fa9b9dbe6",
          "name": "Ziyi Yang",
          "hidden": false
        },
        {
          "_id": "67ca64cdd153739fa9b9dbe7",
          "name": "Fanqi Wan",
          "hidden": false
        },
        {
          "_id": "67ca64cdd153739fa9b9dbe8",
          "name": "Longguang Zhong",
          "hidden": false
        },
        {
          "_id": "67ca64cdd153739fa9b9dbe9",
          "name": "Canbin Huang",
          "hidden": false
        },
        {
          "_id": "67ca64cdd153739fa9b9dbea",
          "name": "Guosheng Liang",
          "hidden": false
        },
        {
          "_id": "67ca64cdd153739fa9b9dbeb",
          "name": "Xiaojun Quan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-06T09:03:36.000Z",
      "title": "FuseChat-3.0: Preference Optimization Meets Heterogeneous Model Fusion",
      "summary": "We introduce FuseChat-3.0, a suite of large language models (LLMs) developed\nby integrating the strengths of heterogeneous source LLMs into more compact\ntarget LLMs. Our source models include the powerful Gemma-2-27B-it,\nMistral-Large-Instruct-2407, Qwen-2.5-72B-Instruct, and Llama-3.1-70B-Instruct.\nFor target models, we focus on three widely-used smaller\nvariants-Llama-3.1-8B-Instruct, Gemma-2-9B-it, and Qwen-2.5-7B-Instruct-along\nwith two ultra-compact options, Llama-3.2-3B-Instruct and\nLlama-3.2-1B-Instruct. To leverage the diverse capabilities of these source\nmodels, we develop a specialized data construction protocol tailored to various\ntasks and domains. The FuseChat-3.0 training pipeline consists of two key\nstages: (1) supervised fine-tuning (SFT) to align the target and source model\ndistributions, and (2) Direct Preference Optimization (DPO) to apply\npreferences from multiple source LLMs to fine-tune the target model. The\nresulting FuseChat-3.0 models exhibit significant performance gains across\ntasks such as instruction following, general knowledge, mathematics, and\ncoding. As illustrated in Figure 1, using Llama-3.1-8B-Instruct as the target\nmodel, our fusion approach achieves an average improvement of 6.8 points across\n14 benchmarks. Moreover, it demonstrates remarkable gains of 37.1 points and\n30.1 points on the instruction-following benchmarks AlpacaEval-2 and\nArena-Hard, respectively. Our code, models, and datasets are available at\nhttps://github.com/SLIT-AI/FuseChat-3.0.",
      "upvotes": 5,
      "discussionId": "67ca64ced153739fa9b9dc1b",
      "projectPage": "https://slit-ai.github.io/FuseChat-3.0/",
      "githubRepo": "https://github.com/SLIT-AI/FuseChat-3.0"
    },
    "publishedAt": "2025-03-06T22:20:34.462Z",
    "title": "FuseChat-3.0: Preference Optimization Meets Heterogeneous Model Fusion",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62ecbffd99112e99c5f7fded/nmr7w6NOioBYwMmNfezcf.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04222.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62ecbffd99112e99c5f7fded",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62ecbffd99112e99c5f7fded/U6iXAJbpm2vaC5qksEPiH.png",
      "fullname": "Fanqi Wan",
      "name": "Wanfq",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 29
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.04094",
      "authors": [
        {
          "_id": "67ca7bcc06501013d727a5d7",
          "user": {
            "_id": "6658e1c8ce1b2838885b2d7f",
            "avatarUrl": "/avatars/8623555f14b62f40fd372da20cb59ccc.svg",
            "isPro": false,
            "fullname": "Seth Karten",
            "user": "milkkarten",
            "type": "user"
          },
          "name": "Seth Karten",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-07T06:07:26.564Z",
          "hidden": false
        },
        {
          "_id": "67ca7bcc06501013d727a5d8",
          "name": "Andy Luu Nguyen",
          "hidden": false
        },
        {
          "_id": "67ca7bcc06501013d727a5d9",
          "user": {
            "_id": "66749c510974bbc971139f6a",
            "avatarUrl": "/avatars/bfab9d8d8bc589bb9bd49925b76e04a4.svg",
            "isPro": false,
            "fullname": "Chi Jin",
            "user": "chijin",
            "type": "user"
          },
          "name": "Chi Jin",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-07T04:53:33.942Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-06T05:06:27.000Z",
      "title": "PokéChamp: an Expert-level Minimax Language Agent",
      "summary": "We introduce Pok\\'eChamp, a minimax agent powered by Large Language Models\n(LLMs) for Pok\\'emon battles. Built on a general framework for two-player\ncompetitive games, Pok\\'eChamp leverages the generalist capabilities of LLMs to\nenhance minimax tree search. Specifically, LLMs replace three key modules: (1)\nplayer action sampling, (2) opponent modeling, and (3) value function\nestimation, enabling the agent to effectively utilize gameplay history and\nhuman knowledge to reduce the search space and address partial observability.\nNotably, our framework requires no additional LLM training. We evaluate\nPok\\'eChamp in the popular Gen 9 OU format. When powered by GPT-4o, it achieves\na win rate of 76% against the best existing LLM-based bot and 84% against the\nstrongest rule-based bot, demonstrating its superior performance. Even with an\nopen-source 8-billion-parameter Llama 3.1 model, Pok\\'eChamp consistently\noutperforms the previous best LLM-based bot, Pok\\'ellmon powered by GPT-4o,\nwith a 64% win rate. Pok\\'eChamp attains a projected Elo of 1300-1500 on the\nPok\\'emon Showdown online ladder, placing it among the top 30%-10% of human\nplayers. In addition, this work compiles the largest real-player Pok\\'emon\nbattle dataset, featuring over 3 million games, including more than 500k\nhigh-Elo matches. Based on this dataset, we establish a series of battle\nbenchmarks and puzzles to evaluate specific battling skills. We further provide\nkey updates to the local game engine. We hope this work fosters further\nresearch that leverage Pok\\'emon battle as benchmark to integrate LLM\ntechnologies with game-theoretic algorithms addressing general multiagent\nproblems. Videos, code, and dataset available at\nhttps://sites.google.com/view/pokechamp-llm.",
      "upvotes": 4,
      "discussionId": "67ca7bcd06501013d727a668",
      "projectPage": "https://sites.google.com/view/pokechamp-llm",
      "githubRepo": "https://github.com/sethkarten/pokechamp"
    },
    "publishedAt": "2025-03-06T23:53:38.838Z",
    "title": "PokéChamp: an Expert-level Minimax Language Agent",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04094.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6299
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.04130",
      "authors": [
        {
          "_id": "67ca7baf6d5c2eafede56d35",
          "name": "Jindong Jiang",
          "hidden": false
        },
        {
          "_id": "67ca7baf6d5c2eafede56d36",
          "name": "Xiuyu Li",
          "hidden": false
        },
        {
          "_id": "67ca7baf6d5c2eafede56d37",
          "name": "Zhijian Liu",
          "hidden": false
        },
        {
          "_id": "67ca7baf6d5c2eafede56d38",
          "name": "Muyang Li",
          "hidden": false
        },
        {
          "_id": "67ca7baf6d5c2eafede56d39",
          "name": "Guo Chen",
          "hidden": false
        },
        {
          "_id": "67ca7baf6d5c2eafede56d3a",
          "name": "Zhiqi Li",
          "hidden": false
        },
        {
          "_id": "67ca7baf6d5c2eafede56d3b",
          "name": "De-An Huang",
          "hidden": false
        },
        {
          "_id": "67ca7baf6d5c2eafede56d3c",
          "name": "Guilin Liu",
          "hidden": false
        },
        {
          "_id": "67ca7baf6d5c2eafede56d3d",
          "name": "Zhiding Yu",
          "hidden": false
        },
        {
          "_id": "67ca7baf6d5c2eafede56d3e",
          "name": "Kurt Keutzer",
          "hidden": false
        },
        {
          "_id": "67ca7baf6d5c2eafede56d3f",
          "name": "Sungjin Ahn",
          "hidden": false
        },
        {
          "_id": "67ca7baf6d5c2eafede56d40",
          "name": "Jan Kautz",
          "hidden": false
        },
        {
          "_id": "67ca7baf6d5c2eafede56d41",
          "name": "Hongxu Yin",
          "hidden": false
        },
        {
          "_id": "67ca7baf6d5c2eafede56d42",
          "name": "Yao Lu",
          "hidden": false
        },
        {
          "_id": "67ca7baf6d5c2eafede56d43",
          "name": "Song Han",
          "hidden": false
        },
        {
          "_id": "67ca7baf6d5c2eafede56d44",
          "name": "Wonmin Byeon",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-06T06:17:38.000Z",
      "title": "Token-Efficient Long Video Understanding for Multimodal LLMs",
      "summary": "Recent advances in video-based multimodal large language models (Video-LLMs)\nhave significantly improved video understanding by processing videos as\nsequences of image frames. However, many existing methods treat frames\nindependently in the vision backbone, lacking explicit temporal modeling, which\nlimits their ability to capture dynamic patterns and efficiently handle long\nvideos. To address these limitations, we introduce STORM\n(Spatiotemporal TOken Reduction for\nMultimodal LLMs), a novel architecture incorporating a dedicated\ntemporal encoder between the image encoder and the LLM. Our temporal encoder\nleverages the Mamba State Space Model to integrate temporal information into\nimage tokens, generating enriched representations that preserve inter-frame\ndynamics across the entire video sequence. This enriched encoding not only\nenhances video reasoning capabilities but also enables effective token\nreduction strategies, including test-time sampling and training-based temporal\nand spatial pooling, substantially reducing computational demands on the LLM\nwithout sacrificing key temporal information. By integrating these techniques,\nour approach simultaneously reduces training and inference latency while\nimproving performance, enabling efficient and robust video understanding over\nextended temporal contexts. Extensive evaluations show that STORM achieves\nstate-of-the-art results across various long video understanding benchmarks\n(more than 5\\% improvement on MLVU and LongVideoBench) while reducing the\ncomputation costs by up to 8times and the decoding latency by\n2.4-2.9times for the fixed numbers of input frames. Project page is\navailable at https://research.nvidia.com/labs/lpr/storm",
      "upvotes": 3,
      "discussionId": "67ca7bb16d5c2eafede56df1"
    },
    "publishedAt": "2025-03-06T23:53:09.588Z",
    "title": "Token-Efficient Long Video Understanding for Multimodal LLMs",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04130.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6299
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.04725",
      "authors": [
        {
          "_id": "67ca9092ba1ee2b914e3fa4a",
          "name": "Zhuo Chen",
          "hidden": false
        },
        {
          "_id": "67ca9092ba1ee2b914e3fa4b",
          "name": "Oriol Mayné i Comas",
          "hidden": false
        },
        {
          "_id": "67ca9092ba1ee2b914e3fa4c",
          "name": "Zhuotao Jin",
          "hidden": false
        },
        {
          "_id": "67ca9092ba1ee2b914e3fa4d",
          "name": "Di Luo",
          "hidden": false
        },
        {
          "_id": "67ca9092ba1ee2b914e3fa4e",
          "name": "Marin Soljačić",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-06T18:59:48.000Z",
      "title": "L^2M: Mutual Information Scaling Law for Long-Context Language\n  Modeling",
      "summary": "We rigorously establish a bipartite mutual information scaling law in natural\nlanguage that governs long-range dependencies. This scaling law, which we show\nis distinct from and scales independently of the conventional two-point mutual\ninformation, is the key to understanding long-context language modeling. Using\nthis scaling law, we formulate the Long-context Language Modeling (L^2M)\ncondition, which relates a model's capacity for effective long context length\nmodeling to the scaling of its latent state size for storing past information.\nOur results are validated through experiments on both transformers and state\nspace models. This work establishes a theoretical foundation that guides the\ndevelopment of large language models toward longer context lengths.",
      "upvotes": 2,
      "discussionId": "67ca90c1ba1ee2b914e405b9",
      "projectPage": "https://github.com/LSquaredM/mutual_info_scaling_law",
      "githubRepo": "https://github.com/LSquaredM/mutual_info_scaling_law"
    },
    "publishedAt": "2025-03-07T01:42:13.847Z",
    "title": "L$^2$M: Mutual Information Scaling Law for Long-Context Language Modeling",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/65e0027c960938e63e4a0157/lMxohK6cMFgsw39hn0jga.jpeg",
      "https://cdn-uploads.huggingface.co/production/uploads/65e0027c960938e63e4a0157/EqSl1OwTeggMI59pVQzeR.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04725.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65e0027c960938e63e4a0157",
      "avatarUrl": "/avatars/c8ca0b082ee8e8004f47a23d9393df67.svg",
      "fullname": "Zhuo Chen",
      "name": "zhuoc3",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.03983",
      "authors": [
        {
          "_id": "67ca66c1cb7e422997cbd148",
          "name": "Sreyan Ghosh",
          "hidden": false
        },
        {
          "_id": "67ca66c1cb7e422997cbd149",
          "user": {
            "_id": "652a4dfc36f031c5e6f8b8a6",
            "avatarUrl": "/avatars/9fb56b025dc25f91ca6c31136eaf74b2.svg",
            "isPro": false,
            "fullname": "Zhifeng Kong",
            "user": "ZhifengKong",
            "type": "user"
          },
          "name": "Zhifeng Kong",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-07T03:23:47.395Z",
          "hidden": false
        },
        {
          "_id": "67ca66c1cb7e422997cbd14a",
          "name": "Sonal Kumar",
          "hidden": false
        },
        {
          "_id": "67ca66c1cb7e422997cbd14b",
          "name": "S Sakshi",
          "hidden": false
        },
        {
          "_id": "67ca66c1cb7e422997cbd14c",
          "name": "Jaehyeon Kim",
          "hidden": false
        },
        {
          "_id": "67ca66c1cb7e422997cbd14d",
          "name": "Wei Ping",
          "hidden": false
        },
        {
          "_id": "67ca66c1cb7e422997cbd14e",
          "name": "Rafael Valle",
          "hidden": false
        },
        {
          "_id": "67ca66c1cb7e422997cbd14f",
          "name": "Dinesh Manocha",
          "hidden": false
        },
        {
          "_id": "67ca66c1cb7e422997cbd150",
          "name": "Bryan Catanzaro",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-06T00:10:26.000Z",
      "title": "Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding\n  and Expert Reasoning Abilities",
      "summary": "Understanding and reasoning over non-speech sounds and music are crucial for\nboth humans and AI agents to interact effectively with their environments. In\nthis paper, we introduce Audio Flamingo 2 (AF2), an Audio-Language Model (ALM)\nwith advanced audio understanding and reasoning capabilities. AF2 leverages (i)\na custom CLAP model, (ii) synthetic Audio QA data for fine-grained audio\nreasoning, and (iii) a multi-stage curriculum learning strategy. AF2 achieves\nstate-of-the-art performance with only a 3B parameter small language model,\nsurpassing large open-source and proprietary models across over 20 benchmarks.\nNext, for the first time, we extend audio understanding to long audio segments\n(30 secs to 5 mins) and propose LongAudio, a large and novel dataset for\ntraining ALMs on long audio captioning and question-answering tasks.\nFine-tuning AF2 on LongAudio leads to exceptional performance on our proposed\nLongAudioBench, an expert annotated benchmark for evaluating ALMs on long audio\nunderstanding capabilities. We conduct extensive ablation studies to confirm\nthe efficacy of our approach. Project Website:\nhttps://research.nvidia.com/labs/adlr/AF2/.",
      "upvotes": 2,
      "discussionId": "67ca66c3cb7e422997cbd178"
    },
    "publishedAt": "2025-03-07T00:12:47.515Z",
    "title": "Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.03983.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c9664eb34e600d7eaa4beb",
      "avatarUrl": "/avatars/ca23ecdec2d31c99ecce97d9b180ae0c.svg",
      "fullname": "Ghosh",
      "name": "Sreyan88",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.20258",
      "authors": [
        {
          "_id": "67ca7b557436e6327ca877ff",
          "name": "Amr Mohamed",
          "hidden": false
        },
        {
          "_id": "67ca7b557436e6327ca87800",
          "name": "Mingmeng Geng",
          "hidden": false
        },
        {
          "_id": "67ca7b557436e6327ca87801",
          "name": "Michalis Vazirgiannis",
          "hidden": false
        },
        {
          "_id": "67ca7b557436e6327ca87802",
          "name": "Guokan Shang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-27T16:46:23.000Z",
      "title": "LLM as a Broken Telephone: Iterative Generation Distorts Information",
      "summary": "As large language models are increasingly responsible for online content,\nconcerns arise about the impact of repeatedly processing their own outputs.\nInspired by the \"broken telephone\" effect in chained human communication, this\nstudy investigates whether LLMs similarly distort information through iterative\ngeneration. Through translation-based experiments, we find that distortion\naccumulates over time, influenced by language choice and chain complexity.\nWhile degradation is inevitable, it can be mitigated through strategic\nprompting techniques. These findings contribute to discussions on the long-term\neffects of AI-mediated information propagation, raising important questions\nabout the reliability of LLM-generated content in iterative workflows.",
      "upvotes": 2,
      "discussionId": "67ca7b577436e6327ca878ec",
      "githubRepo": "https://github.com/amr-mohamedd/LLM-as-a-Broken-Telephone"
    },
    "publishedAt": "2025-03-06T23:56:18.841Z",
    "title": "LLM as a Broken Telephone: Iterative Generation Distorts Information",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20258.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "655efd24afee0e00788bb589",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655efd24afee0e00788bb589/22guLxIWNybbJR3jI-c4w.jpeg",
      "fullname": "Amr Mohamed",
      "name": "amr-mohamed",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.02191",
      "authors": [
        {
          "_id": "67ca7fe72a83a60adcb6611a",
          "name": "Mia Mohammad Imran",
          "hidden": false
        },
        {
          "_id": "67ca7fe72a83a60adcb6611b",
          "name": "Robert Zita",
          "hidden": false
        },
        {
          "_id": "67ca7fe72a83a60adcb6611c",
          "name": "Rebekah Copeland",
          "hidden": false
        },
        {
          "_id": "67ca7fe72a83a60adcb6611d",
          "name": "Preetha Chatterjee",
          "hidden": false
        },
        {
          "_id": "67ca7fe72a83a60adcb6611e",
          "name": "Rahat Rizvi Rahman",
          "hidden": false
        },
        {
          "_id": "67ca7fe72a83a60adcb6611f",
          "name": "Kostadin Damevski",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-04T02:01:37.000Z",
      "title": "Understanding and Predicting Derailment in Toxic Conversations on GitHub",
      "summary": "Software projects thrive on the involvement and contributions of individuals\nfrom different backgrounds. However, toxic language and negative interactions\ncan hinder the participation and retention of contributors and alienate\nnewcomers. Proactive moderation strategies aim to prevent toxicity from\noccurring by addressing conversations that have derailed from their intended\npurpose. This study aims to understand and predict conversational derailment\nleading to toxicity on GitHub.\n  To facilitate this research, we curate a novel dataset comprising 202 toxic\nconversations from GitHub with annotated derailment points, along with 696\nnon-toxic conversations as a baseline. Based on this dataset, we identify\nunique characteristics of toxic conversations and derailment points, including\nlinguistic markers such as second-person pronouns, negation terms, and tones of\nBitter Frustration and Impatience, as well as patterns in conversational\ndynamics between project contributors and external participants.\n  Leveraging these empirical observations, we propose a proactive moderation\napproach to automatically detect and address potentially harmful conversations\nbefore escalation. By utilizing modern LLMs, we develop a conversation\ntrajectory summary technique that captures the evolution of discussions and\nidentifies early signs of derailment. Our experiments demonstrate that LLM\nprompts tailored to provide summaries of GitHub conversations achieve 69%\nF1-Score in predicting conversational derailment, strongly improving over a set\nof baseline approaches.",
      "upvotes": 1,
      "discussionId": "67ca7fe82a83a60adcb6615b",
      "githubRepo": "https://github.com/imranraad07/derailment-oss-replication"
    },
    "publishedAt": "2025-03-07T00:11:25.116Z",
    "title": "Understanding and Predicting Derailment in Toxic Conversations on GitHub",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.02191.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6331c3f618711776b468e9ec",
      "avatarUrl": "/avatars/af2c4bba031e474bf4fd2ea19e415aaf.svg",
      "fullname": "Mia Mohammad Imran",
      "name": "imranraad",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.04606",
      "authors": [
        {
          "_id": "67ca7b8a2a2c299d98944909",
          "name": "Aoxiong Yin",
          "hidden": false
        },
        {
          "_id": "67ca7b8a2a2c299d9894490a",
          "name": "Kai Shen",
          "hidden": false
        },
        {
          "_id": "67ca7b8a2a2c299d9894490b",
          "name": "Yichong Leng",
          "hidden": false
        },
        {
          "_id": "67ca7b8a2a2c299d9894490c",
          "name": "Xu Tan",
          "hidden": false
        },
        {
          "_id": "67ca7b8a2a2c299d9894490d",
          "name": "Xinyu Zhou",
          "hidden": false
        },
        {
          "_id": "67ca7b8a2a2c299d9894490e",
          "name": "Juncheng Li",
          "hidden": false
        },
        {
          "_id": "67ca7b8a2a2c299d9894490f",
          "name": "Siliang Tang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-06T16:53:14.000Z",
      "title": "The Best of Both Worlds: Integrating Language Models and Diffusion\n  Models for Video Generation",
      "summary": "Recent advancements in text-to-video (T2V) generation have been driven by two\ncompeting paradigms: autoregressive language models and diffusion models.\nHowever, each paradigm has intrinsic limitations: language models struggle with\nvisual quality and error accumulation, while diffusion models lack semantic\nunderstanding and causal modeling. In this work, we propose LanDiff, a hybrid\nframework that synergizes the strengths of both paradigms through\ncoarse-to-fine generation. Our architecture introduces three key innovations:\n(1) a semantic tokenizer that compresses 3D visual features into compact 1D\ndiscrete representations through efficient semantic compression, achieving a\nsim14,000times compression ratio; (2) a language model that generates\nsemantic tokens with high-level semantic relationships; (3) a streaming\ndiffusion model that refines coarse semantics into high-fidelity videos.\nExperiments show that LanDiff, a 5B model, achieves a score of 85.43 on the\nVBench T2V benchmark, surpassing the state-of-the-art open-source models\nHunyuan Video (13B) and other commercial models such as Sora, Keling, and\nHailuo. Furthermore, our model also achieves state-of-the-art performance in\nlong video generation, surpassing other open-source models in this field. Our\ndemo can be viewed at https://landiff.github.io/.",
      "upvotes": 1,
      "discussionId": "67ca7b8d2a2c299d989449a8"
    },
    "publishedAt": "2025-03-06T23:52:33.338Z",
    "title": "The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04606.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6299
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.04378",
      "authors": [
        {
          "_id": "67ca637e4cb4283da8ae2979",
          "name": "Zhilin Wang",
          "hidden": false
        },
        {
          "_id": "67ca637e4cb4283da8ae297a",
          "name": "Jiaqi Zeng",
          "hidden": false
        },
        {
          "_id": "67ca637e4cb4283da8ae297b",
          "name": "Olivier Delalleau",
          "hidden": false
        },
        {
          "_id": "67ca637e4cb4283da8ae297c",
          "name": "Daniel Egert",
          "hidden": false
        },
        {
          "_id": "67ca637e4cb4283da8ae297d",
          "name": "Ellie Evans",
          "hidden": false
        },
        {
          "_id": "67ca637e4cb4283da8ae297e",
          "name": "Hoo-Chang Shin",
          "hidden": false
        },
        {
          "_id": "67ca637e4cb4283da8ae297f",
          "name": "Felipe Soares",
          "hidden": false
        },
        {
          "_id": "67ca637e4cb4283da8ae2980",
          "name": "Yi Dong",
          "hidden": false
        },
        {
          "_id": "67ca637e4cb4283da8ae2981",
          "name": "Oleksii Kuchaiev",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-06T12:30:24.000Z",
      "title": "Dedicated Feedback and Edit Models Empower Inference-Time Scaling for\n  Open-Ended General-Domain Tasks",
      "summary": "Inference-Time Scaling has been critical to the success of recent models such\nas OpenAI o1 and DeepSeek R1. However, many techniques used to train models for\ninference-time scaling require tasks to have answers that can be verified,\nlimiting their application to domains such as math, coding and logical\nreasoning. We take inspiration from how humans make first attempts, ask for\ndetailed feedback from others and make improvements based on such feedback\nacross a wide spectrum of open-ended endeavors. To this end, we collect data\nfor and train dedicated Feedback and Edit Models that are capable of performing\ninference-time scaling for open-ended general-domain tasks. In our setup, one\nmodel generates an initial response, which are given feedback by a second\nmodel, that are then used by a third model to edit the response. We show that\nperformance on Arena Hard, a benchmark strongly predictive of Chatbot Arena Elo\ncan be boosted by scaling the number of initial response drafts, effective\nfeedback and edited responses. When scaled optimally, our setup based on 70B\nmodels from the Llama 3 family can reach SoTA performance on Arena Hard at 92.7\nas of 5 Mar 2025, surpassing OpenAI o1-preview-2024-09-12 with 90.4 and\nDeepSeek R1 with 92.3.",
      "upvotes": 1,
      "discussionId": "67ca63804cb4283da8ae29da"
    },
    "publishedAt": "2025-03-06T22:10:18.014Z",
    "title": "Dedicated Feedback and Edit Models Empower Inference-Time Scaling for Open-Ended General-Domain Tasks",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04378.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6299
    },
    "isAuthorParticipating": false
  }
]