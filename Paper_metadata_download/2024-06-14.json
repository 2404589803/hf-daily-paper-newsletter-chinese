[
    {
        "paper": {
            "id": "2406.09414",
            "authors": [
                {
                    "_id": "666b9c7580a130166dbd9efc",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65a3a0342548c41ad9f4e4e7/80Yod9z7O95nC-Y0o-5UI.jpeg",
                        "isPro": false,
                        "fullname": "Lihe Yang",
                        "user": "LiheYoung",
                        "type": "user"
                    },
                    "name": "Lihe Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-14T07:23:21.471Z",
                    "hidden": false
                },
                {
                    "_id": "666b9c7580a130166dbd9efd",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647b5fef6a79fbf5e996c47c/IkSMnDsCY_CyEFCiMDuxe.jpeg",
                        "isPro": false,
                        "fullname": "Bingyi Kang",
                        "user": "bykang",
                        "type": "user"
                    },
                    "name": "Bingyi Kang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-14T07:23:30.766Z",
                    "hidden": false
                },
                {
                    "_id": "666b9c7580a130166dbd9efe",
                    "name": "Zilong Huang",
                    "hidden": false
                },
                {
                    "_id": "666b9c7580a130166dbd9eff",
                    "name": "Zhen Zhao",
                    "hidden": false
                },
                {
                    "_id": "666b9c7580a130166dbd9f00",
                    "name": "Xiaogang Xu",
                    "hidden": false
                },
                {
                    "_id": "666b9c7580a130166dbd9f01",
                    "name": "Jiashi Feng",
                    "hidden": false
                },
                {
                    "_id": "666b9c7580a130166dbd9f02",
                    "name": "Hengshuang Zhao",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-13T17:59:56.000Z",
            "title": "Depth Anything V2",
            "summary": "This work presents Depth Anything V2. Without pursuing fancy techniques, we\naim to reveal crucial findings to pave the way towards building a powerful\nmonocular depth estimation model. Notably, compared with V1, this version\nproduces much finer and more robust depth predictions through three key\npractices: 1) replacing all labeled real images with synthetic images, 2)\nscaling up the capacity of our teacher model, and 3) teaching student models\nvia the bridge of large-scale pseudo-labeled real images. Compared with the\nlatest models built on Stable Diffusion, our models are significantly more\nefficient (more than 10x faster) and more accurate. We offer models of\ndifferent scales (ranging from 25M to 1.3B params) to support extensive\nscenarios. Benefiting from their strong generalization capability, we fine-tune\nthem with metric depth labels to obtain our metric depth models. In addition to\nour models, considering the limited diversity and frequent noise in current\ntest sets, we construct a versatile evaluation benchmark with precise\nannotations and diverse scenes to facilitate future research.",
            "upvotes": 45
        },
        "publishedAt": "2024-06-14T00:20:32.674Z",
        "title": "Depth Anything V2",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.09414.png",
        "numComments": 5,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.09415",
            "authors": [
                {
                    "_id": "666ba4cc3814e415474a9064",
                    "name": "Duy-Kien Nguyen",
                    "hidden": false
                },
                {
                    "_id": "666ba4cc3814e415474a9065",
                    "name": "Mahmoud Assran",
                    "hidden": false
                },
                {
                    "_id": "666ba4cc3814e415474a9066",
                    "name": "Unnat Jain",
                    "hidden": false
                },
                {
                    "_id": "666ba4cc3814e415474a9067",
                    "user": {
                        "avatarUrl": "/avatars/4cb4d3361cfd7f205c82c6e15caa11f6.svg",
                        "isPro": false,
                        "fullname": "Martin Oswald",
                        "user": "martin-r-oswald",
                        "type": "user"
                    },
                    "name": "Martin R. Oswald",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T09:40:04.008Z",
                    "hidden": false
                },
                {
                    "_id": "666ba4cc3814e415474a9068",
                    "name": "Cees G. M. Snoek",
                    "hidden": false
                },
                {
                    "_id": "666ba4cc3814e415474a9069",
                    "user": {
                        "avatarUrl": "/avatars/75262a35b27a2ae1939df9118120d99e.svg",
                        "isPro": false,
                        "fullname": "Xinlei Chen",
                        "user": "endernewton",
                        "type": "user"
                    },
                    "name": "Xinlei Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T09:40:17.429Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-13T17:59:58.000Z",
            "title": "An Image is Worth More Than 16x16 Patches: Exploring Transformers on\n  Individual Pixels",
            "summary": "This work does not introduce a new method. Instead, we present an interesting\nfinding that questions the necessity of the inductive bias -- locality in\nmodern computer vision architectures. Concretely, we find that vanilla\nTransformers can operate by directly treating each individual pixel as a token\nand achieve highly performant results. This is substantially different from the\npopular design in Vision Transformer, which maintains the inductive bias from\nConvNets towards local neighborhoods (e.g. by treating each 16x16 patch as a\ntoken). We mainly showcase the effectiveness of pixels-as-tokens across three\nwell-studied tasks in computer vision: supervised learning for object\nclassification, self-supervised learning via masked autoencoding, and image\ngeneration with diffusion models. Although directly operating on individual\npixels is less computationally practical, we believe the community must be\naware of this surprising piece of knowledge when devising the next generation\nof neural architectures for computer vision.",
            "upvotes": 26
        },
        "publishedAt": "2024-06-14T00:34:23.723Z",
        "title": "An Image is Worth More Than 16x16 Patches: Exploring Transformers on Individual Pixels",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.09415.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.09416",
            "authors": [
                {
                    "_id": "666bb80de59c42693adae02a",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/639f1e519f1f2baab2f00d22/pFjd51WZuVZ3A11rItvmk.jpeg",
                        "isPro": false,
                        "fullname": "Qihao Liu",
                        "user": "QHL067",
                        "type": "user"
                    },
                    "name": "Qihao Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-14T07:22:24.541Z",
                    "hidden": false
                },
                {
                    "_id": "666bb80de59c42693adae02b",
                    "user": {
                        "avatarUrl": "/avatars/f62a9f835279b49ca5899c3aa6228dca.svg",
                        "isPro": false,
                        "fullname": "Zhanpeng Zeng",
                        "user": "mlpen",
                        "type": "user"
                    },
                    "name": "Zhanpeng Zeng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T09:51:20.683Z",
                    "hidden": false
                },
                {
                    "_id": "666bb80de59c42693adae02c",
                    "name": "Ju He",
                    "hidden": false
                },
                {
                    "_id": "666bb80de59c42693adae02d",
                    "user": {
                        "avatarUrl": "/avatars/54419bb6e6ee788aba10cd64cd921204.svg",
                        "isPro": false,
                        "fullname": "Qihang Yu",
                        "user": "yucornetto",
                        "type": "user"
                    },
                    "name": "Qihang Yu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T09:51:41.891Z",
                    "hidden": false
                },
                {
                    "_id": "666bb80de59c42693adae02e",
                    "user": {
                        "avatarUrl": "/avatars/5419f8d6d4d36fa5ac83e30667b9fd99.svg",
                        "isPro": false,
                        "fullname": "Xiaohui Shen",
                        "user": "XiaohuiShen",
                        "type": "user"
                    },
                    "name": "Xiaohui Shen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T09:51:48.062Z",
                    "hidden": false
                },
                {
                    "_id": "666bb80de59c42693adae02f",
                    "name": "Liang-Chieh Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-13T17:59:58.000Z",
            "title": "Alleviating Distortion in Image Generation via Multi-Resolution\n  Diffusion Models",
            "summary": "This paper presents innovative enhancements to diffusion models by\nintegrating a novel multi-resolution network and time-dependent layer\nnormalization. Diffusion models have gained prominence for their effectiveness\nin high-fidelity image generation. While conventional approaches rely on\nconvolutional U-Net architectures, recent Transformer-based designs have\ndemonstrated superior performance and scalability. However, Transformer\narchitectures, which tokenize input data (via \"patchification\"), face a\ntrade-off between visual fidelity and computational complexity due to the\nquadratic nature of self-attention operations concerning token length. While\nlarger patch sizes enable attention computation efficiency, they struggle to\ncapture fine-grained visual details, leading to image distortions. To address\nthis challenge, we propose augmenting the Diffusion model with the\nMulti-Resolution network (DiMR), a framework that refines features across\nmultiple resolutions, progressively enhancing detail from low to high\nresolution. Additionally, we introduce Time-Dependent Layer Normalization\n(TD-LN), a parameter-efficient approach that incorporates time-dependent\nparameters into layer normalization to inject time information and achieve\nsuperior performance. Our method's efficacy is demonstrated on the\nclass-conditional ImageNet generation benchmark, where DiMR-XL variants\noutperform prior diffusion models, setting new state-of-the-art FID scores of\n1.70 on ImageNet 256 x 256 and 2.89 on ImageNet 512 x 512. Project page:\nhttps://qihao067.github.io/projects/DiMR",
            "upvotes": 20
        },
        "publishedAt": "2024-06-14T01:56:07.494Z",
        "title": "Alleviating Distortion in Image Generation via Multi-Resolution Diffusion Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.09416.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/639f1e519f1f2baab2f00d22/pFjd51WZuVZ3A11rItvmk.jpeg",
            "fullname": "Qihao Liu",
            "name": "QHL067",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.09413",
            "authors": [
                {
                    "_id": "666bae6c3214e94388ea7606",
                    "user": {
                        "avatarUrl": "/avatars/7e6d9a096a71d601f66279e317d11e1f.svg",
                        "isPro": false,
                        "fullname": "Amil Dravid",
                        "user": "amildravid4292",
                        "type": "user"
                    },
                    "name": "Amil Dravid",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-14T07:22:40.931Z",
                    "hidden": false
                },
                {
                    "_id": "666bae6c3214e94388ea7607",
                    "user": {
                        "avatarUrl": "/avatars/b0600de531b4c88f4aa7c30c5ceb06e1.svg",
                        "isPro": false,
                        "fullname": "Yossi Gandelsman",
                        "user": "yossig",
                        "type": "user"
                    },
                    "name": "Yossi Gandelsman",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T09:57:15.404Z",
                    "hidden": false
                },
                {
                    "_id": "666bae6c3214e94388ea7608",
                    "user": {
                        "avatarUrl": "/avatars/45bcc3ee5d3231644fe58e4352afe735.svg",
                        "isPro": false,
                        "fullname": "Kuan-Chieh Wang",
                        "user": "wangkua1",
                        "type": "user"
                    },
                    "name": "Kuan-Chieh Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T09:57:20.946Z",
                    "hidden": false
                },
                {
                    "_id": "666bae6c3214e94388ea7609",
                    "user": {
                        "avatarUrl": "/avatars/d1baf7fd17daf4be16ba5bd6cd4f2277.svg",
                        "isPro": false,
                        "fullname": "Rameen Abdal",
                        "user": "RameenAbdal",
                        "type": "user"
                    },
                    "name": "Rameen Abdal",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T09:57:26.935Z",
                    "hidden": false
                },
                {
                    "_id": "666bae6c3214e94388ea760a",
                    "name": "Gordon Wetzstein",
                    "hidden": false
                },
                {
                    "_id": "666bae6c3214e94388ea760b",
                    "name": "Alexei A. Efros",
                    "hidden": false
                },
                {
                    "_id": "666bae6c3214e94388ea760c",
                    "name": "Kfir Aberman",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-13T17:59:56.000Z",
            "title": "Interpreting the Weight Space of Customized Diffusion Models",
            "summary": "We investigate the space of weights spanned by a large collection of\ncustomized diffusion models. We populate this space by creating a dataset of\nover 60,000 models, each of which is a base model fine-tuned to insert a\ndifferent person's visual identity. We model the underlying manifold of these\nweights as a subspace, which we term weights2weights. We demonstrate three\nimmediate applications of this space -- sampling, editing, and inversion.\nFirst, as each point in the space corresponds to an identity, sampling a set of\nweights from it results in a model encoding a novel identity. Next, we find\nlinear directions in this space corresponding to semantic edits of the identity\n(e.g., adding a beard). These edits persist in appearance across generated\nsamples. Finally, we show that inverting a single image into this space\nreconstructs a realistic identity, even if the input image is out of\ndistribution (e.g., a painting). Our results indicate that the weight space of\nfine-tuned diffusion models behaves as an interpretable latent space of\nidentities.",
            "upvotes": 14
        },
        "publishedAt": "2024-06-14T01:24:25.705Z",
        "title": "Interpreting the Weight Space of Customized Diffusion Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.09413.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.09246",
            "authors": [
                {
                    "_id": "666b9d34cf416fdc65f87643",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b017833ffa69983bb8ed37/gdaFjlaJDDPtfH84b75RU.jpeg",
                        "isPro": false,
                        "fullname": "Moo Jin Kim",
                        "user": "moojink",
                        "type": "user"
                    },
                    "name": "Moo Jin Kim",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T07:20:40.563Z",
                    "hidden": false
                },
                {
                    "_id": "666b9d34cf416fdc65f87644",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d4c1ff29b4ac81c265e6e6/GXgs28okxGfpBdqhxov9-.png",
                        "isPro": false,
                        "fullname": "Karl Pertsch",
                        "user": "KarlP",
                        "type": "user"
                    },
                    "name": "Karl Pertsch",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T07:20:48.705Z",
                    "hidden": false
                },
                {
                    "_id": "666b9d34cf416fdc65f87645",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1634666324094-6150b090d84cf0532aa1764b.jpeg",
                        "isPro": true,
                        "fullname": "Siddharth Karamcheti",
                        "user": "skaramcheti",
                        "type": "user"
                    },
                    "name": "Siddharth Karamcheti",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T07:20:55.211Z",
                    "hidden": false
                },
                {
                    "_id": "666b9d34cf416fdc65f87646",
                    "user": {
                        "avatarUrl": "/avatars/bc2f1f1135dbc3460ae30b0e7a0cf39b.svg",
                        "isPro": false,
                        "fullname": "xiao",
                        "user": "tedxiao",
                        "type": "user"
                    },
                    "name": "Ted Xiao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T07:21:02.137Z",
                    "hidden": false
                },
                {
                    "_id": "666b9d34cf416fdc65f87647",
                    "user": {
                        "avatarUrl": "/avatars/dd48cf05b946663dcb927545cd8edddc.svg",
                        "isPro": false,
                        "fullname": "Ashwin Balakrishna",
                        "user": "abalakrishnaTRI",
                        "type": "user"
                    },
                    "name": "Ashwin Balakrishna",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T10:04:21.971Z",
                    "hidden": false
                },
                {
                    "_id": "666b9d34cf416fdc65f87648",
                    "user": {
                        "avatarUrl": "/avatars/370b7b21bd1bc284d3f48c0c40dfa026.svg",
                        "isPro": false,
                        "fullname": "Suraj Nair",
                        "user": "surajnair",
                        "type": "user"
                    },
                    "name": "Suraj Nair",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T10:04:40.855Z",
                    "hidden": false
                },
                {
                    "_id": "666b9d34cf416fdc65f87649",
                    "user": {
                        "avatarUrl": "/avatars/df3a52bd4bc374d7cb4061f449f75c07.svg",
                        "isPro": false,
                        "fullname": "Rafael Rafailov",
                        "user": "rmrafailov",
                        "type": "user"
                    },
                    "name": "Rafael Rafailov",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T10:04:47.337Z",
                    "hidden": false
                },
                {
                    "_id": "666b9d34cf416fdc65f8764a",
                    "name": "Ethan Foster",
                    "hidden": false
                },
                {
                    "_id": "666b9d34cf416fdc65f8764b",
                    "name": "Grace Lam",
                    "hidden": false
                },
                {
                    "_id": "666b9d34cf416fdc65f8764c",
                    "name": "Pannag Sanketi",
                    "hidden": false
                },
                {
                    "_id": "666b9d34cf416fdc65f8764d",
                    "user": {
                        "avatarUrl": "/avatars/4959319e69d69e03a31ff4be326e602a.svg",
                        "isPro": false,
                        "fullname": "Quan Vuong",
                        "user": "kabut1211",
                        "type": "user"
                    },
                    "name": "Quan Vuong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T10:05:19.625Z",
                    "hidden": false
                },
                {
                    "_id": "666b9d34cf416fdc65f8764e",
                    "user": {
                        "avatarUrl": "/avatars/6f86547d8691e061e289dd906deedfdb.svg",
                        "isPro": false,
                        "fullname": "Thomas Kollar",
                        "user": "thomas-kollar",
                        "type": "user"
                    },
                    "name": "Thomas Kollar",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T10:05:34.367Z",
                    "hidden": false
                },
                {
                    "_id": "666b9d34cf416fdc65f8764f",
                    "name": "Benjamin Burchfiel",
                    "hidden": false
                },
                {
                    "_id": "666b9d34cf416fdc65f87650",
                    "name": "Russ Tedrake",
                    "hidden": false
                },
                {
                    "_id": "666b9d34cf416fdc65f87651",
                    "name": "Dorsa Sadigh",
                    "hidden": false
                },
                {
                    "_id": "666b9d34cf416fdc65f87652",
                    "user": {
                        "avatarUrl": "/avatars/e698726e9be61dd50ce2efe372ed5dac.svg",
                        "isPro": false,
                        "fullname": "Sergey Levine",
                        "user": "svlevine",
                        "type": "user"
                    },
                    "name": "Sergey Levine",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T10:05:50.110Z",
                    "hidden": false
                },
                {
                    "_id": "666b9d34cf416fdc65f87653",
                    "user": {
                        "avatarUrl": "/avatars/1fb8c80b60f21f65a0a027319101f236.svg",
                        "isPro": false,
                        "fullname": "Percy Liang",
                        "user": "percyliang",
                        "type": "user"
                    },
                    "name": "Percy Liang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T10:05:55.774Z",
                    "hidden": false
                },
                {
                    "_id": "666b9d34cf416fdc65f87654",
                    "user": {
                        "avatarUrl": "/avatars/fcac4912678ad3cb6e817d40bdee9aea.svg",
                        "isPro": false,
                        "fullname": "Chelsea Finn",
                        "user": "cbfinn",
                        "type": "user"
                    },
                    "name": "Chelsea Finn",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T10:06:01.500Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-13T15:46:55.000Z",
            "title": "OpenVLA: An Open-Source Vision-Language-Action Model",
            "summary": "Large policies pretrained on a combination of Internet-scale vision-language\ndata and diverse robot demonstrations have the potential to change how we teach\nrobots new skills: rather than training new behaviors from scratch, we can\nfine-tune such vision-language-action (VLA) models to obtain robust,\ngeneralizable policies for visuomotor control. Yet, widespread adoption of VLAs\nfor robotics has been challenging as 1) existing VLAs are largely closed and\ninaccessible to the public, and 2) prior work fails to explore methods for\nefficiently fine-tuning VLAs for new tasks, a key component for adoption.\nAddressing these challenges, we introduce OpenVLA, a 7B-parameter open-source\nVLA trained on a diverse collection of 970k real-world robot demonstrations.\nOpenVLA builds on a Llama 2 language model combined with a visual encoder that\nfuses pretrained features from DINOv2 and SigLIP. As a product of the added\ndata diversity and new model components, OpenVLA demonstrates strong results\nfor generalist manipulation, outperforming closed models such as RT-2-X (55B)\nby 16.5% in absolute task success rate across 29 tasks and multiple robot\nembodiments, with 7x fewer parameters. We further show that we can effectively\nfine-tune OpenVLA for new settings, with especially strong generalization\nresults in multi-task environments involving multiple objects and strong\nlanguage grounding abilities, and outperform expressive from-scratch imitation\nlearning methods such as Diffusion Policy by 20.4%. We also explore compute\nefficiency; as a separate contribution, we show that OpenVLA can be fine-tuned\non consumer GPUs via modern low-rank adaptation methods and served efficiently\nvia quantization without a hit to downstream success rate. Finally, we release\nmodel checkpoints, fine-tuning notebooks, and our PyTorch codebase with\nbuilt-in support for training VLAs at scale on Open X-Embodiment datasets.",
            "upvotes": 14
        },
        "publishedAt": "2024-06-14T00:41:10.881Z",
        "title": "OpenVLA: An Open-Source Vision-Language-Action Model",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.09246.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.09170",
            "authors": [
                {
                    "_id": "666bb0c8848be12c2a559cf9",
                    "user": {
                        "avatarUrl": "/avatars/aef5deb8774dee96e8d832be621a710b.svg",
                        "isPro": false,
                        "fullname": "Bahare",
                        "user": "baharef",
                        "type": "user"
                    },
                    "name": "Bahare Fatemi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-14T07:22:39.108Z",
                    "hidden": false
                },
                {
                    "_id": "666bb0c8848be12c2a559cfa",
                    "user": {
                        "avatarUrl": "/avatars/a70e49e319f9716bead091dd8d6ca4de.svg",
                        "isPro": false,
                        "fullname": "Mehran Kazemi",
                        "user": "mehrankazemi",
                        "type": "user"
                    },
                    "name": "Mehran Kazemi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T10:16:53.409Z",
                    "hidden": false
                },
                {
                    "_id": "666bb0c8848be12c2a559cfb",
                    "user": {
                        "avatarUrl": "/avatars/0eed94fcdb6d7c86259bc5b72b5bd41e.svg",
                        "isPro": false,
                        "fullname": "Anton Tsitsulin",
                        "user": "xgfs",
                        "type": "user"
                    },
                    "name": "Anton Tsitsulin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T10:16:59.260Z",
                    "hidden": false
                },
                {
                    "_id": "666bb0c8848be12c2a559cfc",
                    "name": "Karishma Malkan",
                    "hidden": false
                },
                {
                    "_id": "666bb0c8848be12c2a559cfd",
                    "user": {
                        "avatarUrl": "/avatars/f8e912ce8c69474414d0daf1efb93efa.svg",
                        "isPro": false,
                        "fullname": "Jinyeong Yim",
                        "user": "jinyeong",
                        "type": "user"
                    },
                    "name": "Jinyeong Yim",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T10:17:08.343Z",
                    "hidden": false
                },
                {
                    "_id": "666bb0c8848be12c2a559cfe",
                    "user": {
                        "avatarUrl": "/avatars/6252ed2e80ae2bcb006be123e7764672.svg",
                        "isPro": false,
                        "fullname": "John Palowitch",
                        "user": "jpalowitch",
                        "type": "user"
                    },
                    "name": "John Palowitch",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T10:17:13.988Z",
                    "hidden": false
                },
                {
                    "_id": "666bb0c8848be12c2a559cff",
                    "name": "Sungyong Seo",
                    "hidden": false
                },
                {
                    "_id": "666bb0c8848be12c2a559d00",
                    "user": {
                        "avatarUrl": "/avatars/8476944468bee3d75c2429bbf0f08b2f.svg",
                        "isPro": false,
                        "fullname": "Jonathan Halcrow",
                        "user": "jhalcrow",
                        "type": "user"
                    },
                    "name": "Jonathan Halcrow",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T10:17:23.808Z",
                    "hidden": false
                },
                {
                    "_id": "666bb0c8848be12c2a559d01",
                    "name": "Bryan Perozzi",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-13T14:31:19.000Z",
            "title": "Test of Time: A Benchmark for Evaluating LLMs on Temporal Reasoning",
            "summary": "Large language models (LLMs) have showcased remarkable reasoning\ncapabilities, yet they remain susceptible to errors, particularly in temporal\nreasoning tasks involving complex temporal logic. Existing research has\nexplored LLM performance on temporal reasoning using diverse datasets and\nbenchmarks. However, these studies often rely on real-world data that LLMs may\nhave encountered during pre-training or employ anonymization techniques that\ncan inadvertently introduce factual inconsistencies. In this work, we address\nthese limitations by introducing novel synthetic datasets specifically designed\nto assess LLM temporal reasoning abilities in various scenarios. The diversity\nof question types across these datasets enables systematic investigation into\nthe impact of the problem structure, size, question type, fact order, and other\nfactors on LLM performance. Our findings provide valuable insights into the\nstrengths and weaknesses of current LLMs in temporal reasoning tasks. To foster\nfurther research in this area, we are open-sourcing the datasets and evaluation\nframework used in our experiments: https://huggingface.co/datasets/baharef/ToT.",
            "upvotes": 13
        },
        "publishedAt": "2024-06-14T01:25:44.372Z",
        "title": "Test of Time: A Benchmark for Evaluating LLMs on Temporal Reasoning",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.09170.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.08552",
            "authors": [
                {
                    "_id": "666ba072fd6d120d6be23800",
                    "user": {
                        "avatarUrl": "/avatars/bcef9e49974debbcef66390dc096af60.svg",
                        "isPro": false,
                        "fullname": "Zhihang Yuan",
                        "user": "hahnyuan",
                        "type": "user"
                    },
                    "name": "Zhihang Yuan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-14T07:23:10.971Z",
                    "hidden": false
                },
                {
                    "_id": "666ba072fd6d120d6be23801",
                    "name": "Pu Lu",
                    "hidden": false
                },
                {
                    "_id": "666ba072fd6d120d6be23802",
                    "user": {
                        "avatarUrl": "/avatars/b03365861d44280b3da33562714c6079.svg",
                        "isPro": false,
                        "fullname": "Hanling Zhang",
                        "user": "p2o6e100",
                        "type": "user"
                    },
                    "name": "Hanling Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T10:02:13.406Z",
                    "hidden": false
                },
                {
                    "_id": "666ba072fd6d120d6be23803",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678782881444-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Ning",
                        "user": "Foxfi",
                        "type": "user"
                    },
                    "name": "Xuefei Ning",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-14T11:47:16.980Z",
                    "hidden": false
                },
                {
                    "_id": "666ba072fd6d120d6be23804",
                    "name": "Linfeng Zhang",
                    "hidden": false
                },
                {
                    "_id": "666ba072fd6d120d6be23805",
                    "name": "Tianchen Zhao",
                    "hidden": false
                },
                {
                    "_id": "666ba072fd6d120d6be23806",
                    "name": "Shengen Yan",
                    "hidden": false
                },
                {
                    "_id": "666ba072fd6d120d6be23807",
                    "name": "Guohao Dai",
                    "hidden": false
                },
                {
                    "_id": "666ba072fd6d120d6be23808",
                    "name": "Yu Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-12T18:00:08.000Z",
            "title": "DiTFastAttn: Attention Compression for Diffusion Transformer Models",
            "summary": "Diffusion Transformers (DiT) excel at image and video generation but face\ncomputational challenges due to self-attention's quadratic complexity. We\npropose DiTFastAttn, a novel post-training compression method to alleviate\nDiT's computational bottleneck. We identify three key redundancies in the\nattention computation during DiT inference: 1. spatial redundancy, where many\nattention heads focus on local information; 2. temporal redundancy, with high\nsimilarity between neighboring steps' attention outputs; 3. conditional\nredundancy, where conditional and unconditional inferences exhibit significant\nsimilarity. To tackle these redundancies, we propose three techniques: 1.\nWindow Attention with Residual Caching to reduce spatial redundancy; 2.\nTemporal Similarity Reduction to exploit the similarity between steps; 3.\nConditional Redundancy Elimination to skip redundant computations during\nconditional generation. To demonstrate the effectiveness of DiTFastAttn, we\napply it to DiT, PixArt-Sigma for image generation tasks, and OpenSora for\nvideo generation tasks. Evaluation results show that for image generation, our\nmethod reduces up to 88\\% of the FLOPs and achieves up to 1.6x speedup at high\nresolution generation.",
            "upvotes": 13
        },
        "publishedAt": "2024-06-14T00:15:15.263Z",
        "title": "DiTFastAttn: Attention Compression for Diffusion Transformer Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.08552.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.09411",
            "authors": [
                {
                    "_id": "666be4c9f1b3867caf4fef1b",
                    "user": {
                        "avatarUrl": "/avatars/318334e5ddae8fde0cbd06a1b9583ed8.svg",
                        "isPro": false,
                        "fullname": "Fei Wang",
                        "user": "fwnlp",
                        "type": "user"
                    },
                    "name": "Fei Wang",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2024-06-14T06:35:56.504Z",
                    "hidden": false
                },
                {
                    "_id": "666be4c9f1b3867caf4fef1c",
                    "user": {
                        "avatarUrl": "/avatars/4d29cef783603dc8ba123f01cde93ad8.svg",
                        "isPro": false,
                        "fullname": "Xingyu Fu",
                        "user": "Fiaa",
                        "type": "user"
                    },
                    "name": "Xingyu Fu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T10:09:39.699Z",
                    "hidden": false
                },
                {
                    "_id": "666be4c9f1b3867caf4fef1d",
                    "name": "James Y. Huang",
                    "hidden": false
                },
                {
                    "_id": "666be4c9f1b3867caf4fef1e",
                    "name": "Zekun Li",
                    "hidden": false
                },
                {
                    "_id": "666be4c9f1b3867caf4fef1f",
                    "name": "Qin Liu",
                    "hidden": false
                },
                {
                    "_id": "666be4c9f1b3867caf4fef20",
                    "user": {
                        "avatarUrl": "/avatars/50db06ef7a6efc7b2a525e1c90e576f7.svg",
                        "isPro": false,
                        "fullname": "Xiaogeng Liu",
                        "user": "ShletonLiu-N",
                        "type": "user"
                    },
                    "name": "Xiaogeng Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T10:12:41.886Z",
                    "hidden": false
                },
                {
                    "_id": "666be4c9f1b3867caf4fef21",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644c1df68c51ddbe0ea6ddd7/KAsOhnOSbrOK54OHtJpiQ.jpeg",
                        "isPro": true,
                        "fullname": "Mingyu Derek Ma",
                        "user": "derekma",
                        "type": "user"
                    },
                    "name": "Mingyu Derek Ma",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-14T09:17:23.597Z",
                    "hidden": false
                },
                {
                    "_id": "666be4c9f1b3867caf4fef22",
                    "name": "Nan Xu",
                    "hidden": false
                },
                {
                    "_id": "666be4c9f1b3867caf4fef23",
                    "user": {
                        "avatarUrl": "/avatars/c1a7055db6150b1bba92e7b13266b489.svg",
                        "isPro": false,
                        "fullname": "Wenxuan Zhou",
                        "user": "wzhouad",
                        "type": "user"
                    },
                    "name": "Wenxuan Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T10:13:15.931Z",
                    "hidden": false
                },
                {
                    "_id": "666be4c9f1b3867caf4fef24",
                    "name": "Kai Zhang",
                    "hidden": false
                },
                {
                    "_id": "666be4c9f1b3867caf4fef25",
                    "name": "Tianyi Lorena Yan",
                    "hidden": false
                },
                {
                    "_id": "666be4c9f1b3867caf4fef26",
                    "name": "Wenjie Jacky Mo",
                    "hidden": false
                },
                {
                    "_id": "666be4c9f1b3867caf4fef27",
                    "name": "Hsiang-Hui Liu",
                    "hidden": false
                },
                {
                    "_id": "666be4c9f1b3867caf4fef28",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60f5f68fa7fd83d025749234/gCeJAZfzaANAcEvI6v5-P.jpeg",
                        "isPro": false,
                        "fullname": "Pan Lu",
                        "user": "lupantech",
                        "type": "user"
                    },
                    "name": "Pan Lu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T10:14:31.328Z",
                    "hidden": false
                },
                {
                    "_id": "666be4c9f1b3867caf4fef29",
                    "user": {
                        "avatarUrl": "/avatars/430560ec2c2547f819225769ab432f30.svg",
                        "isPro": false,
                        "fullname": "Chunyuan Li",
                        "user": "Chunyuan24",
                        "type": "user"
                    },
                    "name": "Chunyuan Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T10:14:24.265Z",
                    "hidden": false
                },
                {
                    "_id": "666be4c9f1b3867caf4fef2a",
                    "name": "Chaowei Xiao",
                    "hidden": false
                },
                {
                    "_id": "666be4c9f1b3867caf4fef2b",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1622653364258-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Kai-Wei Chang",
                        "user": "kaiweichang",
                        "type": "user"
                    },
                    "name": "Kai-Wei Chang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T10:14:10.075Z",
                    "hidden": false
                },
                {
                    "_id": "666be4c9f1b3867caf4fef2c",
                    "user": {
                        "avatarUrl": "/avatars/d7eadb22aa3d8a067189b924767864e9.svg",
                        "isPro": false,
                        "fullname": "Dan Roth",
                        "user": "danielpaulroth",
                        "type": "user"
                    },
                    "name": "Dan Roth",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T10:13:50.925Z",
                    "hidden": false
                },
                {
                    "_id": "666be4c9f1b3867caf4fef2d",
                    "name": "Sheng Zhang",
                    "hidden": false
                },
                {
                    "_id": "666be4c9f1b3867caf4fef2e",
                    "user": {
                        "avatarUrl": "/avatars/1bfa6d8f82e9223b47630cefd79d7d0e.svg",
                        "isPro": false,
                        "fullname": "Hoifung Poon",
                        "user": "hoifung",
                        "type": "user"
                    },
                    "name": "Hoifung Poon",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T10:13:43.542Z",
                    "hidden": false
                },
                {
                    "_id": "666be4c9f1b3867caf4fef2f",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/666be8ef81f01fbd60e84f01/PiCUqr7XT96HGpa_GVRLr.jpeg",
                        "isPro": false,
                        "fullname": "Muhao Chen",
                        "user": "Muhao",
                        "type": "user"
                    },
                    "name": "Muhao Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-14T07:21:36.017Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-13T17:59:52.000Z",
            "title": "MuirBench: A Comprehensive Benchmark for Robust Multi-image\n  Understanding",
            "summary": "We introduce MuirBench, a comprehensive benchmark that focuses on robust\nmulti-image understanding capabilities of multimodal LLMs. MuirBench consists\nof 12 diverse multi-image tasks (e.g., scene understanding, ordering) that\ninvolve 10 categories of multi-image relations (e.g., multiview, temporal\nrelations). Comprising 11,264 images and 2,600 multiple-choice questions,\nMuirBench is created in a pairwise manner, where each standard instance is\npaired with an unanswerable variant that has minimal semantic differences, in\norder for a reliable assessment. Evaluated upon 20 recent multi-modal LLMs, our\nresults reveal that even the best-performing models like GPT-4o and Gemini Pro\nfind it challenging to solve MuirBench, achieving 68.0% and 49.3% in accuracy.\nOpen-source multimodal LLMs trained on single images can hardly generalize to\nmulti-image questions, hovering below 33.3% in accuracy. These results\nhighlight the importance of MuirBench in encouraging the community to develop\nmultimodal LLMs that can look beyond a single image, suggesting potential\npathways for future improvements.",
            "upvotes": 11
        },
        "publishedAt": "2024-06-14T05:06:26.856Z",
        "title": "MuirBench: A Comprehensive Benchmark for Robust Multi-image Understanding",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.09411.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "/avatars/4d29cef783603dc8ba123f01cde93ad8.svg",
            "fullname": "Xingyu Fu",
            "name": "Fiaa",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.07522",
            "authors": [
                {
                    "_id": "66690da983408bb0da3535da",
                    "user": {
                        "avatarUrl": "/avatars/3419b239d42e091586f1c51b526d88e5.svg",
                        "isPro": false,
                        "fullname": "Liliang Ren",
                        "user": "renll",
                        "type": "user"
                    },
                    "name": "Liliang Ren",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-12T07:29:37.990Z",
                    "hidden": false
                },
                {
                    "_id": "66690da983408bb0da3535db",
                    "name": "Yang Liu",
                    "hidden": false
                },
                {
                    "_id": "66690da983408bb0da3535dc",
                    "name": "Yadong Lu",
                    "hidden": false
                },
                {
                    "_id": "66690da983408bb0da3535dd",
                    "name": "Yelong Shen",
                    "hidden": false
                },
                {
                    "_id": "66690da983408bb0da3535de",
                    "name": "Chen Liang",
                    "hidden": false
                },
                {
                    "_id": "66690da983408bb0da3535df",
                    "name": "Weizhu Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-11T17:50:51.000Z",
            "title": "Samba: Simple Hybrid State Space Models for Efficient Unlimited Context\n  Language Modeling",
            "summary": "Efficiently modeling sequences with infinite context length has been a\nlong-standing problem. Past works suffer from either the quadratic computation\ncomplexity or the limited extrapolation ability on length generalization. In\nthis work, we present Samba, a simple hybrid architecture that layer-wise\ncombines Mamba, a selective State Space Model (SSM), with Sliding Window\nAttention (SWA). Samba selectively compresses a given sequence into recurrent\nhidden states while still maintaining the ability to precisely recall memories\nwith the attention mechanism. We scale Samba up to 3.8B parameters with 3.2T\ntraining tokens and show that Samba substantially outperforms the\nstate-of-the-art models based on pure attention or SSMs on a wide range of\nbenchmarks. When trained on 4K length sequences, Samba can be efficiently\nextrapolated to 256K context length with perfect memory recall and show\nimproved token predictions up to 1M context length. As a linear-time sequence\nmodel, Samba enjoys a 3.73x higher throughput compared to Transformers with\ngrouped-query attention when processing user prompts of 128K length, and 3.64x\nspeedup when generating 64K tokens with unlimited streaming. A sample\nimplementation of Samba is publicly available in\nhttps://github.com/microsoft/Samba.",
            "upvotes": 11
        },
        "publishedAt": "2024-06-14T03:32:53.871Z",
        "title": "Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.07522.png",
        "numComments": 3,
        "submittedBy": {
            "avatarUrl": "/avatars/3419b239d42e091586f1c51b526d88e5.svg",
            "fullname": "Liliang Ren",
            "name": "renll",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.09403",
            "authors": [
                {
                    "_id": "666bb62ed0e6a329a3e46a1d",
                    "user": {
                        "avatarUrl": "/avatars/d7308899b46232cad4a48a0e876449a8.svg",
                        "isPro": false,
                        "fullname": "Yushi Hu",
                        "user": "yushihu",
                        "type": "user"
                    },
                    "name": "Yushi Hu",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2024-06-14T03:17:09.471Z",
                    "hidden": false
                },
                {
                    "_id": "666bb62ed0e6a329a3e46a1e",
                    "user": {
                        "avatarUrl": "/avatars/c55938df5bce82b5d96e592a1ec36a8b.svg",
                        "isPro": false,
                        "fullname": "Weijia Shi",
                        "user": "swj0419",
                        "type": "user"
                    },
                    "name": "Weijia Shi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T10:06:58.721Z",
                    "hidden": false
                },
                {
                    "_id": "666bb62ed0e6a329a3e46a1f",
                    "user": {
                        "avatarUrl": "/avatars/4d29cef783603dc8ba123f01cde93ad8.svg",
                        "isPro": false,
                        "fullname": "Xingyu Fu",
                        "user": "Fiaa",
                        "type": "user"
                    },
                    "name": "Xingyu Fu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T10:07:13.871Z",
                    "hidden": false
                },
                {
                    "_id": "666bb62ed0e6a329a3e46a20",
                    "user": {
                        "avatarUrl": "/avatars/d7eadb22aa3d8a067189b924767864e9.svg",
                        "isPro": false,
                        "fullname": "Dan Roth",
                        "user": "danielpaulroth",
                        "type": "user"
                    },
                    "name": "Dan Roth",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T10:07:21.248Z",
                    "hidden": false
                },
                {
                    "_id": "666bb62ed0e6a329a3e46a21",
                    "name": "Mari Ostendorf",
                    "hidden": false
                },
                {
                    "_id": "666bb62ed0e6a329a3e46a22",
                    "name": "Luke Zettlemoyer",
                    "hidden": false
                },
                {
                    "_id": "666bb62ed0e6a329a3e46a23",
                    "name": "Noah A Smith",
                    "hidden": false
                },
                {
                    "_id": "666bb62ed0e6a329a3e46a24",
                    "user": {
                        "avatarUrl": "/avatars/170e0daa454838deee2bf946f7118651.svg",
                        "isPro": false,
                        "fullname": "Ranjay Krishna",
                        "user": "ranjaykrishna",
                        "type": "user"
                    },
                    "name": "Ranjay Krishna",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T10:07:42.545Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-13T17:59:31.000Z",
            "title": "Visual Sketchpad: Sketching as a Visual Chain of Thought for Multimodal\n  Language Models",
            "summary": "Humans draw to facilitate reasoning: we draw auxiliary lines when solving\ngeometry problems; we mark and circle when reasoning on maps; we use sketches\nto amplify our ideas and relieve our limited-capacity working memory. However,\nsuch actions are missing in current multimodal language models (LMs). Current\nchain-of-thought and tool-use paradigms only use text as intermediate reasoning\nsteps. In this work, we introduce Sketchpad, a framework that gives multimodal\nLMs a visual sketchpad and tools to draw on the sketchpad. The LM conducts\nplanning and reasoning according to the visual artifacts it has drawn.\nDifferent from prior work, which uses text-to-image models to enable LMs to\ndraw, Sketchpad enables LMs to draw with lines, boxes, marks, etc., which is\ncloser to human sketching and better facilitates reasoning. Sketchpad can also\nuse specialist vision models during the sketching process (e.g., draw bounding\nboxes with object detection models, draw masks with segmentation models), to\nfurther enhance visual perception and reasoning. We experiment with a wide\nrange of math tasks (including geometry, functions, graphs, and chess) and\ncomplex visual reasoning tasks. Sketchpad substantially improves performance on\nall tasks over strong base models with no sketching, yielding an average gain\nof 12.7% on math tasks, and 8.6% on vision tasks. GPT-4o with Sketchpad sets a\nnew state of the art on all tasks, including V*Bench (80.3%), BLINK spatial\nreasoning (83.9%), and visual correspondence (80.8%). All codes and data are in\nhttps://visualsketchpad.github.io/.",
            "upvotes": 10
        },
        "publishedAt": "2024-06-14T01:47:23.959Z",
        "title": "Visual Sketchpad: Sketching as a Visual Chain of Thought for Multimodal Language Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.09403.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/4d29cef783603dc8ba123f01cde93ad8.svg",
            "fullname": "Xingyu Fu",
            "name": "Fiaa",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.08673",
            "authors": [
                {
                    "_id": "666ba265fba6c458fcee3cf6",
                    "user": {
                        "avatarUrl": "/avatars/3fa9bee2e9e211723fcf44bddc6110c2.svg",
                        "isPro": false,
                        "fullname": "Zhilin Wang",
                        "user": "zhilinw",
                        "type": "user"
                    },
                    "name": "Zhilin Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T12:09:11.245Z",
                    "hidden": false
                },
                {
                    "_id": "666ba265fba6c458fcee3cf7",
                    "name": "Yi Dong",
                    "hidden": false
                },
                {
                    "_id": "666ba265fba6c458fcee3cf8",
                    "user": {
                        "avatarUrl": "/avatars/ec569729870d7392e806e59a02f37d0c.svg",
                        "isPro": false,
                        "fullname": "Olivier Delalleau",
                        "user": "odelalleau",
                        "type": "user"
                    },
                    "name": "Olivier Delalleau",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T12:09:45.540Z",
                    "hidden": false
                },
                {
                    "_id": "666ba265fba6c458fcee3cf9",
                    "name": "Jiaqi Zeng",
                    "hidden": false
                },
                {
                    "_id": "666ba265fba6c458fcee3cfa",
                    "user": {
                        "avatarUrl": "/avatars/e36f11c962f52e783b4677ca5460c41e.svg",
                        "isPro": false,
                        "fullname": "Gerald Shen",
                        "user": "gshennvm",
                        "type": "user"
                    },
                    "name": "Gerald Shen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T12:10:12.778Z",
                    "hidden": false
                },
                {
                    "_id": "666ba265fba6c458fcee3cfb",
                    "user": {
                        "avatarUrl": "/avatars/f26eb927e803f1ea05d8ae9abfdd2846.svg",
                        "isPro": false,
                        "fullname": "Daniel Egert",
                        "user": "trias702",
                        "type": "user"
                    },
                    "name": "Daniel Egert",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T12:10:24.243Z",
                    "hidden": false
                },
                {
                    "_id": "666ba265fba6c458fcee3cfc",
                    "name": "Jimmy J. Zhang",
                    "hidden": false
                },
                {
                    "_id": "666ba265fba6c458fcee3cfd",
                    "user": {
                        "avatarUrl": "/avatars/9f20fcf2b8bfc75c56c510e7bda4dbd7.svg",
                        "isPro": false,
                        "fullname": "Makesh Sreedhar",
                        "user": "makeshn",
                        "type": "user"
                    },
                    "name": "Makesh Narsimhan Sreedhar",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T12:11:05.118Z",
                    "hidden": false
                },
                {
                    "_id": "666ba265fba6c458fcee3cfe",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1649475932211-622937a4acd5bef90e55c49d.jpeg",
                        "isPro": false,
                        "fullname": "Oleksii Kuchaiev",
                        "user": "okuchaiev",
                        "type": "user"
                    },
                    "name": "Oleksii Kuchaiev",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T12:11:10.896Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-12T22:28:08.000Z",
            "title": "HelpSteer2: Open-source dataset for training top-performing reward\n  models",
            "summary": "High-quality preference datasets are essential for training reward models\nthat can effectively guide large language models (LLMs) in generating\nhigh-quality responses aligned with human preferences. As LLMs become stronger\nand better aligned, permissively licensed preference datasets, such as Open\nAssistant, HH-RLHF, and HelpSteer need to be updated to remain effective for\nreward modeling. Methods that distil preference data from proprietary LLMs such\nas GPT-4 have restrictions on commercial usage imposed by model providers. To\nimprove upon both generated responses and attribute labeling quality, we\nrelease HelpSteer2, a permissively licensed preference dataset (CC-BY-4.0).\nUsing a powerful internal base model trained on HelpSteer2, we are able to\nachieve the SOTA score (92.0%) on Reward-Bench's primary dataset, outperforming\ncurrently listed open and proprietary models, as of June 12th, 2024. Notably,\nHelpSteer2 consists of only ten thousand response pairs, an order of magnitude\nfewer than existing preference datasets (e.g., HH-RLHF), which makes it highly\nefficient for training reward models. Our extensive experiments demonstrate\nthat reward models trained with HelpSteer2 are effective in aligning LLMs. In\nparticular, we propose SteerLM 2.0, a model alignment approach that can\neffectively make use of the rich multi-attribute score predicted by our reward\nmodels. HelpSteer2 is available at\nhttps://huggingface.co/datasets/nvidia/HelpSteer2 and code is available at\nhttps://github.com/NVIDIA/NeMo-Aligner",
            "upvotes": 9
        },
        "publishedAt": "2024-06-14T00:31:19.600Z",
        "title": "HelpSteer2: Open-source dataset for training top-performing reward models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.08673.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.08587",
            "authors": [
                {
                    "_id": "666bb1e1fba6c458fcf39d70",
                    "name": "Xiaoshuai Song",
                    "hidden": false
                },
                {
                    "_id": "666bb1e1fba6c458fcf39d71",
                    "name": "Muxi Diao",
                    "hidden": false
                },
                {
                    "_id": "666bb1e1fba6c458fcf39d72",
                    "name": "Guanting Dong",
                    "hidden": false
                },
                {
                    "_id": "666bb1e1fba6c458fcf39d73",
                    "name": "Zhengyang Wang",
                    "hidden": false
                },
                {
                    "_id": "666bb1e1fba6c458fcf39d74",
                    "name": "Yujia Fu",
                    "hidden": false
                },
                {
                    "_id": "666bb1e1fba6c458fcf39d75",
                    "name": "Runqi Qiao",
                    "hidden": false
                },
                {
                    "_id": "666bb1e1fba6c458fcf39d76",
                    "name": "Zhexu Wang",
                    "hidden": false
                },
                {
                    "_id": "666bb1e1fba6c458fcf39d77",
                    "user": {
                        "avatarUrl": "/avatars/36135ca35bbd4baca37a9092f3e2d82c.svg",
                        "isPro": false,
                        "fullname": "dayuanfu",
                        "user": "fudayuan",
                        "type": "user"
                    },
                    "name": "Dayuan Fu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T12:01:07.517Z",
                    "hidden": false
                },
                {
                    "_id": "666bb1e1fba6c458fcf39d78",
                    "name": "Huangxuan Wu",
                    "hidden": false
                },
                {
                    "_id": "666bb1e1fba6c458fcf39d79",
                    "name": "Bin Liang",
                    "hidden": false
                },
                {
                    "_id": "666bb1e1fba6c458fcf39d7a",
                    "name": "Weihao Zeng",
                    "hidden": false
                },
                {
                    "_id": "666bb1e1fba6c458fcf39d7b",
                    "name": "Yejie Wang",
                    "hidden": false
                },
                {
                    "_id": "666bb1e1fba6c458fcf39d7c",
                    "name": "Zhuoma GongQue",
                    "hidden": false
                },
                {
                    "_id": "666bb1e1fba6c458fcf39d7d",
                    "name": "Jianing Yu",
                    "hidden": false
                },
                {
                    "_id": "666bb1e1fba6c458fcf39d7e",
                    "name": "Qiuna Tan",
                    "hidden": false
                },
                {
                    "_id": "666bb1e1fba6c458fcf39d7f",
                    "name": "Weiran Xu",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-12T18:47:28.000Z",
            "title": "CS-Bench: A Comprehensive Benchmark for Large Language Models towards\n  Computer Science Mastery",
            "summary": "Computer Science (CS) stands as a testament to the intricacies of human\nintelligence, profoundly advancing the development of artificial intelligence\nand modern society. However, the current community of large language models\n(LLMs) overly focuses on benchmarks for analyzing specific foundational skills\n(e.g. mathematics and code generation), neglecting an all-round evaluation of\nthe computer science field. To bridge this gap, we introduce CS-Bench, the\nfirst bilingual (Chinese-English) benchmark dedicated to evaluating the\nperformance of LLMs in computer science. CS-Bench comprises approximately 5K\nmeticulously curated test samples, covering 26 subfields across 4 key areas of\ncomputer science, encompassing various task forms and divisions of knowledge\nand reasoning. Utilizing CS-Bench, we conduct a comprehensive evaluation of\nover 30 mainstream LLMs, revealing the relationship between CS performance and\nmodel scales. We also quantitatively analyze the reasons for failures in\nexisting LLMs and highlight directions for improvements, including knowledge\nsupplementation and CS-specific reasoning. Further cross-capability experiments\nshow a high correlation between LLMs' capabilities in computer science and\ntheir abilities in mathematics and coding. Moreover, expert LLMs specialized in\nmathematics and coding also demonstrate strong performances in several CS\nsubfields. Looking ahead, we envision CS-Bench serving as a cornerstone for LLM\napplications in the CS field and paving new avenues in assessing LLMs' diverse\nreasoning capabilities. The CS-Bench data and evaluation code are available at\nhttps://github.com/csbench/csbench.",
            "upvotes": 8
        },
        "publishedAt": "2024-06-14T01:28:47.066Z",
        "title": "CS-Bench: A Comprehensive Benchmark for Large Language Models towards Computer Science Mastery",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.08587.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.09308",
            "authors": [
                {
                    "_id": "666ba87cfa1b45be6fea76e6",
                    "name": "Wilfried Bounsi",
                    "hidden": false
                },
                {
                    "_id": "666ba87cfa1b45be6fea76e7",
                    "name": "Borja Ibarz",
                    "hidden": false
                },
                {
                    "_id": "666ba87cfa1b45be6fea76e8",
                    "name": "Andrew Dudzik",
                    "hidden": false
                },
                {
                    "_id": "666ba87cfa1b45be6fea76e9",
                    "name": "Jessica B. Hamrick",
                    "hidden": false
                },
                {
                    "_id": "666ba87cfa1b45be6fea76ea",
                    "name": "Larisa Markeeva",
                    "hidden": false
                },
                {
                    "_id": "666ba87cfa1b45be6fea76eb",
                    "name": "Alex Vitvitskyi",
                    "hidden": false
                },
                {
                    "_id": "666ba87cfa1b45be6fea76ec",
                    "name": "Razvan Pascanu",
                    "hidden": false
                },
                {
                    "_id": "666ba87cfa1b45be6fea76ed",
                    "name": "Petar Velikovi",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-13T16:42:06.000Z",
            "title": "Transformers meet Neural Algorithmic Reasoners",
            "summary": "Transformers have revolutionized machine learning with their simple yet\neffective architecture. Pre-training Transformers on massive text datasets from\nthe Internet has led to unmatched generalization for natural language\nunderstanding (NLU) tasks. However, such language models remain fragile when\ntasked with algorithmic forms of reasoning, where computations must be precise\nand robust. To address this limitation, we propose a novel approach that\ncombines the Transformer's language understanding with the robustness of graph\nneural network (GNN)-based neural algorithmic reasoners (NARs). Such NARs\nproved effective as generic solvers for algorithmic tasks, when specified in\ngraph form. To make their embeddings accessible to a Transformer, we propose a\nhybrid architecture with a two-phase training procedure, allowing the tokens in\nthe language model to cross-attend to the node embeddings from the NAR. We\nevaluate our resulting TransNAR model on CLRS-Text, the text-based version of\nthe CLRS-30 benchmark, and demonstrate significant gains over Transformer-only\nmodels for algorithmic reasoning, both in and out of distribution.",
            "upvotes": 8
        },
        "publishedAt": "2024-06-14T00:48:59.260Z",
        "title": "Transformers meet Neural Algorithmic Reasoners",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.09308.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.08862",
            "authors": [
                {
                    "_id": "666ba21cd3dbd0378e786601",
                    "user": {
                        "avatarUrl": "/avatars/3ba871408a9f2b465063b5d2441f393b.svg",
                        "isPro": false,
                        "fullname": "Alexi Gladstone",
                        "user": "alexiglad",
                        "type": "user"
                    },
                    "name": "Alexi Gladstone",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2024-06-14T01:59:28.888Z",
                    "hidden": false
                },
                {
                    "_id": "666ba21cd3dbd0378e786602",
                    "user": {
                        "avatarUrl": "/avatars/7b6ac7b1ff061f392fccf4af66f831d2.svg",
                        "isPro": false,
                        "fullname": "Ganesh Nanduru",
                        "user": "gnanduru",
                        "type": "user"
                    },
                    "name": "Ganesh Nanduru",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T11:55:39.526Z",
                    "hidden": false
                },
                {
                    "_id": "666ba21cd3dbd0378e786603",
                    "user": {
                        "avatarUrl": "/avatars/c623abb79f6dc48b54f2fb8ba64bb4a4.svg",
                        "isPro": false,
                        "fullname": "Md Mofijul Islam",
                        "user": "mmiakashs",
                        "type": "user"
                    },
                    "name": "Md Mofijul Islam",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T11:55:45.553Z",
                    "hidden": false
                },
                {
                    "_id": "666ba21cd3dbd0378e786604",
                    "user": {
                        "avatarUrl": "/avatars/1fac0f575cb6e038aa0fff95eafd0d76.svg",
                        "isPro": false,
                        "fullname": "Aman Chadha",
                        "user": "amanchadha",
                        "type": "user"
                    },
                    "name": "Aman Chadha",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T11:55:50.871Z",
                    "hidden": false
                },
                {
                    "_id": "666ba21cd3dbd0378e786605",
                    "name": "Jundong Li",
                    "hidden": false
                },
                {
                    "_id": "666ba21cd3dbd0378e786606",
                    "name": "Tariq Iqbal",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-13T06:54:37.000Z",
            "title": "Cognitively Inspired Energy-Based World Models",
            "summary": "One of the predominant methods for training world models is autoregressive\nprediction in the output space of the next element of a sequence. In Natural\nLanguage Processing (NLP), this takes the form of Large Language Models (LLMs)\npredicting the next token; in Computer Vision (CV), this takes the form of\nautoregressive models predicting the next frame/token/pixel. However, this\napproach differs from human cognition in several respects. First, human\npredictions about the future actively influence internal cognitive processes.\nSecond, humans naturally evaluate the plausibility of predictions regarding\nfuture states. Based on this capability, and third, by assessing when\npredictions are sufficient, humans allocate a dynamic amount of time to make a\nprediction. This adaptive process is analogous to System 2 thinking in\npsychology. All these capabilities are fundamental to the success of humans at\nhigh-level reasoning and planning. Therefore, to address the limitations of\ntraditional autoregressive models lacking these human-like capabilities, we\nintroduce Energy-Based World Models (EBWM). EBWM involves training an\nEnergy-Based Model (EBM) to predict the compatibility of a given context and a\npredicted future state. In doing so, EBWM enables models to achieve all three\nfacets of human cognition described. Moreover, we developed a variant of the\ntraditional autoregressive transformer tailored for Energy-Based models, termed\nthe Energy-Based Transformer (EBT). Our results demonstrate that EBWM scales\nbetter with data and GPU Hours than traditional autoregressive transformers in\nCV, and that EBWM offers promising early scaling in NLP. Consequently, this\napproach offers an exciting path toward training future models capable of\nSystem 2 thinking and intelligently searching across state spaces.",
            "upvotes": 7
        },
        "publishedAt": "2024-06-14T00:33:24.491Z",
        "title": "Cognitively Inspired Energy-Based World Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.08862.png",
        "numComments": 3,
        "submittedBy": {
            "avatarUrl": "/avatars/3ba871408a9f2b465063b5d2441f393b.svg",
            "fullname": "Alexi Gladstone",
            "name": "alexiglad",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.09162",
            "authors": [
                {
                    "_id": "666ba936435a489ffd15a8f2",
                    "user": {
                        "avatarUrl": "/avatars/8277e7c13f80973bdb9a2dae36e6e728.svg",
                        "isPro": false,
                        "fullname": "Han Yucheng",
                        "user": "listen2you",
                        "type": "user"
                    },
                    "name": "Yucheng Han",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-14T07:22:43.785Z",
                    "hidden": false
                },
                {
                    "_id": "666ba936435a489ffd15a8f3",
                    "user": {
                        "avatarUrl": "/avatars/e8e95ab0d03f58c61846ccc1318502fe.svg",
                        "isPro": false,
                        "fullname": "Rui Wang",
                        "user": "budui",
                        "type": "user"
                    },
                    "name": "Rui Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-14T07:22:46.011Z",
                    "hidden": false
                },
                {
                    "_id": "666ba936435a489ffd15a8f4",
                    "name": "Chi Zhang",
                    "hidden": false
                },
                {
                    "_id": "666ba936435a489ffd15a8f5",
                    "user": {
                        "avatarUrl": "/avatars/a122c2e56e4927f9e45f125d51d3d836.svg",
                        "isPro": false,
                        "fullname": "Juntao Hu",
                        "user": "TBI805",
                        "type": "user"
                    },
                    "name": "Juntao Hu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T11:49:22.802Z",
                    "hidden": false
                },
                {
                    "_id": "666ba936435a489ffd15a8f6",
                    "name": "Pei Cheng",
                    "hidden": false
                },
                {
                    "_id": "666ba936435a489ffd15a8f7",
                    "name": "Bin Fu",
                    "hidden": false
                },
                {
                    "_id": "666ba936435a489ffd15a8f8",
                    "name": "Hanwang Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-13T14:26:43.000Z",
            "title": "EMMA: Your Text-to-Image Diffusion Model Can Secretly Accept Multi-Modal\n  Prompts",
            "summary": "Recent advancements in image generation have enabled the creation of\nhigh-quality images from text conditions. However, when facing multi-modal\nconditions, such as text combined with reference appearances, existing methods\nstruggle to balance multiple conditions effectively, typically showing a\npreference for one modality over others. To address this challenge, we\nintroduce EMMA, a novel image generation model accepting multi-modal prompts\nbuilt upon the state-of-the-art text-to-image (T2I) diffusion model, ELLA. EMMA\nseamlessly incorporates additional modalities alongside text to guide image\ngeneration through an innovative Multi-modal Feature Connector design, which\neffectively integrates textual and supplementary modal information using a\nspecial attention mechanism. By freezing all parameters in the original T2I\ndiffusion model and only adjusting some additional layers, we reveal an\ninteresting finding that the pre-trained T2I diffusion model can secretly\naccept multi-modal prompts. This interesting property facilitates easy\nadaptation to different existing frameworks, making EMMA a flexible and\neffective tool for producing personalized and context-aware images and even\nvideos. Additionally, we introduce a strategy to assemble learned EMMA modules\nto produce images conditioned on multiple modalities simultaneously,\neliminating the need for additional training with mixed multi-modal prompts.\nExtensive experiments demonstrate the effectiveness of EMMA in maintaining high\nfidelity and detail in generated images, showcasing its potential as a robust\nsolution for advanced multi-modal conditional image generation tasks.",
            "upvotes": 6
        },
        "publishedAt": "2024-06-14T04:39:49.544Z",
        "title": "EMMA: Your Text-to-Image Diffusion Model Can Secretly Accept Multi-Modal Prompts",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64196320ed725fef64419c2a/vAZJ3YjN0rvRSopbQ18_I.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.09162.png",
        "numComments": 3,
        "submittedBy": {
            "avatarUrl": "/avatars/96feb22fb5e8931d6c9e0ea06148266f.svg",
            "fullname": "Chi Zhang",
            "name": "DrChiZhang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.08656",
            "authors": [
                {
                    "_id": "666bbcbcd85fde8ee602af1d",
                    "user": {
                        "avatarUrl": "/avatars/6aae6282ddee96b899b7384154e9b0a0.svg",
                        "isPro": false,
                        "fullname": "Weixi Feng",
                        "user": "weixifeng",
                        "type": "user"
                    },
                    "name": "Weixi Feng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-14T07:22:19.259Z",
                    "hidden": false
                },
                {
                    "_id": "666bbcbcd85fde8ee602af1e",
                    "name": "Jiachen Li",
                    "hidden": false
                },
                {
                    "_id": "666bbcbcd85fde8ee602af1f",
                    "name": "Michael Saxon",
                    "hidden": false
                },
                {
                    "_id": "666bbcbcd85fde8ee602af20",
                    "name": "Tsu-jui Fu",
                    "hidden": false
                },
                {
                    "_id": "666bbcbcd85fde8ee602af21",
                    "name": "Wenhu Chen",
                    "hidden": false
                },
                {
                    "_id": "666bbcbcd85fde8ee602af22",
                    "name": "William Yang Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-12T21:41:32.000Z",
            "title": "TC-Bench: Benchmarking Temporal Compositionality in Text-to-Video and\n  Image-to-Video Generation",
            "summary": "Video generation has many unique challenges beyond those of image generation.\nThe temporal dimension introduces extensive possible variations across frames,\nover which consistency and continuity may be violated. In this study, we move\nbeyond evaluating simple actions and argue that generated videos should\nincorporate the emergence of new concepts and their relation transitions like\nin real-world videos as time progresses. To assess the Temporal\nCompositionality of video generation models, we propose TC-Bench, a benchmark\nof meticulously crafted text prompts, corresponding ground truth videos, and\nrobust evaluation metrics. The prompts articulate the initial and final states\nof scenes, effectively reducing ambiguities for frame development and\nsimplifying the assessment of transition completion. In addition, by collecting\naligned real-world videos corresponding to the prompts, we expand TC-Bench's\napplicability from text-conditional models to image-conditional ones that can\nperform generative frame interpolation. We also develop new metrics to measure\nthe completeness of component transitions in generated videos, which\ndemonstrate significantly higher correlations with human judgments than\nexisting metrics. Our comprehensive experimental results reveal that most video\ngenerators achieve less than 20% of the compositional changes, highlighting\nenormous space for future improvement. Our analysis indicates that current\nvideo generation models struggle to interpret descriptions of compositional\nchanges and synthesize various components across different time steps.",
            "upvotes": 6
        },
        "publishedAt": "2024-06-14T02:18:11.398Z",
        "title": "TC-Bench: Benchmarking Temporal Compositionality in Text-to-Video and Image-to-Video Generation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.08656.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/6aae6282ddee96b899b7384154e9b0a0.svg",
            "fullname": "Weixi Feng",
            "name": "weixifeng",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.08707",
            "authors": [
                {
                    "_id": "666c0144623133f1ce4c4dd1",
                    "user": {
                        "avatarUrl": "/avatars/5748195a0ac761e2776548aabc3a53e3.svg",
                        "isPro": false,
                        "fullname": "Matthieu Futeral",
                        "user": "matthieufp",
                        "type": "user"
                    },
                    "name": "Matthieu Futeral",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2024-06-14T08:49:17.543Z",
                    "hidden": false
                },
                {
                    "_id": "666c0144623133f1ce4c4dd2",
                    "name": "Armel Zebaze",
                    "hidden": false
                },
                {
                    "_id": "666c0144623133f1ce4c4dd3",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1632322117476-602705869cad998710e2e8ab.jpeg",
                        "isPro": false,
                        "fullname": "Pedro Ortiz Suarez",
                        "user": "pjox",
                        "type": "user"
                    },
                    "name": "Pedro Ortiz Suarez",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T11:31:24.893Z",
                    "hidden": false
                },
                {
                    "_id": "666c0144623133f1ce4c4dd4",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1632326156496-614b51fbf2a3f96861e548ae.jpeg",
                        "isPro": false,
                        "fullname": "Julien Abadji",
                        "user": "uj",
                        "type": "user"
                    },
                    "name": "Julien Abadji",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T11:31:32.596Z",
                    "hidden": false
                },
                {
                    "_id": "666c0144623133f1ce4c4dd5",
                    "user": {
                        "avatarUrl": "/avatars/67aea8a53278c99126932c9c4fbee246.svg",
                        "isPro": false,
                        "fullname": "Rmi Lacroix",
                        "user": "rlacroix",
                        "type": "user"
                    },
                    "name": "Rmi Lacroix",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T11:31:38.914Z",
                    "hidden": false
                },
                {
                    "_id": "666c0144623133f1ce4c4dd6",
                    "name": "Cordelia Schmid",
                    "hidden": false
                },
                {
                    "_id": "666c0144623133f1ce4c4dd7",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1653912785529-613a0c85e63aa832a98cecc4.png",
                        "isPro": false,
                        "fullname": "Rachel Bawden",
                        "user": "rbawden",
                        "type": "user"
                    },
                    "name": "Rachel Bawden",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T11:31:47.884Z",
                    "hidden": false
                },
                {
                    "_id": "666c0144623133f1ce4c4dd8",
                    "user": {
                        "avatarUrl": "/avatars/ef534712ca682bf74ec7eef17d3d1b5f.svg",
                        "isPro": false,
                        "fullname": "Benot Sagot",
                        "user": "sagot",
                        "type": "user"
                    },
                    "name": "Benot Sagot",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T11:31:55.196Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-13T00:13:32.000Z",
            "title": "mOSCAR: A Large-scale Multilingual and Multimodal Document-level Corpus",
            "summary": "Multimodal Large Language Models (mLLMs) are trained on a large amount of\ntext-image data. While most mLLMs are trained on caption-like data only,\nAlayrac et al. [2022] showed that additionally training them on interleaved\nsequences of text and images can lead to the emergence of in-context learning\ncapabilities. However, the dataset they used, M3W, is not public and is only in\nEnglish. There have been attempts to reproduce their results but the released\ndatasets are English-only. In contrast, current multilingual and multimodal\ndatasets are either composed of caption-like only or medium-scale or fully\nprivate data. This limits mLLM research for the 7,000 other languages spoken in\nthe world. We therefore introduce mOSCAR, to the best of our knowledge the\nfirst large-scale multilingual and multimodal document corpus crawled from the\nweb. It covers 163 languages, 315M documents, 214B tokens and 1.2B images. We\ncarefully conduct a set of filtering and evaluation steps to make sure mOSCAR\nis sufficiently safe, diverse and of good quality. We additionally train two\ntypes of multilingual model to prove the benefits of mOSCAR: (1) a model\ntrained on a subset of mOSCAR and captioning data and (2) a model train on\ncaptioning data only. The model additionally trained on mOSCAR shows a strong\nboost in few-shot learning performance across various multilingual image-text\ntasks and benchmarks, confirming previous findings for English-only mLLMs.",
            "upvotes": 5
        },
        "publishedAt": "2024-06-14T07:20:54.610Z",
        "title": "mOSCAR: A Large-scale Multilingual and Multimodal Document-level Corpus",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.08707.png",
        "numComments": 3,
        "submittedBy": {
            "avatarUrl": "/avatars/5748195a0ac761e2776548aabc3a53e3.svg",
            "fullname": "Matthieu Futeral",
            "name": "matthieufp",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.09412",
            "authors": [
                {
                    "_id": "666bada43637dcab935c1fce",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63176933b58b0184630d2c74/53b5EASwW76zeyyqeJA3O.jpeg",
                        "isPro": false,
                        "fullname": "Yiyuan Zhang",
                        "user": "Yiyuan",
                        "type": "user"
                    },
                    "name": "Yiyuan Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T11:42:10.249Z",
                    "hidden": false
                },
                {
                    "_id": "666bada43637dcab935c1fcf",
                    "name": "Handong Li",
                    "hidden": false
                },
                {
                    "_id": "666bada43637dcab935c1fd0",
                    "name": "Jing Liu",
                    "hidden": false
                },
                {
                    "_id": "666bada43637dcab935c1fd1",
                    "name": "Xiangyu Yue",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-13T17:59:53.000Z",
            "title": "Explore the Limits of Omni-modal Pretraining at Scale",
            "summary": "We propose to build omni-modal intelligence, which is capable of\nunderstanding any modality and learning universal representations. In specific,\nwe propose a scalable pretraining paradigm, named Multimodal Context (MiCo),\nwhich can scale up the numbers of modalities and amount of data, together with\nthe model parameters, in the pretraining process. With MiCo, the pretrained\nmodels show significant emergent abilities in multimodal learning, which are\nevaluated on the following tasks: i) single-modality perception benchmarks of\n10 different modalities, ii) 25 cross-modality understanding tasks of\nretrieval, question-answering, captioning, and iii) 18 multimodal large\nlanguage model benchmarks. Our models establish 37 new records for\nstate-of-the-art performance. We hope that our research could contribute to the\ndevelopment of omni-modal intelligence. Code and Models are at\nhttps://github.com/invictus717/MiCo",
            "upvotes": 5
        },
        "publishedAt": "2024-06-14T01:11:07.585Z",
        "title": "Explore the Limits of Omni-modal Pretraining at Scale",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.09412.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63176933b58b0184630d2c74/53b5EASwW76zeyyqeJA3O.jpeg",
            "fullname": "Yiyuan Zhang",
            "name": "Yiyuan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.08657",
            "authors": [
                {
                    "_id": "666b9e39c636d47bd57e7ff4",
                    "name": "Chen Zheng",
                    "hidden": false
                },
                {
                    "_id": "666b9e39c636d47bd57e7ff5",
                    "name": "Ke Sun",
                    "hidden": false
                },
                {
                    "_id": "666b9e39c636d47bd57e7ff6",
                    "user": {
                        "avatarUrl": "/avatars/b1a4dad90afae5c00df97233a97777db.svg",
                        "isPro": false,
                        "fullname": "xunzhou",
                        "user": "xunzhou",
                        "type": "user"
                    },
                    "name": "Xun Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T11:40:08.786Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-12T21:42:13.000Z",
            "title": "Mistral-C2F: Coarse to Fine Actor for Analytical and Reasoning\n  Enhancement in RLHF and Effective-Merged LLMs",
            "summary": "Despite the advances in Large Language Models (LLMs), exemplified by models\nlike GPT-4 and Claude, smaller-scale LLMs such as Llama and Mistral often\nstruggle with generating in-depth and coherent dialogues. This paper presents a\nnovel two-step Coarse-to-Fine Actor model to address the inherent limitations\nin conversational and analytical capabilities of small-sized LLMs. Our approach\nbegins with the Policy-based Coarse Actor, employing a technique we term\n\"Continuous Maximization\". The Coarse Actor establishes an enhanced,\nknowledge-rich pool adept at aligning with human preference styles in analysis\nand reasoning. Through the RLHF process, it employs Continuous Maximization, a\nstrategy that dynamically and adaptively extends the output length limit,\nenabling the generation of more detailed and analytical content. Subsequently,\nthe Fine Actor refines this analytical content, addressing the generation of\nexcessively redundant information from the Coarse Actor. We introduce a\n\"Knowledge Residue Merger\" approach, refining the content from the Coarse Actor\nand merging it with an existing Instruction model to improve quality,\ncorrectness, and reduce redundancies. We applied our methodology to the popular\nMistral model, creating Mistral-C2F, which has demonstrated exceptional\nperformance across 11 general language tasks and the MT-Bench Dialogue task,\noutperforming similar-scale models and even larger models with 13B and 30B\nparameters. Our model has significantly improved conversational and analytical\nreasoning abilities.",
            "upvotes": 5
        },
        "publishedAt": "2024-06-14T00:26:55.118Z",
        "title": "Mistral-C2F: Coarse to Fine Actor for Analytical and Reasoning Enhancement in RLHF and Effective-Merged LLMs",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.08657.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.08598",
            "authors": [
                {
                    "_id": "666bcb7b3fc0651814cbfb83",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6462ac71514ee1645bd1f7f7/kw2gwI8pyp6uyT4neqV_H.png",
                        "isPro": true,
                        "fullname": "Justin Zhao",
                        "user": "justinxzhao",
                        "type": "user"
                    },
                    "name": "Justin Zhao",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2024-06-14T05:07:00.962Z",
                    "hidden": false
                },
                {
                    "_id": "666bcb7b3fc0651814cbfb84",
                    "user": {
                        "avatarUrl": "/avatars/12ac5037657cfba4e2b48b08c688a629.svg",
                        "isPro": false,
                        "fullname": "Flor Miriam Plaza-del-Arco",
                        "user": "fmplaza",
                        "type": "user"
                    },
                    "name": "Flor Miriam Plaza-del-Arco",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T11:28:12.473Z",
                    "hidden": false
                },
                {
                    "_id": "666bcb7b3fc0651814cbfb85",
                    "user": {
                        "avatarUrl": "/avatars/60710c4627e8924719f3a1c034a6c644.svg",
                        "isPro": false,
                        "fullname": "Amanda Curry",
                        "user": "amandacurry",
                        "type": "user"
                    },
                    "name": "Amanda Cercas Curry",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T11:28:21.671Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-12T19:05:43.000Z",
            "title": "Language Model Council: Benchmarking Foundation Models on Highly\n  Subjective Tasks by Consensus",
            "summary": "The rapid advancement of Large Language Models (LLMs) necessitates robust and\nchallenging benchmarks. Leaderboards like Chatbot Arena rank LLMs based on how\nwell their responses align with human preferences. However, many tasks such as\nthose related to emotional intelligence, creative writing, or persuasiveness,\nare highly subjective and often lack majoritarian human agreement. Judges may\nhave irreconcilable disagreements about what constitutes a better response. To\naddress the challenge of ranking LLMs on highly subjective tasks, we propose a\nnovel benchmarking framework, the Language Model Council (LMC). The LMC\noperates through a democratic process to: 1) formulate a test set through equal\nparticipation, 2) administer the test among council members, and 3) evaluate\nresponses as a collective jury. We deploy a council of 20 newest LLMs on an\nopen-ended emotional intelligence task: responding to interpersonal dilemmas.\nOur results show that the LMC produces rankings that are more separable,\nrobust, and less biased than those from any individual LLM judge, and is more\nconsistent with a human-established leaderboard compared to other benchmarks.",
            "upvotes": 4
        },
        "publishedAt": "2024-06-14T04:02:28.725Z",
        "title": "Language Model Council: Benchmarking Foundation Models on Highly Subjective Tasks by Consensus",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.08598.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6462ac71514ee1645bd1f7f7/kw2gwI8pyp6uyT4neqV_H.png",
            "fullname": "Justin Zhao",
            "name": "justinxzhao",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.08479",
            "authors": [
                {
                    "_id": "666b0dd1a10c1e31ff37e704",
                    "user": {
                        "avatarUrl": "/avatars/462841d3ba16d351182e9aa552c36044.svg",
                        "isPro": true,
                        "fullname": "Hanwen Jiang",
                        "user": "hwjiang",
                        "type": "user"
                    },
                    "name": "Hanwen Jiang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T11:26:03.683Z",
                    "hidden": false
                },
                {
                    "_id": "666b0dd1a10c1e31ff37e705",
                    "name": "Qixing Huang",
                    "hidden": false
                },
                {
                    "_id": "666b0dd1a10c1e31ff37e706",
                    "user": {
                        "avatarUrl": "/avatars/11e7d7a94ae26500c1c2ad62e760726f.svg",
                        "isPro": false,
                        "fullname": "Georgios Pavlakos",
                        "user": "geopavlakos",
                        "type": "user"
                    },
                    "name": "Georgios Pavlakos",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T11:26:29.655Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-12T17:59:08.000Z",
            "title": "Real3D: Scaling Up Large Reconstruction Models with Real-World Images",
            "summary": "The default strategy for training single-view Large Reconstruction Models\n(LRMs) follows the fully supervised route using large-scale datasets of\nsynthetic 3D assets or multi-view captures. Although these resources simplify\nthe training procedure, they are hard to scale up beyond the existing datasets\nand they are not necessarily representative of the real distribution of object\nshapes. To address these limitations, in this paper, we introduce Real3D, the\nfirst LRM system that can be trained using single-view real-world images.\nReal3D introduces a novel self-training framework that can benefit from both\nthe existing synthetic data and diverse single-view real images. We propose two\nunsupervised losses that allow us to supervise LRMs at the pixel- and\nsemantic-level, even for training examples without ground-truth 3D or novel\nviews. To further improve performance and scale up the image data, we develop\nan automatic data curation approach to collect high-quality examples from\nin-the-wild images. Our experiments show that Real3D consistently outperforms\nprior work in four diverse evaluation settings that include real and synthetic\ndata, as well as both in-domain and out-of-domain shapes. Code and model can be\nfound here: https://hwjiang1510.github.io/Real3D/",
            "upvotes": 4
        },
        "publishedAt": "2024-06-14T03:33:19.251Z",
        "title": "Real3D: Scaling Up Large Reconstruction Models with Real-World Images",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.08479.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/462841d3ba16d351182e9aa552c36044.svg",
            "fullname": "Hanwen Jiang",
            "name": "hwjiang",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.09305",
            "authors": [
                {
                    "_id": "666be5c51dcf1f59373e26b5",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1666984965757-noauth.png",
                        "isPro": false,
                        "fullname": "Yufan Zhou",
                        "user": "YfZ",
                        "type": "user"
                    },
                    "name": "Yufan Zhou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-14T07:21:28.121Z",
                    "hidden": false
                },
                {
                    "_id": "666be5c51dcf1f59373e26b6",
                    "user": {
                        "avatarUrl": "/avatars/c9a248b8d70b1a8f7b78265c98690570.svg",
                        "isPro": false,
                        "fullname": "Ruiyi Zhang",
                        "user": "zhangry868",
                        "type": "user"
                    },
                    "name": "Ruiyi Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T11:16:54.638Z",
                    "hidden": false
                },
                {
                    "_id": "666be5c51dcf1f59373e26b7",
                    "name": "Kaizhi Zheng",
                    "hidden": false
                },
                {
                    "_id": "666be5c51dcf1f59373e26b8",
                    "name": "Nanxuan Zhao",
                    "hidden": false
                },
                {
                    "_id": "666be5c51dcf1f59373e26b9",
                    "name": "Jiuxiang Gu",
                    "hidden": false
                },
                {
                    "_id": "666be5c51dcf1f59373e26ba",
                    "name": "Zichao Wang",
                    "hidden": false
                },
                {
                    "_id": "666be5c51dcf1f59373e26bb",
                    "user": {
                        "avatarUrl": "/avatars/05abee0b6317f100923936ca2099e9eb.svg",
                        "isPro": false,
                        "fullname": "Xin Eric Wang",
                        "user": "xw-eric",
                        "type": "user"
                    },
                    "name": "Xin Eric Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-14T15:40:41.214Z",
                    "hidden": false
                },
                {
                    "_id": "666be5c51dcf1f59373e26bc",
                    "name": "Tong Sun",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-13T16:40:39.000Z",
            "title": "Toffee: Efficient Million-Scale Dataset Construction for Subject-Driven\n  Text-to-Image Generation",
            "summary": "In subject-driven text-to-image generation, recent works have achieved\nsuperior performance by training the model on synthetic datasets containing\nnumerous image pairs. Trained on these datasets, generative models can produce\ntext-aligned images for specific subject from arbitrary testing image in a\nzero-shot manner. They even outperform methods which require additional\nfine-tuning on testing images. However, the cost of creating such datasets is\nprohibitive for most researchers. To generate a single training pair, current\nmethods fine-tune a pre-trained text-to-image model on the subject image to\ncapture fine-grained details, then use the fine-tuned model to create images\nfor the same subject based on creative text prompts. Consequently, constructing\na large-scale dataset with millions of subjects can require hundreds of\nthousands of GPU hours. To tackle this problem, we propose Toffee, an efficient\nmethod to construct datasets for subject-driven editing and generation.\nSpecifically, our dataset construction does not need any subject-level\nfine-tuning. After pre-training two generative models, we are able to generate\ninfinite number of high-quality samples. We construct the first large-scale\ndataset for subject-driven image editing and generation, which contains 5\nmillion image pairs, text prompts, and masks. Our dataset is 5 times the size\nof previous largest dataset, yet our cost is tens of thousands of GPU hours\nlower. To test the proposed dataset, we also propose a model which is capable\nof both subject-driven image editing and generation. By simply training the\nmodel on our proposed dataset, it obtains competitive results, illustrating the\neffectiveness of the proposed dataset construction framework.",
            "upvotes": 3
        },
        "publishedAt": "2024-06-14T05:14:24.754Z",
        "title": "Toffee: Efficient Million-Scale Dataset Construction for Subject-Driven Text-to-Image Generation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.09305.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1666984965757-noauth.png",
            "fullname": "Yufan Zhou",
            "name": "YfZ",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.07546",
            "authors": [
                {
                    "_id": "666bb618925d891550f98440",
                    "user": {
                        "avatarUrl": "/avatars/4d29cef783603dc8ba123f01cde93ad8.svg",
                        "isPro": false,
                        "fullname": "Xingyu Fu",
                        "user": "Fiaa",
                        "type": "user"
                    },
                    "name": "Xingyu Fu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T11:24:05.378Z",
                    "hidden": false
                },
                {
                    "_id": "666bb618925d891550f98441",
                    "name": "Muyu He",
                    "hidden": false
                },
                {
                    "_id": "666bb618925d891550f98442",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633cfb6ad5935998f7582b7b/mzU4lI_GWPsYOFZHTBsYA.jpeg",
                        "isPro": true,
                        "fullname": "Yujie Lu",
                        "user": "FunCube",
                        "type": "user"
                    },
                    "name": "Yujie Lu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-14T07:22:26.614Z",
                    "hidden": false
                },
                {
                    "_id": "666bb618925d891550f98443",
                    "user": {
                        "avatarUrl": "/avatars/22f40c81e6b6e09bdb66a5afd978f98b.svg",
                        "isPro": false,
                        "fullname": "William Yang Wang",
                        "user": "wangwilliamyang",
                        "type": "user"
                    },
                    "name": "William Yang Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T11:24:32.258Z",
                    "hidden": false
                },
                {
                    "_id": "666bb618925d891550f98444",
                    "user": {
                        "avatarUrl": "/avatars/d7eadb22aa3d8a067189b924767864e9.svg",
                        "isPro": false,
                        "fullname": "Dan Roth",
                        "user": "danielpaulroth",
                        "type": "user"
                    },
                    "name": "Dan Roth",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T11:24:56.092Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-11T17:59:48.000Z",
            "title": "Commonsense-T2I Challenge: Can Text-to-Image Generation Models\n  Understand Commonsense?",
            "summary": "We present a novel task and benchmark for evaluating the ability of\ntext-to-image(T2I) generation models to produce images that fit commonsense in\nreal life, which we call Commonsense-T2I. Given two adversarial text prompts\ncontaining an identical set of action words with minor differences, such as \"a\nlightbulb without electricity\" v.s. \"a lightbulb with electricity\", we evaluate\nwhether T2I models can conduct visual-commonsense reasoning, e.g. produce\nimages that fit \"the lightbulb is unlit\" vs. \"the lightbulb is lit\"\ncorrespondingly. Commonsense-T2I presents an adversarial challenge, providing\npairwise text prompts along with expected outputs. The dataset is carefully\nhand-curated by experts and annotated with fine-grained labels, such as\ncommonsense type and likelihood of the expected outputs, to assist analyzing\nmodel behavior. We benchmark a variety of state-of-the-art (sota) T2I models\nand surprisingly find that, there is still a large gap between image synthesis\nand real life photos--even the DALL-E 3 model could only achieve 48.92% on\nCommonsense-T2I, and the stable diffusion XL model only achieves 24.92%\naccuracy. Our experiments show that GPT-enriched prompts cannot solve this\nchallenge, and we include a detailed analysis about possible reasons for such\ndeficiency. We aim for Commonsense-T2I to serve as a high-quality evaluation\nbenchmark for T2I commonsense checking, fostering advancements in real life\nimage generation.",
            "upvotes": 3
        },
        "publishedAt": "2024-06-14T05:09:00.284Z",
        "title": "Commonsense-T2I Challenge: Can Text-to-Image Generation Models Understand Commonsense?",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.07546.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/4d29cef783603dc8ba123f01cde93ad8.svg",
            "fullname": "Xingyu Fu",
            "name": "Fiaa",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.09371",
            "authors": [
                {
                    "_id": "666bd7d2f1b3867caf4b73c7",
                    "user": {
                        "avatarUrl": "/avatars/49bea87294118e18df7b774f716af81a.svg",
                        "isPro": false,
                        "fullname": "Desai Xie",
                        "user": "desaix",
                        "type": "user"
                    },
                    "name": "Desai Xie",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T11:09:38.093Z",
                    "hidden": false
                },
                {
                    "_id": "666bd7d2f1b3867caf4b73c8",
                    "user": {
                        "avatarUrl": "/avatars/a69d52a4a198c6c8451c1d137031bf07.svg",
                        "isPro": false,
                        "fullname": "Sai Bi",
                        "user": "saibi",
                        "type": "user"
                    },
                    "name": "Sai Bi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T11:09:44.136Z",
                    "hidden": false
                },
                {
                    "_id": "666bd7d2f1b3867caf4b73c9",
                    "user": {
                        "avatarUrl": "/avatars/ff259233d437833a304329bb973a5a04.svg",
                        "isPro": false,
                        "fullname": "Zhixin Shu",
                        "user": "zhixinshu",
                        "type": "user"
                    },
                    "name": "Zhixin Shu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T11:09:51.363Z",
                    "hidden": false
                },
                {
                    "_id": "666bd7d2f1b3867caf4b73ca",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e0a50242591dda0b9dca5c/c7cBPEBWQDFYimfGnO_SI.png",
                        "isPro": false,
                        "fullname": "Kai Zhang",
                        "user": "drogozhang",
                        "type": "user"
                    },
                    "name": "Kai Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T11:10:15.966Z",
                    "hidden": false
                },
                {
                    "_id": "666bd7d2f1b3867caf4b73cb",
                    "user": {
                        "avatarUrl": "/avatars/c245b19ed05292bf1bf4a9d8cf2a409c.svg",
                        "isPro": false,
                        "fullname": "Zexiang Xu",
                        "user": "zexiangxu",
                        "type": "user"
                    },
                    "name": "Zexiang Xu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T11:10:21.402Z",
                    "hidden": false
                },
                {
                    "_id": "666bd7d2f1b3867caf4b73cc",
                    "user": {
                        "avatarUrl": "/avatars/1c8d6a62e6e61fda98748c2656e17851.svg",
                        "isPro": false,
                        "fullname": "Yi Zhou",
                        "user": "yizhou-seu",
                        "type": "user"
                    },
                    "name": "Yi Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T11:11:08.044Z",
                    "hidden": false
                },
                {
                    "_id": "666bd7d2f1b3867caf4b73cd",
                    "user": {
                        "avatarUrl": "/avatars/467d8949626539a7482dccbee4fb4d25.svg",
                        "isPro": false,
                        "fullname": "Soren Pirk",
                        "user": "pirk",
                        "type": "user"
                    },
                    "name": "Sren Pirk",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2024-06-14T05:56:40.690Z",
                    "hidden": false
                },
                {
                    "_id": "666bd7d2f1b3867caf4b73ce",
                    "name": "Arie Kaufman",
                    "hidden": false
                },
                {
                    "_id": "666bd7d2f1b3867caf4b73cf",
                    "name": "Xin Sun",
                    "hidden": false
                },
                {
                    "_id": "666bd7d2f1b3867caf4b73d0",
                    "name": "Hao Tan",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-13T17:51:00.000Z",
            "title": "LRM-Zero: Training Large Reconstruction Models with Synthesized Data",
            "summary": "We present LRM-Zero, a Large Reconstruction Model (LRM) trained entirely on\nsynthesized 3D data, achieving high-quality sparse-view 3D reconstruction. The\ncore of LRM-Zero is our procedural 3D dataset, Zeroverse, which is\nautomatically synthesized from simple primitive shapes with random texturing\nand augmentations (e.g., height fields, boolean differences, and wireframes).\nUnlike previous 3D datasets (e.g., Objaverse) which are often captured or\ncrafted by humans to approximate real 3D data, Zeroverse completely ignores\nrealistic global semantics but is rich in complex geometric and texture details\nthat are locally similar to or even more intricate than real objects. We\ndemonstrate that our LRM-Zero, trained with our fully synthesized Zeroverse,\ncan achieve high visual quality in the reconstruction of real-world objects,\ncompetitive with models trained on Objaverse. We also analyze several critical\ndesign choices of Zeroverse that contribute to LRM-Zero's capability and\ntraining stability. Our work demonstrates that 3D reconstruction, one of the\ncore tasks in 3D vision, can potentially be addressed without the semantics of\nreal-world objects. The Zeroverse's procedural synthesis code and interactive\nvisualization are available at: https://desaixie.github.io/lrm-zero/.",
            "upvotes": 2
        },
        "publishedAt": "2024-06-14T04:11:19.163Z",
        "title": "LRM-Zero: Training Large Reconstruction Models with Synthesized Data",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6279f597812ee439d9c8de67/qBPm9P2gF62zMnNtCGXok.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.09371.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/49bea87294118e18df7b774f716af81a.svg",
            "fullname": "Desai Xie",
            "name": "desaix",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.09356",
            "authors": [
                {
                    "_id": "666c7348e0b2d43ff35e7f58",
                    "name": "Chunyi Li",
                    "hidden": false
                },
                {
                    "_id": "666c7348e0b2d43ff35e7f59",
                    "name": "Xiele Wu",
                    "hidden": false
                },
                {
                    "_id": "666c7348e0b2d43ff35e7f5a",
                    "name": "Haoning Wu",
                    "hidden": false
                },
                {
                    "_id": "666c7348e0b2d43ff35e7f5b",
                    "name": "Donghui Feng",
                    "hidden": false
                },
                {
                    "_id": "666c7348e0b2d43ff35e7f5c",
                    "name": "Zicheng Zhang",
                    "hidden": false
                },
                {
                    "_id": "666c7348e0b2d43ff35e7f5d",
                    "name": "Guo Lu",
                    "hidden": false
                },
                {
                    "_id": "666c7348e0b2d43ff35e7f5e",
                    "name": "Xiongkuo Min",
                    "hidden": false
                },
                {
                    "_id": "666c7348e0b2d43ff35e7f5f",
                    "name": "Xiaohong Liu",
                    "hidden": false
                },
                {
                    "_id": "666c7348e0b2d43ff35e7f60",
                    "name": "Guangtao Zhai",
                    "hidden": false
                },
                {
                    "_id": "666c7348e0b2d43ff35e7f61",
                    "name": "Weisi Lin",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-13T17:41:37.000Z",
            "title": "CMC-Bench: Towards a New Paradigm of Visual Signal Compression",
            "summary": "Ultra-low bitrate image compression is a challenging and demanding topic.\nWith the development of Large Multimodal Models (LMMs), a Cross Modality\nCompression (CMC) paradigm of Image-Text-Image has emerged. Compared with\ntraditional codecs, this semantic-level compression can reduce image data size\nto 0.1\\% or even lower, which has strong potential applications. However, CMC\nhas certain defects in consistency with the original image and perceptual\nquality. To address this problem, we introduce CMC-Bench, a benchmark of the\ncooperative performance of Image-to-Text (I2T) and Text-to-Image (T2I) models\nfor image compression. This benchmark covers 18,000 and 40,000 images\nrespectively to verify 6 mainstream I2T and 12 T2I models, including 160,000\nsubjective preference scores annotated by human experts. At ultra-low bitrates,\nthis paper proves that the combination of some I2T and T2I models has surpassed\nthe most advanced visual signal codecs; meanwhile, it highlights where LMMs can\nbe further optimized toward the compression task. We encourage LMM developers\nto participate in this test to promote the evolution of visual signal codec\nprotocols.",
            "upvotes": 1
        },
        "publishedAt": "2024-06-14T15:22:44.422Z",
        "title": "CMC-Bench: Towards a New Paradigm of Visual Signal Compression",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/644cf72f6d84e25fad73ae23/fLBdCe1hxqRm6-hR0nmRd.png",
            "https://cdn-uploads.huggingface.co/production/uploads/644cf72f6d84e25fad73ae23/fd8LLv82sQxTEQmX4vv5m.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.09356.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "/avatars/5364b00576bbd9fb549fce54d572ae83.svg",
            "fullname": "Chunyi Li",
            "name": "lcysyzxdxc",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.09297",
            "authors": [
                {
                    "_id": "666be7eef8649b8d8f562e90",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60cf8a354061635e43b28f60/adR3Ah-cRS9UznM6LKao1.jpeg",
                        "isPro": false,
                        "fullname": "Zayd Muhammad Kawakibi Zuhri",
                        "user": "zaydzuhri",
                        "type": "user"
                    },
                    "name": "Zayd Muhammad Kawakibi Zuhri",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-14T07:21:17.828Z",
                    "hidden": false
                },
                {
                    "_id": "666be7eef8649b8d8f562e91",
                    "name": "Muhammad Farid Adilazuarda",
                    "hidden": false
                },
                {
                    "_id": "666be7eef8649b8d8f562e92",
                    "name": "Ayu Purwarianti",
                    "hidden": false
                },
                {
                    "_id": "666be7eef8649b8d8f562e93",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61a4dc053205e107691e0d82/BESoEHlHYXstXudh6dOdT.jpeg",
                        "isPro": true,
                        "fullname": "Alham Fikri Aji",
                        "user": "afaji",
                        "type": "user"
                    },
                    "name": "Alham Fikri Aji",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-14T13:56:01.718Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-13T16:33:44.000Z",
            "title": "MLKV: Multi-Layer Key-Value Heads for Memory Efficient Transformer\n  Decoding",
            "summary": "Auto-regressive inference of transformers benefit greatly from Key-Value (KV)\ncaching, but can lead to major memory bottlenecks as model size, batch size,\nand sequence length grow at scale. We introduce Multi-Layer Key-Value (MLKV)\nsharing, a novel approach extending KV sharing across transformer layers to\nreduce memory usage beyond what was possible with Multi-Query Attention (MQA)\nand Grouped-Query Attention (GQA). Evaluations on various NLP benchmarks and\ninference metrics using uptrained Pythia-160M variants demonstrate that MLKV\nsignificantly reduces memory usage with minimal performance loss, reducing KV\ncache size down to a factor of 6x compared to MQA. These results highlight\nMLKV's potential for efficient deployment of transformer models at scale. We\nprovide code at https://github.com/zaydzuhri/pythia-mlkv",
            "upvotes": 1
        },
        "publishedAt": "2024-06-14T14:31:46.733Z",
        "title": "MLKV: Multi-Layer Key-Value Heads for Memory Efficient Transformer Decoding",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/60cf8a354061635e43b28f60/y3xCgxAavb9nHQg2_jups.jpeg",
            "https://cdn-uploads.huggingface.co/production/uploads/60cf8a354061635e43b28f60/5QrKTEyCpLDXo6_DVivPw.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.09297.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60cf8a354061635e43b28f60/adR3Ah-cRS9UznM6LKao1.jpeg",
            "fullname": "Zayd Muhammad Kawakibi Zuhri",
            "name": "zaydzuhri",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.05967",
            "authors": [
                {
                    "_id": "666c4afb14f1c262fefaa666",
                    "name": "David Romero",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa667",
                    "name": "Chenyang Lyu",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa668",
                    "name": "Haryo Akbarianto Wibowo",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa669",
                    "name": "Teresa Lynn",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa66a",
                    "name": "Injy Hamed",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa66b",
                    "name": "Aditya Nanda Kishore",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa66c",
                    "name": "Aishik Mandal",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa66d",
                    "name": "Alina Dragonetti",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa66e",
                    "name": "Artem Abzaliev",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa66f",
                    "name": "Atnafu Lambebo Tonja",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa670",
                    "name": "Bontu Fufa Balcha",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa671",
                    "name": "Chenxi Whitehouse",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa672",
                    "name": "Christian Salamea",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa673",
                    "name": "Dan John Velasco",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa674",
                    "name": "David Ifeoluwa Adelani",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa675",
                    "name": "David Le Meur",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa676",
                    "name": "Emilio Villa-Cueva",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa677",
                    "name": "Fajri Koto",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa678",
                    "name": "Fauzan Farooqui",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa679",
                    "name": "Frederico Belcavello",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa67a",
                    "name": "Ganzorig Batnasan",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa67b",
                    "name": "Gisela Vallejo",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa67c",
                    "name": "Grainne Caulfield",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa67d",
                    "name": "Guido Ivetta",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa67e",
                    "name": "Haiyue Song",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa67f",
                    "name": "Henok Biadglign Ademtew",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa680",
                    "name": "Hernn Maina",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa681",
                    "name": "Holy Lovenia",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa682",
                    "name": "Israel Abebe Azime",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa683",
                    "name": "Jan Christian Blaise Cruz",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa684",
                    "name": "Jay Gala",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa685",
                    "name": "Jiahui Geng",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa686",
                    "name": "Jesus-German Ortiz-Barajas",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa687",
                    "name": "Jinheon Baek",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa688",
                    "name": "Jocelyn Dunstan",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa689",
                    "name": "Laura Alonso Alemany",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa68a",
                    "name": "Kumaranage Ravindu Yasas Nagasinghe",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa68b",
                    "name": "Luciana Benotti",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa68c",
                    "name": "Luis Fernando D'Haro",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa68d",
                    "name": "Marcelo Viridiano",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa68e",
                    "name": "Marcos Estecha-Garitagoitia",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa68f",
                    "name": "Maria Camila Buitrago Cabrera",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa690",
                    "name": "Mario Rodrguez-Cantelar",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa691",
                    "name": "Mlanie Jouitteau",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa692",
                    "name": "Mihail Mihaylov",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa693",
                    "name": "Mohamed Fazli Mohamed Imam",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa694",
                    "name": "Muhammad Farid Adilazuarda",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa695",
                    "name": "Munkhjargal Gochoo",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa696",
                    "name": "Munkh-Erdene Otgonbold",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa697",
                    "name": "Naome Etori",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa698",
                    "name": "Olivier Niyomugisha",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa699",
                    "name": "Paula Mnica Silva",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa69a",
                    "name": "Pranjal Chitale",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa69b",
                    "name": "Raj Dabre",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa69c",
                    "name": "Rendi Chevi",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa69d",
                    "name": "Ruochen Zhang",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa69e",
                    "name": "Ryandito Diandaru",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa69f",
                    "name": "Samuel Cahyawijaya",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa6a0",
                    "name": "Santiago Gngora",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa6a1",
                    "name": "Soyeong Jeong",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa6a2",
                    "name": "Sukannya Purkayastha",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa6a3",
                    "name": "Tatsuki Kuribayashi",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa6a4",
                    "name": "Thanmay Jayakumar",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa6a5",
                    "name": "Tiago Timponi Torrent",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa6a6",
                    "name": "Toqeer Ehsan",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa6a7",
                    "name": "Vladimir Araujo",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa6a8",
                    "name": "Yova Kementchedjhieva",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa6a9",
                    "name": "Zara Burzo",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa6aa",
                    "name": "Zheng Wei Lim",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa6ab",
                    "name": "Zheng Xin Yong",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa6ac",
                    "name": "Oana Ignat",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa6ad",
                    "name": "Joan Nwatu",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa6ae",
                    "name": "Rada Mihalcea",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa6af",
                    "name": "Thamar Solorio",
                    "hidden": false
                },
                {
                    "_id": "666c4afb14f1c262fefaa6b0",
                    "name": "Alham Fikri Aji",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-10T01:59:00.000Z",
            "title": "CVQA: Culturally-diverse Multilingual Visual Question Answering\n  Benchmark",
            "summary": "Visual Question Answering (VQA) is an important task in multimodal AI, and it\nis often used to test the ability of vision-language models to understand and\nreason on knowledge present in both visual and textual data. However, most of\nthe current VQA models use datasets that are primarily focused on English and a\nfew major world languages, with images that are typically Western-centric.\nWhile recent efforts have tried to increase the number of languages covered on\nVQA datasets, they still lack diversity in low-resource languages. More\nimportantly, although these datasets often extend their linguistic range via\ntranslation or some other approaches, they usually keep images the same,\nresulting in narrow cultural representation. To address these limitations, we\nconstruct CVQA, a new Culturally-diverse multilingual Visual Question Answering\nbenchmark, designed to cover a rich set of languages and cultures, where we\nengage native speakers and cultural experts in the data collection process. As\na result, CVQA includes culturally-driven images and questions from across 28\ncountries on four continents, covering 26 languages with 11 scripts, providing\na total of 9k questions. We then benchmark several Multimodal Large Language\nModels (MLLMs) on CVQA, and show that the dataset is challenging for the\ncurrent state-of-the-art models. This benchmark can serve as a probing\nevaluation suite for assessing the cultural capability and bias of multimodal\nmodels and hopefully encourage more research efforts toward increasing cultural\nawareness and linguistic diversity in this field.",
            "upvotes": 1
        },
        "publishedAt": "2024-06-14T12:28:47.469Z",
        "title": "CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/61a4dc053205e107691e0d82/6SDOmrl3J1uvqb7_r3FUb.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.05967.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61a4dc053205e107691e0d82/BESoEHlHYXstXudh6dOdT.jpeg",
            "fullname": "Alham Fikri Aji",
            "name": "afaji",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.07457",
            "authors": [
                {
                    "_id": "666c64b370f5a2cb4a5fd753",
                    "user": {
                        "avatarUrl": "/avatars/97262276275537ce3926efd93ac5a086.svg",
                        "isPro": false,
                        "fullname": "andrew jesson",
                        "user": "anndvision",
                        "type": "user"
                    },
                    "name": "Andrew Jesson",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T15:45:25.211Z",
                    "hidden": false
                },
                {
                    "_id": "666c64b370f5a2cb4a5fd754",
                    "name": "Nicolas Beltran-Velez",
                    "hidden": false
                },
                {
                    "_id": "666c64b370f5a2cb4a5fd755",
                    "user": {
                        "avatarUrl": "/avatars/2af4ffd863142e1dbccfedab782d7ab0.svg",
                        "isPro": false,
                        "fullname": "Quentin Chu",
                        "user": "qc2321",
                        "type": "user"
                    },
                    "name": "Quentin Chu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T15:45:38.744Z",
                    "hidden": false
                },
                {
                    "_id": "666c64b370f5a2cb4a5fd756",
                    "user": {
                        "avatarUrl": "/avatars/65d44808417b133cf83216122a1b511e.svg",
                        "isPro": false,
                        "fullname": "Sweta Karlekar",
                        "user": "swkarlekar",
                        "type": "user"
                    },
                    "name": "Sweta Karlekar",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-14T15:45:53.881Z",
                    "hidden": false
                },
                {
                    "_id": "666c64b370f5a2cb4a5fd757",
                    "user": {
                        "avatarUrl": "/avatars/8c9503e43f403d655b11ea5c6e23ae79.svg",
                        "isPro": false,
                        "fullname": "Jannik",
                        "user": "jlko",
                        "type": "user"
                    },
                    "name": "Jannik Kossen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-14T15:45:46.815Z",
                    "hidden": false
                },
                {
                    "_id": "666c64b370f5a2cb4a5fd758",
                    "name": "Yarin Gal",
                    "hidden": false
                },
                {
                    "_id": "666c64b370f5a2cb4a5fd759",
                    "name": "John P. Cunningham",
                    "hidden": false
                },
                {
                    "_id": "666c64b370f5a2cb4a5fd75a",
                    "name": "David Blei",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-11T17:01:52.000Z",
            "title": "Estimating the Hallucination Rate of Generative AI",
            "summary": "This work is about estimating the hallucination rate for in-context learning\n(ICL) with Generative AI. In ICL, a conditional generative model (CGM) is\nprompted with a dataset and asked to make a prediction based on that dataset.\nThe Bayesian interpretation of ICL assumes that the CGM is calculating a\nposterior predictive distribution over an unknown Bayesian model of a latent\nparameter and data. With this perspective, we define a hallucination\nas a generated prediction that has low-probability under the true latent\nparameter. We develop a new method that takes an ICL problem -- that is, a CGM,\na dataset, and a prediction question -- and estimates the probability that a\nCGM will generate a hallucination. Our method only requires generating queries\nand responses from the model and evaluating its response log probability. We\nempirically evaluate our method on synthetic regression and natural language\nICL tasks using large language models.",
            "upvotes": 0
        },
        "publishedAt": "2024-06-14T14:14:45.559Z",
        "title": "Estimating the Hallucination Rate of Generative AI",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.07457.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/8c9503e43f403d655b11ea5c6e23ae79.svg",
            "fullname": "Jannik",
            "name": "jlko",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    }
]