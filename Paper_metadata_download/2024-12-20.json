[
    "{'paper': {'id': '2412.15115', 'authors': [{'_id': '6764dda159e2ef96ec013c07', 'name': 'Qwen', 'hidden': False}, {'_id': '6764dda159e2ef96ec013c09', 'name': 'An Yang', 'hidden': False}, {'_id': '6764dda159e2ef96ec013c0a', 'user': {'_id': '64b0a77df12b47366663884c', 'avatarUrl': '/avatars/a212ea862abb5966060e439dd0e7656f.svg', 'isPro': False, 'fullname': 'Baosong Yang', 'user': 'Baosong', 'type': 'user'}, 'name': 'Baosong Yang', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-20T08:42:54.655Z', 'hidden': False}, {'_id': '6764dda159e2ef96ec013c0b', 'name': 'Beichen Zhang', 'hidden': False}, {'_id': '6764dda159e2ef96ec013c0c', 'user': {'_id': '61e4c4ca1ab24785ac11ba69', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/61e4c4ca1ab24785ac11ba69/1Q1zhhyGSJ9RJG9MzwxVv.jpeg', 'isPro': False, 'fullname': 'Binyuan Hui', 'user': 'huybery', 'type': 'user'}, 'name': 'Binyuan Hui', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-20T08:32:55.260Z', 'hidden': False}, {'_id': '6764dda159e2ef96ec013c0d', 'user': {'_id': '62c695ad5aae1c624ca992e2', 'avatarUrl': '/avatars/20d10fb3338e4bd4dc59e88a18cb2617.svg', 'isPro': False, 'fullname': 'Bo Zheng', 'user': 'bzheng', 'type': 'user'}, 'name': 'Bo Zheng', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-20T09:15:45.070Z', 'hidden': False}, {'_id': '6764dda159e2ef96ec013c0e', 'user': {'_id': '6583ab7983a9e1460c67d876', 'avatarUrl': '/avatars/74400bc448c3f07e23a4cd53d68a6af7.svg', 'isPro': False, 'fullname': 'bowen', 'user': 'bowenYu', 'type': 'user'}, 'name': 'Bowen Yu', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-20T08:43:56.820Z', 'hidden': False}, {'_id': '6764dda159e2ef96ec013c0f', 'name': 'Chengyuan Li', 'hidden': False}, {'_id': '6764dda159e2ef96ec013c10', 'user': {'_id': '6434d4989bd5a84b5dd0b0f5', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6434d4989bd5a84b5dd0b0f5/0Elf9qbfG9Hkgypm9pTGm.jpeg', 'isPro': False, 'fullname': 'Dayiheng Liu', 'user': 'Losin94', 'type': 'user'}, 'name': 'Dayiheng Liu', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-20T08:32:51.398Z', 'hidden': False}, {'_id': '6764dda159e2ef96ec013c11', 'name': 'Fei Huang', 'hidden': False}, {'_id': '6764dda159e2ef96ec013c12', 'user': {'_id': '6436618aeef1f55654a9f458', 'avatarUrl': '/avatars/d4b925f7b8523e105dae3c5dd6fae801.svg', 'isPro': False, 'fullname': 'Haoran Wei', 'user': 'HaoranWei', 'type': 'user'}, 'name': 'Haoran Wei', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-20T08:44:49.831Z', 'hidden': False}, {'_id': '6764dda159e2ef96ec013c13', 'user': {'_id': '637c0e8aa8716d64204e2f01', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1671291118635-637c0e8aa8716d64204e2f01.jpeg', 'isPro': False, 'fullname': 'HuanLin', 'user': 'HuanLin', 'type': 'user'}, 'name': 'Huan Lin', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-20T08:44:55.547Z', 'hidden': False}, {'_id': '6764dda159e2ef96ec013c14', 'name': 'Jian Yang', 'hidden': False}, {'_id': '6764dda159e2ef96ec013c15', 'user': {'_id': '654bead777401b47e6424f88', 'avatarUrl': '/avatars/7bcbdbb051c93b004f0dc3ad36c4a0ce.svg', 'isPro': False, 'fullname': 'Jianhong Tu', 'user': 'ToviTu', 'type': 'user'}, 'name': 'Jianhong Tu', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-20T08:45:05.864Z', 'hidden': False}, {'_id': '6764dda159e2ef96ec013c16', 'name': 'Jianwei Zhang', 'hidden': False}, {'_id': '6764dda159e2ef96ec013c17', 'name': 'Jianxin Yang', 'hidden': False}, {'_id': '6764dda159e2ef96ec013c18', 'name': 'Jiaxi Yang', 'hidden': False}, {'_id': '6764dda159e2ef96ec013c19', 'user': {'_id': '602f88f5e8149a962412a667', 'avatarUrl': '/avatars/b78f0e583df8e5d5e3365934fe5f4900.svg', 'isPro': False, 'fullname': 'Zhou', 'user': 'Jingren', 'type': 'user'}, 'name': 'Jingren Zhou', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-20T08:45:52.281Z', 'hidden': False}, {'_id': '6764dda159e2ef96ec013c1a', 'user': {'_id': '620760a26e3b7210c2ff1943', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/620760a26e3b7210c2ff1943/VC-rKqimF6yxGESNVlPoR.jpeg', 'isPro': False, 'fullname': 'Junyang Lin', 'user': 'JustinLin610', 'type': 'user'}, 'name': 'Junyang Lin', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-20T08:45:59.082Z', 'hidden': False}, {'_id': '6764dda159e2ef96ec013c1b', 'user': {'_id': '6712930f0fac3235c56edf5b', 'avatarUrl': '/avatars/cafe7cb56ce7c3b2572f5f2d0b89357a.svg', 'isPro': False, 'fullname': 'kai dang', 'user': '1vk5i', 'type': 'user'}, 'name': 'Kai Dang', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-20T15:42:29.190Z', 'hidden': False}, {'_id': '6764dda159e2ef96ec013c1c', 'user': {'_id': '6453fa96ed6d7fede94408e0', 'avatarUrl': '/avatars/e8c9025ef24cec958c87a1008bb54fd7.svg', 'isPro': False, 'fullname': 'Keming Lu', 'user': 'keminglu', 'type': 'user'}, 'name': 'Keming Lu', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-20T08:46:07.173Z', 'hidden': False}, {'_id': '6764dda159e2ef96ec013c1d', 'user': {'_id': '6154629f6181394cc00cb98b', 'avatarUrl': '/avatars/de2d5cb6fee9c03a99289d03272eddeb.svg', 'isPro': False, 'fullname': 'Keqin Bao', 'user': 'morningStar', 'type': 'user'}, 'name': 'Keqin Bao', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-20T08:42:00.963Z', 'hidden': False}, {'_id': '6764dda159e2ef96ec013c1e', 'user': {'_id': '65b0b3957e5d5a4ecc750de0', 'avatarUrl': '/avatars/e0d79d3265ca4ad5c5411feb01043fb4.svg', 'isPro': False, 'fullname': 'Kexin Yang', 'user': 'dawn0929', 'type': 'user'}, 'name': 'Kexin Yang', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-20T08:46:29.866Z', 'hidden': False}, {'_id': '6764dda159e2ef96ec013c1f', 'name': 'Le Yu', 'hidden': False}, {'_id': '6764dda159e2ef96ec013c20', 'name': 'Mei Li', 'hidden': False}, {'_id': '6764dda159e2ef96ec013c21', 'user': {'_id': '5f8946925d083370c711f296', 'avatarUrl': '/avatars/14246aae3b1f8b7ad050f8ff2c8b260e.svg', 'isPro': False, 'fullname': 'Mingfeng Xue', 'user': 'mingfengxue', 'type': 'user'}, 'name': 'Mingfeng Xue', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-20T08:46:45.959Z', 'hidden': False}, {'_id': '6764dda159e2ef96ec013c22', 'user': {'_id': '634872721df68daaa0125e08', 'avatarUrl': '/avatars/c091b70448a4e1d43fce3a31a7beddf3.svg', 'isPro': False, 'fullname': 'Pei Zhang', 'user': 'peizhang', 'type': 'user'}, 'name': 'Pei Zhang', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-20T08:47:05.519Z', 'hidden': False}, {'_id': '6764dda159e2ef96ec013c23', 'name': 'Qin Zhu', 'hidden': False}, {'_id': '6764dda159e2ef96ec013c24', 'name': 'Rui Men', 'hidden': False}, {'_id': '6764dda159e2ef96ec013c25', 'user': {'_id': '649a52e5de0fb7f3f499e583', 'avatarUrl': '/avatars/25f6106fa168ae57ad5cd8ef55c70d31.svg', 'isPro': False, 'fullname': 'Runji Lin', 'user': 'RunjiLin', 'type': 'user'}, 'name': 'Runji Lin', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-20T08:32:53.455Z', 'hidden': False}, {'_id': '6764dda159e2ef96ec013c26', 'name': 'Tianhao Li', 'hidden': False}, {'_id': '6764dda159e2ef96ec013c27', 'name': 'Tingyu Xia', 'hidden': False}, {'_id': '6764dda159e2ef96ec013c28', 'name': 'Xingzhang Ren', 'hidden': False}, {'_id': '6764dda159e2ef96ec013c29', 'name': 'Xuancheng Ren', 'hidden': False}, {'_id': '6764dda159e2ef96ec013c2a', 'name': 'Yang Fan', 'hidden': False}, {'_id': '6764dda159e2ef96ec013c2b', 'user': {'_id': '63481fea27bd5edd982328f9', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1665670948064-noauth.jpeg', 'isPro': False, 'fullname': 'Yang Su', 'user': 'yang-su2000', 'type': 'user'}, 'name': 'Yang Su', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-20T13:55:55.786Z', 'hidden': False}, {'_id': '6764dda159e2ef96ec013c2c', 'name': 'Yichang Zhang', 'hidden': False}, {'_id': '6764dda159e2ef96ec013c2d', 'name': 'Yu Wan', 'hidden': False}, {'_id': '6764dda159e2ef96ec013c2e', 'user': {'_id': '666aacfb918ba11c7c598194', 'avatarUrl': '/avatars/45bee8f1fdbdd256ee47d25e4bf01a7a.svg', 'isPro': False, 'fullname': 'Yuqiong Liu', 'user': 'lyq333', 'type': 'user'}, 'name': 'Yuqiong Liu', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-20T09:14:40.709Z', 'hidden': False}, {'_id': '6764dda159e2ef96ec013c2f', 'user': {'_id': '672c25ca8cfb61188128eb6f', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/FJWy9Tt7UQmu9KcTOx3Rt.png', 'isPro': False, 'fullname': 'Zeyu Cui', 'user': 'misakamage', 'type': 'user'}, 'name': 'Zeyu Cui', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-20T09:14:34.133Z', 'hidden': False}, {'_id': '6764dda159e2ef96ec013c30', 'name': 'Zhenru Zhang', 'hidden': False}, {'_id': '6764dda159e2ef96ec013c31', 'user': {'_id': '647ccbd6e07cf9bb2d485244', 'avatarUrl': '/avatars/e8915abaff04f6762247e196b7cf84df.svg', 'isPro': False, 'fullname': 'Zihan Qiu', 'user': 'QwQZh', 'type': 'user'}, 'name': 'Zihan Qiu', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-20T08:41:53.155Z', 'hidden': False}], 'publishedAt': '2024-12-19T17:56:09.000Z', 'title': 'Qwen2.5 Technical Report', 'summary': 'In this report, we introduce Qwen2.5, a comprehensive series of large\\nlanguage models (LLMs) designed to meet diverse needs. Compared to previous\\niterations, Qwen 2.5 has been significantly improved during both the\\npre-training and post-training stages. In terms of pre-training, we have scaled\\nthe high-quality pre-training datasets from the previous 7 trillion tokens to\\n18 trillion tokens. This provides a strong foundation for common sense, expert\\nknowledge, and reasoning capabilities. In terms of post-training, we implement\\nintricate supervised finetuning with over 1 million samples, as well as\\nmultistage reinforcement learning. Post-training techniques enhance human\\npreference, and notably improve long text generation, structural data analysis,\\nand instruction following. To handle diverse and varied use cases effectively,\\nwe present Qwen2.5 LLM series in rich sizes. Open-weight offerings include base\\nand instruction-tuned models, with quantized versions available. In addition,\\nfor hosted solutions, the proprietary models currently include two\\nmixture-of-experts (MoE) variants: Qwen2.5-Turbo and Qwen2.5-Plus, both\\navailable from Alibaba Cloud Model Studio. Qwen2.5 has demonstrated top-tier\\nperformance on a wide range of benchmarks evaluating language understanding,\\nreasoning, mathematics, coding, human preference alignment, etc. Specifically,\\nthe open-weight flagship Qwen2.5-72B-Instruct outperforms a number of open and\\nproprietary models and demonstrates competitive performance to the\\nstate-of-the-art open-weight model, Llama-3-405B-Instruct, which is around 5\\ntimes larger. Qwen2.5-Turbo and Qwen2.5-Plus offer superior cost-effectiveness\\nwhile performing competitively against GPT-4o-mini and GPT-4o respectively.\\nAdditionally, as the foundation, Qwen2.5 models have been instrumental in\\ntraining specialized models such as Qwen2.5-Math, Qwen2.5-Coder, QwQ, and\\nmultimodal models.', 'upvotes': 225, 'discussionId': '6764dda259e2ef96ec013c85'}, 'publishedAt': '2024-12-19T22:00:01.704Z', 'title': 'Qwen2.5 Technical Report', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.15115.png', 'numComments': 8, 'submittedBy': {'_id': '61e4c4ca1ab24785ac11ba69', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/61e4c4ca1ab24785ac11ba69/1Q1zhhyGSJ9RJG9MzwxVv.jpeg', 'fullname': 'Binyuan Hui', 'name': 'huybery', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 37}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2412.14835', 'authors': [{'_id': '6764d821772d134796df3f97', 'user': {'_id': '61cd4b833dd34ba1985e0753', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/61cd4b833dd34ba1985e0753/BfHfrwotoMESpXZOHiIe4.png', 'isPro': False, 'fullname': 'KABI', 'user': 'dongguanting', 'type': 'user'}, 'name': 'Guanting Dong', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-20T09:16:05.803Z', 'hidden': False}, {'_id': '6764d821772d134796df3f98', 'user': {'_id': '6710ac3fb4ee4920580a5f0e', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6710ac3fb4ee4920580a5f0e/OhQQFlZmkmLQpMYqKCGP6.jpeg', 'isPro': False, 'fullname': 'Chenghao Zhang', 'user': 'Snow-Nation', 'type': 'user'}, 'name': 'Chenghao Zhang', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-20T08:33:01.155Z', 'hidden': False}, {'_id': '6764d821772d134796df3f99', 'user': {'_id': '65db23d1f386d08eb0d1cec5', 'avatarUrl': '/avatars/fc2217bbf40af09113b4788a011927a9.svg', 'isPro': False, 'fullname': 'dengmengjie', 'user': 'dengmengjie', 'type': 'user'}, 'name': 'Mengjie Deng', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-20T09:16:18.270Z', 'hidden': False}, {'_id': '6764d821772d134796df3f9a', 'user': {'_id': '625e62452a7279d3c77b5c38', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/625e62452a7279d3c77b5c38/zJINew6U4_Gup4WTobb-0.jpeg', 'isPro': False, 'fullname': 'Yutao Zhu', 'user': 'yutaozhu94', 'type': 'user'}, 'name': 'Yutao Zhu', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-20T09:16:23.877Z', 'hidden': False}, {'_id': '6764d821772d134796df3f9b', 'user': {'_id': '66f0bf59e9d50ec57febf751', 'avatarUrl': '/avatars/be97941e60064e5dd806c6fe9db3c537.svg', 'isPro': False, 'fullname': 'Zhicheng Dou', 'user': 'douzc', 'type': 'user'}, 'name': 'Zhicheng Dou', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-20T09:16:29.532Z', 'hidden': False}, {'_id': '6764d821772d134796df3f9c', 'user': {'_id': '64b8c89052b7353d8c6a1013', 'avatarUrl': '/avatars/cd59fffe81f6b07b4519540b8ff3d95f.svg', 'isPro': False, 'fullname': 'Ji-Rong Wen', 'user': 'jrwen', 'type': 'user'}, 'name': 'Ji-Rong Wen', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-20T09:16:35.274Z', 'hidden': False}], 'publishedAt': '2024-12-19T13:25:39.000Z', 'title': 'Progressive Multimodal Reasoning via Active Retrieval', 'summary': 'Multi-step multimodal reasoning tasks pose significant challenges for\\nmultimodal large language models (MLLMs), and finding effective ways to enhance\\ntheir performance in such scenarios remains an unresolved issue. In this paper,\\nwe propose AR-MCTS, a universal framework designed to progressively improve the\\nreasoning capabilities of MLLMs through Active Retrieval (AR) and Monte Carlo\\nTree Search (MCTS). Our approach begins with the development of a unified\\nretrieval module that retrieves key supporting insights for solving complex\\nreasoning problems from a hybrid-modal retrieval corpus. To bridge the gap in\\nautomated multimodal reasoning verification, we employ the MCTS algorithm\\ncombined with an active retrieval mechanism, which enables the automatic\\ngeneration of step-wise annotations. This strategy dynamically retrieves key\\ninsights for each reasoning step, moving beyond traditional beam search\\nsampling to improve the diversity and reliability of the reasoning space.\\nAdditionally, we introduce a process reward model that aligns progressively to\\nsupport the automatic verification of multimodal reasoning tasks. Experimental\\nresults across three complex multimodal reasoning benchmarks confirm the\\neffectiveness of the AR-MCTS framework in enhancing the performance of various\\nmultimodal models. Further analysis demonstrates that AR-MCTS can optimize\\nsampling diversity and accuracy, yielding reliable multimodal reasoning.', 'upvotes': 46, 'discussionId': '6764d822772d134796df3ff6'}, 'publishedAt': '2024-12-19T21:38:10.664Z', 'title': 'Progressive Multimodal Reasoning via Active Retrieval', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.14835.png', 'numComments': 1, 'submittedBy': {'_id': '61cd4b833dd34ba1985e0753', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/61cd4b833dd34ba1985e0753/BfHfrwotoMESpXZOHiIe4.png', 'fullname': 'KABI', 'name': 'dongguanting', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 12}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2412.14475', 'authors': [{'_id': '6764da4e0044fe72b24b009e', 'user': {'_id': '6564a2ceedae9c33b7654a1f', 'avatarUrl': '/avatars/42f09356a1282896573ccb44830cd327.svg', 'isPro': False, 'fullname': 'JUNJIE ZHOU', 'user': 'JUNJIE99', 'type': 'user'}, 'name': 'Junjie Zhou', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-20T08:32:59.134Z', 'hidden': False}, {'_id': '6764da4e0044fe72b24b009f', 'user': {'_id': '64a38c590111d5ff6c3d5f2b', 'avatarUrl': '/avatars/ef13dc7ce243819bc0da9b04e778b432.svg', 'isPro': False, 'fullname': 'zhengliu', 'user': 'zl101', 'type': 'user'}, 'name': 'Zheng Liu', 'status': 'extracted_pending', 'statusLastChangedAt': '2024-12-20T02:45:36.277Z', 'hidden': False}, {'_id': '6764da4e0044fe72b24b00a0', 'name': 'Ze Liu', 'hidden': False}, {'_id': '6764da4e0044fe72b24b00a1', 'user': {'_id': '62612679bbcbd1c34f1638af', 'avatarUrl': '/avatars/c0675d05a52192ee14e9ab1633353956.svg', 'isPro': False, 'fullname': 'Xiao', 'user': 'Shitao', 'type': 'user'}, 'name': 'Shitao Xiao', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-20T09:17:36.576Z', 'hidden': False}, {'_id': '6764da4e0044fe72b24b00a2', 'user': {'_id': '6458b59c7a7e192202df8fa0', 'avatarUrl': '/avatars/33ee716477e5686da8723d01e199cd27.svg', 'isPro': False, 'fullname': 'Yueze Wang', 'user': 'yzwang', 'type': 'user'}, 'name': 'Yueze Wang', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-20T09:17:27.209Z', 'hidden': False}, {'_id': '6764da4e0044fe72b24b00a3', 'user': {'_id': '64acafac1aee69ece03af05a', 'avatarUrl': '/avatars/6380777852546dbd432fed5544d5db0b.svg', 'isPro': True, 'fullname': 'Bo Zhao', 'user': 'BoZhaoHuggingFace', 'type': 'user'}, 'name': 'Bo Zhao', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-20T09:17:20.202Z', 'hidden': False}, {'_id': '6764da4e0044fe72b24b00a4', 'user': {'_id': '6465943fe27766e8921bf4b3', 'avatarUrl': '/avatars/86cc7f24c2348c634711807c3261961d.svg', 'isPro': False, 'fullname': 'chenjason', 'user': 'chenjason', 'type': 'user'}, 'name': 'Chen Jason Zhang', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-20T09:17:04.629Z', 'hidden': False}, {'_id': '6764da4e0044fe72b24b00a5', 'user': {'_id': '66ed026076a8038cb4ae6053', 'avatarUrl': '/avatars/99b6527da6b66c6b5df3fc8261587322.svg', 'isPro': False, 'fullname': 'Defu Lian', 'user': 'dove-ustc', 'type': 'user'}, 'name': 'Defu Lian', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-20T09:16:55.115Z', 'hidden': False}, {'_id': '6764da4e0044fe72b24b00a6', 'name': 'Yongping Xiong', 'hidden': False}], 'publishedAt': '2024-12-19T02:49:55.000Z', 'title': 'MegaPairs: Massive Data Synthesis For Universal Multimodal Retrieval', 'summary': 'Despite the rapidly growing demand for multimodal retrieval, progress in this\\nfield remains severely constrained by a lack of training data. In this paper,\\nwe introduce MegaPairs, a novel data synthesis method that leverages vision\\nlanguage models (VLMs) and open-domain images, together with a massive\\nsynthetic dataset generated from this method. Our empirical analysis shows that\\nMegaPairs generates high-quality data, enabling the multimodal retriever to\\nsignificantly outperform the baseline model trained on 70times more data\\nfrom existing datasets. Moreover, since MegaPairs solely relies on general\\nimage corpora and open-source VLMs, it can be easily scaled up, enabling\\ncontinuous improvements in retrieval performance. In this stage, we produced\\nmore than 26 million training instances and trained several models of varying\\nsizes using this data. These new models achieve state-of-the-art zero-shot\\nperformance across 4 popular composed image retrieval (CIR) benchmarks and the\\nhighest overall performance on the 36 datasets provided by MMEB. They also\\ndemonstrate notable performance improvements with additional downstream\\nfine-tuning. Our produced dataset, well-trained models, and data synthesis\\npipeline will be made publicly available to facilitate the future development\\nof this field.', 'upvotes': 43, 'discussionId': '6764da500044fe72b24b0119'}, 'publishedAt': '2024-12-19T21:55:32.124Z', 'title': 'MegaPairs: Massive Data Synthesis For Universal Multimodal Retrieval', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.14475.png', 'numComments': 1, 'submittedBy': {'_id': '6564a2ceedae9c33b7654a1f', 'avatarUrl': '/avatars/42f09356a1282896573ccb44830cd327.svg', 'fullname': 'JUNJIE ZHOU', 'name': 'JUNJIE99', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 5}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2412.15204', 'authors': [{'_id': '6764d4d46be739a319253381', 'user': {'_id': '64ed568ccf6118a9379a61b8', 'avatarUrl': '/avatars/6d040cbcb4a9b624cbe64c9d01cd5c88.svg', 'isPro': False, 'fullname': 'Yushi Bai', 'user': 'bys0318', 'type': 'user'}, 'name': 'Yushi Bai', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-20T08:33:05.814Z', 'hidden': False}, {'_id': '6764d4d46be739a319253382', 'user': {'_id': '648c48d8c0ddeee6df5b6d22', 'avatarUrl': '/avatars/8706b0b16dfc332b96c91d3ced31bd0b.svg', 'isPro': False, 'fullname': 'Shangqing Tu', 'user': 'tsq2000', 'type': 'user'}, 'name': 'Shangqing Tu', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-20T08:33:03.700Z', 'hidden': False}, {'_id': '6764d4d46be739a319253383', 'user': {'_id': '66cdd285c51a915bd5f2d017', 'avatarUrl': '/avatars/14e5794307e4672b1b51d26b31227e0f.svg', 'isPro': False, 'fullname': 'Jiajie Zhang', 'user': 'NeoZ123', 'type': 'user'}, 'name': 'Jiajie Zhang', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-20T09:20:03.123Z', 'hidden': False}, {'_id': '6764d4d46be739a319253384', 'user': {'_id': '660ec5a2509153ca49775a7c', 'avatarUrl': '/avatars/97570fc245cc8ec7628da9c13bd35b71.svg', 'isPro': False, 'fullname': 'Hao Peng', 'user': 'haopeng01', 'type': 'user'}, 'name': 'Hao Peng', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-20T09:19:34.830Z', 'hidden': False}, {'_id': '6764d4d46be739a319253385', 'user': {'_id': '648c48e7a47a0850f9458b67', 'avatarUrl': '/avatars/47d71d80f9901313feb0199c37296389.svg', 'isPro': False, 'fullname': 'Xiaozhi Wang', 'user': 'wangxz098', 'type': 'user'}, 'name': 'Xiaozhi Wang', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-20T09:19:21.316Z', 'hidden': False}, {'_id': '6764d4d46be739a319253386', 'name': 'Xin Lv', 'hidden': False}, {'_id': '6764d4d46be739a319253387', 'user': {'_id': '62c924a6334a6ee11c2e8dfa', 'avatarUrl': '/avatars/3dbc37af162b94d68cb83665ac4528c3.svg', 'isPro': False, 'fullname': 'ShulinCao', 'user': 'caoshulin', 'type': 'user'}, 'name': 'Shulin Cao', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-20T09:19:04.990Z', 'hidden': False}, {'_id': '6764d4d46be739a319253388', 'user': {'_id': '62d7b131f6e8ba66107af761', 'avatarUrl': '/avatars/f1c5df47aef69c824fd166722df8f670.svg', 'isPro': False, 'fullname': 'Jiazheng Xu', 'user': 'xujz0703', 'type': 'user'}, 'name': 'Jiazheng Xu', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-20T09:18:59.503Z', 'hidden': False}, {'_id': '6764d4d46be739a319253389', 'name': 'Lei Hou', 'hidden': False}, {'_id': '6764d4d46be739a31925338a', 'user': {'_id': '640e73bdfdeaae1390857b62', 'avatarUrl': '/avatars/cd6779e30f716002a7838ed93d5c0754.svg', 'isPro': False, 'fullname': 'Yuxiao Dong', 'user': 'yuxiaod', 'type': 'user'}, 'name': 'Yuxiao Dong', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-20T09:18:30.529Z', 'hidden': False}, {'_id': '6764d4d46be739a31925338b', 'user': {'_id': '640dff05474aa6f89556677e', 'avatarUrl': '/avatars/1b4591c7322d649c797b3125148f1915.svg', 'isPro': False, 'fullname': 'Jie Tang', 'user': 'jerytang', 'type': 'user'}, 'name': 'Jie Tang', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-20T09:18:23.208Z', 'hidden': False}, {'_id': '6764d4d46be739a31925338c', 'user': {'_id': '65df8cbc2705d9672f55d1aa', 'avatarUrl': '/avatars/63e46f15bb76bd9d4508fd0f54f39829.svg', 'isPro': False, 'fullname': 'Juanzi Li', 'user': 'juanli', 'type': 'user'}, 'name': 'Juanzi Li', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-20T09:18:13.826Z', 'hidden': False}], 'publishedAt': '2024-12-19T18:59:17.000Z', 'title': 'LongBench v2: Towards Deeper Understanding and Reasoning on Realistic\\n  Long-context Multitasks', 'summary': 'This paper introduces LongBench v2, a benchmark designed to assess the\\nability of LLMs to handle long-context problems requiring deep understanding\\nand reasoning across real-world multitasks. LongBench v2 consists of 503\\nchallenging multiple-choice questions, with contexts ranging from 8k to 2M\\nwords, across six major task categories: single-document QA, multi-document QA,\\nlong in-context learning, long-dialogue history understanding, code repository\\nunderstanding, and long structured data understanding. To ensure the breadth\\nand the practicality, we collect data from nearly 100 highly educated\\nindividuals with diverse professional backgrounds. We employ both automated and\\nmanual review processes to maintain high quality and difficulty, resulting in\\nhuman experts achieving only 53.7% accuracy under a 15-minute time constraint.\\nOur evaluation reveals that the best-performing model, when directly answers\\nthe questions, achieves only 50.1% accuracy. In contrast, the o1-preview model,\\nwhich includes longer reasoning, achieves 57.7%, surpassing the human baseline\\nby 4%. These results highlight the importance of enhanced reasoning ability and\\nscaling inference-time compute to tackle the long-context challenges in\\nLongBench v2. The project is available at https://longbench2.github.io.', 'upvotes': 21, 'discussionId': '6764d4d66be739a3192533ca'}, 'publishedAt': '2024-12-19T21:45:30.981Z', 'title': 'LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.15204.png', 'numComments': 1, 'submittedBy': {'_id': '64ed568ccf6118a9379a61b8', 'avatarUrl': '/avatars/6d040cbcb4a9b624cbe64c9d01cd5c88.svg', 'fullname': 'Yushi Bai', 'name': 'bys0318', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 9}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2412.14689', 'authors': [{'_id': '6764e1dfc51db09f8c3cd75b', 'user': {'_id': '647ffddeb82adfa7cc1a10d9', 'avatarUrl': '/avatars/26aa168d6b2068298ebb16584aa52b6c.svg', 'isPro': False, 'fullname': 'zhu', 'user': 'xuekai', 'type': 'user'}, 'name': 'Xuekai Zhu', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-20T09:20:24.860Z', 'hidden': False}, {'_id': '6764e1dfc51db09f8c3cd75c', 'user': {'_id': '649e6761f9134a06ed1e0cea', 'avatarUrl': '/avatars/00b5dcb744c54a4aa18fe08efd70d6ff.svg', 'isPro': False, 'fullname': 'Daixuan Cheng', 'user': 'daixuancheng', 'type': 'user'}, 'name': 'Daixuan Cheng', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-20T09:20:32.479Z', 'hidden': False}, {'_id': '6764e1dfc51db09f8c3cd75d', 'user': {'_id': '63256836ff539edeea8a8660', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1663395861151-noauth.png', 'isPro': False, 'fullname': 'Li Hengli', 'user': 'Hengli', 'type': 'user'}, 'name': 'Hengli Li', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-20T09:20:41.812Z', 'hidden': False}, {'_id': '6764e1dfc51db09f8c3cd75e', 'user': {'_id': '60bc94cd85a3ab33829b6211', 'avatarUrl': '/avatars/b57d36c7577fbbb42ea5b963eef4144a.svg', 'isPro': False, 'fullname': 'Kaiyan Zhang', 'user': 'iseesaw', 'type': 'user'}, 'name': 'Kaiyan Zhang', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-20T09:20:57.241Z', 'hidden': False}, {'_id': '6764e1dfc51db09f8c3cd75f', 'name': 'Ermo Hua', 'hidden': False}, {'_id': '6764e1dfc51db09f8c3cd760', 'user': {'_id': '663f07d029be04778ba97871', 'avatarUrl': '/avatars/fb7c9d4a2c537d918a3267e7cbc03f04.svg', 'isPro': False, 'fullname': 'Xingtai Lv', 'user': 'XingtaiHF', 'type': 'user'}, 'name': 'Xingtai Lv', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-20T09:21:08.043Z', 'hidden': False}, {'_id': '6764e1dfc51db09f8c3cd761', 'user': {'_id': '60cf4bcb1ce3775ebb86e5d5', 'avatarUrl': '/avatars/12bcd18d215abf91f297f93007733148.svg', 'isPro': False, 'fullname': 'Ning Ding', 'user': 'stingning', 'type': 'user'}, 'name': 'Ning Ding', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-20T09:21:13.963Z', 'hidden': False}, {'_id': '6764e1dfc51db09f8c3cd762', 'name': 'Zhouhan Lin', 'hidden': False}, {'_id': '6764e1dfc51db09f8c3cd763', 'user': {'_id': '63a95a6a7930fa8c7dd63d4e', 'avatarUrl': '/avatars/d9d0420f7ddfe2f3a7e029fb05f1c89f.svg', 'isPro': False, 'fullname': 'Zilong Zheng', 'user': 'zlzheng', 'type': 'user'}, 'name': 'Zilong Zheng', 'status': 'extracted_pending', 'statusLastChangedAt': '2024-12-20T03:17:52.795Z', 'hidden': False}, {'_id': '6764e1dfc51db09f8c3cd764', 'user': {'_id': '669f614b59adf5b56e05bce3', 'avatarUrl': '/avatars/ffd4189efbceb0e63a03db273065a44b.svg', 'isPro': False, 'fullname': 'BowenZhou', 'user': 'bowenZhou', 'type': 'user'}, 'name': 'Bowen Zhou', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-20T09:21:41.799Z', 'hidden': False}], 'publishedAt': '2024-12-19T09:43:39.000Z', 'title': 'How to Synthesize Text Data without Model Collapse?', 'summary': 'Model collapse in synthetic data indicates that iterative training on\\nself-generated data leads to a gradual decline in performance. With the\\nproliferation of AI models, synthetic data will fundamentally reshape the web\\ndata ecosystem. Future GPT-{n} models will inevitably be trained on a blend\\nof synthetic and human-produced data. In this paper, we focus on two questions:\\nwhat is the impact of synthetic data on language model training, and how to\\nsynthesize data without model collapse? We first pre-train language models\\nacross different proportions of synthetic data, revealing a negative\\ncorrelation between the proportion of synthetic data and model performance. We\\nfurther conduct statistical analysis on synthetic data to uncover\\ndistributional shift phenomenon and over-concentration of n-gram features.\\nInspired by the above findings, we propose token editing on human-produced data\\nto obtain semi-synthetic data. As a proof of concept, we theoretically\\ndemonstrate that token-level editing can prevent model collapse, as the test\\nerror is constrained by a finite upper bound. We conduct extensive experiments\\non pre-training from scratch, continual pre-training, and supervised\\nfine-tuning. The results validate our theoretical proof that token-level\\nediting improves data quality and enhances model performance.', 'upvotes': 16, 'discussionId': '6764e1e0c51db09f8c3cd793'}, 'publishedAt': '2024-12-19T22:23:57.229Z', 'title': 'How to Synthesize Text Data without Model Collapse?', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.14689.png', 'numComments': 1, 'submittedBy': {'_id': '649e6761f9134a06ed1e0cea', 'avatarUrl': '/avatars/00b5dcb744c54a4aa18fe08efd70d6ff.svg', 'fullname': 'Daixuan Cheng', 'name': 'daixuancheng', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 5}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2412.15213', 'authors': [{'_id': '6764dd4d469e108330dc958a', 'user': {'_id': '639f1e519f1f2baab2f00d22', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/639f1e519f1f2baab2f00d22/pFjd51WZuVZ3A11rItvmk.jpeg', 'isPro': False, 'fullname': 'Qihao Liu', 'user': 'QHL067', 'type': 'user'}, 'name': 'Qihao Liu', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-20T09:21:57.934Z', 'hidden': False}, {'_id': '6764dd4d469e108330dc958b', 'name': 'Xi Yin', 'hidden': False}, {'_id': '6764dd4d469e108330dc958c', 'name': 'Alan Yuille', 'hidden': False}, {'_id': '6764dd4d469e108330dc958d', 'name': 'Andrew Brown', 'hidden': False}, {'_id': '6764dd4d469e108330dc958e', 'user': {'_id': '650fece35877b1c077298b81', 'avatarUrl': '/avatars/be9c948ecf9c413ad4c8aa6598d3ba86.svg', 'isPro': False, 'fullname': 'Mannat Singh', 'user': 'mannatsingh', 'type': 'user'}, 'name': 'Mannat Singh', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-20T09:22:34.696Z', 'hidden': False}], 'publishedAt': '2024-12-19T18:59:56.000Z', 'title': 'Flowing from Words to Pixels: A Framework for Cross-Modality Evolution', 'summary': 'Diffusion models, and their generalization, flow matching, have had a\\nremarkable impact on the field of media generation. Here, the conventional\\napproach is to learn the complex mapping from a simple source distribution of\\nGaussian noise to the target media distribution. For cross-modal tasks such as\\ntext-to-image generation, this same mapping from noise to image is learnt\\nwhilst including a conditioning mechanism in the model. One key and thus far\\nrelatively unexplored feature of flow matching is that, unlike Diffusion\\nmodels, they are not constrained for the source distribution to be noise.\\nHence, in this paper, we propose a paradigm shift, and ask the question of\\nwhether we can instead train flow matching models to learn a direct mapping\\nfrom the distribution of one modality to the distribution of another, thus\\nobviating the need for both the noise distribution and conditioning mechanism.\\nWe present a general and simple framework, CrossFlow, for cross-modal flow\\nmatching. We show the importance of applying Variational Encoders to the input\\ndata, and introduce a method to enable Classifier-free guidance. Surprisingly,\\nfor text-to-image, CrossFlow with a vanilla transformer without cross attention\\nslightly outperforms standard flow matching, and we show that it scales better\\nwith training steps and model size, while also allowing for interesting latent\\narithmetic which results in semantically meaningful edits in the output space.\\nTo demonstrate the generalizability of our approach, we also show that\\nCrossFlow is on par with or outperforms the state-of-the-art for various\\ncross-modal / intra-modal mapping tasks, viz. image captioning, depth\\nestimation, and image super-resolution. We hope this paper contributes to\\naccelerating progress in cross-modal media generation.', 'upvotes': 16, 'discussionId': '6764dd4f469e108330dc9601'}, 'publishedAt': '2024-12-19T22:01:33.562Z', 'title': 'Flowing from Words to Pixels: A Framework for Cross-Modality Evolution', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.15213.png', 'numComments': 1, 'submittedBy': {'_id': '639f1e519f1f2baab2f00d22', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/639f1e519f1f2baab2f00d22/pFjd51WZuVZ3A11rItvmk.jpeg', 'fullname': 'Qihao Liu', 'name': 'QHL067', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 2}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2412.15214', 'authors': [{'_id': '6764f548cee1fdbd9765e9bc', 'user': {'_id': '665f059a8947302aa2c63afe', 'avatarUrl': '/avatars/50f560285946532321a0bd526494148d.svg', 'isPro': False, 'fullname': 'hanlin wang', 'user': 'hlwang06', 'type': 'user'}, 'name': 'Hanlin Wang', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-20T08:32:36.964Z', 'hidden': False}, {'_id': '6764f548cee1fdbd9765e9bd', 'name': 'Hao Ouyang', 'hidden': False}, {'_id': '6764f548cee1fdbd9765e9be', 'user': {'_id': '64981bea09cea550852652af', 'avatarUrl': '/avatars/df528e9008972c8e5ae4d278e617476c.svg', 'isPro': False, 'fullname': 'Qiuyu Wang', 'user': 'qiuyuu', 'type': 'user'}, 'name': 'Qiuyu Wang', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-20T08:32:28.937Z', 'hidden': False}, {'_id': '6764f548cee1fdbd9765e9bf', 'name': 'Wen Wang', 'hidden': False}, {'_id': '6764f548cee1fdbd9765e9c0', 'user': {'_id': '64acd2ec39fcfebff8c79c00', 'avatarUrl': '/avatars/9419384846b92182f2c47ce2fbd0f8d3.svg', 'isPro': False, 'fullname': 'Ka Leong Cheng', 'user': 'felixcheng97', 'type': 'user'}, 'name': 'Ka Leong Cheng', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-20T09:26:43.630Z', 'hidden': False}, {'_id': '6764f548cee1fdbd9765e9c1', 'user': {'_id': '6467b121e7a6a374fd19b44b', 'avatarUrl': '/avatars/3f2874d58986d651aef55e3408b05700.svg', 'isPro': False, 'fullname': 'Qifeng Chen', 'user': 'cqf', 'type': 'user'}, 'name': 'Qifeng Chen', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-20T09:26:37.950Z', 'hidden': False}, {'_id': '6764f548cee1fdbd9765e9c2', 'name': 'Yujun Shen', 'hidden': False}, {'_id': '6764f548cee1fdbd9765e9c3', 'user': {'_id': '62c77f4352d8ae531f5511f9', 'avatarUrl': '/avatars/50198ccb02ccd286975a4613fbabee28.svg', 'isPro': False, 'fullname': 'Limin Wang', 'user': 'lmwang', 'type': 'user'}, 'name': 'Limin Wang', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-20T09:26:23.360Z', 'hidden': False}], 'publishedAt': '2024-12-19T18:59:56.000Z', 'title': 'LeviTor: 3D Trajectory Oriented Image-to-Video Synthesis', 'summary': 'The intuitive nature of drag-based interaction has led to its growing\\nadoption for controlling object trajectories in image-to-video synthesis.\\nStill, existing methods that perform dragging in the 2D space usually face\\nambiguity when handling out-of-plane movements. In this work, we augment the\\ninteraction with a new dimension, i.e., the depth dimension, such that users\\nare allowed to assign a relative depth for each point on the trajectory. That\\nway, our new interaction paradigm not only inherits the convenience from 2D\\ndragging, but facilitates trajectory control in the 3D space, broadening the\\nscope of creativity. We propose a pioneering method for 3D trajectory control\\nin image-to-video synthesis by abstracting object masks into a few cluster\\npoints. These points, accompanied by the depth information and the instance\\ninformation, are finally fed into a video diffusion model as the control\\nsignal. Extensive experiments validate the effectiveness of our approach,\\ndubbed LeviTor, in precisely manipulating the object movements when producing\\nphoto-realistic videos from static images. Project page:\\nhttps://ppetrichor.github.io/levitor.github.io/', 'upvotes': 12, 'discussionId': '6764f549cee1fdbd9765ea31'}, 'publishedAt': '2024-12-19T23:42:38.162Z', 'title': 'LeviTor: 3D Trajectory Oriented Image-to-Video Synthesis', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.15214.png', 'numComments': 2, 'submittedBy': {'_id': '64981bea09cea550852652af', 'avatarUrl': '/avatars/df528e9008972c8e5ae4d278e617476c.svg', 'fullname': 'Qiuyu Wang', 'name': 'qiuyuu', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2412.14462', 'authors': [{'_id': '6764e0339aeafaa0d8405e74', 'user': {'_id': '64a5bda1733731f38378fdd3', 'avatarUrl': '/avatars/3ec7c553f92ced4ae0420ce16f7e71b9.svg', 'isPro': False, 'fullname': 'Jixuan HE', 'user': 'Kakituken', 'type': 'user'}, 'name': 'Jixuan He', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-20T09:24:47.947Z', 'hidden': False}, {'_id': '6764e0339aeafaa0d8405e75', 'user': {'_id': '658bb7e47459b6e471b9d2e6', 'avatarUrl': '/avatars/efd8051b468b4dbcb5d149479de67c58.svg', 'isPro': False, 'fullname': 'Wanhua Li', 'user': 'EthanTaylor', 'type': 'user'}, 'name': 'Wanhua Li', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-20T08:32:44.737Z', 'hidden': False}, {'_id': '6764e0339aeafaa0d8405e76', 'name': 'Ye Liu', 'hidden': False}, {'_id': '6764e0339aeafaa0d8405e77', 'name': 'Junsik Kim', 'hidden': False}, {'_id': '6764e0339aeafaa0d8405e78', 'user': {'_id': '628c2c8ab80bb09700d6cb1d', 'avatarUrl': '/avatars/384f75778fd5f07f249f2815a3039dca.svg', 'isPro': False, 'fullname': 'donglai wei', 'user': 'dwei', 'type': 'user'}, 'name': 'Donglai Wei', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-20T09:25:00.011Z', 'hidden': False}, {'_id': '6764e0339aeafaa0d8405e79', 'user': {'_id': '62acc69e36f7c7b7f65fccca', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/62acc69e36f7c7b7f65fccca/S8o0XE6TaQwLU8q3QPkct.png', 'isPro': False, 'fullname': 'Hanspeter Pfister', 'user': 'hpfister', 'type': 'user'}, 'name': 'Hanspeter Pfister', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-20T09:24:54.200Z', 'hidden': False}], 'publishedAt': '2024-12-19T02:23:13.000Z', 'title': 'Affordance-Aware Object Insertion via Mask-Aware Dual Diffusion', 'summary': 'As a common image editing operation, image composition involves integrating\\nforeground objects into background scenes. In this paper, we expand the\\napplication of the concept of Affordance from human-centered image composition\\ntasks to a more general object-scene composition framework, addressing the\\ncomplex interplay between foreground objects and background scenes. Following\\nthe principle of Affordance, we define the affordance-aware object insertion\\ntask, which aims to seamlessly insert any object into any scene with various\\nposition prompts. To address the limited data issue and incorporate this task,\\nwe constructed the SAM-FB dataset, which contains over 3 million examples\\nacross more than 3,000 object categories. Furthermore, we propose the\\nMask-Aware Dual Diffusion (MADD) model, which utilizes a dual-stream\\narchitecture to simultaneously denoise the RGB image and the insertion mask. By\\nexplicitly modeling the insertion mask in the diffusion process, MADD\\neffectively facilitates the notion of affordance. Extensive experimental\\nresults show that our method outperforms the state-of-the-art methods and\\nexhibits strong generalization performance on in-the-wild images. Please refer\\nto our code on https://github.com/KaKituken/affordance-aware-any.', 'upvotes': 12, 'discussionId': '6764e0389aeafaa0d8405f9e'}, 'publishedAt': '2024-12-19T22:20:23.883Z', 'title': 'Affordance-Aware Object Insertion via Mask-Aware Dual Diffusion', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/658bb7e47459b6e471b9d2e6/BeBx0G4iyjtUIO5RHxGdL.qt'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.14462.png', 'numComments': 1, 'submittedBy': {'_id': '658bb7e47459b6e471b9d2e6', 'avatarUrl': '/avatars/efd8051b468b4dbcb5d149479de67c58.svg', 'fullname': 'Wanhua Li', 'name': 'EthanTaylor', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2412.15200', 'authors': [{'_id': '6764da71bdc5692a8d6bccf6', 'user': {'_id': '66054d519af76210c2ee4b8e', 'avatarUrl': '/avatars/c1854765b1e45ec33602b2cb9443f82a.svg', 'isPro': True, 'fullname': 'Wang Zhao', 'user': 'thuzhaowang', 'type': 'user'}, 'name': 'Wang Zhao', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-20T08:32:57.404Z', 'hidden': False}, {'_id': '6764da71bdc5692a8d6bccf7', 'user': {'_id': '638066faf022c8a5803f7eb8', 'avatarUrl': '/avatars/4cfd699c3f6c5461b12b7dc5e3fe183d.svg', 'isPro': False, 'fullname': 'Yanpei Cao', 'user': 'pookiefoof', 'type': 'user'}, 'name': 'Yan-Pei Cao', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-20T09:27:44.668Z', 'hidden': False}, {'_id': '6764da71bdc5692a8d6bccf8', 'user': {'_id': '62c695829db11473f08af1cd', 'avatarUrl': '/avatars/cacb54077892a44aef81454dc107df4f.svg', 'isPro': True, 'fullname': 'Jiale Xu', 'user': 'bluestyle97', 'type': 'user'}, 'name': 'Jiale Xu', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-20T09:28:27.765Z', 'hidden': False}, {'_id': '6764da71bdc5692a8d6bccf9', 'name': 'Yuejiang Dong', 'hidden': False}, {'_id': '6764da71bdc5692a8d6bccfa', 'user': {'_id': '63ca3ddc04c979828310bfcb', 'avatarUrl': '/avatars/615e0d8622950b4408b40d550f02a894.svg', 'isPro': False, 'fullname': 'Ying Shan', 'user': 'yshan2u', 'type': 'user'}, 'name': 'Ying Shan', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-20T09:28:06.210Z', 'hidden': False}], 'publishedAt': '2024-12-19T18:58:46.000Z', 'title': 'DI-PCG: Diffusion-based Efficient Inverse Procedural Content Generation\\n  for High-quality 3D Asset Creation', 'summary': 'Procedural Content Generation (PCG) is powerful in creating high-quality 3D\\ncontents, yet controlling it to produce desired shapes is difficult and often\\nrequires extensive parameter tuning. Inverse Procedural Content Generation aims\\nto automatically find the best parameters under the input condition. However,\\nexisting sampling-based and neural network-based methods still suffer from\\nnumerous sample iterations or limited controllability. In this work, we present\\nDI-PCG, a novel and efficient method for Inverse PCG from general image\\nconditions. At its core is a lightweight diffusion transformer model, where PCG\\nparameters are directly treated as the denoising target and the observed images\\nas conditions to control parameter generation. DI-PCG is efficient and\\neffective. With only 7.6M network parameters and 30 GPU hours to train, it\\ndemonstrates superior performance in recovering parameters accurately, and\\ngeneralizing well to in-the-wild images. Quantitative and qualitative\\nexperiment results validate the effectiveness of DI-PCG in inverse PCG and\\nimage-to-3D generation tasks. DI-PCG offers a promising approach for efficient\\ninverse PCG and represents a valuable exploration step towards a 3D generation\\npath that models how to construct a 3D asset using parametric models.', 'upvotes': 8, 'discussionId': '6764da76bdc5692a8d6bcedf'}, 'publishedAt': '2024-12-19T22:24:46.171Z', 'title': 'DI-PCG: Diffusion-based Efficient Inverse Procedural Content Generation for High-quality 3D Asset Creation', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.15200.png', 'numComments': 1, 'submittedBy': {'_id': '66054d519af76210c2ee4b8e', 'avatarUrl': '/avatars/c1854765b1e45ec33602b2cb9443f82a.svg', 'fullname': 'Wang Zhao', 'name': 'thuzhaowang', 'type': 'user', 'isPro': True, 'isHf': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2412.15084', 'authors': [{'_id': '6764e3e30afbb34519fd2018', 'user': {'_id': '65f33b1c9f7970ccc0234cbf', 'avatarUrl': '/avatars/99fbab303912e3674663251c04279907.svg', 'isPro': False, 'fullname': 'Zihan Liu', 'user': 'zihanliu', 'type': 'user'}, 'name': 'Zihan Liu', 'status': 'extracted_pending', 'statusLastChangedAt': '2024-12-20T03:26:28.621Z', 'hidden': False}, {'_id': '6764e3e30afbb34519fd2019', 'user': {'_id': '62bc9d90e81dfd65cced9316', 'avatarUrl': '/avatars/41b9e57988866de7d4dacb8a09fa3b62.svg', 'isPro': False, 'fullname': 'Yang Chen', 'user': 'ychenNLP', 'type': 'user'}, 'name': 'Yang Chen', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-20T08:32:40.034Z', 'hidden': False}, {'_id': '6764e3e30afbb34519fd201a', 'user': {'_id': '6641544c695975af2cbd0da6', 'avatarUrl': '/avatars/0ad3c18dcba585259b064fe9b00a07ce.svg', 'isPro': False, 'fullname': 'Mohammad Shoeybi', 'user': 'shoeybi', 'type': 'user'}, 'name': 'Mohammad Shoeybi', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-20T09:27:29.660Z', 'hidden': False}, {'_id': '6764e3e30afbb34519fd201b', 'user': {'_id': '6311021788942700629e6247', 'avatarUrl': '/avatars/e7adc1632b76e80e7e4a590033d1c20a.svg', 'isPro': False, 'fullname': 'Bryan Catanzaro', 'user': 'ctnzr', 'type': 'user'}, 'name': 'Bryan Catanzaro', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-20T09:27:23.912Z', 'hidden': False}, {'_id': '6764e3e30afbb34519fd201c', 'user': {'_id': '663ee43bfeeb49803537da98', 'avatarUrl': '/avatars/17c3e9c435cc36fb04b4589e6176a243.svg', 'isPro': False, 'fullname': 'Wei Ping', 'user': 'wping', 'type': 'user'}, 'name': 'Wei Ping', 'status': 'extracted_pending', 'statusLastChangedAt': '2024-12-20T03:26:28.621Z', 'hidden': False}], 'publishedAt': '2024-12-19T17:29:44.000Z', 'title': 'AceMath: Advancing Frontier Math Reasoning with Post-Training and Reward\\n  Modeling', 'summary': 'In this paper, we introduce AceMath, a suite of frontier math models that\\nexcel in solving complex math problems, along with highly effective reward\\nmodels capable of evaluating generated solutions and reliably identifying the\\ncorrect ones. To develop the instruction-tuned math models, we propose a\\nsupervised fine-tuning (SFT) process that first achieves competitive\\nperformance across general domains, followed by targeted fine-tuning for the\\nmath domain using a carefully curated set of prompts and synthetically\\ngenerated responses. The resulting model, AceMath-72B-Instruct greatly\\noutperforms Qwen2.5-Math-72B-Instruct, GPT-4o and Claude-3.5 Sonnet. To develop\\nmath-specialized reward model, we first construct AceMath-RewardBench, a\\ncomprehensive and robust benchmark for evaluating math reward models across\\ndiverse problems and difficulty levels. After that, we present a systematic\\napproach to build our math reward models. The resulting model, AceMath-72B-RM,\\nconsistently outperforms state-of-the-art reward models. Furthermore, when\\ncombining AceMath-72B-Instruct with AceMath-72B-RM, we achieve the highest\\naverage rm@8 score across the math reasoning benchmarks. We will release model\\nweights, training data, and evaluation benchmarks at:\\nhttps://research.nvidia.com/labs/adlr/acemath', 'upvotes': 6, 'discussionId': '6764e3e40afbb34519fd206d'}, 'publishedAt': '2024-12-19T22:27:39.645Z', 'title': 'AceMath: Advancing Frontier Math Reasoning with Post-Training and Reward Modeling', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.15084.png', 'numComments': 1, 'submittedBy': {'_id': '62bc9d90e81dfd65cced9316', 'avatarUrl': '/avatars/41b9e57988866de7d4dacb8a09fa3b62.svg', 'fullname': 'Yang Chen', 'name': 'ychenNLP', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 2}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2412.14233', 'authors': [{'_id': '6764e3c22086097d58dc7fc4', 'user': {'_id': '64297212e5f33939cf3a3d9b', 'avatarUrl': '/avatars/bd21759ab5d7e526b99fcb7ed813ffb3.svg', 'isPro': False, 'fullname': 'yanpeng_sun', 'user': 'syp115', 'type': 'user'}, 'name': 'Yanpeng Sun', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-20T08:32:42.316Z', 'hidden': False}, {'_id': '6764e3c22086097d58dc7fc5', 'name': 'Jing Hao', 'hidden': False}, {'_id': '6764e3c22086097d58dc7fc6', 'name': 'Ke Zhu', 'hidden': False}, {'_id': '6764e3c22086097d58dc7fc7', 'name': 'Jiang-Jiang Liu', 'hidden': False}, {'_id': '6764e3c22086097d58dc7fc8', 'user': {'_id': '641966f6e8a6183a8caa3145', 'avatarUrl': '/avatars/a7d7096fed7e49fcc04f2ef494e9c381.svg', 'isPro': False, 'fullname': 'yuxiang zhao', 'user': 'cloud913', 'type': 'user'}, 'name': 'Yuxiang Zhao', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-20T09:33:04.173Z', 'hidden': False}, {'_id': '6764e3c22086097d58dc7fc9', 'user': {'_id': '65f425f9bfb2f1451f237827', 'avatarUrl': '/avatars/36b0bce9364d88f10081847befd29787.svg', 'isPro': False, 'fullname': 'Xiaofan Li', 'user': 'FuNz', 'type': 'user'}, 'name': 'Xiaofan Li', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-20T09:33:18.741Z', 'hidden': False}, {'_id': '6764e3c22086097d58dc7fca', 'name': 'Gang Zhang', 'hidden': False}, {'_id': '6764e3c22086097d58dc7fcb', 'name': 'Zechao Li', 'hidden': False}, {'_id': '6764e3c22086097d58dc7fcc', 'name': 'Jingdong Wang', 'hidden': False}], 'publishedAt': '2024-12-18T18:45:43.000Z', 'title': 'Descriptive Caption Enhancement with Visual Specialists for Multimodal\\n  Perception', 'summary': 'Training Large Multimodality Models (LMMs) relies on descriptive image\\ncaption that connects image and language. Existing methods either distill the\\ncaption from the LMM models or construct the captions from the internet images\\nor by human. We propose to leverage off-the-shelf visual specialists, which\\nwere trained from annotated images initially not for image captioning, for\\nenhancing the image caption.\\n  Our approach, named DCE, explores object low-level and fine-grained\\nattributes (e.g., depth, emotion and fine-grained categories) and object\\nrelations (e.g., relative location and human-object-interaction (HOI)), and\\ncombine the attributes into the descriptive caption. Experiments demonstrate\\nthat such visual specialists are able to improve the performance for visual\\nunderstanding tasks as well as reasoning that benefits from more accurate\\nvisual understanding. We will release the source code and the pipeline so that\\nother visual specialists are easily combined into the pipeline. The complete\\nsource code of DCE pipeline and datasets will be available at\\nhttps://github.com/syp2ysy/DCE.', 'upvotes': 4, 'discussionId': '6764e3c32086097d58dc8000'}, 'publishedAt': '2024-12-19T22:27:13.562Z', 'title': 'Descriptive Caption Enhancement with Visual Specialists for Multimodal Perception', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.14233.png', 'numComments': 1, 'submittedBy': {'_id': '64297212e5f33939cf3a3d9b', 'avatarUrl': '/avatars/bd21759ab5d7e526b99fcb7ed813ffb3.svg', 'fullname': 'yanpeng_sun', 'name': 'syp115', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2412.15216', 'authors': [{'_id': '6764deae8ae9bee011733928', 'user': {'_id': '63412f2add8853dc7e306a4f', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/tnPbjO1jAvycUkeooUxHD.png', 'isPro': True, 'fullname': 'Enis Simsar', 'user': 'enisimsar', 'type': 'user'}, 'name': 'Enis Simsar', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-20T08:32:47.269Z', 'hidden': False}, {'_id': '6764deae8ae9bee011733929', 'user': {'_id': '63402b30e670ff9cf63d8caa', 'avatarUrl': '/avatars/0aee84d132a78d4ec71663836a57a245.svg', 'isPro': False, 'fullname': 'Alessio Tonioni', 'user': 'Alessiot', 'type': 'user'}, 'name': 'Alessio Tonioni', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-20T09:28:56.892Z', 'hidden': False}, {'_id': '6764deae8ae9bee01173392a', 'name': 'Yongqin Xian', 'hidden': False}, {'_id': '6764deae8ae9bee01173392b', 'user': {'_id': '6630831ce888d89069e6276a', 'avatarUrl': '/avatars/b40d00ac8978405dd2ae66166ac969ba.svg', 'isPro': False, 'fullname': 'Thomas Hofmann', 'user': 'thofmann', 'type': 'user'}, 'name': 'Thomas Hofmann', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-20T09:28:42.685Z', 'hidden': False}, {'_id': '6764deae8ae9bee01173392c', 'name': 'Federico Tombari', 'hidden': False}], 'publishedAt': '2024-12-19T18:59:58.000Z', 'title': 'UIP2P: Unsupervised Instruction-based Image Editing via Cycle Edit\\n  Consistency', 'summary': 'We propose an unsupervised model for instruction-based image editing that\\neliminates the need for ground-truth edited images during training. Existing\\nsupervised methods depend on datasets containing triplets of input image,\\nedited image, and edit instruction. These are generated by either existing\\nediting methods or human-annotations, which introduce biases and limit their\\ngeneralization ability. Our method addresses these challenges by introducing a\\nnovel editing mechanism called Cycle Edit Consistency (CEC), which applies\\nforward and backward edits in one training step and enforces consistency in\\nimage and attention spaces. This allows us to bypass the need for ground-truth\\nedited images and unlock training for the first time on datasets comprising\\neither real image-caption pairs or image-caption-edit triplets. We empirically\\nshow that our unsupervised technique performs better across a broader range of\\nedits with high fidelity and precision. By eliminating the need for\\npre-existing datasets of triplets, reducing biases associated with supervised\\nmethods, and proposing CEC, our work represents a significant advancement in\\nunblocking scaling of instruction-based image editing.', 'upvotes': 4, 'discussionId': '6764deaf8ae9bee0117339a6'}, 'publishedAt': '2024-12-19T22:04:22.958Z', 'title': 'UIP2P: Unsupervised Instruction-based Image Editing via Cycle Edit Consistency', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.15216.png', 'numComments': 1, 'submittedBy': {'_id': '63412f2add8853dc7e306a4f', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/tnPbjO1jAvycUkeooUxHD.png', 'fullname': 'Enis Simsar', 'name': 'enisimsar', 'type': 'user', 'isPro': True, 'isHf': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2412.14642', 'authors': [{'_id': '6764de0ca246952fabef4309', 'name': 'Jiatong Li', 'hidden': False}, {'_id': '6764de0ca246952fabef430a', 'user': {'_id': '656ae4088fb1ddf0d5ec9ac5', 'avatarUrl': '/avatars/e38468d2c0274f3c0f5732f30a2e3436.svg', 'isPro': False, 'fullname': 'Junxian Li', 'user': 'Duke-de-Artois', 'type': 'user'}, 'name': 'Junxian Li', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-20T08:32:48.973Z', 'hidden': False}, {'_id': '6764de0ca246952fabef430b', 'name': 'Yunqing Liu', 'hidden': False}, {'_id': '6764de0ca246952fabef430c', 'user': {'_id': '6538b861613fe158bd581e35', 'avatarUrl': '/avatars/6817dbfe903675721fd227058b0a91ac.svg', 'isPro': False, 'fullname': 'Dongzhan Zhou', 'user': 'schrodingers-tiger', 'type': 'user'}, 'name': 'Dongzhan Zhou', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-20T09:30:10.820Z', 'hidden': False}, {'_id': '6764de0ca246952fabef430d', 'name': 'Qing Li', 'hidden': False}], 'publishedAt': '2024-12-19T08:51:16.000Z', 'title': 'TOMG-Bench: Evaluating LLMs on Text-based Open Molecule Generation', 'summary': 'In this paper, we propose Text-based Open Molecule Generation Benchmark\\n(TOMG-Bench), the first benchmark to evaluate the open-domain molecule\\ngeneration capability of LLMs. TOMG-Bench encompasses a dataset of three major\\ntasks: molecule editing (MolEdit), molecule optimization (MolOpt), and\\ncustomized molecule generation (MolCustom). Each task further contains three\\nsubtasks, with each subtask comprising 5,000 test samples. Given the inherent\\ncomplexity of open molecule generation, we have also developed an automated\\nevaluation system that helps measure both the quality and the accuracy of the\\ngenerated molecules. Our comprehensive benchmarking of 25 LLMs reveals the\\ncurrent limitations and potential areas for improvement in text-guided molecule\\ndiscovery. Furthermore, with the assistance of OpenMolIns, a specialized\\ninstruction tuning dataset proposed for solving challenges raised by\\nTOMG-Bench, Llama3.1-8B could outperform all the open-source general LLMs, even\\nsurpassing GPT-3.5-turbo by 46.5\\\\% on TOMG-Bench. Our codes and datasets are\\navailable through https://github.com/phenixace/TOMG-Bench.', 'upvotes': 3, 'discussionId': '6764de0da246952fabef4389'}, 'publishedAt': '2024-12-19T22:03:19.323Z', 'title': 'TOMG-Bench: Evaluating LLMs on Text-based Open Molecule Generation', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.14642.png', 'numComments': 1, 'submittedBy': {'_id': '6438e55cb2ea24b52ebc45ec', 'avatarUrl': '/avatars/e13c7398f77e7e0bd5eed03102aa5c36.svg', 'fullname': 'Jiatong LI', 'name': 'phenixace', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 2}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2412.14283', 'authors': [{'_id': '6765152de07adde9c961fabb', 'user': {'_id': '64ac49ccb7d86b40fd60a8dd', 'avatarUrl': '/avatars/e9f5482cffdd1d5917523a496a3805f0.svg', 'isPro': False, 'fullname': 'Liyao Jiang', 'user': 'LiyaoJiang', 'type': 'user'}, 'name': 'Liyao Jiang', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-20T08:32:26.476Z', 'hidden': False}, {'_id': '6765152de07adde9c961fabc', 'name': 'Negar Hassanpour', 'hidden': False}, {'_id': '6765152de07adde9c961fabd', 'name': 'Mohammad Salameh', 'hidden': False}, {'_id': '6765152de07adde9c961fabe', 'name': 'Mohammadreza Samadi', 'hidden': False}, {'_id': '6765152de07adde9c961fabf', 'name': 'Jiao He', 'hidden': False}, {'_id': '6765152de07adde9c961fac0', 'name': 'Fengyu Sun', 'hidden': False}, {'_id': '6765152de07adde9c961fac1', 'name': 'Di Niu', 'hidden': False}], 'publishedAt': '2024-12-18T19:24:15.000Z', 'title': 'PixelMan: Consistent Object Editing with Diffusion Models via Pixel\\n  Manipulation and Generation', 'summary': 'Recent research explores the potential of Diffusion Models (DMs) for\\nconsistent object editing, which aims to modify object position, size, and\\ncomposition, etc., while preserving the consistency of objects and background\\nwithout changing their texture and attributes. Current inference-time methods\\noften rely on DDIM inversion, which inherently compromises efficiency and the\\nachievable consistency of edited images. Recent methods also utilize energy\\nguidance which iteratively updates the predicted noise and can drive the\\nlatents away from the original image, resulting in distortions. In this paper,\\nwe propose PixelMan, an inversion-free and training-free method for achieving\\nconsistent object editing via Pixel Manipulation and generation, where we\\ndirectly create a duplicate copy of the source object at target location in the\\npixel space, and introduce an efficient sampling approach to iteratively\\nharmonize the manipulated object into the target location and inpaint its\\noriginal location, while ensuring image consistency by anchoring the edited\\nimage to be generated to the pixel-manipulated image as well as by introducing\\nvarious consistency-preserving optimization techniques during inference.\\nExperimental evaluations based on benchmark datasets as well as extensive\\nvisual comparisons show that in as few as 16 inference steps, PixelMan\\noutperforms a range of state-of-the-art training-based and training-free\\nmethods (usually requiring 50 steps) on multiple consistent object editing\\ntasks.', 'upvotes': 1, 'discussionId': '67651533e07adde9c961fce3'}, 'publishedAt': '2024-12-20T09:51:46.571Z', 'title': 'PixelMan: Consistent Object Editing with Diffusion Models via Pixel Manipulation and Generation', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.14283.png', 'numComments': 1, 'submittedBy': {'_id': '64ac49ccb7d86b40fd60a8dd', 'avatarUrl': '/avatars/e9f5482cffdd1d5917523a496a3805f0.svg', 'fullname': 'Liyao Jiang', 'name': 'LiyaoJiang', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2412.13185', 'authors': [{'_id': '6762c4d82faaf11234a44936', 'user': {'_id': '667b2ee8e005e1dbcc76e2e2', 'avatarUrl': '/avatars/8b2f5f997f0ed5ae9f4f274941933c40.svg', 'isPro': False, 'fullname': 'Hsin-Ping Huang', 'user': 'hsinh', 'type': 'user'}, 'name': 'Hsin-Ping Huang', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-19T18:02:01.287Z', 'hidden': False}, {'_id': '6762c4d82faaf11234a44937', 'name': 'Yang Zhou', 'hidden': False}, {'_id': '6762c4d82faaf11234a44938', 'name': 'Jui-Hsien Wang', 'hidden': False}, {'_id': '6762c4d82faaf11234a44939', 'name': 'Difan Liu', 'hidden': False}, {'_id': '6762c4d82faaf11234a4493a', 'name': 'Feng Liu', 'hidden': False}, {'_id': '6762c4d82faaf11234a4493b', 'name': 'Ming-Hsuan Yang', 'hidden': False}, {'_id': '6762c4d82faaf11234a4493c', 'name': 'Zhan Xu', 'hidden': False}], 'publishedAt': '2024-12-17T18:58:07.000Z', 'title': 'Move-in-2D: 2D-Conditioned Human Motion Generation', 'summary': 'Generating realistic human videos remains a challenging task, with the most\\neffective methods currently relying on a human motion sequence as a control\\nsignal. Existing approaches often use existing motion extracted from other\\nvideos, which restricts applications to specific motion types and global scene\\nmatching. We propose Move-in-2D, a novel approach to generate human motion\\nsequences conditioned on a scene image, allowing for diverse motion that adapts\\nto different scenes. Our approach utilizes a diffusion model that accepts both\\na scene image and text prompt as inputs, producing a motion sequence tailored\\nto the scene. To train this model, we collect a large-scale video dataset\\nfeaturing single-human activities, annotating each video with the corresponding\\nhuman motion as the target output. Experiments demonstrate that our method\\neffectively predicts human motion that aligns with the scene image after\\nprojection. Furthermore, we show that the generated motion sequence improves\\nhuman motion quality in video synthesis tasks.', 'upvotes': 1, 'discussionId': '6762c4d92faaf11234a449a9'}, 'publishedAt': '2024-12-20T05:35:43.828Z', 'title': 'Move-in-2D: 2D-Conditioned Human Motion Generation', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.13185.png', 'numComments': 1, 'submittedBy': {'_id': '667b2ee8e005e1dbcc76e2e2', 'avatarUrl': '/avatars/8b2f5f997f0ed5ae9f4f274941933c40.svg', 'fullname': 'Hsin-Ping Huang', 'name': 'hsinh', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2412.13377', 'authors': [{'_id': '676577a8abcd70b404ad67ca', 'user': {'_id': '60394599033b61166496163b', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1614366097007-noauth.jpeg', 'isPro': False, 'fullname': 'Gagan Bhatia', 'user': 'gagan3012', 'type': 'user'}, 'name': 'Gagan Bhatia', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-20T15:42:23.951Z', 'hidden': False}, {'_id': '676577a8abcd70b404ad67cb', 'name': 'MingZe Tang', 'hidden': False}, {'_id': '676577a8abcd70b404ad67cc', 'name': 'Cristina Mahanta', 'hidden': False}, {'_id': '676577a8abcd70b404ad67cd', 'name': 'Madiha Kazi', 'hidden': False}], 'publishedAt': '2024-12-17T23:25:47.000Z', 'title': 'DateLogicQA: Benchmarking Temporal Biases in Large Language Models', 'summary': \"This paper introduces DateLogicQA, a benchmark with 190 questions covering\\ndiverse date formats, temporal contexts, and reasoning types. We propose the\\nSemantic Integrity Metric to assess tokenization quality and analyse two\\nbiases: Representation-Level Bias, affecting embeddings, and Logical-Level\\nBias, influencing reasoning outputs. Our findings provide a comprehensive\\nevaluation of LLMs' capabilities and limitations in temporal reasoning,\\nhighlighting key challenges in handling temporal data accurately. The GitHub\\nrepository for our work is available at\\nhttps://github.com/gagan3012/EAIS-Temporal-Bias\", 'upvotes': 0, 'discussionId': '676577aaabcd70b404ad687b'}, 'publishedAt': '2024-12-20T08:57:11.189Z', 'title': 'DateLogicQA: Benchmarking Temporal Biases in Large Language Models', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.13377.png', 'numComments': 1, 'submittedBy': {'_id': '60394599033b61166496163b', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1614366097007-noauth.jpeg', 'fullname': 'Gagan Bhatia', 'name': 'gagan3012', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 18}, 'isAuthorParticipating': True}"
]