[
  {
    "paper": {
      "id": "2503.12533",
      "authors": [
        {
          "_id": "67d8eadc045f869fea1ce3f2",
          "name": "Haoqi Yuan",
          "hidden": false
        },
        {
          "_id": "67d8eadc045f869fea1ce3f3",
          "name": "Yu Bai",
          "hidden": false
        },
        {
          "_id": "67d8eadc045f869fea1ce3f4",
          "name": "Yuhui Fu",
          "hidden": false
        },
        {
          "_id": "67d8eadc045f869fea1ce3f5",
          "name": "Bohan Zhou",
          "hidden": false
        },
        {
          "_id": "67d8eadc045f869fea1ce3f6",
          "name": "Yicheng Feng",
          "hidden": false
        },
        {
          "_id": "67d8eadc045f869fea1ce3f7",
          "name": "Xinrun Xu",
          "hidden": false
        },
        {
          "_id": "67d8eadc045f869fea1ce3f8",
          "name": "Yi Zhan",
          "hidden": false
        },
        {
          "_id": "67d8eadc045f869fea1ce3f9",
          "name": "Börje F. Karlsson",
          "hidden": false
        },
        {
          "_id": "67d8eadc045f869fea1ce3fa",
          "name": "Zongqing Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-16T14:53:53.000Z",
      "submittedOnDailyAt": "2025-03-18T02:11:08.263Z",
      "title": "Being-0: A Humanoid Robotic Agent with Vision-Language Models and\n  Modular Skills",
      "submittedOnDailyBy": {
        "_id": "61e52be53d6dbb1da842316a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61e52be53d6dbb1da842316a/gx0WGPcOCClXPymoKglc4.jpeg",
        "isPro": false,
        "fullname": "Börje Karlsson",
        "user": "tellarin",
        "type": "user"
      },
      "summary": "Building autonomous robotic agents capable of achieving human-level\nperformance in real-world embodied tasks is an ultimate goal in humanoid robot\nresearch. Recent advances have made significant progress in high-level\ncognition with Foundation Models (FMs) and low-level skill development for\nhumanoid robots. However, directly combining these components often results in\npoor robustness and efficiency due to compounding errors in long-horizon tasks\nand the varied latency of different modules. We introduce Being-0, a\nhierarchical agent framework that integrates an FM with a modular skill\nlibrary. The FM handles high-level cognitive tasks such as instruction\nunderstanding, task planning, and reasoning, while the skill library provides\nstable locomotion and dexterous manipulation for low-level control. To bridge\nthe gap between these levels, we propose a novel Connector module, powered by a\nlightweight vision-language model (VLM). The Connector enhances the FM's\nembodied capabilities by translating language-based plans into actionable skill\ncommands and dynamically coordinating locomotion and manipulation to improve\ntask success. With all components, except the FM, deployable on low-cost\nonboard computation devices, Being-0 achieves efficient, real-time performance\non a full-sized humanoid robot equipped with dexterous hands and active vision.\nExtensive experiments in large indoor environments demonstrate Being-0's\neffectiveness in solving complex, long-horizon tasks that require challenging\nnavigation and manipulation subtasks. For further details and videos, visit\nhttps://beingbeyond.github.io/being-0.",
      "upvotes": 25,
      "discussionId": "67d8eadd045f869fea1ce44a",
      "projectPage": "https://beingbeyond.github.io/Being-0/",
      "ai_keywords": [
        "Foundation Models (FMs)",
        "modular skill library",
        "high-level cognitive tasks",
        "instruction understanding",
        "task planning",
        "reasoning",
        "stable locomotion",
        "dexterous manipulation",
        "low-level control",
        "Connector module",
        "lightweight vision-language model (VLM",
        "embodied capabilities",
        "language-based plans",
        "actionable skill commands",
        "dynamic coordination",
        "full-sized humanoid robot",
        "dexterous hands",
        "active vision",
        "complex, long-horizon tasks",
        "challenging navigation",
        "manipulation subtasks"
      ]
    },
    "publishedAt": "2025-03-16T10:53:53.000Z",
    "title": "Being-0: A Humanoid Robotic Agent with Vision-Language Models and\n  Modular Skills",
    "summary": "Building autonomous robotic agents capable of achieving human-level\nperformance in real-world embodied tasks is an ultimate goal in humanoid robot\nresearch. Recent advances have made significant progress in high-level\ncognition with Foundation Models (FMs) and low-level skill development for\nhumanoid robots. However, directly combining these components often results in\npoor robustness and efficiency due to compounding errors in long-horizon tasks\nand the varied latency of different modules. We introduce Being-0, a\nhierarchical agent framework that integrates an FM with a modular skill\nlibrary. The FM handles high-level cognitive tasks such as instruction\nunderstanding, task planning, and reasoning, while the skill library provides\nstable locomotion and dexterous manipulation for low-level control. To bridge\nthe gap between these levels, we propose a novel Connector module, powered by a\nlightweight vision-language model (VLM). The Connector enhances the FM's\nembodied capabilities by translating language-based plans into actionable skill\ncommands and dynamically coordinating locomotion and manipulation to improve\ntask success. With all components, except the FM, deployable on low-cost\nonboard computation devices, Being-0 achieves efficient, real-time performance\non a full-sized humanoid robot equipped with dexterous hands and active vision.\nExtensive experiments in large indoor environments demonstrate Being-0's\neffectiveness in solving complex, long-horizon tasks that require challenging\nnavigation and manipulation subtasks. For further details and videos, visit\nhttps://beingbeyond.github.io/being-0.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12533.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61e52be53d6dbb1da842316a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61e52be53d6dbb1da842316a/gx0WGPcOCClXPymoKglc4.jpeg",
      "fullname": "Börje Karlsson",
      "name": "tellarin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 24
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.12885",
      "authors": [
        {
          "_id": "67d8e23afa59a8b15a9057e8",
          "name": "Dewei Zhou",
          "hidden": false
        },
        {
          "_id": "67d8e23afa59a8b15a9057e9",
          "name": "Mingwei Li",
          "hidden": false
        },
        {
          "_id": "67d8e23afa59a8b15a9057ea",
          "name": "Zongxin Yang",
          "hidden": false
        },
        {
          "_id": "67d8e23afa59a8b15a9057eb",
          "name": "Yi Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-17T07:30:16.000Z",
      "submittedOnDailyAt": "2025-03-18T01:33:30.593Z",
      "title": "DreamRenderer: Taming Multi-Instance Attribute Control in Large-Scale\n  Text-to-Image Models",
      "submittedOnDailyBy": {
        "_id": "65eaa1e2b11eeb516a973508",
        "avatarUrl": "/avatars/beecd135bb940fdc02406f9063b3fa67.svg",
        "isPro": false,
        "fullname": "Dewei Zhou",
        "user": "limuloo1999",
        "type": "user"
      },
      "summary": "Image-conditioned generation methods, such as depth- and canny-conditioned\napproaches, have demonstrated remarkable abilities for precise image synthesis.\nHowever, existing models still struggle to accurately control the content of\nmultiple instances (or regions). Even state-of-the-art models like FLUX and\n3DIS face challenges, such as attribute leakage between instances, which limits\nuser control. To address these issues, we introduce DreamRenderer, a\ntraining-free approach built upon the FLUX model. DreamRenderer enables users\nto control the content of each instance via bounding boxes or masks, while\nensuring overall visual harmony. We propose two key innovations: 1) Bridge\nImage Tokens for Hard Text Attribute Binding, which uses replicated image\ntokens as bridge tokens to ensure that T5 text embeddings, pre-trained solely\non text data, bind the correct visual attributes for each instance during Joint\nAttention; 2) Hard Image Attribute Binding applied only to vital layers.\nThrough our analysis of FLUX, we identify the critical layers responsible for\ninstance attribute rendering and apply Hard Image Attribute Binding only in\nthese layers, using soft binding in the others. This approach ensures precise\ncontrol while preserving image quality. Evaluations on the COCO-POS and\nCOCO-MIG benchmarks demonstrate that DreamRenderer improves the Image Success\nRatio by 17.7% over FLUX and enhances the performance of layout-to-image models\nlike GLIGEN and 3DIS by up to 26.8%. Project Page:\nhttps://limuloo.github.io/DreamRenderer/.",
      "upvotes": 21,
      "discussionId": "67d8e23cfa59a8b15a9058ba",
      "projectPage": "https://limuloo.github.io/DreamRenderer/",
      "githubRepo": "https://github.com/limuloo/DreamRenderer",
      "ai_keywords": [
        "Bridge Image Tokens",
        "Hard Text Attribute Binding",
        "Replicated image tokens",
        "T5 text embeddings",
        "Joint Attention",
        "Hard Image Attribute Binding",
        "Vital layers",
        "Soft binding",
        "Image Success Ratio",
        "COCO-POS",
        "COCO-MIG",
        "layout-to-image models",
        "GLIGEN",
        "3DIS"
      ]
    },
    "publishedAt": "2025-03-17T03:30:16.000Z",
    "title": "DreamRenderer: Taming Multi-Instance Attribute Control in Large-Scale\n  Text-to-Image Models",
    "summary": "Image-conditioned generation methods, such as depth- and canny-conditioned\napproaches, have demonstrated remarkable abilities for precise image synthesis.\nHowever, existing models still struggle to accurately control the content of\nmultiple instances (or regions). Even state-of-the-art models like FLUX and\n3DIS face challenges, such as attribute leakage between instances, which limits\nuser control. To address these issues, we introduce DreamRenderer, a\ntraining-free approach built upon the FLUX model. DreamRenderer enables users\nto control the content of each instance via bounding boxes or masks, while\nensuring overall visual harmony. We propose two key innovations: 1) Bridge\nImage Tokens for Hard Text Attribute Binding, which uses replicated image\ntokens as bridge tokens to ensure that T5 text embeddings, pre-trained solely\non text data, bind the correct visual attributes for each instance during Joint\nAttention; 2) Hard Image Attribute Binding applied only to vital layers.\nThrough our analysis of FLUX, we identify the critical layers responsible for\ninstance attribute rendering and apply Hard Image Attribute Binding only in\nthese layers, using soft binding in the others. This approach ensures precise\ncontrol while preserving image quality. Evaluations on the COCO-POS and\nCOCO-MIG benchmarks demonstrate that DreamRenderer improves the Image Success\nRatio by 17.7% over FLUX and enhances the performance of layout-to-image models\nlike GLIGEN and 3DIS by up to 26.8%. Project Page:\nhttps://limuloo.github.io/DreamRenderer/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12885.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "65eaa1e2b11eeb516a973508",
      "avatarUrl": "/avatars/beecd135bb940fdc02406f9063b3fa67.svg",
      "fullname": "Dewei Zhou",
      "name": "limuloo1999",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.13327",
      "authors": [
        {
          "_id": "67d8e00f0922c3dc8866520c",
          "name": "Lan Chen",
          "hidden": false
        },
        {
          "_id": "67d8e00f0922c3dc8866520d",
          "name": "Qi Mao",
          "hidden": false
        },
        {
          "_id": "67d8e00f0922c3dc8866520e",
          "name": "Yuchao Gu",
          "hidden": false
        },
        {
          "_id": "67d8e00f0922c3dc8866520f",
          "name": "Mike Zheng Shou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-17T16:04:44.000Z",
      "submittedOnDailyAt": "2025-03-18T01:26:49.605Z",
      "title": "Edit Transfer: Learning Image Editing via Vision In-Context Relations",
      "submittedOnDailyBy": {
        "_id": "640d704c8036cc2142299c19",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640d704c8036cc2142299c19/Wt9AslcVxWOSSc11epk8l.jpeg",
        "isPro": false,
        "fullname": "Lan Chen",
        "user": "Orannue",
        "type": "user"
      },
      "summary": "We introduce a new setting, Edit Transfer, where a model learns a\ntransformation from just a single source-target example and applies it to a new\nquery image. While text-based methods excel at semantic manipulations through\ntextual prompts, they often struggle with precise geometric details (e.g.,\nposes and viewpoint changes). Reference-based editing, on the other hand,\ntypically focuses on style or appearance and fails at non-rigid\ntransformations. By explicitly learning the editing transformation from a\nsource-target pair, Edit Transfer mitigates the limitations of both text-only\nand appearance-centric references. Drawing inspiration from in-context learning\nin large language models, we propose a visual relation in-context learning\nparadigm, building upon a DiT-based text-to-image model. We arrange the edited\nexample and the query image into a unified four-panel composite, then apply\nlightweight LoRA fine-tuning to capture complex spatial transformations from\nminimal examples. Despite using only 42 training samples, Edit Transfer\nsubstantially outperforms state-of-the-art TIE and RIE methods on diverse\nnon-rigid scenarios, demonstrating the effectiveness of few-shot visual\nrelation learning.",
      "upvotes": 16,
      "discussionId": "67d8e0100922c3dc88665285",
      "projectPage": "https://cuc-mipg.github.io/EditTransfer.github.io",
      "githubRepo": "https://github.com/CUC-MIPG/Edit-Transfer",
      "ai_keywords": [
        "Edit Transfer",
        "visual relation in-context learning",
        "DiT-based text-to-image model",
        "four-panel composite",
        "LoRA fine-tuning",
        "few-shot visual relation learning",
        "non-rigid transformations",
        "TIE methods",
        "RIE methods"
      ]
    },
    "publishedAt": "2025-03-17T12:04:44.000Z",
    "title": "Edit Transfer: Learning Image Editing via Vision In-Context Relations",
    "summary": "We introduce a new setting, Edit Transfer, where a model learns a\ntransformation from just a single source-target example and applies it to a new\nquery image. While text-based methods excel at semantic manipulations through\ntextual prompts, they often struggle with precise geometric details (e.g.,\nposes and viewpoint changes). Reference-based editing, on the other hand,\ntypically focuses on style or appearance and fails at non-rigid\ntransformations. By explicitly learning the editing transformation from a\nsource-target pair, Edit Transfer mitigates the limitations of both text-only\nand appearance-centric references. Drawing inspiration from in-context learning\nin large language models, we propose a visual relation in-context learning\nparadigm, building upon a DiT-based text-to-image model. We arrange the edited\nexample and the query image into a unified four-panel composite, then apply\nlightweight LoRA fine-tuning to capture complex spatial transformations from\nminimal examples. Despite using only 42 training samples, Edit Transfer\nsubstantially outperforms state-of-the-art TIE and RIE methods on diverse\nnon-rigid scenarios, demonstrating the effectiveness of few-shot visual\nrelation learning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13327.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "640d704c8036cc2142299c19",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640d704c8036cc2142299c19/Wt9AslcVxWOSSc11epk8l.jpeg",
      "fullname": "Lan Chen",
      "name": "Orannue",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.13435",
      "authors": [
        {
          "_id": "67d8dd0b924be985c277c8f6",
          "name": "Ling Yang",
          "hidden": false
        },
        {
          "_id": "67d8dd0b924be985c277c8f7",
          "name": "Kaixin Zhu",
          "hidden": false
        },
        {
          "_id": "67d8dd0b924be985c277c8f8",
          "name": "Juanxi Tian",
          "hidden": false
        },
        {
          "_id": "67d8dd0b924be985c277c8f9",
          "name": "Bohan Zeng",
          "hidden": false
        },
        {
          "_id": "67d8dd0b924be985c277c8fa",
          "name": "Mingbao Lin",
          "hidden": false
        },
        {
          "_id": "67d8dd0b924be985c277c8fb",
          "name": "Hongjuan Pei",
          "hidden": false
        },
        {
          "_id": "67d8dd0b924be985c277c8fc",
          "name": "Wentao Zhang",
          "hidden": false
        },
        {
          "_id": "67d8dd0b924be985c277c8fd",
          "name": "Shuicheng Yan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-17T17:58:18.000Z",
      "submittedOnDailyAt": "2025-03-18T01:44:02.731Z",
      "title": "WideRange4D: Enabling High-Quality 4D Reconstruction with Wide-Range\n  Movements and Scenes",
      "submittedOnDailyBy": {
        "_id": "64fde4e252e82dd432b74ce9",
        "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
        "isPro": false,
        "fullname": "Ling Yang",
        "user": "Lingaaaaaaa",
        "type": "user"
      },
      "summary": "With the rapid development of 3D reconstruction technology, research in 4D\nreconstruction is also advancing, existing 4D reconstruction methods can\ngenerate high-quality 4D scenes. However, due to the challenges in acquiring\nmulti-view video data, the current 4D reconstruction benchmarks mainly display\nactions performed in place, such as dancing, within limited scenarios. In\npractical scenarios, many scenes involve wide-range spatial movements,\nhighlighting the limitations of existing 4D reconstruction datasets.\nAdditionally, existing 4D reconstruction methods rely on deformation fields to\nestimate the dynamics of 3D objects, but deformation fields struggle with\nwide-range spatial movements, which limits the ability to achieve high-quality\n4D scene reconstruction with wide-range spatial movements. In this paper, we\nfocus on 4D scene reconstruction with significant object spatial movements and\npropose a novel 4D reconstruction benchmark, WideRange4D. This benchmark\nincludes rich 4D scene data with large spatial variations, allowing for a more\ncomprehensive evaluation of the generation capabilities of 4D generation\nmethods. Furthermore, we introduce a new 4D reconstruction method, Progress4D,\nwhich generates stable and high-quality 4D results across various complex 4D\nscene reconstruction tasks. We conduct both quantitative and qualitative\ncomparison experiments on WideRange4D, showing that our Progress4D outperforms\nexisting state-of-the-art 4D reconstruction methods. Project:\nhttps://github.com/Gen-Verse/WideRange4D",
      "upvotes": 12,
      "discussionId": "67d8dd0d924be985c277c998",
      "projectPage": "https://huggingface.co/datasets/Gen-Verse/WideRange4D",
      "githubRepo": "https://github.com/Gen-Verse/WideRange4D",
      "ai_keywords": [
        "deformation fields",
        "4D reconstruction",
        "scene reconstruction",
        "multi-view video",
        "spatial movements",
        "3D objects",
        "4D scene data",
        "generation capabilities",
        "4D generation methods",
        "WideRange4D",
        "Progress4D"
      ]
    },
    "publishedAt": "2025-03-17T13:58:18.000Z",
    "title": "WideRange4D: Enabling High-Quality 4D Reconstruction with Wide-Range\n  Movements and Scenes",
    "summary": "With the rapid development of 3D reconstruction technology, research in 4D\nreconstruction is also advancing, existing 4D reconstruction methods can\ngenerate high-quality 4D scenes. However, due to the challenges in acquiring\nmulti-view video data, the current 4D reconstruction benchmarks mainly display\nactions performed in place, such as dancing, within limited scenarios. In\npractical scenarios, many scenes involve wide-range spatial movements,\nhighlighting the limitations of existing 4D reconstruction datasets.\nAdditionally, existing 4D reconstruction methods rely on deformation fields to\nestimate the dynamics of 3D objects, but deformation fields struggle with\nwide-range spatial movements, which limits the ability to achieve high-quality\n4D scene reconstruction with wide-range spatial movements. In this paper, we\nfocus on 4D scene reconstruction with significant object spatial movements and\npropose a novel 4D reconstruction benchmark, WideRange4D. This benchmark\nincludes rich 4D scene data with large spatial variations, allowing for a more\ncomprehensive evaluation of the generation capabilities of 4D generation\nmethods. Furthermore, we introduce a new 4D reconstruction method, Progress4D,\nwhich generates stable and high-quality 4D results across various complex 4D\nscene reconstruction tasks. We conduct both quantitative and qualitative\ncomparison experiments on WideRange4D, showing that our Progress4D outperforms\nexisting state-of-the-art 4D reconstruction methods. Project:\nhttps://github.com/Gen-Verse/WideRange4D",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13435.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64fde4e252e82dd432b74ce9",
      "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
      "fullname": "Ling Yang",
      "name": "Lingaaaaaaa",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.12590",
      "authors": [
        {
          "_id": "67d8de85f7809eea577c4805",
          "name": "Haoran Feng",
          "hidden": false
        },
        {
          "_id": "67d8de85f7809eea577c4806",
          "name": "Zehuan Huang",
          "hidden": false
        },
        {
          "_id": "67d8de85f7809eea577c4807",
          "name": "Lin Li",
          "hidden": false
        },
        {
          "_id": "67d8de85f7809eea577c4808",
          "name": "Hairong Lv",
          "hidden": false
        },
        {
          "_id": "67d8de85f7809eea577c4809",
          "name": "Lu Sheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-16T17:51:16.000Z",
      "submittedOnDailyAt": "2025-03-18T01:18:31.307Z",
      "title": "Personalize Anything for Free with Diffusion Transformer",
      "submittedOnDailyBy": {
        "_id": "6375d136dee28348a9c63cbf",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6375d136dee28348a9c63cbf/gK465HBrQWIOZ-qtHb-Vh.jpeg",
        "isPro": false,
        "fullname": "zehuan-huang",
        "user": "huanngzh",
        "type": "user"
      },
      "summary": "Personalized image generation aims to produce images of user-specified\nconcepts while enabling flexible editing. Recent training-free approaches,\nwhile exhibit higher computational efficiency than training-based methods,\nstruggle with identity preservation, applicability, and compatibility with\ndiffusion transformers (DiTs). In this paper, we uncover the untapped potential\nof DiT, where simply replacing denoising tokens with those of a reference\nsubject achieves zero-shot subject reconstruction. This simple yet effective\nfeature injection technique unlocks diverse scenarios, from personalization to\nimage editing. Building upon this observation, we propose Personalize\nAnything, a training-free framework that achieves personalized image\ngeneration in DiT through: 1) timestep-adaptive token replacement that enforces\nsubject consistency via early-stage injection and enhances flexibility through\nlate-stage regularization, and 2) patch perturbation strategies to boost\nstructural diversity. Our method seamlessly supports layout-guided generation,\nmulti-subject personalization, and mask-controlled editing. Evaluations\ndemonstrate state-of-the-art performance in identity preservation and\nversatility. Our work establishes new insights into DiTs while delivering a\npractical paradigm for efficient personalization.",
      "upvotes": 12,
      "discussionId": "67d8de89f7809eea577c4930",
      "projectPage": "https://fenghora.github.io/Personalize-Anything-Page/",
      "githubRepo": "https://github.com/fenghora/personalize-anything",
      "ai_keywords": [
        "diffusion transformers (DiTs)",
        "denoising tokens",
        "zero-shot subject reconstruction",
        "timestep-adaptive token replacement",
        "early-stage injection",
        "late-stage regularization",
        "patch perturbation strategies",
        "layout-guided generation",
        "multi-subject personalization",
        "mask-controlled editing",
        "identity preservation",
        "versatility"
      ]
    },
    "publishedAt": "2025-03-16T13:51:16.000Z",
    "title": "Personalize Anything for Free with Diffusion Transformer",
    "summary": "Personalized image generation aims to produce images of user-specified\nconcepts while enabling flexible editing. Recent training-free approaches,\nwhile exhibit higher computational efficiency than training-based methods,\nstruggle with identity preservation, applicability, and compatibility with\ndiffusion transformers (DiTs). In this paper, we uncover the untapped potential\nof DiT, where simply replacing denoising tokens with those of a reference\nsubject achieves zero-shot subject reconstruction. This simple yet effective\nfeature injection technique unlocks diverse scenarios, from personalization to\nimage editing. Building upon this observation, we propose Personalize\nAnything, a training-free framework that achieves personalized image\ngeneration in DiT through: 1) timestep-adaptive token replacement that enforces\nsubject consistency via early-stage injection and enhances flexibility through\nlate-stage regularization, and 2) patch perturbation strategies to boost\nstructural diversity. Our method seamlessly supports layout-guided generation,\nmulti-subject personalization, and mask-controlled editing. Evaluations\ndemonstrate state-of-the-art performance in identity preservation and\nversatility. Our work establishes new insights into DiTs while delivering a\npractical paradigm for efficient personalization.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12590.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6375d136dee28348a9c63cbf",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6375d136dee28348a9c63cbf/gK465HBrQWIOZ-qtHb-Vh.jpeg",
      "fullname": "zehuan-huang",
      "name": "huanngzh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 24
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.13399",
      "authors": [
        {
          "_id": "67d8d99a0983992037cdf33f",
          "name": "James Burgess",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf340",
          "name": "Jeffrey J Nirschl",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf341",
          "name": "Laura Bravo-Sánchez",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf342",
          "name": "Alejandro Lozano",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf343",
          "name": "Sanket Rajan Gupte",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf344",
          "name": "Jesus G. Galaz-Montoya",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf345",
          "name": "Yuhui Zhang",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf346",
          "name": "Yuchang Su",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf347",
          "name": "Disha Bhowmik",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf348",
          "name": "Zachary Coman",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf349",
          "name": "Sarina M. Hasan",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf34a",
          "name": "Alexandra Johannesson",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf34b",
          "name": "William D. Leineweber",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf34c",
          "name": "Malvika G Nair",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf34d",
          "name": "Ridhi Yarlagadda",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf34e",
          "name": "Connor Zuraski",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf34f",
          "name": "Wah Chiu",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf350",
          "name": "Sarah Cohen",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf351",
          "name": "Jan N. Hansen",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf352",
          "name": "Manuel D Leonetti",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf353",
          "name": "Chad Liu",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf354",
          "name": "Emma Lundberg",
          "hidden": false
        },
        {
          "_id": "67d8d99a0983992037cdf355",
          "name": "Serena Yeung-Levy",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-17T17:33:10.000Z",
      "submittedOnDailyAt": "2025-03-18T01:06:54.667Z",
      "title": "MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based\n  Scientific Research",
      "submittedOnDailyBy": {
        "_id": "650871aeb44445e9b3625c7b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/650871aeb44445e9b3625c7b/mtx3EnkuNF4z29IosnhaQ.png",
        "isPro": false,
        "fullname": "James Burgess",
        "user": "jmhb",
        "type": "user"
      },
      "summary": "Scientific research demands sophisticated reasoning over multimodal data, a\nchallenge especially prevalent in biology. Despite recent advances in\nmultimodal large language models (MLLMs) for AI-assisted research, existing\nmultimodal reasoning benchmarks only target up to college-level difficulty,\nwhile research-level benchmarks emphasize lower-level perception, falling short\nof the complex multimodal reasoning needed for scientific discovery. To bridge\nthis gap, we introduce MicroVQA, a visual-question answering (VQA) benchmark\ndesigned to assess three reasoning capabilities vital in research workflows:\nexpert image understanding, hypothesis generation, and experiment proposal.\nMicroVQA consists of 1,042 multiple-choice questions (MCQs) curated by biology\nexperts across diverse microscopy modalities, ensuring VQA samples represent\nreal scientific practice. In constructing the benchmark, we find that standard\nMCQ generation methods induce language shortcuts, motivating a new two-stage\npipeline: an optimized LLM prompt structures question-answer pairs into MCQs;\nthen, an agent-based `RefineBot' updates them to remove shortcuts. Benchmarking\non state-of-the-art MLLMs reveal a peak performance of 53\\%; models with\nsmaller LLMs only slightly underperform top models, suggesting that\nlanguage-based reasoning is less challenging than multimodal reasoning; and\ntuning with scientific articles enhances performance. Expert analysis of\nchain-of-thought responses shows that perception errors are the most frequent,\nfollowed by knowledge errors and then overgeneralization errors. These insights\nhighlight the challenges in multimodal scientific reasoning, showing MicroVQA\nis a valuable resource advancing AI-driven biomedical research. MicroVQA is\navailable at https://huggingface.co/datasets/jmhb/microvqa, and project page at\nhttps://jmhb0.github.io/microvqa.",
      "upvotes": 10,
      "discussionId": "67d8d99f0983992037cdf47e",
      "projectPage": "https://jmhb0.github.io/microvqa/",
      "githubRepo": "https://github.com/jmhb0/microvqa",
      "ai_keywords": [
        "multimodal large language models",
        "visual-question answering (VQA)",
        "multiple-choice questions (MCQs)",
        "biology experts",
        "microscopy modalities",
        "standard MCQ generation methods",
        "optimized LLM prompt",
        "agent-based `RefineBot'",
        "chain-of-thought responses",
        "perception errors",
        "knowledge errors",
        "overgeneralization errors"
      ]
    },
    "publishedAt": "2025-03-17T13:33:10.000Z",
    "title": "MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based\n  Scientific Research",
    "summary": "Scientific research demands sophisticated reasoning over multimodal data, a\nchallenge especially prevalent in biology. Despite recent advances in\nmultimodal large language models (MLLMs) for AI-assisted research, existing\nmultimodal reasoning benchmarks only target up to college-level difficulty,\nwhile research-level benchmarks emphasize lower-level perception, falling short\nof the complex multimodal reasoning needed for scientific discovery. To bridge\nthis gap, we introduce MicroVQA, a visual-question answering (VQA) benchmark\ndesigned to assess three reasoning capabilities vital in research workflows:\nexpert image understanding, hypothesis generation, and experiment proposal.\nMicroVQA consists of 1,042 multiple-choice questions (MCQs) curated by biology\nexperts across diverse microscopy modalities, ensuring VQA samples represent\nreal scientific practice. In constructing the benchmark, we find that standard\nMCQ generation methods induce language shortcuts, motivating a new two-stage\npipeline: an optimized LLM prompt structures question-answer pairs into MCQs;\nthen, an agent-based `RefineBot' updates them to remove shortcuts. Benchmarking\non state-of-the-art MLLMs reveal a peak performance of 53\\%; models with\nsmaller LLMs only slightly underperform top models, suggesting that\nlanguage-based reasoning is less challenging than multimodal reasoning; and\ntuning with scientific articles enhances performance. Expert analysis of\nchain-of-thought responses shows that perception errors are the most frequent,\nfollowed by knowledge errors and then overgeneralization errors. These insights\nhighlight the challenges in multimodal scientific reasoning, showing MicroVQA\nis a valuable resource advancing AI-driven biomedical research. MicroVQA is\navailable at https://huggingface.co/datasets/jmhb/microvqa, and project page at\nhttps://jmhb0.github.io/microvqa.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13399.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "650871aeb44445e9b3625c7b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/650871aeb44445e9b3625c7b/mtx3EnkuNF4z29IosnhaQ.png",
      "fullname": "James Burgess",
      "name": "jmhb",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.11751",
      "authors": [
        {
          "_id": "67d8e861fa59a8b15a921052",
          "name": "Zhaofeng Wu",
          "hidden": false
        },
        {
          "_id": "67d8e861fa59a8b15a921053",
          "name": "Michihiro Yasunaga",
          "hidden": false
        },
        {
          "_id": "67d8e861fa59a8b15a921054",
          "name": "Andrew Cohen",
          "hidden": false
        },
        {
          "_id": "67d8e861fa59a8b15a921055",
          "name": "Yoon Kim",
          "hidden": false
        },
        {
          "_id": "67d8e861fa59a8b15a921056",
          "name": "Asli Celikyilmaz",
          "hidden": false
        },
        {
          "_id": "67d8e861fa59a8b15a921057",
          "name": "Marjan Ghazvininejad",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-14T17:59:41.000Z",
      "submittedOnDailyAt": "2025-03-18T01:59:35.073Z",
      "title": "reWordBench: Benchmarking and Improving the Robustness of Reward Models\n  with Transformed Inputs",
      "submittedOnDailyBy": {
        "_id": "6351712b40dffad651f128c7",
        "avatarUrl": "/avatars/87708c86c1baef548ef556f5d32dca71.svg",
        "isPro": false,
        "fullname": "Zhaofeng Wu",
        "user": "ZhaofengWu",
        "type": "user"
      },
      "summary": "Reward models have become a staple in modern NLP, serving as not only a\nscalable text evaluator, but also an indispensable component in many alignment\nrecipes and inference-time algorithms. However, while recent reward models\nincrease performance on standard benchmarks, this may partly be due to\noverfitting effects, which would confound an understanding of their true\ncapability. In this work, we scrutinize the robustness of reward models and the\nextent of such overfitting. We build **reWordBench**, which systematically\ntransforms reward model inputs in meaning- or ranking-preserving ways. We show\nthat state-of-the-art reward models suffer from substantial performance\ndegradation even with minor input transformations, sometimes dropping to\nsignificantly below-random accuracy, suggesting brittleness. To improve reward\nmodel robustness, we propose to explicitly train them to assign similar scores\nto paraphrases, and find that this approach also improves robustness to other\ndistinct kinds of transformations. For example, our robust reward model reduces\nsuch degradation by roughly half for the Chat Hard subset in RewardBench.\nFurthermore, when used in alignment, our robust reward models demonstrate\nbetter utility and lead to higher-quality outputs, winning in up to 59% of\ninstances against a standardly trained RM.",
      "upvotes": 10,
      "discussionId": "67d8e866fa59a8b15a92117c",
      "ai_keywords": [
        "reward models",
        "NLP",
        "text evaluator",
        "alignment",
        "inference-time algorithms",
        "overfitting",
        "reWordBench",
        "meaning-preserving",
        "ranking-preserving",
        "state-of-the-art",
        "performance degradation",
        "below-random accuracy",
        "brittleness",
        "paraphrases",
        "robust reward model",
        "Chat Hard subset",
        "RewardBench",
        "utility",
        "higher-quality outputs"
      ]
    },
    "publishedAt": "2025-03-14T13:59:41.000Z",
    "title": "reWordBench: Benchmarking and Improving the Robustness of Reward Models\n  with Transformed Inputs",
    "summary": "Reward models have become a staple in modern NLP, serving as not only a\nscalable text evaluator, but also an indispensable component in many alignment\nrecipes and inference-time algorithms. However, while recent reward models\nincrease performance on standard benchmarks, this may partly be due to\noverfitting effects, which would confound an understanding of their true\ncapability. In this work, we scrutinize the robustness of reward models and the\nextent of such overfitting. We build **reWordBench**, which systematically\ntransforms reward model inputs in meaning- or ranking-preserving ways. We show\nthat state-of-the-art reward models suffer from substantial performance\ndegradation even with minor input transformations, sometimes dropping to\nsignificantly below-random accuracy, suggesting brittleness. To improve reward\nmodel robustness, we propose to explicitly train them to assign similar scores\nto paraphrases, and find that this approach also improves robustness to other\ndistinct kinds of transformations. For example, our robust reward model reduces\nsuch degradation by roughly half for the Chat Hard subset in RewardBench.\nFurthermore, when used in alignment, our robust reward models demonstrate\nbetter utility and lead to higher-quality outputs, winning in up to 59% of\ninstances against a standardly trained RM.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11751.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6351712b40dffad651f128c7",
      "avatarUrl": "/avatars/87708c86c1baef548ef556f5d32dca71.svg",
      "fullname": "Zhaofeng Wu",
      "name": "ZhaofengWu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.13434",
      "authors": [
        {
          "_id": "67d916b500030726e0df2a67",
          "name": "Yaowei Li",
          "hidden": false
        },
        {
          "_id": "67d916b500030726e0df2a68",
          "name": "Lingen Li",
          "hidden": false
        },
        {
          "_id": "67d916b500030726e0df2a69",
          "name": "Zhaoyang Zhang",
          "hidden": false
        },
        {
          "_id": "67d916b500030726e0df2a6a",
          "name": "Xiaoyu Li",
          "hidden": false
        },
        {
          "_id": "67d916b500030726e0df2a6b",
          "name": "Guangzhi Wang",
          "hidden": false
        },
        {
          "_id": "67d916b500030726e0df2a6c",
          "name": "Hongxiang Li",
          "hidden": false
        },
        {
          "_id": "67d916b500030726e0df2a6d",
          "name": "Xiaodong Cun",
          "hidden": false
        },
        {
          "_id": "67d916b500030726e0df2a6e",
          "name": "Ying Shan",
          "hidden": false
        },
        {
          "_id": "67d916b500030726e0df2a6f",
          "name": "Yuexian Zou",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/658409ceca19ccf6d9989add/fhyGe83BiDhDf9a71kUyP.mp4"
      ],
      "publishedAt": "2025-03-17T17:58:05.000Z",
      "submittedOnDailyAt": "2025-03-18T05:20:30.708Z",
      "title": "BlobCtrl: A Unified and Flexible Framework for Element-level Image\n  Generation and Editing",
      "submittedOnDailyBy": {
        "_id": "658409ceca19ccf6d9989add",
        "avatarUrl": "/avatars/3ac1dbd25e52435185babdeb3da28875.svg",
        "isPro": false,
        "fullname": "Zhaoyang Zhang",
        "user": "ZyZcuhk",
        "type": "user"
      },
      "summary": "Element-level visual manipulation is essential in digital content creation,\nbut current diffusion-based methods lack the precision and flexibility of\ntraditional tools. In this work, we introduce BlobCtrl, a framework that\nunifies element-level generation and editing using a probabilistic blob-based\nrepresentation. By employing blobs as visual primitives, our approach\neffectively decouples and represents spatial location, semantic content, and\nidentity information, enabling precise element-level manipulation. Our key\ncontributions include: 1) a dual-branch diffusion architecture with\nhierarchical feature fusion for seamless foreground-background integration; 2)\na self-supervised training paradigm with tailored data augmentation and score\nfunctions; and 3) controllable dropout strategies to balance fidelity and\ndiversity. To support further research, we introduce BlobData for large-scale\ntraining and BlobBench for systematic evaluation. Experiments show that\nBlobCtrl excels in various element-level manipulation tasks while maintaining\ncomputational efficiency, offering a practical solution for precise and\nflexible visual content creation. Project page:\nhttps://liyaowei-stu.github.io/project/BlobCtrl/",
      "upvotes": 9,
      "discussionId": "67d916bc00030726e0df2c3e",
      "projectPage": "https://liyaowei-stu.github.io/project/BlobCtrl/",
      "githubRepo": "https://github.com/TencentARC/BlobCtrl",
      "ai_keywords": [
        "probabilistic blob-based representation",
        "dual-branch diffusion architecture",
        "hierarchical feature fusion",
        "self-supervised training paradigm",
        "tailored data augmentation",
        "score functions",
        "controllable dropout strategies",
        "BlobData",
        "BlobBench"
      ]
    },
    "publishedAt": "2025-03-17T13:58:05.000Z",
    "title": "BlobCtrl: A Unified and Flexible Framework for Element-level Image\n  Generation and Editing",
    "summary": "Element-level visual manipulation is essential in digital content creation,\nbut current diffusion-based methods lack the precision and flexibility of\ntraditional tools. In this work, we introduce BlobCtrl, a framework that\nunifies element-level generation and editing using a probabilistic blob-based\nrepresentation. By employing blobs as visual primitives, our approach\neffectively decouples and represents spatial location, semantic content, and\nidentity information, enabling precise element-level manipulation. Our key\ncontributions include: 1) a dual-branch diffusion architecture with\nhierarchical feature fusion for seamless foreground-background integration; 2)\na self-supervised training paradigm with tailored data augmentation and score\nfunctions; and 3) controllable dropout strategies to balance fidelity and\ndiversity. To support further research, we introduce BlobData for large-scale\ntraining and BlobBench for systematic evaluation. Experiments show that\nBlobCtrl excels in various element-level manipulation tasks while maintaining\ncomputational efficiency, offering a practical solution for precise and\nflexible visual content creation. Project page:\nhttps://liyaowei-stu.github.io/project/BlobCtrl/",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/658409ceca19ccf6d9989add/fhyGe83BiDhDf9a71kUyP.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13434.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "658409ceca19ccf6d9989add",
      "avatarUrl": "/avatars/3ac1dbd25e52435185babdeb3da28875.svg",
      "fullname": "Zhaoyang Zhang",
      "name": "ZyZcuhk",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.06053",
      "authors": [
        {
          "_id": "67cfd2d7bc539099da9ebecb",
          "name": "Runze Zhang",
          "hidden": false
        },
        {
          "_id": "67cfd2d7bc539099da9ebecc",
          "user": {
            "_id": "6474a63f7d131daf633d10f2",
            "avatarUrl": "/avatars/5e5d1ce5731987a810448835a1a69c91.svg",
            "isPro": false,
            "fullname": "GeorgeDu",
            "user": "georgedu",
            "type": "user"
          },
          "name": "Guoguang Du",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-17T08:45:36.478Z",
          "hidden": false
        },
        {
          "_id": "67cfd2d7bc539099da9ebecd",
          "user": {
            "_id": "66b01dc4e48856bb718f2ba8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66b01dc4e48856bb718f2ba8/MHZyIDd3BEtsnH9M2BvzM.jpeg",
            "isPro": false,
            "fullname": "Xiaochuan Li",
            "user": "lixiaochuan",
            "type": "user"
          },
          "name": "Xiaochuan Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-17T08:45:39.724Z",
          "hidden": false
        },
        {
          "_id": "67cfd2d7bc539099da9ebece",
          "name": "Qi Jia",
          "hidden": false
        },
        {
          "_id": "67cfd2d7bc539099da9ebecf",
          "name": "Liang Jin",
          "hidden": false
        },
        {
          "_id": "67cfd2d7bc539099da9ebed0",
          "user": {
            "_id": "66f67725cdcb9a4eaef04027",
            "avatarUrl": "/avatars/fb5f4b467cc4d73e129fa9aa60ef344d.svg",
            "isPro": false,
            "fullname": "Ellen Liu",
            "user": "EllenAP",
            "type": "user"
          },
          "name": "Lu Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-17T08:45:32.476Z",
          "hidden": false
        },
        {
          "_id": "67cfd2d7bc539099da9ebed1",
          "name": "Jingjing Wang",
          "hidden": false
        },
        {
          "_id": "67cfd2d7bc539099da9ebed2",
          "name": "Cong Xu",
          "hidden": false
        },
        {
          "_id": "67cfd2d7bc539099da9ebed3",
          "name": "Zhenhua Guo",
          "hidden": false
        },
        {
          "_id": "67cfd2d7bc539099da9ebed4",
          "name": "Yaqian Zhao",
          "hidden": false
        },
        {
          "_id": "67cfd2d7bc539099da9ebed5",
          "name": "Xiaoli Gong",
          "hidden": false
        },
        {
          "_id": "67cfd2d7bc539099da9ebed6",
          "name": "Rengang Li",
          "hidden": false
        },
        {
          "_id": "67cfd2d7bc539099da9ebed7",
          "name": "Baoyu Fan",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/66b01dc4e48856bb718f2ba8/_R9279HPwaWKB34BDJmAv.png"
      ],
      "publishedAt": "2025-03-08T04:37:38.000Z",
      "submittedOnDailyAt": "2025-03-18T05:40:31.378Z",
      "title": "DropletVideo: A Dataset and Approach to Explore Integral Spatio-Temporal\n  Consistent Video Generation",
      "submittedOnDailyBy": {
        "_id": "66b01dc4e48856bb718f2ba8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66b01dc4e48856bb718f2ba8/MHZyIDd3BEtsnH9M2BvzM.jpeg",
        "isPro": false,
        "fullname": "Xiaochuan Li",
        "user": "lixiaochuan",
        "type": "user"
      },
      "summary": "Spatio-temporal consistency is a critical research topic in video generation.\nA qualified generated video segment must ensure plot plausibility and coherence\nwhile maintaining visual consistency of objects and scenes across varying\nviewpoints. Prior research, especially in open-source projects, primarily\nfocuses on either temporal or spatial consistency, or their basic combination,\nsuch as appending a description of a camera movement after a prompt without\nconstraining the outcomes of this movement. However, camera movement may\nintroduce new objects to the scene or eliminate existing ones, thereby\noverlaying and affecting the preceding narrative. Especially in videos with\nnumerous camera movements, the interplay between multiple plots becomes\nincreasingly complex. This paper introduces and examines integral\nspatio-temporal consistency, considering the synergy between plot progression\nand camera techniques, and the long-term impact of prior content on subsequent\ngeneration. Our research encompasses dataset construction through to the\ndevelopment of the model. Initially, we constructed a DropletVideo-10M dataset,\nwhich comprises 10 million videos featuring dynamic camera motion and object\nactions. Each video is annotated with an average caption of 206 words,\ndetailing various camera movements and plot developments. Following this, we\ndeveloped and trained the DropletVideo model, which excels in preserving\nspatio-temporal coherence during video generation. The DropletVideo dataset and\nmodel are accessible at https://dropletx.github.io.",
      "upvotes": 8,
      "discussionId": "67cfd2debc539099da9ec061",
      "ai_keywords": [
        "spatio-temporal consistency",
        "video generation",
        "plot plausibility",
        "visual consistency",
        "objects",
        "scenes",
        "viewpoints",
        "camera movement",
        "prompt",
        "narrative",
        "plot progression",
        "camera techniques",
        "long-term impact",
        "dataset construction",
        "DropletVideo-10M dataset",
        "dynamic camera motion",
        "object actions",
        "caption",
        "DropletVideo model",
        "spatio-temporal coherence"
      ]
    },
    "publishedAt": "2025-03-07T23:37:38.000Z",
    "title": "DropletVideo: A Dataset and Approach to Explore Integral Spatio-Temporal\n  Consistent Video Generation",
    "summary": "Spatio-temporal consistency is a critical research topic in video generation.\nA qualified generated video segment must ensure plot plausibility and coherence\nwhile maintaining visual consistency of objects and scenes across varying\nviewpoints. Prior research, especially in open-source projects, primarily\nfocuses on either temporal or spatial consistency, or their basic combination,\nsuch as appending a description of a camera movement after a prompt without\nconstraining the outcomes of this movement. However, camera movement may\nintroduce new objects to the scene or eliminate existing ones, thereby\noverlaying and affecting the preceding narrative. Especially in videos with\nnumerous camera movements, the interplay between multiple plots becomes\nincreasingly complex. This paper introduces and examines integral\nspatio-temporal consistency, considering the synergy between plot progression\nand camera techniques, and the long-term impact of prior content on subsequent\ngeneration. Our research encompasses dataset construction through to the\ndevelopment of the model. Initially, we constructed a DropletVideo-10M dataset,\nwhich comprises 10 million videos featuring dynamic camera motion and object\nactions. Each video is annotated with an average caption of 206 words,\ndetailing various camera movements and plot developments. Following this, we\ndeveloped and trained the DropletVideo model, which excels in preserving\nspatio-temporal coherence during video generation. The DropletVideo dataset and\nmodel are accessible at https://dropletx.github.io.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/66b01dc4e48856bb718f2ba8/_R9279HPwaWKB34BDJmAv.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06053.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66b01dc4e48856bb718f2ba8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66b01dc4e48856bb718f2ba8/MHZyIDd3BEtsnH9M2BvzM.jpeg",
      "fullname": "Xiaochuan Li",
      "name": "lixiaochuan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.13444",
      "authors": [
        {
          "_id": "67d8eeb17e184aa2954d19f4",
          "name": "Ye Liu",
          "hidden": false
        },
        {
          "_id": "67d8eeb17e184aa2954d19f5",
          "name": "Kevin Qinghong Lin",
          "hidden": false
        },
        {
          "_id": "67d8eeb17e184aa2954d19f6",
          "name": "Chang Wen Chen",
          "hidden": false
        },
        {
          "_id": "67d8eeb17e184aa2954d19f7",
          "name": "Mike Zheng Shou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-17T17:59:33.000Z",
      "submittedOnDailyAt": "2025-03-18T02:25:58.731Z",
      "title": "VideoMind: A Chain-of-LoRA Agent for Long Video Reasoning",
      "submittedOnDailyBy": {
        "_id": "64440be5af034cdfd69ca3a7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg",
        "isPro": true,
        "fullname": "Qinghong (Kevin) Lin",
        "user": "KevinQHLin",
        "type": "user"
      },
      "summary": "Videos, with their unique temporal dimension, demand precise grounded\nunderstanding, where answers are directly linked to visual, interpretable\nevidence. Despite significant breakthroughs in reasoning capabilities within\nLarge Language Models, multi-modal reasoning - especially for videos - remains\nunexplored. In this work, we introduce VideoMind, a novel video-language agent\ndesigned for temporal-grounded video understanding. VideoMind incorporates two\nkey innovations: (i) We identify essential capabilities for video temporal\nreasoning and develop a role-based agentic workflow, including a planner for\ncoordinating different roles, a grounder for temporal localization, a verifier\nto assess temporal interval accuracy, and an answerer for question-answering.\n(ii) To efficiently integrate these diverse roles, we propose a novel\nChain-of-LoRA strategy, enabling seamless role-switching via lightweight LoRA\nadaptors while avoiding the overhead of multiple models, thus balancing\nefficiency and flexibility. Extensive experiments on 14 public benchmarks\ndemonstrate that our agent achieves state-of-the-art performance on diverse\nvideo understanding tasks, including 3 on grounded video question-answering, 6\non video temporal grounding, and 5 on general video question-answering,\nunderscoring its effectiveness in advancing video agent and long-form temporal\nreasoning.",
      "upvotes": 4,
      "discussionId": "67d8eeb37e184aa2954d1a39",
      "ai_keywords": [
        "VideoMind",
        "temporal-grounded video understanding",
        "role-based agentic workflow",
        "planner",
        "grounder",
        "temporale localization",
        "verifier",
        "temporal interval accuracy",
        "answerer",
        "question-answering",
        "Chain-of-LoRA",
        "LoRA adaptors",
        "grounded video question-answering",
        "video temporal grounding",
        "general video question-answering",
        "temporal reasoning"
      ]
    },
    "publishedAt": "2025-03-17T13:59:33.000Z",
    "title": "VideoMind: A Chain-of-LoRA Agent for Long Video Reasoning",
    "summary": "Videos, with their unique temporal dimension, demand precise grounded\nunderstanding, where answers are directly linked to visual, interpretable\nevidence. Despite significant breakthroughs in reasoning capabilities within\nLarge Language Models, multi-modal reasoning - especially for videos - remains\nunexplored. In this work, we introduce VideoMind, a novel video-language agent\ndesigned for temporal-grounded video understanding. VideoMind incorporates two\nkey innovations: (i) We identify essential capabilities for video temporal\nreasoning and develop a role-based agentic workflow, including a planner for\ncoordinating different roles, a grounder for temporal localization, a verifier\nto assess temporal interval accuracy, and an answerer for question-answering.\n(ii) To efficiently integrate these diverse roles, we propose a novel\nChain-of-LoRA strategy, enabling seamless role-switching via lightweight LoRA\nadaptors while avoiding the overhead of multiple models, thus balancing\nefficiency and flexibility. Extensive experiments on 14 public benchmarks\ndemonstrate that our agent achieves state-of-the-art performance on diverse\nvideo understanding tasks, including 3 on grounded video question-answering, 6\non video temporal grounding, and 5 on general video question-answering,\nunderscoring its effectiveness in advancing video agent and long-form temporal\nreasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13444.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64440be5af034cdfd69ca3a7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg",
      "fullname": "Qinghong (Kevin) Lin",
      "name": "KevinQHLin",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 21
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.12937",
      "authors": [
        {
          "_id": "67d8eb0c18de6ef86c4eb457",
          "name": "Jingyi Zhang",
          "hidden": false
        },
        {
          "_id": "67d8eb0c18de6ef86c4eb458",
          "name": "Jiaxing Huang",
          "hidden": false
        },
        {
          "_id": "67d8eb0c18de6ef86c4eb459",
          "name": "Huanjin Yao",
          "hidden": false
        },
        {
          "_id": "67d8eb0c18de6ef86c4eb45a",
          "name": "Shunyu Liu",
          "hidden": false
        },
        {
          "_id": "67d8eb0c18de6ef86c4eb45b",
          "name": "Xikun Zhang",
          "hidden": false
        },
        {
          "_id": "67d8eb0c18de6ef86c4eb45c",
          "name": "Shijian Lu",
          "hidden": false
        },
        {
          "_id": "67d8eb0c18de6ef86c4eb45d",
          "name": "Dacheng Tao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-17T08:51:44.000Z",
      "submittedOnDailyAt": "2025-03-18T02:10:10.429Z",
      "title": "R1-VL: Learning to Reason with Multimodal Large Language Models via\n  Step-wise Group Relative Policy Optimization",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Recent studies generally enhance MLLMs' reasoning capabilities via supervised\nfine-tuning on high-quality chain-of-thought reasoning data, which often leads\nmodels to merely imitate successful reasoning paths without understanding what\nthe wrong reasoning paths are. In this work, we aim to enhance the MLLMs'\nreasoning ability beyond passively imitating positive reasoning paths. To this\nend, we design Step-wise Group Relative Policy Optimization (StepGRPO), a new\nonline reinforcement learning framework that enables MLLMs to self-improve\nreasoning ability via simple, effective and dense step-wise rewarding.\nSpecifically, StepGRPO introduces two novel rule-based reasoning rewards:\nStep-wise Reasoning Accuracy Reward (StepRAR) and Step-wise Reasoning Validity\nReward (StepRVR). StepRAR rewards the reasoning paths that contain necessary\nintermediate reasoning steps via a soft key-step matching technique, while\nStepRAR rewards reasoning paths that follow a well-structured and logically\nconsistent reasoning process through a reasoning completeness and logic\nevaluation strategy. With the proposed StepGRPO, we introduce R1-VL, a series\nof MLLMs with outstanding capabilities in step-by-step reasoning. Extensive\nexperiments over 8 benchmarks demonstrate the superiority of our methods.",
      "upvotes": 4,
      "discussionId": "67d8eb0d18de6ef86c4eb4aa",
      "ai_keywords": [
        "Step-wise Group Relative Policy Optimization (StepGRPO)",
        "online reinforcement learning",
        "Step-wise Reasoning Accuracy Reward (StepRAR)",
        "Step-wise Reasoning Validity Reward (StepRVR)",
        "soft key-step matching",
        "reasoning completeness",
        "logic evaluation",
        "R1-VL",
        "step-by-step reasoning"
      ]
    },
    "publishedAt": "2025-03-17T04:51:44.000Z",
    "title": "R1-VL: Learning to Reason with Multimodal Large Language Models via\n  Step-wise Group Relative Policy Optimization",
    "summary": "Recent studies generally enhance MLLMs' reasoning capabilities via supervised\nfine-tuning on high-quality chain-of-thought reasoning data, which often leads\nmodels to merely imitate successful reasoning paths without understanding what\nthe wrong reasoning paths are. In this work, we aim to enhance the MLLMs'\nreasoning ability beyond passively imitating positive reasoning paths. To this\nend, we design Step-wise Group Relative Policy Optimization (StepGRPO), a new\nonline reinforcement learning framework that enables MLLMs to self-improve\nreasoning ability via simple, effective and dense step-wise rewarding.\nSpecifically, StepGRPO introduces two novel rule-based reasoning rewards:\nStep-wise Reasoning Accuracy Reward (StepRAR) and Step-wise Reasoning Validity\nReward (StepRVR). StepRAR rewards the reasoning paths that contain necessary\nintermediate reasoning steps via a soft key-step matching technique, while\nStepRAR rewards reasoning paths that follow a well-structured and logically\nconsistent reasoning process through a reasoning completeness and logic\nevaluation strategy. With the proposed StepGRPO, we introduce R1-VL, a series\nof MLLMs with outstanding capabilities in step-by-step reasoning. Extensive\nexperiments over 8 benchmarks demonstrate the superiority of our methods.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12937.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6388
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.11495",
      "authors": [
        {
          "_id": "67d8ca56e94f1237cb3ba3ca",
          "name": "Zixu Cheng",
          "hidden": false
        },
        {
          "_id": "67d8ca56e94f1237cb3ba3cb",
          "name": "Jian Hu",
          "hidden": false
        },
        {
          "_id": "67d8ca56e94f1237cb3ba3cc",
          "name": "Ziquan Liu",
          "hidden": false
        },
        {
          "_id": "67d8ca56e94f1237cb3ba3cd",
          "name": "Chenyang Si",
          "hidden": false
        },
        {
          "_id": "67d8ca56e94f1237cb3ba3ce",
          "name": "Wei Li",
          "hidden": false
        },
        {
          "_id": "67d8ca56e94f1237cb3ba3cf",
          "name": "Shaogang Gong",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/65e1b6e9501590df0173cbd3/A2JrOm6FBAq5keCRRdVMK.png"
      ],
      "publishedAt": "2025-03-14T15:21:44.000Z",
      "submittedOnDailyAt": "2025-03-18T01:11:08.161Z",
      "title": "V-STaR: Benchmarking Video-LLMs on Video Spatio-Temporal Reasoning",
      "submittedOnDailyBy": {
        "_id": "65e1b6e9501590df0173cbd3",
        "avatarUrl": "/avatars/a73e2139700e23eff455734c99cef5ba.svg",
        "isPro": false,
        "fullname": "Jian Hu",
        "user": "lwpyh",
        "type": "user"
      },
      "summary": "Human processes video reasoning in a sequential spatio-temporal reasoning\nlogic, we first identify the relevant frames (\"when\") and then analyse the\nspatial relationships (\"where\") between key objects, and finally leverage these\nrelationships to draw inferences (\"what\"). However, can Video Large Language\nModels (Video-LLMs) also \"reason through a sequential spatio-temporal logic\" in\nvideos? Existing Video-LLM benchmarks primarily focus on assessing object\npresence, neglecting relational reasoning. Consequently, it is difficult to\nmeasure whether a model truly comprehends object interactions (actions/events)\nin videos or merely relies on pre-trained \"memory\" of co-occurrences as biases\nin generating answers. In this work, we introduce a Video Spatio-Temporal\nReasoning (V-STaR) benchmark to address these shortcomings. The key idea is to\ndecompose video understanding into a Reverse Spatio-Temporal Reasoning (RSTR)\ntask that simultaneously evaluates what objects are present, when events occur,\nand where they are located while capturing the underlying Chain-of-thought\n(CoT) logic. To support this evaluation, we construct a dataset to elicit the\nspatial-temporal reasoning process of Video-LLMs. It contains coarse-to-fine\nCoT questions generated by a semi-automated GPT-4-powered pipeline, embedding\nexplicit reasoning chains to mimic human cognition. Experiments from 14\nVideo-LLMs on our V-STaR reveal significant gaps between current Video-LLMs and\nthe needs for robust and consistent spatio-temporal reasoning.",
      "upvotes": 4,
      "discussionId": "67d8ca59e94f1237cb3ba47c",
      "ai_keywords": [
        "Video Large Language Models (Video-LLMs)",
        "Reverse Spatio-Temporal Reasoning (RSTR)",
        "Chain-of-thought (CoT)",
        "GPT-4",
        "Video Spatio-Temporal Reasoning (V-STaR)",
        "CoT questions",
        "CoT logic",
        "human cognition"
      ]
    },
    "publishedAt": "2025-03-14T11:21:44.000Z",
    "title": "V-STaR: Benchmarking Video-LLMs on Video Spatio-Temporal Reasoning",
    "summary": "Human processes video reasoning in a sequential spatio-temporal reasoning\nlogic, we first identify the relevant frames (\"when\") and then analyse the\nspatial relationships (\"where\") between key objects, and finally leverage these\nrelationships to draw inferences (\"what\"). However, can Video Large Language\nModels (Video-LLMs) also \"reason through a sequential spatio-temporal logic\" in\nvideos? Existing Video-LLM benchmarks primarily focus on assessing object\npresence, neglecting relational reasoning. Consequently, it is difficult to\nmeasure whether a model truly comprehends object interactions (actions/events)\nin videos or merely relies on pre-trained \"memory\" of co-occurrences as biases\nin generating answers. In this work, we introduce a Video Spatio-Temporal\nReasoning (V-STaR) benchmark to address these shortcomings. The key idea is to\ndecompose video understanding into a Reverse Spatio-Temporal Reasoning (RSTR)\ntask that simultaneously evaluates what objects are present, when events occur,\nand where they are located while capturing the underlying Chain-of-thought\n(CoT) logic. To support this evaluation, we construct a dataset to elicit the\nspatial-temporal reasoning process of Video-LLMs. It contains coarse-to-fine\nCoT questions generated by a semi-automated GPT-4-powered pipeline, embedding\nexplicit reasoning chains to mimic human cognition. Experiments from 14\nVideo-LLMs on our V-STaR reveal significant gaps between current Video-LLMs and\nthe needs for robust and consistent spatio-temporal reasoning.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/65e1b6e9501590df0173cbd3/A2JrOm6FBAq5keCRRdVMK.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11495.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65e1b6e9501590df0173cbd3",
      "avatarUrl": "/avatars/a73e2139700e23eff455734c99cef5ba.svg",
      "fullname": "Jian Hu",
      "name": "lwpyh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.13070",
      "authors": [
        {
          "_id": "67d8fbf641d31cc626e4d7b9",
          "name": "Yihong Luo",
          "hidden": false
        },
        {
          "_id": "67d8fbf641d31cc626e4d7ba",
          "name": "Tianyang Hu",
          "hidden": false
        },
        {
          "_id": "67d8fbf641d31cc626e4d7bb",
          "name": "Weijian Luo",
          "hidden": false
        },
        {
          "_id": "67d8fbf641d31cc626e4d7bc",
          "name": "Kenji Kawaguchi",
          "hidden": false
        },
        {
          "_id": "67d8fbf641d31cc626e4d7bd",
          "name": "Jing Tang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-17T11:21:43.000Z",
      "submittedOnDailyAt": "2025-03-18T03:24:24.806Z",
      "title": "Rewards Are Enough for Fast Photo-Realistic Text-to-image Generation",
      "submittedOnDailyBy": {
        "_id": "65f7e6856bd4bac5b6a4ecc3",
        "avatarUrl": "/avatars/9c0c48310fb5e99c19700316a0ab531e.svg",
        "isPro": false,
        "fullname": "Yihong Luo",
        "user": "Luo-Yihong",
        "type": "user"
      },
      "summary": "Aligning generated images to complicated text prompts and human preferences\nis a central challenge in Artificial Intelligence-Generated Content (AIGC).\nWith reward-enhanced diffusion distillation emerging as a promising approach\nthat boosts controllability and fidelity of text-to-image models, we identify a\nfundamental paradigm shift: as conditions become more specific and reward\nsignals stronger, the rewards themselves become the dominant force in\ngeneration. In contrast, the diffusion losses serve as an overly expensive form\nof regularization. To thoroughly validate our hypothesis, we introduce R0, a\nnovel conditional generation approach via regularized reward maximization.\nInstead of relying on tricky diffusion distillation losses, R0 proposes a new\nperspective that treats image generations as an optimization problem in data\nspace which aims to search for valid images that have high compositional\nrewards. By innovative designs of the generator parameterization and proper\nregularization techniques, we train state-of-the-art few-step text-to-image\ngenerative models with R0 at scales. Our results challenge the conventional\nwisdom of diffusion post-training and conditional generation by demonstrating\nthat rewards play a dominant role in scenarios with complex conditions. We hope\nour findings can contribute to further research into human-centric and\nreward-centric generation paradigms across the broader field of AIGC. Code is\navailable at https://github.com/Luo-Yihong/R0.",
      "upvotes": 3,
      "discussionId": "67d8fbf841d31cc626e4d812",
      "githubRepo": "https://github.com/Luo-Yihong/R0",
      "ai_keywords": [
        "reward-enhanced diffusion distillation",
        "diffusion losses",
        "R0",
        "regularized reward maximization",
        "optimization problem in data space",
        "compositional rewards",
        "generator parameterization",
        "state-of-the-art few-step text-to-image generative models",
        "diffusion post-training",
        "human-centric generation",
        "reward-centric generation paradigms"
      ]
    },
    "publishedAt": "2025-03-17T07:21:43.000Z",
    "title": "Rewards Are Enough for Fast Photo-Realistic Text-to-image Generation",
    "summary": "Aligning generated images to complicated text prompts and human preferences\nis a central challenge in Artificial Intelligence-Generated Content (AIGC).\nWith reward-enhanced diffusion distillation emerging as a promising approach\nthat boosts controllability and fidelity of text-to-image models, we identify a\nfundamental paradigm shift: as conditions become more specific and reward\nsignals stronger, the rewards themselves become the dominant force in\ngeneration. In contrast, the diffusion losses serve as an overly expensive form\nof regularization. To thoroughly validate our hypothesis, we introduce R0, a\nnovel conditional generation approach via regularized reward maximization.\nInstead of relying on tricky diffusion distillation losses, R0 proposes a new\nperspective that treats image generations as an optimization problem in data\nspace which aims to search for valid images that have high compositional\nrewards. By innovative designs of the generator parameterization and proper\nregularization techniques, we train state-of-the-art few-step text-to-image\ngenerative models with R0 at scales. Our results challenge the conventional\nwisdom of diffusion post-training and conditional generation by demonstrating\nthat rewards play a dominant role in scenarios with complex conditions. We hope\nour findings can contribute to further research into human-centric and\nreward-centric generation paradigms across the broader field of AIGC. Code is\navailable at https://github.com/Luo-Yihong/R0.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13070.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65f7e6856bd4bac5b6a4ecc3",
      "avatarUrl": "/avatars/9c0c48310fb5e99c19700316a0ab531e.svg",
      "fullname": "Yihong Luo",
      "name": "Luo-Yihong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.11412",
      "authors": [
        {
          "_id": "67d8f8b77f61dda9ea6512b7",
          "name": "Shiyuan Yang",
          "hidden": false
        },
        {
          "_id": "67d8f8b77f61dda9ea6512b8",
          "name": "Zheng Gu",
          "hidden": false
        },
        {
          "_id": "67d8f8b77f61dda9ea6512b9",
          "name": "Liang Hou",
          "hidden": false
        },
        {
          "_id": "67d8f8b77f61dda9ea6512ba",
          "name": "Xin Tao",
          "hidden": false
        },
        {
          "_id": "67d8f8b77f61dda9ea6512bb",
          "user": {
            "_id": "662f93942510ef5735d7ad00",
            "avatarUrl": "/avatars/dc9486db75869ce902d0a638eea126bd.svg",
            "isPro": false,
            "fullname": "magicwpf",
            "user": "magicwpf",
            "type": "user"
          },
          "name": "Pengfei Wan",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-18T04:42:06.437Z",
          "hidden": false
        },
        {
          "_id": "67d8f8b77f61dda9ea6512bc",
          "name": "Xiaodong Chen",
          "hidden": false
        },
        {
          "_id": "67d8f8b77f61dda9ea6512bd",
          "name": "Jing Liao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63316d499e3604f3f17f5d89/jV8ETjZh0NTDoFBhSNv8I.mp4"
      ],
      "publishedAt": "2025-03-14T13:54:10.000Z",
      "submittedOnDailyAt": "2025-03-18T03:20:01.050Z",
      "title": "MTV-Inpaint: Multi-Task Long Video Inpainting",
      "submittedOnDailyBy": {
        "_id": "63316d499e3604f3f17f5d89",
        "avatarUrl": "/avatars/c7e0c8bdd7e598276cfd133d5222c5e8.svg",
        "isPro": false,
        "fullname": "catfood",
        "user": "ysy31415926",
        "type": "user"
      },
      "summary": "Video inpainting involves modifying local regions within a video, ensuring\nspatial and temporal consistency. Most existing methods focus primarily on\nscene completion (i.e., filling missing regions) and lack the capability to\ninsert new objects into a scene in a controllable manner. Fortunately, recent\nadvancements in text-to-video (T2V) diffusion models pave the way for\ntext-guided video inpainting. However, directly adapting T2V models for\ninpainting remains limited in unifying completion and insertion tasks, lacks\ninput controllability, and struggles with long videos, thereby restricting\ntheir applicability and flexibility. To address these challenges, we propose\nMTV-Inpaint, a unified multi-task video inpainting framework capable of\nhandling both traditional scene completion and novel object insertion tasks. To\nunify these distinct tasks, we design a dual-branch spatial attention mechanism\nin the T2V diffusion U-Net, enabling seamless integration of scene completion\nand object insertion within a single framework. In addition to textual\nguidance, MTV-Inpaint supports multimodal control by integrating various image\ninpainting models through our proposed image-to-video (I2V) inpainting mode.\nAdditionally, we propose a two-stage pipeline that combines keyframe inpainting\nwith in-between frame propagation, enabling MTV-Inpaint to effectively handle\nlong videos with hundreds of frames. Extensive experiments demonstrate that\nMTV-Inpaint achieves state-of-the-art performance in both scene completion and\nobject insertion tasks. Furthermore, it demonstrates versatility in derived\napplications such as multi-modal inpainting, object editing, removal, image\nobject brush, and the ability to handle long videos. Project page:\nhttps://mtv-inpaint.github.io/.",
      "upvotes": 3,
      "discussionId": "67d8f8bf7f61dda9ea6514a7",
      "projectPage": "https://mtv-inpaint.github.io/",
      "ai_keywords": [
        "text-to-video (T2V) diffusion models",
        "text-guided video inpainting",
        "dual-branch spatial attention mechanism",
        "T2V diffusion U-Net",
        "multimodal control",
        "image-to-video (I2V) inpainting mode",
        "keyframe inpainting",
        "in-between frame propagation",
        "multi-modal inpainting",
        "object editing",
        "object removal",
        "image object brush"
      ]
    },
    "publishedAt": "2025-03-14T09:54:10.000Z",
    "title": "MTV-Inpaint: Multi-Task Long Video Inpainting",
    "summary": "Video inpainting involves modifying local regions within a video, ensuring\nspatial and temporal consistency. Most existing methods focus primarily on\nscene completion (i.e., filling missing regions) and lack the capability to\ninsert new objects into a scene in a controllable manner. Fortunately, recent\nadvancements in text-to-video (T2V) diffusion models pave the way for\ntext-guided video inpainting. However, directly adapting T2V models for\ninpainting remains limited in unifying completion and insertion tasks, lacks\ninput controllability, and struggles with long videos, thereby restricting\ntheir applicability and flexibility. To address these challenges, we propose\nMTV-Inpaint, a unified multi-task video inpainting framework capable of\nhandling both traditional scene completion and novel object insertion tasks. To\nunify these distinct tasks, we design a dual-branch spatial attention mechanism\nin the T2V diffusion U-Net, enabling seamless integration of scene completion\nand object insertion within a single framework. In addition to textual\nguidance, MTV-Inpaint supports multimodal control by integrating various image\ninpainting models through our proposed image-to-video (I2V) inpainting mode.\nAdditionally, we propose a two-stage pipeline that combines keyframe inpainting\nwith in-between frame propagation, enabling MTV-Inpaint to effectively handle\nlong videos with hundreds of frames. Extensive experiments demonstrate that\nMTV-Inpaint achieves state-of-the-art performance in both scene completion and\nobject insertion tasks. Furthermore, it demonstrates versatility in derived\napplications such as multi-modal inpainting, object editing, removal, image\nobject brush, and the ability to handle long videos. Project page:\nhttps://mtv-inpaint.github.io/.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63316d499e3604f3f17f5d89/jV8ETjZh0NTDoFBhSNv8I.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11412.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63316d499e3604f3f17f5d89",
      "avatarUrl": "/avatars/c7e0c8bdd7e598276cfd133d5222c5e8.svg",
      "fullname": "catfood",
      "name": "ysy31415926",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.10704",
      "authors": [
        {
          "_id": "67d7eba831dd5b46c3e6fdcb",
          "name": "Jing Wang",
          "hidden": false
        },
        {
          "_id": "67d7eba831dd5b46c3e6fdcc",
          "name": "Fengzhuo Zhang",
          "hidden": false
        },
        {
          "_id": "67d7eba831dd5b46c3e6fdcd",
          "name": "Xiaoli Li",
          "hidden": false
        },
        {
          "_id": "67d7eba831dd5b46c3e6fdce",
          "name": "Vincent Y. F. Tan",
          "hidden": false
        },
        {
          "_id": "67d7eba831dd5b46c3e6fdcf",
          "name": "Tianyu Pang",
          "hidden": false
        },
        {
          "_id": "67d7eba831dd5b46c3e6fdd0",
          "name": "Chao Du",
          "hidden": false
        },
        {
          "_id": "67d7eba831dd5b46c3e6fdd1",
          "name": "Aixin Sun",
          "hidden": false
        },
        {
          "_id": "67d7eba831dd5b46c3e6fdd2",
          "user": {
            "_id": "6397873ec0b27f432db8693f",
            "avatarUrl": "/avatars/1db65fe55002ad5c137c4a59bbcd239d.svg",
            "isPro": false,
            "fullname": "Zhuoran Yang",
            "user": "zhuoran",
            "type": "user"
          },
          "name": "Zhuoran Yang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-17T09:30:18.764Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-12T15:32:44.000Z",
      "submittedOnDailyAt": "2025-03-18T03:10:46.254Z",
      "title": "Error Analyses of Auto-Regressive Video Diffusion Models: A Unified\n  Framework",
      "submittedOnDailyBy": {
        "_id": "633c2310c0fb6fd232f0accf",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633c2310c0fb6fd232f0accf/AnvEvxMtd-BpSZbaoA4ha.jpeg",
        "isPro": false,
        "fullname": "Wang Jing",
        "user": "k-nick",
        "type": "user"
      },
      "summary": "A variety of Auto-Regressive Video Diffusion Models (ARVDM) have achieved\nremarkable successes in generating realistic long-form videos. However,\ntheoretical analyses of these models remain scant. In this work, we develop\ntheoretical underpinnings for these models and use our insights to improve the\nperformance of existing models. We first develop Meta-ARVDM, a unified\nframework of ARVDMs that subsumes most existing methods. Using Meta-ARVDM, we\nanalyze the KL-divergence between the videos generated by Meta-ARVDM and the\ntrue videos. Our analysis uncovers two important phenomena inherent to ARVDM --\nerror accumulation and memory bottleneck. By deriving an information-theoretic\nimpossibility result, we show that the memory bottleneck phenomenon cannot be\navoided. To mitigate the memory bottleneck, we design various network\nstructures to explicitly use more past frames. We also achieve a significantly\nimproved trade-off between the mitigation of the memory bottleneck and the\ninference efficiency by compressing the frames. Experimental results on DMLab\nand Minecraft validate the efficacy of our methods. Our experiments also\ndemonstrate a Pareto-frontier between the error accumulation and memory\nbottleneck across different methods.",
      "upvotes": 2,
      "discussionId": "67d7ebaa31dd5b46c3e6fe5a",
      "projectPage": "https://sail-sg.github.io/AR-Video-Diffusion",
      "ai_keywords": [
        "Auto-Regressive Video Diffusion Models",
        "Meta-ARVDM",
        "KL-divergence",
        "error accumulation",
        "memory bottleneck",
        "information-theoretic impossibility result",
        "network structures",
        "frame compression",
        "DMLab",
        "Minecraft",
        "Pareto-frontier"
      ]
    },
    "publishedAt": "2025-03-12T11:32:44.000Z",
    "title": "Error Analyses of Auto-Regressive Video Diffusion Models: A Unified\n  Framework",
    "summary": "A variety of Auto-Regressive Video Diffusion Models (ARVDM) have achieved\nremarkable successes in generating realistic long-form videos. However,\ntheoretical analyses of these models remain scant. In this work, we develop\ntheoretical underpinnings for these models and use our insights to improve the\nperformance of existing models. We first develop Meta-ARVDM, a unified\nframework of ARVDMs that subsumes most existing methods. Using Meta-ARVDM, we\nanalyze the KL-divergence between the videos generated by Meta-ARVDM and the\ntrue videos. Our analysis uncovers two important phenomena inherent to ARVDM --\nerror accumulation and memory bottleneck. By deriving an information-theoretic\nimpossibility result, we show that the memory bottleneck phenomenon cannot be\navoided. To mitigate the memory bottleneck, we design various network\nstructures to explicitly use more past frames. We also achieve a significantly\nimproved trade-off between the mitigation of the memory bottleneck and the\ninference efficiency by compressing the frames. Experimental results on DMLab\nand Minecraft validate the efficacy of our methods. Our experiments also\ndemonstrate a Pareto-frontier between the error accumulation and memory\nbottleneck across different methods.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10704.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "633c2310c0fb6fd232f0accf",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633c2310c0fb6fd232f0accf/AnvEvxMtd-BpSZbaoA4ha.jpeg",
      "fullname": "Wang Jing",
      "name": "k-nick",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.12530",
      "authors": [
        {
          "_id": "67d8f0fa53f713733d6c6b1c",
          "name": "Hunter Sawyer",
          "hidden": false
        },
        {
          "_id": "67d8f0fa53f713733d6c6b1d",
          "user": {
            "_id": "63c19eb3a0ffa3857eae2efa",
            "avatarUrl": "/avatars/35b06ca092f615a6d11ee99683d0376a.svg",
            "isPro": false,
            "fullname": "Jesse Roberts",
            "user": "JesseTNRoberts",
            "type": "user"
          },
          "name": "Jesse Roberts",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-18T04:05:16.739Z",
          "hidden": false
        },
        {
          "_id": "67d8f0fa53f713733d6c6b1e",
          "name": "Kyle Moore",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-16T14:50:54.000Z",
      "submittedOnDailyAt": "2025-03-18T02:37:23.781Z",
      "title": "Basic Category Usage in Vision Language Models",
      "submittedOnDailyBy": {
        "_id": "63c19eb3a0ffa3857eae2efa",
        "avatarUrl": "/avatars/35b06ca092f615a6d11ee99683d0376a.svg",
        "isPro": false,
        "fullname": "Jesse Roberts",
        "user": "JesseTNRoberts",
        "type": "user"
      },
      "summary": "The field of psychology has long recognized a basic level of categorization\nthat humans use when labeling visual stimuli, a term coined by Rosch in 1976.\nThis level of categorization has been found to be used most frequently, to have\nhigher information density, and to aid in visual language tasks with priming in\nhumans. Here, we investigate basic level categorization in two recently\nreleased, open-source vision-language models (VLMs). This paper demonstrates\nthat Llama 3.2 Vision Instruct (11B) and Molmo 7B-D both prefer basic level\ncategorization consistent with human behavior. Moreover, the models'\npreferences are consistent with nuanced human behaviors like the biological\nversus non-biological basic level effects and the well established expert basic\nlevel shift, further suggesting that VLMs acquire cognitive categorization\nbehaviors from the human data on which they are trained.",
      "upvotes": 1,
      "discussionId": "67d8f0fc53f713733d6c6b89",
      "ai_keywords": [
        "vision-language models (VLMs)",
        "basic level categorization",
        "biological versus non-biological basic level effects",
        "expert basic level shift",
        "cognitive categorization behaviors"
      ]
    },
    "publishedAt": "2025-03-16T10:50:54.000Z",
    "title": "Basic Category Usage in Vision Language Models",
    "summary": "The field of psychology has long recognized a basic level of categorization\nthat humans use when labeling visual stimuli, a term coined by Rosch in 1976.\nThis level of categorization has been found to be used most frequently, to have\nhigher information density, and to aid in visual language tasks with priming in\nhumans. Here, we investigate basic level categorization in two recently\nreleased, open-source vision-language models (VLMs). This paper demonstrates\nthat Llama 3.2 Vision Instruct (11B) and Molmo 7B-D both prefer basic level\ncategorization consistent with human behavior. Moreover, the models'\npreferences are consistent with nuanced human behaviors like the biological\nversus non-biological basic level effects and the well established expert basic\nlevel shift, further suggesting that VLMs acquire cognitive categorization\nbehaviors from the human data on which they are trained.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12530.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63c19eb3a0ffa3857eae2efa",
      "avatarUrl": "/avatars/35b06ca092f615a6d11ee99683d0376a.svg",
      "fullname": "Jesse Roberts",
      "name": "JesseTNRoberts",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.12528",
      "authors": [
        {
          "_id": "67d8f044f8b0e148f60cef0d",
          "name": "Kyle Moore",
          "hidden": false
        },
        {
          "_id": "67d8f044f8b0e148f60cef0e",
          "user": {
            "_id": "63c19eb3a0ffa3857eae2efa",
            "avatarUrl": "/avatars/35b06ca092f615a6d11ee99683d0376a.svg",
            "isPro": false,
            "fullname": "Jesse Roberts",
            "user": "JesseTNRoberts",
            "type": "user"
          },
          "name": "Jesse Roberts",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-18T04:02:14.260Z",
          "hidden": false
        },
        {
          "_id": "67d8f044f8b0e148f60cef0f",
          "name": "Daryl Watson",
          "hidden": false
        },
        {
          "_id": "67d8f044f8b0e148f60cef10",
          "name": "Pamela Wisniewski",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-16T14:45:43.000Z",
      "submittedOnDailyAt": "2025-03-18T02:34:14.582Z",
      "title": "Investigating Human-Aligned Large Language Model Uncertainty",
      "submittedOnDailyBy": {
        "_id": "63c19eb3a0ffa3857eae2efa",
        "avatarUrl": "/avatars/35b06ca092f615a6d11ee99683d0376a.svg",
        "isPro": false,
        "fullname": "Jesse Roberts",
        "user": "JesseTNRoberts",
        "type": "user"
      },
      "summary": "Recent work has sought to quantify large language model uncertainty to\nfacilitate model control and modulate user trust. Previous works focus on\nmeasures of uncertainty that are theoretically grounded or reflect the average\novert behavior of the model. In this work, we investigate a variety of\nuncertainty measures, in order to identify measures that correlate with human\ngroup-level uncertainty. We find that Bayesian measures and a variation on\nentropy measures, top-k entropy, tend to agree with human behavior as a\nfunction of model size. We find that some strong measures decrease in\nhuman-similarity with model size, but, by multiple linear regression, we find\nthat combining multiple uncertainty measures provide comparable human-alignment\nwith reduced size-dependency.",
      "upvotes": 1,
      "discussionId": "67d8f046f8b0e148f60cef95",
      "ai_keywords": [
        "Bayesian measures",
        "entropy measures",
        "top-k entropy",
        "human-similarity",
        "human-alignment",
        "multiple linear regression"
      ]
    },
    "publishedAt": "2025-03-16T10:45:43.000Z",
    "title": "Investigating Human-Aligned Large Language Model Uncertainty",
    "summary": "Recent work has sought to quantify large language model uncertainty to\nfacilitate model control and modulate user trust. Previous works focus on\nmeasures of uncertainty that are theoretically grounded or reflect the average\novert behavior of the model. In this work, we investigate a variety of\nuncertainty measures, in order to identify measures that correlate with human\ngroup-level uncertainty. We find that Bayesian measures and a variation on\nentropy measures, top-k entropy, tend to agree with human behavior as a\nfunction of model size. We find that some strong measures decrease in\nhuman-similarity with model size, but, by multiple linear regression, we find\nthat combining multiple uncertainty measures provide comparable human-alignment\nwith reduced size-dependency.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12528.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63c19eb3a0ffa3857eae2efa",
      "avatarUrl": "/avatars/35b06ca092f615a6d11ee99683d0376a.svg",
      "fullname": "Jesse Roberts",
      "name": "JesseTNRoberts",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]