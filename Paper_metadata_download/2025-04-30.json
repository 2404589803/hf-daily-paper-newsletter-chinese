[
  {
    "paper": {
      "id": "2504.20734",
      "authors": [
        {
          "_id": "6811966ae20ba7d0683b8adc",
          "name": "Woongyeong Yeo",
          "hidden": false
        },
        {
          "_id": "6811966ae20ba7d0683b8add",
          "name": "Kangsan Kim",
          "hidden": false
        },
        {
          "_id": "6811966ae20ba7d0683b8ade",
          "name": "Soyeong Jeong",
          "hidden": false
        },
        {
          "_id": "6811966ae20ba7d0683b8adf",
          "name": "Jinheon Baek",
          "hidden": false
        },
        {
          "_id": "6811966ae20ba7d0683b8ae0",
          "name": "Sung Ju Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-29T13:18:58.000Z",
      "submittedOnDailyAt": "2025-04-30T01:50:28.624Z",
      "title": "UniversalRAG: Retrieval-Augmented Generation over Multiple Corpora with\n  Diverse Modalities and Granularities",
      "submittedOnDailyBy": {
        "_id": "66d30f5fad293ffc4b7672bc",
        "avatarUrl": "/avatars/6f164d813b947940a088820f8fd4dbe8.svg",
        "isPro": false,
        "fullname": "Woongyeong Yeo",
        "user": "wgcyeo",
        "type": "user"
      },
      "summary": "Retrieval-Augmented Generation (RAG) has shown substantial promise in\nimproving factual accuracy by grounding model responses with external knowledge\nrelevant to queries. However, most existing RAG approaches are limited to a\ntext-only corpus, and while recent efforts have extended RAG to other\nmodalities such as images and videos, they typically operate over a single\nmodality-specific corpus. In contrast, real-world queries vary widely in the\ntype of knowledge they require, which a single type of knowledge source cannot\naddress. To address this, we introduce UniversalRAG, a novel RAG framework\ndesigned to retrieve and integrate knowledge from heterogeneous sources with\ndiverse modalities and granularities. Specifically, motivated by the\nobservation that forcing all modalities into a unified representation space\nderived from a single combined corpus causes a modality gap, where the\nretrieval tends to favor items from the same modality as the query, we propose\na modality-aware routing mechanism that dynamically identifies the most\nappropriate modality-specific corpus and performs targeted retrieval within it.\nAlso, beyond modality, we organize each modality into multiple granularity\nlevels, enabling fine-tuned retrieval tailored to the complexity and scope of\nthe query. We validate UniversalRAG on 8 benchmarks spanning multiple\nmodalities, showing its superiority over modality-specific and unified\nbaselines.",
      "upvotes": 34,
      "discussionId": "6811966ae20ba7d0683b8b0e",
      "projectPage": "https://universalrag.github.io",
      "githubRepo": "https://github.com/wgcyeo/UniversalRAG",
      "ai_keywords": [
        "Retrieval-Augmented Generation (RAG)",
        "factual accuracy",
        "external knowledge",
        "text-only corpus",
        "modality-specific corpus",
        "heterogenous sources",
        "diverse modalities",
        "granularities",
        "modality gap",
        "modality-aware routing mechanism",
        "targeted retrieval",
        "granularity levels",
        "fine-tuned retrieval",
        "multi-modal benchmarks",
        "modality-specific baselines",
        "unified baselines"
      ]
    },
    "publishedAt": "2025-04-29T09:18:58.000Z",
    "title": "UniversalRAG: Retrieval-Augmented Generation over Multiple Corpora with\n  Diverse Modalities and Granularities",
    "summary": "Retrieval-Augmented Generation (RAG) has shown substantial promise in\nimproving factual accuracy by grounding model responses with external knowledge\nrelevant to queries. However, most existing RAG approaches are limited to a\ntext-only corpus, and while recent efforts have extended RAG to other\nmodalities such as images and videos, they typically operate over a single\nmodality-specific corpus. In contrast, real-world queries vary widely in the\ntype of knowledge they require, which a single type of knowledge source cannot\naddress. To address this, we introduce UniversalRAG, a novel RAG framework\ndesigned to retrieve and integrate knowledge from heterogeneous sources with\ndiverse modalities and granularities. Specifically, motivated by the\nobservation that forcing all modalities into a unified representation space\nderived from a single combined corpus causes a modality gap, where the\nretrieval tends to favor items from the same modality as the query, we propose\na modality-aware routing mechanism that dynamically identifies the most\nappropriate modality-specific corpus and performs targeted retrieval within it.\nAlso, beyond modality, we organize each modality into multiple granularity\nlevels, enabling fine-tuned retrieval tailored to the complexity and scope of\nthe query. We validate UniversalRAG on 8 benchmarks spanning multiple\nmodalities, showing its superiority over modality-specific and unified\nbaselines.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20734.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66d30f5fad293ffc4b7672bc",
      "avatarUrl": "/avatars/6f164d813b947940a088820f8fd4dbe8.svg",
      "fullname": "Woongyeong Yeo",
      "name": "wgcyeo",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.20571",
      "authors": [
        {
          "_id": "681187ddda5ce4cbd7556714",
          "name": "Yiping Wang",
          "hidden": false
        },
        {
          "_id": "681187ddda5ce4cbd7556715",
          "name": "Qing Yang",
          "hidden": false
        },
        {
          "_id": "681187ddda5ce4cbd7556716",
          "name": "Zhiyuan Zeng",
          "hidden": false
        },
        {
          "_id": "681187ddda5ce4cbd7556717",
          "name": "Liliang Ren",
          "hidden": false
        },
        {
          "_id": "681187ddda5ce4cbd7556718",
          "name": "Lucas Liu",
          "hidden": false
        },
        {
          "_id": "681187ddda5ce4cbd7556719",
          "name": "Baolin Peng",
          "hidden": false
        },
        {
          "_id": "681187ddda5ce4cbd755671a",
          "name": "Hao Cheng",
          "hidden": false
        },
        {
          "_id": "681187ddda5ce4cbd755671b",
          "name": "Xuehai He",
          "hidden": false
        },
        {
          "_id": "681187ddda5ce4cbd755671c",
          "name": "Kuan Wang",
          "hidden": false
        },
        {
          "_id": "681187ddda5ce4cbd755671d",
          "name": "Jianfeng Gao",
          "hidden": false
        },
        {
          "_id": "681187ddda5ce4cbd755671e",
          "name": "Weizhu Chen",
          "hidden": false
        },
        {
          "_id": "681187ddda5ce4cbd755671f",
          "name": "Shuohang Wang",
          "hidden": false
        },
        {
          "_id": "681187ddda5ce4cbd7556720",
          "name": "Simon Shaolei Du",
          "hidden": false
        },
        {
          "_id": "681187ddda5ce4cbd7556721",
          "name": "Yelong Shen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-29T09:24:30.000Z",
      "submittedOnDailyAt": "2025-04-30T00:46:23.617Z",
      "title": "Reinforcement Learning for Reasoning in Large Language Models with One\n  Training Example",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "We show that reinforcement learning with verifiable reward using one training\nexample (1-shot RLVR) is effective in incentivizing the math reasoning\ncapabilities of large language models (LLMs). Applying RLVR to the base model\nQwen2.5-Math-1.5B, we identify a single example that elevates model performance\non MATH500 from 36.0% to 73.6%, and improves the average performance across six\ncommon mathematical reasoning benchmarks from 17.6% to 35.7%. This result\nmatches the performance obtained using the 1.2k DeepScaleR subset (MATH500:\n73.6%, average: 35.9%), which includes the aforementioned example. Similar\nsubstantial improvements are observed across various models (Qwen2.5-Math-7B,\nLlama3.2-3B-Instruct, DeepSeek-R1-Distill-Qwen-1.5B), RL algorithms (GRPO and\nPPO), and different math examples (many of which yield approximately 30% or\ngreater improvement on MATH500 when employed as a single training example). In\naddition, we identify some interesting phenomena during 1-shot RLVR, including\ncross-domain generalization, increased frequency of self-reflection, and\nsustained test performance improvement even after the training accuracy has\nsaturated, a phenomenon we term post-saturation generalization. Moreover, we\nverify that the effectiveness of 1-shot RLVR primarily arises from the policy\ngradient loss, distinguishing it from the \"grokking\" phenomenon. We also show\nthe critical role of promoting exploration (e.g., by adding entropy loss with\nan appropriate coefficient) in 1-shot RLVR training. As a bonus, we observe\nthat applying entropy loss alone, without any outcome reward, significantly\nenhances Qwen2.5-Math-1.5B's performance on MATH500 by 27.4%. These findings\ncan inspire future work on RLVR data efficiency and encourage a re-examination\nof both recent progress and the underlying mechanisms in RLVR. Our code, model,\nand data are open source at https://github.com/ypwang61/One-Shot-RLVR",
      "upvotes": 21,
      "discussionId": "681187ddda5ce4cbd7556754",
      "ai_keywords": [
        "reinforcement learning with verifiable reward (RLVR)",
        "1-shot RLVR",
        "large language models (LLMs)",
        "Qwen2.5-Math-1.5B",
        "MATH500",
        "mathematical reasoning benchmarks",
        "Qwen2.5-Math-7B",
        "Llama3.2-3B-Instruct",
        "DeepSeek-R1-Distill-Qwen-1.5B",
        "GRPO",
        "PPO",
        "cross-domain generalization",
        "self-reflection",
        "post-saturation generalization",
        "policy gradient loss",
        "entropic exploration",
        "entropy loss"
      ]
    },
    "publishedAt": "2025-04-29T05:24:30.000Z",
    "title": "Reinforcement Learning for Reasoning in Large Language Models with One\n  Training Example",
    "summary": "We show that reinforcement learning with verifiable reward using one training\nexample (1-shot RLVR) is effective in incentivizing the math reasoning\ncapabilities of large language models (LLMs). Applying RLVR to the base model\nQwen2.5-Math-1.5B, we identify a single example that elevates model performance\non MATH500 from 36.0% to 73.6%, and improves the average performance across six\ncommon mathematical reasoning benchmarks from 17.6% to 35.7%. This result\nmatches the performance obtained using the 1.2k DeepScaleR subset (MATH500:\n73.6%, average: 35.9%), which includes the aforementioned example. Similar\nsubstantial improvements are observed across various models (Qwen2.5-Math-7B,\nLlama3.2-3B-Instruct, DeepSeek-R1-Distill-Qwen-1.5B), RL algorithms (GRPO and\nPPO), and different math examples (many of which yield approximately 30% or\ngreater improvement on MATH500 when employed as a single training example). In\naddition, we identify some interesting phenomena during 1-shot RLVR, including\ncross-domain generalization, increased frequency of self-reflection, and\nsustained test performance improvement even after the training accuracy has\nsaturated, a phenomenon we term post-saturation generalization. Moreover, we\nverify that the effectiveness of 1-shot RLVR primarily arises from the policy\ngradient loss, distinguishing it from the \"grokking\" phenomenon. We also show\nthe critical role of promoting exploration (e.g., by adding entropy loss with\nan appropriate coefficient) in 1-shot RLVR training. As a bonus, we observe\nthat applying entropy loss alone, without any outcome reward, significantly\nenhances Qwen2.5-Math-1.5B's performance on MATH500 by 27.4%. These findings\ncan inspire future work on RLVR data efficiency and encourage a re-examination\nof both recent progress and the underlying mechanisms in RLVR. Our code, model,\nand data are open source at https://github.com/ypwang61/One-Shot-RLVR",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20571.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6748
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.20595",
      "authors": [
        {
          "_id": "68118a9f4570c2ba44bf4418",
          "name": "Rulin Shao",
          "hidden": false
        },
        {
          "_id": "68118a9f4570c2ba44bf4419",
          "name": "Rui Qiao",
          "hidden": false
        },
        {
          "_id": "68118a9f4570c2ba44bf441a",
          "name": "Varsha Kishore",
          "hidden": false
        },
        {
          "_id": "68118a9f4570c2ba44bf441b",
          "name": "Niklas Muennighoff",
          "hidden": false
        },
        {
          "_id": "68118a9f4570c2ba44bf441c",
          "name": "Xi Victoria Lin",
          "hidden": false
        },
        {
          "_id": "68118a9f4570c2ba44bf441d",
          "name": "Daniela Rus",
          "hidden": false
        },
        {
          "_id": "68118a9f4570c2ba44bf441e",
          "name": "Bryan Kian Hsiang Low",
          "hidden": false
        },
        {
          "_id": "68118a9f4570c2ba44bf441f",
          "name": "Sewon Min",
          "hidden": false
        },
        {
          "_id": "68118a9f4570c2ba44bf4420",
          "name": "Wen-tau Yih",
          "hidden": false
        },
        {
          "_id": "68118a9f4570c2ba44bf4421",
          "name": "Pang Wei Koh",
          "hidden": false
        },
        {
          "_id": "68118a9f4570c2ba44bf4422",
          "name": "Luke Zettlemoyer",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-29T09:49:28.000Z",
      "submittedOnDailyAt": "2025-04-30T00:58:16.950Z",
      "title": "ReasonIR: Training Retrievers for Reasoning Tasks",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "We present ReasonIR-8B, the first retriever specifically trained for general\nreasoning tasks. Existing retrievers have shown limited gains on reasoning\ntasks, in part because existing training datasets focus on short factual\nqueries tied to documents that straightforwardly answer them. We develop a\nsynthetic data generation pipeline that, for each document, our pipeline\ncreates a challenging and relevant query, along with a plausibly related but\nultimately unhelpful hard negative. By training on a mixture of our synthetic\ndata and existing public data, ReasonIR-8B achieves a new state-of-the-art of\n29.9 nDCG@10 without reranker and 36.9 nDCG@10 with reranker on BRIGHT, a\nwidely-used reasoning-intensive information retrieval (IR) benchmark. When\napplied to RAG tasks, ReasonIR-8B improves MMLU and GPQA performance by 6.4%\nand 22.6% respectively, relative to the closed-book baseline, outperforming\nother retrievers and search engines. In addition, ReasonIR-8B uses test-time\ncompute more effectively: on BRIGHT, its performance consistently increases\nwith longer and more information-rich rewritten queries; it continues to\noutperform other retrievers when combined with an LLM reranker. Our training\nrecipe is general and can be easily extended to future LLMs; to this end, we\nopen-source our code, data, and model.",
      "upvotes": 19,
      "discussionId": "68118aa44570c2ba44bf457b",
      "ai_keywords": [
        "retriever",
        "ReasonIR-8B",
        "general reasoning tasks",
        "synthetic data generation pipeline",
        "hard negative",
        "nDCG@10",
        "BRIGHT",
        "information retrieval (IR) benchmark",
        "RAG tasks",
        "MMLU",
        "GPQA",
        "closed-book baseline",
        "LLM reranker",
        "test-time compute",
        "rewritten queries",
        "LLM"
      ]
    },
    "publishedAt": "2025-04-29T05:49:28.000Z",
    "title": "ReasonIR: Training Retrievers for Reasoning Tasks",
    "summary": "We present ReasonIR-8B, the first retriever specifically trained for general\nreasoning tasks. Existing retrievers have shown limited gains on reasoning\ntasks, in part because existing training datasets focus on short factual\nqueries tied to documents that straightforwardly answer them. We develop a\nsynthetic data generation pipeline that, for each document, our pipeline\ncreates a challenging and relevant query, along with a plausibly related but\nultimately unhelpful hard negative. By training on a mixture of our synthetic\ndata and existing public data, ReasonIR-8B achieves a new state-of-the-art of\n29.9 nDCG@10 without reranker and 36.9 nDCG@10 with reranker on BRIGHT, a\nwidely-used reasoning-intensive information retrieval (IR) benchmark. When\napplied to RAG tasks, ReasonIR-8B improves MMLU and GPQA performance by 6.4%\nand 22.6% respectively, relative to the closed-book baseline, outperforming\nother retrievers and search engines. In addition, ReasonIR-8B uses test-time\ncompute more effectively: on BRIGHT, its performance consistently increases\nwith longer and more information-rich rewritten queries; it continues to\noutperform other retrievers when combined with an LLM reranker. Our training\nrecipe is general and can be easily extended to future LLMs; to this end, we\nopen-source our code, data, and model.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20595.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6748
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.20157",
      "authors": [
        {
          "_id": "68119750ff0764f3840a7f93",
          "name": "Zae Myung Kim",
          "hidden": false
        },
        {
          "_id": "68119750ff0764f3840a7f94",
          "name": "Chanwoo Park",
          "hidden": false
        },
        {
          "_id": "68119750ff0764f3840a7f95",
          "name": "Vipul Raheja",
          "hidden": false
        },
        {
          "_id": "68119750ff0764f3840a7f96",
          "name": "Dongyeop Kang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6434b6619bd5a84b5dcfa4de/tHS8gWUK0ptmNTs6lZck6.png",
        "https://cdn-uploads.huggingface.co/production/uploads/6434b6619bd5a84b5dcfa4de/uMD9av8pogPwYTW-KNFJ2.png"
      ],
      "publishedAt": "2025-04-28T18:02:35.000Z",
      "submittedOnDailyAt": "2025-04-30T02:04:16.540Z",
      "title": "Toward Evaluative Thinking: Meta Policy Optimization with Evolving\n  Reward Models",
      "submittedOnDailyBy": {
        "_id": "6434b6619bd5a84b5dcfa4de",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6434b6619bd5a84b5dcfa4de/h8Q6kPNjFNc03wmdboHzq.jpeg",
        "isPro": true,
        "fullname": "Young-Jun Lee",
        "user": "passing2961",
        "type": "user"
      },
      "summary": "Reward-based alignment methods for large language models (LLMs) face two key\nlimitations: vulnerability to reward hacking, where models exploit flaws in the\nreward signal; and reliance on brittle, labor-intensive prompt engineering when\nLLMs are used as reward models. We introduce Meta Policy Optimization (MPO), a\nframework that addresses these challenges by integrating a meta-reward model\nthat dynamically refines the reward model's prompt throughout training. In MPO,\nthe meta-reward model monitors the evolving training context and continuously\nadjusts the reward model's prompt to maintain high alignment, providing an\nadaptive reward signal that resists exploitation by the policy. This\nmeta-learning approach promotes a more stable policy optimization, and greatly\nreduces the need for manual reward prompt design. It yields performance on par\nwith or better than models guided by extensively hand-crafted reward prompts.\nFurthermore, we show that MPO maintains its effectiveness across diverse tasks,\nsuch as question answering and mathematical reasoning, without requiring\nspecialized reward designs. Beyond standard RLAIF, MPO's meta-learning\nformulation is readily extensible to higher-level alignment frameworks.\nOverall, this method addresses theoretical and practical challenges in\nreward-based RL alignment for LLMs, paving the way for more robust and\nadaptable alignment strategies. The code and models will be publicly shared.",
      "upvotes": 13,
      "discussionId": "68119751ff0764f3840a7fc5",
      "ai_keywords": [
        "Meta Policy Optimization (MPO)",
        "meta-reward model",
        "reward hacking",
        "prompt engineering",
        "policy optimization",
        "adaptive reward signal",
        "meta-learning approach",
        "prompt design",
        "reward-based RL alignment",
        "question answering",
        "mathematical reasoning"
      ]
    },
    "publishedAt": "2025-04-28T14:02:35.000Z",
    "title": "Toward Evaluative Thinking: Meta Policy Optimization with Evolving\n  Reward Models",
    "summary": "Reward-based alignment methods for large language models (LLMs) face two key\nlimitations: vulnerability to reward hacking, where models exploit flaws in the\nreward signal; and reliance on brittle, labor-intensive prompt engineering when\nLLMs are used as reward models. We introduce Meta Policy Optimization (MPO), a\nframework that addresses these challenges by integrating a meta-reward model\nthat dynamically refines the reward model's prompt throughout training. In MPO,\nthe meta-reward model monitors the evolving training context and continuously\nadjusts the reward model's prompt to maintain high alignment, providing an\nadaptive reward signal that resists exploitation by the policy. This\nmeta-learning approach promotes a more stable policy optimization, and greatly\nreduces the need for manual reward prompt design. It yields performance on par\nwith or better than models guided by extensively hand-crafted reward prompts.\nFurthermore, we show that MPO maintains its effectiveness across diverse tasks,\nsuch as question answering and mathematical reasoning, without requiring\nspecialized reward designs. Beyond standard RLAIF, MPO's meta-learning\nformulation is readily extensible to higher-level alignment frameworks.\nOverall, this method addresses theoretical and practical challenges in\nreward-based RL alignment for LLMs, paving the way for more robust and\nadaptable alignment strategies. The code and models will be publicly shared.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6434b6619bd5a84b5dcfa4de/tHS8gWUK0ptmNTs6lZck6.png",
      "https://cdn-uploads.huggingface.co/production/uploads/6434b6619bd5a84b5dcfa4de/uMD9av8pogPwYTW-KNFJ2.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20157.png",
    "numComments": 6,
    "submittedBy": {
      "_id": "6434b6619bd5a84b5dcfa4de",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6434b6619bd5a84b5dcfa4de/h8Q6kPNjFNc03wmdboHzq.jpeg",
      "fullname": "Young-Jun Lee",
      "name": "passing2961",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.20995",
      "authors": [
        {
          "_id": "68118c049c2765c9323de70b",
          "name": "Haoyu Zhen",
          "hidden": false
        },
        {
          "_id": "68118c049c2765c9323de70c",
          "name": "Qiao Sun",
          "hidden": false
        },
        {
          "_id": "68118c049c2765c9323de70d",
          "name": "Hongxin Zhang",
          "hidden": false
        },
        {
          "_id": "68118c049c2765c9323de70e",
          "name": "Junyan Li",
          "hidden": false
        },
        {
          "_id": "68118c049c2765c9323de70f",
          "name": "Siyuan Zhou",
          "hidden": false
        },
        {
          "_id": "68118c049c2765c9323de710",
          "name": "Yilun Du",
          "hidden": false
        },
        {
          "_id": "68118c049c2765c9323de711",
          "name": "Chuang Gan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-29T17:59:30.000Z",
      "submittedOnDailyAt": "2025-04-30T01:05:28.658Z",
      "title": "TesserAct: Learning 4D Embodied World Models",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "This paper presents an effective approach for learning novel 4D embodied\nworld models, which predict the dynamic evolution of 3D scenes over time in\nresponse to an embodied agent's actions, providing both spatial and temporal\nconsistency. We propose to learn a 4D world model by training on RGB-DN (RGB,\nDepth, and Normal) videos. This not only surpasses traditional 2D models by\nincorporating detailed shape, configuration, and temporal changes into their\npredictions, but also allows us to effectively learn accurate inverse dynamic\nmodels for an embodied agent. Specifically, we first extend existing robotic\nmanipulation video datasets with depth and normal information leveraging\noff-the-shelf models. Next, we fine-tune a video generation model on this\nannotated dataset, which jointly predicts RGB-DN (RGB, Depth, and Normal) for\neach frame. We then present an algorithm to directly convert generated RGB,\nDepth, and Normal videos into a high-quality 4D scene of the world. Our method\nensures temporal and spatial coherence in 4D scene predictions from embodied\nscenarios, enables novel view synthesis for embodied environments, and\nfacilitates policy learning that significantly outperforms those derived from\nprior video-based world models.",
      "upvotes": 8,
      "discussionId": "68118c089c2765c9323de81d",
      "ai_keywords": [
        "embodied world models",
        "4D world models",
        "RGB-DN (RGB, Depth, and Normal) videos",
        "video generation model",
        "inverse dynamic models",
        "robotic manipulation video datasets",
        "temporal coherence",
        "spatial coherence",
        "novel view synthesis",
        "policy learning"
      ]
    },
    "publishedAt": "2025-04-29T13:59:30.000Z",
    "title": "TesserAct: Learning 4D Embodied World Models",
    "summary": "This paper presents an effective approach for learning novel 4D embodied\nworld models, which predict the dynamic evolution of 3D scenes over time in\nresponse to an embodied agent's actions, providing both spatial and temporal\nconsistency. We propose to learn a 4D world model by training on RGB-DN (RGB,\nDepth, and Normal) videos. This not only surpasses traditional 2D models by\nincorporating detailed shape, configuration, and temporal changes into their\npredictions, but also allows us to effectively learn accurate inverse dynamic\nmodels for an embodied agent. Specifically, we first extend existing robotic\nmanipulation video datasets with depth and normal information leveraging\noff-the-shelf models. Next, we fine-tune a video generation model on this\nannotated dataset, which jointly predicts RGB-DN (RGB, Depth, and Normal) for\neach frame. We then present an algorithm to directly convert generated RGB,\nDepth, and Normal videos into a high-quality 4D scene of the world. Our method\nensures temporal and spatial coherence in 4D scene predictions from embodied\nscenarios, enables novel view synthesis for embodied environments, and\nfacilitates policy learning that significantly outperforms those derived from\nprior video-based world models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20995.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6748
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.16046",
      "authors": [
        {
          "_id": "68119c70e8a3493171fadce2",
          "name": "Jingyu Zhang",
          "hidden": false
        },
        {
          "_id": "68119c70e8a3493171fadce3",
          "name": "Jiacan Yu",
          "hidden": false
        },
        {
          "_id": "68119c70e8a3493171fadce4",
          "name": "Marc Marone",
          "hidden": false
        },
        {
          "_id": "68119c70e8a3493171fadce5",
          "name": "Benjamin Van Durme",
          "hidden": false
        },
        {
          "_id": "68119c70e8a3493171fadce6",
          "name": "Daniel Khashabi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-22T17:16:53.000Z",
      "submittedOnDailyAt": "2025-04-30T02:15:12.625Z",
      "title": "Certified Mitigation of Worst-Case LLM Copyright Infringement",
      "submittedOnDailyBy": {
        "_id": "62fb40b59af1d16bc0ac60f4",
        "avatarUrl": "/avatars/03ff66a419db8f2bc8e89a3b47aaaeac.svg",
        "isPro": false,
        "fullname": "Jack Zhang",
        "user": "jackzhang",
        "type": "user"
      },
      "summary": "The exposure of large language models (LLMs) to copyrighted material during\npre-training raises concerns about unintentional copyright infringement post\ndeployment. This has driven the development of \"copyright takedown\" methods,\npost-training approaches aimed at preventing models from generating content\nsubstantially similar to copyrighted ones. While current mitigation approaches\nare somewhat effective for average-case risks, we demonstrate that they\noverlook worst-case copyright risks exhibits by the existence of long, verbatim\nquotes from copyrighted sources. We propose BloomScrub, a remarkably simple yet\nhighly effective inference-time approach that provides certified copyright\ntakedown. Our method repeatedly interleaves quote detection with rewriting\ntechniques to transform potentially infringing segments. By leveraging\nefficient data sketches (Bloom filters), our approach enables scalable\ncopyright screening even for large-scale real-world corpora. When quotes beyond\na length threshold cannot be removed, the system can abstain from responding,\noffering certified risk reduction. Experimental results show that BloomScrub\nreduces infringement risk, preserves utility, and accommodates different levels\nof enforcement stringency with adaptive abstention. Our results suggest that\nlightweight, inference-time methods can be surprisingly effective for copyright\nprevention.",
      "upvotes": 6,
      "discussionId": "68119c70e8a3493171fadd11",
      "ai_keywords": [
        "BloomScrub",
        "Bloom filters",
        "quotation detection",
        "rewriting techniques",
        "copyright screening",
        "adaptive abstention"
      ]
    },
    "publishedAt": "2025-04-22T13:16:53.000Z",
    "title": "Certified Mitigation of Worst-Case LLM Copyright Infringement",
    "summary": "The exposure of large language models (LLMs) to copyrighted material during\npre-training raises concerns about unintentional copyright infringement post\ndeployment. This has driven the development of \"copyright takedown\" methods,\npost-training approaches aimed at preventing models from generating content\nsubstantially similar to copyrighted ones. While current mitigation approaches\nare somewhat effective for average-case risks, we demonstrate that they\noverlook worst-case copyright risks exhibits by the existence of long, verbatim\nquotes from copyrighted sources. We propose BloomScrub, a remarkably simple yet\nhighly effective inference-time approach that provides certified copyright\ntakedown. Our method repeatedly interleaves quote detection with rewriting\ntechniques to transform potentially infringing segments. By leveraging\nefficient data sketches (Bloom filters), our approach enables scalable\ncopyright screening even for large-scale real-world corpora. When quotes beyond\na length threshold cannot be removed, the system can abstain from responding,\noffering certified risk reduction. Experimental results show that BloomScrub\nreduces infringement risk, preserves utility, and accommodates different levels\nof enforcement stringency with adaptive abstention. Our results suggest that\nlightweight, inference-time methods can be surprisingly effective for copyright\nprevention.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16046.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62fb40b59af1d16bc0ac60f4",
      "avatarUrl": "/avatars/03ff66a419db8f2bc8e89a3b47aaaeac.svg",
      "fullname": "Jack Zhang",
      "name": "jackzhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.20998",
      "authors": [
        {
          "_id": "6811899ba6198824c5589ed7",
          "name": "Thao Nguyen",
          "hidden": false
        },
        {
          "_id": "6811899ba6198824c5589ed8",
          "name": "Krishna Kumar Singh",
          "hidden": false
        },
        {
          "_id": "6811899ba6198824c5589ed9",
          "name": "Jing Shi",
          "hidden": false
        },
        {
          "_id": "6811899ba6198824c5589eda",
          "name": "Trung Bui",
          "hidden": false
        },
        {
          "_id": "6811899ba6198824c5589edb",
          "name": "Yong Jae Lee",
          "hidden": false
        },
        {
          "_id": "6811899ba6198824c5589edc",
          "name": "Yuheng Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-29T17:59:57.000Z",
      "submittedOnDailyAt": "2025-04-30T00:54:06.806Z",
      "title": "YoChameleon: Personalized Vision and Language Generation",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Large Multimodal Models (e.g., GPT-4, Gemini, Chameleon) have evolved into\npowerful tools with millions of users. However, they remain generic models and\nlack personalized knowledge of specific user concepts. Previous work has\nexplored personalization for text generation, yet it remains unclear how these\nmethods can be adapted to new modalities, such as image generation. In this\npaper, we introduce Yo'Chameleon, the first attempt to study personalization\nfor large multimodal models. Given 3-5 images of a particular concept,\nYo'Chameleon leverages soft-prompt tuning to embed subject-specific information\nto (i) answer questions about the subject and (ii) recreate pixel-level details\nto produce images of the subject in new contexts. Yo'Chameleon is trained with\n(i) a self-prompting optimization mechanism to balance performance across\nmultiple modalities, and (ii) a ``soft-positive\" image generation approach to\nenhance image quality in a few-shot setting.",
      "upvotes": 5,
      "discussionId": "6811899ca6198824c5589f45",
      "ai_keywords": [
        "soft-prompt tuning",
        "subject-specific information",
        "self-prompting optimization mechanism",
        "soft-positive image generation approach"
      ]
    },
    "publishedAt": "2025-04-29T13:59:57.000Z",
    "title": "YoChameleon: Personalized Vision and Language Generation",
    "summary": "Large Multimodal Models (e.g., GPT-4, Gemini, Chameleon) have evolved into\npowerful tools with millions of users. However, they remain generic models and\nlack personalized knowledge of specific user concepts. Previous work has\nexplored personalization for text generation, yet it remains unclear how these\nmethods can be adapted to new modalities, such as image generation. In this\npaper, we introduce Yo'Chameleon, the first attempt to study personalization\nfor large multimodal models. Given 3-5 images of a particular concept,\nYo'Chameleon leverages soft-prompt tuning to embed subject-specific information\nto (i) answer questions about the subject and (ii) recreate pixel-level details\nto produce images of the subject in new contexts. Yo'Chameleon is trained with\n(i) a self-prompting optimization mechanism to balance performance across\nmultiple modalities, and (ii) a ``soft-positive\" image generation approach to\nenhance image quality in a few-shot setting.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20998.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6748
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.20879",
      "authors": [
        {
          "_id": "6811ae6b7f4f553788e905b8",
          "name": "Shivalika Singh",
          "hidden": false
        },
        {
          "_id": "6811ae6b7f4f553788e905b9",
          "name": "Yiyang Nan",
          "hidden": false
        },
        {
          "_id": "6811ae6b7f4f553788e905ba",
          "name": "Alex Wang",
          "hidden": false
        },
        {
          "_id": "6811ae6b7f4f553788e905bb",
          "name": "Daniel D'Souza",
          "hidden": false
        },
        {
          "_id": "6811ae6b7f4f553788e905bc",
          "name": "Sayash Kapoor",
          "hidden": false
        },
        {
          "_id": "6811ae6b7f4f553788e905bd",
          "name": "Ahmet Üstün",
          "hidden": false
        },
        {
          "_id": "6811ae6b7f4f553788e905be",
          "name": "Sanmi Koyejo",
          "hidden": false
        },
        {
          "_id": "6811ae6b7f4f553788e905bf",
          "name": "Yuntian Deng",
          "hidden": false
        },
        {
          "_id": "6811ae6b7f4f553788e905c0",
          "name": "Shayne Longpre",
          "hidden": false
        },
        {
          "_id": "6811ae6b7f4f553788e905c1",
          "name": "Noah Smith",
          "hidden": false
        },
        {
          "_id": "6811ae6b7f4f553788e905c2",
          "name": "Beyza Ermis",
          "hidden": false
        },
        {
          "_id": "6811ae6b7f4f553788e905c3",
          "name": "Marzieh Fadaee",
          "hidden": false
        },
        {
          "_id": "6811ae6b7f4f553788e905c4",
          "name": "Sara Hooker",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-29T15:48:49.000Z",
      "submittedOnDailyAt": "2025-04-30T03:36:53.331Z",
      "title": "The Leaderboard Illusion",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Measuring progress is fundamental to the advancement of any scientific field.\nAs benchmarks play an increasingly central role, they also grow more\nsusceptible to distortion. Chatbot Arena has emerged as the go-to leaderboard\nfor ranking the most capable AI systems. Yet, in this work we identify\nsystematic issues that have resulted in a distorted playing field. We find that\nundisclosed private testing practices benefit a handful of providers who are\nable to test multiple variants before public release and retract scores if\ndesired. We establish that the ability of these providers to choose the best\nscore leads to biased Arena scores due to selective disclosure of performance\nresults. At an extreme, we identify 27 private LLM variants tested by Meta in\nthe lead-up to the Llama-4 release. We also establish that proprietary closed\nmodels are sampled at higher rates (number of battles) and have fewer models\nremoved from the arena than open-weight and open-source alternatives. Both\nthese policies lead to large data access asymmetries over time. Providers like\nGoogle and OpenAI have received an estimated 19.2% and 20.4% of all data on the\narena, respectively. In contrast, a combined 83 open-weight models have only\nreceived an estimated 29.7% of the total data. We show that access to Chatbot\nArena data yields substantial benefits; even limited additional data can result\nin relative performance gains of up to 112% on the arena distribution, based on\nour conservative estimates. Together, these dynamics result in overfitting to\nArena-specific dynamics rather than general model quality. The Arena builds on\nthe substantial efforts of both the organizers and an open community that\nmaintains this valuable evaluation platform. We offer actionable\nrecommendations to reform the Chatbot Arena's evaluation framework and promote\nfairer, more transparent benchmarking for the field",
      "upvotes": 3,
      "discussionId": "6811ae6c7f4f553788e905fc"
    },
    "publishedAt": "2025-04-29T11:48:49.000Z",
    "title": "The Leaderboard Illusion",
    "summary": "Measuring progress is fundamental to the advancement of any scientific field.\nAs benchmarks play an increasingly central role, they also grow more\nsusceptible to distortion. Chatbot Arena has emerged as the go-to leaderboard\nfor ranking the most capable AI systems. Yet, in this work we identify\nsystematic issues that have resulted in a distorted playing field. We find that\nundisclosed private testing practices benefit a handful of providers who are\nable to test multiple variants before public release and retract scores if\ndesired. We establish that the ability of these providers to choose the best\nscore leads to biased Arena scores due to selective disclosure of performance\nresults. At an extreme, we identify 27 private LLM variants tested by Meta in\nthe lead-up to the Llama-4 release. We also establish that proprietary closed\nmodels are sampled at higher rates (number of battles) and have fewer models\nremoved from the arena than open-weight and open-source alternatives. Both\nthese policies lead to large data access asymmetries over time. Providers like\nGoogle and OpenAI have received an estimated 19.2% and 20.4% of all data on the\narena, respectively. In contrast, a combined 83 open-weight models have only\nreceived an estimated 29.7% of the total data. We show that access to Chatbot\nArena data yields substantial benefits; even limited additional data can result\nin relative performance gains of up to 112% on the arena distribution, based on\nour conservative estimates. Together, these dynamics result in overfitting to\nArena-specific dynamics rather than general model quality. The Arena builds on\nthe substantial efforts of both the organizers and an open community that\nmaintains this valuable evaluation platform. We offer actionable\nrecommendations to reform the Chatbot Arena's evaluation framework and promote\nfairer, more transparent benchmarking for the field",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20879.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 77
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.20996",
      "authors": [
        {
          "_id": "6811c55384adfa26b82abd76",
          "name": "Sicheng Mo",
          "hidden": false
        },
        {
          "_id": "6811c55384adfa26b82abd77",
          "name": "Thao Nguyen",
          "hidden": false
        },
        {
          "_id": "6811c55384adfa26b82abd78",
          "name": "Xun Huang",
          "hidden": false
        },
        {
          "_id": "6811c55384adfa26b82abd79",
          "name": "Siddharth Srinivasan Iyer",
          "hidden": false
        },
        {
          "_id": "6811c55384adfa26b82abd7a",
          "name": "Yijun Li",
          "hidden": false
        },
        {
          "_id": "6811c55384adfa26b82abd7b",
          "name": "Yuchen Liu",
          "hidden": false
        },
        {
          "_id": "6811c55384adfa26b82abd7c",
          "name": "Abhishek Tandon",
          "hidden": false
        },
        {
          "_id": "6811c55384adfa26b82abd7d",
          "name": "Eli Shechtman",
          "hidden": false
        },
        {
          "_id": "6811c55384adfa26b82abd7e",
          "name": "Krishna Kumar Singh",
          "hidden": false
        },
        {
          "_id": "6811c55384adfa26b82abd7f",
          "name": "Yong Jae Lee",
          "hidden": false
        },
        {
          "_id": "6811c55384adfa26b82abd80",
          "name": "Bolei Zhou",
          "hidden": false
        },
        {
          "_id": "6811c55384adfa26b82abd81",
          "name": "Yuheng Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-29T17:59:45.000Z",
      "submittedOnDailyAt": "2025-04-30T05:08:54.031Z",
      "title": "X-Fusion: Introducing New Modality to Frozen Large Language Models",
      "submittedOnDailyBy": {
        "_id": "637c94d3f219c71f93eda9ad",
        "avatarUrl": "/avatars/6dae0c30755196ccc0a5a06b3981c47f.svg",
        "isPro": false,
        "fullname": "Sicheng Mo",
        "user": "Sichengmo",
        "type": "user"
      },
      "summary": "We propose X-Fusion, a framework that extends pretrained Large Language\nModels (LLMs) for multimodal tasks while preserving their language\ncapabilities. X-Fusion employs a dual-tower design with modality-specific\nweights, keeping the LLM's parameters frozen while integrating vision-specific\ninformation for both understanding and generation. Our experiments demonstrate\nthat X-Fusion consistently outperforms alternative architectures on both\nimage-to-text and text-to-image tasks. We find that incorporating\nunderstanding-focused data improves generation quality, reducing image data\nnoise enhances overall performance, and feature alignment accelerates\nconvergence for smaller models but has minimal impact on larger ones. Our\nfindings provide valuable insights into building efficient unified multimodal\nmodels.",
      "upvotes": 1,
      "discussionId": "6811c55584adfa26b82abdfe",
      "projectPage": "https://sichengmo.github.io/XFusion/",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "multimodal tasks",
        "dual-tower design",
        "modality-specific weights",
        "vision-specific information",
        "image-to-text",
        "text-to-image",
        "understanding-focused data",
        "feature alignment",
        "unified multimodal models"
      ]
    },
    "publishedAt": "2025-04-29T13:59:45.000Z",
    "title": "X-Fusion: Introducing New Modality to Frozen Large Language Models",
    "summary": "We propose X-Fusion, a framework that extends pretrained Large Language\nModels (LLMs) for multimodal tasks while preserving their language\ncapabilities. X-Fusion employs a dual-tower design with modality-specific\nweights, keeping the LLM's parameters frozen while integrating vision-specific\ninformation for both understanding and generation. Our experiments demonstrate\nthat X-Fusion consistently outperforms alternative architectures on both\nimage-to-text and text-to-image tasks. We find that incorporating\nunderstanding-focused data improves generation quality, reducing image data\nnoise enhances overall performance, and feature alignment accelerates\nconvergence for smaller models but has minimal impact on larger ones. Our\nfindings provide valuable insights into building efficient unified multimodal\nmodels.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20996.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "637c94d3f219c71f93eda9ad",
      "avatarUrl": "/avatars/6dae0c30755196ccc0a5a06b3981c47f.svg",
      "fullname": "Sicheng Mo",
      "name": "Sichengmo",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  }
]