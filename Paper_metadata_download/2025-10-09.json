[
  {
    "paper": {
      "id": "2510.06917",
      "authors": [
        {
          "_id": "68e70dc97ae125f9582e68c8",
          "name": "Cheng-Han Chiang",
          "hidden": false
        },
        {
          "_id": "68e70dc97ae125f9582e68c9",
          "name": "Xiaofei Wang",
          "hidden": false
        },
        {
          "_id": "68e70dc97ae125f9582e68ca",
          "name": "Linjie Li",
          "hidden": false
        },
        {
          "_id": "68e70dc97ae125f9582e68cb",
          "name": "Chung-Ching Lin",
          "hidden": false
        },
        {
          "_id": "68e70dc97ae125f9582e68cc",
          "name": "Kevin Lin",
          "hidden": false
        },
        {
          "_id": "68e70dc97ae125f9582e68cd",
          "name": "Shujie Liu",
          "hidden": false
        },
        {
          "_id": "68e70dc97ae125f9582e68ce",
          "name": "Zhendong Wang",
          "hidden": false
        },
        {
          "_id": "68e70dc97ae125f9582e68cf",
          "name": "Zhengyuan Yang",
          "hidden": false
        },
        {
          "_id": "68e70dc97ae125f9582e68d0",
          "name": "Hung-yi Lee",
          "hidden": false
        },
        {
          "_id": "68e70dc97ae125f9582e68d1",
          "name": "Lijuan Wang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/622326ae0129f2097d69a3e2/N4e_VG2KksHuxbjoCTmWW.mp4"
      ],
      "publishedAt": "2025-10-08T11:48:59.000Z",
      "submittedOnDailyAt": "2025-10-09T00:11:57.362Z",
      "title": "SHANKS: Simultaneous Hearing and Thinking for Spoken Language Models",
      "submittedOnDailyBy": {
        "_id": "622326ae0129f2097d69a3e2",
        "avatarUrl": "/avatars/7665223e3fc8b820ce001e6003daf4d2.svg",
        "isPro": false,
        "fullname": "Cheng-Han Chiang",
        "user": "dcml0714",
        "type": "user"
      },
      "summary": "Current large language models (LLMs) and spoken language models (SLMs) begin\nthinking and taking actions only after the user has finished their turn. This\nprevents the model from interacting during the user's turn and can lead to high\nresponse latency while it waits to think. Consequently, thinking after\nreceiving the full input is not suitable for speech-to-speech interaction,\nwhere real-time, low-latency exchange is important. We address this by noting\nthat humans naturally \"think while listening.\" In this paper, we propose\nSHANKS, a general inference framework that enables SLMs to generate unspoken\nchain-of-thought reasoning while listening to the user input. SHANKS streams\nthe input speech in fixed-duration chunks and, as soon as a chunk is received,\ngenerates unspoken reasoning based on all previous speech and reasoning, while\nthe user continues speaking. SHANKS uses this unspoken reasoning to decide\nwhether to interrupt the user and to make tool calls to complete the task. We\ndemonstrate that SHANKS enhances real-time user-SLM interaction in two\nscenarios: (1) when the user is presenting a step-by-step solution to a math\nproblem, SHANKS can listen, reason, and interrupt when the user makes a\nmistake, achieving 37.1% higher interruption accuracy than a baseline that\ninterrupts without thinking; and (2) in a tool-augmented dialogue, SHANKS can\ncomplete 56.9% of the tool calls before the user finishes their turn. Overall,\nSHANKS moves toward models that keep thinking throughout the conversation, not\nonly after a turn ends. Animated illustrations of Shanks can be found at\nhttps://d223302.github.io/SHANKS/",
      "upvotes": 28,
      "discussionId": "68e70dca7ae125f9582e68d2",
      "projectPage": "https://d223302.github.io/SHANKS/",
      "ai_summary": "SHANKS, a general inference framework, enables spoken language models to generate unspoken reasoning while listening to user input, enhancing real-time interaction and task completion.",
      "ai_keywords": [
        "SHANKS",
        "spoken language models",
        "unspoken chain-of-thought reasoning",
        "real-time interaction",
        "tool calls",
        "interruption accuracy"
      ]
    },
    "publishedAt": "2025-10-08T07:48:59.000Z",
    "title": "SHANKS: Simultaneous Hearing and Thinking for Spoken Language Models",
    "summary": "Current large language models (LLMs) and spoken language models (SLMs) begin\nthinking and taking actions only after the user has finished their turn. This\nprevents the model from interacting during the user's turn and can lead to high\nresponse latency while it waits to think. Consequently, thinking after\nreceiving the full input is not suitable for speech-to-speech interaction,\nwhere real-time, low-latency exchange is important. We address this by noting\nthat humans naturally \"think while listening.\" In this paper, we propose\nSHANKS, a general inference framework that enables SLMs to generate unspoken\nchain-of-thought reasoning while listening to the user input. SHANKS streams\nthe input speech in fixed-duration chunks and, as soon as a chunk is received,\ngenerates unspoken reasoning based on all previous speech and reasoning, while\nthe user continues speaking. SHANKS uses this unspoken reasoning to decide\nwhether to interrupt the user and to make tool calls to complete the task. We\ndemonstrate that SHANKS enhances real-time user-SLM interaction in two\nscenarios: (1) when the user is presenting a step-by-step solution to a math\nproblem, SHANKS can listen, reason, and interrupt when the user makes a\nmistake, achieving 37.1% higher interruption accuracy than a baseline that\ninterrupts without thinking; and (2) in a tool-augmented dialogue, SHANKS can\ncomplete 56.9% of the tool calls before the user finishes their turn. Overall,\nSHANKS moves toward models that keep thinking throughout the conversation, not\nonly after a turn ends. Animated illustrations of Shanks can be found at\nhttps://d223302.github.io/SHANKS/",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/622326ae0129f2097d69a3e2/N4e_VG2KksHuxbjoCTmWW.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.06917.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "622326ae0129f2097d69a3e2",
      "avatarUrl": "/avatars/7665223e3fc8b820ce001e6003daf4d2.svg",
      "fullname": "Cheng-Han Chiang",
      "name": "dcml0714",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.06308",
      "authors": [
        {
          "_id": "68e70bdd7ae125f9582e6883",
          "name": "Yi Xin",
          "hidden": false
        },
        {
          "_id": "68e70bdd7ae125f9582e6884",
          "user": {
            "_id": "66bb136002fd8eb58bc84ffb",
            "avatarUrl": "/avatars/122cb8f59c502392768099b3c2afe043.svg",
            "isPro": false,
            "fullname": "qinqi",
            "user": "Dakerqi",
            "type": "user"
          },
          "name": "Qi Qin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-09T06:50:46.173Z",
          "hidden": false
        },
        {
          "_id": "68e70bdd7ae125f9582e6885",
          "name": "Siqi Luo",
          "hidden": false
        },
        {
          "_id": "68e70bdd7ae125f9582e6886",
          "name": "Kaiwen Zhu",
          "hidden": false
        },
        {
          "_id": "68e70bdd7ae125f9582e6887",
          "name": "Juncheng Yan",
          "hidden": false
        },
        {
          "_id": "68e70bdd7ae125f9582e6888",
          "name": "Yan Tai",
          "hidden": false
        },
        {
          "_id": "68e70bdd7ae125f9582e6889",
          "user": {
            "_id": "64c3c72e8f31d1e6c664b052",
            "avatarUrl": "/avatars/af1ad5048eaa9dc417837ad02f927911.svg",
            "isPro": false,
            "fullname": "jiayi lei",
            "user": "jyjyjyjy",
            "type": "user"
          },
          "name": "Jiayi Lei",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-09T06:50:43.828Z",
          "hidden": false
        },
        {
          "_id": "68e70bdd7ae125f9582e688a",
          "name": "Yuewen Cao",
          "hidden": false
        },
        {
          "_id": "68e70bdd7ae125f9582e688b",
          "name": "Keqi Wang",
          "hidden": false
        },
        {
          "_id": "68e70bdd7ae125f9582e688c",
          "name": "Yibin Wang",
          "hidden": false
        },
        {
          "_id": "68e70bdd7ae125f9582e688d",
          "name": "Jinbin Bai",
          "hidden": false
        },
        {
          "_id": "68e70bdd7ae125f9582e688e",
          "name": "Qian Yu",
          "hidden": false
        },
        {
          "_id": "68e70bdd7ae125f9582e688f",
          "name": "Dengyang Jiang",
          "hidden": false
        },
        {
          "_id": "68e70bdd7ae125f9582e6890",
          "name": "Yuandong Pu",
          "hidden": false
        },
        {
          "_id": "68e70bdd7ae125f9582e6891",
          "name": "Haoxing Chen",
          "hidden": false
        },
        {
          "_id": "68e70bdd7ae125f9582e6892",
          "name": "Le Zhuo",
          "hidden": false
        },
        {
          "_id": "68e70bdd7ae125f9582e6893",
          "name": "Junjun He",
          "hidden": false
        },
        {
          "_id": "68e70bdd7ae125f9582e6894",
          "name": "Gen Luo",
          "hidden": false
        },
        {
          "_id": "68e70bdd7ae125f9582e6895",
          "name": "Tianbin Li",
          "hidden": false
        },
        {
          "_id": "68e70bdd7ae125f9582e6896",
          "name": "Ming Hu",
          "hidden": false
        },
        {
          "_id": "68e70bdd7ae125f9582e6897",
          "name": "Jin Ye",
          "hidden": false
        },
        {
          "_id": "68e70bdd7ae125f9582e6898",
          "name": "Shenglong Ye",
          "hidden": false
        },
        {
          "_id": "68e70bdd7ae125f9582e6899",
          "name": "Bo Zhang",
          "hidden": false
        },
        {
          "_id": "68e70bdd7ae125f9582e689a",
          "name": "Chang Xu",
          "hidden": false
        },
        {
          "_id": "68e70bdd7ae125f9582e689b",
          "name": "Wenhai Wang",
          "hidden": false
        },
        {
          "_id": "68e70bdd7ae125f9582e689c",
          "name": "Hongsheng Li",
          "hidden": false
        },
        {
          "_id": "68e70bdd7ae125f9582e689d",
          "name": "Guangtao Zhai",
          "hidden": false
        },
        {
          "_id": "68e70bdd7ae125f9582e689e",
          "name": "Tianfan Xue",
          "hidden": false
        },
        {
          "_id": "68e70bdd7ae125f9582e689f",
          "name": "Bin Fu",
          "hidden": false
        },
        {
          "_id": "68e70bdd7ae125f9582e68a0",
          "name": "Xiaohong Liu",
          "hidden": false
        },
        {
          "_id": "68e70bdd7ae125f9582e68a1",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "68e70bdd7ae125f9582e68a2",
          "name": "Yihao Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-07T17:59:20.000Z",
      "submittedOnDailyAt": "2025-10-09T01:00:29.190Z",
      "title": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal\n  Generation and Understanding",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We introduce Lumina-DiMOO, an open-source foundational model for seamless\nmulti-modal generation and understanding. Lumina-DiMOO sets itself apart from\nprior unified models by utilizing a fully discrete diffusion modeling to handle\ninputs and outputs across various modalities. This innovative approach allows\nLumina-DiMOO to achieve higher sampling efficiency compared to previous\nautoregressive (AR) or hybrid AR-Diffusion paradigms and adeptly support a\nbroad spectrum of multi-modal tasks, including text-to-image generation,\nimage-to-image generation (e.g., image editing, subject-driven generation, and\nimage inpainting, etc.), as well as image understanding. Lumina-DiMOO achieves\nstate-of-the-art performance on multiple benchmarks, surpassing existing\nopen-source unified multi-modal models. To foster further advancements in\nmulti-modal and discrete diffusion model research, we release our code and\ncheckpoints to the community. Project Page:\nhttps://synbol.github.io/Lumina-DiMOO.",
      "upvotes": 27,
      "discussionId": "68e70bde7ae125f9582e68a3",
      "projectPage": "https://synbol.github.io/Lumina-DiMOO/",
      "githubRepo": "https://github.com/Alpha-VLLM/Lumina-DiMOO",
      "ai_summary": "Lumina-DiMOO, an open-source foundational model, uses fully discrete diffusion modeling for efficient multi-modal generation and understanding, outperforming existing models.",
      "ai_keywords": [
        "discrete diffusion modeling",
        "autoregressive",
        "hybrid AR-Diffusion",
        "text-to-image generation",
        "image-to-image generation",
        "image editing",
        "subject-driven generation",
        "image inpainting",
        "image understanding"
      ],
      "githubStars": 720,
      "organization": {
        "_id": "64bd3427b567ae97c332277f",
        "name": "Alpha-VLLM",
        "fullname": "Alpha-VLLM",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/646f1bef075e11ca78da3bb7/mD7gobrnznzDXnpZ9ZiT8.png"
      }
    },
    "publishedAt": "2025-10-07T13:59:20.000Z",
    "title": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal\n  Generation and Understanding",
    "summary": "We introduce Lumina-DiMOO, an open-source foundational model for seamless\nmulti-modal generation and understanding. Lumina-DiMOO sets itself apart from\nprior unified models by utilizing a fully discrete diffusion modeling to handle\ninputs and outputs across various modalities. This innovative approach allows\nLumina-DiMOO to achieve higher sampling efficiency compared to previous\nautoregressive (AR) or hybrid AR-Diffusion paradigms and adeptly support a\nbroad spectrum of multi-modal tasks, including text-to-image generation,\nimage-to-image generation (e.g., image editing, subject-driven generation, and\nimage inpainting, etc.), as well as image understanding. Lumina-DiMOO achieves\nstate-of-the-art performance on multiple benchmarks, surpassing existing\nopen-source unified multi-modal models. To foster further advancements in\nmulti-modal and discrete diffusion model research, we release our code and\ncheckpoints to the community. Project Page:\nhttps://synbol.github.io/Lumina-DiMOO.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.06308.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 122
    },
    "organization": {
      "_id": "64bd3427b567ae97c332277f",
      "name": "Alpha-VLLM",
      "fullname": "Alpha-VLLM",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/646f1bef075e11ca78da3bb7/mD7gobrnznzDXnpZ9ZiT8.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.03215",
      "authors": [
        {
          "_id": "68e7171b7ae125f9582e6952",
          "user": {
            "_id": "6445fd9ba56444c355dcbcba",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6445fd9ba56444c355dcbcba/NCyRCD-MK-yA0_qY6I2y0.png",
            "isPro": false,
            "fullname": "Tianyu Fu",
            "user": "fuvty",
            "type": "user"
          },
          "name": "Tianyu Fu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-09T06:50:21.158Z",
          "hidden": false
        },
        {
          "_id": "68e7171b7ae125f9582e6953",
          "name": "Zihan Min",
          "hidden": false
        },
        {
          "_id": "68e7171b7ae125f9582e6954",
          "name": "Hanling Zhang",
          "hidden": false
        },
        {
          "_id": "68e7171b7ae125f9582e6955",
          "name": "Jichao Yan",
          "hidden": false
        },
        {
          "_id": "68e7171b7ae125f9582e6956",
          "name": "Guohao Dai",
          "hidden": false
        },
        {
          "_id": "68e7171b7ae125f9582e6957",
          "name": "Wanli Ouyang",
          "hidden": false
        },
        {
          "_id": "68e7171b7ae125f9582e6958",
          "name": "Yu Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-03T17:52:32.000Z",
      "submittedOnDailyAt": "2025-10-09T00:33:46.090Z",
      "title": "Cache-to-Cache: Direct Semantic Communication Between Large Language\n  Models",
      "submittedOnDailyBy": {
        "_id": "6445fd9ba56444c355dcbcba",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6445fd9ba56444c355dcbcba/NCyRCD-MK-yA0_qY6I2y0.png",
        "isPro": false,
        "fullname": "Tianyu Fu",
        "user": "fuvty",
        "type": "user"
      },
      "summary": "Multi-LLM systems harness the complementary strengths of diverse Large\nLanguage Models, achieving performance and efficiency gains unattainable by a\nsingle model. In existing designs, LLMs communicate through text, forcing\ninternal representations to be transformed into output token sequences. This\nprocess both loses rich semantic information and incurs token-by-token\ngeneration latency. Motivated by these limitations, we ask: Can LLMs\ncommunicate beyond text? Oracle experiments show that enriching the KV-Cache\nsemantics can improve response quality without increasing cache size,\nsupporting KV-Cache as an effective medium for inter-model communication. Thus,\nwe propose Cache-to-Cache (C2C), a new paradigm for direct semantic\ncommunication between LLMs. C2C uses a neural network to project and fuse the\nsource model's KV-cache with that of the target model to enable direct semantic\ntransfer. A learnable gating mechanism selects the target layers that benefit\nfrom cache communication. Compared with text communication, C2C utilizes the\ndeep, specialized semantics from both models, while avoiding explicit\nintermediate text generation. Experiments show that C2C achieves 8.5-10.5%\nhigher average accuracy than individual models. It further outperforms the text\ncommunication paradigm by approximately 3.0-5.0%, while delivering an average\n2.0x speedup in latency. Our code is available at\nhttps://github.com/thu-nics/C2C.",
      "upvotes": 25,
      "discussionId": "68e7171b7ae125f9582e6959",
      "projectPage": "https://fuvty.github.io/C2C_Project_Page/",
      "githubRepo": "https://github.com/thu-nics/C2C",
      "ai_summary": "Cache-to-Cache (C2C) enables direct semantic communication between LLMs using neural network projections, improving accuracy and reducing latency compared to text-based communication.",
      "ai_keywords": [
        "Large Language Models",
        "KV-Cache",
        "Cache-to-Cache",
        "neural network",
        "semantic communication",
        "gating mechanism",
        "accuracy",
        "latency"
      ],
      "githubStars": 12,
      "organization": {
        "_id": "64b74b5fb727f8771ab887f9",
        "name": "nics-efc",
        "fullname": "Tsinghua-NICS-EFC",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/641031b1a78453b8d96b8420/vmgxct2WsyHcKT2x2NAYT.jpeg"
      }
    },
    "publishedAt": "2025-10-03T13:52:32.000Z",
    "title": "Cache-to-Cache: Direct Semantic Communication Between Large Language\n  Models",
    "summary": "Multi-LLM systems harness the complementary strengths of diverse Large\nLanguage Models, achieving performance and efficiency gains unattainable by a\nsingle model. In existing designs, LLMs communicate through text, forcing\ninternal representations to be transformed into output token sequences. This\nprocess both loses rich semantic information and incurs token-by-token\ngeneration latency. Motivated by these limitations, we ask: Can LLMs\ncommunicate beyond text? Oracle experiments show that enriching the KV-Cache\nsemantics can improve response quality without increasing cache size,\nsupporting KV-Cache as an effective medium for inter-model communication. Thus,\nwe propose Cache-to-Cache (C2C), a new paradigm for direct semantic\ncommunication between LLMs. C2C uses a neural network to project and fuse the\nsource model's KV-cache with that of the target model to enable direct semantic\ntransfer. A learnable gating mechanism selects the target layers that benefit\nfrom cache communication. Compared with text communication, C2C utilizes the\ndeep, specialized semantics from both models, while avoiding explicit\nintermediate text generation. Experiments show that C2C achieves 8.5-10.5%\nhigher average accuracy than individual models. It further outperforms the text\ncommunication paradigm by approximately 3.0-5.0%, while delivering an average\n2.0x speedup in latency. Our code is available at\nhttps://github.com/thu-nics/C2C.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.03215.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6445fd9ba56444c355dcbcba",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6445fd9ba56444c355dcbcba/NCyRCD-MK-yA0_qY6I2y0.png",
      "fullname": "Tianyu Fu",
      "name": "fuvty",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "organization": {
      "_id": "64b74b5fb727f8771ab887f9",
      "name": "nics-efc",
      "fullname": "Tsinghua-NICS-EFC",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/641031b1a78453b8d96b8420/vmgxct2WsyHcKT2x2NAYT.jpeg"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.07315",
      "authors": [
        {
          "_id": "68e714517ae125f9582e6903",
          "user": {
            "_id": "61d53df2062444ea769d3b79",
            "avatarUrl": "/avatars/fa771202368b6b2626a8fdf1c4369239.svg",
            "isPro": false,
            "fullname": "Ming Zhong",
            "user": "MingZhong",
            "type": "user"
          },
          "name": "Ming Zhong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-09T06:50:34.588Z",
          "hidden": false
        },
        {
          "_id": "68e714517ae125f9582e6904",
          "name": "Xiang Zhou",
          "hidden": false
        },
        {
          "_id": "68e714517ae125f9582e6905",
          "name": "Ting-Yun Chang",
          "hidden": false
        },
        {
          "_id": "68e714517ae125f9582e6906",
          "name": "Qingze Wang",
          "hidden": false
        },
        {
          "_id": "68e714517ae125f9582e6907",
          "name": "Nan Xu",
          "hidden": false
        },
        {
          "_id": "68e714517ae125f9582e6908",
          "name": "Xiance Si",
          "hidden": false
        },
        {
          "_id": "68e714517ae125f9582e6909",
          "name": "Dan Garrette",
          "hidden": false
        },
        {
          "_id": "68e714517ae125f9582e690a",
          "name": "Shyam Upadhyay",
          "hidden": false
        },
        {
          "_id": "68e714517ae125f9582e690b",
          "name": "Jeremiah Liu",
          "hidden": false
        },
        {
          "_id": "68e714517ae125f9582e690c",
          "name": "Jiawei Han",
          "hidden": false
        },
        {
          "_id": "68e714517ae125f9582e690d",
          "name": "Benoit Schillings",
          "hidden": false
        },
        {
          "_id": "68e714517ae125f9582e690e",
          "name": "Jiao Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-08T17:59:19.000Z",
      "submittedOnDailyAt": "2025-10-09T00:25:56.326Z",
      "title": "Vibe Checker: Aligning Code Evaluation with Human Preference",
      "submittedOnDailyBy": {
        "_id": "61d53df2062444ea769d3b79",
        "avatarUrl": "/avatars/fa771202368b6b2626a8fdf1c4369239.svg",
        "isPro": false,
        "fullname": "Ming Zhong",
        "user": "MingZhong",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) have catalyzed vibe coding, where users leverage\nLLMs to generate and iteratively refine code through natural language\ninteractions until it passes their vibe check. Vibe check is tied to real-world\nhuman preference and goes beyond functionality: the solution should feel right,\nread cleanly, preserve intent, and remain correct. However, current code\nevaluation remains anchored to pass@k and captures only functional correctness,\noverlooking the non-functional instructions that users routinely apply. In this\npaper, we hypothesize that instruction following is the missing piece\nunderlying vibe check that represents human preference in coding besides\nfunctional correctness. To quantify models' code instruction following\ncapabilities with measurable signals, we present VeriCode, a taxonomy of 30\nverifiable code instructions together with corresponding deterministic\nverifiers. We use the taxonomy to augment established evaluation suites,\nresulting in Vibe Checker, a testbed to assess both code instruction following\nand functional correctness. Upon evaluating 31 leading LLMs, we show that even\nthe strongest models struggle to comply with multiple instructions and exhibit\nclear functional regression. Most importantly, a composite score of functional\ncorrectness and instruction following correlates the best with human\npreference, with the latter emerging as the primary differentiator on\nreal-world programming tasks. Our work identifies core factors of the vibe\ncheck, providing a concrete path for benchmarking and developing models that\nbetter align with user preferences in coding.",
      "upvotes": 23,
      "discussionId": "68e714517ae125f9582e690f",
      "ai_summary": "Vibe Checker evaluates LLMs by combining functional correctness and instruction following to better align with human coding preferences.",
      "ai_keywords": [
        "Large Language Models",
        "LLMs",
        "vibe coding",
        "Vibe Checker",
        "VeriCode",
        "verifiable code instructions",
        "deterministic verifiers",
        "instruction following",
        "functional correctness",
        "human preference",
        "coding preferences"
      ],
      "organization": {
        "_id": "60f6cbb2852126bac698c89e",
        "name": "deepmind",
        "fullname": "Deepmind",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1638956859875-5f1158120c833276f61f1a84.jpeg"
      }
    },
    "publishedAt": "2025-10-08T13:59:19.000Z",
    "title": "Vibe Checker: Aligning Code Evaluation with Human Preference",
    "summary": "Large Language Models (LLMs) have catalyzed vibe coding, where users leverage\nLLMs to generate and iteratively refine code through natural language\ninteractions until it passes their vibe check. Vibe check is tied to real-world\nhuman preference and goes beyond functionality: the solution should feel right,\nread cleanly, preserve intent, and remain correct. However, current code\nevaluation remains anchored to pass@k and captures only functional correctness,\noverlooking the non-functional instructions that users routinely apply. In this\npaper, we hypothesize that instruction following is the missing piece\nunderlying vibe check that represents human preference in coding besides\nfunctional correctness. To quantify models' code instruction following\ncapabilities with measurable signals, we present VeriCode, a taxonomy of 30\nverifiable code instructions together with corresponding deterministic\nverifiers. We use the taxonomy to augment established evaluation suites,\nresulting in Vibe Checker, a testbed to assess both code instruction following\nand functional correctness. Upon evaluating 31 leading LLMs, we show that even\nthe strongest models struggle to comply with multiple instructions and exhibit\nclear functional regression. Most importantly, a composite score of functional\ncorrectness and instruction following correlates the best with human\npreference, with the latter emerging as the primary differentiator on\nreal-world programming tasks. Our work identifies core factors of the vibe\ncheck, providing a concrete path for benchmarking and developing models that\nbetter align with user preferences in coding.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.07315.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61d53df2062444ea769d3b79",
      "avatarUrl": "/avatars/fa771202368b6b2626a8fdf1c4369239.svg",
      "fullname": "Ming Zhong",
      "name": "MingZhong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "organization": {
      "_id": "60f6cbb2852126bac698c89e",
      "name": "deepmind",
      "fullname": "Deepmind",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1638956859875-5f1158120c833276f61f1a84.jpeg"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.07310",
      "authors": [
        {
          "_id": "68e71a787ae125f9582e6969",
          "name": "Siyoon Jin",
          "hidden": false
        },
        {
          "_id": "68e71a787ae125f9582e696a",
          "user": {
            "_id": "637c49ec9c470afa3880b137",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/pdcMPz8N6vQM1tc8IA1lV.png",
            "isPro": false,
            "fullname": "Seongchan Kim",
            "user": "Seongchan",
            "type": "user"
          },
          "name": "Seongchan Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-09T06:50:18.453Z",
          "hidden": false
        },
        {
          "_id": "68e71a787ae125f9582e696b",
          "name": "Dahyun Chung",
          "hidden": false
        },
        {
          "_id": "68e71a787ae125f9582e696c",
          "name": "Jaeho Lee",
          "hidden": false
        },
        {
          "_id": "68e71a787ae125f9582e696d",
          "name": "Hyunwook Choi",
          "hidden": false
        },
        {
          "_id": "68e71a787ae125f9582e696e",
          "name": "Jisu Nam",
          "hidden": false
        },
        {
          "_id": "68e71a787ae125f9582e696f",
          "name": "Jiyoung Kim",
          "hidden": false
        },
        {
          "_id": "68e71a787ae125f9582e6970",
          "name": "Seungryong Kim",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-08T17:57:38.000Z",
      "submittedOnDailyAt": "2025-10-09T00:44:33.311Z",
      "title": "MATRIX: Mask Track Alignment for Interaction-aware Video Generation",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Video DiTs have advanced video generation, yet they still struggle to model\nmulti-instance or subject-object interactions. This raises a key question: How\ndo these models internally represent interactions? To answer this, we curate\nMATRIX-11K, a video dataset with interaction-aware captions and multi-instance\nmask tracks. Using this dataset, we conduct a systematic analysis that\nformalizes two perspectives of video DiTs: semantic grounding, via\nvideo-to-text attention, which evaluates whether noun and verb tokens capture\ninstances and their relations; and semantic propagation, via video-to-video\nattention, which assesses whether instance bindings persist across frames. We\nfind both effects concentrate in a small subset of interaction-dominant layers.\nMotivated by this, we introduce MATRIX, a simple and effective regularization\nthat aligns attention in specific layers of video DiTs with multi-instance mask\ntracks from the MATRIX-11K dataset, enhancing both grounding and propagation.\nWe further propose InterGenEval, an evaluation protocol for interaction-aware\nvideo generation. In experiments, MATRIX improves both interaction fidelity and\nsemantic alignment while reducing drift and hallucination. Extensive ablations\nvalidate our design choices. Codes and weights will be released.",
      "upvotes": 22,
      "discussionId": "68e71a787ae125f9582e6971",
      "projectPage": "https://cvlab-kaist.github.io/MATRIX/",
      "githubRepo": "https://github.com/cvlab-kaist/MATRIX",
      "ai_summary": "MATRIX-11K dataset and MATRIX regularization enhance interaction fidelity and semantic alignment in video DiTs by aligning attention with multi-instance mask tracks.",
      "ai_keywords": [
        "video DiTs",
        "interaction-aware captions",
        "multi-instance mask tracks",
        "video-to-text attention",
        "video-to-video attention",
        "semantic grounding",
        "semantic propagation",
        "MATRIX regularization",
        "InterGenEval",
        "interaction fidelity",
        "semantic alignment",
        "drift",
        "hallucination"
      ],
      "githubStars": 16
    },
    "publishedAt": "2025-10-08T13:57:38.000Z",
    "title": "MATRIX: Mask Track Alignment for Interaction-aware Video Generation",
    "summary": "Video DiTs have advanced video generation, yet they still struggle to model\nmulti-instance or subject-object interactions. This raises a key question: How\ndo these models internally represent interactions? To answer this, we curate\nMATRIX-11K, a video dataset with interaction-aware captions and multi-instance\nmask tracks. Using this dataset, we conduct a systematic analysis that\nformalizes two perspectives of video DiTs: semantic grounding, via\nvideo-to-text attention, which evaluates whether noun and verb tokens capture\ninstances and their relations; and semantic propagation, via video-to-video\nattention, which assesses whether instance bindings persist across frames. We\nfind both effects concentrate in a small subset of interaction-dominant layers.\nMotivated by this, we introduce MATRIX, a simple and effective regularization\nthat aligns attention in specific layers of video DiTs with multi-instance mask\ntracks from the MATRIX-11K dataset, enhancing both grounding and propagation.\nWe further propose InterGenEval, an evaluation protocol for interaction-aware\nvideo generation. In experiments, MATRIX improves both interaction fidelity and\nsemantic alignment while reducing drift and hallucination. Extensive ablations\nvalidate our design choices. Codes and weights will be released.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.07310.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 122
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.04678",
      "authors": [
        {
          "_id": "68e62542975ac4c405ef21eb",
          "name": "Zhanfeng Mo",
          "hidden": false
        },
        {
          "_id": "68e62542975ac4c405ef21ec",
          "name": "Xingxuan Li",
          "hidden": false
        },
        {
          "_id": "68e62542975ac4c405ef21ed",
          "name": "Yuntao Chen",
          "hidden": false
        },
        {
          "_id": "68e62542975ac4c405ef21ee",
          "name": "Lidong Bing",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-06T10:44:04.000Z",
      "submittedOnDailyAt": "2025-10-09T00:20:37.134Z",
      "title": "Multi-Agent Tool-Integrated Policy Optimization",
      "submittedOnDailyBy": {
        "_id": "6362a77dd3be91534c2e9213",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6362a77dd3be91534c2e9213/3uUM3B1m2CFMukbkA0yDv.png",
        "isPro": true,
        "fullname": "Xingxuan Li",
        "user": "veggiebird",
        "type": "user"
      },
      "summary": "Large language models (LLMs) increasingly rely on multi-turn tool-integrated\nplanning for knowledge-intensive and complex reasoning tasks. Existing\nimplementations typically rely on a single agent, but they suffer from limited\ncontext length and noisy tool responses. A natural solution is to adopt a\nmulti-agent framework with planner- and worker-agents to manage context.\nHowever, no existing methods support effective reinforcement learning\npost-training of tool-integrated multi-agent frameworks. To address this gap,\nwe propose Multi-Agent Tool-Integrated Policy Optimization (MATPO), which\nenables distinct roles (planner and worker) to be trained within a single LLM\ninstance using role-specific prompts via reinforcement learning. MATPO is\nderived from a principled credit assignment mechanism across planner and worker\nrollouts. This design eliminates the need to deploy multiple LLMs, which would\nbe memory-intensive, while preserving the benefits of specialization.\nExperiments on GAIA-text, WebWalkerQA, and FRAMES show that MATPO consistently\noutperforms single-agent baselines by an average of 18.38% relative improvement\nin performance and exhibits greater robustness to noisy tool outputs. Our\nfindings highlight the effectiveness of unifying multiple agent roles within a\nsingle LLM and provide practical insights for stable and efficient multi-agent\nRL training.",
      "upvotes": 14,
      "discussionId": "68e62596975ac4c405ef21ef",
      "githubRepo": "https://github.com/mzf666/MATPO",
      "ai_summary": "MATPO, a reinforcement learning method, optimizes tool-integrated multi-agent roles within a single LLM, improving performance and robustness over single-agent systems.",
      "ai_keywords": [
        "multi-turn tool-integrated planning",
        "multi-agent framework",
        "planner-agents",
        "worker-agents",
        "reinforcement learning",
        "role-specific prompts",
        "credit assignment mechanism",
        "planner and worker rollouts",
        "multi-agent RL training"
      ],
      "githubStars": 11
    },
    "publishedAt": "2025-10-06T06:44:04.000Z",
    "title": "Multi-Agent Tool-Integrated Policy Optimization",
    "summary": "Large language models (LLMs) increasingly rely on multi-turn tool-integrated\nplanning for knowledge-intensive and complex reasoning tasks. Existing\nimplementations typically rely on a single agent, but they suffer from limited\ncontext length and noisy tool responses. A natural solution is to adopt a\nmulti-agent framework with planner- and worker-agents to manage context.\nHowever, no existing methods support effective reinforcement learning\npost-training of tool-integrated multi-agent frameworks. To address this gap,\nwe propose Multi-Agent Tool-Integrated Policy Optimization (MATPO), which\nenables distinct roles (planner and worker) to be trained within a single LLM\ninstance using role-specific prompts via reinforcement learning. MATPO is\nderived from a principled credit assignment mechanism across planner and worker\nrollouts. This design eliminates the need to deploy multiple LLMs, which would\nbe memory-intensive, while preserving the benefits of specialization.\nExperiments on GAIA-text, WebWalkerQA, and FRAMES show that MATPO consistently\noutperforms single-agent baselines by an average of 18.38% relative improvement\nin performance and exhibits greater robustness to noisy tool outputs. Our\nfindings highlight the effectiveness of unifying multiple agent roles within a\nsingle LLM and provide practical insights for stable and efficient multi-agent\nRL training.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.04678.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6362a77dd3be91534c2e9213",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6362a77dd3be91534c2e9213/3uUM3B1m2CFMukbkA0yDv.png",
      "fullname": "Xingxuan Li",
      "name": "veggiebird",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.05862",
      "authors": [
        {
          "_id": "68e64fcf975ac4c405ef2263",
          "name": "Zecheng Tang",
          "hidden": false
        },
        {
          "_id": "68e64fcf975ac4c405ef2264",
          "name": "Baibei Ji",
          "hidden": false
        },
        {
          "_id": "68e64fcf975ac4c405ef2265",
          "name": "Juntao Li",
          "hidden": false
        },
        {
          "_id": "68e64fcf975ac4c405ef2266",
          "name": "Lijun Wu",
          "hidden": false
        },
        {
          "_id": "68e64fcf975ac4c405ef2267",
          "name": "Haijia Gui",
          "hidden": false
        },
        {
          "_id": "68e64fcf975ac4c405ef2268",
          "name": "Min Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-07T12:32:23.000Z",
      "submittedOnDailyAt": "2025-10-09T00:40:43.016Z",
      "title": "Revisiting Long-context Modeling from Context Denoising Perspective",
      "submittedOnDailyBy": {
        "_id": "64096ef79e9f790c905b846d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64096ef79e9f790c905b846d/hVzw656lXdzbCxTtnheud.jpeg",
        "isPro": false,
        "fullname": "Zecheng Tang",
        "user": "ZetangForward",
        "type": "user"
      },
      "summary": "Long-context models (LCMs) have demonstrated great potential in processing\nlong sequences, facilitating many real-world applications. The success of LCMs\ncan be attributed to their ability to locate implicit critical information\nwithin the context for further prediction. However, recent research reveals\nthat LCMs are often susceptible to contextual noise, i.e., irrelevant tokens,\nthat can mislead model attention. In this paper, we conduct a fine-grained\nanalysis of the context noise and propose an effective metric, the Integrated\nGradient (IG) score, to detect and quantify the noise information within the\ncontext. Our findings reveal that even simple mitigation of detected context\nnoise can substantially boost the model's attention on critical tokens and\nbenefit subsequent predictions. Building on this insight, we propose Context\nDenoising Training (CDT), a straightforward yet effective training strategy\nthat improves attention on critical tokens while reinforcing their influence on\nmodel predictions. Extensive experiments across four tasks, under both context\nwindow scaling and long-context alignment settings, demonstrate the superiority\nof CDT. Notably, when trained with CDT, an open-source 8B model can achieve\nperformance (50.92) comparable to GPT-4o (51.00).",
      "upvotes": 12,
      "discussionId": "68e64fcf975ac4c405ef2269",
      "ai_summary": "Context Denoising Training (CDT) improves long-context models' performance by mitigating contextual noise and enhancing attention on critical tokens.",
      "ai_keywords": [
        "long-context models",
        "contextual noise",
        "Integrated Gradient (IG) score",
        "Context Denoising Training (CDT)",
        "context window scaling",
        "long-context alignment"
      ],
      "organization": {
        "_id": "61f8e653129c9ff1b911293d",
        "name": "SUDA",
        "fullname": "Soochow University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1643701821817-61f8e5934a8e5a275b2b3e5a.png"
      }
    },
    "publishedAt": "2025-10-07T08:32:23.000Z",
    "title": "Revisiting Long-context Modeling from Context Denoising Perspective",
    "summary": "Long-context models (LCMs) have demonstrated great potential in processing\nlong sequences, facilitating many real-world applications. The success of LCMs\ncan be attributed to their ability to locate implicit critical information\nwithin the context for further prediction. However, recent research reveals\nthat LCMs are often susceptible to contextual noise, i.e., irrelevant tokens,\nthat can mislead model attention. In this paper, we conduct a fine-grained\nanalysis of the context noise and propose an effective metric, the Integrated\nGradient (IG) score, to detect and quantify the noise information within the\ncontext. Our findings reveal that even simple mitigation of detected context\nnoise can substantially boost the model's attention on critical tokens and\nbenefit subsequent predictions. Building on this insight, we propose Context\nDenoising Training (CDT), a straightforward yet effective training strategy\nthat improves attention on critical tokens while reinforcing their influence on\nmodel predictions. Extensive experiments across four tasks, under both context\nwindow scaling and long-context alignment settings, demonstrate the superiority\nof CDT. Notably, when trained with CDT, an open-source 8B model can achieve\nperformance (50.92) comparable to GPT-4o (51.00).",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.05862.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64096ef79e9f790c905b846d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64096ef79e9f790c905b846d/hVzw656lXdzbCxTtnheud.jpeg",
      "fullname": "Zecheng Tang",
      "name": "ZetangForward",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "organization": {
      "_id": "61f8e653129c9ff1b911293d",
      "name": "SUDA",
      "fullname": "Soochow University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1643701821817-61f8e5934a8e5a275b2b3e5a.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.07318",
      "authors": [
        {
          "_id": "68e713227ae125f9582e68fb",
          "name": "Yunhao Fang",
          "hidden": false
        },
        {
          "_id": "68e713227ae125f9582e68fc",
          "name": "Weihao Yu",
          "hidden": false
        },
        {
          "_id": "68e713227ae125f9582e68fd",
          "name": "Shu Zhong",
          "hidden": false
        },
        {
          "_id": "68e713227ae125f9582e68fe",
          "name": "Qinghao Ye",
          "hidden": false
        },
        {
          "_id": "68e713227ae125f9582e68ff",
          "name": "Xuehan Xiong",
          "hidden": false
        },
        {
          "_id": "68e713227ae125f9582e6900",
          "name": "Lai Wei",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-08T17:59:55.000Z",
      "submittedOnDailyAt": "2025-10-09T00:16:06.988Z",
      "title": "Artificial Hippocampus Networks for Efficient Long-Context Modeling",
      "submittedOnDailyBy": {
        "_id": "5df833bdda6d0311fd3d5403",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5df833bdda6d0311fd3d5403/62OtGJEQXdOuhV9yCd4HS.png",
        "isPro": false,
        "fullname": "Weihao Yu",
        "user": "whyu",
        "type": "user"
      },
      "summary": "Long-sequence modeling faces a fundamental trade-off between the efficiency\nof compressive fixed-size memory in RNN-like models and the fidelity of\nlossless growing memory in attention-based Transformers. Inspired by the\nMulti-Store Model in cognitive science, we introduce a memory framework of\nartificial neural networks. Our method maintains a sliding window of the\nTransformer's KV cache as lossless short-term memory, while a learnable module\ntermed Artificial Hippocampus Network (AHN) recurrently compresses\nout-of-window information into a fixed-size compact long-term memory. To\nvalidate this framework, we instantiate AHNs using modern RNN-like\narchitectures, including Mamba2, DeltaNet, and Gated DeltaNet. Extensive\nexperiments on long-context benchmarks LV-Eval and InfiniteBench demonstrate\nthat AHN-augmented models consistently outperform sliding window baselines and\nachieve performance comparable or even superior to full-attention models, while\nsubstantially reducing computational and memory requirements. For instance,\naugmenting the Qwen2.5-3B-Instruct with AHNs reduces inference FLOPs by 40.5%\nand memory cache by 74.0%, while improving its average score on LV-Eval (128k\nsequence length) from 4.41 to 5.88. Code is available at:\nhttps://github.com/ByteDance-Seed/AHN.",
      "upvotes": 10,
      "discussionId": "68e713227ae125f9582e6901",
      "githubRepo": "https://github.com/ByteDance-Seed/AHN",
      "ai_summary": "A memory framework combining short-term and long-term memory in neural networks improves long-sequence modeling efficiency and performance.",
      "ai_keywords": [
        "RNN-like models",
        "attention-based Transformers",
        "Multi-Store Model",
        "sliding window",
        "KV cache",
        "Artificial Hippocampus Network (AHN)",
        "Mamba2",
        "DeltaNet",
        "Gated DeltaNet",
        "LV-Eval",
        "InfiniteBench",
        "inference FLOPs",
        "memory cache"
      ],
      "githubStars": 13,
      "organization": {
        "_id": "67d1140985ea0644e2f14b99",
        "name": "ByteDance-Seed",
        "fullname": "ByteDance Seed",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
      }
    },
    "publishedAt": "2025-10-08T13:59:55.000Z",
    "title": "Artificial Hippocampus Networks for Efficient Long-Context Modeling",
    "summary": "Long-sequence modeling faces a fundamental trade-off between the efficiency\nof compressive fixed-size memory in RNN-like models and the fidelity of\nlossless growing memory in attention-based Transformers. Inspired by the\nMulti-Store Model in cognitive science, we introduce a memory framework of\nartificial neural networks. Our method maintains a sliding window of the\nTransformer's KV cache as lossless short-term memory, while a learnable module\ntermed Artificial Hippocampus Network (AHN) recurrently compresses\nout-of-window information into a fixed-size compact long-term memory. To\nvalidate this framework, we instantiate AHNs using modern RNN-like\narchitectures, including Mamba2, DeltaNet, and Gated DeltaNet. Extensive\nexperiments on long-context benchmarks LV-Eval and InfiniteBench demonstrate\nthat AHN-augmented models consistently outperform sliding window baselines and\nachieve performance comparable or even superior to full-attention models, while\nsubstantially reducing computational and memory requirements. For instance,\naugmenting the Qwen2.5-3B-Instruct with AHNs reduces inference FLOPs by 40.5%\nand memory cache by 74.0%, while improving its average score on LV-Eval (128k\nsequence length) from 4.41 to 5.88. Code is available at:\nhttps://github.com/ByteDance-Seed/AHN.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.07318.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5df833bdda6d0311fd3d5403",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5df833bdda6d0311fd3d5403/62OtGJEQXdOuhV9yCd4HS.png",
      "fullname": "Weihao Yu",
      "name": "whyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "organization": {
      "_id": "67d1140985ea0644e2f14b99",
      "name": "ByteDance-Seed",
      "fullname": "ByteDance Seed",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.06751",
      "authors": [
        {
          "_id": "68e722757ae125f9582e69d9",
          "user": {
            "_id": "67d5848f179ad2756600eca3",
            "avatarUrl": "/avatars/158168a753271b6e024e1fbdf52c9e73.svg",
            "isPro": false,
            "fullname": "Junhan ZHU",
            "user": "Alrightlone",
            "type": "user"
          },
          "name": "Junhan Zhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-09T06:50:12.218Z",
          "hidden": false
        },
        {
          "_id": "68e722757ae125f9582e69da",
          "name": "Hesong Wang",
          "hidden": false
        },
        {
          "_id": "68e722757ae125f9582e69db",
          "name": "Mingluo Su",
          "hidden": false
        },
        {
          "_id": "68e722757ae125f9582e69dc",
          "name": "Zefang Wang",
          "hidden": false
        },
        {
          "_id": "68e722757ae125f9582e69dd",
          "user": {
            "_id": "62b624f3b52bef716e248fd7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b624f3b52bef716e248fd7/AllcccKH-eBWduA8KVnOQ.png",
            "isPro": false,
            "fullname": "Huan Wang",
            "user": "Huan-WhoRegisteredMyName",
            "type": "user"
          },
          "name": "Huan Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-09T06:50:08.795Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-08T08:19:15.000Z",
      "submittedOnDailyAt": "2025-10-09T01:20:49.892Z",
      "title": "OBS-Diff: Accurate Pruning For Diffusion Models in One-Shot",
      "submittedOnDailyBy": {
        "_id": "67a4a26d5e65aa63c6d30e68",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67a4a26d5e65aa63c6d30e68/GtodlJGw-_IL2DTXQTucz.jpeg",
        "isPro": false,
        "fullname": "Sicheng Feng",
        "user": "FSCCS",
        "type": "user"
      },
      "summary": "Large-scale text-to-image diffusion models, while powerful, suffer from\nprohibitive computational cost. Existing one-shot network pruning methods can\nhardly be directly applied to them due to the iterative denoising nature of\ndiffusion models. To bridge the gap, this paper presents OBS-Diff, a novel\none-shot pruning framework that enables accurate and training-free compression\nof large-scale text-to-image diffusion models. Specifically, (i) OBS-Diff\nrevitalizes the classic Optimal Brain Surgeon (OBS), adapting it to the complex\narchitectures of modern diffusion models and supporting diverse pruning\ngranularity, including unstructured, N:M semi-structured, and structured (MHA\nheads and FFN neurons) sparsity; (ii) To align the pruning criteria with the\niterative dynamics of the diffusion process, by examining the problem from an\nerror-accumulation perspective, we propose a novel timestep-aware Hessian\nconstruction that incorporates a logarithmic-decrease weighting scheme,\nassigning greater importance to earlier timesteps to mitigate potential error\naccumulation; (iii) Furthermore, a computationally efficient group-wise\nsequential pruning strategy is proposed to amortize the expensive calibration\nprocess. Extensive experiments show that OBS-Diff achieves state-of-the-art\none-shot pruning for diffusion models, delivering inference acceleration with\nminimal degradation in visual quality.",
      "upvotes": 10,
      "discussionId": "68e722767ae125f9582e69de",
      "projectPage": "https://alrightlone.github.io/OBS-Diff-Webpage/",
      "githubRepo": "https://github.com/Alrightlone/OBS-Diff",
      "ai_summary": "OBS-Diff is a novel one-shot pruning framework that compresses large-scale text-to-image diffusion models with minimal quality loss and significant inference acceleration.",
      "ai_keywords": [
        "diffusion models",
        "one-shot pruning",
        "OBS-Diff",
        "Optimal Brain Surgeon",
        "unstructured",
        "N:M semi-structured",
        "structured",
        "MHA heads",
        "FFN neurons",
        "timestep-aware Hessian",
        "logarithmic-decrease weighting scheme",
        "group-wise sequential pruning"
      ],
      "githubStars": 19,
      "organization": {
        "_id": "643cb10025681c3afab0f1a6",
        "name": "Westlake-University",
        "fullname": "Westlake University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6437eca0819f3ab20d162e14/SQRCHUyjPRyqdtV3um42X.jpeg"
      }
    },
    "publishedAt": "2025-10-08T04:19:15.000Z",
    "title": "OBS-Diff: Accurate Pruning For Diffusion Models in One-Shot",
    "summary": "Large-scale text-to-image diffusion models, while powerful, suffer from\nprohibitive computational cost. Existing one-shot network pruning methods can\nhardly be directly applied to them due to the iterative denoising nature of\ndiffusion models. To bridge the gap, this paper presents OBS-Diff, a novel\none-shot pruning framework that enables accurate and training-free compression\nof large-scale text-to-image diffusion models. Specifically, (i) OBS-Diff\nrevitalizes the classic Optimal Brain Surgeon (OBS), adapting it to the complex\narchitectures of modern diffusion models and supporting diverse pruning\ngranularity, including unstructured, N:M semi-structured, and structured (MHA\nheads and FFN neurons) sparsity; (ii) To align the pruning criteria with the\niterative dynamics of the diffusion process, by examining the problem from an\nerror-accumulation perspective, we propose a novel timestep-aware Hessian\nconstruction that incorporates a logarithmic-decrease weighting scheme,\nassigning greater importance to earlier timesteps to mitigate potential error\naccumulation; (iii) Furthermore, a computationally efficient group-wise\nsequential pruning strategy is proposed to amortize the expensive calibration\nprocess. Extensive experiments show that OBS-Diff achieves state-of-the-art\none-shot pruning for diffusion models, delivering inference acceleration with\nminimal degradation in visual quality.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.06751.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67a4a26d5e65aa63c6d30e68",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67a4a26d5e65aa63c6d30e68/GtodlJGw-_IL2DTXQTucz.jpeg",
      "fullname": "Sicheng Feng",
      "name": "FSCCS",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "organization": {
      "_id": "643cb10025681c3afab0f1a6",
      "name": "Westlake-University",
      "fullname": "Westlake University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6437eca0819f3ab20d162e14/SQRCHUyjPRyqdtV3um42X.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.07238",
      "authors": [
        {
          "_id": "68e728707ae125f9582e69f4",
          "user": {
            "_id": "645efecd7c6bff857734ceeb",
            "avatarUrl": "/avatars/d1cda8b0b943165fb2de6769d28a6787.svg",
            "isPro": false,
            "fullname": "XunyiJiang",
            "user": "Coooori",
            "type": "user"
          },
          "name": "Xunyi Jiang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-09T06:50:03.984Z",
          "hidden": false
        },
        {
          "_id": "68e728707ae125f9582e69f5",
          "name": "Dingyi Chang",
          "hidden": false
        },
        {
          "_id": "68e728707ae125f9582e69f6",
          "name": "Julian McAuley",
          "hidden": false
        },
        {
          "_id": "68e728707ae125f9582e69f7",
          "user": {
            "_id": "6190ab805ca89a28e9f66873",
            "avatarUrl": "/avatars/3c7ecc398fbf851acd2a132e947a92be.svg",
            "isPro": false,
            "fullname": "Xin Xu",
            "user": "XinXuNLPer",
            "type": "user"
          },
          "name": "Xin Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-09T06:50:06.149Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-08T17:06:07.000Z",
      "submittedOnDailyAt": "2025-10-09T01:51:19.017Z",
      "title": "When Benchmarks Age: Temporal Misalignment through Large Language Model\n  Factuality Evaluation",
      "submittedOnDailyBy": {
        "_id": "6190ab805ca89a28e9f66873",
        "avatarUrl": "/avatars/3c7ecc398fbf851acd2a132e947a92be.svg",
        "isPro": false,
        "fullname": "Xin Xu",
        "user": "XinXuNLPer",
        "type": "user"
      },
      "summary": "The rapid evolution of large language models (LLMs) and the real world has\noutpaced the static nature of widely used evaluation benchmarks, raising\nconcerns about their reliability for evaluating LLM factuality. While\nsubstantial works continue to rely on the popular but old benchmarks, their\ntemporal misalignment with real-world facts and modern LLMs, and their effects\non LLM factuality evaluation remain underexplored. Therefore, in this work, we\npresent a systematic investigation of this issue by examining five popular\nfactuality benchmarks and eight LLMs released across different years. An\nup-to-date fact retrieval pipeline and three metrics are tailored to quantify\nbenchmark aging and its impact on LLM factuality evaluation. Experimental\nresults and analysis illustrate that a considerable portion of samples in the\nwidely used factuality benchmarks are outdated, leading to unreliable\nassessments of LLM factuality. We hope our work can provide a testbed to assess\nthe reliability of a benchmark for LLM factuality evaluation and inspire more\nresearch on the benchmark aging issue. Codes are available in\nhttps://github.com/JiangXunyi/BenchAge.",
      "upvotes": 9,
      "discussionId": "68e728717ae125f9582e69f8",
      "githubRepo": "https://github.com/JiangXunyi/BenchAge",
      "ai_summary": "Research investigates the aging of factuality benchmarks and its impact on evaluating the factuality of large language models, revealing significant unreliability due to outdated samples.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "factuality benchmarks",
        "fact retrieval pipeline",
        "benchmark aging",
        "LLM factuality evaluation"
      ],
      "githubStars": 2,
      "organization": {
        "_id": "65af44a1637e10fba942ed0c",
        "name": "McAuley-Lab",
        "fullname": "McAuley-Lab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64daab70c38427829daf5958/OWlh6vciWnY_MeyM099wZ.png"
      }
    },
    "publishedAt": "2025-10-08T13:06:07.000Z",
    "title": "When Benchmarks Age: Temporal Misalignment through Large Language Model\n  Factuality Evaluation",
    "summary": "The rapid evolution of large language models (LLMs) and the real world has\noutpaced the static nature of widely used evaluation benchmarks, raising\nconcerns about their reliability for evaluating LLM factuality. While\nsubstantial works continue to rely on the popular but old benchmarks, their\ntemporal misalignment with real-world facts and modern LLMs, and their effects\non LLM factuality evaluation remain underexplored. Therefore, in this work, we\npresent a systematic investigation of this issue by examining five popular\nfactuality benchmarks and eight LLMs released across different years. An\nup-to-date fact retrieval pipeline and three metrics are tailored to quantify\nbenchmark aging and its impact on LLM factuality evaluation. Experimental\nresults and analysis illustrate that a considerable portion of samples in the\nwidely used factuality benchmarks are outdated, leading to unreliable\nassessments of LLM factuality. We hope our work can provide a testbed to assess\nthe reliability of a benchmark for LLM factuality evaluation and inspire more\nresearch on the benchmark aging issue. Codes are available in\nhttps://github.com/JiangXunyi/BenchAge.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.07238.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6190ab805ca89a28e9f66873",
      "avatarUrl": "/avatars/3c7ecc398fbf851acd2a132e947a92be.svg",
      "fullname": "Xin Xu",
      "name": "XinXuNLPer",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "organization": {
      "_id": "65af44a1637e10fba942ed0c",
      "name": "McAuley-Lab",
      "fullname": "McAuley-Lab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64daab70c38427829daf5958/OWlh6vciWnY_MeyM099wZ.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.05057",
      "authors": [
        {
          "_id": "68e753a77ae125f9582e6ad4",
          "user": {
            "_id": "652e25d2e647b0ee0a024f26",
            "avatarUrl": "/avatars/b5c65cf6c8d0ddc9b8ef0226e0295d56.svg",
            "isPro": false,
            "fullname": "Mingyu Liu",
            "user": "MingyuLiu",
            "type": "user"
          },
          "name": "Mingyu Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-09T06:49:44.012Z",
          "hidden": false
        },
        {
          "_id": "68e753a77ae125f9582e6ad5",
          "name": "Jiuhe Shu",
          "hidden": false
        },
        {
          "_id": "68e753a77ae125f9582e6ad6",
          "name": "Hui Chen",
          "hidden": false
        },
        {
          "_id": "68e753a77ae125f9582e6ad7",
          "name": "Zeju Li",
          "hidden": false
        },
        {
          "_id": "68e753a77ae125f9582e6ad8",
          "name": "Canyu Zhao",
          "hidden": false
        },
        {
          "_id": "68e753a77ae125f9582e6ad9",
          "name": "Jiange Yang",
          "hidden": false
        },
        {
          "_id": "68e753a77ae125f9582e6ada",
          "name": "Shenyuan Gao",
          "hidden": false
        },
        {
          "_id": "68e753a77ae125f9582e6adb",
          "name": "Hao Chen",
          "hidden": false
        },
        {
          "_id": "68e753a77ae125f9582e6adc",
          "name": "Chunhua Shen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-06T17:37:24.000Z",
      "submittedOnDailyAt": "2025-10-09T04:50:11.977Z",
      "title": "StaMo: Unsupervised Learning of Generalizable Robot Motion from Compact\n  State Representation",
      "submittedOnDailyBy": {
        "_id": "652e25d2e647b0ee0a024f26",
        "avatarUrl": "/avatars/b5c65cf6c8d0ddc9b8ef0226e0295d56.svg",
        "isPro": false,
        "fullname": "Mingyu Liu",
        "user": "MingyuLiu",
        "type": "user"
      },
      "summary": "A fundamental challenge in embodied intelligence is developing expressive and\ncompact state representations for efficient world modeling and decision making.\nHowever, existing methods often fail to achieve this balance, yielding\nrepresentations that are either overly redundant or lacking in task-critical\ninformation. We propose an unsupervised approach that learns a highly\ncompressed two-token state representation using a lightweight encoder and a\npre-trained Diffusion Transformer (DiT) decoder, capitalizing on its strong\ngenerative prior. Our representation is efficient, interpretable, and\nintegrates seamlessly into existing VLA-based models, improving performance by\n14.3% on LIBERO and 30% in real-world task success with minimal inference\noverhead. More importantly, we find that the difference between these tokens,\nobtained via latent interpolation, naturally serves as a highly effective\nlatent action, which can be further decoded into executable robot actions. This\nemergent capability reveals that our representation captures structured\ndynamics without explicit supervision. We name our method StaMo for its ability\nto learn generalizable robotic Motion from compact State representation, which\nis encoded from static images, challenging the prevalent dependence to learning\nlatent action on complex architectures and video data. The resulting latent\nactions also enhance policy co-training, outperforming prior methods by 10.4%\nwith improved interpretability. Moreover, our approach scales effectively\nacross diverse data sources, including real-world robot data, simulation, and\nhuman egocentric video.",
      "upvotes": 8,
      "discussionId": "68e753a87ae125f9582e6add",
      "ai_summary": "An unsupervised method learns a compact state representation using a lightweight encoder and Diffusion Transformer decoder, improving robotic performance and enabling latent action decoding from static images.",
      "ai_keywords": [
        "Diffusion Transformer",
        "DiT",
        "VLA-based models",
        "LIBERO",
        "latent interpolation",
        "latent action",
        "StaMo",
        "policy co-training",
        "human egocentric video"
      ],
      "organization": {
        "_id": "6345aadf5efccdc07f1365a5",
        "name": "ZhejiangUniversity",
        "fullname": "Zhejiang University"
      }
    },
    "publishedAt": "2025-10-06T13:37:24.000Z",
    "title": "StaMo: Unsupervised Learning of Generalizable Robot Motion from Compact\n  State Representation",
    "summary": "A fundamental challenge in embodied intelligence is developing expressive and\ncompact state representations for efficient world modeling and decision making.\nHowever, existing methods often fail to achieve this balance, yielding\nrepresentations that are either overly redundant or lacking in task-critical\ninformation. We propose an unsupervised approach that learns a highly\ncompressed two-token state representation using a lightweight encoder and a\npre-trained Diffusion Transformer (DiT) decoder, capitalizing on its strong\ngenerative prior. Our representation is efficient, interpretable, and\nintegrates seamlessly into existing VLA-based models, improving performance by\n14.3% on LIBERO and 30% in real-world task success with minimal inference\noverhead. More importantly, we find that the difference between these tokens,\nobtained via latent interpolation, naturally serves as a highly effective\nlatent action, which can be further decoded into executable robot actions. This\nemergent capability reveals that our representation captures structured\ndynamics without explicit supervision. We name our method StaMo for its ability\nto learn generalizable robotic Motion from compact State representation, which\nis encoded from static images, challenging the prevalent dependence to learning\nlatent action on complex architectures and video data. The resulting latent\nactions also enhance policy co-training, outperforming prior methods by 10.4%\nwith improved interpretability. Moreover, our approach scales effectively\nacross diverse data sources, including real-world robot data, simulation, and\nhuman egocentric video.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.05057.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "652e25d2e647b0ee0a024f26",
      "avatarUrl": "/avatars/b5c65cf6c8d0ddc9b8ef0226e0295d56.svg",
      "fullname": "Mingyu Liu",
      "name": "MingyuLiu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "organization": {
      "_id": "6345aadf5efccdc07f1365a5",
      "name": "ZhejiangUniversity",
      "fullname": "Zhejiang University"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.04212",
      "authors": [
        {
          "_id": "68e7159d7ae125f9582e6916",
          "user": {
            "_id": "66cfda1386e3d910d7d88550",
            "avatarUrl": "/avatars/e6c6537c0dc0a5e1b9d4f7fd62af60af.svg",
            "isPro": false,
            "fullname": "Haiquan Qiu",
            "user": "huggingaaaaa",
            "type": "user"
          },
          "name": "Haiquan Qiu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-09T06:50:30.338Z",
          "hidden": false
        },
        {
          "_id": "68e7159d7ae125f9582e6917",
          "name": "Quanming Yao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-05T14:01:24.000Z",
      "submittedOnDailyAt": "2025-10-09T00:35:51.220Z",
      "title": "Why Low-Precision Transformer Training Fails: An Analysis on Flash\n  Attention",
      "submittedOnDailyBy": {
        "_id": "66cfda1386e3d910d7d88550",
        "avatarUrl": "/avatars/e6c6537c0dc0a5e1b9d4f7fd62af60af.svg",
        "isPro": false,
        "fullname": "Haiquan Qiu",
        "user": "huggingaaaaa",
        "type": "user"
      },
      "summary": "The pursuit of computational efficiency has driven the adoption of\nlow-precision formats for training transformer models. However, this progress\nis often hindered by notorious training instabilities. This paper provides the\nfirst mechanistic explanation for a long-standing and unresolved failure case\nwhere training with flash attention in low-precision settings leads to\ncatastrophic loss explosions. Our in-depth analysis reveals that the failure is\nnot a random artifact but caused by two intertwined phenomena: the emergence of\nsimilar low-rank representations within the attention mechanism and the\ncompounding effect of biased rounding errors inherent in low-precision\narithmetic. We demonstrate how these factors create a vicious cycle of error\naccumulation that corrupts weight updates, ultimately derailing the training\ndynamics. To validate our findings, we introduce a minimal modification to the\nflash attention that mitigates the bias in rounding errors. This simple change\nstabilizes the training process, confirming our analysis and offering a\npractical solution to this persistent problem.",
      "upvotes": 8,
      "discussionId": "68e7159d7ae125f9582e6918",
      "githubRepo": "https://github.com/ucker/why-low-precision-training-fails",
      "ai_summary": "Low-precision training of transformer models with flash attention suffers from catastrophic loss explosions due to low-rank representations and biased rounding errors, which are addressed by a minimal modification to the flash attention mechanism.",
      "ai_keywords": [
        "low-precision formats",
        "transformer models",
        "flash attention",
        "training instabilities",
        "low-rank representations",
        "biased rounding errors",
        "error accumulation",
        "weight updates",
        "training dynamics"
      ],
      "githubStars": 0,
      "organization": {
        "_id": "64cc8e9b214a472dd85e7e1d",
        "name": "THU1911",
        "fullname": "Tsinghua University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61f8e5934a8e5a275b2b3e5a/oKO6FK_rTzzPHXihicZou.jpeg"
      }
    },
    "publishedAt": "2025-10-05T10:01:24.000Z",
    "title": "Why Low-Precision Transformer Training Fails: An Analysis on Flash\n  Attention",
    "summary": "The pursuit of computational efficiency has driven the adoption of\nlow-precision formats for training transformer models. However, this progress\nis often hindered by notorious training instabilities. This paper provides the\nfirst mechanistic explanation for a long-standing and unresolved failure case\nwhere training with flash attention in low-precision settings leads to\ncatastrophic loss explosions. Our in-depth analysis reveals that the failure is\nnot a random artifact but caused by two intertwined phenomena: the emergence of\nsimilar low-rank representations within the attention mechanism and the\ncompounding effect of biased rounding errors inherent in low-precision\narithmetic. We demonstrate how these factors create a vicious cycle of error\naccumulation that corrupts weight updates, ultimately derailing the training\ndynamics. To validate our findings, we introduce a minimal modification to the\nflash attention that mitigates the bias in rounding errors. This simple change\nstabilizes the training process, confirming our analysis and offering a\npractical solution to this persistent problem.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.04212.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66cfda1386e3d910d7d88550",
      "avatarUrl": "/avatars/e6c6537c0dc0a5e1b9d4f7fd62af60af.svg",
      "fullname": "Haiquan Qiu",
      "name": "huggingaaaaa",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "64cc8e9b214a472dd85e7e1d",
      "name": "THU1911",
      "fullname": "Tsinghua University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61f8e5934a8e5a275b2b3e5a/oKO6FK_rTzzPHXihicZou.jpeg"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.07143",
      "authors": [
        {
          "_id": "68e750887ae125f9582e6a98",
          "name": "Chenfei Liao",
          "hidden": false
        },
        {
          "_id": "68e750887ae125f9582e6a99",
          "name": "Wensong Wang",
          "hidden": false
        },
        {
          "_id": "68e750887ae125f9582e6a9a",
          "name": "Zichen Wen",
          "hidden": false
        },
        {
          "_id": "68e750887ae125f9582e6a9b",
          "name": "Xu Zheng",
          "hidden": false
        },
        {
          "_id": "68e750887ae125f9582e6a9c",
          "name": "Yiyu Wang",
          "hidden": false
        },
        {
          "_id": "68e750887ae125f9582e6a9d",
          "name": "Haocong He",
          "hidden": false
        },
        {
          "_id": "68e750887ae125f9582e6a9e",
          "name": "Yuanhuiyi Lyu",
          "hidden": false
        },
        {
          "_id": "68e750887ae125f9582e6a9f",
          "name": "Lutao Jiang",
          "hidden": false
        },
        {
          "_id": "68e750887ae125f9582e6aa0",
          "name": "Xin Zou",
          "hidden": false
        },
        {
          "_id": "68e750887ae125f9582e6aa1",
          "name": "Yuqian Fu",
          "hidden": false
        },
        {
          "_id": "68e750887ae125f9582e6aa2",
          "name": "Bin Ren",
          "hidden": false
        },
        {
          "_id": "68e750887ae125f9582e6aa3",
          "name": "Linfeng Zhang",
          "hidden": false
        },
        {
          "_id": "68e750887ae125f9582e6aa4",
          "name": "Xuming Hu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-08T15:44:28.000Z",
      "submittedOnDailyAt": "2025-10-09T04:38:23.634Z",
      "title": "Are We Using the Right Benchmark: An Evaluation Framework for Visual\n  Token Compression Methods",
      "submittedOnDailyBy": {
        "_id": "6806464ed918f6d2fee2bc8b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6806464ed918f6d2fee2bc8b/rgpG2oO0m6PT0KltCF_Wf.jpeg",
        "isPro": false,
        "fullname": "Chenfei Liao",
        "user": "Chenfei-Liao",
        "type": "user"
      },
      "summary": "Recent endeavors to accelerate inference in Multimodal Large Language Models\n(MLLMs) have primarily focused on visual token compression. The effectiveness\nof these methods is typically assessed by measuring the accuracy drop on\nestablished benchmarks, comparing model performance before and after\ncompression. However, these benchmarks are originally designed to assess the\nperception and reasoning capabilities of MLLMs, rather than to evaluate\ncompression techniques. As a result, directly applying them to visual token\ncompression introduces a task mismatch. Strikingly, our investigation reveals\nthat simple image downsampling consistently outperforms many advanced\ncompression methods across multiple widely used benchmarks. Through extensive\nexperiments, we make the following observations: (i) Current benchmarks are\nnoisy for the visual token compression task. (ii) Down-sampling is able to\nserve as a data filter to evaluate the difficulty of samples in the visual\ntoken compression task. Motivated by these findings, we introduce VTC-Bench, an\nevaluation framework that incorporates a data filtering mechanism to denoise\nexisting benchmarks, thereby enabling fairer and more accurate assessment of\nvisual token compression methods. All data and code are available at\nhttps://github.com/Chenfei-Liao/VTC-Bench.",
      "upvotes": 6,
      "discussionId": "68e750887ae125f9582e6aa5",
      "ai_summary": "VTC-Bench is introduced to provide a fair evaluation framework for visual token compression by incorporating a data filtering mechanism to denoise existing benchmarks.",
      "ai_keywords": [
        "Multimodal Large Language Models",
        "visual token compression",
        "image downsampling",
        "VTC-Bench",
        "data filtering mechanism"
      ]
    },
    "publishedAt": "2025-10-08T11:44:28.000Z",
    "title": "Are We Using the Right Benchmark: An Evaluation Framework for Visual\n  Token Compression Methods",
    "summary": "Recent endeavors to accelerate inference in Multimodal Large Language Models\n(MLLMs) have primarily focused on visual token compression. The effectiveness\nof these methods is typically assessed by measuring the accuracy drop on\nestablished benchmarks, comparing model performance before and after\ncompression. However, these benchmarks are originally designed to assess the\nperception and reasoning capabilities of MLLMs, rather than to evaluate\ncompression techniques. As a result, directly applying them to visual token\ncompression introduces a task mismatch. Strikingly, our investigation reveals\nthat simple image downsampling consistently outperforms many advanced\ncompression methods across multiple widely used benchmarks. Through extensive\nexperiments, we make the following observations: (i) Current benchmarks are\nnoisy for the visual token compression task. (ii) Down-sampling is able to\nserve as a data filter to evaluate the difficulty of samples in the visual\ntoken compression task. Motivated by these findings, we introduce VTC-Bench, an\nevaluation framework that incorporates a data filtering mechanism to denoise\nexisting benchmarks, thereby enabling fairer and more accurate assessment of\nvisual token compression methods. All data and code are available at\nhttps://github.com/Chenfei-Liao/VTC-Bench.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.07143.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6806464ed918f6d2fee2bc8b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6806464ed918f6d2fee2bc8b/rgpG2oO0m6PT0KltCF_Wf.jpeg",
      "fullname": "Chenfei Liao",
      "name": "Chenfei-Liao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.07313",
      "authors": [
        {
          "_id": "68e71cac7ae125f9582e699d",
          "name": "Zezhong Qian",
          "hidden": false
        },
        {
          "_id": "68e71cac7ae125f9582e699e",
          "name": "Xiaowei Chi",
          "hidden": false
        },
        {
          "_id": "68e71cac7ae125f9582e699f",
          "name": "Yuming Li",
          "hidden": false
        },
        {
          "_id": "68e71cac7ae125f9582e69a0",
          "name": "Shizun Wang",
          "hidden": false
        },
        {
          "_id": "68e71cac7ae125f9582e69a1",
          "name": "Zhiyuan Qin",
          "hidden": false
        },
        {
          "_id": "68e71cac7ae125f9582e69a2",
          "name": "Xiaozhu Ju",
          "hidden": false
        },
        {
          "_id": "68e71cac7ae125f9582e69a3",
          "name": "Sirui Han",
          "hidden": false
        },
        {
          "_id": "68e71cac7ae125f9582e69a4",
          "name": "Shanghang Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-08T17:59:08.000Z",
      "submittedOnDailyAt": "2025-10-09T01:05:55.489Z",
      "title": "WristWorld: Generating Wrist-Views via 4D World Models for Robotic\n  Manipulation",
      "submittedOnDailyBy": {
        "_id": "67852e5a3d49517c54365945",
        "avatarUrl": "/avatars/3ab8df5628e91bf89ac39f6fd6955c3e.svg",
        "isPro": false,
        "fullname": "Zezhong Qian",
        "user": "XuWuLingYu",
        "type": "user"
      },
      "summary": "Wrist-view observations are crucial for VLA models as they capture\nfine-grained hand-object interactions that directly enhance manipulation\nperformance. Yet large-scale datasets rarely include such recordings, resulting\nin a substantial gap between abundant anchor views and scarce wrist views.\nExisting world models cannot bridge this gap, as they require a wrist-view\nfirst frame and thus fail to generate wrist-view videos from anchor views\nalone. Amid this gap, recent visual geometry models such as VGGT emerge with\ngeometric and cross-view priors that make it possible to address extreme\nviewpoint shifts. Inspired by these insights, we propose WristWorld, the first\n4D world model that generates wrist-view videos solely from anchor views.\nWristWorld operates in two stages: (i) Reconstruction, which extends VGGT and\nincorporates our Spatial Projection Consistency (SPC) Loss to estimate\ngeometrically consistent wrist-view poses and 4D point clouds; (ii) Generation,\nwhich employs our video generation model to synthesize temporally coherent\nwrist-view videos from the reconstructed perspective. Experiments on Droid,\nCalvin, and Franka Panda demonstrate state-of-the-art video generation with\nsuperior spatial consistency, while also improving VLA performance, raising the\naverage task completion length on Calvin by 3.81% and closing 42.4% of the\nanchor-wrist view gap.",
      "upvotes": 5,
      "discussionId": "68e71cac7ae125f9582e69a5",
      "ai_summary": "WristWorld is a 4D world model that generates wrist-view videos from anchor views, improving video generation consistency and VLA performance.",
      "ai_keywords": [
        "VGGT",
        "Spatial Projection Consistency (SPC) Loss",
        "4D world model",
        "wrist-view videos",
        "anchor views",
        "geometrically consistent wrist-view poses",
        "4D point clouds",
        "temporally coherent wrist-view videos",
        "Droid",
        "Calvin",
        "Franka Panda",
        "task completion length",
        "anchor-wrist view gap"
      ],
      "organization": {
        "_id": "61dcd8e344f59573371b5cb6",
        "name": "PekingUniversity",
        "fullname": "Peking University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
      }
    },
    "publishedAt": "2025-10-08T13:59:08.000Z",
    "title": "WristWorld: Generating Wrist-Views via 4D World Models for Robotic\n  Manipulation",
    "summary": "Wrist-view observations are crucial for VLA models as they capture\nfine-grained hand-object interactions that directly enhance manipulation\nperformance. Yet large-scale datasets rarely include such recordings, resulting\nin a substantial gap between abundant anchor views and scarce wrist views.\nExisting world models cannot bridge this gap, as they require a wrist-view\nfirst frame and thus fail to generate wrist-view videos from anchor views\nalone. Amid this gap, recent visual geometry models such as VGGT emerge with\ngeometric and cross-view priors that make it possible to address extreme\nviewpoint shifts. Inspired by these insights, we propose WristWorld, the first\n4D world model that generates wrist-view videos solely from anchor views.\nWristWorld operates in two stages: (i) Reconstruction, which extends VGGT and\nincorporates our Spatial Projection Consistency (SPC) Loss to estimate\ngeometrically consistent wrist-view poses and 4D point clouds; (ii) Generation,\nwhich employs our video generation model to synthesize temporally coherent\nwrist-view videos from the reconstructed perspective. Experiments on Droid,\nCalvin, and Franka Panda demonstrate state-of-the-art video generation with\nsuperior spatial consistency, while also improving VLA performance, raising the\naverage task completion length on Calvin by 3.81% and closing 42.4% of the\nanchor-wrist view gap.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.07313.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67852e5a3d49517c54365945",
      "avatarUrl": "/avatars/3ab8df5628e91bf89ac39f6fd6955c3e.svg",
      "fullname": "Zezhong Qian",
      "name": "XuWuLingYu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "61dcd8e344f59573371b5cb6",
      "name": "PekingUniversity",
      "fullname": "Peking University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.06261",
      "authors": [
        {
          "_id": "68e722727ae125f9582e69c6",
          "name": "Zhanke Zhou",
          "hidden": false
        },
        {
          "_id": "68e722727ae125f9582e69c7",
          "name": "Chentao Cao",
          "hidden": false
        },
        {
          "_id": "68e722727ae125f9582e69c8",
          "name": "Xiao Feng",
          "hidden": false
        },
        {
          "_id": "68e722727ae125f9582e69c9",
          "name": "Xuan Li",
          "hidden": false
        },
        {
          "_id": "68e722727ae125f9582e69ca",
          "name": "Zongze Li",
          "hidden": false
        },
        {
          "_id": "68e722727ae125f9582e69cb",
          "name": "Xiangyu Lu",
          "hidden": false
        },
        {
          "_id": "68e722727ae125f9582e69cc",
          "name": "Jiangchao Yao",
          "hidden": false
        },
        {
          "_id": "68e722727ae125f9582e69cd",
          "name": "Weikai Huang",
          "hidden": false
        },
        {
          "_id": "68e722727ae125f9582e69ce",
          "name": "Linrui Xu",
          "hidden": false
        },
        {
          "_id": "68e722727ae125f9582e69cf",
          "name": "Tian Cheng",
          "hidden": false
        },
        {
          "_id": "68e722727ae125f9582e69d0",
          "name": "Guanyu Jiang",
          "hidden": false
        },
        {
          "_id": "68e722727ae125f9582e69d1",
          "name": "Yiming Zheng",
          "hidden": false
        },
        {
          "_id": "68e722727ae125f9582e69d2",
          "name": "Brando Miranda",
          "hidden": false
        },
        {
          "_id": "68e722727ae125f9582e69d3",
          "name": "Tongliang Liu",
          "hidden": false
        },
        {
          "_id": "68e722727ae125f9582e69d4",
          "name": "Sanmi Koyejo",
          "hidden": false
        },
        {
          "_id": "68e722727ae125f9582e69d5",
          "name": "Masashi Sugiyama",
          "hidden": false
        },
        {
          "_id": "68e722727ae125f9582e69d6",
          "name": "Bo Han",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-05T15:42:24.000Z",
      "submittedOnDailyAt": "2025-10-09T01:18:36.760Z",
      "title": "AlphaApollo: Orchestrating Foundation Models and Professional Tools into\n  a Self-Evolving System for Deep Agentic Reasoning",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We present AlphaApollo, a self-evolving agentic reasoning system that aims to\naddress two bottlenecks in foundation model (FM) reasoning-limited\nmodel-intrinsic capacity and unreliable test-time iteration. AlphaApollo\norchestrates multiple models with professional tools to enable deliberate,\nverifiable reasoning. It couples (i) a computation tool (Python with numerical\nand symbolic libraries) and (ii) a retrieval tool (task-relevant external\ninformation) to execute exact calculations and ground decisions. The system\nfurther supports multi-round, multi-model solution evolution via a shared state\nmap that records candidates, executable checks, and feedback for iterative\nrefinement. In evaluations on AIME 2024/2025 across multiple models,\nAlphaApollo delivers consistent gains: +5.15% Average@32 and +23.34% Pass@32\nfor Qwen2.5-14B-Instruct, and +8.91% Average@32 with +26.67% Pass@32 for\nLlama-3.3-70B-Instruct. Tool-use analysis shows that more than 80% of tool\ncalls are successfully executed, with consistent outperformance of non-tool\nbaselines, thereby lifting the capability ceiling of FMs. More empirical\nresults and implementation details will be updated at\nhttps://github.com/tmlr-group/AlphaApollo.",
      "upvotes": 4,
      "discussionId": "68e722727ae125f9582e69d7",
      "githubRepo": "https://github.com/tmlr-group/AlphaApollo",
      "ai_summary": "AlphaApollo, a self-evolving reasoning system, enhances foundation model performance through tool integration and iterative refinement, achieving significant improvements in accuracy and pass rates.",
      "ai_keywords": [
        "self-evolving",
        "agentic reasoning system",
        "foundation model",
        "model-intrinsic capacity",
        "test-time iteration",
        "computation tool",
        "retrieval tool",
        "shared state map",
        "iterative refinement",
        "AIME",
        "Qwen2.5-14B-Instruct",
        "Llama-3.3-70B-Instruct",
        "tool-use analysis"
      ],
      "githubStars": 5
    },
    "publishedAt": "2025-10-05T11:42:24.000Z",
    "title": "AlphaApollo: Orchestrating Foundation Models and Professional Tools into\n  a Self-Evolving System for Deep Agentic Reasoning",
    "summary": "We present AlphaApollo, a self-evolving agentic reasoning system that aims to\naddress two bottlenecks in foundation model (FM) reasoning-limited\nmodel-intrinsic capacity and unreliable test-time iteration. AlphaApollo\norchestrates multiple models with professional tools to enable deliberate,\nverifiable reasoning. It couples (i) a computation tool (Python with numerical\nand symbolic libraries) and (ii) a retrieval tool (task-relevant external\ninformation) to execute exact calculations and ground decisions. The system\nfurther supports multi-round, multi-model solution evolution via a shared state\nmap that records candidates, executable checks, and feedback for iterative\nrefinement. In evaluations on AIME 2024/2025 across multiple models,\nAlphaApollo delivers consistent gains: +5.15% Average@32 and +23.34% Pass@32\nfor Qwen2.5-14B-Instruct, and +8.91% Average@32 with +26.67% Pass@32 for\nLlama-3.3-70B-Instruct. Tool-use analysis shows that more than 80% of tool\ncalls are successfully executed, with consistent outperformance of non-tool\nbaselines, thereby lifting the capability ceiling of FMs. More empirical\nresults and implementation details will be updated at\nhttps://github.com/tmlr-group/AlphaApollo.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.06261.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 122
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.01954",
      "authors": [
        {
          "_id": "68e64046975ac4c405ef2237",
          "name": "Yongyi Su",
          "hidden": false
        },
        {
          "_id": "68e64046975ac4c405ef2238",
          "name": "Haojie Zhang",
          "hidden": false
        },
        {
          "_id": "68e64046975ac4c405ef2239",
          "name": "Shijie Li",
          "hidden": false
        },
        {
          "_id": "68e64046975ac4c405ef223a",
          "name": "Nanqing Liu",
          "hidden": false
        },
        {
          "_id": "68e64046975ac4c405ef223b",
          "name": "Jingyi Liao",
          "hidden": false
        },
        {
          "_id": "68e64046975ac4c405ef223c",
          "name": "Junyi Pan",
          "hidden": false
        },
        {
          "_id": "68e64046975ac4c405ef223d",
          "name": "Yuan Liu",
          "hidden": false
        },
        {
          "_id": "68e64046975ac4c405ef223e",
          "name": "Xiaofen Xing",
          "hidden": false
        },
        {
          "_id": "68e64046975ac4c405ef223f",
          "name": "Chong Sun",
          "hidden": false
        },
        {
          "_id": "68e64046975ac4c405ef2240",
          "name": "Chen Li",
          "hidden": false
        },
        {
          "_id": "68e64046975ac4c405ef2241",
          "name": "Nancy F. Chen",
          "hidden": false
        },
        {
          "_id": "68e64046975ac4c405ef2242",
          "name": "Shuicheng Yan",
          "hidden": false
        },
        {
          "_id": "68e64046975ac4c405ef2243",
          "name": "Xulei Yang",
          "hidden": false
        },
        {
          "_id": "68e64046975ac4c405ef2244",
          "name": "Xun Xu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-02T12:23:57.000Z",
      "submittedOnDailyAt": "2025-10-09T05:56:50.657Z",
      "title": "Patch-as-Decodable-Token: Towards Unified Multi-Modal Vision Tasks in\n  MLLMs",
      "submittedOnDailyBy": {
        "_id": "64a0ed5ed5374ca472cfb0ac",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a0ed5ed5374ca472cfb0ac/n_wXamXfR_PPn0hRbnR1X.jpeg",
        "isPro": false,
        "fullname": "ZhimingMa",
        "user": "JimmyMa99",
        "type": "user"
      },
      "summary": "Multimodal large language models (MLLMs) have advanced rapidly in recent\nyears. However, existing approaches for vision tasks often rely on indirect\nrepresentations, such as generating coordinates as text for detection, which\nlimits performance and prevents dense prediction tasks like segmentation. To\novercome these challenges, we introduce Patch-as-Decodable Token (PaDT), a\nunified paradigm that enables MLLMs to directly generate both textual and\ndiverse visual outputs. Central to PaDT are Visual Reference Tokens (VRTs),\nderived from visual patch embeddings of query images and interleaved seamlessly\nwith LLM's output textual tokens. A lightweight decoder then transforms LLM's\noutputs into detection, segmentation, and grounding predictions. Unlike prior\nmethods, PaDT processes VRTs independently at each forward pass and dynamically\nexpands the embedding table, thus improving localization and differentiation\namong similar objects. We further tailor a training strategy for PaDT by\nrandomly selecting VRTs for supervised fine-tuning and introducing a robust\nper-token cross-entropy loss. Our empirical studies across four visual\nperception and understanding tasks suggest PaDT consistently achieving\nstate-of-the-art performance, even compared with significantly larger MLLM\nmodels. The code is available at https://github.com/Gorilla-Lab-SCUT/PaDT.",
      "upvotes": 4,
      "discussionId": "68e64046975ac4c405ef2245",
      "githubRepo": "https://github.com/Gorilla-Lab-SCUT/PaDT",
      "ai_summary": "PaDT, a unified paradigm for multimodal large language models, directly generates both textual and visual outputs, achieving state-of-the-art performance in visual perception tasks.",
      "ai_keywords": [
        "Multimodal large language models",
        "Patch-as-Decodable Token",
        "PaDT",
        "Visual Reference Tokens",
        "VRTs",
        "visual patch embeddings",
        "lightweight decoder",
        "detection",
        "segmentation",
        "grounding predictions",
        "supervised fine-tuning",
        "per-token cross-entropy loss"
      ],
      "githubStars": 11
    },
    "publishedAt": "2025-10-02T08:23:57.000Z",
    "title": "Patch-as-Decodable-Token: Towards Unified Multi-Modal Vision Tasks in\n  MLLMs",
    "summary": "Multimodal large language models (MLLMs) have advanced rapidly in recent\nyears. However, existing approaches for vision tasks often rely on indirect\nrepresentations, such as generating coordinates as text for detection, which\nlimits performance and prevents dense prediction tasks like segmentation. To\novercome these challenges, we introduce Patch-as-Decodable Token (PaDT), a\nunified paradigm that enables MLLMs to directly generate both textual and\ndiverse visual outputs. Central to PaDT are Visual Reference Tokens (VRTs),\nderived from visual patch embeddings of query images and interleaved seamlessly\nwith LLM's output textual tokens. A lightweight decoder then transforms LLM's\noutputs into detection, segmentation, and grounding predictions. Unlike prior\nmethods, PaDT processes VRTs independently at each forward pass and dynamically\nexpands the embedding table, thus improving localization and differentiation\namong similar objects. We further tailor a training strategy for PaDT by\nrandomly selecting VRTs for supervised fine-tuning and introducing a robust\nper-token cross-entropy loss. Our empirical studies across four visual\nperception and understanding tasks suggest PaDT consistently achieving\nstate-of-the-art performance, even compared with significantly larger MLLM\nmodels. The code is available at https://github.com/Gorilla-Lab-SCUT/PaDT.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.01954.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a0ed5ed5374ca472cfb0ac",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a0ed5ed5374ca472cfb0ac/n_wXamXfR_PPn0hRbnR1X.jpeg",
      "fullname": "ZhimingMa",
      "name": "JimmyMa99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.07307",
      "authors": [
        {
          "_id": "68e71d397ae125f9582e69a7",
          "user": {
            "_id": "6466e31a14e059dde8bbe4be",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6466e31a14e059dde8bbe4be/D_DWGWEkhOnFdbbdCNP3N.jpeg",
            "isPro": false,
            "fullname": "Rushi Qiang",
            "user": "Jerrycool",
            "type": "user"
          },
          "name": "Rushi Qiang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-09T06:50:16.218Z",
          "hidden": false
        },
        {
          "_id": "68e71d397ae125f9582e69a8",
          "name": "Yuchen Zhuang",
          "hidden": false
        },
        {
          "_id": "68e71d397ae125f9582e69a9",
          "name": "Anikait Singh",
          "hidden": false
        },
        {
          "_id": "68e71d397ae125f9582e69aa",
          "name": "Percy Liang",
          "hidden": false
        },
        {
          "_id": "68e71d397ae125f9582e69ab",
          "name": "Chao Zhang",
          "hidden": false
        },
        {
          "_id": "68e71d397ae125f9582e69ac",
          "name": "Sherry Yang",
          "hidden": false
        },
        {
          "_id": "68e71d397ae125f9582e69ad",
          "name": "Bo Dai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-08T17:57:19.000Z",
      "submittedOnDailyAt": "2025-10-09T00:57:37.094Z",
      "title": "MLE-Smith: Scaling MLE Tasks with Automated Multi-Agent Pipeline",
      "submittedOnDailyBy": {
        "_id": "6466e31a14e059dde8bbe4be",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6466e31a14e059dde8bbe4be/D_DWGWEkhOnFdbbdCNP3N.jpeg",
        "isPro": false,
        "fullname": "Rushi Qiang",
        "user": "Jerrycool",
        "type": "user"
      },
      "summary": "While Language Models (LMs) have made significant progress in automating\nmachine learning engineering (MLE), the acquisition of high-quality MLE\ntraining data is significantly constrained. Current MLE benchmarks suffer from\nlow scalability and limited applicability because they rely on static, manually\ncurated tasks, demanding extensive time and manual effort to produce. We\nintroduce MLE-Smith, a fully automated multi-agent pipeline, to transform raw\ndatasets into competition-style MLE challenges through an efficient\ngenerate-verify-execute paradigm for scaling MLE tasks with verifiable quality,\nreal-world usability, and rich diversity. The proposed multi-agent pipeline in\nMLE-Smith drives structured task design and standardized refactoring, coupled\nwith a hybrid verification mechanism that enforces strict structural rules and\nhigh-level semantic soundness. It further validates empirical solvability and\nreal-world fidelity through interactive execution. We apply MLE-Smith to 224 of\nreal-world datasets and generate 606 tasks spanning multiple categories,\nobjectives, and modalities, demonstrating that MLE-Smith can work effectively\nacross a wide range of real-world datasets. Evaluation on the generated tasks\nshows that the performance of eight mainstream and cutting-edge LLMs on\nMLE-Smith tasks is strongly correlated with their performance on carefully\nhuman-designed tasks, highlighting the effectiveness of the MLE-Smith to\nscaling up MLE tasks, while maintaining task quality.",
      "upvotes": 3,
      "discussionId": "68e71d397ae125f9582e69ae",
      "ai_summary": "MLE-Smith automates the creation of high-quality, diverse MLE tasks from raw datasets using a multi-agent pipeline, improving scalability and maintaining task quality.",
      "ai_keywords": [
        "Language Models",
        "machine learning engineering",
        "MLE",
        "MLE-Smith",
        "multi-agent pipeline",
        "generate-verify-execute",
        "task design",
        "standardized refactoring",
        "hybrid verification",
        "structural rules",
        "semantic soundness",
        "empirical solvability",
        "real-world fidelity",
        "interactive execution"
      ],
      "organization": {
        "_id": "6808009bdd0bcdd9cfa788ae",
        "name": "MLE-Dojo",
        "fullname": "MLE-Dojo",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6466e31a14e059dde8bbe4be/B7UNsZbUycsSrRgoswiYk.png"
      }
    },
    "publishedAt": "2025-10-08T13:57:19.000Z",
    "title": "MLE-Smith: Scaling MLE Tasks with Automated Multi-Agent Pipeline",
    "summary": "While Language Models (LMs) have made significant progress in automating\nmachine learning engineering (MLE), the acquisition of high-quality MLE\ntraining data is significantly constrained. Current MLE benchmarks suffer from\nlow scalability and limited applicability because they rely on static, manually\ncurated tasks, demanding extensive time and manual effort to produce. We\nintroduce MLE-Smith, a fully automated multi-agent pipeline, to transform raw\ndatasets into competition-style MLE challenges through an efficient\ngenerate-verify-execute paradigm for scaling MLE tasks with verifiable quality,\nreal-world usability, and rich diversity. The proposed multi-agent pipeline in\nMLE-Smith drives structured task design and standardized refactoring, coupled\nwith a hybrid verification mechanism that enforces strict structural rules and\nhigh-level semantic soundness. It further validates empirical solvability and\nreal-world fidelity through interactive execution. We apply MLE-Smith to 224 of\nreal-world datasets and generate 606 tasks spanning multiple categories,\nobjectives, and modalities, demonstrating that MLE-Smith can work effectively\nacross a wide range of real-world datasets. Evaluation on the generated tasks\nshows that the performance of eight mainstream and cutting-edge LLMs on\nMLE-Smith tasks is strongly correlated with their performance on carefully\nhuman-designed tasks, highlighting the effectiveness of the MLE-Smith to\nscaling up MLE tasks, while maintaining task quality.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.07307.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6466e31a14e059dde8bbe4be",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6466e31a14e059dde8bbe4be/D_DWGWEkhOnFdbbdCNP3N.jpeg",
      "fullname": "Rushi Qiang",
      "name": "Jerrycool",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "organization": {
      "_id": "6808009bdd0bcdd9cfa788ae",
      "name": "MLE-Dojo",
      "fullname": "MLE-Dojo",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6466e31a14e059dde8bbe4be/B7UNsZbUycsSrRgoswiYk.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.06855",
      "authors": [
        {
          "_id": "68e752627ae125f9582e6ac1",
          "name": "Hyungrok Jung",
          "hidden": false
        },
        {
          "_id": "68e752627ae125f9582e6ac2",
          "user": {
            "_id": "636b20591340f879a2eb98d0",
            "avatarUrl": "/avatars/4fc5cb13f916bcbc842ccf387bd5f6c0.svg",
            "isPro": false,
            "fullname": "Daneul Kim",
            "user": "carpedkm",
            "type": "user"
          },
          "name": "Daneul Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-09T06:49:46.233Z",
          "hidden": false
        },
        {
          "_id": "68e752627ae125f9582e6ac3",
          "name": "Seunggyun Lim",
          "hidden": false
        },
        {
          "_id": "68e752627ae125f9582e6ac4",
          "name": "Jeany Son",
          "hidden": false
        },
        {
          "_id": "68e752627ae125f9582e6ac5",
          "name": "Jonghyun Choi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-08T10:23:45.000Z",
      "submittedOnDailyAt": "2025-10-09T04:43:32.642Z",
      "title": "Online Generic Event Boundary Detection",
      "submittedOnDailyBy": {
        "_id": "636b20591340f879a2eb98d0",
        "avatarUrl": "/avatars/4fc5cb13f916bcbc842ccf387bd5f6c0.svg",
        "isPro": false,
        "fullname": "Daneul Kim",
        "user": "carpedkm",
        "type": "user"
      },
      "summary": "Generic Event Boundary Detection (GEBD) aims to interpret long-form videos\nthrough the lens of human perception. However, current GEBD methods require\nprocessing complete video frames to make predictions, unlike humans processing\ndata online and in real-time. To bridge this gap, we introduce a new task,\nOnline Generic Event Boundary Detection (On-GEBD), aiming to detect boundaries\nof generic events immediately in streaming videos. This task faces unique\nchallenges of identifying subtle, taxonomy-free event changes in real-time,\nwithout the access to future frames. To tackle these challenges, we propose a\nnovel On-GEBD framework, Estimator, inspired by Event Segmentation Theory (EST)\nwhich explains how humans segment ongoing activity into events by leveraging\nthe discrepancies between predicted and actual information. Our framework\nconsists of two key components: the Consistent Event Anticipator (CEA), and the\nOnline Boundary Discriminator (OBD). Specifically, the CEA generates a\nprediction of the future frame reflecting current event dynamics based solely\non prior frames. Then, the OBD measures the prediction error and adaptively\nadjusts the threshold using statistical tests on past errors to capture\ndiverse, subtle event transitions. Experimental results demonstrate that\nEstimator outperforms all baselines adapted from recent online video\nunderstanding models and achieves performance comparable to prior offline-GEBD\nmethods on the Kinetics-GEBD and TAPOS datasets.",
      "upvotes": 3,
      "discussionId": "68e752627ae125f9582e6ac6",
      "projectPage": "https://carpedkm.github.io/projects/online_gebd/index.html",
      "ai_summary": "A novel framework for real-time event boundary detection in streaming videos uses prediction and error measurement to identify subtle event changes.",
      "ai_keywords": [
        "Online Generic Event Boundary Detection",
        "On-GEBD",
        "Event Segmentation Theory",
        "Consistent Event Anticipator",
        "Online Boundary Discriminator",
        "prediction error",
        "statistical tests",
        "Kinetics-GEBD",
        "TAPOS datasets"
      ]
    },
    "publishedAt": "2025-10-08T06:23:45.000Z",
    "title": "Online Generic Event Boundary Detection",
    "summary": "Generic Event Boundary Detection (GEBD) aims to interpret long-form videos\nthrough the lens of human perception. However, current GEBD methods require\nprocessing complete video frames to make predictions, unlike humans processing\ndata online and in real-time. To bridge this gap, we introduce a new task,\nOnline Generic Event Boundary Detection (On-GEBD), aiming to detect boundaries\nof generic events immediately in streaming videos. This task faces unique\nchallenges of identifying subtle, taxonomy-free event changes in real-time,\nwithout the access to future frames. To tackle these challenges, we propose a\nnovel On-GEBD framework, Estimator, inspired by Event Segmentation Theory (EST)\nwhich explains how humans segment ongoing activity into events by leveraging\nthe discrepancies between predicted and actual information. Our framework\nconsists of two key components: the Consistent Event Anticipator (CEA), and the\nOnline Boundary Discriminator (OBD). Specifically, the CEA generates a\nprediction of the future frame reflecting current event dynamics based solely\non prior frames. Then, the OBD measures the prediction error and adaptively\nadjusts the threshold using statistical tests on past errors to capture\ndiverse, subtle event transitions. Experimental results demonstrate that\nEstimator outperforms all baselines adapted from recent online video\nunderstanding models and achieves performance comparable to prior offline-GEBD\nmethods on the Kinetics-GEBD and TAPOS datasets.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.06855.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "636b20591340f879a2eb98d0",
      "avatarUrl": "/avatars/4fc5cb13f916bcbc842ccf387bd5f6c0.svg",
      "fullname": "Daneul Kim",
      "name": "carpedkm",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.06783",
      "authors": [
        {
          "_id": "68e71bfa7ae125f9582e6991",
          "name": "Akshit Singh",
          "hidden": false
        },
        {
          "_id": "68e71bfa7ae125f9582e6992",
          "name": "Shyam Marjit",
          "hidden": false
        },
        {
          "_id": "68e71bfa7ae125f9582e6993",
          "name": "Wei Lin",
          "hidden": false
        },
        {
          "_id": "68e71bfa7ae125f9582e6994",
          "name": "Paul Gavrikov",
          "hidden": false
        },
        {
          "_id": "68e71bfa7ae125f9582e6995",
          "name": "Serena Yeung-Levy",
          "hidden": false
        },
        {
          "_id": "68e71bfa7ae125f9582e6996",
          "name": "Hilde Kuehne",
          "hidden": false
        },
        {
          "_id": "68e71bfa7ae125f9582e6997",
          "name": "Rogerio Feris",
          "hidden": false
        },
        {
          "_id": "68e71bfa7ae125f9582e6998",
          "name": "Sivan Doveh",
          "hidden": false
        },
        {
          "_id": "68e71bfa7ae125f9582e6999",
          "name": "James Glass",
          "hidden": false
        },
        {
          "_id": "68e71bfa7ae125f9582e699a",
          "name": "M. Jehanzeb Mirza",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-08T09:10:31.000Z",
      "submittedOnDailyAt": "2025-10-09T00:51:16.689Z",
      "title": "TTRV: Test-Time Reinforcement Learning for Vision Language Models",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Existing methods for extracting reward signals in Reinforcement Learning\ntypically rely on labeled data and dedicated training splits, a setup that\ncontrasts with how humans learn directly from their environment. In this work,\nwe propose TTRV to enhance vision language understanding by adapting the model\non the fly at inference time, without the need for any labeled data.\nConcretely, we enhance the Group Relative Policy Optimization (GRPO) framework\nby designing rewards based on the frequency of the base model's output, while\ninferring on each test sample multiple times. Further, we also propose to\ncontrol the diversity of the model's output by simultaneously rewarding the\nmodel for obtaining low entropy of the output empirical distribution. Our\napproach delivers consistent gains across both object recognition and visual\nquestion answering (VQA), with improvements of up to 52.4% and 29.8%,\nrespectively, and average boosts of 24.6% and 10.0% across 16\ndatasets.Remarkably, on image recognition, TTRV applied to InternVL 8B\nsurpasses GPT-4o by an average of 2.3% over 8 benchmarks, while remaining\nhighly competitive on VQA, demonstrating that test-time reinforcement learning\ncan match or exceed the strongest proprietary models. Finally, we find many\ninteresting properties of test-time RL for VLMs: for example, even in extremely\ndata-constrained scenarios, where adaptation is performed on a single randomly\nchosen unlabeled test example, TTRV still yields non-trivial improvements of up\nto 5.5% in recognition tasks.",
      "upvotes": 3,
      "discussionId": "68e71bfb7ae125f9582e699b",
      "projectPage": "https://akshit21112002.github.io/ttrvproject/",
      "githubRepo": "https://github.com/Akshit21112002/TTRV",
      "ai_summary": "TTRV enhances vision language understanding through test-time reinforcement learning, improving performance on object recognition and VQA without labeled data.",
      "ai_keywords": [
        "Reinforcement Learning",
        "Group Relative Policy Optimization (GRPO)",
        "entropy",
        "test-time reinforcement learning",
        "vision language models (VLMs)",
        "InternVL 8B",
        "GPT-4o"
      ],
      "githubStars": 3
    },
    "publishedAt": "2025-10-08T05:10:31.000Z",
    "title": "TTRV: Test-Time Reinforcement Learning for Vision Language Models",
    "summary": "Existing methods for extracting reward signals in Reinforcement Learning\ntypically rely on labeled data and dedicated training splits, a setup that\ncontrasts with how humans learn directly from their environment. In this work,\nwe propose TTRV to enhance vision language understanding by adapting the model\non the fly at inference time, without the need for any labeled data.\nConcretely, we enhance the Group Relative Policy Optimization (GRPO) framework\nby designing rewards based on the frequency of the base model's output, while\ninferring on each test sample multiple times. Further, we also propose to\ncontrol the diversity of the model's output by simultaneously rewarding the\nmodel for obtaining low entropy of the output empirical distribution. Our\napproach delivers consistent gains across both object recognition and visual\nquestion answering (VQA), with improvements of up to 52.4% and 29.8%,\nrespectively, and average boosts of 24.6% and 10.0% across 16\ndatasets.Remarkably, on image recognition, TTRV applied to InternVL 8B\nsurpasses GPT-4o by an average of 2.3% over 8 benchmarks, while remaining\nhighly competitive on VQA, demonstrating that test-time reinforcement learning\ncan match or exceed the strongest proprietary models. Finally, we find many\ninteresting properties of test-time RL for VLMs: for example, even in extremely\ndata-constrained scenarios, where adaptation is performed on a single randomly\nchosen unlabeled test example, TTRV still yields non-trivial improvements of up\nto 5.5% in recognition tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.06783.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 122
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.04999",
      "authors": [
        {
          "_id": "68e548b5975ac4c405ef1f19",
          "user": {
            "_id": "63cbb330f488db9bb3be6fe6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63cbb330f488db9bb3be6fe6/66bg2FdIg4YSsqWzP8WAd.png",
            "isPro": false,
            "fullname": "Nilay K. Bhatnagar",
            "user": "nnilayy",
            "type": "user"
          },
          "name": "Nilay Kumar",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-08T08:01:20.349Z",
          "hidden": false
        },
        {
          "_id": "68e548b5975ac4c405ef1f1a",
          "name": "Priyansh Bhandari",
          "hidden": false
        },
        {
          "_id": "68e548b5975ac4c405ef1f1b",
          "name": "G. Maragatham",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-06T16:39:05.000Z",
      "submittedOnDailyAt": "2025-10-09T00:48:18.817Z",
      "title": "Bridging Text and Video Generation: A Survey",
      "submittedOnDailyBy": {
        "_id": "63cbb330f488db9bb3be6fe6",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63cbb330f488db9bb3be6fe6/66bg2FdIg4YSsqWzP8WAd.png",
        "isPro": false,
        "fullname": "Nilay K. Bhatnagar",
        "user": "nnilayy",
        "type": "user"
      },
      "summary": "Text-to-video (T2V) generation technology holds potential to transform\nmultiple domains such as education, marketing, entertainment, and assistive\ntechnologies for individuals with visual or reading comprehension challenges,\nby creating coherent visual content from natural language prompts. From its\ninception, the field has advanced from adversarial models to diffusion-based\nmodels, yielding higher-fidelity, temporally consistent outputs. Yet challenges\npersist, such as alignment, long-range coherence, and computational efficiency.\nAddressing this evolving landscape, we present a comprehensive survey of\ntext-to-video generative models, tracing their development from early GANs and\nVAEs to hybrid Diffusion-Transformer (DiT) architectures, detailing how these\nmodels work, what limitations they addressed in their predecessors, and why\nshifts toward new architectural paradigms were necessary to overcome challenges\nin quality, coherence, and control. We provide a systematic account of the\ndatasets, which the surveyed text-to-video models were trained and evaluated\non, and, to support reproducibility and assess the accessibility of training\nsuch models, we detail their training configurations, including their hardware\nspecifications, GPU counts, batch sizes, learning rates, optimizers, epochs,\nand other key hyperparameters. Further, we outline the evaluation metrics\ncommonly used for evaluating such models and present their performance across\nstandard benchmarks, while also discussing the limitations of these metrics and\nthe emerging shift toward more holistic, perception-aligned evaluation\nstrategies. Finally, drawing from our analysis, we outline the current open\nchallenges and propose a few promising future directions, laying out a\nperspective for future researchers to explore and build upon in advancing T2V\nresearch and applications.",
      "upvotes": 3,
      "discussionId": "68e548b5975ac4c405ef1f1c",
      "ai_summary": "A survey of text-to-video generative models from GANs and VAEs to hybrid Diffusion-Transformer architectures, detailing their development, limitations, and future directions.",
      "ai_keywords": [
        "adversarial models",
        "diffusion-based models",
        "GANs",
        "VAEs",
        "Diffusion-Transformer (DiT) architectures",
        "datasets",
        "training configurations",
        "evaluation metrics",
        "perception-aligned evaluation strategies"
      ]
    },
    "publishedAt": "2025-10-06T12:39:05.000Z",
    "title": "Bridging Text and Video Generation: A Survey",
    "summary": "Text-to-video (T2V) generation technology holds potential to transform\nmultiple domains such as education, marketing, entertainment, and assistive\ntechnologies for individuals with visual or reading comprehension challenges,\nby creating coherent visual content from natural language prompts. From its\ninception, the field has advanced from adversarial models to diffusion-based\nmodels, yielding higher-fidelity, temporally consistent outputs. Yet challenges\npersist, such as alignment, long-range coherence, and computational efficiency.\nAddressing this evolving landscape, we present a comprehensive survey of\ntext-to-video generative models, tracing their development from early GANs and\nVAEs to hybrid Diffusion-Transformer (DiT) architectures, detailing how these\nmodels work, what limitations they addressed in their predecessors, and why\nshifts toward new architectural paradigms were necessary to overcome challenges\nin quality, coherence, and control. We provide a systematic account of the\ndatasets, which the surveyed text-to-video models were trained and evaluated\non, and, to support reproducibility and assess the accessibility of training\nsuch models, we detail their training configurations, including their hardware\nspecifications, GPU counts, batch sizes, learning rates, optimizers, epochs,\nand other key hyperparameters. Further, we outline the evaluation metrics\ncommonly used for evaluating such models and present their performance across\nstandard benchmarks, while also discussing the limitations of these metrics and\nthe emerging shift toward more holistic, perception-aligned evaluation\nstrategies. Finally, drawing from our analysis, we outline the current open\nchallenges and propose a few promising future directions, laying out a\nperspective for future researchers to explore and build upon in advancing T2V\nresearch and applications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.04999.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63cbb330f488db9bb3be6fe6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63cbb330f488db9bb3be6fe6/66bg2FdIg4YSsqWzP8WAd.png",
      "fullname": "Nilay K. Bhatnagar",
      "name": "nnilayy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.01982",
      "authors": [
        {
          "_id": "68e73f567ae125f9582e6a2c",
          "name": "Yujie Zhou",
          "hidden": false
        },
        {
          "_id": "68e73f567ae125f9582e6a2d",
          "name": "Pengyang Ling",
          "hidden": false
        },
        {
          "_id": "68e73f567ae125f9582e6a2e",
          "name": "Jiazi Bu",
          "hidden": false
        },
        {
          "_id": "68e73f567ae125f9582e6a2f",
          "name": "Yibin Wang",
          "hidden": false
        },
        {
          "_id": "68e73f567ae125f9582e6a30",
          "user": {
            "_id": "63859cf3b2906edaf83af9f0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63859cf3b2906edaf83af9f0/kajwuVzd4pDucSPlwghxo.png",
            "isPro": false,
            "fullname": "Yuhang Zang",
            "user": "yuhangzang",
            "type": "user"
          },
          "name": "Yuhang Zang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-09T06:49:50.572Z",
          "hidden": false
        },
        {
          "_id": "68e73f567ae125f9582e6a31",
          "name": "Jiaqi Wang",
          "hidden": false
        },
        {
          "_id": "68e73f567ae125f9582e6a32",
          "name": "Li Niu",
          "hidden": false
        },
        {
          "_id": "68e73f567ae125f9582e6a33",
          "name": "Guangtao Zhai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-02T12:57:12.000Z",
      "submittedOnDailyAt": "2025-10-09T03:23:48.979Z",
      "title": "G^2RPO: Granular GRPO for Precise Reward in Flow Models",
      "submittedOnDailyBy": {
        "_id": "64b4eec4faa3181a5eab9c46",
        "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
        "isPro": true,
        "fullname": "Jiaqi Wang",
        "user": "myownskyW7",
        "type": "user"
      },
      "summary": "The integration of online reinforcement learning (RL) into diffusion and flow\nmodels has recently emerged as a promising approach for aligning generative\nmodels with human preferences. Stochastic sampling via Stochastic Differential\nEquations (SDE) is employed during the denoising process to generate diverse\ndenoising directions for RL exploration. While existing methods effectively\nexplore potential high-value samples, they suffer from sub-optimal preference\nalignment due to sparse and narrow reward signals. To address these challenges,\nwe propose a novel Granular-GRPO (G^2RPO ) framework that achieves\nprecise and comprehensive reward assessments of sampling directions in\nreinforcement learning of flow models. Specifically, a Singular Stochastic\nSampling strategy is introduced to support step-wise stochastic exploration\nwhile enforcing a high correlation between the reward and the injected noise,\nthereby facilitating a faithful reward for each SDE perturbation. Concurrently,\nto eliminate the bias inherent in fixed-granularity denoising, we introduce a\nMulti-Granularity Advantage Integration module that aggregates advantages\ncomputed at multiple diffusion scales, producing a more comprehensive and\nrobust evaluation of the sampling directions. Experiments conducted on various\nreward models, including both in-domain and out-of-domain evaluations,\ndemonstrate that our G^2RPO significantly outperforms existing\nflow-based GRPO baselines,highlighting its effectiveness and robustness.",
      "upvotes": 3,
      "discussionId": "68e73f567ae125f9582e6a34",
      "projectPage": "https://bujiazi.github.io/g2rpo.github.io/",
      "githubRepo": "https://github.com/bcmi/Granular-GRPO",
      "ai_summary": "A novel Granular-GRPO framework enhances reinforcement learning in diffusion and flow models by improving reward assessment and reducing bias in denoising.",
      "ai_keywords": [
        "online reinforcement learning",
        "diffusion models",
        "flow models",
        "Stochastic Differential Equations",
        "SDE",
        "denoising process",
        "Singular Stochastic Sampling",
        "Multi-Granularity Advantage Integration",
        "reward signals",
        "reward assessment",
        "sampling directions",
        "in-domain evaluations",
        "out-of-domain evaluations",
        "GRPO"
      ],
      "githubStars": 14,
      "organization": {
        "_id": "6839ab3c81ae61b2b722cad0",
        "name": "OpenIXCLab",
        "fullname": "IXCLab@Shanghai AI Lab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64b4eec4faa3181a5eab9c46/LPNnJo6hQXiPRrjajaiil.jpeg"
      }
    },
    "publishedAt": "2025-10-02T08:57:12.000Z",
    "title": "G^2RPO: Granular GRPO for Precise Reward in Flow Models",
    "summary": "The integration of online reinforcement learning (RL) into diffusion and flow\nmodels has recently emerged as a promising approach for aligning generative\nmodels with human preferences. Stochastic sampling via Stochastic Differential\nEquations (SDE) is employed during the denoising process to generate diverse\ndenoising directions for RL exploration. While existing methods effectively\nexplore potential high-value samples, they suffer from sub-optimal preference\nalignment due to sparse and narrow reward signals. To address these challenges,\nwe propose a novel Granular-GRPO (G^2RPO ) framework that achieves\nprecise and comprehensive reward assessments of sampling directions in\nreinforcement learning of flow models. Specifically, a Singular Stochastic\nSampling strategy is introduced to support step-wise stochastic exploration\nwhile enforcing a high correlation between the reward and the injected noise,\nthereby facilitating a faithful reward for each SDE perturbation. Concurrently,\nto eliminate the bias inherent in fixed-granularity denoising, we introduce a\nMulti-Granularity Advantage Integration module that aggregates advantages\ncomputed at multiple diffusion scales, producing a more comprehensive and\nrobust evaluation of the sampling directions. Experiments conducted on various\nreward models, including both in-domain and out-of-domain evaluations,\ndemonstrate that our G^2RPO significantly outperforms existing\nflow-based GRPO baselines,highlighting its effectiveness and robustness.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.01982.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b4eec4faa3181a5eab9c46",
      "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
      "fullname": "Jiaqi Wang",
      "name": "myownskyW7",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 23
    },
    "organization": {
      "_id": "6839ab3c81ae61b2b722cad0",
      "name": "OpenIXCLab",
      "fullname": "IXCLab@Shanghai AI Lab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64b4eec4faa3181a5eab9c46/LPNnJo6hQXiPRrjajaiil.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.05644",
      "authors": [
        {
          "_id": "68e70c147ae125f9582e68a5",
          "user": {
            "_id": "66feebe57d722f0879d6b247",
            "avatarUrl": "/avatars/f9194aa76da3ea2b0212e9ef6b5093d1.svg",
            "isPro": false,
            "fullname": "Sheriff I",
            "user": "imsheriff",
            "type": "user"
          },
          "name": "Sheriff Issaka",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-09T06:50:40.784Z",
          "hidden": false
        },
        {
          "_id": "68e70c147ae125f9582e68a6",
          "name": "Keyi Wang",
          "hidden": false
        },
        {
          "_id": "68e70c147ae125f9582e68a7",
          "name": "Yinka Ajibola",
          "hidden": false
        },
        {
          "_id": "68e70c147ae125f9582e68a8",
          "name": "Oluwatumininu Samuel-Ipaye",
          "hidden": false
        },
        {
          "_id": "68e70c147ae125f9582e68a9",
          "name": "Zhaoyi Zhang",
          "hidden": false
        },
        {
          "_id": "68e70c147ae125f9582e68aa",
          "name": "Nicte Aguillon Jimenez",
          "hidden": false
        },
        {
          "_id": "68e70c147ae125f9582e68ab",
          "name": "Evans Kofi Agyei",
          "hidden": false
        },
        {
          "_id": "68e70c147ae125f9582e68ac",
          "name": "Abraham Lin",
          "hidden": false
        },
        {
          "_id": "68e70c147ae125f9582e68ad",
          "name": "Rohan Ramachandran",
          "hidden": false
        },
        {
          "_id": "68e70c147ae125f9582e68ae",
          "name": "Sadick Abdul Mumin",
          "hidden": false
        },
        {
          "_id": "68e70c147ae125f9582e68af",
          "name": "Faith Nchifor",
          "hidden": false
        },
        {
          "_id": "68e70c147ae125f9582e68b0",
          "name": "Mohammed Shuraim",
          "hidden": false
        },
        {
          "_id": "68e70c147ae125f9582e68b1",
          "name": "Lieqi Liu",
          "hidden": false
        },
        {
          "_id": "68e70c147ae125f9582e68b2",
          "name": "Erick Rosas Gonzalez",
          "hidden": false
        },
        {
          "_id": "68e70c147ae125f9582e68b3",
          "name": "Sylvester Kpei",
          "hidden": false
        },
        {
          "_id": "68e70c147ae125f9582e68b4",
          "name": "Jemimah Osei",
          "hidden": false
        },
        {
          "_id": "68e70c147ae125f9582e68b5",
          "name": "Carlene Ajeneza",
          "hidden": false
        },
        {
          "_id": "68e70c147ae125f9582e68b6",
          "name": "Persis Boateng",
          "hidden": false
        },
        {
          "_id": "68e70c147ae125f9582e68b7",
          "name": "Prisca Adwoa Dufie Yeboah",
          "hidden": false
        },
        {
          "_id": "68e70c147ae125f9582e68b8",
          "name": "Saadia Gabriel",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-07T07:42:52.000Z",
      "submittedOnDailyAt": "2025-10-09T05:54:59.662Z",
      "title": "The African Languages Lab: A Collaborative Approach to Advancing\n  Low-Resource African NLP",
      "submittedOnDailyBy": {
        "_id": "66feebe57d722f0879d6b247",
        "avatarUrl": "/avatars/f9194aa76da3ea2b0212e9ef6b5093d1.svg",
        "isPro": false,
        "fullname": "Sheriff I",
        "user": "imsheriff",
        "type": "user"
      },
      "summary": "Despite representing nearly one-third of the world's languages, African\nlanguages remain critically underserved by modern NLP technologies, with 88\\%\nclassified as severely underrepresented or completely ignored in computational\nlinguistics. We present the African Languages Lab (All Lab), a comprehensive\nresearch initiative that addresses this technological gap through systematic\ndata collection, model development, and capacity building. Our contributions\ninclude: (1) a quality-controlled data collection pipeline, yielding the\nlargest validated African multi-modal speech and text dataset spanning 40\nlanguages with 19 billion tokens of monolingual text and 12,628 hours of\naligned speech data; (2) extensive experimental validation demonstrating that\nour dataset, combined with fine-tuning, achieves substantial improvements over\nbaseline models, averaging +23.69 ChrF++, +0.33 COMET, and +15.34 BLEU points\nacross 31 evaluated languages; and (3) a structured research program that has\nsuccessfully mentored fifteen early-career researchers, establishing\nsustainable local capacity. Our comparative evaluation against Google Translate\nreveals competitive performance in several languages while identifying areas\nthat require continued development.",
      "upvotes": 2,
      "discussionId": "68e70c157ae125f9582e68b9",
      "ai_summary": "The African Languages Lab addresses the underserved status of African languages in NLP by creating a large dataset and demonstrating improved model performance through fine-tuning.",
      "ai_keywords": [
        "multi-modal speech and text dataset",
        "fine-tuning",
        "ChrF++",
        "COMET",
        "BLEU",
        "Google Translate"
      ],
      "organization": {
        "_id": "67784c39dac147922d8d09f0",
        "name": "UCLA",
        "fullname": "University of California, Los Angeles",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67784bd637dfa531fbce95a2/Nf0seEMEn66sPL3QsJXj4.png"
      }
    },
    "publishedAt": "2025-10-07T03:42:52.000Z",
    "title": "The African Languages Lab: A Collaborative Approach to Advancing\n  Low-Resource African NLP",
    "summary": "Despite representing nearly one-third of the world's languages, African\nlanguages remain critically underserved by modern NLP technologies, with 88\\%\nclassified as severely underrepresented or completely ignored in computational\nlinguistics. We present the African Languages Lab (All Lab), a comprehensive\nresearch initiative that addresses this technological gap through systematic\ndata collection, model development, and capacity building. Our contributions\ninclude: (1) a quality-controlled data collection pipeline, yielding the\nlargest validated African multi-modal speech and text dataset spanning 40\nlanguages with 19 billion tokens of monolingual text and 12,628 hours of\naligned speech data; (2) extensive experimental validation demonstrating that\nour dataset, combined with fine-tuning, achieves substantial improvements over\nbaseline models, averaging +23.69 ChrF++, +0.33 COMET, and +15.34 BLEU points\nacross 31 evaluated languages; and (3) a structured research program that has\nsuccessfully mentored fifteen early-career researchers, establishing\nsustainable local capacity. Our comparative evaluation against Google Translate\nreveals competitive performance in several languages while identifying areas\nthat require continued development.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.05644.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66feebe57d722f0879d6b247",
      "avatarUrl": "/avatars/f9194aa76da3ea2b0212e9ef6b5093d1.svg",
      "fullname": "Sheriff I",
      "name": "imsheriff",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "67784c39dac147922d8d09f0",
      "name": "UCLA",
      "fullname": "University of California, Los Angeles",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67784bd637dfa531fbce95a2/Nf0seEMEn66sPL3QsJXj4.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.07041",
      "authors": [
        {
          "_id": "68e71b637ae125f9582e6985",
          "name": "Fenghe Tang",
          "hidden": false
        },
        {
          "_id": "68e71b637ae125f9582e6986",
          "name": "Chengqi Dong",
          "hidden": false
        },
        {
          "_id": "68e71b637ae125f9582e6987",
          "name": "Wenxin Ma",
          "hidden": false
        },
        {
          "_id": "68e71b637ae125f9582e6988",
          "name": "Zikang Xu",
          "hidden": false
        },
        {
          "_id": "68e71b637ae125f9582e6989",
          "name": "Heqin Zhu",
          "hidden": false
        },
        {
          "_id": "68e71b637ae125f9582e698a",
          "name": "Zihang Jiang",
          "hidden": false
        },
        {
          "_id": "68e71b637ae125f9582e698b",
          "name": "Rongsheng Wang",
          "hidden": false
        },
        {
          "_id": "68e71b637ae125f9582e698c",
          "name": "Yuhao Wang",
          "hidden": false
        },
        {
          "_id": "68e71b637ae125f9582e698d",
          "name": "Chenxu Wu",
          "hidden": false
        },
        {
          "_id": "68e71b637ae125f9582e698e",
          "name": "Shaohua Kevin Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-08T14:06:17.000Z",
      "submittedOnDailyAt": "2025-10-09T00:48:27.252Z",
      "title": "U-Bench: A Comprehensive Understanding of U-Net through 100-Variant\n  Benchmarking",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Over the past decade, U-Net has been the dominant architecture in medical\nimage segmentation, leading to the development of thousands of U-shaped\nvariants. Despite its widespread adoption, there is still no comprehensive\nbenchmark to systematically evaluate their performance and utility, largely\nbecause of insufficient statistical validation and limited consideration of\nefficiency and generalization across diverse datasets. To bridge this gap, we\npresent U-Bench, the first large-scale, statistically rigorous benchmark that\nevaluates 100 U-Net variants across 28 datasets and 10 imaging modalities. Our\ncontributions are threefold: (1) Comprehensive Evaluation: U-Bench evaluates\nmodels along three key dimensions: statistical robustness, zero-shot\ngeneralization, and computational efficiency. We introduce a novel metric,\nU-Score, which jointly captures the performance-efficiency trade-off, offering\na deployment-oriented perspective on model progress. (2) Systematic Analysis\nand Model Selection Guidance: We summarize key findings from the large-scale\nevaluation and systematically analyze the impact of dataset characteristics and\narchitectural paradigms on model performance. Based on these insights, we\npropose a model advisor agent to guide researchers in selecting the most\nsuitable models for specific datasets and tasks. (3) Public Availability: We\nprovide all code, models, protocols, and weights, enabling the community to\nreproduce our results and extend the benchmark with future methods. In summary,\nU-Bench not only exposes gaps in previous evaluations but also establishes a\nfoundation for fair, reproducible, and practically relevant benchmarking in the\nnext decade of U-Net-based segmentation models. The project can be accessed at:\nhttps://fenghetan9.github.io/ubench. Code is available at:\nhttps://github.com/FengheTan9/U-Bench.",
      "upvotes": 1,
      "discussionId": "68e71b637ae125f9582e698f",
      "projectPage": "https://fenghetan9.github.io/ubench/",
      "githubRepo": "https://github.com/FengheTan9/U-Bench",
      "ai_summary": "U-Bench is a comprehensive benchmark evaluating 100 U-Net variants across 28 datasets and 10 imaging modalities, focusing on statistical robustness, zero-shot generalization, and computational efficiency.",
      "ai_keywords": [
        "U-Net",
        "U-Bench",
        "U-Score",
        "zero-shot generalization",
        "computational efficiency",
        "statistical robustness",
        "model advisor agent"
      ],
      "githubStars": 13
    },
    "publishedAt": "2025-10-08T10:06:17.000Z",
    "title": "U-Bench: A Comprehensive Understanding of U-Net through 100-Variant\n  Benchmarking",
    "summary": "Over the past decade, U-Net has been the dominant architecture in medical\nimage segmentation, leading to the development of thousands of U-shaped\nvariants. Despite its widespread adoption, there is still no comprehensive\nbenchmark to systematically evaluate their performance and utility, largely\nbecause of insufficient statistical validation and limited consideration of\nefficiency and generalization across diverse datasets. To bridge this gap, we\npresent U-Bench, the first large-scale, statistically rigorous benchmark that\nevaluates 100 U-Net variants across 28 datasets and 10 imaging modalities. Our\ncontributions are threefold: (1) Comprehensive Evaluation: U-Bench evaluates\nmodels along three key dimensions: statistical robustness, zero-shot\ngeneralization, and computational efficiency. We introduce a novel metric,\nU-Score, which jointly captures the performance-efficiency trade-off, offering\na deployment-oriented perspective on model progress. (2) Systematic Analysis\nand Model Selection Guidance: We summarize key findings from the large-scale\nevaluation and systematically analyze the impact of dataset characteristics and\narchitectural paradigms on model performance. Based on these insights, we\npropose a model advisor agent to guide researchers in selecting the most\nsuitable models for specific datasets and tasks. (3) Public Availability: We\nprovide all code, models, protocols, and weights, enabling the community to\nreproduce our results and extend the benchmark with future methods. In summary,\nU-Bench not only exposes gaps in previous evaluations but also establishes a\nfoundation for fair, reproducible, and practically relevant benchmarking in the\nnext decade of U-Net-based segmentation models. The project can be accessed at:\nhttps://fenghetan9.github.io/ubench. Code is available at:\nhttps://github.com/FengheTan9/U-Bench.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.07041.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 122
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.07037",
      "authors": [
        {
          "_id": "68e746a67ae125f9582e6a78",
          "user": {
            "_id": "66e1425c919f283fbd7dfb5e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66e1425c919f283fbd7dfb5e/lfX3gDTLKvwX5QgAivq4S.png",
            "isPro": false,
            "fullname": "Rajvee Sheth",
            "user": "RajveeSheth",
            "type": "user"
          },
          "name": "Rajvee Sheth",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-09T06:49:48.441Z",
          "hidden": false
        },
        {
          "_id": "68e746a67ae125f9582e6a79",
          "name": "Samridhi Raj Sinha",
          "hidden": false
        },
        {
          "_id": "68e746a67ae125f9582e6a7a",
          "name": "Mahavir Patil",
          "hidden": false
        },
        {
          "_id": "68e746a67ae125f9582e6a7b",
          "name": "Himanshu Beniwal",
          "hidden": false
        },
        {
          "_id": "68e746a67ae125f9582e6a7c",
          "name": "Mayank Singh",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-08T14:04:14.000Z",
      "submittedOnDailyAt": "2025-10-09T03:54:04.288Z",
      "title": "Beyond Monolingual Assumptions: A Survey of Code-Switched NLP in the Era\n  of Large Language Models",
      "submittedOnDailyBy": {
        "_id": "66e1425c919f283fbd7dfb5e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66e1425c919f283fbd7dfb5e/lfX3gDTLKvwX5QgAivq4S.png",
        "isPro": false,
        "fullname": "Rajvee Sheth",
        "user": "RajveeSheth",
        "type": "user"
      },
      "summary": "Code-switching (CSW), the alternation of languages and scripts within a\nsingle utterance, remains a fundamental challenge for multiling ual NLP, even\namidst the rapid advances of large language models (LLMs). Most LLMs still\nstruggle with mixed-language inputs, limited CSW datasets, and evaluation\nbiases, hindering deployment in multilingual societies. This survey provides\nthe first comprehensive analysis of CSW-aware LLM research, reviewing\nunique_references studies spanning five research areas, 12 NLP tasks,\n30+ datasets, and 80+ languages. We classify recent advances by architecture,\ntraining strategy, and evaluation methodology, outlining how LLMs have reshaped\nCSW modeling and what challenges persist. The paper concludes with a roadmap\nemphasizing the need for inclusive datasets, fair evaluation, and\nlinguistically grounded models to achieve truly multilingual intelligence. A\ncurated collection of all resources is maintained at\nhttps://github.com/lingo-iitgn/awesome-code-mixing/.",
      "upvotes": 1,
      "discussionId": "68e746a77ae125f9582e6a7d",
      "projectPage": "https://lingo.iitgn.ac.in/codemixing/",
      "githubRepo": "https://github.com/lingo-iitgn/awesome-code-mixing/",
      "ai_summary": "This survey analyzes the current state of code-switching aware large language models, highlighting advancements and challenges in multilingual NLP.",
      "ai_keywords": [
        "code-switching",
        "multilingual NLP",
        "large language models",
        "mixed-language inputs",
        "CSW datasets",
        "evaluation biases",
        "CSW-aware LLM research",
        "architecture",
        "training strategy",
        "evaluation methodology",
        "inclusive datasets",
        "fair evaluation",
        "linguistically grounded models"
      ],
      "githubStars": 1,
      "organization": {
        "_id": "667eb54cc9fc5e32c079544d",
        "name": "LingoIITGN",
        "fullname": "Lingo Research Group",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/667b8f8ba271fc5a8e6929de/8xB-4Az0x50XC4PS3ZIbL.jpeg"
      }
    },
    "publishedAt": "2025-10-08T10:04:14.000Z",
    "title": "Beyond Monolingual Assumptions: A Survey of Code-Switched NLP in the Era\n  of Large Language Models",
    "summary": "Code-switching (CSW), the alternation of languages and scripts within a\nsingle utterance, remains a fundamental challenge for multiling ual NLP, even\namidst the rapid advances of large language models (LLMs). Most LLMs still\nstruggle with mixed-language inputs, limited CSW datasets, and evaluation\nbiases, hindering deployment in multilingual societies. This survey provides\nthe first comprehensive analysis of CSW-aware LLM research, reviewing\nunique_references studies spanning five research areas, 12 NLP tasks,\n30+ datasets, and 80+ languages. We classify recent advances by architecture,\ntraining strategy, and evaluation methodology, outlining how LLMs have reshaped\nCSW modeling and what challenges persist. The paper concludes with a roadmap\nemphasizing the need for inclusive datasets, fair evaluation, and\nlinguistically grounded models to achieve truly multilingual intelligence. A\ncurated collection of all resources is maintained at\nhttps://github.com/lingo-iitgn/awesome-code-mixing/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.07037.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66e1425c919f283fbd7dfb5e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66e1425c919f283fbd7dfb5e/lfX3gDTLKvwX5QgAivq4S.png",
      "fullname": "Rajvee Sheth",
      "name": "RajveeSheth",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "organization": {
      "_id": "667eb54cc9fc5e32c079544d",
      "name": "LingoIITGN",
      "fullname": "Lingo Research Group",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/667b8f8ba271fc5a8e6929de/8xB-4Az0x50XC4PS3ZIbL.jpeg"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.05891",
      "authors": [
        {
          "_id": "68e663cd975ac4c405ef228a",
          "name": "Yanran Zhang",
          "hidden": false
        },
        {
          "_id": "68e663cd975ac4c405ef228b",
          "name": "Bingyao Yu",
          "hidden": false
        },
        {
          "_id": "68e663cd975ac4c405ef228c",
          "name": "Yu Zheng",
          "hidden": false
        },
        {
          "_id": "68e663cd975ac4c405ef228d",
          "name": "Wenzhao Zheng",
          "hidden": false
        },
        {
          "_id": "68e663cd975ac4c405ef228e",
          "name": "Yueqi Duan",
          "hidden": false
        },
        {
          "_id": "68e663cd975ac4c405ef228f",
          "name": "Lei Chen",
          "hidden": false
        },
        {
          "_id": "68e663cd975ac4c405ef2290",
          "name": "Jie Zhou",
          "hidden": false
        },
        {
          "_id": "68e663cd975ac4c405ef2291",
          "name": "Jiwen Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-07T13:02:27.000Z",
      "submittedOnDailyAt": "2025-10-09T00:36:44.189Z",
      "title": "D^3QE: Learning Discrete Distribution Discrepancy-aware\n  Quantization Error for Autoregressive-Generated Image Detection",
      "submittedOnDailyBy": {
        "_id": "661cfae9a853782abad2a495",
        "avatarUrl": "/avatars/39723a07bf9efed8278e009fe966d044.svg",
        "isPro": false,
        "fullname": "Yanran Zhang",
        "user": "Yanran21",
        "type": "user"
      },
      "summary": "The emergence of visual autoregressive (AR) models has revolutionized image\ngeneration while presenting new challenges for synthetic image detection.\nUnlike previous GAN or diffusion-based methods, AR models generate images\nthrough discrete token prediction, exhibiting both marked improvements in image\nsynthesis quality and unique characteristics in their vector-quantized\nrepresentations. In this paper, we propose to leverage Discrete Distribution\nDiscrepancy-aware Quantization Error (D^3QE) for autoregressive-generated\nimage detection that exploits the distinctive patterns and the frequency\ndistribution bias of the codebook existing in real and fake images. We\nintroduce a discrete distribution discrepancy-aware transformer that integrates\ndynamic codebook frequency statistics into its attention mechanism, fusing\nsemantic features and quantization error latent. To evaluate our method, we\nconstruct a comprehensive dataset termed ARForensics covering 7 mainstream\nvisual AR models. Experiments demonstrate superior detection accuracy and\nstrong generalization of D^3QE across different AR models, with robustness to\nreal-world perturbations. Code is available at\nhttps://github.com/Zhangyr2022/D3QE{https://github.com/Zhangyr2022/D3QE}.",
      "upvotes": 1,
      "discussionId": "68e663ce975ac4c405ef2292",
      "projectPage": "https://ivg-yanranzhang.github.io/D3QE/",
      "githubRepo": "https://github.com/Zhangyr2022/D3QE",
      "ai_summary": "A novel method using Discrete Distribution Discrepancy-aware Quantization Error (D$^3$QE) detects images generated by visual autoregressive models by analyzing codebook frequency statistics and quantization errors.",
      "ai_keywords": [
        "visual autoregressive (AR) models",
        "image generation",
        "synthetic image detection",
        "discrete token prediction",
        "vector-quantized representations",
        "Discrete Distribution Discrepancy-aware Quantization Error (D$^3$QE)",
        "discrete distribution discrepancy-aware transformer",
        "attention mechanism",
        "semantic features",
        "quantization error latent",
        "ARForensics dataset"
      ],
      "githubStars": 1
    },
    "publishedAt": "2025-10-07T09:02:27.000Z",
    "title": "D^3QE: Learning Discrete Distribution Discrepancy-aware\n  Quantization Error for Autoregressive-Generated Image Detection",
    "summary": "The emergence of visual autoregressive (AR) models has revolutionized image\ngeneration while presenting new challenges for synthetic image detection.\nUnlike previous GAN or diffusion-based methods, AR models generate images\nthrough discrete token prediction, exhibiting both marked improvements in image\nsynthesis quality and unique characteristics in their vector-quantized\nrepresentations. In this paper, we propose to leverage Discrete Distribution\nDiscrepancy-aware Quantization Error (D^3QE) for autoregressive-generated\nimage detection that exploits the distinctive patterns and the frequency\ndistribution bias of the codebook existing in real and fake images. We\nintroduce a discrete distribution discrepancy-aware transformer that integrates\ndynamic codebook frequency statistics into its attention mechanism, fusing\nsemantic features and quantization error latent. To evaluate our method, we\nconstruct a comprehensive dataset termed ARForensics covering 7 mainstream\nvisual AR models. Experiments demonstrate superior detection accuracy and\nstrong generalization of D^3QE across different AR models, with robustness to\nreal-world perturbations. Code is available at\nhttps://github.com/Zhangyr2022/D3QE{https://github.com/Zhangyr2022/D3QE}.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.05891.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "661cfae9a853782abad2a495",
      "avatarUrl": "/avatars/39723a07bf9efed8278e009fe966d044.svg",
      "fullname": "Yanran Zhang",
      "name": "Yanran21",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.06710",
      "authors": [
        {
          "_id": "68e764c97ae125f9582e6bbc",
          "name": "Hongzhi Zang",
          "hidden": false
        },
        {
          "_id": "68e764c97ae125f9582e6bbd",
          "name": "Mingjie Wei",
          "hidden": false
        },
        {
          "_id": "68e764c97ae125f9582e6bbe",
          "name": "Si Xu",
          "hidden": false
        },
        {
          "_id": "68e764c97ae125f9582e6bbf",
          "name": "Yongji Wu",
          "hidden": false
        },
        {
          "_id": "68e764c97ae125f9582e6bc0",
          "name": "Zhen Guo",
          "hidden": false
        },
        {
          "_id": "68e764c97ae125f9582e6bc1",
          "name": "Yuanqing Wang",
          "hidden": false
        },
        {
          "_id": "68e764c97ae125f9582e6bc2",
          "name": "Hao Lin",
          "hidden": false
        },
        {
          "_id": "68e764c97ae125f9582e6bc3",
          "name": "Liangzhi Shi",
          "hidden": false
        },
        {
          "_id": "68e764c97ae125f9582e6bc4",
          "name": "Yuqing Xie",
          "hidden": false
        },
        {
          "_id": "68e764c97ae125f9582e6bc5",
          "name": "Zhexuan Xu",
          "hidden": false
        },
        {
          "_id": "68e764c97ae125f9582e6bc6",
          "name": "Zhihao Liu",
          "hidden": false
        },
        {
          "_id": "68e764c97ae125f9582e6bc7",
          "name": "Kang Chen",
          "hidden": false
        },
        {
          "_id": "68e764c97ae125f9582e6bc8",
          "name": "Wenhao Tang",
          "hidden": false
        },
        {
          "_id": "68e764c97ae125f9582e6bc9",
          "name": "Quanlu Zhang",
          "hidden": false
        },
        {
          "_id": "68e764c97ae125f9582e6bca",
          "name": "Weinan Zhang",
          "hidden": false
        },
        {
          "_id": "68e764c97ae125f9582e6bcb",
          "name": "Chao Yu",
          "hidden": false
        },
        {
          "_id": "68e764c97ae125f9582e6bcc",
          "name": "Yu Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-08T07:05:13.000Z",
      "submittedOnDailyAt": "2025-10-09T06:06:46.600Z",
      "title": "RLinf-VLA: A Unified and Efficient Framework for VLA+RL Training",
      "submittedOnDailyBy": {
        "_id": "64ba0f8d842aa47891cb972b",
        "avatarUrl": "/avatars/3bcacd9b778a146e88e20887b0b00720.svg",
        "isPro": false,
        "fullname": "Chao Yu",
        "user": "zoeyuchao",
        "type": "user"
      },
      "summary": "Recent progress in vision and language foundation models has significantly\nadvanced multimodal understanding, reasoning, and generation, inspiring a surge\nof interest in extending such capabilities to embodied settings through\nvision-language-action (VLA) models. Yet, most VLA models are still trained\nwith supervised fine-tuning (SFT), which struggles to generalize under\ndistribution shifts due to error accumulation. Reinforcement learning (RL)\noffers a promising alternative by directly optimizing task performance through\ninteraction, but existing attempts remain fragmented and lack a unified\nplatform for fair and systematic comparison across model architectures and\nalgorithmic designs. To address this gap, we introduce RLinf-VLA, a unified and\nefficient framework for scalable RL training of VLA models. The system adopts a\nhighly flexible resource allocation design that addresses the challenge of\nintegrating rendering, training, and inference in RL+VLA training. In\nparticular, for GPU-parallelized simulators, RLinf-VLA implements a novel\nhybrid fine-grained pipeline allocation mode, achieving a 1.61x-1.88x speedup\nin training. Through a unified interface, RLinf-VLA seamlessly supports diverse\nVLA architectures (e.g., OpenVLA, OpenVLA-OFT), multiple RL algorithms (e.g.,\nPPO, GRPO), and various simulators (e.g., ManiSkill, LIBERO). In simulation, a\nunified model achieves 98.11\\% across 130 LIBERO tasks and 97.66\\% across 25\nManiSkill tasks. Beyond empirical performance, our study distills a set of best\npractices for applying RL to VLA training and sheds light on emerging patterns\nin this integration. Furthermore, we present preliminary deployment on a\nreal-world Franka robot, where RL-trained policies exhibit stronger\ngeneralization than those trained with SFT. We envision RLinf-VLA as a\nfoundation to accelerate and standardize research on embodied intelligence.",
      "upvotes": 0,
      "discussionId": "68e764c97ae125f9582e6bcd",
      "ai_summary": "RLinf-VLA is a unified framework for scalable reinforcement learning training of vision-language-action models, offering improved performance and generalization compared to supervised fine-tuning.",
      "ai_keywords": [
        "vision-language-action models",
        "supervised fine-tuning",
        "reinforcement learning",
        "RLinf-VLA",
        "resource allocation",
        "GPU-parallelized simulators",
        "hybrid fine-grained pipeline allocation",
        "OpenVLA",
        "OpenVLA-OFT",
        "PPO",
        "GRPO",
        "ManiSkill",
        "LIBERO",
        "Franka robot"
      ],
      "organization": {
        "_id": "689ea978824b212c988bc8f5",
        "name": "RLinf",
        "fullname": "RLinf",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/689ea8a1a73ecc6940dbba3d/T2RGCw18z6lYP1WfkIGJ3.jpeg"
      }
    },
    "publishedAt": "2025-10-08T03:05:13.000Z",
    "title": "RLinf-VLA: A Unified and Efficient Framework for VLA+RL Training",
    "summary": "Recent progress in vision and language foundation models has significantly\nadvanced multimodal understanding, reasoning, and generation, inspiring a surge\nof interest in extending such capabilities to embodied settings through\nvision-language-action (VLA) models. Yet, most VLA models are still trained\nwith supervised fine-tuning (SFT), which struggles to generalize under\ndistribution shifts due to error accumulation. Reinforcement learning (RL)\noffers a promising alternative by directly optimizing task performance through\ninteraction, but existing attempts remain fragmented and lack a unified\nplatform for fair and systematic comparison across model architectures and\nalgorithmic designs. To address this gap, we introduce RLinf-VLA, a unified and\nefficient framework for scalable RL training of VLA models. The system adopts a\nhighly flexible resource allocation design that addresses the challenge of\nintegrating rendering, training, and inference in RL+VLA training. In\nparticular, for GPU-parallelized simulators, RLinf-VLA implements a novel\nhybrid fine-grained pipeline allocation mode, achieving a 1.61x-1.88x speedup\nin training. Through a unified interface, RLinf-VLA seamlessly supports diverse\nVLA architectures (e.g., OpenVLA, OpenVLA-OFT), multiple RL algorithms (e.g.,\nPPO, GRPO), and various simulators (e.g., ManiSkill, LIBERO). In simulation, a\nunified model achieves 98.11\\% across 130 LIBERO tasks and 97.66\\% across 25\nManiSkill tasks. Beyond empirical performance, our study distills a set of best\npractices for applying RL to VLA training and sheds light on emerging patterns\nin this integration. Furthermore, we present preliminary deployment on a\nreal-world Franka robot, where RL-trained policies exhibit stronger\ngeneralization than those trained with SFT. We envision RLinf-VLA as a\nfoundation to accelerate and standardize research on embodied intelligence.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.06710.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ba0f8d842aa47891cb972b",
      "avatarUrl": "/avatars/3bcacd9b778a146e88e20887b0b00720.svg",
      "fullname": "Chao Yu",
      "name": "zoeyuchao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "689ea978824b212c988bc8f5",
      "name": "RLinf",
      "fullname": "RLinf",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/689ea8a1a73ecc6940dbba3d/T2RGCw18z6lYP1WfkIGJ3.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.06673",
      "authors": [
        {
          "_id": "68e75ac67ae125f9582e6b0c",
          "user": {
            "_id": "637c7d8a88699fba70e1e1ff",
            "avatarUrl": "/avatars/e7f60fab6ca9ad5ecd320bd7cd51ce2e.svg",
            "isPro": false,
            "fullname": "yongxinzhu",
            "user": "youngsheen",
            "type": "user"
          },
          "name": "Yongxin Zhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-09T06:49:41.425Z",
          "hidden": false
        },
        {
          "_id": "68e75ac67ae125f9582e6b0d",
          "name": "Jiawei Chen",
          "hidden": false
        },
        {
          "_id": "68e75ac67ae125f9582e6b0e",
          "name": "Yuanzhe Chen",
          "hidden": false
        },
        {
          "_id": "68e75ac67ae125f9582e6b0f",
          "name": "Zhuo Chen",
          "hidden": false
        },
        {
          "_id": "68e75ac67ae125f9582e6b10",
          "name": "Dongya Jia",
          "hidden": false
        },
        {
          "_id": "68e75ac67ae125f9582e6b11",
          "name": "Jian Cong",
          "hidden": false
        },
        {
          "_id": "68e75ac67ae125f9582e6b12",
          "name": "Xiaobin Zhuang",
          "hidden": false
        },
        {
          "_id": "68e75ac67ae125f9582e6b13",
          "name": "Yuping Wang",
          "hidden": false
        },
        {
          "_id": "68e75ac67ae125f9582e6b14",
          "name": "Yuxuan Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-08T05:54:46.000Z",
      "submittedOnDailyAt": "2025-10-09T05:19:41.365Z",
      "title": "Heptapod: Language Modeling on Visual Signals",
      "submittedOnDailyBy": {
        "_id": "637c7d8a88699fba70e1e1ff",
        "avatarUrl": "/avatars/e7f60fab6ca9ad5ecd320bd7cd51ce2e.svg",
        "isPro": false,
        "fullname": "yongxinzhu",
        "user": "youngsheen",
        "type": "user"
      },
      "summary": "We introduce Heptapod, an image autoregressive model that adheres to the\nfoundational principles of language modeling. Heptapod employs causal\nattention, eliminates reliance on CFG, and eschews the trend\nof semantic tokenizers. Our key innovation is next 2D distribution\nprediction: a causal Transformer with reconstruction-focused visual tokenizer,\nlearns to predict the distribution over the entire 2D spatial grid of images at\neach timestep. This learning objective unifies the sequential modeling of\nautoregressive framework with the holistic self-supervised learning of masked\nautoencoding, enabling the model to capture comprehensive image semantics via\ngenerative training. On the ImageNet generation benchmark, Heptapod achieves an\nFID of 2.70, significantly outperforming previous causal autoregressive\napproaches. We hope our work inspires a principled rethinking of language\nmodeling on visual signals and beyond.",
      "upvotes": 0,
      "discussionId": "68e75ac77ae125f9582e6b15",
      "ai_summary": "Heptapod, an image autoregressive model using causal attention and next 2D distribution prediction, achieves superior performance on ImageNet generation by combining sequential modeling with holistic self-supervised learning.",
      "ai_keywords": [
        "causal attention",
        "CFG",
        "semantic tokenizers",
        "next 2D distribution prediction",
        "causal Transformer",
        "visual tokenizer",
        "masked autoencoding",
        "FID"
      ],
      "organization": {
        "_id": "67d1140985ea0644e2f14b99",
        "name": "ByteDance-Seed",
        "fullname": "ByteDance Seed",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
      }
    },
    "publishedAt": "2025-10-08T01:54:46.000Z",
    "title": "Heptapod: Language Modeling on Visual Signals",
    "summary": "We introduce Heptapod, an image autoregressive model that adheres to the\nfoundational principles of language modeling. Heptapod employs causal\nattention, eliminates reliance on CFG, and eschews the trend\nof semantic tokenizers. Our key innovation is next 2D distribution\nprediction: a causal Transformer with reconstruction-focused visual tokenizer,\nlearns to predict the distribution over the entire 2D spatial grid of images at\neach timestep. This learning objective unifies the sequential modeling of\nautoregressive framework with the holistic self-supervised learning of masked\nautoencoding, enabling the model to capture comprehensive image semantics via\ngenerative training. On the ImageNet generation benchmark, Heptapod achieves an\nFID of 2.70, significantly outperforming previous causal autoregressive\napproaches. We hope our work inspires a principled rethinking of language\nmodeling on visual signals and beyond.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.06673.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "637c7d8a88699fba70e1e1ff",
      "avatarUrl": "/avatars/e7f60fab6ca9ad5ecd320bd7cd51ce2e.svg",
      "fullname": "yongxinzhu",
      "name": "youngsheen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "organization": {
      "_id": "67d1140985ea0644e2f14b99",
      "name": "ByteDance-Seed",
      "fullname": "ByteDance Seed",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.05491",
      "authors": [
        {
          "_id": "68e75b447ae125f9582e6b17",
          "name": "Zichong Li",
          "hidden": false
        },
        {
          "_id": "68e75b447ae125f9582e6b18",
          "name": "Liming Liu",
          "hidden": false
        },
        {
          "_id": "68e75b447ae125f9582e6b19",
          "name": "Chen Liang",
          "hidden": false
        },
        {
          "_id": "68e75b447ae125f9582e6b1a",
          "name": "Weizhu Chen",
          "hidden": false
        },
        {
          "_id": "68e75b447ae125f9582e6b1b",
          "name": "Tuo Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-07T01:13:41.000Z",
      "submittedOnDailyAt": "2025-10-09T05:22:50.855Z",
      "title": "NorMuon: Making Muon more efficient and scalable",
      "submittedOnDailyBy": {
        "_id": "63e6b5e22d2c508de9001afd",
        "avatarUrl": "/avatars/43cec9e8b8d490bd259e383954846a1e.svg",
        "isPro": false,
        "fullname": "Chen Liang",
        "user": "cliang1453",
        "type": "user"
      },
      "summary": "The choice of optimizer significantly impacts the training efficiency and\ncomputational costs of large language models (LLMs). Recently, the Muon\noptimizer has demonstrated promising results by orthogonalizing parameter\nupdates, improving optimization geometry through better conditioning. Despite\nMuon's emergence as a candidate successor to Adam, the potential for jointly\nleveraging their strengths has not been systematically explored. In this work,\nwe bridge this gap by proposing NorMuon (Neuron-wise Normalized Muon), an\noptimizer that synergistically combines orthogonalization with neuron-level\nadaptive learning rates. Our analysis reveals that while Muon effectively\nreduces condition numbers, the resulting updates exhibit highly non-uniform\nneuron norms, causing certain neurons to dominate the optimization process.\nNorMuon addresses this imbalance by maintaining second-order momentum\nstatistics for each neuron and applying row-wise normalization after\northogonalization, ensuring balanced parameter utilization while preserving\nMuon's conditioning benefits. To enable practical deployment at scale, we\ndevelop an efficient distributed implementation under the FSDP2 framework that\nstrategically distributes orthogonalization computations across devices.\nExperiments across multiple model scales demonstrate that NorMuon consistently\noutperforms both Adam and Muon, achieving 21.74% better training efficiency\nthan Adam and 11.31% improvement over Muon on 1.1 B pretraining setting, while\nmaintaining a comparable memory footprint to Muon. Our findings suggest that\northogonalization and adaptive learning rates are complementary rather than\ncompeting approaches, opening new avenues for optimizer design in large-scale\ndeep learning.",
      "upvotes": 0,
      "discussionId": "68e75b457ae125f9582e6b1c",
      "ai_summary": "NorMuon, a novel optimizer combining orthogonalization with neuron-level adaptive learning rates, enhances training efficiency and balances parameter utilization in large language models.",
      "ai_keywords": [
        "Muon",
        "orthogonalization",
        "optimization geometry",
        "conditioning",
        "neuron-level adaptive learning rates",
        "second-order momentum statistics",
        "row-wise normalization",
        "FSDP2",
        "training efficiency",
        "memory footprint"
      ]
    },
    "publishedAt": "2025-10-06T21:13:41.000Z",
    "title": "NorMuon: Making Muon more efficient and scalable",
    "summary": "The choice of optimizer significantly impacts the training efficiency and\ncomputational costs of large language models (LLMs). Recently, the Muon\noptimizer has demonstrated promising results by orthogonalizing parameter\nupdates, improving optimization geometry through better conditioning. Despite\nMuon's emergence as a candidate successor to Adam, the potential for jointly\nleveraging their strengths has not been systematically explored. In this work,\nwe bridge this gap by proposing NorMuon (Neuron-wise Normalized Muon), an\noptimizer that synergistically combines orthogonalization with neuron-level\nadaptive learning rates. Our analysis reveals that while Muon effectively\nreduces condition numbers, the resulting updates exhibit highly non-uniform\nneuron norms, causing certain neurons to dominate the optimization process.\nNorMuon addresses this imbalance by maintaining second-order momentum\nstatistics for each neuron and applying row-wise normalization after\northogonalization, ensuring balanced parameter utilization while preserving\nMuon's conditioning benefits. To enable practical deployment at scale, we\ndevelop an efficient distributed implementation under the FSDP2 framework that\nstrategically distributes orthogonalization computations across devices.\nExperiments across multiple model scales demonstrate that NorMuon consistently\noutperforms both Adam and Muon, achieving 21.74% better training efficiency\nthan Adam and 11.31% improvement over Muon on 1.1 B pretraining setting, while\nmaintaining a comparable memory footprint to Muon. Our findings suggest that\northogonalization and adaptive learning rates are complementary rather than\ncompeting approaches, opening new avenues for optimizer design in large-scale\ndeep learning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.05491.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63e6b5e22d2c508de9001afd",
      "avatarUrl": "/avatars/43cec9e8b8d490bd259e383954846a1e.svg",
      "fullname": "Chen Liang",
      "name": "cliang1453",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  }
]