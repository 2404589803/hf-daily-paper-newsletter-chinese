[
  {
    "paper": {
      "id": "2508.13167",
      "authors": [
        {
          "_id": "68a535f16cf0bf898542ec6c",
          "name": "Weizhen Li",
          "hidden": false
        },
        {
          "_id": "68a535f16cf0bf898542ec6d",
          "name": "Jianbo Lin",
          "hidden": false
        },
        {
          "_id": "68a535f16cf0bf898542ec6e",
          "name": "Zhuosong Jiang",
          "hidden": false
        },
        {
          "_id": "68a535f16cf0bf898542ec6f",
          "name": "Jingyi Cao",
          "hidden": false
        },
        {
          "_id": "68a535f16cf0bf898542ec70",
          "name": "Xinpeng Liu",
          "hidden": false
        },
        {
          "_id": "68a535f16cf0bf898542ec71",
          "name": "Jiayu Zhang",
          "hidden": false
        },
        {
          "_id": "68a535f16cf0bf898542ec72",
          "name": "Zhenqiang Huang",
          "hidden": false
        },
        {
          "_id": "68a535f16cf0bf898542ec73",
          "name": "Qianben Chen",
          "hidden": false
        },
        {
          "_id": "68a535f16cf0bf898542ec74",
          "name": "Weichen Sun",
          "hidden": false
        },
        {
          "_id": "68a535f16cf0bf898542ec75",
          "name": "Qiexiang Wang",
          "hidden": false
        },
        {
          "_id": "68a535f16cf0bf898542ec76",
          "name": "Hongxuan Lu",
          "hidden": false
        },
        {
          "_id": "68a535f16cf0bf898542ec77",
          "name": "Tianrui Qin",
          "hidden": false
        },
        {
          "_id": "68a535f16cf0bf898542ec78",
          "name": "Chenghao Zhu",
          "hidden": false
        },
        {
          "_id": "68a535f16cf0bf898542ec79",
          "name": "Yi Yao",
          "hidden": false
        },
        {
          "_id": "68a535f16cf0bf898542ec7a",
          "name": "Shuying Fan",
          "hidden": false
        },
        {
          "_id": "68a535f16cf0bf898542ec7b",
          "name": "Xiaowan Li",
          "hidden": false
        },
        {
          "_id": "68a535f16cf0bf898542ec7c",
          "name": "Tiannan Wang",
          "hidden": false
        },
        {
          "_id": "68a535f16cf0bf898542ec7d",
          "name": "Pai Liu",
          "hidden": false
        },
        {
          "_id": "68a535f16cf0bf898542ec7e",
          "name": "King Zhu",
          "hidden": false
        },
        {
          "_id": "68a535f16cf0bf898542ec7f",
          "name": "He Zhu",
          "hidden": false
        },
        {
          "_id": "68a535f16cf0bf898542ec80",
          "name": "Dingfeng Shi",
          "hidden": false
        },
        {
          "_id": "68a535f16cf0bf898542ec81",
          "name": "Piaohong Wang",
          "hidden": false
        },
        {
          "_id": "68a535f16cf0bf898542ec82",
          "name": "Yeyi Guan",
          "hidden": false
        },
        {
          "_id": "68a535f16cf0bf898542ec83",
          "name": "Xiangru Tang",
          "hidden": false
        },
        {
          "_id": "68a535f16cf0bf898542ec84",
          "name": "Minghao Liu",
          "hidden": false
        },
        {
          "_id": "68a535f16cf0bf898542ec85",
          "name": "Yuchen Eleanor Jiang",
          "hidden": false
        },
        {
          "_id": "68a535f16cf0bf898542ec86",
          "name": "Jian Yang",
          "hidden": false
        },
        {
          "_id": "68a535f16cf0bf898542ec87",
          "name": "Jiaheng Liu",
          "hidden": false
        },
        {
          "_id": "68a535f16cf0bf898542ec88",
          "name": "Ge Zhang",
          "hidden": false
        },
        {
          "_id": "68a535f16cf0bf898542ec89",
          "name": "Wangchunshu Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-06T17:01:02.000Z",
      "submittedOnDailyAt": "2025-08-20T01:12:39.713Z",
      "title": "Chain-of-Agents: End-to-End Agent Foundation Models via Multi-Agent\n  Distillation and Agentic RL",
      "submittedOnDailyBy": {
        "_id": "628c8598ef14f971b698107f",
        "avatarUrl": "/avatars/3a4ad87e6b5f9e836a1160d869df1447.svg",
        "isPro": false,
        "fullname": "Zhou",
        "user": "Wangchunshu",
        "type": "user"
      },
      "summary": "Recent advances in large language models (LLMs) and multi-agent systems have\ndemonstrated remarkable capabilities in complex problem-solving tasks such as\ndeep research, vibe coding, and mathematical reasoning. However, most existing\nmulti-agent systems are built upon manual prompt/workflow engineering with\nsophisticated agent frameworks, making them computationally inefficient, less\ncapable, and can not benefit from data-centric learning. In this work, we\nintroduce Chain-of-Agents (CoA), a novel paradigm of LLM reasoning that enables\nnative end-to-end complex problem-solving in the same way as a multi-agent\nsystem (i.e., multi-turn problem solving with multiple tools and multiple\nagents) within one model. In chain-of-agents problem-solving, the model\ndynamically activates different tool agents and role-playing agents to simulate\nmulti-agent collaboration in an end-to-end fashion. To elicit end-to-end\nchain-of-agents problem-solving abilities in LLMs, we introduce a multi-agent\ndistillation framework to distill state-of-the-art multi-agent systems into\nchain-of-agents trajectories for agentic supervised fine-tuning. We then use\nagentic reinforcement learning on verifiable agentic tasks to further improve\nthe models' capabilities on chain-of-agents problem solving. We call the\nresulting models Agent Foundation Models (AFMs). Our empirical studies\ndemonstrate that AFM establishes new state-of-the-art performance across\ndiverse benchmarks in both web agent and code agent settings. We make the\nentire research, including the model weights, code for training and evaluation,\nand the training data, fully open-sourced, which offers a solid starting point\nfor future research on agent models and agentic RL.",
      "upvotes": 37,
      "discussionId": "68a535f16cf0bf898542ec8a",
      "projectPage": "https://chain-of-agents-afm.github.io/",
      "githubRepo": "https://github.com/OPPO-PersonalAI/Agent_Foundation_Models",
      "ai_summary": "Chain-of-Agents (CoA) paradigm enables end-to-end complex problem-solving in LLMs through dynamic agent activation, improving performance via multi-agent distillation and agentic reinforcement learning.",
      "ai_keywords": [
        "large language models",
        "multi-agent systems",
        "deep research",
        "vibe coding",
        "mathematical reasoning",
        "prompt/workflow engineering",
        "agent frameworks",
        "chain-of-agents",
        "tool agents",
        "role-playing agents",
        "multi-agent distillation",
        "agentic supervised fine-tuning",
        "agentic reinforcement learning",
        "Agent Foundation Models",
        "AFMs",
        "web agent",
        "code agent",
        "verifiable agentic tasks"
      ],
      "githubStars": 32
    },
    "publishedAt": "2025-08-06T13:01:02.000Z",
    "title": "Chain-of-Agents: End-to-End Agent Foundation Models via Multi-Agent\n  Distillation and Agentic RL",
    "summary": "Recent advances in large language models (LLMs) and multi-agent systems have\ndemonstrated remarkable capabilities in complex problem-solving tasks such as\ndeep research, vibe coding, and mathematical reasoning. However, most existing\nmulti-agent systems are built upon manual prompt/workflow engineering with\nsophisticated agent frameworks, making them computationally inefficient, less\ncapable, and can not benefit from data-centric learning. In this work, we\nintroduce Chain-of-Agents (CoA), a novel paradigm of LLM reasoning that enables\nnative end-to-end complex problem-solving in the same way as a multi-agent\nsystem (i.e., multi-turn problem solving with multiple tools and multiple\nagents) within one model. In chain-of-agents problem-solving, the model\ndynamically activates different tool agents and role-playing agents to simulate\nmulti-agent collaboration in an end-to-end fashion. To elicit end-to-end\nchain-of-agents problem-solving abilities in LLMs, we introduce a multi-agent\ndistillation framework to distill state-of-the-art multi-agent systems into\nchain-of-agents trajectories for agentic supervised fine-tuning. We then use\nagentic reinforcement learning on verifiable agentic tasks to further improve\nthe models' capabilities on chain-of-agents problem solving. We call the\nresulting models Agent Foundation Models (AFMs). Our empirical studies\ndemonstrate that AFM establishes new state-of-the-art performance across\ndiverse benchmarks in both web agent and code agent settings. We make the\nentire research, including the model weights, code for training and evaluation,\nand the training data, fully open-sourced, which offers a solid starting point\nfor future research on agent models and agentic RL.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.13167.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "628c8598ef14f971b698107f",
      "avatarUrl": "/avatars/3a4ad87e6b5f9e836a1160d869df1447.svg",
      "fullname": "Zhou",
      "name": "Wangchunshu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.14041",
      "authors": [
        {
          "_id": "68a5270e6cf0bf898542ec12",
          "name": "Chin-Yang Lin",
          "hidden": false
        },
        {
          "_id": "68a5270e6cf0bf898542ec13",
          "name": "Cheng Sun",
          "hidden": false
        },
        {
          "_id": "68a5270e6cf0bf898542ec14",
          "name": "Fu-En Yang",
          "hidden": false
        },
        {
          "_id": "68a5270e6cf0bf898542ec15",
          "name": "Min-Hung Chen",
          "hidden": false
        },
        {
          "_id": "68a5270e6cf0bf898542ec16",
          "name": "Yen-Yu Lin",
          "hidden": false
        },
        {
          "_id": "68a5270e6cf0bf898542ec17",
          "name": "Yu-Lun Liu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/dCg9OGuyLHLqwelz7AgUB.mp4"
      ],
      "publishedAt": "2025-08-19T17:59:56.000Z",
      "submittedOnDailyAt": "2025-08-20T01:41:48.509Z",
      "title": "LongSplat: Robust Unposed 3D Gaussian Splatting for Casual Long Videos",
      "submittedOnDailyBy": {
        "_id": "6459d5da3b6fafd9664807ab",
        "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
        "isPro": false,
        "fullname": "Yu-Lun Liu",
        "user": "yulunliu",
        "type": "user"
      },
      "summary": "LongSplat addresses critical challenges in novel view synthesis (NVS) from\ncasually captured long videos characterized by irregular camera motion, unknown\ncamera poses, and expansive scenes. Current methods often suffer from pose\ndrift, inaccurate geometry initialization, and severe memory limitations. To\naddress these issues, we introduce LongSplat, a robust unposed 3D Gaussian\nSplatting framework featuring: (1) Incremental Joint Optimization that\nconcurrently optimizes camera poses and 3D Gaussians to avoid local minima and\nensure global consistency; (2) a robust Pose Estimation Module leveraging\nlearned 3D priors; and (3) an efficient Octree Anchor Formation mechanism that\nconverts dense point clouds into anchors based on spatial density. Extensive\nexperiments on challenging benchmarks demonstrate that LongSplat achieves\nstate-of-the-art results, substantially improving rendering quality, pose\naccuracy, and computational efficiency compared to prior approaches. Project\npage: https://linjohnss.github.io/longsplat/",
      "upvotes": 28,
      "discussionId": "68a5270e6cf0bf898542ec18",
      "projectPage": "https://linjohnss.github.io/longsplat/",
      "githubRepo": "https://github.com/NVlabs/LongSplat",
      "ai_summary": "LongSplat improves novel view synthesis from long videos with irregular motion through joint optimization, robust pose estimation, and efficient anchor formation.",
      "ai_keywords": [
        "novel view synthesis",
        "3D Gaussian Splatting",
        "Incremental Joint Optimization",
        "camera poses",
        "3D Gaussians",
        "Pose Estimation Module",
        "3D priors",
        "Octree Anchor Formation",
        "dense point clouds",
        "anchors",
        "rendering quality",
        "pose accuracy",
        "computational efficiency"
      ],
      "githubStars": 37
    },
    "publishedAt": "2025-08-19T13:59:56.000Z",
    "title": "LongSplat: Robust Unposed 3D Gaussian Splatting for Casual Long Videos",
    "summary": "LongSplat addresses critical challenges in novel view synthesis (NVS) from\ncasually captured long videos characterized by irregular camera motion, unknown\ncamera poses, and expansive scenes. Current methods often suffer from pose\ndrift, inaccurate geometry initialization, and severe memory limitations. To\naddress these issues, we introduce LongSplat, a robust unposed 3D Gaussian\nSplatting framework featuring: (1) Incremental Joint Optimization that\nconcurrently optimizes camera poses and 3D Gaussians to avoid local minima and\nensure global consistency; (2) a robust Pose Estimation Module leveraging\nlearned 3D priors; and (3) an efficient Octree Anchor Formation mechanism that\nconverts dense point clouds into anchors based on spatial density. Extensive\nexperiments on challenging benchmarks demonstrate that LongSplat achieves\nstate-of-the-art results, substantially improving rendering quality, pose\naccuracy, and computational efficiency compared to prior approaches. Project\npage: https://linjohnss.github.io/longsplat/",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/dCg9OGuyLHLqwelz7AgUB.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.14041.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6459d5da3b6fafd9664807ab",
      "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
      "fullname": "Yu-Lun Liu",
      "name": "yulunliu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.13948",
      "authors": [
        {
          "_id": "68a5366b6cf0bf898542ec8c",
          "name": "Yuge Zhang",
          "hidden": false
        },
        {
          "_id": "68a5366b6cf0bf898542ec8d",
          "name": "Nan Chen",
          "hidden": false
        },
        {
          "_id": "68a5366b6cf0bf898542ec8e",
          "name": "Jiahang Xu",
          "hidden": false
        },
        {
          "_id": "68a5366b6cf0bf898542ec8f",
          "name": "Yuqing Yang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6466d323ac657f60661d2778/rnVtFjaxiAUmMm2vyb7Uu.mp4"
      ],
      "publishedAt": "2025-08-19T15:37:29.000Z",
      "submittedOnDailyAt": "2025-08-20T01:28:00.780Z",
      "title": "Prompt Orchestration Markup Language",
      "submittedOnDailyBy": {
        "_id": "6466d323ac657f60661d2778",
        "avatarUrl": "/avatars/62f70630cdf1c252b80b4d5eaa5a4150.svg",
        "isPro": false,
        "fullname": "Yuge Zhang",
        "user": "ultmaster",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) require sophisticated prompting, yet current\npractices face challenges in structure, data integration, format sensitivity,\nand tooling. Existing methods lack comprehensive solutions for organizing\ncomplex prompts involving diverse data types (documents, tables, images) or\nmanaging presentation variations systematically. To address these gaps, we\nintroduce POML (Prompt Orchestration Markup Language). POML employs\ncomponent-based markup for logical structure (roles, tasks, examples),\nspecialized tags for seamless data integration, and a CSS-like styling system\nto decouple content from presentation, reducing formatting sensitivity. It\nincludes templating for dynamic prompts and a comprehensive developer toolkit\n(IDE support, SDKs) to improve version control and collaboration. We validate\nPOML through two case studies demonstrating its impact on complex application\nintegration (PomLink) and accuracy performance (TableQA), as well as a user\nstudy assessing its effectiveness in real-world development scenarios.",
      "upvotes": 12,
      "discussionId": "68a5366c6cf0bf898542ec90",
      "projectPage": "https://microsoft.github.io/poml/",
      "githubRepo": "https://github.com/microsoft/poml",
      "ai_summary": "POML addresses challenges in prompting Large Language Models by providing a structured, data-integrated, and format-sensitive markup language with templating and developer tools.",
      "ai_keywords": [
        "POML",
        "Prompt Orchestration Markup Language",
        "component-based markup",
        "specialized tags",
        "CSS-like styling system",
        "templating",
        "developer toolkit",
        "IDE support",
        "SDKs",
        "version control",
        "collaboration",
        "PomLink",
        "TableQA"
      ],
      "githubStars": 3443
    },
    "publishedAt": "2025-08-19T11:37:29.000Z",
    "title": "Prompt Orchestration Markup Language",
    "summary": "Large Language Models (LLMs) require sophisticated prompting, yet current\npractices face challenges in structure, data integration, format sensitivity,\nand tooling. Existing methods lack comprehensive solutions for organizing\ncomplex prompts involving diverse data types (documents, tables, images) or\nmanaging presentation variations systematically. To address these gaps, we\nintroduce POML (Prompt Orchestration Markup Language). POML employs\ncomponent-based markup for logical structure (roles, tasks, examples),\nspecialized tags for seamless data integration, and a CSS-like styling system\nto decouple content from presentation, reducing formatting sensitivity. It\nincludes templating for dynamic prompts and a comprehensive developer toolkit\n(IDE support, SDKs) to improve version control and collaboration. We validate\nPOML through two case studies demonstrating its impact on complex application\nintegration (PomLink) and accuracy performance (TableQA), as well as a user\nstudy assessing its effectiveness in real-world development scenarios.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6466d323ac657f60661d2778/rnVtFjaxiAUmMm2vyb7Uu.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.13948.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6466d323ac657f60661d2778",
      "avatarUrl": "/avatars/62f70630cdf1c252b80b4d5eaa5a4150.svg",
      "fullname": "Yuge Zhang",
      "name": "ultmaster",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.06905",
      "authors": [
        {
          "_id": "689d51b2b083e610d741e9e4",
          "name": "Ruoxi Chen",
          "hidden": false
        },
        {
          "_id": "689d51b2b083e610d741e9e5",
          "name": "Dongping Chen",
          "hidden": false
        },
        {
          "_id": "689d51b2b083e610d741e9e6",
          "name": "Siyuan Wu",
          "hidden": false
        },
        {
          "_id": "689d51b2b083e610d741e9e7",
          "name": "Sinan Wang",
          "hidden": false
        },
        {
          "_id": "689d51b2b083e610d741e9e8",
          "name": "Shiyun Lang",
          "hidden": false
        },
        {
          "_id": "689d51b2b083e610d741e9e9",
          "name": "Petr Sushko",
          "hidden": false
        },
        {
          "_id": "689d51b2b083e610d741e9ea",
          "name": "Gaoyang Jiang",
          "hidden": false
        },
        {
          "_id": "689d51b2b083e610d741e9eb",
          "name": "Yao Wan",
          "hidden": false
        },
        {
          "_id": "689d51b2b083e610d741e9ec",
          "name": "Ranjay Krishna",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-09T09:36:21.000Z",
      "submittedOnDailyAt": "2025-08-20T03:20:45.314Z",
      "title": "MultiRef: Controllable Image Generation with Multiple Visual References",
      "submittedOnDailyBy": {
        "_id": "643be8879f5d314db2d9ed23",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643be8879f5d314db2d9ed23/VrW2UtJ7ppOnGIYjTWd7b.png",
        "isPro": false,
        "fullname": "Chen Dongping",
        "user": "shuaishuaicdp",
        "type": "user"
      },
      "summary": "Visual designers naturally draw inspiration from multiple visual references,\ncombining diverse elements and aesthetic principles to create artwork. However,\ncurrent image generative frameworks predominantly rely on single-source inputs\n-- either text prompts or individual reference images. In this paper, we focus\non the task of controllable image generation using multiple visual references.\nWe introduce MultiRef-bench, a rigorous evaluation framework comprising 990\nsynthetic and 1,000 real-world samples that require incorporating visual\ncontent from multiple reference images. The synthetic samples are synthetically\ngenerated through our data engine RefBlend, with 10 reference types and 33\nreference combinations. Based on RefBlend, we further construct a dataset\nMultiRef containing 38k high-quality images to facilitate further research. Our\nexperiments across three interleaved image-text models (i.e., OmniGen, ACE, and\nShow-o) and six agentic frameworks (e.g., ChatDiT and LLM + SD) reveal that\neven state-of-the-art systems struggle with multi-reference conditioning, with\nthe best model OmniGen achieving only 66.6% in synthetic samples and 79.0% in\nreal-world cases on average compared to the golden answer. These findings\nprovide valuable directions for developing more flexible and human-like\ncreative tools that can effectively integrate multiple sources of visual\ninspiration. The dataset is publicly available at: https://multiref.github.io/.",
      "upvotes": 11,
      "discussionId": "689d51b2b083e610d741e9ed",
      "ai_summary": "Experiments with multiple image-text models and agentic frameworks show that even state-of-the-art systems struggle with generating images from multiple visual references, highlighting the need for more flexible creative tools.",
      "ai_keywords": [
        "MultiRef-bench",
        "RefBlend",
        "MultiRef",
        "OmniGen",
        "ACE",
        "Show-o",
        "ChatDiT",
        "LLM + SD"
      ]
    },
    "publishedAt": "2025-08-09T05:36:21.000Z",
    "title": "MultiRef: Controllable Image Generation with Multiple Visual References",
    "summary": "Visual designers naturally draw inspiration from multiple visual references,\ncombining diverse elements and aesthetic principles to create artwork. However,\ncurrent image generative frameworks predominantly rely on single-source inputs\n-- either text prompts or individual reference images. In this paper, we focus\non the task of controllable image generation using multiple visual references.\nWe introduce MultiRef-bench, a rigorous evaluation framework comprising 990\nsynthetic and 1,000 real-world samples that require incorporating visual\ncontent from multiple reference images. The synthetic samples are synthetically\ngenerated through our data engine RefBlend, with 10 reference types and 33\nreference combinations. Based on RefBlend, we further construct a dataset\nMultiRef containing 38k high-quality images to facilitate further research. Our\nexperiments across three interleaved image-text models (i.e., OmniGen, ACE, and\nShow-o) and six agentic frameworks (e.g., ChatDiT and LLM + SD) reveal that\neven state-of-the-art systems struggle with multi-reference conditioning, with\nthe best model OmniGen achieving only 66.6% in synthetic samples and 79.0% in\nreal-world cases on average compared to the golden answer. These findings\nprovide valuable directions for developing more flexible and human-like\ncreative tools that can effectively integrate multiple sources of visual\ninspiration. The dataset is publicly available at: https://multiref.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.06905.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643be8879f5d314db2d9ed23",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643be8879f5d314db2d9ed23/VrW2UtJ7ppOnGIYjTWd7b.png",
      "fullname": "Chen Dongping",
      "name": "shuaishuaicdp",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.09131",
      "authors": [
        {
          "_id": "689ea8bea4caabb4320e5d9b",
          "user": {
            "_id": "6503ccaf13d750b4604649e4",
            "avatarUrl": "/avatars/96bebff9284d61f37c83e7da6a7e9bac.svg",
            "isPro": false,
            "fullname": "Zixin Yin",
            "user": "zachary-yin",
            "type": "user"
          },
          "name": "Zixin Yin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-18T06:57:13.352Z",
          "hidden": false
        },
        {
          "_id": "689ea8bea4caabb4320e5d9c",
          "name": "Xili Dai",
          "hidden": false
        },
        {
          "_id": "689ea8bea4caabb4320e5d9d",
          "name": "Ling-Hao Chen",
          "hidden": false
        },
        {
          "_id": "689ea8bea4caabb4320e5d9e",
          "name": "Deyu Zhou",
          "hidden": false
        },
        {
          "_id": "689ea8bea4caabb4320e5d9f",
          "name": "Jianan Wang",
          "hidden": false
        },
        {
          "_id": "689ea8bea4caabb4320e5da0",
          "name": "Duomin Wang",
          "hidden": false
        },
        {
          "_id": "689ea8bea4caabb4320e5da1",
          "name": "Gang Yu",
          "hidden": false
        },
        {
          "_id": "689ea8bea4caabb4320e5da2",
          "name": "Lionel M. Ni",
          "hidden": false
        },
        {
          "_id": "689ea8bea4caabb4320e5da3",
          "name": "Lei Zhang",
          "hidden": false
        },
        {
          "_id": "689ea8bea4caabb4320e5da4",
          "name": "Heung-Yeung Shum",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-12T17:57:04.000Z",
      "submittedOnDailyAt": "2025-08-20T01:14:07.228Z",
      "title": "Training-Free Text-Guided Color Editing with Multi-Modal Diffusion\n  Transformer",
      "submittedOnDailyBy": {
        "_id": "6503ccaf13d750b4604649e4",
        "avatarUrl": "/avatars/96bebff9284d61f37c83e7da6a7e9bac.svg",
        "isPro": false,
        "fullname": "Zixin Yin",
        "user": "zachary-yin",
        "type": "user"
      },
      "summary": "Text-guided color editing in images and videos is a fundamental yet unsolved\nproblem, requiring fine-grained manipulation of color attributes, including\nalbedo, light source color, and ambient lighting, while preserving physical\nconsistency in geometry, material properties, and light-matter interactions.\nExisting training-free methods offer broad applicability across editing tasks\nbut struggle with precise color control and often introduce visual\ninconsistency in both edited and non-edited regions. In this work, we present\nColorCtrl, a training-free color editing method that leverages the attention\nmechanisms of modern Multi-Modal Diffusion Transformers (MM-DiT). By\ndisentangling structure and color through targeted manipulation of attention\nmaps and value tokens, our method enables accurate and consistent color\nediting, along with word-level control of attribute intensity. Our method\nmodifies only the intended regions specified by the prompt, leaving unrelated\nareas untouched. Extensive experiments on both SD3 and FLUX.1-dev demonstrate\nthat ColorCtrl outperforms existing training-free approaches and achieves\nstate-of-the-art performances in both edit quality and consistency.\nFurthermore, our method surpasses strong commercial models such as FLUX.1\nKontext Max and GPT-4o Image Generation in terms of consistency. When extended\nto video models like CogVideoX, our approach exhibits greater advantages,\nparticularly in maintaining temporal coherence and editing stability. Finally,\nour method also generalizes to instruction-based editing diffusion models such\nas Step1X-Edit and FLUX.1 Kontext dev, further demonstrating its versatility.",
      "upvotes": 9,
      "discussionId": "689ea8bea4caabb4320e5da5",
      "ai_summary": "ColorCtrl, a training-free method using Multi-Modal Diffusion Transformers, achieves precise and consistent color editing in images and videos with word-level control and superior performance compared to existing approaches.",
      "ai_keywords": [
        "Multi-Modal Diffusion Transformers",
        "MM-DiT",
        "attention mechanisms",
        "attention maps",
        "value tokens",
        "color editing",
        "edit quality",
        "consistency",
        "temporal coherence",
        "editing stability",
        "instruction-based editing diffusion models"
      ]
    },
    "publishedAt": "2025-08-12T13:57:04.000Z",
    "title": "Training-Free Text-Guided Color Editing with Multi-Modal Diffusion\n  Transformer",
    "summary": "Text-guided color editing in images and videos is a fundamental yet unsolved\nproblem, requiring fine-grained manipulation of color attributes, including\nalbedo, light source color, and ambient lighting, while preserving physical\nconsistency in geometry, material properties, and light-matter interactions.\nExisting training-free methods offer broad applicability across editing tasks\nbut struggle with precise color control and often introduce visual\ninconsistency in both edited and non-edited regions. In this work, we present\nColorCtrl, a training-free color editing method that leverages the attention\nmechanisms of modern Multi-Modal Diffusion Transformers (MM-DiT). By\ndisentangling structure and color through targeted manipulation of attention\nmaps and value tokens, our method enables accurate and consistent color\nediting, along with word-level control of attribute intensity. Our method\nmodifies only the intended regions specified by the prompt, leaving unrelated\nareas untouched. Extensive experiments on both SD3 and FLUX.1-dev demonstrate\nthat ColorCtrl outperforms existing training-free approaches and achieves\nstate-of-the-art performances in both edit quality and consistency.\nFurthermore, our method surpasses strong commercial models such as FLUX.1\nKontext Max and GPT-4o Image Generation in terms of consistency. When extended\nto video models like CogVideoX, our approach exhibits greater advantages,\nparticularly in maintaining temporal coherence and editing stability. Finally,\nour method also generalizes to instruction-based editing diffusion models such\nas Step1X-Edit and FLUX.1 Kontext dev, further demonstrating its versatility.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.09131.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6503ccaf13d750b4604649e4",
      "avatarUrl": "/avatars/96bebff9284d61f37c83e7da6a7e9bac.svg",
      "fullname": "Zixin Yin",
      "name": "zachary-yin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2508.13632",
      "authors": [
        {
          "_id": "68a533b66cf0bf898542ec55",
          "name": "Yutong Feng",
          "hidden": false
        },
        {
          "_id": "68a533b66cf0bf898542ec56",
          "name": "Linlin Zhang",
          "hidden": false
        },
        {
          "_id": "68a533b66cf0bf898542ec57",
          "name": "Hengyuan Cao",
          "hidden": false
        },
        {
          "_id": "68a533b66cf0bf898542ec58",
          "name": "Yiming Chen",
          "hidden": false
        },
        {
          "_id": "68a533b66cf0bf898542ec59",
          "name": "Xiaoduan Feng",
          "hidden": false
        },
        {
          "_id": "68a533b66cf0bf898542ec5a",
          "name": "Jian Cao",
          "hidden": false
        },
        {
          "_id": "68a533b66cf0bf898542ec5b",
          "name": "Yuxiong Wu",
          "hidden": false
        },
        {
          "_id": "68a533b66cf0bf898542ec5c",
          "name": "Bin Wang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64a54e468cfaa458bd6844bf/rhpuJxc1U-nbYvSCOLqJC.png",
        "https://cdn-uploads.huggingface.co/production/uploads/64a54e468cfaa458bd6844bf/35kf2hrsNQcheYU8YiObo.jpeg"
      ],
      "publishedAt": "2025-08-19T08:47:31.000Z",
      "submittedOnDailyAt": "2025-08-20T01:08:25.013Z",
      "title": "OmniTry: Virtual Try-On Anything without Masks",
      "submittedOnDailyBy": {
        "_id": "64a54e468cfaa458bd6844bf",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a54e468cfaa458bd6844bf/5Gmf4tAr59GNl-2VaZDbu.png",
        "isPro": false,
        "fullname": "Yutong Feng",
        "user": "fengyutong",
        "type": "user"
      },
      "summary": "Virtual Try-ON (VTON) is a practical and widely-applied task, for which most\nof existing works focus on clothes. This paper presents OmniTry, a unified\nframework that extends VTON beyond garment to encompass any wearable objects,\ne.g., jewelries and accessories, with mask-free setting for more practical\napplication. When extending to various types of objects, data curation is\nchallenging for obtaining paired images, i.e., the object image and the\ncorresponding try-on result. To tackle this problem, we propose a two-staged\npipeline: For the first stage, we leverage large-scale unpaired images, i.e.,\nportraits with any wearable items, to train the model for mask-free\nlocalization. Specifically, we repurpose the inpainting model to automatically\ndraw objects in suitable positions given an empty mask. For the second stage,\nthe model is further fine-tuned with paired images to transfer the consistency\nof object appearance. We observed that the model after the first stage shows\nquick convergence even with few paired samples. OmniTry is evaluated on a\ncomprehensive benchmark consisting of 12 common classes of wearable objects,\nwith both in-shop and in-the-wild images. Experimental results suggest that\nOmniTry shows better performance on both object localization and\nID-preservation compared with existing methods. The code, model weights, and\nevaluation benchmark of OmniTry will be made publicly available at\nhttps://omnitry.github.io/.",
      "upvotes": 7,
      "discussionId": "68a533b66cf0bf898542ec5d",
      "projectPage": "https://omnitry.github.io/",
      "githubRepo": "https://github.com/Kunbyte-AI/OmniTry",
      "ai_summary": "OmniTry extends Virtual Try-ON to various wearable objects using a two-stage pipeline that combines unpaired and paired image training to improve object localization and appearance consistency.",
      "ai_keywords": [
        "Virtual Try-ON",
        "OmniTry",
        "mask-free localization",
        "inpainting model",
        "fine-tuning",
        "object localization",
        "ID-preservation"
      ],
      "githubStars": 8
    },
    "publishedAt": "2025-08-19T04:47:31.000Z",
    "title": "OmniTry: Virtual Try-On Anything without Masks",
    "summary": "Virtual Try-ON (VTON) is a practical and widely-applied task, for which most\nof existing works focus on clothes. This paper presents OmniTry, a unified\nframework that extends VTON beyond garment to encompass any wearable objects,\ne.g., jewelries and accessories, with mask-free setting for more practical\napplication. When extending to various types of objects, data curation is\nchallenging for obtaining paired images, i.e., the object image and the\ncorresponding try-on result. To tackle this problem, we propose a two-staged\npipeline: For the first stage, we leverage large-scale unpaired images, i.e.,\nportraits with any wearable items, to train the model for mask-free\nlocalization. Specifically, we repurpose the inpainting model to automatically\ndraw objects in suitable positions given an empty mask. For the second stage,\nthe model is further fine-tuned with paired images to transfer the consistency\nof object appearance. We observed that the model after the first stage shows\nquick convergence even with few paired samples. OmniTry is evaluated on a\ncomprehensive benchmark consisting of 12 common classes of wearable objects,\nwith both in-shop and in-the-wild images. Experimental results suggest that\nOmniTry shows better performance on both object localization and\nID-preservation compared with existing methods. The code, model weights, and\nevaluation benchmark of OmniTry will be made publicly available at\nhttps://omnitry.github.io/.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64a54e468cfaa458bd6844bf/rhpuJxc1U-nbYvSCOLqJC.png",
      "https://cdn-uploads.huggingface.co/production/uploads/64a54e468cfaa458bd6844bf/35kf2hrsNQcheYU8YiObo.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.13632.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a54e468cfaa458bd6844bf",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a54e468cfaa458bd6844bf/5Gmf4tAr59GNl-2VaZDbu.png",
      "fullname": "Yutong Feng",
      "name": "fengyutong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.11548",
      "authors": [
        {
          "_id": "68a28870a4caabb4320e63cc",
          "name": "Zhenhua Xu",
          "hidden": false
        },
        {
          "_id": "68a28870a4caabb4320e63cd",
          "name": "Xubin Yue",
          "hidden": false
        },
        {
          "_id": "68a28870a4caabb4320e63ce",
          "user": {
            "_id": "6746d09dfb7a1de1dca3cc21",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6746d09dfb7a1de1dca3cc21/uGNJVBm3g_O6N_R77yoMs.png",
            "isPro": false,
            "fullname": "Zhebo Wang",
            "user": "BreynaldDva",
            "type": "user"
          },
          "name": "Zhebo Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-18T06:53:58.643Z",
          "hidden": false
        },
        {
          "_id": "68a28870a4caabb4320e63cf",
          "name": "Qichen Liu",
          "hidden": false
        },
        {
          "_id": "68a28870a4caabb4320e63d0",
          "name": "Xixiang Zhao",
          "hidden": false
        },
        {
          "_id": "68a28870a4caabb4320e63d1",
          "name": "Jingxuan Zhang",
          "hidden": false
        },
        {
          "_id": "68a28870a4caabb4320e63d2",
          "name": "Wenjun Zeng",
          "hidden": false
        },
        {
          "_id": "68a28870a4caabb4320e63d3",
          "name": "Wengpeng Xing",
          "hidden": false
        },
        {
          "_id": "68a28870a4caabb4320e63d4",
          "name": "Dezhang Kong",
          "hidden": false
        },
        {
          "_id": "68a28870a4caabb4320e63d5",
          "name": "Changting Lin",
          "hidden": false
        },
        {
          "_id": "68a28870a4caabb4320e63d6",
          "name": "Meng Han",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-15T15:50:20.000Z",
      "submittedOnDailyAt": "2025-08-20T06:00:06.343Z",
      "title": "Copyright Protection for Large Language Models: A Survey of Methods,\n  Challenges, and Trends",
      "submittedOnDailyBy": {
        "_id": "6746d09dfb7a1de1dca3cc21",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6746d09dfb7a1de1dca3cc21/uGNJVBm3g_O6N_R77yoMs.png",
        "isPro": false,
        "fullname": "Zhebo Wang",
        "user": "BreynaldDva",
        "type": "user"
      },
      "summary": "Copyright protection for large language models is of critical importance,\ngiven their substantial development costs, proprietary value, and potential for\nmisuse. Existing surveys have predominantly focused on techniques for tracing\nLLM-generated content-namely, text watermarking-while a systematic exploration\nof methods for protecting the models themselves (i.e., model watermarking and\nmodel fingerprinting) remains absent. Moreover, the relationships and\ndistinctions among text watermarking, model watermarking, and model\nfingerprinting have not been comprehensively clarified. This work presents a\ncomprehensive survey of the current state of LLM copyright protection\ntechnologies, with a focus on model fingerprinting, covering the following\naspects: (1) clarifying the conceptual connection from text watermarking to\nmodel watermarking and fingerprinting, and adopting a unified terminology that\nincorporates model watermarking into the broader fingerprinting framework; (2)\nproviding an overview and comparison of diverse text watermarking techniques,\nhighlighting cases where such methods can function as model fingerprinting; (3)\nsystematically categorizing and comparing existing model fingerprinting\napproaches for LLM copyright protection; (4) presenting, for the first time,\ntechniques for fingerprint transfer and fingerprint removal; (5) summarizing\nevaluation metrics for model fingerprints, including effectiveness,\nharmlessness, robustness, stealthiness, and reliability; and (6) discussing\nopen challenges and future research directions. This survey aims to offer\nresearchers a thorough understanding of both text watermarking and model\nfingerprinting technologies in the era of LLMs, thereby fostering further\nadvances in protecting their intellectual property.",
      "upvotes": 5,
      "discussionId": "68a28871a4caabb4320e63d7",
      "githubRepo": "https://github.com/Xuzhenhua55/awesome-llm-copyright-protection",
      "ai_summary": "A survey of LLM copyright protection technologies, focusing on model fingerprinting, clarifies its relationship with text watermarking and provides a comprehensive overview of fingerprinting techniques, evaluation metrics, and open challenges.",
      "ai_keywords": [
        "text watermarking",
        "model watermarking",
        "model fingerprinting",
        "fingerprint transfer",
        "fingerprint removal",
        "evaluation metrics",
        "effectiveness",
        "harmlessness",
        "robustness",
        "stealthiness",
        "reliability"
      ]
    },
    "publishedAt": "2025-08-15T11:50:20.000Z",
    "title": "Copyright Protection for Large Language Models: A Survey of Methods,\n  Challenges, and Trends",
    "summary": "Copyright protection for large language models is of critical importance,\ngiven their substantial development costs, proprietary value, and potential for\nmisuse. Existing surveys have predominantly focused on techniques for tracing\nLLM-generated content-namely, text watermarking-while a systematic exploration\nof methods for protecting the models themselves (i.e., model watermarking and\nmodel fingerprinting) remains absent. Moreover, the relationships and\ndistinctions among text watermarking, model watermarking, and model\nfingerprinting have not been comprehensively clarified. This work presents a\ncomprehensive survey of the current state of LLM copyright protection\ntechnologies, with a focus on model fingerprinting, covering the following\naspects: (1) clarifying the conceptual connection from text watermarking to\nmodel watermarking and fingerprinting, and adopting a unified terminology that\nincorporates model watermarking into the broader fingerprinting framework; (2)\nproviding an overview and comparison of diverse text watermarking techniques,\nhighlighting cases where such methods can function as model fingerprinting; (3)\nsystematically categorizing and comparing existing model fingerprinting\napproaches for LLM copyright protection; (4) presenting, for the first time,\ntechniques for fingerprint transfer and fingerprint removal; (5) summarizing\nevaluation metrics for model fingerprints, including effectiveness,\nharmlessness, robustness, stealthiness, and reliability; and (6) discussing\nopen challenges and future research directions. This survey aims to offer\nresearchers a thorough understanding of both text watermarking and model\nfingerprinting technologies in the era of LLMs, thereby fostering further\nadvances in protecting their intellectual property.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.11548.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6746d09dfb7a1de1dca3cc21",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6746d09dfb7a1de1dca3cc21/uGNJVBm3g_O6N_R77yoMs.png",
      "fullname": "Zhebo Wang",
      "name": "BreynaldDva",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2508.10830",
      "authors": [
        {
          "_id": "689e9b67a4caabb4320e5d1d",
          "name": "Kai Li",
          "hidden": false
        },
        {
          "_id": "689e9b67a4caabb4320e5d1e",
          "name": "Guo Chen",
          "hidden": false
        },
        {
          "_id": "689e9b67a4caabb4320e5d1f",
          "name": "Wendi Sang",
          "hidden": false
        },
        {
          "_id": "689e9b67a4caabb4320e5d20",
          "name": "Yi Luo",
          "hidden": false
        },
        {
          "_id": "689e9b67a4caabb4320e5d21",
          "name": "Zhuo Chen",
          "hidden": false
        },
        {
          "_id": "689e9b67a4caabb4320e5d22",
          "name": "Shuai Wang",
          "hidden": false
        },
        {
          "_id": "689e9b67a4caabb4320e5d23",
          "name": "Shulin He",
          "hidden": false
        },
        {
          "_id": "689e9b67a4caabb4320e5d24",
          "name": "Zhong-Qiu Wang",
          "hidden": false
        },
        {
          "_id": "689e9b67a4caabb4320e5d25",
          "name": "Andong Li",
          "hidden": false
        },
        {
          "_id": "689e9b67a4caabb4320e5d26",
          "name": "Zhiyong Wu",
          "hidden": false
        },
        {
          "_id": "689e9b67a4caabb4320e5d27",
          "name": "Xiaolin Hu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-14T16:54:34.000Z",
      "submittedOnDailyAt": "2025-08-20T06:05:33.367Z",
      "title": "Advances in Speech Separation: Techniques, Challenges, and Future Trends",
      "submittedOnDailyBy": {
        "_id": "6387676c23da90491eb9fb16",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669818175965-noauth.jpeg",
        "isPro": false,
        "fullname": "Kai Li",
        "user": "JusperLee",
        "type": "user"
      },
      "summary": "The field of speech separation, addressing the \"cocktail party problem\", has\nseen revolutionary advances with DNNs. Speech separation enhances clarity in\ncomplex acoustic environments and serves as crucial pre-processing for speech\nrecognition and speaker recognition. However, current literature focuses\nnarrowly on specific architectures or isolated approaches, creating fragmented\nunderstanding. This survey addresses this gap by providing systematic\nexamination of DNN-based speech separation techniques. Our work differentiates\nitself through: (I) Comprehensive perspective: We systematically investigate\nlearning paradigms, separation scenarios with known/unknown speakers,\ncomparative analysis of supervised/self-supervised/unsupervised frameworks, and\narchitectural components from encoders to estimation strategies. (II)\nTimeliness: Coverage of cutting-edge developments ensures access to current\ninnovations and benchmarks. (III) Unique insights: Beyond summarization, we\nevaluate technological trajectories, identify emerging patterns, and highlight\npromising directions including domain-robust frameworks, efficient\narchitectures, multimodal integration, and novel self-supervised paradigms.\n(IV) Fair evaluation: We provide quantitative evaluations on standard datasets,\nrevealing true capabilities and limitations of different methods. This\ncomprehensive survey serves as an accessible reference for experienced\nresearchers and newcomers navigating speech separation's complex landscape.",
      "upvotes": 5,
      "discussionId": "689e9b68a4caabb4320e5d28",
      "projectPage": "https://cslikai.cn/Speech-Separation-Paper-Tutorial",
      "githubRepo": "https://github.com/JusperLee/Speech-Separation-Paper-Tutorial",
      "ai_summary": "A survey of DNN-based speech separation techniques, covering learning paradigms, separation scenarios, and architectural components, with a focus on current advancements and promising future directions.",
      "ai_keywords": [
        "DNNs",
        "speech separation",
        "cocktail party problem",
        "learning paradigms",
        "separation scenarios",
        "supervised frameworks",
        "self-supervised frameworks",
        "unsupervised frameworks",
        "encoders",
        "estimation strategies",
        "domain-robust frameworks",
        "efficient architectures",
        "multimodal integration",
        "novel self-supervised paradigms"
      ]
    },
    "publishedAt": "2025-08-14T12:54:34.000Z",
    "title": "Advances in Speech Separation: Techniques, Challenges, and Future Trends",
    "summary": "The field of speech separation, addressing the \"cocktail party problem\", has\nseen revolutionary advances with DNNs. Speech separation enhances clarity in\ncomplex acoustic environments and serves as crucial pre-processing for speech\nrecognition and speaker recognition. However, current literature focuses\nnarrowly on specific architectures or isolated approaches, creating fragmented\nunderstanding. This survey addresses this gap by providing systematic\nexamination of DNN-based speech separation techniques. Our work differentiates\nitself through: (I) Comprehensive perspective: We systematically investigate\nlearning paradigms, separation scenarios with known/unknown speakers,\ncomparative analysis of supervised/self-supervised/unsupervised frameworks, and\narchitectural components from encoders to estimation strategies. (II)\nTimeliness: Coverage of cutting-edge developments ensures access to current\ninnovations and benchmarks. (III) Unique insights: Beyond summarization, we\nevaluate technological trajectories, identify emerging patterns, and highlight\npromising directions including domain-robust frameworks, efficient\narchitectures, multimodal integration, and novel self-supervised paradigms.\n(IV) Fair evaluation: We provide quantitative evaluations on standard datasets,\nrevealing true capabilities and limitations of different methods. This\ncomprehensive survey serves as an accessible reference for experienced\nresearchers and newcomers navigating speech separation's complex landscape.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.10830.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6387676c23da90491eb9fb16",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669818175965-noauth.jpeg",
      "fullname": "Kai Li",
      "name": "JusperLee",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 24
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.12669",
      "authors": [
        {
          "_id": "68a53eea6cf0bf898542ecb6",
          "name": "Bishanka Seal",
          "hidden": false
        },
        {
          "_id": "68a53eea6cf0bf898542ecb7",
          "name": "Rahul Seetharaman",
          "hidden": false
        },
        {
          "_id": "68a53eea6cf0bf898542ecb8",
          "name": "Aman Bansal",
          "hidden": false
        },
        {
          "_id": "68a53eea6cf0bf898542ecb9",
          "name": "Abhilash Nandy",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-18T07:02:59.000Z",
      "submittedOnDailyAt": "2025-08-20T01:55:07.512Z",
      "title": "Leveraging Large Language Models for Predictive Analysis of Human Misery",
      "submittedOnDailyBy": {
        "_id": "5f89da6c5d083370c711f37c",
        "avatarUrl": "/avatars/c2f17a4a636973817fd5da2ae6dbaac3.svg",
        "isPro": false,
        "fullname": "Abhilash Nandy",
        "user": "abhi1nandy2",
        "type": "user"
      },
      "summary": "This study investigates the use of Large Language Models (LLMs) for\npredicting human-perceived misery scores from natural language descriptions of\nreal-world scenarios. The task is framed as a regression problem, where the\nmodel assigns a scalar value from 0 to 100 to each input statement. We evaluate\nmultiple prompting strategies, including zero-shot, fixed-context few-shot, and\nretrieval-based prompting using BERT sentence embeddings. Few-shot approaches\nconsistently outperform zero-shot baselines, underscoring the value of\ncontextual examples in affective prediction. To move beyond static evaluation,\nwe introduce the \"Misery Game Show\", a novel gamified framework inspired by a\ntelevision format. It tests LLMs through structured rounds involving ordinal\ncomparison, binary classification, scalar estimation, and feedback-driven\nreasoning. This setup enables us to assess not only predictive accuracy but\nalso the model's ability to adapt based on corrective feedback. The gamified\nevaluation highlights the broader potential of LLMs in dynamic emotional\nreasoning tasks beyond standard regression. Code and data link:\nhttps://github.com/abhi1nandy2/Misery_Data_Exps_GitHub",
      "upvotes": 3,
      "discussionId": "68a53eeb6cf0bf898542ecba",
      "ai_summary": "LLMs predict misery scores from text using various prompting strategies, with few-shot approaches outperforming zero-shot, and a gamified framework assessing their adaptability in emotional reasoning tasks.",
      "ai_keywords": [
        "Large Language Models",
        "LLMs",
        "regression problem",
        "zero-shot",
        "few-shot",
        "BERT sentence embeddings",
        "Misery Game Show",
        "ordinal comparison",
        "binary classification",
        "scalar estimation",
        "feedback-driven reasoning"
      ]
    },
    "publishedAt": "2025-08-18T03:02:59.000Z",
    "title": "Leveraging Large Language Models for Predictive Analysis of Human Misery",
    "summary": "This study investigates the use of Large Language Models (LLMs) for\npredicting human-perceived misery scores from natural language descriptions of\nreal-world scenarios. The task is framed as a regression problem, where the\nmodel assigns a scalar value from 0 to 100 to each input statement. We evaluate\nmultiple prompting strategies, including zero-shot, fixed-context few-shot, and\nretrieval-based prompting using BERT sentence embeddings. Few-shot approaches\nconsistently outperform zero-shot baselines, underscoring the value of\ncontextual examples in affective prediction. To move beyond static evaluation,\nwe introduce the \"Misery Game Show\", a novel gamified framework inspired by a\ntelevision format. It tests LLMs through structured rounds involving ordinal\ncomparison, binary classification, scalar estimation, and feedback-driven\nreasoning. This setup enables us to assess not only predictive accuracy but\nalso the model's ability to adapt based on corrective feedback. The gamified\nevaluation highlights the broader potential of LLMs in dynamic emotional\nreasoning tasks beyond standard regression. Code and data link:\nhttps://github.com/abhi1nandy2/Misery_Data_Exps_GitHub",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.12669.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f89da6c5d083370c711f37c",
      "avatarUrl": "/avatars/c2f17a4a636973817fd5da2ae6dbaac3.svg",
      "fullname": "Abhilash Nandy",
      "name": "abhi1nandy2",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.04324",
      "authors": [
        {
          "_id": "68a56e446cf0bf898542ed36",
          "name": "Xiaoxuan He",
          "hidden": false
        },
        {
          "_id": "68a56e446cf0bf898542ed37",
          "name": "Siming Fu",
          "hidden": false
        },
        {
          "_id": "68a56e446cf0bf898542ed38",
          "name": "Yuke Zhao",
          "hidden": false
        },
        {
          "_id": "68a56e446cf0bf898542ed39",
          "name": "Wanli Li",
          "hidden": false
        },
        {
          "_id": "68a56e446cf0bf898542ed3a",
          "name": "Jian Yang",
          "hidden": false
        },
        {
          "_id": "68a56e446cf0bf898542ed3b",
          "name": "Dacheng Yin",
          "hidden": false
        },
        {
          "_id": "68a56e446cf0bf898542ed3c",
          "name": "Fengyun Rao",
          "hidden": false
        },
        {
          "_id": "68a56e446cf0bf898542ed3d",
          "name": "Bo Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-06T11:10:39.000Z",
      "submittedOnDailyAt": "2025-08-20T05:16:03.933Z",
      "title": "TempFlow-GRPO: When Timing Matters for GRPO in Flow Models",
      "submittedOnDailyBy": {
        "_id": "6485dd6d07a2c1915060f603",
        "avatarUrl": "/avatars/8594d647359a7d19ab29b8ec91d1444e.svg",
        "isPro": false,
        "fullname": "fu",
        "user": "simingfu",
        "type": "user"
      },
      "summary": "Recent flow matching models for text-to-image generation have achieved\nremarkable quality, yet their integration with reinforcement learning for human\npreference alignment remains suboptimal, hindering fine-grained reward-based\noptimization. We observe that the key impediment to effective GRPO training of\nflow models is the temporal uniformity assumption in existing approaches:\nsparse terminal rewards with uniform credit assignment fail to capture the\nvarying criticality of decisions across generation timesteps, resulting in\ninefficient exploration and suboptimal convergence. To remedy this shortcoming,\nwe introduce TempFlow-GRPO (Temporal Flow GRPO), a principled GRPO\nframework that captures and exploits the temporal structure inherent in\nflow-based generation. TempFlow-GRPO introduces two key innovations: (i) a\ntrajectory branching mechanism that provides process rewards by concentrating\nstochasticity at designated branching points, enabling precise credit\nassignment without requiring specialized intermediate reward models; and (ii) a\nnoise-aware weighting scheme that modulates policy optimization according to\nthe intrinsic exploration potential of each timestep, prioritizing learning\nduring high-impact early stages while ensuring stable refinement in later\nphases. These innovations endow the model with temporally-aware optimization\nthat respects the underlying generative dynamics, leading to state-of-the-art\nperformance in human preference alignment and standard text-to-image\nbenchmarks.",
      "upvotes": 3,
      "discussionId": "68a56e446cf0bf898542ed3e",
      "projectPage": "https://tempflowgrpo.github.io/",
      "githubRepo": "https://github.com/Shredded-Pork/TempFlow-GRPO",
      "ai_summary": "TempFlow-GRPO enhances text-to-image generation by addressing temporal credit assignment and noise-aware optimization in flow models, improving human preference alignment and benchmark performance.",
      "ai_keywords": [
        "flow matching models",
        "text-to-image generation",
        "reinforcement learning",
        "human preference alignment",
        "GRPO",
        "temporal uniformity assumption",
        "trajectory branching mechanism",
        "process rewards",
        "noise-aware weighting scheme",
        "temporally-aware optimization",
        "generative dynamics"
      ],
      "githubStars": 97
    },
    "publishedAt": "2025-08-06T07:10:39.000Z",
    "title": "TempFlow-GRPO: When Timing Matters for GRPO in Flow Models",
    "summary": "Recent flow matching models for text-to-image generation have achieved\nremarkable quality, yet their integration with reinforcement learning for human\npreference alignment remains suboptimal, hindering fine-grained reward-based\noptimization. We observe that the key impediment to effective GRPO training of\nflow models is the temporal uniformity assumption in existing approaches:\nsparse terminal rewards with uniform credit assignment fail to capture the\nvarying criticality of decisions across generation timesteps, resulting in\ninefficient exploration and suboptimal convergence. To remedy this shortcoming,\nwe introduce TempFlow-GRPO (Temporal Flow GRPO), a principled GRPO\nframework that captures and exploits the temporal structure inherent in\nflow-based generation. TempFlow-GRPO introduces two key innovations: (i) a\ntrajectory branching mechanism that provides process rewards by concentrating\nstochasticity at designated branching points, enabling precise credit\nassignment without requiring specialized intermediate reward models; and (ii) a\nnoise-aware weighting scheme that modulates policy optimization according to\nthe intrinsic exploration potential of each timestep, prioritizing learning\nduring high-impact early stages while ensuring stable refinement in later\nphases. These innovations endow the model with temporally-aware optimization\nthat respects the underlying generative dynamics, leading to state-of-the-art\nperformance in human preference alignment and standard text-to-image\nbenchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.04324.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6485dd6d07a2c1915060f603",
      "avatarUrl": "/avatars/8594d647359a7d19ab29b8ec91d1444e.svg",
      "fullname": "fu",
      "name": "simingfu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.13139",
      "authors": [
        {
          "_id": "68a553016cf0bf898542eccd",
          "name": "Ling-Hao Chen",
          "hidden": false
        },
        {
          "_id": "68a553016cf0bf898542ecce",
          "name": "Yuhong Zhang",
          "hidden": false
        },
        {
          "_id": "68a553016cf0bf898542eccf",
          "name": "Zixin Yin",
          "hidden": false
        },
        {
          "_id": "68a553016cf0bf898542ecd0",
          "name": "Zhiyang Dou",
          "hidden": false
        },
        {
          "_id": "68a553016cf0bf898542ecd1",
          "name": "Xin Chen",
          "hidden": false
        },
        {
          "_id": "68a553016cf0bf898542ecd2",
          "name": "Jingbo Wang",
          "hidden": false
        },
        {
          "_id": "68a553016cf0bf898542ecd3",
          "name": "Taku Komura",
          "hidden": false
        },
        {
          "_id": "68a553016cf0bf898542ecd4",
          "name": "Lei Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-18T17:50:31.000Z",
      "submittedOnDailyAt": "2025-08-20T03:34:25.932Z",
      "title": "Motion2Motion: Cross-topology Motion Transfer with Sparse Correspondence",
      "submittedOnDailyBy": {
        "_id": "63109a4d61cab0446e48c83b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63109a4d61cab0446e48c83b/JQlVkQp0ok586ND1GmB0w.png",
        "isPro": false,
        "fullname": "Ling-Hao Chen",
        "user": "EvanTHU",
        "type": "user"
      },
      "summary": "This work studies the challenge of transfer animations between characters\nwhose skeletal topologies differ substantially. While many techniques have\nadvanced retargeting techniques in decades, transfer motions across diverse\ntopologies remains less-explored. The primary obstacle lies in the inherent\ntopological inconsistency between source and target skeletons, which restricts\nthe establishment of straightforward one-to-one bone correspondences. Besides,\nthe current lack of large-scale paired motion datasets spanning different\ntopological structures severely constrains the development of data-driven\napproaches. To address these limitations, we introduce Motion2Motion, a novel,\ntraining-free framework. Simply yet effectively, Motion2Motion works with only\none or a few example motions on the target skeleton, by accessing a sparse set\nof bone correspondences between the source and target skeletons. Through\ncomprehensive qualitative and quantitative evaluations, we demonstrate that\nMotion2Motion achieves efficient and reliable performance in both\nsimilar-skeleton and cross-species skeleton transfer scenarios. The practical\nutility of our approach is further evidenced by its successful integration in\ndownstream applications and user interfaces, highlighting its potential for\nindustrial applications. Code and data are available at\nhttps://lhchen.top/Motion2Motion.",
      "upvotes": 2,
      "discussionId": "68a553026cf0bf898542ecd5",
      "ai_summary": "Motion2Motion is a training-free framework that efficiently transfers animations between characters with different skeletal topologies using sparse bone correspondences.",
      "ai_keywords": [
        "Motion2Motion",
        "retargeting techniques",
        "skeletal topologies",
        "bone correspondences",
        "similar-skeleton",
        "cross-species skeleton transfer"
      ]
    },
    "publishedAt": "2025-08-18T13:50:31.000Z",
    "title": "Motion2Motion: Cross-topology Motion Transfer with Sparse Correspondence",
    "summary": "This work studies the challenge of transfer animations between characters\nwhose skeletal topologies differ substantially. While many techniques have\nadvanced retargeting techniques in decades, transfer motions across diverse\ntopologies remains less-explored. The primary obstacle lies in the inherent\ntopological inconsistency between source and target skeletons, which restricts\nthe establishment of straightforward one-to-one bone correspondences. Besides,\nthe current lack of large-scale paired motion datasets spanning different\ntopological structures severely constrains the development of data-driven\napproaches. To address these limitations, we introduce Motion2Motion, a novel,\ntraining-free framework. Simply yet effectively, Motion2Motion works with only\none or a few example motions on the target skeleton, by accessing a sparse set\nof bone correspondences between the source and target skeletons. Through\ncomprehensive qualitative and quantitative evaluations, we demonstrate that\nMotion2Motion achieves efficient and reliable performance in both\nsimilar-skeleton and cross-species skeleton transfer scenarios. The practical\nutility of our approach is further evidenced by its successful integration in\ndownstream applications and user interfaces, highlighting its potential for\nindustrial applications. Code and data are available at\nhttps://lhchen.top/Motion2Motion.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.13139.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63109a4d61cab0446e48c83b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63109a4d61cab0446e48c83b/JQlVkQp0ok586ND1GmB0w.png",
      "fullname": "Ling-Hao Chen",
      "name": "EvanTHU",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.04326",
      "authors": [
        {
          "_id": "68a43b91b65388761d074602",
          "user": {
            "_id": "67b1cf0b8a1b0f0b4880e56f",
            "avatarUrl": "/avatars/bdaa4d9cd2280ceccc84c24a5ba1b5b4.svg",
            "isPro": false,
            "fullname": "Ke Li",
            "user": "cocolinux",
            "type": "user"
          },
          "name": "Ke Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-19T15:58:49.505Z",
          "hidden": false
        },
        {
          "_id": "68a43b91b65388761d074603",
          "name": "Mana Masuda",
          "hidden": false
        },
        {
          "_id": "68a43b91b65388761d074604",
          "name": "Susanne Schmidt",
          "hidden": false
        },
        {
          "_id": "68a43b91b65388761d074605",
          "name": "Shohei Mori",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/67b1cf0b8a1b0f0b4880e56f/xFJaNueE2DK5UhlfTH-sw.jpeg"
      ],
      "publishedAt": "2025-08-06T11:14:06.000Z",
      "submittedOnDailyAt": "2025-08-20T06:15:15.882Z",
      "title": "Radiance Fields in XR: A Survey on How Radiance Fields are Envisioned\n  and Addressed for XR Research",
      "submittedOnDailyBy": {
        "_id": "67b1cf0b8a1b0f0b4880e56f",
        "avatarUrl": "/avatars/bdaa4d9cd2280ceccc84c24a5ba1b5b4.svg",
        "isPro": false,
        "fullname": "Ke Li",
        "user": "cocolinux",
        "type": "user"
      },
      "summary": "The development of radiance fields (RF), such as 3D Gaussian Splatting (3DGS)\nand Neural Radiance Fields (NeRF), has revolutionized interactive\nphotorealistic view synthesis and presents enormous opportunities for XR\nresearch and applications. However, despite the exponential growth of RF\nresearch, RF-related contributions to the XR community remain sparse. To better\nunderstand this research gap, we performed a systematic survey of current RF\nliterature to analyze (i) how RF is envisioned for XR applications, (ii) how\nthey have already been implemented, and (iii) the remaining research gaps. We\ncollected 365 RF contributions related to XR from computer vision, computer\ngraphics, robotics, multimedia, human-computer interaction, and XR communities,\nseeking to answer the above research questions. Among the 365 papers, we\nperformed an analysis of 66 papers that already addressed a detailed aspect of\nRF research for XR. With this survey, we extended and positioned XR-specific RF\nresearch topics in the broader RF research field and provide a helpful resource\nfor the XR community to navigate within the rapid development of RF research.",
      "upvotes": 2,
      "discussionId": "68a43b9db65388761d074606",
      "projectPage": "https://mediated-reality.github.io/rf4xr/papers/li_tvcg25/",
      "githubRepo": "https://github.com/mediated-reality/awesome-rf4xr",
      "ai_summary": "A systematic survey of radiance fields in XR applications identifies implementation details, research gaps, and future directions within the broader RF research field.",
      "ai_keywords": [
        "radiance fields",
        "3D Gaussian Splatting",
        "Neural Radiance Fields",
        "XR",
        "computer vision",
        "computer graphics",
        "robotics",
        "multimedia",
        "human-computer interaction"
      ]
    },
    "publishedAt": "2025-08-06T07:14:06.000Z",
    "title": "Radiance Fields in XR: A Survey on How Radiance Fields are Envisioned\n  and Addressed for XR Research",
    "summary": "The development of radiance fields (RF), such as 3D Gaussian Splatting (3DGS)\nand Neural Radiance Fields (NeRF), has revolutionized interactive\nphotorealistic view synthesis and presents enormous opportunities for XR\nresearch and applications. However, despite the exponential growth of RF\nresearch, RF-related contributions to the XR community remain sparse. To better\nunderstand this research gap, we performed a systematic survey of current RF\nliterature to analyze (i) how RF is envisioned for XR applications, (ii) how\nthey have already been implemented, and (iii) the remaining research gaps. We\ncollected 365 RF contributions related to XR from computer vision, computer\ngraphics, robotics, multimedia, human-computer interaction, and XR communities,\nseeking to answer the above research questions. Among the 365 papers, we\nperformed an analysis of 66 papers that already addressed a detailed aspect of\nRF research for XR. With this survey, we extended and positioned XR-specific RF\nresearch topics in the broader RF research field and provide a helpful resource\nfor the XR community to navigate within the rapid development of RF research.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/67b1cf0b8a1b0f0b4880e56f/xFJaNueE2DK5UhlfTH-sw.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.04326.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67b1cf0b8a1b0f0b4880e56f",
      "avatarUrl": "/avatars/bdaa4d9cd2280ceccc84c24a5ba1b5b4.svg",
      "fullname": "Ke Li",
      "name": "cocolinux",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2508.13992",
      "authors": [
        {
          "_id": "68a539856cf0bf898542ec92",
          "name": "Sonal Kumar",
          "hidden": false
        },
        {
          "_id": "68a539856cf0bf898542ec93",
          "name": "imon Sedlek",
          "hidden": false
        },
        {
          "_id": "68a539856cf0bf898542ec94",
          "name": "Vaibhavi Lokegaonkar",
          "hidden": false
        },
        {
          "_id": "68a539856cf0bf898542ec95",
          "name": "Fernando Lpez",
          "hidden": false
        },
        {
          "_id": "68a539856cf0bf898542ec96",
          "name": "Wenyi Yu",
          "hidden": false
        },
        {
          "_id": "68a539856cf0bf898542ec97",
          "name": "Nishit Anand",
          "hidden": false
        },
        {
          "_id": "68a539856cf0bf898542ec98",
          "name": "Hyeonggon Ryu",
          "hidden": false
        },
        {
          "_id": "68a539856cf0bf898542ec99",
          "name": "Lichang Chen",
          "hidden": false
        },
        {
          "_id": "68a539856cf0bf898542ec9a",
          "name": "Maxim Plika",
          "hidden": false
        },
        {
          "_id": "68a539856cf0bf898542ec9b",
          "name": "Miroslav Hlavek",
          "hidden": false
        },
        {
          "_id": "68a539856cf0bf898542ec9c",
          "name": "William Fineas Ellingwood",
          "hidden": false
        },
        {
          "_id": "68a539856cf0bf898542ec9d",
          "name": "Sathvik Udupa",
          "hidden": false
        },
        {
          "_id": "68a539856cf0bf898542ec9e",
          "name": "Siyuan Hou",
          "hidden": false
        },
        {
          "_id": "68a539856cf0bf898542ec9f",
          "name": "Allison Ferner",
          "hidden": false
        },
        {
          "_id": "68a539856cf0bf898542eca0",
          "name": "Sara Barahona",
          "hidden": false
        },
        {
          "_id": "68a539856cf0bf898542eca1",
          "name": "Cecilia Bolaos",
          "hidden": false
        },
        {
          "_id": "68a539856cf0bf898542eca2",
          "name": "Satish Rahi",
          "hidden": false
        },
        {
          "_id": "68a539856cf0bf898542eca3",
          "name": "Laura Herrera-Alarcn",
          "hidden": false
        },
        {
          "_id": "68a539856cf0bf898542eca4",
          "name": "Satvik Dixit",
          "hidden": false
        },
        {
          "_id": "68a539856cf0bf898542eca5",
          "name": "Siddhi Patil",
          "hidden": false
        },
        {
          "_id": "68a539856cf0bf898542eca6",
          "name": "Soham Deshmukh",
          "hidden": false
        },
        {
          "_id": "68a539856cf0bf898542eca7",
          "name": "Lasha Koroshinadze",
          "hidden": false
        },
        {
          "_id": "68a539856cf0bf898542eca8",
          "name": "Yao Liu",
          "hidden": false
        },
        {
          "_id": "68a539856cf0bf898542eca9",
          "name": "Leibny Paola Garcia Perera",
          "hidden": false
        },
        {
          "_id": "68a539856cf0bf898542ecaa",
          "name": "Eleni Zanou",
          "hidden": false
        },
        {
          "_id": "68a539856cf0bf898542ecab",
          "name": "Themos Stafylakis",
          "hidden": false
        },
        {
          "_id": "68a539856cf0bf898542ecac",
          "name": "Joon Son Chung",
          "hidden": false
        },
        {
          "_id": "68a539856cf0bf898542ecad",
          "name": "David Harwath",
          "hidden": false
        },
        {
          "_id": "68a539856cf0bf898542ecae",
          "name": "Chao Zhang",
          "hidden": false
        },
        {
          "_id": "68a539856cf0bf898542ecaf",
          "name": "Dinesh Manocha",
          "hidden": false
        },
        {
          "_id": "68a539856cf0bf898542ecb0",
          "name": "Alicia Lozano-Diez",
          "hidden": false
        },
        {
          "_id": "68a539856cf0bf898542ecb1",
          "name": "Santosh Kesiraju",
          "hidden": false
        },
        {
          "_id": "68a539856cf0bf898542ecb2",
          "name": "Sreyan Ghosh",
          "hidden": false
        },
        {
          "_id": "68a539856cf0bf898542ecb3",
          "name": "Ramani Duraiswami",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-19T16:33:49.000Z",
      "submittedOnDailyAt": "2025-08-20T01:34:00.837Z",
      "title": "MMAU-Pro: A Challenging and Comprehensive Benchmark for Holistic\n  Evaluation of Audio General Intelligence",
      "submittedOnDailyBy": {
        "_id": "62c9664eb34e600d7eaa4beb",
        "avatarUrl": "/avatars/ca23ecdec2d31c99ecce97d9b180ae0c.svg",
        "isPro": false,
        "fullname": "Ghosh",
        "user": "Sreyan88",
        "type": "user"
      },
      "summary": "Audio comprehension-including speech, non-speech sounds, and music-is\nessential for achieving human-level intelligence. Consequently, AI agents must\ndemonstrate holistic audio understanding to qualify as generally intelligent.\nHowever, evaluating auditory intelligence comprehensively remains challenging.\nTo address this gap, we introduce MMAU-Pro, the most comprehensive and\nrigorously curated benchmark for assessing audio intelligence in AI systems.\nMMAU-Pro contains 5,305 instances, where each instance has one or more audios\npaired with human expert-generated question-answer pairs, spanning speech,\nsound, music, and their combinations. Unlike existing benchmarks, MMAU-Pro\nevaluates auditory intelligence across 49 unique skills and multiple complex\ndimensions, including long-form audio comprehension, spatial audio reasoning,\nmulti-audio understanding, among others. All questions are meticulously\ndesigned to require deliberate multi-hop reasoning, including both\nmultiple-choice and open-ended response formats. Importantly, audio data is\nsourced directly ``from the wild\" rather than from existing datasets with known\ndistributions. We evaluate 22 leading open-source and proprietary multimodal AI\nmodels, revealing significant limitations: even state-of-the-art models such as\nGemini 2.5 Flash and Audio Flamingo 3 achieve only 59.2% and 51.7% accuracy,\nrespectively, approaching random performance in multiple categories. Our\nextensive analysis highlights specific shortcomings and provides novel\ninsights, offering actionable perspectives for the community to enhance future\nAI systems' progression toward audio general intelligence. The benchmark and\ncode is available at https://sonalkum.github.io/mmau-pro.",
      "upvotes": 1,
      "discussionId": "68a539866cf0bf898542ecb4",
      "ai_summary": "MMAU-Pro is a comprehensive benchmark for evaluating audio intelligence in AI systems, assessing 49 unique skills across speech, sound, music, and their combinations, revealing significant limitations in current models.",
      "ai_keywords": [
        "MMAU-Pro",
        "long-form audio comprehension",
        "spatial audio reasoning",
        "multi-audio understanding",
        "multi-hop reasoning",
        "multimodal AI models",
        "Gemini 2.5 Flash",
        "Audio Flamingo 3"
      ]
    },
    "publishedAt": "2025-08-19T12:33:49.000Z",
    "title": "MMAU-Pro: A Challenging and Comprehensive Benchmark for Holistic\n  Evaluation of Audio General Intelligence",
    "summary": "Audio comprehension-including speech, non-speech sounds, and music-is\nessential for achieving human-level intelligence. Consequently, AI agents must\ndemonstrate holistic audio understanding to qualify as generally intelligent.\nHowever, evaluating auditory intelligence comprehensively remains challenging.\nTo address this gap, we introduce MMAU-Pro, the most comprehensive and\nrigorously curated benchmark for assessing audio intelligence in AI systems.\nMMAU-Pro contains 5,305 instances, where each instance has one or more audios\npaired with human expert-generated question-answer pairs, spanning speech,\nsound, music, and their combinations. Unlike existing benchmarks, MMAU-Pro\nevaluates auditory intelligence across 49 unique skills and multiple complex\ndimensions, including long-form audio comprehension, spatial audio reasoning,\nmulti-audio understanding, among others. All questions are meticulously\ndesigned to require deliberate multi-hop reasoning, including both\nmultiple-choice and open-ended response formats. Importantly, audio data is\nsourced directly ``from the wild\" rather than from existing datasets with known\ndistributions. We evaluate 22 leading open-source and proprietary multimodal AI\nmodels, revealing significant limitations: even state-of-the-art models such as\nGemini 2.5 Flash and Audio Flamingo 3 achieve only 59.2% and 51.7% accuracy,\nrespectively, approaching random performance in multiple categories. Our\nextensive analysis highlights specific shortcomings and provides novel\ninsights, offering actionable perspectives for the community to enhance future\nAI systems' progression toward audio general intelligence. The benchmark and\ncode is available at https://sonalkum.github.io/mmau-pro.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.13992.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c9664eb34e600d7eaa4beb",
      "avatarUrl": "/avatars/ca23ecdec2d31c99ecce97d9b180ae0c.svg",
      "fullname": "Ghosh",
      "name": "Sreyan88",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.04038",
      "authors": [
        {
          "_id": "6899e842996e0a645a52a801",
          "user": {
            "_id": "6535c28d9ded17e619e4aec2",
            "avatarUrl": "/avatars/fc6f3238d2be5da77ec6acd218caee79.svg",
            "isPro": false,
            "fullname": "Zechen Li",
            "user": "zechenli03",
            "type": "user"
          },
          "name": "Zechen Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-13T07:22:02.583Z",
          "hidden": false
        },
        {
          "_id": "6899e842996e0a645a52a802",
          "name": "Baiyu Chen",
          "hidden": false
        },
        {
          "_id": "6899e842996e0a645a52a803",
          "name": "Hao Xue",
          "hidden": false
        },
        {
          "_id": "6899e842996e0a645a52a804",
          "name": "Flora D. Salim",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-06T02:57:57.000Z",
      "submittedOnDailyAt": "2025-08-20T05:57:22.702Z",
      "title": "ZARA: Zero-shot Motion Time-Series Analysis via Knowledge and Retrieval\n  Driven LLM Agents",
      "submittedOnDailyBy": {
        "_id": "67b5890b0d878eff1a36ce8d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Py6177rLRA6KQtafyAZ90.jpeg",
        "isPro": false,
        "fullname": "Breeze Chen",
        "user": "Breezelled",
        "type": "user"
      },
      "summary": "Motion sensor time-series are central to human activity recognition (HAR),\nwith applications in health, sports, and smart devices. However, existing\nmethods are trained for fixed activity sets and require costly retraining when\nnew behaviours or sensor setups appear. Recent attempts to use large language\nmodels (LLMs) for HAR, typically by converting signals into text or images,\nsuffer from limited accuracy and lack verifiable interpretability. We propose\nZARA, the first agent-based framework for zero-shot, explainable HAR directly\nfrom raw motion time-series. ZARA integrates an automatically derived pair-wise\nfeature knowledge base that captures discriminative statistics for every\nactivity pair, a multi-sensor retrieval module that surfaces relevant evidence,\nand a hierarchical agent pipeline that guides the LLM to iteratively select\nfeatures, draw on this evidence, and produce both activity predictions and\nnatural-language explanations. ZARA enables flexible and interpretable HAR\nwithout any fine-tuning or task-specific classifiers. Extensive experiments on\n8 HAR benchmarks show that ZARA achieves SOTA zero-shot performance, delivering\nclear reasoning while exceeding the strongest baselines by 2.53x in macro F1.\nAblation studies further confirm the necessity of each module, marking ZARA as\na promising step toward trustworthy, plug-and-play motion time-series analysis.\nOur codes are available at https://github.com/zechenli03/ZARA.",
      "upvotes": 1,
      "discussionId": "6899e843996e0a645a52a805",
      "githubRepo": "https://github.com/zechenli03/ZARA",
      "ai_summary": "ZARA is an agent-based framework for zero-shot, explainable human activity recognition from raw motion time-series, achieving state-of-the-art performance without fine-tuning.",
      "ai_keywords": [
        "agent-based framework",
        "zero-shot",
        "explainable HAR",
        "raw motion time-series",
        "feature knowledge base",
        "multi-sensor retrieval module",
        "hierarchical agent pipeline",
        "LLM",
        "macro F1",
        "plug-and-play motion time-series analysis"
      ]
    },
    "publishedAt": "2025-08-05T22:57:57.000Z",
    "title": "ZARA: Zero-shot Motion Time-Series Analysis via Knowledge and Retrieval\n  Driven LLM Agents",
    "summary": "Motion sensor time-series are central to human activity recognition (HAR),\nwith applications in health, sports, and smart devices. However, existing\nmethods are trained for fixed activity sets and require costly retraining when\nnew behaviours or sensor setups appear. Recent attempts to use large language\nmodels (LLMs) for HAR, typically by converting signals into text or images,\nsuffer from limited accuracy and lack verifiable interpretability. We propose\nZARA, the first agent-based framework for zero-shot, explainable HAR directly\nfrom raw motion time-series. ZARA integrates an automatically derived pair-wise\nfeature knowledge base that captures discriminative statistics for every\nactivity pair, a multi-sensor retrieval module that surfaces relevant evidence,\nand a hierarchical agent pipeline that guides the LLM to iteratively select\nfeatures, draw on this evidence, and produce both activity predictions and\nnatural-language explanations. ZARA enables flexible and interpretable HAR\nwithout any fine-tuning or task-specific classifiers. Extensive experiments on\n8 HAR benchmarks show that ZARA achieves SOTA zero-shot performance, delivering\nclear reasoning while exceeding the strongest baselines by 2.53x in macro F1.\nAblation studies further confirm the necessity of each module, marking ZARA as\na promising step toward trustworthy, plug-and-play motion time-series analysis.\nOur codes are available at https://github.com/zechenli03/ZARA.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.04038.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67b5890b0d878eff1a36ce8d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Py6177rLRA6KQtafyAZ90.jpeg",
      "fullname": "Breeze Chen",
      "name": "Breezelled",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]