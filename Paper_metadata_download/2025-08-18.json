[
  {
    "paper": {
      "id": "2508.11630",
      "authors": [
        {
          "_id": "68a286afa4caabb4320e63b6",
          "name": "Yi-Fan Zhang",
          "hidden": false
        },
        {
          "_id": "68a286afa4caabb4320e63b7",
          "name": "Xingyu Lu",
          "hidden": false
        },
        {
          "_id": "68a286afa4caabb4320e63b8",
          "name": "Shukang Yin",
          "hidden": false
        },
        {
          "_id": "68a286afa4caabb4320e63b9",
          "name": "Chaoyou Fu",
          "hidden": false
        },
        {
          "_id": "68a286afa4caabb4320e63ba",
          "name": "Wei Chen",
          "hidden": false
        },
        {
          "_id": "68a286afa4caabb4320e63bb",
          "name": "Xiao Hu",
          "hidden": false
        },
        {
          "_id": "68a286afa4caabb4320e63bc",
          "name": "Bin Wen",
          "hidden": false
        },
        {
          "_id": "68a286afa4caabb4320e63bd",
          "name": "Kaiyu Jiang",
          "hidden": false
        },
        {
          "_id": "68a286afa4caabb4320e63be",
          "name": "Changyi Liu",
          "hidden": false
        },
        {
          "_id": "68a286afa4caabb4320e63bf",
          "name": "Tianke Zhang",
          "hidden": false
        },
        {
          "_id": "68a286afa4caabb4320e63c0",
          "name": "Haonan Fan",
          "hidden": false
        },
        {
          "_id": "68a286afa4caabb4320e63c1",
          "name": "Kaibing Chen",
          "hidden": false
        },
        {
          "_id": "68a286afa4caabb4320e63c2",
          "name": "Jiankang Chen",
          "hidden": false
        },
        {
          "_id": "68a286afa4caabb4320e63c3",
          "name": "Haojie Ding",
          "hidden": false
        },
        {
          "_id": "68a286afa4caabb4320e63c4",
          "name": "Kaiyu Tang",
          "hidden": false
        },
        {
          "_id": "68a286afa4caabb4320e63c5",
          "name": "Zhang Zhang",
          "hidden": false
        },
        {
          "_id": "68a286afa4caabb4320e63c6",
          "name": "Liang Wang",
          "hidden": false
        },
        {
          "_id": "68a286afa4caabb4320e63c7",
          "name": "Fan Yang",
          "hidden": false
        },
        {
          "_id": "68a286afa4caabb4320e63c8",
          "name": "Tingting Gao",
          "hidden": false
        },
        {
          "_id": "68a286afa4caabb4320e63c9",
          "name": "Guorui Zhou",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/623d8ca4c29adf5ef6175615/EugCupc41u3ZdJyP6uqWm.png"
      ],
      "publishedAt": "2025-08-15T17:59:49.000Z",
      "submittedOnDailyAt": "2025-08-18T00:21:09.935Z",
      "title": "Thyme: Think Beyond Images",
      "submittedOnDailyBy": {
        "_id": "623d8ca4c29adf5ef6175615",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg",
        "isPro": false,
        "fullname": "Yi-Fan Zhang",
        "user": "yifanzhang114",
        "type": "user"
      },
      "summary": "Following OpenAI's introduction of the ``thinking with images'' concept,\nrecent efforts have explored stimulating the use of visual information in the\nreasoning process to enhance model performance in perception and reasoning\ntasks. However, to the best of our knowledge, no open-source work currently\noffers a feature set as rich as proprietary models (O3), which can perform\ndiverse image manipulations and simultaneously enhance logical reasoning\ncapabilities through code. In this paper, we make a preliminary attempt in this\ndirection by introducing Thyme (Think Beyond Images), a novel paradigm for\nenabling MLLMs to transcend existing ``think with images'' approaches by\nautonomously generating and executing diverse image processing and\ncomputational operations via executable code. This approach not only\nfacilitates a rich, on-the-fly set of image manipulations (e.g., cropping,\nrotation, contrast enhancement) but also allows for mathematical computations,\nall while maintaining high autonomy in deciding when and how to apply these\noperations. We activate this capability through a two-stage training strategy:\nan initial SFT on a curated dataset of 500K samples to teach code generation,\nfollowed by a RL phase to refine decision-making. For the RL stage, we manually\ncollect and design high-resolution question-answer pairs to increase the\nlearning difficulty, and we propose GRPO-ATS (Group Relative Policy\nOptimization with Adaptive Temperature Sampling), an algorithm that applies\ndistinct temperatures to text and code generation to balance reasoning\nexploration with code execution precision. We conduct extensive experimental\nanalysis and ablation studies. Comprehensive evaluations on nearly 20\nbenchmarks show that Thyme yields significant and consistent performance gains,\nparticularly in challenging high-resolution perception and complex reasoning\ntasks.",
      "upvotes": 20,
      "discussionId": "68a286afa4caabb4320e63ca",
      "projectPage": "https://thyme-vl.github.io/",
      "githubRepo": "https://github.com/yfzhang114/Thyme",
      "ai_summary": "Thyme, a novel paradigm, enables MLLMs to autonomously perform image manipulations and computations, enhancing performance in perception and reasoning tasks through a two-stage training strategy and GRPO-ATS algorithm.",
      "ai_keywords": [
        "MLLMs",
        "think with images",
        "image processing",
        "computational operations",
        "executable code",
        "SFT",
        "RL",
        "GRPO-ATS",
        "Group Relative Policy Optimization",
        "Adaptive Temperature Sampling"
      ],
      "githubStars": 53
    },
    "publishedAt": "2025-08-15T13:59:49.000Z",
    "title": "Thyme: Think Beyond Images",
    "summary": "Following OpenAI's introduction of the ``thinking with images'' concept,\nrecent efforts have explored stimulating the use of visual information in the\nreasoning process to enhance model performance in perception and reasoning\ntasks. However, to the best of our knowledge, no open-source work currently\noffers a feature set as rich as proprietary models (O3), which can perform\ndiverse image manipulations and simultaneously enhance logical reasoning\ncapabilities through code. In this paper, we make a preliminary attempt in this\ndirection by introducing Thyme (Think Beyond Images), a novel paradigm for\nenabling MLLMs to transcend existing ``think with images'' approaches by\nautonomously generating and executing diverse image processing and\ncomputational operations via executable code. This approach not only\nfacilitates a rich, on-the-fly set of image manipulations (e.g., cropping,\nrotation, contrast enhancement) but also allows for mathematical computations,\nall while maintaining high autonomy in deciding when and how to apply these\noperations. We activate this capability through a two-stage training strategy:\nan initial SFT on a curated dataset of 500K samples to teach code generation,\nfollowed by a RL phase to refine decision-making. For the RL stage, we manually\ncollect and design high-resolution question-answer pairs to increase the\nlearning difficulty, and we propose GRPO-ATS (Group Relative Policy\nOptimization with Adaptive Temperature Sampling), an algorithm that applies\ndistinct temperatures to text and code generation to balance reasoning\nexploration with code execution precision. We conduct extensive experimental\nanalysis and ablation studies. Comprehensive evaluations on nearly 20\nbenchmarks show that Thyme yields significant and consistent performance gains,\nparticularly in challenging high-resolution perception and complex reasoning\ntasks.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/623d8ca4c29adf5ef6175615/EugCupc41u3ZdJyP6uqWm.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.11630.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "623d8ca4c29adf5ef6175615",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg",
      "fullname": "Yi-Fan Zhang",
      "name": "yifanzhang114",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.11203",
      "authors": [
        {
          "_id": "68a29748a4caabb4320e640d",
          "name": "Seungmi Lee",
          "hidden": false
        },
        {
          "_id": "68a29748a4caabb4320e640e",
          "name": "Kwan Yun",
          "hidden": false
        },
        {
          "_id": "68a29748a4caabb4320e640f",
          "name": "Junyong Noh",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/639d445524af4747d8d2af52/okc2oEtYhUONdwsmLM3gf.jpeg"
      ],
      "publishedAt": "2025-08-15T04:29:46.000Z",
      "submittedOnDailyAt": "2025-08-18T01:36:55.327Z",
      "title": "StyleMM: Stylized 3D Morphable Face Model via Text-Driven Aligned Image\n  Translation",
      "submittedOnDailyBy": {
        "_id": "639d445524af4747d8d2af52",
        "avatarUrl": "/avatars/6ca870edc993fd3db6b0db4e4848ef7a.svg",
        "isPro": false,
        "fullname": "kwan yun",
        "user": "kwanY",
        "type": "user"
      },
      "summary": "We introduce StyleMM, a novel framework that can construct a stylized 3D\nMorphable Model (3DMM) based on user-defined text descriptions specifying a\ntarget style. Building upon a pre-trained mesh deformation network and a\ntexture generator for original 3DMM-based realistic human faces, our approach\nfine-tunes these models using stylized facial images generated via text-guided\nimage-to-image (i2i) translation with a diffusion model, which serve as\nstylization targets for the rendered mesh. To prevent undesired changes in\nidentity, facial alignment, or expressions during i2i translation, we introduce\na stylization method that explicitly preserves the facial attributes of the\nsource image. By maintaining these critical attributes during image\nstylization, the proposed approach ensures consistent 3D style transfer across\nthe 3DMM parameter space through image-based training. Once trained, StyleMM\nenables feed-forward generation of stylized face meshes with explicit control\nover shape, expression, and texture parameters, producing meshes with\nconsistent vertex connectivity and animatability. Quantitative and qualitative\nevaluations demonstrate that our approach outperforms state-of-the-art methods\nin terms of identity-level facial diversity and stylization capability. The\ncode and videos are available at\n[kwanyun.github.io/stylemm_page](kwanyun.github.io/stylemm_page).",
      "upvotes": 4,
      "discussionId": "68a29748a4caabb4320e6410",
      "projectPage": "https://kwanyun.github.io/stylemm_page/",
      "githubRepo": "https://github.com/kwanyun/EAS",
      "ai_summary": "StyleMM constructs stylized 3DMMs from text descriptions using a diffusion model for image-to-image translation while preserving facial attributes.",
      "ai_keywords": [
        "3D Morphable Model",
        "3DMM",
        "mesh deformation network",
        "texture generator",
        "text-guided image-to-image translation",
        "diffusion model",
        "stylization method",
        "facial attributes",
        "identity-level facial diversity",
        "stylization capability"
      ],
      "githubStars": 3
    },
    "publishedAt": "2025-08-15T00:29:46.000Z",
    "title": "StyleMM: Stylized 3D Morphable Face Model via Text-Driven Aligned Image\n  Translation",
    "summary": "We introduce StyleMM, a novel framework that can construct a stylized 3D\nMorphable Model (3DMM) based on user-defined text descriptions specifying a\ntarget style. Building upon a pre-trained mesh deformation network and a\ntexture generator for original 3DMM-based realistic human faces, our approach\nfine-tunes these models using stylized facial images generated via text-guided\nimage-to-image (i2i) translation with a diffusion model, which serve as\nstylization targets for the rendered mesh. To prevent undesired changes in\nidentity, facial alignment, or expressions during i2i translation, we introduce\na stylization method that explicitly preserves the facial attributes of the\nsource image. By maintaining these critical attributes during image\nstylization, the proposed approach ensures consistent 3D style transfer across\nthe 3DMM parameter space through image-based training. Once trained, StyleMM\nenables feed-forward generation of stylized face meshes with explicit control\nover shape, expression, and texture parameters, producing meshes with\nconsistent vertex connectivity and animatability. Quantitative and qualitative\nevaluations demonstrate that our approach outperforms state-of-the-art methods\nin terms of identity-level facial diversity and stylization capability. The\ncode and videos are available at\n[kwanyun.github.io/stylemm_page](kwanyun.github.io/stylemm_page).",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/639d445524af4747d8d2af52/okc2oEtYhUONdwsmLM3gf.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.11203.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "639d445524af4747d8d2af52",
      "avatarUrl": "/avatars/6ca870edc993fd3db6b0db4e4848ef7a.svg",
      "fullname": "kwan yun",
      "name": "kwanY",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.11116",
      "authors": [
        {
          "_id": "68a28285a4caabb4320e63ae",
          "user": {
            "_id": "63664c8fa2abcdf2fd6425ed",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63664c8fa2abcdf2fd6425ed/IywpB0DXZ_twkmZmVSCCD.jpeg",
            "isPro": false,
            "fullname": "Li Zhuoqun",
            "user": "lzq2021",
            "type": "user"
          },
          "name": "Zhuoqun Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-18T06:54:00.985Z",
          "hidden": false
        },
        {
          "_id": "68a28285a4caabb4320e63af",
          "name": "Xuanang Chen",
          "hidden": false
        },
        {
          "_id": "68a28285a4caabb4320e63b0",
          "name": "Hongyu Lin",
          "hidden": false
        },
        {
          "_id": "68a28285a4caabb4320e63b1",
          "name": "Yaojie Lu",
          "hidden": false
        },
        {
          "_id": "68a28285a4caabb4320e63b2",
          "name": "Xianpei Han",
          "hidden": false
        },
        {
          "_id": "68a28285a4caabb4320e63b3",
          "name": "Le Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-14T23:43:46.000Z",
      "submittedOnDailyAt": "2025-08-18T00:02:29.714Z",
      "title": "PaperRegister: Boosting Flexible-grained Paper Search via Hierarchical\n  Register Indexing",
      "submittedOnDailyBy": {
        "_id": "63664c8fa2abcdf2fd6425ed",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63664c8fa2abcdf2fd6425ed/IywpB0DXZ_twkmZmVSCCD.jpeg",
        "isPro": false,
        "fullname": "Li Zhuoqun",
        "user": "lzq2021",
        "type": "user"
      },
      "summary": "Paper search is an important activity for researchers, typically involving\nusing a query with description of a topic to find relevant papers. As research\ndeepens, paper search requirements may become more flexible, sometimes\ninvolving specific details such as module configuration rather than being\nlimited to coarse-grained topics. However, previous paper search systems are\nunable to meet these flexible-grained requirements, as these systems mainly\ncollect paper abstracts to construct index of corpus, which lack detailed\ninformation to support retrieval by finer-grained queries. In this work, we\npropose PaperRegister, consisted of offline hierarchical indexing and online\nadaptive retrieval, transforming traditional abstract-based index into\nhierarchical index tree for paper search, thereby supporting queries at\nflexible granularity. Experiments on paper search tasks across a range of\ngranularity demonstrate that PaperRegister achieves the state-of-the-art\nperformance, and particularly excels in fine-grained scenarios, highlighting\nthe good potential as an effective solution for flexible-grained paper search\nin real-world applications. Code for this work is in\nhttps://github.com/Li-Z-Q/PaperRegister.",
      "upvotes": 3,
      "discussionId": "68a28285a4caabb4320e63b4",
      "githubRepo": "https://github.com/Li-Z-Q/PaperRegister",
      "ai_summary": "PaperRegister enhances paper search by using hierarchical indexing and adaptive retrieval, supporting flexible and fine-grained queries beyond traditional abstract-based systems.",
      "ai_keywords": [
        "hierarchical indexing",
        "adaptive retrieval",
        "fine-grained queries"
      ],
      "githubStars": 2
    },
    "publishedAt": "2025-08-14T19:43:46.000Z",
    "title": "PaperRegister: Boosting Flexible-grained Paper Search via Hierarchical\n  Register Indexing",
    "summary": "Paper search is an important activity for researchers, typically involving\nusing a query with description of a topic to find relevant papers. As research\ndeepens, paper search requirements may become more flexible, sometimes\ninvolving specific details such as module configuration rather than being\nlimited to coarse-grained topics. However, previous paper search systems are\nunable to meet these flexible-grained requirements, as these systems mainly\ncollect paper abstracts to construct index of corpus, which lack detailed\ninformation to support retrieval by finer-grained queries. In this work, we\npropose PaperRegister, consisted of offline hierarchical indexing and online\nadaptive retrieval, transforming traditional abstract-based index into\nhierarchical index tree for paper search, thereby supporting queries at\nflexible granularity. Experiments on paper search tasks across a range of\ngranularity demonstrate that PaperRegister achieves the state-of-the-art\nperformance, and particularly excels in fine-grained scenarios, highlighting\nthe good potential as an effective solution for flexible-grained paper search\nin real-world applications. Code for this work is in\nhttps://github.com/Li-Z-Q/PaperRegister.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.11116.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63664c8fa2abcdf2fd6425ed",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63664c8fa2abcdf2fd6425ed/IywpB0DXZ_twkmZmVSCCD.jpeg",
      "fullname": "Li Zhuoqun",
      "name": "lzq2021",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2508.11616",
      "authors": [
        {
          "_id": "68a2a72ba4caabb4320e6421",
          "user": {
            "_id": "6355d5c460c1b72f62693983",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6355d5c460c1b72f62693983/RA4FJCgPyqMXgTaSvBUGq.jpeg",
            "isPro": false,
            "fullname": "Oscar Mañas",
            "user": "oscmansan",
            "type": "user"
          },
          "name": "Oscar Mañas",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-18T06:53:56.284Z",
          "hidden": false
        },
        {
          "_id": "68a2a72ba4caabb4320e6422",
          "name": "Pierluca D'Oro",
          "hidden": false
        },
        {
          "_id": "68a2a72ba4caabb4320e6423",
          "name": "Koustuv Sinha",
          "hidden": false
        },
        {
          "_id": "68a2a72ba4caabb4320e6424",
          "name": "Adriana Romero-Soriano",
          "hidden": false
        },
        {
          "_id": "68a2a72ba4caabb4320e6425",
          "name": "Michal Drozdzal",
          "hidden": false
        },
        {
          "_id": "68a2a72ba4caabb4320e6426",
          "name": "Aishwarya Agrawal",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-15T17:29:06.000Z",
      "submittedOnDailyAt": "2025-08-18T02:39:06.425Z",
      "title": "Controlling Multimodal LLMs via Reward-guided Decoding",
      "submittedOnDailyBy": {
        "_id": "6355d5c460c1b72f62693983",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6355d5c460c1b72f62693983/RA4FJCgPyqMXgTaSvBUGq.jpeg",
        "isPro": false,
        "fullname": "Oscar Mañas",
        "user": "oscmansan",
        "type": "user"
      },
      "summary": "As Multimodal Large Language Models (MLLMs) gain widespread applicability, it\nis becoming increasingly desirable to adapt them for diverse user needs. In\nthis paper, we study the adaptation of MLLMs through controlled decoding. To\nachieve this, we introduce the first method for reward-guided decoding of MLLMs\nand demonstrate its application in improving their visual grounding. Our method\ninvolves building reward models for visual grounding and using them to guide\nthe MLLM's decoding process. Concretely, we build two separate reward models to\nindependently control the degree of object precision and recall in the model's\noutput. Our approach enables on-the-fly controllability of an MLLM's inference\nprocess in two ways: first, by giving control over the relative importance of\neach reward function during decoding, allowing a user to dynamically trade off\nobject precision for recall in image captioning tasks; second, by giving\ncontrol over the breadth of the search during decoding, allowing the user to\ncontrol the trade-off between the amount of test-time compute and the degree of\nvisual grounding. We evaluate our method on standard object hallucination\nbenchmarks, showing that it provides significant controllability over MLLM\ninference, while consistently outperforming existing hallucination mitigation\nmethods.",
      "upvotes": 1,
      "discussionId": "68a2a72ba4caabb4320e6427",
      "ai_summary": "A reward-guided decoding method for Multimodal Large Language Models (MLLMs) improves visual grounding by controlling object precision and recall, offering dynamic trade-offs between compute and grounding quality.",
      "ai_keywords": [
        "Multimodal Large Language Models",
        "MLLMs",
        "reward-guided decoding",
        "visual grounding",
        "reward models",
        "object precision",
        "object recall",
        "image captioning",
        "object hallucination",
        "hallucination mitigation"
      ]
    },
    "publishedAt": "2025-08-15T13:29:06.000Z",
    "title": "Controlling Multimodal LLMs via Reward-guided Decoding",
    "summary": "As Multimodal Large Language Models (MLLMs) gain widespread applicability, it\nis becoming increasingly desirable to adapt them for diverse user needs. In\nthis paper, we study the adaptation of MLLMs through controlled decoding. To\nachieve this, we introduce the first method for reward-guided decoding of MLLMs\nand demonstrate its application in improving their visual grounding. Our method\ninvolves building reward models for visual grounding and using them to guide\nthe MLLM's decoding process. Concretely, we build two separate reward models to\nindependently control the degree of object precision and recall in the model's\noutput. Our approach enables on-the-fly controllability of an MLLM's inference\nprocess in two ways: first, by giving control over the relative importance of\neach reward function during decoding, allowing a user to dynamically trade off\nobject precision for recall in image captioning tasks; second, by giving\ncontrol over the breadth of the search during decoding, allowing the user to\ncontrol the trade-off between the amount of test-time compute and the degree of\nvisual grounding. We evaluate our method on standard object hallucination\nbenchmarks, showing that it provides significant controllability over MLLM\ninference, while consistently outperforming existing hallucination mitigation\nmethods.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.11616.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6355d5c460c1b72f62693983",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6355d5c460c1b72f62693983/RA4FJCgPyqMXgTaSvBUGq.jpeg",
      "fullname": "Oscar Mañas",
      "name": "oscmansan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2508.11255",
      "authors": [
        {
          "_id": "68a2ccd3a4caabb4320e6473",
          "name": "MengChao Wang",
          "hidden": false
        },
        {
          "_id": "68a2ccd3a4caabb4320e6474",
          "user": {
            "_id": "653b195c5f1703225b2fd571",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653b195c5f1703225b2fd571/aCDRKL6PBkzhh8-nzDiMo.jpeg",
            "isPro": false,
            "fullname": "wangqiang",
            "user": "wangqiang9",
            "type": "user"
          },
          "name": "Qiang Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-18T06:53:09.368Z",
          "hidden": false
        },
        {
          "_id": "68a2ccd3a4caabb4320e6475",
          "name": "Fan Jiang",
          "hidden": false
        },
        {
          "_id": "68a2ccd3a4caabb4320e6476",
          "name": "Mu Xu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-15T06:43:46.000Z",
      "submittedOnDailyAt": "2025-08-18T05:19:49.395Z",
      "title": "FantasyTalking2: Timestep-Layer Adaptive Preference Optimization for\n  Audio-Driven Portrait Animation",
      "submittedOnDailyBy": {
        "_id": "653b195c5f1703225b2fd571",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653b195c5f1703225b2fd571/aCDRKL6PBkzhh8-nzDiMo.jpeg",
        "isPro": false,
        "fullname": "wangqiang",
        "user": "wangqiang9",
        "type": "user"
      },
      "summary": "Recent advances in audio-driven portrait animation have demonstrated\nimpressive capabilities. However, existing methods struggle to align with\nfine-grained human preferences across multiple dimensions, such as motion\nnaturalness, lip-sync accuracy, and visual quality. This is due to the\ndifficulty of optimizing among competing preference objectives, which often\nconflict with one another, and the scarcity of large-scale, high-quality\ndatasets with multidimensional preference annotations. To address these, we\nfirst introduce Talking-Critic, a multimodal reward model that learns\nhuman-aligned reward functions to quantify how well generated videos satisfy\nmultidimensional expectations. Leveraging this model, we curate Talking-NSQ, a\nlarge-scale multidimensional human preference dataset containing 410K\npreference pairs. Finally, we propose Timestep-Layer adaptive multi-expert\nPreference Optimization (TLPO), a novel framework for aligning diffusion-based\nportrait animation models with fine-grained, multidimensional preferences. TLPO\ndecouples preferences into specialized expert modules, which are then fused\nacross timesteps and network layers, enabling comprehensive, fine-grained\nenhancement across all dimensions without mutual interference. Experiments\ndemonstrate that Talking-Critic significantly outperforms existing methods in\naligning with human preference ratings. Meanwhile, TLPO achieves substantial\nimprovements over baseline models in lip-sync accuracy, motion naturalness, and\nvisual quality, exhibiting superior performance in both qualitative and\nquantitative evaluations. Ours project page:\nhttps://fantasy-amap.github.io/fantasy-talking2/",
      "upvotes": 1,
      "discussionId": "68a2ccd4a4caabb4320e6477",
      "projectPage": "https://fantasy-amap.github.io/fantasy-talking2/",
      "githubRepo": "https://github.com/Fantasy-AMAP/fantasy-talking2",
      "ai_summary": "A multimodal reward model and adaptive preference optimization framework improve audio-driven portrait animation by aligning with human preferences across multiple dimensions.",
      "ai_keywords": [
        "Talking-Critic",
        "multimodal reward model",
        "Talking-NSQ",
        "Timestep-Layer adaptive multi-expert Preference Optimization (TLPO)",
        "diffusion-based portrait animation models",
        "expert modules",
        "lip-sync accuracy",
        "motion naturalness",
        "visual quality"
      ],
      "githubStars": 7
    },
    "publishedAt": "2025-08-15T02:43:46.000Z",
    "title": "FantasyTalking2: Timestep-Layer Adaptive Preference Optimization for\n  Audio-Driven Portrait Animation",
    "summary": "Recent advances in audio-driven portrait animation have demonstrated\nimpressive capabilities. However, existing methods struggle to align with\nfine-grained human preferences across multiple dimensions, such as motion\nnaturalness, lip-sync accuracy, and visual quality. This is due to the\ndifficulty of optimizing among competing preference objectives, which often\nconflict with one another, and the scarcity of large-scale, high-quality\ndatasets with multidimensional preference annotations. To address these, we\nfirst introduce Talking-Critic, a multimodal reward model that learns\nhuman-aligned reward functions to quantify how well generated videos satisfy\nmultidimensional expectations. Leveraging this model, we curate Talking-NSQ, a\nlarge-scale multidimensional human preference dataset containing 410K\npreference pairs. Finally, we propose Timestep-Layer adaptive multi-expert\nPreference Optimization (TLPO), a novel framework for aligning diffusion-based\nportrait animation models with fine-grained, multidimensional preferences. TLPO\ndecouples preferences into specialized expert modules, which are then fused\nacross timesteps and network layers, enabling comprehensive, fine-grained\nenhancement across all dimensions without mutual interference. Experiments\ndemonstrate that Talking-Critic significantly outperforms existing methods in\naligning with human preference ratings. Meanwhile, TLPO achieves substantial\nimprovements over baseline models in lip-sync accuracy, motion naturalness, and\nvisual quality, exhibiting superior performance in both qualitative and\nquantitative evaluations. Ours project page:\nhttps://fantasy-amap.github.io/fantasy-talking2/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.11255.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "653b195c5f1703225b2fd571",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653b195c5f1703225b2fd571/aCDRKL6PBkzhh8-nzDiMo.jpeg",
      "fullname": "wangqiang",
      "name": "wangqiang9",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2508.10868",
      "authors": [
        {
          "_id": "689e9c73a4caabb4320e5d2a",
          "user": {
            "_id": "66bb4d475d27918b4bd8595f",
            "avatarUrl": "/avatars/974ec48aa6d41166799300b2ff44a92c.svg",
            "isPro": false,
            "fullname": "Yibo",
            "user": "YiboZhang2001",
            "type": "user"
          },
          "name": "Yibo Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-18T06:57:29.607Z",
          "hidden": false
        },
        {
          "_id": "689e9c73a4caabb4320e5d2b",
          "name": "Li Zhang",
          "hidden": false
        },
        {
          "_id": "689e9c73a4caabb4320e5d2c",
          "name": "Rui Ma",
          "hidden": false
        },
        {
          "_id": "689e9c73a4caabb4320e5d2d",
          "name": "Nan Cao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-14T17:43:25.000Z",
      "submittedOnDailyAt": "2025-08-18T05:52:17.314Z",
      "title": "TexVerse: A Universe of 3D Objects with High-Resolution Textures",
      "submittedOnDailyBy": {
        "_id": "66bb4d475d27918b4bd8595f",
        "avatarUrl": "/avatars/974ec48aa6d41166799300b2ff44a92c.svg",
        "isPro": false,
        "fullname": "Yibo",
        "user": "YiboZhang2001",
        "type": "user"
      },
      "summary": "We introduce TexVerse, a large-scale 3D dataset featuring high-resolution\ntextures. While recent advances in large-scale 3D datasets have enhanced\nhigh-resolution geometry generation, creating high-resolution textures\nend-to-end remains underexplored due to the lack of suitable datasets. TexVerse\nfills this gap with a curated collection of over 858K unique high-resolution 3D\nmodels sourced from Sketchfab, including more than 158K models with physically\nbased rendering (PBR) materials. Each model encompasses all of its\nhigh-resolution variants, bringing the total to 1.6M 3D instances. TexVerse\nalso includes specialized subsets: TexVerse-Skeleton, with 69K rigged models,\nand TexVerse-Animation, with 54K animated models, both preserving original\nskeleton and animation data uploaded by the user. We also provide detailed\nmodel annotations describing overall characteristics, structural components,\nand intricate features. TexVerse offers a high-quality data resource with\nwide-ranging potential applications in texture synthesis, PBR material\ndevelopment, animation, and various 3D vision and graphics tasks.",
      "upvotes": 1,
      "discussionId": "689e9c73a4caabb4320e5d2e",
      "githubRepo": "https://github.com/yiboz2001/TexVerse",
      "ai_summary": "TexVerse is a large-scale 3D dataset with high-resolution textures, including PBR materials, rigged models, and animated models, suitable for texture synthesis, PBR material development, and 3D vision tasks.",
      "ai_keywords": [
        "physically based rendering",
        "PBR materials",
        "rigged models",
        "animated models",
        "texture synthesis",
        "3D vision",
        "3D graphics"
      ]
    },
    "publishedAt": "2025-08-14T13:43:25.000Z",
    "title": "TexVerse: A Universe of 3D Objects with High-Resolution Textures",
    "summary": "We introduce TexVerse, a large-scale 3D dataset featuring high-resolution\ntextures. While recent advances in large-scale 3D datasets have enhanced\nhigh-resolution geometry generation, creating high-resolution textures\nend-to-end remains underexplored due to the lack of suitable datasets. TexVerse\nfills this gap with a curated collection of over 858K unique high-resolution 3D\nmodels sourced from Sketchfab, including more than 158K models with physically\nbased rendering (PBR) materials. Each model encompasses all of its\nhigh-resolution variants, bringing the total to 1.6M 3D instances. TexVerse\nalso includes specialized subsets: TexVerse-Skeleton, with 69K rigged models,\nand TexVerse-Animation, with 54K animated models, both preserving original\nskeleton and animation data uploaded by the user. We also provide detailed\nmodel annotations describing overall characteristics, structural components,\nand intricate features. TexVerse offers a high-quality data resource with\nwide-ranging potential applications in texture synthesis, PBR material\ndevelopment, animation, and various 3D vision and graphics tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.10868.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "66bb4d475d27918b4bd8595f",
      "avatarUrl": "/avatars/974ec48aa6d41166799300b2ff44a92c.svg",
      "fullname": "Yibo",
      "name": "YiboZhang2001",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.06429",
      "authors": [
        {
          "_id": "68a29911a4caabb4320e6417",
          "name": "Guido Manni",
          "hidden": false
        },
        {
          "_id": "68a29911a4caabb4320e6418",
          "name": "Clemente Lauretti",
          "hidden": false
        },
        {
          "_id": "68a29911a4caabb4320e6419",
          "name": "Loredana Zollo",
          "hidden": false
        },
        {
          "_id": "68a29911a4caabb4320e641a",
          "name": "Paolo Soda",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-08T16:16:43.000Z",
      "submittedOnDailyAt": "2025-08-18T01:38:03.140Z",
      "title": "SPARSE Data, Rich Results: Few-Shot Semi-Supervised Learning via\n  Class-Conditioned Image Translation",
      "submittedOnDailyBy": {
        "_id": "67bc77b8a4562caa581556ee",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/qpVVbYJhqXK8e9KKDLbHR.png",
        "isPro": false,
        "fullname": "Guido Manni",
        "user": "gvide",
        "type": "user"
      },
      "summary": "Deep learning has revolutionized medical imaging, but its effectiveness is\nseverely limited by insufficient labeled training data. This paper introduces a\nnovel GAN-based semi-supervised learning framework specifically designed for\nlow labeled-data regimes, evaluated across settings with 5 to 50 labeled\nsamples per class. Our approach integrates three specialized neural networks --\na generator for class-conditioned image translation, a discriminator for\nauthenticity assessment and classification, and a dedicated classifier --\nwithin a three-phase training framework. The method alternates between\nsupervised training on limited labeled data and unsupervised learning that\nleverages abundant unlabeled images through image-to-image translation rather\nthan generation from noise. We employ ensemble-based pseudo-labeling that\ncombines confidence-weighted predictions from the discriminator and classifier\nwith temporal consistency through exponential moving averaging, enabling\nreliable label estimation for unlabeled data. Comprehensive evaluation across\neleven MedMNIST datasets demonstrates that our approach achieves statistically\nsignificant improvements over six state-of-the-art GAN-based semi-supervised\nmethods, with particularly strong performance in the extreme 5-shot setting\nwhere the scarcity of labeled data is most challenging. The framework maintains\nits superiority across all evaluated settings (5, 10, 20, and 50 shots per\nclass). Our approach offers a practical solution for medical imaging\napplications where annotation costs are prohibitive, enabling robust\nclassification performance even with minimal labeled data. Code is available at\nhttps://github.com/GuidoManni/SPARSE.",
      "upvotes": 1,
      "discussionId": "68a29912a4caabb4320e641b",
      "githubRepo": "https://github.com/GuidoManni/SPARSE",
      "ai_summary": "A GAN-based semi-supervised learning framework improves medical image classification with minimal labeled data by integrating specialized neural networks and ensemble-based pseudo-labeling.",
      "ai_keywords": [
        "GAN-based semi-supervised learning",
        "class-conditioned image translation",
        "discriminator",
        "classifier",
        "three-phase training framework",
        "ensemble-based pseudo-labeling",
        "confidence-weighted predictions",
        "exponential moving averaging",
        "MedMNIST datasets",
        "5-shot setting"
      ],
      "githubStars": 0
    },
    "publishedAt": "2025-08-08T12:16:43.000Z",
    "title": "SPARSE Data, Rich Results: Few-Shot Semi-Supervised Learning via\n  Class-Conditioned Image Translation",
    "summary": "Deep learning has revolutionized medical imaging, but its effectiveness is\nseverely limited by insufficient labeled training data. This paper introduces a\nnovel GAN-based semi-supervised learning framework specifically designed for\nlow labeled-data regimes, evaluated across settings with 5 to 50 labeled\nsamples per class. Our approach integrates three specialized neural networks --\na generator for class-conditioned image translation, a discriminator for\nauthenticity assessment and classification, and a dedicated classifier --\nwithin a three-phase training framework. The method alternates between\nsupervised training on limited labeled data and unsupervised learning that\nleverages abundant unlabeled images through image-to-image translation rather\nthan generation from noise. We employ ensemble-based pseudo-labeling that\ncombines confidence-weighted predictions from the discriminator and classifier\nwith temporal consistency through exponential moving averaging, enabling\nreliable label estimation for unlabeled data. Comprehensive evaluation across\neleven MedMNIST datasets demonstrates that our approach achieves statistically\nsignificant improvements over six state-of-the-art GAN-based semi-supervised\nmethods, with particularly strong performance in the extreme 5-shot setting\nwhere the scarcity of labeled data is most challenging. The framework maintains\nits superiority across all evaluated settings (5, 10, 20, and 50 shots per\nclass). Our approach offers a practical solution for medical imaging\napplications where annotation costs are prohibitive, enabling robust\nclassification performance even with minimal labeled data. Code is available at\nhttps://github.com/GuidoManni/SPARSE.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.06429.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67bc77b8a4562caa581556ee",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/qpVVbYJhqXK8e9KKDLbHR.png",
      "fullname": "Guido Manni",
      "name": "gvide",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]