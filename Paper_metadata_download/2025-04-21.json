[
  {
    "paper": {
      "id": "2504.13835",
      "authors": [
        {
          "_id": "6805b38355d3c792e1a9d0dd",
          "name": "Yicheng Chen",
          "hidden": false
        },
        {
          "_id": "6805b38355d3c792e1a9d0de",
          "name": "Yining Li",
          "hidden": false
        },
        {
          "_id": "6805b38355d3c792e1a9d0df",
          "name": "Kai Hu",
          "hidden": false
        },
        {
          "_id": "6805b38355d3c792e1a9d0e0",
          "name": "Zerun Ma",
          "hidden": false
        },
        {
          "_id": "6805b38355d3c792e1a9d0e1",
          "name": "Haochen Ye",
          "hidden": false
        },
        {
          "_id": "6805b38355d3c792e1a9d0e2",
          "name": "Kai Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-18T17:59:46.000Z",
      "submittedOnDailyAt": "2025-04-21T01:39:08.191Z",
      "title": "MIG: Automatic Data Selection for Instruction Tuning by Maximizing\n  Information Gain in Semantic Space",
      "submittedOnDailyBy": {
        "_id": "649988726677f66c2b486392",
        "avatarUrl": "/avatars/b649a77370660e129726e29504daba34.svg",
        "isPro": false,
        "fullname": "Yining Li",
        "user": "ly015",
        "type": "user"
      },
      "summary": "Data quality and diversity are key to the construction of effective\ninstruction-tuning datasets. % With the increasing availability of open-source\ninstruction-tuning datasets, it is advantageous to automatically select\nhigh-quality and diverse subsets from a vast amount of data. % Existing methods\ntypically prioritize instance quality and use heuristic rules to maintain\ndiversity. % However, this absence of a comprehensive view of the entire\ncollection often leads to suboptimal results. % Moreover, heuristic rules\ngenerally focus on distance or clustering within the embedding space, which\nfails to accurately capture the intent of complex instructions in the semantic\nspace. % To bridge this gap, we propose a unified method for quantifying the\ninformation content of datasets. This method models the semantic space by\nconstructing a label graph and quantifies diversity based on the distribution\nof information within the graph. % Based on such a measurement, we further\nintroduce an efficient sampling method that selects data samples iteratively to\nMaximize the Information Gain (MIG) in semantic\nspace. % Experiments on various datasets and base models demonstrate that MIG\nconsistently outperforms state-of-the-art methods. % Notably, the model\nfine-tuned with 5\\% Tulu3 data sampled by MIG achieves comparable performance\nto the official SFT model trained on the full dataset, with improvements of\n+5.73\\% on AlpacaEval and +6.89\\% on Wildbench.",
      "upvotes": 24,
      "discussionId": "6805b38555d3c792e1a9d155",
      "ai_keywords": [
        "label graph",
        "semantic space",
        "information content",
        "Maximize the Information Gain (MIG)"
      ]
    },
    "publishedAt": "2025-04-18T13:59:46.000Z",
    "title": "MIG: Automatic Data Selection for Instruction Tuning by Maximizing\n  Information Gain in Semantic Space",
    "summary": "Data quality and diversity are key to the construction of effective\ninstruction-tuning datasets. % With the increasing availability of open-source\ninstruction-tuning datasets, it is advantageous to automatically select\nhigh-quality and diverse subsets from a vast amount of data. % Existing methods\ntypically prioritize instance quality and use heuristic rules to maintain\ndiversity. % However, this absence of a comprehensive view of the entire\ncollection often leads to suboptimal results. % Moreover, heuristic rules\ngenerally focus on distance or clustering within the embedding space, which\nfails to accurately capture the intent of complex instructions in the semantic\nspace. % To bridge this gap, we propose a unified method for quantifying the\ninformation content of datasets. This method models the semantic space by\nconstructing a label graph and quantifies diversity based on the distribution\nof information within the graph. % Based on such a measurement, we further\nintroduce an efficient sampling method that selects data samples iteratively to\nMaximize the Information Gain (MIG) in semantic\nspace. % Experiments on various datasets and base models demonstrate that MIG\nconsistently outperforms state-of-the-art methods. % Notably, the model\nfine-tuned with 5\\% Tulu3 data sampled by MIG achieves comparable performance\nto the official SFT model trained on the full dataset, with improvements of\n+5.73\\% on AlpacaEval and +6.89\\% on Wildbench.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13835.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649988726677f66c2b486392",
      "avatarUrl": "/avatars/b649a77370660e129726e29504daba34.svg",
      "fullname": "Yining Li",
      "name": "ly015",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.13837",
      "authors": [
        {
          "_id": "6805b9ec7c5fa8020f595642",
          "name": "Yang Yue",
          "hidden": false
        },
        {
          "_id": "6805b9ec7c5fa8020f595643",
          "name": "Zhiqi Chen",
          "hidden": false
        },
        {
          "_id": "6805b9ec7c5fa8020f595644",
          "name": "Rui Lu",
          "hidden": false
        },
        {
          "_id": "6805b9ec7c5fa8020f595645",
          "name": "Andrew Zhao",
          "hidden": false
        },
        {
          "_id": "6805b9ec7c5fa8020f595646",
          "name": "Zhaokai Wang",
          "hidden": false
        },
        {
          "_id": "6805b9ec7c5fa8020f595647",
          "name": "Yang Yue",
          "hidden": false
        },
        {
          "_id": "6805b9ec7c5fa8020f595648",
          "name": "Shiji Song",
          "hidden": false
        },
        {
          "_id": "6805b9ec7c5fa8020f595649",
          "name": "Gao Huang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/649d475111592b1a765ac1a3/2KWQqFdVDUCAu-kSu87fa.jpeg",
        "https://cdn-uploads.huggingface.co/production/uploads/649d475111592b1a765ac1a3/rF4cAa6DAI3EaejDV_dgG.mp4"
      ],
      "publishedAt": "2025-04-18T17:59:56.000Z",
      "submittedOnDailyAt": "2025-04-21T01:54:36.096Z",
      "title": "Does Reinforcement Learning Really Incentivize Reasoning Capacity in\n  LLMs Beyond the Base Model?",
      "submittedOnDailyBy": {
        "_id": "649d475111592b1a765ac1a3",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649d475111592b1a765ac1a3/rjORJjErJq-mthghan08U.jpeg",
        "isPro": false,
        "fullname": "Yang Yue",
        "user": "Yang130",
        "type": "user"
      },
      "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently\ndemonstrated notable success in enhancing the reasoning capabilities of LLMs,\nparticularly in mathematics and programming tasks. It is widely believed that\nRLVR enables LLMs to continuously self-improve, thus acquiring novel reasoning\nabilities that exceed corresponding base models' capacity. In this study,\nhowever, we critically re-examines this assumption by measuring the\npass@k metric with large values of k to explore the reasoning\ncapability boundary of the models across a wide range of model families and\nbenchmarks. Surprisingly, the RL does not, in fact, elicit fundamentally\nnew reasoning patterns. While RL-trained models outperform their base models at\nsmaller values of k (\\eg, k=1), base models can achieve a comparable or\neven higher pass@k score compared to their RL counterparts at large k\nvalues. The reasoning paths generated by RL-trained models are already included\nin the base models' sampling distribution, suggesting that most reasoning\nabilities manifested in RL-trained models are already obtained by base models.\nFurther analysis shows that RL training boosts the performance by biasing the\nmodel's output distribution toward paths that are more likely to yield rewards,\ntherefore sampling correct responses more efficiently. But this also results in\na narrower reasoning capability boundary compared to base models. Similar\nresults are observed in visual reasoning tasks trained with RLVR. Moreover, we\nfind that distillation can genuinely introduce new knowledge into the model,\ndifferent from RLVR. These findings underscore a critical limitation of RLVR in\nadvancing LLM reasoning abilities which requires us to fundamentally rethink\nthe impact of RL training in reasoning LLMs and the need of a better paradigm.\nProject Page: https://limit-of-RLVR.github.io",
      "upvotes": 20,
      "discussionId": "6805b9ed7c5fa8020f59568c",
      "projectPage": "https://limit-of-rlvr.github.io/",
      "githubRepo": "https://github.com/LeapLabTHU/limit-of-RLVR",
      "ai_keywords": [
        "Reinforcement Learning",
        "Verifiable Rewards",
        "RLVR",
        "LLMs (Large Language Models)",
        "reasoning capabilities",
        "mathematics",
        "programming tasks",
        "pass@\\textit{k}",
        "benchmark",
        "RL-trained models",
        "base models",
        "reasoning paths",
        "sampling distribution",
        "performance",
        "biasing",
        "output distribution",
        "visual reasoning tasks",
        "distillation"
      ]
    },
    "publishedAt": "2025-04-18T13:59:56.000Z",
    "title": "Does Reinforcement Learning Really Incentivize Reasoning Capacity in\n  LLMs Beyond the Base Model?",
    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently\ndemonstrated notable success in enhancing the reasoning capabilities of LLMs,\nparticularly in mathematics and programming tasks. It is widely believed that\nRLVR enables LLMs to continuously self-improve, thus acquiring novel reasoning\nabilities that exceed corresponding base models' capacity. In this study,\nhowever, we critically re-examines this assumption by measuring the\npass@k metric with large values of k to explore the reasoning\ncapability boundary of the models across a wide range of model families and\nbenchmarks. Surprisingly, the RL does not, in fact, elicit fundamentally\nnew reasoning patterns. While RL-trained models outperform their base models at\nsmaller values of k (\\eg, k=1), base models can achieve a comparable or\neven higher pass@k score compared to their RL counterparts at large k\nvalues. The reasoning paths generated by RL-trained models are already included\nin the base models' sampling distribution, suggesting that most reasoning\nabilities manifested in RL-trained models are already obtained by base models.\nFurther analysis shows that RL training boosts the performance by biasing the\nmodel's output distribution toward paths that are more likely to yield rewards,\ntherefore sampling correct responses more efficiently. But this also results in\na narrower reasoning capability boundary compared to base models. Similar\nresults are observed in visual reasoning tasks trained with RLVR. Moreover, we\nfind that distillation can genuinely introduce new knowledge into the model,\ndifferent from RLVR. These findings underscore a critical limitation of RLVR in\nadvancing LLM reasoning abilities which requires us to fundamentally rethink\nthe impact of RL training in reasoning LLMs and the need of a better paradigm.\nProject Page: https://limit-of-RLVR.github.io",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/649d475111592b1a765ac1a3/2KWQqFdVDUCAu-kSu87fa.jpeg",
      "https://cdn-uploads.huggingface.co/production/uploads/649d475111592b1a765ac1a3/rF4cAa6DAI3EaejDV_dgG.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13837.png",
    "numComments": 6,
    "submittedBy": {
      "_id": "649d475111592b1a765ac1a3",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649d475111592b1a765ac1a3/rjORJjErJq-mthghan08U.jpeg",
      "fullname": "Yang Yue",
      "name": "Yang130",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.11833",
      "authors": [
        {
          "_id": "6805bb01747a412bca737b53",
          "name": "Changjiang Gao",
          "hidden": false
        },
        {
          "_id": "6805bb01747a412bca737b54",
          "name": "Xu Huang",
          "hidden": false
        },
        {
          "_id": "6805bb01747a412bca737b55",
          "name": "Wenhao Zhu",
          "hidden": false
        },
        {
          "_id": "6805bb01747a412bca737b56",
          "name": "Shujian Huang",
          "hidden": false
        },
        {
          "_id": "6805bb01747a412bca737b57",
          "name": "Lei Li",
          "hidden": false
        },
        {
          "_id": "6805bb01747a412bca737b58",
          "name": "Fei Yuan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-16T07:45:10.000Z",
      "submittedOnDailyAt": "2025-04-21T01:57:09.327Z",
      "title": "Could Thinking Multilingually Empower LLM Reasoning?",
      "submittedOnDailyBy": {
        "_id": "65fed45b08d35929362dd651",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65fed45b08d35929362dd651/KLMxsyRN6_HhCZP1iDw6K.png",
        "isPro": false,
        "fullname": "FeiYuan",
        "user": "FeYuan",
        "type": "user"
      },
      "summary": "Previous work indicates that large language models exhibit a significant\n\"English bias\", i.e. they often perform better when tasks are presented in\nEnglish. Interestingly, we have observed that using certain other languages in\nreasoning tasks can yield better performance than English. However, this\nphenomenon remains under-explored. In this paper, we explore the upper bound of\nharnessing multilingualism in reasoning tasks, suggesting that multilingual\nreasoning promises significantly (by nearly 10 Acc@k points) and robustly\n(tolerance for variations in translation quality and language choice) higher\nupper bounds than English-only reasoning. Besides analyzing the reason behind\nthe upper bound and challenges in reaching it, we also find that common answer\nselection methods cannot achieve this upper bound, due to their limitations and\nbiases. These insights could pave the way for future research aimed at fully\nharnessing the potential of multilingual reasoning in LLMs.",
      "upvotes": 13,
      "discussionId": "6805bb02747a412bca737b7e",
      "githubRepo": "https://github.com/CONE-MT/multilingual_reasoning"
    },
    "publishedAt": "2025-04-16T03:45:10.000Z",
    "title": "Could Thinking Multilingually Empower LLM Reasoning?",
    "summary": "Previous work indicates that large language models exhibit a significant\n\"English bias\", i.e. they often perform better when tasks are presented in\nEnglish. Interestingly, we have observed that using certain other languages in\nreasoning tasks can yield better performance than English. However, this\nphenomenon remains under-explored. In this paper, we explore the upper bound of\nharnessing multilingualism in reasoning tasks, suggesting that multilingual\nreasoning promises significantly (by nearly 10 Acc@k points) and robustly\n(tolerance for variations in translation quality and language choice) higher\nupper bounds than English-only reasoning. Besides analyzing the reason behind\nthe upper bound and challenges in reaching it, we also find that common answer\nselection methods cannot achieve this upper bound, due to their limitations and\nbiases. These insights could pave the way for future research aimed at fully\nharnessing the potential of multilingual reasoning in LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11833.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65fed45b08d35929362dd651",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65fed45b08d35929362dd651/KLMxsyRN6_HhCZP1iDw6K.png",
      "fullname": "FeiYuan",
      "name": "FeYuan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 21
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.11544",
      "authors": [
        {
          "_id": "6804ca9fd8538baa1c39ca93",
          "name": "Tianyang Xu",
          "hidden": false
        },
        {
          "_id": "6804ca9fd8538baa1c39ca94",
          "name": "Haojie Zheng",
          "hidden": false
        },
        {
          "_id": "6804ca9fd8538baa1c39ca95",
          "name": "Chengze Li",
          "hidden": false
        },
        {
          "_id": "6804ca9fd8538baa1c39ca96",
          "name": "Haoxiang Chen",
          "hidden": false
        },
        {
          "_id": "6804ca9fd8538baa1c39ca97",
          "name": "Yixin Liu",
          "hidden": false
        },
        {
          "_id": "6804ca9fd8538baa1c39ca98",
          "name": "Ruoxi Chen",
          "hidden": false
        },
        {
          "_id": "6804ca9fd8538baa1c39ca99",
          "name": "Lichao Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-15T18:24:00.000Z",
      "submittedOnDailyAt": "2025-04-21T01:38:46.380Z",
      "title": "NodeRAG: Structuring Graph-based RAG with Heterogeneous Nodes",
      "submittedOnDailyBy": {
        "_id": "6610fb736504d9bed5890d58",
        "avatarUrl": "/avatars/832b186fc51c639f1709025d442b3f4b.svg",
        "isPro": false,
        "fullname": "Tianyang Xu",
        "user": "TerryXu666",
        "type": "user"
      },
      "summary": "Retrieval-augmented generation (RAG) empowers large language models to access\nexternal and private corpus, enabling factually consistent responses in\nspecific domains. By exploiting the inherent structure of the corpus,\ngraph-based RAG methods further enrich this process by building a knowledge\ngraph index and leveraging the structural nature of graphs. However, current\ngraph-based RAG approaches seldom prioritize the design of graph structures.\nInadequately designed graph not only impede the seamless integration of diverse\ngraph algorithms but also result in workflow inconsistencies and degraded\nperformance. To further unleash the potential of graph for RAG, we propose\nNodeRAG, a graph-centric framework introducing heterogeneous graph structures\nthat enable the seamless and holistic integration of graph-based methodologies\ninto the RAG workflow. By aligning closely with the capabilities of LLMs, this\nframework ensures a fully cohesive and efficient end-to-end process. Through\nextensive experiments, we demonstrate that NodeRAG exhibits performance\nadvantages over previous methods, including GraphRAG and LightRAG, not only in\nindexing time, query time, and storage efficiency but also in delivering\nsuperior question-answering performance on multi-hop benchmarks and open-ended\nhead-to-head evaluations with minimal retrieval tokens. Our GitHub repository\ncould be seen at https://github.com/Terry-Xu-666/NodeRAG.",
      "upvotes": 4,
      "discussionId": "6804caa0d8538baa1c39cac2",
      "projectPage": "https://terry-xu-666.github.io/NodeRAG_web/",
      "githubRepo": "https://github.com/Terry-Xu-666/NodeRAG",
      "ai_keywords": [
        "Retrieval-augmented generation (RAG)",
        "external and private corpus",
        "factually consistent responses",
        "knowledge graph index",
        "graph-based RAG methods",
        "heterogeneous graph structures",
        "seamless and holistic integration",
        "end-to-end process",
        "question-answering performance",
        "multi-hop benchmarks",
        "open-ended head-to-head evaluations",
        "retrieval tokens"
      ]
    },
    "publishedAt": "2025-04-15T14:24:00.000Z",
    "title": "NodeRAG: Structuring Graph-based RAG with Heterogeneous Nodes",
    "summary": "Retrieval-augmented generation (RAG) empowers large language models to access\nexternal and private corpus, enabling factually consistent responses in\nspecific domains. By exploiting the inherent structure of the corpus,\ngraph-based RAG methods further enrich this process by building a knowledge\ngraph index and leveraging the structural nature of graphs. However, current\ngraph-based RAG approaches seldom prioritize the design of graph structures.\nInadequately designed graph not only impede the seamless integration of diverse\ngraph algorithms but also result in workflow inconsistencies and degraded\nperformance. To further unleash the potential of graph for RAG, we propose\nNodeRAG, a graph-centric framework introducing heterogeneous graph structures\nthat enable the seamless and holistic integration of graph-based methodologies\ninto the RAG workflow. By aligning closely with the capabilities of LLMs, this\nframework ensures a fully cohesive and efficient end-to-end process. Through\nextensive experiments, we demonstrate that NodeRAG exhibits performance\nadvantages over previous methods, including GraphRAG and LightRAG, not only in\nindexing time, query time, and storage efficiency but also in delivering\nsuperior question-answering performance on multi-hop benchmarks and open-ended\nhead-to-head evaluations with minimal retrieval tokens. Our GitHub repository\ncould be seen at https://github.com/Terry-Xu-666/NodeRAG.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11544.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6610fb736504d9bed5890d58",
      "avatarUrl": "/avatars/832b186fc51c639f1709025d442b3f4b.svg",
      "fullname": "Tianyang Xu",
      "name": "TerryXu666",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.13157",
      "authors": [
        {
          "_id": "6804392129303a3402c4f38e",
          "user": {
            "_id": "631bfb21f6bc4be4a6592afc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/631bfb21f6bc4be4a6592afc/FRgc7nwHylQZ9QURrr88y.jpeg",
            "isPro": false,
            "fullname": "Khiem Vuong",
            "user": "kvuong2711",
            "type": "user"
          },
          "name": "Khiem Vuong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-20T15:01:35.224Z",
          "hidden": false
        },
        {
          "_id": "6804392129303a3402c4f38f",
          "name": "Anurag Ghosh",
          "hidden": false
        },
        {
          "_id": "6804392129303a3402c4f390",
          "name": "Deva Ramanan",
          "hidden": false
        },
        {
          "_id": "6804392129303a3402c4f391",
          "name": "Srinivasa Narasimhan",
          "hidden": false
        },
        {
          "_id": "6804392129303a3402c4f392",
          "name": "Shubham Tulsiani",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/631bfb21f6bc4be4a6592afc/x2dR6H7Gl0l8qbXY9YxGP.jpeg",
        "https://cdn-uploads.huggingface.co/production/uploads/631bfb21f6bc4be4a6592afc/Kf60HymR1YXGVqoSmQNYb.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/631bfb21f6bc4be4a6592afc/O0wHR9zr6E56IE7sjs0UI.jpeg"
      ],
      "publishedAt": "2025-04-17T17:57:05.000Z",
      "submittedOnDailyAt": "2025-04-21T01:28:10.375Z",
      "title": "AerialMegaDepth: Learning Aerial-Ground Reconstruction and View\n  Synthesis",
      "submittedOnDailyBy": {
        "_id": "631bfb21f6bc4be4a6592afc",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/631bfb21f6bc4be4a6592afc/FRgc7nwHylQZ9QURrr88y.jpeg",
        "isPro": false,
        "fullname": "Khiem Vuong",
        "user": "kvuong2711",
        "type": "user"
      },
      "summary": "We explore the task of geometric reconstruction of images captured from a\nmixture of ground and aerial views. Current state-of-the-art learning-based\napproaches fail to handle the extreme viewpoint variation between aerial-ground\nimage pairs. Our hypothesis is that the lack of high-quality, co-registered\naerial-ground datasets for training is a key reason for this failure. Such data\nis difficult to assemble precisely because it is difficult to reconstruct in a\nscalable way. To overcome this challenge, we propose a scalable framework\ncombining pseudo-synthetic renderings from 3D city-wide meshes (e.g., Google\nEarth) with real, ground-level crowd-sourced images (e.g., MegaDepth). The\npseudo-synthetic data simulates a wide range of aerial viewpoints, while the\nreal, crowd-sourced images help improve visual fidelity for ground-level images\nwhere mesh-based renderings lack sufficient detail, effectively bridging the\ndomain gap between real images and pseudo-synthetic renderings. Using this\nhybrid dataset, we fine-tune several state-of-the-art algorithms and achieve\nsignificant improvements on real-world, zero-shot aerial-ground tasks. For\nexample, we observe that baseline DUSt3R localizes fewer than 5% of\naerial-ground pairs within 5 degrees of camera rotation error, while\nfine-tuning with our data raises accuracy to nearly 56%, addressing a major\nfailure point in handling large viewpoint changes. Beyond camera estimation and\nscene reconstruction, our dataset also improves performance on downstream tasks\nlike novel-view synthesis in challenging aerial-ground scenarios, demonstrating\nthe practical value of our approach in real-world applications.",
      "upvotes": 3,
      "discussionId": "6804392329303a3402c4f3e8",
      "projectPage": "https://aerial-megadepth.github.io/",
      "githubRepo": "https://github.com/kvuong2711/aerial-megadepth",
      "ai_keywords": [
        "geometric reconstruction",
        "learning-based approaches",
        "extreme viewpoint variation",
        "co-registered",
        "aerial-ground datasets",
        "pseudo-synthetic renderings",
        "3D city-wide meshes",
        "crowd-sourced images",
        "visual fidelity",
        "domain gap",
        "mesh-based renderings",
        "fine-tuning",
        "DUSt3R",
        "camera rotation error",
        "scene reconstruction",
        "novel-view synthesis",
        "downstream tasks"
      ]
    },
    "publishedAt": "2025-04-17T13:57:05.000Z",
    "title": "AerialMegaDepth: Learning Aerial-Ground Reconstruction and View\n  Synthesis",
    "summary": "We explore the task of geometric reconstruction of images captured from a\nmixture of ground and aerial views. Current state-of-the-art learning-based\napproaches fail to handle the extreme viewpoint variation between aerial-ground\nimage pairs. Our hypothesis is that the lack of high-quality, co-registered\naerial-ground datasets for training is a key reason for this failure. Such data\nis difficult to assemble precisely because it is difficult to reconstruct in a\nscalable way. To overcome this challenge, we propose a scalable framework\ncombining pseudo-synthetic renderings from 3D city-wide meshes (e.g., Google\nEarth) with real, ground-level crowd-sourced images (e.g., MegaDepth). The\npseudo-synthetic data simulates a wide range of aerial viewpoints, while the\nreal, crowd-sourced images help improve visual fidelity for ground-level images\nwhere mesh-based renderings lack sufficient detail, effectively bridging the\ndomain gap between real images and pseudo-synthetic renderings. Using this\nhybrid dataset, we fine-tune several state-of-the-art algorithms and achieve\nsignificant improvements on real-world, zero-shot aerial-ground tasks. For\nexample, we observe that baseline DUSt3R localizes fewer than 5% of\naerial-ground pairs within 5 degrees of camera rotation error, while\nfine-tuning with our data raises accuracy to nearly 56%, addressing a major\nfailure point in handling large viewpoint changes. Beyond camera estimation and\nscene reconstruction, our dataset also improves performance on downstream tasks\nlike novel-view synthesis in challenging aerial-ground scenarios, demonstrating\nthe practical value of our approach in real-world applications.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/631bfb21f6bc4be4a6592afc/x2dR6H7Gl0l8qbXY9YxGP.jpeg",
      "https://cdn-uploads.huggingface.co/production/uploads/631bfb21f6bc4be4a6592afc/Kf60HymR1YXGVqoSmQNYb.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/631bfb21f6bc4be4a6592afc/O0wHR9zr6E56IE7sjs0UI.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13157.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "631bfb21f6bc4be4a6592afc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/631bfb21f6bc4be4a6592afc/FRgc7nwHylQZ9QURrr88y.jpeg",
      "fullname": "Khiem Vuong",
      "name": "kvuong2711",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.13072",
      "authors": [
        {
          "_id": "6805bfc5e332a61dd90160b0",
          "name": "Wenqi Dong",
          "hidden": false
        },
        {
          "_id": "6805bfc5e332a61dd90160b1",
          "name": "Bangbang Yang",
          "hidden": false
        },
        {
          "_id": "6805bfc5e332a61dd90160b2",
          "name": "Zesong Yang",
          "hidden": false
        },
        {
          "_id": "6805bfc5e332a61dd90160b3",
          "name": "Yuan Li",
          "hidden": false
        },
        {
          "_id": "6805bfc5e332a61dd90160b4",
          "name": "Tao Hu",
          "hidden": false
        },
        {
          "_id": "6805bfc5e332a61dd90160b5",
          "name": "Hujun Bao",
          "hidden": false
        },
        {
          "_id": "6805bfc5e332a61dd90160b6",
          "name": "Yuewen Ma",
          "hidden": false
        },
        {
          "_id": "6805bfc5e332a61dd90160b7",
          "name": "Zhaopeng Cui",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63d748ff6f49aa82306b7e48/DNTOfy5oOkjTJwLBeGOUH.qt"
      ],
      "publishedAt": "2025-04-17T16:33:39.000Z",
      "submittedOnDailyAt": "2025-04-21T03:26:56.809Z",
      "title": "HiScene: Creating Hierarchical 3D Scenes with Isometric View Generation",
      "submittedOnDailyBy": {
        "_id": "63d748ff6f49aa82306b7e48",
        "avatarUrl": "/avatars/9f0b8b8a09b14d76e52ed1bd312e6b63.svg",
        "isPro": false,
        "fullname": "BB Yang",
        "user": "ybbbbt",
        "type": "user"
      },
      "summary": "Scene-level 3D generation represents a critical frontier in multimedia and\ncomputer graphics, yet existing approaches either suffer from limited object\ncategories or lack editing flexibility for interactive applications. In this\npaper, we present HiScene, a novel hierarchical framework that bridges the gap\nbetween 2D image generation and 3D object generation and delivers high-fidelity\nscenes with compositional identities and aesthetic scene content. Our key\ninsight is treating scenes as hierarchical \"objects\" under isometric views,\nwhere a room functions as a complex object that can be further decomposed into\nmanipulatable items. This hierarchical approach enables us to generate 3D\ncontent that aligns with 2D representations while maintaining compositional\nstructure. To ensure completeness and spatial alignment of each decomposed\ninstance, we develop a video-diffusion-based amodal completion technique that\neffectively handles occlusions and shadows between objects, and introduce shape\nprior injection to ensure spatial coherence within the scene. Experimental\nresults demonstrate that our method produces more natural object arrangements\nand complete object instances suitable for interactive applications, while\nmaintaining physical plausibility and alignment with user inputs.",
      "upvotes": 3,
      "discussionId": "6805bfc9e332a61dd901618b",
      "ai_keywords": [
        "hierarchical framework",
        "2D image generation",
        "3D object generation",
        "video-diffusion-based amodal completion",
        "occlusions",
        "shadows",
        "shape prior injection",
        "spatial coherence",
        "natural object arrangements",
        "complete object instances",
        "interactive applications",
        "physical plausibility",
        "user inputs"
      ]
    },
    "publishedAt": "2025-04-17T12:33:39.000Z",
    "title": "HiScene: Creating Hierarchical 3D Scenes with Isometric View Generation",
    "summary": "Scene-level 3D generation represents a critical frontier in multimedia and\ncomputer graphics, yet existing approaches either suffer from limited object\ncategories or lack editing flexibility for interactive applications. In this\npaper, we present HiScene, a novel hierarchical framework that bridges the gap\nbetween 2D image generation and 3D object generation and delivers high-fidelity\nscenes with compositional identities and aesthetic scene content. Our key\ninsight is treating scenes as hierarchical \"objects\" under isometric views,\nwhere a room functions as a complex object that can be further decomposed into\nmanipulatable items. This hierarchical approach enables us to generate 3D\ncontent that aligns with 2D representations while maintaining compositional\nstructure. To ensure completeness and spatial alignment of each decomposed\ninstance, we develop a video-diffusion-based amodal completion technique that\neffectively handles occlusions and shadows between objects, and introduce shape\nprior injection to ensure spatial coherence within the scene. Experimental\nresults demonstrate that our method produces more natural object arrangements\nand complete object instances suitable for interactive applications, while\nmaintaining physical plausibility and alignment with user inputs.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63d748ff6f49aa82306b7e48/DNTOfy5oOkjTJwLBeGOUH.qt"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13072.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63d748ff6f49aa82306b7e48",
      "avatarUrl": "/avatars/9f0b8b8a09b14d76e52ed1bd312e6b63.svg",
      "fullname": "BB Yang",
      "name": "ybbbbt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.13173",
      "authors": [
        {
          "_id": "6805c4dab15a57fcb59b6f08",
          "name": "Ali Behrouz",
          "hidden": false
        },
        {
          "_id": "6805c4dab15a57fcb59b6f09",
          "name": "Meisam Razaviyayn",
          "hidden": false
        },
        {
          "_id": "6805c4dab15a57fcb59b6f0a",
          "name": "Peilin Zhong",
          "hidden": false
        },
        {
          "_id": "6805c4dab15a57fcb59b6f0b",
          "name": "Vahab Mirrokni",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-17T17:59:33.000Z",
      "submittedOnDailyAt": "2025-04-21T02:39:28.607Z",
      "title": "It's All Connected: A Journey Through Test-Time Memorization,\n  Attentional Bias, Retention, and Online Optimization",
      "submittedOnDailyBy": {
        "_id": "65cccd5134a5d74cbaa9446c",
        "avatarUrl": "/avatars/5255b734628992106598eae4f2c5848f.svg",
        "isPro": false,
        "fullname": "Ali Behrouz",
        "user": "AliBehrouz",
        "type": "user"
      },
      "summary": "Designing efficient and effective architectural backbones has been in the\ncore of research efforts to enhance the capability of foundation models.\nInspired by the human cognitive phenomenon of attentional bias-the natural\ntendency to prioritize certain events or stimuli-we reconceptualize neural\narchitectures, including Transformers, Titans, and modern linear recurrent\nneural networks as associative memory modules that learn a mapping of keys and\nvalues using an internal objective, referred to as attentional bias.\nSurprisingly, we observed that most existing sequence models leverage either\n(1) dot-product similarity, or (2) L2 regression objectives as their\nattentional bias. Going beyond these objectives, we present a set of\nalternative attentional bias configurations along with their effective\napproximations to stabilize their training procedure. We then reinterpret\nforgetting mechanisms in modern deep learning architectures as a form of\nretention regularization, providing a novel set of forget gates for sequence\nmodels. Building upon these insights, we present Miras, a general framework to\ndesign deep learning architectures based on four choices of: (i) associative\nmemory architecture, (ii) attentional bias objective, (iii) retention gate, and\n(iv) memory learning algorithm. We present three novel sequence models-Moneta,\nYaad, and Memora-that go beyond the power of existing linear RNNs while\nmaintaining a fast parallelizable training process. Our experiments show\ndifferent design choices in Miras yield models with varying strengths. For\nexample, certain instances of Miras achieve exceptional performance in special\ntasks such as language modeling, commonsense reasoning, and recall intensive\ntasks, even outperforming Transformers and other modern linear recurrent\nmodels.",
      "upvotes": 2,
      "discussionId": "6805c4dbb15a57fcb59b6f3d",
      "ai_keywords": [
        "Transformers",
        "Titans",
        "linear recurrent neural networks",
        "associative memory modules",
        "attentional bias",
        "dot-product similarity",
        "L2 regression",
        "retention regularization",
        "forget gates",
        "Miras",
        "Moneta",
        "Yaad",
        "Memora",
        "parallelizable training process",
        "language modeling",
        "commonsense reasoning",
        "recall intensive tasks"
      ]
    },
    "publishedAt": "2025-04-17T13:59:33.000Z",
    "title": "It's All Connected: A Journey Through Test-Time Memorization,\n  Attentional Bias, Retention, and Online Optimization",
    "summary": "Designing efficient and effective architectural backbones has been in the\ncore of research efforts to enhance the capability of foundation models.\nInspired by the human cognitive phenomenon of attentional bias-the natural\ntendency to prioritize certain events or stimuli-we reconceptualize neural\narchitectures, including Transformers, Titans, and modern linear recurrent\nneural networks as associative memory modules that learn a mapping of keys and\nvalues using an internal objective, referred to as attentional bias.\nSurprisingly, we observed that most existing sequence models leverage either\n(1) dot-product similarity, or (2) L2 regression objectives as their\nattentional bias. Going beyond these objectives, we present a set of\nalternative attentional bias configurations along with their effective\napproximations to stabilize their training procedure. We then reinterpret\nforgetting mechanisms in modern deep learning architectures as a form of\nretention regularization, providing a novel set of forget gates for sequence\nmodels. Building upon these insights, we present Miras, a general framework to\ndesign deep learning architectures based on four choices of: (i) associative\nmemory architecture, (ii) attentional bias objective, (iii) retention gate, and\n(iv) memory learning algorithm. We present three novel sequence models-Moneta,\nYaad, and Memora-that go beyond the power of existing linear RNNs while\nmaintaining a fast parallelizable training process. Our experiments show\ndifferent design choices in Miras yield models with varying strengths. For\nexample, certain instances of Miras achieve exceptional performance in special\ntasks such as language modeling, commonsense reasoning, and recall intensive\ntasks, even outperforming Transformers and other modern linear recurrent\nmodels.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13173.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65cccd5134a5d74cbaa9446c",
      "avatarUrl": "/avatars/5255b734628992106598eae4f2c5848f.svg",
      "fullname": "Ali Behrouz",
      "name": "AliBehrouz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.09621",
      "authors": [
        {
          "_id": "6800ef5509eaa9d1d87a6eaf",
          "name": "Jiuchen Chen",
          "hidden": false
        },
        {
          "_id": "6800ef5509eaa9d1d87a6eb0",
          "user": {
            "_id": "6672c01fa6eb488f049ecb80",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6672c01fa6eb488f049ecb80/f6SetkETOWgyXy1KmBPhK.jpeg",
            "isPro": false,
            "fullname": "Xinyu Yan",
            "user": "fengyanzi",
            "type": "user"
          },
          "name": "Xinyu Yan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-19T15:17:09.181Z",
          "hidden": false
        },
        {
          "_id": "6800ef5509eaa9d1d87a6eb1",
          "name": "Qizhi Xu",
          "hidden": false
        },
        {
          "_id": "6800ef5509eaa9d1d87a6eb2",
          "name": "Kaiqi Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-13T15:41:25.000Z",
      "submittedOnDailyAt": "2025-04-21T05:45:47.747Z",
      "title": "Tokenize Image Patches: Global Context Fusion for Effective Haze Removal\n  in Large Images",
      "submittedOnDailyBy": {
        "_id": "6672c01fa6eb488f049ecb80",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6672c01fa6eb488f049ecb80/f6SetkETOWgyXy1KmBPhK.jpeg",
        "isPro": false,
        "fullname": "Xinyu Yan",
        "user": "fengyanzi",
        "type": "user"
      },
      "summary": "Global contextual information and local detail features are essential for\nhaze removal tasks. Deep learning models perform well on small, low-resolution\nimages, but they encounter difficulties with large, high-resolution ones due to\nGPU memory limitations. As a compromise, they often resort to image slicing or\ndownsampling. The former diminishes global information, while the latter\ndiscards high-frequency details. To address these challenges, we propose\nDehazeXL, a haze removal method that effectively balances global context and\nlocal feature extraction, enabling end-to-end modeling of large images on\nmainstream GPU hardware. Additionally, to evaluate the efficiency of global\ncontext utilization in haze removal performance, we design a visual attribution\nmethod tailored to the characteristics of haze removal tasks. Finally,\nrecognizing the lack of benchmark datasets for haze removal in large images, we\nhave developed an ultra-high-resolution haze removal dataset (8KDehaze) to\nsupport model training and testing. It includes 10000 pairs of clear and hazy\nremote sensing images, each sized at 8192 times 8192 pixels. Extensive\nexperiments demonstrate that DehazeXL can infer images up to 10240 times\n10240 pixels with only 21 GB of memory, achieving state-of-the-art results\namong all evaluated methods. The source code and experimental dataset are\navailable at https://github.com/CastleChen339/DehazeXL.",
      "upvotes": 2,
      "discussionId": "6800ef5709eaa9d1d87a6f76",
      "projectPage": "https://castlechen339.github.io/DehazeXL.github.io/",
      "githubRepo": "https://github.com/CastleChen339/DehazeXL",
      "ai_keywords": [
        "haze removal",
        "image slicing",
        "downsampling",
        "DehazeXL",
        "global context",
        "local feature extraction",
        "end-to-end modeling",
        "visual attribution",
        "8KDehaze",
        "ultra-high-resolution haze removal dataset",
        "remote sensing images"
      ]
    },
    "publishedAt": "2025-04-13T11:41:25.000Z",
    "title": "Tokenize Image Patches: Global Context Fusion for Effective Haze Removal\n  in Large Images",
    "summary": "Global contextual information and local detail features are essential for\nhaze removal tasks. Deep learning models perform well on small, low-resolution\nimages, but they encounter difficulties with large, high-resolution ones due to\nGPU memory limitations. As a compromise, they often resort to image slicing or\ndownsampling. The former diminishes global information, while the latter\ndiscards high-frequency details. To address these challenges, we propose\nDehazeXL, a haze removal method that effectively balances global context and\nlocal feature extraction, enabling end-to-end modeling of large images on\nmainstream GPU hardware. Additionally, to evaluate the efficiency of global\ncontext utilization in haze removal performance, we design a visual attribution\nmethod tailored to the characteristics of haze removal tasks. Finally,\nrecognizing the lack of benchmark datasets for haze removal in large images, we\nhave developed an ultra-high-resolution haze removal dataset (8KDehaze) to\nsupport model training and testing. It includes 10000 pairs of clear and hazy\nremote sensing images, each sized at 8192 times 8192 pixels. Extensive\nexperiments demonstrate that DehazeXL can infer images up to 10240 times\n10240 pixels with only 21 GB of memory, achieving state-of-the-art results\namong all evaluated methods. The source code and experimental dataset are\navailable at https://github.com/CastleChen339/DehazeXL.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.09621.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6672c01fa6eb488f049ecb80",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6672c01fa6eb488f049ecb80/f6SetkETOWgyXy1KmBPhK.jpeg",
      "fullname": "Xinyu Yan",
      "name": "fengyanzi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]