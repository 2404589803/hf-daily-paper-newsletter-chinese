[
  {
    "paper": {
      "id": "2504.21635",
      "authors": [
        {
          "_id": "68130be55342cbe1ddefb262",
          "user": {
            "_id": "65704741e1cfce1764ce652e",
            "avatarUrl": "/avatars/9189aaf417426af4ebe381ed364a6c0e.svg",
            "isPro": false,
            "fullname": "Zeina Aldallal",
            "user": "ZeinaD",
            "type": "user"
          },
          "name": "Zeina Aldallal",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-01T05:51:34.441Z",
          "hidden": false
        },
        {
          "_id": "68130be55342cbe1ddefb263",
          "name": "Sara Chrouf",
          "hidden": false
        },
        {
          "_id": "68130be55342cbe1ddefb264",
          "user": {
            "_id": "65276c7911a8a521c91bc10f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65276c7911a8a521c91bc10f/39dbuUtqQTJERTJ_WkxSh.jpeg",
            "isPro": false,
            "fullname": "Khalil Hennara",
            "user": "Hennara",
            "type": "user"
          },
          "name": "Khalil Hennara",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-01T06:55:15.787Z",
          "hidden": false
        },
        {
          "_id": "68130be55342cbe1ddefb265",
          "user": {
            "_id": "63aa7667769a10efc404fbbc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63aa7667769a10efc404fbbc/tn8ZxUmTEMS0Gze7_F7JL.jpeg",
            "isPro": false,
            "fullname": "Mohamed Motasim Hamed",
            "user": "Moatasem444",
            "type": "user"
          },
          "name": "Mohamed Motaism Hamed",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-01T07:00:18.613Z",
          "hidden": false
        },
        {
          "_id": "68130be55342cbe1ddefb266",
          "user": {
            "_id": "6496df4b3c64d75523a11973",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6496df4b3c64d75523a11973/I_Qn5-3Czngle-NsGmabO.jpeg",
            "isPro": false,
            "fullname": "Muhammad Hreden",
            "user": "hr99",
            "type": "user"
          },
          "name": "Muhammad Hreden",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-01T07:16:03.684Z",
          "hidden": false
        },
        {
          "_id": "68130be55342cbe1ddefb267",
          "name": "Safwan AlModhayan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-30T13:37:24.000Z",
      "submittedOnDailyAt": "2025-05-01T05:10:38.792Z",
      "title": "Sadeed: Advancing Arabic Diacritization Through Small Language Model",
      "submittedOnDailyBy": {
        "_id": "65276c7911a8a521c91bc10f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65276c7911a8a521c91bc10f/39dbuUtqQTJERTJ_WkxSh.jpeg",
        "isPro": false,
        "fullname": "Khalil Hennara",
        "user": "Hennara",
        "type": "user"
      },
      "summary": "Arabic text diacritization remains a persistent challenge in natural language\nprocessing due to the language's morphological richness. In this paper, we\nintroduce Sadeed, a novel approach based on a fine-tuned decoder-only language\nmodel adapted from Kuwain 1.5B Hennara et al. [2025], a compact model\noriginally trained on diverse Arabic corpora. Sadeed is fine-tuned on carefully\ncurated, high-quality diacritized datasets, constructed through a rigorous\ndata-cleaning and normalization pipeline. Despite utilizing modest\ncomputational resources, Sadeed achieves competitive results compared to\nproprietary large language models and outperforms traditional models trained on\nsimilar domains. Additionally, we highlight key limitations in current\nbenchmarking practices for Arabic diacritization. To address these issues, we\nintroduce SadeedDiac-25, a new benchmark designed to enable fairer and more\ncomprehensive evaluation across diverse text genres and complexity levels.\nTogether, Sadeed and SadeedDiac-25 provide a robust foundation for advancing\nArabic NLP applications, including machine translation, text-to-speech, and\nlanguage learning tools.",
      "upvotes": 29,
      "discussionId": "68130be65342cbe1ddefb2a6",
      "ai_keywords": [
        "decoder-only language model",
        "Kuwain 1.5B Hennara",
        "fine-tuned",
        "diacritized datasets",
        "data-cleaning",
        "normalization",
        "benchmarking",
        "SadeedDiac-25"
      ]
    },
    "publishedAt": "2025-04-30T09:37:24.000Z",
    "title": "Sadeed: Advancing Arabic Diacritization Through Small Language Model",
    "summary": "Arabic text diacritization remains a persistent challenge in natural language\nprocessing due to the language's morphological richness. In this paper, we\nintroduce Sadeed, a novel approach based on a fine-tuned decoder-only language\nmodel adapted from Kuwain 1.5B Hennara et al. [2025], a compact model\noriginally trained on diverse Arabic corpora. Sadeed is fine-tuned on carefully\ncurated, high-quality diacritized datasets, constructed through a rigorous\ndata-cleaning and normalization pipeline. Despite utilizing modest\ncomputational resources, Sadeed achieves competitive results compared to\nproprietary large language models and outperforms traditional models trained on\nsimilar domains. Additionally, we highlight key limitations in current\nbenchmarking practices for Arabic diacritization. To address these issues, we\nintroduce SadeedDiac-25, a new benchmark designed to enable fairer and more\ncomprehensive evaluation across diverse text genres and complexity levels.\nTogether, Sadeed and SadeedDiac-25 provide a robust foundation for advancing\nArabic NLP applications, including machine translation, text-to-speech, and\nlanguage learning tools.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.21635.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65276c7911a8a521c91bc10f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65276c7911a8a521c91bc10f/39dbuUtqQTJERTJ_WkxSh.jpeg",
      "fullname": "Khalil Hennara",
      "name": "Hennara",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.21776",
      "authors": [
        {
          "_id": "6812d593060494e99e4835e0",
          "name": "Xiaoxi Li",
          "hidden": false
        },
        {
          "_id": "6812d593060494e99e4835e1",
          "name": "Jiajie Jin",
          "hidden": false
        },
        {
          "_id": "6812d593060494e99e4835e2",
          "name": "Guanting Dong",
          "hidden": false
        },
        {
          "_id": "6812d593060494e99e4835e3",
          "name": "Hongjin Qian",
          "hidden": false
        },
        {
          "_id": "6812d593060494e99e4835e4",
          "name": "Yutao Zhu",
          "hidden": false
        },
        {
          "_id": "6812d593060494e99e4835e5",
          "name": "Yongkang Wu",
          "hidden": false
        },
        {
          "_id": "6812d593060494e99e4835e6",
          "name": "Ji-Rong Wen",
          "hidden": false
        },
        {
          "_id": "6812d593060494e99e4835e7",
          "name": "Zhicheng Dou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-30T16:25:25.000Z",
      "submittedOnDailyAt": "2025-05-01T00:33:55.498Z",
      "title": "WebThinker: Empowering Large Reasoning Models with Deep Research\n  Capability",
      "submittedOnDailyBy": {
        "_id": "61cd4b833dd34ba1985e0753",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61cd4b833dd34ba1985e0753/BfHfrwotoMESpXZOHiIe4.png",
        "isPro": false,
        "fullname": "KABI",
        "user": "dongguanting",
        "type": "user"
      },
      "summary": "Large reasoning models (LRMs), such as OpenAI-o1 and DeepSeek-R1, demonstrate\nimpressive long-horizon reasoning capabilities. However, their reliance on\nstatic internal knowledge limits their performance on complex,\nknowledge-intensive tasks and hinders their ability to produce comprehensive\nresearch reports requiring synthesis of diverse web information. To address\nthis, we propose WebThinker, a deep research agent that empowers LRMs\nto autonomously search the web, navigate web pages, and draft research reports\nduring the reasoning process. WebThinker integrates a Deep Web\nExplorer module, enabling LRMs to dynamically search, navigate, and extract\ninformation from the web when encountering knowledge gaps. It also employs an\nAutonomous Think-Search-and-Draft strategy, allowing the model to\nseamlessly interleave reasoning, information gathering, and report writing in\nreal time. To further enhance research tool utilization, we introduce an\nRL-based training strategy via iterative online Direct Preference\nOptimization (DPO). Extensive experiments on complex reasoning benchmarks\n(GPQA, GAIA, WebWalkerQA, HLE) and scientific report generation tasks (Glaive)\ndemonstrate that WebThinker significantly outperforms existing methods and\nstrong proprietary systems. Our approach enhances LRM reliability and\napplicability in complex scenarios, paving the way for more capable and\nversatile deep research systems. The code is available at\nhttps://github.com/RUC-NLPIR/WebThinker.",
      "upvotes": 14,
      "discussionId": "6812d594060494e99e48361c",
      "ai_keywords": [
        "Large reasoning models (LRMs)",
        "WebThinker",
        "Deep Web Explorer",
        "Autonomous Think-Search-and-Draft strategy",
        "RL-based training strategy",
        "iterative online Direct Preference Optimization (DPO)",
        "GPQA",
        "GAIA",
        "WebWalkerQA",
        "HLE",
        "Glaive"
      ]
    },
    "publishedAt": "2025-04-30T12:25:25.000Z",
    "title": "WebThinker: Empowering Large Reasoning Models with Deep Research\n  Capability",
    "summary": "Large reasoning models (LRMs), such as OpenAI-o1 and DeepSeek-R1, demonstrate\nimpressive long-horizon reasoning capabilities. However, their reliance on\nstatic internal knowledge limits their performance on complex,\nknowledge-intensive tasks and hinders their ability to produce comprehensive\nresearch reports requiring synthesis of diverse web information. To address\nthis, we propose WebThinker, a deep research agent that empowers LRMs\nto autonomously search the web, navigate web pages, and draft research reports\nduring the reasoning process. WebThinker integrates a Deep Web\nExplorer module, enabling LRMs to dynamically search, navigate, and extract\ninformation from the web when encountering knowledge gaps. It also employs an\nAutonomous Think-Search-and-Draft strategy, allowing the model to\nseamlessly interleave reasoning, information gathering, and report writing in\nreal time. To further enhance research tool utilization, we introduce an\nRL-based training strategy via iterative online Direct Preference\nOptimization (DPO). Extensive experiments on complex reasoning benchmarks\n(GPQA, GAIA, WebWalkerQA, HLE) and scientific report generation tasks (Glaive)\ndemonstrate that WebThinker significantly outperforms existing methods and\nstrong proprietary systems. Our approach enhances LRM reliability and\napplicability in complex scenarios, paving the way for more capable and\nversatile deep research systems. The code is available at\nhttps://github.com/RUC-NLPIR/WebThinker.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.21776.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61cd4b833dd34ba1985e0753",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61cd4b833dd34ba1985e0753/BfHfrwotoMESpXZOHiIe4.png",
      "fullname": "KABI",
      "name": "dongguanting",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 18
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.21233",
      "authors": [
        {
          "_id": "6812d62ae74b39182bd17c9c",
          "name": "Haoran Xu",
          "hidden": false
        },
        {
          "_id": "6812d62ae74b39182bd17c9d",
          "name": "Baolin Peng",
          "hidden": false
        },
        {
          "_id": "6812d62ae74b39182bd17c9e",
          "name": "Hany Awadalla",
          "hidden": false
        },
        {
          "_id": "6812d62ae74b39182bd17c9f",
          "name": "Dongdong Chen",
          "hidden": false
        },
        {
          "_id": "6812d62ae74b39182bd17ca0",
          "name": "Yen-Chun Chen",
          "hidden": false
        },
        {
          "_id": "6812d62ae74b39182bd17ca1",
          "name": "Mei Gao",
          "hidden": false
        },
        {
          "_id": "6812d62ae74b39182bd17ca2",
          "name": "Young Jin Kim",
          "hidden": false
        },
        {
          "_id": "6812d62ae74b39182bd17ca3",
          "name": "Yunsheng Li",
          "hidden": false
        },
        {
          "_id": "6812d62ae74b39182bd17ca4",
          "name": "Liliang Ren",
          "hidden": false
        },
        {
          "_id": "6812d62ae74b39182bd17ca5",
          "name": "Yelong Shen",
          "hidden": false
        },
        {
          "_id": "6812d62ae74b39182bd17ca6",
          "name": "Shuohang Wang",
          "hidden": false
        },
        {
          "_id": "6812d62ae74b39182bd17ca7",
          "name": "Weijian Xu",
          "hidden": false
        },
        {
          "_id": "6812d62ae74b39182bd17ca8",
          "name": "Jianfeng Gao",
          "hidden": false
        },
        {
          "_id": "6812d62ae74b39182bd17ca9",
          "name": "Weizhu Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-30T00:04:35.000Z",
      "submittedOnDailyAt": "2025-05-01T00:32:47.316Z",
      "title": "Phi-4-Mini-Reasoning: Exploring the Limits of Small Reasoning Language\n  Models in Math",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Chain-of-Thought (CoT) significantly enhances formal reasoning capabilities\nin Large Language Models (LLMs) by training them to explicitly generate\nintermediate reasoning steps. While LLMs readily benefit from such techniques,\nimproving reasoning in Small Language Models (SLMs) remains challenging due to\ntheir limited model capacity. Recent work by Deepseek-R1 demonstrates that\ndistillation from LLM-generated synthetic data can substantially improve the\nreasoning ability of SLM. However, the detailed modeling recipe is not\ndisclosed. In this work, we present a systematic training recipe for SLMs that\nconsists of four steps: (1) large-scale mid-training on diverse distilled\nlong-CoT data, (2) supervised fine-tuning on high-quality long-CoT data, (3)\nRollout DPO leveraging a carefully curated preference dataset, and (4)\nReinforcement Learning (RL) with Verifiable Reward. We apply our method on\nPhi-4-Mini, a compact 3.8B-parameter model. The resulting Phi-4-Mini-Reasoning\nmodel exceeds, on math reasoning tasks, much larger reasoning models, e.g.,\noutperforming DeepSeek-R1-Distill-Qwen-7B by 3.2 points and\nDeepSeek-R1-Distill-Llama-8B by 7.7 points on Math-500. Our results validate\nthat a carefully designed training recipe, with large-scale high-quality CoT\ndata, is effective to unlock strong reasoning capabilities even in\nresource-constrained small models.",
      "upvotes": 12,
      "discussionId": "6812d62be74b39182bd17cdb",
      "ai_keywords": [
        "Chain-of-Thought (CoT)",
        "Large Language Models (LLMs)",
        "Small Language Models (SLMs)",
        "distillation",
        "synthetic data",
        "mid-training",
        "diverse distilled long-CoT data",
        "supervised fine-tuning",
        "high-quality long-CoT data",
        "Rollout DPO",
        "preference dataset",
        "Reinforcement Learning (RL)",
        "Verifiable Reward",
        "Phi-4-Mini",
        "resource-constrained small models"
      ]
    },
    "publishedAt": "2025-04-29T20:04:35.000Z",
    "title": "Phi-4-Mini-Reasoning: Exploring the Limits of Small Reasoning Language\n  Models in Math",
    "summary": "Chain-of-Thought (CoT) significantly enhances formal reasoning capabilities\nin Large Language Models (LLMs) by training them to explicitly generate\nintermediate reasoning steps. While LLMs readily benefit from such techniques,\nimproving reasoning in Small Language Models (SLMs) remains challenging due to\ntheir limited model capacity. Recent work by Deepseek-R1 demonstrates that\ndistillation from LLM-generated synthetic data can substantially improve the\nreasoning ability of SLM. However, the detailed modeling recipe is not\ndisclosed. In this work, we present a systematic training recipe for SLMs that\nconsists of four steps: (1) large-scale mid-training on diverse distilled\nlong-CoT data, (2) supervised fine-tuning on high-quality long-CoT data, (3)\nRollout DPO leveraging a carefully curated preference dataset, and (4)\nReinforcement Learning (RL) with Verifiable Reward. We apply our method on\nPhi-4-Mini, a compact 3.8B-parameter model. The resulting Phi-4-Mini-Reasoning\nmodel exceeds, on math reasoning tasks, much larger reasoning models, e.g.,\noutperforming DeepSeek-R1-Distill-Qwen-7B by 3.2 points and\nDeepSeek-R1-Distill-Llama-8B by 7.7 points on Math-500. Our results validate\nthat a carefully designed training recipe, with large-scale high-quality CoT\ndata, is effective to unlock strong reasoning capabilities even in\nresource-constrained small models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.21233.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6751
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.20966",
      "authors": [
        {
          "_id": "6812e473060494e99e4c5116",
          "name": "Zayd M. K. Zuhri",
          "hidden": false
        },
        {
          "_id": "6812e473060494e99e4c5117",
          "name": "Erland Hilman Fuadi",
          "hidden": false
        },
        {
          "_id": "6812e473060494e99e4c5118",
          "name": "Alham Fikri Aji",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/60cf8a354061635e43b28f60/ZQTvDnn0F_MaUg5zlVk62.png"
      ],
      "publishedAt": "2025-04-29T17:36:18.000Z",
      "submittedOnDailyAt": "2025-05-01T01:38:03.643Z",
      "title": "Softpick: No Attention Sink, No Massive Activations with Rectified\n  Softmax",
      "submittedOnDailyBy": {
        "_id": "60cf8a354061635e43b28f60",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60cf8a354061635e43b28f60/o8gv9mG5cvnopJZJnNibi.jpeg",
        "isPro": true,
        "fullname": "Zayd Muhammad Kawakibi Zuhri",
        "user": "zaydzuhri",
        "type": "user"
      },
      "summary": "We introduce softpick, a rectified, not sum-to-one, drop-in replacement for\nsoftmax in transformer attention mechanisms that eliminates attention sink and\nmassive activations. Our experiments with 340M parameter models demonstrate\nthat softpick maintains performance parity with softmax on standard benchmarks\nwhile achieving 0% sink rate. The softpick transformer produces hidden states\nwith significantly lower kurtosis (340 vs 33,510) and creates sparse attention\nmaps (46.97% sparsity). Models using softpick consistently outperform softmax\nwhen quantized, with particularly pronounced advantages at lower bit\nprecisions. Our analysis and discussion shows how softpick has the potential to\nopen new possibilities for quantization, low-precision training, sparsity\noptimization, pruning, and interpretability. Our code is available at\nhttps://github.com/zaydzuhri/softpick-attention.",
      "upvotes": 9,
      "discussionId": "6812e475060494e99e4c519f",
      "ai_keywords": [
        "softpick",
        "rectified",
        "drop-in replacement",
        "softmax",
        "transformer attention mechanisms",
        "attention sink",
        "massive activations",
        "performance parity",
        "kurtosis",
        "sparse attention maps",
        "quantized",
        "bit precisions",
        "quantization",
        "low-precision training",
        "sparsity optimization",
        "pruning",
        "interpretability"
      ]
    },
    "publishedAt": "2025-04-29T13:36:18.000Z",
    "title": "Softpick: No Attention Sink, No Massive Activations with Rectified\n  Softmax",
    "summary": "We introduce softpick, a rectified, not sum-to-one, drop-in replacement for\nsoftmax in transformer attention mechanisms that eliminates attention sink and\nmassive activations. Our experiments with 340M parameter models demonstrate\nthat softpick maintains performance parity with softmax on standard benchmarks\nwhile achieving 0% sink rate. The softpick transformer produces hidden states\nwith significantly lower kurtosis (340 vs 33,510) and creates sparse attention\nmaps (46.97% sparsity). Models using softpick consistently outperform softmax\nwhen quantized, with particularly pronounced advantages at lower bit\nprecisions. Our analysis and discussion shows how softpick has the potential to\nopen new possibilities for quantization, low-precision training, sparsity\noptimization, pruning, and interpretability. Our code is available at\nhttps://github.com/zaydzuhri/softpick-attention.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/60cf8a354061635e43b28f60/ZQTvDnn0F_MaUg5zlVk62.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20966.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "60cf8a354061635e43b28f60",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60cf8a354061635e43b28f60/o8gv9mG5cvnopJZJnNibi.jpeg",
      "fullname": "Zayd Muhammad Kawakibi Zuhri",
      "name": "zaydzuhri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.21318",
      "authors": [
        {
          "_id": "6812d3c3e74b39182bd0dc82",
          "name": "Marah Abdin",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc83",
          "name": "Sahaj Agarwal",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc84",
          "name": "Ahmed Awadallah",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc85",
          "name": "Vidhisha Balachandran",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc86",
          "name": "Harkirat Behl",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc87",
          "name": "Lingjiao Chen",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc88",
          "name": "Gustavo de Rosa",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc89",
          "name": "Suriya Gunasekar",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc8a",
          "name": "Mojan Javaheripi",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc8b",
          "name": "Neel Joshi",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc8c",
          "name": "Piero Kauffmann",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc8d",
          "name": "Yash Lara",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc8e",
          "name": "Caio CÃ©sar Teodoro Mendes",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc8f",
          "name": "Arindam Mitra",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc90",
          "name": "Besmira Nushi",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc91",
          "name": "Dimitris Papailiopoulos",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc92",
          "name": "Olli Saarikivi",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc93",
          "name": "Shital Shah",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc94",
          "name": "Vaishnavi Shrivastava",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc95",
          "name": "Vibhav Vineet",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc96",
          "name": "Yue Wu",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc97",
          "name": "Safoora Yousefi",
          "hidden": false
        },
        {
          "_id": "6812d3c3e74b39182bd0dc98",
          "name": "Guoqing Zheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-30T05:05:09.000Z",
      "submittedOnDailyAt": "2025-05-01T00:22:34.583Z",
      "title": "Phi-4-reasoning Technical Report",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "We introduce Phi-4-reasoning, a 14-billion parameter reasoning model that\nachieves strong performance on complex reasoning tasks. Trained via supervised\nfine-tuning of Phi-4 on carefully curated set of \"teachable\" prompts-selected\nfor the right level of complexity and diversity-and reasoning demonstrations\ngenerated using o3-mini, Phi-4-reasoning generates detailed reasoning chains\nthat effectively leverage inference-time compute. We further develop\nPhi-4-reasoning-plus, a variant enhanced through a short phase of outcome-based\nreinforcement learning that offers higher performance by generating longer\nreasoning traces. Across a wide range of reasoning tasks, both models\noutperform significantly larger open-weight models such as\nDeepSeek-R1-Distill-Llama-70B model and approach the performance levels of full\nDeepSeek-R1 model. Our comprehensive evaluations span benchmarks in math and\nscientific reasoning, coding, algorithmic problem solving, planning, and\nspatial understanding. Interestingly, we observe a non-trivial transfer of\nimprovements to general-purpose benchmarks as well. In this report, we provide\ninsights into our training data, our training methodologies, and our\nevaluations. We show that the benefit of careful data curation for supervised\nfine-tuning (SFT) extends to reasoning language models, and can be further\namplified by reinforcement learning (RL). Finally, our evaluation points to\nopportunities for improving how we assess the performance and robustness of\nreasoning models.",
      "upvotes": 7,
      "discussionId": "6812d3c4e74b39182bd0dcd1",
      "ai_keywords": [
        "parameter reasoning model",
        "supervised fine-tuning",
        "reasoning demonstrations",
        "inference-time compute",
        "outcome-based reinforcement learning",
        "reasoning chains",
        "reasoning traces",
        "reasoning language models",
        "general-purpose benchmarks",
        "performance assessment",
        "robustness assessment"
      ]
    },
    "publishedAt": "2025-04-30T01:05:09.000Z",
    "title": "Phi-4-reasoning Technical Report",
    "summary": "We introduce Phi-4-reasoning, a 14-billion parameter reasoning model that\nachieves strong performance on complex reasoning tasks. Trained via supervised\nfine-tuning of Phi-4 on carefully curated set of \"teachable\" prompts-selected\nfor the right level of complexity and diversity-and reasoning demonstrations\ngenerated using o3-mini, Phi-4-reasoning generates detailed reasoning chains\nthat effectively leverage inference-time compute. We further develop\nPhi-4-reasoning-plus, a variant enhanced through a short phase of outcome-based\nreinforcement learning that offers higher performance by generating longer\nreasoning traces. Across a wide range of reasoning tasks, both models\noutperform significantly larger open-weight models such as\nDeepSeek-R1-Distill-Llama-70B model and approach the performance levels of full\nDeepSeek-R1 model. Our comprehensive evaluations span benchmarks in math and\nscientific reasoning, coding, algorithmic problem solving, planning, and\nspatial understanding. Interestingly, we observe a non-trivial transfer of\nimprovements to general-purpose benchmarks as well. In this report, we provide\ninsights into our training data, our training methodologies, and our\nevaluations. We show that the benefit of careful data curation for supervised\nfine-tuning (SFT) extends to reasoning language models, and can be further\namplified by reinforcement learning (RL). Finally, our evaluation points to\nopportunities for improving how we assess the performance and robustness of\nreasoning models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.21318.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6751
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.19720",
      "authors": [
        {
          "_id": "68118b63439785d7ff81a879",
          "user": {
            "_id": "647e96507f9ad5e44bac50bd",
            "avatarUrl": "/avatars/2551f99401540676cfe5cd0ed99e70c1.svg",
            "isPro": false,
            "fullname": "Ranran Zhen",
            "user": "zenRRan",
            "type": "user"
          },
          "name": "Ranran Zhen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-30T07:56:15.143Z",
          "hidden": false
        },
        {
          "_id": "68118b63439785d7ff81a87a",
          "name": "Juntao Li",
          "hidden": false
        },
        {
          "_id": "68118b63439785d7ff81a87b",
          "name": "Yixin Ji",
          "hidden": false
        },
        {
          "_id": "68118b63439785d7ff81a87c",
          "name": "Zhenlin Yang",
          "hidden": false
        },
        {
          "_id": "68118b63439785d7ff81a87d",
          "name": "Tong Liu",
          "hidden": false
        },
        {
          "_id": "68118b63439785d7ff81a87e",
          "name": "Qingrong Xia",
          "hidden": false
        },
        {
          "_id": "68118b63439785d7ff81a87f",
          "name": "Xinyu Duan",
          "hidden": false
        },
        {
          "_id": "68118b63439785d7ff81a880",
          "name": "Zhefeng Wang",
          "hidden": false
        },
        {
          "_id": "68118b63439785d7ff81a881",
          "name": "Baoxing Huai",
          "hidden": false
        },
        {
          "_id": "68118b63439785d7ff81a882",
          "name": "Min Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-28T12:14:02.000Z",
      "submittedOnDailyAt": "2025-05-01T02:08:35.108Z",
      "title": "Taming the Titans: A Survey of Efficient LLM Inference Serving",
      "submittedOnDailyBy": {
        "_id": "647e96507f9ad5e44bac50bd",
        "avatarUrl": "/avatars/2551f99401540676cfe5cd0ed99e70c1.svg",
        "isPro": false,
        "fullname": "Ranran Zhen",
        "user": "zenRRan",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) for Generative AI have achieved remarkable\nprogress, evolving into sophisticated and versatile tools widely adopted across\nvarious domains and applications. However, the substantial memory overhead\ncaused by their vast number of parameters, combined with the high computational\ndemands of the attention mechanism, poses significant challenges in achieving\nlow latency and high throughput for LLM inference services. Recent\nadvancements, driven by groundbreaking research, have significantly accelerated\nprogress in this field. This paper provides a comprehensive survey of these\nmethods, covering fundamental instance-level approaches, in-depth cluster-level\nstrategies, emerging scenario directions, and other miscellaneous but important\nareas. At the instance level, we review model placement, request scheduling,\ndecoding length prediction, storage management, and the disaggregation\nparadigm. At the cluster level, we explore GPU cluster deployment,\nmulti-instance load balancing, and cloud service solutions. For emerging\nscenarios, we organize the discussion around specific tasks, modules, and\nauxiliary methods. To ensure a holistic overview, we also highlight several\nniche yet critical areas. Finally, we outline potential research directions to\nfurther advance the field of LLM inference serving.",
      "upvotes": 5,
      "discussionId": "68118b64439785d7ff81a8cd",
      "githubRepo": "https://github.com/zenrran4nlp/Awesome-LLM-Inference-Serving",
      "ai_keywords": [
        "large language models (LLMs)",
        "generative AI",
        "attention mechanism",
        "model placement",
        "request scheduling",
        "decoding length prediction",
        "storage management",
        "disaggregation paradigm",
        "GPU cluster deployment",
        "multi-instance load balancing",
        "cloud service solutions"
      ]
    },
    "publishedAt": "2025-04-28T08:14:02.000Z",
    "title": "Taming the Titans: A Survey of Efficient LLM Inference Serving",
    "summary": "Large Language Models (LLMs) for Generative AI have achieved remarkable\nprogress, evolving into sophisticated and versatile tools widely adopted across\nvarious domains and applications. However, the substantial memory overhead\ncaused by their vast number of parameters, combined with the high computational\ndemands of the attention mechanism, poses significant challenges in achieving\nlow latency and high throughput for LLM inference services. Recent\nadvancements, driven by groundbreaking research, have significantly accelerated\nprogress in this field. This paper provides a comprehensive survey of these\nmethods, covering fundamental instance-level approaches, in-depth cluster-level\nstrategies, emerging scenario directions, and other miscellaneous but important\nareas. At the instance level, we review model placement, request scheduling,\ndecoding length prediction, storage management, and the disaggregation\nparadigm. At the cluster level, we explore GPU cluster deployment,\nmulti-instance load balancing, and cloud service solutions. For emerging\nscenarios, we organize the discussion around specific tasks, modules, and\nauxiliary methods. To ensure a holistic overview, we also highlight several\nniche yet critical areas. Finally, we outline potential research directions to\nfurther advance the field of LLM inference serving.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.19720.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647e96507f9ad5e44bac50bd",
      "avatarUrl": "/avatars/2551f99401540676cfe5cd0ed99e70c1.svg",
      "fullname": "Ranran Zhen",
      "name": "zenRRan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.18904",
      "authors": [
        {
          "_id": "6812d36790b45f422b0bfdc2",
          "name": "Haoran Geng",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdc3",
          "name": "Feishi Wang",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdc4",
          "name": "Songlin Wei",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdc5",
          "name": "Yuyang Li",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdc6",
          "name": "Bangjun Wang",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdc7",
          "name": "Boshi An",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdc8",
          "name": "Charlie Tianyue Cheng",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdc9",
          "name": "Haozhe Lou",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdca",
          "name": "Peihao Li",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdcb",
          "name": "Yen-Jen Wang",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdcc",
          "name": "Yutong Liang",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdcd",
          "name": "Dylan Goetting",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdce",
          "name": "Chaoyi Xu",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdcf",
          "name": "Haozhe Chen",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdd0",
          "name": "Yuxi Qian",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdd1",
          "name": "Yiran Geng",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdd2",
          "name": "Jiageng Mao",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdd3",
          "name": "Weikang Wan",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdd4",
          "name": "Mingtong Zhang",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdd5",
          "name": "Jiangran Lyu",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdd6",
          "name": "Siheng Zhao",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdd7",
          "name": "Jiazhao Zhang",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdd8",
          "name": "Jialiang Zhang",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdd9",
          "name": "Chengyang Zhao",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdda",
          "name": "Haoran Lu",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfddb",
          "name": "Yufei Ding",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfddc",
          "name": "Ran Gong",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfddd",
          "name": "Yuran Wang",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfdde",
          "name": "Yuxuan Kuang",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfddf",
          "name": "Ruihai Wu",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfde0",
          "name": "Baoxiong Jia",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfde1",
          "name": "Carlo Sferrazza",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfde2",
          "name": "Hao Dong",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfde3",
          "name": "Siyuan Huang",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfde4",
          "name": "Yue Wang",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfde5",
          "name": "Jitendra Malik",
          "hidden": false
        },
        {
          "_id": "6812d36790b45f422b0bfde6",
          "name": "Pieter Abbeel",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-26T12:31:04.000Z",
      "submittedOnDailyAt": "2025-05-01T00:21:25.704Z",
      "title": "RoboVerse: Towards a Unified Platform, Dataset and Benchmark for\n  Scalable and Generalizable Robot Learning",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Data scaling and standardized evaluation benchmarks have driven significant\nadvances in natural language processing and computer vision. However, robotics\nfaces unique challenges in scaling data and establishing evaluation protocols.\nCollecting real-world data is resource-intensive and inefficient, while\nbenchmarking in real-world scenarios remains highly complex. Synthetic data and\nsimulation offer promising alternatives, yet existing efforts often fall short\nin data quality, diversity, and benchmark standardization. To address these\nchallenges, we introduce RoboVerse, a comprehensive framework comprising a\nsimulation platform, a synthetic dataset, and unified benchmarks. Our\nsimulation platform supports multiple simulators and robotic embodiments,\nenabling seamless transitions between different environments. The synthetic\ndataset, featuring high-fidelity physics and photorealistic rendering, is\nconstructed through multiple approaches. Additionally, we propose unified\nbenchmarks for imitation learning and reinforcement learning, enabling\nevaluation across different levels of generalization. At the core of the\nsimulation platform is MetaSim, an infrastructure that abstracts diverse\nsimulation environments into a universal interface. It restructures existing\nsimulation environments into a simulator-agnostic configuration system, as well\nas an API aligning different simulator functionalities, such as launching\nsimulation environments, loading assets with initial states, stepping the\nphysics engine, etc. This abstraction ensures interoperability and\nextensibility. Comprehensive experiments demonstrate that RoboVerse enhances\nthe performance of imitation learning, reinforcement learning, world model\nlearning, and sim-to-real transfer. These results validate the reliability of\nour dataset and benchmarks, establishing RoboVerse as a robust solution for\nadvancing robot learning.",
      "upvotes": 5,
      "discussionId": "6812d36c90b45f422b0bff56",
      "ai_keywords": [
        "simulation platform",
        "synthetic dataset",
        "unified benchmarks",
        "MetaSim",
        "simulator-agnostic configuration system",
        "API",
        "physics engine",
        "imitation learning",
        "reinforcement learning",
        "world model learning",
        "sim-to-real transfer"
      ]
    },
    "publishedAt": "2025-04-26T08:31:04.000Z",
    "title": "RoboVerse: Towards a Unified Platform, Dataset and Benchmark for\n  Scalable and Generalizable Robot Learning",
    "summary": "Data scaling and standardized evaluation benchmarks have driven significant\nadvances in natural language processing and computer vision. However, robotics\nfaces unique challenges in scaling data and establishing evaluation protocols.\nCollecting real-world data is resource-intensive and inefficient, while\nbenchmarking in real-world scenarios remains highly complex. Synthetic data and\nsimulation offer promising alternatives, yet existing efforts often fall short\nin data quality, diversity, and benchmark standardization. To address these\nchallenges, we introduce RoboVerse, a comprehensive framework comprising a\nsimulation platform, a synthetic dataset, and unified benchmarks. Our\nsimulation platform supports multiple simulators and robotic embodiments,\nenabling seamless transitions between different environments. The synthetic\ndataset, featuring high-fidelity physics and photorealistic rendering, is\nconstructed through multiple approaches. Additionally, we propose unified\nbenchmarks for imitation learning and reinforcement learning, enabling\nevaluation across different levels of generalization. At the core of the\nsimulation platform is MetaSim, an infrastructure that abstracts diverse\nsimulation environments into a universal interface. It restructures existing\nsimulation environments into a simulator-agnostic configuration system, as well\nas an API aligning different simulator functionalities, such as launching\nsimulation environments, loading assets with initial states, stepping the\nphysics engine, etc. This abstraction ensures interoperability and\nextensibility. Comprehensive experiments demonstrate that RoboVerse enhances\nthe performance of imitation learning, reinforcement learning, world model\nlearning, and sim-to-real transfer. These results validate the reliability of\nour dataset and benchmarks, establishing RoboVerse as a robust solution for\nadvancing robot learning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.18904.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6751
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.21850",
      "authors": [
        {
          "_id": "6812d4d6ce88881cb51b3b70",
          "name": "Xindi Wu",
          "hidden": false
        },
        {
          "_id": "6812d4d6ce88881cb51b3b71",
          "name": "Hee Seung Hwang",
          "hidden": false
        },
        {
          "_id": "6812d4d6ce88881cb51b3b72",
          "name": "Polina Kirichenko",
          "hidden": false
        },
        {
          "_id": "6812d4d6ce88881cb51b3b73",
          "name": "Olga Russakovsky",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-30T17:57:22.000Z",
      "submittedOnDailyAt": "2025-05-01T00:32:13.807Z",
      "title": "COMPACT: COMPositional Atomic-to-Complex Visual Capability Tuning",
      "submittedOnDailyBy": {
        "_id": "613940c0905b1938233881e3",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/613940c0905b1938233881e3/Vb4kFWKEq6AILUP9KmCZA.png",
        "isPro": false,
        "fullname": "Xindi Wu",
        "user": "xindiw",
        "type": "user"
      },
      "summary": "Multimodal Large Language Models (MLLMs) excel at simple vision-language\ntasks but struggle when faced with complex tasks that require multiple\ncapabilities, such as simultaneously recognizing objects, counting them, and\nunderstanding their spatial relationships. This might be partially the result\nof the fact that Visual Instruction Tuning (VIT), a critical training step for\nMLLMs, has traditionally focused on scaling data volume, but not the\ncompositional complexity of training examples. We propose COMPACT\n(COMPositional Atomic-to-complex visual Capability Tuning), which generates a\ntraining dataset explicitly controlling for the compositional complexity of the\ntraining examples. The data from COMPACT allows MLLMs to train on combinations\nof atomic capabilities to learn complex capabilities more efficiently. Across\nall benchmarks, COMPACT achieves comparable performance to the LLaVA-665k VIT\nwhile using less than 10% of its data budget, and even outperforms it on\nseveral, especially those involving complex multi-capability tasks. For\nexample, COMPACT achieves substantial 83.3% improvement on MMStar and 94.0%\nimprovement on MM-Vet compared to the full-scale VIT on particularly complex\nquestions that require four or more atomic capabilities. COMPACT offers a\nscalable, data-efficient, visual compositional tuning recipe to improve on\ncomplex visual-language tasks.",
      "upvotes": 4,
      "discussionId": "6812d4d7ce88881cb51b3bad",
      "projectPage": "https://princetonvisualai.github.io/compact/",
      "githubRepo": "https://github.com/princetonvisualai/compact",
      "ai_keywords": [
        "Multimodal Large Language Models (MLLMs)",
        "Visual Instruction Tuning (VIT)",
        "compositional complexity",
        "COMPACT (COMPositional Atomic-to-complex visual Capability Tuning)",
        "atomic capabilities",
        "complex capabilities",
        "MMStar",
        "MM-Vet",
        "visual compositional tuning"
      ]
    },
    "publishedAt": "2025-04-30T13:57:22.000Z",
    "title": "COMPACT: COMPositional Atomic-to-Complex Visual Capability Tuning",
    "summary": "Multimodal Large Language Models (MLLMs) excel at simple vision-language\ntasks but struggle when faced with complex tasks that require multiple\ncapabilities, such as simultaneously recognizing objects, counting them, and\nunderstanding their spatial relationships. This might be partially the result\nof the fact that Visual Instruction Tuning (VIT), a critical training step for\nMLLMs, has traditionally focused on scaling data volume, but not the\ncompositional complexity of training examples. We propose COMPACT\n(COMPositional Atomic-to-complex visual Capability Tuning), which generates a\ntraining dataset explicitly controlling for the compositional complexity of the\ntraining examples. The data from COMPACT allows MLLMs to train on combinations\nof atomic capabilities to learn complex capabilities more efficiently. Across\nall benchmarks, COMPACT achieves comparable performance to the LLaVA-665k VIT\nwhile using less than 10% of its data budget, and even outperforms it on\nseveral, especially those involving complex multi-capability tasks. For\nexample, COMPACT achieves substantial 83.3% improvement on MMStar and 94.0%\nimprovement on MM-Vet compared to the full-scale VIT on particularly complex\nquestions that require four or more atomic capabilities. COMPACT offers a\nscalable, data-efficient, visual compositional tuning recipe to improve on\ncomplex visual-language tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.21850.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "613940c0905b1938233881e3",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/613940c0905b1938233881e3/Vb4kFWKEq6AILUP9KmCZA.png",
      "fullname": "Xindi Wu",
      "name": "xindiw",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.21855",
      "authors": [
        {
          "_id": "6812db1567abbb1d11fb4e9d",
          "name": "Qihao Liu",
          "hidden": false
        },
        {
          "_id": "6812db1567abbb1d11fb4e9e",
          "name": "Ju He",
          "hidden": false
        },
        {
          "_id": "6812db1567abbb1d11fb4e9f",
          "name": "Qihang Yu",
          "hidden": false
        },
        {
          "_id": "6812db1567abbb1d11fb4ea0",
          "name": "Liang-Chieh Chen",
          "hidden": false
        },
        {
          "_id": "6812db1567abbb1d11fb4ea1",
          "name": "Alan Yuille",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/639f1e519f1f2baab2f00d22/h3IWiXbIAOy1NxTWKDAkO.mp4"
      ],
      "publishedAt": "2025-04-30T17:59:56.000Z",
      "submittedOnDailyAt": "2025-05-01T00:56:11.135Z",
      "title": "ReVision: High-Quality, Low-Cost Video Generation with Explicit 3D\n  Physics Modeling for Complex Motion and Interaction",
      "submittedOnDailyBy": {
        "_id": "639f1e519f1f2baab2f00d22",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/639f1e519f1f2baab2f00d22/pFjd51WZuVZ3A11rItvmk.jpeg",
        "isPro": true,
        "fullname": "Qihao Liu",
        "user": "QHL067",
        "type": "user"
      },
      "summary": "In recent years, video generation has seen significant advancements. However,\nchallenges still persist in generating complex motions and interactions. To\naddress these challenges, we introduce ReVision, a plug-and-play framework that\nexplicitly integrates parameterized 3D physical knowledge into a pretrained\nconditional video generation model, significantly enhancing its ability to\ngenerate high-quality videos with complex motion and interactions.\nSpecifically, ReVision consists of three stages. First, a video diffusion model\nis used to generate a coarse video. Next, we extract a set of 2D and 3D\nfeatures from the coarse video to construct a 3D object-centric representation,\nwhich is then refined by our proposed parameterized physical prior model to\nproduce an accurate 3D motion sequence. Finally, this refined motion sequence\nis fed back into the same video diffusion model as additional conditioning,\nenabling the generation of motion-consistent videos, even in scenarios\ninvolving complex actions and interactions. We validate the effectiveness of\nour approach on Stable Video Diffusion, where ReVision significantly improves\nmotion fidelity and coherence. Remarkably, with only 1.5B parameters, it even\noutperforms a state-of-the-art video generation model with over 13B parameters\non complex video generation by a substantial margin. Our results suggest that,\nby incorporating 3D physical knowledge, even a relatively small video diffusion\nmodel can generate complex motions and interactions with greater realism and\ncontrollability, offering a promising solution for physically plausible video\ngeneration.",
      "upvotes": 2,
      "discussionId": "6812db1767abbb1d11fb4f42",
      "projectPage": "https://revision-video.github.io/",
      "ai_keywords": [
        "video diffusion model",
        "3D object-centric representation",
        "parameterized physical prior model",
        "motion-consistent videos",
        "3D physical knowledge",
        "motion fidelity",
        "coherence",
        "physically plausible video generation"
      ]
    },
    "publishedAt": "2025-04-30T13:59:56.000Z",
    "title": "ReVision: High-Quality, Low-Cost Video Generation with Explicit 3D\n  Physics Modeling for Complex Motion and Interaction",
    "summary": "In recent years, video generation has seen significant advancements. However,\nchallenges still persist in generating complex motions and interactions. To\naddress these challenges, we introduce ReVision, a plug-and-play framework that\nexplicitly integrates parameterized 3D physical knowledge into a pretrained\nconditional video generation model, significantly enhancing its ability to\ngenerate high-quality videos with complex motion and interactions.\nSpecifically, ReVision consists of three stages. First, a video diffusion model\nis used to generate a coarse video. Next, we extract a set of 2D and 3D\nfeatures from the coarse video to construct a 3D object-centric representation,\nwhich is then refined by our proposed parameterized physical prior model to\nproduce an accurate 3D motion sequence. Finally, this refined motion sequence\nis fed back into the same video diffusion model as additional conditioning,\nenabling the generation of motion-consistent videos, even in scenarios\ninvolving complex actions and interactions. We validate the effectiveness of\nour approach on Stable Video Diffusion, where ReVision significantly improves\nmotion fidelity and coherence. Remarkably, with only 1.5B parameters, it even\noutperforms a state-of-the-art video generation model with over 13B parameters\non complex video generation by a substantial margin. Our results suggest that,\nby incorporating 3D physical knowledge, even a relatively small video diffusion\nmodel can generate complex motions and interactions with greater realism and\ncontrollability, offering a promising solution for physically plausible video\ngeneration.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/639f1e519f1f2baab2f00d22/h3IWiXbIAOy1NxTWKDAkO.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.21855.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "639f1e519f1f2baab2f00d22",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/639f1e519f1f2baab2f00d22/pFjd51WZuVZ3A11rItvmk.jpeg",
      "fullname": "Qihao Liu",
      "name": "QHL067",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.21039",
      "authors": [
        {
          "_id": "6812f89338bee548818e68a1",
          "user": {
            "_id": "6573a9fe769f3ee9bdf4d9c7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/xC41F7Vp9SVzVHc3cUiRU.jpeg",
            "isPro": false,
            "fullname": "Paul Kassianik",
            "user": "paulkass",
            "type": "user"
          },
          "name": "Paul Kassianik",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-01T04:29:53.088Z",
          "hidden": false
        },
        {
          "_id": "6812f89338bee548818e68a2",
          "name": "Baturay Saglam",
          "hidden": false
        },
        {
          "_id": "6812f89338bee548818e68a3",
          "name": "Alexander Chen",
          "hidden": false
        },
        {
          "_id": "6812f89338bee548818e68a4",
          "name": "Blaine Nelson",
          "hidden": false
        },
        {
          "_id": "6812f89338bee548818e68a5",
          "name": "Anu Vellore",
          "hidden": false
        },
        {
          "_id": "6812f89338bee548818e68a6",
          "name": "Massimo Aufiero",
          "hidden": false
        },
        {
          "_id": "6812f89338bee548818e68a7",
          "name": "Fraser Burch",
          "hidden": false
        },
        {
          "_id": "6812f89338bee548818e68a8",
          "name": "Dhruv Kedia",
          "hidden": false
        },
        {
          "_id": "6812f89338bee548818e68a9",
          "name": "Avi Zohary",
          "hidden": false
        },
        {
          "_id": "6812f89338bee548818e68aa",
          "name": "Sajana Weerawardhena",
          "hidden": false
        },
        {
          "_id": "6812f89338bee548818e68ab",
          "name": "Aman Priyanshu",
          "hidden": false
        },
        {
          "_id": "6812f89338bee548818e68ac",
          "name": "Adam Swanda",
          "hidden": false
        },
        {
          "_id": "6812f89338bee548818e68ad",
          "name": "Amy Chang",
          "hidden": false
        },
        {
          "_id": "6812f89338bee548818e68ae",
          "name": "Hyrum Anderson",
          "hidden": false
        },
        {
          "_id": "6812f89338bee548818e68af",
          "name": "Kojin Oshiba",
          "hidden": false
        },
        {
          "_id": "6812f89338bee548818e68b0",
          "name": "Omar Santos",
          "hidden": false
        },
        {
          "_id": "6812f89338bee548818e68b1",
          "name": "Yaron Singer",
          "hidden": false
        },
        {
          "_id": "6812f89338bee548818e68b2",
          "name": "Amin Karbasi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-28T08:41:12.000Z",
      "submittedOnDailyAt": "2025-05-01T03:07:21.083Z",
      "title": "Llama-3.1-FoundationAI-SecurityLLM-Base-8B Technical Report",
      "submittedOnDailyBy": {
        "_id": "620042b28c2eb991da50d34e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/620042b28c2eb991da50d34e/Q5cj1GIq3XnKj-K64Mtyd.jpeg",
        "isPro": true,
        "fullname": "Aman Priyanshu",
        "user": "AmanPriyanshu",
        "type": "user"
      },
      "summary": "As transformer-based large language models (LLMs) increasingly permeate\nsociety, they have revolutionized domains such as software engineering,\ncreative writing, and digital arts. However, their adoption in cybersecurity\nremains limited due to challenges like scarcity of specialized training data\nand complexity of representing cybersecurity-specific knowledge. To address\nthese gaps, we present Foundation-Sec-8B, a cybersecurity-focused LLM built on\nthe Llama 3.1 architecture and enhanced through continued pretraining on a\ncarefully curated cybersecurity corpus. We evaluate Foundation-Sec-8B across\nboth established and new cybersecurity benchmarks, showing that it matches\nLlama 3.1-70B and GPT-4o-mini in certain cybersecurity-specific tasks. By\nreleasing our model to the public, we aim to accelerate progress and adoption\nof AI-driven tools in both public and private cybersecurity contexts.",
      "upvotes": 2,
      "discussionId": "6812f89338bee548818e68e3",
      "ai_keywords": [
        "transformer-based large language models (LLMs)",
        "cybersecurity-focused LLM",
        "Llama 3.1 architecture",
        "continued pretraining",
        "cybersecurity corpus",
        "cybersecurity benchmarks",
        "Llama 3.1-70B",
        "GPT-4o-mini",
        "cybersecurity-specific tasks"
      ]
    },
    "publishedAt": "2025-04-28T04:41:12.000Z",
    "title": "Llama-3.1-FoundationAI-SecurityLLM-Base-8B Technical Report",
    "summary": "As transformer-based large language models (LLMs) increasingly permeate\nsociety, they have revolutionized domains such as software engineering,\ncreative writing, and digital arts. However, their adoption in cybersecurity\nremains limited due to challenges like scarcity of specialized training data\nand complexity of representing cybersecurity-specific knowledge. To address\nthese gaps, we present Foundation-Sec-8B, a cybersecurity-focused LLM built on\nthe Llama 3.1 architecture and enhanced through continued pretraining on a\ncarefully curated cybersecurity corpus. We evaluate Foundation-Sec-8B across\nboth established and new cybersecurity benchmarks, showing that it matches\nLlama 3.1-70B and GPT-4o-mini in certain cybersecurity-specific tasks. By\nreleasing our model to the public, we aim to accelerate progress and adoption\nof AI-driven tools in both public and private cybersecurity contexts.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.21039.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620042b28c2eb991da50d34e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/620042b28c2eb991da50d34e/Q5cj1GIq3XnKj-K64Mtyd.jpeg",
      "fullname": "Aman Priyanshu",
      "name": "AmanPriyanshu",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  }
]