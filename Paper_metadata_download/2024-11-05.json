[
    {
        "paper": {
            "id": "2410.24024",
            "authors": [
                {
                    "_id": "67299eee6cc63cc1ba362e9b",
                    "name": "Yifan Xu",
                    "hidden": false
                },
                {
                    "_id": "67299eee6cc63cc1ba362e9c",
                    "user": {
                        "_id": "62f098602ca4d32a7cd87aba",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1659934844914-62f098602ca4d32a7cd87aba.jpeg",
                        "isPro": false,
                        "fullname": "Xiao Liu",
                        "user": "ShawLiu",
                        "type": "user"
                    },
                    "name": "Xiao Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-11-05T16:09:19.323Z",
                    "hidden": false
                },
                {
                    "_id": "67299eee6cc63cc1ba362e9d",
                    "user": {
                        "_id": "6729afc3c519b000f45d053b",
                        "avatarUrl": "/avatars/1d47cfd495e72316420fc16f7286df2b.svg",
                        "isPro": false,
                        "fullname": "Xueqiao Sun",
                        "user": "snow1007",
                        "type": "user"
                    },
                    "name": "Xueqiao Sun",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-05T13:38:12.414Z",
                    "hidden": false
                },
                {
                    "_id": "67299eee6cc63cc1ba362e9e",
                    "user": {
                        "_id": "654cf05258c9d74272181118",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Dfx-hp3RLkP95gQtOM7T0.png",
                        "isPro": false,
                        "fullname": "Siyi Cheng",
                        "user": "LeakyKayO",
                        "type": "user"
                    },
                    "name": "Siyi Cheng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-05T13:38:51.528Z",
                    "hidden": false
                },
                {
                    "_id": "67299eee6cc63cc1ba362e9f",
                    "name": "Hao Yu",
                    "hidden": false
                },
                {
                    "_id": "67299eee6cc63cc1ba362ea0",
                    "user": {
                        "_id": "62ea2d48b493272c269e8f34",
                        "avatarUrl": "/avatars/89b215dafa503b51ab212a9b63c82aca.svg",
                        "isPro": false,
                        "fullname": "Hanyu Lai",
                        "user": "hanyullai",
                        "type": "user"
                    },
                    "name": "Hanyu Lai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-05T13:38:57.325Z",
                    "hidden": false
                },
                {
                    "_id": "67299eee6cc63cc1ba362ea1",
                    "user": {
                        "_id": "637883f3736e2989331da910",
                        "avatarUrl": "/avatars/f72b579724d2d43889907e8b732e79e6.svg",
                        "isPro": false,
                        "fullname": "Shudan Zhang",
                        "user": "zdaniel0222",
                        "type": "user"
                    },
                    "name": "Shudan Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-05T13:39:04.042Z",
                    "hidden": false
                },
                {
                    "_id": "67299eee6cc63cc1ba362ea2",
                    "name": "Dan Zhang",
                    "hidden": false
                },
                {
                    "_id": "67299eee6cc63cc1ba362ea3",
                    "user": {
                        "_id": "640dff05474aa6f89556677e",
                        "avatarUrl": "/avatars/1b4591c7322d649c797b3125148f1915.svg",
                        "isPro": false,
                        "fullname": "Jie Tang",
                        "user": "jerytang",
                        "type": "user"
                    },
                    "name": "Jie Tang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-05T13:37:40.424Z",
                    "hidden": false
                },
                {
                    "_id": "67299eee6cc63cc1ba362ea4",
                    "user": {
                        "_id": "640e73bdfdeaae1390857b62",
                        "avatarUrl": "/avatars/cd6779e30f716002a7838ed93d5c0754.svg",
                        "isPro": false,
                        "fullname": "Yuxiao Dong",
                        "user": "yuxiaod",
                        "type": "user"
                    },
                    "name": "Yuxiao Dong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-05T13:36:14.681Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-31T15:25:20.000Z",
            "title": "AndroidLab: Training and Systematic Benchmarking of Android Autonomous\n  Agents",
            "summary": "Autonomous agents have become increasingly important for interacting with the\nreal world. Android agents, in particular, have been recently a\nfrequently-mentioned interaction method. However, existing studies for training\nand evaluating Android agents lack systematic research on both open-source and\nclosed-source models. In this work, we propose AndroidLab as a systematic\nAndroid agent framework. It includes an operation environment with different\nmodalities, action space, and a reproducible benchmark. It supports both large\nlanguage models (LLMs) and multimodal models (LMMs) in the same action space.\nAndroidLab benchmark includes predefined Android virtual devices and 138 tasks\nacross nine apps built on these devices. By using the AndroidLab environment,\nwe develop an Android Instruction dataset and train six open-source LLMs and\nLMMs, lifting the average success rates from 4.59% to 21.50% for LLMs and from\n1.93% to 13.28% for LMMs. AndroidLab is open-sourced and publicly available at\nhttps://github.com/THUDM/Android-Lab.",
            "upvotes": 33,
            "discussionId": "67299ef56cc63cc1ba363153"
        },
        "publishedAt": "2024-11-05T03:00:37.108Z",
        "title": "AndroidLab: Training and Systematic Benchmarking of Android Autonomous Agents",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.24024.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1659934844914-62f098602ca4d32a7cd87aba.jpeg",
            "fullname": "Xiao Liu",
            "name": "ShawLiu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 4
        }
    },
    {
        "paper": {
            "id": "2411.02337",
            "authors": [
                {
                    "_id": "67299dd8ff3de35f30a2bffe",
                    "user": {
                        "_id": "650ace9fa80a948447812fc5",
                        "avatarUrl": "/avatars/e01fe0d973008f5e3cd3c99f895b5946.svg",
                        "isPro": false,
                        "fullname": "zehanqi",
                        "user": "zehanqi",
                        "type": "user"
                    },
                    "name": "Zehan Qi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-05T13:58:00.956Z",
                    "hidden": false
                },
                {
                    "_id": "67299dd8ff3de35f30a2bfff",
                    "user": {
                        "_id": "62f098602ca4d32a7cd87aba",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1659934844914-62f098602ca4d32a7cd87aba.jpeg",
                        "isPro": false,
                        "fullname": "Xiao Liu",
                        "user": "ShawLiu",
                        "type": "user"
                    },
                    "name": "Xiao Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-05T13:58:45.111Z",
                    "hidden": false
                },
                {
                    "_id": "67299dd8ff3de35f30a2c000",
                    "name": "Iat Long Iong",
                    "hidden": false
                },
                {
                    "_id": "67299dd8ff3de35f30a2c001",
                    "user": {
                        "_id": "62ea2d48b493272c269e8f34",
                        "avatarUrl": "/avatars/89b215dafa503b51ab212a9b63c82aca.svg",
                        "isPro": false,
                        "fullname": "Hanyu Lai",
                        "user": "hanyullai",
                        "type": "user"
                    },
                    "name": "Hanyu Lai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-05T13:56:47.720Z",
                    "hidden": false
                },
                {
                    "_id": "67299dd8ff3de35f30a2c002",
                    "user": {
                        "_id": "6729afc3c519b000f45d053b",
                        "avatarUrl": "/avatars/1d47cfd495e72316420fc16f7286df2b.svg",
                        "isPro": false,
                        "fullname": "Xueqiao Sun",
                        "user": "snow1007",
                        "type": "user"
                    },
                    "name": "Xueqiao Sun",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-05T13:56:56.205Z",
                    "hidden": false
                },
                {
                    "_id": "67299dd8ff3de35f30a2c003",
                    "name": "Xinyue Yang",
                    "hidden": false
                },
                {
                    "_id": "67299dd8ff3de35f30a2c004",
                    "user": {
                        "_id": "64e82f2c9a3cd93b370e9936",
                        "avatarUrl": "/avatars/942e6008e8e78583c2750d678fcaadb1.svg",
                        "isPro": false,
                        "fullname": "jiadaisun",
                        "user": "jiadaisun",
                        "type": "user"
                    },
                    "name": "Jiadai Sun",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-05T13:57:27.980Z",
                    "hidden": false
                },
                {
                    "_id": "67299dd8ff3de35f30a2c005",
                    "user": {
                        "_id": "60a53adbf9b53404e7806277",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1621441193991-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Yu Yang",
                        "user": "yuyang",
                        "type": "user"
                    },
                    "name": "Yu Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-05T13:57:33.794Z",
                    "hidden": false
                },
                {
                    "_id": "67299dd8ff3de35f30a2c006",
                    "user": {
                        "_id": "6464c91ef4f3102bb6c7418a",
                        "avatarUrl": "/avatars/1e6ce129d8cd51e81d517e23ad9aee43.svg",
                        "isPro": false,
                        "fullname": "Shuntian Yao",
                        "user": "TMIP",
                        "type": "user"
                    },
                    "name": "Shuntian Yao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-05T13:57:53.493Z",
                    "hidden": false
                },
                {
                    "_id": "67299dd8ff3de35f30a2c007",
                    "user": {
                        "_id": "64dceda0f44d2407e60179f8",
                        "avatarUrl": "/avatars/726d5ed318b40ae886b7387a532217e4.svg",
                        "isPro": false,
                        "fullname": "Tianjie Zhang",
                        "user": "tianjiezhang",
                        "type": "user"
                    },
                    "name": "Tianjie Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-11-05T07:57:23.819Z",
                    "hidden": false
                },
                {
                    "_id": "67299dd8ff3de35f30a2c008",
                    "name": "Wei Xu",
                    "hidden": false
                },
                {
                    "_id": "67299dd8ff3de35f30a2c009",
                    "name": "Jie Tang",
                    "hidden": false
                },
                {
                    "_id": "67299dd8ff3de35f30a2c00a",
                    "name": "Yuxiao Dong",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-04T17:59:58.000Z",
            "title": "WebRL: Training LLM Web Agents via Self-Evolving Online Curriculum\n  Reinforcement Learning",
            "summary": "Large language models (LLMs) have shown remarkable potential as autonomous\nagents, particularly in web-based tasks. However, existing LLM web agents\nheavily rely on expensive proprietary LLM APIs, while open LLMs lack the\nnecessary decision-making capabilities. This paper introduces WebRL, a\nself-evolving online curriculum reinforcement learning framework designed to\ntrain high-performance web agents using open LLMs. WebRL addresses three key\nchallenges in building LLM web agents, including the scarcity of training\ntasks, sparse feedback signals, and policy distribution drift in online\nlearning. Specifically, WebRL incorporates 1) a self-evolving curriculum that\ngenerates new tasks from unsuccessful attempts, 2) a robust outcome-supervised\nreward model (ORM), and 3) adaptive reinforcement learning strategies to ensure\nconsistent improvements. We apply WebRL to transform open Llama-3.1 and GLM-4\nmodels into proficient web agents. On WebArena-Lite, WebRL improves the success\nrate of Llama-3.1-8B from 4.8% to 42.4%, and from 6.1% to 43% for GLM-4-9B.\nThese open models significantly surpass the performance of GPT-4-Turbo (17.6%)\nand GPT-4o (13.9%) and outperform previous state-of-the-art web agents trained\non open LLMs (AutoWebGLM, 18.2%). Our findings demonstrate WebRL's\neffectiveness in bridging the gap between open and proprietary LLM-based web\nagents, paving the way for more accessible and powerful autonomous web\ninteraction systems.",
            "upvotes": 19,
            "discussionId": "67299ddaff3de35f30a2c0b3"
        },
        "publishedAt": "2024-11-05T02:55:57.606Z",
        "title": "WebRL: Training LLM Web Agents via Self-Evolving Online Curriculum Reinforcement Learning",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/62f098602ca4d32a7cd87aba/gLpzHlQfI9VL9rovO-l8t.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.02337.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1659934844914-62f098602ca4d32a7cd87aba.jpeg",
            "fullname": "Xiao Liu",
            "name": "ShawLiu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 4
        }
    },
    {
        "paper": {
            "id": "2411.02395",
            "authors": [
                {
                    "_id": "6729ba9886503ffe1cc31897",
                    "user": {
                        "_id": "6311d9ee04f842f79916158c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/PvOYgDEpe0x5ExLyj1BK-.png",
                        "isPro": false,
                        "fullname": "chen",
                        "user": "antonio-c",
                        "type": "user"
                    },
                    "name": "Anthony Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-11-05T07:57:18.751Z",
                    "hidden": false
                },
                {
                    "_id": "6729ba9886503ffe1cc31898",
                    "user": {
                        "_id": "6323c5a899c4839e1c6223e9",
                        "avatarUrl": "/avatars/7890aa86a309e2c86e7341212f8afc40.svg",
                        "isPro": false,
                        "fullname": "Jianjin Xu",
                        "user": "atlantix",
                        "type": "user"
                    },
                    "name": "Jianjin Xu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-05T13:59:09.353Z",
                    "hidden": false
                },
                {
                    "_id": "6729ba9886503ffe1cc31899",
                    "name": "Wenzhao Zheng",
                    "hidden": false
                },
                {
                    "_id": "6729ba9886503ffe1cc3189a",
                    "user": {
                        "_id": "668d9de515511820b9c31a51",
                        "avatarUrl": "/avatars/409f1334969f659a331a61bb0f4ce15c.svg",
                        "isPro": false,
                        "fullname": "Gaole Dai",
                        "user": "barkleydai",
                        "type": "user"
                    },
                    "name": "Gaole Dai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-05T13:59:23.394Z",
                    "hidden": false
                },
                {
                    "_id": "6729ba9886503ffe1cc3189b",
                    "user": {
                        "_id": "647068944be5cf1f33491cb9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Aqudl2PTSKPAylBhlccWr.png",
                        "isPro": false,
                        "fullname": "Yida Wang",
                        "user": "wangyida",
                        "type": "user"
                    },
                    "name": "Yida Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-11-05T07:57:13.829Z",
                    "hidden": false
                },
                {
                    "_id": "6729ba9886503ffe1cc3189c",
                    "name": "Renrui Zhang",
                    "hidden": false
                },
                {
                    "_id": "6729ba9886503ffe1cc3189d",
                    "user": {
                        "_id": "637745113a63a2983ffbde13",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669187672174-637745113a63a2983ffbde13.jpeg",
                        "isPro": false,
                        "fullname": "Haofan Wang",
                        "user": "wanghaofan",
                        "type": "user"
                    },
                    "name": "Haofan Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-11-05T07:57:15.554Z",
                    "hidden": false
                },
                {
                    "_id": "6729ba9886503ffe1cc3189e",
                    "name": "Shanghang Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-04T18:59:05.000Z",
            "title": "Training-free Regional Prompting for Diffusion Transformers",
            "summary": "Diffusion models have demonstrated excellent capabilities in text-to-image\ngeneration. Their semantic understanding (i.e., prompt following) ability has\nalso been greatly improved with large language models (e.g., T5, Llama).\nHowever, existing models cannot perfectly handle long and complex text prompts,\nespecially when the text prompts contain various objects with numerous\nattributes and interrelated spatial relationships. While many regional\nprompting methods have been proposed for UNet-based models (SD1.5, SDXL), but\nthere are still no implementations based on the recent Diffusion Transformer\n(DiT) architecture, such as SD3 and FLUX.1.In this report, we propose and\nimplement regional prompting for FLUX.1 based on attention manipulation, which\nenables DiT with fined-grained compositional text-to-image generation\ncapability in a training-free manner. Code is available at\nhttps://github.com/antonioo-c/Regional-Prompting-FLUX.",
            "upvotes": 15,
            "discussionId": "6729ba9986503ffe1cc318d9"
        },
        "publishedAt": "2024-11-05T04:57:16.995Z",
        "title": "Training-free Regional Prompting for Diffusion Transformers",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/637745113a63a2983ffbde13/wdNvysEe2tIHmZaqW1gFY.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.02395.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669187672174-637745113a63a2983ffbde13.jpeg",
            "fullname": "Haofan Wang",
            "name": "wanghaofan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 61
        }
    },
    {
        "paper": {
            "id": "2411.00860",
            "authors": [
                {
                    "_id": "6729cc9f35af989bcce75f03",
                    "user": {
                        "_id": "60c50f18754747f54fa37114",
                        "avatarUrl": "/avatars/648ae58b81806dbd93a68546666047e3.svg",
                        "isPro": false,
                        "fullname": "Siddhesh",
                        "user": "sidicity",
                        "type": "user"
                    },
                    "name": "Siddhesh Pawar",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-11-05T07:57:08.217Z",
                    "hidden": false
                },
                {
                    "_id": "6729cc9f35af989bcce75f04",
                    "user": {
                        "_id": "659f4d7bb62804e6f43319ce",
                        "avatarUrl": "/avatars/b6ce28db5670411b42458d9edb28854c.svg",
                        "isPro": false,
                        "fullname": "Junyeong Park",
                        "user": "frechele",
                        "type": "user"
                    },
                    "name": "Junyeong Park",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-05T14:00:17.822Z",
                    "hidden": true
                },
                {
                    "_id": "6729cc9f35af989bcce75f05",
                    "user": {
                        "_id": "65e52c1c57459fa149c7fd48",
                        "avatarUrl": "/avatars/594749486825ea3cce7d0aa2c673d644.svg",
                        "isPro": false,
                        "fullname": "Jiho Jin",
                        "user": "jinjh0123",
                        "type": "user"
                    },
                    "name": "Jiho Jin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-11-05T08:49:26.630Z",
                    "hidden": false
                },
                {
                    "_id": "6729cc9f35af989bcce75f06",
                    "name": "Arnav Arora",
                    "hidden": false
                },
                {
                    "_id": "6729cc9f35af989bcce75f07",
                    "user": {
                        "_id": "628c9b7ed1ed15f46a96a258",
                        "avatarUrl": "/avatars/1dc91fee8b3766759caf00f0a3c588dd.svg",
                        "isPro": false,
                        "fullname": "Junho Myung",
                        "user": "Junho00211",
                        "type": "user"
                    },
                    "name": "Junho Myung",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-05T14:02:44.622Z",
                    "hidden": false
                },
                {
                    "_id": "6729cc9f35af989bcce75f08",
                    "user": {
                        "_id": "60c7ccd1dfbd57a384a0111e",
                        "avatarUrl": "/avatars/70e55e861656a52ae535182db17d3789.svg",
                        "isPro": false,
                        "fullname": "Srishti",
                        "user": "srishtiy",
                        "type": "user"
                    },
                    "name": "Srishti Yadav",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-11-05T13:36:23.103Z",
                    "hidden": false
                },
                {
                    "_id": "6729cc9f35af989bcce75f09",
                    "user": {
                        "_id": "634627a5ac1cb29fb2ac485f",
                        "avatarUrl": "/avatars/275ad360aacbd9956f09aa1dffff9fd7.svg",
                        "isPro": false,
                        "fullname": "Faiz Ghifari Haznitrama",
                        "user": "haznitrama",
                        "type": "user"
                    },
                    "name": "Faiz Ghifari Haznitrama",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-05T14:02:10.088Z",
                    "hidden": false
                },
                {
                    "_id": "6729cc9f35af989bcce75f0a",
                    "user": {
                        "_id": "63249643dabddff8da6dae26",
                        "avatarUrl": "/avatars/256b917445920feb92f3e2de8609d65d.svg",
                        "isPro": false,
                        "fullname": "Inhwa Song",
                        "user": "greenina",
                        "type": "user"
                    },
                    "name": "Inhwa Song",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-05T14:02:19.643Z",
                    "hidden": false
                },
                {
                    "_id": "6729cc9f35af989bcce75f0b",
                    "user": {
                        "_id": "60e0251ea9b5d8282481f2b7",
                        "avatarUrl": "/avatars/43441373af054a6184c22097bfeb97e4.svg",
                        "isPro": false,
                        "fullname": "Alice Oh",
                        "user": "aliceoh",
                        "type": "user"
                    },
                    "name": "Alice Oh",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-05T14:02:28.798Z",
                    "hidden": false
                },
                {
                    "_id": "6729cc9f35af989bcce75f0c",
                    "user": {
                        "_id": "608918b7df398c3b285ce960",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1621507769190-608918b7df398c3b285ce960.jpeg",
                        "isPro": false,
                        "fullname": "Isabelle Augenstein",
                        "user": "IAugenstein",
                        "type": "user"
                    },
                    "name": "Isabelle Augenstein",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-05T14:02:35.372Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-30T16:37:50.000Z",
            "title": "Survey of Cultural Awareness in Language Models: Text and Beyond",
            "summary": "Large-scale deployment of large language models (LLMs) in various\napplications, such as chatbots and virtual assistants, requires LLMs to be\nculturally sensitive to the user to ensure inclusivity. Culture has been widely\nstudied in psychology and anthropology, and there has been a recent surge in\nresearch on making LLMs more culturally inclusive in LLMs that goes beyond\nmultilinguality and builds on findings from psychology and anthropology. In\nthis paper, we survey efforts towards incorporating cultural awareness into\ntext-based and multimodal LLMs. We start by defining cultural awareness in\nLLMs, taking the definitions of culture from anthropology and psychology as a\npoint of departure. We then examine methodologies adopted for creating\ncross-cultural datasets, strategies for cultural inclusion in downstream tasks,\nand methodologies that have been used for benchmarking cultural awareness in\nLLMs. Further, we discuss the ethical implications of cultural alignment, the\nrole of Human-Computer Interaction in driving cultural inclusion in LLMs, and\nthe role of cultural alignment in driving social science research. We finally\nprovide pointers to future research based on our findings about gaps in the\nliterature.",
            "upvotes": 13,
            "discussionId": "6729cca035af989bcce75f71"
        },
        "publishedAt": "2024-11-05T07:06:11.365Z",
        "title": "Survey of Cultural Awareness in Language Models: Text and Beyond",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/65e52c1c57459fa149c7fd48/t7hEK_rqiUL1jQUGJrMn3.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.00860.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/594749486825ea3cce7d0aa2c673d644.svg",
            "fullname": "Jiho Jin",
            "name": "jinjh0123",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2411.02265",
            "authors": [
                {
                    "_id": "67298ecccdd83d27791f1dc9",
                    "name": "Xingwu Sun",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1dca",
                    "user": {
                        "_id": "64c99c82bd5f839f2fdeb173",
                        "avatarUrl": "/avatars/194b73285cef5e6e71338cafaae48eeb.svg",
                        "isPro": false,
                        "fullname": "Yanfeng Chen",
                        "user": "YanfengChen",
                        "type": "user"
                    },
                    "name": "Yanfeng Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-05T08:03:55.794Z",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1dcb",
                    "user": {
                        "_id": "64f81e1f2003abb61b09eed1",
                        "avatarUrl": "/avatars/e6282a815e44fb355b53a9a99a56fe88.svg",
                        "isPro": false,
                        "fullname": "Yiqing Huang",
                        "user": "Mimosa77",
                        "type": "user"
                    },
                    "name": "Yiqing Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-05T08:49:02.756Z",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1dcc",
                    "user": {
                        "_id": "6622443b9b0614a760dd8123",
                        "avatarUrl": "/avatars/acb6c1c9c429af1112530dcf76a8e420.svg",
                        "isPro": false,
                        "fullname": "Ruobing Xie",
                        "user": "Ruobing-Xie",
                        "type": "user"
                    },
                    "name": "Ruobing Xie",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-05T08:49:11.945Z",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1dcd",
                    "name": "Jiaqi Zhu",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1dce",
                    "name": "Kai Zhang",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1dcf",
                    "name": "Shuaipeng Li",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1dd0",
                    "name": "Zhen Yang",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1dd1",
                    "name": "Jonny Han",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1dd2",
                    "name": "Xiaobo Shu",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1dd3",
                    "name": "Jiahao Bu",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1dd4",
                    "name": "Zhongzhi Chen",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1dd5",
                    "name": "Xuemeng Huang",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1dd6",
                    "name": "Fengzong Lian",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1dd7",
                    "name": "Saiyong Yang",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1dd8",
                    "name": "Jianfeng Yan",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1dd9",
                    "name": "Yuyuan Zeng",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1dda",
                    "name": "Xiaoqin Ren",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1ddb",
                    "name": "Chao Yu",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1ddc",
                    "name": "Lulu Wu",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1ddd",
                    "name": "Yue Mao",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1dde",
                    "name": "Tao Yang",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1ddf",
                    "name": "Suncong Zheng",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1de0",
                    "name": "Kan Wu",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1de1",
                    "name": "Dian Jiao",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1de2",
                    "name": "Jinbao Xue",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1de3",
                    "name": "Xipeng Zhang",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1de4",
                    "name": "Decheng Wu",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1de5",
                    "name": "Kai Liu",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1de6",
                    "name": "Dengpeng Wu",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1de7",
                    "name": "Guanghui Xu",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1de8",
                    "name": "Shaohua Chen",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1de9",
                    "name": "Shuang Chen",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1dea",
                    "name": "Xiao Feng",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1deb",
                    "name": "Yigeng Hong",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1dec",
                    "name": "Junqiang Zheng",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1ded",
                    "name": "Chengcheng Xu",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1dee",
                    "name": "Zongwei Li",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1def",
                    "name": "Xiong Kuang",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1df0",
                    "name": "Jianglu Hu",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1df1",
                    "name": "Yiqi Chen",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1df2",
                    "name": "Yuchi Deng",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1df3",
                    "name": "Guiyang Li",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1df4",
                    "name": "Ao Liu",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1df5",
                    "user": {
                        "_id": "64b74b906ab5d14ca7f289cd",
                        "avatarUrl": "/avatars/b131b7c4ce5216708ca4a678f35ead0a.svg",
                        "isPro": false,
                        "fullname": "zcc",
                        "user": "xxzcc",
                        "type": "user"
                    },
                    "name": "Chenchen Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-11-05T13:36:26.125Z",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1df6",
                    "name": "Shihui Hu",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1df7",
                    "name": "Zilong Zhao",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1df8",
                    "name": "Zifan Wu",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1df9",
                    "name": "Yao Ding",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1dfa",
                    "name": "Weichao Wang",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1dfb",
                    "name": "Han Liu",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1dfc",
                    "name": "Roberts Wang",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1dfd",
                    "name": "Hao Fei",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1dfe",
                    "name": "Peijie She",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1dff",
                    "name": "Ze Zhao",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1e00",
                    "name": "Xun Cao",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1e01",
                    "name": "Hai Wang",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1e02",
                    "name": "Fusheng Xiang",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1e03",
                    "name": "Mengyuan Huang",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1e04",
                    "name": "Zhiyuan Xiong",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1e05",
                    "name": "Bin Hu",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1e06",
                    "name": "Xuebin Hou",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1e07",
                    "name": "Lei Jiang",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1e08",
                    "name": "Jiajia Wu",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1e09",
                    "name": "Yaping Deng",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1e0a",
                    "name": "Yi Shen",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1e0b",
                    "name": "Qian Wang",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1e0c",
                    "name": "Weijie Liu",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1e0d",
                    "name": "Jie Liu",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1e0e",
                    "name": "Meng Chen",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1e0f",
                    "name": "Liang Dong",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1e10",
                    "name": "Weiwen Jia",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1e11",
                    "name": "Hu Chen",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1e12",
                    "name": "Feifei Liu",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1e13",
                    "name": "Rui Yuan",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1e14",
                    "name": "Huilin Xu",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1e15",
                    "name": "Zhenxiang Yan",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1e16",
                    "name": "Tengfei Cao",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1e17",
                    "name": "Zhichao Hu",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1e18",
                    "name": "Xinhua Feng",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1e19",
                    "name": "Dong Du",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1e1a",
                    "name": "Tinghao She",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1e1b",
                    "name": "Yangyu Tao",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1e1c",
                    "name": "Feng Zhang",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1e1d",
                    "name": "Jianchen Zhu",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1e1e",
                    "name": "Chengzhong Xu",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1e1f",
                    "name": "Xirui Li",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1e20",
                    "name": "Chong Zha",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1e21",
                    "name": "Wen Ouyang",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1e22",
                    "name": "Yinben Xia",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1e23",
                    "name": "Xiang Li",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1e24",
                    "name": "Zekun He",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1e25",
                    "name": "Rongpeng Chen",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1e26",
                    "name": "Jiawei Song",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1e27",
                    "name": "Ruibin Chen",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1e28",
                    "name": "Fan Jiang",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1e29",
                    "name": "Chongqing Zhao",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1e2a",
                    "name": "Bo Wang",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1e2b",
                    "name": "Hao Gong",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1e2c",
                    "name": "Rong Gan",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1e2d",
                    "name": "Winston Hu",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1e2e",
                    "name": "Zhanhui Kang",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1e2f",
                    "name": "Yong Yang",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1e30",
                    "name": "Yuhong Liu",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1e31",
                    "name": "Di Wang",
                    "hidden": false
                },
                {
                    "_id": "67298ecccdd83d27791f1e32",
                    "name": "Jie Jiang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-04T16:56:26.000Z",
            "title": "Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated\n  Parameters by Tencent",
            "summary": "In this paper, we introduce Hunyuan-Large, which is currently the largest\nopen-source Transformer-based mixture of experts model, with a total of 389\nbillion parameters and 52 billion activation parameters, capable of handling up\nto 256K tokens. We conduct a thorough evaluation of Hunyuan-Large's superior\nperformance across various benchmarks including language understanding and\ngeneration, logical reasoning, mathematical problem-solving, coding,\nlong-context, and aggregated tasks, where it outperforms LLama3.1-70B and\nexhibits comparable performance when compared to the significantly larger\nLLama3.1-405B model. Key practice of Hunyuan-Large include large-scale\nsynthetic data that is orders larger than in previous literature, a mixed\nexpert routing strategy, a key-value cache compression technique, and an\nexpert-specific learning rate strategy. Additionally, we also investigate the\nscaling laws and learning rate schedule of mixture of experts models, providing\nvaluable insights and guidances for future model development and optimization.\nThe code and checkpoints of Hunyuan-Large are released to facilitate future\ninnovations and applications.\n  Codes: https://github.com/Tencent/Hunyuan-Large\n  Models: https://huggingface.co/tencent/Tencent-Hunyuan-Large",
            "upvotes": 13,
            "discussionId": "67298ecccdd83d27791f1e6f"
        },
        "publishedAt": "2024-11-05T04:31:53.677Z",
        "title": "Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated Parameters by Tencent",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.02265.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/b131b7c4ce5216708ca4a678f35ead0a.svg",
            "fullname": "zcc",
            "name": "xxzcc",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2411.00836",
            "authors": [
                {
                    "_id": "67297e6f6cc63cc1ba29886a",
                    "user": {
                        "_id": "660a37f54fa3a72a97b09546",
                        "avatarUrl": "/avatars/d6306e87e9343abfd16c1fe08a2f9358.svg",
                        "isPro": true,
                        "fullname": "Chengke Zou",
                        "user": "OwenZou",
                        "type": "user"
                    },
                    "name": "Chengke Zou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-05T14:03:14.097Z",
                    "hidden": false
                },
                {
                    "_id": "67297e6f6cc63cc1ba29886b",
                    "name": "Xingang Guo",
                    "hidden": false
                },
                {
                    "_id": "67297e6f6cc63cc1ba29886c",
                    "user": {
                        "_id": "64d45451c34a346181b130dd",
                        "avatarUrl": "/avatars/9bb8205b889337df5d321539c9b5d69d.svg",
                        "isPro": false,
                        "fullname": "Rui Yang",
                        "user": "Ray2333",
                        "type": "user"
                    },
                    "name": "Rui Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-11-05T07:58:11.859Z",
                    "hidden": false
                },
                {
                    "_id": "67297e6f6cc63cc1ba29886d",
                    "user": {
                        "_id": "6719bfd07c6e6c83a388aeae",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6719bfd07c6e6c83a388aeae/jHxryk04dzHo23TX5F5sz.png",
                        "isPro": false,
                        "fullname": "Junyu Zhang",
                        "user": "jyzhang1208",
                        "type": "user"
                    },
                    "name": "Junyu Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-05T14:03:42.928Z",
                    "hidden": false
                },
                {
                    "_id": "67297e6f6cc63cc1ba29886e",
                    "name": "Bin Hu",
                    "hidden": true
                },
                {
                    "_id": "67297e6f6cc63cc1ba29886f",
                    "user": {
                        "_id": "6719d581a6cad13741b8bc7f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6719d581a6cad13741b8bc7f/w4EttqfXRgWZJc6HpYOS9.jpeg",
                        "isPro": false,
                        "fullname": "Huan Zhang",
                        "user": "huanzhang12",
                        "type": "user"
                    },
                    "name": "Huan Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-05T14:04:05.546Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-29T17:29:19.000Z",
            "title": "DynaMath: A Dynamic Visual Benchmark for Evaluating Mathematical\n  Reasoning Robustness of Vision Language Models",
            "summary": "The rapid advancements in Vision-Language Models (VLMs) have shown great\npotential in tackling mathematical reasoning tasks that involve visual context.\nUnlike humans who can reliably apply solution steps to similar problems with\nminor modifications, we found that SOTA VLMs like GPT-4o can consistently fail\nin these scenarios, revealing limitations in their mathematical reasoning\ncapabilities. In this paper, we investigate the mathematical reasoning\nrobustness in VLMs and evaluate how well these models perform under different\nvariants of the same question, such as changes in visual numerical values or\nfunction graphs. While several vision-based math benchmarks have been developed\nto assess VLMs' problem-solving capabilities, these benchmarks contain only\nstatic sets of problems and cannot easily evaluate mathematical reasoning\nrobustness. To fill this gap, we introduce DynaMath, a dynamic visual math\nbenchmark designed for in-depth assessment of VLMs. DynaMath includes 501\nhigh-quality, multi-topic seed questions, each represented as a Python program.\nThose programs are carefully designed and annotated to enable the automatic\ngeneration of a much larger set of concrete questions, including many different\ntypes of visual and textual variations. DynaMath allows us to evaluate the\ngeneralization ability of VLMs, by assessing their performance under varying\ninput conditions of a seed question. We evaluated 14 SOTA VLMs with 5,010\ngenerated concrete questions. Our results show that the worst-case model\naccuracy, defined as the percentage of correctly answered seed questions in all\n10 variants, is significantly lower than the average-case accuracy. Our\nanalysis emphasizes the need to study the robustness of VLMs' reasoning\nabilities, and DynaMath provides valuable insights to guide the development of\nmore reliable models for mathematical reasoning.",
            "upvotes": 11,
            "discussionId": "67297e716cc63cc1ba2988f9"
        },
        "publishedAt": "2024-11-05T00:47:47.628Z",
        "title": "DynaMath: A Dynamic Visual Benchmark for Evaluating Mathematical Reasoning Robustness of Vision Language Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.00836.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/9bb8205b889337df5d321539c9b5d69d.svg",
            "fullname": "Rui Yang",
            "name": "Ray2333",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 3
        }
    },
    {
        "paper": {
            "id": "2411.02385",
            "authors": [
                {
                    "_id": "67298a7f6f4fe77234c6a1ff",
                    "user": {
                        "_id": "647b5fef6a79fbf5e996c47c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647b5fef6a79fbf5e996c47c/IkSMnDsCY_CyEFCiMDuxe.jpeg",
                        "isPro": false,
                        "fullname": "Bingyi Kang",
                        "user": "bykang",
                        "type": "user"
                    },
                    "name": "Bingyi Kang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-05T14:04:24.046Z",
                    "hidden": false
                },
                {
                    "_id": "67298a7f6f4fe77234c6a200",
                    "user": {
                        "_id": "649d475111592b1a765ac1a3",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649d475111592b1a765ac1a3/rjORJjErJq-mthghan08U.jpeg",
                        "isPro": false,
                        "fullname": "Yang Yue",
                        "user": "Yang130",
                        "type": "user"
                    },
                    "name": "Yang Yue",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-11-05T07:57:49.069Z",
                    "hidden": false
                },
                {
                    "_id": "67298a7f6f4fe77234c6a201",
                    "name": "Rui Lu",
                    "hidden": false
                },
                {
                    "_id": "67298a7f6f4fe77234c6a202",
                    "name": "Zhijie Lin",
                    "hidden": false
                },
                {
                    "_id": "67298a7f6f4fe77234c6a203",
                    "name": "Yang Zhao",
                    "hidden": false
                },
                {
                    "_id": "67298a7f6f4fe77234c6a204",
                    "name": "Kaixin Wang",
                    "hidden": false
                },
                {
                    "_id": "67298a7f6f4fe77234c6a205",
                    "name": "Gao Huang",
                    "hidden": false
                },
                {
                    "_id": "67298a7f6f4fe77234c6a206",
                    "user": {
                        "_id": "67298e44017b96a1d0101dc4",
                        "avatarUrl": "/avatars/1f8ed1a3e911e6a3021087b9371d284c.svg",
                        "isPro": false,
                        "fullname": "Jiashi Feng",
                        "user": "jshfeng",
                        "type": "user"
                    },
                    "name": "Jiashi Feng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-05T14:04:36.108Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-04T18:53:05.000Z",
            "title": "How Far is Video Generation from World Model: A Physical Law Perspective",
            "summary": "OpenAI's Sora highlights the potential of video generation for developing\nworld models that adhere to fundamental physical laws. However, the ability of\nvideo generation models to discover such laws purely from visual data without\nhuman priors can be questioned. A world model learning the true law should give\npredictions robust to nuances and correctly extrapolate on unseen scenarios. In\nthis work, we evaluate across three key scenarios: in-distribution,\nout-of-distribution, and combinatorial generalization. We developed a 2D\nsimulation testbed for object movement and collisions to generate videos\ndeterministically governed by one or more classical mechanics laws. This\nprovides an unlimited supply of data for large-scale experimentation and\nenables quantitative evaluation of whether the generated videos adhere to\nphysical laws. We trained diffusion-based video generation models to predict\nobject movements based on initial frames. Our scaling experiments show perfect\ngeneralization within the distribution, measurable scaling behavior for\ncombinatorial generalization, but failure in out-of-distribution scenarios.\nFurther experiments reveal two key insights about the generalization mechanisms\nof these models: (1) the models fail to abstract general physical rules and\ninstead exhibit \"case-based\" generalization behavior, i.e., mimicking the\nclosest training example; (2) when generalizing to new cases, models are\nobserved to prioritize different factors when referencing training data: color\n> size > velocity > shape. Our study suggests that scaling alone is\ninsufficient for video generation models to uncover fundamental physical laws,\ndespite its role in Sora's broader success. See our project page at\nhttps://phyworld.github.io",
            "upvotes": 9,
            "discussionId": "67298a806f4fe77234c6a253"
        },
        "publishedAt": "2024-11-05T01:33:34.761Z",
        "title": "How Far is Video Generation from World Model: A Physical Law Perspective",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/647b5fef6a79fbf5e996c47c/HPPAk0GOTcFBOvu-M-Aio.mp4",
            "https://cdn-uploads.huggingface.co/production/uploads/647b5fef6a79fbf5e996c47c/bkrqZm3P5kvX_SmZw_zCS.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.02385.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647b5fef6a79fbf5e996c47c/IkSMnDsCY_CyEFCiMDuxe.jpeg",
            "fullname": "Bingyi Kang",
            "name": "bykang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 2
        }
    },
    {
        "paper": {
            "id": "2411.02397",
            "authors": [
                {
                    "_id": "672992dd829cb1cce1201665",
                    "user": {
                        "_id": "65a12ec4c9873890d15c4ab9",
                        "avatarUrl": "/avatars/ce4d46f575ba757f78eabdb25b394171.svg",
                        "isPro": false,
                        "fullname": "Kumara Kahatapitiya",
                        "user": "kumarak",
                        "type": "user"
                    },
                    "name": "Kumara Kahatapitiya",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-11-05T07:57:47.053Z",
                    "hidden": false
                },
                {
                    "_id": "672992dd829cb1cce1201666",
                    "name": "Haozhe Liu",
                    "hidden": false
                },
                {
                    "_id": "672992dd829cb1cce1201667",
                    "name": "Sen He",
                    "hidden": false
                },
                {
                    "_id": "672992dd829cb1cce1201668",
                    "name": "Ding Liu",
                    "hidden": false
                },
                {
                    "_id": "672992dd829cb1cce1201669",
                    "name": "Menglin Jia",
                    "hidden": false
                },
                {
                    "_id": "672992dd829cb1cce120166a",
                    "name": "Michael S. Ryoo",
                    "hidden": false
                },
                {
                    "_id": "672992dd829cb1cce120166b",
                    "name": "Tian Xie",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-04T18:59:44.000Z",
            "title": "Adaptive Caching for Faster Video Generation with Diffusion Transformers",
            "summary": "Generating temporally-consistent high-fidelity videos can be computationally\nexpensive, especially over longer temporal spans. More-recent Diffusion\nTransformers (DiTs) -- despite making significant headway in this context --\nhave only heightened such challenges as they rely on larger models and heavier\nattention mechanisms, resulting in slower inference speeds. In this paper, we\nintroduce a training-free method to accelerate video DiTs, termed Adaptive\nCaching (AdaCache), which is motivated by the fact that \"not all videos are\ncreated equal\": meaning, some videos require fewer denoising steps to attain a\nreasonable quality than others. Building on this, we not only cache\ncomputations through the diffusion process, but also devise a caching schedule\ntailored to each video generation, maximizing the quality-latency trade-off. We\nfurther introduce a Motion Regularization (MoReg) scheme to utilize video\ninformation within AdaCache, essentially controlling the compute allocation\nbased on motion content. Altogether, our plug-and-play contributions grant\nsignificant inference speedups (e.g. up to 4.7x on Open-Sora 720p - 2s video\ngeneration) without sacrificing the generation quality, across multiple video\nDiT baselines.",
            "upvotes": 8,
            "discussionId": "672992e3829cb1cce12019d9"
        },
        "publishedAt": "2024-11-05T02:09:03.282Z",
        "title": "Adaptive Caching for Faster Video Generation with Diffusion Transformers",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.02397.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/ce4d46f575ba757f78eabdb25b394171.svg",
            "fullname": "Kumara Kahatapitiya",
            "name": "kumarak",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 2
        }
    },
    {
        "paper": {
            "id": "2411.02336",
            "authors": [
                {
                    "_id": "672a3b03d0448c8bae6dc97b",
                    "user": {
                        "_id": "64b914c8ace99c0723ad83a9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/udUHjj6fby82zh8LDjXhL.jpeg",
                        "isPro": false,
                        "fullname": "Wei Cheng",
                        "user": "wchengad",
                        "type": "user"
                    },
                    "name": "Wei Cheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-11-05T16:09:12.427Z",
                    "hidden": false
                },
                {
                    "_id": "672a3b03d0448c8bae6dc97c",
                    "name": "Juncheng Mu",
                    "hidden": false
                },
                {
                    "_id": "672a3b03d0448c8bae6dc97d",
                    "name": "Xianfang Zeng",
                    "hidden": false
                },
                {
                    "_id": "672a3b03d0448c8bae6dc97e",
                    "name": "Xin Chen",
                    "hidden": false
                },
                {
                    "_id": "672a3b03d0448c8bae6dc97f",
                    "name": "Anqi Pang",
                    "hidden": false
                },
                {
                    "_id": "672a3b03d0448c8bae6dc980",
                    "name": "Chi Zhang",
                    "hidden": false
                },
                {
                    "_id": "672a3b03d0448c8bae6dc981",
                    "name": "Zhibin Wang",
                    "hidden": false
                },
                {
                    "_id": "672a3b03d0448c8bae6dc982",
                    "name": "Bin Fu",
                    "hidden": false
                },
                {
                    "_id": "672a3b03d0448c8bae6dc983",
                    "name": "Gang Yu",
                    "hidden": false
                },
                {
                    "_id": "672a3b03d0448c8bae6dc984",
                    "name": "Ziwei Liu",
                    "hidden": false
                },
                {
                    "_id": "672a3b03d0448c8bae6dc985",
                    "user": {
                        "_id": "64ef30fb0de94d31b7921175",
                        "avatarUrl": "/avatars/d4f4c7be460befc1cedec13bbf9db972.svg",
                        "isPro": false,
                        "fullname": "pan",
                        "user": "pldeqiushui",
                        "type": "user"
                    },
                    "name": "Liang Pan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-11-05T16:09:10.438Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-04T17:59:39.000Z",
            "title": "MVPaint: Synchronized Multi-View Diffusion for Painting Anything 3D",
            "summary": "Texturing is a crucial step in the 3D asset production workflow, which\nenhances the visual appeal and diversity of 3D assets. Despite recent\nadvancements in Text-to-Texture (T2T) generation, existing methods often yield\nsubpar results, primarily due to local discontinuities, inconsistencies across\nmultiple views, and their heavy dependence on UV unwrapping outcomes. To tackle\nthese challenges, we propose a novel generation-refinement 3D texturing\nframework called MVPaint, which can generate high-resolution, seamless textures\nwhile emphasizing multi-view consistency. MVPaint mainly consists of three key\nmodules. 1) Synchronized Multi-view Generation (SMG). Given a 3D mesh model,\nMVPaint first simultaneously generates multi-view images by employing an SMG\nmodel, which leads to coarse texturing results with unpainted parts due to\nmissing observations. 2) Spatial-aware 3D Inpainting (S3I). To ensure complete\n3D texturing, we introduce the S3I method, specifically designed to effectively\ntexture previously unobserved areas. 3) UV Refinement (UVR). Furthermore,\nMVPaint employs a UVR module to improve the texture quality in the UV space,\nwhich first performs a UV-space Super-Resolution, followed by a Spatial-aware\nSeam-Smoothing algorithm for revising spatial texturing discontinuities caused\nby UV unwrapping. Moreover, we establish two T2T evaluation benchmarks: the\nObjaverse T2T benchmark and the GSO T2T benchmark, based on selected\nhigh-quality 3D meshes from the Objaverse dataset and the entire GSO dataset,\nrespectively. Extensive experimental results demonstrate that MVPaint surpasses\nexisting state-of-the-art methods. Notably, MVPaint could generate\nhigh-fidelity textures with minimal Janus issues and highly enhanced cross-view\nconsistency.",
            "upvotes": 6,
            "discussionId": "672a3b07d0448c8bae6dca9c"
        },
        "publishedAt": "2024-11-05T14:06:50.844Z",
        "title": "MVPaint: Synchronized Multi-View Diffusion for Painting Anything 3D",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.02336.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/udUHjj6fby82zh8LDjXhL.jpeg",
            "fullname": "Wei Cheng",
            "name": "wchengad",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2411.01747",
            "authors": [
                {
                    "_id": "6729bd832f893196814b31b3",
                    "user": {
                        "_id": "64ba4c2565535cf237da429a",
                        "avatarUrl": "/avatars/a8af0e748686e9d3364f1beaa5039ddb.svg",
                        "isPro": false,
                        "fullname": "Dang Nguyen",
                        "user": "dangmn",
                        "type": "user"
                    },
                    "name": "Dang Nguyen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-11-05T16:09:14.338Z",
                    "hidden": false
                },
                {
                    "_id": "6729bd832f893196814b31b4",
                    "user": {
                        "_id": "63a0a29245edac9f75fc5d54",
                        "avatarUrl": "/avatars/67ba006edfd1ceddc54ea920c09c972b.svg",
                        "isPro": false,
                        "fullname": "Viet Lai",
                        "user": "laiviet",
                        "type": "user"
                    },
                    "name": "Viet Dac Lai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-11-05T16:09:16.220Z",
                    "hidden": false
                },
                {
                    "_id": "6729bd832f893196814b31b5",
                    "name": "Seunghyun Yoon",
                    "hidden": false
                },
                {
                    "_id": "6729bd832f893196814b31b6",
                    "name": "Ryan A. Rossi",
                    "hidden": false
                },
                {
                    "_id": "6729bd832f893196814b31b7",
                    "name": "Handong Zhao",
                    "hidden": false
                },
                {
                    "_id": "6729bd832f893196814b31b8",
                    "name": "Ruiyi Zhang",
                    "hidden": false
                },
                {
                    "_id": "6729bd832f893196814b31b9",
                    "name": "Puneet Mathur",
                    "hidden": false
                },
                {
                    "_id": "6729bd832f893196814b31ba",
                    "name": "Nedim Lipka",
                    "hidden": false
                },
                {
                    "_id": "6729bd832f893196814b31bb",
                    "name": "Yu Wang",
                    "hidden": false
                },
                {
                    "_id": "6729bd832f893196814b31bc",
                    "name": "Trung Bui",
                    "hidden": false
                },
                {
                    "_id": "6729bd832f893196814b31bd",
                    "user": {
                        "_id": "62c5947524171688a9feb992",
                        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
                        "isPro": false,
                        "fullname": "Franck Dernoncourt",
                        "user": "Franck-Dernoncourt",
                        "type": "user"
                    },
                    "name": "Franck Dernoncourt",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-11-05T07:57:10.629Z",
                    "hidden": false
                },
                {
                    "_id": "6729bd832f893196814b31be",
                    "name": "Tianyi Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-04T02:08:59.000Z",
            "title": "DynaSaur: Large Language Agents Beyond Predefined Actions",
            "summary": "Existing LLM agent systems typically select actions from a fixed and\npredefined set at every step. While this approach is effective in closed,\nnarrowly-scoped environments, we argue that it presents two major challenges\nwhen deploying LLM agents in real-world scenarios: (1) selecting from a fixed\nset of actions significantly restricts the planning and acting capabilities of\nLLM agents, and (2) this approach requires substantial human effort to\nenumerate and implement all possible actions, which becomes impractical in\ncomplex environments with a vast number of potential actions. In this work, we\npropose an LLM agent framework that enables the dynamic creation and\ncomposition of actions in an online manner. In this framework, the agent\ninteracts with the environment by generating and executing programs written in\na general-purpose programming language at each step. Furthermore, generated\nactions are accumulated over time for future reuse. Our extensive experiments\non the GAIA benchmark demonstrate that this framework offers significantly\ngreater flexibility and outperforms previous methods. Notably, it allows an LLM\nagent to recover in scenarios where no relevant action exists in the predefined\nset or when existing actions fail due to unforeseen edge cases. At the time of\nwriting, we hold the top position on the GAIA public leaderboard. Our code can\nbe found in\nhttps://github.com/adobe-research/dynasaur{https://github.com/adobe-research/dynasaur}.",
            "upvotes": 6,
            "discussionId": "6729bd842f893196814b31fa"
        },
        "publishedAt": "2024-11-05T05:09:05.995Z",
        "title": "DynaSaur: Large Language Agents Beyond Predefined Actions",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.01747.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "fullname": "Franck Dernoncourt",
            "name": "Franck-Dernoncourt",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        }
    },
    {
        "paper": {
            "id": "2411.02319",
            "authors": [
                {
                    "_id": "6729a976ac899234c0609d00",
                    "name": "Yuyang Zhao",
                    "hidden": false
                },
                {
                    "_id": "6729a976ac899234c0609d01",
                    "name": "Chung-Ching Lin",
                    "hidden": false
                },
                {
                    "_id": "6729a976ac899234c0609d02",
                    "user": {
                        "_id": "6298fd95b58e71e2ac9f3ad8",
                        "avatarUrl": "/avatars/7d34644d537bc5c17cf1e4ce4095355c.svg",
                        "isPro": false,
                        "fullname": "Kevin Lin",
                        "user": "kevinlin311tw",
                        "type": "user"
                    },
                    "name": "Kevin Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-05T14:42:13.832Z",
                    "hidden": false
                },
                {
                    "_id": "6729a976ac899234c0609d03",
                    "user": {
                        "_id": "636eae4d9b61600b629e3815",
                        "avatarUrl": "/avatars/bf11616d5cefe6a0679d2902e3416257.svg",
                        "isPro": false,
                        "fullname": "Zhiwen Yan",
                        "user": "Zhiweny",
                        "type": "user"
                    },
                    "name": "Zhiwen Yan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-05T14:41:36.966Z",
                    "hidden": false
                },
                {
                    "_id": "6729a976ac899234c0609d04",
                    "user": {
                        "_id": "63db16fff03c3d71ef397206",
                        "avatarUrl": "/avatars/bfb7e0d730b7d03302799d5d2828d97d.svg",
                        "isPro": false,
                        "fullname": "Linjie Li",
                        "user": "linjieli222",
                        "type": "user"
                    },
                    "name": "Linjie Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-05T14:42:00.444Z",
                    "hidden": false
                },
                {
                    "_id": "6729a976ac899234c0609d05",
                    "user": {
                        "_id": "630713411801ecc7d2592a7c",
                        "avatarUrl": "/avatars/fb36f69f03421c3a2a7f72ba0858fa60.svg",
                        "isPro": false,
                        "fullname": "Zhengyuan Yang",
                        "user": "zyang39",
                        "type": "user"
                    },
                    "name": "Zhengyuan Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-05T14:41:44.006Z",
                    "hidden": false
                },
                {
                    "_id": "6729a976ac899234c0609d06",
                    "user": {
                        "_id": "641404e74e5305c14f269b3c",
                        "avatarUrl": "/avatars/0dbe862cde7e4bef25bedf860584d8c1.svg",
                        "isPro": false,
                        "fullname": "Jianfeng Wang",
                        "user": "amsword8",
                        "type": "user"
                    },
                    "name": "Jianfeng Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-05T14:41:12.628Z",
                    "hidden": false
                },
                {
                    "_id": "6729a976ac899234c0609d07",
                    "user": {
                        "_id": "64bc7568b7375f6b84503ce2",
                        "avatarUrl": "/avatars/d970c9990d9d84ac51ff225c1b26e304.svg",
                        "isPro": false,
                        "fullname": "Gim Hee Lee",
                        "user": "gimhee-lee",
                        "type": "user"
                    },
                    "name": "Gim Hee Lee",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-05T14:40:55.992Z",
                    "hidden": false
                },
                {
                    "_id": "6729a976ac899234c0609d08",
                    "name": "Lijuan Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-04T17:45:44.000Z",
            "title": "GenXD: Generating Any 3D and 4D Scenes",
            "summary": "Recent developments in 2D visual generation have been remarkably successful.\nHowever, 3D and 4D generation remain challenging in real-world applications due\nto the lack of large-scale 4D data and effective model design. In this paper,\nwe propose to jointly investigate general 3D and 4D generation by leveraging\ncamera and object movements commonly observed in daily life. Due to the lack of\nreal-world 4D data in the community, we first propose a data curation pipeline\nto obtain camera poses and object motion strength from videos. Based on this\npipeline, we introduce a large-scale real-world 4D scene dataset: CamVid-30K.\nBy leveraging all the 3D and 4D data, we develop our framework, GenXD, which\nallows us to produce any 3D or 4D scene. We propose multiview-temporal modules,\nwhich disentangle camera and object movements, to seamlessly learn from both 3D\nand 4D data. Additionally, GenXD employs masked latent conditions to support a\nvariety of conditioning views. GenXD can generate videos that follow the camera\ntrajectory as well as consistent 3D views that can be lifted into 3D\nrepresentations. We perform extensive evaluations across various real-world and\nsynthetic datasets, demonstrating GenXD's effectiveness and versatility\ncompared to previous methods in 3D and 4D generation.",
            "upvotes": 6,
            "discussionId": "6729a983ac899234c060a210"
        },
        "publishedAt": "2024-11-05T04:32:54.115Z",
        "title": "GenXD: Generating Any 3D and 4D Scenes",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64638bd36c27a7e33b26654b/VXjTsYMO1SW3JA8UuFu3w.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.02319.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/2ef5aeb94ef7016082975b4cc201873e.svg",
            "fullname": "Yuyang",
            "name": "Yuyang-z",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2411.02335",
            "authors": [
                {
                    "_id": "6729a58ae53061b3dc4bb379",
                    "name": "Yuqi Luo",
                    "hidden": false
                },
                {
                    "_id": "6729a58ae53061b3dc4bb37a",
                    "user": {
                        "_id": "64c09684e56520a63d35ec87",
                        "avatarUrl": "/avatars/86a338e8d122a66a94143bbb9bf3ebf8.svg",
                        "isPro": false,
                        "fullname": "Chenyang Song",
                        "user": "Raincleared",
                        "type": "user"
                    },
                    "name": "Chenyang Song",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-05T14:43:14.625Z",
                    "hidden": false
                },
                {
                    "_id": "6729a58ae53061b3dc4bb37b",
                    "user": {
                        "_id": "65b8bd1dd4f282b64711d049",
                        "avatarUrl": "/avatars/4e507f0ef4f3420bcff2677fd8ee14e5.svg",
                        "isPro": false,
                        "fullname": "Xu Han",
                        "user": "SillyXu",
                        "type": "user"
                    },
                    "name": "Xu Han",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2024-11-05T04:57:20.885Z",
                    "hidden": false
                },
                {
                    "_id": "6729a58ae53061b3dc4bb37c",
                    "user": {
                        "_id": "6144e4667f2544bb450787b2",
                        "avatarUrl": "/avatars/c7a77d8b0aeb2a81bffac5c09edad9ca.svg",
                        "isPro": false,
                        "fullname": "Yingfa Chen",
                        "user": "chen-yingfa",
                        "type": "user"
                    },
                    "name": "Yingfa Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-05T14:43:08.749Z",
                    "hidden": false
                },
                {
                    "_id": "6729a58ae53061b3dc4bb37d",
                    "user": {
                        "_id": "608f6d72283d0a8d7be9d1f9",
                        "avatarUrl": "/avatars/7f499a37019359a3c488ba6cc11751fc.svg",
                        "isPro": false,
                        "fullname": "Chaojun XIAO",
                        "user": "xcjthu",
                        "type": "user"
                    },
                    "name": "Chaojun Xiao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-05T14:43:01.749Z",
                    "hidden": false
                },
                {
                    "_id": "6729a58ae53061b3dc4bb37e",
                    "name": "Zhiyuan Liu",
                    "hidden": false
                },
                {
                    "_id": "6729a58ae53061b3dc4bb37f",
                    "name": "Maosong Sun",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-04T17:59:04.000Z",
            "title": "Sparsing Law: Towards Large Language Models with Greater Activation\n  Sparsity",
            "summary": "Activation sparsity denotes the existence of substantial weakly-contributed\nelements within activation outputs that can be eliminated, benefiting many\nimportant applications concerned with large language models (LLMs). Although\npromoting greater activation sparsity within LLMs deserves deep studies,\nexisting works lack comprehensive and quantitative research on the correlation\nbetween activation sparsity and potentially influential factors. In this paper,\nwe present a comprehensive study on the quantitative scaling properties and\ninfluential factors of the activation sparsity within decoder-only\nTransformer-based LLMs. Specifically, we propose PPL-p% sparsity, a precise\nand performance-aware activation sparsity metric that is applicable to any\nactivation function. Through extensive experiments, we find several important\nphenomena. Firstly, different activation functions exhibit comparable\nperformance but opposite training-time sparsity trends. The activation ratio\n(i.e., 1-sparsity ratio) evolves as a convergent increasing\npower-law and decreasing logspace power-law with the amount of training data\nfor SiLU-activated and ReLU-activated LLMs, respectively. These demonstrate\nthat ReLU is more efficient as the activation function than SiLU and can\nleverage more training data to improve activation sparsity. Secondly, the\nactivation ratio linearly increases with the width-depth ratio below a certain\nbottleneck point, indicating the potential advantage of a deeper architecture\nat a fixed parameter scale. Finally, at similar width-depth ratios, we\nsurprisingly find that the limit value of activation sparsity varies weakly\nwith the parameter scale, i.e., the activation patterns within LLMs are\ninsensitive to the parameter scale. These empirical laws towards LLMs with\ngreater activation sparsity have important implications for making LLMs more\nefficient and interpretable.",
            "upvotes": 6,
            "discussionId": "6729a5b0e53061b3dc4bbdcf"
        },
        "publishedAt": "2024-11-05T03:27:24.494Z",
        "title": "Sparsing Law: Towards Large Language Models with Greater Activation Sparsity",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.02335.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/86a338e8d122a66a94143bbb9bf3ebf8.svg",
            "fullname": "Chenyang Song",
            "name": "Raincleared",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 9
        }
    },
    {
        "paper": {
            "id": "2411.02355",
            "authors": [
                {
                    "_id": "672a1fde369f174c868341ec",
                    "user": {
                        "_id": "628e0ce4e53bbd334577fcb0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1668801593252-628e0ce4e53bbd334577fcb0.jpeg",
                        "isPro": false,
                        "fullname": "Eldar Kurtic",
                        "user": "ekurtic",
                        "type": "user"
                    },
                    "name": "Eldar Kurtic",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-11-05T13:41:42.690Z",
                    "hidden": false
                },
                {
                    "_id": "672a1fde369f174c868341ed",
                    "user": {
                        "_id": "64ee5432f9ed75901c09a590",
                        "avatarUrl": "/avatars/992163fe824381af39cedec073136967.svg",
                        "isPro": false,
                        "fullname": "Alexandre Marques",
                        "user": "alexmarques",
                        "type": "user"
                    },
                    "name": "Alexandre Marques",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-05T14:46:19.705Z",
                    "hidden": false
                },
                {
                    "_id": "672a1fde369f174c868341ee",
                    "user": {
                        "_id": "632c828a54e2c512c8f55144",
                        "avatarUrl": "/avatars/a30e4b33b21750833a38ab44a874a4db.svg",
                        "isPro": false,
                        "fullname": "Shubhra Pandit",
                        "user": "shubhrapandit",
                        "type": "user"
                    },
                    "name": "Shubhra Pandit",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-05T14:45:54.783Z",
                    "hidden": false
                },
                {
                    "_id": "672a1fde369f174c868341ef",
                    "name": "Mark Kurtz",
                    "hidden": false
                },
                {
                    "_id": "672a1fde369f174c868341f0",
                    "user": {
                        "_id": "64d100c5d8d0927372e3d4c0",
                        "avatarUrl": "/avatars/91d9e4f1dab25b70d901783cdfcd2fd1.svg",
                        "isPro": false,
                        "fullname": "Dan Alistarh",
                        "user": "dalistarh",
                        "type": "user"
                    },
                    "name": "Dan Alistarh",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-05T14:45:08.138Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-04T18:21:59.000Z",
            "title": "\"Give Me BF16 or Give Me Death\"? Accuracy-Performance Trade-Offs in LLM\n  Quantization",
            "summary": "Despite the popularity of large language model (LLM) quantization for\ninference acceleration, significant uncertainty remains regarding the\naccuracy-performance trade-offs associated with various quantization formats.\nWe present a comprehensive empirical study of quantized accuracy, evaluating\npopular quantization formats (FP8, INT8, INT4) across academic benchmarks and\nreal-world tasks, on the entire Llama-3.1 model family. Additionally, our study\nexamines the difference in text generated by quantized models versus their\nuncompressed counterparts. Beyond benchmarks, we also present a couple of\nquantization improvements which allowed us to obtain state-of-the-art accuracy\nrecovery results. Our investigation, encompassing over 500,000 individual\nevaluations, yields several key findings: (1) FP8 weight and activation\nquantization (W8A8-FP) is lossless across all model scales, (2) INT8 weight and\nactivation quantization (W8A8-INT), when properly tuned, incurs surprisingly\nlow 1-3% accuracy degradation, and (3) INT4 weight-only quantization\n(W4A16-INT) is competitive with 8-bit integer weight and activation\nquantization. To address the question of the \"best\" format for a given\ndeployment environment, we conduct inference performance analysis using the\npopular open-source vLLM framework on various GPU architectures. We find that\nW4A16 offers the best cost-efficiency for synchronous deployments, and for\nasynchronous deployment on mid-tier GPUs. At the same time, W8A8 formats excel\nin asynchronous \"continuous batching\" deployment of mid- and large-size models\non high-end GPUs. Our results provide a set of practical guidelines for\ndeploying quantized LLMs across scales and performance requirements.",
            "upvotes": 5,
            "discussionId": "672a1fdf369f174c86834215"
        },
        "publishedAt": "2024-11-05T12:31:24.256Z",
        "title": "\"Give Me BF16 or Give Me Death\"? Accuracy-Performance Trade-Offs in LLM Quantization",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/628e0ce4e53bbd334577fcb0/kCHV7YOrfCVOsdqal_zIi.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.02355.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1668801593252-628e0ce4e53bbd334577fcb0.jpeg",
            "fullname": "Eldar Kurtic",
            "name": "ekurtic",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 3
        }
    },
    {
        "paper": {
            "id": "2411.00918",
            "authors": [
                {
                    "_id": "672979dafc78f157e47a8ad0",
                    "user": {
                        "_id": "64c2bea2ada7df214276913b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c2bea2ada7df214276913b/QFCtmCn439Afsr7uqyoMT.jpeg",
                        "isPro": false,
                        "fullname": "Nguyen Van Nam",
                        "user": "DavidNguyen",
                        "type": "user"
                    },
                    "name": "Nam V. Nguyen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-11-05T07:58:16.335Z",
                    "hidden": false
                },
                {
                    "_id": "672979dafc78f157e47a8ad1",
                    "user": {
                        "_id": "63e8ad2accae1fe5c61878d6",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676193023976-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Doan Tien Thong",
                        "user": "doantienthongbku",
                        "type": "user"
                    },
                    "name": "Thong T. Doan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-11-05T07:58:14.469Z",
                    "hidden": false
                },
                {
                    "_id": "672979dafc78f157e47a8ad2",
                    "name": "Luong Tran",
                    "hidden": false
                },
                {
                    "_id": "672979dafc78f157e47a8ad3",
                    "name": "Van Nguyen",
                    "hidden": false
                },
                {
                    "_id": "672979dafc78f157e47a8ad4",
                    "name": "Quang Pham",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-01T14:04:36.000Z",
            "title": "LIBMoE: A Library for comprehensive benchmarking Mixture of Experts in\n  Large Language Models",
            "summary": "Mixture of Experts (MoEs) plays an important role in the development of more\nefficient and effective large language models (LLMs). Due to the enormous\nresource requirements, studying large scale MoE algorithms remain in-accessible\nto many researchers. This work develops LibMoE, a comprehensive and\nmodular framework to streamline the research, training, and evaluation of MoE\nalgorithms. Built upon three core principles: (i) modular design, (ii)\nefficient training; (iii) comprehensive evaluation, LibMoE brings MoE in LLMs\nmore accessible to a wide range of researchers by standardizing the training\nand evaluation pipelines. Using LibMoE, we extensively benchmarked five\nstate-of-the-art MoE algorithms over three different LLMs and 11 datasets under\nthe zero-shot setting. The results show that despite the unique\ncharacteristics, all MoE algorithms perform roughly similar when averaged\nacross a wide range of tasks. With the modular design and extensive evaluation,\nwe believe LibMoE will be invaluable for researchers to make meaningful\nprogress towards the next generation of MoE and LLMs. Project page:\nhttps://fsoft-aic.github.io/fsoft-LibMoE.github.io.",
            "upvotes": 5,
            "discussionId": "672979dbfc78f157e47a8b20"
        },
        "publishedAt": "2024-11-05T00:57:15.530Z",
        "title": "LIBMoE: A Library for comprehensive benchmarking Mixture of Experts in Large Language Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.00918.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c2bea2ada7df214276913b/QFCtmCn439Afsr7uqyoMT.jpeg",
            "fullname": "Nguyen Van Nam",
            "name": "DavidNguyen",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2411.02327",
            "authors": [
                {
                    "_id": "672a17a8341f15009010af9d",
                    "user": {
                        "_id": "650f0e3988cdfe73a864b9c5",
                        "avatarUrl": "/avatars/ce74b96743a99819a762c23b4f2204fd.svg",
                        "isPro": false,
                        "fullname": "Ruyang Liu",
                        "user": "farewellthree",
                        "type": "user"
                    },
                    "name": "Ruyang Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-11-05T13:36:20.999Z",
                    "hidden": false
                },
                {
                    "_id": "672a17a8341f15009010af9e",
                    "name": "Haoran Tang",
                    "hidden": false
                },
                {
                    "_id": "672a17a8341f15009010af9f",
                    "name": "Haibo Liu",
                    "hidden": false
                },
                {
                    "_id": "672a17a8341f15009010afa0",
                    "name": "Yixiao Ge",
                    "hidden": false
                },
                {
                    "_id": "672a17a8341f15009010afa1",
                    "name": "Ying Shan",
                    "hidden": false
                },
                {
                    "_id": "672a17a8341f15009010afa2",
                    "name": "Chen Li",
                    "hidden": false
                },
                {
                    "_id": "672a17a8341f15009010afa3",
                    "name": "Jiankun Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-04T17:50:36.000Z",
            "title": "PPLLaVA: Varied Video Sequence Understanding With Prompt Guidance",
            "summary": "The past year has witnessed the significant advancement of video-based large\nlanguage models. However, the challenge of developing a unified model for both\nshort and long video understanding remains unresolved. Most existing video LLMs\ncannot handle hour-long videos, while methods custom for long videos tend to be\nineffective for shorter videos and images. In this paper, we identify the key\nissue as the redundant content in videos. To address this, we propose a novel\npooling strategy that simultaneously achieves token compression and\ninstruction-aware visual feature aggregation. Our model is termed Prompt-guided\nPooling LLaVA, or PPLLaVA for short. Specifically, PPLLaVA consists of three\ncore components: the CLIP-based visual-prompt alignment that extracts visual\ninformation relevant to the user's instructions, the prompt-guided pooling that\ncompresses the visual sequence to arbitrary scales using convolution-style\npooling, and the clip context extension designed for lengthy prompt common in\nvisual dialogue. Moreover, our codebase also integrates the most advanced video\nDirect Preference Optimization (DPO) and visual interleave training. Extensive\nexperiments have validated the performance of our model. With superior\nthroughput and only 1024 visual context, PPLLaVA achieves better results on\nimage benchmarks as a video LLM, while achieving state-of-the-art performance\nacross various video benchmarks, excelling in tasks ranging from caption\ngeneration to multiple-choice questions, and handling video lengths from\nseconds to hours. Codes have been available at\nhttps://github.com/farewellthree/PPLLaVA.",
            "upvotes": 4,
            "discussionId": "672a17aa341f15009010b025"
        },
        "publishedAt": "2024-11-05T13:39:45.005Z",
        "title": "PPLLaVA: Varied Video Sequence Understanding With Prompt Guidance",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.02327.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/ce74b96743a99819a762c23b4f2204fd.svg",
            "fullname": "Ruyang Liu",
            "name": "farewellthree",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        }
    },
    {
        "paper": {
            "id": "2411.00743",
            "authors": [
                {
                    "_id": "67298c6932c59bb502696f25",
                    "user": {
                        "_id": "64755a83e0b188d3cb2579d8",
                        "avatarUrl": "/avatars/2c50590905f4bd398a4c9991e1b4b5bb.svg",
                        "isPro": false,
                        "fullname": "Aashiq Muhamed",
                        "user": "aashiqmuhamed",
                        "type": "user"
                    },
                    "name": "Aashiq Muhamed",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-05T16:31:59.109Z",
                    "hidden": false
                },
                {
                    "_id": "67298c6932c59bb502696f26",
                    "name": "Mona Diab",
                    "hidden": false
                },
                {
                    "_id": "67298c6932c59bb502696f27",
                    "name": "Virginia Smith",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-01T17:09:34.000Z",
            "title": "Decoding Dark Matter: Specialized Sparse Autoencoders for Interpreting\n  Rare Concepts in Foundation Models",
            "summary": "Understanding and mitigating the potential risks associated with foundation\nmodels (FMs) hinges on developing effective interpretability methods. Sparse\nAutoencoders (SAEs) have emerged as a promising tool for disentangling FM\nrepresentations, but they struggle to capture rare, yet crucial concepts in the\ndata. We introduce Specialized Sparse Autoencoders (SSAEs), designed to\nilluminate these elusive dark matter features by focusing on specific\nsubdomains. We present a practical recipe for training SSAEs, demonstrating the\nefficacy of dense retrieval for data selection and the benefits of Tilted\nEmpirical Risk Minimization as a training objective to improve concept recall.\nOur evaluation of SSAEs on standard metrics, such as downstream perplexity and\nL_0 sparsity, show that they effectively capture subdomain tail concepts,\nexceeding the capabilities of general-purpose SAEs. We showcase the practical\nutility of SSAEs in a case study on the Bias in Bios dataset, where SSAEs\nachieve a 12.5\\% increase in worst-group classification accuracy when applied\nto remove spurious gender information. SSAEs provide a powerful new lens for\npeering into the inner workings of FMs in subdomains.",
            "upvotes": 4,
            "discussionId": "67298c6b32c59bb502697020"
        },
        "publishedAt": "2024-11-05T01:40:29.693Z",
        "title": "Decoding Dark Matter: Specialized Sparse Autoencoders for Interpreting Rare Concepts in Foundation Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.00743.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/2c50590905f4bd398a4c9991e1b4b5bb.svg",
            "fullname": "Aashiq Muhamed",
            "name": "aashiqmuhamed",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        }
    },
    {
        "paper": {
            "id": "2411.00492",
            "authors": [
                {
                    "_id": "67284e2f24e97c8a0709304b",
                    "user": {
                        "_id": "63a9a0d13453852ef53c0b37",
                        "avatarUrl": "/avatars/411c4ffc2a3e89047a23c6e7442f6ed5.svg",
                        "isPro": false,
                        "fullname": "Do Xuan Long",
                        "user": "dxlong2000",
                        "type": "user"
                    },
                    "name": "Do Xuan Long",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-11-05T16:09:22.404Z",
                    "hidden": false
                },
                {
                    "_id": "67284e2f24e97c8a0709304c",
                    "user": {
                        "_id": "64807cb87d65d9ac173045ad",
                        "avatarUrl": "/avatars/9f65a57b440e9772f1de5fd6fd425db0.svg",
                        "isPro": false,
                        "fullname": "Duong Ngoc Yen",
                        "user": "duongngocyen",
                        "type": "user"
                    },
                    "name": "Duong Ngoc Yen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-05T16:47:43.606Z",
                    "hidden": false
                },
                {
                    "_id": "67284e2f24e97c8a0709304d",
                    "user": {
                        "_id": "655722e80438e0854fae7554",
                        "avatarUrl": "/avatars/b93a74f7c7880f9fe0f3ffb47e2aef5e.svg",
                        "isPro": false,
                        "fullname": "Luu Anh Tuan",
                        "user": "anhtuanluu36",
                        "type": "user"
                    },
                    "name": "Anh Tuan Luu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-05T16:47:49.354Z",
                    "hidden": false
                },
                {
                    "_id": "67284e2f24e97c8a0709304e",
                    "name": "Kenji Kawaguchi",
                    "hidden": false
                },
                {
                    "_id": "67284e2f24e97c8a0709304f",
                    "user": {
                        "_id": "628f137053028c04a3b750e4",
                        "avatarUrl": "/avatars/85d174cc041ab91e724bb9d9d4e46c88.svg",
                        "isPro": false,
                        "fullname": "Min-Yen Kan",
                        "user": "knmnyn",
                        "type": "user"
                    },
                    "name": "Min-Yen Kan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-05T16:48:01.861Z",
                    "hidden": false
                },
                {
                    "_id": "67284e2f24e97c8a07093050",
                    "name": "Nancy F. Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-01T10:06:52.000Z",
            "title": "Multi-expert Prompting Improves Reliability, Safety, and Usefulness of\n  Large Language Models",
            "summary": "We present Multi-expert Prompting, a novel enhancement of ExpertPrompting (Xu\net al., 2023), designed to improve the large language model (LLM) generation.\nSpecifically, it guides an LLM to fulfill an input instruction by simulating\nmultiple experts, aggregating their responses, and selecting the best among\nindividual and aggregated responses. This process is performed in a single\nchain of thoughts through our seven carefully designed subtasks derived from\nthe Nominal Group Technique (Ven and Delbecq, 1974), a well-established\ndecision-making framework. Our evaluations demonstrate that Multi-expert\nPrompting significantly outperforms ExpertPrompting and comparable baselines in\nenhancing the truthfulness, factuality, informativeness, and usefulness of\nresponses while reducing toxicity and hurtfulness. It further achieves\nstate-of-the-art truthfulness by outperforming the best baseline by 8.69% with\nChatGPT. Multi-expert Prompting is efficient, explainable, and highly adaptable\nto diverse scenarios, eliminating the need for manual prompt construction.",
            "upvotes": 1,
            "discussionId": "67284e3424e97c8a07093193"
        },
        "publishedAt": "2024-11-05T14:47:35.320Z",
        "title": "Multi-expert Prompting Improves Reliability, Safety, and Usefulness of Large Language Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.00492.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/411c4ffc2a3e89047a23c6e7442f6ed5.svg",
            "fullname": "Do Xuan Long",
            "name": "dxlong2000",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        }
    },
    {
        "paper": {
            "id": "2411.01192",
            "authors": [
                {
                    "_id": "6729ea15374c0ab99330f8b7",
                    "user": {
                        "_id": "60394599033b61166496163b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1614366097007-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Gagan Bhatia",
                        "user": "gagan3012",
                        "type": "user"
                    },
                    "name": "Gagan Bhatia",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-05T14:44:10.856Z",
                    "hidden": false
                },
                {
                    "_id": "6729ea15374c0ab99330f8b8",
                    "name": "El Moatez Billah Nagoudi",
                    "hidden": false
                },
                {
                    "_id": "6729ea15374c0ab99330f8b9",
                    "user": {
                        "_id": "631650cb894404e2506aac43",
                        "avatarUrl": "/avatars/d47134b29a968b71c5c9c2d65093945c.svg",
                        "isPro": false,
                        "fullname": "Abdellah EL MEKKI",
                        "user": "3ebdola",
                        "type": "user"
                    },
                    "name": "Abdellah El Mekki",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-05T14:44:22.207Z",
                    "hidden": false
                },
                {
                    "_id": "6729ea15374c0ab99330f8ba",
                    "user": {
                        "_id": "6265c5fe5909b93b24d797f1",
                        "avatarUrl": "/avatars/94f8e5f925c740d498c2709fe784a40b.svg",
                        "isPro": false,
                        "fullname": "Fakhraddin Alwajih",
                        "user": "Fakhraddin",
                        "type": "user"
                    },
                    "name": "Fakhraddin Alwajih",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-05T14:44:27.624Z",
                    "hidden": false
                },
                {
                    "_id": "6729ea15374c0ab99330f8bb",
                    "user": {
                        "_id": "5feeae8dd8e0b2d24ed258cc",
                        "avatarUrl": "/avatars/1d66840db4ed6dc12bdee7ba8168ea9f.svg",
                        "isPro": false,
                        "fullname": "Muhammad Abdul-Mageed",
                        "user": "mageed",
                        "type": "user"
                    },
                    "name": "Muhammad Abdul-Mageed",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-05T14:44:33.167Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-02T09:39:49.000Z",
            "title": "Swan and ArabicMTEB: Dialect-Aware, Arabic-Centric, Cross-Lingual, and\n  Cross-Cultural Embedding Models and Benchmarks",
            "summary": "We introduce Swan, a family of embedding models centred around the Arabic\nlanguage, addressing both small-scale and large-scale use cases. Swan includes\ntwo variants: Swan-Small, based on ARBERTv2, and Swan-Large, built on\nArMistral, a pretrained Arabic large language model. To evaluate these models,\nwe propose ArabicMTEB, a comprehensive benchmark suite that assesses\ncross-lingual, multi-dialectal, multi-domain, and multi-cultural Arabic text\nembedding performance, covering eight diverse tasks and spanning 94 datasets.\nSwan-Large achieves state-of-the-art results, outperforming\nMultilingual-E5-large in most Arabic tasks, while the Swan-Small consistently\nsurpasses Multilingual-E5 base. Our extensive evaluations demonstrate that Swan\nmodels are both dialectally and culturally aware, excelling across various\nArabic domains while offering significant monetary efficiency. This work\nsignificantly advances the field of Arabic language modelling and provides\nvaluable resources for future research and applications in Arabic natural\nlanguage processing. Our models and benchmark will be made publicly accessible\nfor research.",
            "upvotes": 1,
            "discussionId": "6729ea16374c0ab99330f928"
        },
        "publishedAt": "2024-11-05T08:19:26.462Z",
        "title": "Swan and ArabicMTEB: Dialect-Aware, Arabic-Centric, Cross-Lingual, and Cross-Cultural Embedding Models and Benchmarks",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.01192.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1614366097007-noauth.jpeg",
            "fullname": "Gagan Bhatia",
            "name": "gagan3012",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 17
        }
    }
]