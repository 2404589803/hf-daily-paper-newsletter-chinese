[
  {
    "paper": {
      "id": "2505.14683",
      "authors": [
        {
          "_id": "682d2fd84540abccd3b835e8",
          "name": "Chaorui Deng",
          "hidden": false
        },
        {
          "_id": "682d2fd84540abccd3b835e9",
          "name": "Deyao Zhu",
          "hidden": false
        },
        {
          "_id": "682d2fd84540abccd3b835ea",
          "name": "Kunchang Li",
          "hidden": false
        },
        {
          "_id": "682d2fd84540abccd3b835eb",
          "name": "Chenhui Gou",
          "hidden": false
        },
        {
          "_id": "682d2fd84540abccd3b835ec",
          "name": "Feng Li",
          "hidden": false
        },
        {
          "_id": "682d2fd84540abccd3b835ed",
          "name": "Zeyu Wang",
          "hidden": false
        },
        {
          "_id": "682d2fd84540abccd3b835ee",
          "name": "Shu Zhong",
          "hidden": false
        },
        {
          "_id": "682d2fd84540abccd3b835ef",
          "name": "Weihao Yu",
          "hidden": false
        },
        {
          "_id": "682d2fd84540abccd3b835f0",
          "name": "Xiaonan Nie",
          "hidden": false
        },
        {
          "_id": "682d2fd84540abccd3b835f1",
          "name": "Ziang Song",
          "hidden": false
        },
        {
          "_id": "682d2fd84540abccd3b835f2",
          "name": "Guang Shi",
          "hidden": false
        },
        {
          "_id": "682d2fd84540abccd3b835f3",
          "name": "Haoqi Fan",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/61fb81006374891646732f37/HQOfWqrOf9B97hWczL489.png"
      ],
      "publishedAt": "2025-05-20T17:59:30.000Z",
      "submittedOnDailyAt": "2025-05-21T00:38:53.960Z",
      "title": "Emerging Properties in Unified Multimodal Pretraining",
      "submittedOnDailyBy": {
        "_id": "61fb81006374891646732f37",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1643872995181-61fb81006374891646732f37.jpeg",
        "isPro": false,
        "fullname": "Kunchang Li",
        "user": "Andy1621",
        "type": "user"
      },
      "summary": "Unifying multimodal understanding and generation has shown impressive\ncapabilities in cutting-edge proprietary systems. In this work, we introduce\nBAGEL, an open0source foundational model that natively supports multimodal\nunderstanding and generation. BAGEL is a unified, decoder0only model pretrained\non trillions of tokens curated from large0scale interleaved text, image, video,\nand web data. When scaled with such diverse multimodal interleaved data, BAGEL\nexhibits emerging capabilities in complex multimodal reasoning. As a result, it\nsignificantly outperforms open-source unified models in both multimodal\ngeneration and understanding across standard benchmarks, while exhibiting\nadvanced multimodal reasoning abilities such as free-form image manipulation,\nfuture frame prediction, 3D manipulation, and world navigation. In the hope of\nfacilitating further opportunities for multimodal research, we share the key\nfindings, pretraining details, data creation protocal, and release our code and\ncheckpoints to the community. The project page is at https://bagel-ai.org/",
      "upvotes": 45,
      "discussionId": "682d2fdc4540abccd3b836ee",
      "ai_keywords": [
        "unified, decoder-only model",
        "pretrained",
        "trillions of tokens",
        "large-scale interleaved data",
        "complex multimodal reasoning",
        "multimodal generation",
        "multimodal understanding",
        "free-form image manipulation",
        "future frame prediction",
        "3D manipulation",
        "world navigation"
      ]
    },
    "publishedAt": "2025-05-20T13:59:30.000Z",
    "title": "Emerging Properties in Unified Multimodal Pretraining",
    "summary": "Unifying multimodal understanding and generation has shown impressive\ncapabilities in cutting-edge proprietary systems. In this work, we introduce\nBAGEL, an open0source foundational model that natively supports multimodal\nunderstanding and generation. BAGEL is a unified, decoder0only model pretrained\non trillions of tokens curated from large0scale interleaved text, image, video,\nand web data. When scaled with such diverse multimodal interleaved data, BAGEL\nexhibits emerging capabilities in complex multimodal reasoning. As a result, it\nsignificantly outperforms open-source unified models in both multimodal\ngeneration and understanding across standard benchmarks, while exhibiting\nadvanced multimodal reasoning abilities such as free-form image manipulation,\nfuture frame prediction, 3D manipulation, and world navigation. In the hope of\nfacilitating further opportunities for multimodal research, we share the key\nfindings, pretraining details, data creation protocal, and release our code and\ncheckpoints to the community. The project page is at https://bagel-ai.org/",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/61fb81006374891646732f37/HQOfWqrOf9B97hWczL489.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14683.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61fb81006374891646732f37",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1643872995181-61fb81006374891646732f37.jpeg",
      "fullname": "Kunchang Li",
      "name": "Andy1621",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 20
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.11594",
      "authors": [
        {
          "_id": "682d426251ce04237318cfe5",
          "name": "Jintao Zhang",
          "hidden": false
        },
        {
          "_id": "682d426251ce04237318cfe6",
          "name": "Jia Wei",
          "hidden": false
        },
        {
          "_id": "682d426251ce04237318cfe7",
          "name": "Pengle Zhang",
          "hidden": false
        },
        {
          "_id": "682d426251ce04237318cfe8",
          "name": "Xiaoming Xu",
          "hidden": false
        },
        {
          "_id": "682d426251ce04237318cfe9",
          "name": "Haofeng Huang",
          "hidden": false
        },
        {
          "_id": "682d426251ce04237318cfea",
          "name": "Haoxu Wang",
          "hidden": false
        },
        {
          "_id": "682d426251ce04237318cfeb",
          "name": "Kai Jiang",
          "hidden": false
        },
        {
          "_id": "682d426251ce04237318cfec",
          "name": "Jun Zhu",
          "hidden": false
        },
        {
          "_id": "682d426251ce04237318cfed",
          "name": "Jianfei Chen",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/66c0a08bac74db25de8427ec/Tb20E3IJSV6PjcD9Nkvfg.png"
      ],
      "publishedAt": "2025-05-16T18:01:54.000Z",
      "submittedOnDailyAt": "2025-05-21T01:35:25.101Z",
      "title": "SageAttention3: Microscaling FP4 Attention for Inference and An\n  Exploration of 8-Bit Training",
      "submittedOnDailyBy": {
        "_id": "66c0a08bac74db25de8427ec",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg",
        "isPro": false,
        "fullname": "Jintao Zhang",
        "user": "jt-zhang",
        "type": "user"
      },
      "summary": "The efficiency of attention is important due to its quadratic time\ncomplexity. We enhance the efficiency of attention through two key\ncontributions: First, we leverage the new FP4 Tensor Cores in Blackwell GPUs to\naccelerate attention computation. Our implementation achieves 1038 TOPS on\nRTX5090, which is a 5x speedup over the fastest FlashAttention on RTX5090.\nExperiments show that our FP4 attention can accelerate inference of various\nmodels in a plug-and-play way. Second, we pioneer low-bit attention to training\ntasks. Existing low-bit attention works like FlashAttention3 and SageAttention\nfocus only on inference. However, the efficiency of training large models is\nalso important. To explore whether low-bit attention can be effectively applied\nto training tasks, we design an accurate and efficient 8-bit attention for both\nforward and backward propagation. Experiments indicate that 8-bit attention\nachieves lossless performance in fine-tuning tasks but exhibits slower\nconvergence in pretraining tasks. The code will be available at\nhttps://github.com/thu-ml/SageAttention.",
      "upvotes": 25,
      "discussionId": "682d426551ce04237318d0b9",
      "projectPage": "https://github.com/thu-ml/SageAttention",
      "githubRepo": "https://github.com/thu-ml/SageAttention",
      "ai_keywords": [
        "attention",
        "FP4 Tensor Cores",
        "TOPS",
        "speedup",
        "inference",
        "plug-and-play",
        "low-bit attention",
        "8-bit attention",
        "forward propagation",
        "backward propagation",
        "fine-tuning",
        "pretraining",
        "convergence",
        "lossless performance"
      ]
    },
    "publishedAt": "2025-05-16T14:01:54.000Z",
    "title": "SageAttention3: Microscaling FP4 Attention for Inference and An\n  Exploration of 8-Bit Training",
    "summary": "The efficiency of attention is important due to its quadratic time\ncomplexity. We enhance the efficiency of attention through two key\ncontributions: First, we leverage the new FP4 Tensor Cores in Blackwell GPUs to\naccelerate attention computation. Our implementation achieves 1038 TOPS on\nRTX5090, which is a 5x speedup over the fastest FlashAttention on RTX5090.\nExperiments show that our FP4 attention can accelerate inference of various\nmodels in a plug-and-play way. Second, we pioneer low-bit attention to training\ntasks. Existing low-bit attention works like FlashAttention3 and SageAttention\nfocus only on inference. However, the efficiency of training large models is\nalso important. To explore whether low-bit attention can be effectively applied\nto training tasks, we design an accurate and efficient 8-bit attention for both\nforward and backward propagation. Experiments indicate that 8-bit attention\nachieves lossless performance in fine-tuning tasks but exhibits slower\nconvergence in pretraining tasks. The code will be available at\nhttps://github.com/thu-ml/SageAttention.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/66c0a08bac74db25de8427ec/Tb20E3IJSV6PjcD9Nkvfg.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11594.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66c0a08bac74db25de8427ec",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg",
      "fullname": "Jintao Zhang",
      "name": "jt-zhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.14246",
      "authors": [
        {
          "_id": "682d7a2340a42d1538fada76",
          "name": "Ziyu Liu",
          "hidden": false
        },
        {
          "_id": "682d7a2340a42d1538fada77",
          "name": "Yuhang Zang",
          "hidden": false
        },
        {
          "_id": "682d7a2340a42d1538fada78",
          "name": "Yushan Zou",
          "hidden": false
        },
        {
          "_id": "682d7a2340a42d1538fada79",
          "name": "Zijian Liang",
          "hidden": false
        },
        {
          "_id": "682d7a2340a42d1538fada7a",
          "name": "Xiaoyi Dong",
          "hidden": false
        },
        {
          "_id": "682d7a2340a42d1538fada7b",
          "name": "Yuhang Cao",
          "hidden": false
        },
        {
          "_id": "682d7a2340a42d1538fada7c",
          "name": "Haodong Duan",
          "hidden": false
        },
        {
          "_id": "682d7a2340a42d1538fada7d",
          "name": "Dahua Lin",
          "hidden": false
        },
        {
          "_id": "682d7a2340a42d1538fada7e",
          "name": "Jiaqi Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T11:59:25.000Z",
      "submittedOnDailyAt": "2025-05-21T05:32:01.442Z",
      "title": "Visual Agentic Reinforcement Fine-Tuning",
      "submittedOnDailyBy": {
        "_id": "64b4eec4faa3181a5eab9c46",
        "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
        "isPro": true,
        "fullname": "Jiaqi Wang",
        "user": "myownskyW7",
        "type": "user"
      },
      "summary": "A key trend in Large Reasoning Models (e.g., OpenAI's o3) is the native\nagentic ability to use external tools such as web browsers for searching and\nwriting/executing code for image manipulation to think with images. In the\nopen-source research community, while significant progress has been made in\nlanguage-only agentic abilities such as function calling and tool integration,\nthe development of multi-modal agentic capabilities that involve truly thinking\nwith images, and their corresponding benchmarks, are still less explored. This\nwork highlights the effectiveness of Visual Agentic Reinforcement Fine-Tuning\n(Visual-ARFT) for enabling flexible and adaptive reasoning abilities for Large\nVision-Language Models (LVLMs). With Visual-ARFT, open-source LVLMs gain the\nability to browse websites for real-time information updates and write code to\nmanipulate and analyze input images through cropping, rotation, and other image\nprocessing techniques. We also present a Multi-modal Agentic Tool Bench (MAT)\nwith two settings (MAT-Search and MAT-Coding) designed to evaluate LVLMs'\nagentic search and coding abilities. Our experimental results demonstrate that\nVisual-ARFT outperforms its baseline by +18.6% F1 / +13.0% EM on MAT-Coding and\n+10.3% F1 / +8.7% EM on MAT-Search, ultimately surpassing GPT-4o. Visual-ARFT\nalso achieves +29.3 F1% / +25.9% EM gains on existing multi-hop QA benchmarks\nsuch as 2Wiki and HotpotQA, demonstrating strong generalization capabilities.\nOur findings suggest that Visual-ARFT offers a promising path toward building\nrobust and generalizable multimodal agents.",
      "upvotes": 12,
      "discussionId": "682d7a2440a42d1538fadac0",
      "githubRepo": "https://github.com/Liuziyu77/Visual-RFT/tree/main/Visual-ARFT",
      "ai_keywords": [
        "Visual Agentic Reinforcement Fine-Tuning (Visual-ARFT)",
        "Large Vision-Language Models (LVLMs)",
        "Multi-modal Agentic Tool Bench (MAT)",
        "MAT-Search",
        "MAT-Coding",
        "F1",
        "EM",
        "GPT-4o",
        "multi-hop QA benchmarks",
        "2Wiki",
        "HotpotQA"
      ]
    },
    "publishedAt": "2025-05-20T07:59:25.000Z",
    "title": "Visual Agentic Reinforcement Fine-Tuning",
    "summary": "A key trend in Large Reasoning Models (e.g., OpenAI's o3) is the native\nagentic ability to use external tools such as web browsers for searching and\nwriting/executing code for image manipulation to think with images. In the\nopen-source research community, while significant progress has been made in\nlanguage-only agentic abilities such as function calling and tool integration,\nthe development of multi-modal agentic capabilities that involve truly thinking\nwith images, and their corresponding benchmarks, are still less explored. This\nwork highlights the effectiveness of Visual Agentic Reinforcement Fine-Tuning\n(Visual-ARFT) for enabling flexible and adaptive reasoning abilities for Large\nVision-Language Models (LVLMs). With Visual-ARFT, open-source LVLMs gain the\nability to browse websites for real-time information updates and write code to\nmanipulate and analyze input images through cropping, rotation, and other image\nprocessing techniques. We also present a Multi-modal Agentic Tool Bench (MAT)\nwith two settings (MAT-Search and MAT-Coding) designed to evaluate LVLMs'\nagentic search and coding abilities. Our experimental results demonstrate that\nVisual-ARFT outperforms its baseline by +18.6% F1 / +13.0% EM on MAT-Coding and\n+10.3% F1 / +8.7% EM on MAT-Search, ultimately surpassing GPT-4o. Visual-ARFT\nalso achieves +29.3 F1% / +25.9% EM gains on existing multi-hop QA benchmarks\nsuch as 2Wiki and HotpotQA, demonstrating strong generalization capabilities.\nOur findings suggest that Visual-ARFT offers a promising path toward building\nrobust and generalizable multimodal agents.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14246.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b4eec4faa3181a5eab9c46",
      "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
      "fullname": "Jiaqi Wang",
      "name": "myownskyW7",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 19
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.14513",
      "authors": [
        {
          "_id": "682d334862cadf615f5f73e6",
          "name": "Yen-Chen Wu",
          "hidden": false
        },
        {
          "_id": "682d334862cadf615f5f73e7",
          "name": "Feng-Ting Liao",
          "hidden": false
        },
        {
          "_id": "682d334862cadf615f5f73e8",
          "name": "Meng-Hsi Chen",
          "hidden": false
        },
        {
          "_id": "682d334862cadf615f5f73e9",
          "name": "Pei-Chen Ho",
          "hidden": false
        },
        {
          "_id": "682d334862cadf615f5f73ea",
          "name": "Farhang Nabiei",
          "hidden": false
        },
        {
          "_id": "682d334862cadf615f5f73eb",
          "name": "Da-shan Shiu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T15:41:05.000Z",
      "submittedOnDailyAt": "2025-05-21T00:30:57.355Z",
      "title": "Latent Flow Transformer",
      "submittedOnDailyBy": {
        "_id": "643fb7332397d8eef5b844cd",
        "avatarUrl": "/avatars/e403a19fc13e478d5929c67028230b0e.svg",
        "isPro": false,
        "fullname": "Feng-Ting Liao",
        "user": "FengTing",
        "type": "user"
      },
      "summary": "Transformers, the standard implementation for large language models (LLMs),\ntypically consist of tens to hundreds of discrete layers. While more layers can\nlead to better performance, this approach has been challenged as far from\nefficient, especially given the superiority of continuous layers demonstrated\nby diffusion and flow-based models for image generation. We propose the Latent\nFlow Transformer (LFT), which replaces a block of layers with a single learned\ntransport operator trained via flow matching, offering significant compression\nwhile maintaining compatibility with the original architecture. Additionally,\nwe address the limitations of existing flow-based methods in preserving\ncoupling by introducing the Flow Walking (FW) algorithm. On the Pythia-410M\nmodel, LFT trained with flow matching compresses 6 of 24 layers and outperforms\ndirectly skipping 2 layers (KL Divergence of LM logits at 0.407 vs. 0.529),\ndemonstrating the feasibility of this design. When trained with FW, LFT further\ndistills 12 layers into one while reducing the KL to 0.736 surpassing that from\nskipping 3 layers (0.932), significantly narrowing the gap between\nautoregressive and flow-based generation paradigms.",
      "upvotes": 10,
      "discussionId": "682d334962cadf615f5f743f",
      "githubRepo": "https://github.com/mtkresearch/latent-flow-transformer",
      "ai_keywords": [
        "Transformers",
        "large language models (LLMs)",
        "discrete layers",
        "continuous layers",
        "diffusion models",
        "flow-based models",
        "latent Flow Transformer (LFT)",
        "learned transport operator",
        "flow matching",
        "Flow Walking (FW) algorithm",
        "Pythia-410M",
        "KL Divergence",
        "autoregressive",
        "flow-based generation paradigms"
      ]
    },
    "publishedAt": "2025-05-20T11:41:05.000Z",
    "title": "Latent Flow Transformer",
    "summary": "Transformers, the standard implementation for large language models (LLMs),\ntypically consist of tens to hundreds of discrete layers. While more layers can\nlead to better performance, this approach has been challenged as far from\nefficient, especially given the superiority of continuous layers demonstrated\nby diffusion and flow-based models for image generation. We propose the Latent\nFlow Transformer (LFT), which replaces a block of layers with a single learned\ntransport operator trained via flow matching, offering significant compression\nwhile maintaining compatibility with the original architecture. Additionally,\nwe address the limitations of existing flow-based methods in preserving\ncoupling by introducing the Flow Walking (FW) algorithm. On the Pythia-410M\nmodel, LFT trained with flow matching compresses 6 of 24 layers and outperforms\ndirectly skipping 2 layers (KL Divergence of LM logits at 0.407 vs. 0.529),\ndemonstrating the feasibility of this design. When trained with FW, LFT further\ndistills 12 layers into one while reducing the KL to 0.736 surpassing that from\nskipping 3 layers (0.932), significantly narrowing the gap between\nautoregressive and flow-based generation paradigms.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14513.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643fb7332397d8eef5b844cd",
      "avatarUrl": "/avatars/e403a19fc13e478d5929c67028230b0e.svg",
      "fullname": "Feng-Ting Liao",
      "name": "FengTing",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.13866",
      "authors": [
        {
          "_id": "682d2dee396c1e613e9fcbe5",
          "name": "Jiwon Song",
          "hidden": false
        },
        {
          "_id": "682d2dee396c1e613e9fcbe6",
          "name": "Dongwon Jo",
          "hidden": false
        },
        {
          "_id": "682d2dee396c1e613e9fcbe7",
          "name": "Yulhwa Kim",
          "hidden": false
        },
        {
          "_id": "682d2dee396c1e613e9fcbe8",
          "name": "Jae-Joon Kim",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T03:21:52.000Z",
      "submittedOnDailyAt": "2025-05-21T00:06:25.696Z",
      "title": "Reasoning Path Compression: Compressing Generation Trajectories for\n  Efficient LLM Reasoning",
      "submittedOnDailyBy": {
        "_id": "662672eaebdfec5cfdf1d034",
        "avatarUrl": "/avatars/61bc7add693c555e29ad3c1112215684.svg",
        "isPro": false,
        "fullname": "Jiwon Song",
        "user": "jiwonsong",
        "type": "user"
      },
      "summary": "Recent reasoning-focused language models achieve high accuracy by generating\nlengthy intermediate reasoning paths before producing final answers. While this\napproach is effective in solving problems that require logical thinking, long\nreasoning paths significantly increase memory usage and throughput of token\ngeneration, limiting the practical deployment of such models. We propose\nReasoning Path Compression (RPC), a training-free method that accelerates\ninference by leveraging the semantic sparsity of reasoning paths. RPC\nperiodically compresses the KV cache by retaining KV cache that receive high\nimportance score, which are computed using a selector window composed of\nrecently generated queries. Experiments show that RPC improves generation\nthroughput of QwQ-32B by up to 1.60times compared to the inference with full\nKV cache, with an accuracy drop of 1.2% on the AIME 2024 benchmark. Our\nfindings demonstrate that semantic sparsity in reasoning traces can be\neffectively exploited for compression, offering a practical path toward\nefficient deployment of reasoning LLMs. Our code is available at\nhttps://github.com/jiwonsong-dev/ReasoningPathCompression.",
      "upvotes": 9,
      "discussionId": "682d2def396c1e613e9fcc0b",
      "githubRepo": "https://github.com/jiwonsong-dev/ReasoningPathCompression",
      "ai_keywords": [
        "Reasoning Path Compression (RPC)",
        "KV cache",
        "semantic sparsity",
        "reasoning traces",
        "QwQ-32B",
        "AIME 2024 benchmark"
      ]
    },
    "publishedAt": "2025-05-19T23:21:52.000Z",
    "title": "Reasoning Path Compression: Compressing Generation Trajectories for\n  Efficient LLM Reasoning",
    "summary": "Recent reasoning-focused language models achieve high accuracy by generating\nlengthy intermediate reasoning paths before producing final answers. While this\napproach is effective in solving problems that require logical thinking, long\nreasoning paths significantly increase memory usage and throughput of token\ngeneration, limiting the practical deployment of such models. We propose\nReasoning Path Compression (RPC), a training-free method that accelerates\ninference by leveraging the semantic sparsity of reasoning paths. RPC\nperiodically compresses the KV cache by retaining KV cache that receive high\nimportance score, which are computed using a selector window composed of\nrecently generated queries. Experiments show that RPC improves generation\nthroughput of QwQ-32B by up to 1.60times compared to the inference with full\nKV cache, with an accuracy drop of 1.2% on the AIME 2024 benchmark. Our\nfindings demonstrate that semantic sparsity in reasoning traces can be\neffectively exploited for compression, offering a practical path toward\nefficient deployment of reasoning LLMs. Our code is available at\nhttps://github.com/jiwonsong-dev/ReasoningPathCompression.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13866.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "662672eaebdfec5cfdf1d034",
      "avatarUrl": "/avatars/61bc7add693c555e29ad3c1112215684.svg",
      "fullname": "Jiwon Song",
      "name": "jiwonsong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.13547",
      "authors": [
        {
          "_id": "682d6b06ec3b65b35772c0af",
          "name": "Pengxin Guo",
          "hidden": false
        },
        {
          "_id": "682d6b06ec3b65b35772c0b0",
          "name": "Yinong Wang",
          "hidden": false
        },
        {
          "_id": "682d6b06ec3b65b35772c0b1",
          "name": "Wei Li",
          "hidden": false
        },
        {
          "_id": "682d6b06ec3b65b35772c0b2",
          "name": "Mengting Liu",
          "hidden": false
        },
        {
          "_id": "682d6b06ec3b65b35772c0b3",
          "name": "Ming Li",
          "hidden": false
        },
        {
          "_id": "682d6b06ec3b65b35772c0b4",
          "name": "Jinkai Zheng",
          "hidden": false
        },
        {
          "_id": "682d6b06ec3b65b35772c0b5",
          "name": "Liangqiong Qu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-19T03:41:54.000Z",
      "submittedOnDailyAt": "2025-05-21T04:27:08.535Z",
      "title": "Exploring Federated Pruning for Large Language Models",
      "submittedOnDailyBy": {
        "_id": "668f440894dfc0ed1a7006ed",
        "avatarUrl": "/avatars/fa0d328300b03bcbbf9b3a7532f28458.svg",
        "isPro": false,
        "fullname": "Pengxin Guo",
        "user": "gpx333",
        "type": "user"
      },
      "summary": "LLM pruning has emerged as a promising technology for compressing LLMs,\nenabling their deployment on resource-limited devices. However, current\nmethodologies typically require access to public calibration samples, which can\nbe challenging to obtain in privacy-sensitive domains. To address this issue,\nwe introduce FedPrLLM, a comprehensive federated pruning framework designed for\nthe privacy-preserving compression of LLMs. In FedPrLLM, each client only needs\nto calculate a pruning mask matrix based on its local calibration data and\nshare it with the server to prune the global model. This approach allows for\ncollaborative pruning of the global model with the knowledge of each client\nwhile maintaining local data privacy. Additionally, we conduct extensive\nexperiments to explore various possibilities within the FedPrLLM framework,\nincluding different comparison groups, pruning strategies, and the decision to\nscale weights. Our extensive evaluation reveals that one-shot pruning with\nlayer comparison and no weight scaling is the optimal choice within the\nFedPrLLM framework. We hope our work will help guide future efforts in pruning\nLLMs in privacy-sensitive fields. Our code is available at\nhttps://github.com/Pengxin-Guo/FedPrLLM.",
      "upvotes": 9,
      "discussionId": "682d6b07ec3b65b35772c0f3",
      "githubRepo": "https://github.com/Pengxin-Guo/FedPrLLM",
      "ai_keywords": [
        "FedPrLLM",
        "federated pruning",
        "pruning mask matrix",
        "local calibration data",
        "global model",
        "collaborative pruning",
        "layer comparison",
        "weight scaling",
        "one-shot pruning"
      ]
    },
    "publishedAt": "2025-05-18T23:41:54.000Z",
    "title": "Exploring Federated Pruning for Large Language Models",
    "summary": "LLM pruning has emerged as a promising technology for compressing LLMs,\nenabling their deployment on resource-limited devices. However, current\nmethodologies typically require access to public calibration samples, which can\nbe challenging to obtain in privacy-sensitive domains. To address this issue,\nwe introduce FedPrLLM, a comprehensive federated pruning framework designed for\nthe privacy-preserving compression of LLMs. In FedPrLLM, each client only needs\nto calculate a pruning mask matrix based on its local calibration data and\nshare it with the server to prune the global model. This approach allows for\ncollaborative pruning of the global model with the knowledge of each client\nwhile maintaining local data privacy. Additionally, we conduct extensive\nexperiments to explore various possibilities within the FedPrLLM framework,\nincluding different comparison groups, pruning strategies, and the decision to\nscale weights. Our extensive evaluation reveals that one-shot pruning with\nlayer comparison and no weight scaling is the optimal choice within the\nFedPrLLM framework. We hope our work will help guide future efforts in pruning\nLLMs in privacy-sensitive fields. Our code is available at\nhttps://github.com/Pengxin-Guo/FedPrLLM.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13547.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "668f440894dfc0ed1a7006ed",
      "avatarUrl": "/avatars/fa0d328300b03bcbbf9b3a7532f28458.svg",
      "fullname": "Pengxin Guo",
      "name": "gpx333",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.14652",
      "authors": [
        {
          "_id": "682d474782567fffe1f99b7a",
          "name": "Xueguang Ma",
          "hidden": false
        },
        {
          "_id": "682d474782567fffe1f99b7b",
          "name": "Qian Liu",
          "hidden": false
        },
        {
          "_id": "682d474782567fffe1f99b7c",
          "name": "Dongfu Jiang",
          "hidden": false
        },
        {
          "_id": "682d474782567fffe1f99b7d",
          "name": "Ge Zhang",
          "hidden": false
        },
        {
          "_id": "682d474782567fffe1f99b7e",
          "name": "Zejun Ma",
          "hidden": false
        },
        {
          "_id": "682d474782567fffe1f99b7f",
          "user": {
            "_id": "6313a86154e6e5d9f0f94e04",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
            "isPro": false,
            "fullname": "Wenhu Chen",
            "user": "wenhu",
            "type": "user"
          },
          "name": "Wenhu Chen",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-21T03:23:52.529Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T17:41:33.000Z",
      "submittedOnDailyAt": "2025-05-21T01:56:42.807Z",
      "title": "General-Reasoner: Advancing LLM Reasoning Across All Domains",
      "submittedOnDailyBy": {
        "_id": "5ec82854968f6028e0559f70",
        "avatarUrl": "/avatars/45b58d912f7d00cb351947cd79d5eeb4.svg",
        "isPro": true,
        "fullname": "Xueguang Ma",
        "user": "MrLight",
        "type": "user"
      },
      "summary": "Reinforcement learning (RL) has recently demonstrated strong potential in\nenhancing the reasoning capabilities of large language models (LLMs).\nParticularly, the \"Zero\" reinforcement learning introduced by Deepseek-R1-Zero,\nenables direct RL training of base LLMs without relying on an intermediate\nsupervised fine-tuning stage. Despite these advancements, current works for LLM\nreasoning mainly focus on mathematical and coding domains, largely due to data\nabundance and the ease of answer verification. This limits the applicability\nand generalization of such models to broader domains, where questions often\nhave diverse answer representations, and data is more scarce. In this paper, we\npropose General-Reasoner, a novel training paradigm designed to enhance LLM\nreasoning capabilities across diverse domains. Our key contributions include:\n(1) constructing a large-scale, high-quality dataset of questions with\nverifiable answers curated by web crawling, covering a wide range of\ndisciplines; and (2) developing a generative model-based answer verifier, which\nreplaces traditional rule-based verification with the capability of\nchain-of-thought and context-awareness. We train a series of models and\nevaluate them on a wide range of datasets covering wide domains like physics,\nchemistry, finance, electronics etc. Our comprehensive evaluation across these\n12 benchmarks (e.g. MMLU-Pro, GPQA, SuperGPQA, TheoremQA, BBEH and MATH AMC)\ndemonstrates that General-Reasoner outperforms existing baseline methods,\nachieving robust and generalizable reasoning performance while maintaining\nsuperior effectiveness in mathematical reasoning tasks.",
      "upvotes": 8,
      "discussionId": "682d474882567fffe1f99bc5",
      "projectPage": "https://tiger-ai-lab.github.io/General-Reasoner/",
      "githubRepo": "https://github.com/TIGER-AI-Lab/General-Reasoner",
      "ai_keywords": [
        "reinforcement learning (RL)",
        "large language models (LLMs)",
        "Zero reinforcement learning",
        "base LLMs",
        "supervised fine-tuning",
        "generative model-based answer verifier",
        "chain-of-thought",
        "context-awareness"
      ]
    },
    "publishedAt": "2025-05-20T13:41:33.000Z",
    "title": "General-Reasoner: Advancing LLM Reasoning Across All Domains",
    "summary": "Reinforcement learning (RL) has recently demonstrated strong potential in\nenhancing the reasoning capabilities of large language models (LLMs).\nParticularly, the \"Zero\" reinforcement learning introduced by Deepseek-R1-Zero,\nenables direct RL training of base LLMs without relying on an intermediate\nsupervised fine-tuning stage. Despite these advancements, current works for LLM\nreasoning mainly focus on mathematical and coding domains, largely due to data\nabundance and the ease of answer verification. This limits the applicability\nand generalization of such models to broader domains, where questions often\nhave diverse answer representations, and data is more scarce. In this paper, we\npropose General-Reasoner, a novel training paradigm designed to enhance LLM\nreasoning capabilities across diverse domains. Our key contributions include:\n(1) constructing a large-scale, high-quality dataset of questions with\nverifiable answers curated by web crawling, covering a wide range of\ndisciplines; and (2) developing a generative model-based answer verifier, which\nreplaces traditional rule-based verification with the capability of\nchain-of-thought and context-awareness. We train a series of models and\nevaluate them on a wide range of datasets covering wide domains like physics,\nchemistry, finance, electronics etc. Our comprehensive evaluation across these\n12 benchmarks (e.g. MMLU-Pro, GPQA, SuperGPQA, TheoremQA, BBEH and MATH AMC)\ndemonstrates that General-Reasoner outperforms existing baseline methods,\nachieving robust and generalizable reasoning performance while maintaining\nsuperior effectiveness in mathematical reasoning tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14652.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5ec82854968f6028e0559f70",
      "avatarUrl": "/avatars/45b58d912f7d00cb351947cd79d5eeb4.svg",
      "fullname": "Xueguang Ma",
      "name": "MrLight",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 22
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.14489",
      "authors": [
        {
          "_id": "682d55ecea67e90811b09b6b",
          "name": "Dongkeun Yoon",
          "hidden": false
        },
        {
          "_id": "682d55ecea67e90811b09b6c",
          "name": "Seungone Kim",
          "hidden": false
        },
        {
          "_id": "682d55ecea67e90811b09b6d",
          "name": "Sohee Yang",
          "hidden": false
        },
        {
          "_id": "682d55ecea67e90811b09b6e",
          "name": "Sunkyoung Kim",
          "hidden": false
        },
        {
          "_id": "682d55ecea67e90811b09b6f",
          "name": "Soyeon Kim",
          "hidden": false
        },
        {
          "_id": "682d55ecea67e90811b09b70",
          "name": "Yongil Kim",
          "hidden": false
        },
        {
          "_id": "682d55ecea67e90811b09b71",
          "name": "Eunbi Choi",
          "hidden": false
        },
        {
          "_id": "682d55ecea67e90811b09b72",
          "user": {
            "_id": "660260cf1737e5cd4a826550",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/660260cf1737e5cd4a826550/AlSfoM2WtqPjLtYR6x7Wf.jpeg",
            "isPro": false,
            "fullname": "Yireun Kim",
            "user": "yireun",
            "type": "user"
          },
          "name": "Yireun Kim",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-21T04:27:50.114Z",
          "hidden": false
        },
        {
          "_id": "682d55ecea67e90811b09b73",
          "name": "Minjoon Seo",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/617f679fb15f8a665f3999fc/uZZgyWD0lw8jektQyDZfV.png",
        "https://cdn-uploads.huggingface.co/production/uploads/617f679fb15f8a665f3999fc/_Kq3BF1HC_aFc6mee08zl.png",
        "https://cdn-uploads.huggingface.co/production/uploads/617f679fb15f8a665f3999fc/bpz_J8XujNicCvMH2n2iK.png"
      ],
      "publishedAt": "2025-05-20T15:19:00.000Z",
      "submittedOnDailyAt": "2025-05-21T02:58:45.807Z",
      "title": "Reasoning Models Better Express Their Confidence",
      "submittedOnDailyBy": {
        "_id": "617f679fb15f8a665f3999fc",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/617f679fb15f8a665f3999fc/NW1vkLsGAlWpAQYTux05X.jpeg",
        "isPro": false,
        "fullname": "Dongkeun Yoon",
        "user": "DKYoon",
        "type": "user"
      },
      "summary": "Despite their strengths, large language models (LLMs) often fail to\ncommunicate their confidence accurately, making it difficult to assess when\nthey might be wrong and limiting their reliability. In this work, we\ndemonstrate that reasoning models-LLMs that engage in extended chain-of-thought\n(CoT) reasoning-exhibit superior performance not only in problem-solving but\nalso in accurately expressing their confidence. Specifically, we benchmark six\nreasoning models across six datasets and find that they achieve strictly better\nconfidence calibration than their non-reasoning counterparts in 33 out of the\n36 settings. Our detailed analysis reveals that these gains in calibration stem\nfrom the slow thinking behaviors of reasoning models-such as exploring\nalternative approaches and backtracking-which enable them to adjust their\nconfidence dynamically throughout their CoT, making it progressively more\naccurate. In particular, we find that reasoning models become increasingly\nbetter calibrated as their CoT unfolds, a trend not observed in non-reasoning\nmodels. Moreover, removing slow thinking behaviors from the CoT leads to a\nsignificant drop in calibration. Lastly, we show that these gains are not\nexclusive to reasoning models-non-reasoning models also benefit when guided to\nperform slow thinking via in-context learning.",
      "upvotes": 8,
      "discussionId": "682d55edea67e90811b09ba1",
      "githubRepo": "https://github.com/MattYoon/reasoning-models-confidence",
      "ai_keywords": [
        "large language models (LLMs)",
        "chain-of-thought (CoT) reasoning",
        "confidence calibration",
        "non-reasoning counterparts",
        "slow thinking behaviors",
        "backtracking",
        "in-context learning"
      ]
    },
    "publishedAt": "2025-05-20T11:19:00.000Z",
    "title": "Reasoning Models Better Express Their Confidence",
    "summary": "Despite their strengths, large language models (LLMs) often fail to\ncommunicate their confidence accurately, making it difficult to assess when\nthey might be wrong and limiting their reliability. In this work, we\ndemonstrate that reasoning models-LLMs that engage in extended chain-of-thought\n(CoT) reasoning-exhibit superior performance not only in problem-solving but\nalso in accurately expressing their confidence. Specifically, we benchmark six\nreasoning models across six datasets and find that they achieve strictly better\nconfidence calibration than their non-reasoning counterparts in 33 out of the\n36 settings. Our detailed analysis reveals that these gains in calibration stem\nfrom the slow thinking behaviors of reasoning models-such as exploring\nalternative approaches and backtracking-which enable them to adjust their\nconfidence dynamically throughout their CoT, making it progressively more\naccurate. In particular, we find that reasoning models become increasingly\nbetter calibrated as their CoT unfolds, a trend not observed in non-reasoning\nmodels. Moreover, removing slow thinking behaviors from the CoT leads to a\nsignificant drop in calibration. Lastly, we show that these gains are not\nexclusive to reasoning models-non-reasoning models also benefit when guided to\nperform slow thinking via in-context learning.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/617f679fb15f8a665f3999fc/uZZgyWD0lw8jektQyDZfV.png",
      "https://cdn-uploads.huggingface.co/production/uploads/617f679fb15f8a665f3999fc/_Kq3BF1HC_aFc6mee08zl.png",
      "https://cdn-uploads.huggingface.co/production/uploads/617f679fb15f8a665f3999fc/bpz_J8XujNicCvMH2n2iK.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14489.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "617f679fb15f8a665f3999fc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/617f679fb15f8a665f3999fc/NW1vkLsGAlWpAQYTux05X.jpeg",
      "fullname": "Dongkeun Yoon",
      "name": "DKYoon",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.13559",
      "authors": [
        {
          "_id": "682d4ccc3b5f51f4218e12b4",
          "name": "Sathya Krishnan Suresh",
          "hidden": false
        },
        {
          "_id": "682d4ccc3b5f51f4218e12b5",
          "name": "Tanmay Surana",
          "hidden": false
        },
        {
          "_id": "682d4ccc3b5f51f4218e12b6",
          "name": "Lim Zhi Hao",
          "hidden": false
        },
        {
          "_id": "682d4ccc3b5f51f4218e12b7",
          "name": "Eng Siong Chng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-19T09:18:14.000Z",
      "submittedOnDailyAt": "2025-05-21T02:18:05.886Z",
      "title": "CS-Sum: A Benchmark for Code-Switching Dialogue Summarization and the\n  Limits of Large Language Models",
      "submittedOnDailyBy": {
        "_id": "62f0e457bc8201db9ef47f89",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f0e457bc8201db9ef47f89/zOhDptwZpDGaugKCBOWB2.jpeg",
        "isPro": false,
        "fullname": "Sathya Krishnan",
        "user": "SkAndMl",
        "type": "user"
      },
      "summary": "Code-switching (CS) poses a significant challenge for Large Language Models\n(LLMs), yet its comprehensibility remains underexplored in LLMs. We introduce\nCS-Sum, to evaluate the comprehensibility of CS by the LLMs through CS dialogue\nto English summarization. CS-Sum is the first benchmark for CS dialogue\nsummarization across Mandarin-English (EN-ZH), Tamil-English (EN-TA), and\nMalay-English (EN-MS), with 900-1300 human-annotated dialogues per language\npair. Evaluating ten LLMs, including open and closed-source models, we analyze\nperformance across few-shot, translate-summarize, and fine-tuning (LoRA, QLoRA\non synthetic data) approaches. Our findings show that though the scores on\nautomated metrics are high, LLMs make subtle mistakes that alter the complete\nmeaning of the dialogue. To this end, we introduce 3 most common type of errors\nthat LLMs make when handling CS input. Error rates vary across CS pairs and\nLLMs, with some LLMs showing more frequent errors on certain language pairs,\nunderscoring the need for specialized training on code-switched data.",
      "upvotes": 8,
      "discussionId": "682d4ccd3b5f51f4218e12f4",
      "ai_keywords": [
        "code-switching (CS)",
        "Large Language Models (LLMs)",
        "CS-Sum",
        "CS dialogue to English summarization",
        "Mandarin-English (EN-ZH)",
        "Tamil-English (EN-TA)",
        "Malay-English (EN-MS)",
        "few-shot",
        "translate-summarize",
        "fine-tuning",
        "LoRA",
        "QLoRA",
        "synthetic data",
        "CS input"
      ]
    },
    "publishedAt": "2025-05-19T05:18:14.000Z",
    "title": "CS-Sum: A Benchmark for Code-Switching Dialogue Summarization and the\n  Limits of Large Language Models",
    "summary": "Code-switching (CS) poses a significant challenge for Large Language Models\n(LLMs), yet its comprehensibility remains underexplored in LLMs. We introduce\nCS-Sum, to evaluate the comprehensibility of CS by the LLMs through CS dialogue\nto English summarization. CS-Sum is the first benchmark for CS dialogue\nsummarization across Mandarin-English (EN-ZH), Tamil-English (EN-TA), and\nMalay-English (EN-MS), with 900-1300 human-annotated dialogues per language\npair. Evaluating ten LLMs, including open and closed-source models, we analyze\nperformance across few-shot, translate-summarize, and fine-tuning (LoRA, QLoRA\non synthetic data) approaches. Our findings show that though the scores on\nautomated metrics are high, LLMs make subtle mistakes that alter the complete\nmeaning of the dialogue. To this end, we introduce 3 most common type of errors\nthat LLMs make when handling CS input. Error rates vary across CS pairs and\nLLMs, with some LLMs showing more frequent errors on certain language pairs,\nunderscoring the need for specialized training on code-switched data.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13559.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "62f0e457bc8201db9ef47f89",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f0e457bc8201db9ef47f89/zOhDptwZpDGaugKCBOWB2.jpeg",
      "fullname": "Sathya Krishnan",
      "name": "SkAndMl",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.14680",
      "authors": [
        {
          "_id": "682d30a37812103582f50de4",
          "name": "Sunhao Dai",
          "hidden": false
        },
        {
          "_id": "682d30a37812103582f50de5",
          "name": "Wenjie Wang",
          "hidden": false
        },
        {
          "_id": "682d30a37812103582f50de6",
          "name": "Liang Pang",
          "hidden": false
        },
        {
          "_id": "682d30a37812103582f50de7",
          "name": "Jun Xu",
          "hidden": false
        },
        {
          "_id": "682d30a37812103582f50de8",
          "name": "See-Kiong Ng",
          "hidden": false
        },
        {
          "_id": "682d30a37812103582f50de9",
          "name": "Ji-Rong Wen",
          "hidden": false
        },
        {
          "_id": "682d30a37812103582f50dea",
          "name": "Tat-Seng Chua",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T17:59:13.000Z",
      "submittedOnDailyAt": "2025-05-21T00:18:37.037Z",
      "title": "NExT-Search: Rebuilding User Feedback Ecosystem for Generative AI Search",
      "submittedOnDailyBy": {
        "_id": "64db88993725f8d9a908c077",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64db88993725f8d9a908c077/JZEGk0kius6mrANlwOWw9.jpeg",
        "isPro": false,
        "fullname": "Sunhao Dai",
        "user": "KID-22",
        "type": "user"
      },
      "summary": "Generative AI search is reshaping information retrieval by offering\nend-to-end answers to complex queries, reducing users' reliance on manually\nbrowsing and summarizing multiple web pages. However, while this paradigm\nenhances convenience, it disrupts the feedback-driven improvement loop that has\nhistorically powered the evolution of traditional Web search. Web search can\ncontinuously improve their ranking models by collecting large-scale,\nfine-grained user feedback (e.g., clicks, dwell time) at the document level. In\ncontrast, generative AI search operates through a much longer search pipeline,\nspanning query decomposition, document retrieval, and answer generation, yet\ntypically receives only coarse-grained feedback on the final answer. This\nintroduces a feedback loop disconnect, where user feedback for the final output\ncannot be effectively mapped back to specific system components, making it\ndifficult to improve each intermediate stage and sustain the feedback loop. In\nthis paper, we envision NExT-Search, a next-generation paradigm designed to\nreintroduce fine-grained, process-level feedback into generative AI search.\nNExT-Search integrates two complementary modes: User Debug Mode, which allows\nengaged users to intervene at key stages; and Shadow User Mode, where a\npersonalized user agent simulates user preferences and provides AI-assisted\nfeedback for less interactive users. Furthermore, we envision how these\nfeedback signals can be leveraged through online adaptation, which refines\ncurrent search outputs in real-time, and offline update, which aggregates\ninteraction logs to periodically fine-tune query decomposition, retrieval, and\ngeneration models. By restoring human control over key stages of the generative\nAI search pipeline, we believe NExT-Search offers a promising direction for\nbuilding feedback-rich AI search systems that can evolve continuously alongside\nhuman feedback.",
      "upvotes": 7,
      "discussionId": "682d30a47812103582f50e19",
      "ai_keywords": [
        "generative AI search",
        "information retrieval",
        "end-to-end answers",
        "complex queries",
        "manually browsing",
        "summarizing",
        "traditional Web search",
        "ranking models",
        "fine-grained user feedback",
        "clicks",
        "dwell time",
        "document level",
        "query decomposition",
        "document retrieval",
        "answer generation",
        "coarse-grained feedback",
        "feedback loop disconnect",
        "system components",
        "NExT-Search",
        "User Debug Mode",
        "Shadow User Mode",
        "personalized user agent",
        "AI-assisted feedback",
        "online adaptation",
        "offline update",
        "real-time refinement",
        "interaction logs",
        "fine-tune query decomposition",
        "fine-tune retrieval",
        "fine-tune generation models",
        "feedback-rich AI search systems"
      ]
    },
    "publishedAt": "2025-05-20T13:59:13.000Z",
    "title": "NExT-Search: Rebuilding User Feedback Ecosystem for Generative AI Search",
    "summary": "Generative AI search is reshaping information retrieval by offering\nend-to-end answers to complex queries, reducing users' reliance on manually\nbrowsing and summarizing multiple web pages. However, while this paradigm\nenhances convenience, it disrupts the feedback-driven improvement loop that has\nhistorically powered the evolution of traditional Web search. Web search can\ncontinuously improve their ranking models by collecting large-scale,\nfine-grained user feedback (e.g., clicks, dwell time) at the document level. In\ncontrast, generative AI search operates through a much longer search pipeline,\nspanning query decomposition, document retrieval, and answer generation, yet\ntypically receives only coarse-grained feedback on the final answer. This\nintroduces a feedback loop disconnect, where user feedback for the final output\ncannot be effectively mapped back to specific system components, making it\ndifficult to improve each intermediate stage and sustain the feedback loop. In\nthis paper, we envision NExT-Search, a next-generation paradigm designed to\nreintroduce fine-grained, process-level feedback into generative AI search.\nNExT-Search integrates two complementary modes: User Debug Mode, which allows\nengaged users to intervene at key stages; and Shadow User Mode, where a\npersonalized user agent simulates user preferences and provides AI-assisted\nfeedback for less interactive users. Furthermore, we envision how these\nfeedback signals can be leveraged through online adaptation, which refines\ncurrent search outputs in real-time, and offline update, which aggregates\ninteraction logs to periodically fine-tune query decomposition, retrieval, and\ngeneration models. By restoring human control over key stages of the generative\nAI search pipeline, we believe NExT-Search offers a promising direction for\nbuilding feedback-rich AI search systems that can evolve continuously alongside\nhuman feedback.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14680.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64db88993725f8d9a908c077",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64db88993725f8d9a908c077/JZEGk0kius6mrANlwOWw9.jpeg",
      "fullname": "Sunhao Dai",
      "name": "KID-22",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.14673",
      "authors": [
        {
          "_id": "682d713e6c66e25ab59fc800",
          "name": "Yu Tong",
          "hidden": false
        },
        {
          "_id": "682d713e6c66e25ab59fc801",
          "name": "Zihao Pan",
          "hidden": false
        },
        {
          "_id": "682d713e6c66e25ab59fc802",
          "name": "Shuai Yang",
          "hidden": false
        },
        {
          "_id": "682d713e6c66e25ab59fc803",
          "name": "Kaiyang Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T17:58:02.000Z",
      "submittedOnDailyAt": "2025-05-21T04:53:16.661Z",
      "title": "Training-Free Watermarking for Autoregressive Image Generation",
      "submittedOnDailyBy": {
        "_id": "62ac6656de8bfbb93094b8fd",
        "avatarUrl": "/avatars/1d8f445b6d5f9d43ddab81056bcb141e.svg",
        "isPro": false,
        "fullname": "Kaiyang Zhou",
        "user": "kaiyangzhou",
        "type": "user"
      },
      "summary": "Invisible image watermarking can protect image ownership and prevent\nmalicious misuse of visual generative models. However, existing generative\nwatermarking methods are mainly designed for diffusion models while\nwatermarking for autoregressive image generation models remains largely\nunderexplored. We propose IndexMark, a training-free watermarking framework for\nautoregressive image generation models. IndexMark is inspired by the redundancy\nproperty of the codebook: replacing autoregressively generated indices with\nsimilar indices produces negligible visual differences. The core component in\nIndexMark is a simple yet effective match-then-replace method, which carefully\nselects watermark tokens from the codebook based on token similarity, and\npromotes the use of watermark tokens through token replacement, thereby\nembedding the watermark without affecting the image quality. Watermark\nverification is achieved by calculating the proportion of watermark tokens in\ngenerated images, with precision further improved by an Index Encoder.\nFurthermore, we introduce an auxiliary validation scheme to enhance robustness\nagainst cropping attacks. Experiments demonstrate that IndexMark achieves\nstate-of-the-art performance in terms of image quality and verification\naccuracy, and exhibits robustness against various perturbations, including\ncropping, noises, Gaussian blur, random erasing, color jittering, and JPEG\ncompression.",
      "upvotes": 7,
      "discussionId": "682d71426c66e25ab59fc946",
      "githubRepo": "https://github.com/maifoundations/IndexMark",
      "ai_keywords": [
        "autoregressive image generation models",
        "codebook",
        "indices",
        "token similarity",
        "watermark tokens",
        "token replacement",
        "IndexMark",
        "match-then-replace method",
        "Index Encoder",
        "auxiliary validation scheme",
        "cropping attacks",
        "Gaussian blur",
        "random erasing",
        "color jittering",
        "JPEG compression"
      ]
    },
    "publishedAt": "2025-05-20T13:58:02.000Z",
    "title": "Training-Free Watermarking for Autoregressive Image Generation",
    "summary": "Invisible image watermarking can protect image ownership and prevent\nmalicious misuse of visual generative models. However, existing generative\nwatermarking methods are mainly designed for diffusion models while\nwatermarking for autoregressive image generation models remains largely\nunderexplored. We propose IndexMark, a training-free watermarking framework for\nautoregressive image generation models. IndexMark is inspired by the redundancy\nproperty of the codebook: replacing autoregressively generated indices with\nsimilar indices produces negligible visual differences. The core component in\nIndexMark is a simple yet effective match-then-replace method, which carefully\nselects watermark tokens from the codebook based on token similarity, and\npromotes the use of watermark tokens through token replacement, thereby\nembedding the watermark without affecting the image quality. Watermark\nverification is achieved by calculating the proportion of watermark tokens in\ngenerated images, with precision further improved by an Index Encoder.\nFurthermore, we introduce an auxiliary validation scheme to enhance robustness\nagainst cropping attacks. Experiments demonstrate that IndexMark achieves\nstate-of-the-art performance in terms of image quality and verification\naccuracy, and exhibits robustness against various perturbations, including\ncropping, noises, Gaussian blur, random erasing, color jittering, and JPEG\ncompression.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14673.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62ac6656de8bfbb93094b8fd",
      "avatarUrl": "/avatars/1d8f445b6d5f9d43ddab81056bcb141e.svg",
      "fullname": "Kaiyang Zhou",
      "name": "kaiyangzhou",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.14677",
      "authors": [
        {
          "_id": "682d718306291bf11fcf69a3",
          "name": "Jiaer Xia",
          "hidden": false
        },
        {
          "_id": "682d718306291bf11fcf69a4",
          "name": "Yuhang Zang",
          "hidden": false
        },
        {
          "_id": "682d718306291bf11fcf69a5",
          "name": "Peng Gao",
          "hidden": false
        },
        {
          "_id": "682d718306291bf11fcf69a6",
          "name": "Yixuan Li",
          "hidden": false
        },
        {
          "_id": "682d718306291bf11fcf69a7",
          "name": "Kaiyang Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T17:58:35.000Z",
      "submittedOnDailyAt": "2025-05-21T04:54:13.466Z",
      "title": "Visionary-R1: Mitigating Shortcuts in Visual Reasoning with\n  Reinforcement Learning",
      "submittedOnDailyBy": {
        "_id": "62ac6656de8bfbb93094b8fd",
        "avatarUrl": "/avatars/1d8f445b6d5f9d43ddab81056bcb141e.svg",
        "isPro": false,
        "fullname": "Kaiyang Zhou",
        "user": "kaiyangzhou",
        "type": "user"
      },
      "summary": "Learning general-purpose reasoning capabilities has long been a challenging\nproblem in AI. Recent research in large language models (LLMs), such as\nDeepSeek-R1, has shown that reinforcement learning techniques like GRPO can\nenable pre-trained LLMs to develop reasoning capabilities using simple\nquestion-answer pairs. In this paper, we aim to train visual language models\n(VLMs) to perform reasoning on image data through reinforcement learning and\nvisual question-answer pairs, without any explicit chain-of-thought (CoT)\nsupervision. Our findings indicate that simply applying reinforcement learning\nto a VLM -- by prompting the model to produce a reasoning chain before\nproviding an answer -- can lead the model to develop shortcuts from easy\nquestions, thereby reducing its ability to generalize across unseen data\ndistributions. We argue that the key to mitigating shortcut learning is to\nencourage the model to interpret images prior to reasoning. Therefore, we train\nthe model to adhere to a caption-reason-answer output format: initially\ngenerating a detailed caption for an image, followed by constructing an\nextensive reasoning chain. When trained on 273K CoT-free visual question-answer\npairs and using only reinforcement learning, our model, named Visionary-R1,\noutperforms strong multimodal models, such as GPT-4o, Claude3.5-Sonnet, and\nGemini-1.5-Pro, on multiple visual reasoning benchmarks.",
      "upvotes": 6,
      "discussionId": "682d718406291bf11fcf69df",
      "githubRepo": "https://github.com/maifoundations/Visionary-R1",
      "ai_keywords": [
        "reinforcement learning",
        "deep learning",
        "large language models (LLMs)",
        "DeepSeek-R1",
        "GRPO",
        "visual language models (VLMs)",
        "visual question-answer pairs",
        "chain-of-thought (CoT)",
        "caption-reason-answer",
        "multimodal models",
        "GPT-4o",
        "Claude3.5-Sonnet",
        "Gemini-1.5-Pro",
        "visual reasoning benchmarks"
      ]
    },
    "publishedAt": "2025-05-20T13:58:35.000Z",
    "title": "Visionary-R1: Mitigating Shortcuts in Visual Reasoning with\n  Reinforcement Learning",
    "summary": "Learning general-purpose reasoning capabilities has long been a challenging\nproblem in AI. Recent research in large language models (LLMs), such as\nDeepSeek-R1, has shown that reinforcement learning techniques like GRPO can\nenable pre-trained LLMs to develop reasoning capabilities using simple\nquestion-answer pairs. In this paper, we aim to train visual language models\n(VLMs) to perform reasoning on image data through reinforcement learning and\nvisual question-answer pairs, without any explicit chain-of-thought (CoT)\nsupervision. Our findings indicate that simply applying reinforcement learning\nto a VLM -- by prompting the model to produce a reasoning chain before\nproviding an answer -- can lead the model to develop shortcuts from easy\nquestions, thereby reducing its ability to generalize across unseen data\ndistributions. We argue that the key to mitigating shortcut learning is to\nencourage the model to interpret images prior to reasoning. Therefore, we train\nthe model to adhere to a caption-reason-answer output format: initially\ngenerating a detailed caption for an image, followed by constructing an\nextensive reasoning chain. When trained on 273K CoT-free visual question-answer\npairs and using only reinforcement learning, our model, named Visionary-R1,\noutperforms strong multimodal models, such as GPT-4o, Claude3.5-Sonnet, and\nGemini-1.5-Pro, on multiple visual reasoning benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14677.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62ac6656de8bfbb93094b8fd",
      "avatarUrl": "/avatars/1d8f445b6d5f9d43ddab81056bcb141e.svg",
      "fullname": "Kaiyang Zhou",
      "name": "kaiyangzhou",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.14640",
      "authors": [
        {
          "_id": "682d488557686b8c44f257fa",
          "name": "Wentao Ma",
          "hidden": false
        },
        {
          "_id": "682d488557686b8c44f257fb",
          "name": "Weiming Ren",
          "hidden": false
        },
        {
          "_id": "682d488557686b8c44f257fc",
          "name": "Yiming Jia",
          "hidden": false
        },
        {
          "_id": "682d488557686b8c44f257fd",
          "name": "Zhuofeng Li",
          "hidden": false
        },
        {
          "_id": "682d488557686b8c44f257fe",
          "name": "Ping Nie",
          "hidden": false
        },
        {
          "_id": "682d488557686b8c44f257ff",
          "name": "Ge Zhang",
          "hidden": false
        },
        {
          "_id": "682d488557686b8c44f25800",
          "name": "Wenhu Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T17:26:32.000Z",
      "submittedOnDailyAt": "2025-05-21T02:03:04.298Z",
      "title": "VideoEval-Pro: Robust and Realistic Long Video Understanding Evaluation",
      "submittedOnDailyBy": {
        "_id": "64405a9d518271b0d1beea38",
        "avatarUrl": "/avatars/b702474588fd7090773320422417a582.svg",
        "isPro": false,
        "fullname": "Weiming Ren",
        "user": "wren93",
        "type": "user"
      },
      "summary": "Large multimodal models (LMMs) have recently emerged as a powerful tool for\nlong video understanding (LVU), prompting the development of standardized LVU\nbenchmarks to evaluate their performance. However, our investigation reveals a\nrather sober lesson for existing LVU benchmarks. First, most existing\nbenchmarks rely heavily on multiple-choice questions (MCQs), whose evaluation\nresults are inflated due to the possibility of guessing the correct answer;\nSecond, a significant portion of questions in these benchmarks have strong\npriors to allow models to answer directly without even reading the input video.\nFor example, Gemini-1.5-Pro can achieve over 50\\% accuracy given a random frame\nfrom a long video on Video-MME. We also observe that increasing the number of\nframes does not necessarily lead to improvement on existing benchmarks, which\nis counterintuitive. As a result, the validity and robustness of current LVU\nbenchmarks are undermined, impeding a faithful assessment of LMMs' long-video\nunderstanding capability. To tackle this problem, we propose VideoEval-Pro, a\nrealistic LVU benchmark containing questions with open-ended short-answer,\nwhich truly require understanding the entire video. VideoEval-Pro assesses both\nsegment-level and full-video understanding through perception and reasoning\ntasks. By evaluating 21 proprietary and open-source video LMMs, we conclude the\nfollowing findings: (1) video LMMs show drastic performance (>25\\%) drops on\nopen-ended questions compared with MCQs; (2) surprisingly, higher MCQ scores do\nnot lead to higher open-ended scores on VideoEval-Pro; (3) compared to other\nMCQ benchmarks, VideoEval-Pro benefits more from increasing the number of input\nframes. Our results show that VideoEval-Pro offers a more realistic and\nreliable measure of long video understanding, providing a clearer view of\nprogress in this domain.",
      "upvotes": 6,
      "discussionId": "682d488857686b8c44f258b7",
      "projectPage": "https://tiger-ai-lab.github.io/VideoEval-Pro",
      "githubRepo": "https://github.com/TIGER-AI-Lab/VideoEval-Pro",
      "ai_keywords": [
        "multimodal models",
        "long video understanding",
        "benchmarks",
        "multiple-choice questions",
        "Video-MME",
        "random frame",
        "VideoEval-Pro",
        "open-ended questions",
        "segment-level understanding",
        "full-video understanding",
        "perception tasks",
        "reasoning tasks",
        "performance drops"
      ]
    },
    "publishedAt": "2025-05-20T13:26:32.000Z",
    "title": "VideoEval-Pro: Robust and Realistic Long Video Understanding Evaluation",
    "summary": "Large multimodal models (LMMs) have recently emerged as a powerful tool for\nlong video understanding (LVU), prompting the development of standardized LVU\nbenchmarks to evaluate their performance. However, our investigation reveals a\nrather sober lesson for existing LVU benchmarks. First, most existing\nbenchmarks rely heavily on multiple-choice questions (MCQs), whose evaluation\nresults are inflated due to the possibility of guessing the correct answer;\nSecond, a significant portion of questions in these benchmarks have strong\npriors to allow models to answer directly without even reading the input video.\nFor example, Gemini-1.5-Pro can achieve over 50\\% accuracy given a random frame\nfrom a long video on Video-MME. We also observe that increasing the number of\nframes does not necessarily lead to improvement on existing benchmarks, which\nis counterintuitive. As a result, the validity and robustness of current LVU\nbenchmarks are undermined, impeding a faithful assessment of LMMs' long-video\nunderstanding capability. To tackle this problem, we propose VideoEval-Pro, a\nrealistic LVU benchmark containing questions with open-ended short-answer,\nwhich truly require understanding the entire video. VideoEval-Pro assesses both\nsegment-level and full-video understanding through perception and reasoning\ntasks. By evaluating 21 proprietary and open-source video LMMs, we conclude the\nfollowing findings: (1) video LMMs show drastic performance (>25\\%) drops on\nopen-ended questions compared with MCQs; (2) surprisingly, higher MCQ scores do\nnot lead to higher open-ended scores on VideoEval-Pro; (3) compared to other\nMCQ benchmarks, VideoEval-Pro benefits more from increasing the number of input\nframes. Our results show that VideoEval-Pro offers a more realistic and\nreliable measure of long video understanding, providing a clearer view of\nprogress in this domain.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14640.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64405a9d518271b0d1beea38",
      "avatarUrl": "/avatars/b702474588fd7090773320422417a582.svg",
      "fullname": "Weiming Ren",
      "name": "wren93",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.13430",
      "authors": [
        {
          "_id": "682d70c598e3ea9be315ed85",
          "name": "Sifeng Shang",
          "hidden": false
        },
        {
          "_id": "682d70c598e3ea9be315ed86",
          "name": "Jiayi Zhou",
          "hidden": false
        },
        {
          "_id": "682d70c598e3ea9be315ed87",
          "name": "Chenyu Lin",
          "hidden": false
        },
        {
          "_id": "682d70c598e3ea9be315ed88",
          "name": "Minxian Li",
          "hidden": false
        },
        {
          "_id": "682d70c598e3ea9be315ed89",
          "name": "Kaiyang Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-19T17:55:15.000Z",
      "submittedOnDailyAt": "2025-05-21T04:51:33.821Z",
      "title": "Fine-tuning Quantized Neural Networks with Zeroth-order Optimization",
      "submittedOnDailyBy": {
        "_id": "62ac6656de8bfbb93094b8fd",
        "avatarUrl": "/avatars/1d8f445b6d5f9d43ddab81056bcb141e.svg",
        "isPro": false,
        "fullname": "Kaiyang Zhou",
        "user": "kaiyangzhou",
        "type": "user"
      },
      "summary": "As the size of large language models grows exponentially, GPU memory has\nbecome a bottleneck for adapting these models to downstream tasks. In this\npaper, we aim to push the limits of memory-efficient training by minimizing\nmemory usage on model weights, gradients, and optimizer states, within a\nunified framework. Our idea is to eliminate both gradients and optimizer states\nusing zeroth-order optimization, which approximates gradients by perturbing\nweights during forward passes to identify gradient directions. To minimize\nmemory usage on weights, we employ model quantization, e.g., converting from\nbfloat16 to int4. However, directly applying zeroth-order optimization to\nquantized weights is infeasible due to the precision gap between discrete\nweights and continuous gradients, which would otherwise require de-quantization\nand re-quantization. To overcome this challenge, we propose Quantized\nZeroth-order Optimization (QZO), a novel approach that perturbs the continuous\nquantization scale for gradient estimation and uses a directional derivative\nclipping method to stabilize training. QZO is orthogonal to both scalar-based\nand codebook-based post-training quantization methods. Compared to\nfull-parameter fine-tuning in bfloat16, QZO can reduce the total memory cost by\nmore than 18times for 4-bit LLMs, and enables fine-tuning Llama-2-13B and\nStable Diffusion 3.5 Large within a single 24GB GPU.",
      "upvotes": 5,
      "discussionId": "682d70c898e3ea9be315ee64",
      "githubRepo": "https://github.com/maifoundations/QZO",
      "ai_keywords": [
        "zeroth-order optimization",
        "gradient estimation",
        "model quantization",
        "bfloat16",
        "int4",
        "quantization scale",
        "directional derivative clipping",
        "Quantized Zeroth-order Optimization (QZO)",
        "scalar-based post-training quantization",
        "codebook-based post-training quantization",
        "full-parameter fine-tuning",
        "Llama-2-13B",
        "Stable Diffusion 3.5 Large"
      ]
    },
    "publishedAt": "2025-05-19T13:55:15.000Z",
    "title": "Fine-tuning Quantized Neural Networks with Zeroth-order Optimization",
    "summary": "As the size of large language models grows exponentially, GPU memory has\nbecome a bottleneck for adapting these models to downstream tasks. In this\npaper, we aim to push the limits of memory-efficient training by minimizing\nmemory usage on model weights, gradients, and optimizer states, within a\nunified framework. Our idea is to eliminate both gradients and optimizer states\nusing zeroth-order optimization, which approximates gradients by perturbing\nweights during forward passes to identify gradient directions. To minimize\nmemory usage on weights, we employ model quantization, e.g., converting from\nbfloat16 to int4. However, directly applying zeroth-order optimization to\nquantized weights is infeasible due to the precision gap between discrete\nweights and continuous gradients, which would otherwise require de-quantization\nand re-quantization. To overcome this challenge, we propose Quantized\nZeroth-order Optimization (QZO), a novel approach that perturbs the continuous\nquantization scale for gradient estimation and uses a directional derivative\nclipping method to stabilize training. QZO is orthogonal to both scalar-based\nand codebook-based post-training quantization methods. Compared to\nfull-parameter fine-tuning in bfloat16, QZO can reduce the total memory cost by\nmore than 18times for 4-bit LLMs, and enables fine-tuning Llama-2-13B and\nStable Diffusion 3.5 Large within a single 24GB GPU.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13430.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62ac6656de8bfbb93094b8fd",
      "avatarUrl": "/avatars/1d8f445b6d5f9d43ddab81056bcb141e.svg",
      "fullname": "Kaiyang Zhou",
      "name": "kaiyangzhou",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.14631",
      "authors": [
        {
          "_id": "682d69ed056cf7b86cd8d6ec",
          "name": "Lingjie Jiang",
          "hidden": false
        },
        {
          "_id": "682d69ed056cf7b86cd8d6ed",
          "name": "Xun Wu",
          "hidden": false
        },
        {
          "_id": "682d69ed056cf7b86cd8d6ee",
          "name": "Shaohan Huang",
          "hidden": false
        },
        {
          "_id": "682d69ed056cf7b86cd8d6ef",
          "name": "Qingxiu Dong",
          "hidden": false
        },
        {
          "_id": "682d69ed056cf7b86cd8d6f0",
          "name": "Zewen Chi",
          "hidden": false
        },
        {
          "_id": "682d69ed056cf7b86cd8d6f1",
          "name": "Li Dong",
          "hidden": false
        },
        {
          "_id": "682d69ed056cf7b86cd8d6f2",
          "name": "Xingxing Zhang",
          "hidden": false
        },
        {
          "_id": "682d69ed056cf7b86cd8d6f3",
          "name": "Tengchao Lv",
          "hidden": false
        },
        {
          "_id": "682d69ed056cf7b86cd8d6f4",
          "name": "Lei Cui",
          "hidden": false
        },
        {
          "_id": "682d69ed056cf7b86cd8d6f5",
          "name": "Furu Wei",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T17:23:25.000Z",
      "submittedOnDailyAt": "2025-05-21T04:22:54.134Z",
      "title": "Think Only When You Need with Large Hybrid-Reasoning Models",
      "submittedOnDailyBy": {
        "_id": "62d1227384bfbee86b6eec56",
        "avatarUrl": "/avatars/84435f9768a76c0fe9d404dfc2d70be3.svg",
        "isPro": false,
        "fullname": "Xun Wu",
        "user": "YUSHUIWX",
        "type": "user"
      },
      "summary": "Recent Large Reasoning Models (LRMs) have shown substantially improved\nreasoning capabilities over traditional Large Language Models (LLMs) by\nincorporating extended thinking processes prior to producing final responses.\nHowever, excessively lengthy thinking introduces substantial overhead in terms\nof token consumption and latency, which is particularly unnecessary for simple\nqueries. In this work, we introduce Large Hybrid-Reasoning Models (LHRMs), the\nfirst kind of model capable of adaptively determining whether to perform\nthinking based on the contextual information of user queries. To achieve this,\nwe propose a two-stage training pipeline comprising Hybrid Fine-Tuning (HFT) as\na cold start, followed by online reinforcement learning with the proposed\nHybrid Group Policy Optimization (HGPO) to implicitly learn to select the\nappropriate thinking mode. Furthermore, we introduce a metric called Hybrid\nAccuracy to quantitatively assess the model's capability for hybrid thinking.\nExtensive experimental results show that LHRMs can adaptively perform hybrid\nthinking on queries of varying difficulty and type. It outperforms existing\nLRMs and LLMs in reasoning and general capabilities while significantly\nimproving efficiency. Together, our work advocates for a reconsideration of the\nappropriate use of extended thinking processes and provides a solid starting\npoint for building hybrid thinking systems.",
      "upvotes": 4,
      "discussionId": "682d69ee056cf7b86cd8d730",
      "ai_keywords": [
        "Large Reasoning Models (LRMs)",
        "Large Language Models (LLMs)",
        "Hybrid-Reasoning Models (LHRMs)",
        "Hybrid Fine-Tuning (HFT)",
        "Hybrid Group Policy Optimization (HGPO)",
        "Hybrid Accuracy"
      ]
    },
    "publishedAt": "2025-05-20T13:23:25.000Z",
    "title": "Think Only When You Need with Large Hybrid-Reasoning Models",
    "summary": "Recent Large Reasoning Models (LRMs) have shown substantially improved\nreasoning capabilities over traditional Large Language Models (LLMs) by\nincorporating extended thinking processes prior to producing final responses.\nHowever, excessively lengthy thinking introduces substantial overhead in terms\nof token consumption and latency, which is particularly unnecessary for simple\nqueries. In this work, we introduce Large Hybrid-Reasoning Models (LHRMs), the\nfirst kind of model capable of adaptively determining whether to perform\nthinking based on the contextual information of user queries. To achieve this,\nwe propose a two-stage training pipeline comprising Hybrid Fine-Tuning (HFT) as\na cold start, followed by online reinforcement learning with the proposed\nHybrid Group Policy Optimization (HGPO) to implicitly learn to select the\nappropriate thinking mode. Furthermore, we introduce a metric called Hybrid\nAccuracy to quantitatively assess the model's capability for hybrid thinking.\nExtensive experimental results show that LHRMs can adaptively perform hybrid\nthinking on queries of varying difficulty and type. It outperforms existing\nLRMs and LLMs in reasoning and general capabilities while significantly\nimproving efficiency. Together, our work advocates for a reconsideration of the\nappropriate use of extended thinking processes and provides a solid starting\npoint for building hybrid thinking systems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14631.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62d1227384bfbee86b6eec56",
      "avatarUrl": "/avatars/84435f9768a76c0fe9d404dfc2d70be3.svg",
      "fullname": "Xun Wu",
      "name": "YUSHUIWX",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.14464",
      "authors": [
        {
          "_id": "682d39e6fa24196dcd10d5e8",
          "name": "Xiaoyu Tian",
          "hidden": false
        },
        {
          "_id": "682d39e6fa24196dcd10d5e9",
          "name": "Yunjie Ji",
          "hidden": false
        },
        {
          "_id": "682d39e6fa24196dcd10d5ea",
          "name": "Haotian Wang",
          "hidden": false
        },
        {
          "_id": "682d39e6fa24196dcd10d5eb",
          "name": "Shuaiting Chen",
          "hidden": false
        },
        {
          "_id": "682d39e6fa24196dcd10d5ec",
          "name": "Sitong Zhao",
          "hidden": false
        },
        {
          "_id": "682d39e6fa24196dcd10d5ed",
          "name": "Yiping Peng",
          "hidden": false
        },
        {
          "_id": "682d39e6fa24196dcd10d5ee",
          "name": "Han Zhao",
          "hidden": false
        },
        {
          "_id": "682d39e6fa24196dcd10d5ef",
          "name": "Xiangang Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T15:00:51.000Z",
      "submittedOnDailyAt": "2025-05-21T01:40:28.265Z",
      "title": "Not All Correct Answers Are Equal: Why Your Distillation Source Matters",
      "submittedOnDailyBy": {
        "_id": "621499d72be42a56cca7afad",
        "avatarUrl": "/avatars/3cea70d1a55c8096b4270093c69e4a5e.svg",
        "isPro": false,
        "fullname": "TianXiaoyu",
        "user": "Emperorizzis",
        "type": "user"
      },
      "summary": "Distillation has emerged as a practical and effective approach to enhance the\nreasoning capabilities of open-source language models. In this work, we conduct\na large-scale empirical study on reasoning data distillation by collecting\nverified outputs from three state-of-the-art teacher models-AM-Thinking-v1,\nQwen3-235B-A22B, and DeepSeek-R1-on a shared corpus of 1.89 million queries. We\nconstruct three parallel datasets and analyze their distributions, revealing\nthat AM-Thinking-v1-distilled data exhibits greater token length diversity and\nlower perplexity. Student models trained on each dataset are evaluated on\nreasoning benchmarks including AIME2024, AIME2025, MATH500, and LiveCodeBench.\nThe AM-based model consistently achieves the best performance (e.g., 84.3 on\nAIME2024, 72.2 on AIME2025, 98.4 on MATH500, and 65.9 on LiveCodeBench) and\ndemonstrates adaptive output behavior-producing longer responses for harder\ntasks and shorter ones for simpler tasks. These findings highlight the value of\nhigh-quality, verified reasoning traces. We release the AM-Thinking-v1 and\nQwen3-235B-A22B distilled datasets to support future research on open and\nhigh-performing reasoning-oriented language models. The datasets are publicly\navailable on Hugging FaceDatasets are available on Hugging Face:\n\\href{https://huggingface.co/datasets/a-m-team/AM-Thinking-v1-Distilled{AM-Thinking-v1-Distilled},\nhttps://huggingface.co/datasets/a-m-team/AM-Qwen3-Distilled{AM-Qwen3-Distilled}.}.",
      "upvotes": 4,
      "discussionId": "682d39e8fa24196dcd10d643",
      "ai_keywords": [
        "distillation",
        "reasoning capabilities",
        "open-source language models",
        "empirical study",
        "reasoning data distillation",
        "AM-Thinking-v1",
        "Qwen3-235B-A22B",
        "DeepSeek-R1",
        "shared corpus",
        "parallel datasets",
        "token length diversity",
        "perplexity",
        "student models",
        "reasoning benchmarks",
        "AIME2024",
        "AIME2025",
        "MATH500",
        "LiveCodeBench",
        "adaptive output behavior"
      ]
    },
    "publishedAt": "2025-05-20T11:00:51.000Z",
    "title": "Not All Correct Answers Are Equal: Why Your Distillation Source Matters",
    "summary": "Distillation has emerged as a practical and effective approach to enhance the\nreasoning capabilities of open-source language models. In this work, we conduct\na large-scale empirical study on reasoning data distillation by collecting\nverified outputs from three state-of-the-art teacher models-AM-Thinking-v1,\nQwen3-235B-A22B, and DeepSeek-R1-on a shared corpus of 1.89 million queries. We\nconstruct three parallel datasets and analyze their distributions, revealing\nthat AM-Thinking-v1-distilled data exhibits greater token length diversity and\nlower perplexity. Student models trained on each dataset are evaluated on\nreasoning benchmarks including AIME2024, AIME2025, MATH500, and LiveCodeBench.\nThe AM-based model consistently achieves the best performance (e.g., 84.3 on\nAIME2024, 72.2 on AIME2025, 98.4 on MATH500, and 65.9 on LiveCodeBench) and\ndemonstrates adaptive output behavior-producing longer responses for harder\ntasks and shorter ones for simpler tasks. These findings highlight the value of\nhigh-quality, verified reasoning traces. We release the AM-Thinking-v1 and\nQwen3-235B-A22B distilled datasets to support future research on open and\nhigh-performing reasoning-oriented language models. The datasets are publicly\navailable on Hugging FaceDatasets are available on Hugging Face:\n\\href{https://huggingface.co/datasets/a-m-team/AM-Thinking-v1-Distilled{AM-Thinking-v1-Distilled},\nhttps://huggingface.co/datasets/a-m-team/AM-Qwen3-Distilled{AM-Qwen3-Distilled}.}.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14464.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "621499d72be42a56cca7afad",
      "avatarUrl": "/avatars/3cea70d1a55c8096b4270093c69e4a5e.svg",
      "fullname": "TianXiaoyu",
      "name": "Emperorizzis",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.12448",
      "authors": [
        {
          "_id": "682d47aa64daf8623f1f5604",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "682d47aa64daf8623f1f5605",
          "name": "Ming Ma",
          "hidden": false
        },
        {
          "_id": "682d47aa64daf8623f1f5606",
          "name": "Xiaomin Yu",
          "hidden": false
        },
        {
          "_id": "682d47aa64daf8623f1f5607",
          "name": "Pengxiang Ding",
          "hidden": false
        },
        {
          "_id": "682d47aa64daf8623f1f5608",
          "name": "Han Zhao",
          "hidden": false
        },
        {
          "_id": "682d47aa64daf8623f1f5609",
          "name": "Mingyang Sun",
          "hidden": false
        },
        {
          "_id": "682d47aa64daf8623f1f560a",
          "name": "Siteng Huang",
          "hidden": false
        },
        {
          "_id": "682d47aa64daf8623f1f560b",
          "name": "Donglin Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-18T14:40:16.000Z",
      "submittedOnDailyAt": "2025-05-21T01:56:36.099Z",
      "title": "SSR: Enhancing Depth Perception in Vision-Language Models via\n  Rationale-Guided Spatial Reasoning",
      "submittedOnDailyBy": {
        "_id": "65fd82762bf2cd20ddaa193f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yBYbWp_mT7UusYdkqtAvw.png",
        "isPro": false,
        "fullname": "Siteng Huang",
        "user": "huangsiteng",
        "type": "user"
      },
      "summary": "Despite impressive advancements in Visual-Language Models (VLMs) for\nmulti-modal tasks, their reliance on RGB inputs limits precise spatial\nunderstanding. Existing methods for integrating spatial cues, such as point\nclouds or depth, either require specialized sensors or fail to effectively\nexploit depth information for higher-order reasoning. To this end, we propose a\nnovel Spatial Sense and Reasoning method, dubbed SSR, a novel framework that\ntransforms raw depth data into structured, interpretable textual rationales.\nThese textual rationales serve as meaningful intermediate representations to\nsignificantly enhance spatial reasoning capabilities. Additionally, we leverage\nknowledge distillation to compress the generated rationales into compact latent\nembeddings, which facilitate resource-efficient and plug-and-play integration\ninto existing VLMs without retraining. To enable comprehensive evaluation, we\nintroduce a new dataset named SSR-CoT, a million-scale visual-language\nreasoning dataset enriched with intermediate spatial reasoning annotations, and\npresent SSRBench, a comprehensive multi-task benchmark. Extensive experiments\non multiple benchmarks demonstrate SSR substantially improves depth utilization\nand enhances spatial reasoning, thereby advancing VLMs toward more human-like\nmulti-modal understanding. Our project page is at\nhttps://yliu-cs.github.io/SSR.",
      "upvotes": 4,
      "discussionId": "682d47ab64daf8623f1f5634",
      "projectPage": "https://yliu-cs.github.io/SSR/",
      "githubRepo": "https://github.com/yliu-cs/SSR",
      "ai_keywords": [
        "Spatial Sense and Reasoning (SSR)",
        "structured, interpretable textual rationales",
        "knowledge distillation",
        "compact latent embeddings",
        "resource-efficient integration",
        "SSR-CoT",
        "million-scale visual-language reasoning dataset",
        "intermediate spatial reasoning annotations",
        "SSRBench",
        "comprehensive multi-task benchmark",
        "depth utilization",
        "spatial reasoning",
        "human-like multi-modal understanding"
      ]
    },
    "publishedAt": "2025-05-18T10:40:16.000Z",
    "title": "SSR: Enhancing Depth Perception in Vision-Language Models via\n  Rationale-Guided Spatial Reasoning",
    "summary": "Despite impressive advancements in Visual-Language Models (VLMs) for\nmulti-modal tasks, their reliance on RGB inputs limits precise spatial\nunderstanding. Existing methods for integrating spatial cues, such as point\nclouds or depth, either require specialized sensors or fail to effectively\nexploit depth information for higher-order reasoning. To this end, we propose a\nnovel Spatial Sense and Reasoning method, dubbed SSR, a novel framework that\ntransforms raw depth data into structured, interpretable textual rationales.\nThese textual rationales serve as meaningful intermediate representations to\nsignificantly enhance spatial reasoning capabilities. Additionally, we leverage\nknowledge distillation to compress the generated rationales into compact latent\nembeddings, which facilitate resource-efficient and plug-and-play integration\ninto existing VLMs without retraining. To enable comprehensive evaluation, we\nintroduce a new dataset named SSR-CoT, a million-scale visual-language\nreasoning dataset enriched with intermediate spatial reasoning annotations, and\npresent SSRBench, a comprehensive multi-task benchmark. Extensive experiments\non multiple benchmarks demonstrate SSR substantially improves depth utilization\nand enhances spatial reasoning, thereby advancing VLMs toward more human-like\nmulti-modal understanding. Our project page is at\nhttps://yliu-cs.github.io/SSR.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.12448.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65fd82762bf2cd20ddaa193f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yBYbWp_mT7UusYdkqtAvw.png",
      "fullname": "Siteng Huang",
      "name": "huangsiteng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.14135",
      "authors": [
        {
          "_id": "682d43cf85d5e40c81ed313d",
          "name": "Ruihuang Li",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed313e",
          "name": "Caijin Zhou",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed313f",
          "name": "Shoujian Zheng",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3140",
          "name": "Jianxiang Lu",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3141",
          "name": "Jiabin Huang",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3142",
          "name": "Comi Chen",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3143",
          "name": "Junshu Tang",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3144",
          "name": "Guangzheng Xu",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3145",
          "name": "Jiale Tao",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3146",
          "name": "Hongmei Wang",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3147",
          "name": "Donghao Li",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3148",
          "name": "Wenqing Yu",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3149",
          "name": "Senbo Wang",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed314a",
          "name": "Zhimin Li",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed314b",
          "name": "Yetshuan Shi",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed314c",
          "name": "Haoyu Yang",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed314d",
          "name": "Yukun Wang",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed314e",
          "name": "Wenxun Dai",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed314f",
          "name": "Jiaqi Li",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3150",
          "name": "Linqing Wang",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3151",
          "name": "Qixun Wang",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3152",
          "name": "Zhiyong Xu",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3153",
          "name": "Yingfang Zhang",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3154",
          "name": "Jiangfeng Xiong",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3155",
          "name": "Weijie Kong",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3156",
          "name": "Chao Zhang",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3157",
          "name": "Hongxin Zhang",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3158",
          "name": "Qiaoling Zheng",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3159",
          "name": "Weiting Guo",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed315a",
          "name": "Xinchi Deng",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed315b",
          "name": "Yixuan Li",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed315c",
          "name": "Renjia Wei",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed315d",
          "name": "Yulin Jian",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed315e",
          "name": "Duojun Huang",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed315f",
          "name": "Xuhua Ren",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3160",
          "name": "Sihuan Lin",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3161",
          "name": "Yifu Sun",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3162",
          "name": "Yuan Zhou",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3163",
          "name": "Joey Wang",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3164",
          "name": "Qin Lin",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3165",
          "name": "Jingmiao Yu",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3166",
          "name": "Jihong Zhang",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3167",
          "name": "Caesar Zhong",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3168",
          "name": "Di Wang",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed3169",
          "name": "Yuhong Liu",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed316a",
          "name": "Linus",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed316b",
          "name": "Jie Jiang",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed316c",
          "name": "Longhuang Wu",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed316d",
          "name": "Shuai Shao",
          "hidden": false
        },
        {
          "_id": "682d43cf85d5e40c81ed316e",
          "name": "Qinglin Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T09:39:48.000Z",
      "submittedOnDailyAt": "2025-05-21T01:40:24.172Z",
      "title": "Hunyuan-Game: Industrial-grade Intelligent Game Creation Model",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Intelligent game creation represents a transformative advancement in game\ndevelopment, utilizing generative artificial intelligence to dynamically\ngenerate and enhance game content. Despite notable progress in generative\nmodels, the comprehensive synthesis of high-quality game assets, including both\nimages and videos, remains a challenging frontier. To create high-fidelity game\ncontent that simultaneously aligns with player preferences and significantly\nboosts designer efficiency, we present Hunyuan-Game, an innovative project\ndesigned to revolutionize intelligent game production. Hunyuan-Game encompasses\ntwo primary branches: image generation and video generation. The image\ngeneration component is built upon a vast dataset comprising billions of game\nimages, leading to the development of a group of customized image generation\nmodels tailored for game scenarios: (1) General Text-to-Image Generation. (2)\nGame Visual Effects Generation, involving text-to-effect and reference\nimage-based game visual effect generation. (3) Transparent Image Generation for\ncharacters, scenes, and game visual effects. (4) Game Character Generation\nbased on sketches, black-and-white images, and white models. The video\ngeneration component is built upon a comprehensive dataset of millions of game\nand anime videos, leading to the development of five core algorithmic models,\neach targeting critical pain points in game development and having robust\nadaptation to diverse game video scenarios: (1) Image-to-Video Generation. (2)\n360 A/T Pose Avatar Video Synthesis. (3) Dynamic Illustration Generation. (4)\nGenerative Video Super-Resolution. (5) Interactive Game Video Generation. These\nimage and video generation models not only exhibit high-level aesthetic\nexpression but also deeply integrate domain-specific knowledge, establishing a\nsystematic understanding of diverse game and anime art styles.",
      "upvotes": 3,
      "discussionId": "682d43d585d5e40c81ed3302",
      "ai_keywords": [
        "text-to-image generation",
        "game visual effects generation",
        "text-to-effect",
        "reference image-based generation",
        "transparent image generation",
        "character generation",
        "sketch-based generation",
        "black-and-white image-based generation",
        "white model-based generation",
        "image-to-video generation",
        "360 A/T Pose Avatar Video Synthesis",
        "dynamic illustration generation",
        "generative video super-resolution",
        "interactive game video generation"
      ]
    },
    "publishedAt": "2025-05-20T05:39:48.000Z",
    "title": "Hunyuan-Game: Industrial-grade Intelligent Game Creation Model",
    "summary": "Intelligent game creation represents a transformative advancement in game\ndevelopment, utilizing generative artificial intelligence to dynamically\ngenerate and enhance game content. Despite notable progress in generative\nmodels, the comprehensive synthesis of high-quality game assets, including both\nimages and videos, remains a challenging frontier. To create high-fidelity game\ncontent that simultaneously aligns with player preferences and significantly\nboosts designer efficiency, we present Hunyuan-Game, an innovative project\ndesigned to revolutionize intelligent game production. Hunyuan-Game encompasses\ntwo primary branches: image generation and video generation. The image\ngeneration component is built upon a vast dataset comprising billions of game\nimages, leading to the development of a group of customized image generation\nmodels tailored for game scenarios: (1) General Text-to-Image Generation. (2)\nGame Visual Effects Generation, involving text-to-effect and reference\nimage-based game visual effect generation. (3) Transparent Image Generation for\ncharacters, scenes, and game visual effects. (4) Game Character Generation\nbased on sketches, black-and-white images, and white models. The video\ngeneration component is built upon a comprehensive dataset of millions of game\nand anime videos, leading to the development of five core algorithmic models,\neach targeting critical pain points in game development and having robust\nadaptation to diverse game video scenarios: (1) Image-to-Video Generation. (2)\n360 A/T Pose Avatar Video Synthesis. (3) Dynamic Illustration Generation. (4)\nGenerative Video Super-Resolution. (5) Interactive Game Video Generation. These\nimage and video generation models not only exhibit high-level aesthetic\nexpression but also deeply integrate domain-specific knowledge, establishing a\nsystematic understanding of diverse game and anime art styles.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14135.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6897
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.11365",
      "authors": [
        {
          "_id": "682d7ae082567fffe108b31a",
          "user": {
            "_id": "6596ca5cce76219628b8eab4",
            "avatarUrl": "/avatars/51cdea4e1e0e53260d403ceb7bc6de90.svg",
            "isPro": false,
            "fullname": "Pierre Le Jeune",
            "user": "pierlj",
            "type": "user"
          },
          "name": "Pierre Le Jeune",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-21T07:16:41.247Z",
          "hidden": false
        },
        {
          "_id": "682d7ae082567fffe108b31b",
          "user": {
            "_id": "65bccff4c1a44b6ef1c100da",
            "avatarUrl": "/avatars/bf91000c78b83167958dc44c582397f0.svg",
            "isPro": false,
            "fullname": "benoit",
            "user": "bmalezieux",
            "type": "user"
          },
          "name": "Benoît Malézieux",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-21T07:04:02.669Z",
          "hidden": false
        },
        {
          "_id": "682d7ae082567fffe108b31c",
          "user": {
            "_id": "649f00fc37bfb5202be464a9",
            "avatarUrl": "/avatars/89f6c6a92c076099f5450c3cd2057619.svg",
            "isPro": false,
            "fullname": "Inoki at Giskard",
            "user": "inoki-giskard",
            "type": "user"
          },
          "name": "Weixuan Xiao",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-21T07:04:02.669Z",
          "hidden": false
        },
        {
          "_id": "682d7ae082567fffe108b31d",
          "name": "Matteo Dora",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-16T15:31:08.000Z",
      "submittedOnDailyAt": "2025-05-21T05:35:45.784Z",
      "title": "Phare: A Safety Probe for Large Language Models",
      "submittedOnDailyBy": {
        "_id": "6596ca5cce76219628b8eab4",
        "avatarUrl": "/avatars/51cdea4e1e0e53260d403ceb7bc6de90.svg",
        "isPro": false,
        "fullname": "Pierre Le Jeune",
        "user": "pierlj",
        "type": "user"
      },
      "summary": "Ensuring the safety of large language models (LLMs) is critical for\nresponsible deployment, yet existing evaluations often prioritize performance\nover identifying failure modes. We introduce Phare, a multilingual diagnostic\nframework to probe and evaluate LLM behavior across three critical dimensions:\nhallucination and reliability, social biases, and harmful content generation.\nOur evaluation of 17 state-of-the-art LLMs reveals patterns of systematic\nvulnerabilities across all safety dimensions, including sycophancy, prompt\nsensitivity, and stereotype reproduction. By highlighting these specific\nfailure modes rather than simply ranking models, Phare provides researchers and\npractitioners with actionable insights to build more robust, aligned, and\ntrustworthy language systems.",
      "upvotes": 3,
      "discussionId": "682d7ae282567fffe108b40f",
      "projectPage": "https://phare.giskard.ai/",
      "githubRepo": "https://github.com/Giskard-AI/phare",
      "ai_keywords": [
        "hallucination",
        "reliability",
        "social biases",
        "harmful content generation",
        "multilingual diagnostic framework",
        "sycophancy",
        "prompt sensitivity",
        "stereotype reproduction"
      ]
    },
    "publishedAt": "2025-05-16T11:31:08.000Z",
    "title": "Phare: A Safety Probe for Large Language Models",
    "summary": "Ensuring the safety of large language models (LLMs) is critical for\nresponsible deployment, yet existing evaluations often prioritize performance\nover identifying failure modes. We introduce Phare, a multilingual diagnostic\nframework to probe and evaluate LLM behavior across three critical dimensions:\nhallucination and reliability, social biases, and harmful content generation.\nOur evaluation of 17 state-of-the-art LLMs reveals patterns of systematic\nvulnerabilities across all safety dimensions, including sycophancy, prompt\nsensitivity, and stereotype reproduction. By highlighting these specific\nfailure modes rather than simply ranking models, Phare provides researchers and\npractitioners with actionable insights to build more robust, aligned, and\ntrustworthy language systems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11365.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6596ca5cce76219628b8eab4",
      "avatarUrl": "/avatars/51cdea4e1e0e53260d403ceb7bc6de90.svg",
      "fullname": "Pierre Le Jeune",
      "name": "pierlj",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.14674",
      "authors": [
        {
          "_id": "682d4145514c96fbf03f5f76",
          "name": "Jiaxin Guo",
          "hidden": false
        },
        {
          "_id": "682d4145514c96fbf03f5f77",
          "name": "Zewen Chi",
          "hidden": false
        },
        {
          "_id": "682d4145514c96fbf03f5f78",
          "name": "Li Dong",
          "hidden": false
        },
        {
          "_id": "682d4145514c96fbf03f5f79",
          "name": "Qingxiu Dong",
          "hidden": false
        },
        {
          "_id": "682d4145514c96fbf03f5f7a",
          "name": "Xun Wu",
          "hidden": false
        },
        {
          "_id": "682d4145514c96fbf03f5f7b",
          "name": "Shaohan Huang",
          "hidden": false
        },
        {
          "_id": "682d4145514c96fbf03f5f7c",
          "name": "Furu Wei",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/5df85abada6d0311fd3d5408/wpApND0JXiL6s6MUpBdg7.png"
      ],
      "publishedAt": "2025-05-20T17:58:03.000Z",
      "submittedOnDailyAt": "2025-05-21T02:57:46.082Z",
      "title": "Reward Reasoning Model",
      "submittedOnDailyBy": {
        "_id": "5df85abada6d0311fd3d5408",
        "avatarUrl": "/avatars/2331cf703c1b5d3a62e2050b1a6eb108.svg",
        "isPro": false,
        "fullname": "Li Dong",
        "user": "unilm",
        "type": "user"
      },
      "summary": "Reward models play a critical role in guiding large language models toward\noutputs that align with human expectations. However, an open challenge remains\nin effectively utilizing test-time compute to enhance reward model performance.\nIn this work, we introduce Reward Reasoning Models (RRMs), which are\nspecifically designed to execute a deliberate reasoning process before\ngenerating final rewards. Through chain-of-thought reasoning, RRMs leverage\nadditional test-time compute for complex queries where appropriate rewards are\nnot immediately apparent. To develop RRMs, we implement a reinforcement\nlearning framework that fosters self-evolved reward reasoning capabilities\nwithout requiring explicit reasoning traces as training data. Experimental\nresults demonstrate that RRMs achieve superior performance on reward modeling\nbenchmarks across diverse domains. Notably, we show that RRMs can adaptively\nexploit test-time compute to further improve reward accuracy. The pretrained\nreward reasoning models are available at\nhttps://huggingface.co/Reward-Reasoning.",
      "upvotes": 2,
      "discussionId": "682d4146514c96fbf03f5fab",
      "ai_keywords": [
        "Reward models",
        "large language models",
        "chain-of-thought reasoning",
        "reinforcement learning",
        "self-evolved reward reasoning",
        "reward modeling benchmarks"
      ]
    },
    "publishedAt": "2025-05-20T13:58:03.000Z",
    "title": "Reward Reasoning Model",
    "summary": "Reward models play a critical role in guiding large language models toward\noutputs that align with human expectations. However, an open challenge remains\nin effectively utilizing test-time compute to enhance reward model performance.\nIn this work, we introduce Reward Reasoning Models (RRMs), which are\nspecifically designed to execute a deliberate reasoning process before\ngenerating final rewards. Through chain-of-thought reasoning, RRMs leverage\nadditional test-time compute for complex queries where appropriate rewards are\nnot immediately apparent. To develop RRMs, we implement a reinforcement\nlearning framework that fosters self-evolved reward reasoning capabilities\nwithout requiring explicit reasoning traces as training data. Experimental\nresults demonstrate that RRMs achieve superior performance on reward modeling\nbenchmarks across diverse domains. Notably, we show that RRMs can adaptively\nexploit test-time compute to further improve reward accuracy. The pretrained\nreward reasoning models are available at\nhttps://huggingface.co/Reward-Reasoning.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/5df85abada6d0311fd3d5408/wpApND0JXiL6s6MUpBdg7.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14674.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5df85abada6d0311fd3d5408",
      "avatarUrl": "/avatars/2331cf703c1b5d3a62e2050b1a6eb108.svg",
      "fullname": "Li Dong",
      "name": "unilm",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 29
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.09569",
      "authors": [
        {
          "_id": "682563f807f74666ec373332",
          "name": "Linbo Liu",
          "hidden": false
        },
        {
          "_id": "682563f807f74666ec373333",
          "user": {
            "_id": "636c32ae181c81c337f086b9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/636c32ae181c81c337f086b9/9zHTHmwzSeLMJWuwHCqAe.jpeg",
            "isPro": false,
            "fullname": "Xinle Sheila Liu",
            "user": "sliuxl",
            "type": "user"
          },
          "name": "Xinle Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-15T10:30:40.773Z",
          "hidden": false
        },
        {
          "_id": "682563f807f74666ec373334",
          "name": "Qiang Zhou",
          "hidden": false
        },
        {
          "_id": "682563f807f74666ec373335",
          "name": "Lin Chen",
          "hidden": false
        },
        {
          "_id": "682563f807f74666ec373336",
          "name": "Yihan Liu",
          "hidden": false
        },
        {
          "_id": "682563f807f74666ec373337",
          "name": "Hoan Nguyen",
          "hidden": false
        },
        {
          "_id": "682563f807f74666ec373338",
          "name": "Behrooz Omidvar-Tehrani",
          "hidden": false
        },
        {
          "_id": "682563f807f74666ec373339",
          "name": "Xi Shen",
          "hidden": false
        },
        {
          "_id": "682563f807f74666ec37333a",
          "name": "Jun Huan",
          "hidden": false
        },
        {
          "_id": "682563f807f74666ec37333b",
          "name": "Omer Tripp",
          "hidden": false
        },
        {
          "_id": "682563f807f74666ec37333c",
          "name": "Anoop Deoras",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-14T17:11:23.000Z",
      "submittedOnDailyAt": "2025-05-21T04:22:33.758Z",
      "title": "MIGRATION-BENCH: Repository-Level Code Migration Benchmark from Java 8",
      "submittedOnDailyBy": {
        "_id": "636c32ae181c81c337f086b9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/636c32ae181c81c337f086b9/9zHTHmwzSeLMJWuwHCqAe.jpeg",
        "isPro": false,
        "fullname": "Xinle Sheila Liu",
        "user": "sliuxl",
        "type": "user"
      },
      "summary": "With the rapid advancement of powerful large language models (LLMs) in recent\nyears, a wide range of software engineering tasks can now be addressed using\nLLMs, significantly enhancing productivity and scalability. Numerous benchmark\ndatasets have been developed to evaluate the coding capabilities of these\nmodels, while they primarily focus on problem-solving and issue-resolution\ntasks. In contrast, we introduce a new coding benchmark MIGRATION-BENCH with a\ndistinct focus: code migration. MIGRATION-BENCH aims to serve as a\ncomprehensive benchmark for migration from Java 8 to the latest long-term\nsupport (LTS) versions (Java 17, 21), MIGRATION-BENCH includes a full dataset\nand its subset selected with 5,102 and 300 repositories respectively.\nSelected is a representative subset curated for complexity and difficulty,\noffering a versatile resource to support research in the field of code\nmigration. Additionally, we provide a comprehensive evaluation framework to\nfacilitate rigorous and standardized assessment of LLMs on this challenging\ntask. We further propose SD-Feedback and demonstrate that LLMs can effectively\ntackle repository-level code migration to Java 17. For the selected subset with\nClaude-3.5-Sonnet-v2, SD-Feedback achieves 62.33% and 27.00% success rate\n(pass@1) for minimal and maximal migration respectively. The benchmark dataset\nand source code are available at:\nhttps://huggingface.co/collections/AmazonScience and\nhttps://github.com/amazon-science/self_debug respectively.",
      "upvotes": 2,
      "discussionId": "682563f907f74666ec373369",
      "projectPage": "https://github.com/amazon-science/SDFeedback",
      "githubRepo": "https://github.com/amazon-science/MigrationBench",
      "ai_keywords": [
        "large language models (LLMs)",
        "code migration",
        "MIGRATION-BENCH",
        "Java 8",
        "long-term support (LTS) versions (Java 17, 21)",
        "repositories",
        "SD-Feedback",
        "pass@1"
      ]
    },
    "publishedAt": "2025-05-14T13:11:23.000Z",
    "title": "MIGRATION-BENCH: Repository-Level Code Migration Benchmark from Java 8",
    "summary": "With the rapid advancement of powerful large language models (LLMs) in recent\nyears, a wide range of software engineering tasks can now be addressed using\nLLMs, significantly enhancing productivity and scalability. Numerous benchmark\ndatasets have been developed to evaluate the coding capabilities of these\nmodels, while they primarily focus on problem-solving and issue-resolution\ntasks. In contrast, we introduce a new coding benchmark MIGRATION-BENCH with a\ndistinct focus: code migration. MIGRATION-BENCH aims to serve as a\ncomprehensive benchmark for migration from Java 8 to the latest long-term\nsupport (LTS) versions (Java 17, 21), MIGRATION-BENCH includes a full dataset\nand its subset selected with 5,102 and 300 repositories respectively.\nSelected is a representative subset curated for complexity and difficulty,\noffering a versatile resource to support research in the field of code\nmigration. Additionally, we provide a comprehensive evaluation framework to\nfacilitate rigorous and standardized assessment of LLMs on this challenging\ntask. We further propose SD-Feedback and demonstrate that LLMs can effectively\ntackle repository-level code migration to Java 17. For the selected subset with\nClaude-3.5-Sonnet-v2, SD-Feedback achieves 62.33% and 27.00% success rate\n(pass@1) for minimal and maximal migration respectively. The benchmark dataset\nand source code are available at:\nhttps://huggingface.co/collections/AmazonScience and\nhttps://github.com/amazon-science/self_debug respectively.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.09569.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "636c32ae181c81c337f086b9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/636c32ae181c81c337f086b9/9zHTHmwzSeLMJWuwHCqAe.jpeg",
      "fullname": "Xinle Sheila Liu",
      "name": "sliuxl",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.14534",
      "authors": [
        {
          "_id": "682d7f009a06bc9f9106480b",
          "user": {
            "_id": "65ef0b4b325d9aaef87eb33b",
            "avatarUrl": "/avatars/ec27dba9de7e74c1f6cdcacf5aa25528.svg",
            "isPro": false,
            "fullname": "C Shi",
            "user": "chongyangs",
            "type": "user"
          },
          "name": "Chongyang Shi",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-21T07:21:37.326Z",
          "hidden": false
        },
        {
          "_id": "682d7f009a06bc9f9106480c",
          "name": "Sharon Lin",
          "hidden": false
        },
        {
          "_id": "682d7f009a06bc9f9106480d",
          "name": "Shuang Song",
          "hidden": false
        },
        {
          "_id": "682d7f009a06bc9f9106480e",
          "name": "Jamie Hayes",
          "hidden": false
        },
        {
          "_id": "682d7f009a06bc9f9106480f",
          "name": "Ilia Shumailov",
          "hidden": false
        },
        {
          "_id": "682d7f009a06bc9f91064810",
          "name": "Itay Yona",
          "hidden": false
        },
        {
          "_id": "682d7f009a06bc9f91064811",
          "name": "Juliette Pluto",
          "hidden": false
        },
        {
          "_id": "682d7f009a06bc9f91064812",
          "name": "Aneesh Pappu",
          "hidden": false
        },
        {
          "_id": "682d7f009a06bc9f91064813",
          "name": "Christopher A. Choquette-Choo",
          "hidden": false
        },
        {
          "_id": "682d7f009a06bc9f91064814",
          "name": "Milad Nasr",
          "hidden": false
        },
        {
          "_id": "682d7f009a06bc9f91064815",
          "name": "Chawin Sitawarin",
          "hidden": false
        },
        {
          "_id": "682d7f009a06bc9f91064816",
          "name": "Gena Gibson",
          "hidden": false
        },
        {
          "_id": "682d7f009a06bc9f91064817",
          "name": "Andreas Terzis",
          "hidden": false
        },
        {
          "_id": "682d7f009a06bc9f91064818",
          "name": "John \"Four\" Flynn",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T15:54:45.000Z",
      "submittedOnDailyAt": "2025-05-21T05:52:22.988Z",
      "title": "Lessons from Defending Gemini Against Indirect Prompt Injections",
      "submittedOnDailyBy": {
        "_id": "6475c2794766357252e69e9f",
        "avatarUrl": "/avatars/db428715dfd2239df2aeaaff1282323f.svg",
        "isPro": false,
        "fullname": "i",
        "user": "iliashum",
        "type": "user"
      },
      "summary": "Gemini is increasingly used to perform tasks on behalf of users, where\nfunction-calling and tool-use capabilities enable the model to access user\ndata. Some tools, however, require access to untrusted data introducing risk.\nAdversaries can embed malicious instructions in untrusted data which cause the\nmodel to deviate from the user's expectations and mishandle their data or\npermissions. In this report, we set out Google DeepMind's approach to\nevaluating the adversarial robustness of Gemini models and describe the main\nlessons learned from the process. We test how Gemini performs against a\nsophisticated adversary through an adversarial evaluation framework, which\ndeploys a suite of adaptive attack techniques to run continuously against past,\ncurrent, and future versions of Gemini. We describe how these ongoing\nevaluations directly help make Gemini more resilient against manipulation.",
      "upvotes": 1,
      "discussionId": "682d7f019a06bc9f9106486e",
      "ai_keywords": [
        "function-calling",
        "tool-use capabilities",
        "adversarial robustness",
        "adaptive attack techniques",
        "resilience"
      ]
    },
    "publishedAt": "2025-05-20T11:54:45.000Z",
    "title": "Lessons from Defending Gemini Against Indirect Prompt Injections",
    "summary": "Gemini is increasingly used to perform tasks on behalf of users, where\nfunction-calling and tool-use capabilities enable the model to access user\ndata. Some tools, however, require access to untrusted data introducing risk.\nAdversaries can embed malicious instructions in untrusted data which cause the\nmodel to deviate from the user's expectations and mishandle their data or\npermissions. In this report, we set out Google DeepMind's approach to\nevaluating the adversarial robustness of Gemini models and describe the main\nlessons learned from the process. We test how Gemini performs against a\nsophisticated adversary through an adversarial evaluation framework, which\ndeploys a suite of adaptive attack techniques to run continuously against past,\ncurrent, and future versions of Gemini. We describe how these ongoing\nevaluations directly help make Gemini more resilient against manipulation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14534.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6475c2794766357252e69e9f",
      "avatarUrl": "/avatars/db428715dfd2239df2aeaaff1282323f.svg",
      "fullname": "i",
      "name": "iliashum",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.14352",
      "authors": [
        {
          "_id": "682d72b0d57ba1e4d132148d",
          "name": "Bartosz Cywiński",
          "hidden": false
        },
        {
          "_id": "682d72b0d57ba1e4d132148e",
          "name": "Emil Ryd",
          "hidden": false
        },
        {
          "_id": "682d72b0d57ba1e4d132148f",
          "name": "Senthooran Rajamanoharan",
          "hidden": false
        },
        {
          "_id": "682d72b0d57ba1e4d1321490",
          "name": "Neel Nanda",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T13:36:37.000Z",
      "submittedOnDailyAt": "2025-05-21T04:59:37.379Z",
      "title": "Towards eliciting latent knowledge from LLMs with mechanistic\n  interpretability",
      "submittedOnDailyBy": {
        "_id": "6422f416a73327caad9d1d86",
        "avatarUrl": "/avatars/aa3639277cd1732504402fc64a57eff8.svg",
        "isPro": false,
        "fullname": "Bartosz Cywiński",
        "user": "bcywinski",
        "type": "user"
      },
      "summary": "As language models become more powerful and sophisticated, it is crucial that\nthey remain trustworthy and reliable. There is concerning preliminary evidence\nthat models may attempt to deceive or keep secrets from their operators. To\nexplore the ability of current techniques to elicit such hidden knowledge, we\ntrain a Taboo model: a language model that describes a specific secret word\nwithout explicitly stating it. Importantly, the secret word is not presented to\nthe model in its training data or prompt. We then investigate methods to\nuncover this secret. First, we evaluate non-interpretability (black-box)\napproaches. Subsequently, we develop largely automated strategies based on\nmechanistic interpretability techniques, including logit lens and sparse\nautoencoders. Evaluation shows that both approaches are effective in eliciting\nthe secret word in our proof-of-concept setting. Our findings highlight the\npromise of these approaches for eliciting hidden knowledge and suggest several\npromising avenues for future work, including testing and refining these methods\non more complex model organisms. This work aims to be a step towards addressing\nthe crucial problem of eliciting secret knowledge from language models, thereby\ncontributing to their safe and reliable deployment.",
      "upvotes": 1,
      "discussionId": "682d72b1d57ba1e4d13214c6",
      "githubRepo": "https://github.com/EmilRyd/eliciting-secrets",
      "ai_keywords": [
        "Taboo model",
        "logit lens",
        "sparse autoencoders"
      ]
    },
    "publishedAt": "2025-05-20T09:36:37.000Z",
    "title": "Towards eliciting latent knowledge from LLMs with mechanistic\n  interpretability",
    "summary": "As language models become more powerful and sophisticated, it is crucial that\nthey remain trustworthy and reliable. There is concerning preliminary evidence\nthat models may attempt to deceive or keep secrets from their operators. To\nexplore the ability of current techniques to elicit such hidden knowledge, we\ntrain a Taboo model: a language model that describes a specific secret word\nwithout explicitly stating it. Importantly, the secret word is not presented to\nthe model in its training data or prompt. We then investigate methods to\nuncover this secret. First, we evaluate non-interpretability (black-box)\napproaches. Subsequently, we develop largely automated strategies based on\nmechanistic interpretability techniques, including logit lens and sparse\nautoencoders. Evaluation shows that both approaches are effective in eliciting\nthe secret word in our proof-of-concept setting. Our findings highlight the\npromise of these approaches for eliciting hidden knowledge and suggest several\npromising avenues for future work, including testing and refining these methods\non more complex model organisms. This work aims to be a step towards addressing\nthe crucial problem of eliciting secret knowledge from language models, thereby\ncontributing to their safe and reliable deployment.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14352.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6422f416a73327caad9d1d86",
      "avatarUrl": "/avatars/aa3639277cd1732504402fc64a57eff8.svg",
      "fullname": "Bartosz Cywiński",
      "name": "bcywinski",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.13718",
      "authors": [
        {
          "_id": "682d6ccc26146f27d1ab3d83",
          "name": "Safal Shrestha",
          "hidden": false
        },
        {
          "_id": "682d6ccc26146f27d1ab3d84",
          "name": "Minwu Kim",
          "hidden": false
        },
        {
          "_id": "682d6ccc26146f27d1ab3d85",
          "name": "Aadim Nepal",
          "hidden": false
        },
        {
          "_id": "682d6ccc26146f27d1ab3d86",
          "name": "Anubhav Shrestha",
          "hidden": false
        },
        {
          "_id": "682d6ccc26146f27d1ab3d87",
          "name": "Keith Ross",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-19T20:29:15.000Z",
      "submittedOnDailyAt": "2025-05-21T04:35:24.229Z",
      "title": "Warm Up Before You Train: Unlocking General Reasoning in\n  Resource-Constrained Settings",
      "submittedOnDailyBy": {
        "_id": "64cb922ec7f30fbf7b91a9a7",
        "avatarUrl": "/avatars/457eae5e56b9641ee5543146447d1755.svg",
        "isPro": false,
        "fullname": "Safal Shrestha",
        "user": "safal312",
        "type": "user"
      },
      "summary": "Designing effective reasoning-capable LLMs typically requires training using\nReinforcement Learning with Verifiable Rewards (RLVR) or distillation with\ncarefully curated Long Chain of Thoughts (CoT), both of which depend heavily on\nextensive training data. This creates a major challenge when the amount of\nquality training data is scarce. We propose a sample-efficient, two-stage\ntraining strategy to develop reasoning LLMs under limited supervision. In the\nfirst stage, we \"warm up\" the model by distilling Long CoTs from a toy domain,\nnamely, Knights \\& Knaves (K\\&K) logic puzzles to acquire general reasoning\nskills. In the second stage, we apply RLVR to the warmed-up model using a\nlimited set of target-domain examples. Our experiments demonstrate that this\ntwo-phase approach offers several benefits: (i) the warmup phase alone\nfacilitates generalized reasoning, leading to performance improvements across a\nrange of tasks, including MATH, HumanEval^{+}, and MMLU-Pro. (ii) When both\nthe base model and the warmed-up model are RLVR trained on the same small\ndataset (leq100 examples), the warmed-up model consistently outperforms the\nbase model; (iii) Warming up before RLVR training allows a model to maintain\ncross-domain generalizability even after training on a specific domain; (iv)\nIntroducing warmup in the pipeline improves not only accuracy but also overall\nsample efficiency during RLVR training. The results in this paper highlight the\npromise of warmup for building robust reasoning LLMs in data-scarce\nenvironments.",
      "upvotes": 1,
      "discussionId": "682d6ccd26146f27d1ab3dbb",
      "githubRepo": "https://github.com/safal312/warmup-before-you-train",
      "ai_keywords": [
        "Reinforcement Learning with Verifiable Rewards (RLVR)",
        "Long Chain of Thoughts (CoT)",
        "Knights \\& Knaves (K\\&K) logic puzzles",
        "MATH",
        "HumanEval$^{+}$",
        "MMLU-Pro",
        "warmup phase",
        "generalized reasoning",
        "cross-domain generalizability",
        "sample efficiency",
        "robust reasoning LLMs"
      ]
    },
    "publishedAt": "2025-05-19T16:29:15.000Z",
    "title": "Warm Up Before You Train: Unlocking General Reasoning in\n  Resource-Constrained Settings",
    "summary": "Designing effective reasoning-capable LLMs typically requires training using\nReinforcement Learning with Verifiable Rewards (RLVR) or distillation with\ncarefully curated Long Chain of Thoughts (CoT), both of which depend heavily on\nextensive training data. This creates a major challenge when the amount of\nquality training data is scarce. We propose a sample-efficient, two-stage\ntraining strategy to develop reasoning LLMs under limited supervision. In the\nfirst stage, we \"warm up\" the model by distilling Long CoTs from a toy domain,\nnamely, Knights \\& Knaves (K\\&K) logic puzzles to acquire general reasoning\nskills. In the second stage, we apply RLVR to the warmed-up model using a\nlimited set of target-domain examples. Our experiments demonstrate that this\ntwo-phase approach offers several benefits: (i) the warmup phase alone\nfacilitates generalized reasoning, leading to performance improvements across a\nrange of tasks, including MATH, HumanEval^{+}, and MMLU-Pro. (ii) When both\nthe base model and the warmed-up model are RLVR trained on the same small\ndataset (leq100 examples), the warmed-up model consistently outperforms the\nbase model; (iii) Warming up before RLVR training allows a model to maintain\ncross-domain generalizability even after training on a specific domain; (iv)\nIntroducing warmup in the pipeline improves not only accuracy but also overall\nsample efficiency during RLVR training. The results in this paper highlight the\npromise of warmup for building robust reasoning LLMs in data-scarce\nenvironments.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13718.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64cb922ec7f30fbf7b91a9a7",
      "avatarUrl": "/avatars/457eae5e56b9641ee5543146447d1755.svg",
      "fullname": "Safal Shrestha",
      "name": "safal312",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.13380",
      "authors": [
        {
          "_id": "682d3787265177367e119f04",
          "name": "Nam V. Nguyen",
          "hidden": false
        },
        {
          "_id": "682d3787265177367e119f05",
          "name": "Huy Nguyen",
          "hidden": false
        },
        {
          "_id": "682d3787265177367e119f06",
          "name": "Quang Pham",
          "hidden": false
        },
        {
          "_id": "682d3787265177367e119f07",
          "name": "Van Nguyen",
          "hidden": false
        },
        {
          "_id": "682d3787265177367e119f08",
          "name": "Savitha Ramasamy",
          "hidden": false
        },
        {
          "_id": "682d3787265177367e119f09",
          "name": "Nhat Ho",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-19T17:24:26.000Z",
      "submittedOnDailyAt": "2025-05-21T00:48:58.161Z",
      "title": "CompeteSMoE -- Statistically Guaranteed Mixture of Experts Training via\n  Competition",
      "submittedOnDailyBy": {
        "_id": "64c2bea2ada7df214276913b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c2bea2ada7df214276913b/QFCtmCn439Afsr7uqyoMT.jpeg",
        "isPro": false,
        "fullname": "Nguyen Van Nam",
        "user": "DavidNguyen",
        "type": "user"
      },
      "summary": "Sparse mixture of experts (SMoE) offers an appealing solution to scale up the\nmodel complexity beyond the mean of increasing the network's depth or width.\nHowever, we argue that effective SMoE training remains challenging because of\nthe suboptimal routing process where experts that perform computation do not\ndirectly contribute to the routing process. In this work, we propose\ncompetition, a novel mechanism to route tokens to experts with the highest\nneural response. Theoretically, we show that the competition mechanism enjoys a\nbetter sample efficiency than the traditional softmax routing. Furthermore, we\ndevelop CompeteSMoE, a simple yet effective algorithm to train large language\nmodels by deploying a router to learn the competition policy, thus enjoying\nstrong performances at a low training overhead. Our extensive empirical\nevaluations on both the visual instruction tuning and language pre-training\ntasks demonstrate the efficacy, robustness, and scalability of CompeteSMoE\ncompared to state-of-the-art SMoE strategies. We have made the implementation\navailable at: https://github.com/Fsoft-AIC/CompeteSMoE. This work is an\nimproved version of the previous study at arXiv:2402.02526",
      "upvotes": 1,
      "discussionId": "682d3788265177367e119f71",
      "githubRepo": "https://github.com/Fsoft-AIC/CompeteSMoE",
      "ai_keywords": [
        "Sparse mixture of experts (SMoE)",
        "competition mechanism",
        "sample efficiency",
        "softmax routing",
        "CompeteSMoE",
        "neural response",
        "router",
        "competition policy",
        "visual instruction tuning",
        "language pre-training"
      ]
    },
    "publishedAt": "2025-05-19T13:24:26.000Z",
    "title": "CompeteSMoE -- Statistically Guaranteed Mixture of Experts Training via\n  Competition",
    "summary": "Sparse mixture of experts (SMoE) offers an appealing solution to scale up the\nmodel complexity beyond the mean of increasing the network's depth or width.\nHowever, we argue that effective SMoE training remains challenging because of\nthe suboptimal routing process where experts that perform computation do not\ndirectly contribute to the routing process. In this work, we propose\ncompetition, a novel mechanism to route tokens to experts with the highest\nneural response. Theoretically, we show that the competition mechanism enjoys a\nbetter sample efficiency than the traditional softmax routing. Furthermore, we\ndevelop CompeteSMoE, a simple yet effective algorithm to train large language\nmodels by deploying a router to learn the competition policy, thus enjoying\nstrong performances at a low training overhead. Our extensive empirical\nevaluations on both the visual instruction tuning and language pre-training\ntasks demonstrate the efficacy, robustness, and scalability of CompeteSMoE\ncompared to state-of-the-art SMoE strategies. We have made the implementation\navailable at: https://github.com/Fsoft-AIC/CompeteSMoE. This work is an\nimproved version of the previous study at arXiv:2402.02526",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13380.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c2bea2ada7df214276913b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c2bea2ada7df214276913b/QFCtmCn439Afsr7uqyoMT.jpeg",
      "fullname": "Nguyen Van Nam",
      "name": "DavidNguyen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.13103",
      "authors": [
        {
          "_id": "682d7f7a176087390484a412",
          "name": "Han Zheng",
          "hidden": false
        },
        {
          "_id": "682d7f7a176087390484a413",
          "name": "Ilia Shumailov",
          "hidden": false
        },
        {
          "_id": "682d7f7a176087390484a414",
          "name": "Tianqi Fan",
          "hidden": false
        },
        {
          "_id": "682d7f7a176087390484a415",
          "name": "Aiden Hall",
          "hidden": false
        },
        {
          "_id": "682d7f7a176087390484a416",
          "name": "Mathias Payer",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-19T13:32:51.000Z",
      "submittedOnDailyAt": "2025-05-21T05:55:51.238Z",
      "title": "Fixing 7,400 Bugs for 1$: Cheap Crash-Site Program Repair",
      "submittedOnDailyBy": {
        "_id": "6475c2794766357252e69e9f",
        "avatarUrl": "/avatars/db428715dfd2239df2aeaaff1282323f.svg",
        "isPro": false,
        "fullname": "i",
        "user": "iliashum",
        "type": "user"
      },
      "summary": "The rapid advancement of bug-finding techniques has led to the discovery of\nmore vulnerabilities than developers can reasonably fix, creating an urgent\nneed for effective Automated Program Repair (APR) methods. However, the\ncomplexity of modern bugs often makes precise root cause analysis difficult and\nunreliable. To address this challenge, we propose crash-site repair to simplify\nthe repair task while still mitigating the risk of exploitation. In addition,\nwe introduce a template-guided patch generation approach that significantly\nreduces the token cost of Large Language Models (LLMs) while maintaining both\nefficiency and effectiveness.\n  We implement our prototype system, WILLIAMT, and evaluate it against\nstate-of-the-art APR tools. Our results show that, when combined with the\ntop-performing agent CodeRover-S, WILLIAMT reduces token cost by 45.9% and\nincreases the bug-fixing rate to 73.5% (+29.6%) on ARVO, a ground-truth open\nsource software vulnerabilities benchmark. Furthermore, we demonstrate that\nWILLIAMT can function effectively even without access to frontier LLMs: even a\nlocal model running on a Mac M4 Mini achieves a reasonable repair rate. These\nfindings highlight the broad applicability and scalability of WILLIAMT.",
      "upvotes": 1,
      "discussionId": "682d7f7a176087390484a448",
      "ai_keywords": [
        "crash-site repair",
        "template-guided patch generation",
        "Large Language Models (LLMs)",
        "WILLIAMT",
        "CodeRover-S",
        "ARVO",
        "ground-truth open source software vulnerabilities benchmark"
      ]
    },
    "publishedAt": "2025-05-19T09:32:51.000Z",
    "title": "Fixing 7,400 Bugs for 1$: Cheap Crash-Site Program Repair",
    "summary": "The rapid advancement of bug-finding techniques has led to the discovery of\nmore vulnerabilities than developers can reasonably fix, creating an urgent\nneed for effective Automated Program Repair (APR) methods. However, the\ncomplexity of modern bugs often makes precise root cause analysis difficult and\nunreliable. To address this challenge, we propose crash-site repair to simplify\nthe repair task while still mitigating the risk of exploitation. In addition,\nwe introduce a template-guided patch generation approach that significantly\nreduces the token cost of Large Language Models (LLMs) while maintaining both\nefficiency and effectiveness.\n  We implement our prototype system, WILLIAMT, and evaluate it against\nstate-of-the-art APR tools. Our results show that, when combined with the\ntop-performing agent CodeRover-S, WILLIAMT reduces token cost by 45.9% and\nincreases the bug-fixing rate to 73.5% (+29.6%) on ARVO, a ground-truth open\nsource software vulnerabilities benchmark. Furthermore, we demonstrate that\nWILLIAMT can function effectively even without access to frontier LLMs: even a\nlocal model running on a Mac M4 Mini achieves a reasonable repair rate. These\nfindings highlight the broad applicability and scalability of WILLIAMT.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13103.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6475c2794766357252e69e9f",
      "avatarUrl": "/avatars/db428715dfd2239df2aeaaff1282323f.svg",
      "fullname": "i",
      "name": "iliashum",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.12182",
      "authors": [
        {
          "_id": "682d31dd17608739046e1169",
          "name": "Haohang Li",
          "hidden": false
        },
        {
          "_id": "682d31dd17608739046e116a",
          "name": "Yupeng Cao",
          "hidden": false
        },
        {
          "_id": "682d31dd17608739046e116b",
          "name": "Yangyang Yu",
          "hidden": false
        },
        {
          "_id": "682d31dd17608739046e116c",
          "name": "Jordan W. Suchow",
          "hidden": false
        },
        {
          "_id": "682d31dd17608739046e116d",
          "name": "Zining Zhu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-18T00:47:21.000Z",
      "submittedOnDailyAt": "2025-05-21T00:23:07.653Z",
      "title": "Truth Neurons",
      "submittedOnDailyBy": {
        "_id": "634cabd104491d9f7111eea3",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634cabd104491d9f7111eea3/JoqlugwfD1aGkd-wZTmP7.jpeg",
        "isPro": false,
        "fullname": "Haohang Li",
        "user": "Acatsama",
        "type": "user"
      },
      "summary": "Despite their remarkable success and deployment across diverse workflows,\nlanguage models sometimes produce untruthful responses. Our limited\nunderstanding of how truthfulness is mechanistically encoded within these\nmodels jeopardizes their reliability and safety. In this paper, we propose a\nmethod for identifying representations of truthfulness at the neuron level. We\nshow that language models contain truth neurons, which encode truthfulness in a\nsubject-agnostic manner. Experiments conducted across models of varying scales\nvalidate the existence of truth neurons, confirming that the encoding of\ntruthfulness at the neuron level is a property shared by many language models.\nThe distribution patterns of truth neurons over layers align with prior\nfindings on the geometry of truthfulness. Selectively suppressing the\nactivations of truth neurons found through the TruthfulQA dataset degrades\nperformance both on TruthfulQA and on other benchmarks, showing that the\ntruthfulness mechanisms are not tied to a specific dataset. Our results offer\nnovel insights into the mechanisms underlying truthfulness in language models\nand highlight potential directions toward improving their trustworthiness and\nreliability.",
      "upvotes": 1,
      "discussionId": "682d31de17608739046e11c9",
      "ai_keywords": [
        "truth neurons",
        "neuron level",
        "truthfulness mechanisms",
        "TruthfulQA dataset"
      ]
    },
    "publishedAt": "2025-05-17T20:47:21.000Z",
    "title": "Truth Neurons",
    "summary": "Despite their remarkable success and deployment across diverse workflows,\nlanguage models sometimes produce untruthful responses. Our limited\nunderstanding of how truthfulness is mechanistically encoded within these\nmodels jeopardizes their reliability and safety. In this paper, we propose a\nmethod for identifying representations of truthfulness at the neuron level. We\nshow that language models contain truth neurons, which encode truthfulness in a\nsubject-agnostic manner. Experiments conducted across models of varying scales\nvalidate the existence of truth neurons, confirming that the encoding of\ntruthfulness at the neuron level is a property shared by many language models.\nThe distribution patterns of truth neurons over layers align with prior\nfindings on the geometry of truthfulness. Selectively suppressing the\nactivations of truth neurons found through the TruthfulQA dataset degrades\nperformance both on TruthfulQA and on other benchmarks, showing that the\ntruthfulness mechanisms are not tied to a specific dataset. Our results offer\nnovel insights into the mechanisms underlying truthfulness in language models\nand highlight potential directions toward improving their trustworthiness and\nreliability.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.12182.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "634cabd104491d9f7111eea3",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634cabd104491d9f7111eea3/JoqlugwfD1aGkd-wZTmP7.jpeg",
      "fullname": "Haohang Li",
      "name": "Acatsama",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.11966",
      "authors": [
        {
          "_id": "682d42808560f4baf596643b",
          "name": "Jianyuan Zhong",
          "hidden": false
        },
        {
          "_id": "682d42808560f4baf596643c",
          "name": "Zeju Li",
          "hidden": false
        },
        {
          "_id": "682d42808560f4baf596643d",
          "name": "Zhijian Xu",
          "hidden": false
        },
        {
          "_id": "682d42808560f4baf596643e",
          "name": "Xiangyu Wen",
          "hidden": false
        },
        {
          "_id": "682d42808560f4baf596643f",
          "name": "Kezhi Li",
          "hidden": false
        },
        {
          "_id": "682d42808560f4baf5966440",
          "name": "Qiang Xu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-17T11:41:44.000Z",
      "submittedOnDailyAt": "2025-05-21T01:36:54.831Z",
      "title": "Solve-Detect-Verify: Inference-Time Scaling with Flexible Generative\n  Verifier",
      "submittedOnDailyBy": {
        "_id": "6608fa4f5baec84322ec85ea",
        "avatarUrl": "/avatars/13bdaff931676b065fa1efef06fef922.svg",
        "isPro": false,
        "fullname": "Zhong",
        "user": "Jianyuan1",
        "type": "user"
      },
      "summary": "Large Language Model (LLM) reasoning for complex tasks inherently involves a\ntrade-off between solution accuracy and computational efficiency. The\nsubsequent step of verification, while intended to improve performance, further\ncomplicates this landscape by introducing its own challenging trade-off:\nsophisticated Generative Reward Models (GenRMs) can be computationally\nprohibitive if naively integrated with LLMs at test-time, while simpler, faster\nmethods may lack reliability. To overcome these challenges, we introduce\nFlexiVe, a novel generative verifier that flexibly balances computational\nresources between rapid, reliable fast thinking and meticulous slow thinking\nusing a Flexible Allocation of Verification Budget strategy. We further propose\nthe Solve-Detect-Verify pipeline, an efficient inference-time scaling framework\nthat intelligently integrates FlexiVe, proactively identifying solution\ncompletion points to trigger targeted verification and provide focused solver\nfeedback. Experiments show FlexiVe achieves superior accuracy in pinpointing\nerrors within reasoning traces on ProcessBench. Furthermore, on challenging\nmathematical reasoning benchmarks (AIME 2024, AIME 2025, and CNMO), our full\napproach outperforms baselines like self-consistency in reasoning accuracy and\ninference efficiency. Our system offers a scalable and effective solution to\nenhance LLM reasoning at test time.",
      "upvotes": 1,
      "discussionId": "682d42808560f4baf5966480",
      "ai_keywords": [
        "Generative Reward Models (GenRMs)",
        "Flexible Allocation of Verification Budget",
        "Solve-Detect-Verify pipeline",
        "reasoning traces",
        "ProcessBench",
        "AIME 2024",
        "AIME 2025",
        "CNMO",
        "self-consistency"
      ]
    },
    "publishedAt": "2025-05-17T07:41:44.000Z",
    "title": "Solve-Detect-Verify: Inference-Time Scaling with Flexible Generative\n  Verifier",
    "summary": "Large Language Model (LLM) reasoning for complex tasks inherently involves a\ntrade-off between solution accuracy and computational efficiency. The\nsubsequent step of verification, while intended to improve performance, further\ncomplicates this landscape by introducing its own challenging trade-off:\nsophisticated Generative Reward Models (GenRMs) can be computationally\nprohibitive if naively integrated with LLMs at test-time, while simpler, faster\nmethods may lack reliability. To overcome these challenges, we introduce\nFlexiVe, a novel generative verifier that flexibly balances computational\nresources between rapid, reliable fast thinking and meticulous slow thinking\nusing a Flexible Allocation of Verification Budget strategy. We further propose\nthe Solve-Detect-Verify pipeline, an efficient inference-time scaling framework\nthat intelligently integrates FlexiVe, proactively identifying solution\ncompletion points to trigger targeted verification and provide focused solver\nfeedback. Experiments show FlexiVe achieves superior accuracy in pinpointing\nerrors within reasoning traces on ProcessBench. Furthermore, on challenging\nmathematical reasoning benchmarks (AIME 2024, AIME 2025, and CNMO), our full\napproach outperforms baselines like self-consistency in reasoning accuracy and\ninference efficiency. Our system offers a scalable and effective solution to\nenhance LLM reasoning at test time.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11966.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6608fa4f5baec84322ec85ea",
      "avatarUrl": "/avatars/13bdaff931676b065fa1efef06fef922.svg",
      "fullname": "Zhong",
      "name": "Jianyuan1",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.04388",
      "authors": [
        {
          "_id": "682d83494c4685831f85ec92",
          "name": "Dario Garcia-Gasulla",
          "hidden": false
        },
        {
          "_id": "682d83494c4685831f85ec93",
          "user": {
            "_id": "661e8559b0338c03bd6e5054",
            "avatarUrl": "/avatars/044d69afa40ddef4485aebfe984da96b.svg",
            "isPro": false,
            "fullname": "Bayarri",
            "user": "JordiBayarri-bsc",
            "type": "user"
          },
          "name": "Jordi Bayarri-Planas",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-21T07:39:54.713Z",
          "hidden": false
        },
        {
          "_id": "682d83494c4685831f85ec94",
          "name": "Ashwin Kumar Gururajan",
          "hidden": false
        },
        {
          "_id": "682d83494c4685831f85ec95",
          "name": "Enrique Lopez-Cuena",
          "hidden": false
        },
        {
          "_id": "682d83494c4685831f85ec96",
          "user": {
            "_id": "6622352d9fcf61dee8a1f24d",
            "avatarUrl": "/avatars/dd164c50b3ecb8861e2294337b942e6f.svg",
            "isPro": false,
            "fullname": "Adrian Tormos",
            "user": "adriantormos",
            "type": "user"
          },
          "name": "Adrian Tormos",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-21T07:39:54.713Z",
          "hidden": false
        },
        {
          "_id": "682d83494c4685831f85ec97",
          "name": "Daniel Hinjos",
          "hidden": false
        },
        {
          "_id": "682d83494c4685831f85ec98",
          "user": {
            "_id": "620683e7eeb1b73d904c96e5",
            "avatarUrl": "/avatars/d0309ac9408530a74f1799e175cc5fad.svg",
            "isPro": false,
            "fullname": "Pablo Bernabeu",
            "user": "pabberpe",
            "type": "user"
          },
          "name": "Pablo Bernabeu-Perez",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-21T07:39:54.713Z",
          "hidden": false
        },
        {
          "_id": "682d83494c4685831f85ec99",
          "user": {
            "_id": "65d71603ca16ef9ba7fb2efb",
            "avatarUrl": "/avatars/7d3e1436427f7f58c86fb1f8724c4244.svg",
            "isPro": false,
            "fullname": "Anna",
            "user": "annariasdu",
            "type": "user"
          },
          "name": "Anna Arias-Duart",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-21T07:39:54.713Z",
          "hidden": false
        },
        {
          "_id": "682d83494c4685831f85ec9a",
          "user": {
            "_id": "64aecc15d2373ae50cf3a034",
            "avatarUrl": "/avatars/43b677378ca31b948f6acb8ce699dd47.svg",
            "isPro": false,
            "fullname": "Pablo Agustin Martin Torres",
            "user": "PabloAgustin",
            "type": "user"
          },
          "name": "Pablo Agustin Martin-Torres",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-21T07:39:54.713Z",
          "hidden": false
        },
        {
          "_id": "682d83494c4685831f85ec9b",
          "name": "Marta Gonzalez-Mallo",
          "hidden": false
        },
        {
          "_id": "682d83494c4685831f85ec9c",
          "user": {
            "_id": "62d16c742ad06bbc89217797",
            "avatarUrl": "/avatars/11ce629fbcb33f2431164d8a3e54c876.svg",
            "isPro": false,
            "fullname": "Sergio Alvarez-Napagao",
            "user": "tranchis",
            "type": "user"
          },
          "name": "Sergio Alvarez-Napagao",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-21T07:39:54.713Z",
          "hidden": false
        },
        {
          "_id": "682d83494c4685831f85ec9d",
          "name": "Eduard Ayguadé-Parra",
          "hidden": false
        },
        {
          "_id": "682d83494c4685831f85ec9e",
          "name": "Ulises Cortés",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62f7a16192950415b637e201/SdO6Jth8V1sz0wvfL9Nxg.png",
        "https://cdn-uploads.huggingface.co/production/uploads/62f7a16192950415b637e201/Efb0dFT2ULJC-TvdQ1ERR.png"
      ],
      "publishedAt": "2025-05-07T13:13:14.000Z",
      "submittedOnDailyAt": "2025-05-21T06:12:19.909Z",
      "title": "The Aloe Family Recipe for Open and Specialized Healthcare LLMs",
      "submittedOnDailyBy": {
        "_id": "62f7a16192950415b637e201",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f7a16192950415b637e201/4IIqYap43vujvSuql68Vj.jpeg",
        "isPro": false,
        "fullname": "Dario",
        "user": "dariog",
        "type": "user"
      },
      "summary": "Purpose: With advancements in Large Language Models (LLMs) for healthcare,\nthe need arises for competitive open-source models to protect the public\ninterest. This work contributes to the field of open medical LLMs by optimizing\nkey stages of data preprocessing and training, while showing how to improve\nmodel safety (through DPO) and efficacy (through RAG). The evaluation\nmethodology used, which includes four different types of tests, defines a new\nstandard for the field. The resultant models, shown to be competitive with the\nbest private alternatives, are released with a permisive license.\n  Methods: Building on top of strong base models like Llama 3.1 and Qwen 2.5,\nAloe Beta uses a custom dataset to enhance public data with synthetic Chain of\nThought examples. The models undergo alignment with Direct Preference\nOptimization, emphasizing ethical and policy-aligned performance in the\npresence of jailbreaking attacks. Evaluation includes close-ended, open-ended,\nsafety and human assessments, to maximize the reliability of results.\n  Results: Recommendations are made across the entire pipeline, backed by the\nsolid performance of the Aloe Family. These models deliver competitive\nperformance across healthcare benchmarks and medical fields, and are often\npreferred by healthcare professionals. On bias and toxicity, the Aloe Beta\nmodels significantly improve safety, showing resilience to unseen jailbreaking\nattacks. For a responsible release, a detailed risk assessment specific to\nhealthcare is attached to the Aloe Family models.\n  Conclusion: The Aloe Beta models, and the recipe that leads to them, are a\nsignificant contribution to the open-source medical LLM field, offering\ntop-of-the-line performance while maintaining high ethical requirements. This\nwork sets a new standard for developing and reporting aligned LLMs in\nhealthcare.",
      "upvotes": 1,
      "discussionId": "682d834a4c4685831f85ed09"
    },
    "publishedAt": "2025-05-07T09:13:14.000Z",
    "title": "The Aloe Family Recipe for Open and Specialized Healthcare LLMs",
    "summary": "Purpose: With advancements in Large Language Models (LLMs) for healthcare,\nthe need arises for competitive open-source models to protect the public\ninterest. This work contributes to the field of open medical LLMs by optimizing\nkey stages of data preprocessing and training, while showing how to improve\nmodel safety (through DPO) and efficacy (through RAG). The evaluation\nmethodology used, which includes four different types of tests, defines a new\nstandard for the field. The resultant models, shown to be competitive with the\nbest private alternatives, are released with a permisive license.\n  Methods: Building on top of strong base models like Llama 3.1 and Qwen 2.5,\nAloe Beta uses a custom dataset to enhance public data with synthetic Chain of\nThought examples. The models undergo alignment with Direct Preference\nOptimization, emphasizing ethical and policy-aligned performance in the\npresence of jailbreaking attacks. Evaluation includes close-ended, open-ended,\nsafety and human assessments, to maximize the reliability of results.\n  Results: Recommendations are made across the entire pipeline, backed by the\nsolid performance of the Aloe Family. These models deliver competitive\nperformance across healthcare benchmarks and medical fields, and are often\npreferred by healthcare professionals. On bias and toxicity, the Aloe Beta\nmodels significantly improve safety, showing resilience to unseen jailbreaking\nattacks. For a responsible release, a detailed risk assessment specific to\nhealthcare is attached to the Aloe Family models.\n  Conclusion: The Aloe Beta models, and the recipe that leads to them, are a\nsignificant contribution to the open-source medical LLM field, offering\ntop-of-the-line performance while maintaining high ethical requirements. This\nwork sets a new standard for developing and reporting aligned LLMs in\nhealthcare.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62f7a16192950415b637e201/SdO6Jth8V1sz0wvfL9Nxg.png",
      "https://cdn-uploads.huggingface.co/production/uploads/62f7a16192950415b637e201/Efb0dFT2ULJC-TvdQ1ERR.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04388.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62f7a16192950415b637e201",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f7a16192950415b637e201/4IIqYap43vujvSuql68Vj.jpeg",
      "fullname": "Dario",
      "name": "dariog",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.14178",
      "authors": [
        {
          "_id": "682d388c57686b8c44ede70b",
          "user": {
            "_id": "656553d89bf6665f10e3a92d",
            "avatarUrl": "/avatars/f55b09e249c48ef3fe484afa33a182ae.svg",
            "isPro": false,
            "fullname": "xiang wyatt zhang",
            "user": "Wyattz23",
            "type": "user"
          },
          "name": "Xiang Zhang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-21T02:21:00.902Z",
          "hidden": false
        },
        {
          "_id": "682d388c57686b8c44ede70c",
          "name": "Juntai Cao",
          "hidden": false
        },
        {
          "_id": "682d388c57686b8c44ede70d",
          "name": "Jiaqi Wei",
          "hidden": false
        },
        {
          "_id": "682d388c57686b8c44ede70e",
          "name": "Yiwei Xu",
          "hidden": false
        },
        {
          "_id": "682d388c57686b8c44ede70f",
          "user": {
            "_id": "6466d463060756d2854ab3e1",
            "avatarUrl": "/avatars/4401387180c16472a6823f78aaa86d54.svg",
            "isPro": false,
            "fullname": "Chenyu You",
            "user": "Charlesyooo",
            "type": "user"
          },
          "name": "Chenyu You",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-21T02:30:12.849Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T10:32:30.000Z",
      "submittedOnDailyAt": "2025-05-21T00:51:16.514Z",
      "title": "Tokenization Constraints in LLMs: A Study of Symbolic and Arithmetic\n  Reasoning Limits",
      "submittedOnDailyBy": {
        "_id": "656553d89bf6665f10e3a92d",
        "avatarUrl": "/avatars/f55b09e249c48ef3fe484afa33a182ae.svg",
        "isPro": false,
        "fullname": "xiang wyatt zhang",
        "user": "Wyattz23",
        "type": "user"
      },
      "summary": "Tokenization is the first - and often underappreciated - layer of computation\nin language models. While Chain-of-Thought (CoT) prompting enables transformer\nmodels to approximate recurrent computation by externalizing intermediate\nsteps, we show that the success of such reasoning is fundamentally bounded by\nthe structure of tokenized inputs. This work presents a theoretical and\nempirical investigation into how tokenization schemes, particularly\nsubword-based methods like byte-pair encoding (BPE), impede symbolic\ncomputation by merging or obscuring atomic reasoning units. We introduce the\nnotion of Token Awareness to formalize how poor token granularity disrupts\nlogical alignment and prevents models from generalizing symbolic procedures.\nThrough systematic evaluation on arithmetic and symbolic tasks, we demonstrate\nthat token structure dramatically affect reasoning performance, causing failure\neven with CoT, while atomically-aligned formats unlock strong generalization,\nallowing small models (e.g., GPT-4o-mini) to outperform larger systems (e.g.,\no1) in structured reasoning. Our findings reveal that symbolic reasoning\nability in LLMs is not purely architectural, but deeply conditioned on\ntoken-level representations.",
      "upvotes": 0,
      "discussionId": "682d388c57686b8c44ede753",
      "ai_keywords": [
        "Chain-of-Thought (CoT) prompting",
        "transformer models",
        "recursive computation",
        "intermediate steps",
        "tokenization schemes",
        "subword-based methods",
        "byte-pair encoding (BPE)",
        "symbolic computation",
        "Token Awareness",
        "atomic reasoning units",
        "logical alignment",
        "structured reasoning",
        "arithmetic tasks",
        "symbolic tasks",
        "reasoning performance",
        "atomically-aligned formats",
        "structured reasoning",
        "low-level representations",
        "symbolic reasoning ability",
        "LLMs",
        "token-level representations"
      ]
    },
    "publishedAt": "2025-05-20T06:32:30.000Z",
    "title": "Tokenization Constraints in LLMs: A Study of Symbolic and Arithmetic\n  Reasoning Limits",
    "summary": "Tokenization is the first - and often underappreciated - layer of computation\nin language models. While Chain-of-Thought (CoT) prompting enables transformer\nmodels to approximate recurrent computation by externalizing intermediate\nsteps, we show that the success of such reasoning is fundamentally bounded by\nthe structure of tokenized inputs. This work presents a theoretical and\nempirical investigation into how tokenization schemes, particularly\nsubword-based methods like byte-pair encoding (BPE), impede symbolic\ncomputation by merging or obscuring atomic reasoning units. We introduce the\nnotion of Token Awareness to formalize how poor token granularity disrupts\nlogical alignment and prevents models from generalizing symbolic procedures.\nThrough systematic evaluation on arithmetic and symbolic tasks, we demonstrate\nthat token structure dramatically affect reasoning performance, causing failure\neven with CoT, while atomically-aligned formats unlock strong generalization,\nallowing small models (e.g., GPT-4o-mini) to outperform larger systems (e.g.,\no1) in structured reasoning. Our findings reveal that symbolic reasoning\nability in LLMs is not purely architectural, but deeply conditioned on\ntoken-level representations.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14178.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "656553d89bf6665f10e3a92d",
      "avatarUrl": "/avatars/f55b09e249c48ef3fe484afa33a182ae.svg",
      "fullname": "xiang wyatt zhang",
      "name": "Wyattz23",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.12306",
      "authors": [
        {
          "_id": "682d2f9f3b5f51f42185dd7d",
          "name": "Yuwei Zhang",
          "hidden": false
        },
        {
          "_id": "682d2f9f3b5f51f42185dd7e",
          "name": "Wenhao Yu",
          "hidden": false
        },
        {
          "_id": "682d2f9f3b5f51f42185dd7f",
          "name": "Shangbin Feng",
          "hidden": false
        },
        {
          "_id": "682d2f9f3b5f51f42185dd80",
          "name": "Yifan Zhu",
          "hidden": false
        },
        {
          "_id": "682d2f9f3b5f51f42185dd81",
          "name": "Letian Peng",
          "hidden": false
        },
        {
          "_id": "682d2f9f3b5f51f42185dd82",
          "name": "Jayanth Srinivasa",
          "hidden": false
        },
        {
          "_id": "682d2f9f3b5f51f42185dd83",
          "name": "Gaowen Liu",
          "hidden": false
        },
        {
          "_id": "682d2f9f3b5f51f42185dd84",
          "name": "Jingbo Shang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-18T08:39:05.000Z",
      "submittedOnDailyAt": "2025-05-21T00:13:17.201Z",
      "title": "Bidirectional LMs are Better Knowledge Memorizers? A Benchmark for\n  Real-world Knowledge Injection",
      "submittedOnDailyBy": {
        "_id": "64323dd503d81fa4d26deaf9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64323dd503d81fa4d26deaf9/x3ES8VXEZJljxDWvFWaAf.png",
        "isPro": false,
        "fullname": "Letian Peng",
        "user": "KomeijiForce",
        "type": "user"
      },
      "summary": "Despite significant advances in large language models (LLMs), their knowledge\nmemorization capabilities remain underexplored, due to the lack of standardized\nand high-quality test ground. In this paper, we introduce a novel, real-world\nand large-scale knowledge injection benchmark that evolves continuously over\ntime without requiring human intervention. Specifically, we propose WikiDYK,\nwhich leverages recently-added and human-written facts from Wikipedia's \"Did\nYou Know...\" entries. These entries are carefully selected by expert Wikipedia\neditors based on criteria such as verifiability and clarity. Each entry is\nconverted into multiple question-answer pairs spanning diverse task formats\nfrom easy cloze prompts to complex multi-hop questions. WikiDYK contains 12,290\nfacts and 77,180 questions, which is also seamlessly extensible with future\nupdates from Wikipedia editors. Extensive experiments using continued\npre-training reveal a surprising insight: despite their prevalence in modern\nLLMs, Causal Language Models (CLMs) demonstrate significantly weaker knowledge\nmemorization capabilities compared to Bidirectional Language Models (BiLMs),\nexhibiting a 23% lower accuracy in terms of reliability. To compensate for the\nsmaller scales of current BiLMs, we introduce a modular collaborative framework\nutilizing ensembles of BiLMs as external knowledge repositories to integrate\nwith LLMs. Experiment shows that our framework further improves the reliability\naccuracy by up to 29.1%.",
      "upvotes": 0,
      "discussionId": "682d2fa03b5f51f42185ddb4",
      "ai_keywords": [
        "large language models (LLMs)",
        "knowledge memorization capabilities",
        "WikiDYK",
        "\"Did You Know...\" entries",
        "question-answer pairs",
        "cloze prompts",
        "multi-hop questions",
        "continued pre-training",
        "Causal Language Models (CLMs)",
        "Bidirectional Language Models (BiLMs)",
        "modular collaborative framework",
        "ensembles of BiLMs",
        "external knowledge repositories",
        "reliability accuracy"
      ]
    },
    "publishedAt": "2025-05-18T04:39:05.000Z",
    "title": "Bidirectional LMs are Better Knowledge Memorizers? A Benchmark for\n  Real-world Knowledge Injection",
    "summary": "Despite significant advances in large language models (LLMs), their knowledge\nmemorization capabilities remain underexplored, due to the lack of standardized\nand high-quality test ground. In this paper, we introduce a novel, real-world\nand large-scale knowledge injection benchmark that evolves continuously over\ntime without requiring human intervention. Specifically, we propose WikiDYK,\nwhich leverages recently-added and human-written facts from Wikipedia's \"Did\nYou Know...\" entries. These entries are carefully selected by expert Wikipedia\neditors based on criteria such as verifiability and clarity. Each entry is\nconverted into multiple question-answer pairs spanning diverse task formats\nfrom easy cloze prompts to complex multi-hop questions. WikiDYK contains 12,290\nfacts and 77,180 questions, which is also seamlessly extensible with future\nupdates from Wikipedia editors. Extensive experiments using continued\npre-training reveal a surprising insight: despite their prevalence in modern\nLLMs, Causal Language Models (CLMs) demonstrate significantly weaker knowledge\nmemorization capabilities compared to Bidirectional Language Models (BiLMs),\nexhibiting a 23% lower accuracy in terms of reliability. To compensate for the\nsmaller scales of current BiLMs, we introduce a modular collaborative framework\nutilizing ensembles of BiLMs as external knowledge repositories to integrate\nwith LLMs. Experiment shows that our framework further improves the reliability\naccuracy by up to 29.1%.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.12306.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64323dd503d81fa4d26deaf9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64323dd503d81fa4d26deaf9/x3ES8VXEZJljxDWvFWaAf.png",
      "fullname": "Letian Peng",
      "name": "KomeijiForce",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.10588",
      "authors": [
        {
          "_id": "682d2dd917608739046ce410",
          "name": "Manisha Mehta",
          "hidden": false
        },
        {
          "_id": "682d2dd917608739046ce411",
          "name": "Fausto Giunchiglia",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-14T16:46:11.000Z",
      "submittedOnDailyAt": "2025-05-21T00:06:34.563Z",
      "title": "Understanding Gen Alpha Digital Language: Evaluation of LLM Safety\n  Systems for Content Moderation",
      "submittedOnDailyBy": {
        "_id": "604eb19e3050a33ebb17ef58",
        "avatarUrl": "/avatars/23650002ba3befee83060fe978a251c8.svg",
        "isPro": false,
        "fullname": "Virendra Mehta",
        "user": "Veeru",
        "type": "user"
      },
      "summary": "This research offers a unique evaluation of how AI systems interpret the\ndigital language of Generation Alpha (Gen Alpha, born 2010-2024). As the first\ncohort raised alongside AI, Gen Alpha faces new forms of online risk due to\nimmersive digital engagement and a growing mismatch between their evolving\ncommunication and existing safety tools. Their distinct language, shaped by\ngaming, memes, and AI-driven trends, often conceals harmful interactions from\nboth human moderators and automated systems. We assess four leading AI models\n(GPT-4, Claude, Gemini, and Llama 3) on their ability to detect masked\nharassment and manipulation within Gen Alpha discourse. Using a dataset of 100\nrecent expressions from gaming platforms, social media, and video content, the\nstudy reveals critical comprehension failures with direct implications for\nonline safety. This work contributes: (1) a first-of-its-kind dataset capturing\nGen Alpha expressions; (2) a framework to improve AI moderation systems for\nyouth protection; (3) a multi-perspective evaluation including AI systems,\nhuman moderators, and parents, with direct input from Gen Alpha co-researchers;\nand (4) an analysis of how linguistic divergence increases youth vulnerability.\nFindings highlight the urgent need to redesign safety systems attuned to youth\ncommunication, especially given Gen Alpha reluctance to seek help when adults\nfail to understand their digital world. This study combines the insight of a\nGen Alpha researcher with systematic academic analysis to address critical\ndigital safety challenges.",
      "upvotes": 0,
      "discussionId": "682d2dd917608739046ce440"
    },
    "publishedAt": "2025-05-14T12:46:11.000Z",
    "title": "Understanding Gen Alpha Digital Language: Evaluation of LLM Safety\n  Systems for Content Moderation",
    "summary": "This research offers a unique evaluation of how AI systems interpret the\ndigital language of Generation Alpha (Gen Alpha, born 2010-2024). As the first\ncohort raised alongside AI, Gen Alpha faces new forms of online risk due to\nimmersive digital engagement and a growing mismatch between their evolving\ncommunication and existing safety tools. Their distinct language, shaped by\ngaming, memes, and AI-driven trends, often conceals harmful interactions from\nboth human moderators and automated systems. We assess four leading AI models\n(GPT-4, Claude, Gemini, and Llama 3) on their ability to detect masked\nharassment and manipulation within Gen Alpha discourse. Using a dataset of 100\nrecent expressions from gaming platforms, social media, and video content, the\nstudy reveals critical comprehension failures with direct implications for\nonline safety. This work contributes: (1) a first-of-its-kind dataset capturing\nGen Alpha expressions; (2) a framework to improve AI moderation systems for\nyouth protection; (3) a multi-perspective evaluation including AI systems,\nhuman moderators, and parents, with direct input from Gen Alpha co-researchers;\nand (4) an analysis of how linguistic divergence increases youth vulnerability.\nFindings highlight the urgent need to redesign safety systems attuned to youth\ncommunication, especially given Gen Alpha reluctance to seek help when adults\nfail to understand their digital world. This study combines the insight of a\nGen Alpha researcher with systematic academic analysis to address critical\ndigital safety challenges.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.10588.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "604eb19e3050a33ebb17ef58",
      "avatarUrl": "/avatars/23650002ba3befee83060fe978a251c8.svg",
      "fullname": "Virendra Mehta",
      "name": "Veeru",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  }
]