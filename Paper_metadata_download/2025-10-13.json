[
  {
    "paper": {
      "id": "2510.08673",
      "authors": [
        {
          "_id": "68ec5edfcd07fb414898c93e",
          "name": "Kang Liao",
          "hidden": false
        },
        {
          "_id": "68ec5edfcd07fb414898c93f",
          "name": "Size Wu",
          "hidden": false
        },
        {
          "_id": "68ec5edfcd07fb414898c940",
          "name": "Zhonghua Wu",
          "hidden": false
        },
        {
          "_id": "68ec5edfcd07fb414898c941",
          "name": "Linyi Jin",
          "hidden": false
        },
        {
          "_id": "68ec5edfcd07fb414898c942",
          "name": "Chao Wang",
          "hidden": false
        },
        {
          "_id": "68ec5edfcd07fb414898c943",
          "name": "Yikai Wang",
          "hidden": false
        },
        {
          "_id": "68ec5edfcd07fb414898c944",
          "name": "Fei Wang",
          "hidden": false
        },
        {
          "_id": "68ec5edfcd07fb414898c945",
          "name": "Wei Li",
          "hidden": false
        },
        {
          "_id": "68ec5edfcd07fb414898c946",
          "name": "Chen Change Loy",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-09T17:59:29.000Z",
      "submittedOnDailyAt": "2025-10-13T00:45:06.202Z",
      "title": "Thinking with Camera: A Unified Multimodal Model for Camera-Centric\n  Understanding and Generation",
      "submittedOnDailyBy": {
        "_id": "65bc98383b879593a5a2f5e5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65bc98383b879593a5a2f5e5/p2ZtoTFN6tW-QkcPJf7YT.jpeg",
        "isPro": true,
        "fullname": "Kang Liao",
        "user": "KangLiao",
        "type": "user"
      },
      "summary": "Camera-centric understanding and generation are two cornerstones of spatial\nintelligence, yet they are typically studied in isolation. We present Puffin, a\nunified camera-centric multimodal model that extends spatial awareness along\nthe camera dimension. Puffin integrates language regression and diffusion-based\ngeneration to interpret and create scenes from arbitrary viewpoints. To bridge\nthe modality gap between cameras and vision-language, we introduce a novel\nparadigm that treats camera as language, enabling thinking with camera. This\nguides the model to align spatially grounded visual cues with photographic\nterminology while reasoning across geometric context. Puffin is trained on\nPuffin-4M, a large-scale dataset of 4 million vision-language-camera triplets.\nWe incorporate both global camera parameters and pixel-wise camera maps,\nyielding flexible and reliable spatial generation. Experiments demonstrate\nPuffin superior performance over specialized models for camera-centric\ngeneration and understanding. With instruction tuning, Puffin generalizes to\ndiverse cross-view tasks such as spatial imagination, world exploration, and\nphotography guidance. We will release the code, models, dataset pipeline, and\nbenchmark to advance multimodal spatial intelligence research.",
      "upvotes": 57,
      "discussionId": "68ec5ee0cd07fb414898c947",
      "projectPage": "https://kangliao929.github.io/projects/puffin/",
      "githubRepo": "https://github.com/KangLiao929/Puffin",
      "ai_summary": "Puffin, a unified multimodal model, integrates language regression and diffusion-based generation to enhance camera-centric spatial understanding and generation by treating camera parameters as language.",
      "ai_keywords": [
        "language regression",
        "diffusion-based generation",
        "camera-centric",
        "multimodal model",
        "spatial awareness",
        "vision-language",
        "camera as language",
        "geometric context",
        "Puffin-4M",
        "global camera parameters",
        "pixel-wise camera maps",
        "spatial generation",
        "instruction tuning",
        "spatial imagination",
        "world exploration",
        "photography guidance"
      ],
      "githubStars": 40
    },
    "publishedAt": "2025-10-09T13:59:29.000Z",
    "title": "Thinking with Camera: A Unified Multimodal Model for Camera-Centric\n  Understanding and Generation",
    "summary": "Camera-centric understanding and generation are two cornerstones of spatial\nintelligence, yet they are typically studied in isolation. We present Puffin, a\nunified camera-centric multimodal model that extends spatial awareness along\nthe camera dimension. Puffin integrates language regression and diffusion-based\ngeneration to interpret and create scenes from arbitrary viewpoints. To bridge\nthe modality gap between cameras and vision-language, we introduce a novel\nparadigm that treats camera as language, enabling thinking with camera. This\nguides the model to align spatially grounded visual cues with photographic\nterminology while reasoning across geometric context. Puffin is trained on\nPuffin-4M, a large-scale dataset of 4 million vision-language-camera triplets.\nWe incorporate both global camera parameters and pixel-wise camera maps,\nyielding flexible and reliable spatial generation. Experiments demonstrate\nPuffin superior performance over specialized models for camera-centric\ngeneration and understanding. With instruction tuning, Puffin generalizes to\ndiverse cross-view tasks such as spatial imagination, world exploration, and\nphotography guidance. We will release the code, models, dataset pipeline, and\nbenchmark to advance multimodal spatial intelligence research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.08673.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65bc98383b879593a5a2f5e5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65bc98383b879593a5a2f5e5/p2ZtoTFN6tW-QkcPJf7YT.jpeg",
      "fullname": "Kang Liao",
      "name": "KangLiao",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.04533",
      "authors": [
        {
          "_id": "68e67c91975ac4c405ef2334",
          "user": {
            "_id": "680ed019a9918bb1cbc66248",
            "avatarUrl": "/avatars/904d243f2fad99341f11795e93788993.svg",
            "isPro": true,
            "fullname": "Hyunmin Cho",
            "user": "hyeoncho01",
            "type": "user"
          },
          "name": "Hyunmin Cho",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-09T06:51:20.523Z",
          "hidden": false
        },
        {
          "_id": "68e67c91975ac4c405ef2335",
          "name": "Donghoon Ahn",
          "hidden": false
        },
        {
          "_id": "68e67c91975ac4c405ef2336",
          "name": "Susung Hong",
          "hidden": false
        },
        {
          "_id": "68e67c91975ac4c405ef2337",
          "name": "Jee Eun Kim",
          "hidden": false
        },
        {
          "_id": "68e67c91975ac4c405ef2338",
          "name": "Seungryong Kim",
          "hidden": false
        },
        {
          "_id": "68e67c91975ac4c405ef2339",
          "name": "Kyong Hwan Jin",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/680ed019a9918bb1cbc66248/QFu-1RXfK2GP-IUrFzCwl.png"
      ],
      "publishedAt": "2025-10-06T06:53:29.000Z",
      "submittedOnDailyAt": "2025-10-13T00:05:19.216Z",
      "title": "TAG:Tangential Amplifying Guidance for Hallucination-Resistant Diffusion\n  Sampling",
      "submittedOnDailyBy": {
        "_id": "680ed019a9918bb1cbc66248",
        "avatarUrl": "/avatars/904d243f2fad99341f11795e93788993.svg",
        "isPro": true,
        "fullname": "Hyunmin Cho",
        "user": "hyeoncho01",
        "type": "user"
      },
      "summary": "Recent diffusion models achieve the state-of-the-art performance in image\ngeneration, but often suffer from semantic inconsistencies or hallucinations.\nWhile various inference-time guidance methods can enhance generation, they\noften operate indirectly by relying on external signals or architectural\nmodifications, which introduces additional computational overhead. In this\npaper, we propose Tangential Amplifying Guidance (TAG), a more efficient and\ndirect guidance method that operates solely on trajectory signals without\nmodifying the underlying diffusion model. TAG leverages an intermediate sample\nas a projection basis and amplifies the tangential components of the estimated\nscores with respect to this basis to correct the sampling trajectory. We\nformalize this guidance process by leveraging a first-order Taylor expansion,\nwhich demonstrates that amplifying the tangential component steers the state\ntoward higher-probability regions, thereby reducing inconsistencies and\nenhancing sample quality. TAG is a plug-and-play, architecture-agnostic module\nthat improves diffusion sampling fidelity with minimal computational addition,\noffering a new perspective on diffusion guidance.",
      "upvotes": 34,
      "discussionId": "68e67c91975ac4c405ef233a",
      "projectPage": "https://hyeon-cho.github.io/TAG/",
      "githubRepo": "https://github.com/hyeon-cho/Tangential-Amplifying-Guidance",
      "ai_summary": "Tangential Amplifying Guidance (TAG) improves diffusion model sample quality by directly amplifying tangential components of estimated scores without modifying the model architecture.",
      "ai_keywords": [
        "diffusion models",
        "image generation",
        "semantic inconsistencies",
        "hallucinations",
        "inference-time guidance",
        "trajectory signals",
        "intermediate sample",
        "projection basis",
        "tangential components",
        "first-order Taylor expansion",
        "sampling trajectory",
        "higher-probability regions",
        "plug-and-play",
        "architecture-agnostic"
      ],
      "githubStars": 1
    },
    "publishedAt": "2025-10-06T02:53:29.000Z",
    "title": "TAG:Tangential Amplifying Guidance for Hallucination-Resistant Diffusion\n  Sampling",
    "summary": "Recent diffusion models achieve the state-of-the-art performance in image\ngeneration, but often suffer from semantic inconsistencies or hallucinations.\nWhile various inference-time guidance methods can enhance generation, they\noften operate indirectly by relying on external signals or architectural\nmodifications, which introduces additional computational overhead. In this\npaper, we propose Tangential Amplifying Guidance (TAG), a more efficient and\ndirect guidance method that operates solely on trajectory signals without\nmodifying the underlying diffusion model. TAG leverages an intermediate sample\nas a projection basis and amplifies the tangential components of the estimated\nscores with respect to this basis to correct the sampling trajectory. We\nformalize this guidance process by leveraging a first-order Taylor expansion,\nwhich demonstrates that amplifying the tangential component steers the state\ntoward higher-probability regions, thereby reducing inconsistencies and\nenhancing sample quality. TAG is a plug-and-play, architecture-agnostic module\nthat improves diffusion sampling fidelity with minimal computational addition,\noffering a new perspective on diffusion guidance.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/680ed019a9918bb1cbc66248/QFu-1RXfK2GP-IUrFzCwl.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.04533.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "680ed019a9918bb1cbc66248",
      "avatarUrl": "/avatars/904d243f2fad99341f11795e93788993.svg",
      "fullname": "Hyunmin Cho",
      "name": "hyeoncho01",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.09201",
      "authors": [
        {
          "_id": "68ec632fcd07fb414898c97b",
          "name": "Yumin Choi",
          "hidden": false
        },
        {
          "_id": "68ec632fcd07fb414898c97c",
          "name": "Dongki Kim",
          "hidden": false
        },
        {
          "_id": "68ec632fcd07fb414898c97d",
          "name": "Jinheon Baek",
          "hidden": false
        },
        {
          "_id": "68ec632fcd07fb414898c97e",
          "name": "Sung Ju Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-10T09:41:25.000Z",
      "submittedOnDailyAt": "2025-10-13T00:56:45.053Z",
      "title": "Multimodal Prompt Optimization: Why Not Leverage Multiple Modalities for\n  MLLMs",
      "submittedOnDailyBy": {
        "_id": "64cfa0b9749587dbe01d0079",
        "avatarUrl": "/avatars/93ca0a1d9c5578d052c5af0d4d1a0252.svg",
        "isPro": false,
        "fullname": "Yumin Choi",
        "user": "YuminChoi",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) have shown remarkable success, and their\nmultimodal expansions (MLLMs) further unlock capabilities spanning images,\nvideos, and other modalities beyond text. However, despite this shift, prompt\noptimization approaches, designed to reduce the burden of manual prompt\ncrafting while maximizing performance, remain confined to text, ultimately\nlimiting the full potential of MLLMs. Motivated by this gap, we introduce the\nnew problem of multimodal prompt optimization, which expands the prior\ndefinition of prompt optimization to the multimodal space defined by the pairs\nof textual and non-textual prompts. To tackle this problem, we then propose the\nMultimodal Prompt Optimizer (MPO), a unified framework that not only performs\nthe joint optimization of multimodal prompts through alignment-preserving\nupdates but also guides the selection process of candidate prompts by\nleveraging earlier evaluations as priors in a Bayesian-based selection\nstrategy. Through extensive experiments across diverse modalities that go\nbeyond text, such as images, videos, and even molecules, we demonstrate that\nMPO outperforms leading text-only optimization methods, establishing multimodal\nprompt optimization as a crucial step to realizing the potential of MLLMs.",
      "upvotes": 31,
      "discussionId": "68ec632fcd07fb414898c97f",
      "githubRepo": "https://github.com/Dozi01/MPO",
      "ai_summary": "Multimodal Prompt Optimizer (MPO) extends prompt optimization to handle multiple data types, improving performance over text-only methods in various applications.",
      "ai_keywords": [
        "Large Language Models",
        "multimodal expansions",
        "prompt optimization",
        "Multimodal Prompt Optimizer",
        "alignment-preserving updates",
        "Bayesian-based selection strategy"
      ],
      "githubStars": 1,
      "organization": {
        "_id": "6475760c33192631bad2bb38",
        "name": "kaist-ai",
        "fullname": "KAIST AI",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6469949654873f0043b09c22/aaZFiyXe1qR-Dmy_xq67m.png"
      }
    },
    "publishedAt": "2025-10-10T05:41:25.000Z",
    "title": "Multimodal Prompt Optimization: Why Not Leverage Multiple Modalities for\n  MLLMs",
    "summary": "Large Language Models (LLMs) have shown remarkable success, and their\nmultimodal expansions (MLLMs) further unlock capabilities spanning images,\nvideos, and other modalities beyond text. However, despite this shift, prompt\noptimization approaches, designed to reduce the burden of manual prompt\ncrafting while maximizing performance, remain confined to text, ultimately\nlimiting the full potential of MLLMs. Motivated by this gap, we introduce the\nnew problem of multimodal prompt optimization, which expands the prior\ndefinition of prompt optimization to the multimodal space defined by the pairs\nof textual and non-textual prompts. To tackle this problem, we then propose the\nMultimodal Prompt Optimizer (MPO), a unified framework that not only performs\nthe joint optimization of multimodal prompts through alignment-preserving\nupdates but also guides the selection process of candidate prompts by\nleveraging earlier evaluations as priors in a Bayesian-based selection\nstrategy. Through extensive experiments across diverse modalities that go\nbeyond text, such as images, videos, and even molecules, we demonstrate that\nMPO outperforms leading text-only optimization methods, establishing multimodal\nprompt optimization as a crucial step to realizing the potential of MLLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.09201.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64cfa0b9749587dbe01d0079",
      "avatarUrl": "/avatars/93ca0a1d9c5578d052c5af0d4d1a0252.svg",
      "fullname": "Yumin Choi",
      "name": "YuminChoi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "organization": {
      "_id": "6475760c33192631bad2bb38",
      "name": "kaist-ai",
      "fullname": "KAIST AI",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6469949654873f0043b09c22/aaZFiyXe1qR-Dmy_xq67m.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.05684",
      "authors": [
        {
          "_id": "68ec8eb7cd07fb414898ca8a",
          "name": "Suwhan Choi",
          "hidden": false
        },
        {
          "_id": "68ec8eb7cd07fb414898ca8b",
          "name": "Jaeyoon Jung",
          "hidden": false
        },
        {
          "_id": "68ec8eb7cd07fb414898ca8c",
          "name": "Haebin Seong",
          "hidden": false
        },
        {
          "_id": "68ec8eb7cd07fb414898ca8d",
          "name": "Minchan Kim",
          "hidden": false
        },
        {
          "_id": "68ec8eb7cd07fb414898ca8e",
          "name": "Minyeong Kim",
          "hidden": false
        },
        {
          "_id": "68ec8eb7cd07fb414898ca8f",
          "name": "Yongjun Cho",
          "hidden": false
        },
        {
          "_id": "68ec8eb7cd07fb414898ca90",
          "name": "Yoonshik Kim",
          "hidden": false
        },
        {
          "_id": "68ec8eb7cd07fb414898ca91",
          "name": "Yubeen Park",
          "hidden": false
        },
        {
          "_id": "68ec8eb7cd07fb414898ca92",
          "name": "Youngjae Yu",
          "hidden": false
        },
        {
          "_id": "68ec8eb7cd07fb414898ca93",
          "name": "Yunsung Lee",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/646484cfb90150b2706df03b/c36wRRNT6d9wbZQ1iXQ9Y.mp4"
      ],
      "publishedAt": "2025-10-07T08:40:33.000Z",
      "submittedOnDailyAt": "2025-10-13T06:15:34.299Z",
      "title": "D2E: Scaling Vision-Action Pretraining on Desktop Data for Transfer to\n  Embodied AI",
      "submittedOnDailyBy": {
        "_id": "646484cfb90150b2706df03b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646484cfb90150b2706df03b/8ocSbXBSbrruhlhcxwzEt.png",
        "isPro": false,
        "fullname": "Jaeyoon Jung",
        "user": "lastdefiance20",
        "type": "user"
      },
      "summary": "Large language models leverage internet-scale text data, yet embodied AI\nremains constrained by the prohibitive costs of physical trajectory collection.\nDesktop environments -- particularly gaming -- offer a compelling alternative:\nthey provide rich sensorimotor interactions at scale while maintaining the\nstructured observation-action coupling essential for embodied learning. We\npresent D2E (Desktop to Embodied AI), a framework that demonstrates desktop\ninteractions can serve as an effective pretraining substrate for robotics\nembodied AI tasks. Unlike prior work that remained domain-specific (e.g., VPT\nfor Minecraft) or kept data proprietary (e.g., SIMA), D2E establishes a\ncomplete pipeline from scalable desktop data collection to verified transfer in\nembodied domains. Our framework comprises three components: (1) the OWA Toolkit\nthat unifies diverse desktop interactions into a standardized format with 152x\ncompression, (2) the Generalist-IDM that achieves strong zero-shot\ngeneralization across unseen games through timestamp-based event prediction,\nenabling internet-scale pseudo-labeling, and (3) VAPT that transfers\ndesktop-pretrained representations to physical manipulation and navigation.\nUsing 1.3K+ hours of data (259 hours of human demonstrations, and 1K+ hours of\npseudo-labeled gameplay), we achieve a total of 96.6% success rate on LIBERO\nmanipulation and 83.3% on CANVAS navigation benchmarks. This validates that\nsensorimotor primitives in digital interactions exhibit sufficient invariance\nto transfer meaningfully to physical embodied tasks, establishing desktop\npretraining as a practical paradigm for robotics. We will make all our work\npublic, including the OWA toolkit, datasets of human-collected and\npseudo-labeled, and VAPT-trained models available at\nhttps://worv-ai.github.io/d2e/",
      "upvotes": 31,
      "discussionId": "68ec8eb7cd07fb414898ca94",
      "projectPage": "https://worv-ai.github.io/d2e/",
      "githubRepo": "https://github.com/worv-ai/D2E",
      "ai_summary": "D2E framework uses desktop interactions to pretrain embodied AI, achieving high success rates in physical manipulation and navigation tasks.",
      "ai_keywords": [
        "embodied AI",
        "desktop environments",
        "sensorimotor interactions",
        "OWA Toolkit",
        "Generalist-IDM",
        "timestamp-based event prediction",
        "zero-shot generalization",
        "internet-scale pseudo-labeling",
        "VAPT",
        "LIBERO manipulation",
        "CANVAS navigation",
        "desktop pretraining"
      ],
      "organization": {
        "_id": "64d32903129a210e56a3f3d3",
        "name": "maum-ai",
        "fullname": "maum-ai",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/646484cfb90150b2706df03b/QMrYBmsRfGZeU1DigPUzk.png"
      }
    },
    "publishedAt": "2025-10-07T04:40:33.000Z",
    "title": "D2E: Scaling Vision-Action Pretraining on Desktop Data for Transfer to\n  Embodied AI",
    "summary": "Large language models leverage internet-scale text data, yet embodied AI\nremains constrained by the prohibitive costs of physical trajectory collection.\nDesktop environments -- particularly gaming -- offer a compelling alternative:\nthey provide rich sensorimotor interactions at scale while maintaining the\nstructured observation-action coupling essential for embodied learning. We\npresent D2E (Desktop to Embodied AI), a framework that demonstrates desktop\ninteractions can serve as an effective pretraining substrate for robotics\nembodied AI tasks. Unlike prior work that remained domain-specific (e.g., VPT\nfor Minecraft) or kept data proprietary (e.g., SIMA), D2E establishes a\ncomplete pipeline from scalable desktop data collection to verified transfer in\nembodied domains. Our framework comprises three components: (1) the OWA Toolkit\nthat unifies diverse desktop interactions into a standardized format with 152x\ncompression, (2) the Generalist-IDM that achieves strong zero-shot\ngeneralization across unseen games through timestamp-based event prediction,\nenabling internet-scale pseudo-labeling, and (3) VAPT that transfers\ndesktop-pretrained representations to physical manipulation and navigation.\nUsing 1.3K+ hours of data (259 hours of human demonstrations, and 1K+ hours of\npseudo-labeled gameplay), we achieve a total of 96.6% success rate on LIBERO\nmanipulation and 83.3% on CANVAS navigation benchmarks. This validates that\nsensorimotor primitives in digital interactions exhibit sufficient invariance\nto transfer meaningfully to physical embodied tasks, establishing desktop\npretraining as a practical paradigm for robotics. We will make all our work\npublic, including the OWA toolkit, datasets of human-collected and\npseudo-labeled, and VAPT-trained models available at\nhttps://worv-ai.github.io/d2e/",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/646484cfb90150b2706df03b/c36wRRNT6d9wbZQ1iXQ9Y.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.05684.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "646484cfb90150b2706df03b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646484cfb90150b2706df03b/8ocSbXBSbrruhlhcxwzEt.png",
      "fullname": "Jaeyoon Jung",
      "name": "lastdefiance20",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "organization": {
      "_id": "64d32903129a210e56a3f3d3",
      "name": "maum-ai",
      "fullname": "maum-ai",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/646484cfb90150b2706df03b/QMrYBmsRfGZeU1DigPUzk.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.09558",
      "authors": [
        {
          "_id": "68ec771ccd07fb414898ca1f",
          "name": "Qiguang Chen",
          "hidden": false
        },
        {
          "_id": "68ec771ccd07fb414898ca20",
          "name": "Zheng Yan",
          "hidden": false
        },
        {
          "_id": "68ec771ccd07fb414898ca21",
          "name": "Mingda Yang",
          "hidden": false
        },
        {
          "_id": "68ec771ccd07fb414898ca22",
          "name": "Libo Qin",
          "hidden": false
        },
        {
          "_id": "68ec771ccd07fb414898ca23",
          "name": "Yixin Yuan",
          "hidden": false
        },
        {
          "_id": "68ec771ccd07fb414898ca24",
          "name": "Hanjing Li",
          "hidden": false
        },
        {
          "_id": "68ec771ccd07fb414898ca25",
          "name": "Jinhao Liu",
          "hidden": false
        },
        {
          "_id": "68ec771ccd07fb414898ca26",
          "name": "Yiyan Ji",
          "hidden": false
        },
        {
          "_id": "68ec771ccd07fb414898ca27",
          "name": "Dengyun Peng",
          "hidden": false
        },
        {
          "_id": "68ec771ccd07fb414898ca28",
          "name": "Jiannan Guan",
          "hidden": false
        },
        {
          "_id": "68ec771ccd07fb414898ca29",
          "name": "Mengkang Hu",
          "hidden": false
        },
        {
          "_id": "68ec771ccd07fb414898ca2a",
          "name": "Yantao Du",
          "hidden": false
        },
        {
          "_id": "68ec771ccd07fb414898ca2b",
          "name": "Wanxiang Che",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-10T17:08:36.000Z",
      "submittedOnDailyAt": "2025-10-13T02:29:55.098Z",
      "title": "AutoPR: Let's Automate Your Academic Promotion!",
      "submittedOnDailyBy": {
        "_id": "636f526a6cd69d9a36ff2b53",
        "avatarUrl": "/avatars/8f2271a193fcac609d9be270552b5afa.svg",
        "isPro": false,
        "fullname": "Qiguang Chen",
        "user": "LightChen2333",
        "type": "user"
      },
      "summary": "As the volume of peer-reviewed research surges, scholars increasingly rely on\nsocial platforms for discovery, while authors invest considerable effort in\npromoting their work to ensure visibility and citations. To streamline this\nprocess and reduce the reliance on human effort, we introduce Automatic\nPromotion (AutoPR), a novel task that transforms research papers into accurate,\nengaging, and timely public content. To enable rigorous evaluation, we release\nPRBench, a multimodal benchmark that links 512 peer-reviewed articles to\nhigh-quality promotional posts, assessing systems along three axes: Fidelity\n(accuracy and tone), Engagement (audience targeting and appeal), and Alignment\n(timing and channel optimization). We also introduce PRAgent, a multi-agent\nframework that automates AutoPR in three stages: content extraction with\nmultimodal preparation, collaborative synthesis for polished outputs, and\nplatform-specific adaptation to optimize norms, tone, and tagging for maximum\nreach. When compared to direct LLM pipelines on PRBench, PRAgent demonstrates\nsubstantial improvements, including a 604% increase in total watch time, a 438%\nrise in likes, and at least a 2.9x boost in overall engagement. Ablation\nstudies show that platform modeling and targeted promotion contribute the most\nto these gains. Our results position AutoPR as a tractable, measurable research\nproblem and provide a roadmap for scalable, impactful automated scholarly\ncommunication.",
      "upvotes": 20,
      "discussionId": "68ec771ccd07fb414898ca2c",
      "projectPage": "https://yzweak.github.io/autopr.github.io/",
      "githubRepo": "https://github.com/LightChen233/AutoPR",
      "ai_summary": "AutoPR, a multi-agent framework, automates the promotion of research papers by transforming them into engaging public content, significantly improving engagement metrics compared to direct LLM pipelines.",
      "ai_keywords": [
        "multimodal benchmark",
        "content extraction",
        "collaborative synthesis",
        "platform-specific adaptation",
        "LLM pipelines"
      ],
      "githubStars": 1
    },
    "publishedAt": "2025-10-10T13:08:36.000Z",
    "title": "AutoPR: Let's Automate Your Academic Promotion!",
    "summary": "As the volume of peer-reviewed research surges, scholars increasingly rely on\nsocial platforms for discovery, while authors invest considerable effort in\npromoting their work to ensure visibility and citations. To streamline this\nprocess and reduce the reliance on human effort, we introduce Automatic\nPromotion (AutoPR), a novel task that transforms research papers into accurate,\nengaging, and timely public content. To enable rigorous evaluation, we release\nPRBench, a multimodal benchmark that links 512 peer-reviewed articles to\nhigh-quality promotional posts, assessing systems along three axes: Fidelity\n(accuracy and tone), Engagement (audience targeting and appeal), and Alignment\n(timing and channel optimization). We also introduce PRAgent, a multi-agent\nframework that automates AutoPR in three stages: content extraction with\nmultimodal preparation, collaborative synthesis for polished outputs, and\nplatform-specific adaptation to optimize norms, tone, and tagging for maximum\nreach. When compared to direct LLM pipelines on PRBench, PRAgent demonstrates\nsubstantial improvements, including a 604% increase in total watch time, a 438%\nrise in likes, and at least a 2.9x boost in overall engagement. Ablation\nstudies show that platform modeling and targeted promotion contribute the most\nto these gains. Our results position AutoPR as a tractable, measurable research\nproblem and provide a roadmap for scalable, impactful automated scholarly\ncommunication.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.09558.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "636f526a6cd69d9a36ff2b53",
      "avatarUrl": "/avatars/8f2271a193fcac609d9be270552b5afa.svg",
      "fullname": "Qiguang Chen",
      "name": "LightChen2333",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.06499",
      "authors": [
        {
          "_id": "68e7f1de93680d7965e7c838",
          "name": "Zhepeng Cen",
          "hidden": false
        },
        {
          "_id": "68e7f1de93680d7965e7c839",
          "user": {
            "_id": "65e0c21ebd0c59bbf271004c",
            "avatarUrl": "/avatars/3210f41fd433d798fd7857c943288625.svg",
            "isPro": false,
            "fullname": "Haolin Chen",
            "user": "hlnchen",
            "type": "user"
          },
          "name": "Haolin Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-10T04:33:41.584Z",
          "hidden": false
        },
        {
          "_id": "68e7f1de93680d7965e7c83a",
          "name": "Shiyu Wang",
          "hidden": false
        },
        {
          "_id": "68e7f1de93680d7965e7c83b",
          "name": "Zuxin Liu",
          "hidden": false
        },
        {
          "_id": "68e7f1de93680d7965e7c83c",
          "name": "Zhiwei Liu",
          "hidden": false
        },
        {
          "_id": "68e7f1de93680d7965e7c83d",
          "name": "Ding Zhao",
          "hidden": false
        },
        {
          "_id": "68e7f1de93680d7965e7c83e",
          "name": "Silvio Savarese",
          "hidden": false
        },
        {
          "_id": "68e7f1de93680d7965e7c83f",
          "name": "Caiming Xiong",
          "hidden": false
        },
        {
          "_id": "68e7f1de93680d7965e7c840",
          "name": "Huan Wang",
          "hidden": false
        },
        {
          "_id": "68e7f1de93680d7965e7c841",
          "name": "Weiran Yao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/632cdea254e2c512c8f95b12/gvsr0maJXcj_81DKU8MPf.jpeg"
      ],
      "publishedAt": "2025-10-07T22:30:59.000Z",
      "submittedOnDailyAt": "2025-10-13T01:05:10.732Z",
      "title": "Webscale-RL: Automated Data Pipeline for Scaling RL Data to Pretraining\n  Levels",
      "submittedOnDailyBy": {
        "_id": "632cdea254e2c512c8f95b12",
        "avatarUrl": "/avatars/a6d06cdd75861ae7d589f1343d81a5c5.svg",
        "isPro": false,
        "fullname": "Weiran Yao",
        "user": "weirayao",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) have achieved remarkable success through\nimitation learning on vast text corpora, but this paradigm creates a\ntraining-generation gap and limits robust reasoning. Reinforcement learning\n(RL) offers a more data-efficient solution capable of bridging this gap, yet\nits application has been constrained by a critical data bottleneck: existing RL\ndatasets are orders of magnitude smaller and less diverse than web-scale\npre-training corpora. To address this, we introduce the Webscale-RL pipeline, a\nscalable data engine that systematically converts large-scale pre-training\ndocuments into millions of diverse, verifiable question-answer pairs for RL.\nUsing this pipeline, we construct the Webscale-RL dataset, containing 1.2\nmillion examples across more than 9 domains. Our experiments show that the\nmodel trained on this dataset significantly outperforms continual pretraining\nand strong data refinement baselines across a suite of benchmarks. Notably, RL\ntraining with our dataset proves substantially more efficient, achieving the\nperformance of continual pre-training with up to 100times fewer tokens. Our\nwork presents a viable path toward scaling RL to pre-training levels, enabling\nmore capable and efficient language models.",
      "upvotes": 19,
      "discussionId": "68e7f1de93680d7965e7c842",
      "projectPage": "https://huggingface.co/datasets/Salesforce/Webscale-RL",
      "githubRepo": "https://github.com/SalesforceAIResearch/PretrainRL-pipeline",
      "ai_summary": "A scalable data engine converts large-scale pre-training documents into diverse question-answer pairs for reinforcement learning, significantly improving model performance and efficiency.",
      "ai_keywords": [
        "Large Language Models",
        "imitation learning",
        "reinforcement learning",
        "Webscale-RL pipeline",
        "Webscale-RL dataset",
        "continual pretraining",
        "data refinement",
        "benchmarks"
      ],
      "githubStars": 38,
      "organization": {
        "_id": "5f6d64475e78cc6b0ed31e4c",
        "name": "Salesforce",
        "fullname": "Salesforce",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1602756670970-noauth.jpeg"
      }
    },
    "publishedAt": "2025-10-07T18:30:59.000Z",
    "title": "Webscale-RL: Automated Data Pipeline for Scaling RL Data to Pretraining\n  Levels",
    "summary": "Large Language Models (LLMs) have achieved remarkable success through\nimitation learning on vast text corpora, but this paradigm creates a\ntraining-generation gap and limits robust reasoning. Reinforcement learning\n(RL) offers a more data-efficient solution capable of bridging this gap, yet\nits application has been constrained by a critical data bottleneck: existing RL\ndatasets are orders of magnitude smaller and less diverse than web-scale\npre-training corpora. To address this, we introduce the Webscale-RL pipeline, a\nscalable data engine that systematically converts large-scale pre-training\ndocuments into millions of diverse, verifiable question-answer pairs for RL.\nUsing this pipeline, we construct the Webscale-RL dataset, containing 1.2\nmillion examples across more than 9 domains. Our experiments show that the\nmodel trained on this dataset significantly outperforms continual pretraining\nand strong data refinement baselines across a suite of benchmarks. Notably, RL\ntraining with our dataset proves substantially more efficient, achieving the\nperformance of continual pre-training with up to 100times fewer tokens. Our\nwork presents a viable path toward scaling RL to pre-training levels, enabling\nmore capable and efficient language models.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/632cdea254e2c512c8f95b12/gvsr0maJXcj_81DKU8MPf.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.06499.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "632cdea254e2c512c8f95b12",
      "avatarUrl": "/avatars/a6d06cdd75861ae7d589f1343d81a5c5.svg",
      "fullname": "Weiran Yao",
      "name": "weirayao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "organization": {
      "_id": "5f6d64475e78cc6b0ed31e4c",
      "name": "Salesforce",
      "fullname": "Salesforce",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1602756670970-noauth.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.08189",
      "authors": [
        {
          "_id": "68e9eab7012880a10864b725",
          "user": {
            "_id": "61fde97843eb0913fa2df67b",
            "avatarUrl": "/avatars/6b739fa8ab23ba69accb5614d96b243b.svg",
            "isPro": false,
            "fullname": "Luyi",
            "user": "lulululuyi",
            "type": "user"
          },
          "name": "Yi Lu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-11T13:26:23.631Z",
          "hidden": false
        },
        {
          "_id": "68e9eab7012880a10864b726",
          "name": "Jianing Wang",
          "hidden": false
        },
        {
          "_id": "68e9eab7012880a10864b727",
          "name": "Linsen Guo",
          "hidden": false
        },
        {
          "_id": "68e9eab7012880a10864b728",
          "name": "Wei He",
          "hidden": false
        },
        {
          "_id": "68e9eab7012880a10864b729",
          "name": "Hongyin Tang",
          "hidden": false
        },
        {
          "_id": "68e9eab7012880a10864b72a",
          "name": "Tao Gui",
          "hidden": false
        },
        {
          "_id": "68e9eab7012880a10864b72b",
          "name": "Xuanjing Huang",
          "hidden": false
        },
        {
          "_id": "68e9eab7012880a10864b72c",
          "name": "Xuezhi Cao",
          "hidden": false
        },
        {
          "_id": "68e9eab7012880a10864b72d",
          "name": "Wei Wang",
          "hidden": false
        },
        {
          "_id": "68e9eab7012880a10864b72e",
          "name": "Xunliang Cai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-09T13:16:22.000Z",
      "submittedOnDailyAt": "2025-10-13T00:44:04.754Z",
      "title": "R-Horizon: How Far Can Your Large Reasoning Model Really Go in Breadth\n  and Depth?",
      "submittedOnDailyBy": {
        "_id": "61fde97843eb0913fa2df67b",
        "avatarUrl": "/avatars/6b739fa8ab23ba69accb5614d96b243b.svg",
        "isPro": false,
        "fullname": "Luyi",
        "user": "lulululuyi",
        "type": "user"
      },
      "summary": "Recent trends in test-time scaling for reasoning models (e.g., OpenAI o1,\nDeepSeek-R1) have led to remarkable improvements through long Chain-of-Thought\n(CoT). However, existing benchmarks mainly focus on immediate, single-horizon\ntasks, failing to adequately evaluate models' ability to understand and respond\nto complex, long-horizon scenarios. To address this incomplete evaluation of\nLarge Reasoning Models (LRMs), we propose R-HORIZON, a method designed to\nstimulate long-horizon reasoning behaviors in LRMs through query composition.\nBased on R-HORIZON, we construct a long-horizon reasoning benchmark, comprising\ncomplex multi-step reasoning tasks with interdependent problems that span long\nreasoning horizons. Through comprehensive evaluation of LRMs using the\nR-HORIZON benchmark, we find that even the most advanced LRMs suffer\nsignificant performance degradation. Our analysis reveals that LRMs exhibit\nlimited effective reasoning length and struggle to allocate thinking budget\nacross multiple problems appropriately. Recognizing these limitations, we use\nR-HORIZON to construct long-horizon reasoning data for reinforcement learning\nwith verified rewards (RLVR). Compared to training with single-horizon data,\nRLVR with R-HORIZON not only substantially improves performance on the\nmulti-horizon reasoning tasks, but also promotes accuracy on standard reasoning\ntasks, with an increase of 7.5 on AIME2024. These results position R-HORIZON as\na scalable, controllable, and low-cost paradigm for enhancing and evaluating\nthe long-horizon reasoning capabilities of LRMs.",
      "upvotes": 18,
      "discussionId": "68e9eab8012880a10864b72f",
      "projectPage": "https://reasoning-horizon.github.io/",
      "githubRepo": "https://github.com/LuLuLuyi/R-HORIZON",
      "ai_summary": "R-HORIZON, a method using query composition, improves long-horizon reasoning in Large Reasoning Models through a benchmark of complex multi-step tasks, enhancing performance and accuracy.",
      "ai_keywords": [
        "Chain-of-Thought",
        "CoT",
        "Large Reasoning Models",
        "LRMs",
        "long-horizon reasoning",
        "query composition",
        "reinforcement learning with verified rewards",
        "RLVR",
        "AIME2024"
      ],
      "githubStars": 10,
      "organization": {
        "_id": "68b28d79a176a9beb30d2049",
        "name": "meituan-longcat",
        "fullname": "LongCat",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"
      }
    },
    "publishedAt": "2025-10-09T09:16:22.000Z",
    "title": "R-Horizon: How Far Can Your Large Reasoning Model Really Go in Breadth\n  and Depth?",
    "summary": "Recent trends in test-time scaling for reasoning models (e.g., OpenAI o1,\nDeepSeek-R1) have led to remarkable improvements through long Chain-of-Thought\n(CoT). However, existing benchmarks mainly focus on immediate, single-horizon\ntasks, failing to adequately evaluate models' ability to understand and respond\nto complex, long-horizon scenarios. To address this incomplete evaluation of\nLarge Reasoning Models (LRMs), we propose R-HORIZON, a method designed to\nstimulate long-horizon reasoning behaviors in LRMs through query composition.\nBased on R-HORIZON, we construct a long-horizon reasoning benchmark, comprising\ncomplex multi-step reasoning tasks with interdependent problems that span long\nreasoning horizons. Through comprehensive evaluation of LRMs using the\nR-HORIZON benchmark, we find that even the most advanced LRMs suffer\nsignificant performance degradation. Our analysis reveals that LRMs exhibit\nlimited effective reasoning length and struggle to allocate thinking budget\nacross multiple problems appropriately. Recognizing these limitations, we use\nR-HORIZON to construct long-horizon reasoning data for reinforcement learning\nwith verified rewards (RLVR). Compared to training with single-horizon data,\nRLVR with R-HORIZON not only substantially improves performance on the\nmulti-horizon reasoning tasks, but also promotes accuracy on standard reasoning\ntasks, with an increase of 7.5 on AIME2024. These results position R-HORIZON as\na scalable, controllable, and low-cost paradigm for enhancing and evaluating\nthe long-horizon reasoning capabilities of LRMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.08189.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61fde97843eb0913fa2df67b",
      "avatarUrl": "/avatars/6b739fa8ab23ba69accb5614d96b243b.svg",
      "fullname": "Luyi",
      "name": "lulululuyi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "68b28d79a176a9beb30d2049",
      "name": "meituan-longcat",
      "fullname": "LongCat",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.09606",
      "authors": [
        {
          "_id": "68ec5975cd07fb414898c8ea",
          "name": "Peiwen Sun",
          "hidden": false
        },
        {
          "_id": "68ec5975cd07fb414898c8eb",
          "name": "Shiqiang Lang",
          "hidden": false
        },
        {
          "_id": "68ec5975cd07fb414898c8ec",
          "name": "Dongming Wu",
          "hidden": false
        },
        {
          "_id": "68ec5975cd07fb414898c8ed",
          "name": "Yi Ding",
          "hidden": false
        },
        {
          "_id": "68ec5975cd07fb414898c8ee",
          "name": "Kaituo Feng",
          "hidden": false
        },
        {
          "_id": "68ec5975cd07fb414898c8ef",
          "name": "Huadai Liu",
          "hidden": false
        },
        {
          "_id": "68ec5975cd07fb414898c8f0",
          "name": "Zhen Ye",
          "hidden": false
        },
        {
          "_id": "68ec5975cd07fb414898c8f1",
          "name": "Rui Liu",
          "hidden": false
        },
        {
          "_id": "68ec5975cd07fb414898c8f2",
          "name": "Yun-Hui Liu",
          "hidden": false
        },
        {
          "_id": "68ec5975cd07fb414898c8f3",
          "name": "Jianan Wang",
          "hidden": false
        },
        {
          "_id": "68ec5975cd07fb414898c8f4",
          "name": "Xiangyu Yue",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-10T17:59:46.000Z",
      "submittedOnDailyAt": "2025-10-13T00:14:31.615Z",
      "title": "SpaceVista: All-Scale Visual Spatial Reasoning from mm to km",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "With the current surge in spatial reasoning explorations, researchers have\nmade significant progress in understanding indoor scenes, but still struggle\nwith diverse applications such as robotics and autonomous driving. This paper\naims to advance all-scale spatial reasoning across diverse scenarios by\ntackling two key challenges: 1) the heavy reliance on indoor 3D scans and\nlabor-intensive manual annotations for dataset curation; 2) the absence of\neffective all-scale scene modeling, which often leads to overfitting to\nindividual scenes. In this paper, we introduce a holistic solution that\nintegrates a structured spatial reasoning knowledge system, scale-aware\nmodeling, and a progressive training paradigm, as the first attempt to broaden\nthe all-scale spatial intelligence of MLLMs to the best of our knowledge. Using\na task-specific, specialist-driven automated pipeline, we curate over 38K video\nscenes across 5 spatial scales to create SpaceVista-1M, a dataset comprising\napproximately 1M spatial QA pairs spanning 19 diverse task types. While\nspecialist models can inject useful domain knowledge, they are not reliable for\nevaluation. We then build an all-scale benchmark with precise annotations by\nmanually recording, retrieving, and assembling video-based data. However, naive\ntraining with SpaceVista-1M often yields suboptimal results due to the\npotential knowledge conflict. Accordingly, we introduce SpaceVista-7B, a\nspatial reasoning model that accepts dense inputs beyond semantics and uses\nscale as an anchor for scale-aware experts and progressive rewards. Finally,\nextensive evaluations across 5 benchmarks, including our SpaceVista-Bench,\ndemonstrate competitive performance, showcasing strong generalization across\nall scales and scenarios. Our dataset, model, and benchmark will be released on\nhttps://peiwensun2000.github.io/mm2km .",
      "upvotes": 15,
      "discussionId": "68ec5975cd07fb414898c8f5",
      "projectPage": "https://peiwensun2000.github.io/mm2km/",
      "githubRepo": "https://github.com/PeiwenSun2000/SpaceVista",
      "ai_summary": "A spatial reasoning model using scale-aware experts and progressive rewards demonstrates competitive performance across diverse tasks and scales using a large, curated dataset.",
      "ai_keywords": [
        "structured spatial reasoning",
        "scale-aware modeling",
        "progressive training",
        "spatial QA pairs",
        "spatial reasoning model",
        "scale-aware experts",
        "progressive rewards",
        "SpaceVista-1M",
        "SpaceVista-7B",
        "SpaceVista-Bench"
      ],
      "githubStars": 13
    },
    "publishedAt": "2025-10-10T13:59:46.000Z",
    "title": "SpaceVista: All-Scale Visual Spatial Reasoning from mm to km",
    "summary": "With the current surge in spatial reasoning explorations, researchers have\nmade significant progress in understanding indoor scenes, but still struggle\nwith diverse applications such as robotics and autonomous driving. This paper\naims to advance all-scale spatial reasoning across diverse scenarios by\ntackling two key challenges: 1) the heavy reliance on indoor 3D scans and\nlabor-intensive manual annotations for dataset curation; 2) the absence of\neffective all-scale scene modeling, which often leads to overfitting to\nindividual scenes. In this paper, we introduce a holistic solution that\nintegrates a structured spatial reasoning knowledge system, scale-aware\nmodeling, and a progressive training paradigm, as the first attempt to broaden\nthe all-scale spatial intelligence of MLLMs to the best of our knowledge. Using\na task-specific, specialist-driven automated pipeline, we curate over 38K video\nscenes across 5 spatial scales to create SpaceVista-1M, a dataset comprising\napproximately 1M spatial QA pairs spanning 19 diverse task types. While\nspecialist models can inject useful domain knowledge, they are not reliable for\nevaluation. We then build an all-scale benchmark with precise annotations by\nmanually recording, retrieving, and assembling video-based data. However, naive\ntraining with SpaceVista-1M often yields suboptimal results due to the\npotential knowledge conflict. Accordingly, we introduce SpaceVista-7B, a\nspatial reasoning model that accepts dense inputs beyond semantics and uses\nscale as an anchor for scale-aware experts and progressive rewards. Finally,\nextensive evaluations across 5 benchmarks, including our SpaceVista-Bench,\ndemonstrate competitive performance, showcasing strong generalization across\nall scales and scenarios. Our dataset, model, and benchmark will be released on\nhttps://peiwensun2000.github.io/mm2km .",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.09606.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 124
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.08457",
      "authors": [
        {
          "_id": "68ebaa94012880a10864bbee",
          "name": "Shuang Chen",
          "hidden": false
        },
        {
          "_id": "68ebaa94012880a10864bbef",
          "name": "Yue Guo",
          "hidden": false
        },
        {
          "_id": "68ebaa94012880a10864bbf0",
          "name": "Yimeng Ye",
          "hidden": false
        },
        {
          "_id": "68ebaa94012880a10864bbf1",
          "name": "Shijue Huang",
          "hidden": false
        },
        {
          "_id": "68ebaa94012880a10864bbf2",
          "name": "Wenbo Hu",
          "hidden": false
        },
        {
          "_id": "68ebaa94012880a10864bbf3",
          "name": "Haoxi Li",
          "hidden": false
        },
        {
          "_id": "68ebaa94012880a10864bbf4",
          "name": "Manyuan Zhang",
          "hidden": false
        },
        {
          "_id": "68ebaa94012880a10864bbf5",
          "name": "Jiayu Chen",
          "hidden": false
        },
        {
          "_id": "68ebaa94012880a10864bbf6",
          "name": "Song Guo",
          "hidden": false
        },
        {
          "_id": "68ebaa94012880a10864bbf7",
          "name": "Nanyun Peng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-09T17:03:28.000Z",
      "submittedOnDailyAt": "2025-10-13T02:25:56.610Z",
      "title": "ARES: Multimodal Adaptive Reasoning via Difficulty-Aware Token-Level\n  Entropy Shaping",
      "submittedOnDailyBy": {
        "_id": "64ce05c631c655ff8a2e183c",
        "avatarUrl": "/avatars/f2de7f8a1348b05f46946085e3e9718e.svg",
        "isPro": false,
        "fullname": "Shijue Huang",
        "user": "JoeYing",
        "type": "user"
      },
      "summary": "Recent advances in multimodal large reasoning models (MLRMs) have\nsubstantially improved their ability to solve complex textual and visual tasks.\nHowever, these models tend to overthink on simple problems, producing\nunnecessarily lengthy reasoning traces, while under-exploring on challenging\nones, leading to missed solutions. To address this imbalance, we propose ARES,\na unified open-source framework for adaptive reasoning that dynamically\nallocates exploration effort based on task difficulty. Our approach is\nmotivated by two key empirical findings: (i) while single-token entropy is\nnoisy, high window-entropy (HWE) tokens (token-level entropies averaged under a\nsliding window) can reliably capture reasoning-critical moments; and (ii)\nreducing HWE usage benefits easy problems, while increasing it is essential for\nsolving hard ones. Building on these insights, ARES introduces a two-stage\ntraining pipeline. In the Adaptive Cold-Start stage, we curate multimodal and\ntextual data paired with reasoning traces of length proportional to problem\ndifficulty, equipping the model with initial difficulty awareness. In the\nsecond stage, we develop Adaptive Entropy Policy Optimization (AEPO), which\nuses HWE tokens as exploration triggers to decide when to explore, and a\nhierarchical entropy reward with dynamic KL control to decide how much to\nexplore. Extensive experiments demonstrate that ARES achieves superior\nperformance and reasoning efficiency across diverse mathematical, logical, and\nmultimodal benchmarks, while closing the gap to leading commercial systems\nunder significantly lower inference costs.",
      "upvotes": 10,
      "discussionId": "68ebaa94012880a10864bbf8",
      "ai_summary": "ARES, a unified framework for adaptive reasoning, dynamically adjusts exploration effort based on task difficulty using high window-entropy tokens and hierarchical entropy rewards, improving performance and efficiency across various benchmarks.",
      "ai_keywords": [
        "multimodal large reasoning models",
        "MLRMs",
        "adaptive reasoning",
        "high window-entropy",
        "HWE",
        "Adaptive Cold-Start",
        "Adaptive Entropy Policy Optimization",
        "AEPO",
        "entropy reward",
        "dynamic KL control"
      ]
    },
    "publishedAt": "2025-10-09T13:03:28.000Z",
    "title": "ARES: Multimodal Adaptive Reasoning via Difficulty-Aware Token-Level\n  Entropy Shaping",
    "summary": "Recent advances in multimodal large reasoning models (MLRMs) have\nsubstantially improved their ability to solve complex textual and visual tasks.\nHowever, these models tend to overthink on simple problems, producing\nunnecessarily lengthy reasoning traces, while under-exploring on challenging\nones, leading to missed solutions. To address this imbalance, we propose ARES,\na unified open-source framework for adaptive reasoning that dynamically\nallocates exploration effort based on task difficulty. Our approach is\nmotivated by two key empirical findings: (i) while single-token entropy is\nnoisy, high window-entropy (HWE) tokens (token-level entropies averaged under a\nsliding window) can reliably capture reasoning-critical moments; and (ii)\nreducing HWE usage benefits easy problems, while increasing it is essential for\nsolving hard ones. Building on these insights, ARES introduces a two-stage\ntraining pipeline. In the Adaptive Cold-Start stage, we curate multimodal and\ntextual data paired with reasoning traces of length proportional to problem\ndifficulty, equipping the model with initial difficulty awareness. In the\nsecond stage, we develop Adaptive Entropy Policy Optimization (AEPO), which\nuses HWE tokens as exploration triggers to decide when to explore, and a\nhierarchical entropy reward with dynamic KL control to decide how much to\nexplore. Extensive experiments demonstrate that ARES achieves superior\nperformance and reasoning efficiency across diverse mathematical, logical, and\nmultimodal benchmarks, while closing the gap to leading commercial systems\nunder significantly lower inference costs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.08457.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ce05c631c655ff8a2e183c",
      "avatarUrl": "/avatars/f2de7f8a1348b05f46946085e3e9718e.svg",
      "fullname": "Shijue Huang",
      "name": "JoeYing",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 13
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.09608",
      "authors": [
        {
          "_id": "68ec5910cd07fb414898c8e1",
          "name": "Ruyi Xu",
          "hidden": false
        },
        {
          "_id": "68ec5910cd07fb414898c8e2",
          "name": "Guangxuan Xiao",
          "hidden": false
        },
        {
          "_id": "68ec5910cd07fb414898c8e3",
          "name": "Yukang Chen",
          "hidden": false
        },
        {
          "_id": "68ec5910cd07fb414898c8e4",
          "name": "Liuning He",
          "hidden": false
        },
        {
          "_id": "68ec5910cd07fb414898c8e5",
          "name": "Kelly Peng",
          "hidden": false
        },
        {
          "_id": "68ec5910cd07fb414898c8e6",
          "name": "Yao Lu",
          "hidden": false
        },
        {
          "_id": "68ec5910cd07fb414898c8e7",
          "name": "Song Han",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-10T17:59:58.000Z",
      "submittedOnDailyAt": "2025-10-13T00:12:40.683Z",
      "title": "StreamingVLM: Real-Time Understanding for Infinite Video Streams",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Vision-language models (VLMs) could power real-time assistants and autonomous\nagents, but they face a critical challenge: understanding near-infinite video\nstreams without escalating latency and memory usage. Processing entire videos\nwith full attention leads to quadratic computational costs and poor performance\non long videos. Meanwhile, simple sliding window methods are also flawed, as\nthey either break coherence or suffer from high latency due to redundant\nrecomputation. In this paper, we introduce StreamingVLM, a model designed for\nreal-time, stable understanding of infinite visual input. Our approach is a\nunified framework that aligns training with streaming inference. During\ninference, we maintain a compact KV cache by reusing states of attention sinks,\na short window of recent vision tokens, and a long window of recent text\ntokens. This streaming ability is instilled via a simple supervised fine-tuning\n(SFT) strategy that applies full attention on short, overlapped video chunks,\nwhich effectively mimics the inference-time attention pattern without training\non prohibitively long contexts. For evaluation, we build Inf-Streams-Eval, a\nnew benchmark with videos averaging over two hours that requires dense,\nper-second alignment between frames and text. On Inf-Streams-Eval, StreamingVLM\nachieves a 66.18% win rate against GPT-4O mini and maintains stable, real-time\nperformance at up to 8 FPS on a single NVIDIA H100. Notably, our SFT strategy\nalso enhances general VQA abilities without any VQA-specific fine-tuning,\nimproving performance on LongVideoBench by +4.30 and OVOBench Realtime by\n+5.96. Code is available at https://github.com/mit-han-lab/streaming-vlm.",
      "upvotes": 9,
      "discussionId": "68ec5910cd07fb414898c8e8",
      "githubRepo": "https://github.com/mit-han-lab/streaming-vlm",
      "ai_summary": "StreamingVLM is a real-time vision-language model that efficiently processes infinite video streams using a compact KV cache and supervised fine-tuning, achieving high performance on long videos and diverse benchmarks.",
      "ai_keywords": [
        "vision-language models",
        "VLMs",
        "real-time assistants",
        "autonomous agents",
        "video streams",
        "quadratic computational costs",
        "sliding window methods",
        "attention sinks",
        "vision tokens",
        "text tokens",
        "supervised fine-tuning",
        "SFT",
        "inference-time attention pattern",
        "Inf-Streams-Eval",
        "win rate",
        "NVIDIA H100",
        "VQA abilities",
        "LongVideoBench",
        "OVOBench Realtime"
      ],
      "githubStars": 40
    },
    "publishedAt": "2025-10-10T13:59:58.000Z",
    "title": "StreamingVLM: Real-Time Understanding for Infinite Video Streams",
    "summary": "Vision-language models (VLMs) could power real-time assistants and autonomous\nagents, but they face a critical challenge: understanding near-infinite video\nstreams without escalating latency and memory usage. Processing entire videos\nwith full attention leads to quadratic computational costs and poor performance\non long videos. Meanwhile, simple sliding window methods are also flawed, as\nthey either break coherence or suffer from high latency due to redundant\nrecomputation. In this paper, we introduce StreamingVLM, a model designed for\nreal-time, stable understanding of infinite visual input. Our approach is a\nunified framework that aligns training with streaming inference. During\ninference, we maintain a compact KV cache by reusing states of attention sinks,\na short window of recent vision tokens, and a long window of recent text\ntokens. This streaming ability is instilled via a simple supervised fine-tuning\n(SFT) strategy that applies full attention on short, overlapped video chunks,\nwhich effectively mimics the inference-time attention pattern without training\non prohibitively long contexts. For evaluation, we build Inf-Streams-Eval, a\nnew benchmark with videos averaging over two hours that requires dense,\nper-second alignment between frames and text. On Inf-Streams-Eval, StreamingVLM\nachieves a 66.18% win rate against GPT-4O mini and maintains stable, real-time\nperformance at up to 8 FPS on a single NVIDIA H100. Notably, our SFT strategy\nalso enhances general VQA abilities without any VQA-specific fine-tuning,\nimproving performance on LongVideoBench by +4.30 and OVOBench Realtime by\n+5.96. Code is available at https://github.com/mit-han-lab/streaming-vlm.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.09608.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 124
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.08696",
      "authors": [
        {
          "_id": "68ec56f8cd07fb414898c8d4",
          "name": "Yunzhen Feng",
          "hidden": false
        },
        {
          "_id": "68ec56f8cd07fb414898c8d5",
          "name": "Parag Jain",
          "hidden": false
        },
        {
          "_id": "68ec56f8cd07fb414898c8d6",
          "name": "Anthony Hartshorn",
          "hidden": false
        },
        {
          "_id": "68ec56f8cd07fb414898c8d7",
          "name": "Yaqi Duan",
          "hidden": false
        },
        {
          "_id": "68ec56f8cd07fb414898c8d8",
          "name": "Julia Kempe",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-09T18:01:44.000Z",
      "submittedOnDailyAt": "2025-10-13T00:04:12.104Z",
      "title": "Don't Waste Mistakes: Leveraging Negative RL-Groups via Confidence\n  Reweighting",
      "submittedOnDailyBy": {
        "_id": "65cbfa6c968742be942e6cba",
        "avatarUrl": "/avatars/1a6cc0983edc28fa92178d3abc283ba1.svg",
        "isPro": false,
        "fullname": "Feng",
        "user": "Yunzhen",
        "type": "user"
      },
      "summary": "Reinforcement learning with verifiable rewards (RLVR) has become a standard\nrecipe for improving large language models (LLMs) on reasoning tasks, with\nGroup Relative Policy Optimization (GRPO) widely used in practice. Yet GRPO\nwastes substantial compute on negative groups: groups in which no sampled\nresponse is correct yield zero advantage and thus no gradient. We ask whether\nnegative groups can be leveraged without extra supervision. Starting from a\nmaximum-likelihood (MLE) objective in reward modeling, we show that the MLE\ngradient is equivalent to a policy gradient for a modified value function. This\nvalue function adds a confidence-weighted penalty on incorrect responses,\nimposing larger penalties on more confident mistakes. We refer to this as\nLikelihood Estimation with Negative Samples\n(LENS). LENS modifies GRPO to assign non-zero, confidence-dependent\nrewards to incorrect generations, making negative groups informative and\nconverting previously wasted samples into useful gradient updates. On the MATH\nbenchmark with Llama-3.1-8B and Qwen-2.5-3B, the proposed variant consistently\noutperforms GRPO baseline, with significant gains on harder items. These\nresults demonstrate a principled and practical way to \"rescue\" negative groups,\nimproving efficiency and performance in RLVR.",
      "upvotes": 8,
      "discussionId": "68ec56f8cd07fb414898c8d9",
      "ai_summary": "LENS modifies GRPO by assigning confidence-dependent rewards to incorrect responses, improving efficiency and performance in reinforcement learning with verifiable rewards.",
      "ai_keywords": [
        "Reinforcement learning with verifiable rewards",
        "RLVR",
        "Group Relative Policy Optimization",
        "GRPO",
        "maximum-likelihood",
        "MLE",
        "policy gradient",
        "value function",
        "confidence-weighted penalty",
        "LENS",
        "MATH benchmark",
        "Llama-3.1-8B",
        "Qwen-2.5-3B"
      ]
    },
    "publishedAt": "2025-10-09T14:01:44.000Z",
    "title": "Don't Waste Mistakes: Leveraging Negative RL-Groups via Confidence\n  Reweighting",
    "summary": "Reinforcement learning with verifiable rewards (RLVR) has become a standard\nrecipe for improving large language models (LLMs) on reasoning tasks, with\nGroup Relative Policy Optimization (GRPO) widely used in practice. Yet GRPO\nwastes substantial compute on negative groups: groups in which no sampled\nresponse is correct yield zero advantage and thus no gradient. We ask whether\nnegative groups can be leveraged without extra supervision. Starting from a\nmaximum-likelihood (MLE) objective in reward modeling, we show that the MLE\ngradient is equivalent to a policy gradient for a modified value function. This\nvalue function adds a confidence-weighted penalty on incorrect responses,\nimposing larger penalties on more confident mistakes. We refer to this as\nLikelihood Estimation with Negative Samples\n(LENS). LENS modifies GRPO to assign non-zero, confidence-dependent\nrewards to incorrect generations, making negative groups informative and\nconverting previously wasted samples into useful gradient updates. On the MATH\nbenchmark with Llama-3.1-8B and Qwen-2.5-3B, the proposed variant consistently\noutperforms GRPO baseline, with significant gains on harder items. These\nresults demonstrate a principled and practical way to \"rescue\" negative groups,\nimproving efficiency and performance in RLVR.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.08696.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65cbfa6c968742be942e6cba",
      "avatarUrl": "/avatars/1a6cc0983edc28fa92178d3abc283ba1.svg",
      "fullname": "Feng",
      "name": "Yunzhen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.09517",
      "authors": [
        {
          "_id": "68ec7109cd07fb414898c9da",
          "name": "Yuchen Lu",
          "hidden": false
        },
        {
          "_id": "68ec7109cd07fb414898c9db",
          "name": "Run Yang",
          "hidden": false
        },
        {
          "_id": "68ec7109cd07fb414898c9dc",
          "name": "Yichen Zhang",
          "hidden": false
        },
        {
          "_id": "68ec7109cd07fb414898c9dd",
          "name": "Shuguang Yu",
          "hidden": false
        },
        {
          "_id": "68ec7109cd07fb414898c9de",
          "name": "Runpeng Dai",
          "hidden": false
        },
        {
          "_id": "68ec7109cd07fb414898c9df",
          "name": "Ziwei Wang",
          "hidden": false
        },
        {
          "_id": "68ec7109cd07fb414898c9e0",
          "name": "Jiayi Xiang",
          "hidden": false
        },
        {
          "_id": "68ec7109cd07fb414898c9e1",
          "name": "Wenxin E",
          "hidden": false
        },
        {
          "_id": "68ec7109cd07fb414898c9e2",
          "name": "Siran Gao",
          "hidden": false
        },
        {
          "_id": "68ec7109cd07fb414898c9e3",
          "name": "Xinyao Ruan",
          "hidden": false
        },
        {
          "_id": "68ec7109cd07fb414898c9e4",
          "name": "Yirui Huang",
          "hidden": false
        },
        {
          "_id": "68ec7109cd07fb414898c9e5",
          "name": "Chenjing Xi",
          "hidden": false
        },
        {
          "_id": "68ec7109cd07fb414898c9e6",
          "name": "Haibo Hu",
          "hidden": false
        },
        {
          "_id": "68ec7109cd07fb414898c9e7",
          "name": "Yueming Fu",
          "hidden": false
        },
        {
          "_id": "68ec7109cd07fb414898c9e8",
          "name": "Qinglan Yu",
          "hidden": false
        },
        {
          "_id": "68ec7109cd07fb414898c9e9",
          "name": "Xiaobing Wei",
          "hidden": false
        },
        {
          "_id": "68ec7109cd07fb414898c9ea",
          "name": "Jiani Gu",
          "hidden": false
        },
        {
          "_id": "68ec7109cd07fb414898c9eb",
          "name": "Rui Sun",
          "hidden": false
        },
        {
          "_id": "68ec7109cd07fb414898c9ec",
          "name": "Jiaxuan Jia",
          "hidden": false
        },
        {
          "_id": "68ec7109cd07fb414898c9ed",
          "name": "Fan Zhou",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/65037565da2d88e201f63b7a/2h1nHiFO7alGKj79s1cyr.png"
      ],
      "publishedAt": "2025-10-10T16:28:43.000Z",
      "submittedOnDailyAt": "2025-10-13T01:54:58.129Z",
      "title": "StatEval: A Comprehensive Benchmark for Large Language Models in\n  Statistics",
      "submittedOnDailyBy": {
        "_id": "65037565da2d88e201f63b7a",
        "avatarUrl": "/avatars/d1b6ce17236360e9583b8bb4cb87e506.svg",
        "isPro": true,
        "fullname": "Runpeng Dai",
        "user": "Leo-Dai",
        "type": "user"
      },
      "summary": "Large language models (LLMs) have demonstrated remarkable advances in\nmathematical and logical reasoning, yet statistics, as a distinct and\nintegrative discipline, remains underexplored in benchmarking efforts. To\naddress this gap, we introduce StatEval, the first comprehensive\nbenchmark dedicated to statistics, spanning both breadth and depth across\ndifficulty levels. StatEval consists of 13,817 foundational problems covering\nundergraduate and graduate curricula, together with 2374 research-level proof\ntasks extracted from leading journals. To construct the benchmark, we design a\nscalable multi-agent pipeline with human-in-the-loop validation that automates\nlarge-scale problem extraction, rewriting, and quality control, while ensuring\nacademic rigor. We further propose a robust evaluation framework tailored to\nboth computational and proof-based tasks, enabling fine-grained assessment of\nreasoning ability. Experimental results reveal that while closed-source models\nsuch as GPT5-mini achieve below 57\\% on research-level problems, with\nopen-source models performing significantly lower. These findings highlight the\nunique challenges of statistical reasoning and the limitations of current LLMs.\nWe expect StatEval to serve as a rigorous benchmark for advancing statistical\nintelligence in large language models. All data and code are available on our\nweb platform: https://stateval.github.io/.",
      "upvotes": 6,
      "discussionId": "68ec7109cd07fb414898c9ee",
      "ai_summary": "StatEval is a comprehensive benchmark for statistical reasoning, covering foundational and research-level problems, and highlights the limitations of current LLMs in this domain.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "mathematical reasoning",
        "logical reasoning",
        "statistics",
        "benchmark",
        "foundational problems",
        "research-level proof tasks",
        "multi-agent pipeline",
        "human-in-the-loop validation",
        "evaluation framework",
        "computational tasks",
        "proof-based tasks",
        "reasoning ability",
        "GPT5-mini",
        "statistical intelligence"
      ],
      "organization": {
        "_id": "641ba0cea63c4e8062373b0c",
        "name": "SUFE",
        "fullname": "Shanghai University of Finance and Economics",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/641b9fdbaebaa27e0752f494/VeZw5Qs9k0KkqujT7aLJQ.jpeg"
      }
    },
    "publishedAt": "2025-10-10T12:28:43.000Z",
    "title": "StatEval: A Comprehensive Benchmark for Large Language Models in\n  Statistics",
    "summary": "Large language models (LLMs) have demonstrated remarkable advances in\nmathematical and logical reasoning, yet statistics, as a distinct and\nintegrative discipline, remains underexplored in benchmarking efforts. To\naddress this gap, we introduce StatEval, the first comprehensive\nbenchmark dedicated to statistics, spanning both breadth and depth across\ndifficulty levels. StatEval consists of 13,817 foundational problems covering\nundergraduate and graduate curricula, together with 2374 research-level proof\ntasks extracted from leading journals. To construct the benchmark, we design a\nscalable multi-agent pipeline with human-in-the-loop validation that automates\nlarge-scale problem extraction, rewriting, and quality control, while ensuring\nacademic rigor. We further propose a robust evaluation framework tailored to\nboth computational and proof-based tasks, enabling fine-grained assessment of\nreasoning ability. Experimental results reveal that while closed-source models\nsuch as GPT5-mini achieve below 57\\% on research-level problems, with\nopen-source models performing significantly lower. These findings highlight the\nunique challenges of statistical reasoning and the limitations of current LLMs.\nWe expect StatEval to serve as a rigorous benchmark for advancing statistical\nintelligence in large language models. All data and code are available on our\nweb platform: https://stateval.github.io/.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/65037565da2d88e201f63b7a/2h1nHiFO7alGKj79s1cyr.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.09517.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65037565da2d88e201f63b7a",
      "avatarUrl": "/avatars/d1b6ce17236360e9583b8bb4cb87e506.svg",
      "fullname": "Runpeng Dai",
      "name": "Leo-Dai",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "organization": {
      "_id": "641ba0cea63c4e8062373b0c",
      "name": "SUFE",
      "fullname": "Shanghai University of Finance and Economics",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/641b9fdbaebaa27e0752f494/VeZw5Qs9k0KkqujT7aLJQ.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.04759",
      "authors": [
        {
          "_id": "68e9249395e8e6771df38ce2",
          "user": {
            "_id": "68de2ad5e795997c50a409eb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68de2ad5e795997c50a409eb/VdVD6njmMlMbZIqe23cxT.jpeg",
            "isPro": false,
            "fullname": "YAN, Chi",
            "user": "yanchi3dv",
            "type": "user"
          },
          "name": "Chi Yan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-11T13:45:21.924Z",
          "hidden": false
        },
        {
          "_id": "68e9249395e8e6771df38ce3",
          "user": {
            "_id": "66feab48651e00e22f33222e",
            "avatarUrl": "/avatars/7344377e2c796c7ec85194bb2fc78521.svg",
            "isPro": false,
            "fullname": "Dan Xu",
            "user": "danxuhk",
            "type": "user"
          },
          "name": "Dan Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-11T13:45:14.344Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-06T12:36:07.000Z",
      "submittedOnDailyAt": "2025-10-13T00:32:19.506Z",
      "title": "Progressive Gaussian Transformer with Anisotropy-aware Sampling for Open\n  Vocabulary Occupancy Prediction",
      "submittedOnDailyBy": {
        "_id": "68de2ad5e795997c50a409eb",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68de2ad5e795997c50a409eb/VdVD6njmMlMbZIqe23cxT.jpeg",
        "isPro": false,
        "fullname": "YAN, Chi",
        "user": "yanchi3dv",
        "type": "user"
      },
      "summary": "The 3D occupancy prediction task has witnessed remarkable progress in recent\nyears, playing a crucial role in vision-based autonomous driving systems. While\ntraditional methods are limited to fixed semantic categories, recent approaches\nhave moved towards predicting text-aligned features to enable open-vocabulary\ntext queries in real-world scenes. However, there exists a trade-off in\ntext-aligned scene modeling: sparse Gaussian representation struggles to\ncapture small objects in the scene, while dense representation incurs\nsignificant computational overhead. To address these limitations, we present\nPG-Occ, an innovative Progressive Gaussian Transformer Framework that enables\nopen-vocabulary 3D occupancy prediction. Our framework employs progressive\nonline densification, a feed-forward strategy that gradually enhances the 3D\nGaussian representation to capture fine-grained scene details. By iteratively\nenhancing the representation, the framework achieves increasingly precise and\ndetailed scene understanding. Another key contribution is the introduction of\nan anisotropy-aware sampling strategy with spatio-temporal fusion, which\nadaptively assigns receptive fields to Gaussians at different scales and\nstages, enabling more effective feature aggregation and richer scene\ninformation capture. Through extensive evaluations, we demonstrate that PG-Occ\nachieves state-of-the-art performance with a relative 14.3% mIoU improvement\nover the previous best performing method. Code and pretrained models will be\nreleased upon publication on our project page:\nhttps://yanchi-3dv.github.io/PG-Occ",
      "upvotes": 6,
      "discussionId": "68e9249495e8e6771df38ce4",
      "projectPage": "https://yanchi-3dv.github.io/PG-Occ/",
      "githubRepo": "https://github.com/yanchi-3dv/PG-Occ",
      "ai_summary": "PG-Occ, a Progressive Gaussian Transformer Framework, enhances 3D occupancy prediction with progressive densification and anisotropy-aware sampling, achieving state-of-the-art performance.",
      "ai_keywords": [
        "3D occupancy prediction",
        "Gaussian representation",
        "dense representation",
        "Progressive Gaussian Transformer Framework",
        "progressive online densification",
        "feed-forward strategy",
        "anisotropy-aware sampling",
        "spatio-temporal fusion",
        "mIoU improvement"
      ],
      "githubStars": 13
    },
    "publishedAt": "2025-10-06T08:36:07.000Z",
    "title": "Progressive Gaussian Transformer with Anisotropy-aware Sampling for Open\n  Vocabulary Occupancy Prediction",
    "summary": "The 3D occupancy prediction task has witnessed remarkable progress in recent\nyears, playing a crucial role in vision-based autonomous driving systems. While\ntraditional methods are limited to fixed semantic categories, recent approaches\nhave moved towards predicting text-aligned features to enable open-vocabulary\ntext queries in real-world scenes. However, there exists a trade-off in\ntext-aligned scene modeling: sparse Gaussian representation struggles to\ncapture small objects in the scene, while dense representation incurs\nsignificant computational overhead. To address these limitations, we present\nPG-Occ, an innovative Progressive Gaussian Transformer Framework that enables\nopen-vocabulary 3D occupancy prediction. Our framework employs progressive\nonline densification, a feed-forward strategy that gradually enhances the 3D\nGaussian representation to capture fine-grained scene details. By iteratively\nenhancing the representation, the framework achieves increasingly precise and\ndetailed scene understanding. Another key contribution is the introduction of\nan anisotropy-aware sampling strategy with spatio-temporal fusion, which\nadaptively assigns receptive fields to Gaussians at different scales and\nstages, enabling more effective feature aggregation and richer scene\ninformation capture. Through extensive evaluations, we demonstrate that PG-Occ\nachieves state-of-the-art performance with a relative 14.3% mIoU improvement\nover the previous best performing method. Code and pretrained models will be\nreleased upon publication on our project page:\nhttps://yanchi-3dv.github.io/PG-Occ",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.04759.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "68de2ad5e795997c50a409eb",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68de2ad5e795997c50a409eb/VdVD6njmMlMbZIqe23cxT.jpeg",
      "fullname": "YAN, Chi",
      "name": "yanchi3dv",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.09510",
      "authors": [
        {
          "_id": "68ec7396cd07fb414898ca08",
          "name": "Siyue Zhang",
          "hidden": false
        },
        {
          "_id": "68ec7396cd07fb414898ca09",
          "name": "Yuan Gao",
          "hidden": false
        },
        {
          "_id": "68ec7396cd07fb414898ca0a",
          "name": "Xiao Zhou",
          "hidden": false
        },
        {
          "_id": "68ec7396cd07fb414898ca0b",
          "name": "Yilun Zhao",
          "hidden": false
        },
        {
          "_id": "68ec7396cd07fb414898ca0c",
          "name": "Tingyu Song",
          "hidden": false
        },
        {
          "_id": "68ec7396cd07fb414898ca0d",
          "name": "Arman Cohan",
          "hidden": false
        },
        {
          "_id": "68ec7396cd07fb414898ca0e",
          "name": "Anh Tuan Luu",
          "hidden": false
        },
        {
          "_id": "68ec7396cd07fb414898ca0f",
          "name": "Chen Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-10T16:14:56.000Z",
      "submittedOnDailyAt": "2025-10-13T02:06:35.111Z",
      "title": "MRMR: A Realistic and Expert-Level Multidisciplinary Benchmark for\n  Reasoning-Intensive Multimodal Retrieval",
      "submittedOnDailyBy": {
        "_id": "638f1803c67af472d317a922",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638f1803c67af472d317a922/9BMVXqHa-AsdZPmBprcbd.jpeg",
        "isPro": false,
        "fullname": "siyue zhang",
        "user": "siyue",
        "type": "user"
      },
      "summary": "We introduce MRMR, the first expert-level multidisciplinary multimodal\nretrieval benchmark requiring intensive reasoning. MRMR contains 1,502 queries\nspanning 23 domains, with positive documents carefully verified by human\nexperts. Compared to prior benchmarks, MRMR introduces three key advancements.\nFirst, it challenges retrieval systems across diverse areas of expertise,\nenabling fine-grained model comparison across domains. Second, queries are\nreasoning-intensive, with images requiring deeper interpretation such as\ndiagnosing microscopic slides. We further introduce Contradiction Retrieval, a\nnovel task requiring models to identify conflicting concepts. Finally, queries\nand documents are constructed as image-text interleaved sequences. Unlike\nearlier benchmarks restricted to single images or unimodal documents, MRMR\noffers a realistic setting with multi-image queries and mixed-modality corpus\ndocuments. We conduct an extensive evaluation of 4 categories of multimodal\nretrieval systems and 14 frontier models on MRMR. The text embedding model\nQwen3-Embedding with LLM-generated image captions achieves the highest\nperformance, highlighting substantial room for improving multimodal retrieval\nmodels. Although latest multimodal models such as Ops-MM-Embedding perform\ncompetitively on expert-domain queries, they fall short on reasoning-intensive\ntasks. We believe that MRMR paves the way for advancing multimodal retrieval in\nmore realistic and challenging scenarios.",
      "upvotes": 5,
      "discussionId": "68ec7397cd07fb414898ca10",
      "ai_summary": "MRMR is a benchmark for expert-level multidisciplinary multimodal retrieval that includes reasoning-intensive tasks, contradiction retrieval, and image-text interleaved sequences, highlighting the need for improved multimodal models.",
      "ai_keywords": [
        "multimodal retrieval",
        "expert-level",
        "multidisciplinary",
        "reasoning-intensive",
        "Contradiction Retrieval",
        "image-text interleaved",
        "multimodal models",
        "text embedding",
        "LLM-generated image captions",
        "Ops-MM-Embedding"
      ]
    },
    "publishedAt": "2025-10-10T12:14:56.000Z",
    "title": "MRMR: A Realistic and Expert-Level Multidisciplinary Benchmark for\n  Reasoning-Intensive Multimodal Retrieval",
    "summary": "We introduce MRMR, the first expert-level multidisciplinary multimodal\nretrieval benchmark requiring intensive reasoning. MRMR contains 1,502 queries\nspanning 23 domains, with positive documents carefully verified by human\nexperts. Compared to prior benchmarks, MRMR introduces three key advancements.\nFirst, it challenges retrieval systems across diverse areas of expertise,\nenabling fine-grained model comparison across domains. Second, queries are\nreasoning-intensive, with images requiring deeper interpretation such as\ndiagnosing microscopic slides. We further introduce Contradiction Retrieval, a\nnovel task requiring models to identify conflicting concepts. Finally, queries\nand documents are constructed as image-text interleaved sequences. Unlike\nearlier benchmarks restricted to single images or unimodal documents, MRMR\noffers a realistic setting with multi-image queries and mixed-modality corpus\ndocuments. We conduct an extensive evaluation of 4 categories of multimodal\nretrieval systems and 14 frontier models on MRMR. The text embedding model\nQwen3-Embedding with LLM-generated image captions achieves the highest\nperformance, highlighting substantial room for improving multimodal retrieval\nmodels. Although latest multimodal models such as Ops-MM-Embedding perform\ncompetitively on expert-domain queries, they fall short on reasoning-intensive\ntasks. We believe that MRMR paves the way for advancing multimodal retrieval in\nmore realistic and challenging scenarios.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.09510.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "638f1803c67af472d317a922",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638f1803c67af472d317a922/9BMVXqHa-AsdZPmBprcbd.jpeg",
      "fullname": "siyue zhang",
      "name": "siyue",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.09577",
      "authors": [
        {
          "_id": "68ec56edcd07fb414898c8c9",
          "name": "Xiao Yu",
          "hidden": false
        },
        {
          "_id": "68ec56edcd07fb414898c8ca",
          "name": "Baolin Peng",
          "hidden": false
        },
        {
          "_id": "68ec56edcd07fb414898c8cb",
          "name": "Michel Galley",
          "hidden": false
        },
        {
          "_id": "68ec56edcd07fb414898c8cc",
          "name": "Hao Cheng",
          "hidden": false
        },
        {
          "_id": "68ec56edcd07fb414898c8cd",
          "name": "Qianhui Wu",
          "hidden": false
        },
        {
          "_id": "68ec56edcd07fb414898c8ce",
          "name": "Janardhan Kulkarni",
          "hidden": false
        },
        {
          "_id": "68ec56edcd07fb414898c8cf",
          "name": "Suman Nath",
          "hidden": false
        },
        {
          "_id": "68ec56edcd07fb414898c8d0",
          "name": "Zhou Yu",
          "hidden": false
        },
        {
          "_id": "68ec56edcd07fb414898c8d1",
          "name": "Jianfeng Gao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-10T17:30:18.000Z",
      "submittedOnDailyAt": "2025-10-13T00:04:14.137Z",
      "title": "Dyna-Mind: Learning to Simulate from Experience for Better AI Agents",
      "submittedOnDailyBy": {
        "_id": "6234fd736dcfc5fe9f5b8601",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1647639915850-noauth.jpeg",
        "isPro": false,
        "fullname": "Xiao Yu",
        "user": "jasonyux",
        "type": "user"
      },
      "summary": "Reasoning models have recently shown remarkable progress in domains such as\nmath and coding. However, their expert-level abilities in math and coding\ncontrast sharply with their performance in long-horizon, interactive tasks such\nas web navigation and computer/phone-use. Inspired by literature on human\ncognition, we argue that current AI agents need ''vicarious trial and error'' -\nthe capacity to mentally simulate alternative futures before acting - in order\nto enhance their understanding and performance in complex interactive\nenvironments. We introduce Dyna-Mind, a two-stage training framework that\nexplicitly teaches (V)LM agents to integrate such simulation into their\nreasoning. In stage 1, we introduce Reasoning with Simulations (ReSim), which\ntrains the agent to generate structured reasoning traces from expanded search\ntrees built from real experience gathered through environment interactions.\nReSim thus grounds the agent's reasoning in faithful world dynamics and equips\nit with the ability to anticipate future states in its reasoning. In stage 2,\nwe propose Dyna-GRPO, an online reinforcement learning method to further\nstrengthen the agent's simulation and decision-making ability by using both\noutcome rewards and intermediate states as feedback from real rollouts.\nExperiments on two synthetic benchmarks (Sokoban and ALFWorld) and one\nrealistic benchmark (AndroidWorld) demonstrate that (1) ReSim effectively\ninfuses simulation ability into AI agents, and (2) Dyna-GRPO leverages outcome\nand interaction-level signals to learn better policies for long-horizon,\nplanning-intensive tasks. Together, these results highlight the central role of\nsimulation in enabling AI agents to reason, plan, and act more effectively in\nthe ever more challenging environments.",
      "upvotes": 4,
      "discussionId": "68ec56edcd07fb414898c8d2",
      "ai_summary": "Introducing Dyna-Mind, a two-stage training framework that enhances AI agents' reasoning and planning abilities through simulation, leading to improved performance in complex interactive environments.",
      "ai_keywords": [
        "vicarious trial and error",
        "Dyna-Mind",
        "Reasoning with Simulations",
        "ReSim",
        "Dyna-GRPO",
        "online reinforcement learning",
        "simulation ability",
        "reasoning",
        "planning",
        "Sokoban",
        "ALFWorld",
        "AndroidWorld"
      ]
    },
    "publishedAt": "2025-10-10T13:30:18.000Z",
    "title": "Dyna-Mind: Learning to Simulate from Experience for Better AI Agents",
    "summary": "Reasoning models have recently shown remarkable progress in domains such as\nmath and coding. However, their expert-level abilities in math and coding\ncontrast sharply with their performance in long-horizon, interactive tasks such\nas web navigation and computer/phone-use. Inspired by literature on human\ncognition, we argue that current AI agents need ''vicarious trial and error'' -\nthe capacity to mentally simulate alternative futures before acting - in order\nto enhance their understanding and performance in complex interactive\nenvironments. We introduce Dyna-Mind, a two-stage training framework that\nexplicitly teaches (V)LM agents to integrate such simulation into their\nreasoning. In stage 1, we introduce Reasoning with Simulations (ReSim), which\ntrains the agent to generate structured reasoning traces from expanded search\ntrees built from real experience gathered through environment interactions.\nReSim thus grounds the agent's reasoning in faithful world dynamics and equips\nit with the ability to anticipate future states in its reasoning. In stage 2,\nwe propose Dyna-GRPO, an online reinforcement learning method to further\nstrengthen the agent's simulation and decision-making ability by using both\noutcome rewards and intermediate states as feedback from real rollouts.\nExperiments on two synthetic benchmarks (Sokoban and ALFWorld) and one\nrealistic benchmark (AndroidWorld) demonstrate that (1) ReSim effectively\ninfuses simulation ability into AI agents, and (2) Dyna-GRPO leverages outcome\nand interaction-level signals to learn better policies for long-horizon,\nplanning-intensive tasks. Together, these results highlight the central role of\nsimulation in enabling AI agents to reason, plan, and act more effectively in\nthe ever more challenging environments.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.09577.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6234fd736dcfc5fe9f5b8601",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1647639915850-noauth.jpeg",
      "fullname": "Xiao Yu",
      "name": "jasonyux",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.08697",
      "authors": [
        {
          "_id": "68ec5e33cd07fb414898c90f",
          "name": "Terry Yue Zhuo",
          "hidden": false
        },
        {
          "_id": "68ec5e33cd07fb414898c910",
          "name": "Xiaolong Jin",
          "hidden": false
        },
        {
          "_id": "68ec5e33cd07fb414898c911",
          "name": "Hange Liu",
          "hidden": false
        },
        {
          "_id": "68ec5e33cd07fb414898c912",
          "name": "Juyong Jiang",
          "hidden": false
        },
        {
          "_id": "68ec5e33cd07fb414898c913",
          "name": "Tianyang Liu",
          "hidden": false
        },
        {
          "_id": "68ec5e33cd07fb414898c914",
          "name": "Chen Gong",
          "hidden": false
        },
        {
          "_id": "68ec5e33cd07fb414898c915",
          "name": "Bhupesh Bishnoi",
          "hidden": false
        },
        {
          "_id": "68ec5e33cd07fb414898c916",
          "name": "Vaisakhi Mishra",
          "hidden": false
        },
        {
          "_id": "68ec5e33cd07fb414898c917",
          "name": "Marek Suppa",
          "hidden": false
        },
        {
          "_id": "68ec5e33cd07fb414898c918",
          "name": "Noah Ziems",
          "hidden": false
        },
        {
          "_id": "68ec5e33cd07fb414898c919",
          "name": "Saiteja Utpala",
          "hidden": false
        },
        {
          "_id": "68ec5e33cd07fb414898c91a",
          "name": "Ming Xu",
          "hidden": false
        },
        {
          "_id": "68ec5e33cd07fb414898c91b",
          "name": "Guangyu Song",
          "hidden": false
        },
        {
          "_id": "68ec5e33cd07fb414898c91c",
          "name": "Kaixin Li",
          "hidden": false
        },
        {
          "_id": "68ec5e33cd07fb414898c91d",
          "name": "Yuhan Cao",
          "hidden": false
        },
        {
          "_id": "68ec5e33cd07fb414898c91e",
          "name": "Bo Liu",
          "hidden": false
        },
        {
          "_id": "68ec5e33cd07fb414898c91f",
          "name": "Zheng Liu",
          "hidden": false
        },
        {
          "_id": "68ec5e33cd07fb414898c920",
          "name": "Sabina Abdurakhmanova",
          "hidden": false
        },
        {
          "_id": "68ec5e33cd07fb414898c921",
          "name": "Wenhao Yu",
          "hidden": false
        },
        {
          "_id": "68ec5e33cd07fb414898c922",
          "name": "Mengzhao Jia",
          "hidden": false
        },
        {
          "_id": "68ec5e33cd07fb414898c923",
          "name": "Jihan Yao",
          "hidden": false
        },
        {
          "_id": "68ec5e33cd07fb414898c924",
          "name": "Kenneth Hamilton",
          "hidden": false
        },
        {
          "_id": "68ec5e33cd07fb414898c925",
          "name": "Kumar Shridhar",
          "hidden": false
        },
        {
          "_id": "68ec5e33cd07fb414898c926",
          "name": "Minh Chien Vu",
          "hidden": false
        },
        {
          "_id": "68ec5e33cd07fb414898c927",
          "name": "Dingmin Wang",
          "hidden": false
        },
        {
          "_id": "68ec5e33cd07fb414898c928",
          "name": "Jiawei Liu",
          "hidden": false
        },
        {
          "_id": "68ec5e33cd07fb414898c929",
          "name": "Zijian Wang",
          "hidden": false
        },
        {
          "_id": "68ec5e33cd07fb414898c92a",
          "name": "Qian Liu",
          "hidden": false
        },
        {
          "_id": "68ec5e33cd07fb414898c92b",
          "name": "Binyuan Hui",
          "hidden": false
        },
        {
          "_id": "68ec5e33cd07fb414898c92c",
          "name": "Meg Risdal",
          "hidden": false
        },
        {
          "_id": "68ec5e33cd07fb414898c92d",
          "name": "Ahsen Khaliq",
          "hidden": false
        },
        {
          "_id": "68ec5e33cd07fb414898c92e",
          "name": "Atin Sood",
          "hidden": false
        },
        {
          "_id": "68ec5e33cd07fb414898c92f",
          "name": "Zhenchang Xing",
          "hidden": false
        },
        {
          "_id": "68ec5e33cd07fb414898c930",
          "name": "Wasi Uddin Ahmad",
          "hidden": false
        },
        {
          "_id": "68ec5e33cd07fb414898c931",
          "name": "John Grundy",
          "hidden": false
        },
        {
          "_id": "68ec5e33cd07fb414898c932",
          "name": "David Lo",
          "hidden": false
        },
        {
          "_id": "68ec5e33cd07fb414898c933",
          "name": "Banghua Zhu",
          "hidden": false
        },
        {
          "_id": "68ec5e33cd07fb414898c934",
          "name": "Xiaoning Du",
          "hidden": false
        },
        {
          "_id": "68ec5e33cd07fb414898c935",
          "name": "Torsten Scholak",
          "hidden": false
        },
        {
          "_id": "68ec5e33cd07fb414898c936",
          "name": "Leandro von Werra",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-09T18:01:47.000Z",
      "submittedOnDailyAt": "2025-10-13T00:34:36.848Z",
      "title": "BigCodeArena: Unveiling More Reliable Human Preferences in Code\n  Generation via Execution",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Crowdsourced model evaluation platforms, such as Chatbot Arena, enable\nreal-time evaluation from human perspectives to assess the quality of model\nresponses. In the coding domain, manually examining the quality of\nLLM-generated content is extremely challenging, as it requires understanding\nlong chunks of raw code and deliberately simulating code execution. To this\nend, we introduce BigCodeArena, an open human evaluation platform for code\ngeneration backed by a comprehensive and on-the-fly execution environment.\nBuilt on top of Chatbot Arena, BigCodeArena enables the execution of\nLLM-generated code and allows humans to interact with the execution process and\noutcomes. We collected over 14,000 raw code-centric conversation sessions\nacross 10 widely used LLMs, spanning 10 languages and 8 types of execution\nenvironments. Among these conversations, we identified more than 4,700\nmulti-turn samples with pairwise human preferences. Further analysis uncovers\nunderexplored preferences of LLMs in fine-grained domains characterized by\ntasks, languages, and frameworks. To systematically examine code understanding\nand generation capabilities of frontier LLMs, we curated two benchmarks based\non the collected data, namely BigCodeReward and AutoCodeArena. For\nBigCodeReward, we post-processed the 4,700 conversations and evaluated the\nconsistency between reward models and human preferences. The evaluation shows\nthat most LLMs have superior performance in judging coding preferences when the\nexecution results are available. Inspired by these findings, we propose\nAutoCodeArena, an automatic Elo rating benchmark designed to assess the coding\nquality of LLMs without human involvement. We find that proprietary LLMs like\nGPT-5, Claude-Sonnet-4, and Claude-Opus-4 still lead in code generation\nperformance among recent emerging models.",
      "upvotes": 4,
      "discussionId": "68ec5e33cd07fb414898c937",
      "ai_summary": "BigCodeArena is an open human evaluation platform for code generation that enables real-time execution and interaction, revealing preferences and capabilities of LLMs in coding tasks.",
      "ai_keywords": [
        "Chatbot Arena",
        "BigCodeArena",
        "LLM-generated code",
        "code execution",
        "human evaluation",
        "BigCodeReward",
        "AutoCodeArena",
        "Elo rating benchmark",
        "code understanding",
        "code generation"
      ]
    },
    "publishedAt": "2025-10-09T14:01:47.000Z",
    "title": "BigCodeArena: Unveiling More Reliable Human Preferences in Code\n  Generation via Execution",
    "summary": "Crowdsourced model evaluation platforms, such as Chatbot Arena, enable\nreal-time evaluation from human perspectives to assess the quality of model\nresponses. In the coding domain, manually examining the quality of\nLLM-generated content is extremely challenging, as it requires understanding\nlong chunks of raw code and deliberately simulating code execution. To this\nend, we introduce BigCodeArena, an open human evaluation platform for code\ngeneration backed by a comprehensive and on-the-fly execution environment.\nBuilt on top of Chatbot Arena, BigCodeArena enables the execution of\nLLM-generated code and allows humans to interact with the execution process and\noutcomes. We collected over 14,000 raw code-centric conversation sessions\nacross 10 widely used LLMs, spanning 10 languages and 8 types of execution\nenvironments. Among these conversations, we identified more than 4,700\nmulti-turn samples with pairwise human preferences. Further analysis uncovers\nunderexplored preferences of LLMs in fine-grained domains characterized by\ntasks, languages, and frameworks. To systematically examine code understanding\nand generation capabilities of frontier LLMs, we curated two benchmarks based\non the collected data, namely BigCodeReward and AutoCodeArena. For\nBigCodeReward, we post-processed the 4,700 conversations and evaluated the\nconsistency between reward models and human preferences. The evaluation shows\nthat most LLMs have superior performance in judging coding preferences when the\nexecution results are available. Inspired by these findings, we propose\nAutoCodeArena, an automatic Elo rating benchmark designed to assess the coding\nquality of LLMs without human involvement. We find that proprietary LLMs like\nGPT-5, Claude-Sonnet-4, and Claude-Opus-4 still lead in code generation\nperformance among recent emerging models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.08697.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 124
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.08047",
      "authors": [
        {
          "_id": "68ec5661cd07fb414898c8b7",
          "name": "Yi-Cheng Lin",
          "hidden": false
        },
        {
          "_id": "68ec5661cd07fb414898c8b8",
          "name": "Yu-Hsuan Li Liang",
          "hidden": false
        },
        {
          "_id": "68ec5661cd07fb414898c8b9",
          "name": "Hsuan Su",
          "hidden": false
        },
        {
          "_id": "68ec5661cd07fb414898c8ba",
          "name": "Tzu-Quan Lin",
          "hidden": false
        },
        {
          "_id": "68ec5661cd07fb414898c8bb",
          "name": "Shang-Tse Chen",
          "hidden": false
        },
        {
          "_id": "68ec5661cd07fb414898c8bc",
          "name": "Yun-Nung Chen",
          "hidden": false
        },
        {
          "_id": "68ec5661cd07fb414898c8bd",
          "name": "Hung-yi Lee",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-09T10:31:47.000Z",
      "submittedOnDailyAt": "2025-10-13T00:01:41.326Z",
      "title": "Pseudo2Real: Task Arithmetic for Pseudo-Label Correction in Automatic\n  Speech Recognition",
      "submittedOnDailyBy": {
        "_id": "608abf1272b50b02c4b02865",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1619708309549-608abf1272b50b02c4b02865.jpeg",
        "isPro": false,
        "fullname": "Hsuan Su",
        "user": "jacksukk",
        "type": "user"
      },
      "summary": "Robust ASR under domain shift is crucial because real-world systems encounter\nunseen accents and domains with limited labeled data. Although pseudo-labeling\noffers a practical workaround, it often introduces systematic, accent-specific\nerrors that filtering fails to fix. We ask: How can we correct these recurring\nbiases without target ground truth? We propose a simple parameter-space\ncorrection: in a source domain containing both real and pseudo-labeled data,\ntwo ASR models are fine-tuned from the same initialization, one on ground-truth\nlabels and the other on pseudo-labels, and their weight difference forms a\ncorrection vector that captures pseudo-label biases. When applied to a\npseudo-labeled target model, this vector enhances recognition, achieving up to\na 35% relative Word Error Rate (WER) reduction on AfriSpeech-200 across ten\nAfrican accents with the Whisper tiny model.",
      "upvotes": 4,
      "discussionId": "68ec5662cd07fb414898c8be",
      "ai_summary": "A parameter-space correction method reduces Word Error Rate in ASR systems by addressing pseudo-label biases without target ground truth.",
      "ai_keywords": [
        "ASR",
        "domain shift",
        "pseudo-labeling",
        "parameter-space correction",
        "fine-tuning",
        "correction vector",
        "Word Error Rate (WER)",
        "AfriSpeech-200",
        "Whisper tiny model"
      ]
    },
    "publishedAt": "2025-10-09T06:31:47.000Z",
    "title": "Pseudo2Real: Task Arithmetic for Pseudo-Label Correction in Automatic\n  Speech Recognition",
    "summary": "Robust ASR under domain shift is crucial because real-world systems encounter\nunseen accents and domains with limited labeled data. Although pseudo-labeling\noffers a practical workaround, it often introduces systematic, accent-specific\nerrors that filtering fails to fix. We ask: How can we correct these recurring\nbiases without target ground truth? We propose a simple parameter-space\ncorrection: in a source domain containing both real and pseudo-labeled data,\ntwo ASR models are fine-tuned from the same initialization, one on ground-truth\nlabels and the other on pseudo-labels, and their weight difference forms a\ncorrection vector that captures pseudo-label biases. When applied to a\npseudo-labeled target model, this vector enhances recognition, achieving up to\na 35% relative Word Error Rate (WER) reduction on AfriSpeech-200 across ten\nAfrican accents with the Whisper tiny model.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.08047.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "608abf1272b50b02c4b02865",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1619708309549-608abf1272b50b02c4b02865.jpeg",
      "fullname": "Hsuan Su",
      "name": "jacksukk",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.07745",
      "authors": [
        {
          "_id": "68ec636dcd07fb414898c981",
          "name": "Runyang You",
          "hidden": false
        },
        {
          "_id": "68ec636dcd07fb414898c982",
          "name": "Yongqi Li",
          "hidden": false
        },
        {
          "_id": "68ec636dcd07fb414898c983",
          "name": "Meng Liu",
          "hidden": false
        },
        {
          "_id": "68ec636dcd07fb414898c984",
          "name": "Wenjie Wang",
          "hidden": false
        },
        {
          "_id": "68ec636dcd07fb414898c985",
          "name": "Liqiang Nie",
          "hidden": false
        },
        {
          "_id": "68ec636dcd07fb414898c986",
          "name": "Wenjie Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-09T03:33:00.000Z",
      "submittedOnDailyAt": "2025-10-13T01:01:52.962Z",
      "title": "Parallel Test-Time Scaling for Latent Reasoning Models",
      "submittedOnDailyBy": {
        "_id": "64e14c5b12a5504dda70e60d",
        "avatarUrl": "/avatars/944b486bb037364ef7d9d2c826526708.svg",
        "isPro": false,
        "fullname": "Runyang",
        "user": "dd101bb",
        "type": "user"
      },
      "summary": "Parallel test-time scaling (TTS) is a pivotal approach for enhancing large\nlanguage models (LLMs), typically by sampling multiple token-based\nchains-of-thought in parallel and aggregating outcomes through voting or\nsearch. Recent advances in latent reasoning, where intermediate reasoning\nunfolds in continuous vector spaces, offer a more efficient alternative to\nexplicit Chain-of-Thought, yet whether such latent models can similarly benefit\nfrom parallel TTS remains open, mainly due to the absence of sampling\nmechanisms in continuous space, and the lack of probabilistic signals for\nadvanced trajectory aggregation. \\ This work enables parallel TTS for latent\nreasoning models by addressing the above issues. For sampling, we introduce two\nuncertainty-inspired stochastic strategies: Monte Carlo Dropout and Additive\nGaussian Noise. For aggregation, we design a Latent Reward Model (LatentRM)\ntrained with step-wise contrastive objective to score and guide latent\nreasoning. Extensive experiments and visualization analyses show that both\nsampling strategies scale effectively with compute and exhibit distinct\nexploration dynamics, while LatentRM enables effective trajectory selection.\nTogether, our explorations open a new direction for scalable inference in\ncontinuous spaces. Code released at https://github.com/YRYangang/LatentTTS.",
      "upvotes": 4,
      "discussionId": "68ec636ecd07fb414898c987",
      "githubRepo": "https://github.com/YRYangang/LatentTTS",
      "ai_summary": "Parallel test-time scaling is enabled for latent reasoning models using uncertainty-inspired sampling strategies and a Latent Reward Model for effective trajectory selection.",
      "ai_keywords": [
        "Parallel test-time scaling",
        "large language models",
        "token-based chains-of-thought",
        "latent reasoning",
        "continuous vector spaces",
        "Monte Carlo Dropout",
        "Additive Gaussian Noise",
        "Latent Reward Model",
        "step-wise contrastive objective",
        "scalable inference"
      ],
      "githubStars": 0,
      "organization": {
        "_id": "646ecc368d316fde87b3b6e3",
        "name": "PolyUHK",
        "fullname": "The Hong Kong Polytechnic University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/646ecbc0cbb7bb996513e298/Akb4zKqIP9kb9PQoUPUmj.jpeg"
      }
    },
    "publishedAt": "2025-10-08T23:33:00.000Z",
    "title": "Parallel Test-Time Scaling for Latent Reasoning Models",
    "summary": "Parallel test-time scaling (TTS) is a pivotal approach for enhancing large\nlanguage models (LLMs), typically by sampling multiple token-based\nchains-of-thought in parallel and aggregating outcomes through voting or\nsearch. Recent advances in latent reasoning, where intermediate reasoning\nunfolds in continuous vector spaces, offer a more efficient alternative to\nexplicit Chain-of-Thought, yet whether such latent models can similarly benefit\nfrom parallel TTS remains open, mainly due to the absence of sampling\nmechanisms in continuous space, and the lack of probabilistic signals for\nadvanced trajectory aggregation. \\ This work enables parallel TTS for latent\nreasoning models by addressing the above issues. For sampling, we introduce two\nuncertainty-inspired stochastic strategies: Monte Carlo Dropout and Additive\nGaussian Noise. For aggregation, we design a Latent Reward Model (LatentRM)\ntrained with step-wise contrastive objective to score and guide latent\nreasoning. Extensive experiments and visualization analyses show that both\nsampling strategies scale effectively with compute and exhibit distinct\nexploration dynamics, while LatentRM enables effective trajectory selection.\nTogether, our explorations open a new direction for scalable inference in\ncontinuous spaces. Code released at https://github.com/YRYangang/LatentTTS.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.07745.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64e14c5b12a5504dda70e60d",
      "avatarUrl": "/avatars/944b486bb037364ef7d9d2c826526708.svg",
      "fullname": "Runyang",
      "name": "dd101bb",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "646ecc368d316fde87b3b6e3",
      "name": "PolyUHK",
      "fullname": "The Hong Kong Polytechnic University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/646ecbc0cbb7bb996513e298/Akb4zKqIP9kb9PQoUPUmj.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.09592",
      "authors": [
        {
          "_id": "68ec5bc2cd07fb414898c8f7",
          "name": "Donghang Wu",
          "hidden": false
        },
        {
          "_id": "68ec5bc2cd07fb414898c8f8",
          "name": "Haoyang Zhang",
          "hidden": false
        },
        {
          "_id": "68ec5bc2cd07fb414898c8f9",
          "name": "Jun Chen",
          "hidden": false
        },
        {
          "_id": "68ec5bc2cd07fb414898c8fa",
          "name": "Xiangyu",
          "hidden": false
        },
        {
          "_id": "68ec5bc2cd07fb414898c8fb",
          "name": "Zhang",
          "hidden": false
        },
        {
          "_id": "68ec5bc2cd07fb414898c8fc",
          "name": "Hexin Liu",
          "hidden": false
        },
        {
          "_id": "68ec5bc2cd07fb414898c8fd",
          "name": "Eng Siong Chng",
          "hidden": false
        },
        {
          "_id": "68ec5bc2cd07fb414898c8fe",
          "name": "Fei Tian",
          "hidden": false
        },
        {
          "_id": "68ec5bc2cd07fb414898c8ff",
          "name": "Xuerui Yang",
          "hidden": false
        },
        {
          "_id": "68ec5bc2cd07fb414898c900",
          "name": "Xiangyu Zhang",
          "hidden": false
        },
        {
          "_id": "68ec5bc2cd07fb414898c901",
          "name": "Daxin Jiang",
          "hidden": false
        },
        {
          "_id": "68ec5bc2cd07fb414898c902",
          "name": "Gang Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-10T17:50:59.000Z",
      "submittedOnDailyAt": "2025-10-13T00:24:12.723Z",
      "title": "Mind-Paced Speaking: A Dual-Brain Approach to Real-Time Reasoning in\n  Spoken Language Models",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Real-time Spoken Language Models (SLMs) struggle to leverage Chain-of-Thought\n(CoT) reasoning due to the prohibitive latency of generating the entire thought\nprocess sequentially. Enabling SLMs to think while speaking, similar to humans,\nis attracting increasing attention. We present, for the first time, Mind-Paced\nSpeaking (MPS), a brain-inspired framework that enables high-fidelity,\nreal-time reasoning. Similar to how humans utilize distinct brain regions for\nthinking and responding, we propose a novel dual-brain approach, employing a\n\"Formulation Brain\" for high-level reasoning to pace and guide a separate\n\"Articulation Brain\" for fluent speech generation. This division of labor\neliminates mode-switching, preserving the integrity of the reasoning process.\nExperiments show that MPS significantly outperforms existing\nthink-while-speaking methods and achieves reasoning performance comparable to\nmodels that pre-compute the full CoT before speaking, while drastically\nreducing latency. Under a zero-latency configuration, the proposed method\nachieves an accuracy of 92.8% on the mathematical reasoning task Spoken-MQA and\nattains a score of 82.5 on the speech conversation task URO-Bench. Our work\neffectively bridges the gap between high-quality reasoning and real-time\ninteraction.",
      "upvotes": 3,
      "discussionId": "68ec5bc3cd07fb414898c903",
      "githubRepo": "https://github.com/stepfun-ai/Step-MPS",
      "ai_summary": "Mind-Paced Speaking (MPS) is a brain-inspired framework that enables real-time reasoning and fluent speech generation by dividing the process into a \"Formulation Brain\" for reasoning and an \"Articulation Brain\" for speech, achieving high accuracy with low latency.",
      "ai_keywords": [
        "Chain-of-Thought (CoT) reasoning",
        "Mind-Paced Speaking (MPS)",
        "dual-brain approach",
        "Formulation Brain",
        "Articulation Brain",
        "real-time reasoning",
        "Spoken-MQA",
        "URO-Bench"
      ],
      "githubStars": 8
    },
    "publishedAt": "2025-10-10T13:50:59.000Z",
    "title": "Mind-Paced Speaking: A Dual-Brain Approach to Real-Time Reasoning in\n  Spoken Language Models",
    "summary": "Real-time Spoken Language Models (SLMs) struggle to leverage Chain-of-Thought\n(CoT) reasoning due to the prohibitive latency of generating the entire thought\nprocess sequentially. Enabling SLMs to think while speaking, similar to humans,\nis attracting increasing attention. We present, for the first time, Mind-Paced\nSpeaking (MPS), a brain-inspired framework that enables high-fidelity,\nreal-time reasoning. Similar to how humans utilize distinct brain regions for\nthinking and responding, we propose a novel dual-brain approach, employing a\n\"Formulation Brain\" for high-level reasoning to pace and guide a separate\n\"Articulation Brain\" for fluent speech generation. This division of labor\neliminates mode-switching, preserving the integrity of the reasoning process.\nExperiments show that MPS significantly outperforms existing\nthink-while-speaking methods and achieves reasoning performance comparable to\nmodels that pre-compute the full CoT before speaking, while drastically\nreducing latency. Under a zero-latency configuration, the proposed method\nachieves an accuracy of 92.8% on the mathematical reasoning task Spoken-MQA and\nattains a score of 82.5 on the speech conversation task URO-Bench. Our work\neffectively bridges the gap between high-quality reasoning and real-time\ninteraction.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.09592.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 124
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.08867",
      "authors": [
        {
          "_id": "68ec5814cd07fb414898c8db",
          "name": "Gaurav Sahu",
          "hidden": false
        },
        {
          "_id": "68ec5814cd07fb414898c8dc",
          "name": "Hugo Larochelle",
          "hidden": false
        },
        {
          "_id": "68ec5814cd07fb414898c8dd",
          "name": "Laurent Charlin",
          "hidden": false
        },
        {
          "_id": "68ec5814cd07fb414898c8de",
          "name": "Christopher Pal",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-09T23:53:19.000Z",
      "submittedOnDailyAt": "2025-10-13T00:09:58.734Z",
      "title": "ReviewerToo: Should AI Join The Program Committee? A Look At The Future\n  of Peer Review",
      "submittedOnDailyBy": {
        "_id": "6377ac12f5fe4a39f783b05d",
        "avatarUrl": "/avatars/5f8b6d999cf48dd4703bbd70236c38c8.svg",
        "isPro": false,
        "fullname": "G Sahu",
        "user": "demfier",
        "type": "user"
      },
      "summary": "Peer review is the cornerstone of scientific publishing, yet it suffers from\ninconsistencies, reviewer subjectivity, and scalability challenges. We\nintroduce ReviewerToo, a modular framework for studying and deploying\nAI-assisted peer review to complement human judgment with systematic and\nconsistent assessments. ReviewerToo supports systematic experiments with\nspecialized reviewer personas and structured evaluation criteria, and can be\npartially or fully integrated into real conference workflows. We validate\nReviewerToo on a carefully curated dataset of 1,963 paper submissions from ICLR\n2025, where our experiments with the gpt-oss-120b model achieves 81.8% accuracy\nfor the task of categorizing a paper as accept/reject compared to 83.9% for the\naverage human reviewer. Additionally, ReviewerToo-generated reviews are rated\nas higher quality than the human average by an LLM judge, though still trailing\nthe strongest expert contributions. Our analysis highlights domains where AI\nreviewers excel (e.g., fact-checking, literature coverage) and where they\nstruggle (e.g., assessing methodological novelty and theoretical\ncontributions), underscoring the continued need for human expertise. Based on\nthese findings, we propose guidelines for integrating AI into peer-review\npipelines, showing how AI can enhance consistency, coverage, and fairness while\nleaving complex evaluative judgments to domain experts. Our work provides a\nfoundation for systematic, hybrid peer-review systems that scale with the\ngrowth of scientific publishing.",
      "upvotes": 3,
      "discussionId": "68ec5814cd07fb414898c8df",
      "ai_summary": "ReviewerToo, a modular AI-assisted peer review framework, complements human judgment with systematic assessments, achieving high accuracy and quality in specific domains while highlighting areas where human expertise remains essential.",
      "ai_keywords": [
        "AI-assisted peer review",
        "reviewer personas",
        "structured evaluation criteria",
        "gpt-oss-120b",
        "accuracy",
        "LLM judge",
        "methodological novelty",
        "theoretical contributions",
        "hybrid peer-review systems"
      ]
    },
    "publishedAt": "2025-10-09T19:53:19.000Z",
    "title": "ReviewerToo: Should AI Join The Program Committee? A Look At The Future\n  of Peer Review",
    "summary": "Peer review is the cornerstone of scientific publishing, yet it suffers from\ninconsistencies, reviewer subjectivity, and scalability challenges. We\nintroduce ReviewerToo, a modular framework for studying and deploying\nAI-assisted peer review to complement human judgment with systematic and\nconsistent assessments. ReviewerToo supports systematic experiments with\nspecialized reviewer personas and structured evaluation criteria, and can be\npartially or fully integrated into real conference workflows. We validate\nReviewerToo on a carefully curated dataset of 1,963 paper submissions from ICLR\n2025, where our experiments with the gpt-oss-120b model achieves 81.8% accuracy\nfor the task of categorizing a paper as accept/reject compared to 83.9% for the\naverage human reviewer. Additionally, ReviewerToo-generated reviews are rated\nas higher quality than the human average by an LLM judge, though still trailing\nthe strongest expert contributions. Our analysis highlights domains where AI\nreviewers excel (e.g., fact-checking, literature coverage) and where they\nstruggle (e.g., assessing methodological novelty and theoretical\ncontributions), underscoring the continued need for human expertise. Based on\nthese findings, we propose guidelines for integrating AI into peer-review\npipelines, showing how AI can enhance consistency, coverage, and fairness while\nleaving complex evaluative judgments to domain experts. Our work provides a\nfoundation for systematic, hybrid peer-review systems that scale with the\ngrowth of scientific publishing.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.08867.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6377ac12f5fe4a39f783b05d",
      "avatarUrl": "/avatars/5f8b6d999cf48dd4703bbd70236c38c8.svg",
      "fullname": "G Sahu",
      "name": "demfier",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.05608",
      "authors": [
        {
          "_id": "68ec6e11cd07fb414898c998",
          "name": "Shuzheng Si",
          "hidden": false
        },
        {
          "_id": "68ec6e11cd07fb414898c999",
          "name": "Haozhe Zhao",
          "hidden": false
        },
        {
          "_id": "68ec6e11cd07fb414898c99a",
          "name": "Kangyang Luo",
          "hidden": false
        },
        {
          "_id": "68ec6e11cd07fb414898c99b",
          "name": "Gang Chen",
          "hidden": false
        },
        {
          "_id": "68ec6e11cd07fb414898c99c",
          "name": "Fanchao Qi",
          "hidden": false
        },
        {
          "_id": "68ec6e11cd07fb414898c99d",
          "name": "Minjia Zhang",
          "hidden": false
        },
        {
          "_id": "68ec6e11cd07fb414898c99e",
          "name": "Baobao Chang",
          "hidden": false
        },
        {
          "_id": "68ec6e11cd07fb414898c99f",
          "name": "Maosong Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-07T06:10:53.000Z",
      "submittedOnDailyAt": "2025-10-13T01:43:50.352Z",
      "title": "A Goal Without a Plan Is Just a Wish: Efficient and Effective Global\n  Planner Training for Long-Horizon Agent Tasks",
      "submittedOnDailyBy": {
        "_id": "637c99bbfe115289cfedfb44",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c99bbfe115289cfedfb44/344NN9KKF_XXTlVYaGaMW.png",
        "isPro": false,
        "fullname": "ssz",
        "user": "ssz1111",
        "type": "user"
      },
      "summary": "Agents based on large language models (LLMs) struggle with brainless\ntrial-and-error and generating hallucinatory actions due to a lack of global\nplanning in long-horizon tasks. In this paper, we introduce a plan-and-execute\nframework and propose EAGLET, an efficient and effective planner training\nmethod to enhance the executor agent's planning abilities without human effort.\nSpecifically, we train a plug-and-play global planner through a two-step\nprocess: we first synthesize high-quality plans from an advanced LLM using our\nproposed homologous consensus filtering strategy, and apply fine-tuning as a\ncold start. Moreover, we further improve the planner with a rule-based\nreinforcement learning stage using a novel executor capability gain reward,\nensuring it can handle task instructions of varying difficulty. Experiments on\nthree long-horizon agent tasks show that executor agents equipped with our\nplanner outperform existing methods, achieving new state-of-the-art\nperformance. Meanwhile, EAGLET reduces training costs by 8x compared to\nRL-based baselines, and it does not require manual effort or extra training\ndata, offering an efficient and effective solution.",
      "upvotes": 3,
      "discussionId": "68ec6e11cd07fb414898c9a0",
      "ai_summary": "A plan-and-execute framework with EAGLET enhances LLM-based agents' planning abilities, achieving state-of-the-art performance in long-horizon tasks with reduced training costs.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "plan-and-execute framework",
        "EAGLET",
        "global planner",
        "homologous consensus filtering",
        "fine-tuning",
        "rule-based reinforcement learning",
        "executor capability gain reward",
        "long-horizon tasks"
      ]
    },
    "publishedAt": "2025-10-07T02:10:53.000Z",
    "title": "A Goal Without a Plan Is Just a Wish: Efficient and Effective Global\n  Planner Training for Long-Horizon Agent Tasks",
    "summary": "Agents based on large language models (LLMs) struggle with brainless\ntrial-and-error and generating hallucinatory actions due to a lack of global\nplanning in long-horizon tasks. In this paper, we introduce a plan-and-execute\nframework and propose EAGLET, an efficient and effective planner training\nmethod to enhance the executor agent's planning abilities without human effort.\nSpecifically, we train a plug-and-play global planner through a two-step\nprocess: we first synthesize high-quality plans from an advanced LLM using our\nproposed homologous consensus filtering strategy, and apply fine-tuning as a\ncold start. Moreover, we further improve the planner with a rule-based\nreinforcement learning stage using a novel executor capability gain reward,\nensuring it can handle task instructions of varying difficulty. Experiments on\nthree long-horizon agent tasks show that executor agents equipped with our\nplanner outperform existing methods, achieving new state-of-the-art\nperformance. Meanwhile, EAGLET reduces training costs by 8x compared to\nRL-based baselines, and it does not require manual effort or extra training\ndata, offering an efficient and effective solution.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.05608.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "637c99bbfe115289cfedfb44",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c99bbfe115289cfedfb44/344NN9KKF_XXTlVYaGaMW.png",
      "fullname": "ssz",
      "name": "ssz1111",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.09507",
      "authors": [
        {
          "_id": "68ec63a6cd07fb414898c989",
          "name": "Zixin Zhang",
          "hidden": false
        },
        {
          "_id": "68ec63a6cd07fb414898c98a",
          "name": "Kanghao Chen",
          "hidden": false
        },
        {
          "_id": "68ec63a6cd07fb414898c98b",
          "name": "Xingwang Lin",
          "hidden": false
        },
        {
          "_id": "68ec63a6cd07fb414898c98c",
          "name": "Lutao Jiang",
          "hidden": false
        },
        {
          "_id": "68ec63a6cd07fb414898c98d",
          "name": "Xu Zheng",
          "hidden": false
        },
        {
          "_id": "68ec63a6cd07fb414898c98e",
          "name": "Yuanhuiyi Lyu",
          "hidden": false
        },
        {
          "_id": "68ec63a6cd07fb414898c98f",
          "name": "Litao Guo",
          "hidden": false
        },
        {
          "_id": "68ec63a6cd07fb414898c990",
          "name": "Yinchuan Li",
          "hidden": false
        },
        {
          "_id": "68ec63a6cd07fb414898c991",
          "name": "Ying-Cong Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-10T16:10:45.000Z",
      "submittedOnDailyAt": "2025-10-13T00:57:50.917Z",
      "title": "PhysToolBench: Benchmarking Physical Tool Understanding for MLLMs",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "The ability to use, understand, and create tools is a hallmark of human\nintelligence, enabling sophisticated interaction with the physical world. For\nany general-purpose intelligent agent to achieve true versatility, it must also\nmaster these fundamental skills. While modern Multimodal Large Language Models\n(MLLMs) leverage their extensive common knowledge for high-level planning in\nembodied AI and in downstream Vision-Language-Action (VLA) models, the extent\nof their true understanding of physical tools remains unquantified. To bridge\nthis gap, we present PhysToolBench, the first benchmark dedicated to evaluating\nthe comprehension of physical tools by MLLMs. Our benchmark is structured as a\nVisual Question Answering (VQA) dataset comprising over 1,000 image-text pairs.\nIt assesses capabilities across three distinct difficulty levels: (1) Tool\nRecognition: Requiring the recognition of a tool's primary function. (2) Tool\nUnderstanding: Testing the ability to grasp the underlying principles of a\ntool's operation. (3) Tool Creation: Challenging the model to fashion a new\ntool from surrounding objects when conventional options are unavailable. Our\ncomprehensive evaluation of 32 MLLMs-spanning proprietary, open-source,\nspecialized embodied, and backbones in VLAs-reveals a significant deficiency in\ntool understanding. Furthermore, we provide an in-depth analysis and propose\npreliminary solutions. Code and dataset are publicly available.",
      "upvotes": 2,
      "discussionId": "68ec63a6cd07fb414898c992",
      "githubRepo": "https://github.com/EnVision-Research/PhysToolBench",
      "ai_summary": "PhysToolBench evaluates MLLMs' comprehension of physical tools through a VQA dataset, revealing significant deficiencies in tool understanding.",
      "ai_keywords": [
        "Multimodal Large Language Models",
        "MLLMs",
        "embodied AI",
        "Vision-Language-Action",
        "VLA",
        "Visual Question Answering",
        "VQA",
        "Tool Recognition",
        "Tool Understanding",
        "Tool Creation"
      ],
      "githubStars": 8
    },
    "publishedAt": "2025-10-10T12:10:45.000Z",
    "title": "PhysToolBench: Benchmarking Physical Tool Understanding for MLLMs",
    "summary": "The ability to use, understand, and create tools is a hallmark of human\nintelligence, enabling sophisticated interaction with the physical world. For\nany general-purpose intelligent agent to achieve true versatility, it must also\nmaster these fundamental skills. While modern Multimodal Large Language Models\n(MLLMs) leverage their extensive common knowledge for high-level planning in\nembodied AI and in downstream Vision-Language-Action (VLA) models, the extent\nof their true understanding of physical tools remains unquantified. To bridge\nthis gap, we present PhysToolBench, the first benchmark dedicated to evaluating\nthe comprehension of physical tools by MLLMs. Our benchmark is structured as a\nVisual Question Answering (VQA) dataset comprising over 1,000 image-text pairs.\nIt assesses capabilities across three distinct difficulty levels: (1) Tool\nRecognition: Requiring the recognition of a tool's primary function. (2) Tool\nUnderstanding: Testing the ability to grasp the underlying principles of a\ntool's operation. (3) Tool Creation: Challenging the model to fashion a new\ntool from surrounding objects when conventional options are unavailable. Our\ncomprehensive evaluation of 32 MLLMs-spanning proprietary, open-source,\nspecialized embodied, and backbones in VLAs-reveals a significant deficiency in\ntool understanding. Furthermore, we provide an in-depth analysis and propose\npreliminary solutions. Code and dataset are publicly available.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.09507.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 124
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.08872",
      "authors": [
        {
          "_id": "68ec51cecd07fb414898c8b1",
          "name": "Siqi Zhu",
          "hidden": false
        },
        {
          "_id": "68ec51cecd07fb414898c8b2",
          "name": "David Zhang",
          "hidden": false
        },
        {
          "_id": "68ec51cecd07fb414898c8b3",
          "name": "Pedro Cisneros-Velarde",
          "hidden": false
        },
        {
          "_id": "68ec51cecd07fb414898c8b4",
          "name": "Jiaxuan You",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-10T00:05:14.000Z",
      "submittedOnDailyAt": "2025-10-13T02:22:59.294Z",
      "title": "GTAlign: Game-Theoretic Alignment of LLM Assistants for Mutual Welfare",
      "submittedOnDailyBy": {
        "_id": "65621fd68631d43d2baf33b2",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/1JemNCnPkS1mE3SNygsE2.png",
        "isPro": false,
        "fullname": "siqi zhu",
        "user": "zsqzz",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) have achieved remarkable progress in reasoning,\nyet sometimes produce responses that are suboptimal for users in tasks such as\nwriting, information seeking, or providing practical guidance. Conventional\nalignment practices typically assume that maximizing model reward also\nmaximizes user welfare, but this assumption frequently fails in practice:\nmodels may over-clarify or generate overly verbose reasoning when users prefer\nconcise answers. Such behaviors resemble the prisoner's dilemma, where\nindividually rational choices lead to socially suboptimal outcomes. The\nfundamental challenge is the lack of a principled decision making mechanism\nthat mutually benefits both the LLM and the user. We propose Game-Theoretic\nAlignment (GTAlign), an alignment framework that integrates game-theoretic\ndecision making into both reasoning and training. During reasoning, the model\nexplicitly treats user-LLM interaction as a strategic game: it constructs\npayoff matrices within its reasoning chain to estimate welfare for both itself\nand the user, and then selects actions that are mutually beneficial. During\ntraining, we introduce a mutual welfare reward that reinforces cooperative\nresponses, aligning model behavior with socially efficient outcomes. In\naddition, we introduce an inference technique that leverages game-theoretic\nreasoning to dynamically adapt LLM's response when pricing policies of LLM\nservice change. Extensive experiments demonstrate that GTAlign substantially\nimproves reasoning efficiency, answer quality, and mutual welfare compared to\nbaselines across diverse tasks. The code is available at\nhttps://github.com/ulab-uiuc/GTAlign .",
      "upvotes": 2,
      "discussionId": "68ec51cecd07fb414898c8b5",
      "ai_summary": "Game-Theoretic Alignment (GTAlign) improves Large Language Model (LLM) performance by integrating game-theoretic decision making into reasoning and training, enhancing efficiency, answer quality, and mutual welfare.",
      "ai_keywords": [
        "Large Language Models",
        "Game-Theoretic Alignment",
        "GTAlign",
        "strategic game",
        "payoff matrices",
        "mutual welfare reward",
        "inference technique",
        "reasoning efficiency",
        "answer quality",
        "mutual welfare"
      ],
      "githubStars": 0,
      "organization": {
        "_id": "65448bef5b5d9185ba3202b9",
        "name": "UIUC-CS",
        "fullname": "University of Illinois at Urbana-Champaign",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"
      }
    },
    "publishedAt": "2025-10-09T20:05:14.000Z",
    "title": "GTAlign: Game-Theoretic Alignment of LLM Assistants for Mutual Welfare",
    "summary": "Large Language Models (LLMs) have achieved remarkable progress in reasoning,\nyet sometimes produce responses that are suboptimal for users in tasks such as\nwriting, information seeking, or providing practical guidance. Conventional\nalignment practices typically assume that maximizing model reward also\nmaximizes user welfare, but this assumption frequently fails in practice:\nmodels may over-clarify or generate overly verbose reasoning when users prefer\nconcise answers. Such behaviors resemble the prisoner's dilemma, where\nindividually rational choices lead to socially suboptimal outcomes. The\nfundamental challenge is the lack of a principled decision making mechanism\nthat mutually benefits both the LLM and the user. We propose Game-Theoretic\nAlignment (GTAlign), an alignment framework that integrates game-theoretic\ndecision making into both reasoning and training. During reasoning, the model\nexplicitly treats user-LLM interaction as a strategic game: it constructs\npayoff matrices within its reasoning chain to estimate welfare for both itself\nand the user, and then selects actions that are mutually beneficial. During\ntraining, we introduce a mutual welfare reward that reinforces cooperative\nresponses, aligning model behavior with socially efficient outcomes. In\naddition, we introduce an inference technique that leverages game-theoretic\nreasoning to dynamically adapt LLM's response when pricing policies of LLM\nservice change. Extensive experiments demonstrate that GTAlign substantially\nimproves reasoning efficiency, answer quality, and mutual welfare compared to\nbaselines across diverse tasks. The code is available at\nhttps://github.com/ulab-uiuc/GTAlign .",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.08872.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "65621fd68631d43d2baf33b2",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/1JemNCnPkS1mE3SNygsE2.png",
      "fullname": "siqi zhu",
      "name": "zsqzz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "organization": {
      "_id": "65448bef5b5d9185ba3202b9",
      "name": "UIUC-CS",
      "fullname": "University of Illinois at Urbana-Champaign",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.07861",
      "authors": [
        {
          "_id": "68ec6ff6cd07fb414898c9c8",
          "name": "Tianyu Fan",
          "hidden": false
        },
        {
          "_id": "68ec6ff6cd07fb414898c9c9",
          "name": "Xinyao Niu",
          "hidden": false
        },
        {
          "_id": "68ec6ff6cd07fb414898c9ca",
          "name": "Yuxiang Zheng",
          "hidden": false
        },
        {
          "_id": "68ec6ff6cd07fb414898c9cb",
          "name": "Fengji Zhang",
          "hidden": false
        },
        {
          "_id": "68ec6ff6cd07fb414898c9cc",
          "name": "Chengen Huang",
          "hidden": false
        },
        {
          "_id": "68ec6ff6cd07fb414898c9cd",
          "name": "Bei Chen",
          "hidden": false
        },
        {
          "_id": "68ec6ff6cd07fb414898c9ce",
          "name": "Junyang Lin",
          "hidden": false
        },
        {
          "_id": "68ec6ff6cd07fb414898c9cf",
          "name": "Chao Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-09T07:03:43.000Z",
      "submittedOnDailyAt": "2025-10-13T01:58:17.988Z",
      "title": "Understanding DeepResearch via Reports",
      "submittedOnDailyBy": {
        "_id": "65f3ee4efe1935d2e9d4a678",
        "avatarUrl": "/avatars/ffd9f945183b4c890f885c82739c28a6.svg",
        "isPro": false,
        "fullname": "Tianyu Fan",
        "user": "T1anyu",
        "type": "user"
      },
      "summary": "DeepResearch agents represent a transformative AI paradigm, conducting\nexpert-level research through sophisticated reasoning and multi-tool\nintegration. However, evaluating these systems remains critically challenging\ndue to open-ended research scenarios and existing benchmarks that focus on\nisolated capabilities rather than holistic performance. Unlike traditional LLM\ntasks, DeepResearch systems must synthesize diverse sources, generate insights,\nand present coherent findings, which are capabilities that resist simple\nverification. To address this gap, we introduce DeepResearch-ReportEval, a\ncomprehensive framework designed to assess DeepResearch systems through their\nmost representative outputs: research reports. Our approach systematically\nmeasures three dimensions: quality, redundancy, and factuality, using an\ninnovative LLM-as-a-Judge methodology achieving strong expert concordance. We\ncontribute a standardized benchmark of 100 curated queries spanning 12\nreal-world categories, enabling systematic capability comparison. Our\nevaluation of four leading commercial systems reveals distinct design\nphilosophies and performance trade-offs, establishing foundational insights as\nDeepResearch evolves from information assistants toward intelligent research\npartners. Source code and data are available at:\nhttps://github.com/HKUDS/DeepResearch-Eval.",
      "upvotes": 2,
      "discussionId": "68ec6ff6cd07fb414898c9d0",
      "ai_summary": "A framework evaluates DeepResearch systems by assessing the quality, redundancy, and factuality of their research reports using an LLM-as-a-Judge methodology.",
      "ai_keywords": [
        "DeepResearch",
        "LLM",
        "research reports",
        "LLM-as-a-Judge",
        "expert concordance",
        "standardized benchmark",
        "real-world categories",
        "intelligent research partners"
      ]
    },
    "publishedAt": "2025-10-09T03:03:43.000Z",
    "title": "Understanding DeepResearch via Reports",
    "summary": "DeepResearch agents represent a transformative AI paradigm, conducting\nexpert-level research through sophisticated reasoning and multi-tool\nintegration. However, evaluating these systems remains critically challenging\ndue to open-ended research scenarios and existing benchmarks that focus on\nisolated capabilities rather than holistic performance. Unlike traditional LLM\ntasks, DeepResearch systems must synthesize diverse sources, generate insights,\nand present coherent findings, which are capabilities that resist simple\nverification. To address this gap, we introduce DeepResearch-ReportEval, a\ncomprehensive framework designed to assess DeepResearch systems through their\nmost representative outputs: research reports. Our approach systematically\nmeasures three dimensions: quality, redundancy, and factuality, using an\ninnovative LLM-as-a-Judge methodology achieving strong expert concordance. We\ncontribute a standardized benchmark of 100 curated queries spanning 12\nreal-world categories, enabling systematic capability comparison. Our\nevaluation of four leading commercial systems reveals distinct design\nphilosophies and performance trade-offs, establishing foundational insights as\nDeepResearch evolves from information assistants toward intelligent research\npartners. Source code and data are available at:\nhttps://github.com/HKUDS/DeepResearch-Eval.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.07861.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65f3ee4efe1935d2e9d4a678",
      "avatarUrl": "/avatars/ffd9f945183b4c890f885c82739c28a6.svg",
      "fullname": "Tianyu Fan",
      "name": "T1anyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.02898",
      "authors": [
        {
          "_id": "68e3c2e473e20ab5778421ed",
          "user": {
            "_id": "63c67770b167901a39346c27",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673951039741-noauth.jpeg",
            "isPro": false,
            "fullname": "Lorenzo Bianchi",
            "user": "lorebianchi98",
            "type": "user"
          },
          "name": "Lorenzo Bianchi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-08T08:03:13.038Z",
          "hidden": false
        },
        {
          "_id": "68e3c2e473e20ab5778421ee",
          "user": {
            "_id": "640d000904c679553cfa275b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640d000904c679553cfa275b/v18gSEHfT1FvEpYTkWJNj.png",
            "isPro": false,
            "fullname": "Giacomo Pacini",
            "user": "Ruggero1912",
            "type": "user"
          },
          "name": "Giacomo Pacini",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-07T12:28:10.096Z",
          "hidden": false
        },
        {
          "_id": "68e3c2e473e20ab5778421ef",
          "name": "Fabio Carrara",
          "hidden": false
        },
        {
          "_id": "68e3c2e473e20ab5778421f0",
          "name": "Nicola Messina",
          "hidden": false
        },
        {
          "_id": "68e3c2e473e20ab5778421f1",
          "name": "Giuseppe Amato",
          "hidden": false
        },
        {
          "_id": "68e3c2e473e20ab5778421f2",
          "name": "Fabrizio Falchi",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/640d000904c679553cfa275b/dX4ywkPPTvdYDFK_3Ansm.png",
        "https://cdn-uploads.huggingface.co/production/uploads/640d000904c679553cfa275b/BfiQpwyxYnalJSk9RbebA.webp"
      ],
      "publishedAt": "2025-10-03T11:05:56.000Z",
      "submittedOnDailyAt": "2025-10-13T06:13:08.962Z",
      "title": "One Patch to Caption Them All: A Unified Zero-Shot Captioning Framework",
      "submittedOnDailyBy": {
        "_id": "640d000904c679553cfa275b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640d000904c679553cfa275b/v18gSEHfT1FvEpYTkWJNj.png",
        "isPro": false,
        "fullname": "Giacomo Pacini",
        "user": "Ruggero1912",
        "type": "user"
      },
      "summary": "Zero-shot captioners are recently proposed models that utilize common-space\nvision-language representations to caption images without relying on paired\nimage-text data. To caption an image, they proceed by textually decoding a\ntext-aligned image feature, but they limit their scope to global\nrepresentations and whole-image captions. We present , a\nunified framework for zero-shot captioning that shifts from an image-centric to\na patch-centric paradigm, enabling the captioning of arbitrary regions without\nthe need of region-level supervision. Instead of relying on global image\nrepresentations, we treat individual patches as atomic captioning units and\naggregate them to describe arbitrary regions, from single patches to\nnon-contiguous areas and entire images. We analyze the key ingredients that\nenable current latent captioners to work in our novel proposed framework.\nExperiments demonstrate that backbones producing meaningful, dense visual\nfeatures, such as DINO, are key to achieving state-of-the-art performance in\nmultiple region-based captioning tasks. Compared to other baselines and\nstate-of-the-art competitors, our models achieve better performance on\nzero-shot dense, region-set, and a newly introduced trace captioning task,\nhighlighting the effectiveness of patch-wise semantic representations for\nscalable caption generation. Project page at https://paciosoft.com/Patch-ioner/ .",
      "upvotes": 2,
      "discussionId": "68e3c2e473e20ab5778421f3",
      "projectPage": "https://paciosoft.com/Patch-ioner/",
      "githubRepo": "https://github.com/Ruggero1912/Patch-ioner",
      "ai_summary": "A patch-centric framework for zero-shot captioning achieves state-of-the-art performance by using dense visual features from models like DINO to caption arbitrary image regions.",
      "ai_keywords": [
        "zero-shot captioners",
        "common-space vision-language representations",
        "text-aligned image feature",
        "patch-centric paradigm",
        "region-level supervision",
        "atomic captioning units",
        "latent captioners",
        "dense visual features",
        "DINO",
        "region-based captioning tasks",
        "zero-shot dense captioning",
        "region-set captioning",
        "trace captioning task",
        "patch-wise semantic representations"
      ],
      "githubStars": 5
    },
    "publishedAt": "2025-10-03T07:05:56.000Z",
    "title": "One Patch to Caption Them All: A Unified Zero-Shot Captioning Framework",
    "summary": "Zero-shot captioners are recently proposed models that utilize common-space\nvision-language representations to caption images without relying on paired\nimage-text data. To caption an image, they proceed by textually decoding a\ntext-aligned image feature, but they limit their scope to global\nrepresentations and whole-image captions. We present , a\nunified framework for zero-shot captioning that shifts from an image-centric to\na patch-centric paradigm, enabling the captioning of arbitrary regions without\nthe need of region-level supervision. Instead of relying on global image\nrepresentations, we treat individual patches as atomic captioning units and\naggregate them to describe arbitrary regions, from single patches to\nnon-contiguous areas and entire images. We analyze the key ingredients that\nenable current latent captioners to work in our novel proposed framework.\nExperiments demonstrate that backbones producing meaningful, dense visual\nfeatures, such as DINO, are key to achieving state-of-the-art performance in\nmultiple region-based captioning tasks. Compared to other baselines and\nstate-of-the-art competitors, our models achieve better performance on\nzero-shot dense, region-set, and a newly introduced trace captioning task,\nhighlighting the effectiveness of patch-wise semantic representations for\nscalable caption generation. Project page at https://paciosoft.com/Patch-ioner/ .",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/640d000904c679553cfa275b/dX4ywkPPTvdYDFK_3Ansm.png",
      "https://cdn-uploads.huggingface.co/production/uploads/640d000904c679553cfa275b/BfiQpwyxYnalJSk9RbebA.webp"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.02898.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "640d000904c679553cfa275b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640d000904c679553cfa275b/v18gSEHfT1FvEpYTkWJNj.png",
      "fullname": "Giacomo Pacini",
      "name": "Ruggero1912",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.09561",
      "authors": [
        {
          "_id": "68ec5fdccd07fb414898c964",
          "name": "Minkyoung Cho",
          "hidden": false
        },
        {
          "_id": "68ec5fdccd07fb414898c965",
          "name": "Ruben Ohana",
          "hidden": false
        },
        {
          "_id": "68ec5fdccd07fb414898c966",
          "name": "Christian Jacobsen",
          "hidden": false
        },
        {
          "_id": "68ec5fdccd07fb414898c967",
          "name": "Adityan Jothi",
          "hidden": false
        },
        {
          "_id": "68ec5fdccd07fb414898c968",
          "name": "Min-Hung Chen",
          "hidden": false
        },
        {
          "_id": "68ec5fdccd07fb414898c969",
          "name": "Z. Morley Mao",
          "hidden": false
        },
        {
          "_id": "68ec5fdccd07fb414898c96a",
          "name": "Ethem Can",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-10T17:13:02.000Z",
      "submittedOnDailyAt": "2025-10-13T00:43:02.002Z",
      "title": "TC-LoRA: Temporally Modulated Conditional LoRA for Adaptive Diffusion\n  Control",
      "submittedOnDailyBy": {
        "_id": "64ae22dd1aee69ece065cdcd",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png",
        "isPro": false,
        "fullname": "Min-Hung Chen",
        "user": "cmhungsteve",
        "type": "user"
      },
      "summary": "Current controllable diffusion models typically rely on fixed architectures\nthat modify intermediate activations to inject guidance conditioned on a new\nmodality. This approach uses a static conditioning strategy for a dynamic,\nmulti-stage denoising process, limiting the model's ability to adapt its\nresponse as the generation evolves from coarse structure to fine detail. We\nintroduce TC-LoRA (Temporally Modulated Conditional LoRA), a new paradigm that\nenables dynamic, context-aware control by conditioning the model's weights\ndirectly. Our framework uses a hypernetwork to generate LoRA adapters\non-the-fly, tailoring weight modifications for the frozen backbone at each\ndiffusion step based on time and the user's condition. This mechanism enables\nthe model to learn and execute an explicit, adaptive strategy for applying\nconditional guidance throughout the entire generation process. Through\nexperiments on various data domains, we demonstrate that this dynamic,\nparametric control significantly enhances generative fidelity and adherence to\nspatial conditions compared to static, activation-based methods. TC-LoRA\nestablishes an alternative approach in which the model's conditioning strategy\nis modified through a deeper functional adaptation of its weights, allowing\ncontrol to align with the dynamic demands of the task and generative stage.",
      "upvotes": 1,
      "discussionId": "68ec5fddcd07fb414898c96b",
      "ai_summary": "TC-LoRA enhances generative fidelity and adherence to spatial conditions by dynamically conditioning model weights through a hypernetwork, improving upon static activation-based methods in diffusion models.",
      "ai_keywords": [
        "diffusion models",
        "TC-LoRA",
        "Temporally Modulated Conditional LoRA",
        "hypernetwork",
        "LoRA adapters",
        "generative fidelity",
        "spatial conditions",
        "dynamic conditioning",
        "static conditioning",
        "activation-based methods"
      ]
    },
    "publishedAt": "2025-10-10T13:13:02.000Z",
    "title": "TC-LoRA: Temporally Modulated Conditional LoRA for Adaptive Diffusion\n  Control",
    "summary": "Current controllable diffusion models typically rely on fixed architectures\nthat modify intermediate activations to inject guidance conditioned on a new\nmodality. This approach uses a static conditioning strategy for a dynamic,\nmulti-stage denoising process, limiting the model's ability to adapt its\nresponse as the generation evolves from coarse structure to fine detail. We\nintroduce TC-LoRA (Temporally Modulated Conditional LoRA), a new paradigm that\nenables dynamic, context-aware control by conditioning the model's weights\ndirectly. Our framework uses a hypernetwork to generate LoRA adapters\non-the-fly, tailoring weight modifications for the frozen backbone at each\ndiffusion step based on time and the user's condition. This mechanism enables\nthe model to learn and execute an explicit, adaptive strategy for applying\nconditional guidance throughout the entire generation process. Through\nexperiments on various data domains, we demonstrate that this dynamic,\nparametric control significantly enhances generative fidelity and adherence to\nspatial conditions compared to static, activation-based methods. TC-LoRA\nestablishes an alternative approach in which the model's conditioning strategy\nis modified through a deeper functional adaptation of its weights, allowing\ncontrol to align with the dynamic demands of the task and generative stage.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.09561.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ae22dd1aee69ece065cdcd",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png",
      "fullname": "Min-Hung Chen",
      "name": "cmhungsteve",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.08994",
      "authors": [
        {
          "_id": "68ec791ccd07fb414898ca2e",
          "name": "Yao Teng",
          "hidden": false
        },
        {
          "_id": "68ec791ccd07fb414898ca2f",
          "name": "Fuyun Wang",
          "hidden": false
        },
        {
          "_id": "68ec791ccd07fb414898ca30",
          "name": "Xian Liu",
          "hidden": false
        },
        {
          "_id": "68ec791ccd07fb414898ca31",
          "name": "Zhekai Chen",
          "hidden": false
        },
        {
          "_id": "68ec791ccd07fb414898ca32",
          "name": "Han Shi",
          "hidden": false
        },
        {
          "_id": "68ec791ccd07fb414898ca33",
          "name": "Yu Wang",
          "hidden": false
        },
        {
          "_id": "68ec791ccd07fb414898ca34",
          "name": "Zhenguo Li",
          "hidden": false
        },
        {
          "_id": "68ec791ccd07fb414898ca35",
          "name": "Weiyang Liu",
          "hidden": false
        },
        {
          "_id": "68ec791ccd07fb414898ca36",
          "name": "Difan Zou",
          "hidden": false
        },
        {
          "_id": "68ec791ccd07fb414898ca37",
          "name": "Xihui Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-10T04:30:45.000Z",
      "submittedOnDailyAt": "2025-10-13T02:32:03.582Z",
      "title": "Speculative Jacobi-Denoising Decoding for Accelerating Autoregressive\n  Text-to-image Generation",
      "submittedOnDailyBy": {
        "_id": "6427e08288215cee63b1c44d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6427e08288215cee63b1c44d/rzaG978FF-ywzicWNl_xl.jpeg",
        "isPro": false,
        "fullname": "yao teng",
        "user": "tytyt",
        "type": "user"
      },
      "summary": "As a new paradigm of visual content generation, autoregressive text-to-image\nmodels suffer from slow inference due to their sequential token-by-token\ndecoding process, often requiring thousands of model forward passes to generate\na single image. To address this inefficiency, we propose Speculative\nJacobi-Denoising Decoding (SJD2), a framework that incorporates the denoising\nprocess into Jacobi iterations to enable parallel token generation in\nautoregressive models. Our method introduces a next-clean-token prediction\nparadigm that enables the pre-trained autoregressive models to accept\nnoise-perturbed token embeddings and predict the next clean tokens through\nlow-cost fine-tuning. This denoising paradigm guides the model towards more\nstable Jacobi trajectories. During inference, our method initializes token\nsequences with Gaussian noise and performs iterative\nnext-clean-token-prediction in the embedding space. We employ a probabilistic\ncriterion to verify and accept multiple tokens in parallel, and refine the\nunaccepted tokens for the next iteration with the denoising trajectory.\nExperiments show that our method can accelerate generation by reducing model\nforward passes while maintaining the visual quality of generated images.",
      "upvotes": 1,
      "discussionId": "68ec791ccd07fb414898ca38",
      "ai_summary": "Speculative Jacobi-Denoising Decoding accelerates autoregressive text-to-image generation by enabling parallel token prediction and reducing model forward passes.",
      "ai_keywords": [
        "autoregressive text-to-image models",
        "Speculative Jacobi-Denoising Decoding",
        "denoising process",
        "Jacobi iterations",
        "next-clean-token prediction",
        "token embeddings",
        "low-cost fine-tuning",
        "Gaussian noise",
        "next-clean-token-prediction",
        "probabilistic criterion",
        "visual quality"
      ]
    },
    "publishedAt": "2025-10-10T00:30:45.000Z",
    "title": "Speculative Jacobi-Denoising Decoding for Accelerating Autoregressive\n  Text-to-image Generation",
    "summary": "As a new paradigm of visual content generation, autoregressive text-to-image\nmodels suffer from slow inference due to their sequential token-by-token\ndecoding process, often requiring thousands of model forward passes to generate\na single image. To address this inefficiency, we propose Speculative\nJacobi-Denoising Decoding (SJD2), a framework that incorporates the denoising\nprocess into Jacobi iterations to enable parallel token generation in\nautoregressive models. Our method introduces a next-clean-token prediction\nparadigm that enables the pre-trained autoregressive models to accept\nnoise-perturbed token embeddings and predict the next clean tokens through\nlow-cost fine-tuning. This denoising paradigm guides the model towards more\nstable Jacobi trajectories. During inference, our method initializes token\nsequences with Gaussian noise and performs iterative\nnext-clean-token-prediction in the embedding space. We employ a probabilistic\ncriterion to verify and accept multiple tokens in parallel, and refine the\nunaccepted tokens for the next iteration with the denoising trajectory.\nExperiments show that our method can accelerate generation by reducing model\nforward passes while maintaining the visual quality of generated images.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.08994.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6427e08288215cee63b1c44d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6427e08288215cee63b1c44d/rzaG978FF-ywzicWNl_xl.jpeg",
      "fullname": "yao teng",
      "name": "tytyt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.07959",
      "authors": [
        {
          "_id": "68eca176cd07fb414898cb10",
          "name": "Alexander Rubinstein",
          "hidden": false
        },
        {
          "_id": "68eca176cd07fb414898cb11",
          "name": "Benjamin Raible",
          "hidden": false
        },
        {
          "_id": "68eca176cd07fb414898cb12",
          "name": "Martin Gubri",
          "hidden": false
        },
        {
          "_id": "68eca176cd07fb414898cb13",
          "name": "Seong Joon Oh",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64198d7efdfc2970b350f48f/KzJwt3Jv-s5EonmEb4-TA.jpeg"
      ],
      "publishedAt": "2025-10-09T08:53:59.000Z",
      "submittedOnDailyAt": "2025-10-13T05:26:41.534Z",
      "title": "DISCO: Diversifying Sample Condensation for Efficient Model Evaluation",
      "submittedOnDailyBy": {
        "_id": "64198d7efdfc2970b350f48f",
        "avatarUrl": "/avatars/c0a0f30e1cbc22f1eb6bbc4549a5709c.svg",
        "isPro": false,
        "fullname": "Alexander Rubinstein",
        "user": "arubique",
        "type": "user"
      },
      "summary": "Evaluating modern machine learning models has become prohibitively expensive.\nBenchmarks such as LMMs-Eval and HELM demand thousands of GPU hours per model.\nCostly evaluation reduces inclusivity, slows the cycle of innovation, and\nworsens environmental impact. The typical approach follows two steps. First,\nselect an anchor subset of data. Second, train a mapping from the accuracy on\nthis subset to the final test result. The drawback is that anchor selection\ndepends on clustering, which can be complex and sensitive to design choices. We\nargue that promoting diversity among samples is not essential; what matters is\nto select samples that maximise diversity in model responses. Our\nmethod, Diversifying Sample Condensation (DISCO), selects the top-k\nsamples with the greatest model disagreements. This uses greedy, sample-wise\nstatistics rather than global clustering. The approach is conceptually simpler.\nFrom a theoretical view, inter-model disagreement provides an\ninformation-theoretically optimal rule for such greedy selection.\nDISCO shows empirical gains over prior methods, achieving\nstate-of-the-art results in performance prediction across MMLU, Hellaswag,\nWinogrande, and ARC. Code is available here:\nhttps://github.com/arubique/disco-public.",
      "upvotes": 1,
      "discussionId": "68eca177cd07fb414898cb14",
      "ai_summary": "A method called DISCO selects samples with the greatest model disagreements to predict performance, achieving state-of-the-art results across various benchmarks with reduced computational cost.",
      "ai_keywords": [
        "Diversifying Sample Condensation (DISCO)",
        "model disagreements",
        "greedy selection",
        "information-theoretically optimal rule",
        "MMLU",
        "Hellaswag",
        "Winogrande",
        "ARC"
      ],
      "organization": {
        "_id": "67efb00c6e51a83b10018b42",
        "name": "UniTuebingen",
        "fullname": "Eberhard Karls Universität Tübingen",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67efaed96e51a83b10013635/IBHUSQLny8BmeCdJro55K.png"
      }
    },
    "publishedAt": "2025-10-09T04:53:59.000Z",
    "title": "DISCO: Diversifying Sample Condensation for Efficient Model Evaluation",
    "summary": "Evaluating modern machine learning models has become prohibitively expensive.\nBenchmarks such as LMMs-Eval and HELM demand thousands of GPU hours per model.\nCostly evaluation reduces inclusivity, slows the cycle of innovation, and\nworsens environmental impact. The typical approach follows two steps. First,\nselect an anchor subset of data. Second, train a mapping from the accuracy on\nthis subset to the final test result. The drawback is that anchor selection\ndepends on clustering, which can be complex and sensitive to design choices. We\nargue that promoting diversity among samples is not essential; what matters is\nto select samples that maximise diversity in model responses. Our\nmethod, Diversifying Sample Condensation (DISCO), selects the top-k\nsamples with the greatest model disagreements. This uses greedy, sample-wise\nstatistics rather than global clustering. The approach is conceptually simpler.\nFrom a theoretical view, inter-model disagreement provides an\ninformation-theoretically optimal rule for such greedy selection.\nDISCO shows empirical gains over prior methods, achieving\nstate-of-the-art results in performance prediction across MMLU, Hellaswag,\nWinogrande, and ARC. Code is available here:\nhttps://github.com/arubique/disco-public.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64198d7efdfc2970b350f48f/KzJwt3Jv-s5EonmEb4-TA.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.07959.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64198d7efdfc2970b350f48f",
      "avatarUrl": "/avatars/c0a0f30e1cbc22f1eb6bbc4549a5709c.svg",
      "fullname": "Alexander Rubinstein",
      "name": "arubique",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "organization": {
      "_id": "67efb00c6e51a83b10018b42",
      "name": "UniTuebingen",
      "fullname": "Eberhard Karls Universität Tübingen",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67efaed96e51a83b10013635/IBHUSQLny8BmeCdJro55K.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.07896",
      "authors": [
        {
          "_id": "68ec7ad6cd07fb414898ca3a",
          "name": "Jiayu Yang",
          "hidden": false
        },
        {
          "_id": "68ec7ad6cd07fb414898ca3b",
          "name": "Yuxuan Fan",
          "hidden": false
        },
        {
          "_id": "68ec7ad6cd07fb414898ca3c",
          "name": "Songning Lai",
          "hidden": false
        },
        {
          "_id": "68ec7ad6cd07fb414898ca3d",
          "name": "Shengen Wu",
          "hidden": false
        },
        {
          "_id": "68ec7ad6cd07fb414898ca3e",
          "name": "Jiaqi Tang",
          "hidden": false
        },
        {
          "_id": "68ec7ad6cd07fb414898ca3f",
          "name": "Chun Kang",
          "hidden": false
        },
        {
          "_id": "68ec7ad6cd07fb414898ca40",
          "name": "Zhijiang Guo",
          "hidden": false
        },
        {
          "_id": "68ec7ad6cd07fb414898ca41",
          "name": "Yutao Yue",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-09T07:46:08.000Z",
      "submittedOnDailyAt": "2025-10-13T02:45:04.644Z",
      "title": "ACE: Attribution-Controlled Knowledge Editing for Multi-hop Factual\n  Recall",
      "submittedOnDailyBy": {
        "_id": "658247c592b5a9664de63882",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658247c592b5a9664de63882/Jn03voLQjDlB3YjpSi-PI.jpeg",
        "isPro": false,
        "fullname": "fan",
        "user": "EasonFan",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) require efficient knowledge editing (KE) to\nupdate factual information, yet existing methods exhibit significant\nperformance decay in multi-hop factual recall. This failure is particularly\nacute when edits involve intermediate implicit subjects within reasoning\nchains. Through causal analysis, we reveal that this limitation stems from an\noversight of how chained knowledge is dynamically represented and utilized at\nthe neuron level. We discover that during multi hop reasoning, implicit\nsubjects function as query neurons, which sequentially activate corresponding\nvalue neurons across transformer layers to accumulate information toward the\nfinal answer, a dynamic prior KE work has overlooked. Guided by this insight,\nwe propose ACE: Attribution-Controlled Knowledge Editing for Multi-hop Factual\nRecall, a framework that leverages neuron-level attribution to identify and\nedit these critical query-value (Q-V) pathways. ACE provides a mechanistically\ngrounded solution for multi-hop KE, empirically outperforming state-of-the-art\nmethods by 9.44% on GPT-J and 37.46% on Qwen3-8B. Our analysis further reveals\nmore fine-grained activation patterns in Qwen3 and demonstrates that the\nsemantic interpretability of value neurons is orchestrated by query-driven\naccumulation. These findings establish a new pathway for advancing KE\ncapabilities based on the principled understanding of internal reasoning\nmechanisms.",
      "upvotes": 1,
      "discussionId": "68ec7ad6cd07fb414898ca42",
      "ai_summary": "ACE, a framework using neuron-level attribution, enhances multi-hop factual recall in LLMs by editing critical query-value pathways, outperforming existing methods.",
      "ai_keywords": [
        "Large Language Models",
        "knowledge editing",
        "multi-hop factual recall",
        "causal analysis",
        "query neurons",
        "value neurons",
        "transformer layers",
        "Attribution-Controlled Knowledge Editing",
        "GPT-J",
        "Qwen3-8B",
        "semantic interpretability"
      ]
    },
    "publishedAt": "2025-10-09T03:46:08.000Z",
    "title": "ACE: Attribution-Controlled Knowledge Editing for Multi-hop Factual\n  Recall",
    "summary": "Large Language Models (LLMs) require efficient knowledge editing (KE) to\nupdate factual information, yet existing methods exhibit significant\nperformance decay in multi-hop factual recall. This failure is particularly\nacute when edits involve intermediate implicit subjects within reasoning\nchains. Through causal analysis, we reveal that this limitation stems from an\noversight of how chained knowledge is dynamically represented and utilized at\nthe neuron level. We discover that during multi hop reasoning, implicit\nsubjects function as query neurons, which sequentially activate corresponding\nvalue neurons across transformer layers to accumulate information toward the\nfinal answer, a dynamic prior KE work has overlooked. Guided by this insight,\nwe propose ACE: Attribution-Controlled Knowledge Editing for Multi-hop Factual\nRecall, a framework that leverages neuron-level attribution to identify and\nedit these critical query-value (Q-V) pathways. ACE provides a mechanistically\ngrounded solution for multi-hop KE, empirically outperforming state-of-the-art\nmethods by 9.44% on GPT-J and 37.46% on Qwen3-8B. Our analysis further reveals\nmore fine-grained activation patterns in Qwen3 and demonstrates that the\nsemantic interpretability of value neurons is orchestrated by query-driven\naccumulation. These findings establish a new pathway for advancing KE\ncapabilities based on the principled understanding of internal reasoning\nmechanisms.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.07896.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "658247c592b5a9664de63882",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658247c592b5a9664de63882/Jn03voLQjDlB3YjpSi-PI.jpeg",
      "fullname": "fan",
      "name": "EasonFan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.07319",
      "authors": [
        {
          "_id": "68e717097ae125f9582e694a",
          "name": "Ci-Siang Lin",
          "hidden": false
        },
        {
          "_id": "68e717097ae125f9582e694b",
          "user": {
            "_id": "64ae22dd1aee69ece065cdcd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png",
            "isPro": false,
            "fullname": "Min-Hung Chen",
            "user": "cmhungsteve",
            "type": "user"
          },
          "name": "Min-Hung Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-09T06:50:25.264Z",
          "hidden": false
        },
        {
          "_id": "68e717097ae125f9582e694c",
          "name": "I-Jieh Liu",
          "hidden": false
        },
        {
          "_id": "68e717097ae125f9582e694d",
          "name": "Chien-Yi Wang",
          "hidden": false
        },
        {
          "_id": "68e717097ae125f9582e694e",
          "name": "Sifei Liu",
          "hidden": false
        },
        {
          "_id": "68e717097ae125f9582e694f",
          "name": "Yu-Chiang Frank Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-08T17:59:57.000Z",
      "submittedOnDailyAt": "2025-10-13T00:41:09.823Z",
      "title": "Temporal Prompting Matters: Rethinking Referring Video Object\n  Segmentation",
      "submittedOnDailyBy": {
        "_id": "64ae22dd1aee69ece065cdcd",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png",
        "isPro": false,
        "fullname": "Min-Hung Chen",
        "user": "cmhungsteve",
        "type": "user"
      },
      "summary": "Referring Video Object Segmentation (RVOS) aims to segment the object\nreferred to by the query sentence in the video. Most existing methods require\nend-to-end training with dense mask annotations, which could be\ncomputation-consuming and less scalable. In this work, we rethink the RVOS\nproblem and aim to investigate the key to this task. Based on existing\nfoundation segmentation models, we decompose the RVOS task into referring,\nvideo, and segmentation factors, and propose a Temporal Prompt Generation and\nSelection (Tenet) framework to address the referring and video factors while\nleaving the segmentation problem to foundation models. To efficiently adapt\nimage-based foundation segmentation models to referring video object\nsegmentation, we leverage off-the-shelf object detectors and trackers to\nproduce temporal prompts associated with the referring sentence. While\nhigh-quality temporal prompts could be produced, they can not be easily\nidentified from confidence scores. To tackle this issue, we propose Prompt\nPreference Learning to evaluate the quality of the produced temporal prompts.\nBy taking such prompts to instruct image-based foundation segmentation models,\nwe would be able to produce high-quality masks for the referred object,\nenabling efficient model adaptation to referring video object segmentation.\nExperiments on RVOS benchmarks demonstrate the effectiveness of the Tenet\nframework.",
      "upvotes": 1,
      "discussionId": "68e717097ae125f9582e6950",
      "ai_summary": "The Tenet framework decomposes the RVOS task into referring, video, and segmentation factors, using temporal prompts and prompt preference learning to adapt image-based foundation segmentation models for efficient RVOS.",
      "ai_keywords": [
        "Referring Video Object Segmentation (RVOS)",
        "Temporal Prompt Generation and Selection (Tenet)",
        "foundation segmentation models",
        "temporal prompts",
        "Prompt Preference Learning",
        "object detectors",
        "trackers"
      ]
    },
    "publishedAt": "2025-10-08T13:59:57.000Z",
    "title": "Temporal Prompting Matters: Rethinking Referring Video Object\n  Segmentation",
    "summary": "Referring Video Object Segmentation (RVOS) aims to segment the object\nreferred to by the query sentence in the video. Most existing methods require\nend-to-end training with dense mask annotations, which could be\ncomputation-consuming and less scalable. In this work, we rethink the RVOS\nproblem and aim to investigate the key to this task. Based on existing\nfoundation segmentation models, we decompose the RVOS task into referring,\nvideo, and segmentation factors, and propose a Temporal Prompt Generation and\nSelection (Tenet) framework to address the referring and video factors while\nleaving the segmentation problem to foundation models. To efficiently adapt\nimage-based foundation segmentation models to referring video object\nsegmentation, we leverage off-the-shelf object detectors and trackers to\nproduce temporal prompts associated with the referring sentence. While\nhigh-quality temporal prompts could be produced, they can not be easily\nidentified from confidence scores. To tackle this issue, we propose Prompt\nPreference Learning to evaluate the quality of the produced temporal prompts.\nBy taking such prompts to instruct image-based foundation segmentation models,\nwe would be able to produce high-quality masks for the referred object,\nenabling efficient model adaptation to referring video object segmentation.\nExperiments on RVOS benchmarks demonstrate the effectiveness of the Tenet\nframework.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.07319.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ae22dd1aee69ece065cdcd",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png",
      "fullname": "Min-Hung Chen",
      "name": "cmhungsteve",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.09535",
      "authors": [
        {
          "_id": "68ec6e2dcd07fb414898c9a2",
          "name": "Feifan Song",
          "hidden": false
        },
        {
          "_id": "68ec6e2dcd07fb414898c9a3",
          "name": "Shaohang Wei",
          "hidden": false
        },
        {
          "_id": "68ec6e2dcd07fb414898c9a4",
          "name": "Bofei Gao",
          "hidden": false
        },
        {
          "_id": "68ec6e2dcd07fb414898c9a5",
          "name": "Yejie Wang",
          "hidden": false
        },
        {
          "_id": "68ec6e2dcd07fb414898c9a6",
          "name": "Wen Luo",
          "hidden": false
        },
        {
          "_id": "68ec6e2dcd07fb414898c9a7",
          "name": "Wei Li",
          "hidden": false
        },
        {
          "_id": "68ec6e2dcd07fb414898c9a8",
          "name": "Linli Yao",
          "hidden": false
        },
        {
          "_id": "68ec6e2dcd07fb414898c9a9",
          "name": "Weimin Xiong",
          "hidden": false
        },
        {
          "_id": "68ec6e2dcd07fb414898c9aa",
          "name": "Liang Chen",
          "hidden": false
        },
        {
          "_id": "68ec6e2dcd07fb414898c9ab",
          "name": "Tianyu Liu",
          "hidden": false
        },
        {
          "_id": "68ec6e2dcd07fb414898c9ac",
          "name": "Houfeng Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-10T16:49:03.000Z",
      "submittedOnDailyAt": "2025-10-13T01:43:12.548Z",
      "title": "Mitigating Overthinking through Reasoning Shaping",
      "submittedOnDailyBy": {
        "_id": "6447ca6ca478b20f1755b294",
        "avatarUrl": "/avatars/5049856b5ed1b74533fff902e14b4c7c.svg",
        "isPro": false,
        "fullname": "Feifan Song",
        "user": "songff",
        "type": "user"
      },
      "summary": "Large reasoning models (LRMs) boosted by Reinforcement Learning from Verifier\nReward (RLVR) have shown great power in problem solving, yet they often cause\noverthinking: excessive, meandering reasoning that inflates computational cost.\nPrior designs of penalization in RLVR manage to reduce token consumption while\noften harming model performance, which arises from the oversimplicity of\ntoken-level supervision. In this paper, we argue that the granularity of\nsupervision plays a crucial role in balancing efficiency and accuracy, and\npropose Group Relative Segment Penalization (GRSP), a step-level method to\nregularize reasoning. Since preliminary analyses show that reasoning segments\nare strongly correlated with token consumption and model performance, we design\na length-aware weighting mechanism across segment clusters. Extensive\nexperiments demonstrate that GRSP achieves superior token efficiency without\nheavily compromising accuracy, especially the advantages with harder problems.\nMoreover, GRSP stabilizes RL training and scales effectively across model\nsizes.",
      "upvotes": 0,
      "discussionId": "68ec6e2dcd07fb414898c9ad",
      "ai_summary": "Group Relative Segment Penalization (GRSP) improves token efficiency in large reasoning models without significantly reducing accuracy, especially for complex problems, by regularizing reasoning at the step level.",
      "ai_keywords": [
        "Reinforcement Learning from Verifier Reward (RLVR)",
        "overthinking",
        "token consumption",
        "model performance",
        "token-level supervision",
        "Group Relative Segment Penalization (GRSP)",
        "step-level method",
        "reasoning segments",
        "length-aware weighting mechanism",
        "segment clusters",
        "RL training",
        "model sizes"
      ]
    },
    "publishedAt": "2025-10-10T12:49:03.000Z",
    "title": "Mitigating Overthinking through Reasoning Shaping",
    "summary": "Large reasoning models (LRMs) boosted by Reinforcement Learning from Verifier\nReward (RLVR) have shown great power in problem solving, yet they often cause\noverthinking: excessive, meandering reasoning that inflates computational cost.\nPrior designs of penalization in RLVR manage to reduce token consumption while\noften harming model performance, which arises from the oversimplicity of\ntoken-level supervision. In this paper, we argue that the granularity of\nsupervision plays a crucial role in balancing efficiency and accuracy, and\npropose Group Relative Segment Penalization (GRSP), a step-level method to\nregularize reasoning. Since preliminary analyses show that reasoning segments\nare strongly correlated with token consumption and model performance, we design\na length-aware weighting mechanism across segment clusters. Extensive\nexperiments demonstrate that GRSP achieves superior token efficiency without\nheavily compromising accuracy, especially the advantages with harder problems.\nMoreover, GRSP stabilizes RL training and scales effectively across model\nsizes.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.09535.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6447ca6ca478b20f1755b294",
      "avatarUrl": "/avatars/5049856b5ed1b74533fff902e14b4c7c.svg",
      "fullname": "Feifan Song",
      "name": "songff",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.08492",
      "authors": [
        {
          "_id": "68eb5c10012880a10864bba4",
          "name": "Sharut Gupta",
          "hidden": false
        },
        {
          "_id": "68eb5c10012880a10864bba5",
          "name": "Shobhita Sundaram",
          "hidden": false
        },
        {
          "_id": "68eb5c10012880a10864bba6",
          "name": "Chenyu Wang",
          "hidden": false
        },
        {
          "_id": "68eb5c10012880a10864bba7",
          "name": "Stefanie Jegelka",
          "hidden": false
        },
        {
          "_id": "68eb5c10012880a10864bba8",
          "name": "Phillip Isola",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/5f1158120c833276f61f1a84/mNKF2e-xCHiUIdFH5sxxN.jpeg"
      ],
      "publishedAt": "2025-10-09T17:32:23.000Z",
      "submittedOnDailyAt": "2025-10-13T06:01:00.997Z",
      "title": "Better Together: Leveraging Unpaired Multimodal Data for Stronger\n  Unimodal Models",
      "submittedOnDailyBy": {
        "_id": "5f1158120c833276f61f1a84",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
        "isPro": true,
        "fullname": "Niels Rogge",
        "user": "nielsr",
        "type": "user"
      },
      "summary": "Traditional multimodal learners find unified representations for tasks like\nvisual question answering, but rely heavily on paired datasets. However, an\noverlooked yet potentially powerful question is: can one leverage auxiliary\nunpaired multimodal data to directly enhance representation learning in a\ntarget modality? We introduce UML: Unpaired Multimodal Learner, a\nmodality-agnostic training paradigm in which a single model alternately\nprocesses inputs from different modalities while sharing parameters across\nthem. This design exploits the assumption that different modalities are\nprojections of a shared underlying reality, allowing the model to benefit from\ncross-modal structure without requiring explicit pairs. Theoretically, under\nlinear data-generating assumptions, we show that unpaired auxiliary data can\nyield representations strictly more informative about the data-generating\nprocess than unimodal training. Empirically, we show that using unpaired data\nfrom auxiliary modalities -- such as text, audio, or images -- consistently\nimproves downstream performance across diverse unimodal targets such as image\nand audio. Our project page: https://unpaired-multimodal.github.io/",
      "upvotes": 0,
      "discussionId": "68eb5c10012880a10864bba9",
      "projectPage": "https://unpaired-multimodal.github.io/",
      "githubRepo": "https://github.com/Sharut/Unpaired-Multimodal-Learning/",
      "ai_summary": "UML, an unpaired multimodal learner, enhances representation learning in a target modality by leveraging auxiliary unpaired data from different modalities.",
      "ai_keywords": [
        "multimodal learners",
        "visual question answering",
        "paired datasets",
        "unpaired multimodal data",
        "modality-agnostic training",
        "cross-modal structure",
        "unimodal training",
        "downstream performance"
      ],
      "organization": {
        "_id": "63728bde14d543d507ae970d",
        "name": "MIT",
        "fullname": "Massachusetts Institute of Technology",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"
      }
    },
    "publishedAt": "2025-10-09T13:32:23.000Z",
    "title": "Better Together: Leveraging Unpaired Multimodal Data for Stronger\n  Unimodal Models",
    "summary": "Traditional multimodal learners find unified representations for tasks like\nvisual question answering, but rely heavily on paired datasets. However, an\noverlooked yet potentially powerful question is: can one leverage auxiliary\nunpaired multimodal data to directly enhance representation learning in a\ntarget modality? We introduce UML: Unpaired Multimodal Learner, a\nmodality-agnostic training paradigm in which a single model alternately\nprocesses inputs from different modalities while sharing parameters across\nthem. This design exploits the assumption that different modalities are\nprojections of a shared underlying reality, allowing the model to benefit from\ncross-modal structure without requiring explicit pairs. Theoretically, under\nlinear data-generating assumptions, we show that unpaired auxiliary data can\nyield representations strictly more informative about the data-generating\nprocess than unimodal training. Empirically, we show that using unpaired data\nfrom auxiliary modalities -- such as text, audio, or images -- consistently\nimproves downstream performance across diverse unimodal targets such as image\nand audio. Our project page: https://unpaired-multimodal.github.io/",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/5f1158120c833276f61f1a84/mNKF2e-xCHiUIdFH5sxxN.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.08492.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f1158120c833276f61f1a84",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
      "fullname": "Niels Rogge",
      "name": "nielsr",
      "type": "user",
      "isPro": true,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 993
    },
    "organization": {
      "_id": "63728bde14d543d507ae970d",
      "name": "MIT",
      "fullname": "Massachusetts Institute of Technology",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.01119",
      "authors": [
        {
          "_id": "68ec5ec4cd07fb414898c939",
          "name": "Zhanpeng Luo",
          "hidden": false
        },
        {
          "_id": "68ec5ec4cd07fb414898c93a",
          "name": "Haoxi Ran",
          "hidden": false
        },
        {
          "_id": "68ec5ec4cd07fb414898c93b",
          "name": "Li Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-01T17:07:21.000Z",
      "submittedOnDailyAt": "2025-10-13T00:37:38.826Z",
      "title": "Instant4D: 4D Gaussian Splatting in Minutes",
      "submittedOnDailyBy": {
        "_id": "62a527f17157a88f920afc50",
        "avatarUrl": "/avatars/d20c9afcdc42db686e05472459fc09f6.svg",
        "isPro": false,
        "fullname": "Haoxi Ran",
        "user": "Hancy",
        "type": "user"
      },
      "summary": "Dynamic view synthesis has seen significant advances, yet reconstructing\nscenes from uncalibrated, casual video remains challenging due to slow\noptimization and complex parameter estimation. In this work, we present\nInstant4D, a monocular reconstruction system that leverages native 4D\nrepresentation to efficiently process casual video sequences within minutes,\nwithout calibrated cameras or depth sensors. Our method begins with geometric\nrecovery through deep visual SLAM, followed by grid pruning to optimize scene\nrepresentation. Our design significantly reduces redundancy while maintaining\ngeometric integrity, cutting model size to under 10% of its original footprint.\nTo handle temporal dynamics efficiently, we introduce a streamlined 4D Gaussian\nrepresentation, achieving a 30x speed-up and reducing training time to within\ntwo minutes, while maintaining competitive performance across several\nbenchmarks. Our method reconstruct a single video within 10 minutes on the\nDycheck dataset or for a typical 200-frame video. We further apply our model to\nin-the-wild videos, showcasing its generalizability. Our project website is\npublished at https://instant4d.github.io/.",
      "upvotes": 0,
      "discussionId": "68ec5ec4cd07fb414898c93c",
      "ai_summary": "Instant4D uses deep visual SLAM and a 4D Gaussian representation to efficiently reconstruct scenes from uncalibrated video sequences in minutes.",
      "ai_keywords": [
        "deep visual SLAM",
        "4D representation",
        "grid pruning",
        "4D Gaussian representation"
      ]
    },
    "publishedAt": "2025-10-01T13:07:21.000Z",
    "title": "Instant4D: 4D Gaussian Splatting in Minutes",
    "summary": "Dynamic view synthesis has seen significant advances, yet reconstructing\nscenes from uncalibrated, casual video remains challenging due to slow\noptimization and complex parameter estimation. In this work, we present\nInstant4D, a monocular reconstruction system that leverages native 4D\nrepresentation to efficiently process casual video sequences within minutes,\nwithout calibrated cameras or depth sensors. Our method begins with geometric\nrecovery through deep visual SLAM, followed by grid pruning to optimize scene\nrepresentation. Our design significantly reduces redundancy while maintaining\ngeometric integrity, cutting model size to under 10% of its original footprint.\nTo handle temporal dynamics efficiently, we introduce a streamlined 4D Gaussian\nrepresentation, achieving a 30x speed-up and reducing training time to within\ntwo minutes, while maintaining competitive performance across several\nbenchmarks. Our method reconstruct a single video within 10 minutes on the\nDycheck dataset or for a typical 200-frame video. We further apply our model to\nin-the-wild videos, showcasing its generalizability. Our project website is\npublished at https://instant4d.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.01119.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62a527f17157a88f920afc50",
      "avatarUrl": "/avatars/d20c9afcdc42db686e05472459fc09f6.svg",
      "fullname": "Haoxi Ran",
      "name": "Hancy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]