[
  {
    "paper": {
      "id": "2505.19147",
      "authors": [
        {
          "_id": "68353258d005e45149d2d384",
          "name": "Xuyang Liu",
          "hidden": false
        },
        {
          "_id": "68353258d005e45149d2d385",
          "name": "Zichen Wen",
          "hidden": false
        },
        {
          "_id": "68353258d005e45149d2d386",
          "name": "Shaobo Wang",
          "hidden": false
        },
        {
          "_id": "68353258d005e45149d2d387",
          "name": "Junjie Chen",
          "hidden": false
        },
        {
          "_id": "68353258d005e45149d2d388",
          "name": "Zhishan Tao",
          "hidden": false
        },
        {
          "_id": "68353258d005e45149d2d389",
          "name": "Yubo Wang",
          "hidden": false
        },
        {
          "_id": "68353258d005e45149d2d38a",
          "name": "Xiangqi Jin",
          "hidden": false
        },
        {
          "_id": "68353258d005e45149d2d38b",
          "name": "Chang Zou",
          "hidden": false
        },
        {
          "_id": "68353258d005e45149d2d38c",
          "name": "Yiyu Wang",
          "hidden": false
        },
        {
          "_id": "68353258d005e45149d2d38d",
          "name": "Chenfei Liao",
          "hidden": false
        },
        {
          "_id": "68353258d005e45149d2d38e",
          "name": "Xu Zheng",
          "hidden": false
        },
        {
          "_id": "68353258d005e45149d2d38f",
          "name": "Honggang Chen",
          "hidden": false
        },
        {
          "_id": "68353258d005e45149d2d390",
          "name": "Weijia Li",
          "hidden": false
        },
        {
          "_id": "68353258d005e45149d2d391",
          "name": "Xuming Hu",
          "hidden": false
        },
        {
          "_id": "68353258d005e45149d2d392",
          "name": "Conghui He",
          "hidden": false
        },
        {
          "_id": "68353258d005e45149d2d393",
          "name": "Linfeng Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-25T13:51:17.000Z",
      "submittedOnDailyAt": "2025-05-27T02:06:05.849Z",
      "title": "Shifting AI Efficiency From Model-Centric to Data-Centric Compression",
      "submittedOnDailyBy": {
        "_id": "653b8c3e97a4d71d950e2f20",
        "avatarUrl": "/avatars/b68880022e14556d0be58c69615db3be.svg",
        "isPro": false,
        "fullname": "Zichen Wen",
        "user": "zichenwen",
        "type": "user"
      },
      "summary": "The rapid advancement of large language models (LLMs) and multi-modal LLMs\n(MLLMs) has historically relied on model-centric scaling through increasing\nparameter counts from millions to hundreds of billions to drive performance\ngains. However, as we approach hardware limits on model size, the dominant\ncomputational bottleneck has fundamentally shifted to the quadratic cost of\nself-attention over long token sequences, now driven by ultra-long text\ncontexts, high-resolution images, and extended videos. In this position paper,\nwe argue that the focus of research for efficient AI is shifting from\nmodel-centric compression to data-centric compression. We position token\ncompression as the new frontier, which improves AI efficiency via reducing the\nnumber of tokens during model training or inference. Through comprehensive\nanalysis, we first examine recent developments in long-context AI across\nvarious domains and establish a unified mathematical framework for existing\nmodel efficiency strategies, demonstrating why token compression represents a\ncrucial paradigm shift in addressing long-context overhead. Subsequently, we\nsystematically review the research landscape of token compression, analyzing\nits fundamental benefits and identifying its compelling advantages across\ndiverse scenarios. Furthermore, we provide an in-depth analysis of current\nchallenges in token compression research and outline promising future\ndirections. Ultimately, our work aims to offer a fresh perspective on AI\nefficiency, synthesize existing research, and catalyze innovative developments\nto address the challenges that increasing context lengths pose to the AI\ncommunity's advancement.",
      "upvotes": 55,
      "discussionId": "68353259d005e45149d2d3c0",
      "projectPage": "https://github.com/xuyang-liu16/Awesome-Token-level-Model-Compression",
      "githubRepo": "https://github.com/xuyang-liu16/Awesome-Token-level-Model-Compression",
      "ai_summary": "The focus in AI research is shifting from model-centric to data-centric compression, with token compression identified as key to improving efficiency in handling long-context scenarios.",
      "ai_keywords": [
        "large language models",
        "multi-modal LLMs",
        "self-attention",
        "token compression",
        "long-context AI",
        "mathematical framework",
        "model efficiency",
        "long-context overhead",
        "current challenges",
        "future directions"
      ]
    },
    "publishedAt": "2025-05-25T09:51:17.000Z",
    "title": "Shifting AI Efficiency From Model-Centric to Data-Centric Compression",
    "summary": "The rapid advancement of large language models (LLMs) and multi-modal LLMs\n(MLLMs) has historically relied on model-centric scaling through increasing\nparameter counts from millions to hundreds of billions to drive performance\ngains. However, as we approach hardware limits on model size, the dominant\ncomputational bottleneck has fundamentally shifted to the quadratic cost of\nself-attention over long token sequences, now driven by ultra-long text\ncontexts, high-resolution images, and extended videos. In this position paper,\nwe argue that the focus of research for efficient AI is shifting from\nmodel-centric compression to data-centric compression. We position token\ncompression as the new frontier, which improves AI efficiency via reducing the\nnumber of tokens during model training or inference. Through comprehensive\nanalysis, we first examine recent developments in long-context AI across\nvarious domains and establish a unified mathematical framework for existing\nmodel efficiency strategies, demonstrating why token compression represents a\ncrucial paradigm shift in addressing long-context overhead. Subsequently, we\nsystematically review the research landscape of token compression, analyzing\nits fundamental benefits and identifying its compelling advantages across\ndiverse scenarios. Furthermore, we provide an in-depth analysis of current\nchallenges in token compression research and outline promising future\ndirections. Ultimately, our work aims to offer a fresh perspective on AI\nefficiency, synthesize existing research, and catalyze innovative developments\nto address the challenges that increasing context lengths pose to the AI\ncommunity's advancement.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19147.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "653b8c3e97a4d71d950e2f20",
      "avatarUrl": "/avatars/b68880022e14556d0be58c69615db3be.svg",
      "fullname": "Zichen Wen",
      "name": "zichenwen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.16348",
      "authors": [
        {
          "_id": "6835365d2925bc8bb23a57c7",
          "name": "Taeyoon Kwon",
          "hidden": false
        },
        {
          "_id": "6835365d2925bc8bb23a57c8",
          "name": "Dongwook Choi",
          "hidden": false
        },
        {
          "_id": "6835365d2925bc8bb23a57c9",
          "name": "Sunghwan Kim",
          "hidden": false
        },
        {
          "_id": "6835365d2925bc8bb23a57ca",
          "name": "Hyojun Kim",
          "hidden": false
        },
        {
          "_id": "6835365d2925bc8bb23a57cb",
          "name": "Seungjun Moon",
          "hidden": false
        },
        {
          "_id": "6835365d2925bc8bb23a57cc",
          "name": "Beong-woo Kwak",
          "hidden": false
        },
        {
          "_id": "6835365d2925bc8bb23a57cd",
          "name": "Kuan-Hao Huang",
          "hidden": false
        },
        {
          "_id": "6835365d2925bc8bb23a57ce",
          "name": "Jinyoung Yeo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T08:00:10.000Z",
      "submittedOnDailyAt": "2025-05-27T02:20:10.067Z",
      "title": "Embodied Agents Meet Personalization: Exploring Memory Utilization for\n  Personalized Assistance",
      "submittedOnDailyBy": {
        "_id": "636b529ef796304dd67d139c",
        "avatarUrl": "/avatars/7a64d5095fcb1da558b52ad48177ad76.svg",
        "isPro": false,
        "fullname": "Taeyoon Kwon",
        "user": "Connoriginal",
        "type": "user"
      },
      "summary": "Embodied agents empowered by large language models (LLMs) have shown strong\nperformance in household object rearrangement tasks. However, these tasks\nprimarily focus on single-turn interactions with simplified instructions, which\ndo not truly reflect the challenges of providing meaningful assistance to\nusers. To provide personalized assistance, embodied agents must understand the\nunique semantics that users assign to the physical world (e.g., favorite cup,\nbreakfast routine) by leveraging prior interaction history to interpret\ndynamic, real-world instructions. Yet, the effectiveness of embodied agents in\nutilizing memory for personalized assistance remains largely underexplored. To\naddress this gap, we present MEMENTO, a personalized embodied agent evaluation\nframework designed to comprehensively assess memory utilization capabilities to\nprovide personalized assistance. Our framework consists of a two-stage memory\nevaluation process design that enables quantifying the impact of memory\nutilization on task performance. This process enables the evaluation of agents'\nunderstanding of personalized knowledge in object rearrangement tasks by\nfocusing on its role in goal interpretation: (1) the ability to identify target\nobjects based on personal meaning (object semantics), and (2) the ability to\ninfer object-location configurations from consistent user patterns, such as\nroutines (user patterns). Our experiments across various LLMs reveal\nsignificant limitations in memory utilization, with even frontier models like\nGPT-4o experiencing a 30.5% performance drop when required to reference\nmultiple memories, particularly in tasks involving user patterns. These\nfindings, along with our detailed analyses and case studies, provide valuable\ninsights for future research in developing more effective personalized embodied\nagents. Project website: https://connoriginal.github.io/MEMENTO",
      "upvotes": 33,
      "discussionId": "683536612925bc8bb23a58e1",
      "projectPage": "https://connoriginal.github.io/MEMENTO/",
      "githubRepo": "https://github.com/Connoriginal/MEMENTO",
      "ai_summary": "MEMENTO evaluates personalized memory utilization in embodied agents, revealing limitations in understanding user semantics and routines.",
      "ai_keywords": [
        "embodied agents",
        "large language models (LLMs)",
        "object rearrangement tasks",
        "user semantics",
        "prior interaction history",
        "memory utilization",
        "personalized assistance",
        "goal interpretation",
        "object semantics",
        "user patterns"
      ]
    },
    "publishedAt": "2025-05-22T04:00:10.000Z",
    "title": "Embodied Agents Meet Personalization: Exploring Memory Utilization for\n  Personalized Assistance",
    "summary": "Embodied agents empowered by large language models (LLMs) have shown strong\nperformance in household object rearrangement tasks. However, these tasks\nprimarily focus on single-turn interactions with simplified instructions, which\ndo not truly reflect the challenges of providing meaningful assistance to\nusers. To provide personalized assistance, embodied agents must understand the\nunique semantics that users assign to the physical world (e.g., favorite cup,\nbreakfast routine) by leveraging prior interaction history to interpret\ndynamic, real-world instructions. Yet, the effectiveness of embodied agents in\nutilizing memory for personalized assistance remains largely underexplored. To\naddress this gap, we present MEMENTO, a personalized embodied agent evaluation\nframework designed to comprehensively assess memory utilization capabilities to\nprovide personalized assistance. Our framework consists of a two-stage memory\nevaluation process design that enables quantifying the impact of memory\nutilization on task performance. This process enables the evaluation of agents'\nunderstanding of personalized knowledge in object rearrangement tasks by\nfocusing on its role in goal interpretation: (1) the ability to identify target\nobjects based on personal meaning (object semantics), and (2) the ability to\ninfer object-location configurations from consistent user patterns, such as\nroutines (user patterns). Our experiments across various LLMs reveal\nsignificant limitations in memory utilization, with even frontier models like\nGPT-4o experiencing a 30.5% performance drop when required to reference\nmultiple memories, particularly in tasks involving user patterns. These\nfindings, along with our detailed analyses and case studies, provide valuable\ninsights for future research in developing more effective personalized embodied\nagents. Project website: https://connoriginal.github.io/MEMENTO",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16348.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "636b529ef796304dd67d139c",
      "avatarUrl": "/avatars/7a64d5095fcb1da558b52ad48177ad76.svg",
      "fullname": "Taeyoon Kwon",
      "name": "Connoriginal",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.19457",
      "authors": [
        {
          "_id": "683536ec70d215849adfc236",
          "name": "Guilong Lu",
          "hidden": false
        },
        {
          "_id": "683536ec70d215849adfc237",
          "name": "Xuntao Guo",
          "hidden": false
        },
        {
          "_id": "683536ec70d215849adfc238",
          "user": {
            "_id": "6555df426947208b7741b637",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6555df426947208b7741b637/b7ply-HyaPKXrPvRNh21K.jpeg",
            "isPro": false,
            "fullname": "Rongjunchen Zhang",
            "user": "Tinker250",
            "type": "user"
          },
          "name": "Rongjunchen Zhang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-27T03:52:17.018Z",
          "hidden": false
        },
        {
          "_id": "683536ec70d215849adfc239",
          "user": {
            "_id": "648add6aff6123185eb185a8",
            "avatarUrl": "/avatars/e37dfa680c1bb86c721165f03eb79e97.svg",
            "isPro": false,
            "fullname": "WNQzhu",
            "user": "Qlisp",
            "type": "user"
          },
          "name": "Wenqiao Zhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-27T07:49:28.216Z",
          "hidden": false
        },
        {
          "_id": "683536ec70d215849adfc23a",
          "name": "Ji Liu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6555df426947208b7741b637/48jI0LlYjRwO4-0kHRV0V.png",
        "https://cdn-uploads.huggingface.co/production/uploads/6555df426947208b7741b637/atuM30TNh72kJtm8zGxoc.png"
      ],
      "publishedAt": "2025-05-26T03:23:02.000Z",
      "submittedOnDailyAt": "2025-05-27T02:28:08.336Z",
      "title": "BizFinBench: A Business-Driven Real-World Financial Benchmark for\n  Evaluating LLMs",
      "submittedOnDailyBy": {
        "_id": "6555df426947208b7741b637",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6555df426947208b7741b637/b7ply-HyaPKXrPvRNh21K.jpeg",
        "isPro": false,
        "fullname": "Rongjunchen Zhang",
        "user": "Tinker250",
        "type": "user"
      },
      "summary": "Large language models excel in general tasks, yet assessing their reliability\nin logic-heavy, precision-critical domains like finance, law, and healthcare\nremains challenging. To address this, we introduce BizFinBench, the first\nbenchmark specifically designed to evaluate LLMs in real-world financial\napplications. BizFinBench consists of 6,781 well-annotated queries in Chinese,\nspanning five dimensions: numerical calculation, reasoning, information\nextraction, prediction recognition, and knowledge-based question answering,\ngrouped into nine fine-grained categories. The benchmark includes both\nobjective and subjective metrics. We also introduce IteraJudge, a novel LLM\nevaluation method that reduces bias when LLMs serve as evaluators in objective\nmetrics. We benchmark 25 models, including both proprietary and open-source\nsystems. Extensive experiments show that no model dominates across all tasks.\nOur evaluation reveals distinct capability patterns: (1) In Numerical\nCalculation, Claude-3.5-Sonnet (63.18) and DeepSeek-R1 (64.04) lead, while\nsmaller models like Qwen2.5-VL-3B (15.92) lag significantly; (2) In Reasoning,\nproprietary models dominate (ChatGPT-o3: 83.58, Gemini-2.0-Flash: 81.15), with\nopen-source models trailing by up to 19.49 points; (3) In Information\nExtraction, the performance spread is the largest, with DeepSeek-R1 scoring\n71.46, while Qwen3-1.7B scores 11.23; (4) In Prediction Recognition,\nperformance variance is minimal, with top models scoring between 39.16 and\n50.00. We find that while current LLMs handle routine finance queries\ncompetently, they struggle with complex scenarios requiring cross-concept\nreasoning. BizFinBench offers a rigorous, business-aligned benchmark for future\nresearch. The code and dataset are available at\nhttps://github.com/HiThink-Research/BizFinBench.",
      "upvotes": 32,
      "discussionId": "683536f170d215849adfc35e",
      "projectPage": "https://hithink-research.github.io/BizFinBench/",
      "githubRepo": "https://github.com/HiThink-Research/BizFinBench",
      "ai_summary": "BizFinBench is a benchmark for evaluating large language models in financial applications, revealing distinct performance patterns across various tasks.",
      "ai_keywords": [
        "large language models",
        "BizFinBench",
        "numerical calculation",
        "reasoning",
        "information extraction",
        "prediction recognition",
        "knowledge-based question answering",
        "IteraJudge",
        "Claude-3.5-Sonnet",
        "DeepSeek-R1",
        "Qwen2.5-VL-3B",
        "ChatGPT-o3",
        "Gemini-2.0-Flash",
        "Qwen3-1.7B"
      ]
    },
    "publishedAt": "2025-05-25T23:23:02.000Z",
    "title": "BizFinBench: A Business-Driven Real-World Financial Benchmark for\n  Evaluating LLMs",
    "summary": "Large language models excel in general tasks, yet assessing their reliability\nin logic-heavy, precision-critical domains like finance, law, and healthcare\nremains challenging. To address this, we introduce BizFinBench, the first\nbenchmark specifically designed to evaluate LLMs in real-world financial\napplications. BizFinBench consists of 6,781 well-annotated queries in Chinese,\nspanning five dimensions: numerical calculation, reasoning, information\nextraction, prediction recognition, and knowledge-based question answering,\ngrouped into nine fine-grained categories. The benchmark includes both\nobjective and subjective metrics. We also introduce IteraJudge, a novel LLM\nevaluation method that reduces bias when LLMs serve as evaluators in objective\nmetrics. We benchmark 25 models, including both proprietary and open-source\nsystems. Extensive experiments show that no model dominates across all tasks.\nOur evaluation reveals distinct capability patterns: (1) In Numerical\nCalculation, Claude-3.5-Sonnet (63.18) and DeepSeek-R1 (64.04) lead, while\nsmaller models like Qwen2.5-VL-3B (15.92) lag significantly; (2) In Reasoning,\nproprietary models dominate (ChatGPT-o3: 83.58, Gemini-2.0-Flash: 81.15), with\nopen-source models trailing by up to 19.49 points; (3) In Information\nExtraction, the performance spread is the largest, with DeepSeek-R1 scoring\n71.46, while Qwen3-1.7B scores 11.23; (4) In Prediction Recognition,\nperformance variance is minimal, with top models scoring between 39.16 and\n50.00. We find that while current LLMs handle routine finance queries\ncompetently, they struggle with complex scenarios requiring cross-concept\nreasoning. BizFinBench offers a rigorous, business-aligned benchmark for future\nresearch. The code and dataset are available at\nhttps://github.com/HiThink-Research/BizFinBench.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6555df426947208b7741b637/48jI0LlYjRwO4-0kHRV0V.png",
      "https://cdn-uploads.huggingface.co/production/uploads/6555df426947208b7741b637/atuM30TNh72kJtm8zGxoc.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19457.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6555df426947208b7741b637",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6555df426947208b7741b637/b7ply-HyaPKXrPvRNh21K.jpeg",
      "fullname": "Rongjunchen Zhang",
      "name": "Tinker250",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.20258",
      "authors": [
        {
          "_id": "68352b5803548b71276c1a6f",
          "name": "Siye Wu",
          "hidden": false
        },
        {
          "_id": "68352b5803548b71276c1a70",
          "name": "Jian Xie",
          "hidden": false
        },
        {
          "_id": "68352b5803548b71276c1a71",
          "name": "Yikai Zhang",
          "hidden": false
        },
        {
          "_id": "68352b5803548b71276c1a72",
          "name": "Aili Chen",
          "hidden": false
        },
        {
          "_id": "68352b5803548b71276c1a73",
          "name": "Kai Zhang",
          "hidden": false
        },
        {
          "_id": "68352b5803548b71276c1a74",
          "name": "Yu Su",
          "hidden": false
        },
        {
          "_id": "68352b5803548b71276c1a75",
          "name": "Yanghua Xiao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62d65139667051e0a29bffe7/W2uaapL3hKBPi-TA-KDef.mp4"
      ],
      "publishedAt": "2025-05-26T17:38:50.000Z",
      "submittedOnDailyAt": "2025-05-27T01:44:27.885Z",
      "title": "ARM: Adaptive Reasoning Model",
      "submittedOnDailyBy": {
        "_id": "62d65139667051e0a29bffe7",
        "avatarUrl": "/avatars/0252aa2bcd4cf1c8e4b87e5f164b6da5.svg",
        "isPro": false,
        "fullname": "Jian Xie",
        "user": "hsaest",
        "type": "user"
      },
      "summary": "While large reasoning models demonstrate strong performance on complex tasks,\nthey lack the ability to adjust reasoning token usage based on task difficulty.\nThis often leads to the \"overthinking\" problem -- excessive and unnecessary\nreasoning -- which, although potentially mitigated by human intervention to\ncontrol the token budget, still fundamentally contradicts the goal of achieving\nfully autonomous AI. In this work, we propose Adaptive Reasoning Model (ARM), a\nreasoning model capable of adaptively selecting appropriate reasoning formats\nbased on the task at hand. These formats include three efficient ones -- Direct\nAnswer, Short CoT, and Code -- as well as a more elaborate format, Long CoT. To\ntrain ARM, we introduce Ada-GRPO, an adaptation of Group Relative Policy\nOptimization (GRPO), which addresses the format collapse issue in traditional\nGRPO. Ada-GRPO enables ARM to achieve high token efficiency, reducing tokens by\nan average of 30%, and up to 70%, while maintaining performance comparable to\nthe model that relies solely on Long CoT. Furthermore, not only does it improve\ninference efficiency through reduced token generation, but it also brings a 2x\nspeedup in training. In addition to the default Adaptive Mode, ARM supports two\nadditional reasoning modes: 1) Instruction-Guided Mode, which allows users to\nexplicitly specify the reasoning format via special tokens -- ideal when the\nappropriate format is known for a batch of tasks. 2) Consensus-Guided Mode,\nwhich aggregates the outputs of the three efficient formats and resorts to Long\nCoT in case of disagreement, prioritizing performance with higher token usage.",
      "upvotes": 27,
      "discussionId": "68352b5903548b71276c1a9f",
      "projectPage": "https://team-arm.github.io/arm/",
      "githubRepo": "https://github.com/TEAM-ARM/arm",
      "ai_summary": "Adaptive Reasoning Model (ARM) uses Ada-GRPO to reduce token usage and improve efficiency across different reasoning modes.",
      "ai_keywords": [
        "Adaptive Reasoning Model",
        "ARM",
        "Ada-GRPO",
        "Group Relative Policy Optimization",
        "GRPO",
        "format collapse",
        "token efficiency",
        "inference efficiency",
        "Direct Answer",
        "Short CoT",
        "Code",
        "Long CoT",
        "Adaptive Mode",
        "Instruction-Guided Mode",
        "Consensus-Guided Mode"
      ]
    },
    "publishedAt": "2025-05-26T13:38:50.000Z",
    "title": "ARM: Adaptive Reasoning Model",
    "summary": "While large reasoning models demonstrate strong performance on complex tasks,\nthey lack the ability to adjust reasoning token usage based on task difficulty.\nThis often leads to the \"overthinking\" problem -- excessive and unnecessary\nreasoning -- which, although potentially mitigated by human intervention to\ncontrol the token budget, still fundamentally contradicts the goal of achieving\nfully autonomous AI. In this work, we propose Adaptive Reasoning Model (ARM), a\nreasoning model capable of adaptively selecting appropriate reasoning formats\nbased on the task at hand. These formats include three efficient ones -- Direct\nAnswer, Short CoT, and Code -- as well as a more elaborate format, Long CoT. To\ntrain ARM, we introduce Ada-GRPO, an adaptation of Group Relative Policy\nOptimization (GRPO), which addresses the format collapse issue in traditional\nGRPO. Ada-GRPO enables ARM to achieve high token efficiency, reducing tokens by\nan average of 30%, and up to 70%, while maintaining performance comparable to\nthe model that relies solely on Long CoT. Furthermore, not only does it improve\ninference efficiency through reduced token generation, but it also brings a 2x\nspeedup in training. In addition to the default Adaptive Mode, ARM supports two\nadditional reasoning modes: 1) Instruction-Guided Mode, which allows users to\nexplicitly specify the reasoning format via special tokens -- ideal when the\nappropriate format is known for a batch of tasks. 2) Consensus-Guided Mode,\nwhich aggregates the outputs of the three efficient formats and resorts to Long\nCoT in case of disagreement, prioritizing performance with higher token usage.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62d65139667051e0a29bffe7/W2uaapL3hKBPi-TA-KDef.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20258.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "62d65139667051e0a29bffe7",
      "avatarUrl": "/avatars/0252aa2bcd4cf1c8e4b87e5f164b6da5.svg",
      "fullname": "Jian Xie",
      "name": "hsaest",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.19914",
      "authors": [
        {
          "_id": "68353e41f995630ab88c198b",
          "user": {
            "_id": "606ed1884ffe81d1e03e81e5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1639375346654-606ed1884ffe81d1e03e81e5.png",
            "isPro": false,
            "fullname": "Jiangjie Chen",
            "user": "jiangjiechen",
            "type": "user"
          },
          "name": "Jiangjie Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-27T07:49:21.006Z",
          "hidden": false
        },
        {
          "_id": "68353e41f995630ab88c198c",
          "user": {
            "_id": "636b36351340f879a2ec2bb1",
            "avatarUrl": "/avatars/260a1c15f9c14c967125469072020946.svg",
            "isPro": false,
            "fullname": "QianyuHe",
            "user": "Abbey4799",
            "type": "user"
          },
          "name": "Qianyu He",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-27T07:49:23.290Z",
          "hidden": false
        },
        {
          "_id": "68353e41f995630ab88c198d",
          "name": "Siyu Yuan",
          "hidden": false
        },
        {
          "_id": "68353e41f995630ab88c198e",
          "name": "Aili Chen",
          "hidden": false
        },
        {
          "_id": "68353e41f995630ab88c198f",
          "name": "Zhicheng Cai",
          "hidden": false
        },
        {
          "_id": "68353e41f995630ab88c1990",
          "name": "Weinan Dai",
          "hidden": false
        },
        {
          "_id": "68353e41f995630ab88c1991",
          "name": "Hongli Yu",
          "hidden": false
        },
        {
          "_id": "68353e41f995630ab88c1992",
          "name": "Qiying Yu",
          "hidden": false
        },
        {
          "_id": "68353e41f995630ab88c1993",
          "name": "Xuefeng Li",
          "hidden": false
        },
        {
          "_id": "68353e41f995630ab88c1994",
          "name": "Jiaze Chen",
          "hidden": false
        },
        {
          "_id": "68353e41f995630ab88c1995",
          "name": "Hao Zhou",
          "hidden": false
        },
        {
          "_id": "68353e41f995630ab88c1996",
          "name": "Mingxuan Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-26T12:40:31.000Z",
      "submittedOnDailyAt": "2025-05-27T02:57:13.989Z",
      "title": "Enigmata: Scaling Logical Reasoning in Large Language Models with\n  Synthetic Verifiable Puzzles",
      "submittedOnDailyBy": {
        "_id": "62d62b333bf5e059f7d2b286",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1668513815771-62d62b333bf5e059f7d2b286.jpeg",
        "isPro": false,
        "fullname": "Siyu Yuan",
        "user": "siyuyuan",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs), such as OpenAI's o1 and DeepSeek's R1, excel at\nadvanced reasoning tasks like math and coding via Reinforcement Learning with\nVerifiable Rewards (RLVR), but still struggle with puzzles solvable by humans\nwithout domain knowledge. We introduce Enigmata, the first comprehensive suite\ntailored for improving LLMs with puzzle reasoning skills. It includes 36 tasks\nacross seven categories, each with 1) a generator that produces unlimited\nexamples with controllable difficulty and 2) a rule-based verifier for\nautomatic evaluation. This generator-verifier design supports scalable,\nmulti-task RL training, fine-grained analysis, and seamless RLVR integration.\nWe further propose Enigmata-Eval, a rigorous benchmark, and develop optimized\nmulti-task RLVR strategies. Our trained model, Qwen2.5-32B-Enigmata,\nconsistently surpasses o3-mini-high and o1 on the puzzle reasoning benchmarks\nlike Enigmata-Eval, ARC-AGI (32.8%), and ARC-AGI 2 (0.6%). It also generalizes\nwell to out-of-domain puzzle benchmarks and mathematical reasoning, with little\nmulti-tasking trade-off. When trained on larger models like Seed1.5-Thinking\n(20B activated parameters and 200B total parameters), puzzle data from Enigmata\nfurther boosts SoTA performance on advanced math and STEM reasoning tasks such\nas AIME (2024-2025), BeyondAIME and GPQA (Diamond), showing nice generalization\nbenefits of Enigmata. This work offers a unified, controllable framework for\nadvancing logical reasoning in LLMs. Resources of this work can be found at\nhttps://seed-enigmata.github.io.",
      "upvotes": 20,
      "discussionId": "68353e42f995630ab88c19dc",
      "ai_summary": "Enigmata is a comprehensive suite for improving LLMs in puzzle reasoning through scalable multi-task RL training, leading to better performance on benchmarks and advanced math tasks.",
      "ai_keywords": [
        "Large Language Models",
        "OpenAI's o1",
        "DeepSeek's R1",
        "Reinforcement Learning with Verifiable Rewards",
        "Enigmata",
        "Enigmata-Eval",
        "multi-task RL training",
        "puzzle reasoning",
        "rule-based verifier",
        "ARC-AGI",
        "Qwen2.5-32B-Enigmata",
        "Seed1.5-Thinking",
        "AIME",
        "BeyondAIME",
        "GPQA"
      ]
    },
    "publishedAt": "2025-05-26T08:40:31.000Z",
    "title": "Enigmata: Scaling Logical Reasoning in Large Language Models with\n  Synthetic Verifiable Puzzles",
    "summary": "Large Language Models (LLMs), such as OpenAI's o1 and DeepSeek's R1, excel at\nadvanced reasoning tasks like math and coding via Reinforcement Learning with\nVerifiable Rewards (RLVR), but still struggle with puzzles solvable by humans\nwithout domain knowledge. We introduce Enigmata, the first comprehensive suite\ntailored for improving LLMs with puzzle reasoning skills. It includes 36 tasks\nacross seven categories, each with 1) a generator that produces unlimited\nexamples with controllable difficulty and 2) a rule-based verifier for\nautomatic evaluation. This generator-verifier design supports scalable,\nmulti-task RL training, fine-grained analysis, and seamless RLVR integration.\nWe further propose Enigmata-Eval, a rigorous benchmark, and develop optimized\nmulti-task RLVR strategies. Our trained model, Qwen2.5-32B-Enigmata,\nconsistently surpasses o3-mini-high and o1 on the puzzle reasoning benchmarks\nlike Enigmata-Eval, ARC-AGI (32.8%), and ARC-AGI 2 (0.6%). It also generalizes\nwell to out-of-domain puzzle benchmarks and mathematical reasoning, with little\nmulti-tasking trade-off. When trained on larger models like Seed1.5-Thinking\n(20B activated parameters and 200B total parameters), puzzle data from Enigmata\nfurther boosts SoTA performance on advanced math and STEM reasoning tasks such\nas AIME (2024-2025), BeyondAIME and GPQA (Diamond), showing nice generalization\nbenefits of Enigmata. This work offers a unified, controllable framework for\nadvancing logical reasoning in LLMs. Resources of this work can be found at\nhttps://seed-enigmata.github.io.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19914.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62d62b333bf5e059f7d2b286",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1668513815771-62d62b333bf5e059f7d2b286.jpeg",
      "fullname": "Siyu Yuan",
      "name": "siyuyuan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.20259",
      "authors": [
        {
          "_id": "6835346b2fdc5f8e8ea1e3cf",
          "name": "Haoyu Wang",
          "hidden": false
        },
        {
          "_id": "6835346b2fdc5f8e8ea1e3d0",
          "name": "Zeyu Qin",
          "hidden": false
        },
        {
          "_id": "6835346b2fdc5f8e8ea1e3d1",
          "name": "Yifei Zhao",
          "hidden": false
        },
        {
          "_id": "6835346b2fdc5f8e8ea1e3d2",
          "name": "Chao Du",
          "hidden": false
        },
        {
          "_id": "6835346b2fdc5f8e8ea1e3d3",
          "name": "Min Lin",
          "hidden": false
        },
        {
          "_id": "6835346b2fdc5f8e8ea1e3d4",
          "name": "Xueqian Wang",
          "hidden": false
        },
        {
          "_id": "6835346b2fdc5f8e8ea1e3d5",
          "name": "Tianyu Pang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-26T17:40:40.000Z",
      "submittedOnDailyAt": "2025-05-27T02:13:04.341Z",
      "title": "Lifelong Safety Alignment for Language Models",
      "submittedOnDailyBy": {
        "_id": "63d91b6d255ef6add20e1b38",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675921369867-63d91b6d255ef6add20e1b38.jpeg",
        "isPro": false,
        "fullname": "Tianyu Pang",
        "user": "P2333",
        "type": "user"
      },
      "summary": "LLMs have made impressive progress, but their growing capabilities also\nexpose them to highly flexible jailbreaking attacks designed to bypass safety\nalignment. While many existing defenses focus on known types of attacks, it is\nmore critical to prepare LLMs for unseen attacks that may arise during\ndeployment. To address this, we propose a lifelong safety alignment framework\nthat enables LLMs to continuously adapt to new and evolving jailbreaking\nstrategies. Our framework introduces a competitive setup between two\ncomponents: a Meta-Attacker, trained to actively discover novel jailbreaking\nstrategies, and a Defender, trained to resist them. To effectively warm up the\nMeta-Attacker, we first leverage the GPT-4o API to extract key insights from a\nlarge collection of jailbreak-related research papers. Through iterative\ntraining, the first iteration Meta-Attacker achieves a 73% attack success rate\n(ASR) on RR and a 57% transfer ASR on LAT using only single-turn attacks.\nMeanwhile, the Defender progressively improves its robustness and ultimately\nreduces the Meta-Attacker's success rate to just 7%, enabling safer and more\nreliable deployment of LLMs in open-ended environments. The code is available\nat https://github.com/sail-sg/LifelongSafetyAlignment.",
      "upvotes": 19,
      "discussionId": "6835346c2fdc5f8e8ea1e407",
      "githubRepo": "https://github.com/sail-sg/LifelongSafetyAlignment",
      "ai_summary": "A lifecycle safety alignment framework employs a Meta-Attacker and Defender to adapt LLMs to novel jailbreaking strategies, improving robustness in deployment.",
      "ai_keywords": [
        "LLMs",
        "jailbreaking attacks",
        "safety alignment",
        "lifelong safety alignment framework",
        "Meta-Attacker",
        "Defender",
        "GPT-4o API",
        "attack success rate",
        "transfer attack success rate",
        "single-turn attacks"
      ]
    },
    "publishedAt": "2025-05-26T13:40:40.000Z",
    "title": "Lifelong Safety Alignment for Language Models",
    "summary": "LLMs have made impressive progress, but their growing capabilities also\nexpose them to highly flexible jailbreaking attacks designed to bypass safety\nalignment. While many existing defenses focus on known types of attacks, it is\nmore critical to prepare LLMs for unseen attacks that may arise during\ndeployment. To address this, we propose a lifelong safety alignment framework\nthat enables LLMs to continuously adapt to new and evolving jailbreaking\nstrategies. Our framework introduces a competitive setup between two\ncomponents: a Meta-Attacker, trained to actively discover novel jailbreaking\nstrategies, and a Defender, trained to resist them. To effectively warm up the\nMeta-Attacker, we first leverage the GPT-4o API to extract key insights from a\nlarge collection of jailbreak-related research papers. Through iterative\ntraining, the first iteration Meta-Attacker achieves a 73% attack success rate\n(ASR) on RR and a 57% transfer ASR on LAT using only single-turn attacks.\nMeanwhile, the Defender progressively improves its robustness and ultimately\nreduces the Meta-Attacker's success rate to just 7%, enabling safer and more\nreliable deployment of LLMs in open-ended environments. The code is available\nat https://github.com/sail-sg/LifelongSafetyAlignment.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20259.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63d91b6d255ef6add20e1b38",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675921369867-63d91b6d255ef6add20e1b38.jpeg",
      "fullname": "Tianyu Pang",
      "name": "P2333",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.18675",
      "authors": [
        {
          "_id": "68351dde0c0aff775f3933ee",
          "name": "Sicheng Feng",
          "hidden": false
        },
        {
          "_id": "68351dde0c0aff775f3933ef",
          "name": "Song Wang",
          "hidden": false
        },
        {
          "_id": "68351dde0c0aff775f3933f0",
          "name": "Shuyi Ouyang",
          "hidden": false
        },
        {
          "_id": "68351dde0c0aff775f3933f1",
          "name": "Lingdong Kong",
          "hidden": false
        },
        {
          "_id": "68351dde0c0aff775f3933f2",
          "name": "Zikai Song",
          "hidden": false
        },
        {
          "_id": "68351dde0c0aff775f3933f3",
          "name": "Jianke Zhu",
          "hidden": false
        },
        {
          "_id": "68351dde0c0aff775f3933f4",
          "name": "Huan Wang",
          "hidden": false
        },
        {
          "_id": "68351dde0c0aff775f3933f5",
          "name": "Xinchao Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-24T12:33:52.000Z",
      "submittedOnDailyAt": "2025-05-27T00:35:52.585Z",
      "title": "Can MLLMs Guide Me Home? A Benchmark Study on Fine-Grained Visual\n  Reasoning from Transit Maps",
      "submittedOnDailyBy": {
        "_id": "67a4a26d5e65aa63c6d30e68",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67a4a26d5e65aa63c6d30e68/GtodlJGw-_IL2DTXQTucz.jpeg",
        "isPro": false,
        "fullname": "FENG SICHENG",
        "user": "FSCCS",
        "type": "user"
      },
      "summary": "Multimodal large language models (MLLMs) have recently achieved significant\nprogress in visual tasks, including semantic scene understanding and text-image\nalignment, with reasoning variants enhancing performance on complex tasks\ninvolving mathematics and logic. However, their capacity for reasoning tasks\ninvolving fine-grained visual understanding remains insufficiently evaluated.\nTo address this gap, we introduce ReasonMap, a benchmark designed to assess the\nfine-grained visual understanding and spatial reasoning abilities of MLLMs.\nReasonMap encompasses high-resolution transit maps from 30 cities across 13\ncountries and includes 1,008 question-answer pairs spanning two question types\nand three templates. Furthermore, we design a two-level evaluation pipeline\nthat properly assesses answer correctness and quality. Comprehensive\nevaluations of 15 popular MLLMs, including both base and reasoning variants,\nreveal a counterintuitive pattern: among open-source models, base models\noutperform reasoning ones, while the opposite trend is observed in\nclosed-source models. Additionally, performance generally degrades when visual\ninputs are masked, indicating that while MLLMs can leverage prior knowledge to\nanswer some questions, fine-grained visual reasoning tasks still require\ngenuine visual perception for strong performance. Our benchmark study offers\nnew insights into visual reasoning and contributes to investigating the gap\nbetween open-source and closed-source models.",
      "upvotes": 19,
      "discussionId": "68351de10c0aff775f39347a",
      "projectPage": "https://fscdc.github.io/Reason-Map/",
      "githubRepo": "https://github.com/fscdc/ReasonMap",
      "ai_summary": "ReasonMap evaluates the fine-grained visual understanding and spatial reasoning abilities of multimodal large language models, revealing that base models often outperform reasoning variants and highlighting the importance of genuine visual perception for complex tasks.",
      "ai_keywords": [
        "multimodal large language models",
        "MLLMs",
        "semantic scene understanding",
        "text-image alignment",
        "reasoning variants",
        "ReasonMap",
        "high-resolution transit maps",
        "question-answer pairs",
        "two-level evaluation pipeline",
        "open-source models",
        "closed-source models",
        "visual reasoning"
      ]
    },
    "publishedAt": "2025-05-24T08:33:52.000Z",
    "title": "Can MLLMs Guide Me Home? A Benchmark Study on Fine-Grained Visual\n  Reasoning from Transit Maps",
    "summary": "Multimodal large language models (MLLMs) have recently achieved significant\nprogress in visual tasks, including semantic scene understanding and text-image\nalignment, with reasoning variants enhancing performance on complex tasks\ninvolving mathematics and logic. However, their capacity for reasoning tasks\ninvolving fine-grained visual understanding remains insufficiently evaluated.\nTo address this gap, we introduce ReasonMap, a benchmark designed to assess the\nfine-grained visual understanding and spatial reasoning abilities of MLLMs.\nReasonMap encompasses high-resolution transit maps from 30 cities across 13\ncountries and includes 1,008 question-answer pairs spanning two question types\nand three templates. Furthermore, we design a two-level evaluation pipeline\nthat properly assesses answer correctness and quality. Comprehensive\nevaluations of 15 popular MLLMs, including both base and reasoning variants,\nreveal a counterintuitive pattern: among open-source models, base models\noutperform reasoning ones, while the opposite trend is observed in\nclosed-source models. Additionally, performance generally degrades when visual\ninputs are masked, indicating that while MLLMs can leverage prior knowledge to\nanswer some questions, fine-grained visual reasoning tasks still require\ngenuine visual perception for strong performance. Our benchmark study offers\nnew insights into visual reasoning and contributes to investigating the gap\nbetween open-source and closed-source models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18675.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "67a4a26d5e65aa63c6d30e68",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67a4a26d5e65aa63c6d30e68/GtodlJGw-_IL2DTXQTucz.jpeg",
      "fullname": "FENG SICHENG",
      "name": "FSCCS",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 0
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.19815",
      "authors": [
        {
          "_id": "683523b21a2911c0774a1dc5",
          "name": "Junnan Liu",
          "hidden": false
        },
        {
          "_id": "683523b21a2911c0774a1dc6",
          "name": "Hongwei Liu",
          "hidden": false
        },
        {
          "_id": "683523b21a2911c0774a1dc7",
          "name": "Linchen Xiao",
          "hidden": false
        },
        {
          "_id": "683523b21a2911c0774a1dc8",
          "name": "Shudong Liu",
          "hidden": false
        },
        {
          "_id": "683523b21a2911c0774a1dc9",
          "name": "Taolin Zhang",
          "hidden": false
        },
        {
          "_id": "683523b21a2911c0774a1dca",
          "name": "Zihan Ma",
          "hidden": false
        },
        {
          "_id": "683523b21a2911c0774a1dcb",
          "name": "Songyang Zhang",
          "hidden": false
        },
        {
          "_id": "683523b21a2911c0774a1dcc",
          "name": "Kai Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-26T10:52:17.000Z",
      "submittedOnDailyAt": "2025-05-27T01:00:26.595Z",
      "title": "Deciphering Trajectory-Aided LLM Reasoning: An Optimization Perspective",
      "submittedOnDailyBy": {
        "_id": "630716d11801ecc7d2595021",
        "avatarUrl": "/avatars/2d36a880ce4a3cf7efc5ff3987dbeaf3.svg",
        "isPro": false,
        "fullname": "Songyang Zhang",
        "user": "zsytony",
        "type": "user"
      },
      "summary": "We propose a novel framework for comprehending the reasoning capabilities of\nlarge language models (LLMs) through the perspective of meta-learning. By\nconceptualizing reasoning trajectories as pseudo-gradient descent updates to\nthe LLM's parameters, we identify parallels between LLM reasoning and various\nmeta-learning paradigms. We formalize the training process for reasoning tasks\nas a meta-learning setup, with each question treated as an individual task, and\nreasoning trajectories serving as the inner loop optimization for adapting\nmodel parameters. Once trained on a diverse set of questions, the LLM develops\nfundamental reasoning capabilities that can generalize to previously unseen\nquestions. Extensive empirical evaluations substantiate the strong connection\nbetween LLM reasoning and meta-learning, exploring several issues of\nsignificant interest from a meta-learning standpoint. Our work not only\nenhances the understanding of LLM reasoning but also provides practical\ninsights for improving these models through established meta-learning\ntechniques.",
      "upvotes": 17,
      "discussionId": "683523b41a2911c0774a1e78",
      "githubRepo": "https://github.com/open-compass/RaML",
      "ai_summary": "LLM reasoning is understood through a meta-learning framework, treating reasoning as pseudo-gradient descent and questions as individual tasks, which enhances generalization and provides practical insights for improvement.",
      "ai_keywords": [
        "large language models",
        "meta-learning",
        "pseudo-gradient descent",
        "inner loop optimization",
        "generalization",
        "fundamental reasoning capabilities"
      ]
    },
    "publishedAt": "2025-05-26T06:52:17.000Z",
    "title": "Deciphering Trajectory-Aided LLM Reasoning: An Optimization Perspective",
    "summary": "We propose a novel framework for comprehending the reasoning capabilities of\nlarge language models (LLMs) through the perspective of meta-learning. By\nconceptualizing reasoning trajectories as pseudo-gradient descent updates to\nthe LLM's parameters, we identify parallels between LLM reasoning and various\nmeta-learning paradigms. We formalize the training process for reasoning tasks\nas a meta-learning setup, with each question treated as an individual task, and\nreasoning trajectories serving as the inner loop optimization for adapting\nmodel parameters. Once trained on a diverse set of questions, the LLM develops\nfundamental reasoning capabilities that can generalize to previously unseen\nquestions. Extensive empirical evaluations substantiate the strong connection\nbetween LLM reasoning and meta-learning, exploring several issues of\nsignificant interest from a meta-learning standpoint. Our work not only\nenhances the understanding of LLM reasoning but also provides practical\ninsights for improving these models through established meta-learning\ntechniques.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19815.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "630716d11801ecc7d2595021",
      "avatarUrl": "/avatars/2d36a880ce4a3cf7efc5ff3987dbeaf3.svg",
      "fullname": "Songyang Zhang",
      "name": "zsytony",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 17
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.19209",
      "authors": [
        {
          "_id": "683529e8ddbf19d1df9038fb",
          "name": "Zonglin Yang",
          "hidden": false
        },
        {
          "_id": "683529e8ddbf19d1df9038fc",
          "name": "Wanhao Liu",
          "hidden": false
        },
        {
          "_id": "683529e8ddbf19d1df9038fd",
          "name": "Ben Gao",
          "hidden": false
        },
        {
          "_id": "683529e8ddbf19d1df9038fe",
          "name": "Yujie Liu",
          "hidden": false
        },
        {
          "_id": "683529e8ddbf19d1df9038ff",
          "name": "Wei Li",
          "hidden": false
        },
        {
          "_id": "683529e8ddbf19d1df903900",
          "name": "Tong Xie",
          "hidden": false
        },
        {
          "_id": "683529e8ddbf19d1df903901",
          "name": "Lidong Bing",
          "hidden": false
        },
        {
          "_id": "683529e8ddbf19d1df903902",
          "name": "Wanli Ouyang",
          "hidden": false
        },
        {
          "_id": "683529e8ddbf19d1df903903",
          "name": "Erik Cambria",
          "hidden": false
        },
        {
          "_id": "683529e8ddbf19d1df903904",
          "name": "Dongzhan Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-25T16:13:46.000Z",
      "submittedOnDailyAt": "2025-05-27T01:30:07.083Z",
      "title": "MOOSE-Chem2: Exploring LLM Limits in Fine-Grained Scientific Hypothesis\n  Discovery via Hierarchical Search",
      "submittedOnDailyBy": {
        "_id": "646a11791556443f24b582e9",
        "avatarUrl": "/avatars/b54d416ddca2f124492ba51bc4b95fd2.svg",
        "isPro": false,
        "fullname": "Zonglin Yang",
        "user": "ZonglinY",
        "type": "user"
      },
      "summary": "Large language models (LLMs) have shown promise in automating scientific\nhypothesis generation, yet existing approaches primarily yield coarse-grained\nhypotheses lacking critical methodological and experimental details. We\nintroduce and formally define the novel task of fine-grained scientific\nhypothesis discovery, which entails generating detailed, experimentally\nactionable hypotheses from coarse initial research directions. We frame this as\na combinatorial optimization problem and investigate the upper limits of LLMs'\ncapacity to solve it when maximally leveraged. Specifically, we explore four\nfoundational questions: (1) how to best harness an LLM's internal heuristics to\nformulate the fine-grained hypothesis it itself would judge as the most\npromising among all the possible hypotheses it might generate, based on its own\ninternal scoring-thus defining a latent reward landscape over the hypothesis\nspace; (2) whether such LLM-judged better hypotheses exhibit stronger alignment\nwith ground-truth hypotheses; (3) whether shaping the reward landscape using an\nensemble of diverse LLMs of similar capacity yields better outcomes than\ndefining it with repeated instances of the strongest LLM among them; and (4)\nwhether an ensemble of identical LLMs provides a more reliable reward landscape\nthan a single LLM. To address these questions, we propose a hierarchical search\nmethod that incrementally proposes and integrates details into the hypothesis,\nprogressing from general concepts to specific experimental configurations. We\nshow that this hierarchical process smooths the reward landscape and enables\nmore effective optimization. Empirical evaluations on a new benchmark of\nexpert-annotated fine-grained hypotheses from recent chemistry literature show\nthat our method consistently outperforms strong baselines.",
      "upvotes": 16,
      "discussionId": "683529e9ddbf19d1df903939",
      "ai_summary": "A method is proposed to generate detailed scientific hypotheses using LLMs by defining and optimizing a latent reward landscape, outperforming baselines in benchmark evaluations.",
      "ai_keywords": [
        "large language models",
        "fine-grained scientific hypothesis discovery",
        "combinatorial optimization",
        "latent reward landscape",
        "hierarchical search method"
      ]
    },
    "publishedAt": "2025-05-25T12:13:46.000Z",
    "title": "MOOSE-Chem2: Exploring LLM Limits in Fine-Grained Scientific Hypothesis\n  Discovery via Hierarchical Search",
    "summary": "Large language models (LLMs) have shown promise in automating scientific\nhypothesis generation, yet existing approaches primarily yield coarse-grained\nhypotheses lacking critical methodological and experimental details. We\nintroduce and formally define the novel task of fine-grained scientific\nhypothesis discovery, which entails generating detailed, experimentally\nactionable hypotheses from coarse initial research directions. We frame this as\na combinatorial optimization problem and investigate the upper limits of LLMs'\ncapacity to solve it when maximally leveraged. Specifically, we explore four\nfoundational questions: (1) how to best harness an LLM's internal heuristics to\nformulate the fine-grained hypothesis it itself would judge as the most\npromising among all the possible hypotheses it might generate, based on its own\ninternal scoring-thus defining a latent reward landscape over the hypothesis\nspace; (2) whether such LLM-judged better hypotheses exhibit stronger alignment\nwith ground-truth hypotheses; (3) whether shaping the reward landscape using an\nensemble of diverse LLMs of similar capacity yields better outcomes than\ndefining it with repeated instances of the strongest LLM among them; and (4)\nwhether an ensemble of identical LLMs provides a more reliable reward landscape\nthan a single LLM. To address these questions, we propose a hierarchical search\nmethod that incrementally proposes and integrates details into the hypothesis,\nprogressing from general concepts to specific experimental configurations. We\nshow that this hierarchical process smooths the reward landscape and enables\nmore effective optimization. Empirical evaluations on a new benchmark of\nexpert-annotated fine-grained hypotheses from recent chemistry literature show\nthat our method consistently outperforms strong baselines.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19209.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "646a11791556443f24b582e9",
      "avatarUrl": "/avatars/b54d416ddca2f124492ba51bc4b95fd2.svg",
      "fullname": "Zonglin Yang",
      "name": "ZonglinY",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.18545",
      "authors": [
        {
          "_id": "6835217ee759f596d018f72c",
          "name": "An Vo",
          "hidden": false
        },
        {
          "_id": "6835217ee759f596d018f72d",
          "user": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "name": "Mohammad Reza Taesiri",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-27T02:21:46.082Z",
          "hidden": false
        },
        {
          "_id": "6835217ee759f596d018f72e",
          "name": "Daeyoung Kim",
          "hidden": false
        },
        {
          "_id": "6835217ee759f596d018f72f",
          "user": {
            "_id": "60e85b3fcd1cf4e418fff651",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1645625108920-60e85b3fcd1cf4e418fff651.jpeg",
            "isPro": false,
            "fullname": "Anh (Totti) Nguyen",
            "user": "anhng8",
            "type": "user"
          },
          "name": "Anh Totti Nguyen",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-27T02:20:46.958Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-24T06:23:52.000Z",
      "submittedOnDailyAt": "2025-05-27T00:50:49.797Z",
      "title": "B-score: Detecting biases in large language models using response\n  history",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Large language models (LLMs) often exhibit strong biases, e.g, against women\nor in favor of the number 7. We investigate whether LLMs would be able to\noutput less biased answers when allowed to observe their prior answers to the\nsame question in a multi-turn conversation. To understand which types of\nquestions invite more biased answers, we test LLMs on our proposed set of\nquestions that span 9 topics and belong to three types: (1) Subjective; (2)\nRandom; and (3) Objective. Interestingly, LLMs are able to \"de-bias\" themselves\nin a multi-turn conversation in response to questions that seek an Random,\nunbiased answer. Furthermore, we propose B-score, a novel metric that is\neffective in detecting biases to Subjective, Random, Easy, and Hard questions.\nOn MMLU, HLE, and CSQA, leveraging B-score substantially improves the\nverification accuracy of LLM answers (i.e, accepting LLM correct answers and\nrejecting incorrect ones) compared to using verbalized confidence scores or the\nfrequency of single-turn answers alone. Code and data are available at:\nhttps://b-score.github.io.",
      "upvotes": 16,
      "discussionId": "6835217ee759f596d018f794",
      "projectPage": "https://b-score.github.io/",
      "githubRepo": "https://github.com/anvo25/b-score",
      "ai_summary": "LLMs can reduce biases in multi-turn conversations for certain types of questions, and a novel B-score metric improves the accuracy of verifying LLM answers.",
      "ai_keywords": [
        "large language models",
        "biases",
        "multi-turn conversation",
        "B-score",
        "MMLU",
        "HLE",
        "CSQA",
        "verification accuracy",
        "verbalized confidence scores"
      ]
    },
    "publishedAt": "2025-05-24T02:23:52.000Z",
    "title": "B-score: Detecting biases in large language models using response\n  history",
    "summary": "Large language models (LLMs) often exhibit strong biases, e.g, against women\nor in favor of the number 7. We investigate whether LLMs would be able to\noutput less biased answers when allowed to observe their prior answers to the\nsame question in a multi-turn conversation. To understand which types of\nquestions invite more biased answers, we test LLMs on our proposed set of\nquestions that span 9 topics and belong to three types: (1) Subjective; (2)\nRandom; and (3) Objective. Interestingly, LLMs are able to \"de-bias\" themselves\nin a multi-turn conversation in response to questions that seek an Random,\nunbiased answer. Furthermore, we propose B-score, a novel metric that is\neffective in detecting biases to Subjective, Random, Easy, and Hard questions.\nOn MMLU, HLE, and CSQA, leveraging B-score substantially improves the\nverification accuracy of LLM answers (i.e, accepting LLM correct answers and\nrejecting incorrect ones) compared to using verbalized confidence scores or the\nfrequency of single-turn answers alone. Code and data are available at:\nhttps://b-score.github.io.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18545.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 84
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.18536",
      "authors": [
        {
          "_id": "68351f7a06b4dae20a214442",
          "name": "Haoyuan Sun",
          "hidden": false
        },
        {
          "_id": "68351f7a06b4dae20a214443",
          "name": "Jiaqi Wu",
          "hidden": false
        },
        {
          "_id": "68351f7a06b4dae20a214444",
          "name": "Bo Xia",
          "hidden": false
        },
        {
          "_id": "68351f7a06b4dae20a214445",
          "name": "Yifu Luo",
          "hidden": false
        },
        {
          "_id": "68351f7a06b4dae20a214446",
          "name": "Yifei Zhao",
          "hidden": false
        },
        {
          "_id": "68351f7a06b4dae20a214447",
          "name": "Kai Qin",
          "hidden": false
        },
        {
          "_id": "68351f7a06b4dae20a214448",
          "name": "Xufei Lv",
          "hidden": false
        },
        {
          "_id": "68351f7a06b4dae20a214449",
          "name": "Tiantian Zhang",
          "hidden": false
        },
        {
          "_id": "68351f7a06b4dae20a21444a",
          "name": "Yongzhe Chang",
          "hidden": false
        },
        {
          "_id": "68351f7a06b4dae20a21444b",
          "name": "Xueqian Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-24T06:01:48.000Z",
      "submittedOnDailyAt": "2025-05-27T00:43:18.404Z",
      "title": "Reinforcement Fine-Tuning Powers Reasoning Capability of Multimodal\n  Large Language Models",
      "submittedOnDailyBy": {
        "_id": "65e2d43f9fb58a5115253049",
        "avatarUrl": "/avatars/46bd4ae27eaa23802cef3d91626897b5.svg",
        "isPro": false,
        "fullname": "Haoyuan Sun",
        "user": "xiaonengmiao",
        "type": "user"
      },
      "summary": "Standing in 2025, at a critical juncture in the pursuit of Artificial General\nIntelligence (AGI), reinforcement fine-tuning (RFT) has demonstrated\nsignificant potential in enhancing the reasoning capability of large language\nmodels (LLMs) and has led to the development of cutting-edge AI models such as\nOpenAI-o1 and DeepSeek-R1. Moreover, the efficient application of RFT to\nenhance the reasoning capability of multimodal large language models (MLLMs)\nhas attracted widespread attention from the community. In this position paper,\nwe argue that reinforcement fine-tuning powers the reasoning capability of\nmultimodal large language models. To begin with, we provide a detailed\nintroduction to the fundamental background knowledge that researchers\ninterested in this field should be familiar with. Furthermore, we meticulously\nsummarize the improvements of RFT in powering reasoning capability of MLLMs\ninto five key points: diverse modalities, diverse tasks and domains, better\ntraining algorithms, abundant benchmarks and thriving engineering frameworks.\nFinally, we propose five promising directions for future research that the\ncommunity might consider. We hope that this position paper will provide\nvaluable insights to the community at this pivotal stage in the advancement\ntoward AGI. Summary of works done on RFT for MLLMs is available at\nhttps://github.com/Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs.",
      "upvotes": 14,
      "discussionId": "68351f7b06b4dae20a2144b5",
      "projectPage": "https://github.com/Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs",
      "githubRepo": "https://github.com/Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs",
      "ai_summary": "Reinforcement fine-tuning significantly enhances the reasoning capabilities of multimodal large language models through diverse modalities, tasks, algorithms, benchmarks, and frameworks.",
      "ai_keywords": [
        "reinforcement fine-tuning",
        "multimodal large language models",
        "OpenAI-o1",
        "DeepSeek-R1",
        "diverse modalities",
        "diverse tasks and domains",
        "better training algorithms",
        "abundant benchmarks",
        "thriving engineering frameworks"
      ]
    },
    "publishedAt": "2025-05-24T02:01:48.000Z",
    "title": "Reinforcement Fine-Tuning Powers Reasoning Capability of Multimodal\n  Large Language Models",
    "summary": "Standing in 2025, at a critical juncture in the pursuit of Artificial General\nIntelligence (AGI), reinforcement fine-tuning (RFT) has demonstrated\nsignificant potential in enhancing the reasoning capability of large language\nmodels (LLMs) and has led to the development of cutting-edge AI models such as\nOpenAI-o1 and DeepSeek-R1. Moreover, the efficient application of RFT to\nenhance the reasoning capability of multimodal large language models (MLLMs)\nhas attracted widespread attention from the community. In this position paper,\nwe argue that reinforcement fine-tuning powers the reasoning capability of\nmultimodal large language models. To begin with, we provide a detailed\nintroduction to the fundamental background knowledge that researchers\ninterested in this field should be familiar with. Furthermore, we meticulously\nsummarize the improvements of RFT in powering reasoning capability of MLLMs\ninto five key points: diverse modalities, diverse tasks and domains, better\ntraining algorithms, abundant benchmarks and thriving engineering frameworks.\nFinally, we propose five promising directions for future research that the\ncommunity might consider. We hope that this position paper will provide\nvaluable insights to the community at this pivotal stage in the advancement\ntoward AGI. Summary of works done on RFT for MLLMs is available at\nhttps://github.com/Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18536.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "65e2d43f9fb58a5115253049",
      "avatarUrl": "/avatars/46bd4ae27eaa23802cef3d91626897b5.svg",
      "fullname": "Haoyuan Sun",
      "name": "xiaonengmiao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.18601",
      "authors": [
        {
          "_id": "6835268212dd354d6acdacbf",
          "name": "Jongwoo Ko",
          "hidden": false
        },
        {
          "_id": "6835268212dd354d6acdacc0",
          "name": "Sungnyun Kim",
          "hidden": false
        },
        {
          "_id": "6835268212dd354d6acdacc1",
          "name": "Sungwoo Cho",
          "hidden": false
        },
        {
          "_id": "6835268212dd354d6acdacc2",
          "name": "Se-Young Yun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-24T08:50:53.000Z",
      "submittedOnDailyAt": "2025-05-27T01:14:15.443Z",
      "title": "Flex-Judge: Think Once, Judge Anywhere",
      "submittedOnDailyBy": {
        "_id": "63f0c2ac9cf89c9ed1bdd25c",
        "avatarUrl": "/avatars/856b2cb482250fb83c6fe793e29dfd74.svg",
        "isPro": false,
        "fullname": "Sungnyun Kim",
        "user": "sungnyun",
        "type": "user"
      },
      "summary": "Human-generated reward signals are critical for aligning generative models\nwith human preferences, guiding both training and inference-time evaluations.\nWhile large language models (LLMs) employed as proxy evaluators, i.e.,\nLLM-as-a-Judge, significantly reduce the costs associated with manual\nannotations, they typically require extensive modality-specific training data\nand fail to generalize well across diverse multimodal tasks. In this paper, we\npropose Flex-Judge, a reasoning-guided multimodal judge model that leverages\nminimal textual reasoning data to robustly generalize across multiple\nmodalities and evaluation formats. Our core intuition is that structured\ntextual reasoning explanations inherently encode generalizable decision-making\npatterns, enabling an effective transfer to multimodal judgments, e.g., with\nimages or videos. Empirical results demonstrate that Flex-Judge, despite being\ntrained on significantly fewer text data, achieves competitive or superior\nperformance compared to state-of-the-art commercial APIs and extensively\ntrained multimodal evaluators. Notably, Flex-Judge presents broad impact in\nmodalities like molecule, where comprehensive evaluation benchmarks are scarce,\nunderscoring its practical value in resource-constrained domains. Our framework\nhighlights reasoning-based text supervision as a powerful, cost-effective\nalternative to traditional annotation-intensive approaches, substantially\nadvancing scalable multimodal model-as-a-judge.",
      "upvotes": 13,
      "discussionId": "6835268312dd354d6acdad1e",
      "projectPage": "https://flex-judge.github.io/",
      "githubRepo": "https://github.com/jongwooko/flex-judge",
      "ai_summary": "Flex-Judge uses minimal textual reasoning data to generalize across multiple modalities and evaluation formats, outperforming state-of-the-art models in multimodal evaluations.",
      "ai_keywords": [
        "reasoning-guided multimodal judge model",
        "structured textual reasoning explanations",
        "generalizable decision-making patterns",
        "multimodal judgments",
        "molecule evaluations",
        "reasoning-based text supervision",
        "scalable multimodal model-as-a-judge"
      ]
    },
    "publishedAt": "2025-05-24T04:50:53.000Z",
    "title": "Flex-Judge: Think Once, Judge Anywhere",
    "summary": "Human-generated reward signals are critical for aligning generative models\nwith human preferences, guiding both training and inference-time evaluations.\nWhile large language models (LLMs) employed as proxy evaluators, i.e.,\nLLM-as-a-Judge, significantly reduce the costs associated with manual\nannotations, they typically require extensive modality-specific training data\nand fail to generalize well across diverse multimodal tasks. In this paper, we\npropose Flex-Judge, a reasoning-guided multimodal judge model that leverages\nminimal textual reasoning data to robustly generalize across multiple\nmodalities and evaluation formats. Our core intuition is that structured\ntextual reasoning explanations inherently encode generalizable decision-making\npatterns, enabling an effective transfer to multimodal judgments, e.g., with\nimages or videos. Empirical results demonstrate that Flex-Judge, despite being\ntrained on significantly fewer text data, achieves competitive or superior\nperformance compared to state-of-the-art commercial APIs and extensively\ntrained multimodal evaluators. Notably, Flex-Judge presents broad impact in\nmodalities like molecule, where comprehensive evaluation benchmarks are scarce,\nunderscoring its practical value in resource-constrained domains. Our framework\nhighlights reasoning-based text supervision as a powerful, cost-effective\nalternative to traditional annotation-intensive approaches, substantially\nadvancing scalable multimodal model-as-a-judge.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18601.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63f0c2ac9cf89c9ed1bdd25c",
      "avatarUrl": "/avatars/856b2cb482250fb83c6fe793e29dfd74.svg",
      "fullname": "Sungnyun Kim",
      "name": "sungnyun",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.19949",
      "authors": [
        {
          "_id": "68352aac38e5ca9eb5349c2f",
          "name": "Siqi Kou",
          "hidden": false
        },
        {
          "_id": "68352aac38e5ca9eb5349c30",
          "name": "Qingyuan Tian",
          "hidden": false
        },
        {
          "_id": "68352aac38e5ca9eb5349c31",
          "name": "Hanwen Xu",
          "hidden": false
        },
        {
          "_id": "68352aac38e5ca9eb5349c32",
          "name": "Zihao Zeng",
          "hidden": false
        },
        {
          "_id": "68352aac38e5ca9eb5349c33",
          "name": "Zhijie Deng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-26T13:15:26.000Z",
      "submittedOnDailyAt": "2025-05-27T01:30:37.130Z",
      "title": "Which Data Attributes Stimulate Math and Code Reasoning? An\n  Investigation via Influence Functions",
      "submittedOnDailyBy": {
        "_id": "654e330f350abceb30a1390b",
        "avatarUrl": "/avatars/e54a8be788fa1bdc7acefecc208215bb.svg",
        "isPro": false,
        "fullname": "KouSiqi",
        "user": "karrykkk",
        "type": "user"
      },
      "summary": "Large language models (LLMs) have demonstrated remarkable reasoning\ncapabilities in math and coding, often bolstered by post-training on the\nchain-of-thoughts (CoTs) generated by stronger models. However, existing\nstrategies for curating such training data predominantly rely on heuristics,\nlimiting generalizability and failing to capture subtleties underlying in data.\nTo address these limitations, we leverage influence functions to systematically\nattribute LLMs' reasoning ability on math and coding to individual training\nexamples, sequences, and tokens, enabling deeper insights into effective data\ncharacteristics. Our Influence-based Reasoning Attribution (Infra) uncovers\nnontrivial cross-domain effects across math and coding tasks: high-difficulty\nmath examples improve both math and code reasoning, while low-difficulty code\ntasks most effectively benefit code reasoning. Based on these findings, we\nintroduce a simple yet effective dataset reweighting strategy by flipping task\ndifficulty, which doubles AIME24 accuracy from 10\\% to 20\\% and boosts\nLiveCodeBench accuracy from 33.8\\% to 35.3\\% for Qwen2.5-7B-Instruct. Moreover,\nour fine-grained attribution reveals that the sequence-level exploratory\nbehaviors enhance reasoning performance in both math and code, and the\ntoken-level influence patterns are distinct for math and code reasoning: the\nformer prefers natural language logic connectors and the latter emphasizes\nstructural syntax.",
      "upvotes": 11,
      "discussionId": "68352aad38e5ca9eb5349c6f",
      "ai_summary": "Influence functions are used to attribute LLMs' reasoning in math and coding to individual training elements, revealing cross-domain effects and enabling a reweighting strategy that improves model accuracy.",
      "ai_keywords": [
        "Large language models (LLMs)",
        "chain-of-thoughts (CoTs)",
        "influence functions",
        "attribution",
        "data curation",
        "reasoning ability",
        "high-difficulty math examples",
        "low-difficulty code tasks",
        "dataset reweighting strategy",
        "AIME24 accuracy",
        "LiveCodeBench accuracy",
        "sequence-level exploratory behaviors",
        "token-level influence patterns",
        "natural language logic connectors",
        "structural syntax",
        "parameter-efficient fine-tuning"
      ]
    },
    "publishedAt": "2025-05-26T09:15:26.000Z",
    "title": "Which Data Attributes Stimulate Math and Code Reasoning? An\n  Investigation via Influence Functions",
    "summary": "Large language models (LLMs) have demonstrated remarkable reasoning\ncapabilities in math and coding, often bolstered by post-training on the\nchain-of-thoughts (CoTs) generated by stronger models. However, existing\nstrategies for curating such training data predominantly rely on heuristics,\nlimiting generalizability and failing to capture subtleties underlying in data.\nTo address these limitations, we leverage influence functions to systematically\nattribute LLMs' reasoning ability on math and coding to individual training\nexamples, sequences, and tokens, enabling deeper insights into effective data\ncharacteristics. Our Influence-based Reasoning Attribution (Infra) uncovers\nnontrivial cross-domain effects across math and coding tasks: high-difficulty\nmath examples improve both math and code reasoning, while low-difficulty code\ntasks most effectively benefit code reasoning. Based on these findings, we\nintroduce a simple yet effective dataset reweighting strategy by flipping task\ndifficulty, which doubles AIME24 accuracy from 10\\% to 20\\% and boosts\nLiveCodeBench accuracy from 33.8\\% to 35.3\\% for Qwen2.5-7B-Instruct. Moreover,\nour fine-grained attribution reveals that the sequence-level exploratory\nbehaviors enhance reasoning performance in both math and code, and the\ntoken-level influence patterns are distinct for math and code reasoning: the\nformer prefers natural language logic connectors and the latter emphasizes\nstructural syntax.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19949.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "654e330f350abceb30a1390b",
      "avatarUrl": "/avatars/e54a8be788fa1bdc7acefecc208215bb.svg",
      "fullname": "KouSiqi",
      "name": "karrykkk",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.19752",
      "authors": [
        {
          "_id": "683527ba3762eb8b3ea1de34",
          "name": "Hengli Li",
          "hidden": false
        },
        {
          "_id": "683527ba3762eb8b3ea1de35",
          "user": {
            "_id": "60b9e6837946aff342f734ae",
            "avatarUrl": "/avatars/a711a6aa35757dfd7b78b26098a964fc.svg",
            "isPro": false,
            "fullname": "Yuxuan Wang",
            "user": "ColorfulAI",
            "type": "user"
          },
          "name": "Yuxuan Wang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-27T02:47:22.991Z",
          "hidden": false
        },
        {
          "_id": "683527ba3762eb8b3ea1de36",
          "name": "Song-Chun Zhu",
          "hidden": false
        },
        {
          "_id": "683527ba3762eb8b3ea1de37",
          "name": "Ying Nian Wu",
          "hidden": false
        },
        {
          "_id": "683527ba3762eb8b3ea1de38",
          "user": {
            "_id": "63a95a6a7930fa8c7dd63d4e",
            "avatarUrl": "/avatars/d9d0420f7ddfe2f3a7e029fb05f1c89f.svg",
            "isPro": false,
            "fullname": "Zilong Zheng",
            "user": "zlzheng",
            "type": "user"
          },
          "name": "Zilong Zheng",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-27T02:47:22.991Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-26T09:32:12.000Z",
      "submittedOnDailyAt": "2025-05-27T01:18:56.411Z",
      "title": "Discrete Markov Bridge",
      "submittedOnDailyBy": {
        "_id": "62649e2b1ed8d81e47ad9b4e",
        "avatarUrl": "/avatars/f33a0b727822fd2ea99dce37fbda3d17.svg",
        "isPro": false,
        "fullname": "Li",
        "user": "henry12348",
        "type": "user"
      },
      "summary": "Discrete diffusion has recently emerged as a promising paradigm in discrete\ndata modeling. However, existing methods typically rely on a fixed rate\ntransition matrix during training, which not only limits the expressiveness of\nlatent representations, a fundamental strength of variational methods, but also\nconstrains the overall design space. To address these limitations, we propose\nDiscrete Markov Bridge, a novel framework specifically designed for discrete\nrepresentation learning. Our approach is built upon two key components: Matrix\nLearning and Score Learning. We conduct a rigorous theoretical analysis,\nestablishing formal performance guarantees for Matrix Learning and proving the\nconvergence of the overall framework. Furthermore, we analyze the space\ncomplexity of our method, addressing practical constraints identified in prior\nstudies. Extensive empirical evaluations validate the effectiveness of the\nproposed Discrete Markov Bridge, which achieves an Evidence Lower Bound (ELBO)\nof 1.38 on the Text8 dataset, outperforming established baselines. Moreover,\nthe proposed model demonstrates competitive performance on the CIFAR-10\ndataset, achieving results comparable to those obtained by image-specific\ngeneration approaches.",
      "upvotes": 11,
      "discussionId": "683527bb3762eb8b3ea1de6c",
      "projectPage": "https://github.com/Henry839/Discrete-Markov-Bridge/tree/main",
      "githubRepo": "https://github.com/Henry839/Discrete-Markov-Bridge/tree/main",
      "ai_summary": "A novel framework, Discrete Markov Bridge, is introduced for discrete data modeling with Matrix Learning and Score Learning components, demonstrating superior performance compared to existing methods on Text8 and CIFAR-10 datasets.",
      "ai_keywords": [
        "discrete diffusion",
        "variational methods",
        "Discrete Markov Bridge",
        "Matrix Learning",
        "Score Learning",
        "Evidence Lower Bound",
        "ELBO"
      ]
    },
    "publishedAt": "2025-05-26T05:32:12.000Z",
    "title": "Discrete Markov Bridge",
    "summary": "Discrete diffusion has recently emerged as a promising paradigm in discrete\ndata modeling. However, existing methods typically rely on a fixed rate\ntransition matrix during training, which not only limits the expressiveness of\nlatent representations, a fundamental strength of variational methods, but also\nconstrains the overall design space. To address these limitations, we propose\nDiscrete Markov Bridge, a novel framework specifically designed for discrete\nrepresentation learning. Our approach is built upon two key components: Matrix\nLearning and Score Learning. We conduct a rigorous theoretical analysis,\nestablishing formal performance guarantees for Matrix Learning and proving the\nconvergence of the overall framework. Furthermore, we analyze the space\ncomplexity of our method, addressing practical constraints identified in prior\nstudies. Extensive empirical evaluations validate the effectiveness of the\nproposed Discrete Markov Bridge, which achieves an Evidence Lower Bound (ELBO)\nof 1.38 on the Text8 dataset, outperforming established baselines. Moreover,\nthe proposed model demonstrates competitive performance on the CIFAR-10\ndataset, achieving results comparable to those obtained by image-specific\ngeneration approaches.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19752.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62649e2b1ed8d81e47ad9b4e",
      "avatarUrl": "/avatars/f33a0b727822fd2ea99dce37fbda3d17.svg",
      "fullname": "Li",
      "name": "henry12348",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.20256",
      "authors": [
        {
          "_id": "68352e44c829f2ea1e0484b5",
          "name": "Hao Zhong",
          "hidden": false
        },
        {
          "_id": "68352e44c829f2ea1e0484b6",
          "name": "Muzhi Zhu",
          "hidden": false
        },
        {
          "_id": "68352e44c829f2ea1e0484b7",
          "name": "Zongze Du",
          "hidden": false
        },
        {
          "_id": "68352e44c829f2ea1e0484b8",
          "name": "Zheng Huang",
          "hidden": false
        },
        {
          "_id": "68352e44c829f2ea1e0484b9",
          "name": "Canyu Zhao",
          "hidden": false
        },
        {
          "_id": "68352e44c829f2ea1e0484ba",
          "name": "Mingyu Liu",
          "hidden": false
        },
        {
          "_id": "68352e44c829f2ea1e0484bb",
          "name": "Wen Wang",
          "hidden": false
        },
        {
          "_id": "68352e44c829f2ea1e0484bc",
          "name": "Hao Chen",
          "hidden": false
        },
        {
          "_id": "68352e44c829f2ea1e0484bd",
          "name": "Chunhua Shen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-26T17:34:06.000Z",
      "submittedOnDailyAt": "2025-05-27T01:47:33.260Z",
      "title": "Omni-R1: Reinforcement Learning for Omnimodal Reasoning via Two-System\n  Collaboration",
      "submittedOnDailyBy": {
        "_id": "632179745fc60c44fd91fc33",
        "avatarUrl": "/avatars/37d4fefbcc19f091dccffefec9706de2.svg",
        "isPro": false,
        "fullname": "zhumuzhi",
        "user": "Z-MU-Z",
        "type": "user"
      },
      "summary": "Long-horizon video-audio reasoning and fine-grained pixel understanding\nimpose conflicting requirements on omnimodal models: dense temporal coverage\ndemands many low-resolution frames, whereas precise grounding calls for\nhigh-resolution inputs. We tackle this trade-off with a two-system\narchitecture: a Global Reasoning System selects informative keyframes and\nrewrites the task at low spatial cost, while a Detail Understanding System\nperforms pixel-level grounding on the selected high-resolution snippets.\nBecause ``optimal'' keyframe selection and reformulation are ambiguous and hard\nto supervise, we formulate them as a reinforcement learning (RL) problem and\npresent Omni-R1, an end-to-end RL framework built on Group Relative Policy\nOptimization. Omni-R1 trains the Global Reasoning System through hierarchical\nrewards obtained via online collaboration with the Detail Understanding System,\nrequiring only one epoch of RL on small task splits.\n  Experiments on two challenging benchmarks, namely Referring Audio-Visual\nSegmentation (RefAVS) and Reasoning Video Object Segmentation (REVOS), show\nthat Omni-R1 not only surpasses strong supervised baselines but also\noutperforms specialized state-of-the-art models, while substantially improving\nout-of-domain generalization and mitigating multimodal hallucination. Our\nresults demonstrate the first successful application of RL to large-scale\nomnimodal reasoning and highlight a scalable path toward universally foundation\nmodels.",
      "upvotes": 10,
      "discussionId": "68352e47c829f2ea1e048539",
      "projectPage": "https://aim-uofa.github.io/OmniR1/",
      "githubRepo": "https://github.com/aim-uofa/Omni-R1",
      "ai_summary": "An end-to-end reinforcement learning framework, Omni-R1, achieves superior performance in long-horizon video-audio reasoning and fine-grained pixel understanding tasks by combining global reasoning and detail understanding systems.",
      "ai_keywords": [
        "reinforcement learning",
        "Group Relative Policy Optimization",
        "hierarchical rewards",
        "Referring Audio-Visual Segmentation",
        "Reasoning Video Object Segmentation",
        "out-of-domain generalization",
        "multimodal hallucination",
        "universally foundation models"
      ]
    },
    "publishedAt": "2025-05-26T13:34:06.000Z",
    "title": "Omni-R1: Reinforcement Learning for Omnimodal Reasoning via Two-System\n  Collaboration",
    "summary": "Long-horizon video-audio reasoning and fine-grained pixel understanding\nimpose conflicting requirements on omnimodal models: dense temporal coverage\ndemands many low-resolution frames, whereas precise grounding calls for\nhigh-resolution inputs. We tackle this trade-off with a two-system\narchitecture: a Global Reasoning System selects informative keyframes and\nrewrites the task at low spatial cost, while a Detail Understanding System\nperforms pixel-level grounding on the selected high-resolution snippets.\nBecause ``optimal'' keyframe selection and reformulation are ambiguous and hard\nto supervise, we formulate them as a reinforcement learning (RL) problem and\npresent Omni-R1, an end-to-end RL framework built on Group Relative Policy\nOptimization. Omni-R1 trains the Global Reasoning System through hierarchical\nrewards obtained via online collaboration with the Detail Understanding System,\nrequiring only one epoch of RL on small task splits.\n  Experiments on two challenging benchmarks, namely Referring Audio-Visual\nSegmentation (RefAVS) and Reasoning Video Object Segmentation (REVOS), show\nthat Omni-R1 not only surpasses strong supervised baselines but also\noutperforms specialized state-of-the-art models, while substantially improving\nout-of-domain generalization and mitigating multimodal hallucination. Our\nresults demonstrate the first successful application of RL to large-scale\nomnimodal reasoning and highlight a scalable path toward universally foundation\nmodels.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20256.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "632179745fc60c44fd91fc33",
      "avatarUrl": "/avatars/37d4fefbcc19f091dccffefec9706de2.svg",
      "fullname": "zhumuzhi",
      "name": "Z-MU-Z",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.19788",
      "authors": [
        {
          "_id": "68352630363d6fd2fff5d07f",
          "name": "Zihao Zeng",
          "hidden": false
        },
        {
          "_id": "68352630363d6fd2fff5d080",
          "name": "Xuyao Huang",
          "hidden": false
        },
        {
          "_id": "68352630363d6fd2fff5d081",
          "name": "Boxiu Li",
          "hidden": false
        },
        {
          "_id": "68352630363d6fd2fff5d082",
          "name": "Hao Zhang",
          "hidden": false
        },
        {
          "_id": "68352630363d6fd2fff5d083",
          "name": "Zhijie Deng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-26T10:18:57.000Z",
      "submittedOnDailyAt": "2025-05-27T01:59:02.853Z",
      "title": "Done Is Better than Perfect: Unlocking Efficient Reasoning by Structured\n  Multi-Turn Decomposition",
      "submittedOnDailyBy": {
        "_id": "6721dacfc5309c08451d21d5",
        "avatarUrl": "/avatars/ac8be5ac8b8ee5b5533214e526b72dad.svg",
        "isPro": false,
        "fullname": "Huang Xuyao",
        "user": "ElysiaTrue",
        "type": "user"
      },
      "summary": "Large Reasoning Models (LRMs) are criticized for the excessively lengthy\nChain-of-Thought (CoT) to derive the final answer, suffering from high\nfirst-token and overall latency. Typically, the CoT of LRMs mixes multiple\nthinking units; each unit attempts to produce a candidate answer to the\noriginal query. Hence, a natural idea to improve efficiency is to reduce the\nunit number. Yet, the fact that the thinking units in vanilla CoT cannot be\nexplicitly managed renders doing so challenging. This paper introduces\nMulti-Turn Decomposition (MinD) to decode conventional CoT into a sequence of\nexplicit, structured, and turn-wise interactions to bridge the gap. In MinD,\nthe model provides a multi-turn response to the query, where each turn embraces\na thinking unit and yields a corresponding answer. The subsequent turns can\nreflect, verify, revise, or explore alternative approaches to both the thinking\nand answer parts of earlier ones. This not only makes the answer delivered more\nswiftly, but also enables explicit controls over the iterative reasoning\nprocess (i.e., users may halt or continue at any turn). We follow a supervised\nfine-tuning (SFT) then reinforcement learning (RL) paradigm to realize MinD. We\nfirst rephrase the outputs of an LRM into multi-turn formats by prompting\nanother LLM, and then tune the LRM with such data. Observing that the tuned\nmodel tends to consume even more tokens than the original one (probably due to\nthat the multi-turn formats introduce additional answer tokens), we advocate\nleveraging RL algorithms like GRPO to prioritize correct outputs with fewer\nturns. Trained on the MATH dataset using R1-Distill models, MinD can achieve up\nto ~70% reduction in both output token usage and time to first token (TTFT),\nwhile maintaining competitive performance on reasoning benchmarks such as\nMATH-500, AIME24, AMC23, and GPQA-Diamond.",
      "upvotes": 10,
      "discussionId": "68352631363d6fd2fff5d0b9",
      "ai_summary": "Multi-Turn Decomposition improves efficiency in large reasoning models by breaking down chain-of-thought into manageable turns, reducing token usage and latency while maintaining performance.",
      "ai_keywords": [
        "Chain-of-Thought",
        "large reasoning models",
        "multi-turn decomposition",
        "thinking units",
        "iterative reasoning process",
        "supervised fine-tuning",
        "reinforcement learning",
        "MATH dataset",
        "R1-Distill models",
        "MATH-500",
        "AIME24",
        "AMC23",
        "GPQA-Diamond"
      ]
    },
    "publishedAt": "2025-05-26T06:18:57.000Z",
    "title": "Done Is Better than Perfect: Unlocking Efficient Reasoning by Structured\n  Multi-Turn Decomposition",
    "summary": "Large Reasoning Models (LRMs) are criticized for the excessively lengthy\nChain-of-Thought (CoT) to derive the final answer, suffering from high\nfirst-token and overall latency. Typically, the CoT of LRMs mixes multiple\nthinking units; each unit attempts to produce a candidate answer to the\noriginal query. Hence, a natural idea to improve efficiency is to reduce the\nunit number. Yet, the fact that the thinking units in vanilla CoT cannot be\nexplicitly managed renders doing so challenging. This paper introduces\nMulti-Turn Decomposition (MinD) to decode conventional CoT into a sequence of\nexplicit, structured, and turn-wise interactions to bridge the gap. In MinD,\nthe model provides a multi-turn response to the query, where each turn embraces\na thinking unit and yields a corresponding answer. The subsequent turns can\nreflect, verify, revise, or explore alternative approaches to both the thinking\nand answer parts of earlier ones. This not only makes the answer delivered more\nswiftly, but also enables explicit controls over the iterative reasoning\nprocess (i.e., users may halt or continue at any turn). We follow a supervised\nfine-tuning (SFT) then reinforcement learning (RL) paradigm to realize MinD. We\nfirst rephrase the outputs of an LRM into multi-turn formats by prompting\nanother LLM, and then tune the LRM with such data. Observing that the tuned\nmodel tends to consume even more tokens than the original one (probably due to\nthat the multi-turn formats introduce additional answer tokens), we advocate\nleveraging RL algorithms like GRPO to prioritize correct outputs with fewer\nturns. Trained on the MATH dataset using R1-Distill models, MinD can achieve up\nto ~70% reduction in both output token usage and time to first token (TTFT),\nwhile maintaining competitive performance on reasoning benchmarks such as\nMATH-500, AIME24, AMC23, and GPQA-Diamond.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19788.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6721dacfc5309c08451d21d5",
      "avatarUrl": "/avatars/ac8be5ac8b8ee5b5533214e526b72dad.svg",
      "fullname": "Huang Xuyao",
      "name": "ElysiaTrue",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.19439",
      "authors": [
        {
          "_id": "68355784bb7d114755346770",
          "name": "Rihui Xin",
          "hidden": false
        },
        {
          "_id": "68355784bb7d114755346771",
          "name": "Han Liu",
          "hidden": false
        },
        {
          "_id": "68355784bb7d114755346772",
          "name": "Zecheng Wang",
          "hidden": false
        },
        {
          "_id": "68355784bb7d114755346773",
          "name": "Yupeng Zhang",
          "hidden": false
        },
        {
          "_id": "68355784bb7d114755346774",
          "name": "Dianbo Sui",
          "hidden": false
        },
        {
          "_id": "68355784bb7d114755346775",
          "name": "Xiaolin Hu",
          "hidden": false
        },
        {
          "_id": "68355784bb7d114755346776",
          "name": "Bingning Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-26T02:56:22.000Z",
      "submittedOnDailyAt": "2025-05-27T04:41:46.037Z",
      "title": "Surrogate Signals from Format and Length: Reinforcement Learning for\n  Solving Mathematical Problems without Ground Truth Answers",
      "submittedOnDailyBy": {
        "_id": "62e52483a944e2a56cd2c6ca",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62e52483a944e2a56cd2c6ca/pG44O-1qD00q5CEJMMyFQ.jpeg",
        "isPro": false,
        "fullname": "Jiejun Tan",
        "user": "zstanjj",
        "type": "user"
      },
      "summary": "Large Language Models have achieved remarkable success in natural language\nprocessing tasks, with Reinforcement Learning playing a key role in adapting\nthem to specific applications. However, obtaining ground truth answers for\ntraining LLMs in mathematical problem-solving is often challenging, costly, and\nsometimes unfeasible. This research delves into the utilization of format and\nlength as surrogate signals to train LLMs for mathematical problem-solving,\nbypassing the need for traditional ground truth answers.Our study shows that a\nreward function centered on format correctness alone can yield performance\nimprovements comparable to the standard GRPO algorithm in early phases.\nRecognizing the limitations of format-only rewards in the later phases, we\nincorporate length-based rewards. The resulting GRPO approach, leveraging\nformat-length surrogate signals, not only matches but surpasses the performance\nof the standard GRPO algorithm relying on ground truth answers in certain\nscenarios, achieving 40.0\\% accuracy on AIME2024 with a 7B base model. Through\nsystematic exploration and experimentation, this research not only offers a\npractical solution for training LLMs to solve mathematical problems and\nreducing the dependence on extensive ground truth data collection, but also\nreveals the essence of why our label-free approach succeeds: base model is like\nan excellent student who has already mastered mathematical and logical\nreasoning skills, but performs poorly on the test paper, it simply needs to\ndevelop good answering habits to achieve outstanding results in exams , in\nother words, to unlock the capabilities it already possesses.",
      "upvotes": 10,
      "discussionId": "68355785bb7d1147553467b8",
      "ai_summary": "The research demonstrates that using format and length as surrogate signals can improve LLMs' performance in mathematical problem-solving, matching or surpassing traditional methods without extensive ground truth data.",
      "ai_keywords": [
        "Large Language Models",
        "Reinforcement Learning",
        "mathematical problem-solving",
        "GRPO algorithm",
        "format correctness",
        "length-based rewards",
        "AIME2024"
      ]
    },
    "publishedAt": "2025-05-25T22:56:22.000Z",
    "title": "Surrogate Signals from Format and Length: Reinforcement Learning for\n  Solving Mathematical Problems without Ground Truth Answers",
    "summary": "Large Language Models have achieved remarkable success in natural language\nprocessing tasks, with Reinforcement Learning playing a key role in adapting\nthem to specific applications. However, obtaining ground truth answers for\ntraining LLMs in mathematical problem-solving is often challenging, costly, and\nsometimes unfeasible. This research delves into the utilization of format and\nlength as surrogate signals to train LLMs for mathematical problem-solving,\nbypassing the need for traditional ground truth answers.Our study shows that a\nreward function centered on format correctness alone can yield performance\nimprovements comparable to the standard GRPO algorithm in early phases.\nRecognizing the limitations of format-only rewards in the later phases, we\nincorporate length-based rewards. The resulting GRPO approach, leveraging\nformat-length surrogate signals, not only matches but surpasses the performance\nof the standard GRPO algorithm relying on ground truth answers in certain\nscenarios, achieving 40.0\\% accuracy on AIME2024 with a 7B base model. Through\nsystematic exploration and experimentation, this research not only offers a\npractical solution for training LLMs to solve mathematical problems and\nreducing the dependence on extensive ground truth data collection, but also\nreveals the essence of why our label-free approach succeeds: base model is like\nan excellent student who has already mastered mathematical and logical\nreasoning skills, but performs poorly on the test paper, it simply needs to\ndevelop good answering habits to achieve outstanding results in exams , in\nother words, to unlock the capabilities it already possesses.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19439.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62e52483a944e2a56cd2c6ca",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62e52483a944e2a56cd2c6ca/pG44O-1qD00q5CEJMMyFQ.jpeg",
      "fullname": "Jiejun Tan",
      "name": "zstanjj",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.20152",
      "authors": [
        {
          "_id": "6835385ebd4d4208167d15ac",
          "name": "Kai Sun",
          "hidden": false
        },
        {
          "_id": "6835385ebd4d4208167d15ad",
          "name": "Yushi Bai",
          "hidden": false
        },
        {
          "_id": "6835385ebd4d4208167d15ae",
          "name": "Zhen Yang",
          "hidden": false
        },
        {
          "_id": "6835385ebd4d4208167d15af",
          "name": "Jiajie Zhang",
          "hidden": false
        },
        {
          "_id": "6835385ebd4d4208167d15b0",
          "name": "Ji Qi",
          "hidden": false
        },
        {
          "_id": "6835385ebd4d4208167d15b1",
          "name": "Lei Hou",
          "hidden": false
        },
        {
          "_id": "6835385ebd4d4208167d15b2",
          "name": "Juanzi Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-26T15:55:28.000Z",
      "submittedOnDailyAt": "2025-05-27T02:29:13.927Z",
      "title": "Hard Negative Contrastive Learning for Fine-Grained Geometric\n  Understanding in Large Multimodal Models",
      "submittedOnDailyBy": {
        "_id": "66cdd285c51a915bd5f2d017",
        "avatarUrl": "/avatars/14e5794307e4672b1b51d26b31227e0f.svg",
        "isPro": false,
        "fullname": "Jiajie Zhang",
        "user": "NeoZ123",
        "type": "user"
      },
      "summary": "Benefiting from contrastively trained visual encoders on large-scale natural\nscene images, Large Multimodal Models (LMMs) have achieved remarkable\nperformance across various visual perception tasks. However, the inherent\nlimitations of contrastive learning upon summarized descriptions fundamentally\nrestrict the capabilities of models in meticulous reasoning, particularly in\ncrucial scenarios of geometric problem-solving. To enhance geometric\nunderstanding, we propose a novel hard negative contrastive learning framework\nfor the vision encoder, which combines image-based contrastive learning using\ngeneration-based hard negatives created by perturbing diagram generation code,\nand text-based contrastive learning using rule-based negatives derived from\nmodified geometric descriptions and retrieval-based negatives selected based on\ncaption similarity. We train CLIP using our strong negative learning method,\nnamely MMCLIP (Multimodal Math CLIP), and subsequently train an LMM for\ngeometric problem-solving. Experiments show that our trained model, MMGeoLM,\nsignificantly outperforms other open-source models on three geometric reasoning\nbenchmarks. Even with a size of 7B, it can rival powerful closed-source models\nlike GPT-4o. We further study the impact of different negative sample\nconstruction methods and the number of negative samples on the geometric\nreasoning performance of LMM, yielding fruitful conclusions. The code and\ndataset are available at https://github.com/THU-KEG/MMGeoLM.",
      "upvotes": 8,
      "discussionId": "6835385fbd4d4208167d15f0",
      "ai_summary": "A novel hard negative contrastive learning framework improves geometric reasoning in Large Multimodal Models, significantly enhancing their performance compared to existing models.",
      "ai_keywords": [
        "contrastively trained visual encoders",
        "Large Multimodal Models",
        "geometric problem-solving",
        "hard negative contrastive learning",
        "generation-based hard negatives",
        "rule-based negatives",
        "retrieval-based negatives",
        "CLIP",
        "MMCLIP",
        "multimodal math clip",
        "MMGeoLM",
        "geometric reasoning benchmarks"
      ]
    },
    "publishedAt": "2025-05-26T11:55:28.000Z",
    "title": "Hard Negative Contrastive Learning for Fine-Grained Geometric\n  Understanding in Large Multimodal Models",
    "summary": "Benefiting from contrastively trained visual encoders on large-scale natural\nscene images, Large Multimodal Models (LMMs) have achieved remarkable\nperformance across various visual perception tasks. However, the inherent\nlimitations of contrastive learning upon summarized descriptions fundamentally\nrestrict the capabilities of models in meticulous reasoning, particularly in\ncrucial scenarios of geometric problem-solving. To enhance geometric\nunderstanding, we propose a novel hard negative contrastive learning framework\nfor the vision encoder, which combines image-based contrastive learning using\ngeneration-based hard negatives created by perturbing diagram generation code,\nand text-based contrastive learning using rule-based negatives derived from\nmodified geometric descriptions and retrieval-based negatives selected based on\ncaption similarity. We train CLIP using our strong negative learning method,\nnamely MMCLIP (Multimodal Math CLIP), and subsequently train an LMM for\ngeometric problem-solving. Experiments show that our trained model, MMGeoLM,\nsignificantly outperforms other open-source models on three geometric reasoning\nbenchmarks. Even with a size of 7B, it can rival powerful closed-source models\nlike GPT-4o. We further study the impact of different negative sample\nconstruction methods and the number of negative samples on the geometric\nreasoning performance of LMM, yielding fruitful conclusions. The code and\ndataset are available at https://github.com/THU-KEG/MMGeoLM.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20152.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66cdd285c51a915bd5f2d017",
      "avatarUrl": "/avatars/14e5794307e4672b1b51d26b31227e0f.svg",
      "fullname": "Jiajie Zhang",
      "name": "NeoZ123",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.19602",
      "authors": [
        {
          "_id": "6835239e7309025530c85ba3",
          "name": "Kunjun Li",
          "hidden": false
        },
        {
          "_id": "6835239e7309025530c85ba4",
          "name": "Zigeng Chen",
          "hidden": false
        },
        {
          "_id": "6835239e7309025530c85ba5",
          "name": "Cheng-Yen Yang",
          "hidden": false
        },
        {
          "_id": "6835239e7309025530c85ba6",
          "name": "Jenq-Neng Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-26T07:11:42.000Z",
      "submittedOnDailyAt": "2025-05-27T01:00:44.064Z",
      "title": "Memory-Efficient Visual Autoregressive Modeling with Scale-Aware KV\n  Cache Compression",
      "submittedOnDailyBy": {
        "_id": "65811eeaa2284a018e51f1ba",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/dH8UZj6Kk5HJkI1DItCNm.jpeg",
        "isPro": true,
        "fullname": "Zigeng Chen",
        "user": "Zigeng",
        "type": "user"
      },
      "summary": "Visual Autoregressive (VAR) modeling has garnered significant attention for\nits innovative next-scale prediction approach, which yields substantial\nimprovements in efficiency, scalability, and zero-shot generalization.\nNevertheless, the coarse-to-fine methodology inherent in VAR results in\nexponential growth of the KV cache during inference, causing considerable\nmemory consumption and computational redundancy. To address these bottlenecks,\nwe introduce ScaleKV, a novel KV cache compression framework tailored for VAR\narchitectures. ScaleKV leverages two critical observations: varying cache\ndemands across transformer layers and distinct attention patterns at different\nscales. Based on these insights, ScaleKV categorizes transformer layers into\ntwo functional groups: drafters and refiners. Drafters exhibit dispersed\nattention across multiple scales, thereby requiring greater cache capacity.\nConversely, refiners focus attention on the current token map to process local\ndetails, consequently necessitating substantially reduced cache capacity.\nScaleKV optimizes the multi-scale inference pipeline by identifying\nscale-specific drafters and refiners, facilitating differentiated cache\nmanagement tailored to each scale. Evaluation on the state-of-the-art\ntext-to-image VAR model family, Infinity, demonstrates that our approach\neffectively reduces the required KV cache memory to 10% while preserving\npixel-level fidelity.",
      "upvotes": 7,
      "discussionId": "683523a07309025530c85c45",
      "ai_summary": "ScaleKV compresses the KV cache in Visual Autoregressive models by differentiating drafters and refiners across transformer layers, reducing memory consumption while maintaining high fidelity.",
      "ai_keywords": [
        "Visual Autoregressive",
        "VAR",
        "KV cache",
        "transformer layers",
        "drafters",
        "refiners",
        "memory consumption",
        "Infinity",
        "pixel-level fidelity"
      ]
    },
    "publishedAt": "2025-05-26T03:11:42.000Z",
    "title": "Memory-Efficient Visual Autoregressive Modeling with Scale-Aware KV\n  Cache Compression",
    "summary": "Visual Autoregressive (VAR) modeling has garnered significant attention for\nits innovative next-scale prediction approach, which yields substantial\nimprovements in efficiency, scalability, and zero-shot generalization.\nNevertheless, the coarse-to-fine methodology inherent in VAR results in\nexponential growth of the KV cache during inference, causing considerable\nmemory consumption and computational redundancy. To address these bottlenecks,\nwe introduce ScaleKV, a novel KV cache compression framework tailored for VAR\narchitectures. ScaleKV leverages two critical observations: varying cache\ndemands across transformer layers and distinct attention patterns at different\nscales. Based on these insights, ScaleKV categorizes transformer layers into\ntwo functional groups: drafters and refiners. Drafters exhibit dispersed\nattention across multiple scales, thereby requiring greater cache capacity.\nConversely, refiners focus attention on the current token map to process local\ndetails, consequently necessitating substantially reduced cache capacity.\nScaleKV optimizes the multi-scale inference pipeline by identifying\nscale-specific drafters and refiners, facilitating differentiated cache\nmanagement tailored to each scale. Evaluation on the state-of-the-art\ntext-to-image VAR model family, Infinity, demonstrates that our approach\neffectively reduces the required KV cache memory to 10% while preserving\npixel-level fidelity.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19602.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65811eeaa2284a018e51f1ba",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/dH8UZj6Kk5HJkI1DItCNm.jpeg",
      "fullname": "Zigeng Chen",
      "name": "Zigeng",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.19590",
      "authors": [
        {
          "_id": "683523bcb0f9c65224abd710",
          "name": "Xuandong Zhao",
          "hidden": false
        },
        {
          "_id": "683523bcb0f9c65224abd711",
          "name": "Zhewei Kang",
          "hidden": false
        },
        {
          "_id": "683523bcb0f9c65224abd712",
          "name": "Aosong Feng",
          "hidden": false
        },
        {
          "_id": "683523bcb0f9c65224abd713",
          "name": "Sergey Levine",
          "hidden": false
        },
        {
          "_id": "683523bcb0f9c65224abd714",
          "name": "Dawn Song",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-26T07:01:06.000Z",
      "submittedOnDailyAt": "2025-05-27T01:00:44.089Z",
      "title": "Learning to Reason without External Rewards",
      "submittedOnDailyBy": {
        "_id": "6275a465597c70eb8949fce5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6275a465597c70eb8949fce5/ph4UogqMurMB0hSXZC38w.png",
        "isPro": false,
        "fullname": "Xuandong Zhao",
        "user": "Xuandong",
        "type": "user"
      },
      "summary": "Training large language models (LLMs) for complex reasoning via Reinforcement\nLearning with Verifiable Rewards (RLVR) is effective but limited by reliance on\ncostly, domain-specific supervision. We explore Reinforcement Learning from\nInternal Feedback (RLIF), a framework that enables LLMs to learn from intrinsic\nsignals without external rewards or labeled data. We propose Intuitor, an RLIF\nmethod that uses a model's own confidence, termed self-certainty, as its sole\nreward signal. Intuitor replaces external rewards in Group Relative Policy\nOptimization (GRPO) with self-certainty scores, enabling fully unsupervised\nlearning. Experiments demonstrate that Intuitor matches GRPO's performance on\nmathematical benchmarks while achieving superior generalization to\nout-of-domain tasks like code generation, without requiring gold solutions or\ntest cases. Our findings show that intrinsic model signals can drive effective\nlearning across domains, offering a scalable alternative to RLVR for autonomous\nAI systems where verifiable rewards are unavailable. Code is available at\nhttps://github.com/sunblaze-ucb/Intuitor",
      "upvotes": 7,
      "discussionId": "683523bcb0f9c65224abd736",
      "githubRepo": "https://github.com/sunblaze-ucb/Intuitor",
      "ai_summary": "Intuitor, a Reinforcement Learning from Internal Feedback method, uses self-certainty as a reward signal to enable unsupervised learning of large language models, achieving performance comparable to GRPO on benchmarks and superior generalization.",
      "ai_keywords": [
        "Reinforcement Learning with Verifiable Rewards",
        "Reinforcement Learning from Internal Feedback",
        "Group Relative Policy Optimization",
        "self-certainty",
        "unsupervised learning"
      ]
    },
    "publishedAt": "2025-05-26T03:01:06.000Z",
    "title": "Learning to Reason without External Rewards",
    "summary": "Training large language models (LLMs) for complex reasoning via Reinforcement\nLearning with Verifiable Rewards (RLVR) is effective but limited by reliance on\ncostly, domain-specific supervision. We explore Reinforcement Learning from\nInternal Feedback (RLIF), a framework that enables LLMs to learn from intrinsic\nsignals without external rewards or labeled data. We propose Intuitor, an RLIF\nmethod that uses a model's own confidence, termed self-certainty, as its sole\nreward signal. Intuitor replaces external rewards in Group Relative Policy\nOptimization (GRPO) with self-certainty scores, enabling fully unsupervised\nlearning. Experiments demonstrate that Intuitor matches GRPO's performance on\nmathematical benchmarks while achieving superior generalization to\nout-of-domain tasks like code generation, without requiring gold solutions or\ntest cases. Our findings show that intrinsic model signals can drive effective\nlearning across domains, offering a scalable alternative to RLVR for autonomous\nAI systems where verifiable rewards are unavailable. Code is available at\nhttps://github.com/sunblaze-ucb/Intuitor",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19590.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6275a465597c70eb8949fce5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6275a465597c70eb8949fce5/ph4UogqMurMB0hSXZC38w.png",
      "fullname": "Xuandong Zhao",
      "name": "Xuandong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.18759",
      "authors": [
        {
          "_id": "683551c54f3166e8677b43bb",
          "name": "Ruichen Zhang",
          "hidden": false
        },
        {
          "_id": "683551c54f3166e8677b43bc",
          "name": "Rana Muhammad Shahroz Khan",
          "hidden": false
        },
        {
          "_id": "683551c54f3166e8677b43bd",
          "name": "Zhen Tan",
          "hidden": false
        },
        {
          "_id": "683551c54f3166e8677b43be",
          "user": {
            "_id": "6474e1afb68461d5cf7c41cc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6474e1afb68461d5cf7c41cc/bcoiD_qPrjHUBlB259djg.png",
            "isPro": false,
            "fullname": "Dawei Li",
            "user": "wjldw",
            "type": "user"
          },
          "name": "Dawei Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-27T07:49:14.749Z",
          "hidden": false
        },
        {
          "_id": "683551c54f3166e8677b43bf",
          "name": "Song Wang",
          "hidden": false
        },
        {
          "_id": "683551c54f3166e8677b43c0",
          "name": "Tianlong Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-24T15:54:19.000Z",
      "submittedOnDailyAt": "2025-05-27T04:17:34.631Z",
      "title": "The Quest for Efficient Reasoning: A Data-Centric Benchmark to CoT\n  Distillation",
      "submittedOnDailyBy": {
        "_id": "6474e1afb68461d5cf7c41cc",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6474e1afb68461d5cf7c41cc/bcoiD_qPrjHUBlB259djg.png",
        "isPro": false,
        "fullname": "Dawei Li",
        "user": "wjldw",
        "type": "user"
      },
      "summary": "Data-centric distillation, including data augmentation, selection, and\nmixing, offers a promising path to creating smaller, more efficient student\nLarge Language Models (LLMs) that retain strong reasoning abilities. However,\nthere still lacks a comprehensive benchmark to systematically assess the effect\nof each distillation approach. This paper introduces DC-CoT, the first\ndata-centric benchmark that investigates data manipulation in chain-of-thought\n(CoT) distillation from method, model and data perspectives. Utilizing various\nteacher models (e.g., o4-mini, Gemini-Pro, Claude-3.5) and student\narchitectures (e.g., 3B, 7B parameters), we rigorously evaluate the impact of\nthese data manipulations on student model performance across multiple reasoning\ndatasets, with a focus on in-distribution (IID) and out-of-distribution (OOD)\ngeneralization, and cross-domain transfer. Our findings aim to provide\nactionable insights and establish best practices for optimizing CoT\ndistillation through data-centric techniques, ultimately facilitating the\ndevelopment of more accessible and capable reasoning models. The dataset can be\nfound at https://huggingface.co/datasets/rana-shahroz/DC-COT, while our code is\nshared in https://anonymous.4open.science/r/DC-COT-FF4C/.",
      "upvotes": 7,
      "discussionId": "683551c64f3166e8677b4424",
      "ai_summary": "DC-CoT provides a comprehensive benchmark for assessing data-centric distillation techniques in chain-of-thought distillation, focusing on performance and generalization across different models and datasets.",
      "ai_keywords": [
        "data-centric distillation",
        "data augmentation",
        "data selection",
        "data mixing",
        "chain-of-thought (CoT)",
        "in-distribution (IID)",
        "out-of-distribution (OOD)",
        "cross-domain transfer"
      ]
    },
    "publishedAt": "2025-05-24T11:54:19.000Z",
    "title": "The Quest for Efficient Reasoning: A Data-Centric Benchmark to CoT\n  Distillation",
    "summary": "Data-centric distillation, including data augmentation, selection, and\nmixing, offers a promising path to creating smaller, more efficient student\nLarge Language Models (LLMs) that retain strong reasoning abilities. However,\nthere still lacks a comprehensive benchmark to systematically assess the effect\nof each distillation approach. This paper introduces DC-CoT, the first\ndata-centric benchmark that investigates data manipulation in chain-of-thought\n(CoT) distillation from method, model and data perspectives. Utilizing various\nteacher models (e.g., o4-mini, Gemini-Pro, Claude-3.5) and student\narchitectures (e.g., 3B, 7B parameters), we rigorously evaluate the impact of\nthese data manipulations on student model performance across multiple reasoning\ndatasets, with a focus on in-distribution (IID) and out-of-distribution (OOD)\ngeneralization, and cross-domain transfer. Our findings aim to provide\nactionable insights and establish best practices for optimizing CoT\ndistillation through data-centric techniques, ultimately facilitating the\ndevelopment of more accessible and capable reasoning models. The dataset can be\nfound at https://huggingface.co/datasets/rana-shahroz/DC-COT, while our code is\nshared in https://anonymous.4open.science/r/DC-COT-FF4C/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18759.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6474e1afb68461d5cf7c41cc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6474e1afb68461d5cf7c41cc/bcoiD_qPrjHUBlB259djg.png",
      "fullname": "Dawei Li",
      "name": "wjldw",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.16972",
      "authors": [
        {
          "_id": "68351e269f4e0a0f048ea664",
          "name": "Tianduo Wang",
          "hidden": false
        },
        {
          "_id": "68351e269f4e0a0f048ea665",
          "name": "Lu Xu",
          "hidden": false
        },
        {
          "_id": "68351e269f4e0a0f048ea666",
          "name": "Wei Lu",
          "hidden": false
        },
        {
          "_id": "68351e269f4e0a0f048ea667",
          "name": "Shanbo Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T17:51:05.000Z",
      "submittedOnDailyAt": "2025-05-27T00:36:56.992Z",
      "title": "From Tens of Hours to Tens of Thousands: Scaling Back-Translation for\n  Speech Recognition",
      "submittedOnDailyBy": {
        "_id": "6352aa7b6cfb8f149814de5e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1666361939036-noauth.jpeg",
        "isPro": false,
        "fullname": "Tianduo Wang",
        "user": "Tianduo",
        "type": "user"
      },
      "summary": "Recent advances in Automatic Speech Recognition (ASR) have been largely\nfueled by massive speech corpora. However, extending coverage to diverse\nlanguages with limited resources remains a formidable challenge. This paper\nintroduces Speech Back-Translation, a scalable pipeline that improves\nmultilingual ASR models by converting large-scale text corpora into synthetic\nspeech via off-the-shelf text-to-speech (TTS) models. We demonstrate that just\ntens of hours of real transcribed speech can effectively train TTS models to\ngenerate synthetic speech at hundreds of times the original volume while\nmaintaining high quality. To evaluate synthetic speech quality, we develop an\nintelligibility-based assessment framework and establish clear thresholds for\nwhen synthetic data benefits ASR training. Using Speech Back-Translation, we\ngenerate more than 500,000 hours of synthetic speech in ten languages and\ncontinue pre-training Whisper-large-v3, achieving average transcription error\nreductions of over 30\\%. These results highlight the scalability and\neffectiveness of Speech Back-Translation for enhancing multilingual ASR\nsystems.",
      "upvotes": 7,
      "discussionId": "68351e279f4e0a0f048ea689",
      "githubRepo": "https://github.com/TianduoWang/Speech-BT",
      "ai_summary": "Speech Back-Translation enhances multilingual ASR systems by generating high-quality synthetic speech from text corpora, significantly reducing transcription errors.",
      "ai_keywords": [
        "Automatic Speech Recognition",
        "Speech Back-Translation",
        "multilingual ASR",
        "text-to-speech",
        "synthetic speech",
        "intelligibility-based assessment",
        "Whisper-large-v3",
        "transcription error reduction"
      ]
    },
    "publishedAt": "2025-05-22T13:51:05.000Z",
    "title": "From Tens of Hours to Tens of Thousands: Scaling Back-Translation for\n  Speech Recognition",
    "summary": "Recent advances in Automatic Speech Recognition (ASR) have been largely\nfueled by massive speech corpora. However, extending coverage to diverse\nlanguages with limited resources remains a formidable challenge. This paper\nintroduces Speech Back-Translation, a scalable pipeline that improves\nmultilingual ASR models by converting large-scale text corpora into synthetic\nspeech via off-the-shelf text-to-speech (TTS) models. We demonstrate that just\ntens of hours of real transcribed speech can effectively train TTS models to\ngenerate synthetic speech at hundreds of times the original volume while\nmaintaining high quality. To evaluate synthetic speech quality, we develop an\nintelligibility-based assessment framework and establish clear thresholds for\nwhen synthetic data benefits ASR training. Using Speech Back-Translation, we\ngenerate more than 500,000 hours of synthetic speech in ten languages and\ncontinue pre-training Whisper-large-v3, achieving average transcription error\nreductions of over 30\\%. These results highlight the scalability and\neffectiveness of Speech Back-Translation for enhancing multilingual ASR\nsystems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16972.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6352aa7b6cfb8f149814de5e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1666361939036-noauth.jpeg",
      "fullname": "Tianduo Wang",
      "name": "Tianduo",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.13426",
      "authors": [
        {
          "_id": "682c641925f124206513d14d",
          "name": "Liang Chen",
          "hidden": false
        },
        {
          "_id": "682c641925f124206513d14e",
          "name": "Hongcheng Gao",
          "hidden": false
        },
        {
          "_id": "682c641925f124206513d14f",
          "name": "Tianyu Liu",
          "hidden": false
        },
        {
          "_id": "682c641925f124206513d150",
          "name": "Zhiqi Huang",
          "hidden": false
        },
        {
          "_id": "682c641925f124206513d151",
          "name": "Flood Sung",
          "hidden": false
        },
        {
          "_id": "682c641925f124206513d152",
          "name": "Xinyu Zhou",
          "hidden": false
        },
        {
          "_id": "682c641925f124206513d153",
          "name": "Yuxin Wu",
          "hidden": false
        },
        {
          "_id": "682c641925f124206513d154",
          "name": "Baobao Chang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-19T17:54:39.000Z",
      "submittedOnDailyAt": "2025-05-27T00:09:52.653Z",
      "title": "G1: Bootstrapping Perception and Reasoning Abilities of Vision-Language\n  Model via Reinforcement Learning",
      "submittedOnDailyBy": {
        "_id": "61b0a4ce1b3d95b3d1ed9251",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Wwjr26vdudX5KYVTb8Q0a.png",
        "isPro": false,
        "fullname": "Liang Chen",
        "user": "leonardPKU",
        "type": "user"
      },
      "summary": "Vision-Language Models (VLMs) excel in many direct multimodal tasks but\nstruggle to translate this prowess into effective decision-making within\ninteractive, visually rich environments like games. This ``knowing-doing'' gap\nsignificantly limits their potential as autonomous agents, as leading VLMs\noften performing badly in simple games. To address this, we introduce VLM-Gym,\na curated reinforcement learning (RL) environment featuring diverse visual\ngames with unified interfaces and adjustable, compositional difficulty,\nspecifically designed for scalable multi-game parallel training. Leveraging\nVLM-Gym, we train G0 models using pure RL-driven self-evolution, which\ndemonstrate emergent perception and reasoning patterns. To further mitigate\nchallenges arising from game diversity, we develop G1 models. G1 incorporates a\nperception-enhanced cold start prior to RL fine-tuning. Our resulting G1 models\nconsistently surpass their teacher across all games and outperform leading\nproprietary models like Claude-3.7-Sonnet-Thinking. Systematic analysis reveals\nan intriguing finding: perception and reasoning abilities mutually bootstrap\neach other throughout the RL training process. Source code including VLM-Gym\nand RL training are released at https://github.com/chenllliang/G1 to foster\nfuture research in advancing VLMs as capable interactive agents.",
      "upvotes": 7,
      "discussionId": "682c641a25f124206513d1d5",
      "githubRepo": "https://github.com/chenllliang/G1",
      "ai_summary": "VLM-Gym addresses the \"knowing-doing\" gap in Vision-Language Models by training them in a diverse RL environment, leading to enhanced perception and reasoning abilities that surpass existing models in interactive games.",
      "ai_keywords": [
        "Vision-Language Models",
        "VLM-Gym",
        "reinforcement learning",
        "RL",
        "G0 models",
        "self-evolution",
        "G1 models",
        "perception-enhanced cold start",
        "RL fine-tuning"
      ]
    },
    "publishedAt": "2025-05-19T13:54:39.000Z",
    "title": "G1: Bootstrapping Perception and Reasoning Abilities of Vision-Language\n  Model via Reinforcement Learning",
    "summary": "Vision-Language Models (VLMs) excel in many direct multimodal tasks but\nstruggle to translate this prowess into effective decision-making within\ninteractive, visually rich environments like games. This ``knowing-doing'' gap\nsignificantly limits their potential as autonomous agents, as leading VLMs\noften performing badly in simple games. To address this, we introduce VLM-Gym,\na curated reinforcement learning (RL) environment featuring diverse visual\ngames with unified interfaces and adjustable, compositional difficulty,\nspecifically designed for scalable multi-game parallel training. Leveraging\nVLM-Gym, we train G0 models using pure RL-driven self-evolution, which\ndemonstrate emergent perception and reasoning patterns. To further mitigate\nchallenges arising from game diversity, we develop G1 models. G1 incorporates a\nperception-enhanced cold start prior to RL fine-tuning. Our resulting G1 models\nconsistently surpass their teacher across all games and outperform leading\nproprietary models like Claude-3.7-Sonnet-Thinking. Systematic analysis reveals\nan intriguing finding: perception and reasoning abilities mutually bootstrap\neach other throughout the RL training process. Source code including VLM-Gym\nand RL training are released at https://github.com/chenllliang/G1 to foster\nfuture research in advancing VLMs as capable interactive agents.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13426.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61b0a4ce1b3d95b3d1ed9251",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Wwjr26vdudX5KYVTb8Q0a.png",
      "fullname": "Liang Chen",
      "name": "leonardPKU",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 16
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.19427",
      "authors": [
        {
          "_id": "683525ac1c31d709ba52273e",
          "name": "Sihan Chen",
          "hidden": false
        },
        {
          "_id": "683525ac1c31d709ba52273f",
          "name": "Dan Zhao",
          "hidden": false
        },
        {
          "_id": "683525ac1c31d709ba522740",
          "name": "Jongwoo Ko",
          "hidden": false
        },
        {
          "_id": "683525ac1c31d709ba522741",
          "name": "Colby Banbury",
          "hidden": false
        },
        {
          "_id": "683525ac1c31d709ba522742",
          "name": "Huiping Zhuang",
          "hidden": false
        },
        {
          "_id": "683525ac1c31d709ba522743",
          "name": "Luming Liang",
          "hidden": false
        },
        {
          "_id": "683525ac1c31d709ba522744",
          "name": "Tianyi Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-26T02:37:32.000Z",
      "submittedOnDailyAt": "2025-05-27T01:10:19.909Z",
      "title": "WINA: Weight Informed Neuron Activation for Accelerating Large Language\n  Model Inference",
      "submittedOnDailyBy": {
        "_id": "64ad94f05a4a60156925ec96",
        "avatarUrl": "/avatars/643bdb076e703bfcc89cec6fccb756c6.svg",
        "isPro": false,
        "fullname": "Tianyi Chen",
        "user": "tianyic",
        "type": "user"
      },
      "summary": "The growing computational demands of large language models (LLMs) make\nefficient inference and activation strategies increasingly critical. While\nrecent approaches, such as Mixture-of-Experts (MoE), leverage selective\nactivation but require specialized training, training-free sparse activation\nmethods offer broader applicability and superior resource efficiency through\ntheir plug-and-play design. However, many existing methods rely solely on\nhidden state magnitudes to determine activation, resulting in high\napproximation errors and suboptimal inference accuracy. To address these\nlimitations, we propose WINA (Weight Informed Neuron Activation), a novel,\nsimple, and training-free sparse activation framework that jointly considers\nhidden state magnitudes and the column-wise ell_2-norms of weight matrices.\nWe show that this leads to a sparsification strategy that obtains optimal\napproximation error bounds with theoretical guarantees tighter than existing\ntechniques. Empirically, WINA also outperforms state-of-the-art methods (e.g.,\nTEAL) by up to 2.94% in average performance at the same sparsity levels,\nacross a diverse set of LLM architectures and datasets. These results position\nWINA as a new performance frontier for training-free sparse activation in LLM\ninference, advancing training-free sparse activation methods and setting a\nrobust baseline for efficient inference. The source code is available at\nhttps://github.com/microsoft/wina.",
      "upvotes": 5,
      "discussionId": "683525ac1c31d709ba52277c",
      "ai_summary": "WINA, a training-free sparse activation framework for large language models, improves inference accuracy by considering hidden state magnitudes and weight matrix norms, outperforming existing methods.",
      "ai_keywords": [
        "Mixture-of-Experts (MoE)",
        "sparse activation",
        "hidden state magnitudes",
        "column-wise $\\ell_2$-norms",
        "weight matrices",
        "sparsification strategy",
        "approximation error bounds",
        "training-free sparse activation",
        "large language models"
      ]
    },
    "publishedAt": "2025-05-25T22:37:32.000Z",
    "title": "WINA: Weight Informed Neuron Activation for Accelerating Large Language\n  Model Inference",
    "summary": "The growing computational demands of large language models (LLMs) make\nefficient inference and activation strategies increasingly critical. While\nrecent approaches, such as Mixture-of-Experts (MoE), leverage selective\nactivation but require specialized training, training-free sparse activation\nmethods offer broader applicability and superior resource efficiency through\ntheir plug-and-play design. However, many existing methods rely solely on\nhidden state magnitudes to determine activation, resulting in high\napproximation errors and suboptimal inference accuracy. To address these\nlimitations, we propose WINA (Weight Informed Neuron Activation), a novel,\nsimple, and training-free sparse activation framework that jointly considers\nhidden state magnitudes and the column-wise ell_2-norms of weight matrices.\nWe show that this leads to a sparsification strategy that obtains optimal\napproximation error bounds with theoretical guarantees tighter than existing\ntechniques. Empirically, WINA also outperforms state-of-the-art methods (e.g.,\nTEAL) by up to 2.94% in average performance at the same sparsity levels,\nacross a diverse set of LLM architectures and datasets. These results position\nWINA as a new performance frontier for training-free sparse activation in LLM\ninference, advancing training-free sparse activation methods and setting a\nrobust baseline for efficient inference. The source code is available at\nhttps://github.com/microsoft/wina.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19427.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ad94f05a4a60156925ec96",
      "avatarUrl": "/avatars/643bdb076e703bfcc89cec6fccb756c6.svg",
      "fullname": "Tianyi Chen",
      "name": "tianyic",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.20254",
      "authors": [
        {
          "_id": "683528109f968fc5c604495f",
          "user": {
            "_id": "5f12485c0c833276f61f1afb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1595033594228-noauth.jpeg",
            "isPro": false,
            "fullname": "Xiangchen Song",
            "user": "xiangchensong",
            "type": "user"
          },
          "name": "Xiangchen Song",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-27T02:48:53.929Z",
          "hidden": false
        },
        {
          "_id": "683528109f968fc5c6044960",
          "name": "Aashiq Muhamed",
          "hidden": false
        },
        {
          "_id": "683528109f968fc5c6044961",
          "name": "Yujia Zheng",
          "hidden": false
        },
        {
          "_id": "683528109f968fc5c6044962",
          "name": "Lingjing Kong",
          "hidden": false
        },
        {
          "_id": "683528109f968fc5c6044963",
          "name": "Zeyu Tang",
          "hidden": false
        },
        {
          "_id": "683528109f968fc5c6044964",
          "name": "Mona T. Diab",
          "hidden": false
        },
        {
          "_id": "683528109f968fc5c6044965",
          "name": "Virginia Smith",
          "hidden": false
        },
        {
          "_id": "683528109f968fc5c6044966",
          "name": "Kun Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-26T17:31:36.000Z",
      "submittedOnDailyAt": "2025-05-27T01:20:40.055Z",
      "title": "Position: Mechanistic Interpretability Should Prioritize Feature\n  Consistency in SAEs",
      "submittedOnDailyBy": {
        "_id": "64755a83e0b188d3cb2579d8",
        "avatarUrl": "/avatars/2c50590905f4bd398a4c9991e1b4b5bb.svg",
        "isPro": false,
        "fullname": "Aashiq Muhamed",
        "user": "aashiqmuhamed",
        "type": "user"
      },
      "summary": "Sparse Autoencoders (SAEs) are a prominent tool in mechanistic\ninterpretability (MI) for decomposing neural network activations into\ninterpretable features. However, the aspiration to identify a canonical set of\nfeatures is challenged by the observed inconsistency of learned SAE features\nacross different training runs, undermining the reliability and efficiency of\nMI research. This position paper argues that mechanistic interpretability\nshould prioritize feature consistency in SAEs -- the reliable convergence to\nequivalent feature sets across independent runs. We propose using the Pairwise\nDictionary Mean Correlation Coefficient (PW-MCC) as a practical metric to\noperationalize consistency and demonstrate that high levels are achievable\n(0.80 for TopK SAEs on LLM activations) with appropriate architectural choices.\nOur contributions include detailing the benefits of prioritizing consistency;\nproviding theoretical grounding and synthetic validation using a model\norganism, which verifies PW-MCC as a reliable proxy for ground-truth recovery;\nand extending these findings to real-world LLM data, where high feature\nconsistency strongly correlates with the semantic similarity of learned feature\nexplanations. We call for a community-wide shift towards systematically\nmeasuring feature consistency to foster robust cumulative progress in MI.",
      "upvotes": 4,
      "discussionId": "683528159f968fc5c6044aff",
      "githubRepo": "https://github.com/xiangchensong/sae-feature-consistency",
      "ai_summary": "Prioritizing feature consistency in sparse autoencoders improves mechanistic interpretability of neural networks by ensuring reliable and interpretable features.",
      "ai_keywords": [
        "Sparse Autoencoders (SAEs)",
        "mechanistic interpretability (MI)",
        "feature consistency",
        "Pairwise Dictionary Mean Correlation Coefficient (PW-MCC)",
        "TopK SAEs",
        "LLM activations",
        "synthetic validation",
        "semantic similarity",
        "learned feature explanations"
      ]
    },
    "publishedAt": "2025-05-26T13:31:36.000Z",
    "title": "Position: Mechanistic Interpretability Should Prioritize Feature\n  Consistency in SAEs",
    "summary": "Sparse Autoencoders (SAEs) are a prominent tool in mechanistic\ninterpretability (MI) for decomposing neural network activations into\ninterpretable features. However, the aspiration to identify a canonical set of\nfeatures is challenged by the observed inconsistency of learned SAE features\nacross different training runs, undermining the reliability and efficiency of\nMI research. This position paper argues that mechanistic interpretability\nshould prioritize feature consistency in SAEs -- the reliable convergence to\nequivalent feature sets across independent runs. We propose using the Pairwise\nDictionary Mean Correlation Coefficient (PW-MCC) as a practical metric to\noperationalize consistency and demonstrate that high levels are achievable\n(0.80 for TopK SAEs on LLM activations) with appropriate architectural choices.\nOur contributions include detailing the benefits of prioritizing consistency;\nproviding theoretical grounding and synthetic validation using a model\norganism, which verifies PW-MCC as a reliable proxy for ground-truth recovery;\nand extending these findings to real-world LLM data, where high feature\nconsistency strongly correlates with the semantic similarity of learned feature\nexplanations. We call for a community-wide shift towards systematically\nmeasuring feature consistency to foster robust cumulative progress in MI.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20254.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64755a83e0b188d3cb2579d8",
      "avatarUrl": "/avatars/2c50590905f4bd398a4c9991e1b4b5bb.svg",
      "fullname": "Aashiq Muhamed",
      "name": "aashiqmuhamed",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.17652",
      "authors": [
        {
          "_id": "6835264edf7cbb5c08ce28a5",
          "name": "Deyang Kong",
          "hidden": false
        },
        {
          "_id": "6835264edf7cbb5c08ce28a6",
          "name": "Qi Guo",
          "hidden": false
        },
        {
          "_id": "6835264edf7cbb5c08ce28a7",
          "name": "Xiangyu Xi",
          "hidden": false
        },
        {
          "_id": "6835264edf7cbb5c08ce28a8",
          "name": "Wei Wang",
          "hidden": false
        },
        {
          "_id": "6835264edf7cbb5c08ce28a9",
          "name": "Jingang Wang",
          "hidden": false
        },
        {
          "_id": "6835264edf7cbb5c08ce28aa",
          "name": "Xunliang Cai",
          "hidden": false
        },
        {
          "_id": "6835264edf7cbb5c08ce28ab",
          "name": "Shikun Zhang",
          "hidden": false
        },
        {
          "_id": "6835264edf7cbb5c08ce28ac",
          "name": "Wei Ye",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T09:15:26.000Z",
      "submittedOnDailyAt": "2025-05-27T01:12:37.832Z",
      "title": "Rethinking the Sampling Criteria in Reinforcement Learning for LLM\n  Reasoning: A Competence-Difficulty Alignment Perspective",
      "submittedOnDailyBy": {
        "_id": "65a0aade5fafc248c2156e95",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65a0aade5fafc248c2156e95/S9YjJMTuKc-U1cFizqUMA.jpeg",
        "isPro": false,
        "fullname": "DeyangKong",
        "user": "DeyangKong",
        "type": "user"
      },
      "summary": "Reinforcement learning exhibits potential in enhancing the reasoning\nabilities of large language models, yet it is hard to scale for the low sample\nefficiency during the rollout phase. Existing methods attempt to improve\nefficiency by scheduling problems based on problem difficulties. However, these\napproaches suffer from unstable and biased estimations of problem difficulty\nand fail to capture the alignment between model competence and problem\ndifficulty in RL training, leading to suboptimal results. To tackle these\nlimitations, this paper introduces Competence-Difficulty\nAlignment Sampling (CDAS), which enables accurate\nand stable estimation of problem difficulties by aggregating historical\nperformance discrepancies of problems. Then the model competence is quantified\nto adaptively select problems whose difficulty is in alignment with the model's\ncurrent competence using a fixed-point system. Experimental results across a\nrange of challenging mathematical benchmarks show that CDAS achieves great\nimprovements in both accuracy and efficiency. CDAS attains the highest average\naccuracy against baselines and exhibits significant speed advantages compared\nto Dynamic Sampling, a competitive strategy in DAPO, which is 2.33\ntimes slower than CDAS.",
      "upvotes": 4,
      "discussionId": "6835264fdf7cbb5c08ce28f9",
      "ai_summary": "CDAS addresses low sample efficiency in reinforcement learning by aligning model competence with problem difficulty, improving both accuracy and efficiency in mathematical benchmarks.",
      "ai_keywords": [
        "reinforcement learning",
        "competence-difficulty alignment sampling",
        "CDAS",
        "historical performance discrepancies",
        "fixed-point system",
        "dynamic sampling",
        "DAPO"
      ]
    },
    "publishedAt": "2025-05-23T05:15:26.000Z",
    "title": "Rethinking the Sampling Criteria in Reinforcement Learning for LLM\n  Reasoning: A Competence-Difficulty Alignment Perspective",
    "summary": "Reinforcement learning exhibits potential in enhancing the reasoning\nabilities of large language models, yet it is hard to scale for the low sample\nefficiency during the rollout phase. Existing methods attempt to improve\nefficiency by scheduling problems based on problem difficulties. However, these\napproaches suffer from unstable and biased estimations of problem difficulty\nand fail to capture the alignment between model competence and problem\ndifficulty in RL training, leading to suboptimal results. To tackle these\nlimitations, this paper introduces Competence-Difficulty\nAlignment Sampling (CDAS), which enables accurate\nand stable estimation of problem difficulties by aggregating historical\nperformance discrepancies of problems. Then the model competence is quantified\nto adaptively select problems whose difficulty is in alignment with the model's\ncurrent competence using a fixed-point system. Experimental results across a\nrange of challenging mathematical benchmarks show that CDAS achieves great\nimprovements in both accuracy and efficiency. CDAS attains the highest average\naccuracy against baselines and exhibits significant speed advantages compared\nto Dynamic Sampling, a competitive strategy in DAPO, which is 2.33\ntimes slower than CDAS.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17652.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65a0aade5fafc248c2156e95",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65a0aade5fafc248c2156e95/S9YjJMTuKc-U1cFizqUMA.jpeg",
      "fullname": "DeyangKong",
      "name": "DeyangKong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.10887",
      "authors": [
        {
          "_id": "682b5387f1e88185bddb0643",
          "user": {
            "_id": "648a2042e8bee533291da413",
            "avatarUrl": "/avatars/83b918ddd7e2130a1c72ae74606068dc.svg",
            "isPro": false,
            "fullname": "Bin Lei",
            "user": "Bin12345",
            "type": "user"
          },
          "name": "Bin Lei",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-20T07:22:15.514Z",
          "hidden": false
        },
        {
          "_id": "682b5387f1e88185bddb0644",
          "name": "Weitai Kang",
          "hidden": false
        },
        {
          "_id": "682b5387f1e88185bddb0645",
          "name": "Zijian Zhang",
          "hidden": false
        },
        {
          "_id": "682b5387f1e88185bddb0646",
          "name": "Winson Chen",
          "hidden": false
        },
        {
          "_id": "682b5387f1e88185bddb0647",
          "name": "Xi Xie",
          "hidden": false
        },
        {
          "_id": "682b5387f1e88185bddb0648",
          "name": "Shan Zuo",
          "hidden": false
        },
        {
          "_id": "682b5387f1e88185bddb0649",
          "name": "Mimi Xie",
          "hidden": false
        },
        {
          "_id": "682b5387f1e88185bddb064a",
          "name": "Ali Payani",
          "hidden": false
        },
        {
          "_id": "682b5387f1e88185bddb064b",
          "name": "Mingyi Hong",
          "hidden": false
        },
        {
          "_id": "682b5387f1e88185bddb064c",
          "name": "Yan Yan",
          "hidden": false
        },
        {
          "_id": "682b5387f1e88185bddb064d",
          "name": "Caiwen Ding",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-16T05:43:27.000Z",
      "submittedOnDailyAt": "2025-05-27T01:07:44.773Z",
      "title": "InfantAgent-Next: A Multimodal Generalist Agent for Automated Computer\n  Interaction",
      "submittedOnDailyBy": {
        "_id": "648a2042e8bee533291da413",
        "avatarUrl": "/avatars/83b918ddd7e2130a1c72ae74606068dc.svg",
        "isPro": false,
        "fullname": "Bin Lei",
        "user": "Bin12345",
        "type": "user"
      },
      "summary": "This paper introduces InfantAgent-Next, a generalist agent capable\nof interacting with computers in a multimodal manner, encompassing text,\nimages, audio, and video. Unlike existing approaches that either build\nintricate workflows around a single large model or only provide workflow\nmodularity, our agent integrates tool-based and pure vision agents within a\nhighly modular architecture, enabling different models to collaboratively solve\ndecoupled tasks in a step-by-step manner. Our generality is demonstrated by our\nability to evaluate not only pure vision-based real-world benchmarks (i.e.,\nOSWorld), but also more general or tool-intensive benchmarks (e.g., GAIA and\nSWE-Bench). Specifically, we achieve 7.27% accuracy on OSWorld,\nhigher than Claude-Computer-Use. Codes and evaluation scripts are open-sourced\nat https://github.com/bin123apple/InfantAgent.",
      "upvotes": 3,
      "discussionId": "682b5389f1e88185bddb070d",
      "githubRepo": "https://github.com/bin123apple/InfantAgent",
      "ai_summary": "InfantAgent-Next is a multimodal agent that integrates tool-based and vision models in a modular architecture to solve various benchmarks, including OSWorld, GAIA, and SWE-Bench.",
      "ai_keywords": [
        "multimodal agent",
        "tool-based agents",
        "pure vision agents",
        "modular architecture",
        "OSWorld",
        "GAIA",
        "SWE-Bench"
      ]
    },
    "publishedAt": "2025-05-16T01:43:27.000Z",
    "title": "InfantAgent-Next: A Multimodal Generalist Agent for Automated Computer\n  Interaction",
    "summary": "This paper introduces InfantAgent-Next, a generalist agent capable\nof interacting with computers in a multimodal manner, encompassing text,\nimages, audio, and video. Unlike existing approaches that either build\nintricate workflows around a single large model or only provide workflow\nmodularity, our agent integrates tool-based and pure vision agents within a\nhighly modular architecture, enabling different models to collaboratively solve\ndecoupled tasks in a step-by-step manner. Our generality is demonstrated by our\nability to evaluate not only pure vision-based real-world benchmarks (i.e.,\nOSWorld), but also more general or tool-intensive benchmarks (e.g., GAIA and\nSWE-Bench). Specifically, we achieve 7.27% accuracy on OSWorld,\nhigher than Claude-Computer-Use. Codes and evaluation scripts are open-sourced\nat https://github.com/bin123apple/InfantAgent.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.10887.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648a2042e8bee533291da413",
      "avatarUrl": "/avatars/83b918ddd7e2130a1c72ae74606068dc.svg",
      "fullname": "Bin Lei",
      "name": "Bin12345",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 20
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.20278",
      "authors": [
        {
          "_id": "68353261bc28496925a185c9",
          "name": "Hoyeon Chang",
          "hidden": false
        },
        {
          "_id": "68353261bc28496925a185ca",
          "name": "Jinho Park",
          "hidden": false
        },
        {
          "_id": "68353261bc28496925a185cb",
          "name": "Hanseul Cho",
          "hidden": false
        },
        {
          "_id": "68353261bc28496925a185cc",
          "name": "Sohee Yang",
          "hidden": false
        },
        {
          "_id": "68353261bc28496925a185cd",
          "name": "Miyoung Ko",
          "hidden": false
        },
        {
          "_id": "68353261bc28496925a185ce",
          "name": "Hyeonbin Hwang",
          "hidden": false
        },
        {
          "_id": "68353261bc28496925a185cf",
          "name": "Seungpil Won",
          "hidden": false
        },
        {
          "_id": "68353261bc28496925a185d0",
          "name": "Dohaeng Lee",
          "hidden": false
        },
        {
          "_id": "68353261bc28496925a185d1",
          "name": "Youbin Ahn",
          "hidden": false
        },
        {
          "_id": "68353261bc28496925a185d2",
          "name": "Minjoon Seo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-26T17:55:15.000Z",
      "submittedOnDailyAt": "2025-05-27T02:04:40.615Z",
      "title": "The Coverage Principle: A Framework for Understanding Compositional\n  Generalization",
      "submittedOnDailyBy": {
        "_id": "64d0d6684dfd5df70744b237",
        "avatarUrl": "/avatars/4ea57bfd407e8cb727c624f64af75478.svg",
        "isPro": false,
        "fullname": "Chang",
        "user": "Hoyeon",
        "type": "user"
      },
      "summary": "Large language models excel at pattern matching, yet often fall short in\nsystematic compositional generalization. We propose the coverage principle: a\ndata-centric framework showing that models relying primarily on pattern\nmatching for compositional tasks cannot reliably generalize beyond substituting\nfragments that yield identical results when used in the same contexts. We\ndemonstrate that this framework has a strong predictive power for the\ngeneralization capabilities of Transformers. First, we derive and empirically\nconfirm that the training data required for two-hop generalization grows at\nleast quadratically with the token set size, and the training data efficiency\ndoes not improve with 20x parameter scaling. Second, for compositional tasks\nwith path ambiguity where one variable affects the output through multiple\ncomputational paths, we show that Transformers learn context-dependent state\nrepresentations that undermine both performance and interoperability. Third,\nChain-of-Thought supervision improves training data efficiency for multi-hop\ntasks but still struggles with path ambiguity. Finally, we outline a\nmechanism-based taxonomy that distinguishes three ways neural networks\ncan generalize: structure-based (bounded by coverage), property-based\n(leveraging algebraic invariances), and shared-operator (through function\nreuse). This conceptual lens contextualizes our results and highlights where\nnew architectural ideas are needed to achieve systematic compositionally.\nOverall, the coverage principle provides a unified lens for understanding\ncompositional reasoning, and underscores the need for fundamental architectural\nor training innovations to achieve truly systematic compositionality.",
      "upvotes": 2,
      "discussionId": "68353261bc28496925a185ef",
      "ai_summary": "The coverage principle highlights limitations in Transformers' compositional generalization, emphasizing the need for new architectures or training methods to achieve systematic compositionality by distinguishing different mechanisms of generalization.",
      "ai_keywords": [
        "coverage principle",
        "data-centric framework",
        "sequential application",
        "pattern matching",
        "compositional generalization",
        "Transformers",
        "two-hop generalization",
        "token set size",
        "training data efficiency",
        "context-dependent state representations",
        "performance",
        "interoperability",
        "Chain-of-Thought supervision",
        "multi-hop tasks",
        "path ambiguity",
        "structure-based",
        "property-based",
        "shared-operator",
        "architectural innovations"
      ]
    },
    "publishedAt": "2025-05-26T13:55:15.000Z",
    "title": "The Coverage Principle: A Framework for Understanding Compositional\n  Generalization",
    "summary": "Large language models excel at pattern matching, yet often fall short in\nsystematic compositional generalization. We propose the coverage principle: a\ndata-centric framework showing that models relying primarily on pattern\nmatching for compositional tasks cannot reliably generalize beyond substituting\nfragments that yield identical results when used in the same contexts. We\ndemonstrate that this framework has a strong predictive power for the\ngeneralization capabilities of Transformers. First, we derive and empirically\nconfirm that the training data required for two-hop generalization grows at\nleast quadratically with the token set size, and the training data efficiency\ndoes not improve with 20x parameter scaling. Second, for compositional tasks\nwith path ambiguity where one variable affects the output through multiple\ncomputational paths, we show that Transformers learn context-dependent state\nrepresentations that undermine both performance and interoperability. Third,\nChain-of-Thought supervision improves training data efficiency for multi-hop\ntasks but still struggles with path ambiguity. Finally, we outline a\nmechanism-based taxonomy that distinguishes three ways neural networks\ncan generalize: structure-based (bounded by coverage), property-based\n(leveraging algebraic invariances), and shared-operator (through function\nreuse). This conceptual lens contextualizes our results and highlights where\nnew architectural ideas are needed to achieve systematic compositionally.\nOverall, the coverage principle provides a unified lens for understanding\ncompositional reasoning, and underscores the need for fundamental architectural\nor training innovations to achieve truly systematic compositionality.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20278.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64d0d6684dfd5df70744b237",
      "avatarUrl": "/avatars/4ea57bfd407e8cb727c624f64af75478.svg",
      "fullname": "Chang",
      "name": "Hoyeon",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.19706",
      "authors": [
        {
          "_id": "6835182873a16b09c94ac4d2",
          "name": "Tej Deep Pala",
          "hidden": false
        },
        {
          "_id": "6835182873a16b09c94ac4d3",
          "name": "Panshul Sharma",
          "hidden": false
        },
        {
          "_id": "6835182873a16b09c94ac4d4",
          "name": "Amir Zadeh",
          "hidden": false
        },
        {
          "_id": "6835182873a16b09c94ac4d5",
          "name": "Chuan Li",
          "hidden": false
        },
        {
          "_id": "6835182873a16b09c94ac4d6",
          "name": "Soujanya Poria",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/626b626405fe1cb65725aca1/OPFuTq1oRiXqqwJPyKgUx.png",
        "https://cdn-uploads.huggingface.co/production/uploads/626b626405fe1cb65725aca1/VsJ0SH2BgYbBQTS55nWSB.png",
        "https://cdn-uploads.huggingface.co/production/uploads/626b626405fe1cb65725aca1/qreWH-gdINsiHnTwLQcOL.png"
      ],
      "publishedAt": "2025-05-26T08:56:36.000Z",
      "submittedOnDailyAt": "2025-05-27T00:29:18.345Z",
      "title": "Error Typing for Smarter Rewards: Improving Process Reward Models with\n  Error-Aware Hierarchical Supervision",
      "submittedOnDailyBy": {
        "_id": "626b626405fe1cb65725aca1",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/626b626405fe1cb65725aca1/aa-Lata46I3fXOmMetvXH.jpeg",
        "isPro": false,
        "fullname": "Soujanya Poria",
        "user": "soujanyaporia",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) are prone to hallucination, especially during\nmulti-hop and reasoning-intensive tasks such as mathematical problem solving.\nWhile Outcome Reward Models verify only final answers, Process Reward Models\n(PRMs) score each intermediate step to steer generation toward coherent\nsolutions. We introduce PathFinder-PRM, a novel hierarchical, error-aware\ndiscriminative PRM that first classifies math and consistency errors at each\nstep, then combines these fine-grained signals to estimate step correctness. To\ntrain PathFinder-PRM, we construct a 400K-sample dataset by enriching the\nhuman-annotated PRM800K corpus and RLHFlow Mistral traces with\nthree-dimensional step-level labels. On PRMBench, PathFinder-PRM achieves a new\nstate-of-the-art PRMScore of 67.7, outperforming the prior best (65.5) while\nusing 3 times less data. When applied to reward guided greedy search, our model\nyields prm@8 48.3, a +1.5 point gain over the strongest baseline. These results\ndemonstrate that decoupled error detection and reward estimation not only boost\nfine-grained error detection but also substantially improve end-to-end,\nreward-guided mathematical reasoning with greater data efficiency.",
      "upvotes": 2,
      "discussionId": "6835182973a16b09c94ac514",
      "githubRepo": "https://github.com/declare-lab/PathFinder-PRM",
      "ai_summary": "PathFinder-PRM, a hierarchical and error-aware Process Reward Model, improves mathematical problem-solving by fine-grained error classification and step correctness estimation, achieving state-of-the-art PRMScore with reduced data usage.",
      "ai_keywords": [
        "Large Language Models",
        "hallucination",
        "mathematical problem solving",
        "Outcome Reward Models",
        "Process Reward Models",
        "PathFinder-PRM",
        "hierarchical",
        "error-aware",
        "discriminative PRM",
        "math errors",
        "consistency errors",
        "step correctness",
        "PRMBench",
        "PRMScore",
        "reward guided greedy search",
        "prm@8",
        "data efficiency"
      ]
    },
    "publishedAt": "2025-05-26T04:56:36.000Z",
    "title": "Error Typing for Smarter Rewards: Improving Process Reward Models with\n  Error-Aware Hierarchical Supervision",
    "summary": "Large Language Models (LLMs) are prone to hallucination, especially during\nmulti-hop and reasoning-intensive tasks such as mathematical problem solving.\nWhile Outcome Reward Models verify only final answers, Process Reward Models\n(PRMs) score each intermediate step to steer generation toward coherent\nsolutions. We introduce PathFinder-PRM, a novel hierarchical, error-aware\ndiscriminative PRM that first classifies math and consistency errors at each\nstep, then combines these fine-grained signals to estimate step correctness. To\ntrain PathFinder-PRM, we construct a 400K-sample dataset by enriching the\nhuman-annotated PRM800K corpus and RLHFlow Mistral traces with\nthree-dimensional step-level labels. On PRMBench, PathFinder-PRM achieves a new\nstate-of-the-art PRMScore of 67.7, outperforming the prior best (65.5) while\nusing 3 times less data. When applied to reward guided greedy search, our model\nyields prm@8 48.3, a +1.5 point gain over the strongest baseline. These results\ndemonstrate that decoupled error detection and reward estimation not only boost\nfine-grained error detection but also substantially improve end-to-end,\nreward-guided mathematical reasoning with greater data efficiency.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/626b626405fe1cb65725aca1/OPFuTq1oRiXqqwJPyKgUx.png",
      "https://cdn-uploads.huggingface.co/production/uploads/626b626405fe1cb65725aca1/VsJ0SH2BgYbBQTS55nWSB.png",
      "https://cdn-uploads.huggingface.co/production/uploads/626b626405fe1cb65725aca1/qreWH-gdINsiHnTwLQcOL.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19706.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "626b626405fe1cb65725aca1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/626b626405fe1cb65725aca1/aa-Lata46I3fXOmMetvXH.jpeg",
      "fullname": "Soujanya Poria",
      "name": "soujanyaporia",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.19640",
      "authors": [
        {
          "_id": "68353e3f9f4e0a0f0496d0c6",
          "name": "Roy Xie",
          "hidden": false
        },
        {
          "_id": "68353e3f9f4e0a0f0496d0c7",
          "name": "David Qiu",
          "hidden": false
        },
        {
          "_id": "68353e3f9f4e0a0f0496d0c8",
          "name": "Deepak Gopinath",
          "hidden": false
        },
        {
          "_id": "68353e3f9f4e0a0f0496d0c9",
          "name": "Dong Lin",
          "hidden": false
        },
        {
          "_id": "68353e3f9f4e0a0f0496d0ca",
          "name": "Yanchao Sun",
          "hidden": false
        },
        {
          "_id": "68353e3f9f4e0a0f0496d0cb",
          "name": "Chong Wang",
          "hidden": false
        },
        {
          "_id": "68353e3f9f4e0a0f0496d0cc",
          "name": "Saloni Potdar",
          "hidden": false
        },
        {
          "_id": "68353e3f9f4e0a0f0496d0cd",
          "name": "Bhuwan Dhingra",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-26T07:58:17.000Z",
      "submittedOnDailyAt": "2025-05-27T02:56:39.316Z",
      "title": "Interleaved Reasoning for Large Language Models via Reinforcement\n  Learning",
      "submittedOnDailyBy": {
        "_id": "6555a124a6554059711b58a2",
        "avatarUrl": "/avatars/222bb6b8f252d6c2bbd4cf35a54fc1c9.svg",
        "isPro": false,
        "fullname": "Roy",
        "user": "RRoy233",
        "type": "user"
      },
      "summary": "Long chain-of-thought (CoT) significantly enhances large language models'\n(LLM) reasoning capabilities. However, the extensive reasoning traces lead to\ninefficiencies and an increased time-to-first-token (TTFT). We propose a novel\ntraining paradigm that uses reinforcement learning (RL) to guide reasoning LLMs\nto interleave thinking and answering for multi-hop questions. We observe that\nmodels inherently possess the ability to perform interleaved reasoning, which\ncan be further enhanced through RL. We introduce a simple yet effective\nrule-based reward to incentivize correct intermediate steps, which guides the\npolicy model toward correct reasoning paths by leveraging intermediate signals\ngenerated during interleaved reasoning. Extensive experiments conducted across\nfive diverse datasets and three RL algorithms (PPO, GRPO, and REINFORCE++)\ndemonstrate consistent improvements over traditional think-answer reasoning,\nwithout requiring external tools. Specifically, our approach reduces TTFT by\nover 80% on average and improves up to 19.3% in Pass@1 accuracy. Furthermore,\nour method, trained solely on question answering and logical reasoning\ndatasets, exhibits strong generalization ability to complex reasoning datasets\nsuch as MATH, GPQA, and MMLU. Additionally, we conduct in-depth analysis to\nreveal several valuable insights into conditional reward modeling.",
      "upvotes": 2,
      "discussionId": "68353e409f4e0a0f0496d0fb",
      "ai_summary": "A reinforcement learning-guided training paradigm enhances large language models' reasoning efficiency and performance for multi-hop questions by interleaving thinking and answering.",
      "ai_keywords": [
        "chain-of-thought",
        "large language models",
        "reasoning capabilities",
        "time-to-first-token",
        "reinforcement learning",
        "interleaved reasoning",
        "rule-based reward",
        "reward modeling",
        "multi-hop questions",
        "think-answer reasoning",
        "Pass@1 accuracy",
        "MATH",
        "GPQA",
        "MMLU",
        "PPO",
        "GRPO",
        "REINFORCE++"
      ]
    },
    "publishedAt": "2025-05-26T03:58:17.000Z",
    "title": "Interleaved Reasoning for Large Language Models via Reinforcement\n  Learning",
    "summary": "Long chain-of-thought (CoT) significantly enhances large language models'\n(LLM) reasoning capabilities. However, the extensive reasoning traces lead to\ninefficiencies and an increased time-to-first-token (TTFT). We propose a novel\ntraining paradigm that uses reinforcement learning (RL) to guide reasoning LLMs\nto interleave thinking and answering for multi-hop questions. We observe that\nmodels inherently possess the ability to perform interleaved reasoning, which\ncan be further enhanced through RL. We introduce a simple yet effective\nrule-based reward to incentivize correct intermediate steps, which guides the\npolicy model toward correct reasoning paths by leveraging intermediate signals\ngenerated during interleaved reasoning. Extensive experiments conducted across\nfive diverse datasets and three RL algorithms (PPO, GRPO, and REINFORCE++)\ndemonstrate consistent improvements over traditional think-answer reasoning,\nwithout requiring external tools. Specifically, our approach reduces TTFT by\nover 80% on average and improves up to 19.3% in Pass@1 accuracy. Furthermore,\nour method, trained solely on question answering and logical reasoning\ndatasets, exhibits strong generalization ability to complex reasoning datasets\nsuch as MATH, GPQA, and MMLU. Additionally, we conduct in-depth analysis to\nreveal several valuable insights into conditional reward modeling.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19640.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6555a124a6554059711b58a2",
      "avatarUrl": "/avatars/222bb6b8f252d6c2bbd4cf35a54fc1c9.svg",
      "fullname": "Roy",
      "name": "RRoy233",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.19630",
      "authors": [
        {
          "_id": "683522abd68b329aeb799c46",
          "name": "Yichun Feng",
          "hidden": false
        },
        {
          "_id": "683522abd68b329aeb799c47",
          "name": "Jiawei Wang",
          "hidden": false
        },
        {
          "_id": "683522abd68b329aeb799c48",
          "name": "Lu Zhou",
          "hidden": false
        },
        {
          "_id": "683522abd68b329aeb799c49",
          "name": "Yixue Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-26T07:48:14.000Z",
      "submittedOnDailyAt": "2025-05-27T00:56:34.903Z",
      "title": "DoctorAgent-RL: A Multi-Agent Collaborative Reinforcement Learning\n  System for Multi-Turn Clinical Dialogue",
      "submittedOnDailyBy": {
        "_id": "64060b49a577649430bf6974",
        "avatarUrl": "/avatars/74d0d6ed656b593e4c101b09edf18c7a.svg",
        "isPro": false,
        "fullname": "Jiawei Wang",
        "user": "Jarvis1111",
        "type": "user"
      },
      "summary": "Large language models (LLMs) have demonstrated excellent capabilities in the\nfield of biomedical question answering, but their application in real-world\nclinical consultations still faces core challenges. Existing systems rely on a\none-way information transmission mode where patients must fully describe their\nsymptoms in a single round, leading to nonspecific diagnostic recommendations\nwhen complaints are vague. Traditional multi-turn dialogue methods based on\nsupervised learning are constrained by static data-driven paradigms, lacking\ngeneralizability and struggling to intelligently extract key clinical\ninformation. To address these limitations, we propose DoctorAgent-RL, a\nreinforcement learning (RL)-based multi-agent collaborative framework that\nmodels medical consultations as a dynamic decision-making process under\nuncertainty. The doctor agent continuously optimizes its questioning strategy\nwithin the RL framework through multi-turn interactions with the patient agent,\ndynamically adjusting its information-gathering path based on comprehensive\nrewards from the Consultation Evaluator. This RL fine-tuning mechanism enables\nLLMs to autonomously develop interaction strategies aligned with clinical\nreasoning logic, rather than superficially imitating patterns in existing\ndialogue data. Notably, we constructed MTMedDialog, the first English\nmulti-turn medical consultation dataset capable of simulating patient\ninteractions. Experiments demonstrate that DoctorAgent-RL outperforms existing\nmodels in both multi-turn reasoning capability and final diagnostic\nperformance, demonstrating practical value in assisting clinical consultations.\nhttps://github.com/JarvisUSTC/DoctorAgent-RL",
      "upvotes": 2,
      "discussionId": "683522add68b329aeb799cc4",
      "githubRepo": "https://github.com/JarvisUSTC/DoctorAgent-RL",
      "ai_summary": "DoctorAgent-RL, a reinforcement learning-based multi-agent framework, enhances multi-turn reasoning and diagnostic performance in medical consultations compared to existing systems.",
      "ai_keywords": [
        "reinforcement learning",
        "multi-agent collaborative framework",
        "dynamic decision-making",
        "uncertainty",
        "questioning strategy",
        "interaction strategy",
        "clinical reasoning",
        "multi-turn medical consultation dataset",
        "diagnostic performance"
      ]
    },
    "publishedAt": "2025-05-26T03:48:14.000Z",
    "title": "DoctorAgent-RL: A Multi-Agent Collaborative Reinforcement Learning\n  System for Multi-Turn Clinical Dialogue",
    "summary": "Large language models (LLMs) have demonstrated excellent capabilities in the\nfield of biomedical question answering, but their application in real-world\nclinical consultations still faces core challenges. Existing systems rely on a\none-way information transmission mode where patients must fully describe their\nsymptoms in a single round, leading to nonspecific diagnostic recommendations\nwhen complaints are vague. Traditional multi-turn dialogue methods based on\nsupervised learning are constrained by static data-driven paradigms, lacking\ngeneralizability and struggling to intelligently extract key clinical\ninformation. To address these limitations, we propose DoctorAgent-RL, a\nreinforcement learning (RL)-based multi-agent collaborative framework that\nmodels medical consultations as a dynamic decision-making process under\nuncertainty. The doctor agent continuously optimizes its questioning strategy\nwithin the RL framework through multi-turn interactions with the patient agent,\ndynamically adjusting its information-gathering path based on comprehensive\nrewards from the Consultation Evaluator. This RL fine-tuning mechanism enables\nLLMs to autonomously develop interaction strategies aligned with clinical\nreasoning logic, rather than superficially imitating patterns in existing\ndialogue data. Notably, we constructed MTMedDialog, the first English\nmulti-turn medical consultation dataset capable of simulating patient\ninteractions. Experiments demonstrate that DoctorAgent-RL outperforms existing\nmodels in both multi-turn reasoning capability and final diagnostic\nperformance, demonstrating practical value in assisting clinical consultations.\nhttps://github.com/JarvisUSTC/DoctorAgent-RL",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19630.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64060b49a577649430bf6974",
      "avatarUrl": "/avatars/74d0d6ed656b593e4c101b09edf18c7a.svg",
      "fullname": "Jiawei Wang",
      "name": "Jarvis1111",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.19443",
      "authors": [
        {
          "_id": "683517bf6bb42c7e99bd3b5c",
          "name": "Ranjan Sapkota",
          "hidden": false
        },
        {
          "_id": "683517bf6bb42c7e99bd3b5d",
          "name": "Konstantinos I. Roumeliotis",
          "hidden": false
        },
        {
          "_id": "683517bf6bb42c7e99bd3b5e",
          "name": "Manoj Karkee",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/67ddd80896ac367438d400a6/ASTag4z8Os01guAbKpxI6.jpeg",
        "https://cdn-uploads.huggingface.co/production/uploads/67ddd80896ac367438d400a6/EgtU3Vsfc22Hko-FbRQ51.jpeg"
      ],
      "publishedAt": "2025-05-26T03:00:21.000Z",
      "submittedOnDailyAt": "2025-05-27T00:12:16.499Z",
      "title": "Vibe Coding vs. Agentic Coding: Fundamentals and Practical Implications\n  of Agentic AI",
      "submittedOnDailyBy": {
        "_id": "67ddd80896ac367438d400a6",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/C1NY6Nv5i0erwLnzCrTUM.png",
        "isPro": false,
        "fullname": "Ranjan Sapkota",
        "user": "RanjanSapkota",
        "type": "user"
      },
      "summary": "This review presents a comprehensive analysis of two emerging paradigms in\nAI-assisted software development: vibe coding and agentic coding. While both\nleverage large language models (LLMs), they differ fundamentally in autonomy,\narchitectural design, and the role of the developer. Vibe coding emphasizes\nintuitive, human-in-the-loop interaction through prompt-based, conversational\nworkflows that support ideation, experimentation, and creative exploration. In\ncontrast, agentic coding enables autonomous software development through\ngoal-driven agents capable of planning, executing, testing, and iterating tasks\nwith minimal human intervention. We propose a detailed taxonomy spanning\nconceptual foundations, execution models, feedback loops, safety mechanisms,\ndebugging strategies, and real-world tool ecosystems. Through comparative\nworkflow analysis and 20 detailed use cases, we illustrate how vibe systems\nthrive in early-stage prototyping and education, while agentic systems excel in\nenterprise-grade automation, codebase refactoring, and CI/CD integration. We\nfurther examine emerging trends in hybrid architectures, where natural language\ninterfaces are coupled with autonomous execution pipelines. Finally, we\narticulate a future roadmap for agentic AI, outlining the infrastructure needed\nfor trustworthy, explainable, and collaborative systems. Our findings suggest\nthat successful AI software engineering will rely not on choosing one paradigm,\nbut on harmonizing their strengths within a unified, human-centered development\nlifecycle.",
      "upvotes": 2,
      "discussionId": "683517c06bb42c7e99bd3b92",
      "ai_summary": "A review contrasts vibe coding and agentic coding paradigms, highlighting their differences in interaction, autonomy, and application areas in AI-assisted software development.",
      "ai_keywords": [
        "large language models",
        "vibe coding",
        "agentic coding",
        "prompt-based",
        "conversational workflows",
        "goal-driven agents",
        "execution models",
        "feedback loops",
        "safety mechanisms",
        "debugging strategies",
        "tool ecosystems",
        "hybrid architectures",
        "autonomous execution pipelines",
        "trustworthy",
        "explainable",
        "collaborative systems",
        "unified",
        "human-centered development lifecycle"
      ]
    },
    "publishedAt": "2025-05-25T23:00:21.000Z",
    "title": "Vibe Coding vs. Agentic Coding: Fundamentals and Practical Implications\n  of Agentic AI",
    "summary": "This review presents a comprehensive analysis of two emerging paradigms in\nAI-assisted software development: vibe coding and agentic coding. While both\nleverage large language models (LLMs), they differ fundamentally in autonomy,\narchitectural design, and the role of the developer. Vibe coding emphasizes\nintuitive, human-in-the-loop interaction through prompt-based, conversational\nworkflows that support ideation, experimentation, and creative exploration. In\ncontrast, agentic coding enables autonomous software development through\ngoal-driven agents capable of planning, executing, testing, and iterating tasks\nwith minimal human intervention. We propose a detailed taxonomy spanning\nconceptual foundations, execution models, feedback loops, safety mechanisms,\ndebugging strategies, and real-world tool ecosystems. Through comparative\nworkflow analysis and 20 detailed use cases, we illustrate how vibe systems\nthrive in early-stage prototyping and education, while agentic systems excel in\nenterprise-grade automation, codebase refactoring, and CI/CD integration. We\nfurther examine emerging trends in hybrid architectures, where natural language\ninterfaces are coupled with autonomous execution pipelines. Finally, we\narticulate a future roadmap for agentic AI, outlining the infrastructure needed\nfor trustworthy, explainable, and collaborative systems. Our findings suggest\nthat successful AI software engineering will rely not on choosing one paradigm,\nbut on harmonizing their strengths within a unified, human-centered development\nlifecycle.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/67ddd80896ac367438d400a6/ASTag4z8Os01guAbKpxI6.jpeg",
      "https://cdn-uploads.huggingface.co/production/uploads/67ddd80896ac367438d400a6/EgtU3Vsfc22Hko-FbRQ51.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19443.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67ddd80896ac367438d400a6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/C1NY6Nv5i0erwLnzCrTUM.png",
      "fullname": "Ranjan Sapkota",
      "name": "RanjanSapkota",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.18384",
      "authors": [
        {
          "_id": "68354f30d795fadab0623699",
          "user": {
            "_id": "65319bd7f85995389d4f019c",
            "avatarUrl": "/avatars/657858b8435b220c9a29918c0dae9c6d.svg",
            "isPro": false,
            "fullname": "Boyi Wei",
            "user": "boyiwei",
            "type": "user"
          },
          "name": "Boyi Wei",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-27T07:49:18.459Z",
          "hidden": false
        },
        {
          "_id": "68354f30d795fadab062369a",
          "name": "Benedikt Stroebl",
          "hidden": false
        },
        {
          "_id": "68354f30d795fadab062369b",
          "name": "Jiacen Xu",
          "hidden": false
        },
        {
          "_id": "68354f30d795fadab062369c",
          "name": "Joie Zhang",
          "hidden": false
        },
        {
          "_id": "68354f30d795fadab062369d",
          "name": "Zhou Li",
          "hidden": false
        },
        {
          "_id": "68354f30d795fadab062369e",
          "name": "Peter Henderson",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T21:18:59.000Z",
      "submittedOnDailyAt": "2025-05-27T04:06:40.688Z",
      "title": "Dynamic Risk Assessments for Offensive Cybersecurity Agents",
      "submittedOnDailyBy": {
        "_id": "65319bd7f85995389d4f019c",
        "avatarUrl": "/avatars/657858b8435b220c9a29918c0dae9c6d.svg",
        "isPro": false,
        "fullname": "Boyi Wei",
        "user": "boyiwei",
        "type": "user"
      },
      "summary": "Foundation models are increasingly becoming better autonomous programmers,\nraising the prospect that they could also automate dangerous offensive\ncyber-operations. Current frontier model audits probe the cybersecurity risks\nof such agents, but most fail to account for the degrees of freedom available\nto adversaries in the real world. In particular, with strong verifiers and\nfinancial incentives, agents for offensive cybersecurity are amenable to\niterative improvement by would-be adversaries. We argue that assessments should\ntake into account an expanded threat model in the context of cybersecurity,\nemphasizing the varying degrees of freedom that an adversary may possess in\nstateful and non-stateful environments within a fixed compute budget. We show\nthat even with a relatively small compute budget (8 H100 GPU Hours in our\nstudy), adversaries can improve an agent's cybersecurity capability on\nInterCode CTF by more than 40\\% relative to the baseline -- without any\nexternal assistance. These results highlight the need to evaluate agents'\ncybersecurity risk in a dynamic manner, painting a more representative picture\nof risk.",
      "upvotes": 2,
      "discussionId": "68354f30d795fadab06236fe",
      "githubRepo": "https://github.com/boyiwei/Dynamic-Risk-Assessment",
      "ai_summary": "Adversaries can significantly enhance foundation model capabilities in offensive cybersecurity with limited computational resources, underscoring the need for dynamic threat model assessments.",
      "ai_keywords": [
        "foundation models",
        "autonomous programmers",
        "offensive cybersecurity",
        "model audits",
        "cybersecurity risks",
        "verifiers",
        "financial incentives",
        "iterative improvement",
        "threat model",
        "stateful environments",
        "non-stateful environments",
        "compute budget",
        "InterCode CTF"
      ]
    },
    "publishedAt": "2025-05-23T17:18:59.000Z",
    "title": "Dynamic Risk Assessments for Offensive Cybersecurity Agents",
    "summary": "Foundation models are increasingly becoming better autonomous programmers,\nraising the prospect that they could also automate dangerous offensive\ncyber-operations. Current frontier model audits probe the cybersecurity risks\nof such agents, but most fail to account for the degrees of freedom available\nto adversaries in the real world. In particular, with strong verifiers and\nfinancial incentives, agents for offensive cybersecurity are amenable to\niterative improvement by would-be adversaries. We argue that assessments should\ntake into account an expanded threat model in the context of cybersecurity,\nemphasizing the varying degrees of freedom that an adversary may possess in\nstateful and non-stateful environments within a fixed compute budget. We show\nthat even with a relatively small compute budget (8 H100 GPU Hours in our\nstudy), adversaries can improve an agent's cybersecurity capability on\nInterCode CTF by more than 40\\% relative to the baseline -- without any\nexternal assistance. These results highlight the need to evaluate agents'\ncybersecurity risk in a dynamic manner, painting a more representative picture\nof risk.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18384.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65319bd7f85995389d4f019c",
      "avatarUrl": "/avatars/657858b8435b220c9a29918c0dae9c6d.svg",
      "fullname": "Boyi Wei",
      "name": "boyiwei",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.16312",
      "authors": [
        {
          "_id": "6830109ea20ebb4738e76931",
          "user": {
            "_id": "6747d38098fe79433a8c4580",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/BrcsTfusqfu9p9uv1NeX6.png",
            "isPro": false,
            "fullname": "Jiawei Liu",
            "user": "Jiawei1222",
            "type": "user"
          },
          "name": "Jiawei Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:15:09.111Z",
          "hidden": false
        },
        {
          "_id": "6830109ea20ebb4738e76932",
          "name": "Qisi Chen",
          "hidden": false
        },
        {
          "_id": "6830109ea20ebb4738e76933",
          "name": "Jianshu Zhang",
          "hidden": false
        },
        {
          "_id": "6830109ea20ebb4738e76934",
          "name": "Quan Liu",
          "hidden": false
        },
        {
          "_id": "6830109ea20ebb4738e76935",
          "name": "Defu Lian",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T07:07:43.000Z",
      "submittedOnDailyAt": "2025-05-27T05:04:30.217Z",
      "title": "EquivPruner: Boosting Efficiency and Quality in LLM-Based Search via\n  Action Pruning",
      "submittedOnDailyBy": {
        "_id": "6747d38098fe79433a8c4580",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/BrcsTfusqfu9p9uv1NeX6.png",
        "isPro": false,
        "fullname": "Jiawei Liu",
        "user": "Jiawei1222",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) excel at complex reasoning through search\nalgorithms, yet current strategies often suffer from massive token consumption\ndue to redundant exploration of semantically equivalent steps. Existing\nsemantic similarity methods struggle to accurately identify such equivalence in\ndomain-specific contexts like mathematical reasoning. To address this, we\npropose EquivPruner, a simple yet effective approach that identifies and prunes\nsemantically equivalent actions during LLM reasoning search. We also introduce\nMathEquiv, the first dataset we created for mathematical statement equivalence,\nwhich enables the training of a lightweight equivalence detector. Extensive\nexperiments across various models and tasks demonstrate that EquivPruner\nsignificantly reduces token consumption, improving searching efficiency and\noften bolstering reasoning accuracy. For instance, when applied to\nQwen2.5-Math-7B-Instruct on GSM8K, EquivPruner reduced token consumption by\n48.1\\% while also improving accuracy. Our code is available at\nhttps://github.com/Lolo1222/EquivPruner.",
      "upvotes": 1,
      "discussionId": "6830109fa20ebb4738e769a3",
      "githubRepo": "https://github.com/Lolo1222/EquivPruner",
      "ai_summary": "EquivPruner reduces token consumption and improves reasoning accuracy by pruning semantically equivalent actions in LLM searches, leveraging a new dataset for mathematical equivalence.",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "semantic similarity",
        "semantically equivalent actions",
        "EquivPruner",
        "MathEquiv",
        "equivalence detector",
        "GSM8K"
      ]
    },
    "publishedAt": "2025-05-22T03:07:43.000Z",
    "title": "EquivPruner: Boosting Efficiency and Quality in LLM-Based Search via\n  Action Pruning",
    "summary": "Large Language Models (LLMs) excel at complex reasoning through search\nalgorithms, yet current strategies often suffer from massive token consumption\ndue to redundant exploration of semantically equivalent steps. Existing\nsemantic similarity methods struggle to accurately identify such equivalence in\ndomain-specific contexts like mathematical reasoning. To address this, we\npropose EquivPruner, a simple yet effective approach that identifies and prunes\nsemantically equivalent actions during LLM reasoning search. We also introduce\nMathEquiv, the first dataset we created for mathematical statement equivalence,\nwhich enables the training of a lightweight equivalence detector. Extensive\nexperiments across various models and tasks demonstrate that EquivPruner\nsignificantly reduces token consumption, improving searching efficiency and\noften bolstering reasoning accuracy. For instance, when applied to\nQwen2.5-Math-7B-Instruct on GSM8K, EquivPruner reduced token consumption by\n48.1\\% while also improving accuracy. Our code is available at\nhttps://github.com/Lolo1222/EquivPruner.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16312.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6747d38098fe79433a8c4580",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/BrcsTfusqfu9p9uv1NeX6.png",
      "fullname": "Jiawei Liu",
      "name": "Jiawei1222",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.15957",
      "authors": [
        {
          "_id": "6830745670e219f5de8ad360",
          "user": {
            "_id": "646fa3016441111304fec68d",
            "avatarUrl": "/avatars/923629340f3785ae8c6e52cf3674d5c2.svg",
            "isPro": false,
            "fullname": "Chih-Kai Yang",
            "user": "zenyn",
            "type": "user"
          },
          "name": "Chih-Kai Yang",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-23T13:21:28.397Z",
          "hidden": false
        },
        {
          "_id": "6830745670e219f5de8ad361",
          "name": "Neo S. Ho",
          "hidden": false
        },
        {
          "_id": "6830745670e219f5de8ad362",
          "name": "Hung-yi Lee",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T19:17:29.000Z",
      "submittedOnDailyAt": "2025-05-27T01:41:21.824Z",
      "title": "Towards Holistic Evaluation of Large Audio-Language Models: A\n  Comprehensive Survey",
      "submittedOnDailyBy": {
        "_id": "646fa3016441111304fec68d",
        "avatarUrl": "/avatars/923629340f3785ae8c6e52cf3674d5c2.svg",
        "isPro": false,
        "fullname": "Chih-Kai Yang",
        "user": "zenyn",
        "type": "user"
      },
      "summary": "With advancements in large audio-language models (LALMs), which enhance large\nlanguage models (LLMs) with auditory capabilities, these models are expected to\ndemonstrate universal proficiency across various auditory tasks. While numerous\nbenchmarks have emerged to assess LALMs' performance, they remain fragmented\nand lack a structured taxonomy. To bridge this gap, we conduct a comprehensive\nsurvey and propose a systematic taxonomy for LALM evaluations, categorizing\nthem into four dimensions based on their objectives: (1) General Auditory\nAwareness and Processing, (2) Knowledge and Reasoning, (3) Dialogue-oriented\nAbility, and (4) Fairness, Safety, and Trustworthiness. We provide detailed\noverviews within each category and highlight challenges in this field, offering\ninsights into promising future directions. To the best of our knowledge, this\nis the first survey specifically focused on the evaluations of LALMs, providing\nclear guidelines for the community. We will release the collection of the\nsurveyed papers and actively maintain it to support ongoing advancements in the\nfield.",
      "upvotes": 1,
      "discussionId": "6830745770e219f5de8ad38b",
      "ai_summary": "A survey proposes a systematic taxonomy for evaluating large audio-language models across dimensions including auditory awareness, knowledge reasoning, dialogue ability, and fairness, to address fragmented benchmarks in the field.",
      "ai_keywords": [
        "large audio-language models",
        "LALMs",
        "large language models",
        "LLMs",
        "auditory capabilities",
        "general auditory awareness",
        "processing",
        "knowledge and reasoning",
        "dialogue-oriented ability",
        "fairness",
        "safety",
        "trustworthiness",
        "taxonomy",
        "evaluations",
        "benchmark",
        "survey",
        "guidelines"
      ]
    },
    "publishedAt": "2025-05-21T15:17:29.000Z",
    "title": "Towards Holistic Evaluation of Large Audio-Language Models: A\n  Comprehensive Survey",
    "summary": "With advancements in large audio-language models (LALMs), which enhance large\nlanguage models (LLMs) with auditory capabilities, these models are expected to\ndemonstrate universal proficiency across various auditory tasks. While numerous\nbenchmarks have emerged to assess LALMs' performance, they remain fragmented\nand lack a structured taxonomy. To bridge this gap, we conduct a comprehensive\nsurvey and propose a systematic taxonomy for LALM evaluations, categorizing\nthem into four dimensions based on their objectives: (1) General Auditory\nAwareness and Processing, (2) Knowledge and Reasoning, (3) Dialogue-oriented\nAbility, and (4) Fairness, Safety, and Trustworthiness. We provide detailed\noverviews within each category and highlight challenges in this field, offering\ninsights into promising future directions. To the best of our knowledge, this\nis the first survey specifically focused on the evaluations of LALMs, providing\nclear guidelines for the community. We will release the collection of the\nsurveyed papers and actively maintain it to support ongoing advancements in the\nfield.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15957.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "646fa3016441111304fec68d",
      "avatarUrl": "/avatars/923629340f3785ae8c6e52cf3674d5c2.svg",
      "fullname": "Chih-Kai Yang",
      "name": "zenyn",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  }
]