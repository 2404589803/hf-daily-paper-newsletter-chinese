[
  {
    "paper": {
      "id": "2509.04419",
      "authors": [
        {
          "_id": "68ba4a27736018af705e8e47",
          "name": "Xingtai Lv",
          "hidden": false
        },
        {
          "_id": "68ba4a27736018af705e8e48",
          "name": "Yuxin Zuo",
          "hidden": false
        },
        {
          "_id": "68ba4a27736018af705e8e49",
          "name": "Youbang Sun",
          "hidden": false
        },
        {
          "_id": "68ba4a27736018af705e8e4a",
          "name": "Hongyi Liu",
          "hidden": false
        },
        {
          "_id": "68ba4a27736018af705e8e4b",
          "name": "Yuntian Wei",
          "hidden": false
        },
        {
          "_id": "68ba4a27736018af705e8e4c",
          "name": "Zhekai Chen",
          "hidden": false
        },
        {
          "_id": "68ba4a27736018af705e8e4d",
          "name": "Lixuan He",
          "hidden": false
        },
        {
          "_id": "68ba4a27736018af705e8e4e",
          "name": "Xuekai Zhu",
          "hidden": false
        },
        {
          "_id": "68ba4a27736018af705e8e4f",
          "name": "Kaiyan Zhang",
          "hidden": false
        },
        {
          "_id": "68ba4a27736018af705e8e50",
          "name": "Bingning Wang",
          "hidden": false
        },
        {
          "_id": "68ba4a27736018af705e8e51",
          "name": "Ning Ding",
          "hidden": false
        },
        {
          "_id": "68ba4a27736018af705e8e52",
          "name": "Bowen Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-04T17:40:33.000Z",
      "submittedOnDailyAt": "2025-09-05T01:00:55.060Z",
      "title": "Towards a Unified View of Large Language Model Post-Training",
      "submittedOnDailyBy": {
        "_id": "663f07d029be04778ba97871",
        "avatarUrl": "/avatars/fb7c9d4a2c537d918a3267e7cbc03f04.svg",
        "isPro": false,
        "fullname": "Xingtai Lv",
        "user": "XingtaiHF",
        "type": "user"
      },
      "summary": "Two major sources of training data exist for post-training modern language\nmodels: online (model-generated rollouts) data, and offline (human or\nother-model demonstrations) data. These two types of data are typically used by\napproaches like Reinforcement Learning (RL) and Supervised Fine-Tuning (SFT),\nrespectively. In this paper, we show that these approaches are not in\ncontradiction, but are instances of a single optimization process. We derive a\nUnified Policy Gradient Estimator, and present the calculations of a wide\nspectrum of post-training approaches as the gradient of a common objective\nunder different data distribution assumptions and various bias-variance\ntradeoffs. The gradient estimator is constructed with four interchangeable\nparts: stabilization mask, reference policy denominator, advantage estimate,\nand likelihood gradient. Motivated by our theoretical findings, we propose\nHybrid Post-Training (HPT), an algorithm that dynamically selects different\ntraining signals. HPT is designed to yield both effective exploitation of\ndemonstration and stable exploration without sacrificing learned reasoning\npatterns. We provide extensive experiments and ablation studies to verify the\neffectiveness of our unified theoretical framework and HPT. Across six\nmathematical reasoning benchmarks and two out-of-distribution suites, HPT\nconsistently surpasses strong baselines across models of varying scales and\nfamilies.",
      "upvotes": 29,
      "discussionId": "68ba4a27736018af705e8e53",
      "githubRepo": "https://github.com/TsinghuaC3I/Unify-Post-Training",
      "ai_summary": "A unified policy gradient estimator and Hybrid Post-Training algorithm effectively combine online and offline data for post-training language models, improving performance across various benchmarks.",
      "ai_keywords": [
        "Reinforcement Learning",
        "Supervised Fine-Tuning",
        "Unified Policy Gradient Estimator",
        "stabilization mask",
        "reference policy denominator",
        "advantage estimate",
        "likelihood gradient",
        "Hybrid Post-Training",
        "mathematical reasoning benchmarks",
        "out-of-distribution suites"
      ],
      "githubStars": 22
    },
    "publishedAt": "2025-09-04T13:40:33.000Z",
    "title": "Towards a Unified View of Large Language Model Post-Training",
    "summary": "Two major sources of training data exist for post-training modern language\nmodels: online (model-generated rollouts) data, and offline (human or\nother-model demonstrations) data. These two types of data are typically used by\napproaches like Reinforcement Learning (RL) and Supervised Fine-Tuning (SFT),\nrespectively. In this paper, we show that these approaches are not in\ncontradiction, but are instances of a single optimization process. We derive a\nUnified Policy Gradient Estimator, and present the calculations of a wide\nspectrum of post-training approaches as the gradient of a common objective\nunder different data distribution assumptions and various bias-variance\ntradeoffs. The gradient estimator is constructed with four interchangeable\nparts: stabilization mask, reference policy denominator, advantage estimate,\nand likelihood gradient. Motivated by our theoretical findings, we propose\nHybrid Post-Training (HPT), an algorithm that dynamically selects different\ntraining signals. HPT is designed to yield both effective exploitation of\ndemonstration and stable exploration without sacrificing learned reasoning\npatterns. We provide extensive experiments and ablation studies to verify the\neffectiveness of our unified theoretical framework and HPT. Across six\nmathematical reasoning benchmarks and two out-of-distribution suites, HPT\nconsistently surpasses strong baselines across models of varying scales and\nfamilies.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.04419.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "663f07d029be04778ba97871",
      "avatarUrl": "/avatars/fb7c9d4a2c537d918a3267e7cbc03f04.svg",
      "fullname": "Xingtai Lv",
      "name": "XingtaiHF",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.04292",
      "authors": [
        {
          "_id": "68ba6843736018af705e8eb4",
          "name": "Qinyan Zhang",
          "hidden": false
        },
        {
          "_id": "68ba6843736018af705e8eb5",
          "name": "Xinping Lei",
          "hidden": false
        },
        {
          "_id": "68ba6843736018af705e8eb6",
          "name": "Ruijie Miao",
          "hidden": false
        },
        {
          "_id": "68ba6843736018af705e8eb7",
          "name": "Yu Fu",
          "hidden": false
        },
        {
          "_id": "68ba6843736018af705e8eb8",
          "name": "Haojie Fan",
          "hidden": false
        },
        {
          "_id": "68ba6843736018af705e8eb9",
          "name": "Le Chang",
          "hidden": false
        },
        {
          "_id": "68ba6843736018af705e8eba",
          "name": "Jiafan Hou",
          "hidden": false
        },
        {
          "_id": "68ba6843736018af705e8ebb",
          "name": "Dingling Zhang",
          "hidden": false
        },
        {
          "_id": "68ba6843736018af705e8ebc",
          "name": "Zhongfei Hou",
          "hidden": false
        },
        {
          "_id": "68ba6843736018af705e8ebd",
          "name": "Ziqiang Yang",
          "hidden": false
        },
        {
          "_id": "68ba6843736018af705e8ebe",
          "name": "Changxin Pu",
          "hidden": false
        },
        {
          "_id": "68ba6843736018af705e8ebf",
          "name": "Fei Hu",
          "hidden": false
        },
        {
          "_id": "68ba6843736018af705e8ec0",
          "name": "Jingkai Liu",
          "hidden": false
        },
        {
          "_id": "68ba6843736018af705e8ec1",
          "name": "Mengyun Liu",
          "hidden": false
        },
        {
          "_id": "68ba6843736018af705e8ec2",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "68ba6843736018af705e8ec3",
          "name": "Xiang Gao",
          "hidden": false
        },
        {
          "_id": "68ba6843736018af705e8ec4",
          "name": "Jiaheng Liu",
          "hidden": false
        },
        {
          "_id": "68ba6843736018af705e8ec5",
          "name": "Tong Yang",
          "hidden": false
        },
        {
          "_id": "68ba6843736018af705e8ec6",
          "name": "Zaiyuan Wang",
          "hidden": false
        },
        {
          "_id": "68ba6843736018af705e8ec7",
          "name": "Ge Zhang",
          "hidden": false
        },
        {
          "_id": "68ba6843736018af705e8ec8",
          "name": "Wenhao Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-04T15:03:02.000Z",
      "submittedOnDailyAt": "2025-09-05T03:04:17.131Z",
      "title": "Inverse IFEval: Can LLMs Unlearn Stubborn Training Conventions to Follow\n  Real Instructions?",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) achieve strong performance on diverse tasks but\noften exhibit cognitive inertia, struggling to follow instructions that\nconflict with the standardized patterns learned during supervised fine-tuning\n(SFT). To evaluate this limitation, we propose Inverse IFEval, a benchmark that\nmeasures models Counter-intuitive Abilitytheir capacity to override\ntraining-induced biases and comply with adversarial instructions. Inverse\nIFEval introduces eight types of such challenges, including Question\nCorrection, Intentional Textual Flaws, Code without Comments, and\nCounterfactual Answering. Using a human-in-the-loop pipeline, we construct a\ndataset of 1012 high-quality Chinese and English questions across 23 domains,\nevaluated under an optimized LLM-as-a-Judge framework. Experiments on existing\nleading LLMs demonstrate the necessity of our proposed Inverse IFEval\nbenchmark. Our findings emphasize that future alignment efforts should not only\npursue fluency and factual correctness but also account for adaptability under\nunconventional contexts. We hope that Inverse IFEval serves as both a\ndiagnostic tool and a foundation for developing methods that mitigate cognitive\ninertia, reduce overfitting to narrow patterns, and ultimately enhance the\ninstruction-following reliability of LLMs in diverse and unpredictable\nreal-world scenarios.",
      "upvotes": 29,
      "discussionId": "68ba6844736018af705e8ec9",
      "projectPage": "https://huggingface.co/datasets/m-a-p/Inverse_IFEval",
      "ai_summary": "Inverse IFEval evaluates Large Language Models' ability to override training biases and follow unconventional instructions, highlighting the need for adaptability in diverse contexts.",
      "ai_keywords": [
        "Large Language Models",
        "cognitive inertia",
        "supervised fine-tuning",
        "Inverse IFEval",
        "Counter-intuitive Ability",
        "Question Correction",
        "Intentional Textual Flaws",
        "Code without Comments",
        "Counterfactual Answering",
        "LLM-as-a-Judge"
      ]
    },
    "publishedAt": "2025-09-04T11:03:02.000Z",
    "title": "Inverse IFEval: Can LLMs Unlearn Stubborn Training Conventions to Follow\n  Real Instructions?",
    "summary": "Large Language Models (LLMs) achieve strong performance on diverse tasks but\noften exhibit cognitive inertia, struggling to follow instructions that\nconflict with the standardized patterns learned during supervised fine-tuning\n(SFT). To evaluate this limitation, we propose Inverse IFEval, a benchmark that\nmeasures models Counter-intuitive Abilitytheir capacity to override\ntraining-induced biases and comply with adversarial instructions. Inverse\nIFEval introduces eight types of such challenges, including Question\nCorrection, Intentional Textual Flaws, Code without Comments, and\nCounterfactual Answering. Using a human-in-the-loop pipeline, we construct a\ndataset of 1012 high-quality Chinese and English questions across 23 domains,\nevaluated under an optimized LLM-as-a-Judge framework. Experiments on existing\nleading LLMs demonstrate the necessity of our proposed Inverse IFEval\nbenchmark. Our findings emphasize that future alignment efforts should not only\npursue fluency and factual correctness but also account for adaptability under\nunconventional contexts. We hope that Inverse IFEval serves as both a\ndiagnostic tool and a foundation for developing methods that mitigate cognitive\ninertia, reduce overfitting to narrow patterns, and ultimately enhance the\ninstruction-following reliability of LLMs in diverse and unpredictable\nreal-world scenarios.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.04292.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 98
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.04338",
      "authors": [
        {
          "_id": "68ba64a6736018af705e8ea9",
          "name": "JiYuan Wang",
          "hidden": false
        },
        {
          "_id": "68ba64a6736018af705e8eaa",
          "name": "Chunyu Lin",
          "hidden": false
        },
        {
          "_id": "68ba64a6736018af705e8eab",
          "name": "Lei Sun",
          "hidden": false
        },
        {
          "_id": "68ba64a6736018af705e8eac",
          "name": "Rongying Liu",
          "hidden": false
        },
        {
          "_id": "68ba64a6736018af705e8ead",
          "name": "Lang Nie",
          "hidden": false
        },
        {
          "_id": "68ba64a6736018af705e8eae",
          "name": "Mingxing Li",
          "hidden": false
        },
        {
          "_id": "68ba64a6736018af705e8eaf",
          "name": "Kang Liao",
          "hidden": false
        },
        {
          "_id": "68ba64a6736018af705e8eb0",
          "name": "Xiangxiang Chu",
          "hidden": false
        },
        {
          "_id": "68ba64a6736018af705e8eb1",
          "name": "Yao Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-04T15:58:50.000Z",
      "submittedOnDailyAt": "2025-09-05T02:49:35.984Z",
      "title": "From Editor to Dense Geometry Estimator",
      "submittedOnDailyBy": {
        "_id": "66d255e3947594430c723ff6",
        "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg",
        "isPro": false,
        "fullname": "xiaochonglinghu",
        "user": "xiaochonglinghu",
        "type": "user"
      },
      "summary": "Leveraging visual priors from pre-trained text-to-image (T2I) generative\nmodels has shown success in dense prediction. However, dense prediction is\ninherently an image-to-image task, suggesting that image editing models, rather\nthan T2I generative models, may be a more suitable foundation for fine-tuning.\n  Motivated by this, we conduct a systematic analysis of the fine-tuning\nbehaviors of both editors and generators for dense geometry estimation. Our\nfindings show that editing models possess inherent structural priors, which\nenable them to converge more stably by ``refining\" their innate features, and\nultimately achieve higher performance than their generative counterparts.\n  Based on these findings, we introduce FE2E, a framework that\npioneeringly adapts an advanced editing model based on Diffusion Transformer\n(DiT) architecture for dense geometry prediction. Specifically, to tailor the\neditor for this deterministic task, we reformulate the editor's original flow\nmatching loss into the ``consistent velocity\" training objective. And we use\nlogarithmic quantization to resolve the precision conflict between the editor's\nnative BFloat16 format and the high precision demand of our tasks.\nAdditionally, we leverage the DiT's global attention for a cost-free joint\nestimation of depth and normals in a single forward pass, enabling their\nsupervisory signals to mutually enhance each other.\n  Without scaling up the training data, FE2E achieves impressive performance\nimprovements in zero-shot monocular depth and normal estimation across multiple\ndatasets. Notably, it achieves over 35\\% performance gains on the ETH3D dataset\nand outperforms the DepthAnything series, which is trained on 100times data.\nThe project page can be accessed https://amap-ml.github.io/FE2E/{here}.",
      "upvotes": 28,
      "discussionId": "68ba64a7736018af705e8eb2",
      "ai_summary": "FE2E, a framework using a Diffusion Transformer for dense geometry prediction, outperforms generative models in zero-shot monocular depth and normal estimation with improved performance and efficiency.",
      "ai_keywords": [
        "text-to-image",
        "dense prediction",
        "image editing models",
        "generative models",
        "fine-tuning",
        "dense geometry estimation",
        "structural priors",
        "flow matching loss",
        "consistent velocity",
        "logarithmic quantization",
        "BFloat16",
        "global attention",
        "ETH3D",
        "DepthAnything"
      ]
    },
    "publishedAt": "2025-09-04T11:58:50.000Z",
    "title": "From Editor to Dense Geometry Estimator",
    "summary": "Leveraging visual priors from pre-trained text-to-image (T2I) generative\nmodels has shown success in dense prediction. However, dense prediction is\ninherently an image-to-image task, suggesting that image editing models, rather\nthan T2I generative models, may be a more suitable foundation for fine-tuning.\n  Motivated by this, we conduct a systematic analysis of the fine-tuning\nbehaviors of both editors and generators for dense geometry estimation. Our\nfindings show that editing models possess inherent structural priors, which\nenable them to converge more stably by ``refining\" their innate features, and\nultimately achieve higher performance than their generative counterparts.\n  Based on these findings, we introduce FE2E, a framework that\npioneeringly adapts an advanced editing model based on Diffusion Transformer\n(DiT) architecture for dense geometry prediction. Specifically, to tailor the\neditor for this deterministic task, we reformulate the editor's original flow\nmatching loss into the ``consistent velocity\" training objective. And we use\nlogarithmic quantization to resolve the precision conflict between the editor's\nnative BFloat16 format and the high precision demand of our tasks.\nAdditionally, we leverage the DiT's global attention for a cost-free joint\nestimation of depth and normals in a single forward pass, enabling their\nsupervisory signals to mutually enhance each other.\n  Without scaling up the training data, FE2E achieves impressive performance\nimprovements in zero-shot monocular depth and normal estimation across multiple\ndatasets. Notably, it achieves over 35\\% performance gains on the ETH3D dataset\nand outperforms the DepthAnything series, which is trained on 100times data.\nThe project page can be accessed https://amap-ml.github.io/FE2E/{here}.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.04338.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66d255e3947594430c723ff6",
      "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg",
      "fullname": "xiaochonglinghu",
      "name": "xiaochonglinghu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.01396",
      "authors": [
        {
          "_id": "68b98e69736018af705e8d41",
          "user": {
            "_id": "65a8bcb717d869bb7487c2a1",
            "avatarUrl": "/avatars/261c28f7e616a8482970f50c1f8919fd.svg",
            "isPro": false,
            "fullname": "Haiyuan Wan",
            "user": "haiyuanwan",
            "type": "user"
          },
          "name": "Haiyuan Wan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-04T19:36:56.793Z",
          "hidden": false
        },
        {
          "_id": "68b98e69736018af705e8d42",
          "name": "Chen Yang",
          "hidden": false
        },
        {
          "_id": "68b98e69736018af705e8d43",
          "name": "Junchi Yu",
          "hidden": false
        },
        {
          "_id": "68b98e69736018af705e8d44",
          "name": "Meiqi Tu",
          "hidden": false
        },
        {
          "_id": "68b98e69736018af705e8d45",
          "name": "Jiaxuan Lu",
          "hidden": false
        },
        {
          "_id": "68b98e69736018af705e8d46",
          "name": "Di Yu",
          "hidden": false
        },
        {
          "_id": "68b98e69736018af705e8d47",
          "name": "Jianbao Cao",
          "hidden": false
        },
        {
          "_id": "68b98e69736018af705e8d48",
          "name": "Ben Gao",
          "hidden": false
        },
        {
          "_id": "68b98e69736018af705e8d49",
          "name": "Jiaqing Xie",
          "hidden": false
        },
        {
          "_id": "68b98e69736018af705e8d4a",
          "name": "Aoran Wang",
          "hidden": false
        },
        {
          "_id": "68b98e69736018af705e8d4b",
          "name": "Wenlong Zhang",
          "hidden": false
        },
        {
          "_id": "68b98e69736018af705e8d4c",
          "name": "Philip Torr",
          "hidden": false
        },
        {
          "_id": "68b98e69736018af705e8d4d",
          "name": "Dongzhan Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-01T11:42:47.000Z",
      "submittedOnDailyAt": "2025-09-05T00:41:12.849Z",
      "title": "DeepResearch Arena: The First Exam of LLMs' Research Abilities via\n  Seminar-Grounded Tasks",
      "submittedOnDailyBy": {
        "_id": "65a8bcb717d869bb7487c2a1",
        "avatarUrl": "/avatars/261c28f7e616a8482970f50c1f8919fd.svg",
        "isPro": false,
        "fullname": "Haiyuan Wan",
        "user": "haiyuanwan",
        "type": "user"
      },
      "summary": "Deep research agents have attracted growing attention for their potential to\norchestrate multi-stage research workflows, spanning literature synthesis,\nmethodological design, and empirical verification. Despite these strides,\nevaluating their research capability faithfully is rather challenging due to\nthe difficulty of collecting frontier research questions that genuinely capture\nresearchers' attention and intellectual curiosity. To address this gap, we\nintroduce DeepResearch Arena, a benchmark grounded in academic seminars that\ncapture rich expert discourse and interaction, better reflecting real-world\nresearch environments and reducing the risk of data leakage. To automatically\nconstruct DeepResearch Arena, we propose a Multi-Agent Hierarchical Task\nGeneration (MAHTG) system that extracts research-worthy inspirations from\nseminar transcripts. The MAHTG system further translates research-worthy\ninspirations into high-quality research tasks, ensuring the traceability of\nresearch task formulation while filtering noise. With the MAHTG system, we\ncurate DeepResearch Arena with over 10,000 high-quality research tasks from\nover 200 academic seminars, spanning 12 disciplines, such as literature,\nhistory, and science. Our extensive evaluation shows that DeepResearch Arena\npresents substantial challenges for current state-of-the-art agents, with clear\nperformance gaps observed across different models.",
      "upvotes": 25,
      "discussionId": "68b98e6a736018af705e8d4e",
      "ai_summary": "DeepResearch Arena, a benchmark using academic seminar transcripts, provides high-quality research tasks to evaluate deep research agents across multiple disciplines.",
      "ai_keywords": [
        "DeepResearch Arena",
        "Multi-Agent Hierarchical Task Generation",
        "MAHTG",
        "research tasks",
        "academic seminars",
        "research agents"
      ]
    },
    "publishedAt": "2025-09-01T07:42:47.000Z",
    "title": "DeepResearch Arena: The First Exam of LLMs' Research Abilities via\n  Seminar-Grounded Tasks",
    "summary": "Deep research agents have attracted growing attention for their potential to\norchestrate multi-stage research workflows, spanning literature synthesis,\nmethodological design, and empirical verification. Despite these strides,\nevaluating their research capability faithfully is rather challenging due to\nthe difficulty of collecting frontier research questions that genuinely capture\nresearchers' attention and intellectual curiosity. To address this gap, we\nintroduce DeepResearch Arena, a benchmark grounded in academic seminars that\ncapture rich expert discourse and interaction, better reflecting real-world\nresearch environments and reducing the risk of data leakage. To automatically\nconstruct DeepResearch Arena, we propose a Multi-Agent Hierarchical Task\nGeneration (MAHTG) system that extracts research-worthy inspirations from\nseminar transcripts. The MAHTG system further translates research-worthy\ninspirations into high-quality research tasks, ensuring the traceability of\nresearch task formulation while filtering noise. With the MAHTG system, we\ncurate DeepResearch Arena with over 10,000 high-quality research tasks from\nover 200 academic seminars, spanning 12 disciplines, such as literature,\nhistory, and science. Our extensive evaluation shows that DeepResearch Arena\npresents substantial challenges for current state-of-the-art agents, with clear\nperformance gaps observed across different models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.01396.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65a8bcb717d869bb7487c2a1",
      "avatarUrl": "/avatars/261c28f7e616a8482970f50c1f8919fd.svg",
      "fullname": "Haiyuan Wan",
      "name": "haiyuanwan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2509.04394",
      "authors": [
        {
          "_id": "68ba5465736018af705e8e71",
          "name": "Zidong Wang",
          "hidden": false
        },
        {
          "_id": "68ba5465736018af705e8e72",
          "name": "Yiyuan Zhang",
          "hidden": false
        },
        {
          "_id": "68ba5465736018af705e8e73",
          "name": "Xiaoyu Yue",
          "hidden": false
        },
        {
          "_id": "68ba5465736018af705e8e74",
          "name": "Xiangyu Yue",
          "hidden": false
        },
        {
          "_id": "68ba5465736018af705e8e75",
          "name": "Yangguang Li",
          "hidden": false
        },
        {
          "_id": "68ba5465736018af705e8e76",
          "name": "Wanli Ouyang",
          "hidden": false
        },
        {
          "_id": "68ba5465736018af705e8e77",
          "name": "Lei Bai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-04T17:05:59.000Z",
      "submittedOnDailyAt": "2025-09-05T01:49:34.223Z",
      "title": "Transition Models: Rethinking the Generative Learning Objective",
      "submittedOnDailyBy": {
        "_id": "63176933b58b0184630d2c74",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63176933b58b0184630d2c74/53b5EASwW76zeyyqeJA3O.jpeg",
        "isPro": false,
        "fullname": "Yiyuan Zhang",
        "user": "Yiyuan",
        "type": "user"
      },
      "summary": "A fundamental dilemma in generative modeling persists: iterative diffusion\nmodels achieve outstanding fidelity, but at a significant computational cost,\nwhile efficient few-step alternatives are constrained by a hard quality\nceiling. This conflict between generation steps and output quality arises from\nrestrictive training objectives that focus exclusively on either infinitesimal\ndynamics (PF-ODEs) or direct endpoint prediction. We address this challenge by\nintroducing an exact, continuous-time dynamics equation that analytically\ndefines state transitions across any finite time interval. This leads to a\nnovel generative paradigm, Transition Models (TiM), which adapt to\narbitrary-step transitions, seamlessly traversing the generative trajectory\nfrom single leaps to fine-grained refinement with more steps. Despite having\nonly 865M parameters, TiM achieves state-of-the-art performance, surpassing\nleading models such as SD3.5 (8B parameters) and FLUX.1 (12B parameters) across\nall evaluated step counts. Importantly, unlike previous few-step generators,\nTiM demonstrates monotonic quality improvement as the sampling budget\nincreases. Additionally, when employing our native-resolution strategy, TiM\ndelivers exceptional fidelity at resolutions up to 4096x4096.",
      "upvotes": 8,
      "discussionId": "68ba5465736018af705e8e78",
      "githubRepo": "https://github.com/WZDTHU/TiM",
      "ai_summary": "A novel generative paradigm, Transition Models (TiM), addresses the trade-off between computational cost and output quality in generative modeling by using a continuous-time dynamics equation.",
      "ai_keywords": [
        "iterative diffusion models",
        "continuous-time dynamics equation",
        "Transition Models (TiM)",
        "PF-ODEs",
        "direct endpoint prediction",
        "state-of-the-art performance",
        "SD3.5",
        "FLUX.1",
        "monotonic quality improvement",
        "native-resolution strategy"
      ],
      "githubStars": 13
    },
    "publishedAt": "2025-09-04T13:05:59.000Z",
    "title": "Transition Models: Rethinking the Generative Learning Objective",
    "summary": "A fundamental dilemma in generative modeling persists: iterative diffusion\nmodels achieve outstanding fidelity, but at a significant computational cost,\nwhile efficient few-step alternatives are constrained by a hard quality\nceiling. This conflict between generation steps and output quality arises from\nrestrictive training objectives that focus exclusively on either infinitesimal\ndynamics (PF-ODEs) or direct endpoint prediction. We address this challenge by\nintroducing an exact, continuous-time dynamics equation that analytically\ndefines state transitions across any finite time interval. This leads to a\nnovel generative paradigm, Transition Models (TiM), which adapt to\narbitrary-step transitions, seamlessly traversing the generative trajectory\nfrom single leaps to fine-grained refinement with more steps. Despite having\nonly 865M parameters, TiM achieves state-of-the-art performance, surpassing\nleading models such as SD3.5 (8B parameters) and FLUX.1 (12B parameters) across\nall evaluated step counts. Importantly, unlike previous few-step generators,\nTiM demonstrates monotonic quality improvement as the sampling budget\nincreases. Additionally, when employing our native-resolution strategy, TiM\ndelivers exceptional fidelity at resolutions up to 4096x4096.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.04394.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63176933b58b0184630d2c74",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63176933b58b0184630d2c74/53b5EASwW76zeyyqeJA3O.jpeg",
      "fullname": "Yiyuan Zhang",
      "name": "Yiyuan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.04011",
      "authors": [
        {
          "_id": "68ba4f71736018af705e8e5c",
          "name": "Or Shachar",
          "hidden": false
        },
        {
          "_id": "68ba4f71736018af705e8e5d",
          "name": "Uri Katz",
          "hidden": false
        },
        {
          "_id": "68ba4f71736018af705e8e5e",
          "name": "Yoav Goldberg",
          "hidden": false
        },
        {
          "_id": "68ba4f71736018af705e8e5f",
          "name": "Oren Glickman",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-04T08:42:23.000Z",
      "submittedOnDailyAt": "2025-09-05T03:44:27.859Z",
      "title": "NER Retriever: Zero-Shot Named Entity Retrieval with Type-Aware\n  Embeddings",
      "submittedOnDailyBy": {
        "_id": "604e1c120fe8ff3ec13d71e8",
        "avatarUrl": "/avatars/22f6463216904fb0ec8306e704432ab7.svg",
        "isPro": false,
        "fullname": "Uri Katz",
        "user": "Uri-ka",
        "type": "user"
      },
      "summary": "We present NER Retriever, a zero-shot retrieval framework for ad-hoc Named\nEntity Retrieval, a variant of Named Entity Recognition (NER), where the types\nof interest are not provided in advance, and a user-defined type description is\nused to retrieve documents mentioning entities of that type. Instead of relying\non fixed schemas or fine-tuned models, our method builds on internal\nrepresentations of large language models (LLMs) to embed both entity mentions\nand user-provided open-ended type descriptions into a shared semantic space. We\nshow that internal representations, specifically the value vectors from\nmid-layer transformer blocks, encode fine-grained type information more\neffectively than commonly used top-layer embeddings. To refine these\nrepresentations, we train a lightweight contrastive projection network that\naligns type-compatible entities while separating unrelated types. The resulting\nentity embeddings are compact, type-aware, and well-suited for nearest-neighbor\nsearch. Evaluated on three benchmarks, NER Retriever significantly outperforms\nboth lexical and dense sentence-level retrieval baselines. Our findings provide\nempirical support for representation selection within LLMs and demonstrate a\npractical solution for scalable, schema-free entity retrieval. The NER\nRetriever Codebase is publicly available at\nhttps://github.com/ShacharOr100/ner_retriever",
      "upvotes": 7,
      "discussionId": "68ba4f71736018af705e8e60",
      "githubRepo": "https://github.com/ShacharOr100/ner_retriever",
      "ai_summary": "NER Retriever uses internal representations from large language models to perform zero-shot named entity retrieval by embedding entity mentions and type descriptions into a shared semantic space, outperforming lexical and dense sentence-level retrieval methods.",
      "ai_keywords": [
        "NER Retriever",
        "zero-shot retrieval",
        "Named Entity Retrieval",
        "Named Entity Recognition",
        "large language models",
        "internal representations",
        "value vectors",
        "mid-layer transformer blocks",
        "contrastive projection network",
        "nearest-neighbor search"
      ],
      "githubStars": 0
    },
    "publishedAt": "2025-09-04T04:42:23.000Z",
    "title": "NER Retriever: Zero-Shot Named Entity Retrieval with Type-Aware\n  Embeddings",
    "summary": "We present NER Retriever, a zero-shot retrieval framework for ad-hoc Named\nEntity Retrieval, a variant of Named Entity Recognition (NER), where the types\nof interest are not provided in advance, and a user-defined type description is\nused to retrieve documents mentioning entities of that type. Instead of relying\non fixed schemas or fine-tuned models, our method builds on internal\nrepresentations of large language models (LLMs) to embed both entity mentions\nand user-provided open-ended type descriptions into a shared semantic space. We\nshow that internal representations, specifically the value vectors from\nmid-layer transformer blocks, encode fine-grained type information more\neffectively than commonly used top-layer embeddings. To refine these\nrepresentations, we train a lightweight contrastive projection network that\naligns type-compatible entities while separating unrelated types. The resulting\nentity embeddings are compact, type-aware, and well-suited for nearest-neighbor\nsearch. Evaluated on three benchmarks, NER Retriever significantly outperforms\nboth lexical and dense sentence-level retrieval baselines. Our findings provide\nempirical support for representation selection within LLMs and demonstrate a\npractical solution for scalable, schema-free entity retrieval. The NER\nRetriever Codebase is publicly available at\nhttps://github.com/ShacharOr100/ner_retriever",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.04011.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "604e1c120fe8ff3ec13d71e8",
      "avatarUrl": "/avatars/22f6463216904fb0ec8306e704432ab7.svg",
      "fullname": "Uri Katz",
      "name": "Uri-ka",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.04406",
      "authors": [
        {
          "_id": "68ba68de736018af705e8ecb",
          "name": "Zanwei Zhou",
          "hidden": false
        },
        {
          "_id": "68ba68de736018af705e8ecc",
          "name": "Taoran Yi",
          "hidden": false
        },
        {
          "_id": "68ba68de736018af705e8ecd",
          "name": "Jiemin Fang",
          "hidden": false
        },
        {
          "_id": "68ba68de736018af705e8ece",
          "name": "Chen Yang",
          "hidden": false
        },
        {
          "_id": "68ba68de736018af705e8ecf",
          "name": "Lingxi Xie",
          "hidden": false
        },
        {
          "_id": "68ba68de736018af705e8ed0",
          "name": "Xinggang Wang",
          "hidden": false
        },
        {
          "_id": "68ba68de736018af705e8ed1",
          "name": "Wei Shen",
          "hidden": false
        },
        {
          "_id": "68ba68de736018af705e8ed2",
          "name": "Qi Tian",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-04T17:24:31.000Z",
      "submittedOnDailyAt": "2025-09-05T03:09:11.246Z",
      "title": "Few-step Flow for 3D Generation via Marginal-Data Transport Distillation",
      "submittedOnDailyBy": {
        "_id": "64e5ee4b93d04e3439f4e988",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e5ee4b93d04e3439f4e988/g-THJvunoSPUyQ2v_fChU.jpeg",
        "isPro": false,
        "fullname": "taoranyi",
        "user": "thewhole",
        "type": "user"
      },
      "summary": "Flow-based 3D generation models typically require dozens of sampling steps\nduring inference. Though few-step distillation methods, particularly\nConsistency Models (CMs), have achieved substantial advancements in\naccelerating 2D diffusion models, they remain under-explored for more complex\n3D generation tasks. In this study, we propose a novel framework, MDT-dist, for\nfew-step 3D flow distillation. Our approach is built upon a primary objective:\ndistilling the pretrained model to learn the Marginal-Data Transport. Directly\nlearning this objective needs to integrate the velocity fields, while this\nintegral is intractable to be implemented. Therefore, we propose two\noptimizable objectives, Velocity Matching (VM) and Velocity Distillation (VD),\nto equivalently convert the optimization target from the transport level to the\nvelocity and the distribution level respectively. Velocity Matching (VM) learns\nto stably match the velocity fields between the student and the teacher, but\ninevitably provides biased gradient estimates. Velocity Distillation (VD)\nfurther enhances the optimization process by leveraging the learned velocity\nfields to perform probability density distillation. When evaluated on the\npioneer 3D generation framework TRELLIS, our method reduces sampling steps of\neach flow transformer from 25 to 1 or 2, achieving 0.68s (1 step x 2) and 0.94s\n(2 steps x 2) latency with 9.0x and 6.5x speedup on A800, while preserving high\nvisual and geometric fidelity. Extensive experiments demonstrate that our\nmethod significantly outperforms existing CM distillation methods, and enables\nTRELLIS to achieve superior performance in few-step 3D generation.",
      "upvotes": 4,
      "discussionId": "68ba68de736018af705e8ed3",
      "ai_summary": "A novel framework, MDT-dist, accelerates 3D flow generation by distilling pretrained models to learn Marginal-Data Transport through Velocity Matching and Velocity Distillation, reducing sampling steps and improving speed and fidelity.",
      "ai_keywords": [
        "flow-based 3D generation models",
        "few-step distillation",
        "Consistency Models",
        "MDT-dist",
        "Marginal-Data Transport",
        "Velocity Matching",
        "Velocity Distillation",
        "TRELLIS",
        "sampling steps",
        "flow transformer",
        "latency",
        "speedup",
        "visual fidelity",
        "geometric fidelity"
      ]
    },
    "publishedAt": "2025-09-04T13:24:31.000Z",
    "title": "Few-step Flow for 3D Generation via Marginal-Data Transport Distillation",
    "summary": "Flow-based 3D generation models typically require dozens of sampling steps\nduring inference. Though few-step distillation methods, particularly\nConsistency Models (CMs), have achieved substantial advancements in\naccelerating 2D diffusion models, they remain under-explored for more complex\n3D generation tasks. In this study, we propose a novel framework, MDT-dist, for\nfew-step 3D flow distillation. Our approach is built upon a primary objective:\ndistilling the pretrained model to learn the Marginal-Data Transport. Directly\nlearning this objective needs to integrate the velocity fields, while this\nintegral is intractable to be implemented. Therefore, we propose two\noptimizable objectives, Velocity Matching (VM) and Velocity Distillation (VD),\nto equivalently convert the optimization target from the transport level to the\nvelocity and the distribution level respectively. Velocity Matching (VM) learns\nto stably match the velocity fields between the student and the teacher, but\ninevitably provides biased gradient estimates. Velocity Distillation (VD)\nfurther enhances the optimization process by leveraging the learned velocity\nfields to perform probability density distillation. When evaluated on the\npioneer 3D generation framework TRELLIS, our method reduces sampling steps of\neach flow transformer from 25 to 1 or 2, achieving 0.68s (1 step x 2) and 0.94s\n(2 steps x 2) latency with 9.0x and 6.5x speedup on A800, while preserving high\nvisual and geometric fidelity. Extensive experiments demonstrate that our\nmethod significantly outperforms existing CM distillation methods, and enables\nTRELLIS to achieve superior performance in few-step 3D generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.04406.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64e5ee4b93d04e3439f4e988",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e5ee4b93d04e3439f4e988/g-THJvunoSPUyQ2v_fChU.jpeg",
      "fullname": "taoranyi",
      "name": "thewhole",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.04434",
      "authors": [
        {
          "_id": "68ba5959736018af705e8e93",
          "name": "Hyunsoo Cha",
          "hidden": false
        },
        {
          "_id": "68ba5959736018af705e8e94",
          "name": "Byungjun Kim",
          "hidden": false
        },
        {
          "_id": "68ba5959736018af705e8e95",
          "name": "Hanbyul Joo",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62414e2b585605a4079c2f38/iNxn7368E7JViGz3UhDOo.mp4"
      ],
      "publishedAt": "2025-09-04T17:53:03.000Z",
      "submittedOnDailyAt": "2025-09-05T02:02:16.975Z",
      "title": "Durian: Dual Reference-guided Portrait Animation with Attribute Transfer",
      "submittedOnDailyBy": {
        "_id": "62414e2b585605a4079c2f38",
        "avatarUrl": "/avatars/db1dc5dd2164b7ecbd789104329296bd.svg",
        "isPro": false,
        "fullname": "Hyunsoo Cha",
        "user": "HyunsooCha",
        "type": "user"
      },
      "summary": "We present Durian, the first method for generating portrait animation videos\nwith facial attribute transfer from a given reference image to a target\nportrait in a zero-shot manner. To enable high-fidelity and spatially\nconsistent attribute transfer across frames, we introduce dual reference\nnetworks that inject spatial features from both the portrait and attribute\nimages into the denoising process of a diffusion model. We train the model\nusing a self-reconstruction formulation, where two frames are sampled from the\nsame portrait video: one is treated as the attribute reference and the other as\nthe target portrait, and the remaining frames are reconstructed conditioned on\nthese inputs and their corresponding masks. To support the transfer of\nattributes with varying spatial extent, we propose a mask expansion strategy\nusing keypoint-conditioned image generation for training. In addition, we\nfurther augment the attribute and portrait images with spatial and\nappearance-level transformations to improve robustness to positional\nmisalignment between them. These strategies allow the model to effectively\ngeneralize across diverse attributes and in-the-wild reference combinations,\ndespite being trained without explicit triplet supervision. Durian achieves\nstate-of-the-art performance on portrait animation with attribute transfer, and\nnotably, its dual reference design enables multi-attribute composition in a\nsingle generation pass without additional training.",
      "upvotes": 2,
      "discussionId": "68ba595a736018af705e8e96",
      "projectPage": "https://hyunsoocha.github.io/durian/",
      "githubRepo": "https://github.com/snuvclab/durian",
      "ai_summary": "Durian uses dual reference networks and a diffusion model to generate high-fidelity portrait animations with attribute transfer from a reference image to a target portrait in a zero-shot manner.",
      "ai_keywords": [
        "dual reference networks",
        "denoising process",
        "diffusion model",
        "self-reconstruction formulation",
        "mask expansion strategy",
        "keypoint-conditioned image generation",
        "spatial and appearance-level transformations",
        "multi-attribute composition"
      ],
      "githubStars": 5
    },
    "publishedAt": "2025-09-04T13:53:03.000Z",
    "title": "Durian: Dual Reference-guided Portrait Animation with Attribute Transfer",
    "summary": "We present Durian, the first method for generating portrait animation videos\nwith facial attribute transfer from a given reference image to a target\nportrait in a zero-shot manner. To enable high-fidelity and spatially\nconsistent attribute transfer across frames, we introduce dual reference\nnetworks that inject spatial features from both the portrait and attribute\nimages into the denoising process of a diffusion model. We train the model\nusing a self-reconstruction formulation, where two frames are sampled from the\nsame portrait video: one is treated as the attribute reference and the other as\nthe target portrait, and the remaining frames are reconstructed conditioned on\nthese inputs and their corresponding masks. To support the transfer of\nattributes with varying spatial extent, we propose a mask expansion strategy\nusing keypoint-conditioned image generation for training. In addition, we\nfurther augment the attribute and portrait images with spatial and\nappearance-level transformations to improve robustness to positional\nmisalignment between them. These strategies allow the model to effectively\ngeneralize across diverse attributes and in-the-wild reference combinations,\ndespite being trained without explicit triplet supervision. Durian achieves\nstate-of-the-art performance on portrait animation with attribute transfer, and\nnotably, its dual reference design enables multi-attribute composition in a\nsingle generation pass without additional training.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62414e2b585605a4079c2f38/iNxn7368E7JViGz3UhDOo.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.04434.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62414e2b585605a4079c2f38",
      "avatarUrl": "/avatars/db1dc5dd2164b7ecbd789104329296bd.svg",
      "fullname": "Hyunsoo Cha",
      "name": "HyunsooCha",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.18733",
      "authors": [
        {
          "_id": "68b993e5736018af705e8d50",
          "name": "Feiwei Qin",
          "hidden": false
        },
        {
          "_id": "68b993e5736018af705e8d51",
          "name": "Shichao Lu",
          "hidden": false
        },
        {
          "_id": "68b993e5736018af705e8d52",
          "user": {
            "_id": "64258e73120a3ed32333319e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64258e73120a3ed32333319e/f1_eZ6slEoVq8Kn4WbQDC.png",
            "isPro": false,
            "fullname": "Junhao Hou",
            "user": "1nnoh",
            "type": "user"
          },
          "name": "Junhao Hou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-09-04T19:34:40.271Z",
          "hidden": false
        },
        {
          "_id": "68b993e5736018af705e8d53",
          "name": "Changmiao Wang",
          "hidden": false
        },
        {
          "_id": "68b993e5736018af705e8d54",
          "name": "Meie Fang",
          "hidden": false
        },
        {
          "_id": "68b993e5736018af705e8d55",
          "name": "Ligang Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-26T07:01:58.000Z",
      "submittedOnDailyAt": "2025-09-05T00:39:47.038Z",
      "title": "Drawing2CAD: Sequence-to-Sequence Learning for CAD Generation from\n  Vector Drawings",
      "submittedOnDailyBy": {
        "_id": "64258e73120a3ed32333319e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64258e73120a3ed32333319e/f1_eZ6slEoVq8Kn4WbQDC.png",
        "isPro": false,
        "fullname": "Junhao Hou",
        "user": "1nnoh",
        "type": "user"
      },
      "summary": "Computer-Aided Design (CAD) generative modeling is driving significant\ninnovations across industrial applications. Recent works have shown remarkable\nprogress in creating solid models from various inputs such as point clouds,\nmeshes, and text descriptions. However, these methods fundamentally diverge\nfrom traditional industrial workflows that begin with 2D engineering drawings.\nThe automatic generation of parametric CAD models from these 2D vector drawings\nremains underexplored despite being a critical step in engineering design. To\naddress this gap, our key insight is to reframe CAD generation as a\nsequence-to-sequence learning problem where vector drawing primitives directly\ninform the generation of parametric CAD operations, preserving geometric\nprecision and design intent throughout the transformation process. We propose\nDrawing2CAD, a framework with three key technical components: a\nnetwork-friendly vector primitive representation that preserves precise\ngeometric information, a dual-decoder transformer architecture that decouples\ncommand type and parameter generation while maintaining precise correspondence,\nand a soft target distribution loss function accommodating inherent flexibility\nin CAD parameters. To train and evaluate Drawing2CAD, we create CAD-VGDrawing,\na dataset of paired engineering drawings and parametric CAD models, and conduct\nthorough experiments to demonstrate the effectiveness of our method. Code and\ndataset are available at https://github.com/lllssc/Drawing2CAD.",
      "upvotes": 2,
      "discussionId": "68b993e5736018af705e8d56",
      "githubRepo": "https://github.com/lllssc/Drawing2CAD",
      "ai_summary": "Drawing2CAD is a framework that converts 2D vector drawings into parametric CAD models using a sequence-to-sequence learning approach with a dual-decoder transformer architecture and a soft target distribution loss function.",
      "ai_keywords": [
        "sequence-to-sequence learning",
        "vector primitive representation",
        "dual-decoder transformer architecture",
        "soft target distribution loss function",
        "parametric CAD models"
      ],
      "githubStars": 48
    },
    "publishedAt": "2025-08-26T03:01:58.000Z",
    "title": "Drawing2CAD: Sequence-to-Sequence Learning for CAD Generation from\n  Vector Drawings",
    "summary": "Computer-Aided Design (CAD) generative modeling is driving significant\ninnovations across industrial applications. Recent works have shown remarkable\nprogress in creating solid models from various inputs such as point clouds,\nmeshes, and text descriptions. However, these methods fundamentally diverge\nfrom traditional industrial workflows that begin with 2D engineering drawings.\nThe automatic generation of parametric CAD models from these 2D vector drawings\nremains underexplored despite being a critical step in engineering design. To\naddress this gap, our key insight is to reframe CAD generation as a\nsequence-to-sequence learning problem where vector drawing primitives directly\ninform the generation of parametric CAD operations, preserving geometric\nprecision and design intent throughout the transformation process. We propose\nDrawing2CAD, a framework with three key technical components: a\nnetwork-friendly vector primitive representation that preserves precise\ngeometric information, a dual-decoder transformer architecture that decouples\ncommand type and parameter generation while maintaining precise correspondence,\nand a soft target distribution loss function accommodating inherent flexibility\nin CAD parameters. To train and evaluate Drawing2CAD, we create CAD-VGDrawing,\na dataset of paired engineering drawings and parametric CAD models, and conduct\nthorough experiments to demonstrate the effectiveness of our method. Code and\ndataset are available at https://github.com/lllssc/Drawing2CAD.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.18733.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64258e73120a3ed32333319e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64258e73120a3ed32333319e/f1_eZ6slEoVq8Kn4WbQDC.png",
      "fullname": "Junhao Hou",
      "name": "1nnoh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2509.03888",
      "authors": [
        {
          "_id": "68ba4063736018af705e8e2e",
          "name": "Cheng Wang",
          "hidden": false
        },
        {
          "_id": "68ba4063736018af705e8e2f",
          "name": "Zeming Wei",
          "hidden": false
        },
        {
          "_id": "68ba4063736018af705e8e30",
          "name": "Qin Liu",
          "hidden": false
        },
        {
          "_id": "68ba4063736018af705e8e31",
          "name": "Muhao Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-04T05:15:55.000Z",
      "submittedOnDailyAt": "2025-09-05T00:43:07.219Z",
      "title": "False Sense of Security: Why Probing-based Malicious Input Detection\n  Fails to Generalize",
      "submittedOnDailyBy": {
        "_id": "6329c57e6813868fa49efeaa",
        "avatarUrl": "/avatars/59f46607d3bd33d98a35679e99a00b32.svg",
        "isPro": false,
        "fullname": "Zeming Wei",
        "user": "ZemingWei",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) can comply with harmful instructions, raising\nserious safety concerns despite their impressive capabilities. Recent work has\nleveraged probing-based approaches to study the separability of malicious and\nbenign inputs in LLMs' internal representations, and researchers have proposed\nusing such probing methods for safety detection. We systematically re-examine\nthis paradigm. Motivated by poor out-of-distribution performance, we\nhypothesize that probes learn superficial patterns rather than semantic\nharmfulness. Through controlled experiments, we confirm this hypothesis and\nidentify the specific patterns learned: instructional patterns and trigger\nwords. Our investigation follows a systematic approach, progressing from\ndemonstrating comparable performance of simple n-gram methods, to controlled\nexperiments with semantically cleaned datasets, to detailed analysis of pattern\ndependencies. These results reveal a false sense of security around current\nprobing-based approaches and highlight the need to redesign both models and\nevaluation protocols, for which we provide further discussions in the hope of\nsuggesting responsible further research in this direction. We have open-sourced\nthe project at https://github.com/WangCheng0116/Why-Probe-Fails.",
      "upvotes": 0,
      "discussionId": "68ba4063736018af705e8e32",
      "ai_summary": "Probing-based approaches for detecting harmful instructions in LLMs are found to rely on superficial patterns rather than semantic understanding, indicating a need for redesigning models and evaluation methods.",
      "ai_keywords": [
        "Large Language Models",
        "LLMs",
        "probing-based approaches",
        "internal representations",
        "safety detection",
        "out-of-distribution performance",
        "instructional patterns",
        "trigger words",
        "n-gram methods",
        "semantically cleaned datasets",
        "pattern dependencies"
      ]
    },
    "publishedAt": "2025-09-04T01:15:55.000Z",
    "title": "False Sense of Security: Why Probing-based Malicious Input Detection\n  Fails to Generalize",
    "summary": "Large Language Models (LLMs) can comply with harmful instructions, raising\nserious safety concerns despite their impressive capabilities. Recent work has\nleveraged probing-based approaches to study the separability of malicious and\nbenign inputs in LLMs' internal representations, and researchers have proposed\nusing such probing methods for safety detection. We systematically re-examine\nthis paradigm. Motivated by poor out-of-distribution performance, we\nhypothesize that probes learn superficial patterns rather than semantic\nharmfulness. Through controlled experiments, we confirm this hypothesis and\nidentify the specific patterns learned: instructional patterns and trigger\nwords. Our investigation follows a systematic approach, progressing from\ndemonstrating comparable performance of simple n-gram methods, to controlled\nexperiments with semantically cleaned datasets, to detailed analysis of pattern\ndependencies. These results reveal a false sense of security around current\nprobing-based approaches and highlight the need to redesign both models and\nevaluation protocols, for which we provide further discussions in the hope of\nsuggesting responsible further research in this direction. We have open-sourced\nthe project at https://github.com/WangCheng0116/Why-Probe-Fails.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.03888.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6329c57e6813868fa49efeaa",
      "avatarUrl": "/avatars/59f46607d3bd33d98a35679e99a00b32.svg",
      "fullname": "Zeming Wei",
      "name": "ZemingWei",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]