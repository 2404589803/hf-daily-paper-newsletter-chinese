[
  {
    "paper": {
      "id": "2508.18106",
      "authors": [
        {
          "_id": "68b50b5f851c6e7b001eca21",
          "name": "Keke Lian",
          "hidden": false
        },
        {
          "_id": "68b50b5f851c6e7b001eca22",
          "name": "Bin Wang",
          "hidden": false
        },
        {
          "_id": "68b50b5f851c6e7b001eca23",
          "name": "Lei Zhang",
          "hidden": false
        },
        {
          "_id": "68b50b5f851c6e7b001eca24",
          "name": "Libo Chen",
          "hidden": false
        },
        {
          "_id": "68b50b5f851c6e7b001eca25",
          "name": "Junjie Wang",
          "hidden": false
        },
        {
          "_id": "68b50b5f851c6e7b001eca26",
          "name": "Ziming Zhao",
          "hidden": false
        },
        {
          "_id": "68b50b5f851c6e7b001eca27",
          "name": "Yujiu Yang",
          "hidden": false
        },
        {
          "_id": "68b50b5f851c6e7b001eca28",
          "name": "Haotong Duan",
          "hidden": false
        },
        {
          "_id": "68b50b5f851c6e7b001eca29",
          "name": "Haoran Zhao",
          "hidden": false
        },
        {
          "_id": "68b50b5f851c6e7b001eca2a",
          "name": "Shuang Liao",
          "hidden": false
        },
        {
          "_id": "68b50b5f851c6e7b001eca2b",
          "name": "Mingda Guo",
          "hidden": false
        },
        {
          "_id": "68b50b5f851c6e7b001eca2c",
          "name": "Jiazheng Quan",
          "hidden": false
        },
        {
          "_id": "68b50b5f851c6e7b001eca2d",
          "name": "Yilu Zhong",
          "hidden": false
        },
        {
          "_id": "68b50b5f851c6e7b001eca2e",
          "name": "Chenhao He",
          "hidden": false
        },
        {
          "_id": "68b50b5f851c6e7b001eca2f",
          "name": "Zichuan Chen",
          "hidden": false
        },
        {
          "_id": "68b50b5f851c6e7b001eca30",
          "name": "Jie Wu",
          "hidden": false
        },
        {
          "_id": "68b50b5f851c6e7b001eca31",
          "name": "Haoling Li",
          "hidden": false
        },
        {
          "_id": "68b50b5f851c6e7b001eca32",
          "name": "Zhaoxuan Li",
          "hidden": false
        },
        {
          "_id": "68b50b5f851c6e7b001eca33",
          "name": "Jiongchi Yu",
          "hidden": false
        },
        {
          "_id": "68b50b5f851c6e7b001eca34",
          "name": "Hui Li",
          "hidden": false
        },
        {
          "_id": "68b50b5f851c6e7b001eca35",
          "name": "Dong Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-25T15:11:11.000Z",
      "submittedOnDailyAt": "2025-09-01T01:33:35.016Z",
      "title": "A.S.E: A Repository-Level Benchmark for Evaluating Security in\n  AI-Generated Code",
      "submittedOnDailyBy": {
        "_id": "62579c55b98dcaa7e0de285d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62579c55b98dcaa7e0de285d/0YUd5nloul_bW9yolDGGo.jpeg",
        "isPro": false,
        "fullname": "wangjunjie",
        "user": "wanng",
        "type": "user"
      },
      "summary": "The increasing adoption of large language models (LLMs) in software\nengineering necessitates rigorous security evaluation of their generated code.\nHowever, existing benchmarks are inadequate, as they focus on isolated code\nsnippets, employ unstable evaluation methods that lack reproducibility, and\nfail to connect the quality of input context with the security of the output.\nTo address these gaps, we introduce A.S.E (AI Code Generation Security\nEvaluation), a benchmark for repository-level secure code generation. A.S.E\nconstructs tasks from real-world repositories with documented CVEs, preserving\nfull repository context like build systems and cross-file dependencies. Its\nreproducible, containerized evaluation framework uses expert-defined rules to\nprovide stable, auditable assessments of security, build quality, and\ngeneration stability. Our evaluation of leading LLMs on A.S.E reveals three key\nfindings: (1) Claude-3.7-Sonnet achieves the best overall performance. (2) The\nsecurity gap between proprietary and open-source models is narrow;\nQwen3-235B-A22B-Instruct attains the top security score. (3) Concise,\n``fast-thinking'' decoding strategies consistently outperform complex,\n``slow-thinking'' reasoning for security patching.",
      "upvotes": 40,
      "discussionId": "68b50b60851c6e7b001eca36",
      "githubRepo": "https://github.com/Tencent/AICGSecEval",
      "ai_summary": "A.S.E is a benchmark for evaluating the security of code generated by large language models using real-world repositories and expert-defined rules, revealing insights into model performance and decoding strategies.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "software engineering",
        "security evaluation",
        "generated code",
        "benchmarks",
        "code snippets",
        "evaluation methods",
        "reproducibility",
        "input context",
        "repository-level",
        "secure code generation",
        "CVEs",
        "build systems",
        "cross-file dependencies",
        "containerized evaluation",
        "expert-defined rules",
        "security",
        "build quality",
        "generation stability",
        "Claude-3.7-Sonnet",
        "Qwen3-235B-A22B-Instruct",
        "decoding strategies",
        "security patching"
      ],
      "githubStars": 128
    },
    "publishedAt": "2025-08-25T11:11:11.000Z",
    "title": "A.S.E: A Repository-Level Benchmark for Evaluating Security in\n  AI-Generated Code",
    "summary": "The increasing adoption of large language models (LLMs) in software\nengineering necessitates rigorous security evaluation of their generated code.\nHowever, existing benchmarks are inadequate, as they focus on isolated code\nsnippets, employ unstable evaluation methods that lack reproducibility, and\nfail to connect the quality of input context with the security of the output.\nTo address these gaps, we introduce A.S.E (AI Code Generation Security\nEvaluation), a benchmark for repository-level secure code generation. A.S.E\nconstructs tasks from real-world repositories with documented CVEs, preserving\nfull repository context like build systems and cross-file dependencies. Its\nreproducible, containerized evaluation framework uses expert-defined rules to\nprovide stable, auditable assessments of security, build quality, and\ngeneration stability. Our evaluation of leading LLMs on A.S.E reveals three key\nfindings: (1) Claude-3.7-Sonnet achieves the best overall performance. (2) The\nsecurity gap between proprietary and open-source models is narrow;\nQwen3-235B-A22B-Instruct attains the top security score. (3) Concise,\n``fast-thinking'' decoding strategies consistently outperform complex,\n``slow-thinking'' reasoning for security patching.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.18106.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "62579c55b98dcaa7e0de285d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62579c55b98dcaa7e0de285d/0YUd5nloul_bW9yolDGGo.jpeg",
      "fullname": "wangjunjie",
      "name": "wanng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 30
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.21112",
      "authors": [
        {
          "_id": "68b4e4e7851c6e7b001ec9a7",
          "name": "Delin Qu",
          "hidden": false
        },
        {
          "_id": "68b4e4e7851c6e7b001ec9a8",
          "name": "Haoming Song",
          "hidden": false
        },
        {
          "_id": "68b4e4e7851c6e7b001ec9a9",
          "name": "Qizhi Chen",
          "hidden": false
        },
        {
          "_id": "68b4e4e7851c6e7b001ec9aa",
          "name": "Zhaoqing Chen",
          "hidden": false
        },
        {
          "_id": "68b4e4e7851c6e7b001ec9ab",
          "name": "Xianqiang Gao",
          "hidden": false
        },
        {
          "_id": "68b4e4e7851c6e7b001ec9ac",
          "name": "Xinyi Ye",
          "hidden": false
        },
        {
          "_id": "68b4e4e7851c6e7b001ec9ad",
          "name": "Qi Lv",
          "hidden": false
        },
        {
          "_id": "68b4e4e7851c6e7b001ec9ae",
          "name": "Modi Shi",
          "hidden": false
        },
        {
          "_id": "68b4e4e7851c6e7b001ec9af",
          "name": "Guanghui Ren",
          "hidden": false
        },
        {
          "_id": "68b4e4e7851c6e7b001ec9b0",
          "name": "Cheng Ruan",
          "hidden": false
        },
        {
          "_id": "68b4e4e7851c6e7b001ec9b1",
          "name": "Maoqing Yao",
          "hidden": false
        },
        {
          "_id": "68b4e4e7851c6e7b001ec9b2",
          "name": "Haoran Yang",
          "hidden": false
        },
        {
          "_id": "68b4e4e7851c6e7b001ec9b3",
          "name": "Jiacheng Bao",
          "hidden": false
        },
        {
          "_id": "68b4e4e7851c6e7b001ec9b4",
          "name": "Bin Zhao",
          "hidden": false
        },
        {
          "_id": "68b4e4e7851c6e7b001ec9b5",
          "name": "Dong Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-28T17:26:15.000Z",
      "submittedOnDailyAt": "2025-09-01T00:04:28.519Z",
      "title": "EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for\n  General Robot Control",
      "submittedOnDailyBy": {
        "_id": "64daecec888b7e9c400f59b5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64daecec888b7e9c400f59b5/f4pfOfWk6jYJX-Nf2-qHn.png",
        "isPro": false,
        "fullname": "Delin Qu",
        "user": "delinqu",
        "type": "user"
      },
      "summary": "The human ability to seamlessly perform multimodal reasoning and physical\ninteraction in the open world is a core goal for general-purpose embodied\nintelligent systems. Recent vision-language-action (VLA) models, which are\nco-trained on large-scale robot and visual-text data, have demonstrated notable\nprogress in general robot control. However, they still fail to achieve\nhuman-level flexibility in interleaved reasoning and interaction. In this work,\nintroduce EO-Robotics, consists of EO-1 model and EO-Data1.5M dataset. EO-1 is\na unified embodied foundation model that achieves superior performance in\nmultimodal embodied reasoning and robot control through interleaved\nvision-text-action pre-training. The development of EO-1 is based on two key\npillars: (i) a unified architecture that processes multimodal inputs\nindiscriminately (image, text, video, and action), and (ii) a massive,\nhigh-quality multimodal embodied reasoning dataset, EO-Data1.5M, which contains\nover 1.5 million samples with emphasis on interleaved vision-text-action\ncomprehension. EO-1 is trained through synergies between auto-regressive\ndecoding and flow matching denoising on EO-Data1.5M, enabling seamless robot\naction generation and multimodal embodied reasoning. Extensive experiments\ndemonstrate the effectiveness of interleaved vision-text-action learning for\nopen-world understanding and generalization, validated through a variety of\nlong-horizon, dexterous manipulation tasks across multiple embodiments. This\npaper details the architecture of EO-1, the data construction strategy of\nEO-Data1.5M, and the training methodology, offering valuable insights for\ndeveloping advanced embodied foundation models.",
      "upvotes": 37,
      "discussionId": "68b4e4e7851c6e7b001ec9b6",
      "projectPage": "https://eo-robotics.ai/eo-1",
      "githubRepo": "https://github.com/EO-Robotics/EO-1",
      "ai_summary": "EO-Robotics, comprising EO-1 model and EO-Data1.5M dataset, advances multimodal embodied reasoning and robot control through interleaved vision-text-action pre-training.",
      "ai_keywords": [
        "vision-language-action (VLA) models",
        "embodied foundation model",
        "multimodal inputs",
        "auto-regressive decoding",
        "flow matching denoising",
        "interleaved vision-text-action comprehension",
        "long-horizon tasks",
        "dexterous manipulation"
      ],
      "githubStars": 78
    },
    "publishedAt": "2025-08-28T13:26:15.000Z",
    "title": "EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for\n  General Robot Control",
    "summary": "The human ability to seamlessly perform multimodal reasoning and physical\ninteraction in the open world is a core goal for general-purpose embodied\nintelligent systems. Recent vision-language-action (VLA) models, which are\nco-trained on large-scale robot and visual-text data, have demonstrated notable\nprogress in general robot control. However, they still fail to achieve\nhuman-level flexibility in interleaved reasoning and interaction. In this work,\nintroduce EO-Robotics, consists of EO-1 model and EO-Data1.5M dataset. EO-1 is\na unified embodied foundation model that achieves superior performance in\nmultimodal embodied reasoning and robot control through interleaved\nvision-text-action pre-training. The development of EO-1 is based on two key\npillars: (i) a unified architecture that processes multimodal inputs\nindiscriminately (image, text, video, and action), and (ii) a massive,\nhigh-quality multimodal embodied reasoning dataset, EO-Data1.5M, which contains\nover 1.5 million samples with emphasis on interleaved vision-text-action\ncomprehension. EO-1 is trained through synergies between auto-regressive\ndecoding and flow matching denoising on EO-Data1.5M, enabling seamless robot\naction generation and multimodal embodied reasoning. Extensive experiments\ndemonstrate the effectiveness of interleaved vision-text-action learning for\nopen-world understanding and generalization, validated through a variety of\nlong-horizon, dexterous manipulation tasks across multiple embodiments. This\npaper details the architecture of EO-1, the data construction strategy of\nEO-Data1.5M, and the training methodology, offering valuable insights for\ndeveloping advanced embodied foundation models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.21112.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64daecec888b7e9c400f59b5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64daecec888b7e9c400f59b5/f4pfOfWk6jYJX-Nf2-qHn.png",
      "fullname": "Delin Qu",
      "name": "delinqu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.21113",
      "authors": [
        {
          "_id": "68b4f36d851c6e7b001ec9c5",
          "name": "Jie Jiang",
          "hidden": false
        },
        {
          "_id": "68b4f36d851c6e7b001ec9c6",
          "name": "Qi Yang",
          "hidden": false
        },
        {
          "_id": "68b4f36d851c6e7b001ec9c7",
          "name": "Bolin Ni",
          "hidden": false
        },
        {
          "_id": "68b4f36d851c6e7b001ec9c8",
          "name": "Shiming Xiang",
          "hidden": false
        },
        {
          "_id": "68b4f36d851c6e7b001ec9c9",
          "name": "Han Hu",
          "hidden": false
        },
        {
          "_id": "68b4f36d851c6e7b001ec9ca",
          "name": "Houwen Peng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-28T17:48:19.000Z",
      "submittedOnDailyAt": "2025-09-01T00:10:31.599Z",
      "title": "R-4B: Incentivizing General-Purpose Auto-Thinking Capability in MLLMs\n  via Bi-Mode Annealing and Reinforce Learning",
      "submittedOnDailyBy": {
        "_id": "643e45c4c639f8bc9727810a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643e45c4c639f8bc9727810a/BJR1cvSCxcqxr08iS7GsI.jpeg",
        "isPro": false,
        "fullname": "YannQi",
        "user": "YannQi",
        "type": "user"
      },
      "summary": "Multimodal Large Language Models (MLLMs) equipped with step-by-step thinking\ncapabilities have demonstrated remarkable performance on complex reasoning\nproblems. However, this thinking process is redundant for simple problems\nsolvable without complex reasoning. To address this inefficiency, we propose\nR-4B, an auto-thinking MLLM, which can adaptively decide when to think based on\nproblem complexity. The central idea of R-4B is to empower the model with both\nthinking and non-thinking capabilities using bi-mode annealing, and apply\nBi-mode Policy Optimization~(BPO) to improve the model's accuracy in\ndetermining whether to activate the thinking process. Specifically, we first\ntrain the model on a carefully curated dataset spanning various topics, which\ncontains samples from both thinking and non-thinking modes. Then it undergoes a\nsecond phase of training under an improved GRPO framework, where the policy\nmodel is forced to generate responses from both modes for each input query.\nExperimental results show that R-4B achieves state-of-the-art performance\nacross 25 challenging benchmarks. It outperforms Qwen2.5-VL-7B in most tasks\nand achieves performance comparable to larger models such as\nKimi-VL-A3B-Thinking-2506 (16B) on reasoning-intensive benchmarks with lower\ncomputational cost.",
      "upvotes": 30,
      "discussionId": "68b4f36d851c6e7b001ec9cb",
      "githubRepo": "https://github.com/yannqi/R-4B",
      "ai_summary": "R-4B, an auto-thinking multimodal large language model, uses bi-mode annealing and Bi-mode Policy Optimization to adaptively decide on problem-solving strategies, achieving state-of-the-art performance with lower computational cost.",
      "ai_keywords": [
        "multimodal large language models",
        "step-by-step thinking",
        "auto-thinking",
        "bi-mode annealing",
        "Bi-mode Policy Optimization",
        "GRPO framework",
        "policy model",
        "reasoning-intensive benchmarks"
      ],
      "githubStars": 15
    },
    "publishedAt": "2025-08-28T13:48:19.000Z",
    "title": "R-4B: Incentivizing General-Purpose Auto-Thinking Capability in MLLMs\n  via Bi-Mode Annealing and Reinforce Learning",
    "summary": "Multimodal Large Language Models (MLLMs) equipped with step-by-step thinking\ncapabilities have demonstrated remarkable performance on complex reasoning\nproblems. However, this thinking process is redundant for simple problems\nsolvable without complex reasoning. To address this inefficiency, we propose\nR-4B, an auto-thinking MLLM, which can adaptively decide when to think based on\nproblem complexity. The central idea of R-4B is to empower the model with both\nthinking and non-thinking capabilities using bi-mode annealing, and apply\nBi-mode Policy Optimization~(BPO) to improve the model's accuracy in\ndetermining whether to activate the thinking process. Specifically, we first\ntrain the model on a carefully curated dataset spanning various topics, which\ncontains samples from both thinking and non-thinking modes. Then it undergoes a\nsecond phase of training under an improved GRPO framework, where the policy\nmodel is forced to generate responses from both modes for each input query.\nExperimental results show that R-4B achieves state-of-the-art performance\nacross 25 challenging benchmarks. It outperforms Qwen2.5-VL-7B in most tasks\nand achieves performance comparable to larger models such as\nKimi-VL-A3B-Thinking-2506 (16B) on reasoning-intensive benchmarks with lower\ncomputational cost.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.21113.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643e45c4c639f8bc9727810a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643e45c4c639f8bc9727810a/BJR1cvSCxcqxr08iS7GsI.jpeg",
      "fullname": "YannQi",
      "name": "YannQi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.20470",
      "authors": [
        {
          "_id": "68b50e02851c6e7b001eca38",
          "name": "Xiaochuan Li",
          "hidden": false
        },
        {
          "_id": "68b50e02851c6e7b001eca39",
          "name": "Guoguang Du",
          "hidden": false
        },
        {
          "_id": "68b50e02851c6e7b001eca3a",
          "name": "Runze Zhang",
          "hidden": false
        },
        {
          "_id": "68b50e02851c6e7b001eca3b",
          "name": "Liang Jin",
          "hidden": false
        },
        {
          "_id": "68b50e02851c6e7b001eca3c",
          "name": "Qi Jia",
          "hidden": false
        },
        {
          "_id": "68b50e02851c6e7b001eca3d",
          "name": "Lihua Lu",
          "hidden": false
        },
        {
          "_id": "68b50e02851c6e7b001eca3e",
          "name": "Zhenhua Guo",
          "hidden": false
        },
        {
          "_id": "68b50e02851c6e7b001eca3f",
          "name": "Yaqian Zhao",
          "hidden": false
        },
        {
          "_id": "68b50e02851c6e7b001eca40",
          "name": "Haiyang Liu",
          "hidden": false
        },
        {
          "_id": "68b50e02851c6e7b001eca41",
          "name": "Tianqi Wang",
          "hidden": false
        },
        {
          "_id": "68b50e02851c6e7b001eca42",
          "name": "Changsheng Li",
          "hidden": false
        },
        {
          "_id": "68b50e02851c6e7b001eca43",
          "name": "Xiaoli Gong",
          "hidden": false
        },
        {
          "_id": "68b50e02851c6e7b001eca44",
          "name": "Rengang Li",
          "hidden": false
        },
        {
          "_id": "68b50e02851c6e7b001eca45",
          "name": "Baoyu Fan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-28T06:39:41.000Z",
      "submittedOnDailyAt": "2025-09-01T05:34:52.494Z",
      "title": "Droplet3D: Commonsense Priors from Videos Facilitate 3D Generation",
      "submittedOnDailyBy": {
        "_id": "66b01dc4e48856bb718f2ba8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66b01dc4e48856bb718f2ba8/MHZyIDd3BEtsnH9M2BvzM.jpeg",
        "isPro": false,
        "fullname": "Xiaochuan Li",
        "user": "lixiaochuan",
        "type": "user"
      },
      "summary": "Scaling laws have validated the success and promise of large-data-trained\nmodels in creative generation across text, image, and video domains. However,\nthis paradigm faces data scarcity in the 3D domain, as there is far less of it\navailable on the internet compared to the aforementioned modalities.\nFortunately, there exist adequate videos that inherently contain commonsense\npriors, offering an alternative supervisory signal to mitigate the\ngeneralization bottleneck caused by limited native 3D data. On the one hand,\nvideos capturing multiple views of an object or scene provide a spatial\nconsistency prior for 3D generation. On the other hand, the rich semantic\ninformation contained within the videos enables the generated content to be\nmore faithful to the text prompts and semantically plausible. This paper\nexplores how to apply the video modality in 3D asset generation, spanning\ndatasets to models. We introduce Droplet3D-4M, the first large-scale video\ndataset with multi-view level annotations, and train Droplet3D, a generative\nmodel supporting both image and dense text input. Extensive experiments\nvalidate the effectiveness of our approach, demonstrating its ability to\nproduce spatially consistent and semantically plausible content. Moreover, in\ncontrast to the prevailing 3D solutions, our approach exhibits the potential\nfor extension to scene-level applications. This indicates that the commonsense\npriors from the videos significantly facilitate 3D creation. We have\nopen-sourced all resources including the dataset, code, technical framework,\nand model weights: https://dropletx.github.io/.",
      "upvotes": 11,
      "discussionId": "68b50e03851c6e7b001eca46",
      "ai_summary": "Using video data to provide commonsense priors enhances 3D asset generation, enabling spatial consistency and semantic plausibility in 3D content creation.",
      "ai_keywords": [
        "scaling laws",
        "large-data-trained models",
        "creative generation",
        "3D domain",
        "data scarcity",
        "commonsense priors",
        "supervisory signal",
        "generalization bottleneck",
        "spatial consistency prior",
        "semantic information",
        "Droplet3D-4M",
        "Droplet3D",
        "generative model",
        "image input",
        "dense text input",
        "scene-level applications"
      ]
    },
    "publishedAt": "2025-08-28T02:39:41.000Z",
    "title": "Droplet3D: Commonsense Priors from Videos Facilitate 3D Generation",
    "summary": "Scaling laws have validated the success and promise of large-data-trained\nmodels in creative generation across text, image, and video domains. However,\nthis paradigm faces data scarcity in the 3D domain, as there is far less of it\navailable on the internet compared to the aforementioned modalities.\nFortunately, there exist adequate videos that inherently contain commonsense\npriors, offering an alternative supervisory signal to mitigate the\ngeneralization bottleneck caused by limited native 3D data. On the one hand,\nvideos capturing multiple views of an object or scene provide a spatial\nconsistency prior for 3D generation. On the other hand, the rich semantic\ninformation contained within the videos enables the generated content to be\nmore faithful to the text prompts and semantically plausible. This paper\nexplores how to apply the video modality in 3D asset generation, spanning\ndatasets to models. We introduce Droplet3D-4M, the first large-scale video\ndataset with multi-view level annotations, and train Droplet3D, a generative\nmodel supporting both image and dense text input. Extensive experiments\nvalidate the effectiveness of our approach, demonstrating its ability to\nproduce spatially consistent and semantically plausible content. Moreover, in\ncontrast to the prevailing 3D solutions, our approach exhibits the potential\nfor extension to scene-level applications. This indicates that the commonsense\npriors from the videos significantly facilitate 3D creation. We have\nopen-sourced all resources including the dataset, code, technical framework,\nand model weights: https://dropletx.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.20470.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66b01dc4e48856bb718f2ba8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66b01dc4e48856bb718f2ba8/MHZyIDd3BEtsnH9M2BvzM.jpeg",
      "fullname": "Xiaochuan Li",
      "name": "lixiaochuan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.13618",
      "authors": [
        {
          "_id": "68b510b7851c6e7b001eca5a",
          "name": "Shunian Chen",
          "hidden": false
        },
        {
          "_id": "68b510b7851c6e7b001eca5b",
          "name": "Hejin Huang",
          "hidden": false
        },
        {
          "_id": "68b510b7851c6e7b001eca5c",
          "name": "Yexin Liu",
          "hidden": false
        },
        {
          "_id": "68b510b7851c6e7b001eca5d",
          "name": "Zihan Ye",
          "hidden": false
        },
        {
          "_id": "68b510b7851c6e7b001eca5e",
          "name": "Pengcheng Chen",
          "hidden": false
        },
        {
          "_id": "68b510b7851c6e7b001eca5f",
          "name": "Chenghao Zhu",
          "hidden": false
        },
        {
          "_id": "68b510b7851c6e7b001eca60",
          "name": "Michael Guan",
          "hidden": false
        },
        {
          "_id": "68b510b7851c6e7b001eca61",
          "name": "Rongsheng Wang",
          "hidden": false
        },
        {
          "_id": "68b510b7851c6e7b001eca62",
          "name": "Junying Chen",
          "hidden": false
        },
        {
          "_id": "68b510b7851c6e7b001eca63",
          "name": "Guanbin Li",
          "hidden": false
        },
        {
          "_id": "68b510b7851c6e7b001eca64",
          "name": "Ser-Nam Lim",
          "hidden": false
        },
        {
          "_id": "68b510b7851c6e7b001eca65",
          "name": "Harry Yang",
          "hidden": false
        },
        {
          "_id": "68b510b7851c6e7b001eca66",
          "name": "Benyou Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-19T08:31:15.000Z",
      "submittedOnDailyAt": "2025-09-01T01:53:13.364Z",
      "title": "TalkVid: A Large-Scale Diversified Dataset for Audio-Driven Talking Head\n  Synthesis",
      "submittedOnDailyBy": {
        "_id": "623be9e1d1eb227788764959",
        "avatarUrl": "/avatars/b6521b795a59754dbb40123fd4f63b8c.svg",
        "isPro": false,
        "fullname": "Shunian Chen",
        "user": "Shunian",
        "type": "user"
      },
      "summary": "Audio-driven talking head synthesis has achieved remarkable photorealism, yet\nstate-of-the-art (SOTA) models exhibit a critical failure: they lack\ngeneralization to the full spectrum of human diversity in ethnicity, language,\nand age groups. We argue that this generalization gap is a direct symptom of\nlimitations in existing training data, which lack the necessary scale, quality,\nand diversity. To address this challenge, we introduce TalkVid, a new\nlarge-scale, high-quality, and diverse dataset containing 1244 hours of video\nfrom 7729 unique speakers. TalkVid is curated through a principled, multi-stage\nautomated pipeline that rigorously filters for motion stability, aesthetic\nquality, and facial detail, and is validated against human judgments to ensure\nits reliability. Furthermore, we construct and release TalkVid-Bench, a\nstratified evaluation set of 500 clips meticulously balanced across key\ndemographic and linguistic axes. Our experiments demonstrate that a model\ntrained on TalkVid outperforms counterparts trained on previous datasets,\nexhibiting superior cross-dataset generalization. Crucially, our analysis on\nTalkVid-Bench reveals performance disparities across subgroups that are\nobscured by traditional aggregate metrics, underscoring its necessity for\nfuture research. Code and data can be found in\nhttps://github.com/FreedomIntelligence/TalkVid",
      "upvotes": 11,
      "discussionId": "68b510b7851c6e7b001eca67",
      "projectPage": "https://freedomintelligence.github.io/talk-vid/",
      "githubRepo": "https://github.com/FreedomIntelligence/TalkVid",
      "ai_summary": "TalkVid, a large-scale, high-quality, and diverse dataset, improves audio-driven talking head synthesis by enhancing generalization across human diversity and revealing subgroup performance disparities.",
      "ai_keywords": [
        "audio-driven talking head synthesis",
        "photorealism",
        "generalization",
        "training data",
        "large-scale dataset",
        "high-quality dataset",
        "diverse dataset",
        "motion stability",
        "aesthetic quality",
        "facial detail",
        "stratified evaluation set",
        "cross-dataset generalization",
        "performance disparities",
        "aggregate metrics"
      ],
      "githubStars": 68
    },
    "publishedAt": "2025-08-19T04:31:15.000Z",
    "title": "TalkVid: A Large-Scale Diversified Dataset for Audio-Driven Talking Head\n  Synthesis",
    "summary": "Audio-driven talking head synthesis has achieved remarkable photorealism, yet\nstate-of-the-art (SOTA) models exhibit a critical failure: they lack\ngeneralization to the full spectrum of human diversity in ethnicity, language,\nand age groups. We argue that this generalization gap is a direct symptom of\nlimitations in existing training data, which lack the necessary scale, quality,\nand diversity. To address this challenge, we introduce TalkVid, a new\nlarge-scale, high-quality, and diverse dataset containing 1244 hours of video\nfrom 7729 unique speakers. TalkVid is curated through a principled, multi-stage\nautomated pipeline that rigorously filters for motion stability, aesthetic\nquality, and facial detail, and is validated against human judgments to ensure\nits reliability. Furthermore, we construct and release TalkVid-Bench, a\nstratified evaluation set of 500 clips meticulously balanced across key\ndemographic and linguistic axes. Our experiments demonstrate that a model\ntrained on TalkVid outperforms counterparts trained on previous datasets,\nexhibiting superior cross-dataset generalization. Crucially, our analysis on\nTalkVid-Bench reveals performance disparities across subgroups that are\nobscured by traditional aggregate metrics, underscoring its necessity for\nfuture research. Code and data can be found in\nhttps://github.com/FreedomIntelligence/TalkVid",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.13618.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "623be9e1d1eb227788764959",
      "avatarUrl": "/avatars/b6521b795a59754dbb40123fd4f63b8c.svg",
      "fullname": "Shunian Chen",
      "name": "Shunian",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.17677",
      "authors": [
        {
          "_id": "68aec6138a2eb0ba9b54e982",
          "name": "Yifan Wang",
          "hidden": false
        },
        {
          "_id": "68aec6138a2eb0ba9b54e983",
          "name": "Binbin Liu",
          "hidden": false
        },
        {
          "_id": "68aec6138a2eb0ba9b54e984",
          "name": "Fengze Liu",
          "hidden": false
        },
        {
          "_id": "68aec6138a2eb0ba9b54e985",
          "name": "Yuanfan Guo",
          "hidden": false
        },
        {
          "_id": "68aec6138a2eb0ba9b54e986",
          "name": "Jiyao Deng",
          "hidden": false
        },
        {
          "_id": "68aec6138a2eb0ba9b54e987",
          "name": "Xuecheng Wu",
          "hidden": false
        },
        {
          "_id": "68aec6138a2eb0ba9b54e988",
          "name": "Weidong Zhou",
          "hidden": false
        },
        {
          "_id": "68aec6138a2eb0ba9b54e989",
          "name": "Xiaohuan Zhou",
          "hidden": false
        },
        {
          "_id": "68aec6138a2eb0ba9b54e98a",
          "name": "Taifeng Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-25T05:18:32.000Z",
      "submittedOnDailyAt": "2025-09-01T02:09:59.539Z",
      "title": "TiKMiX: Take Data Influence into Dynamic Mixture for Language Model\n  Pre-training",
      "submittedOnDailyBy": {
        "_id": "668f5875b5b3081d776e4094",
        "avatarUrl": "/avatars/8c763393f25afbe5fb8b132f775e746a.svg",
        "isPro": false,
        "fullname": "Xiaohuan Zhou",
        "user": "XiaohuanZhou",
        "type": "user"
      },
      "summary": "The data mixture used in the pre-training of a language model is a\ncornerstone of its final performance. However, a static mixing strategy is\nsuboptimal, as the model's learning preferences for various data domains shift\ndynamically throughout training. Crucially, observing these evolving\npreferences in a computationally efficient manner remains a significant\nchallenge. To address this, we propose TiKMiX, a method that dynamically\nadjusts the data mixture according to the model's evolving preferences. TiKMiX\nintroduces Group Influence, an efficient metric for evaluating the impact of\ndata domains on the model. This metric enables the formulation of the data\nmixing problem as a search for an optimal, influence-maximizing distribution.\nWe solve this via two approaches: TiKMiX-D for direct optimization, and\nTiKMiX-M, which uses a regression model to predict a superior mixture. We\ntrained models with different numbers of parameters, on up to 1 trillion\ntokens. TiKMiX-D exceeds the performance of state-of-the-art methods like\nREGMIX while using just 20% of the computational resources. TiKMiX-M leads to\nan average performance gain of 2% across 9 downstream benchmarks. Our\nexperiments reveal that a model's data preferences evolve with training\nprogress and scale, and we demonstrate that dynamically adjusting the data\nmixture based on Group Influence, a direct measure of these preferences,\nsignificantly improves performance by mitigating the underdigestion of data\nseen with static ratios.",
      "upvotes": 3,
      "discussionId": "68aec6138a2eb0ba9b54e98b",
      "ai_summary": "Dynamic adjustment of data mixture based on Group Influence metric improves language model performance by adapting to evolving learning preferences.",
      "ai_keywords": [
        "Group Influence",
        "TiKMiX",
        "TiKMiX-D",
        "TiKMiX-M",
        "data mixture",
        "language model",
        "pre-training",
        "computational resources",
        "downstream benchmarks",
        "data preferences",
        "underdigestion"
      ]
    },
    "publishedAt": "2025-08-25T01:18:32.000Z",
    "title": "TiKMiX: Take Data Influence into Dynamic Mixture for Language Model\n  Pre-training",
    "summary": "The data mixture used in the pre-training of a language model is a\ncornerstone of its final performance. However, a static mixing strategy is\nsuboptimal, as the model's learning preferences for various data domains shift\ndynamically throughout training. Crucially, observing these evolving\npreferences in a computationally efficient manner remains a significant\nchallenge. To address this, we propose TiKMiX, a method that dynamically\nadjusts the data mixture according to the model's evolving preferences. TiKMiX\nintroduces Group Influence, an efficient metric for evaluating the impact of\ndata domains on the model. This metric enables the formulation of the data\nmixing problem as a search for an optimal, influence-maximizing distribution.\nWe solve this via two approaches: TiKMiX-D for direct optimization, and\nTiKMiX-M, which uses a regression model to predict a superior mixture. We\ntrained models with different numbers of parameters, on up to 1 trillion\ntokens. TiKMiX-D exceeds the performance of state-of-the-art methods like\nREGMIX while using just 20% of the computational resources. TiKMiX-M leads to\nan average performance gain of 2% across 9 downstream benchmarks. Our\nexperiments reveal that a model's data preferences evolve with training\nprogress and scale, and we demonstrate that dynamically adjusting the data\nmixture based on Group Influence, a direct measure of these preferences,\nsignificantly improves performance by mitigating the underdigestion of data\nseen with static ratios.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.17677.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "668f5875b5b3081d776e4094",
      "avatarUrl": "/avatars/8c763393f25afbe5fb8b132f775e746a.svg",
      "fullname": "Xiaohuan Zhou",
      "name": "XiaohuanZhou",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.21290",
      "authors": [
        {
          "_id": "68b5221d851c6e7b001ecaa9",
          "name": "Daria Kryvosheieva",
          "hidden": false
        },
        {
          "_id": "68b5221d851c6e7b001ecaaa",
          "name": "Saba Sturua",
          "hidden": false
        },
        {
          "_id": "68b5221d851c6e7b001ecaab",
          "name": "Michael GÃ¼nther",
          "hidden": false
        },
        {
          "_id": "68b5221d851c6e7b001ecaac",
          "name": "Scott Martens",
          "hidden": false
        },
        {
          "_id": "68b5221d851c6e7b001ecaad",
          "name": "Han Xiao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-29T01:18:15.000Z",
      "submittedOnDailyAt": "2025-09-01T03:03:59.758Z",
      "title": "Efficient Code Embeddings from Code Generation Models",
      "submittedOnDailyBy": {
        "_id": "603763514de52ff951d89793",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/603763514de52ff951d89793/n-QouGYg7oE5QeDaAb3Ns.png",
        "isPro": false,
        "fullname": "Han Xiao",
        "user": "hanxiao",
        "type": "user"
      },
      "summary": "jina-code-embeddings is a novel code embedding model suite designed to\nretrieve code from natural language queries, perform technical\nquestion-answering, and identify semantically similar code snippets across\nprogramming languages. It makes innovative use of an autoregressive backbone\npre-trained on both text and code, generating embeddings via last-token\npooling. We outline the training recipe and demonstrate state-of-the-art\nperformance despite the relatively small size of the models, validating this\napproach to code embedding model construction.",
      "upvotes": 2,
      "discussionId": "68b5221e851c6e7b001ecaae",
      "ai_summary": "Jina-code-embeddings uses an autoregressive backbone pre-trained on text and code to generate embeddings for code retrieval, question-answering, and similarity identification.",
      "ai_keywords": [
        "autoregressive backbone",
        "last-token pooling",
        "code embedding model"
      ]
    },
    "publishedAt": "2025-08-28T21:18:15.000Z",
    "title": "Efficient Code Embeddings from Code Generation Models",
    "summary": "jina-code-embeddings is a novel code embedding model suite designed to\nretrieve code from natural language queries, perform technical\nquestion-answering, and identify semantically similar code snippets across\nprogramming languages. It makes innovative use of an autoregressive backbone\npre-trained on both text and code, generating embeddings via last-token\npooling. We outline the training recipe and demonstrate state-of-the-art\nperformance despite the relatively small size of the models, validating this\napproach to code embedding model construction.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.21290.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "603763514de52ff951d89793",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/603763514de52ff951d89793/n-QouGYg7oE5QeDaAb3Ns.png",
      "fullname": "Han Xiao",
      "name": "hanxiao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.21767",
      "authors": [
        {
          "_id": "68b5103a851c6e7b001eca4e",
          "name": "Zhixiong Zeng",
          "hidden": false
        },
        {
          "_id": "68b5103a851c6e7b001eca4f",
          "name": "Jing Huang",
          "hidden": false
        },
        {
          "_id": "68b5103a851c6e7b001eca50",
          "name": "Liming Zheng",
          "hidden": false
        },
        {
          "_id": "68b5103a851c6e7b001eca51",
          "name": "Wenkang Han",
          "hidden": false
        },
        {
          "_id": "68b5103a851c6e7b001eca52",
          "name": "Yufeng Zhong",
          "hidden": false
        },
        {
          "_id": "68b5103a851c6e7b001eca53",
          "name": "Lei Chen",
          "hidden": false
        },
        {
          "_id": "68b5103a851c6e7b001eca54",
          "name": "Longrong Yang",
          "hidden": false
        },
        {
          "_id": "68b5103a851c6e7b001eca55",
          "name": "Yingjie Chu",
          "hidden": false
        },
        {
          "_id": "68b5103a851c6e7b001eca56",
          "name": "Yuzhi He",
          "hidden": false
        },
        {
          "_id": "68b5103a851c6e7b001eca57",
          "name": "Lin Ma",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-29T16:40:57.000Z",
      "submittedOnDailyAt": "2025-09-01T01:47:29.488Z",
      "title": "UItron: Foundational GUI Agent with Advanced Perception and Planning",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "GUI agent aims to enable automated operations on Mobile/PC devices, which is\nan important task toward achieving artificial general intelligence. The rapid\nadvancement of VLMs accelerates the development of GUI agents, owing to their\npowerful capabilities in visual understanding and task planning. However,\nbuilding a GUI agent remains a challenging task due to the scarcity of\noperation trajectories, the availability of interactive infrastructure, and the\nlimitation of initial capabilities in foundation models. In this work, we\nintroduce UItron, an open-source foundational model for automatic GUI agents,\nfeaturing advanced GUI perception, grounding, and planning capabilities. UItron\nhighlights the necessity of systemic data engineering and interactive\ninfrastructure as foundational components for advancing GUI agent development.\nIt not only systematically studies a series of data engineering strategies to\nenhance training effects, but also establishes an interactive environment\nconnecting both Mobile and PC devices. In training, UItron adopts supervised\nfinetuning over perception and planning tasks in various GUI scenarios, and\nthen develop a curriculum reinforcement learning framework to enable complex\nreasoning and exploration for online environments. As a result, UItron achieves\nsuperior performance in benchmarks of GUI perception, grounding, and planning.\nIn particular, UItron highlights the interaction proficiency with top-tier\nChinese mobile APPs, as we identified a general lack of Chinese capabilities\neven in state-of-the-art solutions. To this end, we manually collect over one\nmillion steps of operation trajectories across the top 100 most popular apps,\nand build the offline and online agent evaluation environments. Experimental\nresults demonstrate that UItron achieves significant progress in Chinese app\nscenarios, propelling GUI agents one step closer to real-world application.",
      "upvotes": 1,
      "discussionId": "68b5103b851c6e7b001eca58",
      "ai_summary": "UItron, an open-source foundational model for GUI agents, enhances visual understanding and task planning through advanced perception, grounding, and planning capabilities, achieving superior performance in Chinese app scenarios.",
      "ai_keywords": [
        "GUI agents",
        "VLMs",
        "visual understanding",
        "task planning",
        "UItron",
        "GUI perception",
        "grounding",
        "planning",
        "supervised finetuning",
        "curriculum reinforcement learning",
        "offline and online agent evaluation environments"
      ]
    },
    "publishedAt": "2025-08-29T12:40:57.000Z",
    "title": "UItron: Foundational GUI Agent with Advanced Perception and Planning",
    "summary": "GUI agent aims to enable automated operations on Mobile/PC devices, which is\nan important task toward achieving artificial general intelligence. The rapid\nadvancement of VLMs accelerates the development of GUI agents, owing to their\npowerful capabilities in visual understanding and task planning. However,\nbuilding a GUI agent remains a challenging task due to the scarcity of\noperation trajectories, the availability of interactive infrastructure, and the\nlimitation of initial capabilities in foundation models. In this work, we\nintroduce UItron, an open-source foundational model for automatic GUI agents,\nfeaturing advanced GUI perception, grounding, and planning capabilities. UItron\nhighlights the necessity of systemic data engineering and interactive\ninfrastructure as foundational components for advancing GUI agent development.\nIt not only systematically studies a series of data engineering strategies to\nenhance training effects, but also establishes an interactive environment\nconnecting both Mobile and PC devices. In training, UItron adopts supervised\nfinetuning over perception and planning tasks in various GUI scenarios, and\nthen develop a curriculum reinforcement learning framework to enable complex\nreasoning and exploration for online environments. As a result, UItron achieves\nsuperior performance in benchmarks of GUI perception, grounding, and planning.\nIn particular, UItron highlights the interaction proficiency with top-tier\nChinese mobile APPs, as we identified a general lack of Chinese capabilities\neven in state-of-the-art solutions. To this end, we manually collect over one\nmillion steps of operation trajectories across the top 100 most popular apps,\nand build the offline and online agent evaluation environments. Experimental\nresults demonstrate that UItron achieves significant progress in Chinese app\nscenarios, propelling GUI agents one step closer to real-world application.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.21767.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 98
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.21456",
      "authors": [
        {
          "_id": "68b50fd6851c6e7b001eca48",
          "name": "Yi-Hao Peng",
          "hidden": false
        },
        {
          "_id": "68b50fd6851c6e7b001eca49",
          "name": "Dingzeyu Li",
          "hidden": false
        },
        {
          "_id": "68b50fd6851c6e7b001eca4a",
          "name": "Jeffrey P. Bigham",
          "hidden": false
        },
        {
          "_id": "68b50fd6851c6e7b001eca4b",
          "name": "Amy Pavel",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-29T09:39:00.000Z",
      "submittedOnDailyAt": "2025-09-01T01:45:43.503Z",
      "title": "Morae: Proactively Pausing UI Agents for User Choices",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "User interface (UI) agents promise to make inaccessible or complex UIs easier\nto access for blind and low-vision (BLV) users. However, current UI agents\ntypically perform tasks end-to-end without involving users in critical choices\nor making them aware of important contextual information, thus reducing user\nagency. For example, in our field study, a BLV participant asked to buy the\ncheapest available sparkling water, and the agent automatically chose one from\nseveral equally priced options, without mentioning alternative products with\ndifferent flavors or better ratings. To address this problem, we introduce\nMorae, a UI agent that automatically identifies decision points during task\nexecution and pauses so that users can make choices. Morae uses large\nmultimodal models to interpret user queries alongside UI code and screenshots,\nand prompt users for clarification when there is a choice to be made. In a\nstudy over real-world web tasks with BLV participants, Morae helped users\ncomplete more tasks and select options that better matched their preferences,\nas compared to baseline agents, including OpenAI Operator. More broadly, this\nwork exemplifies a mixed-initiative approach in which users benefit from the\nautomation of UI agents while being able to express their preferences.",
      "upvotes": 1,
      "discussionId": "68b50fd6851c6e7b001eca4c",
      "ai_summary": "Morae, a UI agent, enhances accessibility for BLV users by involving them in decision-making processes during task execution, using large multimodal models to interpret user queries and UI elements.",
      "ai_keywords": [
        "UI agents",
        "blind and low-vision (BLV) users",
        "decision points",
        "large multimodal models",
        "user queries",
        "UI code",
        "screenshots",
        "mixed-initiative approach"
      ]
    },
    "publishedAt": "2025-08-29T05:39:00.000Z",
    "title": "Morae: Proactively Pausing UI Agents for User Choices",
    "summary": "User interface (UI) agents promise to make inaccessible or complex UIs easier\nto access for blind and low-vision (BLV) users. However, current UI agents\ntypically perform tasks end-to-end without involving users in critical choices\nor making them aware of important contextual information, thus reducing user\nagency. For example, in our field study, a BLV participant asked to buy the\ncheapest available sparkling water, and the agent automatically chose one from\nseveral equally priced options, without mentioning alternative products with\ndifferent flavors or better ratings. To address this problem, we introduce\nMorae, a UI agent that automatically identifies decision points during task\nexecution and pauses so that users can make choices. Morae uses large\nmultimodal models to interpret user queries alongside UI code and screenshots,\nand prompt users for clarification when there is a choice to be made. In a\nstudy over real-world web tasks with BLV participants, Morae helped users\ncomplete more tasks and select options that better matched their preferences,\nas compared to baseline agents, including OpenAI Operator. More broadly, this\nwork exemplifies a mixed-initiative approach in which users benefit from the\nautomation of UI agents while being able to express their preferences.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.21456.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 98
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.21376",
      "authors": [
        {
          "_id": "68b51220851c6e7b001eca69",
          "name": "Tony Lee",
          "hidden": false
        },
        {
          "_id": "68b51220851c6e7b001eca6a",
          "name": "Haoqin Tu",
          "hidden": false
        },
        {
          "_id": "68b51220851c6e7b001eca6b",
          "name": "Chi Heem Wong",
          "hidden": false
        },
        {
          "_id": "68b51220851c6e7b001eca6c",
          "name": "Zijun Wang",
          "hidden": false
        },
        {
          "_id": "68b51220851c6e7b001eca6d",
          "name": "Siwei Yang",
          "hidden": false
        },
        {
          "_id": "68b51220851c6e7b001eca6e",
          "name": "Yifan Mai",
          "hidden": false
        },
        {
          "_id": "68b51220851c6e7b001eca6f",
          "name": "Yuyin Zhou",
          "hidden": false
        },
        {
          "_id": "68b51220851c6e7b001eca70",
          "name": "Cihang Xie",
          "hidden": false
        },
        {
          "_id": "68b51220851c6e7b001eca71",
          "name": "Percy Liang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-29T07:40:39.000Z",
      "submittedOnDailyAt": "2025-09-01T01:55:37.031Z",
      "title": "AHELM: A Holistic Evaluation of Audio-Language Models",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Evaluations of audio-language models (ALMs) -- multimodal models that take\ninterleaved audio and text as input and output text -- are hindered by the lack\nof standardized benchmarks; most benchmarks measure only one or two\ncapabilities and omit evaluative aspects such as fairness or safety.\nFurthermore, comparison across models is difficult as separate evaluations test\na limited number of models and use different prompting methods and inference\nparameters. To address these shortfalls, we introduce AHELM, a benchmark that\naggregates various datasets -- including 2 new synthetic audio-text datasets\ncalled PARADE, which evaluates the ALMs on avoiding stereotypes, and\nCoRe-Bench, which measures reasoning over conversational audio through\ninferential multi-turn question answering -- to holistically measure the\nperformance of ALMs across 10 aspects we have identified as important to the\ndevelopment and usage of ALMs: audio perception, knowledge, reasoning, emotion\ndetection, bias, fairness, multilinguality, robustness, toxicity, and safety.\nWe also standardize the prompts, inference parameters, and evaluation metrics\nto ensure equitable comparisons across models. We test 14 open-weight and\nclosed-API ALMs from 3 developers and 3 additional simple baseline systems each\nconsisting of an automatic speech recognizer and a language model. Our results\nshow that while Gemini 2.5 Pro ranks top in 5 out of 10 aspects, it exhibits\ngroup unfairness (p=0.01) on ASR tasks whereas most of the other models do\nnot. We also find that the baseline systems perform reasonably well on AHELM,\nwith one ranking 5th overall despite having only speech-to-text capabilities.\nFor transparency, all raw prompts, model generations, and outputs are available\non our website at https://crfm.stanford.edu/helm/audio/v1.0.0. AHELM is\nintended to be a living benchmark and new datasets and models will be added\nover time.",
      "upvotes": 1,
      "discussionId": "68b51221851c6e7b001eca72",
      "projectPage": "https://crfm.stanford.edu/helm/audio/v1.0.0/",
      "ai_summary": "AHELM is a comprehensive benchmark for audio-language models that evaluates multiple aspects including fairness, safety, and reasoning across various datasets and models.",
      "ai_keywords": [
        "audio-language models",
        "multimodal models",
        "standardized benchmarks",
        "audio perception",
        "knowledge",
        "reasoning",
        "emotion detection",
        "bias",
        "fairness",
        "multilinguality",
        "robustness",
        "toxicity",
        "safety",
        "AHELM",
        "PARADE",
        "CoRe-Bench",
        "automatic speech recognizer",
        "language model",
        "Gemini 2.5 Pro",
        "group unfairness"
      ]
    },
    "publishedAt": "2025-08-29T03:40:39.000Z",
    "title": "AHELM: A Holistic Evaluation of Audio-Language Models",
    "summary": "Evaluations of audio-language models (ALMs) -- multimodal models that take\ninterleaved audio and text as input and output text -- are hindered by the lack\nof standardized benchmarks; most benchmarks measure only one or two\ncapabilities and omit evaluative aspects such as fairness or safety.\nFurthermore, comparison across models is difficult as separate evaluations test\na limited number of models and use different prompting methods and inference\nparameters. To address these shortfalls, we introduce AHELM, a benchmark that\naggregates various datasets -- including 2 new synthetic audio-text datasets\ncalled PARADE, which evaluates the ALMs on avoiding stereotypes, and\nCoRe-Bench, which measures reasoning over conversational audio through\ninferential multi-turn question answering -- to holistically measure the\nperformance of ALMs across 10 aspects we have identified as important to the\ndevelopment and usage of ALMs: audio perception, knowledge, reasoning, emotion\ndetection, bias, fairness, multilinguality, robustness, toxicity, and safety.\nWe also standardize the prompts, inference parameters, and evaluation metrics\nto ensure equitable comparisons across models. We test 14 open-weight and\nclosed-API ALMs from 3 developers and 3 additional simple baseline systems each\nconsisting of an automatic speech recognizer and a language model. Our results\nshow that while Gemini 2.5 Pro ranks top in 5 out of 10 aspects, it exhibits\ngroup unfairness (p=0.01) on ASR tasks whereas most of the other models do\nnot. We also find that the baseline systems perform reasonably well on AHELM,\nwith one ranking 5th overall despite having only speech-to-text capabilities.\nFor transparency, all raw prompts, model generations, and outputs are available\non our website at https://crfm.stanford.edu/helm/audio/v1.0.0. AHELM is\nintended to be a living benchmark and new datasets and models will be added\nover time.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.21376.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 98
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.21365",
      "authors": [
        {
          "_id": "68b51815851c6e7b001eca9f",
          "name": "Yi Liao",
          "hidden": false
        },
        {
          "_id": "68b51815851c6e7b001ecaa0",
          "name": "Yu Gu",
          "hidden": false
        },
        {
          "_id": "68b51815851c6e7b001ecaa1",
          "name": "Yuan Sui",
          "hidden": false
        },
        {
          "_id": "68b51815851c6e7b001ecaa2",
          "name": "Zining Zhu",
          "hidden": false
        },
        {
          "_id": "68b51815851c6e7b001ecaa3",
          "name": "Yifan Lu",
          "hidden": false
        },
        {
          "_id": "68b51815851c6e7b001ecaa4",
          "name": "Guohua Tang",
          "hidden": false
        },
        {
          "_id": "68b51815851c6e7b001ecaa5",
          "name": "Zhongqian Sun",
          "hidden": false
        },
        {
          "_id": "68b51815851c6e7b001ecaa6",
          "name": "Wei Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-29T07:13:39.000Z",
      "submittedOnDailyAt": "2025-09-01T02:20:52.079Z",
      "title": "Think in Games: Learning to Reason in Games via Reinforcement Learning\n  with Large Language Models",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Large language models (LLMs) excel at complex reasoning tasks such as\nmathematics and coding, yet they frequently struggle with simple interactive\ntasks that young children perform effortlessly. This discrepancy highlights a\ncritical gap between declarative knowledge (knowing about something) and\nprocedural knowledge (knowing how to do something). Although traditional\nreinforcement learning (RL) agents can acquire procedural knowledge through\nenvironmental interaction, they often operate as black boxes and require\nsubstantial training data. In contrast, LLMs possess extensive world knowledge\nand reasoning capabilities, but are unable to effectively convert this static\nknowledge into dynamic decision-making in interactive settings. To address this\nchallenge, we propose Think in Games (TiG), a novel framework that empowers\nLLMs to develop procedural understanding through direct interaction with game\nenvironments, while retaining their inherent reasoning and explanatory\nabilities. Specifically, TiG reformulates RL-based decision-making as a\nlanguage modeling task: LLMs generate language-guided policies, which are\nrefined iteratively through online reinforcement learning based on\nenvironmental feedback. Our experimental results show that TiG successfully\nbridges the gap between declarative and procedural knowledge, achieving\ncompetitive performance with dramatically lower data and computational demands\ncompared to conventional RL methods. Moreover, TiG provides step-by-step\nnatural language explanations for its decisions, greatly improving transparency\nand interpretability in complex interactive tasks.",
      "upvotes": 1,
      "discussionId": "68b51815851c6e7b001ecaa7",
      "ai_summary": "Think in Games (TiG) framework enables large language models to develop procedural knowledge through interactive game environments, achieving competitive performance with reduced data and computational demands while providing transparent explanations.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "complex reasoning tasks",
        "declarative knowledge",
        "procedural knowledge",
        "reinforcement learning",
        "RL",
        "environmental interaction",
        "black boxes",
        "training data",
        "world knowledge",
        "reasoning capabilities",
        "language modeling task",
        "language-guided policies",
        "online reinforcement learning",
        "environmental feedback",
        "competitive performance",
        "data demands",
        "computational demands",
        "step-by-step natural language explanations",
        "transparency",
        "interpretability"
      ]
    },
    "publishedAt": "2025-08-29T03:13:39.000Z",
    "title": "Think in Games: Learning to Reason in Games via Reinforcement Learning\n  with Large Language Models",
    "summary": "Large language models (LLMs) excel at complex reasoning tasks such as\nmathematics and coding, yet they frequently struggle with simple interactive\ntasks that young children perform effortlessly. This discrepancy highlights a\ncritical gap between declarative knowledge (knowing about something) and\nprocedural knowledge (knowing how to do something). Although traditional\nreinforcement learning (RL) agents can acquire procedural knowledge through\nenvironmental interaction, they often operate as black boxes and require\nsubstantial training data. In contrast, LLMs possess extensive world knowledge\nand reasoning capabilities, but are unable to effectively convert this static\nknowledge into dynamic decision-making in interactive settings. To address this\nchallenge, we propose Think in Games (TiG), a novel framework that empowers\nLLMs to develop procedural understanding through direct interaction with game\nenvironments, while retaining their inherent reasoning and explanatory\nabilities. Specifically, TiG reformulates RL-based decision-making as a\nlanguage modeling task: LLMs generate language-guided policies, which are\nrefined iteratively through online reinforcement learning based on\nenvironmental feedback. Our experimental results show that TiG successfully\nbridges the gap between declarative and procedural knowledge, achieving\ncompetitive performance with dramatically lower data and computational demands\ncompared to conventional RL methods. Moreover, TiG provides step-by-step\nnatural language explanations for its decisions, greatly improving transparency\nand interpretability in complex interactive tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.21365.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 98
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.20085",
      "authors": [
        {
          "_id": "68b523c6851c6e7b001ecab0",
          "name": "Zhecheng Yuan",
          "hidden": false
        },
        {
          "_id": "68b523c6851c6e7b001ecab1",
          "name": "Tianming Wei",
          "hidden": false
        },
        {
          "_id": "68b523c6851c6e7b001ecab2",
          "name": "Langzhe Gu",
          "hidden": false
        },
        {
          "_id": "68b523c6851c6e7b001ecab3",
          "name": "Pu Hua",
          "hidden": false
        },
        {
          "_id": "68b523c6851c6e7b001ecab4",
          "name": "Tianhai Liang",
          "hidden": false
        },
        {
          "_id": "68b523c6851c6e7b001ecab5",
          "name": "Yuanpei Chen",
          "hidden": false
        },
        {
          "_id": "68b523c6851c6e7b001ecab6",
          "name": "Huazhe Xu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-27T17:53:46.000Z",
      "submittedOnDailyAt": "2025-09-01T03:14:52.379Z",
      "title": "HERMES: Human-to-Robot Embodied Learning from Multi-Source Motion Data\n  for Mobile Dexterous Manipulation",
      "submittedOnDailyBy": {
        "_id": "64a0dd823886663ff8455f7a",
        "avatarUrl": "/avatars/a8560f9c9de59cbab4ef5ed9494e4051.svg",
        "isPro": false,
        "fullname": "Zhecheng Yuan",
        "user": "gemcollector",
        "type": "user"
      },
      "summary": "Leveraging human motion data to impart robots with versatile manipulation\nskills has emerged as a promising paradigm in robotic manipulation.\nNevertheless, translating multi-source human hand motions into feasible robot\nbehaviors remains challenging, particularly for robots equipped with\nmulti-fingered dexterous hands characterized by complex, high-dimensional\naction spaces. Moreover, existing approaches often struggle to produce policies\ncapable of adapting to diverse environmental conditions. In this paper, we\nintroduce HERMES, a human-to-robot learning framework for mobile bimanual\ndexterous manipulation. First, HERMES formulates a unified reinforcement\nlearning approach capable of seamlessly transforming heterogeneous human hand\nmotions from multiple sources into physically plausible robotic behaviors.\nSubsequently, to mitigate the sim2real gap, we devise an end-to-end, depth\nimage-based sim2real transfer method for improved generalization to real-world\nscenarios. Furthermore, to enable autonomous operation in varied and\nunstructured environments, we augment the navigation foundation model with a\nclosed-loop Perspective-n-Point (PnP) localization mechanism, ensuring precise\nalignment of visual goals and effectively bridging autonomous navigation and\ndexterous manipulation. Extensive experimental results demonstrate that HERMES\nconsistently exhibits generalizable behaviors across diverse, in-the-wild\nscenarios, successfully performing numerous complex mobile bimanual dexterous\nmanipulation tasks. Project Page:https://gemcollector.github.io/HERMES/.",
      "upvotes": 1,
      "discussionId": "68b523c6851c6e7b001ecab7",
      "projectPage": "https://gemcollector.github.io/HERMES/",
      "ai_summary": "HERMES is a human-to-robot learning framework that translates human hand motions into robotic behaviors, using reinforcement learning and sim2real transfer for versatile manipulation in diverse environments.",
      "ai_keywords": [
        "reinforcement learning",
        "sim2real transfer",
        "Perspective-n-Point (PnP) localization",
        "mobile bimanual dexterous manipulation"
      ]
    },
    "publishedAt": "2025-08-27T13:53:46.000Z",
    "title": "HERMES: Human-to-Robot Embodied Learning from Multi-Source Motion Data\n  for Mobile Dexterous Manipulation",
    "summary": "Leveraging human motion data to impart robots with versatile manipulation\nskills has emerged as a promising paradigm in robotic manipulation.\nNevertheless, translating multi-source human hand motions into feasible robot\nbehaviors remains challenging, particularly for robots equipped with\nmulti-fingered dexterous hands characterized by complex, high-dimensional\naction spaces. Moreover, existing approaches often struggle to produce policies\ncapable of adapting to diverse environmental conditions. In this paper, we\nintroduce HERMES, a human-to-robot learning framework for mobile bimanual\ndexterous manipulation. First, HERMES formulates a unified reinforcement\nlearning approach capable of seamlessly transforming heterogeneous human hand\nmotions from multiple sources into physically plausible robotic behaviors.\nSubsequently, to mitigate the sim2real gap, we devise an end-to-end, depth\nimage-based sim2real transfer method for improved generalization to real-world\nscenarios. Furthermore, to enable autonomous operation in varied and\nunstructured environments, we augment the navigation foundation model with a\nclosed-loop Perspective-n-Point (PnP) localization mechanism, ensuring precise\nalignment of visual goals and effectively bridging autonomous navigation and\ndexterous manipulation. Extensive experimental results demonstrate that HERMES\nconsistently exhibits generalizable behaviors across diverse, in-the-wild\nscenarios, successfully performing numerous complex mobile bimanual dexterous\nmanipulation tasks. Project Page:https://gemcollector.github.io/HERMES/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.20085.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a0dd823886663ff8455f7a",
      "avatarUrl": "/avatars/a8560f9c9de59cbab4ef5ed9494e4051.svg",
      "fullname": "Zhecheng Yuan",
      "name": "gemcollector",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.14197",
      "authors": [
        {
          "_id": "68b53dcf851c6e7b001ecad7",
          "name": "Tinghan Yang",
          "hidden": false
        },
        {
          "_id": "68b53dcf851c6e7b001ecad8",
          "name": "Md Ashiqur Rahman",
          "hidden": false
        },
        {
          "_id": "68b53dcf851c6e7b001ecad9",
          "name": "Raymond A. Yeh",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-19T18:43:14.000Z",
      "submittedOnDailyAt": "2025-09-01T05:03:51.327Z",
      "title": "CLIPSym: Delving into Symmetry Detection with CLIP",
      "submittedOnDailyBy": {
        "_id": "661e07e02a8496916011c08a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/6vIkL4LJ0bLeJL99E20F_.jpeg",
        "isPro": false,
        "fullname": "Md Ashiqur Rahman",
        "user": "ashiq24",
        "type": "user"
      },
      "summary": "Symmetry is one of the most fundamental geometric cues in computer vision,\nand detecting it has been an ongoing challenge. With the recent advances in\nvision-language models,~i.e., CLIP, we investigate whether a pre-trained CLIP\nmodel can aid symmetry detection by leveraging the additional symmetry cues\nfound in the natural image descriptions. We propose CLIPSym, which leverages\nCLIP's image and language encoders and a rotation-equivariant decoder based on\na hybrid of Transformer and G-Convolution to detect rotation and reflection\nsymmetries. To fully utilize CLIP's language encoder, we have developed a novel\nprompting technique called Semantic-Aware Prompt Grouping (SAPG), which\naggregates a diverse set of frequent object-based prompts to better integrate\nthe semantic cues for symmetry detection. Empirically, we show that CLIPSym\noutperforms the current state-of-the-art on three standard symmetry detection\ndatasets (DENDI, SDRW, and LDRS). Finally, we conduct detailed ablations\nverifying the benefits of CLIP's pre-training, the proposed equivariant\ndecoder, and the SAPG technique. The code is available at\nhttps://github.com/timyoung2333/CLIPSym.",
      "upvotes": 0,
      "discussionId": "68b53dcf851c6e7b001ecada",
      "ai_summary": "CLIPSym, a vision-language model using CLIP, enhances symmetry detection through a rotation-equivariant decoder and semantic-aware prompting, outperforming existing methods on standard datasets.",
      "ai_keywords": [
        "CLIP",
        "CLIPSym",
        "rotation-equivariant decoder",
        "Transformer",
        "G-Convolution",
        "Semantic-Aware Prompt Grouping",
        "SAPG",
        "symmetry detection",
        "DENDI",
        "SDRW",
        "LDRS"
      ]
    },
    "publishedAt": "2025-08-19T14:43:14.000Z",
    "title": "CLIPSym: Delving into Symmetry Detection with CLIP",
    "summary": "Symmetry is one of the most fundamental geometric cues in computer vision,\nand detecting it has been an ongoing challenge. With the recent advances in\nvision-language models,~i.e., CLIP, we investigate whether a pre-trained CLIP\nmodel can aid symmetry detection by leveraging the additional symmetry cues\nfound in the natural image descriptions. We propose CLIPSym, which leverages\nCLIP's image and language encoders and a rotation-equivariant decoder based on\na hybrid of Transformer and G-Convolution to detect rotation and reflection\nsymmetries. To fully utilize CLIP's language encoder, we have developed a novel\nprompting technique called Semantic-Aware Prompt Grouping (SAPG), which\naggregates a diverse set of frequent object-based prompts to better integrate\nthe semantic cues for symmetry detection. Empirically, we show that CLIPSym\noutperforms the current state-of-the-art on three standard symmetry detection\ndatasets (DENDI, SDRW, and LDRS). Finally, we conduct detailed ablations\nverifying the benefits of CLIP's pre-training, the proposed equivariant\ndecoder, and the SAPG technique. The code is available at\nhttps://github.com/timyoung2333/CLIPSym.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.14197.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "661e07e02a8496916011c08a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/6vIkL4LJ0bLeJL99E20F_.jpeg",
      "fullname": "Md Ashiqur Rahman",
      "name": "ashiq24",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  }
]