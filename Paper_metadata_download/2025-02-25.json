[
  {
    "paper": {
      "id": "2502.17157",
      "authors": [
        {
          "_id": "67bd3285ac4a596a43b53205",
          "name": "Canyu Zhao",
          "hidden": false
        },
        {
          "_id": "67bd3285ac4a596a43b53206",
          "name": "Mingyu Liu",
          "hidden": false
        },
        {
          "_id": "67bd3285ac4a596a43b53207",
          "name": "Huanyi Zheng",
          "hidden": false
        },
        {
          "_id": "67bd3285ac4a596a43b53208",
          "name": "Muzhi Zhu",
          "hidden": false
        },
        {
          "_id": "67bd3285ac4a596a43b53209",
          "name": "Zhiyue Zhao",
          "hidden": false
        },
        {
          "_id": "67bd3285ac4a596a43b5320a",
          "name": "Hao Chen",
          "hidden": false
        },
        {
          "_id": "67bd3285ac4a596a43b5320b",
          "name": "Tong He",
          "hidden": false
        },
        {
          "_id": "67bd3285ac4a596a43b5320c",
          "name": "Chunhua Shen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-24T13:51:06.000Z",
      "title": "DICEPTION: A Generalist Diffusion Model for Visual Perceptual Tasks",
      "summary": "Our primary goal here is to create a good, generalist perception model that\ncan tackle multiple tasks, within limits on computational resources and\ntraining data. To achieve this, we resort to text-to-image diffusion models\npre-trained on billions of images. Our exhaustive evaluation metrics\ndemonstrate that DICEPTION effectively tackles multiple perception tasks,\nachieving performance on par with state-of-the-art models. We achieve results\non par with SAM-vit-h using only 0.06% of their data (e.g., 600K vs. 1B\npixel-level annotated images). Inspired by Wang et al., DICEPTION formulates\nthe outputs of various perception tasks using color encoding; and we show that\nthe strategy of assigning random colors to different instances is highly\neffective in both entity segmentation and semantic segmentation. Unifying\nvarious perception tasks as conditional image generation enables us to fully\nleverage pre-trained text-to-image models. Thus, DICEPTION can be efficiently\ntrained at a cost of orders of magnitude lower, compared to conventional models\nthat were trained from scratch. When adapting our model to other tasks, it only\nrequires fine-tuning on as few as 50 images and 1% of its parameters. DICEPTION\nprovides valuable insights and a more promising solution for visual generalist\nmodels.",
      "upvotes": 30,
      "discussionId": "67bd328aac4a596a43b532ae"
    },
    "publishedAt": "2025-02-24T22:39:29.837Z",
    "title": "DICEPTION: A Generalist Diffusion Model for Visual Perceptual Tasks",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17157.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "646efd223dd912a539e0bd46",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/EOFAv5xvOgJOzuDgh4nSb.png",
      "fullname": "Canyu Zhao",
      "name": "Canyu",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.17129",
      "authors": [
        {
          "_id": "67bd37cb0d41e01cca99aa8b",
          "name": "Xiaoran Liu",
          "hidden": false
        },
        {
          "_id": "67bd37cb0d41e01cca99aa8c",
          "name": "Ruixiao Li",
          "hidden": false
        },
        {
          "_id": "67bd37cb0d41e01cca99aa8d",
          "name": "Mianqiu Huang",
          "hidden": false
        },
        {
          "_id": "67bd37cb0d41e01cca99aa8e",
          "name": "Zhigeng Liu",
          "hidden": false
        },
        {
          "_id": "67bd37cb0d41e01cca99aa8f",
          "name": "Yuerong Song",
          "hidden": false
        },
        {
          "_id": "67bd37cb0d41e01cca99aa90",
          "name": "Qipeng Guo",
          "hidden": false
        },
        {
          "_id": "67bd37cb0d41e01cca99aa91",
          "name": "Siyang He",
          "hidden": false
        },
        {
          "_id": "67bd37cb0d41e01cca99aa92",
          "name": "Qiqi Wang",
          "hidden": false
        },
        {
          "_id": "67bd37cb0d41e01cca99aa93",
          "name": "Linlin Li",
          "hidden": false
        },
        {
          "_id": "67bd37cb0d41e01cca99aa94",
          "name": "Qun Liu",
          "hidden": false
        },
        {
          "_id": "67bd37cb0d41e01cca99aa95",
          "name": "Yaqian Zhou",
          "hidden": false
        },
        {
          "_id": "67bd37cb0d41e01cca99aa96",
          "name": "Xuanjing Huang",
          "hidden": false
        },
        {
          "_id": "67bd37cb0d41e01cca99aa97",
          "name": "Xipeng Qiu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-24T13:19:33.000Z",
      "title": "Thus Spake Long-Context Large Language Model",
      "summary": "Long context is an important topic in Natural Language Processing (NLP),\nrunning through the development of NLP architectures, and offers immense\nopportunities for Large Language Models (LLMs) giving LLMs the lifelong\nlearning potential akin to humans. Unfortunately, the pursuit of a long context\nis accompanied by numerous obstacles. Nevertheless, long context remains a core\ncompetitive advantage for LLMs. In the past two years, the context length of\nLLMs has achieved a breakthrough extension to millions of tokens. Moreover, the\nresearch on long-context LLMs has expanded from length extrapolation to a\ncomprehensive focus on architecture, infrastructure, training, and evaluation\ntechnologies.\n  Inspired by the symphonic poem, Thus Spake Zarathustra, we draw an analogy\nbetween the journey of extending the context of LLM and the attempts of humans\nto transcend its mortality. In this survey, We will illustrate how LLM\nstruggles between the tremendous need for a longer context and its equal need\nto accept the fact that it is ultimately finite. To achieve this, we give a\nglobal picture of the lifecycle of long-context LLMs from four perspectives:\narchitecture, infrastructure, training, and evaluation, showcasing the full\nspectrum of long-context technologies. At the end of this survey, we will\npresent 10 unanswered questions currently faced by long-context LLMs. We hope\nthis survey can serve as a systematic introduction to the research on\nlong-context LLMs.",
      "upvotes": 20,
      "discussionId": "67bd37cc0d41e01cca99ab1e"
    },
    "publishedAt": "2025-02-24T22:27:11.566Z",
    "title": "Thus Spake Long-Context Large Language Model",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17129.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64f033ef82c6eea604c4da8b",
      "avatarUrl": "/avatars/51b93fea7fd68b4274ee03701245dcca.svg",
      "fullname": "Liu Xiaoran",
      "name": "LiuXR",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.17435",
      "authors": [
        {
          "_id": "67bd6b4b8edd1ce8ad5603a0",
          "name": "Chen-Wei Chang",
          "hidden": false
        },
        {
          "_id": "67bd6b4b8edd1ce8ad5603a1",
          "name": "Cheng-De Fan",
          "hidden": false
        },
        {
          "_id": "67bd6b4b8edd1ce8ad5603a2",
          "name": "Chia-Che Chang",
          "hidden": false
        },
        {
          "_id": "67bd6b4b8edd1ce8ad5603a3",
          "name": "Yi-Chen Lo",
          "hidden": false
        },
        {
          "_id": "67bd6b4b8edd1ce8ad5603a4",
          "name": "Yu-Chee Tseng",
          "hidden": false
        },
        {
          "_id": "67bd6b4b8edd1ce8ad5603a5",
          "name": "Jiun-Long Huang",
          "hidden": false
        },
        {
          "_id": "67bd6b4b8edd1ce8ad5603a6",
          "name": "Yu-Lun Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-24T18:59:54.000Z",
      "title": "GCC: Generative Color Constancy via Diffusing a Color Checker",
      "summary": "Color constancy methods often struggle to generalize across different camera\nsensors due to varying spectral sensitivities. We present GCC, which leverages\ndiffusion models to inpaint color checkers into images for illumination\nestimation. Our key innovations include (1) a single-step deterministic\ninference approach that inpaints color checkers reflecting scene illumination,\n(2) a Laplacian decomposition technique that preserves checker structure while\nallowing illumination-dependent color adaptation, and (3) a mask-based data\naugmentation strategy for handling imprecise color checker annotations. GCC\ndemonstrates superior robustness in cross-camera scenarios, achieving\nstate-of-the-art worst-25% error rates of 5.15{\\deg} and 4.32{\\deg} in\nbi-directional evaluations. These results highlight our method's stability and\ngeneralization capability across different camera characteristics without\nrequiring sensor-specific training, making it a versatile solution for\nreal-world applications.",
      "upvotes": 13,
      "discussionId": "67bd6b4d8edd1ce8ad560401"
    },
    "publishedAt": "2025-02-25T02:06:00.809Z",
    "title": "GCC: Generative Color Constancy via Diffusing a Color Checker",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/gDAYQUcbNE2Ps2pQFxg_m.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17435.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6459d5da3b6fafd9664807ab",
      "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
      "fullname": "Yu-Lun Liu",
      "name": "yulunliu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.16584",
      "authors": [
        {
          "_id": "67bd42386959e61abd265a9b",
          "name": "Liumeng Xue",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265a9c",
          "name": "Ziya Zhou",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265a9d",
          "name": "Jiahao Pan",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265a9e",
          "name": "Zixuan Li",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265a9f",
          "name": "Shuai Fan",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aa0",
          "name": "Yinghao Ma",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aa1",
          "name": "Sitong Cheng",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aa2",
          "name": "Dongchao Yang",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aa3",
          "name": "Haohan Guo",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aa4",
          "name": "Yujia Xiao",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aa5",
          "name": "Xinsheng Wang",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aa6",
          "name": "Zixuan Shen",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aa7",
          "name": "Chuanbo Zhu",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aa8",
          "name": "Xinshen Zhang",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aa9",
          "name": "Tianchi Liu",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aaa",
          "name": "Ruibin Yuan",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aab",
          "name": "Zeyue Tian",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aac",
          "name": "Haohe Liu",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aad",
          "name": "Emmanouil Benetos",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aae",
          "name": "Ge Zhang",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aaf",
          "name": "Yike Guo",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265ab0",
          "name": "Wei Xue",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-23T14:24:15.000Z",
      "title": "Audio-FLAN: A Preliminary Release",
      "summary": "Recent advancements in audio tokenization have significantly enhanced the\nintegration of audio capabilities into large language models (LLMs). However,\naudio understanding and generation are often treated as distinct tasks,\nhindering the development of truly unified audio-language models. While\ninstruction tuning has demonstrated remarkable success in improving\ngeneralization and zero-shot learning across text and vision, its application\nto audio remains largely unexplored. A major obstacle is the lack of\ncomprehensive datasets that unify audio understanding and generation. To\naddress this, we introduce Audio-FLAN, a large-scale instruction-tuning dataset\ncovering 80 diverse tasks across speech, music, and sound domains, with over\n100 million instances. Audio-FLAN lays the foundation for unified\naudio-language models that can seamlessly handle both understanding (e.g.,\ntranscription, comprehension) and generation (e.g., speech, music, sound) tasks\nacross a wide range of audio domains in a zero-shot manner. The Audio-FLAN\ndataset is available on HuggingFace and GitHub and will be continuously\nupdated.",
      "upvotes": 13,
      "discussionId": "67bd423b6959e61abd265b88"
    },
    "publishedAt": "2025-02-24T23:14:20.487Z",
    "title": "Audio-FLAN: A Preliminary Release",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16584.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5fd6f670053c8345eddc1b68",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fd6f670053c8345eddc1b68/cuTsu2krRYHC6zYGD2dpQ.jpeg",
      "fullname": "Ruibin Yuan",
      "name": "a43992899",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 13
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.15814",
      "authors": [
        {
          "_id": "67bd3972f077ddf1f98bacda",
          "name": "Gallil Maimon",
          "hidden": false
        },
        {
          "_id": "67bd3972f077ddf1f98bacdb",
          "name": "Avishai Elmakies",
          "hidden": false
        },
        {
          "_id": "67bd3972f077ddf1f98bacdc",
          "name": "Yossi Adi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-19T17:21:15.000Z",
      "title": "Slamming: Training a Speech Language Model on One GPU in a Day",
      "summary": "We introduce Slam, a recipe for training high-quality Speech Language Models\n(SLMs) on a single academic GPU in 24 hours. We do so through empirical\nanalysis of model initialisation and architecture, synthetic training data,\npreference optimisation with synthetic data and tweaking all other components.\nWe empirically demonstrate that this training recipe also scales well with more\ncompute getting results on par with leading SLMs in a fraction of the compute\ncost. We hope these insights will make SLM training and research more\naccessible. In the context of SLM scaling laws, our results far outperform\npredicted compute optimal performance, giving an optimistic view to SLM\nfeasibility. See code, data, models, samples at -\nhttps://pages.cs.huji.ac.il/adiyoss-lab/slamming .",
      "upvotes": 10,
      "discussionId": "67bd3973f077ddf1f98bacf9"
    },
    "publishedAt": "2025-02-24T23:14:12.363Z",
    "title": "Slamming: Training a Speech Language Model on One GPU in a Day",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/66b9bc2dacdbc1d0b39c3b50/t93GkoiYRplnXH1Go0MmY.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15814.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66b9bc2dacdbc1d0b39c3b50",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/hwR0pVfP_E8XjimXIxDOU.jpeg",
      "fullname": "Gallil Maimon",
      "name": "gallilmaimon",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.16614",
      "authors": [
        {
          "_id": "67bd36334a9a04b9ca9bbb68",
          "name": "Alexander Zhang",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb69",
          "name": "Marcus Dong",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb6a",
          "name": "Jiaheng Liu",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb6b",
          "name": "Wei Zhang",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb6c",
          "name": "Yejie Wang",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb6d",
          "name": "Jian Yang",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb6e",
          "name": "Ge Zhang",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb6f",
          "name": "Tianyu Liu",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb70",
          "name": "Zhongyuan Peng",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb71",
          "name": "Yingshui Tan",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb72",
          "name": "Yuanxing Zhang",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb73",
          "name": "Zhexu Wang",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb74",
          "name": "Weixun Wang",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb75",
          "name": "Yancheng He",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb76",
          "name": "Ken Deng",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb77",
          "name": "Wangchunshu Zhou",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb78",
          "name": "Wenhao Huang",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb79",
          "name": "Zhaoxiang Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-23T15:36:43.000Z",
      "title": "CodeCriticBench: A Holistic Code Critique Benchmark for Large Language\n  Models",
      "summary": "The critique capacity of Large Language Models (LLMs) is essential for\nreasoning abilities, which can provide necessary suggestions (e.g., detailed\nanalysis and constructive feedback). Therefore, how to evaluate the critique\ncapacity of LLMs has drawn great attention and several critique benchmarks have\nbeen proposed. However, existing critique benchmarks usually have the following\nlimitations: (1). Focusing on diverse reasoning tasks in general domains and\ninsufficient evaluation on code tasks (e.g., only covering code generation\ntask), where the difficulty of queries is relatively easy (e.g., the code\nqueries of CriticBench are from Humaneval and MBPP). (2). Lacking comprehensive\nevaluation from different dimensions. To address these limitations, we\nintroduce a holistic code critique benchmark for LLMs called CodeCriticBench.\nSpecifically, our CodeCriticBench includes two mainstream code tasks (i.e.,\ncode generation and code QA) with different difficulties. Besides, the\nevaluation protocols include basic critique evaluation and advanced critique\nevaluation for different characteristics, where fine-grained evaluation\nchecklists are well-designed for advanced settings. Finally, we conduct\nextensive experimental results of existing LLMs, which show the effectiveness\nof CodeCriticBench.",
      "upvotes": 10,
      "discussionId": "67bd36354a9a04b9ca9bbc16"
    },
    "publishedAt": "2025-02-24T22:17:28.937Z",
    "title": "CodeCriticBench: A Holistic Code Critique Benchmark for Large Language Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16614.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65377c30e48353201e6fdda0",
      "avatarUrl": "/avatars/a8f803b6f2e598eaee9c52c0d2ddfc16.svg",
      "fullname": "Jiaheng Liu",
      "name": "CheeryLJH",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.16033",
      "authors": [
        {
          "_id": "67bd31d0d055a27740b16a30",
          "name": "Qianqi Yan",
          "hidden": false
        },
        {
          "_id": "67bd31d0d055a27740b16a31",
          "name": "Yue Fan",
          "hidden": false
        },
        {
          "_id": "67bd31d0d055a27740b16a32",
          "name": "Hongquan Li",
          "hidden": false
        },
        {
          "_id": "67bd31d0d055a27740b16a33",
          "name": "Shan Jiang",
          "hidden": false
        },
        {
          "_id": "67bd31d0d055a27740b16a34",
          "name": "Yang Zhao",
          "hidden": false
        },
        {
          "_id": "67bd31d0d055a27740b16a35",
          "name": "Xinze Guan",
          "hidden": false
        },
        {
          "_id": "67bd31d0d055a27740b16a36",
          "name": "Ching-Chen Kuo",
          "hidden": false
        },
        {
          "_id": "67bd31d0d055a27740b16a37",
          "name": "Xin Eric Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-22T01:52:37.000Z",
      "title": "Multimodal Inconsistency Reasoning (MMIR): A New Benchmark for\n  Multimodal Reasoning Models",
      "summary": "Existing Multimodal Large Language Models (MLLMs) are predominantly trained\nand tested on consistent visual-textual inputs, leaving open the question of\nwhether they can handle inconsistencies in real-world, layout-rich content. To\nbridge this gap, we propose the Multimodal Inconsistency Reasoning (MMIR)\nbenchmark to assess MLLMs' ability to detect and reason about semantic\nmismatches in artifacts such as webpages, presentation slides, and posters.\nMMIR comprises 534 challenging samples, each containing synthetically injected\nerrors across five reasoning-heavy categories: Factual Contradiction, Identity\nMisattribution, Contextual Mismatch, Quantitative Discrepancy, and\nTemporal/Spatial Incoherence. We evaluate six state-of-the-art MLLMs, showing\nthat models with dedicated multimodal reasoning capabilities, such as o1,\nsubstantially outperform their counterparts while open-source models remain\nparticularly vulnerable to inconsistency errors. Detailed error analyses\nfurther show that models excel in detecting inconsistencies confined to a\nsingle modality, particularly in text, but struggle with cross-modal conflicts\nand complex layouts. Probing experiments reveal that single-modality prompting,\nincluding Chain-of-Thought (CoT) and Set-of-Mark (SoM) methods, yields marginal\ngains, revealing a key bottleneck in cross-modal reasoning. Our findings\nhighlight the need for advanced multimodal reasoning and point to future\nresearch on multimodal inconsistency.",
      "upvotes": 9,
      "discussionId": "67bd31d2d055a27740b16ad9"
    },
    "publishedAt": "2025-02-24T21:59:50.456Z",
    "title": "Multimodal Inconsistency Reasoning (MMIR): A New Benchmark for Multimodal Reasoning Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16033.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64679a226192d39142245e5e",
      "avatarUrl": "/avatars/05abee0b6317f100923936ca2099e9eb.svg",
      "fullname": "Xin Eric Wang",
      "name": "xw-eric",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.16894",
      "authors": [
        {
          "_id": "67bd396ea06bae99f3866911",
          "name": "Chenghao Fan",
          "hidden": false
        },
        {
          "_id": "67bd396ea06bae99f3866912",
          "name": "Zhenyi Lu",
          "hidden": false
        },
        {
          "_id": "67bd396ea06bae99f3866913",
          "name": "Sichen Liu",
          "hidden": false
        },
        {
          "_id": "67bd396ea06bae99f3866914",
          "name": "Xiaoye Qu",
          "hidden": false
        },
        {
          "_id": "67bd396ea06bae99f3866915",
          "name": "Wei Wei",
          "hidden": false
        },
        {
          "_id": "67bd396ea06bae99f3866916",
          "name": "Chengfeng Gu",
          "hidden": false
        },
        {
          "_id": "67bd396ea06bae99f3866917",
          "name": "Yu Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-24T06:48:13.000Z",
      "title": "Make LoRA Great Again: Boosting LoRA with Adaptive Singular Values and\n  Mixture-of-Experts Optimization Alignment",
      "summary": "While Low-Rank Adaptation (LoRA) enables parameter-efficient fine-tuning for\nLarge Language Models (LLMs), its performance often falls short of Full\nFine-Tuning (Full FT). Current methods optimize LoRA by initializing with\nstatic singular value decomposition (SVD) subsets, leading to suboptimal\nleveraging of pre-trained knowledge. Another path for improving LoRA is\nincorporating a Mixture-of-Experts (MoE) architecture. However, weight\nmisalignment and complex gradient dynamics make it challenging to adopt SVD\nprior to the LoRA MoE architecture. To mitigate these issues, we propose\nGreat LoRA Mixture-of-Expert\n(GOAT), a framework that (1) adaptively integrates relevant priors using an\nSVD-structured MoE, and (2) aligns optimization with full fine-tuned MoE by\nderiving a theoretical scaling factor. We demonstrate that proper scaling,\nwithout modifying the architecture or training algorithms, boosts LoRA MoE's\nefficiency and performance. Experiments across 25 datasets, including natural\nlanguage understanding, commonsense reasoning, image classification, and\nnatural language generation, demonstrate GOAT's state-of-the-art performance,\nclosing the gap with Full FT.",
      "upvotes": 8,
      "discussionId": "67bd396fa06bae99f3866964"
    },
    "publishedAt": "2025-02-24T22:35:41.042Z",
    "title": "Make LoRA Great Again: Boosting LoRA with Adaptive Singular Values and Mixture-of-Experts Optimization Alignment",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16894.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "641aa5e391e3376a057bbd4c",
      "avatarUrl": "/avatars/5818797f27444fde078b503774ee081c.svg",
      "fullname": "Chenghao Fan",
      "name": "Facico",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.17407",
      "authors": [
        {
          "_id": "67bd48d4becb766415a5d19d",
          "name": "Guijin Son",
          "hidden": false
        },
        {
          "_id": "67bd48d4becb766415a5d19e",
          "name": "Jiwoo Hong",
          "hidden": false
        },
        {
          "_id": "67bd48d4becb766415a5d19f",
          "name": "Hyunwoo Ko",
          "hidden": false
        },
        {
          "_id": "67bd48d4becb766415a5d1a0",
          "name": "James Thorne",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-24T18:36:15.000Z",
      "title": "Linguistic Generalizability of Test-Time Scaling in Mathematical\n  Reasoning",
      "summary": "Scaling pre-training compute has proven effective for achieving\nmulitlinguality, but does the same hold for test-time scaling? In this work, we\nintroduce MCLM, a multilingual math benchmark featuring competition-level\nproblems in 55 languages. We test three test-time scaling methods-Outcome\nReward Modeling (ORM), Process Reward Modeling (ORM), and Budget Forcing\n(BF)-on both Qwen2.5-1.5B Math and MR1-1.5B, a multilingual LLM we trained for\nextended reasoning. Our experiments show that using Qwen2.5-1.5B Math with ORM\nachieves a score of 35.8 on MCLM, while BF on MR1-1.5B attains 35.2. Although\n\"thinking LLMs\" have recently garnered significant attention, we find that\ntheir performance is comparable to traditional scaling methods like best-of-N\nonce constrained to similar levels of inference FLOPs. Moreover, while BF\nyields a 20-point improvement on English AIME, it provides only a 1.94-point\naverage gain across other languages-a pattern consistent across the other\ntest-time scaling methods we studied-higlighting that test-time scaling may not\ngeneralize as effectively to multilingual tasks. To foster further research, we\nrelease MCLM, MR1-1.5B, and evaluation results.",
      "upvotes": 7,
      "discussionId": "67bd48d5becb766415a5d1e9"
    },
    "publishedAt": "2025-02-24T23:37:53.138Z",
    "title": "Linguistic Generalizability of Test-Time Scaling in Mathematical Reasoning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17407.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60d3e619b8448e1785bbda2a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60d3e619b8448e1785bbda2a/q2re5u1HNwsCCyIMtid_I.jpeg",
      "fullname": "GUIJIN SON",
      "name": "amphora",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 43
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.16922",
      "authors": [
        {
          "_id": "67bd3d6b60186d7478467208",
          "name": "Zhenglin Wang",
          "hidden": false
        },
        {
          "_id": "67bd3d6b60186d7478467209",
          "name": "Jialong Wu",
          "hidden": false
        },
        {
          "_id": "67bd3d6b60186d747846720a",
          "name": "Pengfei LI",
          "hidden": false
        },
        {
          "_id": "67bd3d6b60186d747846720b",
          "name": "Yong Jiang",
          "hidden": false
        },
        {
          "_id": "67bd3d6b60186d747846720c",
          "name": "Deyu Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-24T07:27:54.000Z",
      "title": "Benchmarking Temporal Reasoning and Alignment Across Chinese Dynasties",
      "summary": "Temporal reasoning is fundamental to human cognition and is crucial for\nvarious real-world applications. While recent advances in Large Language Models\nhave demonstrated promising capabilities in temporal reasoning, existing\nbenchmarks primarily rely on rule-based construction, lack contextual depth,\nand involve a limited range of temporal entities. To address these limitations,\nwe introduce Chinese Time Reasoning (CTM), a benchmark designed to evaluate\nLLMs on temporal reasoning within the extensive scope of Chinese dynastic\nchronology. CTM emphasizes cross-entity relationships, pairwise temporal\nalignment, and contextualized and culturally-grounded reasoning, providing a\ncomprehensive evaluation. Extensive experimental results reveal the challenges\nposed by CTM and highlight potential avenues for improvement.",
      "upvotes": 7,
      "discussionId": "67bd3d6c60186d7478467249"
    },
    "publishedAt": "2025-02-24T22:48:30.357Z",
    "title": "Benchmarking Temporal Reasoning and Alignment Across Chinese Dynasties",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16922.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "644a4fbc2166258fccc664bc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
      "fullname": "Jialong Wu",
      "name": "callanwu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.17110",
      "authors": [
        {
          "_id": "67bd3936daef22cbce6d7ef2",
          "name": "Junyang Wang",
          "hidden": false
        },
        {
          "_id": "67bd3936daef22cbce6d7ef3",
          "name": "Haiyang Xu",
          "hidden": false
        },
        {
          "_id": "67bd3936daef22cbce6d7ef4",
          "name": "Xi Zhang",
          "hidden": false
        },
        {
          "_id": "67bd3936daef22cbce6d7ef5",
          "name": "Ming Yan",
          "hidden": false
        },
        {
          "_id": "67bd3936daef22cbce6d7ef6",
          "name": "Ji Zhang",
          "hidden": false
        },
        {
          "_id": "67bd3936daef22cbce6d7ef7",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "67bd3936daef22cbce6d7ef8",
          "name": "Jitao Sang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-24T12:51:23.000Z",
      "title": "Mobile-Agent-V: Learning Mobile Device Operation Through Video-Guided\n  Multi-Agent Collaboration",
      "summary": "The rapid increase in mobile device usage necessitates improved automation\nfor seamless task management. However, many AI-driven frameworks struggle due\nto insufficient operational knowledge. Manually written knowledge helps but is\nlabor-intensive and inefficient. To address these challenges, we introduce\nMobile-Agent-V, a framework that leverages video guidance to provide rich and\ncost-effective operational knowledge for mobile automation. Mobile-Agent-V\nenhances task execution capabilities by leveraging video inputs without\nrequiring specialized sampling or preprocessing. Mobile-Agent-V integrates a\nsliding window strategy and incorporates a video agent and deep-reflection\nagent to ensure that actions align with user instructions. Through this\ninnovative approach, users can record task processes with guidance, enabling\nthe system to autonomously learn and execute tasks efficiently. Experimental\nresults show that Mobile-Agent-V achieves a 30% performance improvement\ncompared to existing frameworks.",
      "upvotes": 6,
      "discussionId": "67bd3938daef22cbce6d7f9d"
    },
    "publishedAt": "2025-02-24T22:31:17.771Z",
    "title": "Mobile-Agent-V: Learning Mobile Device Operation Through Video-Guided Multi-Agent Collaboration",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/645b10e80c73ea27d13f7aca/mshxtP77rrnN07f6ux6_0.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17110.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645b10e80c73ea27d13f7aca",
      "avatarUrl": "/avatars/95e565306472a15067440b5b43e07a6f.svg",
      "fullname": "xuhaiyang",
      "name": "xhyandwyy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.15894",
      "authors": [
        {
          "_id": "67bd3bd26faf9f04b2170f61",
          "name": "Min Zhao",
          "hidden": false
        },
        {
          "_id": "67bd3bd26faf9f04b2170f62",
          "name": "Guande He",
          "hidden": false
        },
        {
          "_id": "67bd3bd26faf9f04b2170f63",
          "name": "Yixiao Chen",
          "hidden": false
        },
        {
          "_id": "67bd3bd26faf9f04b2170f64",
          "name": "Hongzhou Zhu",
          "hidden": false
        },
        {
          "_id": "67bd3bd26faf9f04b2170f65",
          "name": "Chongxuan Li",
          "hidden": false
        },
        {
          "_id": "67bd3bd26faf9f04b2170f66",
          "name": "Jun Zhu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-21T19:28:05.000Z",
      "title": "RIFLEx: A Free Lunch for Length Extrapolation in Video Diffusion\n  Transformers",
      "summary": "Recent advancements in video generation have enabled models to synthesize\nhigh-quality, minute-long videos. However, generating even longer videos with\ntemporal coherence remains a major challenge, and existing length extrapolation\nmethods lead to temporal repetition or motion deceleration. In this work, we\nsystematically analyze the role of frequency components in positional\nembeddings and identify an intrinsic frequency that primarily governs\nextrapolation behavior. Based on this insight, we propose RIFLEx, a minimal yet\neffective approach that reduces the intrinsic frequency to suppress repetition\nwhile preserving motion consistency, without requiring any additional\nmodifications. RIFLEx offers a true free lunch--achieving high-quality\n2times extrapolation on state-of-the-art video diffusion transformers in a\ncompletely training-free manner. Moreover, it enhances quality and enables\n3times extrapolation by minimal fine-tuning without long videos. Project\npage and codes:\nhttps://riflex-video.github.io/{https://riflex-video.github.io/.}",
      "upvotes": 4,
      "discussionId": "67bd3bd66faf9f04b21710d1"
    },
    "publishedAt": "2025-02-25T00:09:04.483Z",
    "title": "RIFLEx: A Free Lunch for Length Extrapolation in Video Diffusion Transformers",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15894.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6204
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.16707",
      "authors": [
        {
          "_id": "67bd3bcc797e4d53ce0bc70d",
          "name": "Yunhai Feng",
          "hidden": false
        },
        {
          "_id": "67bd3bcc797e4d53ce0bc70e",
          "name": "Jiaming Han",
          "hidden": false
        },
        {
          "_id": "67bd3bcc797e4d53ce0bc70f",
          "name": "Zhuoran Yang",
          "hidden": false
        },
        {
          "_id": "67bd3bcc797e4d53ce0bc710",
          "name": "Xiangyu Yue",
          "hidden": false
        },
        {
          "_id": "67bd3bcc797e4d53ce0bc711",
          "name": "Sergey Levine",
          "hidden": false
        },
        {
          "_id": "67bd3bcc797e4d53ce0bc712",
          "name": "Jianlan Luo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-23T20:42:15.000Z",
      "title": "Reflective Planning: Vision-Language Models for Multi-Stage Long-Horizon\n  Robotic Manipulation",
      "summary": "Solving complex long-horizon robotic manipulation problems requires\nsophisticated high-level planning capabilities, the ability to reason about the\nphysical world, and reactively choose appropriate motor skills. Vision-language\nmodels (VLMs) pretrained on Internet data could in principle offer a framework\nfor tackling such problems. However, in their current form, VLMs lack both the\nnuanced understanding of intricate physics required for robotic manipulation\nand the ability to reason over long horizons to address error compounding\nissues. In this paper, we introduce a novel test-time computation framework\nthat enhances VLMs' physical reasoning capabilities for multi-stage\nmanipulation tasks. At its core, our approach iteratively improves a pretrained\nVLM with a \"reflection\" mechanism - it uses a generative model to imagine\nfuture world states, leverages these predictions to guide action selection, and\ncritically reflects on potential suboptimalities to refine its reasoning.\nExperimental results demonstrate that our method significantly outperforms\nseveral state-of-the-art commercial VLMs as well as other post-training\napproaches such as Monte Carlo Tree Search (MCTS). Videos are available at\nhttps://reflect-vlm.github.io.",
      "upvotes": 3,
      "discussionId": "67bd3bcf797e4d53ce0bc7ff"
    },
    "publishedAt": "2025-02-25T01:02:05.395Z",
    "title": "Reflective Planning: Vision-Language Models for Multi-Stage Long-Horizon Robotic Manipulation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16707.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64f8cb8ed04a890f5380d9a4",
      "avatarUrl": "/avatars/d6fdfdbb0c10141aa3b4c832d928121b.svg",
      "fullname": "Jianlan Luo",
      "name": "jianlanluo",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.15987",
      "authors": [
        {
          "_id": "67bd46ea3e090b402d70f1f4",
          "user": {
            "_id": "64dfbcb18e2084e1d7b51b46",
            "avatarUrl": "/avatars/fafe30beea2d7e8eec3f3ba985c582f7.svg",
            "isPro": false,
            "fullname": "Kushal Raj Bhandari",
            "user": "KBhandari11",
            "type": "user"
          },
          "name": "Kushal Raj Bhandari",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-02-25T04:30:32.676Z",
          "hidden": false
        },
        {
          "_id": "67bd46ea3e090b402d70f1f5",
          "name": "Pin-Yu Chen",
          "hidden": false
        },
        {
          "_id": "67bd46ea3e090b402d70f1f6",
          "name": "Jianxi Gao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-21T22:52:19.000Z",
      "title": "Forecasting Open-Weight AI Model Growth on Hugging Face",
      "summary": "As the open-weight AI landscape continues to proliferate-with model\ndevelopment, significant investment, and user interest-it becomes increasingly\nimportant to predict which models will ultimately drive innovation and shape AI\necosystems. Building on parallels with citation dynamics in scientific\nliterature, we propose a framework to quantify how an open-weight model's\ninfluence evolves. Specifically, we adapt the model introduced by Wang et al.\nfor scientific citations, using three key parameters-immediacy, longevity, and\nrelative fitness-to track the cumulative number of fine-tuned models of an\nopen-weight model. Our findings reveal that this citation-style approach can\neffectively capture the diverse trajectories of open-weight model adoption,\nwith most models fitting well and outliers indicating unique patterns or abrupt\njumps in usage.",
      "upvotes": 3,
      "discussionId": "67bd46ee3e090b402d70f317"
    },
    "publishedAt": "2025-02-24T23:30:36.556Z",
    "title": "Forecasting Open-Weight AI Model Growth on Hugging Face",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/5e67bdd61009063689407479/kQHArNjaT0CM1KCujtDc1.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15987.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "5e67bdd61009063689407479",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1583857146757-5e67bdd61009063689407479.jpeg",
      "fullname": "Clem 🤗",
      "name": "clem",
      "type": "user",
      "isPro": true,
      "isHf": true,
      "isMod": false,
      "followerCount": 2052
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.16701",
      "authors": [
        {
          "_id": "67bd31d6bf6d46017e515a58",
          "user": {
            "_id": "62543749b777cd32720675c2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1658760912583-62543749b777cd32720675c2.jpeg",
            "isPro": false,
            "fullname": "Irene Solaiman",
            "user": "irenesolaiman",
            "type": "user"
          },
          "name": "Irene Solaiman",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-02-25T03:43:21.348Z",
          "hidden": false
        },
        {
          "_id": "67bd31d6bf6d46017e515a59",
          "name": "Rishi Bommasani",
          "hidden": false
        },
        {
          "_id": "67bd31d6bf6d46017e515a5a",
          "name": "Dan Hendrycks",
          "hidden": false
        },
        {
          "_id": "67bd31d6bf6d46017e515a5b",
          "name": "Ariel Herbert-Voss",
          "hidden": false
        },
        {
          "_id": "67bd31d6bf6d46017e515a5c",
          "name": "Yacine Jernite",
          "hidden": false
        },
        {
          "_id": "67bd31d6bf6d46017e515a5d",
          "name": "Aviya Skowron",
          "hidden": false
        },
        {
          "_id": "67bd31d6bf6d46017e515a5e",
          "name": "Andrew Trask",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-23T20:06:12.000Z",
      "title": "Beyond Release: Access Considerations for Generative AI Systems",
      "summary": "Generative AI release decisions determine whether system components are made\navailable, but release does not address many other elements that change how\nusers and stakeholders are able to engage with a system. Beyond release, access\nto system components informs potential risks and benefits. Access refers to\npractical needs, infrastructurally, technically, and societally, in order to\nuse available components in some way. We deconstruct access along three axes:\nresourcing, technical usability, and utility. Within each category, a set of\nvariables per system component clarify tradeoffs. For example, resourcing\nrequires access to computing infrastructure to serve model weights. We also\ncompare the accessibility of four high performance language models, two\nopen-weight and two closed-weight, showing similar considerations for all based\ninstead on access variables. Access variables set the foundation for being able\nto scale or increase access to users; we examine the scale of access and how\nscale affects ability to manage and intervene on risks. This framework better\nencompasses the landscape and risk-benefit tradeoffs of system releases to\ninform system release decisions, research, and policy.",
      "upvotes": 3,
      "discussionId": "67bd31d7bf6d46017e515a7e"
    },
    "publishedAt": "2025-02-24T21:59:15.571Z",
    "title": "Beyond Release: Access Considerations for Generative AI Systems",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62543749b777cd32720675c2/LwZmJUoXiJriC_c1DZ7qM.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16701.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62543749b777cd32720675c2",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1658760912583-62543749b777cd32720675c2.jpeg",
      "fullname": "Irene Solaiman",
      "name": "irenesolaiman",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 79
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.17258",
      "authors": [
        {
          "_id": "67bd515c0417e7f92283d3b8",
          "name": "Xiangpeng Yang",
          "hidden": false
        },
        {
          "_id": "67bd515c0417e7f92283d3b9",
          "name": "Linchao Zhu",
          "hidden": false
        },
        {
          "_id": "67bd515c0417e7f92283d3ba",
          "name": "Hehe Fan",
          "hidden": false
        },
        {
          "_id": "67bd515c0417e7f92283d3bb",
          "name": "Yi Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-24T15:39:14.000Z",
      "title": "VideoGrain: Modulating Space-Time Attention for Multi-grained Video\n  Editing",
      "summary": "Recent advancements in diffusion models have significantly improved video\ngeneration and editing capabilities. However, multi-grained video editing,\nwhich encompasses class-level, instance-level, and part-level modifications,\nremains a formidable challenge. The major difficulties in multi-grained editing\ninclude semantic misalignment of text-to-region control and feature coupling\nwithin the diffusion model. To address these difficulties, we present\nVideoGrain, a zero-shot approach that modulates space-time (cross- and self-)\nattention mechanisms to achieve fine-grained control over video content. We\nenhance text-to-region control by amplifying each local prompt's attention to\nits corresponding spatial-disentangled region while minimizing interactions\nwith irrelevant areas in cross-attention. Additionally, we improve feature\nseparation by increasing intra-region awareness and reducing inter-region\ninterference in self-attention. Extensive experiments demonstrate our method\nachieves state-of-the-art performance in real-world scenarios. Our code, data,\nand demos are available at https://knightyxp.github.io/VideoGrain_project_page/",
      "upvotes": 2,
      "discussionId": "67bd51620417e7f92283d4e9"
    },
    "publishedAt": "2025-02-25T00:13:12.214Z",
    "title": "VideoGrain: Modulating Space-Time Attention for Multi-grained Video Editing",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17258.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6204
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.15122",
      "authors": [
        {
          "_id": "67bbd6d5ba0bb31293e11210",
          "user": {
            "_id": "675f68e3074ff89c5c078bf3",
            "avatarUrl": "/avatars/e3b78d90f032659d411761f47c3cf43e.svg",
            "isPro": false,
            "fullname": "Angus",
            "user": "angus924",
            "type": "user"
          },
          "name": "Angus Dempster",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-02-24T02:18:57.914Z",
          "hidden": false
        },
        {
          "_id": "67bbd6d5ba0bb31293e11211",
          "name": "Navid Mohammadi Foumani",
          "hidden": false
        },
        {
          "_id": "67bbd6d5ba0bb31293e11212",
          "name": "Chang Wei Tan",
          "hidden": false
        },
        {
          "_id": "67bbd6d5ba0bb31293e11213",
          "name": "Lynn Miller",
          "hidden": false
        },
        {
          "_id": "67bbd6d5ba0bb31293e11214",
          "name": "Amish Mishra",
          "hidden": false
        },
        {
          "_id": "67bbd6d5ba0bb31293e11215",
          "name": "Mahsa Salehi",
          "hidden": false
        },
        {
          "_id": "67bbd6d5ba0bb31293e11216",
          "name": "Charlotte Pelletier",
          "hidden": false
        },
        {
          "_id": "67bbd6d5ba0bb31293e11217",
          "name": "Daniel F. Schmidt",
          "hidden": false
        },
        {
          "_id": "67bbd6d5ba0bb31293e11218",
          "name": "Geoffrey I. Webb",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-21T00:54:40.000Z",
      "title": "MONSTER: Monash Scalable Time Series Evaluation Repository",
      "summary": "We introduce MONSTER-the MONash Scalable Time Series Evaluation Repository-a\ncollection of large datasets for time series classification. The field of time\nseries classification has benefitted from common benchmarks set by the UCR and\nUEA time series classification repositories. However, the datasets in these\nbenchmarks are small, with median sizes of 217 and 255 examples, respectively.\nIn consequence they favour a narrow subspace of models that are optimised to\nachieve low classification error on a wide variety of smaller datasets, that\nis, models that minimise variance, and give little weight to computational\nissues such as scalability. Our hope is to diversify the field by introducing\nbenchmarks using larger datasets. We believe that there is enormous potential\nfor new progress in the field by engaging with the theoretical and practical\nchallenges of learning effectively from larger quantities of data.",
      "upvotes": 1,
      "discussionId": "67bbd6d6ba0bb31293e11258"
    },
    "publishedAt": "2025-02-25T00:37:53.138Z",
    "title": "MONSTER: Monash Scalable Time Series Evaluation Repository",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15122.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "675f68e3074ff89c5c078bf3",
      "avatarUrl": "/avatars/e3b78d90f032659d411761f47c3cf43e.svg",
      "fullname": "Angus",
      "name": "angus924",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.17414",
      "authors": [
        {
          "_id": "67bd526001d5bfa0abfcc5ba",
          "name": "Zeyuan Chen",
          "hidden": false
        },
        {
          "_id": "67bd526001d5bfa0abfcc5bb",
          "name": "Hongyi Xu",
          "hidden": false
        },
        {
          "_id": "67bd526001d5bfa0abfcc5bc",
          "name": "Guoxian Song",
          "hidden": false
        },
        {
          "_id": "67bd526001d5bfa0abfcc5bd",
          "name": "You Xie",
          "hidden": false
        },
        {
          "_id": "67bd526001d5bfa0abfcc5be",
          "name": "Chenxu Zhang",
          "hidden": false
        },
        {
          "_id": "67bd526001d5bfa0abfcc5bf",
          "name": "Xin Chen",
          "hidden": false
        },
        {
          "_id": "67bd526001d5bfa0abfcc5c0",
          "name": "Chao Wang",
          "hidden": false
        },
        {
          "_id": "67bd526001d5bfa0abfcc5c1",
          "name": "Di Chang",
          "hidden": false
        },
        {
          "_id": "67bd526001d5bfa0abfcc5c2",
          "name": "Linjie Luo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-24T18:47:54.000Z",
      "title": "X-Dancer: Expressive Music to Human Dance Video Generation",
      "summary": "We present X-Dancer, a novel zero-shot music-driven image animation pipeline\nthat creates diverse and long-range lifelike human dance videos from a single\nstatic image. As its core, we introduce a unified transformer-diffusion\nframework, featuring an autoregressive transformer model that synthesize\nextended and music-synchronized token sequences for 2D body, head and hands\nposes, which then guide a diffusion model to produce coherent and realistic\ndance video frames. Unlike traditional methods that primarily generate human\nmotion in 3D, X-Dancer addresses data limitations and enhances scalability by\nmodeling a wide spectrum of 2D dance motions, capturing their nuanced alignment\nwith musical beats through readily available monocular videos. To achieve this,\nwe first build a spatially compositional token representation from 2D human\npose labels associated with keypoint confidences, encoding both large\narticulated body movements (e.g., upper and lower body) and fine-grained\nmotions (e.g., head and hands). We then design a music-to-motion transformer\nmodel that autoregressively generates music-aligned dance pose token sequences,\nincorporating global attention to both musical style and prior motion context.\nFinally we leverage a diffusion backbone to animate the reference image with\nthese synthesized pose tokens through AdaIN, forming a fully differentiable\nend-to-end framework. Experimental results demonstrate that X-Dancer is able to\nproduce both diverse and characterized dance videos, substantially\noutperforming state-of-the-art methods in term of diversity, expressiveness and\nrealism. Code and model will be available for research purposes.",
      "upvotes": 1,
      "discussionId": "67bd526101d5bfa0abfcc62c"
    },
    "publishedAt": "2025-02-25T00:17:51.431Z",
    "title": "X-Dancer: Expressive Music to Human Dance Video Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17414.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6204
    },
    "isAuthorParticipating": false
  }
]