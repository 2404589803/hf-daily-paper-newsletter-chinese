[
  {
    "paper": {
      "id": "2601.15165",
      "authors": [
        {
          "_id": "6971933ac1c7409747bf9597",
          "name": "Zanlin Ni",
          "hidden": false
        },
        {
          "_id": "6971933ac1c7409747bf9598",
          "name": "Shenzhi Wang",
          "hidden": false
        },
        {
          "_id": "6971933ac1c7409747bf9599",
          "name": "Yang Yue",
          "hidden": false
        },
        {
          "_id": "6971933ac1c7409747bf959a",
          "name": "Tianyu Yu",
          "hidden": false
        },
        {
          "_id": "6971933ac1c7409747bf959b",
          "name": "Weilin Zhao",
          "hidden": false
        },
        {
          "_id": "6971933ac1c7409747bf959c",
          "name": "Yeguo Hua",
          "hidden": false
        },
        {
          "_id": "6971933ac1c7409747bf959d",
          "name": "Tianyi Chen",
          "hidden": false
        },
        {
          "_id": "6971933ac1c7409747bf959e",
          "name": "Jun Song",
          "hidden": false
        },
        {
          "_id": "6971933ac1c7409747bf959f",
          "name": "Cheng Yu",
          "hidden": false
        },
        {
          "_id": "6971933ac1c7409747bf95a0",
          "name": "Bo Zheng",
          "hidden": false
        },
        {
          "_id": "6971933ac1c7409747bf95a1",
          "name": "Gao Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-21T16:41:58.000Z",
      "submittedOnDailyAt": "2026-01-23T00:11:51.141Z",
      "title": "The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models",
      "submittedOnDailyBy": {
        "_id": "63987ffb2ceb55aabe0852f3",
        "avatarUrl": "/avatars/343b796ff6b8906203904e8c620d7eb5.svg",
        "isPro": false,
        "fullname": "Zanlin Ni",
        "user": "nzl-thu",
        "type": "user"
      },
      "summary": "Diffusion Large Language Models (dLLMs) break the rigid left-to-right constraint of traditional LLMs, enabling token generation in arbitrary orders. Intuitively, this flexibility implies a solution space that strictly supersets the fixed autoregressive trajectory, theoretically unlocking superior reasoning potential for general tasks like mathematics and coding. Consequently, numerous works have leveraged reinforcement learning (RL) to elicit the reasoning capability of dLLMs. In this paper, we reveal a counter-intuitive reality: arbitrary order generation, in its current form, narrows rather than expands the reasoning boundary of dLLMs. We find that dLLMs tend to exploit this order flexibility to bypass high-uncertainty tokens that are crucial for exploration, leading to a premature collapse of the solution space. This observation challenges the premise of existing RL approaches for dLLMs, where considerable complexities, such as handling combinatorial trajectories and intractable likelihoods, are often devoted to preserving this flexibility. We demonstrate that effective reasoning is better elicited by intentionally forgoing arbitrary order and applying standard Group Relative Policy Optimization (GRPO) instead. Our approach, JustGRPO, is minimalist yet surprisingly effective (e.g., 89.1% accuracy on GSM8K) while fully retaining the parallel decoding ability of dLLMs. Project page: https://nzl-thu.github.io/the-flexibility-trap",
      "upvotes": 42,
      "discussionId": "6971933ac1c7409747bf95a2",
      "projectPage": "https://nzl-thu.github.io/the-flexibility-trap",
      "githubRepo": "https://github.com/LeapLabTHU/JustGRPO",
      "githubRepoAddedBy": "user",
      "ai_summary": "Arbitrary order generation in diffusion large language models limits reasoning capability by causing premature solution space collapse, making standard policy optimization more effective.",
      "ai_keywords": [
        "diffusion large language models",
        "left-to-right constraint",
        "token generation",
        "reinforcement learning",
        "reasoning potential",
        "mathematical reasoning",
        "coding tasks",
        "combinatorial trajectories",
        "likelihoods",
        "Group Relative Policy Optimization",
        "GRPO",
        "parallel decoding"
      ],
      "githubStars": 45,
      "organization": {
        "_id": "69719700e3846c07669d13ee",
        "name": "Tsinghua-LeapLab",
        "fullname": "Tsinghua-LeapLab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63987ffb2ceb55aabe0852f3/hflTWNTGxeJx83xNkYrDB.png"
      }
    },
    "publishedAt": "2026-01-21T11:41:58.000Z",
    "title": "The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models",
    "summary": "Diffusion Large Language Models (dLLMs) break the rigid left-to-right constraint of traditional LLMs, enabling token generation in arbitrary orders. Intuitively, this flexibility implies a solution space that strictly supersets the fixed autoregressive trajectory, theoretically unlocking superior reasoning potential for general tasks like mathematics and coding. Consequently, numerous works have leveraged reinforcement learning (RL) to elicit the reasoning capability of dLLMs. In this paper, we reveal a counter-intuitive reality: arbitrary order generation, in its current form, narrows rather than expands the reasoning boundary of dLLMs. We find that dLLMs tend to exploit this order flexibility to bypass high-uncertainty tokens that are crucial for exploration, leading to a premature collapse of the solution space. This observation challenges the premise of existing RL approaches for dLLMs, where considerable complexities, such as handling combinatorial trajectories and intractable likelihoods, are often devoted to preserving this flexibility. We demonstrate that effective reasoning is better elicited by intentionally forgoing arbitrary order and applying standard Group Relative Policy Optimization (GRPO) instead. Our approach, JustGRPO, is minimalist yet surprisingly effective (e.g., 89.1% accuracy on GSM8K) while fully retaining the parallel decoding ability of dLLMs. Project page: https://nzl-thu.github.io/the-flexibility-trap",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.15165.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63987ffb2ceb55aabe0852f3",
      "avatarUrl": "/avatars/343b796ff6b8906203904e8c620d7eb5.svg",
      "fullname": "Zanlin Ni",
      "name": "nzl-thu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "69719700e3846c07669d13ee",
      "name": "Tsinghua-LeapLab",
      "fullname": "Tsinghua-LeapLab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63987ffb2ceb55aabe0852f3/hflTWNTGxeJx83xNkYrDB.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.16206",
      "authors": [
        {
          "_id": "6972e04dfb12c92b735b73cf",
          "name": "Daixuan Cheng",
          "hidden": false
        },
        {
          "_id": "6972e04dfb12c92b735b73d0",
          "name": "Shaohan Huang",
          "hidden": false
        },
        {
          "_id": "6972e04dfb12c92b735b73d1",
          "name": "Yuxian Gu",
          "hidden": false
        },
        {
          "_id": "6972e04dfb12c92b735b73d2",
          "name": "Huatong Song",
          "hidden": false
        },
        {
          "_id": "6972e04dfb12c92b735b73d3",
          "name": "Guoxin Chen",
          "hidden": false
        },
        {
          "_id": "6972e04dfb12c92b735b73d4",
          "name": "Li Dong",
          "hidden": false
        },
        {
          "_id": "6972e04dfb12c92b735b73d5",
          "name": "Wayne Xin Zhao",
          "hidden": false
        },
        {
          "_id": "6972e04dfb12c92b735b73d6",
          "name": "Ji-Rong Wen",
          "hidden": false
        },
        {
          "_id": "6972e04dfb12c92b735b73d7",
          "name": "Furu Wei",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-22T18:57:09.000Z",
      "submittedOnDailyAt": "2026-01-23T00:27:12.305Z",
      "title": "LLM-in-Sandbox Elicits General Agentic Intelligence",
      "submittedOnDailyBy": {
        "_id": "649e6761f9134a06ed1e0cea",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649e6761f9134a06ed1e0cea/XNeKceE8xSwI0xWwWUwwJ.jpeg",
        "isPro": false,
        "fullname": "Daixuan Cheng",
        "user": "daixuancheng",
        "type": "user"
      },
      "summary": "We introduce LLM-in-Sandbox, enabling LLMs to explore within a code sandbox (i.e., a virtual computer), to elicit general intelligence in non-code domains. We first demonstrate that strong LLMs, without additional training, exhibit generalization capabilities to leverage the code sandbox for non-code tasks. For example, LLMs spontaneously access external resources to acquire new knowledge, leverage the file system to handle long contexts, and execute scripts to satisfy formatting requirements. We further show that these agentic capabilities can be enhanced through LLM-in-Sandbox Reinforcement Learning (LLM-in-Sandbox-RL), which uses only non-agentic data to train models for sandbox exploration. Experiments demonstrate that LLM-in-Sandbox, in both training-free and post-trained settings, achieves robust generalization spanning mathematics, physics, chemistry, biomedicine, long-context understanding, and instruction following. Finally, we analyze LLM-in-Sandbox's efficiency from computational and system perspectives, and open-source it as a Python package to facilitate real-world deployment.",
      "upvotes": 32,
      "discussionId": "6972e04dfb12c92b735b73d8",
      "projectPage": "https://llm-in-sandbox.github.io",
      "githubRepo": "https://github.com/llm-in-sandbox/llm-in-sandbox",
      "githubRepoAddedBy": "user",
      "ai_summary": "LLM-in-Sandbox enables large language models to perform general intelligence tasks across diverse domains by allowing them to explore a code sandbox environment, achieving robust generalization without additional training.",
      "ai_keywords": [
        "LLM-in-Sandbox",
        "code sandbox",
        "virtual computer",
        "reinforcement learning",
        "non-agentic data",
        "sandbox exploration",
        "general intelligence",
        "long-context understanding",
        "instruction following"
      ],
      "githubStars": 4,
      "organization": {
        "_id": "68151d0f51add3813f3f7d1b",
        "name": "MicrosoftResearch",
        "fullname": "Microsoft Research",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6529a4f2f1205983224fa513/PeuVr7jSuJflmDBBGxoDX.png"
      }
    },
    "publishedAt": "2026-01-22T13:57:09.000Z",
    "title": "LLM-in-Sandbox Elicits General Agentic Intelligence",
    "summary": "We introduce LLM-in-Sandbox, enabling LLMs to explore within a code sandbox (i.e., a virtual computer), to elicit general intelligence in non-code domains. We first demonstrate that strong LLMs, without additional training, exhibit generalization capabilities to leverage the code sandbox for non-code tasks. For example, LLMs spontaneously access external resources to acquire new knowledge, leverage the file system to handle long contexts, and execute scripts to satisfy formatting requirements. We further show that these agentic capabilities can be enhanced through LLM-in-Sandbox Reinforcement Learning (LLM-in-Sandbox-RL), which uses only non-agentic data to train models for sandbox exploration. Experiments demonstrate that LLM-in-Sandbox, in both training-free and post-trained settings, achieves robust generalization spanning mathematics, physics, chemistry, biomedicine, long-context understanding, and instruction following. Finally, we analyze LLM-in-Sandbox's efficiency from computational and system perspectives, and open-source it as a Python package to facilitate real-world deployment.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.16206.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649e6761f9134a06ed1e0cea",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649e6761f9134a06ed1e0cea/XNeKceE8xSwI0xWwWUwwJ.jpeg",
      "fullname": "Daixuan Cheng",
      "name": "daixuancheng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 15,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "68151d0f51add3813f3f7d1b",
      "name": "MicrosoftResearch",
      "fullname": "Microsoft Research",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6529a4f2f1205983224fa513/PeuVr7jSuJflmDBBGxoDX.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.14724",
      "authors": [
        {
          "_id": "6972ee7afb12c92b735b74b4",
          "name": "Haowei Zhang",
          "hidden": false
        },
        {
          "_id": "6972ee7afb12c92b735b74b5",
          "name": "Shudong Yang",
          "hidden": false
        },
        {
          "_id": "6972ee7afb12c92b735b74b6",
          "name": "Jinlan Fu",
          "hidden": false
        },
        {
          "_id": "6972ee7afb12c92b735b74b7",
          "name": "See-Kiong Ng",
          "hidden": false
        },
        {
          "_id": "6972ee7afb12c92b735b74b8",
          "name": "Xipeng Qiu",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-21T07:26:15.000Z",
      "submittedOnDailyAt": "2026-01-23T01:43:37.582Z",
      "title": "HERMES: KV Cache as Hierarchical Memory for Efficient Streaming Video Understanding",
      "submittedOnDailyBy": {
        "_id": "637169557a5e5d8efdc3e58e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1668515232215-637169557a5e5d8efdc3e58e.jpeg",
        "isPro": false,
        "fullname": "Haowei Zhang",
        "user": "freesky",
        "type": "user"
      },
      "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated significant improvement in offline video understanding. However, extending these capabilities to streaming video inputs, remains challenging, as existing models struggle to simultaneously maintain stable understanding performance, real-time responses, and low GPU memory overhead. To address this challenge, we propose HERMES, a novel training-free architecture for real-time and accurate understanding of video streams. Based on a mechanistic attention investigation, we conceptualize KV cache as a hierarchical memory framework that encapsulates video information across multiple granularities. During inference, HERMES reuses a compact KV cache, enabling efficient streaming understanding under resource constraints. Notably, HERMES requires no auxiliary computations upon the arrival of user queries, thereby guaranteeing real-time responses for continuous video stream interactions, which achieves 10times faster TTFT compared to prior SOTA. Even when reducing video tokens by up to 68% compared with uniform sampling, HERMES achieves superior or comparable accuracy across all benchmarks, with up to 11.4% gains on streaming datasets.",
      "upvotes": 30,
      "discussionId": "6972ee7bfb12c92b735b74b9",
      "projectPage": "https://hermes-streaming.github.io/",
      "githubRepo": "https://github.com/haowei-freesky/HERMES",
      "githubRepoAddedBy": "user",
      "ai_summary": "HERMES is a training-free architecture that enables real-time video stream understanding by utilizing a hierarchical memory framework based on KV cache reuse, achieving faster response times and maintained accuracy even with reduced video token input.",
      "ai_keywords": [
        "Multimodal Large Language Models",
        "video understanding",
        "streaming video inputs",
        "real-time responses",
        "KV cache",
        "hierarchical memory framework",
        "mechanistic attention",
        "video tokens",
        "TTFT"
      ],
      "githubStars": 7,
      "organization": {
        "_id": "613b0dee83ec35d460684607",
        "name": "OpenMOSS-Team",
        "fullname": "OpenMOSS",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"
      }
    },
    "publishedAt": "2026-01-21T02:26:15.000Z",
    "title": "HERMES: KV Cache as Hierarchical Memory for Efficient Streaming Video Understanding",
    "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated significant improvement in offline video understanding. However, extending these capabilities to streaming video inputs, remains challenging, as existing models struggle to simultaneously maintain stable understanding performance, real-time responses, and low GPU memory overhead. To address this challenge, we propose HERMES, a novel training-free architecture for real-time and accurate understanding of video streams. Based on a mechanistic attention investigation, we conceptualize KV cache as a hierarchical memory framework that encapsulates video information across multiple granularities. During inference, HERMES reuses a compact KV cache, enabling efficient streaming understanding under resource constraints. Notably, HERMES requires no auxiliary computations upon the arrival of user queries, thereby guaranteeing real-time responses for continuous video stream interactions, which achieves 10times faster TTFT compared to prior SOTA. Even when reducing video tokens by up to 68% compared with uniform sampling, HERMES achieves superior or comparable accuracy across all benchmarks, with up to 11.4% gains on streaming datasets.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.14724.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "637169557a5e5d8efdc3e58e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1668515232215-637169557a5e5d8efdc3e58e.jpeg",
      "fullname": "Haowei Zhang",
      "name": "freesky",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "613b0dee83ec35d460684607",
      "name": "OpenMOSS-Team",
      "fullname": "OpenMOSS",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.15197",
      "authors": [
        {
          "_id": "6971c608c1c7409747bf96a5",
          "user": {
            "_id": "65ec01fd770aa0e25d9374dc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65ec01fd770aa0e25d9374dc/yvLWwBEdAdHb-8EdUHg3n.jpeg",
            "isPro": false,
            "fullname": "Shijie Lian",
            "user": "LiamLian0727",
            "type": "user"
          },
          "name": "Shijie Lian",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-01-22T08:44:25.547Z",
          "hidden": false
        },
        {
          "_id": "6971c608c1c7409747bf96a6",
          "name": "Bin Yu",
          "hidden": false
        },
        {
          "_id": "6971c608c1c7409747bf96a7",
          "name": "Xiaopeng Lin",
          "hidden": false
        },
        {
          "_id": "6971c608c1c7409747bf96a8",
          "name": "Laurence T. Yang",
          "hidden": false
        },
        {
          "_id": "6971c608c1c7409747bf96a9",
          "name": "Zhaolong Shen",
          "hidden": false
        },
        {
          "_id": "6971c608c1c7409747bf96aa",
          "name": "Changti Wu",
          "hidden": false
        },
        {
          "_id": "6971c608c1c7409747bf96ab",
          "name": "Yuzhuo Miao",
          "hidden": false
        },
        {
          "_id": "6971c608c1c7409747bf96ac",
          "name": "Cong Huang",
          "hidden": false
        },
        {
          "_id": "6971c608c1c7409747bf96ad",
          "name": "Kai Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-21T17:15:22.000Z",
      "submittedOnDailyAt": "2026-01-23T00:45:20.588Z",
      "title": "BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries",
      "submittedOnDailyBy": {
        "_id": "65ec01fd770aa0e25d9374dc",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65ec01fd770aa0e25d9374dc/yvLWwBEdAdHb-8EdUHg3n.jpeg",
        "isPro": false,
        "fullname": "Shijie Lian",
        "user": "LiamLian0727",
        "type": "user"
      },
      "summary": "Vision-Language-Action (VLA) models have shown promise in robot manipulation but often struggle to generalize to new instructions or complex multi-task scenarios. We identify a critical pathology in current training paradigms where goal-driven data collection creates a dataset bias. In such datasets, language instructions are highly predictable from visual observations alone, causing the conditional mutual information between instructions and actions to vanish, a phenomenon we term Information Collapse. Consequently, models degenerate into vision-only policies that ignore language constraints and fail in out-of-distribution (OOD) settings. To address this, we propose BayesianVLA, a novel framework that enforces instruction following via Bayesian decomposition. By introducing learnable Latent Action Queries, we construct a dual-branch architecture to estimate both a vision-only prior p(a mid v) and a language-conditioned posterior π(a mid v, ell). We then optimize the policy to maximize the conditional Pointwise Mutual Information (PMI) between actions and instructions. This objective effectively penalizes the vision shortcut and rewards actions that explicitly explain the language command. Without requiring new data, BayesianVLA significantly improves generalization. Extensive experiments across on SimplerEnv and RoboCasa demonstrate substantial gains, including an 11.3% improvement on the challenging OOD SimplerEnv benchmark, validating the ability of our approach to robustly ground language in action.",
      "upvotes": 29,
      "discussionId": "6971c609c1c7409747bf96ae",
      "projectPage": "https://github.com/ZGC-EmbodyAI/BayesianVLA",
      "githubRepo": "https://github.com/ZGC-EmbodyAI/BayesianVLA",
      "githubRepoAddedBy": "user",
      "ai_summary": "BayesianVLA addresses language-action grounding issues in robot manipulation by using Bayesian decomposition to prevent information collapse and improve out-of-distribution generalization.",
      "ai_keywords": [
        "Vision-Language-Action models",
        "Information Collapse",
        "Bayesian decomposition",
        "latent action queries",
        "conditional Pointwise Mutual Information",
        "vision-only policies",
        "out-of-distribution generalization",
        "SimplerEnv",
        "RoboCasa"
      ],
      "githubStars": 7,
      "organization": {
        "_id": "68896d3a716ee5bfb1428441",
        "name": "ZGCA",
        "fullname": "Zhongguancun Academy",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6854c3ab09a3ba7d16243875/aZ3tp3lZk1yQoXDwSklye.png"
      }
    },
    "publishedAt": "2026-01-21T12:15:22.000Z",
    "title": "BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries",
    "summary": "Vision-Language-Action (VLA) models have shown promise in robot manipulation but often struggle to generalize to new instructions or complex multi-task scenarios. We identify a critical pathology in current training paradigms where goal-driven data collection creates a dataset bias. In such datasets, language instructions are highly predictable from visual observations alone, causing the conditional mutual information between instructions and actions to vanish, a phenomenon we term Information Collapse. Consequently, models degenerate into vision-only policies that ignore language constraints and fail in out-of-distribution (OOD) settings. To address this, we propose BayesianVLA, a novel framework that enforces instruction following via Bayesian decomposition. By introducing learnable Latent Action Queries, we construct a dual-branch architecture to estimate both a vision-only prior p(a mid v) and a language-conditioned posterior π(a mid v, ell). We then optimize the policy to maximize the conditional Pointwise Mutual Information (PMI) between actions and instructions. This objective effectively penalizes the vision shortcut and rewards actions that explicitly explain the language command. Without requiring new data, BayesianVLA significantly improves generalization. Extensive experiments across on SimplerEnv and RoboCasa demonstrate substantial gains, including an 11.3% improvement on the challenging OOD SimplerEnv benchmark, validating the ability of our approach to robustly ground language in action.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.15197.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65ec01fd770aa0e25d9374dc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65ec01fd770aa0e25d9374dc/yvLWwBEdAdHb-8EdUHg3n.jpeg",
      "fullname": "Shijie Lian",
      "name": "LiamLian0727",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "68896d3a716ee5bfb1428441",
      "name": "ZGCA",
      "fullname": "Zhongguancun Academy",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6854c3ab09a3ba7d16243875/aZ3tp3lZk1yQoXDwSklye.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2601.16208",
      "authors": [
        {
          "_id": "6972ea8bfb12c92b735b74a8",
          "name": "Shengbang Tong",
          "hidden": false
        },
        {
          "_id": "6972ea8bfb12c92b735b74a9",
          "name": "Boyang Zheng",
          "hidden": false
        },
        {
          "_id": "6972ea8bfb12c92b735b74aa",
          "name": "Ziteng Wang",
          "hidden": false
        },
        {
          "_id": "6972ea8bfb12c92b735b74ab",
          "name": "Bingda Tang",
          "hidden": false
        },
        {
          "_id": "6972ea8bfb12c92b735b74ac",
          "name": "Nanye Ma",
          "hidden": false
        },
        {
          "_id": "6972ea8bfb12c92b735b74ad",
          "name": "Ellis Brown",
          "hidden": false
        },
        {
          "_id": "6972ea8bfb12c92b735b74ae",
          "name": "Jihan Yang",
          "hidden": false
        },
        {
          "_id": "6972ea8bfb12c92b735b74af",
          "name": "Rob Fergus",
          "hidden": false
        },
        {
          "_id": "6972ea8bfb12c92b735b74b0",
          "name": "Yann LeCun",
          "hidden": false
        },
        {
          "_id": "6972ea8bfb12c92b735b74b1",
          "name": "Saining Xie",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-22T18:58:16.000Z",
      "submittedOnDailyAt": "2026-01-23T00:57:17.761Z",
      "title": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders",
      "submittedOnDailyBy": {
        "_id": "6434226da4c9c55871a78052",
        "avatarUrl": "/avatars/3309832b3115bc6ad08ae1d10f43118b.svg",
        "isPro": false,
        "fullname": "BoYang Zheng",
        "user": "bytetriper",
        "type": "user"
      },
      "summary": "Representation Autoencoders (RAEs) have shown distinct advantages in diffusion modeling on ImageNet by training in high-dimensional semantic latent spaces. In this work, we investigate whether this framework can scale to large-scale, freeform text-to-image (T2I) generation. We first scale RAE decoders on the frozen representation encoder (SigLIP-2) beyond ImageNet by training on web, synthetic, and text-rendering data, finding that while scale improves general fidelity, targeted data composition is essential for specific domains like text. We then rigorously stress-test the RAE design choices originally proposed for ImageNet. Our analysis reveals that scaling simplifies the framework: while dimension-dependent noise scheduling remains critical, architectural complexities such as wide diffusion heads and noise-augmented decoding offer negligible benefits at scale Building on this simplified framework, we conduct a controlled comparison of RAE against the state-of-the-art FLUX VAE across diffusion transformer scales from 0.5B to 9.8B parameters. RAEs consistently outperform VAEs during pretraining across all model scales. Further, during finetuning on high-quality datasets, VAE-based models catastrophically overfit after 64 epochs, while RAE models remain stable through 256 epochs and achieve consistently better performance. Across all experiments, RAE-based diffusion models demonstrate faster convergence and better generation quality, establishing RAEs as a simpler and stronger foundation than VAEs for large-scale T2I generation. Additionally, because both visual understanding and generation can operate in a shared representation space, the multimodal model can directly reason over generated latents, opening new possibilities for unified models.",
      "upvotes": 23,
      "discussionId": "6972ea8cfb12c92b735b74b2",
      "projectPage": "https://rae-dit.github.io/scale-rae/",
      "githubRepo": "https://github.com/ZitengWangNYU/Scale-RAE",
      "githubRepoAddedBy": "user",
      "ai_summary": "Representation Autoencoders (RAEs) demonstrate superior performance over VAEs in large-scale text-to-image generation, showing improved stability, faster convergence, and better quality while enabling unified multimodal reasoning in shared representation spaces.",
      "ai_keywords": [
        "representation autoencoders",
        "diffusion modeling",
        "semantic latent spaces",
        "text-to-image generation",
        "frozen representation encoder",
        "SigLIP-2",
        "noise scheduling",
        "diffusion transformers",
        "pretraining",
        "finetuning",
        "catastrophic overfitting",
        "multimodal model",
        "shared representation space"
      ],
      "githubStars": 3,
      "organization": {
        "_id": "662741612ada5b77e310d171",
        "name": "nyu-visionx",
        "fullname": "VISIONx @ NYU",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/626dc5105f7327906f0b2a4e/Kn-QtZjE6TJE-syTndXIW.jpeg"
      }
    },
    "publishedAt": "2026-01-22T13:58:16.000Z",
    "title": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders",
    "summary": "Representation Autoencoders (RAEs) have shown distinct advantages in diffusion modeling on ImageNet by training in high-dimensional semantic latent spaces. In this work, we investigate whether this framework can scale to large-scale, freeform text-to-image (T2I) generation. We first scale RAE decoders on the frozen representation encoder (SigLIP-2) beyond ImageNet by training on web, synthetic, and text-rendering data, finding that while scale improves general fidelity, targeted data composition is essential for specific domains like text. We then rigorously stress-test the RAE design choices originally proposed for ImageNet. Our analysis reveals that scaling simplifies the framework: while dimension-dependent noise scheduling remains critical, architectural complexities such as wide diffusion heads and noise-augmented decoding offer negligible benefits at scale Building on this simplified framework, we conduct a controlled comparison of RAE against the state-of-the-art FLUX VAE across diffusion transformer scales from 0.5B to 9.8B parameters. RAEs consistently outperform VAEs during pretraining across all model scales. Further, during finetuning on high-quality datasets, VAE-based models catastrophically overfit after 64 epochs, while RAE models remain stable through 256 epochs and achieve consistently better performance. Across all experiments, RAE-based diffusion models demonstrate faster convergence and better generation quality, establishing RAEs as a simpler and stronger foundation than VAEs for large-scale T2I generation. Additionally, because both visual understanding and generation can operate in a shared representation space, the multimodal model can directly reason over generated latents, opening new possibilities for unified models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.16208.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6434226da4c9c55871a78052",
      "avatarUrl": "/avatars/3309832b3115bc6ad08ae1d10f43118b.svg",
      "fullname": "BoYang Zheng",
      "name": "bytetriper",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "662741612ada5b77e310d171",
      "name": "nyu-visionx",
      "fullname": "VISIONx @ NYU",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/626dc5105f7327906f0b2a4e/Kn-QtZjE6TJE-syTndXIW.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.16093",
      "authors": [
        {
          "_id": "6972f00ffb12c92b735b74bb",
          "name": "Yikang Zhou",
          "hidden": false
        },
        {
          "_id": "6972f00ffb12c92b735b74bc",
          "name": "Tao Zhang",
          "hidden": false
        },
        {
          "_id": "6972f00ffb12c92b735b74bd",
          "name": "Dengxian Gong",
          "hidden": false
        },
        {
          "_id": "6972f00ffb12c92b735b74be",
          "name": "Yuanzheng Wu",
          "hidden": false
        },
        {
          "_id": "6972f00ffb12c92b735b74bf",
          "name": "Ye Tian",
          "hidden": false
        },
        {
          "_id": "6972f00ffb12c92b735b74c0",
          "name": "Haochen Wang",
          "hidden": false
        },
        {
          "_id": "6972f00ffb12c92b735b74c1",
          "name": "Haobo Yuan",
          "hidden": false
        },
        {
          "_id": "6972f00ffb12c92b735b74c2",
          "name": "Jiacong Wang",
          "hidden": false
        },
        {
          "_id": "6972f00ffb12c92b735b74c3",
          "name": "Lu Qi",
          "hidden": false
        },
        {
          "_id": "6972f00ffb12c92b735b74c4",
          "name": "Hao Fei",
          "hidden": false
        },
        {
          "_id": "6972f00ffb12c92b735b74c5",
          "name": "Anran Wang",
          "hidden": false
        },
        {
          "_id": "6972f00ffb12c92b735b74c6",
          "name": "Zhuochen Wang",
          "hidden": false
        },
        {
          "_id": "6972f00ffb12c92b735b74c7",
          "name": "Yujing Wang",
          "hidden": false
        },
        {
          "_id": "6972f00ffb12c92b735b74c8",
          "name": "Cheng Chen",
          "hidden": false
        },
        {
          "_id": "6972f00ffb12c92b735b74c9",
          "name": "Shunping Ji",
          "hidden": false
        },
        {
          "_id": "6972f00ffb12c92b735b74ca",
          "name": "Xiangtai Li",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-22T16:44:09.000Z",
      "submittedOnDailyAt": "2026-01-23T01:22:36.775Z",
      "title": "SAMTok: Representing Any Mask with Two Words",
      "submittedOnDailyBy": {
        "_id": "63958b4414513eaf9029ebf1",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/U1g5H071pWRswGAG9UTpo.png",
        "isPro": false,
        "fullname": "Xiangtai Li",
        "user": "LXT",
        "type": "user"
      },
      "summary": "Pixel-wise capabilities are essential for building interactive intelligent systems. However, pixel-wise multi-modal LLMs (MLLMs) remain difficult to scale due to complex region-level encoders, specialized segmentation decoders, and incompatible training objectives. To address these challenges, we present SAMTok, a discrete mask tokenizer that converts any region mask into two special tokens and reconstructs the mask using these tokens with high fidelity. By treating masks as new language tokens, SAMTok enables base MLLMs (such as the QwenVL series) to learn pixel-wise capabilities through standard next-token prediction and simple reinforcement learning, without architectural modifications and specialized loss design. SAMTok builds on SAM2 and is trained on 209M diverse masks using a mask encoder and residual vector quantizer to produce discrete, compact, and information-rich tokens. With 5M SAMTok-formatted mask understanding and generation data samples, QwenVL-SAMTok attains state-of-the-art or comparable results on region captioning, region VQA, grounded conversation, referring segmentation, scene graph parsing, and multi-round interactive segmentation. We further introduce a textual answer-matching reward that enables efficient reinforcement learning for mask generation, delivering substantial improvements on GRES and GCG benchmarks. Our results demonstrate a scalable and straightforward paradigm for equipping MLLMs with strong pixel-wise capabilities. Our code and models are available.",
      "upvotes": 23,
      "discussionId": "6972f00ffb12c92b735b74cb",
      "projectPage": "https://zhouyiks.github.io/projects/SAMTok/",
      "githubRepo": "https://github.com/bytedance/Sa2VA/tree/main/projects/samtok",
      "githubRepoAddedBy": "user",
      "ai_summary": "SAMTok enables pixel-wise capabilities in multi-modal LLMs through discrete mask tokenization and standard training methods, achieving state-of-the-art performance on various vision-language tasks.",
      "ai_keywords": [
        "multi-modal LLMs",
        "region mask",
        "discrete mask tokenizer",
        "SAM2",
        "mask encoder",
        "residual vector quantizer",
        "next-token prediction",
        "reinforcement learning",
        "region captioning",
        "region VQA",
        "grounded conversation",
        "referring segmentation",
        "scene graph parsing",
        "multi-round interactive segmentation",
        "textual answer-matching reward",
        "GRES",
        "GCG"
      ],
      "githubStars": 1497,
      "organization": {
        "_id": "653b817d32c97d0655575872",
        "name": "ByteDance",
        "fullname": "ByteDance",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
      }
    },
    "publishedAt": "2026-01-22T11:44:09.000Z",
    "title": "SAMTok: Representing Any Mask with Two Words",
    "summary": "Pixel-wise capabilities are essential for building interactive intelligent systems. However, pixel-wise multi-modal LLMs (MLLMs) remain difficult to scale due to complex region-level encoders, specialized segmentation decoders, and incompatible training objectives. To address these challenges, we present SAMTok, a discrete mask tokenizer that converts any region mask into two special tokens and reconstructs the mask using these tokens with high fidelity. By treating masks as new language tokens, SAMTok enables base MLLMs (such as the QwenVL series) to learn pixel-wise capabilities through standard next-token prediction and simple reinforcement learning, without architectural modifications and specialized loss design. SAMTok builds on SAM2 and is trained on 209M diverse masks using a mask encoder and residual vector quantizer to produce discrete, compact, and information-rich tokens. With 5M SAMTok-formatted mask understanding and generation data samples, QwenVL-SAMTok attains state-of-the-art or comparable results on region captioning, region VQA, grounded conversation, referring segmentation, scene graph parsing, and multi-round interactive segmentation. We further introduce a textual answer-matching reward that enables efficient reinforcement learning for mask generation, delivering substantial improvements on GRES and GCG benchmarks. Our results demonstrate a scalable and straightforward paradigm for equipping MLLMs with strong pixel-wise capabilities. Our code and models are available.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.16093.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63958b4414513eaf9029ebf1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/U1g5H071pWRswGAG9UTpo.png",
      "fullname": "Xiangtai Li",
      "name": "LXT",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 13,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "653b817d32c97d0655575872",
      "name": "ByteDance",
      "fullname": "ByteDance",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.16175",
      "authors": [
        {
          "_id": "6972e02afb12c92b735b73c2",
          "name": "Mert Yuksekgonul",
          "hidden": false
        },
        {
          "_id": "6972e02afb12c92b735b73c3",
          "name": "Daniel Koceja",
          "hidden": false
        },
        {
          "_id": "6972e02afb12c92b735b73c4",
          "name": "Xinhao Li",
          "hidden": false
        },
        {
          "_id": "6972e02afb12c92b735b73c5",
          "name": "Federico Bianchi",
          "hidden": false
        },
        {
          "_id": "6972e02afb12c92b735b73c6",
          "name": "Jed McCaleb",
          "hidden": false
        },
        {
          "_id": "6972e02afb12c92b735b73c7",
          "name": "Xiaolong Wang",
          "hidden": false
        },
        {
          "_id": "6972e02afb12c92b735b73c8",
          "name": "Jan Kautz",
          "hidden": false
        },
        {
          "_id": "6972e02afb12c92b735b73c9",
          "name": "Yejin Choi",
          "hidden": false
        },
        {
          "_id": "6972e02afb12c92b735b73ca",
          "name": "James Zou",
          "hidden": false
        },
        {
          "_id": "6972e02afb12c92b735b73cb",
          "name": "Carlos Guestrin",
          "hidden": false
        },
        {
          "_id": "6972e02afb12c92b735b73cc",
          "name": "Yu Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-22T18:24:00.000Z",
      "submittedOnDailyAt": "2026-01-23T00:15:05.275Z",
      "title": "Learning to Discover at Test Time",
      "submittedOnDailyBy": {
        "_id": "603f7c7af84ebe399f1c85cf",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1625214886903-603f7c7af84ebe399f1c85cf.jpeg",
        "isPro": false,
        "fullname": "Federico Bianchi",
        "user": "vinid",
        "type": "user"
      },
      "summary": "How can we use AI to discover a new state of the art for a scientific problem? Prior work in test-time scaling, such as AlphaEvolve, performs search by prompting a frozen LLM. We perform reinforcement learning at test time, so the LLM can continue to train, but now with experience specific to the test problem. This form of continual learning is quite special, because its goal is to produce one great solution rather than many good ones on average, and to solve this very problem rather than generalize to other problems. Therefore, our learning objective and search subroutine are designed to prioritize the most promising solutions. We call this method Test-Time Training to Discover (TTT-Discover). Following prior work, we focus on problems with continuous rewards. We report results for every problem we attempted, across mathematics, GPU kernel engineering, algorithm design, and biology. TTT-Discover sets the new state of the art in almost all of them: (i) Erdős' minimum overlap problem and an autocorrelation inequality; (ii) a GPUMode kernel competition (up to 2times faster than prior art); (iii) past AtCoder algorithm competitions; and (iv) denoising problem in single-cell analysis. Our solutions are reviewed by experts or the organizers. All our results are achieved with an open model, OpenAI gpt-oss-120b, and can be reproduced with our publicly available code, in contrast to previous best results that required closed frontier models. Our test-time training runs are performed using Tinker, an API by Thinking Machines, with a cost of only a few hundred dollars per problem.",
      "upvotes": 16,
      "discussionId": "6972e02afb12c92b735b73cd",
      "projectPage": "https://test-time-training.github.io/discover/",
      "githubRepo": "https://github.com/test-time-training/discover",
      "githubRepoAddedBy": "user",
      "ai_summary": "Test-time training enables AI systems to discover optimal solutions for specific scientific problems through continual learning focused on individual challenges rather than generalization.",
      "ai_keywords": [
        "reinforcement learning",
        "test-time training",
        "continual learning",
        "search subroutine",
        "learning objective",
        "OpenAI gpt-oss-120b",
        "Tinker API"
      ],
      "githubStars": 41,
      "organization": {
        "_id": "672c672dcf09d152f4da04c4",
        "name": "StanfordUniversity",
        "fullname": "Stanford University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/vJI0POlzGMXL2878t1vz2.jpeg"
      }
    },
    "publishedAt": "2026-01-22T13:24:00.000Z",
    "title": "Learning to Discover at Test Time",
    "summary": "How can we use AI to discover a new state of the art for a scientific problem? Prior work in test-time scaling, such as AlphaEvolve, performs search by prompting a frozen LLM. We perform reinforcement learning at test time, so the LLM can continue to train, but now with experience specific to the test problem. This form of continual learning is quite special, because its goal is to produce one great solution rather than many good ones on average, and to solve this very problem rather than generalize to other problems. Therefore, our learning objective and search subroutine are designed to prioritize the most promising solutions. We call this method Test-Time Training to Discover (TTT-Discover). Following prior work, we focus on problems with continuous rewards. We report results for every problem we attempted, across mathematics, GPU kernel engineering, algorithm design, and biology. TTT-Discover sets the new state of the art in almost all of them: (i) Erdős' minimum overlap problem and an autocorrelation inequality; (ii) a GPUMode kernel competition (up to 2times faster than prior art); (iii) past AtCoder algorithm competitions; and (iv) denoising problem in single-cell analysis. Our solutions are reviewed by experts or the organizers. All our results are achieved with an open model, OpenAI gpt-oss-120b, and can be reproduced with our publicly available code, in contrast to previous best results that required closed frontier models. Our test-time training runs are performed using Tinker, an API by Thinking Machines, with a cost of only a few hundred dollars per problem.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.16175.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "603f7c7af84ebe399f1c85cf",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1625214886903-603f7c7af84ebe399f1c85cf.jpeg",
      "fullname": "Federico Bianchi",
      "name": "vinid",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 21,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "672c672dcf09d152f4da04c4",
      "name": "StanfordUniversity",
      "fullname": "Stanford University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/vJI0POlzGMXL2878t1vz2.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.16125",
      "authors": [
        {
          "_id": "6972e18efb12c92b735b73f1",
          "name": "Tingyu Song",
          "hidden": false
        },
        {
          "_id": "6972e18efb12c92b735b73f2",
          "name": "Yanzhao Zhang",
          "hidden": false
        },
        {
          "_id": "6972e18efb12c92b735b73f3",
          "name": "Mingxin Li",
          "hidden": false
        },
        {
          "_id": "6972e18efb12c92b735b73f4",
          "name": "Zhuoning Guo",
          "hidden": false
        },
        {
          "_id": "6972e18efb12c92b735b73f5",
          "name": "Dingkun Long",
          "hidden": false
        },
        {
          "_id": "6972e18efb12c92b735b73f6",
          "name": "Pengjun Xie",
          "hidden": false
        },
        {
          "_id": "6972e18efb12c92b735b73f7",
          "name": "Siyue Zhang",
          "hidden": false
        },
        {
          "_id": "6972e18efb12c92b735b73f8",
          "name": "Yilun Zhao",
          "hidden": false
        },
        {
          "_id": "6972e18efb12c92b735b73f9",
          "name": "Shu Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-22T17:26:52.000Z",
      "submittedOnDailyAt": "2026-01-23T00:19:40.646Z",
      "title": "Rethinking Composed Image Retrieval Evaluation: A Fine-Grained Benchmark from Image Editing",
      "submittedOnDailyBy": {
        "_id": "64dc29d9b5d625e0e9a6ecb9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/QxGBsnk1cNsBEPqSx4ae-.jpeg",
        "isPro": false,
        "fullname": "Tingyu Song",
        "user": "songtingyu",
        "type": "user"
      },
      "summary": "Composed Image Retrieval (CIR) is a pivotal and complex task in multimodal understanding. Current CIR benchmarks typically feature limited query categories and fail to capture the diverse requirements of real-world scenarios. To bridge this evaluation gap, we leverage image editing to achieve precise control over modification types and content, enabling a pipeline for synthesizing queries across a broad spectrum of categories. Using this pipeline, we construct EDIR, a novel fine-grained CIR benchmark. EDIR encompasses 5,000 high-quality queries structured across five main categories and fifteen subcategories. Our comprehensive evaluation of 13 multimodal embedding models reveals a significant capability gap; even state-of-the-art models (e.g., RzenEmbed and GME) struggle to perform consistently across all subcategories, highlighting the rigorous nature of our benchmark. Through comparative analysis, we further uncover inherent limitations in existing benchmarks, such as modality biases and insufficient categorical coverage. Furthermore, an in-domain training experiment demonstrates the feasibility of our benchmark. This experiment clarifies the task challenges by distinguishing between categories that are solvable with targeted data and those that expose intrinsic limitations of current model architectures.",
      "upvotes": 12,
      "discussionId": "6972e18efb12c92b735b73fa",
      "githubRepo": "https://github.com/SighingSnow/edir",
      "githubRepoAddedBy": "user",
      "ai_summary": "A novel fine-grained composed image retrieval benchmark is introduced through image editing techniques, revealing significant capability gaps in existing multimodal models and exposing limitations of current benchmarks.",
      "ai_keywords": [
        "composed image retrieval",
        "multimodal embedding models",
        "image editing",
        "fine-grained benchmark",
        "modality biases",
        "categorical coverage"
      ],
      "githubStars": 0
    },
    "publishedAt": "2026-01-22T12:26:52.000Z",
    "title": "Rethinking Composed Image Retrieval Evaluation: A Fine-Grained Benchmark from Image Editing",
    "summary": "Composed Image Retrieval (CIR) is a pivotal and complex task in multimodal understanding. Current CIR benchmarks typically feature limited query categories and fail to capture the diverse requirements of real-world scenarios. To bridge this evaluation gap, we leverage image editing to achieve precise control over modification types and content, enabling a pipeline for synthesizing queries across a broad spectrum of categories. Using this pipeline, we construct EDIR, a novel fine-grained CIR benchmark. EDIR encompasses 5,000 high-quality queries structured across five main categories and fifteen subcategories. Our comprehensive evaluation of 13 multimodal embedding models reveals a significant capability gap; even state-of-the-art models (e.g., RzenEmbed and GME) struggle to perform consistently across all subcategories, highlighting the rigorous nature of our benchmark. Through comparative analysis, we further uncover inherent limitations in existing benchmarks, such as modality biases and insufficient categorical coverage. Furthermore, an in-domain training experiment demonstrates the feasibility of our benchmark. This experiment clarifies the task challenges by distinguishing between categories that are solvable with targeted data and those that expose intrinsic limitations of current model architectures.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.16125.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64dc29d9b5d625e0e9a6ecb9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/QxGBsnk1cNsBEPqSx4ae-.jpeg",
      "fullname": "Tingyu Song",
      "name": "songtingyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.15892",
      "authors": [
        {
          "_id": "6972d788fb12c92b735b7397",
          "name": "Chenghao Fan",
          "hidden": false
        },
        {
          "_id": "6972d788fb12c92b735b7398",
          "name": "Wen Heng",
          "hidden": false
        },
        {
          "_id": "6972d788fb12c92b735b7399",
          "name": "Bo Li",
          "hidden": false
        },
        {
          "_id": "6972d788fb12c92b735b739a",
          "name": "Sichen Liu",
          "hidden": false
        },
        {
          "_id": "6972d788fb12c92b735b739b",
          "name": "Yuxuan Song",
          "hidden": false
        },
        {
          "_id": "6972d788fb12c92b735b739c",
          "name": "Jing Su",
          "hidden": false
        },
        {
          "_id": "6972d788fb12c92b735b739d",
          "name": "Xiaoye Qu",
          "hidden": false
        },
        {
          "_id": "6972d788fb12c92b735b739e",
          "name": "Kai Shen",
          "hidden": false
        },
        {
          "_id": "6972d788fb12c92b735b739f",
          "name": "Wei Wei",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-22T12:13:17.000Z",
      "submittedOnDailyAt": "2026-01-23T00:09:35.389Z",
      "title": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model",
      "submittedOnDailyBy": {
        "_id": "641aa5e391e3376a057bbd4c",
        "avatarUrl": "/avatars/5818797f27444fde078b503774ee081c.svg",
        "isPro": false,
        "fullname": "Chenghao Fan",
        "user": "Facico",
        "type": "user"
      },
      "summary": "Diffusion-based language models (DLLMs) offer non-sequential, block-wise generation and richer data reuse compared to autoregressive (AR) models, but existing code DLLMs still lag behind strong AR baselines under comparable budgets. We revisit this setting in a controlled study and introduce Stable-DiffCoder, a block diffusion code model that reuses the Seed-Coder architecture, data, and training pipeline. To enable efficient knowledge learning and stable training, we incorporate a block diffusion continual pretraining (CPT) stage enhanced by a tailored warmup and block-wise clipped noise schedule. Under the same data and architecture, Stable-DiffCoder overall outperforms its AR counterpart on a broad suite of code benchmarks. Moreover, relying only on the CPT and supervised fine-tuning stages, Stable-DiffCoder achieves stronger performance than a wide range of \\~8B ARs and DLLMs, demonstrating that diffusion-based training can improve code modeling quality beyond AR training alone. Moreover, diffusion-based any-order modeling improves structured code modeling for editing and reasoning, and through data augmentation, benefits low-resource coding languages.",
      "upvotes": 11,
      "discussionId": "6972d788fb12c92b735b73a0",
      "projectPage": "https://bytedance-seed.github.io/Stable-DiffCoder/",
      "githubRepo": "https://github.com/ByteDance-Seed/Stable-DiffCoder",
      "githubRepoAddedBy": "user",
      "ai_summary": "Stable-DiffCoder demonstrates superior code modeling performance compared to autoregressive baselines through block diffusion continual pretraining and efficient training mechanisms.",
      "ai_keywords": [
        "diffusion-based language models",
        "autoregressive models",
        "block diffusion",
        "continual pretraining",
        "warmup",
        "clipped noise schedule",
        "supervised fine-tuning",
        "code modeling",
        "structured code modeling",
        "data augmentation"
      ],
      "githubStars": 5,
      "organization": {
        "_id": "67d1140985ea0644e2f14b99",
        "name": "ByteDance-Seed",
        "fullname": "ByteDance Seed",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
      }
    },
    "publishedAt": "2026-01-22T07:13:17.000Z",
    "title": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model",
    "summary": "Diffusion-based language models (DLLMs) offer non-sequential, block-wise generation and richer data reuse compared to autoregressive (AR) models, but existing code DLLMs still lag behind strong AR baselines under comparable budgets. We revisit this setting in a controlled study and introduce Stable-DiffCoder, a block diffusion code model that reuses the Seed-Coder architecture, data, and training pipeline. To enable efficient knowledge learning and stable training, we incorporate a block diffusion continual pretraining (CPT) stage enhanced by a tailored warmup and block-wise clipped noise schedule. Under the same data and architecture, Stable-DiffCoder overall outperforms its AR counterpart on a broad suite of code benchmarks. Moreover, relying only on the CPT and supervised fine-tuning stages, Stable-DiffCoder achieves stronger performance than a wide range of \\~8B ARs and DLLMs, demonstrating that diffusion-based training can improve code modeling quality beyond AR training alone. Moreover, diffusion-based any-order modeling improves structured code modeling for editing and reasoning, and through data augmentation, benefits low-resource coding languages.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.15892.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "641aa5e391e3376a057bbd4c",
      "avatarUrl": "/avatars/5818797f27444fde078b503774ee081c.svg",
      "fullname": "Chenghao Fan",
      "name": "Facico",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "67d1140985ea0644e2f14b99",
      "name": "ByteDance-Seed",
      "fullname": "ByteDance Seed",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.15369",
      "authors": [
        {
          "_id": "6972de28fb12c92b735b73b4",
          "name": "Letian Zhang",
          "hidden": false
        },
        {
          "_id": "6972de28fb12c92b735b73b5",
          "name": "Sucheng Ren",
          "hidden": false
        },
        {
          "_id": "6972de28fb12c92b735b73b6",
          "name": "Yanqing Liu",
          "hidden": false
        },
        {
          "_id": "6972de28fb12c92b735b73b7",
          "name": "Xianhang Li",
          "hidden": false
        },
        {
          "_id": "6972de28fb12c92b735b73b8",
          "name": "Zeyu Wang",
          "hidden": false
        },
        {
          "_id": "6972de28fb12c92b735b73b9",
          "name": "Yuyin Zhou",
          "hidden": false
        },
        {
          "_id": "6972de28fb12c92b735b73ba",
          "name": "Huaxiu Yao",
          "hidden": false
        },
        {
          "_id": "6972de28fb12c92b735b73bb",
          "name": "Zeyu Zheng",
          "hidden": false
        },
        {
          "_id": "6972de28fb12c92b735b73bc",
          "name": "Weili Nie",
          "hidden": false
        },
        {
          "_id": "6972de28fb12c92b735b73bd",
          "name": "Guilin Liu",
          "hidden": false
        },
        {
          "_id": "6972de28fb12c92b735b73be",
          "name": "Zhiding Yu",
          "hidden": false
        },
        {
          "_id": "6972de28fb12c92b735b73bf",
          "name": "Cihang Xie",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-21T18:47:12.000Z",
      "submittedOnDailyAt": "2026-01-23T00:05:34.242Z",
      "title": "OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation",
      "submittedOnDailyBy": {
        "_id": "645eb61da3c5cd8a16efffff",
        "avatarUrl": "/avatars/9112bfeed598dfabf9e077e69e09ecc9.svg",
        "isPro": false,
        "fullname": "Cihang Xie",
        "user": "cihangxie",
        "type": "user"
      },
      "summary": "This paper presents a family of advanced vision encoder, named OpenVision 3, that learns a single, unified visual representation that can serve both image understanding and image generation. Our core architecture is simple: we feed VAE-compressed image latents to a ViT encoder and train its output to support two complementary roles. First, the encoder output is passed to the ViT-VAE decoder to reconstruct the original image, encouraging the representation to capture generative structure. Second, the same representation is optimized with contrastive learning and image-captioning objectives, strengthening semantic features. By jointly optimizing reconstruction- and semantics-driven signals in a shared latent space, the encoder learns representations that synergize and generalize well across both regimes. We validate this unified design through extensive downstream evaluations with the encoder frozen. For multimodal understanding, we plug the encoder into the LLaVA-1.5 framework: it performs comparably with a standard CLIP vision encoder (e.g., 62.4 vs 62.2 on SeedBench, and 83.7 vs 82.9 on POPE). For generation, we test it under the RAE framework: ours substantially surpasses the standard CLIP-based encoder (e.g., gFID: 1.89 vs 2.54 on ImageNet). We hope this work can spur future research on unified modeling.",
      "upvotes": 8,
      "discussionId": "6972de28fb12c92b735b73c0",
      "projectPage": "https://ucsc-vlaa.github.io/OpenVision3/",
      "ai_summary": "An advanced vision encoder named OpenVision 3 learns a unified visual representation for both image understanding and generation by combining VAE-compressed image latents with ViT architecture and joint optimization of reconstruction and semantic signals.",
      "ai_keywords": [
        "vision encoder",
        "VAE-compressed image latents",
        "ViT encoder",
        "ViT-VAE decoder",
        "contrastive learning",
        "image-captioning objectives",
        "shared latent space",
        "multimodal understanding",
        "LLaVA-1.5 framework",
        "RAE framework",
        "unified modeling"
      ],
      "organization": {
        "_id": "65346047b3852ed1cec0c2f4",
        "name": "UCSC-VLAA",
        "fullname": "UCSC-VLAA",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/645eb61da3c5cd8a16efffff/E7m3g_fFhz32pGsnK0eqX.png"
      }
    },
    "publishedAt": "2026-01-21T13:47:12.000Z",
    "title": "OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation",
    "summary": "This paper presents a family of advanced vision encoder, named OpenVision 3, that learns a single, unified visual representation that can serve both image understanding and image generation. Our core architecture is simple: we feed VAE-compressed image latents to a ViT encoder and train its output to support two complementary roles. First, the encoder output is passed to the ViT-VAE decoder to reconstruct the original image, encouraging the representation to capture generative structure. Second, the same representation is optimized with contrastive learning and image-captioning objectives, strengthening semantic features. By jointly optimizing reconstruction- and semantics-driven signals in a shared latent space, the encoder learns representations that synergize and generalize well across both regimes. We validate this unified design through extensive downstream evaluations with the encoder frozen. For multimodal understanding, we plug the encoder into the LLaVA-1.5 framework: it performs comparably with a standard CLIP vision encoder (e.g., 62.4 vs 62.2 on SeedBench, and 83.7 vs 82.9 on POPE). For generation, we test it under the RAE framework: ours substantially surpasses the standard CLIP-based encoder (e.g., gFID: 1.89 vs 2.54 on ImageNet). We hope this work can spur future research on unified modeling.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.15369.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645eb61da3c5cd8a16efffff",
      "avatarUrl": "/avatars/9112bfeed598dfabf9e077e69e09ecc9.svg",
      "fullname": "Cihang Xie",
      "name": "cihangxie",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "65346047b3852ed1cec0c2f4",
      "name": "UCSC-VLAA",
      "fullname": "UCSC-VLAA",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/645eb61da3c5cd8a16efffff/E7m3g_fFhz32pGsnK0eqX.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.15621",
      "authors": [
        {
          "_id": "6972e310fb12c92b735b746a",
          "name": "Hangrui Hu",
          "hidden": false
        },
        {
          "_id": "6972e310fb12c92b735b746b",
          "name": "Xinfa Zhu",
          "hidden": false
        },
        {
          "_id": "6972e310fb12c92b735b746c",
          "name": "Ting He",
          "hidden": false
        },
        {
          "_id": "6972e310fb12c92b735b746d",
          "name": "Dake Guo",
          "hidden": false
        },
        {
          "_id": "6972e310fb12c92b735b746e",
          "name": "Bin Zhang",
          "hidden": false
        },
        {
          "_id": "6972e310fb12c92b735b746f",
          "name": "Xiong Wang",
          "hidden": false
        },
        {
          "_id": "6972e310fb12c92b735b7470",
          "name": "Zhifang Guo",
          "hidden": false
        },
        {
          "_id": "6972e310fb12c92b735b7471",
          "name": "Ziyue Jiang",
          "hidden": false
        },
        {
          "_id": "6972e310fb12c92b735b7472",
          "name": "Hongkun Hao",
          "hidden": false
        },
        {
          "_id": "6972e310fb12c92b735b7473",
          "name": "Zishan Guo",
          "hidden": false
        },
        {
          "_id": "6972e310fb12c92b735b7474",
          "name": "Xinyu Zhang",
          "hidden": false
        },
        {
          "_id": "6972e310fb12c92b735b7475",
          "name": "Pei Zhang",
          "hidden": false
        },
        {
          "_id": "6972e310fb12c92b735b7476",
          "name": "Baosong Yang",
          "hidden": false
        },
        {
          "_id": "6972e310fb12c92b735b7477",
          "name": "Jin Xu",
          "hidden": false
        },
        {
          "_id": "6972e310fb12c92b735b7478",
          "name": "Jingren Zhou",
          "hidden": false
        },
        {
          "_id": "6972e310fb12c92b735b7479",
          "name": "Junyang Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-22T03:51:43.000Z",
      "submittedOnDailyAt": "2026-01-23T00:25:20.168Z",
      "title": "Qwen3-TTS Technical Report",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "In this report, we present the Qwen3-TTS series, a family of advanced multilingual, controllable, robust, and streaming text-to-speech models. Qwen3-TTS supports state-of-the-art 3-second voice cloning and description-based control, allowing both the creation of entirely novel voices and fine-grained manipulation over the output speech. Trained on over 5 million hours of speech data spanning 10 languages, Qwen3-TTS adopts a dual-track LM architecture for real-time synthesis, coupled with two speech tokenizers: 1) Qwen-TTS-Tokenizer-25Hz is a single-codebook codec emphasizing semantic content, which offers seamlessly integration with Qwen-Audio and enables streaming waveform reconstruction via a block-wise DiT. 2) Qwen-TTS-Tokenizer-12Hz achieves extreme bitrate reduction and ultra-low-latency streaming, enabling immediate first-packet emission (97,ms) through its 12.5 Hz, 16-layer multi-codebook design and a lightweight causal ConvNet. Extensive experiments indicate state-of-the-art performance across diverse objective and subjective benchmark (e.g., TTS multilingual test set, InstructTTSEval, and our long speech test set). To facilitate community research and development, we release both tokenizers and models under the Apache 2.0 license.",
      "upvotes": 7,
      "discussionId": "6972e311fb12c92b735b747a",
      "githubRepo": "https://github.com/QwenLM/Qwen3-TTS",
      "githubRepoAddedBy": "user",
      "ai_summary": "The Qwen3-TTS series presents advanced multilingual text-to-speech models with voice cloning and controllable speech generation capabilities, utilizing dual-track LM architecture and specialized speech tokenizers for efficient streaming synthesis.",
      "ai_keywords": [
        "text-to-speech",
        "voice cloning",
        "dual-track LM architecture",
        "speech tokenizers",
        "Qwen-TTS-Tokenizer-25Hz",
        "Qwen-TTS-Tokenizer-12Hz",
        "DiT",
        "ConvNet",
        "streaming waveform reconstruction",
        "multilingual",
        "controllable speech generation"
      ],
      "githubStars": 841,
      "organization": {
        "_id": "64c8b5837fe12ecd0a7e92eb",
        "name": "Qwen",
        "fullname": "Qwen",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"
      }
    },
    "publishedAt": "2026-01-21T22:51:43.000Z",
    "title": "Qwen3-TTS Technical Report",
    "summary": "In this report, we present the Qwen3-TTS series, a family of advanced multilingual, controllable, robust, and streaming text-to-speech models. Qwen3-TTS supports state-of-the-art 3-second voice cloning and description-based control, allowing both the creation of entirely novel voices and fine-grained manipulation over the output speech. Trained on over 5 million hours of speech data spanning 10 languages, Qwen3-TTS adopts a dual-track LM architecture for real-time synthesis, coupled with two speech tokenizers: 1) Qwen-TTS-Tokenizer-25Hz is a single-codebook codec emphasizing semantic content, which offers seamlessly integration with Qwen-Audio and enables streaming waveform reconstruction via a block-wise DiT. 2) Qwen-TTS-Tokenizer-12Hz achieves extreme bitrate reduction and ultra-low-latency streaming, enabling immediate first-packet emission (97,ms) through its 12.5 Hz, 16-layer multi-codebook design and a lightweight causal ConvNet. Extensive experiments indicate state-of-the-art performance across diverse objective and subjective benchmark (e.g., TTS multilingual test set, InstructTTSEval, and our long speech test set). To facilitate community research and development, we release both tokenizers and models under the Apache 2.0 license.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.15621.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 212,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "64c8b5837fe12ecd0a7e92eb",
      "name": "Qwen",
      "fullname": "Qwen",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.16163",
      "authors": [
        {
          "_id": "6972e6e6fb12c92b735b749b",
          "name": "Moo Jin Kim",
          "hidden": false
        },
        {
          "_id": "6972e6e6fb12c92b735b749c",
          "name": "Yihuai Gao",
          "hidden": false
        },
        {
          "_id": "6972e6e6fb12c92b735b749d",
          "name": "Tsung-Yi Lin",
          "hidden": false
        },
        {
          "_id": "6972e6e6fb12c92b735b749e",
          "name": "Yen-Chen Lin",
          "hidden": false
        },
        {
          "_id": "6972e6e6fb12c92b735b749f",
          "name": "Yunhao Ge",
          "hidden": false
        },
        {
          "_id": "6972e6e6fb12c92b735b74a0",
          "name": "Grace Lam",
          "hidden": false
        },
        {
          "_id": "6972e6e6fb12c92b735b74a1",
          "name": "Percy Liang",
          "hidden": false
        },
        {
          "_id": "6972e6e6fb12c92b735b74a2",
          "name": "Shuran Song",
          "hidden": false
        },
        {
          "_id": "6972e6e6fb12c92b735b74a3",
          "name": "Ming-Yu Liu",
          "hidden": false
        },
        {
          "_id": "6972e6e6fb12c92b735b74a4",
          "name": "Chelsea Finn",
          "hidden": false
        },
        {
          "_id": "6972e6e6fb12c92b735b74a5",
          "name": "Jinwei Gu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/Zgvz1YoJRlTbXhCHyBJiS.mp4"
      ],
      "publishedAt": "2026-01-22T18:09:30.000Z",
      "submittedOnDailyAt": "2026-01-23T00:42:13.636Z",
      "title": "Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Recent video generation models demonstrate remarkable ability to capture complex physical interactions and scene evolution over time. To leverage their spatiotemporal priors, robotics works have adapted video models for policy learning but introduce complexity by requiring multiple stages of post-training and new architectural components for action generation. In this work, we introduce Cosmos Policy, a simple approach for adapting a large pretrained video model (Cosmos-Predict2) into an effective robot policy through a single stage of post-training on the robot demonstration data collected on the target platform, with no architectural modifications. Cosmos Policy learns to directly generate robot actions encoded as latent frames within the video model's latent diffusion process, harnessing the model's pretrained priors and core learning algorithm to capture complex action distributions. Additionally, Cosmos Policy generates future state images and values (expected cumulative rewards), which are similarly encoded as latent frames, enabling test-time planning of action trajectories with higher likelihood of success. In our evaluations, Cosmos Policy achieves state-of-the-art performance on the LIBERO and RoboCasa simulation benchmarks (98.5% and 67.1% average success rates, respectively) and the highest average score in challenging real-world bimanual manipulation tasks, outperforming strong diffusion policies trained from scratch, video model-based policies, and state-of-the-art vision-language-action models fine-tuned on the same robot demonstrations. Furthermore, given policy rollout data, Cosmos Policy can learn from experience to refine its world model and value function and leverage model-based planning to achieve even higher success rates in challenging tasks. We release code, models, and training data at https://research.nvidia.com/labs/dir/cosmos-policy/",
      "upvotes": 4,
      "discussionId": "6972e6e6fb12c92b735b74a6",
      "ai_summary": "A pretrained video model is adapted into a robot policy through single-stage post-training, enabling direct action generation and planning capabilities without architectural modifications.",
      "ai_keywords": [
        "video models",
        "robot policy",
        "latent diffusion process",
        "action generation",
        "world model",
        "value function",
        "model-based planning",
        "diffusion policies",
        "vision-language-action models"
      ],
      "organization": {
        "_id": "60262b67268c201cdc8b7d43",
        "name": "nvidia",
        "fullname": "NVIDIA",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
      }
    },
    "publishedAt": "2026-01-22T13:09:30.000Z",
    "title": "Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning",
    "summary": "Recent video generation models demonstrate remarkable ability to capture complex physical interactions and scene evolution over time. To leverage their spatiotemporal priors, robotics works have adapted video models for policy learning but introduce complexity by requiring multiple stages of post-training and new architectural components for action generation. In this work, we introduce Cosmos Policy, a simple approach for adapting a large pretrained video model (Cosmos-Predict2) into an effective robot policy through a single stage of post-training on the robot demonstration data collected on the target platform, with no architectural modifications. Cosmos Policy learns to directly generate robot actions encoded as latent frames within the video model's latent diffusion process, harnessing the model's pretrained priors and core learning algorithm to capture complex action distributions. Additionally, Cosmos Policy generates future state images and values (expected cumulative rewards), which are similarly encoded as latent frames, enabling test-time planning of action trajectories with higher likelihood of success. In our evaluations, Cosmos Policy achieves state-of-the-art performance on the LIBERO and RoboCasa simulation benchmarks (98.5% and 67.1% average success rates, respectively) and the highest average score in challenging real-world bimanual manipulation tasks, outperforming strong diffusion policies trained from scratch, video model-based policies, and state-of-the-art vision-language-action models fine-tuned on the same robot demonstrations. Furthermore, given policy rollout data, Cosmos Policy can learn from experience to refine its world model and value function and leverage model-based planning to achieve even higher success rates in challenging tasks. We release code, models, and training data at https://research.nvidia.com/labs/dir/cosmos-policy/",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/Zgvz1YoJRlTbXhCHyBJiS.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.16163.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 212,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "60262b67268c201cdc8b7d43",
      "name": "nvidia",
      "fullname": "NVIDIA",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.11868",
      "authors": [
        {
          "_id": "6972e295fb12c92b735b7413",
          "name": "Mike A. Merrill",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b7414",
          "name": "Alexander G. Shaw",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b7415",
          "name": "Nicholas Carlini",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b7416",
          "name": "Boxuan Li",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b7417",
          "name": "Harsh Raj",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b7418",
          "name": "Ivan Bercovich",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b7419",
          "name": "Lin Shi",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b741a",
          "name": "Jeong Yeon Shin",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b741b",
          "name": "Thomas Walshe",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b741c",
          "name": "E. Kelly Buchanan",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b741d",
          "name": "Junhong Shen",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b741e",
          "name": "Guanghao Ye",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b741f",
          "name": "Haowei Lin",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b7420",
          "name": "Jason Poulos",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b7421",
          "name": "Maoyu Wang",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b7422",
          "name": "Marianna Nezhurina",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b7423",
          "name": "Jenia Jitsev",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b7424",
          "name": "Di Lu",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b7425",
          "name": "Orfeas Menis Mastromichalakis",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b7426",
          "name": "Zhiwei Xu",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b7427",
          "name": "Zizhao Chen",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b7428",
          "name": "Yue Liu",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b7429",
          "name": "Robert Zhang",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b742a",
          "name": "Leon Liangyu Chen",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b742b",
          "name": "Anurag Kashyap",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b742c",
          "name": "Jan-Lucas Uslu",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b742d",
          "name": "Jeffrey Li",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b742e",
          "name": "Jianbo Wu",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b742f",
          "name": "Minghao Yan",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b7430",
          "name": "Song Bian",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b7431",
          "name": "Vedang Sharma",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b7432",
          "name": "Ke Sun",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b7433",
          "name": "Steven Dillmann",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b7434",
          "name": "Akshay Anand",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b7435",
          "name": "Andrew Lanpouthakoun",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b7436",
          "name": "Bardia Koopah",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b7437",
          "name": "Changran Hu",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b7438",
          "name": "Etash Guha",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b7439",
          "name": "Gabriel H. S. Dreiman",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b743a",
          "name": "Jiacheng Zhu",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b743b",
          "name": "Karl Krauth",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b743c",
          "name": "Li Zhong",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b743d",
          "name": "Niklas Muennighoff",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b743e",
          "name": "Robert Amanfu",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b743f",
          "name": "Shangyin Tan",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b7440",
          "name": "Shreyas Pimpalgaonkar",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b7441",
          "name": "Tushar Aggarwal",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b7442",
          "name": "Xiangning Lin",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b7443",
          "name": "Xin Lan",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b7444",
          "name": "Xuandong Zhao",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b7445",
          "name": "Yiqing Liang",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b7446",
          "name": "Yuanli Wang",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b7447",
          "name": "Zilong Wang",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b7448",
          "name": "Changzhi Zhou",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b7449",
          "name": "David Heineman",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b744a",
          "name": "Hange Liu",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b744b",
          "name": "Harsh Trivedi",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b744c",
          "name": "John Yang",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b744d",
          "name": "Junhong Lin",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b744e",
          "name": "Manish Shetty",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b744f",
          "name": "Michael Yang",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b7450",
          "name": "Nabil Omi",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b7451",
          "name": "Negin Raoof",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b7452",
          "name": "Shanda Li",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b7453",
          "name": "Terry Yue Zhuo",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b7454",
          "name": "Wuwei Lin",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b7455",
          "name": "Yiwei Dai",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b7456",
          "name": "Yuxin Wang",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b7457",
          "name": "Wenhao Chai",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b7458",
          "name": "Shang Zhou",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b7459",
          "name": "Dariush Wahdany",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b745a",
          "name": "Ziyu She",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b745b",
          "name": "Jiaming Hu",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b745c",
          "name": "Zhikang Dong",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b745d",
          "name": "Yuxuan Zhu",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b745e",
          "name": "Sasha Cui",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b745f",
          "name": "Ahson Saiyed",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b7460",
          "name": "Arinbjörn Kolbeinsson",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b7461",
          "name": "Jesse Hu",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b7462",
          "name": "Christopher Michael Rytting",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b7463",
          "name": "Ryan Marten",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b7464",
          "name": "Yixin Wang",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b7465",
          "name": "Alex Dimakis",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b7466",
          "name": "Andy Konwinski",
          "hidden": false
        },
        {
          "_id": "6972e295fb12c92b735b7467",
          "name": "Ludwig Schmidt",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-17T01:29:30.000Z",
      "submittedOnDailyAt": "2026-01-23T00:23:14.492Z",
      "title": "Terminal-Bench: Benchmarking Agents on Hard, Realistic Tasks in Command Line Interfaces",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "AI agents may soon become capable of autonomously completing valuable, long-horizon tasks in diverse domains. Current benchmarks either do not measure real-world tasks, or are not sufficiently difficult to meaningfully measure frontier models. To this end, we present Terminal-Bench 2.0: a carefully curated hard benchmark composed of 89 tasks in computer terminal environments inspired by problems from real workflows. Each task features a unique environment, human-written solution, and comprehensive tests for verification. We show that frontier models and agents score less than 65\\% on the benchmark and conduct an error analysis to identify areas for model and agent improvement. We publish the dataset and evaluation harness to assist developers and researchers in future work at https://www.tbench.ai/ .",
      "upvotes": 4,
      "discussionId": "6972e295fb12c92b735b7468",
      "githubRepo": "https://github.com/laude-institute/terminal-bench",
      "githubRepoAddedBy": "user",
      "ai_summary": "Terminal-Bench 2.0 presents a challenging benchmark with 89 terminal-based tasks to evaluate AI agents' capabilities in real-world scenarios.",
      "ai_keywords": [
        "AI agents",
        "long-horizon tasks",
        "benchmarks",
        "terminal environments",
        "real-world tasks",
        "evaluation harness"
      ],
      "githubStars": 1399
    },
    "publishedAt": "2026-01-16T20:29:30.000Z",
    "title": "Terminal-Bench: Benchmarking Agents on Hard, Realistic Tasks in Command Line Interfaces",
    "summary": "AI agents may soon become capable of autonomously completing valuable, long-horizon tasks in diverse domains. Current benchmarks either do not measure real-world tasks, or are not sufficiently difficult to meaningfully measure frontier models. To this end, we present Terminal-Bench 2.0: a carefully curated hard benchmark composed of 89 tasks in computer terminal environments inspired by problems from real workflows. Each task features a unique environment, human-written solution, and comprehensive tests for verification. We show that frontier models and agents score less than 65\\% on the benchmark and conduct an error analysis to identify areas for model and agent improvement. We publish the dataset and evaluation harness to assist developers and researchers in future work at https://www.tbench.ai/ .",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.11868.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 212,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.15549",
      "authors": [
        {
          "_id": "69730b15fb12c92b735b7540",
          "name": "Ryo Fujii",
          "hidden": false
        },
        {
          "_id": "69730b15fb12c92b735b7541",
          "name": "Hideo Saito",
          "hidden": false
        },
        {
          "_id": "69730b15fb12c92b735b7542",
          "name": "Ryo Hachiuma",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-22T00:35:30.000Z",
      "submittedOnDailyAt": "2026-01-23T03:16:43.867Z",
      "title": "VIOLA: Towards Video In-Context Learning with Minimal Annotations",
      "submittedOnDailyBy": {
        "_id": "65b33e5f7cd0069ad648c4e8",
        "avatarUrl": "/avatars/1a746ea535cffa92ea08006e05ea414a.svg",
        "isPro": false,
        "fullname": "Ryo Hachiuma",
        "user": "rhachiuma",
        "type": "user"
      },
      "summary": "Generalizing Multimodal Large Language Models (MLLMs) to novel video domains is essential for real-world deployment but remains challenging due to the scarcity of labeled data. While In-Context Learning (ICL) offers a training-free adaptation path, standard methods rely on large annotated pools, which are often impractical in specialized environments like industrial or surgical settings since they require the experts' annotations. To bridge this gap, we introduce VIOLA (Video In-cOntext Learning with minimal Annotation), a label-efficient framework that synergizes minimal expert supervision with abundant unlabeled data. First, to maximize the efficiency of a strict annotation budget, we propose density-uncertainty-weighted sampling. Unlike standard diversity or uncertainty strategies that risk selecting visual outliers, our method leverages density estimation to identify samples that are simultaneously diverse, representative, and informative. Second, to utilize the remaining unlabeled data without noise propagation, we construct a hybrid pool and introduce confidence-aware retrieval and confidence-aware prompting. These mechanisms explicitly model label reliability, retrieving demonstrations based on a composite score of similarity and confidence while enabling the MLLM to adaptively distinguish between verified ground truths and noisy pseudo-labels. Extensive experiments across nine diverse benchmarks using four MLLMs demonstrate that our framework significantly outperforms various baselines in low-resource settings, achieving robust adaptation with minimal annotation costs.",
      "upvotes": 2,
      "discussionId": "69730b15fb12c92b735b7543",
      "ai_summary": "VIOLA is a label-efficient framework that combines minimal expert supervision with abundant unlabeled data to enable effective multimodal large language model adaptation in low-resource video domains through density-uncertainty-weighted sampling and confidence-aware retrieval mechanisms.",
      "ai_keywords": [
        "In-Context Learning",
        "multimodal large language models",
        "label-efficient framework",
        "density-uncertainty-weighted sampling",
        "confidence-aware retrieval",
        "confidence-aware prompting",
        "pseudo-labels",
        "hybrid pool",
        "video domains",
        "low-resource settings"
      ]
    },
    "publishedAt": "2026-01-21T19:35:30.000Z",
    "title": "VIOLA: Towards Video In-Context Learning with Minimal Annotations",
    "summary": "Generalizing Multimodal Large Language Models (MLLMs) to novel video domains is essential for real-world deployment but remains challenging due to the scarcity of labeled data. While In-Context Learning (ICL) offers a training-free adaptation path, standard methods rely on large annotated pools, which are often impractical in specialized environments like industrial or surgical settings since they require the experts' annotations. To bridge this gap, we introduce VIOLA (Video In-cOntext Learning with minimal Annotation), a label-efficient framework that synergizes minimal expert supervision with abundant unlabeled data. First, to maximize the efficiency of a strict annotation budget, we propose density-uncertainty-weighted sampling. Unlike standard diversity or uncertainty strategies that risk selecting visual outliers, our method leverages density estimation to identify samples that are simultaneously diverse, representative, and informative. Second, to utilize the remaining unlabeled data without noise propagation, we construct a hybrid pool and introduce confidence-aware retrieval and confidence-aware prompting. These mechanisms explicitly model label reliability, retrieving demonstrations based on a composite score of similarity and confidence while enabling the MLLM to adaptively distinguish between verified ground truths and noisy pseudo-labels. Extensive experiments across nine diverse benchmarks using four MLLMs demonstrate that our framework significantly outperforms various baselines in low-resource settings, achieving robust adaptation with minimal annotation costs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.15549.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65b33e5f7cd0069ad648c4e8",
      "avatarUrl": "/avatars/1a746ea535cffa92ea08006e05ea414a.svg",
      "fullname": "Ryo Hachiuma",
      "name": "rhachiuma",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.16192",
      "authors": [
        {
          "_id": "69732303fb12c92b735b758a",
          "name": "Ziyi Wu",
          "hidden": false
        },
        {
          "_id": "69732303fb12c92b735b758b",
          "name": "Daniel Watson",
          "hidden": false
        },
        {
          "_id": "69732303fb12c92b735b758c",
          "name": "Andrea Tagliasacchi",
          "hidden": false
        },
        {
          "_id": "69732303fb12c92b735b758d",
          "name": "David J. Fleet",
          "hidden": false
        },
        {
          "_id": "69732303fb12c92b735b758e",
          "name": "Marcus A. Brubaker",
          "hidden": false
        },
        {
          "_id": "69732303fb12c92b735b758f",
          "name": "Saurabh Saxena",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-22T18:45:59.000Z",
      "submittedOnDailyAt": "2026-01-23T05:04:19.153Z",
      "title": "360Anything: Geometry-Free Lifting of Images and Videos to 360°",
      "submittedOnDailyBy": {
        "_id": "62980664ff0acd7e027d6686",
        "avatarUrl": "/avatars/364d4c8432c24775a099641fc576dbdc.svg",
        "isPro": false,
        "fullname": "Ziyi Wu",
        "user": "Dazitu616",
        "type": "user"
      },
      "summary": "Lifting perspective images and videos to 360° panoramas enables immersive 3D world generation. Existing approaches often rely on explicit geometric alignment between the perspective and the equirectangular projection (ERP) space. Yet, this requires known camera metadata, obscuring the application to in-the-wild data where such calibration is typically absent or noisy. We propose 360Anything, a geometry-free framework built upon pre-trained diffusion transformers. By treating the perspective input and the panorama target simply as token sequences, 360Anything learns the perspective-to-equirectangular mapping in a purely data-driven way, eliminating the need for camera information. Our approach achieves state-of-the-art performance on both image and video perspective-to-360° generation, outperforming prior works that use ground-truth camera information. We also trace the root cause of the seam artifacts at ERP boundaries to zero-padding in the VAE encoder, and introduce Circular Latent Encoding to facilitate seamless generation. Finally, we show competitive results in zero-shot camera FoV and orientation estimation benchmarks, demonstrating 360Anything's deep geometric understanding and broader utility in computer vision tasks. Additional results are available at https://360anything.github.io/.",
      "upvotes": 1,
      "discussionId": "69732304fb12c92b735b7590",
      "ai_summary": "A geometry-free framework using pre-trained diffusion transformers lifts perspective images and videos to 360° panoramas without requiring camera metadata, achieving state-of-the-art performance through token sequence processing and addressing seam artifacts via circular latent encoding.",
      "ai_keywords": [
        "diffusion transformers",
        "perspective-to-equirectangular mapping",
        "token sequences",
        "zero-padding",
        "VAE encoder",
        "circular latent encoding",
        "360° panoramas",
        "geometric alignment",
        "camera metadata",
        "seam artifacts"
      ],
      "organization": {
        "_id": "60f6cbb2852126bac698c89e",
        "name": "deepmind",
        "fullname": "Deepmind",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1638956859875-5f1158120c833276f61f1a84.jpeg"
      }
    },
    "publishedAt": "2026-01-22T13:45:59.000Z",
    "title": "360Anything: Geometry-Free Lifting of Images and Videos to 360°",
    "summary": "Lifting perspective images and videos to 360° panoramas enables immersive 3D world generation. Existing approaches often rely on explicit geometric alignment between the perspective and the equirectangular projection (ERP) space. Yet, this requires known camera metadata, obscuring the application to in-the-wild data where such calibration is typically absent or noisy. We propose 360Anything, a geometry-free framework built upon pre-trained diffusion transformers. By treating the perspective input and the panorama target simply as token sequences, 360Anything learns the perspective-to-equirectangular mapping in a purely data-driven way, eliminating the need for camera information. Our approach achieves state-of-the-art performance on both image and video perspective-to-360° generation, outperforming prior works that use ground-truth camera information. We also trace the root cause of the seam artifacts at ERP boundaries to zero-padding in the VAE encoder, and introduce Circular Latent Encoding to facilitate seamless generation. Finally, we show competitive results in zero-shot camera FoV and orientation estimation benchmarks, demonstrating 360Anything's deep geometric understanding and broader utility in computer vision tasks. Additional results are available at https://360anything.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.16192.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62980664ff0acd7e027d6686",
      "avatarUrl": "/avatars/364d4c8432c24775a099641fc576dbdc.svg",
      "fullname": "Ziyi Wu",
      "name": "Dazitu616",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "60f6cbb2852126bac698c89e",
      "name": "deepmind",
      "fullname": "Deepmind",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1638956859875-5f1158120c833276f61f1a84.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.16004",
      "authors": [
        {
          "_id": "6972e4fefb12c92b735b748c",
          "name": "Christopher Altman",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-22T14:30:09.000Z",
      "submittedOnDailyAt": "2026-01-23T00:34:30.652Z",
      "title": "Wigner's Friend as a Circuit: Inter-Branch Communication Witness Benchmarks on Superconducting Quantum Hardware",
      "submittedOnDailyBy": {
        "_id": "6366110f575c93cedafde54e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6366110f575c93cedafde54e/IEpI-uIMEgLBe-TiFsJ_Q.jpeg",
        "isPro": false,
        "fullname": "Christopher Altman",
        "user": "Cohaerence",
        "type": "user"
      },
      "summary": "We implement and benchmark on IBM Quantum hardware the circuit family proposed by Violaris for estimating operational inter-branch communication witnesses, defined as correlations in classical measurement records produced by compiled Wigner's-friend-style circuits. We realize a five-qubit instance of the protocol as an inter-register message-transfer pattern within a single circuit, rather than physical signaling, and evaluate its behavior under realistic device noise and compilation constraints. The circuit encodes branch-conditioned evolution of an observer subsystem whose dynamics depend on a control qubit, followed by a controlled transfer operation that probes correlations between conditional measurement contexts.\n  Executing on the ibm_fez backend with 20000 shots, we observe population-based visibility of 0.877, coherence witnesses of 0.840 and -0.811 along orthogonal axes, and a phase-sensitive magnitude of approximately 1.17. While the visibility metric is insensitive to some classes of dephasing, the coherence witnesses provide complementary sensitivity to off-diagonal noise.\n  This work does not test or discriminate among interpretations of quantum mechanics. Instead, it provides a reproducible operational constraint pipeline for evaluating detectability of non-ideal channels relative to calibrated device noise.",
      "upvotes": 0,
      "discussionId": "6972e4fefb12c92b735b748d",
      "githubRepo": "https://github.com/christopher-altman/ibm-qml-kernel",
      "githubRepoAddedBy": "user",
      "ai_summary": "Implementation and benchmarking of quantum circuits for estimating operational inter-branch communication witnesses on IBM Quantum hardware demonstrates visibility and coherence witness measurements under realistic device conditions.",
      "ai_keywords": [
        "quantum circuits",
        "Wigner's-friend-style circuits",
        "inter-branch communication witnesses",
        "classical measurement records",
        "compiled circuits",
        "observer subsystem",
        "control qubit",
        "controlled transfer operation",
        "population-based visibility",
        "coherence witnesses",
        "dephasing",
        "off-diagonal noise",
        "quantum hardware",
        "ibm_fez backend",
        "quantum computing"
      ],
      "githubStars": 3
    },
    "publishedAt": "2026-01-22T09:30:09.000Z",
    "title": "Wigner's Friend as a Circuit: Inter-Branch Communication Witness Benchmarks on Superconducting Quantum Hardware",
    "summary": "We implement and benchmark on IBM Quantum hardware the circuit family proposed by Violaris for estimating operational inter-branch communication witnesses, defined as correlations in classical measurement records produced by compiled Wigner's-friend-style circuits. We realize a five-qubit instance of the protocol as an inter-register message-transfer pattern within a single circuit, rather than physical signaling, and evaluate its behavior under realistic device noise and compilation constraints. The circuit encodes branch-conditioned evolution of an observer subsystem whose dynamics depend on a control qubit, followed by a controlled transfer operation that probes correlations between conditional measurement contexts.\n  Executing on the ibm_fez backend with 20000 shots, we observe population-based visibility of 0.877, coherence witnesses of 0.840 and -0.811 along orthogonal axes, and a phase-sensitive magnitude of approximately 1.17. While the visibility metric is insensitive to some classes of dephasing, the coherence witnesses provide complementary sensitivity to off-diagonal noise.\n  This work does not test or discriminate among interpretations of quantum mechanics. Instead, it provides a reproducible operational constraint pipeline for evaluating detectability of non-ideal channels relative to calibrated device noise.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.16004.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6366110f575c93cedafde54e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6366110f575c93cedafde54e/IEpI-uIMEgLBe-TiFsJ_Q.jpeg",
      "fullname": "Christopher Altman",
      "name": "Cohaerence",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.15440",
      "authors": [
        {
          "_id": "69731ad1fb12c92b735b7578",
          "name": "Sandy H. S. Herho",
          "hidden": false
        },
        {
          "_id": "69731ad1fb12c92b735b7579",
          "name": "Faiz R. Fajary",
          "hidden": false
        },
        {
          "_id": "69731ad1fb12c92b735b757a",
          "name": "Iwan P. Anwar",
          "hidden": false
        },
        {
          "_id": "69731ad1fb12c92b735b757b",
          "name": "Faruq Khadami",
          "hidden": false
        },
        {
          "_id": "69731ad1fb12c92b735b757c",
          "name": "Nurjanna J. Trilaksono",
          "hidden": false
        },
        {
          "_id": "69731ad1fb12c92b735b757d",
          "name": "Rusmawan Suwarman",
          "hidden": false
        },
        {
          "_id": "69731ad1fb12c92b735b757e",
          "name": "Dasapta E. Irawan",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/68d51061ab9204b0c8a0ceb2/xc2sFaH8p_t0CDjm1vfUL.gif"
      ],
      "publishedAt": "2026-01-21T20:10:12.000Z",
      "submittedOnDailyAt": "2026-01-23T04:33:19.018Z",
      "title": "Numba-Accelerated 2D Diffusion-Limited Aggregation: Implementation and Fractal Characterization",
      "submittedOnDailyBy": {
        "_id": "68d51061ab9204b0c8a0ceb2",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68d51061ab9204b0c8a0ceb2/CVmg6wo4yBqIZgGZUYSX8.jpeg",
        "isPro": false,
        "fullname": "Sandy Hardian Susanto Herho",
        "user": "sandyherho",
        "type": "user"
      },
      "summary": "We present dla-ideal-solver, a high-performance framework for simulating two-dimensional Diffusion-Limited Aggregation (DLA) using Numba-accelerated Python. By leveraging just-in-time (JIT) compilation, we achieve computational throughput comparable to legacy static implementations while retaining high-level flexibility. We investigate the Laplacian growth instability across varying injection geometries and walker concentrations. Our analysis confirms the robustness of the standard fractal dimension D_f approx 1.71 for dilute regimes, consistent with the Witten-Sander universality class. However, we report a distinct crossover to Eden-like compact growth (D_f approx 1.87) in high-density environments, attributed to the saturation of the screening length. Beyond standard mass-radius scaling, we employ generalized Rényi dimensions and lacunarity metrics to quantify the monofractal character and spatial heterogeneity of the aggregates. This work establishes a reproducible, open-source testbed for exploring phase transitions in non-equilibrium statistical mechanics.",
      "upvotes": 0,
      "discussionId": "69731ad2fb12c92b735b757f",
      "projectPage": "https://pypi.org/project/dla-ideal-solver/",
      "githubRepo": "https://github.com/sandyherho/dla-ideal-solver",
      "githubRepoAddedBy": "user",
      "githubStars": 1,
      "organization": {
        "_id": "643f6b84ec817b766686ba98",
        "name": "ITB",
        "fullname": "Institut Teknologi Bandung",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/FLKHDmfoM8RvukvHEgXj4.png"
      }
    },
    "publishedAt": "2026-01-21T15:10:12.000Z",
    "title": "Numba-Accelerated 2D Diffusion-Limited Aggregation: Implementation and Fractal Characterization",
    "summary": "We present dla-ideal-solver, a high-performance framework for simulating two-dimensional Diffusion-Limited Aggregation (DLA) using Numba-accelerated Python. By leveraging just-in-time (JIT) compilation, we achieve computational throughput comparable to legacy static implementations while retaining high-level flexibility. We investigate the Laplacian growth instability across varying injection geometries and walker concentrations. Our analysis confirms the robustness of the standard fractal dimension D_f approx 1.71 for dilute regimes, consistent with the Witten-Sander universality class. However, we report a distinct crossover to Eden-like compact growth (D_f approx 1.87) in high-density environments, attributed to the saturation of the screening length. Beyond standard mass-radius scaling, we employ generalized Rényi dimensions and lacunarity metrics to quantify the monofractal character and spatial heterogeneity of the aggregates. This work establishes a reproducible, open-source testbed for exploring phase transitions in non-equilibrium statistical mechanics.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/68d51061ab9204b0c8a0ceb2/xc2sFaH8p_t0CDjm1vfUL.gif"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.15440.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "68d51061ab9204b0c8a0ceb2",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68d51061ab9204b0c8a0ceb2/CVmg6wo4yBqIZgGZUYSX8.jpeg",
      "fullname": "Sandy Hardian Susanto Herho",
      "name": "sandyherho",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "643f6b84ec817b766686ba98",
      "name": "ITB",
      "fullname": "Institut Teknologi Bandung",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/FLKHDmfoM8RvukvHEgXj4.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.14255",
      "authors": [
        {
          "_id": "697324effb12c92b735b7592",
          "name": "Sangbeom Lim",
          "hidden": false
        },
        {
          "_id": "697324effb12c92b735b7593",
          "name": "Seoung Wug Oh",
          "hidden": false
        },
        {
          "_id": "697324effb12c92b735b7594",
          "name": "Jiahui Huang",
          "hidden": false
        },
        {
          "_id": "697324effb12c92b735b7595",
          "name": "Heeji Yoon",
          "hidden": false
        },
        {
          "_id": "697324effb12c92b735b7596",
          "name": "Seungryong Kim",
          "hidden": false
        },
        {
          "_id": "697324effb12c92b735b7597",
          "name": "Joon-Young Lee",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64842d1edc475c315e41123a/NTmavgTwpypwK_8HUh_o2.mp4"
      ],
      "publishedAt": "2026-01-20T18:59:56.000Z",
      "submittedOnDailyAt": "2026-01-23T05:07:12.040Z",
      "title": "VideoMaMa: Mask-Guided Video Matting via Generative Prior",
      "submittedOnDailyBy": {
        "_id": "64842d1edc475c315e41123a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64842d1edc475c315e41123a/Yqam_dy0MnQm4Rn1_1_4N.jpeg",
        "isPro": true,
        "fullname": "Sangbeom Lim",
        "user": "SammyLim",
        "type": "user"
      },
      "summary": "Generalizing video matting models to real-world videos remains a significant challenge due to the scarcity of labeled data. To address this, we present Video Mask-to-Matte Model (VideoMaMa) that converts coarse segmentation masks into pixel accurate alpha mattes, by leveraging pretrained video diffusion models. VideoMaMa demonstrates strong zero-shot generalization to real-world footage, even though it is trained solely on synthetic data. Building on this capability, we develop a scalable pseudo-labeling pipeline for large-scale video matting and construct the Matting Anything in Video (MA-V) dataset, which offers high-quality matting annotations for more than 50K real-world videos spanning diverse scenes and motions. To validate the effectiveness of this dataset, we fine-tune the SAM2 model on MA-V to obtain SAM2-Matte, which outperforms the same model trained on existing matting datasets in terms of robustness on in-the-wild videos. These findings emphasize the importance of large-scale pseudo-labeled video matting and showcase how generative priors and accessible segmentation cues can drive scalable progress in video matting research.",
      "upvotes": 0,
      "discussionId": "697324f0fb12c92b735b7598",
      "ai_summary": "VideoMaMa converts coarse masks to accurate alpha mattes using pretrained video diffusion models, enabling zero-shot generalization and scalable pseudo-labeling for video matting.",
      "ai_keywords": [
        "video matting",
        "video diffusion models",
        "pseudo-labeling",
        "Matting Anything in Video",
        "SAM2",
        "alpha mattes",
        "coarse segmentation masks",
        "zero-shot generalization"
      ],
      "organization": {
        "_id": "61e5d14f77496de0a6d95c6b",
        "name": "adobe",
        "fullname": "Adobe",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1645217431826-61e35e517ac6b6d06cfa8081.png"
      }
    },
    "publishedAt": "2026-01-20T13:59:56.000Z",
    "title": "VideoMaMa: Mask-Guided Video Matting via Generative Prior",
    "summary": "Generalizing video matting models to real-world videos remains a significant challenge due to the scarcity of labeled data. To address this, we present Video Mask-to-Matte Model (VideoMaMa) that converts coarse segmentation masks into pixel accurate alpha mattes, by leveraging pretrained video diffusion models. VideoMaMa demonstrates strong zero-shot generalization to real-world footage, even though it is trained solely on synthetic data. Building on this capability, we develop a scalable pseudo-labeling pipeline for large-scale video matting and construct the Matting Anything in Video (MA-V) dataset, which offers high-quality matting annotations for more than 50K real-world videos spanning diverse scenes and motions. To validate the effectiveness of this dataset, we fine-tune the SAM2 model on MA-V to obtain SAM2-Matte, which outperforms the same model trained on existing matting datasets in terms of robustness on in-the-wild videos. These findings emphasize the importance of large-scale pseudo-labeled video matting and showcase how generative priors and accessible segmentation cues can drive scalable progress in video matting research.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64842d1edc475c315e41123a/NTmavgTwpypwK_8HUh_o2.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.14255.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64842d1edc475c315e41123a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64842d1edc475c315e41123a/Yqam_dy0MnQm4Rn1_1_4N.jpeg",
      "fullname": "Sangbeom Lim",
      "name": "SammyLim",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "61e5d14f77496de0a6d95c6b",
      "name": "adobe",
      "fullname": "Adobe",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1645217431826-61e35e517ac6b6d06cfa8081.png"
    },
    "isAuthorParticipating": false
  }
]