[
    "{'paper': {'id': '2501.00958', 'authors': [{'_id': '67776143829d5102834151fb', 'name': 'Wenqi Zhang', 'hidden': False}, {'_id': '67776143829d5102834151fc', 'name': 'Hang Zhang', 'hidden': False}, {'_id': '67776143829d5102834151fd', 'name': 'Xin Li', 'hidden': False}, {'_id': '67776143829d5102834151fe', 'name': 'Jiashuo Sun', 'hidden': False}, {'_id': '67776143829d5102834151ff', 'name': 'Yongliang Shen', 'hidden': False}, {'_id': '67776143829d510283415200', 'name': 'Weiming Lu', 'hidden': False}, {'_id': '67776143829d510283415201', 'name': 'Deli Zhao', 'hidden': False}, {'_id': '67776143829d510283415202', 'name': 'Yueting Zhuang', 'hidden': False}, {'_id': '67776143829d510283415203', 'name': 'Lidong Bing', 'hidden': False}], 'publishedAt': '2025-01-01T21:29:37.000Z', 'title': '2.5 Years in Class: A Multimodal Textbook for Vision-Language\\n  Pretraining', 'summary': 'Compared to image-text pair data, interleaved corpora enable Vision-Language\\nModels (VLMs) to understand the world more naturally like humans. However, such\\nexisting datasets are crawled from webpage, facing challenges like low\\nknowledge density, loose image-text relations, and poor logical coherence\\nbetween images. On the other hand, the internet hosts vast instructional videos\\n(e.g., online geometry courses) that are widely used by humans to learn\\nfoundational subjects, yet these valuable resources remain underexplored in VLM\\ntraining. In this paper, we introduce a high-quality multimodal\\ntextbook corpus with richer foundational knowledge for VLM pretraining. It\\ncollects over 2.5 years of instructional videos, totaling 22,000 class hours.\\nWe first use an LLM-proposed taxonomy to systematically gather instructional\\nvideos. Then we progressively extract and refine visual (keyframes), audio\\n(ASR), and textual knowledge (OCR) from the videos, and organize as an\\nimage-text interleaved corpus based on temporal order. Compared to its\\ncounterparts, our video-centric textbook offers more coherent context, richer\\nknowledge, and better image-text alignment. Experiments demonstrate its superb\\npretraining performance, particularly in knowledge- and reasoning-intensive\\ntasks like ScienceQA and MathVista. Moreover, VLMs pre-trained on our textbook\\nexhibit outstanding interleaved context awareness, leveraging visual and\\ntextual cues in their few-shot context for task solving~Our code are\\navailable at \\\\url{https://github.com/DAMO-NLP-SG/multimodal_textbook}.', 'upvotes': 37, 'discussionId': '67776145829d5102834152a8'}, 'publishedAt': '2025-01-02T23:45:19.765Z', 'title': '2.5 Years in Class: A Multimodal Textbook for Vision-Language Pretraining', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.00958.png', 'numComments': 6, 'submittedBy': {'_id': '6485bd278d14bcd5cdbb7c8d', 'avatarUrl': '/avatars/1427cf1a72b5db0cb263ad45885cf925.svg', 'fullname': 'Wenqi Zhang', 'name': 'zwq2018', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 4}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2501.01427', 'authors': [{'_id': '677752371ab3b33411033089', 'user': {'_id': '65fd2da40e543c5a84586eb5', 'avatarUrl': '/avatars/ebb4e4bda4b025f167fb9fb4099e4cfd.svg', 'isPro': False, 'fullname': 'yuanpeng', 'user': 'Tuyuanpeng', 'type': 'user'}, 'name': 'Yuanpeng Tu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-03T14:03:30.344Z', 'hidden': False}, {'_id': '677752371ab3b3341103308a', 'name': 'Hao Luo', 'hidden': False}, {'_id': '677752371ab3b3341103308b', 'name': 'Xi Chen', 'hidden': False}, {'_id': '677752371ab3b3341103308c', 'name': 'Sihui Ji', 'hidden': False}, {'_id': '677752371ab3b3341103308d', 'name': 'Xiang Bai', 'hidden': False}, {'_id': '677752371ab3b3341103308e', 'name': 'Hengshuang Zhao', 'hidden': False}], 'publishedAt': '2025-01-02T18:59:54.000Z', 'title': 'VideoAnydoor: High-fidelity Video Object Insertion with Precise Motion\\n  Control', 'summary': 'Despite significant advancements in video generation, inserting a given\\nobject into videos remains a challenging task. The difficulty lies in\\npreserving the appearance details of the reference object and accurately\\nmodeling coherent motions at the same time. In this paper, we propose\\nVideoAnydoor, a zero-shot video object insertion framework with high-fidelity\\ndetail preservation and precise motion control. Starting from a text-to-video\\nmodel, we utilize an ID extractor to inject the global identity and leverage a\\nbox sequence to control the overall motion. To preserve the detailed appearance\\nand meanwhile support fine-grained motion control, we design a pixel warper. It\\ntakes the reference image with arbitrary key-points and the corresponding\\nkey-point trajectories as inputs. It warps the pixel details according to the\\ntrajectories and fuses the warped features with the diffusion U-Net, thus\\nimproving detail preservation and supporting users in manipulating the motion\\ntrajectories. In addition, we propose a training strategy involving both videos\\nand static images with a reweight reconstruction loss to enhance insertion\\nquality. VideoAnydoor demonstrates significant superiority over existing\\nmethods and naturally supports various downstream applications (e.g., talking\\nhead generation, video virtual try-on, multi-region editing) without\\ntask-specific fine-tuning.', 'upvotes': 30, 'discussionId': '6777523c1ab3b3341103325a'}, 'publishedAt': '2025-01-02T22:41:28.381Z', 'title': 'VideoAnydoor: High-fidelity Video Object Insertion with Precise Motion Control', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.01427.png', 'numComments': 1, 'submittedBy': {'_id': '644a1b6401e18bf93a6f45c1', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/644a1b6401e18bf93a6f45c1/P0i_CgCrIzOS2tYRlxoE9.png', 'fullname': 'xichen', 'name': 'xichenhku', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 40}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2501.00599', 'authors': [{'_id': '677761253c2cb54a3ac7918e', 'name': 'Yuqian Yuan', 'hidden': False}, {'_id': '677761253c2cb54a3ac7918f', 'name': 'Hang Zhang', 'hidden': False}, {'_id': '677761253c2cb54a3ac79190', 'user': {'_id': '64c48a78d07620bdc99777d4', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64c48a78d07620bdc99777d4/NJC4Ot0a7YSdU5RC6dgga.jpeg', 'isPro': False, 'fullname': 'LI WENTONG', 'user': 'sunshine-lwt', 'type': 'user'}, 'name': 'Wentong Li', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-03T14:03:13.776Z', 'hidden': False}, {'_id': '677761253c2cb54a3ac79191', 'name': 'Zesen Cheng', 'hidden': False}, {'_id': '677761253c2cb54a3ac79192', 'name': 'Boqiang Zhang', 'hidden': False}, {'_id': '677761253c2cb54a3ac79193', 'name': 'Long Li', 'hidden': False}, {'_id': '677761253c2cb54a3ac79194', 'name': 'Xin Li', 'hidden': False}, {'_id': '677761253c2cb54a3ac79195', 'name': 'Deli Zhao', 'hidden': False}, {'_id': '677761253c2cb54a3ac79196', 'name': 'Wenqiao Zhang', 'hidden': False}, {'_id': '677761253c2cb54a3ac79197', 'name': 'Yueting Zhuang', 'hidden': False}, {'_id': '677761253c2cb54a3ac79198', 'name': 'Jianke Zhu', 'hidden': False}, {'_id': '677761253c2cb54a3ac79199', 'name': 'Lidong Bing', 'hidden': False}], 'publishedAt': '2024-12-31T18:56:46.000Z', 'title': 'VideoRefer Suite: Advancing Spatial-Temporal Object Understanding with\\n  Video LLM', 'summary': 'Video Large Language Models (Video LLMs) have recently exhibited remarkable\\ncapabilities in general video understanding. However, they mainly focus on\\nholistic comprehension and struggle with capturing fine-grained spatial and\\ntemporal details. Besides, the lack of high-quality object-level video\\ninstruction data and a comprehensive benchmark further hinders their\\nadvancements. To tackle these challenges, we introduce the VideoRefer Suite to\\nempower Video LLM for finer-level spatial-temporal video understanding, i.e.,\\nenabling perception and reasoning on any objects throughout the video.\\nSpecially, we thoroughly develop VideoRefer Suite across three essential\\naspects: dataset, model, and benchmark. Firstly, we introduce a multi-agent\\ndata engine to meticulously curate a large-scale, high-quality object-level\\nvideo instruction dataset, termed VideoRefer-700K. Next, we present the\\nVideoRefer model, which equips a versatile spatial-temporal object encoder to\\ncapture precise regional and sequential representations. Finally, we\\nmeticulously create a VideoRefer-Bench to comprehensively assess the\\nspatial-temporal understanding capability of a Video LLM, evaluating it across\\nvarious aspects. Extensive experiments and analyses demonstrate that our\\nVideoRefer model not only achieves promising performance on video referring\\nbenchmarks but also facilitates general video understanding capabilities.', 'upvotes': 27, 'discussionId': '677761283c2cb54a3ac79251'}, 'publishedAt': '2025-01-02T23:09:43.423Z', 'title': 'VideoRefer Suite: Advancing Spatial-Temporal Object Understanding with Video LLM', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/64a3fe3dde901eb01df12398/FXY8u9gsbbaE2-0k8DH9r.mp4'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.00599.png', 'numComments': 1, 'submittedBy': {'_id': '64a3fe3dde901eb01df12398', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64a3fe3dde901eb01df12398/Js2bEx4rxKuEKVt5z9I2D.jpeg', 'fullname': 'YuqianYuan', 'name': 'CircleRadon', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 3}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2501.01257', 'authors': [{'_id': '6777515fab01c44c478e51e9', 'user': {'_id': '64b9954845ce8d7ad607c14d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64b9954845ce8d7ad607c14d/LZ9yeTOz4J_YKrnGkcmnL.jpeg', 'isPro': False, 'fullname': 'Shanghaoran Quan', 'user': 'quanshr', 'type': 'user'}, 'name': 'Shanghaoran Quan', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-03T14:03:42.563Z', 'hidden': False}, {'_id': '6777515fab01c44c478e51ea', 'name': 'Jiaxi Yang', 'hidden': False}, {'_id': '6777515fab01c44c478e51eb', 'name': 'Bowen Yu', 'hidden': False}, {'_id': '6777515fab01c44c478e51ec', 'name': 'Bo Zheng', 'hidden': False}, {'_id': '6777515fab01c44c478e51ed', 'name': 'Dayiheng Liu', 'hidden': False}, {'_id': '6777515fab01c44c478e51ee', 'name': 'An Yang', 'hidden': False}, {'_id': '6777515fab01c44c478e51ef', 'name': 'Xuancheng Ren', 'hidden': False}, {'_id': '6777515fab01c44c478e51f0', 'name': 'Bofei Gao', 'hidden': False}, {'_id': '6777515fab01c44c478e51f1', 'name': 'Yibo Miao', 'hidden': False}, {'_id': '6777515fab01c44c478e51f2', 'user': {'_id': '5df83428da6d0311fd3d5404', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1613644244177-5df83428da6d0311fd3d5404.jpeg', 'isPro': False, 'fullname': 'Feng YunLong', 'user': 'ylfeng', 'type': 'user'}, 'name': 'Yunlong Feng', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-03T14:03:38.392Z', 'hidden': False}, {'_id': '6777515fab01c44c478e51f3', 'name': 'Zekun Wang', 'hidden': False}, {'_id': '6777515fab01c44c478e51f4', 'name': 'Jian Yang', 'hidden': False}, {'_id': '6777515fab01c44c478e51f5', 'name': 'Zeyu Cui', 'hidden': False}, {'_id': '6777515fab01c44c478e51f6', 'name': 'Yang Fan', 'hidden': False}, {'_id': '6777515fab01c44c478e51f7', 'name': 'Yichang Zhang', 'hidden': False}, {'_id': '6777515fab01c44c478e51f8', 'user': {'_id': '61e4c4ca1ab24785ac11ba69', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/61e4c4ca1ab24785ac11ba69/1Q1zhhyGSJ9RJG9MzwxVv.jpeg', 'isPro': False, 'fullname': 'Binyuan Hui', 'user': 'huybery', 'type': 'user'}, 'name': 'Binyuan Hui', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-03T14:03:40.621Z', 'hidden': False}, {'_id': '6777515fab01c44c478e51f9', 'name': 'Junyang Lin', 'hidden': False}], 'publishedAt': '2025-01-02T13:49:00.000Z', 'title': 'CodeElo: Benchmarking Competition-level Code Generation of LLMs with\\n  Human-comparable Elo Ratings', 'summary': 'With the increasing code reasoning capabilities of existing large language\\nmodels (LLMs) and breakthroughs in reasoning models like OpenAI o1 and o3,\\nthere is a growing need to develop more challenging and comprehensive\\nbenchmarks that effectively test their sophisticated competition-level coding\\nabilities. Existing benchmarks, like LiveCodeBench and USACO, fall short due to\\nthe unavailability of private test cases, lack of support for special judges,\\nand misaligned execution environments. To bridge this gap, we introduce\\nCodeElo, a standardized competition-level code generation benchmark that\\neffectively addresses all these challenges for the first time. CodeElo\\nbenchmark is mainly based on the official CodeForces platform and tries to\\nalign with the platform as much as possible. We compile the recent six months\\nof contest problems on CodeForces with detailed information such as contest\\ndivisions, problem difficulty ratings, and problem algorithm tags. We introduce\\na unique judging method in which problems are submitted directly to the\\nplatform and develop a reliable Elo rating calculation system that aligns with\\nthe platform and is comparable with human participants but has lower variance.\\nBy testing on our CodeElo, we provide the Elo ratings of 30 existing popular\\nopen-source and 3 proprietary LLMs for the first time. The results show that\\no1-mini and QwQ-32B-Preview stand out significantly, achieving Elo ratings of\\n1578 and 1261, respectively, while other models struggle even with the easiest\\nproblems, placing in the lowest 20 percent among all human participants.\\nDetailed analysis experiments are also conducted to provide insights into\\nperformance across algorithms and comparisons between using C++ and Python,\\nwhich can suggest directions for future studies.', 'upvotes': 24, 'discussionId': '67775160ab01c44c478e5259'}, 'publishedAt': '2025-01-02T23:34:02.756Z', 'title': 'CodeElo: Benchmarking Competition-level Code Generation of LLMs with Human-comparable Elo Ratings', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.01257.png', 'numComments': 1, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isMod': False, 'followerCount': 5533}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2501.01423', 'authors': [{'_id': '677753a28376dfe003a3fbd3', 'name': 'Jingfeng Yao', 'hidden': False}, {'_id': '677753a28376dfe003a3fbd4', 'name': 'Xinggang Wang', 'hidden': False}], 'publishedAt': '2025-01-02T18:59:40.000Z', 'title': 'Reconstruction vs. Generation: Taming Optimization Dilemma in Latent\\n  Diffusion Models', 'summary': 'Latent diffusion models with Transformer architectures excel at generating\\nhigh-fidelity images. However, recent studies reveal an optimization dilemma in\\nthis two-stage design: while increasing the per-token feature dimension in\\nvisual tokenizers improves reconstruction quality, it requires substantially\\nlarger diffusion models and more training iterations to achieve comparable\\ngeneration performance. Consequently, existing systems often settle for\\nsub-optimal solutions, either producing visual artifacts due to information\\nloss within tokenizers or failing to converge fully due to expensive\\ncomputation costs. We argue that this dilemma stems from the inherent\\ndifficulty in learning unconstrained high-dimensional latent spaces. To address\\nthis, we propose aligning the latent space with pre-trained vision foundation\\nmodels when training the visual tokenizers. Our proposed VA-VAE (Vision\\nfoundation model Aligned Variational AutoEncoder) significantly expands the\\nreconstruction-generation frontier of latent diffusion models, enabling faster\\nconvergence of Diffusion Transformers (DiT) in high-dimensional latent spaces.\\nTo exploit the full potential of VA-VAE, we build an enhanced DiT baseline with\\nimproved training strategies and architecture designs, termed LightningDiT. The\\nintegrated system achieves state-of-the-art (SOTA) performance on ImageNet\\n256x256 generation with an FID score of 1.35 while demonstrating remarkable\\ntraining efficiency by reaching an FID score of 2.11 in just 64\\nepochs--representing an over 21 times convergence speedup compared to the\\noriginal DiT. Models and codes are available at:\\nhttps://github.com/hustvl/LightningDiT.', 'upvotes': 24, 'discussionId': '677753a38376dfe003a3fc2b'}, 'publishedAt': '2025-01-02T22:04:45.023Z', 'title': 'Reconstruction vs. Generation: Taming Optimization Dilemma in Latent Diffusion Models', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.01423.png', 'numComments': 1, 'submittedBy': {'_id': '6375dfa7f9aafd41ce145254', 'avatarUrl': '/avatars/19e7460dd7ea6c60e1c52d5707660cc8.svg', 'fullname': 'Lunbin Zeng', 'name': 'xiazhi', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2501.01264', 'authors': [{'_id': '677751f23308d0c478a26abe', 'name': 'Xiaoshuai Song', 'hidden': False}, {'_id': '677751f23308d0c478a26abf', 'name': 'Yanan Wu', 'hidden': False}, {'_id': '677751f23308d0c478a26ac0', 'name': 'Weixun Wang', 'hidden': False}, {'_id': '677751f23308d0c478a26ac1', 'name': 'Jiaheng Liu', 'hidden': False}, {'_id': '677751f23308d0c478a26ac2', 'name': 'Wenbo Su', 'hidden': False}, {'_id': '677751f23308d0c478a26ac3', 'name': 'Bo Zheng', 'hidden': False}], 'publishedAt': '2025-01-02T13:59:20.000Z', 'title': 'ProgCo: Program Helps Self-Correction of Large Language Models', 'summary': 'Self-Correction aims to enable large language models (LLMs) to self-verify\\nand self-refine their initial responses without external feedback. However,\\nLLMs often fail to effectively self-verify and generate correct feedback,\\nfurther misleading refinement and leading to the failure of self-correction,\\nespecially in complex reasoning tasks. In this paper, we propose Program-driven\\nSelf-Correction (ProgCo). First, program-driven verification (ProgVe) achieves\\ncomplex verification logic and extensive validation through self-generated,\\nself-executing verification pseudo-programs. Then, program-driven refinement\\n(ProgRe) receives feedback from ProgVe, conducts dual reflection and refinement\\non both responses and verification programs to mitigate misleading of incorrect\\nfeedback in complex reasoning tasks. Experiments on three instruction-following\\nand mathematical benchmarks indicate that ProgCo achieves effective\\nself-correction, and can be further enhance performance when combined with real\\nprogram tools.', 'upvotes': 17, 'discussionId': '677751f33308d0c478a26b14'}, 'publishedAt': '2025-01-02T21:58:41.673Z', 'title': 'ProgCo: Program Helps Self-Correction of Large Language Models', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.01264.png', 'numComments': 1, 'submittedBy': {'_id': '61cd4b833dd34ba1985e0753', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/61cd4b833dd34ba1985e0753/BfHfrwotoMESpXZOHiIe4.png', 'fullname': 'KABI', 'name': 'dongguanting', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 12}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2501.00316', 'authors': [{'_id': '6777829e2f92b6f8edf97422', 'user': {'_id': '6565a60e0ff3292512bae26d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6565a60e0ff3292512bae26d/5P6wgEhCI0ndFVgX9KX8k.jpeg', 'isPro': False, 'fullname': 'Mahir Labib Dihan', 'user': 'mahirlabibdihan', 'type': 'user'}, 'name': 'Mahir Labib Dihan', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-03T14:03:04.107Z', 'hidden': False}, {'_id': '6777829e2f92b6f8edf97423', 'name': 'Md Tanvir Hassan', 'hidden': False}, {'_id': '6777829e2f92b6f8edf97424', 'name': 'Md Tanvir Parvez', 'hidden': False}, {'_id': '6777829e2f92b6f8edf97425', 'name': 'Md Hasebul Hasan', 'hidden': False}, {'_id': '6777829e2f92b6f8edf97426', 'name': 'Md Almash Alam', 'hidden': False}, {'_id': '6777829e2f92b6f8edf97427', 'name': 'Muhammad Aamir Cheema', 'hidden': False}, {'_id': '6777829e2f92b6f8edf97428', 'user': {'_id': '6777b4b1414115dd5bcd0991', 'avatarUrl': '/avatars/6d2f6283823c79f70d99732c656d7f4a.svg', 'isPro': False, 'fullname': 'Mohammed Eunus Ali', 'user': 'eunus', 'type': 'user'}, 'name': 'Mohammed Eunus Ali', 'status': 'extracted_pending', 'statusLastChangedAt': '2025-01-03T09:58:41.523Z', 'hidden': False}, {'_id': '6777829e2f92b6f8edf97429', 'name': 'Md Rizwan Parvez', 'hidden': False}], 'publishedAt': '2024-12-31T07:20:32.000Z', 'title': 'MapEval: A Map-Based Evaluation of Geo-Spatial Reasoning in Foundation\\n  Models', 'summary': \"Recent advancements in foundation models have enhanced AI systems'\\ncapabilities in autonomous tool usage and reasoning. However, their ability in\\nlocation or map-based reasoning - which improves daily life by optimizing\\nnavigation, facilitating resource discovery, and streamlining logistics - has\\nnot been systematically studied. To bridge this gap, we introduce MapEval, a\\nbenchmark designed to assess diverse and complex map-based user queries with\\ngeo-spatial reasoning. MapEval features three task types (textual, API-based,\\nand visual) that require collecting world information via map tools, processing\\nheterogeneous geo-spatial contexts (e.g., named entities, travel distances,\\nuser reviews or ratings, images), and compositional reasoning, which all\\nstate-of-the-art foundation models find challenging. Comprising 700 unique\\nmultiple-choice questions about locations across 180 cities and 54 countries,\\nMapEval evaluates foundation models' ability to handle spatial relationships,\\nmap infographics, travel planning, and navigation challenges. Using MapEval, we\\nconducted a comprehensive evaluation of 28 prominent foundation models. While\\nno single model excelled across all tasks, Claude-3.5-Sonnet, GPT-4o, and\\nGemini-1.5-Pro achieved competitive performance overall. However, substantial\\nperformance gaps emerged, particularly in MapEval, where agents with\\nClaude-3.5-Sonnet outperformed GPT-4o and Gemini-1.5-Pro by 16% and 21%,\\nrespectively, and the gaps became even more amplified when compared to\\nopen-source LLMs. Our detailed analyses provide insights into the strengths and\\nweaknesses of current models, though all models still fall short of human\\nperformance by more than 20% on average, struggling with complex map images and\\nrigorous geo-spatial reasoning. This gap highlights MapEval's critical role in\\nadvancing general-purpose foundation models with stronger geo-spatial\\nunderstanding.\", 'upvotes': 14, 'discussionId': '6777829f2f92b6f8edf9749d'}, 'publishedAt': '2025-01-03T01:57:16.057Z', 'title': 'MapEval: A Map-Based Evaluation of Geo-Spatial Reasoning in Foundation Models', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.00316.png', 'numComments': 1, 'submittedBy': {'_id': '6565a60e0ff3292512bae26d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6565a60e0ff3292512bae26d/5P6wgEhCI0ndFVgX9KX8k.jpeg', 'fullname': 'Mahir Labib Dihan', 'name': 'mahirlabibdihan', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2501.01149', 'authors': [{'_id': '6777587d7fcef9e7b225cbae', 'user': {'_id': '6458ce236fa580137af5aa95', 'avatarUrl': '/avatars/db65a7332e375eb5daad5c1b076b1e3b.svg', 'isPro': False, 'fullname': 'Yuxiang Chai', 'user': 'Yuxiang007', 'type': 'user'}, 'name': 'Yuxiang Chai', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-03T14:03:27.243Z', 'hidden': False}, {'_id': '6777587d7fcef9e7b225cbaf', 'name': 'Hanhao Li', 'hidden': False}, {'_id': '6777587d7fcef9e7b225cbb0', 'name': 'Jiayu Zhang', 'hidden': False}, {'_id': '6777587d7fcef9e7b225cbb1', 'name': 'Liang Liu', 'hidden': False}, {'_id': '6777587d7fcef9e7b225cbb2', 'name': 'Guozhi Wang', 'hidden': False}, {'_id': '6777587d7fcef9e7b225cbb3', 'name': 'Shuai Ren', 'hidden': False}, {'_id': '6777587d7fcef9e7b225cbb4', 'name': 'Siyuan Huang', 'hidden': False}, {'_id': '6777587d7fcef9e7b225cbb5', 'name': 'Hongsheng Li', 'hidden': False}], 'publishedAt': '2025-01-02T09:03:56.000Z', 'title': 'A3: Android Agent Arena for Mobile GUI Agents', 'summary': 'AI agents have become increasingly prevalent in recent years, driven by\\nsignificant advancements in the field of large language models (LLMs). Mobile\\nGUI agents, a subset of AI agents, are designed to autonomously perform tasks\\non mobile devices. While numerous studies have introduced agents, datasets, and\\nbenchmarks to advance mobile GUI agent research, many existing datasets focus\\non static frame evaluations and fail to provide a comprehensive platform for\\nassessing performance on real-world, in-the-wild tasks. To address this gap, we\\npresent Android Agent Arena (A3), a novel evaluation platform. Unlike existing\\nin-the-wild systems, A3 offers: (1) meaningful and practical tasks, such as\\nreal-time online information retrieval and operational instructions; (2) a\\nlarger, more flexible action space, enabling compatibility with agents trained\\non any dataset; and (3) automated business-level LLM-based evaluation process.\\nA3 includes 21 widely used general third-party apps and 201 tasks\\nrepresentative of common user scenarios, providing a robust foundation for\\nevaluating mobile GUI agents in real-world situations and a new autonomous\\nevaluation process for less human labor and coding expertise. The project is\\navailable at https://yuxiangchai.github.io/Android-Agent-Arena/.', 'upvotes': 13, 'discussionId': '677758817fcef9e7b225cf0a'}, 'publishedAt': '2025-01-02T22:28:45.077Z', 'title': 'A3: Android Agent Arena for Mobile GUI Agents', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.01149.png', 'numComments': 2, 'submittedBy': {'_id': '6458ce236fa580137af5aa95', 'avatarUrl': '/avatars/db65a7332e375eb5daad5c1b076b1e3b.svg', 'fullname': 'Yuxiang Chai', 'name': 'Yuxiang007', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2501.01054', 'authors': [{'_id': '67774afb5c6bccc41be7ba19', 'user': {'_id': '6384c07fdfffab4824ff45fb', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1669644372381-noauth.jpeg', 'isPro': False, 'fullname': 'Zeyao Ma', 'user': 'KAKA22', 'type': 'user'}, 'name': 'Zeyao Ma', 'status': 'extracted_pending', 'statusLastChangedAt': '2025-01-03T02:27:07.916Z', 'hidden': False}, {'_id': '67774afb5c6bccc41be7ba1a', 'name': 'Xiaokang Zhang', 'hidden': False}, {'_id': '67774afb5c6bccc41be7ba1b', 'name': 'Jing Zhang', 'hidden': False}, {'_id': '67774afb5c6bccc41be7ba1c', 'name': 'Jifan Yu', 'hidden': False}, {'_id': '67774afb5c6bccc41be7ba1d', 'name': 'Sijia Luo', 'hidden': False}, {'_id': '67774afb5c6bccc41be7ba1e', 'name': 'Jie Tang', 'hidden': False}], 'publishedAt': '2025-01-02T04:33:31.000Z', 'title': 'Dynamic Scaling of Unit Tests for Code Reward Modeling', 'summary': 'Current large language models (LLMs) often struggle to produce accurate\\nresponses on the first attempt for complex reasoning tasks like code\\ngeneration. Prior research tackles this challenge by generating multiple\\ncandidate solutions and validating them with LLM-generated unit tests. The\\nexecution results of unit tests serve as reward signals to identify correct\\nsolutions. As LLMs always confidently make mistakes, these unit tests are not\\nreliable, thereby diminishing the quality of reward signals. Motivated by the\\nobservation that scaling the number of solutions improves LLM performance, we\\nexplore the impact of scaling unit tests to enhance reward signal quality. Our\\npioneer experiment reveals a positive correlation between the number of unit\\ntests and reward signal quality, with greater benefits observed in more\\nchallenging problems. Based on these insights, we propose CodeRM-8B, a\\nlightweight yet effective unit test generator that enables efficient and\\nhigh-quality unit test scaling. Additionally, we implement a dynamic scaling\\nmechanism that adapts the number of unit tests based on problem difficulty,\\nfurther improving efficiency. Experimental results show that our approach\\nsignificantly improves performance across various models on three benchmarks\\n(e.g., with gains of 18.43% for Llama3-8B and 3.42% for GPT-4o-mini on\\nHumanEval Plus).', 'upvotes': 12, 'discussionId': '67774afb5c6bccc41be7ba62'}, 'publishedAt': '2025-01-02T22:59:12.319Z', 'title': 'Dynamic Scaling of Unit Tests for Code Reward Modeling', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.01054.png', 'numComments': 1, 'submittedBy': {'_id': '6384c07fdfffab4824ff45fb', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1669644372381-noauth.jpeg', 'fullname': 'Zeyao Ma', 'name': 'KAKA22', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2501.00192', 'authors': [{'_id': '6777591138f9a731d4f575bb', 'user': {'_id': '64dfcc62e8b6f3f3baa950e0', 'avatarUrl': '/avatars/21bbff67d46c08044efe2406575aa77e.svg', 'isPro': False, 'fullname': 'Zhenting Wang', 'user': 'ztwang', 'type': 'user'}, 'name': 'Zhenting Wang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-03T14:03:15.648Z', 'hidden': False}, {'_id': '6777591138f9a731d4f575bc', 'name': 'Shuming Hu', 'hidden': False}, {'_id': '6777591138f9a731d4f575bd', 'name': 'Shiyu Zhao', 'hidden': False}, {'_id': '6777591138f9a731d4f575be', 'name': 'Xiaowen Lin', 'hidden': False}, {'_id': '6777591138f9a731d4f575bf', 'name': 'Felix Juefei-Xu', 'hidden': False}, {'_id': '6777591138f9a731d4f575c0', 'name': 'Zhuowei Li', 'hidden': False}, {'_id': '6777591138f9a731d4f575c1', 'name': 'Ligong Han', 'hidden': False}, {'_id': '6777591138f9a731d4f575c2', 'name': 'Harihar Subramanyam', 'hidden': False}, {'_id': '6777591138f9a731d4f575c3', 'name': 'Li Chen', 'hidden': False}, {'_id': '6777591138f9a731d4f575c4', 'name': 'Jianfa Chen', 'hidden': False}, {'_id': '6777591138f9a731d4f575c5', 'name': 'Nan Jiang', 'hidden': False}, {'_id': '6777591138f9a731d4f575c6', 'name': 'Lingjuan Lyu', 'hidden': False}, {'_id': '6777591138f9a731d4f575c7', 'name': 'Shiqing Ma', 'hidden': False}, {'_id': '6777591138f9a731d4f575c8', 'name': 'Dimitris N. Metaxas', 'hidden': False}, {'_id': '6777591138f9a731d4f575c9', 'name': 'Ankit Jain', 'hidden': False}], 'publishedAt': '2024-12-31T00:06:04.000Z', 'title': 'MLLM-as-a-Judge for Image Safety without Human Labeling', 'summary': 'Image content safety has become a significant challenge with the rise of\\nvisual media on online platforms. Meanwhile, in the age of AI-generated content\\n(AIGC), many image generation models are capable of producing harmful content,\\nsuch as images containing sexual or violent material. Thus, it becomes crucial\\nto identify such unsafe images based on established safety rules. Pre-trained\\nMultimodal Large Language Models (MLLMs) offer potential in this regard, given\\ntheir strong pattern recognition abilities. Existing approaches typically\\nfine-tune MLLMs with human-labeled datasets, which however brings a series of\\ndrawbacks. First, relying on human annotators to label data following intricate\\nand detailed guidelines is both expensive and labor-intensive. Furthermore,\\nusers of safety judgment systems may need to frequently update safety rules,\\nmaking fine-tuning on human-based annotation more challenging. This raises the\\nresearch question: Can we detect unsafe images by querying MLLMs in a zero-shot\\nsetting using a predefined safety constitution (a set of safety rules)? Our\\nresearch showed that simply querying pre-trained MLLMs does not yield\\nsatisfactory results. This lack of effectiveness stems from factors such as the\\nsubjectivity of safety rules, the complexity of lengthy constitutions, and the\\ninherent biases in the models. To address these challenges, we propose a\\nMLLM-based method includes objectifying safety rules, assessing the relevance\\nbetween rules and images, making quick judgments based on debiased token\\nprobabilities with logically complete yet simplified precondition chains for\\nsafety rules, and conducting more in-depth reasoning with cascaded\\nchain-of-thought processes if necessary. Experiment results demonstrate that\\nour method is highly effective for zero-shot image safety judgment tasks.', 'upvotes': 10, 'discussionId': '6777591238f9a731d4f5762f'}, 'publishedAt': '2025-01-02T22:27:32.841Z', 'title': 'MLLM-as-a-Judge for Image Safety without Human Labeling', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.00192.png', 'numComments': 1, 'submittedBy': {'_id': '64dfcc62e8b6f3f3baa950e0', 'avatarUrl': '/avatars/21bbff67d46c08044efe2406575aa77e.svg', 'fullname': 'Zhenting Wang', 'name': 'ztwang', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2501.00103', 'authors': [{'_id': '6777ef01cbb36f508a37aa44', 'name': 'Yoav HaCohen', 'hidden': False}, {'_id': '6777ef01cbb36f508a37aa45', 'name': 'Nisan Chiprut', 'hidden': False}, {'_id': '6777ef01cbb36f508a37aa46', 'name': 'Benny Brazowski', 'hidden': False}, {'_id': '6777ef01cbb36f508a37aa47', 'name': 'Daniel Shalem', 'hidden': False}, {'_id': '6777ef01cbb36f508a37aa48', 'name': 'Dudu Moshe', 'hidden': False}, {'_id': '6777ef01cbb36f508a37aa49', 'name': 'Eitan Richardson', 'hidden': False}, {'_id': '6777ef01cbb36f508a37aa4a', 'name': 'Eran Levin', 'hidden': False}, {'_id': '6777ef01cbb36f508a37aa4b', 'name': 'Guy Shiran', 'hidden': False}, {'_id': '6777ef01cbb36f508a37aa4c', 'name': 'Nir Zabari', 'hidden': False}, {'_id': '6777ef01cbb36f508a37aa4d', 'name': 'Ori Gordon', 'hidden': False}, {'_id': '6777ef01cbb36f508a37aa4e', 'name': 'Poriya Panet', 'hidden': False}, {'_id': '6777ef01cbb36f508a37aa4f', 'name': 'Sapir Weissbuch', 'hidden': False}, {'_id': '6777ef01cbb36f508a37aa50', 'name': 'Victor Kulikov', 'hidden': False}, {'_id': '6777ef01cbb36f508a37aa51', 'name': 'Yaki Bitterman', 'hidden': False}, {'_id': '6777ef01cbb36f508a37aa52', 'name': 'Zeev Melumian', 'hidden': False}, {'_id': '6777ef01cbb36f508a37aa53', 'name': 'Ofir Bibi', 'hidden': False}], 'publishedAt': '2024-12-30T19:00:25.000Z', 'title': 'LTX-Video: Realtime Video Latent Diffusion', 'summary': \"We introduce LTX-Video, a transformer-based latent diffusion model that\\nadopts a holistic approach to video generation by seamlessly integrating the\\nresponsibilities of the Video-VAE and the denoising transformer. Unlike\\nexisting methods, which treat these components as independent, LTX-Video aims\\nto optimize their interaction for improved efficiency and quality. At its core\\nis a carefully designed Video-VAE that achieves a high compression ratio of\\n1:192, with spatiotemporal downscaling of 32 x 32 x 8 pixels per token, enabled\\nby relocating the patchifying operation from the transformer's input to the\\nVAE's input. Operating in this highly compressed latent space enables the\\ntransformer to efficiently perform full spatiotemporal self-attention, which is\\nessential for generating high-resolution videos with temporal consistency.\\nHowever, the high compression inherently limits the representation of fine\\ndetails. To address this, our VAE decoder is tasked with both latent-to-pixel\\nconversion and the final denoising step, producing the clean result directly in\\npixel space. This approach preserves the ability to generate fine details\\nwithout incurring the runtime cost of a separate upsampling module. Our model\\nsupports diverse use cases, including text-to-video and image-to-video\\ngeneration, with both capabilities trained simultaneously. It achieves\\nfaster-than-real-time generation, producing 5 seconds of 24 fps video at\\n768x512 resolution in just 2 seconds on an Nvidia H100 GPU, outperforming all\\nexisting models of similar scale. The source code and pre-trained models are\\npublicly available, setting a new benchmark for accessible and scalable video\\ngeneration.\", 'upvotes': 8, 'discussionId': '6777ef06cbb36f508a37ab84'}, 'publishedAt': '2025-01-03T09:07:20.481Z', 'title': 'LTX-Video: Realtime Video Latent Diffusion', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.00103.png', 'numComments': 1, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isMod': False, 'followerCount': 5533}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2412.21015', 'authors': [{'_id': '67740b5f846a267749caed44', 'user': {'_id': '6565a60e0ff3292512bae26d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6565a60e0ff3292512bae26d/5P6wgEhCI0ndFVgX9KX8k.jpeg', 'isPro': False, 'fullname': 'Mahir Labib Dihan', 'user': 'mahirlabibdihan', 'type': 'user'}, 'name': 'Mahir Labib Dihan', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-01T20:12:31.356Z', 'hidden': False}, {'_id': '67740b5f846a267749caed45', 'name': 'Mohammed Eunus Ali', 'hidden': False}, {'_id': '67740b5f846a267749caed46', 'name': 'Md Rizwan Parvez', 'hidden': False}], 'publishedAt': '2024-12-30T15:33:19.000Z', 'title': 'MapQaTor: A System for Efficient Annotation of Map Query Datasets', 'summary': 'Mapping and navigation services like Google Maps, Apple Maps, Openstreet\\nMaps, are essential for accessing various location-based data, yet they often\\nstruggle to handle natural language geospatial queries. Recent advancements in\\nLarge Language Models (LLMs) show promise in question answering (QA), but\\ncreating reliable geospatial QA datasets from map services remains challenging.\\nWe introduce MapQaTor, a web application that streamlines the creation of\\nreproducible, traceable map-based QA datasets. With its plug-and-play\\narchitecture, MapQaTor enables seamless integration with any maps API, allowing\\nusers to gather and visualize data from diverse sources with minimal setup. By\\ncaching API responses, the platform ensures consistent ground truth, enhancing\\nthe reliability of the data even as real-world information evolves. MapQaTor\\ncentralizes data retrieval, annotation, and visualization within a single\\nplatform, offering a unique opportunity to evaluate the current state of\\nLLM-based geospatial reasoning while advancing their capabilities for improved\\ngeospatial understanding. Evaluation metrics show that, MapQaTor speeds up the\\nannotation process by at least 30 times compared to manual methods,\\nunderscoring its potential for developing geospatial resources, such as complex\\nmap reasoning datasets. The website is live at: https://mapqator.github.io/ and\\na demo video is available at: https://youtu.be/7_aV9Wmhs6Q.', 'upvotes': 5, 'discussionId': '67740b60846a267749caed9d'}, 'publishedAt': '2025-01-03T01:46:41.606Z', 'title': 'MapQaTor: A System for Efficient Annotation of Map Query Datasets', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/6565a60e0ff3292512bae26d/waIMg83aLLRbGMcHwCIIr.mp4'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.21015.png', 'numComments': 1, 'submittedBy': {'_id': '6565a60e0ff3292512bae26d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6565a60e0ff3292512bae26d/5P6wgEhCI0ndFVgX9KX8k.jpeg', 'fullname': 'Mahir Labib Dihan', 'name': 'mahirlabibdihan', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2501.00658', 'authors': [{'_id': '67777d99f215dad5eb2e85c6', 'name': 'Peihao Wang', 'hidden': False}, {'_id': '67777d99f215dad5eb2e85c7', 'name': 'Ruisi Cai', 'hidden': False}, {'_id': '67777d99f215dad5eb2e85c8', 'name': 'Yuehao Wang', 'hidden': False}, {'_id': '67777d99f215dad5eb2e85c9', 'user': {'_id': '65dc42ad1ed8dc6d82e3e4d8', 'avatarUrl': '/avatars/d2001d58b4fff24509c9e8e952042fe5.svg', 'isPro': False, 'fullname': 'Jiajun Zhu', 'user': 'lanczos', 'type': 'user'}, 'name': 'Jiajun Zhu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-03T14:03:05.945Z', 'hidden': False}, {'_id': '67777d99f215dad5eb2e85ca', 'user': {'_id': '639ccab166106be1436e1640', 'avatarUrl': '/avatars/1e3806e18ac427be20e93e5400f153d4.svg', 'isPro': False, 'fullname': 'Pragya Srivastava', 'user': 'pragsri8', 'type': 'user'}, 'name': 'Pragya Srivastava', 'status': 'extracted_confirmed', 'statusLastChangedAt': '2025-01-03T06:05:01.330Z', 'hidden': False}, {'_id': '67777d99f215dad5eb2e85cb', 'name': 'Zhangyang Wang', 'hidden': False}, {'_id': '67777d99f215dad5eb2e85cc', 'name': 'Pan Li', 'hidden': False}], 'publishedAt': '2024-12-31T22:06:39.000Z', 'title': 'Understanding and Mitigating Bottlenecks of State Space Models through\\n  the Lens of Recency and Over-smoothing', 'summary': \"Structured State Space Models (SSMs) have emerged as alternatives to\\ntransformers. While SSMs are often regarded as effective in capturing\\nlong-sequence dependencies, we rigorously demonstrate that they are inherently\\nlimited by strong recency bias. Our empirical studies also reveal that this\\nbias impairs the models' ability to recall distant information and introduces\\nrobustness issues. Our scaling experiments then discovered that deeper\\nstructures in SSMs can facilitate the learning of long contexts. However,\\nsubsequent theoretical analysis reveals that as SSMs increase in depth, they\\nexhibit another inevitable tendency toward over-smoothing, e.g., token\\nrepresentations becoming increasingly indistinguishable. This fundamental\\ndilemma between recency and over-smoothing hinders the scalability of existing\\nSSMs. Inspired by our theoretical findings, we propose to polarize two channels\\nof the state transition matrices in SSMs, setting them to zero and one,\\nrespectively, simultaneously addressing recency bias and over-smoothing.\\nExperiments demonstrate that our polarization technique consistently enhances\\nthe associative recall accuracy of long-range tokens and unlocks SSMs to\\nbenefit further from deeper architectures. All source codes are released at\\nhttps://github.com/VITA-Group/SSM-Bottleneck.\", 'upvotes': 4, 'discussionId': '67777d9af215dad5eb2e8617'}, 'publishedAt': '2025-01-03T01:04:51.213Z', 'title': 'Understanding and Mitigating Bottlenecks of State Space Models through the Lens of Recency and Over-smoothing', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.00658.png', 'numComments': 1, 'submittedBy': {'_id': '62c8f1773e28b8ee4cddca09', 'avatarUrl': '/avatars/9edbcc84e4ab2b335011daab8acfdbb8.svg', 'fullname': 'Peihao Wang', 'name': 'peihaowang', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2501.01320', 'authors': [{'_id': '67779a845d0e6d1912b339a7', 'name': 'Jianyi Wang', 'hidden': False}, {'_id': '67779a845d0e6d1912b339a8', 'name': 'Zhijie Lin', 'hidden': False}, {'_id': '67779a845d0e6d1912b339a9', 'name': 'Meng Wei', 'hidden': False}, {'_id': '67779a845d0e6d1912b339aa', 'name': 'Yang Zhao', 'hidden': False}, {'_id': '67779a845d0e6d1912b339ab', 'name': 'Ceyuan Yang', 'hidden': False}, {'_id': '67779a845d0e6d1912b339ac', 'name': 'Chen Change Loy', 'hidden': False}, {'_id': '67779a845d0e6d1912b339ad', 'name': 'Lu Jiang', 'hidden': False}], 'publishedAt': '2025-01-02T16:19:48.000Z', 'title': 'SeedVR: Seeding Infinity in Diffusion Transformer Towards Generic Video\\n  Restoration', 'summary': \"Video restoration poses non-trivial challenges in maintaining fidelity while\\nrecovering temporally consistent details from unknown degradations in the wild.\\nDespite recent advances in diffusion-based restoration, these methods often\\nface limitations in generation capability and sampling efficiency. In this\\nwork, we present SeedVR, a diffusion transformer designed to handle real-world\\nvideo restoration with arbitrary length and resolution. The core design of\\nSeedVR lies in the shifted window attention that facilitates effective\\nrestoration on long video sequences. SeedVR further supports variable-sized\\nwindows near the boundary of both spatial and temporal dimensions, overcoming\\nthe resolution constraints of traditional window attention. Equipped with\\ncontemporary practices, including causal video autoencoder, mixed image and\\nvideo training, and progressive training, SeedVR achieves highly-competitive\\nperformance on both synthetic and real-world benchmarks, as well as\\nAI-generated videos. Extensive experiments demonstrate SeedVR's superiority\\nover existing methods for generic video restoration.\", 'upvotes': 3, 'discussionId': '67779a895d0e6d1912b33b28'}, 'publishedAt': '2025-01-03T03:20:50.640Z', 'title': 'SeedVR: Seeding Infinity in Diffusion Transformer Towards Generic Video Restoration', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.01320.png', 'numComments': 1, 'submittedBy': {'_id': '63043db17373aacccd89f49d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/63043db17373aacccd89f49d/jzP_fPCFXeYJvAD8uA_N7.jpeg', 'fullname': 'JIANYI WANG', 'name': 'Iceclear', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 18}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2501.01245', 'authors': [{'_id': '67776b23b2a6ac5157bafa60', 'name': 'Yongle Huang', 'hidden': False}, {'_id': '67776b23b2a6ac5157bafa61', 'user': {'_id': '6570450a78d7aca0c361a177', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6570450a78d7aca0c361a177/z0GrnXEsjK2_G-hFfQhKv.jpeg', 'isPro': False, 'fullname': 'Harold Chen', 'user': 'Harold328', 'type': 'user'}, 'name': 'Haodong Chen', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-03T14:03:10.538Z', 'hidden': False}, {'_id': '67776b23b2a6ac5157bafa62', 'name': 'Zhenbang Xu', 'hidden': False}, {'_id': '67776b23b2a6ac5157bafa63', 'name': 'Zihan Jia', 'hidden': False}, {'_id': '67776b23b2a6ac5157bafa64', 'name': 'Haozhou Sun', 'hidden': False}, {'_id': '67776b23b2a6ac5157bafa65', 'name': 'Dian Shao', 'hidden': False}], 'publishedAt': '2025-01-02T13:12:12.000Z', 'title': 'SeFAR: Semi-supervised Fine-grained Action Recognition with Temporal\\n  Perturbation and Learning Stabilization', 'summary': 'Human action understanding is crucial for the advancement of multimodal\\nsystems. While recent developments, driven by powerful large language models\\n(LLMs), aim to be general enough to cover a wide range of categories, they\\noften overlook the need for more specific capabilities. In this work, we\\naddress the more challenging task of Fine-grained Action Recognition (FAR),\\nwhich focuses on detailed semantic labels within shorter temporal duration\\n(e.g., \"salto backward tucked with 1 turn\"). Given the high costs of annotating\\nfine-grained labels and the substantial data needed for fine-tuning LLMs, we\\npropose to adopt semi-supervised learning (SSL). Our framework, SeFAR,\\nincorporates several innovative designs to tackle these challenges.\\nSpecifically, to capture sufficient visual details, we construct Dual-level\\ntemporal elements as more effective representations, based on which we design a\\nnew strong augmentation strategy for the Teacher-Student learning paradigm\\nthrough involving moderate temporal perturbation. Furthermore, to handle the\\nhigh uncertainty within the teacher model\\'s predictions for FAR, we propose the\\nAdaptive Regulation to stabilize the learning process. Experiments show that\\nSeFAR achieves state-of-the-art performance on two FAR datasets, FineGym and\\nFineDiving, across various data scopes. It also outperforms other\\nsemi-supervised methods on two classical coarse-grained datasets, UCF101 and\\nHMDB51. Further analysis and ablation studies validate the effectiveness of our\\ndesigns. Additionally, we show that the features extracted by our SeFAR could\\nlargely promote the ability of multimodal foundation models to understand\\nfine-grained and domain-specific semantics.', 'upvotes': 3, 'discussionId': '67776b25b2a6ac5157bafaf4'}, 'publishedAt': '2025-01-02T23:46:31.347Z', 'title': 'SeFAR: Semi-supervised Fine-grained Action Recognition with Temporal Perturbation and Learning Stabilization', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.01245.png', 'numComments': 1, 'submittedBy': {'_id': '6570450a78d7aca0c361a177', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6570450a78d7aca0c361a177/z0GrnXEsjK2_G-hFfQhKv.jpeg', 'fullname': 'Harold Chen', 'name': 'Harold328', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2501.01407', 'authors': [{'_id': '67780d1a007c81c7ace7d132', 'name': 'Or Patashnik', 'hidden': False}, {'_id': '67780d1a007c81c7ace7d133', 'name': 'Rinon Gal', 'hidden': False}, {'_id': '67780d1a007c81c7ace7d134', 'name': 'Daniil Ostashev', 'hidden': False}, {'_id': '67780d1a007c81c7ace7d135', 'name': 'Sergey Tulyakov', 'hidden': False}, {'_id': '67780d1a007c81c7ace7d136', 'name': 'Kfir Aberman', 'hidden': False}, {'_id': '67780d1a007c81c7ace7d137', 'name': 'Daniel Cohen-Or', 'hidden': False}], 'publishedAt': '2025-01-02T18:52:11.000Z', 'title': 'Nested Attention: Semantic-aware Attention Values for Concept\\n  Personalization', 'summary': \"Personalizing text-to-image models to generate images of specific subjects\\nacross diverse scenes and styles is a rapidly advancing field. Current\\napproaches often face challenges in maintaining a balance between identity\\npreservation and alignment with the input text prompt. Some methods rely on a\\nsingle textual token to represent a subject, which limits expressiveness, while\\nothers employ richer representations but disrupt the model's prior, diminishing\\nprompt alignment. In this work, we introduce Nested Attention, a novel\\nmechanism that injects a rich and expressive image representation into the\\nmodel's existing cross-attention layers. Our key idea is to generate\\nquery-dependent subject values, derived from nested attention layers that learn\\nto select relevant subject features for each region in the generated image. We\\nintegrate these nested layers into an encoder-based personalization method, and\\nshow that they enable high identity preservation while adhering to input text\\nprompts. Our approach is general and can be trained on various domains.\\nAdditionally, its prior preservation allows us to combine multiple personalized\\nsubjects from different domains in a single image.\", 'upvotes': 1, 'discussionId': '67780d21007c81c7ace7d397'}, 'publishedAt': '2025-01-03T11:16:20.978Z', 'title': 'Nested Attention: Semantic-aware Attention Values for Concept Personalization', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.01407.png', 'numComments': 1, 'submittedBy': {'_id': '62853516e483e0d37b354ce1', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/62853516e483e0d37b354ce1/t5Tyd3E07w26B9Z3XpZWI.jpeg', 'fullname': 'Or Patashnik', 'name': 'orpatashnik', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 2}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2501.00910', 'authors': [{'_id': '677745822519a7b849ccec01', 'user': {'_id': '62785fda0c66a9124a01a2c7', 'avatarUrl': '/avatars/0bb2c76d7c05e0e03a832317458a7f15.svg', 'isPro': False, 'fullname': 'Yang Li', 'user': 'littlestone111', 'type': 'user'}, 'name': 'Yang Li', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-03T14:03:46.231Z', 'hidden': False}, {'_id': '677745822519a7b849ccec02', 'name': 'Han Meng', 'hidden': False}, {'_id': '677745822519a7b849ccec03', 'user': {'_id': '65429721c9483f8861915c81', 'avatarUrl': '/avatars/67d93b9e337874512805f203e01fc7b7.svg', 'isPro': False, 'fullname': 'Zhenyu Bi', 'user': 'bzyzz', 'type': 'user'}, 'name': 'Zhenyu Bi', 'status': 'extracted_pending', 'statusLastChangedAt': '2025-01-03T02:03:47.856Z', 'hidden': False}, {'_id': '677745822519a7b849ccec04', 'name': 'Ingolv T. Urnes', 'hidden': False}, {'_id': '677745822519a7b849ccec05', 'name': 'Haipeng Chen', 'hidden': False}], 'publishedAt': '2025-01-01T17:53:43.000Z', 'title': 'Population Aware Diffusion for Time Series Generation', 'summary': 'Diffusion models have shown promising ability in generating high-quality time\\nseries (TS) data. Despite the initial success, existing works mostly focus on\\nthe authenticity of data at the individual level, but pay less attention to\\npreserving the population-level properties on the entire dataset. Such\\npopulation-level properties include value distributions for each dimension and\\ndistributions of certain functional dependencies (e.g., cross-correlation, CC)\\nbetween different dimensions. For instance, when generating house energy\\nconsumption TS data, the value distributions of the outside temperature and the\\nkitchen temperature should be preserved, as well as the distribution of CC\\nbetween them. Preserving such TS population-level properties is critical in\\nmaintaining the statistical insights of the datasets, mitigating model bias,\\nand augmenting downstream tasks like TS prediction. Yet, it is often overlooked\\nby existing models. Hence, data generated by existing models often bear\\ndistribution shifts from the original data. We propose Population-aware\\nDiffusion for Time Series (PaD-TS), a new TS generation model that better\\npreserves the population-level properties. The key novelties of PaD-TS include\\n1) a new training method explicitly incorporating TS population-level property\\npreservation, and 2) a new dual-channel encoder model architecture that better\\ncaptures the TS data structure. Empirical results in major benchmark datasets\\nshow that PaD-TS can improve the average CC distribution shift score between\\nreal and synthetic data by 5.9x while maintaining a performance comparable to\\nstate-of-the-art models on individual-level authenticity.', 'upvotes': 1, 'discussionId': '677745832519a7b849ccec48'}, 'publishedAt': '2025-01-03T11:07:50.900Z', 'title': 'Population Aware Diffusion for Time Series Generation', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.00910.png', 'numComments': 1, 'submittedBy': {'_id': '62785fda0c66a9124a01a2c7', 'avatarUrl': '/avatars/0bb2c76d7c05e0e03a832317458a7f15.svg', 'fullname': 'Yang Li', 'name': 'littlestone111', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2501.00712', 'authors': [{'_id': '67778d3621a9158f25a2d23b', 'user': {'_id': '65dc42ad1ed8dc6d82e3e4d8', 'avatarUrl': '/avatars/d2001d58b4fff24509c9e8e952042fe5.svg', 'isPro': False, 'fullname': 'Jiajun Zhu', 'user': 'lanczos', 'type': 'user'}, 'name': 'Jiajun Zhu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-03T14:02:56.780Z', 'hidden': False}, {'_id': '67778d3621a9158f25a2d23c', 'name': 'Peihao Wang', 'hidden': False}, {'_id': '67778d3621a9158f25a2d23d', 'name': 'Ruisi Cai', 'hidden': False}, {'_id': '67778d3621a9158f25a2d23e', 'user': {'_id': '64ad70f7f9f761a4c3910c63', 'avatarUrl': '/avatars/69b6c167299ca7a174a69ddc5a95e4a2.svg', 'isPro': False, 'fullname': 'Jason Lee', 'user': 'Jasondlee', 'type': 'user'}, 'name': 'Jason D. Lee', 'status': 'extracted_pending', 'statusLastChangedAt': '2025-01-03T07:09:43.441Z', 'hidden': False}, {'_id': '67778d3621a9158f25a2d23f', 'name': 'Pan Li', 'hidden': False}, {'_id': '67778d3621a9158f25a2d240', 'name': 'Zhangyang Wang', 'hidden': False}], 'publishedAt': '2025-01-01T03:23:00.000Z', 'title': 'Rethinking Addressing in Language Models via Contexualized Equivariant\\n  Positional Encoding', 'summary': 'Transformers rely on both content-based and position-based addressing\\nmechanisms to make predictions, but existing positional encoding techniques\\noften diminish the effectiveness of position-based addressing. Many current\\nmethods enforce rigid patterns in attention maps, limiting the ability to model\\nlong-range dependencies and adapt to diverse tasks. Additionally, most\\npositional encodings are learned as general biases, lacking the specialization\\nrequired for different instances within a dataset. To address this, we propose\\nconTextualized equivariAnt Position\\nEmbedding (TAPE), a novel framework that enhances\\npositional embeddings by incorporating sequence content across layers. TAPE\\nintroduces dynamic, context-aware positional encodings, overcoming the\\nconstraints of traditional fixed patterns. By enforcing permutation and\\northogonal equivariance, TAPE ensures the stability of positional encodings\\nduring updates, improving robustness and adaptability. Our method can be easily\\nintegrated into pre-trained transformers, offering parameter-efficient\\nfine-tuning with minimal overhead. Extensive experiments shows that TAPE\\nachieves superior performance in language modeling, arithmetic reasoning, and\\nlong-context retrieval tasks compared to existing positional embedding\\ntechniques.', 'upvotes': 1, 'discussionId': '67778d3721a9158f25a2d2a1'}, 'publishedAt': '2025-01-03T09:21:08.086Z', 'title': 'Rethinking Addressing in Language Models via Contexualized Equivariant Positional Encoding', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.00712.png', 'numComments': 1, 'submittedBy': {'_id': '65dc42ad1ed8dc6d82e3e4d8', 'avatarUrl': '/avatars/d2001d58b4fff24509c9e8e952042fe5.svg', 'fullname': 'Jiajun Zhu', 'name': 'lanczos', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}, 'isAuthorParticipating': True}"
]