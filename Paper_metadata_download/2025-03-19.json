[
  {
    "paper": {
      "id": "2503.14456",
      "authors": [
        {
          "_id": "67da21ed78c08b432f9fee0c",
          "name": "Bo Peng",
          "hidden": false
        },
        {
          "_id": "67da21ed78c08b432f9fee0d",
          "name": "Ruichong Zhang",
          "hidden": false
        },
        {
          "_id": "67da21ed78c08b432f9fee0e",
          "name": "Daniel Goldstein",
          "hidden": false
        },
        {
          "_id": "67da21ed78c08b432f9fee0f",
          "name": "Eric Alcaide",
          "hidden": false
        },
        {
          "_id": "67da21ed78c08b432f9fee10",
          "name": "Haowen Hou",
          "hidden": false
        },
        {
          "_id": "67da21ed78c08b432f9fee11",
          "name": "Janna Lu",
          "hidden": false
        },
        {
          "_id": "67da21ed78c08b432f9fee12",
          "name": "William Merrill",
          "hidden": false
        },
        {
          "_id": "67da21ed78c08b432f9fee13",
          "name": "Guangyu Song",
          "hidden": false
        },
        {
          "_id": "67da21ed78c08b432f9fee14",
          "name": "Kaifeng Tan",
          "hidden": false
        },
        {
          "_id": "67da21ed78c08b432f9fee15",
          "name": "Saiteja Utpala",
          "hidden": false
        },
        {
          "_id": "67da21ed78c08b432f9fee16",
          "name": "Nathan Wilce",
          "hidden": false
        },
        {
          "_id": "67da21ed78c08b432f9fee17",
          "name": "Johan S. Wind",
          "hidden": false
        },
        {
          "_id": "67da21ed78c08b432f9fee18",
          "name": "Tianyi Wu",
          "hidden": false
        },
        {
          "_id": "67da21ed78c08b432f9fee19",
          "name": "Daniel Wuttke",
          "hidden": false
        },
        {
          "_id": "67da21ed78c08b432f9fee1a",
          "name": "Christian Zhou-Zheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-18T17:31:05.000Z",
      "submittedOnDailyAt": "2025-03-19T00:29:42.147Z",
      "title": "RWKV-7 \"Goose\" with Expressive Dynamic State Evolution",
      "submittedOnDailyBy": {
        "_id": "6418629fd13ffa408128d7ae",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1679319546731-noauth.png",
        "isPro": false,
        "fullname": "Zhang Ruichong",
        "user": "ZhangRC",
        "type": "user"
      },
      "summary": "We present RWKV-7 \"Goose\", a new sequence modeling architecture, along with\npre-trained language models that establish a new state-of-the-art in downstream\nperformance at the 3 billion parameter scale on multilingual tasks, and match\ncurrent SoTA English language performance despite being trained on dramatically\nfewer tokens than other top 3B models. Nevertheless, RWKV-7 models require only\nconstant memory usage and constant inference time per token. RWKV-7 introduces\na newly generalized formulation of the delta rule with vector-valued gating and\nin-context learning rates, as well as a relaxed value replacement rule. We show\nthat RWKV-7 can perform state tracking and recognize all regular languages,\nwhile retaining parallelizability of training. This exceeds the capabilities of\nTransformers under standard complexity conjectures, which are limited to\nTC^0. To demonstrate RWKV-7's language modeling capability, we also\npresent an extended open source 3.1 trillion token multilingual corpus, and\ntrain four RWKV-7 models ranging from 0.19 billion to 2.9 billion parameters on\nthis dataset.\n  To foster openness, reproduction, and adoption, we release our models and\ndataset component listing at https://huggingface.co/RWKV, and our training and\ninference code at https://github.com/RWKV/RWKV-LM all under the Apache 2.0\nLicense.",
      "upvotes": 56,
      "discussionId": "67da21ee78c08b432f9fee71",
      "projectPage": "https://rwkv.cn",
      "githubRepo": "https://github.com/RWKV/RWKV-LM",
      "ai_keywords": [
        "sequence modeling architecture",
        "pre-trained language models",
        "downstream performance",
        "multilingual tasks",
        "in-context learning rates",
        "delta rule",
        "vector-valued gating",
        "value replacement rule",
        "state tracking",
        "regular languages",
        "parallelizability of training",
        "Transformers",
        "$\\mathsf{TC}^0$"
      ]
    },
    "publishedAt": "2025-03-18T13:31:05.000Z",
    "title": "RWKV-7 \"Goose\" with Expressive Dynamic State Evolution",
    "summary": "We present RWKV-7 \"Goose\", a new sequence modeling architecture, along with\npre-trained language models that establish a new state-of-the-art in downstream\nperformance at the 3 billion parameter scale on multilingual tasks, and match\ncurrent SoTA English language performance despite being trained on dramatically\nfewer tokens than other top 3B models. Nevertheless, RWKV-7 models require only\nconstant memory usage and constant inference time per token. RWKV-7 introduces\na newly generalized formulation of the delta rule with vector-valued gating and\nin-context learning rates, as well as a relaxed value replacement rule. We show\nthat RWKV-7 can perform state tracking and recognize all regular languages,\nwhile retaining parallelizability of training. This exceeds the capabilities of\nTransformers under standard complexity conjectures, which are limited to\nTC^0. To demonstrate RWKV-7's language modeling capability, we also\npresent an extended open source 3.1 trillion token multilingual corpus, and\ntrain four RWKV-7 models ranging from 0.19 billion to 2.9 billion parameters on\nthis dataset.\n  To foster openness, reproduction, and adoption, we release our models and\ndataset component listing at https://huggingface.co/RWKV, and our training and\ninference code at https://github.com/RWKV/RWKV-LM all under the Apache 2.0\nLicense.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14456.png",
    "numComments": 5,
    "submittedBy": {
      "_id": "6418629fd13ffa408128d7ae",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1679319546731-noauth.png",
      "fullname": "Zhang Ruichong",
      "name": "ZhangRC",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.14378",
      "authors": [
        {
          "_id": "67da1ee1f1a4a52e8a1e0241",
          "name": "Zechen Bai",
          "hidden": false
        },
        {
          "_id": "67da1ee1f1a4a52e8a1e0242",
          "name": "Hai Ci",
          "hidden": false
        },
        {
          "_id": "67da1ee1f1a4a52e8a1e0243",
          "user": {
            "_id": "63a55320ce5763e06f78519c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1671779060549-noauth.jpeg",
            "isPro": false,
            "fullname": "Mike Shou",
            "user": "mikeshou",
            "type": "user"
          },
          "name": "Mike Zheng Shou",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-19T01:33:23.736Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64b7833aa5018e3c7c9b50d8/1jxSsiEAyMr5fSng7GOB3.mp4"
      ],
      "publishedAt": "2025-03-18T16:10:24.000Z",
      "submittedOnDailyAt": "2025-03-19T00:11:44.927Z",
      "title": "Impossible Videos",
      "submittedOnDailyBy": {
        "_id": "64b7833aa5018e3c7c9b50d8",
        "avatarUrl": "/avatars/782415605ed786b73f484fcc86a6384f.svg",
        "isPro": false,
        "fullname": "Zechen Bai",
        "user": "ZechenBai",
        "type": "user"
      },
      "summary": "Synthetic videos nowadays is widely used to complement data scarcity and\ndiversity of real-world videos. Current synthetic datasets primarily replicate\nreal-world scenarios, leaving impossible, counterfactual and anti-reality video\nconcepts underexplored. This work aims to answer two questions: 1) Can today's\nvideo generation models effectively follow prompts to create impossible video\ncontent? 2) Are today's video understanding models good enough for\nunderstanding impossible videos? To this end, we introduce IPV-Bench, a novel\nbenchmark designed to evaluate and foster progress in video understanding and\ngeneration. IPV-Bench is underpinned by a comprehensive taxonomy, encompassing\n4 domains, 14 categories. It features diverse scenes that defy physical,\nbiological, geographical, or social laws. Based on the taxonomy, a prompt suite\nis constructed to evaluate video generation models, challenging their prompt\nfollowing and creativity capabilities. In addition, a video benchmark is\ncurated to assess Video-LLMs on their ability of understanding impossible\nvideos, which particularly requires reasoning on temporal dynamics and world\nknowledge. Comprehensive evaluations reveal limitations and insights for future\ndirections of video models, paving the way for next-generation video models.",
      "upvotes": 35,
      "discussionId": "67da1ee3f1a4a52e8a1e02df",
      "projectPage": "https://showlab.github.io/Impossible-Videos/",
      "githubRepo": "https://github.com/showlab/Impossible-Videos",
      "ai_keywords": [
        "IPV-Bench",
        "taxonomy",
        "prompt suite",
        "video generation models",
        "prompt following",
        "creativity capabilities",
        "video benchmark",
        "Video-LLMs",
        "temporal dynamics",
        "world knowledge"
      ]
    },
    "publishedAt": "2025-03-18T12:10:24.000Z",
    "title": "Impossible Videos",
    "summary": "Synthetic videos nowadays is widely used to complement data scarcity and\ndiversity of real-world videos. Current synthetic datasets primarily replicate\nreal-world scenarios, leaving impossible, counterfactual and anti-reality video\nconcepts underexplored. This work aims to answer two questions: 1) Can today's\nvideo generation models effectively follow prompts to create impossible video\ncontent? 2) Are today's video understanding models good enough for\nunderstanding impossible videos? To this end, we introduce IPV-Bench, a novel\nbenchmark designed to evaluate and foster progress in video understanding and\ngeneration. IPV-Bench is underpinned by a comprehensive taxonomy, encompassing\n4 domains, 14 categories. It features diverse scenes that defy physical,\nbiological, geographical, or social laws. Based on the taxonomy, a prompt suite\nis constructed to evaluate video generation models, challenging their prompt\nfollowing and creativity capabilities. In addition, a video benchmark is\ncurated to assess Video-LLMs on their ability of understanding impossible\nvideos, which particularly requires reasoning on temporal dynamics and world\nknowledge. Comprehensive evaluations reveal limitations and insights for future\ndirections of video models, paving the way for next-generation video models.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64b7833aa5018e3c7c9b50d8/1jxSsiEAyMr5fSng7GOB3.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14378.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b7833aa5018e3c7c9b50d8",
      "avatarUrl": "/avatars/782415605ed786b73f484fcc86a6384f.svg",
      "fullname": "Zechen Bai",
      "name": "ZechenBai",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.14478",
      "authors": [
        {
          "_id": "67da34a648348387ebac36ff",
          "name": "Xinyu Fang",
          "hidden": false
        },
        {
          "_id": "67da34a648348387ebac3700",
          "name": "Zhijian Chen",
          "hidden": false
        },
        {
          "_id": "67da34a648348387ebac3701",
          "name": "Kai Lan",
          "hidden": false
        },
        {
          "_id": "67da34a648348387ebac3702",
          "name": "Shengyuan Ding",
          "hidden": false
        },
        {
          "_id": "67da34a648348387ebac3703",
          "name": "Yingji Liang",
          "hidden": false
        },
        {
          "_id": "67da34a648348387ebac3704",
          "name": "Xiangyu Zhao",
          "hidden": false
        },
        {
          "_id": "67da34a648348387ebac3705",
          "name": "Farong Wen",
          "hidden": false
        },
        {
          "_id": "67da34a648348387ebac3706",
          "name": "Zicheng Zhang",
          "hidden": false
        },
        {
          "_id": "67da34a648348387ebac3707",
          "name": "Guofeng Zhang",
          "hidden": false
        },
        {
          "_id": "67da34a648348387ebac3708",
          "name": "Haodong Duan",
          "hidden": false
        },
        {
          "_id": "67da34a648348387ebac3709",
          "name": "Kai Chen",
          "hidden": false
        },
        {
          "_id": "67da34a648348387ebac370a",
          "name": "Dahua Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-18T17:51:34.000Z",
      "submittedOnDailyAt": "2025-03-19T01:40:40.617Z",
      "title": "Creation-MMBench: Assessing Context-Aware Creative Intelligence in MLLM",
      "submittedOnDailyBy": {
        "_id": "64f5f8dd9b17cd59c453c57f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f5f8dd9b17cd59c453c57f/MulhwLcePFUWUQel8LQZ8.jpeg",
        "isPro": false,
        "fullname": "Xinyu Fang",
        "user": "nebulae09",
        "type": "user"
      },
      "summary": "Creativity is a fundamental aspect of intelligence, involving the ability to\ngenerate novel and appropriate solutions across diverse contexts. While Large\nLanguage Models (LLMs) have been extensively evaluated for their creative\ncapabilities, the assessment of Multimodal Large Language Models (MLLMs) in\nthis domain remains largely unexplored. To address this gap, we introduce\nCreation-MMBench, a multimodal benchmark specifically designed to evaluate the\ncreative capabilities of MLLMs in real-world, image-based tasks. The benchmark\ncomprises 765 test cases spanning 51 fine-grained tasks. To ensure rigorous\nevaluation, we define instance-specific evaluation criteria for each test case,\nguiding the assessment of both general response quality and factual consistency\nwith visual inputs. Experimental results reveal that current open-source MLLMs\nsignificantly underperform compared to proprietary models in creative tasks.\nFurthermore, our analysis demonstrates that visual fine-tuning can negatively\nimpact the base LLM's creative abilities. Creation-MMBench provides valuable\ninsights for advancing MLLM creativity and establishes a foundation for future\nimprovements in multimodal generative intelligence. Full data and evaluation\ncode is released on https://github.com/open-compass/Creation-MMBench.",
      "upvotes": 31,
      "discussionId": "67da34ad48348387ebac3926",
      "projectPage": "https://open-compass.github.io/Creation-MMBench/",
      "githubRepo": "https://github.com/open-compass/Creation-MMBench",
      "ai_keywords": [
        "Multimodal Large Language Models (MLLMs)",
        "Creation-MMBench",
        "image-based tasks",
        "instance-specific evaluation criteria",
        "visual fine-tuning",
        "multimodal generative intelligence"
      ]
    },
    "publishedAt": "2025-03-18T13:51:34.000Z",
    "title": "Creation-MMBench: Assessing Context-Aware Creative Intelligence in MLLM",
    "summary": "Creativity is a fundamental aspect of intelligence, involving the ability to\ngenerate novel and appropriate solutions across diverse contexts. While Large\nLanguage Models (LLMs) have been extensively evaluated for their creative\ncapabilities, the assessment of Multimodal Large Language Models (MLLMs) in\nthis domain remains largely unexplored. To address this gap, we introduce\nCreation-MMBench, a multimodal benchmark specifically designed to evaluate the\ncreative capabilities of MLLMs in real-world, image-based tasks. The benchmark\ncomprises 765 test cases spanning 51 fine-grained tasks. To ensure rigorous\nevaluation, we define instance-specific evaluation criteria for each test case,\nguiding the assessment of both general response quality and factual consistency\nwith visual inputs. Experimental results reveal that current open-source MLLMs\nsignificantly underperform compared to proprietary models in creative tasks.\nFurthermore, our analysis demonstrates that visual fine-tuning can negatively\nimpact the base LLM's creative abilities. Creation-MMBench provides valuable\ninsights for advancing MLLM creativity and establishes a foundation for future\nimprovements in multimodal generative intelligence. Full data and evaluation\ncode is released on https://github.com/open-compass/Creation-MMBench.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14478.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64f5f8dd9b17cd59c453c57f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f5f8dd9b17cd59c453c57f/MulhwLcePFUWUQel8LQZ8.jpeg",
      "fullname": "Xinyu Fang",
      "name": "nebulae09",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.12797",
      "authors": [
        {
          "_id": "67da2c83aa2c34f7d95e46ff",
          "name": "Xinyu Ma",
          "hidden": false
        },
        {
          "_id": "67da2c83aa2c34f7d95e4700",
          "name": "Ziyang Ding",
          "hidden": false
        },
        {
          "_id": "67da2c83aa2c34f7d95e4701",
          "name": "Zhicong Luo",
          "hidden": false
        },
        {
          "_id": "67da2c83aa2c34f7d95e4702",
          "name": "Chi Chen",
          "hidden": false
        },
        {
          "_id": "67da2c83aa2c34f7d95e4703",
          "name": "Zonghao Guo",
          "hidden": false
        },
        {
          "_id": "67da2c83aa2c34f7d95e4704",
          "name": "Derek F. Wong",
          "hidden": false
        },
        {
          "_id": "67da2c83aa2c34f7d95e4705",
          "name": "Xiaoyi Feng",
          "hidden": false
        },
        {
          "_id": "67da2c83aa2c34f7d95e4706",
          "name": "Maosong Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-17T04:06:34.000Z",
      "submittedOnDailyAt": "2025-03-19T01:06:07.094Z",
      "title": "DeepPerception: Advancing R1-like Cognitive Visual Perception in MLLMs\n  for Knowledge-Intensive Visual Grounding",
      "submittedOnDailyBy": {
        "_id": "642086ed290342c5df85662d",
        "avatarUrl": "/avatars/915a4d7b89455ae97b8544c79286ddf8.svg",
        "isPro": false,
        "fullname": "Chi Chen",
        "user": "carboncoo",
        "type": "user"
      },
      "summary": "Human experts excel at fine-grained visual discrimination by leveraging\ndomain knowledge to refine perceptual features, a capability that remains\nunderdeveloped in current Multimodal Large Language Models (MLLMs). Despite\npossessing vast expert-level knowledge, MLLMs struggle to integrate reasoning\ninto visual perception, often generating direct responses without deeper\nanalysis. To bridge this gap, we introduce knowledge-intensive visual grounding\n(KVG), a novel visual grounding task that requires both fine-grained perception\nand domain-specific knowledge integration. To address the challenges of KVG, we\npropose DeepPerception, an MLLM enhanced with cognitive visual perception\ncapabilities. Our approach consists of (1) an automated data synthesis pipeline\nthat generates high-quality, knowledge-aligned training samples, and (2) a\ntwo-stage training framework combining supervised fine-tuning for cognitive\nreasoning scaffolding and reinforcement learning to optimize\nperception-cognition synergy. To benchmark performance, we introduce KVG-Bench\na comprehensive dataset spanning 10 domains with 1.3K manually curated test\ncases. Experimental results demonstrate that DeepPerception significantly\noutperforms direct fine-tuning, achieving +8.08\\% accuracy improvements on\nKVG-Bench and exhibiting +4.60\\% superior cross-domain generalization over\nbaseline approaches. Our findings highlight the importance of integrating\ncognitive processes into MLLMs for human-like visual perception and open new\ndirections for multimodal reasoning research. The data, codes, and models are\nreleased at https://github.com/thunlp/DeepPerception.",
      "upvotes": 17,
      "discussionId": "67da2c85aa2c34f7d95e4796",
      "projectPage": "https://deepperception-kvg.github.io",
      "githubRepo": "https://github.com/thunlp/DeepPerception",
      "ai_keywords": [
        "knowledge-intensive visual grounding (KVG)",
        "DeepPerception",
        "automated data synthesis pipeline",
        "supervised fine-tuning",
        "reinforcement learning",
        "perception-cognition synergy",
        "KVG-Bench",
        "cognitive reasoning scaffolding",
        "cross-domain generalization",
        "cognitive processes",
        "multimodal reasoning"
      ]
    },
    "publishedAt": "2025-03-17T00:06:34.000Z",
    "title": "DeepPerception: Advancing R1-like Cognitive Visual Perception in MLLMs\n  for Knowledge-Intensive Visual Grounding",
    "summary": "Human experts excel at fine-grained visual discrimination by leveraging\ndomain knowledge to refine perceptual features, a capability that remains\nunderdeveloped in current Multimodal Large Language Models (MLLMs). Despite\npossessing vast expert-level knowledge, MLLMs struggle to integrate reasoning\ninto visual perception, often generating direct responses without deeper\nanalysis. To bridge this gap, we introduce knowledge-intensive visual grounding\n(KVG), a novel visual grounding task that requires both fine-grained perception\nand domain-specific knowledge integration. To address the challenges of KVG, we\npropose DeepPerception, an MLLM enhanced with cognitive visual perception\ncapabilities. Our approach consists of (1) an automated data synthesis pipeline\nthat generates high-quality, knowledge-aligned training samples, and (2) a\ntwo-stage training framework combining supervised fine-tuning for cognitive\nreasoning scaffolding and reinforcement learning to optimize\nperception-cognition synergy. To benchmark performance, we introduce KVG-Bench\na comprehensive dataset spanning 10 domains with 1.3K manually curated test\ncases. Experimental results demonstrate that DeepPerception significantly\noutperforms direct fine-tuning, achieving +8.08\\% accuracy improvements on\nKVG-Bench and exhibiting +4.60\\% superior cross-domain generalization over\nbaseline approaches. Our findings highlight the importance of integrating\ncognitive processes into MLLMs for human-like visual perception and open new\ndirections for multimodal reasoning research. The data, codes, and models are\nreleased at https://github.com/thunlp/DeepPerception.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12797.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642086ed290342c5df85662d",
      "avatarUrl": "/avatars/915a4d7b89455ae97b8544c79286ddf8.svg",
      "fullname": "Chi Chen",
      "name": "carboncoo",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.12329",
      "authors": [
        {
          "_id": "67d8e115f55b855ae6d8f29b",
          "user": {
            "_id": "63340dbbd92c5842ae71d1e9",
            "avatarUrl": "/avatars/3a3182996bd41b526dcbfa8687d91963.svg",
            "isPro": false,
            "fullname": "Kanzhi Cheng",
            "user": "cckevinn",
            "type": "user"
          },
          "name": "Kanzhi Cheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-18T11:34:32.729Z",
          "hidden": false
        },
        {
          "_id": "67d8e115f55b855ae6d8f29c",
          "name": "Wenpo Song",
          "hidden": false
        },
        {
          "_id": "67d8e115f55b855ae6d8f29d",
          "name": "Jiaxin Fan",
          "hidden": false
        },
        {
          "_id": "67d8e115f55b855ae6d8f29e",
          "name": "Zheng Ma",
          "hidden": false
        },
        {
          "_id": "67d8e115f55b855ae6d8f29f",
          "name": "Qiushi Sun",
          "hidden": false
        },
        {
          "_id": "67d8e115f55b855ae6d8f2a0",
          "name": "Fangzhi Xu",
          "hidden": false
        },
        {
          "_id": "67d8e115f55b855ae6d8f2a1",
          "name": "Chenyang Yan",
          "hidden": false
        },
        {
          "_id": "67d8e115f55b855ae6d8f2a2",
          "name": "Nuo Chen",
          "hidden": false
        },
        {
          "_id": "67d8e115f55b855ae6d8f2a3",
          "name": "Jianbing Zhang",
          "hidden": false
        },
        {
          "_id": "67d8e115f55b855ae6d8f2a4",
          "name": "Jiajun Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-16T02:56:09.000Z",
      "submittedOnDailyAt": "2025-03-19T02:04:01.010Z",
      "title": "CapArena: Benchmarking and Analyzing Detailed Image Captioning in the\n  LLM Era",
      "submittedOnDailyBy": {
        "_id": "63340dbbd92c5842ae71d1e9",
        "avatarUrl": "/avatars/3a3182996bd41b526dcbfa8687d91963.svg",
        "isPro": false,
        "fullname": "Kanzhi Cheng",
        "user": "cckevinn",
        "type": "user"
      },
      "summary": "Image captioning has been a longstanding challenge in vision-language\nresearch. With the rise of LLMs, modern Vision-Language Models (VLMs) generate\ndetailed and comprehensive image descriptions. However, benchmarking the\nquality of such captions remains unresolved. This paper addresses two key\nquestions: (1) How well do current VLMs actually perform on image captioning,\nparticularly compared to humans? We built CapArena, a platform with over 6000\npairwise caption battles and high-quality human preference votes. Our\narena-style evaluation marks a milestone, showing that leading models like\nGPT-4o achieve or even surpass human performance, while most open-source models\nlag behind. (2) Can automated metrics reliably assess detailed caption quality?\nUsing human annotations from CapArena, we evaluate traditional and recent\ncaptioning metrics, as well as VLM-as-a-Judge. Our analysis reveals that while\nsome metrics (e.g., METEOR) show decent caption-level agreement with humans,\ntheir systematic biases lead to inconsistencies in model ranking. In contrast,\nVLM-as-a-Judge demonstrates robust discernment at both the caption and model\nlevels. Building on these insights, we release CapArena-Auto, an accurate and\nefficient automated benchmark for detailed captioning, achieving 94.3%\ncorrelation with human rankings at just $4 per test. Data and resources will be\nopen-sourced at https://caparena.github.io.",
      "upvotes": 17,
      "discussionId": "67d8e118f55b855ae6d8f34e",
      "projectPage": "https://caparena.github.io/",
      "githubRepo": "https://github.com/njucckevin/CapArena",
      "ai_keywords": [
        "Vision-Language Models (VLMs)",
        "GPT-4o",
        "CapArena",
        "pairwise caption battles",
        "high-quality human preference votes",
        "VLM-as-a-Judge",
        "METEOR",
        "CapArena-Auto"
      ]
    },
    "publishedAt": "2025-03-15T22:56:09.000Z",
    "title": "CapArena: Benchmarking and Analyzing Detailed Image Captioning in the\n  LLM Era",
    "summary": "Image captioning has been a longstanding challenge in vision-language\nresearch. With the rise of LLMs, modern Vision-Language Models (VLMs) generate\ndetailed and comprehensive image descriptions. However, benchmarking the\nquality of such captions remains unresolved. This paper addresses two key\nquestions: (1) How well do current VLMs actually perform on image captioning,\nparticularly compared to humans? We built CapArena, a platform with over 6000\npairwise caption battles and high-quality human preference votes. Our\narena-style evaluation marks a milestone, showing that leading models like\nGPT-4o achieve or even surpass human performance, while most open-source models\nlag behind. (2) Can automated metrics reliably assess detailed caption quality?\nUsing human annotations from CapArena, we evaluate traditional and recent\ncaptioning metrics, as well as VLM-as-a-Judge. Our analysis reveals that while\nsome metrics (e.g., METEOR) show decent caption-level agreement with humans,\ntheir systematic biases lead to inconsistencies in model ranking. In contrast,\nVLM-as-a-Judge demonstrates robust discernment at both the caption and model\nlevels. Building on these insights, we release CapArena-Auto, an accurate and\nefficient automated benchmark for detailed captioning, achieving 94.3%\ncorrelation with human rankings at just $4 per test. Data and resources will be\nopen-sourced at https://caparena.github.io.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12329.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63340dbbd92c5842ae71d1e9",
      "avatarUrl": "/avatars/3a3182996bd41b526dcbfa8687d91963.svg",
      "fullname": "Kanzhi Cheng",
      "name": "cckevinn",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.14476",
      "authors": [
        {
          "_id": "67da2b54e5335651349e262c",
          "name": "Qiying Yu",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e262d",
          "name": "Zheng Zhang",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e262e",
          "name": "Ruofei Zhu",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e262f",
          "name": "Yufeng Yuan",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e2630",
          "name": "Xiaochen Zuo",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e2631",
          "name": "Yu Yue",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e2632",
          "name": "Tiantian Fan",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e2633",
          "name": "Gaohong Liu",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e2634",
          "name": "Lingjun Liu",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e2635",
          "name": "Xin Liu",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e2636",
          "name": "Haibin Lin",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e2637",
          "name": "Zhiqi Lin",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e2638",
          "name": "Bole Ma",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e2639",
          "name": "Guangming Sheng",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e263a",
          "name": "Yuxuan Tong",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e263b",
          "name": "Chi Zhang",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e263c",
          "name": "Mofan Zhang",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e263d",
          "name": "Wang Zhang",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e263e",
          "name": "Hang Zhu",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e263f",
          "name": "Jinhua Zhu",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e2640",
          "name": "Jiaze Chen",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e2641",
          "name": "Jiangjie Chen",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e2642",
          "name": "Chengyi Wang",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e2643",
          "name": "Hongli Yu",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e2644",
          "name": "Weinan Dai",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e2645",
          "name": "Yuxuan Song",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e2646",
          "name": "Xiangpeng Wei",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e2647",
          "name": "Hao Zhou",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e2648",
          "name": "Jingjing Liu",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e2649",
          "name": "Wei-Ying Ma",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e264a",
          "name": "Ya-Qin Zhang",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e264b",
          "name": "Lin Yan",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e264c",
          "name": "Mu Qiao",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e264d",
          "name": "Yonghui Wu",
          "hidden": false
        },
        {
          "_id": "67da2b54e5335651349e264e",
          "name": "Mingxuan Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-18T17:49:06.000Z",
      "submittedOnDailyAt": "2025-03-19T00:56:39.773Z",
      "title": "DAPO: An Open-Source LLM Reinforcement Learning System at Scale",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Inference scaling empowers LLMs with unprecedented reasoning ability, with\nreinforcement learning as the core technique to elicit complex reasoning.\nHowever, key technical details of state-of-the-art reasoning LLMs are concealed\n(such as in OpenAI o1 blog and DeepSeek R1 technical report), thus the\ncommunity still struggles to reproduce their RL training results. We propose\nthe Decoupled Clip and Dynamic sAmpling\nPolicy Optimization (DAPO) algorithm, and\nfully open-source a state-of-the-art large-scale RL system that achieves 50\npoints on AIME 2024 using Qwen2.5-32B base model. Unlike previous works that\nwithhold training details, we introduce four key techniques of our algorithm\nthat make large-scale LLM RL a success. In addition, we open-source our\ntraining code, which is built on the verl framework, along with a carefully\ncurated and processed dataset. These components of our open-source system\nenhance reproducibility and support future research in large-scale LLM RL.",
      "upvotes": 15,
      "discussionId": "67da2b55e5335651349e26c7",
      "ai_keywords": [
        "inference scaling",
        "LLMs (Large Language Models)",
        "reinforcement learning (RL)",
        "Decoupled Clip and Dynamic Sampling Action Policy Optimization (DAPO)",
        "Qwen2.5-32B",
        "AIME 2024",
        "verl framework"
      ]
    },
    "publishedAt": "2025-03-18T13:49:06.000Z",
    "title": "DAPO: An Open-Source LLM Reinforcement Learning System at Scale",
    "summary": "Inference scaling empowers LLMs with unprecedented reasoning ability, with\nreinforcement learning as the core technique to elicit complex reasoning.\nHowever, key technical details of state-of-the-art reasoning LLMs are concealed\n(such as in OpenAI o1 blog and DeepSeek R1 technical report), thus the\ncommunity still struggles to reproduce their RL training results. We propose\nthe Decoupled Clip and Dynamic sAmpling\nPolicy Optimization (DAPO) algorithm, and\nfully open-source a state-of-the-art large-scale RL system that achieves 50\npoints on AIME 2024 using Qwen2.5-32B base model. Unlike previous works that\nwithhold training details, we introduce four key techniques of our algorithm\nthat make large-scale LLM RL a success. In addition, we open-source our\ntraining code, which is built on the verl framework, along with a carefully\ncurated and processed dataset. These components of our open-source system\nenhance reproducibility and support future research in large-scale LLM RL.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14476.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6398
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.13424",
      "authors": [
        {
          "_id": "67da22b75fe852c86d3c419b",
          "name": "Xinyu Lian",
          "hidden": false
        },
        {
          "_id": "67da22b75fe852c86d3c419c",
          "name": "Zichao Yu",
          "hidden": false
        },
        {
          "_id": "67da22b75fe852c86d3c419d",
          "name": "Ruiming Liang",
          "hidden": false
        },
        {
          "_id": "67da22b75fe852c86d3c419e",
          "name": "Yitong Wang",
          "hidden": false
        },
        {
          "_id": "67da22b75fe852c86d3c419f",
          "name": "Li Ray Luo",
          "hidden": false
        },
        {
          "_id": "67da22b75fe852c86d3c41a0",
          "name": "Kaixu Chen",
          "hidden": false
        },
        {
          "_id": "67da22b75fe852c86d3c41a1",
          "name": "Yuanzhen Zhou",
          "hidden": false
        },
        {
          "_id": "67da22b75fe852c86d3c41a2",
          "name": "Qihong Tang",
          "hidden": false
        },
        {
          "_id": "67da22b75fe852c86d3c41a3",
          "name": "Xudong Xu",
          "hidden": false
        },
        {
          "_id": "67da22b75fe852c86d3c41a4",
          "name": "Zhaoyang Lyu",
          "hidden": false
        },
        {
          "_id": "67da22b75fe852c86d3c41a5",
          "name": "Bo Dai",
          "hidden": false
        },
        {
          "_id": "67da22b75fe852c86d3c41a6",
          "name": "Jiangmiao Pang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63f2ec797ddf724fbcc75aee/7Is4N1AFDor-EuzkZEiui.mp4"
      ],
      "publishedAt": "2025-03-17T17:53:56.000Z",
      "submittedOnDailyAt": "2025-03-19T00:24:14.520Z",
      "title": "Infinite Mobility: Scalable High-Fidelity Synthesis of Articulated\n  Objects via Procedural Generation",
      "submittedOnDailyBy": {
        "_id": "63f2ec797ddf724fbcc75aee",
        "avatarUrl": "/avatars/e93432ad11da703d46fe5e594d69f8c0.svg",
        "isPro": false,
        "fullname": "Zhaoyang Lyu",
        "user": "ZhaoyangLyu",
        "type": "user"
      },
      "summary": "Large-scale articulated objects with high quality are desperately needed for\nmultiple tasks related to embodied AI. Most existing methods for creating\narticulated objects are either data-driven or simulation based, which are\nlimited by the scale and quality of the training data or the fidelity and heavy\nlabour of the simulation. In this paper, we propose Infinite Mobility, a novel\nmethod for synthesizing high-fidelity articulated objects through procedural\ngeneration. User study and quantitative evaluation demonstrate that our method\ncan produce results that excel current state-of-the-art methods and are\ncomparable to human-annotated datasets in both physics property and mesh\nquality. Furthermore, we show that our synthetic data can be used as training\ndata for generative models, enabling next-step scaling up. Code is available at\nhttps://github.com/Intern-Nexus/Infinite-Mobility",
      "upvotes": 13,
      "discussionId": "67da22bb5fe852c86d3c4304",
      "projectPage": "https://infinite-mobility.github.io/",
      "githubRepo": "https://github.com/Intern-Nexus/Infinite-Mobility",
      "ai_keywords": [
        "articulated objects",
        "high-fidelity",
        "embodied AI",
        "data-driven",
        "simulation-based",
        "procedural generation",
        "physics property",
        "mesh quality",
        "generative models"
      ]
    },
    "publishedAt": "2025-03-17T13:53:56.000Z",
    "title": "Infinite Mobility: Scalable High-Fidelity Synthesis of Articulated\n  Objects via Procedural Generation",
    "summary": "Large-scale articulated objects with high quality are desperately needed for\nmultiple tasks related to embodied AI. Most existing methods for creating\narticulated objects are either data-driven or simulation based, which are\nlimited by the scale and quality of the training data or the fidelity and heavy\nlabour of the simulation. In this paper, we propose Infinite Mobility, a novel\nmethod for synthesizing high-fidelity articulated objects through procedural\ngeneration. User study and quantitative evaluation demonstrate that our method\ncan produce results that excel current state-of-the-art methods and are\ncomparable to human-annotated datasets in both physics property and mesh\nquality. Furthermore, we show that our synthetic data can be used as training\ndata for generative models, enabling next-step scaling up. Code is available at\nhttps://github.com/Intern-Nexus/Infinite-Mobility",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63f2ec797ddf724fbcc75aee/7Is4N1AFDor-EuzkZEiui.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13424.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63f2ec797ddf724fbcc75aee",
      "avatarUrl": "/avatars/e93432ad11da703d46fe5e594d69f8c0.svg",
      "fullname": "Zhaoyang Lyu",
      "name": "ZhaoyangLyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.14125",
      "authors": [
        {
          "_id": "67da200db41738a058666623",
          "name": "Defa Zhu",
          "hidden": false
        },
        {
          "_id": "67da200db41738a058666624",
          "name": "Hongzhi Huang",
          "hidden": false
        },
        {
          "_id": "67da200db41738a058666625",
          "name": "Jundong Zhou",
          "hidden": false
        },
        {
          "_id": "67da200db41738a058666626",
          "name": "Zihao Huang",
          "hidden": false
        },
        {
          "_id": "67da200db41738a058666627",
          "name": "Yutao Zeng",
          "hidden": false
        },
        {
          "_id": "67da200db41738a058666628",
          "name": "Banggu Wu",
          "hidden": false
        },
        {
          "_id": "67da200db41738a058666629",
          "name": "Qiyang Min",
          "hidden": false
        },
        {
          "_id": "67da200db41738a05866662a",
          "name": "Xun Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-18T10:37:50.000Z",
      "submittedOnDailyAt": "2025-03-19T00:09:57.233Z",
      "title": "Frac-Connections: Fractional Extension of Hyper-Connections",
      "submittedOnDailyBy": {
        "_id": "667505f4361b960c79e35486",
        "avatarUrl": "/avatars/d352639c520075220f6abaae23c39376.svg",
        "isPro": false,
        "fullname": "Defa Zhu",
        "user": "mathfinder",
        "type": "user"
      },
      "summary": "Residual connections are central to modern deep learning architectures,\nenabling the training of very deep networks by mitigating gradient vanishing.\nHyper-Connections recently generalized residual connections by introducing\nmultiple connection strengths at different depths, thereby addressing the\nseesaw effect between gradient vanishing and representation collapse. However,\nHyper-Connections increase memory access costs by expanding the width of hidden\nstates. In this paper, we propose Frac-Connections, a novel approach that\ndivides hidden states into multiple parts rather than expanding their width.\nFrac-Connections retain partial benefits of Hyper-Connections while reducing\nmemory consumption. To validate their effectiveness, we conduct large-scale\nexperiments on language tasks, with the largest being a 7B MoE model trained on\nup to 3T tokens, demonstrating that Frac-Connections significantly outperform\nresidual connections.",
      "upvotes": 10,
      "discussionId": "67da200eb41738a058666690",
      "ai_keywords": [
        "residual connections",
        "gradient vanishing",
        "Hyper-Connections",
        "multiple connection strengths",
        "seesaw effect",
        "representation collapse",
        "Frac-Connections",
        "hidden states",
        "language tasks",
        "MoE model"
      ]
    },
    "publishedAt": "2025-03-18T06:37:50.000Z",
    "title": "Frac-Connections: Fractional Extension of Hyper-Connections",
    "summary": "Residual connections are central to modern deep learning architectures,\nenabling the training of very deep networks by mitigating gradient vanishing.\nHyper-Connections recently generalized residual connections by introducing\nmultiple connection strengths at different depths, thereby addressing the\nseesaw effect between gradient vanishing and representation collapse. However,\nHyper-Connections increase memory access costs by expanding the width of hidden\nstates. In this paper, we propose Frac-Connections, a novel approach that\ndivides hidden states into multiple parts rather than expanding their width.\nFrac-Connections retain partial benefits of Hyper-Connections while reducing\nmemory consumption. To validate their effectiveness, we conduct large-scale\nexperiments on language tasks, with the largest being a 7B MoE model trained on\nup to 3T tokens, demonstrating that Frac-Connections significantly outperform\nresidual connections.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14125.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "667505f4361b960c79e35486",
      "avatarUrl": "/avatars/d352639c520075220f6abaae23c39376.svg",
      "fullname": "Defa Zhu",
      "name": "mathfinder",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.14499",
      "authors": [
        {
          "_id": "67da2e831bba0f73374fd5a0",
          "name": "Thomas Kwa",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5a1",
          "name": "Ben West",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5a2",
          "name": "Joel Becker",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5a3",
          "name": "Amy Deng",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5a4",
          "name": "Katharyn Garcia",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5a5",
          "name": "Max Hasin",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5a6",
          "name": "Sami Jawhar",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5a7",
          "name": "Megan Kinniment",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5a8",
          "name": "Nate Rush",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5a9",
          "name": "Sydney Von Arx",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5aa",
          "name": "Ryan Bloom",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5ab",
          "name": "Thomas Broadley",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5ac",
          "name": "Haoxing Du",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5ad",
          "name": "Brian Goodrich",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5ae",
          "name": "Nikola Jurkovic",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5af",
          "name": "Luke Harold Miles",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5b0",
          "name": "Seraphina Nix",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5b1",
          "name": "Tao Lin",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5b2",
          "name": "Neev Parikh",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5b3",
          "name": "David Rein",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5b4",
          "name": "Lucas Jun Koba Sato",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5b5",
          "name": "Hjalmar Wijk",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5b6",
          "name": "Daniel M. Ziegler",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5b7",
          "name": "Elizabeth Barnes",
          "hidden": false
        },
        {
          "_id": "67da2e831bba0f73374fd5b8",
          "name": "Lawrence Chan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-18T17:59:31.000Z",
      "submittedOnDailyAt": "2025-03-19T01:10:23.636Z",
      "title": "Measuring AI Ability to Complete Long Tasks",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Despite rapid progress on AI benchmarks, the real-world meaning of benchmark\nperformance remains unclear. To quantify the capabilities of AI systems in\nterms of human capabilities, we propose a new metric: 50%-task-completion time\nhorizon. This is the time humans typically take to complete tasks that AI\nmodels can complete with 50% success rate. We first timed humans with relevant\ndomain expertise on a combination of RE-Bench, HCAST, and 66 novel shorter\ntasks. On these tasks, current frontier AI models such as Claude 3.7 Sonnet\nhave a 50% time horizon of around 50 minutes. Furthermore, frontier AI time\nhorizon has been doubling approximately every seven months since 2019, though\nthe trend may have accelerated in 2024. The increase in AI models' time\nhorizons seems to be primarily driven by greater reliability and ability to\nadapt to mistakes, combined with better logical reasoning and tool use\ncapabilities. We discuss the limitations of our results -- including their\ndegree of external validity -- and the implications of increased autonomy for\ndangerous capabilities. If these results generalize to real-world software\ntasks, extrapolation of this trend predicts that within 5 years, AI systems\nwill be capable of automating many software tasks that currently take humans a\nmonth.",
      "upvotes": 4,
      "discussionId": "67da2e8a1bba0f73374fd89e",
      "ai_keywords": [
        "RE-Bench",
        "HCAST",
        "Claude 3.7 Sonnet",
        "50%-task-completion time horizon",
        "reliability",
        "ability to adapt to mistakes",
        "logical reasoning",
        "tool use capabilities"
      ]
    },
    "publishedAt": "2025-03-18T13:59:31.000Z",
    "title": "Measuring AI Ability to Complete Long Tasks",
    "summary": "Despite rapid progress on AI benchmarks, the real-world meaning of benchmark\nperformance remains unclear. To quantify the capabilities of AI systems in\nterms of human capabilities, we propose a new metric: 50%-task-completion time\nhorizon. This is the time humans typically take to complete tasks that AI\nmodels can complete with 50% success rate. We first timed humans with relevant\ndomain expertise on a combination of RE-Bench, HCAST, and 66 novel shorter\ntasks. On these tasks, current frontier AI models such as Claude 3.7 Sonnet\nhave a 50% time horizon of around 50 minutes. Furthermore, frontier AI time\nhorizon has been doubling approximately every seven months since 2019, though\nthe trend may have accelerated in 2024. The increase in AI models' time\nhorizons seems to be primarily driven by greater reliability and ability to\nadapt to mistakes, combined with better logical reasoning and tool use\ncapabilities. We discuss the limitations of our results -- including their\ndegree of external validity -- and the implications of increased autonomy for\ndangerous capabilities. If these results generalize to real-world software\ntasks, extrapolation of this trend predicts that within 5 years, AI systems\nwill be capable of automating many software tasks that currently take humans a\nmonth.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14499.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6398
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.14495",
      "authors": [
        {
          "_id": "67da454fd5132b0eebd066ff",
          "name": "Jiacheng Guo",
          "hidden": false
        },
        {
          "_id": "67da454fd5132b0eebd06700",
          "name": "Yue Wu",
          "hidden": false
        },
        {
          "_id": "67da454fd5132b0eebd06701",
          "name": "Jiahao Qiu",
          "hidden": false
        },
        {
          "_id": "67da454fd5132b0eebd06702",
          "name": "Kaixuan Huang",
          "hidden": false
        },
        {
          "_id": "67da454fd5132b0eebd06703",
          "name": "Xinzhe Juan",
          "hidden": false
        },
        {
          "_id": "67da454fd5132b0eebd06704",
          "name": "Ling Yang",
          "hidden": false
        },
        {
          "_id": "67da454fd5132b0eebd06705",
          "name": "Mengdi Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-18T17:58:28.000Z",
      "submittedOnDailyAt": "2025-03-19T02:48:17.494Z",
      "title": "Temporal Consistency for LLM Reasoning Process Error Identification",
      "submittedOnDailyBy": {
        "_id": "64fde4e252e82dd432b74ce9",
        "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
        "isPro": false,
        "fullname": "Ling Yang",
        "user": "Lingaaaaaaa",
        "type": "user"
      },
      "summary": "Verification is crucial for effective mathematical reasoning. We present a\nnew temporal consistency method where verifiers iteratively refine their\njudgments based on the previous assessment. Unlike one-round verification or\nmulti-model debate approaches, our method leverages consistency in a sequence\nof self-reflection actions to improve verification accuracy. Empirical\nevaluations across diverse mathematical process error identification benchmarks\n(Mathcheck, ProcessBench, and PRM800K) show consistent performance improvements\nover baseline methods. When applied to the recent DeepSeek R1 distilled models,\nour method demonstrates strong performance, enabling 7B/8B distilled models to\noutperform all 70B/72B models and GPT-4o on ProcessBench. Notably, the\ndistilled 14B model with our method achieves performance comparable to\nDeepseek-R1. Our codes are available at\nhttps://github.com/jcguo123/Temporal-Consistency",
      "upvotes": 4,
      "discussionId": "67da4550d5132b0eebd0673c",
      "ai_keywords": [
        "temporal consistency",
        "verifiers",
        "iterative refinement",
        "self-reflection actions",
        "verification accuracy",
        "Mathcheck",
        "ProcessBench",
        "PRM800K",
        "DeepSeek R1",
        "distilled models",
        "GPT-4o",
        "performance comparable"
      ]
    },
    "publishedAt": "2025-03-18T13:58:28.000Z",
    "title": "Temporal Consistency for LLM Reasoning Process Error Identification",
    "summary": "Verification is crucial for effective mathematical reasoning. We present a\nnew temporal consistency method where verifiers iteratively refine their\njudgments based on the previous assessment. Unlike one-round verification or\nmulti-model debate approaches, our method leverages consistency in a sequence\nof self-reflection actions to improve verification accuracy. Empirical\nevaluations across diverse mathematical process error identification benchmarks\n(Mathcheck, ProcessBench, and PRM800K) show consistent performance improvements\nover baseline methods. When applied to the recent DeepSeek R1 distilled models,\nour method demonstrates strong performance, enabling 7B/8B distilled models to\noutperform all 70B/72B models and GPT-4o on ProcessBench. Notably, the\ndistilled 14B model with our method achieves performance comparable to\nDeepseek-R1. Our codes are available at\nhttps://github.com/jcguo123/Temporal-Consistency",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14495.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64fde4e252e82dd432b74ce9",
      "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
      "fullname": "Ling Yang",
      "name": "Lingaaaaaaa",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.14492",
      "authors": [
        {
          "_id": "67da2cbde5335651349e98c8",
          "name": "NVIDIA",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98ca",
          "name": "Hassan Abu Alhaija",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98cb",
          "name": "Jose Alvarez",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98cc",
          "name": "Maciej Bala",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98cd",
          "name": "Tiffany Cai",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98ce",
          "name": "Tianshi Cao",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98cf",
          "name": "Liz Cha",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98d0",
          "name": "Joshua Chen",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98d1",
          "name": "Mike Chen",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98d2",
          "name": "Francesco Ferroni",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98d3",
          "name": "Sanja Fidler",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98d4",
          "name": "Dieter Fox",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98d5",
          "name": "Yunhao Ge",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98d6",
          "name": "Jinwei Gu",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98d7",
          "name": "Ali Hassani",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98d8",
          "name": "Michael Isaev",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98d9",
          "name": "Pooya Jannaty",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98da",
          "name": "Shiyi Lan",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98db",
          "name": "Tobias Lasser",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98dc",
          "name": "Huan Ling",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98dd",
          "name": "Ming-Yu Liu",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98de",
          "name": "Xian Liu",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98df",
          "name": "Yifan Lu",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98e0",
          "name": "Alice Luo",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98e1",
          "name": "Qianli Ma",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98e2",
          "name": "Hanzi Mao",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98e3",
          "name": "Fabio Ramos",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98e4",
          "name": "Xuanchi Ren",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98e5",
          "name": "Tianchang Shen",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98e6",
          "name": "Shitao Tang",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98e7",
          "name": "Ting-Chun Wang",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98e8",
          "name": "Jay Wu",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98e9",
          "name": "Jiashu Xu",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98ea",
          "name": "Stella Xu",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98eb",
          "name": "Kevin Xie",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98ec",
          "name": "Yuchong Ye",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98ed",
          "name": "Xiaodong Yang",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98ee",
          "name": "Xiaohui Zeng",
          "hidden": false
        },
        {
          "_id": "67da2cbde5335651349e98ef",
          "name": "Yu Zeng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-18T17:57:54.000Z",
      "submittedOnDailyAt": "2025-03-19T01:03:48.943Z",
      "title": "Cosmos-Transfer1: Conditional World Generation with Adaptive Multimodal\n  Control",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "We introduce Cosmos-Transfer, a conditional world generation model that can\ngenerate world simulations based on multiple spatial control inputs of various\nmodalities such as segmentation, depth, and edge. In the design, the spatial\nconditional scheme is adaptive and customizable. It allows weighting different\nconditional inputs differently at different spatial locations. This enables\nhighly controllable world generation and finds use in various world-to-world\ntransfer use cases, including Sim2Real. We conduct extensive evaluations to\nanalyze the proposed model and demonstrate its applications for Physical AI,\nincluding robotics Sim2Real and autonomous vehicle data enrichment. We further\ndemonstrate an inference scaling strategy to achieve real-time world generation\nwith an NVIDIA GB200 NVL72 rack. To help accelerate research development in the\nfield, we open-source our models and code at\nhttps://github.com/nvidia-cosmos/cosmos-transfer1.",
      "upvotes": 4,
      "discussionId": "67da2cc1e5335651349e9a3e",
      "ai_keywords": [
        "conditional world generation model",
        "world simulations",
        "spatial control inputs",
        "segmentation",
        "depth",
        "edge",
        "spatial conditional scheme",
        "adaptive",
        "customizable",
        "high controllable world generation",
        "world-to-world transfer",
        "Sim2Real",
        "Physical AI",
        "robotics Sim2Real",
        "autonomous vehicle data enrichment",
        "inference scaling strategy",
        "real-time world generation",
        "NVIDIA GB200 NVL72 rack"
      ]
    },
    "publishedAt": "2025-03-18T13:57:54.000Z",
    "title": "Cosmos-Transfer1: Conditional World Generation with Adaptive Multimodal\n  Control",
    "summary": "We introduce Cosmos-Transfer, a conditional world generation model that can\ngenerate world simulations based on multiple spatial control inputs of various\nmodalities such as segmentation, depth, and edge. In the design, the spatial\nconditional scheme is adaptive and customizable. It allows weighting different\nconditional inputs differently at different spatial locations. This enables\nhighly controllable world generation and finds use in various world-to-world\ntransfer use cases, including Sim2Real. We conduct extensive evaluations to\nanalyze the proposed model and demonstrate its applications for Physical AI,\nincluding robotics Sim2Real and autonomous vehicle data enrichment. We further\ndemonstrate an inference scaling strategy to achieve real-time world generation\nwith an NVIDIA GB200 NVL72 rack. To help accelerate research development in the\nfield, we open-source our models and code at\nhttps://github.com/nvidia-cosmos/cosmos-transfer1.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14492.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6398
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.12505",
      "authors": [
        {
          "_id": "67d9442ec37d05ff0ab28e44",
          "name": "Zhaopan Xu",
          "hidden": false
        },
        {
          "_id": "67d9442ec37d05ff0ab28e45",
          "name": "Pengfei Zhou",
          "hidden": false
        },
        {
          "_id": "67d9442ec37d05ff0ab28e46",
          "name": "Jiaxin Ai",
          "hidden": false
        },
        {
          "_id": "67d9442ec37d05ff0ab28e47",
          "name": "Wangbo Zhao",
          "hidden": false
        },
        {
          "_id": "67d9442ec37d05ff0ab28e48",
          "name": "Kai Wang",
          "hidden": false
        },
        {
          "_id": "67d9442ec37d05ff0ab28e49",
          "name": "Xiaojiang Peng",
          "hidden": false
        },
        {
          "_id": "67d9442ec37d05ff0ab28e4a",
          "name": "Wenqi Shao",
          "hidden": false
        },
        {
          "_id": "67d9442ec37d05ff0ab28e4b",
          "name": "Hongxun Yao",
          "hidden": false
        },
        {
          "_id": "67d9442ec37d05ff0ab28e4c",
          "name": "Kaipeng Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-16T13:50:38.000Z",
      "submittedOnDailyAt": "2025-03-19T00:16:20.299Z",
      "title": "MPBench: A Comprehensive Multimodal Reasoning Benchmark for Process\n  Errors Identification",
      "submittedOnDailyBy": {
        "_id": "65f1713552c38a91e0a445e8",
        "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
        "isPro": false,
        "fullname": "kaipeng",
        "user": "kpzhang996",
        "type": "user"
      },
      "summary": "Reasoning is an essential capacity for large language models (LLMs) to\naddress complex tasks, where the identification of process errors is vital for\nimproving this ability. Recently, process-level reward models (PRMs) were\nproposed to provide step-wise rewards that facilitate reinforcement learning\nand data production during training and guide LLMs toward correct steps during\ninference, thereby improving reasoning accuracy. However, existing benchmarks\nof PRMs are text-based and focus on error detection, neglecting other scenarios\nlike reasoning search. To address this gap, we introduce MPBench, a\ncomprehensive, multi-task, multimodal benchmark designed to systematically\nassess the effectiveness of PRMs in diverse scenarios. MPBench employs three\nevaluation paradigms, each targeting a specific role of PRMs in the reasoning\nprocess: (1) Step Correctness, which assesses the correctness of each\nintermediate reasoning step; (2) Answer Aggregation, which aggregates multiple\nsolutions and selects the best one; and (3) Reasoning Process Search, which\nguides the search for optimal reasoning steps during inference. Through these\nparadigms, MPBench makes comprehensive evaluations and provides insights into\nthe development of multimodal PRMs.",
      "upvotes": 4,
      "discussionId": "67d94430c37d05ff0ab28eb3",
      "ai_keywords": [
        "process-level reward models (PRMs)",
        "reinforcement learning",
        "step-wise rewards",
        "error detection",
        "reasoning search",
        "MPBench",
        "multi-task",
        "multimodal benchmark",
        "Step Correctness",
        "Answer Aggregation",
        "Reasoning Process Search"
      ]
    },
    "publishedAt": "2025-03-16T09:50:38.000Z",
    "title": "MPBench: A Comprehensive Multimodal Reasoning Benchmark for Process\n  Errors Identification",
    "summary": "Reasoning is an essential capacity for large language models (LLMs) to\naddress complex tasks, where the identification of process errors is vital for\nimproving this ability. Recently, process-level reward models (PRMs) were\nproposed to provide step-wise rewards that facilitate reinforcement learning\nand data production during training and guide LLMs toward correct steps during\ninference, thereby improving reasoning accuracy. However, existing benchmarks\nof PRMs are text-based and focus on error detection, neglecting other scenarios\nlike reasoning search. To address this gap, we introduce MPBench, a\ncomprehensive, multi-task, multimodal benchmark designed to systematically\nassess the effectiveness of PRMs in diverse scenarios. MPBench employs three\nevaluation paradigms, each targeting a specific role of PRMs in the reasoning\nprocess: (1) Step Correctness, which assesses the correctness of each\nintermediate reasoning step; (2) Answer Aggregation, which aggregates multiple\nsolutions and selects the best one; and (3) Reasoning Process Search, which\nguides the search for optimal reasoning steps during inference. Through these\nparadigms, MPBench makes comprehensive evaluations and provides insights into\nthe development of multimodal PRMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12505.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65f1713552c38a91e0a445e8",
      "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
      "fullname": "kaipeng",
      "name": "kpzhang996",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.14504",
      "authors": [
        {
          "_id": "67da436711b6db6920802e9e",
          "name": "Tao Yu",
          "hidden": false
        },
        {
          "_id": "67da436711b6db6920802e9f",
          "user": {
            "_id": "623d8ca4c29adf5ef6175615",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg",
            "isPro": false,
            "fullname": "Yi-Fan Zhang",
            "user": "yifanzhang114",
            "type": "user"
          },
          "name": "Yi-Fan Zhang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-19T04:09:15.055Z",
          "hidden": false
        },
        {
          "_id": "67da436711b6db6920802ea0",
          "name": "Chaoyou Fu",
          "hidden": false
        },
        {
          "_id": "67da436711b6db6920802ea1",
          "name": "Junkang Wu",
          "hidden": false
        },
        {
          "_id": "67da436711b6db6920802ea2",
          "name": "Jinda Lu",
          "hidden": false
        },
        {
          "_id": "67da436711b6db6920802ea3",
          "name": "Kun Wang",
          "hidden": false
        },
        {
          "_id": "67da436711b6db6920802ea4",
          "name": "Xingyu Lu",
          "hidden": false
        },
        {
          "_id": "67da436711b6db6920802ea5",
          "name": "Yunhang Shen",
          "hidden": false
        },
        {
          "_id": "67da436711b6db6920802ea6",
          "name": "Guibin Zhang",
          "hidden": false
        },
        {
          "_id": "67da436711b6db6920802ea7",
          "name": "Dingjie Song",
          "hidden": false
        },
        {
          "_id": "67da436711b6db6920802ea8",
          "name": "Yibo Yan",
          "hidden": false
        },
        {
          "_id": "67da436711b6db6920802ea9",
          "name": "Tianlong Xu",
          "hidden": false
        },
        {
          "_id": "67da436711b6db6920802eaa",
          "name": "Qingsong Wen",
          "hidden": false
        },
        {
          "_id": "67da436711b6db6920802eab",
          "name": "Zhang Zhang",
          "hidden": false
        },
        {
          "_id": "67da436711b6db6920802eac",
          "name": "Yan Huang",
          "hidden": false
        },
        {
          "_id": "67da436711b6db6920802ead",
          "name": "Liang Wang",
          "hidden": false
        },
        {
          "_id": "67da436711b6db6920802eae",
          "name": "Tieniu Tan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-18T17:59:56.000Z",
      "submittedOnDailyAt": "2025-03-19T02:39:30.454Z",
      "title": "Aligning Multimodal LLM with Human Preference: A Survey",
      "submittedOnDailyBy": {
        "_id": "623d8ca4c29adf5ef6175615",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg",
        "isPro": false,
        "fullname": "Yi-Fan Zhang",
        "user": "yifanzhang114",
        "type": "user"
      },
      "summary": "Large language models (LLMs) can handle a wide variety of general tasks with\nsimple prompts, without the need for task-specific training. Multimodal Large\nLanguage Models (MLLMs), built upon LLMs, have demonstrated impressive\npotential in tackling complex tasks involving visual, auditory, and textual\ndata. However, critical issues related to truthfulness, safety, o1-like\nreasoning, and alignment with human preference remain insufficiently addressed.\nThis gap has spurred the emergence of various alignment algorithms, each\ntargeting different application scenarios and optimization goals. Recent\nstudies have shown that alignment algorithms are a powerful approach to\nresolving the aforementioned challenges. In this paper, we aim to provide a\ncomprehensive and systematic review of alignment algorithms for MLLMs.\nSpecifically, we explore four key aspects: (1) the application scenarios\ncovered by alignment algorithms, including general image understanding,\nmulti-image, video, and audio, and extended multimodal applications; (2) the\ncore factors in constructing alignment datasets, including data sources, model\nresponses, and preference annotations; (3) the benchmarks used to evaluate\nalignment algorithms; and (4) a discussion of potential future directions for\nthe development of alignment algorithms. This work seeks to help researchers\norganize current advancements in the field and inspire better alignment\nmethods. The project page of this paper is available at\nhttps://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Alignment.",
      "upvotes": 3,
      "discussionId": "67da436b11b6db6920803040",
      "githubRepo": "https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Alignment",
      "ai_keywords": [
        "Large language models (LLMs)",
        "Multimodal Large Language Models (MLLMs)",
        "Truthfulness",
        "Safety",
        "o1-like reasoning",
        "Alignment with human preference",
        "Alignment algorithms",
        "General image understanding",
        "Multi-image",
        "Video",
        "Audio",
        "Extended multimodal applications",
        "Alignment datasets",
        "Data sources",
        "Model responses",
        "Preference annotations",
        "Benchmarks"
      ]
    },
    "publishedAt": "2025-03-18T13:59:56.000Z",
    "title": "Aligning Multimodal LLM with Human Preference: A Survey",
    "summary": "Large language models (LLMs) can handle a wide variety of general tasks with\nsimple prompts, without the need for task-specific training. Multimodal Large\nLanguage Models (MLLMs), built upon LLMs, have demonstrated impressive\npotential in tackling complex tasks involving visual, auditory, and textual\ndata. However, critical issues related to truthfulness, safety, o1-like\nreasoning, and alignment with human preference remain insufficiently addressed.\nThis gap has spurred the emergence of various alignment algorithms, each\ntargeting different application scenarios and optimization goals. Recent\nstudies have shown that alignment algorithms are a powerful approach to\nresolving the aforementioned challenges. In this paper, we aim to provide a\ncomprehensive and systematic review of alignment algorithms for MLLMs.\nSpecifically, we explore four key aspects: (1) the application scenarios\ncovered by alignment algorithms, including general image understanding,\nmulti-image, video, and audio, and extended multimodal applications; (2) the\ncore factors in constructing alignment datasets, including data sources, model\nresponses, and preference annotations; (3) the benchmarks used to evaluate\nalignment algorithms; and (4) a discussion of potential future directions for\nthe development of alignment algorithms. This work seeks to help researchers\norganize current advancements in the field and inspire better alignment\nmethods. The project page of this paper is available at\nhttps://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Alignment.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14504.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "623d8ca4c29adf5ef6175615",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg",
      "fullname": "Yi-Fan Zhang",
      "name": "yifanzhang114",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.12545",
      "authors": [
        {
          "_id": "67d943d272843a36b74ab41c",
          "name": "Zhaopan Xu",
          "hidden": false
        },
        {
          "_id": "67d943d272843a36b74ab41d",
          "name": "Pengfei Zhou",
          "hidden": false
        },
        {
          "_id": "67d943d272843a36b74ab41e",
          "name": "Weidong Tang",
          "hidden": false
        },
        {
          "_id": "67d943d272843a36b74ab41f",
          "name": "Jiaxin Ai",
          "hidden": false
        },
        {
          "_id": "67d943d272843a36b74ab420",
          "name": "Wangbo Zhao",
          "hidden": false
        },
        {
          "_id": "67d943d272843a36b74ab421",
          "name": "Xiaojiang Peng",
          "hidden": false
        },
        {
          "_id": "67d943d272843a36b74ab422",
          "name": "Kai Wang",
          "hidden": false
        },
        {
          "_id": "67d943d272843a36b74ab423",
          "name": "Yang You",
          "hidden": false
        },
        {
          "_id": "67d943d272843a36b74ab424",
          "name": "Wenqi Shao",
          "hidden": false
        },
        {
          "_id": "67d943d272843a36b74ab425",
          "name": "Hongxun Yao",
          "hidden": false
        },
        {
          "_id": "67d943d272843a36b74ab426",
          "name": "Kaipeng Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-16T15:26:20.000Z",
      "submittedOnDailyAt": "2025-03-19T00:14:50.269Z",
      "title": "PEBench: A Fictitious Dataset to Benchmark Machine Unlearning for\n  Multimodal Large Language Models",
      "submittedOnDailyBy": {
        "_id": "65f1713552c38a91e0a445e8",
        "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
        "isPro": false,
        "fullname": "kaipeng",
        "user": "kpzhang996",
        "type": "user"
      },
      "summary": "In recent years, Multimodal Large Language Models (MLLMs) have demonstrated\nremarkable advancements in tasks such as visual question answering, visual\nunderstanding, and reasoning. However, this impressive progress relies on vast\namounts of data collected from the internet, raising significant concerns about\nprivacy and security. To address these issues, machine unlearning (MU) has\nemerged as a promising solution, enabling the removal of specific knowledge\nfrom an already trained model without requiring retraining from scratch.\nAlthough MU for MLLMs has gained attention, current evaluations of its efficacy\nremain incomplete, and the underlying problem is often poorly defined, which\nhinders the development of strategies for creating more secure and trustworthy\nsystems. To bridge this gap, we introduce a benchmark, named PEBench, which\nincludes a dataset of personal entities and corresponding general event scenes,\ndesigned to comprehensively assess the performance of MU for MLLMs. Through\nPEBench, we aim to provide a standardized and robust framework to advance\nresearch in secure and privacy-preserving multimodal models. We benchmarked 6\nMU methods, revealing their strengths and limitations, and shedding light on\nkey challenges and opportunities for MU in MLLMs.",
      "upvotes": 2,
      "discussionId": "67d943db72843a36b74ab652",
      "ai_keywords": [
        "Multimodal Large Language Models",
        "visual question answering",
        "visual understanding",
        "reasoning",
        "machine unlearning",
        "benchmark",
        "PEBench",
        "dataset",
        "personal entities",
        "general event scenes",
        "secure",
        "privacy-preserving",
        "multimodal models"
      ]
    },
    "publishedAt": "2025-03-16T11:26:20.000Z",
    "title": "PEBench: A Fictitious Dataset to Benchmark Machine Unlearning for\n  Multimodal Large Language Models",
    "summary": "In recent years, Multimodal Large Language Models (MLLMs) have demonstrated\nremarkable advancements in tasks such as visual question answering, visual\nunderstanding, and reasoning. However, this impressive progress relies on vast\namounts of data collected from the internet, raising significant concerns about\nprivacy and security. To address these issues, machine unlearning (MU) has\nemerged as a promising solution, enabling the removal of specific knowledge\nfrom an already trained model without requiring retraining from scratch.\nAlthough MU for MLLMs has gained attention, current evaluations of its efficacy\nremain incomplete, and the underlying problem is often poorly defined, which\nhinders the development of strategies for creating more secure and trustworthy\nsystems. To bridge this gap, we introduce a benchmark, named PEBench, which\nincludes a dataset of personal entities and corresponding general event scenes,\ndesigned to comprehensively assess the performance of MU for MLLMs. Through\nPEBench, we aim to provide a standardized and robust framework to advance\nresearch in secure and privacy-preserving multimodal models. We benchmarked 6\nMU methods, revealing their strengths and limitations, and shedding light on\nkey challenges and opportunities for MU in MLLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12545.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65f1713552c38a91e0a445e8",
      "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
      "fullname": "kaipeng",
      "name": "kpzhang996",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.14151",
      "authors": [
        {
          "_id": "67da71b1c26b43885226a72d",
          "name": "Yong Zhong",
          "hidden": false
        },
        {
          "_id": "67da71b1c26b43885226a72e",
          "name": "Zhuoyi Yang",
          "hidden": false
        },
        {
          "_id": "67da71b1c26b43885226a72f",
          "name": "Jiayan Teng",
          "hidden": false
        },
        {
          "_id": "67da71b1c26b43885226a730",
          "name": "Xiaotao Gu",
          "hidden": false
        },
        {
          "_id": "67da71b1c26b43885226a731",
          "name": "Chongxuan Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-18T11:17:32.000Z",
      "submittedOnDailyAt": "2025-03-19T05:57:05.443Z",
      "title": "Concat-ID: Towards Universal Identity-Preserving Video Synthesis",
      "submittedOnDailyBy": {
        "_id": "63468720dd6d90d82ccf3450",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
        "isPro": false,
        "fullname": "YSH",
        "user": "BestWishYsh",
        "type": "user"
      },
      "summary": "We present Concat-ID, a unified framework for identity-preserving video\ngeneration. Concat-ID employs Variational Autoencoders to extract image\nfeatures, which are concatenated with video latents along the sequence\ndimension, leveraging solely 3D self-attention mechanisms without the need for\nadditional modules. A novel cross-video pairing strategy and a multi-stage\ntraining regimen are introduced to balance identity consistency and facial\neditability while enhancing video naturalness. Extensive experiments\ndemonstrate Concat-ID's superiority over existing methods in both single and\nmulti-identity generation, as well as its seamless scalability to multi-subject\nscenarios, including virtual try-on and background-controllable generation.\nConcat-ID establishes a new benchmark for identity-preserving video synthesis,\nproviding a versatile and scalable solution for a wide range of applications.",
      "upvotes": 1,
      "discussionId": "67da71bdc26b43885226ab4e",
      "projectPage": "https://ml-gsai.github.io/Concat-ID-demo/",
      "githubRepo": "https://github.com/ML-GSAI/Concat-ID",
      "ai_keywords": [
        "Variational Autoencoders",
        "3D self-attention mechanisms",
        "cross-video pairing strategy",
        "multi-stage training regimen",
        "identity consistency",
        "facial editability",
        "video naturalness",
        "identity-preserving video synthesis"
      ]
    },
    "publishedAt": "2025-03-18T07:17:32.000Z",
    "title": "Concat-ID: Towards Universal Identity-Preserving Video Synthesis",
    "summary": "We present Concat-ID, a unified framework for identity-preserving video\ngeneration. Concat-ID employs Variational Autoencoders to extract image\nfeatures, which are concatenated with video latents along the sequence\ndimension, leveraging solely 3D self-attention mechanisms without the need for\nadditional modules. A novel cross-video pairing strategy and a multi-stage\ntraining regimen are introduced to balance identity consistency and facial\neditability while enhancing video naturalness. Extensive experiments\ndemonstrate Concat-ID's superiority over existing methods in both single and\nmulti-identity generation, as well as its seamless scalability to multi-subject\nscenarios, including virtual try-on and background-controllable generation.\nConcat-ID establishes a new benchmark for identity-preserving video synthesis,\nproviding a versatile and scalable solution for a wide range of applications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14151.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63468720dd6d90d82ccf3450",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
      "fullname": "YSH",
      "name": "BestWishYsh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 35
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.12271",
      "authors": [
        {
          "_id": "67d926523acb37a1cfa74cf8",
          "name": "Shufan Li",
          "hidden": false
        },
        {
          "_id": "67d926523acb37a1cfa74cf9",
          "name": "Konstantinos Kallidromitis",
          "hidden": false
        },
        {
          "_id": "67d926523acb37a1cfa74cfa",
          "name": "Akash Gokul",
          "hidden": false
        },
        {
          "_id": "67d926523acb37a1cfa74cfb",
          "name": "Arsh Koneru",
          "hidden": false
        },
        {
          "_id": "67d926523acb37a1cfa74cfc",
          "name": "Yusuke Kato",
          "hidden": false
        },
        {
          "_id": "67d926523acb37a1cfa74cfd",
          "name": "Kazuki Kozuka",
          "hidden": false
        },
        {
          "_id": "67d926523acb37a1cfa74cfe",
          "name": "Aditya Grover",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-15T21:58:12.000Z",
      "submittedOnDailyAt": "2025-03-19T02:08:42.869Z",
      "title": "Reflect-DiT: Inference-Time Scaling for Text-to-Image Diffusion\n  Transformers via In-Context Reflection",
      "submittedOnDailyBy": {
        "_id": "6310531914aa81e1044363ed",
        "avatarUrl": "/avatars/ae7767e591cb7199ea2f62d2db89fc7f.svg",
        "isPro": false,
        "fullname": "Shufan Li",
        "user": "jacklishufan",
        "type": "user"
      },
      "summary": "The predominant approach to advancing text-to-image generation has been\ntraining-time scaling, where larger models are trained on more data using\ngreater computational resources. While effective, this approach is\ncomputationally expensive, leading to growing interest in inference-time\nscaling to improve performance. Currently, inference-time scaling for\ntext-to-image diffusion models is largely limited to best-of-N sampling, where\nmultiple images are generated per prompt and a selection model chooses the best\noutput. Inspired by the recent success of reasoning models like DeepSeek-R1 in\nthe language domain, we introduce an alternative to naive best-of-N sampling by\nequipping text-to-image Diffusion Transformers with in-context reflection\ncapabilities. We propose Reflect-DiT, a method that enables Diffusion\nTransformers to refine their generations using in-context examples of\npreviously generated images alongside textual feedback describing necessary\nimprovements. Instead of passively relying on random sampling and hoping for a\nbetter result in a future generation, Reflect-DiT explicitly tailors its\ngenerations to address specific aspects requiring enhancement. Experimental\nresults demonstrate that Reflect-DiT improves performance on the GenEval\nbenchmark (+0.19) using SANA-1.0-1.6B as a base model. Additionally, it\nachieves a new state-of-the-art score of 0.81 on GenEval while generating only\n20 samples per prompt, surpassing the previous best score of 0.80, which was\nobtained using a significantly larger model (SANA-1.5-4.8B) with 2048 samples\nunder the best-of-N approach.",
      "upvotes": 1,
      "discussionId": "67d926543acb37a1cfa74d9f",
      "ai_keywords": [
        "diffusion models",
        "text-to-image diffusion models",
        "best-of-N sampling",
        "in-context reflection",
        "Diffusion Transformers",
        "Reflect-DiT",
        "GenEval benchmark",
        "in-context examples",
        "textual feedback",
        "performance improvement",
        "state-of-the-art"
      ]
    },
    "publishedAt": "2025-03-15T17:58:12.000Z",
    "title": "Reflect-DiT: Inference-Time Scaling for Text-to-Image Diffusion\n  Transformers via In-Context Reflection",
    "summary": "The predominant approach to advancing text-to-image generation has been\ntraining-time scaling, where larger models are trained on more data using\ngreater computational resources. While effective, this approach is\ncomputationally expensive, leading to growing interest in inference-time\nscaling to improve performance. Currently, inference-time scaling for\ntext-to-image diffusion models is largely limited to best-of-N sampling, where\nmultiple images are generated per prompt and a selection model chooses the best\noutput. Inspired by the recent success of reasoning models like DeepSeek-R1 in\nthe language domain, we introduce an alternative to naive best-of-N sampling by\nequipping text-to-image Diffusion Transformers with in-context reflection\ncapabilities. We propose Reflect-DiT, a method that enables Diffusion\nTransformers to refine their generations using in-context examples of\npreviously generated images alongside textual feedback describing necessary\nimprovements. Instead of passively relying on random sampling and hoping for a\nbetter result in a future generation, Reflect-DiT explicitly tailors its\ngenerations to address specific aspects requiring enhancement. Experimental\nresults demonstrate that Reflect-DiT improves performance on the GenEval\nbenchmark (+0.19) using SANA-1.0-1.6B as a base model. Additionally, it\nachieves a new state-of-the-art score of 0.81 on GenEval while generating only\n20 samples per prompt, surpassing the previous best score of 0.80, which was\nobtained using a significantly larger model (SANA-1.5-4.8B) with 2048 samples\nunder the best-of-N approach.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12271.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6310531914aa81e1044363ed",
      "avatarUrl": "/avatars/ae7767e591cb7199ea2f62d2db89fc7f.svg",
      "fullname": "Shufan Li",
      "name": "jacklishufan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.10546",
      "authors": [
        {
          "_id": "67da141d6b2857e3ec1412a7",
          "name": "Zixian Liu",
          "hidden": false
        },
        {
          "_id": "67da141d6b2857e3ec1412a8",
          "name": "Mingtong Zhang",
          "hidden": false
        },
        {
          "_id": "67da141d6b2857e3ec1412a9",
          "name": "Yunzhu Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-13T16:59:17.000Z",
      "submittedOnDailyAt": "2025-03-19T00:00:05.763Z",
      "title": "KUDA: Keypoints to Unify Dynamics Learning and Visual Prompting for\n  Open-Vocabulary Robotic Manipulation",
      "submittedOnDailyBy": {
        "_id": "671c6a3e255aa50ebb504fc5",
        "avatarUrl": "/avatars/b549bb19990ae21690472799817f951e.svg",
        "isPro": false,
        "fullname": "Mingtong Zhang",
        "user": "Mingtongz",
        "type": "user"
      },
      "summary": "With the rapid advancement of large language models (LLMs) and\nvision-language models (VLMs), significant progress has been made in developing\nopen-vocabulary robotic manipulation systems. However, many existing approaches\noverlook the importance of object dynamics, limiting their applicability to\nmore complex, dynamic tasks. In this work, we introduce KUDA, an\nopen-vocabulary manipulation system that integrates dynamics learning and\nvisual prompting through keypoints, leveraging both VLMs and learning-based\nneural dynamics models. Our key insight is that a keypoint-based target\nspecification is simultaneously interpretable by VLMs and can be efficiently\ntranslated into cost functions for model-based planning. Given language\ninstructions and visual observations, KUDA first assigns keypoints to the RGB\nimage and queries the VLM to generate target specifications. These abstract\nkeypoint-based representations are then converted into cost functions, which\nare optimized using a learned dynamics model to produce robotic trajectories.\nWe evaluate KUDA on a range of manipulation tasks, including free-form language\ninstructions across diverse object categories, multi-object interactions, and\ndeformable or granular objects, demonstrating the effectiveness of our\nframework. The project page is available at http://kuda-dynamics.github.io.",
      "upvotes": 1,
      "discussionId": "67da141e6b2857e3ec141301",
      "projectPage": "https://kuda-dynamics.github.io",
      "githubRepo": "https://github.com/StoreBlank/KUDA",
      "ai_keywords": [
        "LLMs",
        "VLMs",
        "open-vocabulary robotic manipulation systems",
        "object dynamics",
        "KUDA",
        "dynamics learning",
        "visual prompting",
        "keypoints",
        "learning-based neural dynamics models",
        "keypoint-based target specification",
        "cost functions",
        "model-based planning",
        "robotic trajectories",
        "free-form language instructions",
        "multi-object interactions",
        "deformable objects",
        "granular objects"
      ]
    },
    "publishedAt": "2025-03-13T12:59:17.000Z",
    "title": "KUDA: Keypoints to Unify Dynamics Learning and Visual Prompting for\n  Open-Vocabulary Robotic Manipulation",
    "summary": "With the rapid advancement of large language models (LLMs) and\nvision-language models (VLMs), significant progress has been made in developing\nopen-vocabulary robotic manipulation systems. However, many existing approaches\noverlook the importance of object dynamics, limiting their applicability to\nmore complex, dynamic tasks. In this work, we introduce KUDA, an\nopen-vocabulary manipulation system that integrates dynamics learning and\nvisual prompting through keypoints, leveraging both VLMs and learning-based\nneural dynamics models. Our key insight is that a keypoint-based target\nspecification is simultaneously interpretable by VLMs and can be efficiently\ntranslated into cost functions for model-based planning. Given language\ninstructions and visual observations, KUDA first assigns keypoints to the RGB\nimage and queries the VLM to generate target specifications. These abstract\nkeypoint-based representations are then converted into cost functions, which\nare optimized using a learned dynamics model to produce robotic trajectories.\nWe evaluate KUDA on a range of manipulation tasks, including free-form language\ninstructions across diverse object categories, multi-object interactions, and\ndeformable or granular objects, demonstrating the effectiveness of our\nframework. The project page is available at http://kuda-dynamics.github.io.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10546.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "671c6a3e255aa50ebb504fc5",
      "avatarUrl": "/avatars/b549bb19990ae21690472799817f951e.svg",
      "fullname": "Mingtong Zhang",
      "name": "Mingtongz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.10410",
      "authors": [
        {
          "_id": "67d9638d92e48ed07860ecee",
          "user": {
            "_id": "67934b85c67af4a116b5594b",
            "avatarUrl": "/avatars/076cf0803b50e1ab54e5ba4f8f2a8e44.svg",
            "isPro": false,
            "fullname": "yuwendu",
            "user": "yuwendu",
            "type": "user"
          },
          "name": "Yuwen Du",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-18T14:57:49.908Z",
          "hidden": false
        },
        {
          "_id": "67d9638d92e48ed07860ecef",
          "name": "Anning Hu",
          "hidden": false
        },
        {
          "_id": "67d9638d92e48ed07860ecf0",
          "name": "Zichen Chao",
          "hidden": false
        },
        {
          "_id": "67d9638d92e48ed07860ecf1",
          "name": "Yifan Lu",
          "hidden": false
        },
        {
          "_id": "67d9638d92e48ed07860ecf2",
          "name": "Junhao Ge",
          "hidden": false
        },
        {
          "_id": "67d9638d92e48ed07860ecf3",
          "name": "Genjia Liu",
          "hidden": false
        },
        {
          "_id": "67d9638d92e48ed07860ecf4",
          "name": "Weitao Wu",
          "hidden": false
        },
        {
          "_id": "67d9638d92e48ed07860ecf5",
          "name": "Lanjun Wang",
          "hidden": false
        },
        {
          "_id": "67d9638d92e48ed07860ecf6",
          "name": "Siheng Chen",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/67934b85c67af4a116b5594b/rDLvz9Jma7IWsaGubueis.png",
        "https://cdn-uploads.huggingface.co/production/uploads/67934b85c67af4a116b5594b/1PULsx8sLyy9CqDRMuzo1.png"
      ],
      "publishedAt": "2025-03-13T14:33:42.000Z",
      "submittedOnDailyAt": "2025-03-19T00:00:36.900Z",
      "title": "RoCo-Sim: Enhancing Roadside Collaborative Perception through Foreground\n  Simulation",
      "submittedOnDailyBy": {
        "_id": "67934b85c67af4a116b5594b",
        "avatarUrl": "/avatars/076cf0803b50e1ab54e5ba4f8f2a8e44.svg",
        "isPro": false,
        "fullname": "yuwendu",
        "user": "yuwendu",
        "type": "user"
      },
      "summary": "Roadside Collaborative Perception refers to a system where multiple roadside\nunits collaborate to pool their perceptual data, assisting vehicles in\nenhancing their environmental awareness. Existing roadside perception methods\nconcentrate on model design but overlook data issues like calibration errors,\nsparse information, and multi-view consistency, leading to poor performance on\nrecent published datasets. To significantly enhance roadside collaborative\nperception and address critical data issues, we present the first simulation\nframework RoCo-Sim for road-side collaborative perception. RoCo-Sim is capable\nof generating diverse, multi-view consistent simulated roadside data through\ndynamic foreground editing and full-scene style transfer of a single image.\nRoCo-Sim consists of four components: (1) Camera Extrinsic Optimization ensures\naccurate 3D to 2D projection for roadside cameras; (2) A novel Multi-View\nOcclusion-Aware Sampler (MOAS) determines the placement of diverse digital\nassets within 3D space; (3) DepthSAM innovatively models foreground-background\nrelationships from single-frame fixed-view images, ensuring multi-view\nconsistency of foreground; and (4) Scalable Post-Processing Toolkit generates\nmore realistic and enriched scenes through style transfer and other\nenhancements. RoCo-Sim significantly improves roadside 3D object detection,\noutperforming SOTA methods by 83.74 on Rcooper-Intersection and 83.12 on\nTUMTraf-V2X for AP70. RoCo-Sim fills a critical gap in roadside perception\nsimulation. Code and pre-trained models will be released soon:\nhttps://github.com/duyuwen-duen/RoCo-Sim",
      "upvotes": 1,
      "discussionId": "67d9639192e48ed07860ee1f",
      "ai_keywords": [
        "Multi-View Occlusion-Aware Sampler",
        "DepthSAM",
        "Scalable Post-Processing Toolkit",
        "3D object detection"
      ]
    },
    "publishedAt": "2025-03-13T10:33:42.000Z",
    "title": "RoCo-Sim: Enhancing Roadside Collaborative Perception through Foreground\n  Simulation",
    "summary": "Roadside Collaborative Perception refers to a system where multiple roadside\nunits collaborate to pool their perceptual data, assisting vehicles in\nenhancing their environmental awareness. Existing roadside perception methods\nconcentrate on model design but overlook data issues like calibration errors,\nsparse information, and multi-view consistency, leading to poor performance on\nrecent published datasets. To significantly enhance roadside collaborative\nperception and address critical data issues, we present the first simulation\nframework RoCo-Sim for road-side collaborative perception. RoCo-Sim is capable\nof generating diverse, multi-view consistent simulated roadside data through\ndynamic foreground editing and full-scene style transfer of a single image.\nRoCo-Sim consists of four components: (1) Camera Extrinsic Optimization ensures\naccurate 3D to 2D projection for roadside cameras; (2) A novel Multi-View\nOcclusion-Aware Sampler (MOAS) determines the placement of diverse digital\nassets within 3D space; (3) DepthSAM innovatively models foreground-background\nrelationships from single-frame fixed-view images, ensuring multi-view\nconsistency of foreground; and (4) Scalable Post-Processing Toolkit generates\nmore realistic and enriched scenes through style transfer and other\nenhancements. RoCo-Sim significantly improves roadside 3D object detection,\noutperforming SOTA methods by 83.74 on Rcooper-Intersection and 83.12 on\nTUMTraf-V2X for AP70. RoCo-Sim fills a critical gap in roadside perception\nsimulation. Code and pre-trained models will be released soon:\nhttps://github.com/duyuwen-duen/RoCo-Sim",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/67934b85c67af4a116b5594b/rDLvz9Jma7IWsaGubueis.png",
      "https://cdn-uploads.huggingface.co/production/uploads/67934b85c67af4a116b5594b/1PULsx8sLyy9CqDRMuzo1.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10410.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67934b85c67af4a116b5594b",
      "avatarUrl": "/avatars/076cf0803b50e1ab54e5ba4f8f2a8e44.svg",
      "fullname": "yuwendu",
      "name": "yuwendu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]