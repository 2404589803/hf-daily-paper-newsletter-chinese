[
    "{'paper': {'id': '2412.18450', 'authors': [{'_id': '676bbe579484d105b89dba3b', 'user': {'_id': '6363767e572fd34304f49a67', 'avatarUrl': '/avatars/a9fc92b6005d48adf71a45bebf812648.svg', 'isPro': False, 'fullname': 'Tatiana Zemskova', 'user': 'wingrune', 'type': 'user'}, 'name': 'Tatiana Zemskova', 'status': 'extracted_pending', 'statusLastChangedAt': '2024-12-25T08:52:22.544Z', 'hidden': False}, {'_id': '676bbe579484d105b89dba3c', 'name': 'Dmitry Yudin', 'hidden': False}], 'publishedAt': '2024-12-24T14:21:58.000Z', 'title': '3DGraphLLM: Combining Semantic Graphs and Large Language Models for 3D\\n  Scene Understanding', 'summary': 'A 3D scene graph represents a compact scene model, storing information about\\nthe objects and the semantic relationships between them, making its use\\npromising for robotic tasks. When interacting with a user, an embodied\\nintelligent agent should be capable of responding to various queries about the\\nscene formulated in natural language. Large Language Models (LLMs) are\\nbeneficial solutions for user-robot interaction due to their natural language\\nunderstanding and reasoning abilities. Recent methods for creating learnable\\nrepresentations of 3D scenes have demonstrated the potential to improve the\\nquality of LLMs responses by adapting to the 3D world. However, the existing\\nmethods do not explicitly utilize information about the semantic relationships\\nbetween objects, limiting themselves to information about their coordinates. In\\nthis work, we propose a method 3DGraphLLM for constructing a learnable\\nrepresentation of a 3D scene graph. The learnable representation is used as\\ninput for LLMs to perform 3D vision-language tasks. In our experiments on\\npopular ScanRefer, RIORefer, Multi3DRefer, ScanQA, Sqa3D, and Scan2cap\\ndatasets, we demonstrate the advantage of this approach over baseline methods\\nthat do not use information about the semantic relationships between objects.\\nThe code is publicly available at\\nhttps://github.com/CognitiveAISystems/3DGraphLLM.', 'upvotes': 21, 'discussionId': '676bbe599484d105b89dbac5'}, 'publishedAt': '2024-12-25T04:26:52.921Z', 'title': '3DGraphLLM: Combining Semantic Graphs and Large Language Models for 3D Scene Understanding', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/6363767e572fd34304f49a67/W7uO-sU8egl0RxsbSGcPg.png'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.18450.png', 'numComments': 1, 'submittedBy': {'_id': '6363767e572fd34304f49a67', 'avatarUrl': '/avatars/a9fc92b6005d48adf71a45bebf812648.svg', 'fullname': 'Tatiana Zemskova', 'name': 'wingrune', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2412.18153', 'authors': [{'_id': '676b7d07d886f8125a4fb855', 'name': 'Zhiheng Liu', 'hidden': False}, {'_id': '676b7d07d886f8125a4fb856', 'name': 'Ka Leong Cheng', 'hidden': False}, {'_id': '676b7d07d886f8125a4fb857', 'name': 'Qiuyu Wang', 'hidden': False}, {'_id': '676b7d07d886f8125a4fb858', 'name': 'Shuzhe Wang', 'hidden': False}, {'_id': '676b7d07d886f8125a4fb859', 'name': 'Hao Ouyang', 'hidden': False}, {'_id': '676b7d07d886f8125a4fb85a', 'name': 'Bin Tan', 'hidden': False}, {'_id': '676b7d07d886f8125a4fb85b', 'name': 'Kai Zhu', 'hidden': False}, {'_id': '676b7d07d886f8125a4fb85c', 'name': 'Yujun Shen', 'hidden': False}, {'_id': '676b7d07d886f8125a4fb85d', 'name': 'Qifeng Chen', 'hidden': False}, {'_id': '676b7d07d886f8125a4fb85e', 'name': 'Ping Luo', 'hidden': False}], 'publishedAt': '2024-12-24T04:16:38.000Z', 'title': 'DepthLab: From Partial to Complete', 'summary': 'Missing values remain a common challenge for depth data across its wide range\\nof applications, stemming from various causes like incomplete data acquisition\\nand perspective alteration. This work bridges this gap with DepthLab, a\\nfoundation depth inpainting model powered by image diffusion priors. Our model\\nfeatures two notable strengths: (1) it demonstrates resilience to\\ndepth-deficient regions, providing reliable completion for both continuous\\nareas and isolated points, and (2) it faithfully preserves scale consistency\\nwith the conditioned known depth when filling in missing values. Drawing on\\nthese advantages, our approach proves its worth in various downstream tasks,\\nincluding 3D scene inpainting, text-to-3D scene generation, sparse-view\\nreconstruction with DUST3R, and LiDAR depth completion, exceeding current\\nsolutions in both numerical performance and visual quality. Our project page\\nwith source code is available at https://johanan528.github.io/depthlab_web/.', 'upvotes': 21, 'discussionId': '676b7d0bd886f8125a4fb983'}, 'publishedAt': '2024-12-24T22:37:30.445Z', 'title': 'DepthLab: From Partial to Complete', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/6479925ab77e18dbf640bd67/kJ_cJvqOflDjH6dllp3Xe.mp4'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.18153.png', 'numComments': 1, 'submittedBy': {'_id': '6479925ab77e18dbf640bd67', 'avatarUrl': '/avatars/bb52ecd22ca4b49157f8668be35409e7.svg', 'fullname': 'Zhiheng Liu', 'name': 'Johanan0528', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 2}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2412.17739', 'authors': [{'_id': '676a6844bee647b8c004f469', 'name': 'Ermo Hua', 'hidden': False}, {'_id': '676a6844bee647b8c004f46a', 'name': 'Che Jiang', 'hidden': False}, {'_id': '676a6844bee647b8c004f46b', 'name': 'Xingtai Lv', 'hidden': False}, {'_id': '676a6844bee647b8c004f46c', 'name': 'Kaiyan Zhang', 'hidden': False}, {'_id': '676a6844bee647b8c004f46d', 'name': 'Ning Ding', 'hidden': False}, {'_id': '676a6844bee647b8c004f46e', 'name': 'Youbang Sun', 'hidden': False}, {'_id': '676a6844bee647b8c004f46f', 'name': 'Biqing Qi', 'hidden': False}, {'_id': '676a6844bee647b8c004f470', 'name': 'Yuchen Fan', 'hidden': False}, {'_id': '676a6844bee647b8c004f471', 'name': 'Xue Kai Zhu', 'hidden': False}, {'_id': '676a6844bee647b8c004f472', 'name': 'Bowen Zhou', 'hidden': False}], 'publishedAt': '2024-12-23T17:44:01.000Z', 'title': \"Fourier Position Embedding: Enhancing Attention's Periodic Extension for\\n  Length Generalization\", 'summary': \"Extending the context length of Language Models (LMs) by improving Rotary\\nPosition Embedding (RoPE) has become a trend. While existing works mainly\\naddress RoPE's limitations within attention mechanism, this paper provides an\\nanalysis across nearly all parts of LMs, uncovering their adverse effects on\\nlength generalization for RoPE-based attention. Using Discrete Signal\\nProcessing theory, we show that RoPE enables periodic attention by implicitly\\nachieving Non-Uniform Discrete Fourier Transform. However, this periodicity is\\nundermined by the spectral damage caused by: 1) linear layers and activation\\nfunctions outside of attention; 2) insufficiently trained frequency components\\nbrought by time-domain truncation. Building on our observations, we propose\\nFourier Position Embedding (FoPE), which enhances attention's frequency-domain\\nproperties to improve both its periodic extension and length generalization.\\nFoPE constructs Fourier Series and zero-outs the destructive frequency\\ncomponents, increasing model robustness against the spectrum damage.\\nExperiments across various model scales show that, within varying context\\nwindows, FoPE can maintain a more stable perplexity and a more consistent\\naccuracy in a needle-in-haystack task compared to RoPE and ALiBi. Several\\nanalyses and ablations bring further support to our method and theoretical\\nmodeling.\", 'upvotes': 16, 'discussionId': '676a6845bee647b8c004f51c'}, 'publishedAt': '2024-12-24T23:37:35.368Z', 'title': \"Fourier Position Embedding: Enhancing Attention's Periodic Extension for Length Generalization\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.17739.png', 'numComments': 2, 'submittedBy': {'_id': '60bc94cd85a3ab33829b6211', 'avatarUrl': '/avatars/b57d36c7577fbbb42ea5b963eef4144a.svg', 'fullname': 'Kaiyan Zhang', 'name': 'iseesaw', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2412.18597', 'authors': [{'_id': '676b9b876fb4876383b8591b', 'name': 'Minghong Cai', 'hidden': False}, {'_id': '676b9b876fb4876383b8591c', 'name': 'Xiaodong Cun', 'hidden': False}, {'_id': '676b9b876fb4876383b8591d', 'name': 'Xiaoyu Li', 'hidden': False}, {'_id': '676b9b876fb4876383b8591e', 'name': 'Wenze Liu', 'hidden': False}, {'_id': '676b9b876fb4876383b8591f', 'name': 'Zhaoyang Zhang', 'hidden': False}, {'_id': '676b9b876fb4876383b85920', 'name': 'Yong Zhang', 'hidden': False}, {'_id': '676b9b876fb4876383b85921', 'name': 'Ying Shan', 'hidden': False}, {'_id': '676b9b876fb4876383b85922', 'name': 'Xiangyu Yue', 'hidden': False}], 'publishedAt': '2024-12-24T18:51:19.000Z', 'title': 'DiTCtrl: Exploring Attention Control in Multi-Modal Diffusion\\n  Transformer for Tuning-Free Multi-Prompt Longer Video Generation', 'summary': \"Sora-like video generation models have achieved remarkable progress with a\\nMulti-Modal Diffusion Transformer MM-DiT architecture. However, the current\\nvideo generation models predominantly focus on single-prompt, struggling to\\ngenerate coherent scenes with multiple sequential prompts that better reflect\\nreal-world dynamic scenarios. While some pioneering works have explored\\nmulti-prompt video generation, they face significant challenges including\\nstrict training data requirements, weak prompt following, and unnatural\\ntransitions. To address these problems, we propose DiTCtrl, a training-free\\nmulti-prompt video generation method under MM-DiT architectures for the first\\ntime. Our key idea is to take the multi-prompt video generation task as\\ntemporal video editing with smooth transitions. To achieve this goal, we first\\nanalyze MM-DiT's attention mechanism, finding that the 3D full attention\\nbehaves similarly to that of the cross/self-attention blocks in the UNet-like\\ndiffusion models, enabling mask-guided precise semantic control across\\ndifferent prompts with attention sharing for multi-prompt video generation.\\nBased on our careful design, the video generated by DiTCtrl achieves smooth\\ntransitions and consistent object motion given multiple sequential prompts\\nwithout additional training. Besides, we also present MPVBench, a new benchmark\\nspecially designed for multi-prompt video generation to evaluate the\\nperformance of multi-prompt generation. Extensive experiments demonstrate that\\nour method achieves state-of-the-art performance without additional training.\", 'upvotes': 10, 'discussionId': '676b9b886fb4876383b8597d'}, 'publishedAt': '2024-12-25T00:47:02.418Z', 'title': 'DiTCtrl: Exploring Attention Control in Multi-Modal Diffusion Transformer for Tuning-Free Multi-Prompt Longer Video Generation', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/63184c517ca1b876d99b7e0e/GgE-qpq_L56Fc5JKfyhqL.gif'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.18597.png', 'numComments': 1, 'submittedBy': {'_id': '63184c517ca1b876d99b7e0e', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/63184c517ca1b876d99b7e0e/b-qDExoeJuDXK0cJBZKnz.jpeg', 'fullname': 'Xiaodong Cun', 'name': 'vinthony', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 310}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2412.17758', 'authors': [{'_id': '676bd06524bd46fa1990dcec', 'user': {'_id': '600b381d3cc3b87db94bc0ce', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/600b381d3cc3b87db94bc0ce/I3xpr4gzcG1uXawXBpWpD.jpeg', 'isPro': False, 'fullname': 'Łukasz Borchmann', 'user': 'Borchmann', 'type': 'user'}, 'name': 'Łukasz Borchmann', 'status': 'extracted_pending', 'statusLastChangedAt': '2024-12-25T09:29:11.195Z', 'hidden': False}], 'publishedAt': '2024-12-23T18:14:36.000Z', 'title': \"In Case You Missed It: ARC 'Challenge' Is Not That Challenging\", 'summary': 'ARC Challenge appears more difficult than ARC Easy for modern LLMs primarily\\ndue to an evaluation setup that prevents direct comparison of answer choices\\nrather than inherent complexity. Although some researchers have quietly shifted\\nto a more appropriate scheme over the last year, the implications of this\\nchange have yet to be widely acknowledged. We highlight this overlooked shift,\\nshow how similar evaluation practices falsely imply reasoning deficits in other\\nbenchmarks, and demonstrate that fairer methods dramatically reduce performance\\ngaps (e.g. on SIQA) and even yield superhuman results (OpenBookQA). In doing\\nso, we reveal how evaluation shapes perceived difficulty and offer guidelines\\nto ensure that multiple-choice evaluations accurately reflect actual model\\ncapabilities.', 'upvotes': 7, 'discussionId': '676bd06724bd46fa1990dd63'}, 'publishedAt': '2024-12-25T04:33:08.560Z', 'title': \"In Case You Missed It: ARC 'Challenge' Is Not That Challenging\", 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.17758.png', 'numComments': 1, 'submittedBy': {'_id': '600b381d3cc3b87db94bc0ce', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/600b381d3cc3b87db94bc0ce/I3xpr4gzcG1uXawXBpWpD.jpeg', 'fullname': 'Łukasz Borchmann', 'name': 'Borchmann', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 3}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2412.14711', 'authors': [{'_id': '676a25362d7ae887c4f20b6d', 'name': 'Ziteng Wang', 'hidden': False}, {'_id': '676a25362d7ae887c4f20b6e', 'name': 'Jianfei Chen', 'hidden': False}, {'_id': '676a25362d7ae887c4f20b6f', 'name': 'Jun Zhu', 'hidden': False}], 'publishedAt': '2024-12-19T10:21:20.000Z', 'title': 'ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing', 'summary': \"Sparsely activated Mixture-of-Experts (MoE) models are widely adopted to\\nscale up model capacity without increasing the computation budget. However,\\nvanilla TopK routers are trained in a discontinuous, non-differentiable way,\\nlimiting their performance and scalability. To address this issue, we propose\\nReMoE, a fully differentiable MoE architecture that offers a simple yet\\neffective drop-in replacement for the conventional TopK+Softmax routing,\\nutilizing ReLU as the router instead. We further propose methods to regulate\\nthe router's sparsity while balancing the load among experts. ReMoE's\\ncontinuous nature enables efficient dynamic allocation of computation across\\ntokens and layers, while also exhibiting domain specialization. Our experiments\\ndemonstrate that ReMoE consistently outperforms vanilla TopK-routed MoE across\\nvarious model sizes, expert counts, and levels of granularity. Furthermore,\\nReMoE exhibits superior scalability with respect to the number of experts,\\nsurpassing traditional MoE architectures. The implementation based on\\nMegatron-LM is available at https://github.com/thu-ml/ReMoE.\", 'upvotes': 7, 'discussionId': '676a25372d7ae887c4f20bb3'}, 'publishedAt': '2024-12-24T22:42:12.778Z', 'title': 'ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.14711.png', 'numComments': 1, 'submittedBy': {'_id': '66c0a08bac74db25de8427ec', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg', 'fullname': 'Jintao Zhang', 'name': 'jt-zhang', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 2}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2412.18608', 'authors': [{'_id': '676baa15295f85d93eb48f24', 'name': 'Minghao Chen', 'hidden': False}, {'_id': '676baa15295f85d93eb48f25', 'name': 'Roman Shapovalov', 'hidden': False}, {'_id': '676baa15295f85d93eb48f26', 'name': 'Iro Laina', 'hidden': False}, {'_id': '676baa15295f85d93eb48f27', 'name': 'Tom Monnier', 'hidden': False}, {'_id': '676baa15295f85d93eb48f28', 'name': 'Jianyuan Wang', 'hidden': False}, {'_id': '676baa15295f85d93eb48f29', 'name': 'David Novotny', 'hidden': False}, {'_id': '676baa15295f85d93eb48f2a', 'name': 'Andrea Vedaldi', 'hidden': False}], 'publishedAt': '2024-12-24T18:59:43.000Z', 'title': 'PartGen: Part-level 3D Generation and Reconstruction with Multi-View\\n  Diffusion Models', 'summary': 'Text- or image-to-3D generators and 3D scanners can now produce 3D assets\\nwith high-quality shapes and textures. These assets typically consist of a\\nsingle, fused representation, like an implicit neural field, a Gaussian\\nmixture, or a mesh, without any useful structure. However, most applications\\nand creative workflows require assets to be made of several meaningful parts\\nthat can be manipulated independently. To address this gap, we introduce\\nPartGen, a novel approach that generates 3D objects composed of meaningful\\nparts starting from text, an image, or an unstructured 3D object. First, given\\nmultiple views of a 3D object, generated or rendered, a multi-view diffusion\\nmodel extracts a set of plausible and view-consistent part segmentations,\\ndividing the object into parts. Then, a second multi-view diffusion model takes\\neach part separately, fills in the occlusions, and uses those completed views\\nfor 3D reconstruction by feeding them to a 3D reconstruction network. This\\ncompletion process considers the context of the entire object to ensure that\\nthe parts integrate cohesively. The generative completion model can make up for\\nthe information missing due to occlusions; in extreme cases, it can hallucinate\\nentirely invisible parts based on the input 3D asset. We evaluate our method on\\ngenerated and real 3D assets and show that it outperforms segmentation and\\npart-extraction baselines by a large margin. We also showcase downstream\\napplications such as 3D part editing.', 'upvotes': 5, 'discussionId': '676baa1f295f85d93eb4928a'}, 'publishedAt': '2024-12-25T01:53:21.705Z', 'title': 'PartGen: Part-level 3D Generation and Reconstruction with Multi-View Diffusion Models', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.18608.png', 'numComments': 1, 'submittedBy': {'_id': '636a3d8bf8d9af4aea18553f', 'avatarUrl': '/avatars/028a86d088764fd66c36a2ddebf09f9a.svg', 'fullname': 'MINGHAO CHEN', 'name': 'silentchen', 'type': 'user', 'isPro': True, 'isHf': False, 'isMod': False, 'followerCount': 2}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2412.15443', 'authors': [{'_id': '676b765f038795095f73b556', 'name': 'Aakash Mahalingam', 'hidden': False}, {'_id': '676b765f038795095f73b557', 'name': 'Vinesh Kumar Gande', 'hidden': False}, {'_id': '676b765f038795095f73b558', 'name': 'Aman Chadha', 'hidden': False}, {'_id': '676b765f038795095f73b559', 'name': 'Vinija Jain', 'hidden': False}, {'_id': '676b765f038795095f73b55a', 'name': 'Divya Chaudhary', 'hidden': False}], 'publishedAt': '2024-12-19T22:51:56.000Z', 'title': 'SKETCH: Structured Knowledge Enhanced Text Comprehension for Holistic\\n  Retrieval', 'summary': \"Retrieval-Augmented Generation (RAG) systems have become pivotal in\\nleveraging vast corpora to generate informed and contextually relevant\\nresponses, notably reducing hallucinations in Large Language Models. Despite\\nsignificant advancements, these systems struggle to efficiently process and\\nretrieve information from large datasets while maintaining a comprehensive\\nunderstanding of the context. This paper introduces SKETCH, a novel methodology\\nthat enhances the RAG retrieval process by integrating semantic text retrieval\\nwith knowledge graphs, thereby merging structured and unstructured data for a\\nmore holistic comprehension. SKETCH, demonstrates substantial improvements in\\nretrieval performance and maintains superior context integrity compared to\\ntraditional methods. Evaluated across four diverse datasets: QuALITY, QASPER,\\nNarrativeQA, and Italian Cuisine-SKETCH consistently outperforms baseline\\napproaches on key RAGAS metrics such as answer_relevancy, faithfulness,\\ncontext_precision and context_recall. Notably, on the Italian Cuisine dataset,\\nSKETCH achieved an answer relevancy of 0.94 and a context precision of 0.99,\\nrepresenting the highest performance across all evaluated metrics. These\\nresults highlight SKETCH's capability in delivering more accurate and\\ncontextually relevant responses, setting new benchmarks for future retrieval\\nsystems.\", 'upvotes': 4, 'discussionId': '676b7660038795095f73b583'}, 'publishedAt': '2024-12-24T22:25:12.161Z', 'title': 'SKETCH: Structured Knowledge Enhanced Text Comprehension for Holistic Retrieval', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.15443.png', 'numComments': 1, 'submittedBy': {'_id': '63a4754927f1f64ed7238dac', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg', 'fullname': 'Aman Chadha', 'name': 'amanchadha', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2412.15797', 'authors': [{'_id': '676c0c92dd95830fd9d60010', 'name': 'Sungjin Park', 'hidden': False}, {'_id': '676c0c92dd95830fd9d60011', 'name': 'Xiao Liu', 'hidden': False}, {'_id': '676c0c92dd95830fd9d60012', 'name': 'Yeyun Gong', 'hidden': False}, {'_id': '676c0c92dd95830fd9d60013', 'name': 'Edward Choi', 'hidden': False}], 'publishedAt': '2024-12-20T11:14:29.000Z', 'title': 'Ensembling Large Language Models with Process Reward-Guided Tree Search\\n  for Better Complex Reasoning', 'summary': 'Despite recent advances in large language models, open-source models often\\nstruggle to consistently perform well on complex reasoning tasks. Existing\\nensemble methods, whether applied at the token or output levels, fail to\\naddress these challenges. In response, we present Language model Ensemble with\\nMonte Carlo Tree Search (LE-MCTS), a novel framework for process-level\\nensembling of language models. LE-MCTS formulates step-by-step reasoning with\\nan ensemble of language models as a Markov decision process. In this framework,\\nstates represent intermediate reasoning paths, while actions consist of\\ngenerating the next reasoning step using one of the language models selected\\nfrom a predefined pool. Guided by a process-based reward model, LE-MCTS\\nperforms a tree search over the reasoning steps generated by different language\\nmodels, identifying the most accurate reasoning chain. Experimental results on\\nfive mathematical reasoning benchmarks demonstrate that our approach\\noutperforms both single language model decoding algorithms and language model\\nensemble methods. Notably, LE-MCTS improves performance by 3.6% and 4.3% on the\\nMATH and MQA datasets, respectively, highlighting its effectiveness in solving\\ncomplex reasoning problems.', 'upvotes': 1, 'discussionId': '676c0c98dd95830fd9d60172'}, 'publishedAt': '2024-12-25T08:46:30.796Z', 'title': 'Ensembling Large Language Models with Process Reward-Guided Tree Search for Better Complex Reasoning', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.15797.png', 'numComments': 1, 'submittedBy': {'_id': '63fb6e281b4b1bd4e7ffc5be', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1677422062937-noauth.jpeg', 'fullname': 'Xiao Liu', 'name': 'lx865712528', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 6}, 'isAuthorParticipating': False}"
]