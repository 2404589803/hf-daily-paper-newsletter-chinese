[
  {
    "paper": {
      "id": "2508.13491",
      "authors": [
        {
          "_id": "68a685519e4b49496aac6938",
          "name": "Ziyan Kuang",
          "hidden": false
        },
        {
          "_id": "68a685519e4b49496aac6939",
          "name": "Feiyu Zhu",
          "hidden": false
        },
        {
          "_id": "68a685519e4b49496aac693a",
          "name": "Maowei Jiang",
          "hidden": false
        },
        {
          "_id": "68a685519e4b49496aac693b",
          "name": "Yanzhao Lai",
          "hidden": false
        },
        {
          "_id": "68a685519e4b49496aac693c",
          "name": "Zelin Wang",
          "hidden": false
        },
        {
          "_id": "68a685519e4b49496aac693d",
          "name": "Zhitong Wang",
          "hidden": false
        },
        {
          "_id": "68a685519e4b49496aac693e",
          "name": "Meikang Qiu",
          "hidden": false
        },
        {
          "_id": "68a685519e4b49496aac693f",
          "name": "Jiajia Huang",
          "hidden": false
        },
        {
          "_id": "68a685519e4b49496aac6940",
          "name": "Min Peng",
          "hidden": false
        },
        {
          "_id": "68a685519e4b49496aac6941",
          "name": "Qianqian Xie",
          "hidden": false
        },
        {
          "_id": "68a685519e4b49496aac6942",
          "name": "Sophia Ananiadou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-19T03:52:15.000Z",
      "submittedOnDailyAt": "2025-08-21T02:04:42.491Z",
      "title": "From Scores to Skills: A Cognitive Diagnosis Framework for Evaluating\n  Financial Large Language Models",
      "submittedOnDailyBy": {
        "_id": "663adb42e14047f710dc1d29",
        "avatarUrl": "/avatars/7ca49d67a4a8b4cf0ee896e07646715f.svg",
        "isPro": false,
        "fullname": "Mengxi Xiao",
        "user": "ElsaShaw",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) have shown promise for financial applications,\nyet their suitability for this high-stakes domain remains largely unproven due\nto inadequacies in existing benchmarks. Existing benchmarks solely rely on\nscore-level evaluation, summarizing performance with a single score that\nobscures the nuanced understanding of what models truly know and their precise\nlimitations. They also rely on datasets that cover only a narrow subset of\nfinancial concepts, while overlooking other essentials for real-world\napplications. To address these gaps, we introduce FinCDM, the first cognitive\ndiagnosis evaluation framework tailored for financial LLMs, enabling the\nevaluation of LLMs at the knowledge-skill level, identifying what financial\nskills and knowledge they have or lack based on their response patterns across\nskill-tagged tasks, rather than a single aggregated number. We construct\nCPA-QKA, the first cognitively informed financial evaluation dataset derived\nfrom the Certified Public Accountant (CPA) examination, with comprehensive\ncoverage of real-world accounting and financial skills. It is rigorously\nannotated by domain experts, who author, validate, and annotate questions with\nhigh inter-annotator agreement and fine-grained knowledge labels. Our extensive\nexperiments on 30 proprietary, open-source, and domain-specific LLMs show that\nFinCDM reveals hidden knowledge gaps, identifies under-tested areas such as tax\nand regulatory reasoning overlooked by traditional benchmarks, and uncovers\nbehavioral clusters among models. FinCDM introduces a new paradigm for\nfinancial LLM evaluation by enabling interpretable, skill-aware diagnosis that\nsupports more trustworthy and targeted model development, and all datasets and\nevaluation scripts will be publicly released to support further research.",
      "upvotes": 33,
      "discussionId": "68a685529e4b49496aac6943",
      "ai_summary": "FinCDM, a cognitive diagnosis framework, evaluates financial LLMs at the knowledge-skill level using a comprehensive dataset, revealing hidden knowledge gaps and supporting more trustworthy model development.",
      "ai_keywords": [
        "Large Language Models",
        "FinCDM",
        "cognitive diagnosis evaluation framework",
        "CPA-QKA",
        "Certified Public Accountant",
        "accounting and financial skills",
        "knowledge-skill level",
        "knowledge gaps",
        "tax and regulatory reasoning",
        "behavioral clusters",
        "skill-aware diagnosis"
      ]
    },
    "publishedAt": "2025-08-18T23:52:15.000Z",
    "title": "From Scores to Skills: A Cognitive Diagnosis Framework for Evaluating\n  Financial Large Language Models",
    "summary": "Large Language Models (LLMs) have shown promise for financial applications,\nyet their suitability for this high-stakes domain remains largely unproven due\nto inadequacies in existing benchmarks. Existing benchmarks solely rely on\nscore-level evaluation, summarizing performance with a single score that\nobscures the nuanced understanding of what models truly know and their precise\nlimitations. They also rely on datasets that cover only a narrow subset of\nfinancial concepts, while overlooking other essentials for real-world\napplications. To address these gaps, we introduce FinCDM, the first cognitive\ndiagnosis evaluation framework tailored for financial LLMs, enabling the\nevaluation of LLMs at the knowledge-skill level, identifying what financial\nskills and knowledge they have or lack based on their response patterns across\nskill-tagged tasks, rather than a single aggregated number. We construct\nCPA-QKA, the first cognitively informed financial evaluation dataset derived\nfrom the Certified Public Accountant (CPA) examination, with comprehensive\ncoverage of real-world accounting and financial skills. It is rigorously\nannotated by domain experts, who author, validate, and annotate questions with\nhigh inter-annotator agreement and fine-grained knowledge labels. Our extensive\nexperiments on 30 proprietary, open-source, and domain-specific LLMs show that\nFinCDM reveals hidden knowledge gaps, identifies under-tested areas such as tax\nand regulatory reasoning overlooked by traditional benchmarks, and uncovers\nbehavioral clusters among models. FinCDM introduces a new paradigm for\nfinancial LLM evaluation by enabling interpretable, skill-aware diagnosis that\nsupports more trustworthy and targeted model development, and all datasets and\nevaluation scripts will be publicly released to support further research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.13491.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "663adb42e14047f710dc1d29",
      "avatarUrl": "/avatars/7ca49d67a4a8b4cf0ee896e07646715f.svg",
      "fullname": "Mengxi Xiao",
      "name": "ElsaShaw",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.11987",
      "authors": [
        {
          "_id": "68a43789b65388761d0745dc",
          "name": "Zhiyuan Zeng",
          "hidden": false
        },
        {
          "_id": "68a43789b65388761d0745dd",
          "name": "Jiashuo Liu",
          "hidden": false
        },
        {
          "_id": "68a43789b65388761d0745de",
          "name": "Siyuan Chen",
          "hidden": false
        },
        {
          "_id": "68a43789b65388761d0745df",
          "name": "Tianci He",
          "hidden": false
        },
        {
          "_id": "68a43789b65388761d0745e0",
          "name": "Yali Liao",
          "hidden": false
        },
        {
          "_id": "68a43789b65388761d0745e1",
          "name": "Jinpeng Wang",
          "hidden": false
        },
        {
          "_id": "68a43789b65388761d0745e2",
          "name": "Zaiyuan Wang",
          "hidden": false
        },
        {
          "_id": "68a43789b65388761d0745e3",
          "name": "Yang Yang",
          "hidden": false
        },
        {
          "_id": "68a43789b65388761d0745e4",
          "name": "Lingyue Yin",
          "hidden": false
        },
        {
          "_id": "68a43789b65388761d0745e5",
          "name": "Mingren Yin",
          "hidden": false
        },
        {
          "_id": "68a43789b65388761d0745e6",
          "name": "Zhenwei Zhu",
          "hidden": false
        },
        {
          "_id": "68a43789b65388761d0745e7",
          "name": "Tianle Cai",
          "hidden": false
        },
        {
          "_id": "68a43789b65388761d0745e8",
          "name": "Zehui Chen",
          "hidden": false
        },
        {
          "_id": "68a43789b65388761d0745e9",
          "name": "Jiecao Chen",
          "hidden": false
        },
        {
          "_id": "68a43789b65388761d0745ea",
          "name": "Yantao Du",
          "hidden": false
        },
        {
          "_id": "68a43789b65388761d0745eb",
          "name": "Xiang Gao",
          "hidden": false
        },
        {
          "_id": "68a43789b65388761d0745ec",
          "name": "Jiacheng Guo",
          "hidden": false
        },
        {
          "_id": "68a43789b65388761d0745ed",
          "name": "Liang Hu",
          "hidden": false
        },
        {
          "_id": "68a43789b65388761d0745ee",
          "name": "Jianpeng Jiao",
          "hidden": false
        },
        {
          "_id": "68a43789b65388761d0745ef",
          "name": "Xiangsheng Li",
          "hidden": false
        },
        {
          "_id": "68a43789b65388761d0745f0",
          "name": "Jingkai Liu",
          "hidden": false
        },
        {
          "_id": "68a43789b65388761d0745f1",
          "name": "Shuang Ni",
          "hidden": false
        },
        {
          "_id": "68a43789b65388761d0745f2",
          "name": "Zhoufutu Wen",
          "hidden": false
        },
        {
          "_id": "68a43789b65388761d0745f3",
          "user": {
            "_id": "638efcf4c67af472d316d424",
            "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
            "isPro": false,
            "fullname": "Ge Zhang",
            "user": "zhangysk",
            "type": "user"
          },
          "name": "Ge Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-20T08:52:32.340Z",
          "hidden": false
        },
        {
          "_id": "68a43789b65388761d0745f4",
          "name": "Kaiyuan Zhang",
          "hidden": false
        },
        {
          "_id": "68a43789b65388761d0745f5",
          "name": "Xin Zhou",
          "hidden": false
        },
        {
          "_id": "68a43789b65388761d0745f6",
          "name": "Jose Blanchet",
          "hidden": false
        },
        {
          "_id": "68a43789b65388761d0745f7",
          "name": "Xipeng Qiu",
          "hidden": false
        },
        {
          "_id": "68a43789b65388761d0745f8",
          "name": "Mengdi Wang",
          "hidden": false
        },
        {
          "_id": "68a43789b65388761d0745f9",
          "name": "Wenhao Huang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6842e440d029f9bcf58077b4/zDuiKey8NEonyLxc5XC_d.png"
      ],
      "publishedAt": "2025-08-16T08:54:08.000Z",
      "submittedOnDailyAt": "2025-08-21T01:39:14.470Z",
      "title": "FutureX: An Advanced Live Benchmark for LLM Agents in Future Prediction",
      "submittedOnDailyBy": {
        "_id": "6842e440d029f9bcf58077b4",
        "avatarUrl": "/avatars/bbdc6c603c38849d762055ae10d41d03.svg",
        "isPro": false,
        "fullname": "jiashuo liu",
        "user": "liujiashuo77",
        "type": "user"
      },
      "summary": "Future prediction is a complex task for LLM agents, requiring a high level of\nanalytical thinking, information gathering, contextual understanding, and\ndecision-making under uncertainty. Agents must not only gather and interpret\nvast amounts of dynamic information but also integrate diverse data sources,\nweigh uncertainties, and adapt predictions based on emerging trends, just as\nhuman experts do in fields like politics, economics, and finance. Despite its\nimportance, no large-scale benchmark exists for evaluating agents on future\nprediction, largely due to challenges in handling real-time updates and\nretrieving timely, accurate answers. To address this, we introduce\nFutureX, a dynamic and live evaluation benchmark specifically\ndesigned for LLM agents performing future prediction tasks. FutureX is the\nlargest and most diverse live benchmark for future prediction, supporting\nreal-time daily updates and eliminating data contamination through an automated\npipeline for question gathering and answer collection. We evaluate 25 LLM/agent\nmodels, including those with reasoning, search capabilities, and integration of\nexternal tools such as the open-source Deep Research Agent and closed-source\nDeep Research models. This comprehensive evaluation assesses agents' adaptive\nreasoning and performance in dynamic environments. Additionally, we provide\nin-depth analyses of agents' failure modes and performance pitfalls in\nfuture-oriented tasks, including the vulnerability to fake web pages and the\ntemporal validity. Our goal is to establish a dynamic, contamination-free\nevaluation standard that drives the development of LLM agents capable of\nperforming at the level of professional human analysts in complex reasoning and\npredictive thinking.",
      "upvotes": 29,
      "discussionId": "68a4378ab65388761d0745fa",
      "projectPage": "https://futurex-ai.github.io/",
      "ai_summary": "FutureX is a dynamic, live benchmark for evaluating LLM agents in future prediction tasks, addressing challenges in real-time updates and data contamination.",
      "ai_keywords": [
        "LLM agents",
        "future prediction",
        "adaptive reasoning",
        "real-time updates",
        "data contamination",
        "automated pipeline",
        "question gathering",
        "answer collection",
        "failure modes",
        "performance pitfalls",
        "fake web pages",
        "temporal validity"
      ]
    },
    "publishedAt": "2025-08-16T04:54:08.000Z",
    "title": "FutureX: An Advanced Live Benchmark for LLM Agents in Future Prediction",
    "summary": "Future prediction is a complex task for LLM agents, requiring a high level of\nanalytical thinking, information gathering, contextual understanding, and\ndecision-making under uncertainty. Agents must not only gather and interpret\nvast amounts of dynamic information but also integrate diverse data sources,\nweigh uncertainties, and adapt predictions based on emerging trends, just as\nhuman experts do in fields like politics, economics, and finance. Despite its\nimportance, no large-scale benchmark exists for evaluating agents on future\nprediction, largely due to challenges in handling real-time updates and\nretrieving timely, accurate answers. To address this, we introduce\nFutureX, a dynamic and live evaluation benchmark specifically\ndesigned for LLM agents performing future prediction tasks. FutureX is the\nlargest and most diverse live benchmark for future prediction, supporting\nreal-time daily updates and eliminating data contamination through an automated\npipeline for question gathering and answer collection. We evaluate 25 LLM/agent\nmodels, including those with reasoning, search capabilities, and integration of\nexternal tools such as the open-source Deep Research Agent and closed-source\nDeep Research models. This comprehensive evaluation assesses agents' adaptive\nreasoning and performance in dynamic environments. Additionally, we provide\nin-depth analyses of agents' failure modes and performance pitfalls in\nfuture-oriented tasks, including the vulnerability to fake web pages and the\ntemporal validity. Our goal is to establish a dynamic, contamination-free\nevaluation standard that drives the development of LLM agents capable of\nperforming at the level of professional human analysts in complex reasoning and\npredictive thinking.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6842e440d029f9bcf58077b4/zDuiKey8NEonyLxc5XC_d.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.11987.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6842e440d029f9bcf58077b4",
      "avatarUrl": "/avatars/bbdc6c603c38849d762055ae10d41d03.svg",
      "fullname": "jiashuo liu",
      "name": "liujiashuo77",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2508.14811",
      "authors": [
        {
          "_id": "68a690589e4b49496aac698a",
          "name": "Canyu Zhao",
          "hidden": false
        },
        {
          "_id": "68a690589e4b49496aac698b",
          "name": "Xiaoman Li",
          "hidden": false
        },
        {
          "_id": "68a690589e4b49496aac698c",
          "name": "Tianjian Feng",
          "hidden": false
        },
        {
          "_id": "68a690589e4b49496aac698d",
          "name": "Zhiyue Zhao",
          "hidden": false
        },
        {
          "_id": "68a690589e4b49496aac698e",
          "name": "Hao Chen",
          "hidden": false
        },
        {
          "_id": "68a690589e4b49496aac698f",
          "name": "Chunhua Shen",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/646efd223dd912a539e0bd46/Rk_JTxNPyxfOwly7MC3Wc.mp4"
      ],
      "publishedAt": "2025-08-20T16:02:59.000Z",
      "submittedOnDailyAt": "2025-08-21T01:50:49.884Z",
      "title": "Tinker: Diffusion's Gift to 3D--Multi-View Consistent Editing From\n  Sparse Inputs without Per-Scene Optimization",
      "submittedOnDailyBy": {
        "_id": "646efd223dd912a539e0bd46",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/EOFAv5xvOgJOzuDgh4nSb.png",
        "isPro": false,
        "fullname": "Canyu Zhao",
        "user": "Canyu",
        "type": "user"
      },
      "summary": "We introduce Tinker, a versatile framework for high-fidelity 3D editing that\noperates in both one-shot and few-shot regimes without any per-scene\nfinetuning. Unlike prior techniques that demand extensive per-scene\noptimization to ensure multi-view consistency or to produce dozens of\nconsistent edited input views, Tinker delivers robust, multi-view consistent\nedits from as few as one or two images. This capability stems from repurposing\npretrained diffusion models, which unlocks their latent 3D awareness. To drive\nresearch in this space, we curate the first large-scale multi-view editing\ndataset and data pipeline, spanning diverse scenes and styles. Building on this\ndataset, we develop our framework capable of generating multi-view consistent\nedited views without per-scene training, which consists of two novel\ncomponents: (1) Referring multi-view editor: Enables precise, reference-driven\nedits that remain coherent across all viewpoints. (2) Any-view-to-video\nsynthesizer: Leverages spatial-temporal priors from video diffusion to perform\nhigh-quality scene completion and novel-view generation even from sparse\ninputs. Through extensive experiments, Tinker significantly reduces the barrier\nto generalizable 3D content creation, achieving state-of-the-art performance on\nediting, novel-view synthesis, and rendering enhancement tasks. We believe that\nTinker represents a key step towards truly scalable, zero-shot 3D editing.\nProject webpage: https://aim-uofa.github.io/Tinker",
      "upvotes": 19,
      "discussionId": "68a690599e4b49496aac6990",
      "projectPage": "https://aim-uofa.github.io/Tinker/",
      "githubRepo": "https://github.com/aim-uofa/Tinker",
      "ai_summary": "Tinker is a framework for high-fidelity 3D editing using pretrained diffusion models, enabling multi-view consistency with minimal per-scene training.",
      "ai_keywords": [
        "diffusion models",
        "latent 3D awareness",
        "multi-view editing",
        "referring multi-view editor",
        "any-view-to-video synthesizer",
        "video diffusion",
        "scene completion",
        "novel-view generation",
        "zero-shot 3D editing"
      ],
      "githubStars": 13
    },
    "publishedAt": "2025-08-20T12:02:59.000Z",
    "title": "Tinker: Diffusion's Gift to 3D--Multi-View Consistent Editing From\n  Sparse Inputs without Per-Scene Optimization",
    "summary": "We introduce Tinker, a versatile framework for high-fidelity 3D editing that\noperates in both one-shot and few-shot regimes without any per-scene\nfinetuning. Unlike prior techniques that demand extensive per-scene\noptimization to ensure multi-view consistency or to produce dozens of\nconsistent edited input views, Tinker delivers robust, multi-view consistent\nedits from as few as one or two images. This capability stems from repurposing\npretrained diffusion models, which unlocks their latent 3D awareness. To drive\nresearch in this space, we curate the first large-scale multi-view editing\ndataset and data pipeline, spanning diverse scenes and styles. Building on this\ndataset, we develop our framework capable of generating multi-view consistent\nedited views without per-scene training, which consists of two novel\ncomponents: (1) Referring multi-view editor: Enables precise, reference-driven\nedits that remain coherent across all viewpoints. (2) Any-view-to-video\nsynthesizer: Leverages spatial-temporal priors from video diffusion to perform\nhigh-quality scene completion and novel-view generation even from sparse\ninputs. Through extensive experiments, Tinker significantly reduces the barrier\nto generalizable 3D content creation, achieving state-of-the-art performance on\nediting, novel-view synthesis, and rendering enhancement tasks. We believe that\nTinker represents a key step towards truly scalable, zero-shot 3D editing.\nProject webpage: https://aim-uofa.github.io/Tinker",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/646efd223dd912a539e0bd46/Rk_JTxNPyxfOwly7MC3Wc.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.14811.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "646efd223dd912a539e0bd46",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/EOFAv5xvOgJOzuDgh4nSb.png",
      "fullname": "Canyu Zhao",
      "name": "Canyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.14879",
      "authors": [
        {
          "_id": "68a68c549e4b49496aac696e",
          "name": "Bingquan Dai",
          "hidden": false
        },
        {
          "_id": "68a68c549e4b49496aac696f",
          "name": "Li Ray Luo",
          "hidden": false
        },
        {
          "_id": "68a68c549e4b49496aac6970",
          "name": "Qihong Tang",
          "hidden": false
        },
        {
          "_id": "68a68c549e4b49496aac6971",
          "name": "Jie Wang",
          "hidden": false
        },
        {
          "_id": "68a68c549e4b49496aac6972",
          "name": "Xinyu Lian",
          "hidden": false
        },
        {
          "_id": "68a68c549e4b49496aac6973",
          "name": "Hao Xu",
          "hidden": false
        },
        {
          "_id": "68a68c549e4b49496aac6974",
          "name": "Minghan Qin",
          "hidden": false
        },
        {
          "_id": "68a68c549e4b49496aac6975",
          "name": "Xudong Xu",
          "hidden": false
        },
        {
          "_id": "68a68c549e4b49496aac6976",
          "name": "Bo Dai",
          "hidden": false
        },
        {
          "_id": "68a68c549e4b49496aac6977",
          "name": "Haoqian Wang",
          "hidden": false
        },
        {
          "_id": "68a68c549e4b49496aac6978",
          "name": "Zhaoyang Lyu",
          "hidden": false
        },
        {
          "_id": "68a68c549e4b49496aac6979",
          "name": "Jiangmiao Pang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63f2ec797ddf724fbcc75aee/Ol5wGXJbiZPqL_OGDddxv.mp4"
      ],
      "publishedAt": "2025-08-20T17:50:15.000Z",
      "submittedOnDailyAt": "2025-08-21T01:50:57.946Z",
      "title": "MeshCoder: LLM-Powered Structured Mesh Code Generation from Point Clouds",
      "submittedOnDailyBy": {
        "_id": "63f2ec797ddf724fbcc75aee",
        "avatarUrl": "/avatars/e93432ad11da703d46fe5e594d69f8c0.svg",
        "isPro": false,
        "fullname": "Zhaoyang Lyu",
        "user": "ZhaoyangLyu",
        "type": "user"
      },
      "summary": "Reconstructing 3D objects into editable programs is pivotal for applications\nlike reverse engineering and shape editing. However, existing methods often\nrely on limited domain-specific languages (DSLs) and small-scale datasets,\nrestricting their ability to model complex geometries and structures. To\naddress these challenges, we introduce MeshCoder, a novel framework that\nreconstructs complex 3D objects from point clouds into editable Blender Python\nscripts. We develop a comprehensive set of expressive Blender Python APIs\ncapable of synthesizing intricate geometries. Leveraging these APIs, we\nconstruct a large-scale paired object-code dataset, where the code for each\nobject is decomposed into distinct semantic parts. Subsequently, we train a\nmultimodal large language model (LLM) that translates 3D point cloud into\nexecutable Blender Python scripts. Our approach not only achieves superior\nperformance in shape-to-code reconstruction tasks but also facilitates\nintuitive geometric and topological editing through convenient code\nmodifications. Furthermore, our code-based representation enhances the\nreasoning capabilities of LLMs in 3D shape understanding tasks. Together, these\ncontributions establish MeshCoder as a powerful and flexible solution for\nprogrammatic 3D shape reconstruction and understanding.",
      "upvotes": 16,
      "discussionId": "68a68c549e4b49496aac697a",
      "projectPage": "https://daibingquan.github.io/MeshCoder",
      "githubRepo": "https://github.com/ZhaoyangLyu/MeshCoder",
      "ai_summary": "MeshCoder reconstructs complex 3D objects from point clouds into editable Blender Python scripts, enhancing shape-to-code reconstruction and 3D shape understanding through a multimodal large language model.",
      "ai_keywords": [
        "MeshCoder",
        "Blender Python APIs",
        "multimodal large language model",
        "point clouds",
        "shape-to-code reconstruction",
        "3D shape understanding"
      ],
      "githubStars": 19
    },
    "publishedAt": "2025-08-20T13:50:15.000Z",
    "title": "MeshCoder: LLM-Powered Structured Mesh Code Generation from Point Clouds",
    "summary": "Reconstructing 3D objects into editable programs is pivotal for applications\nlike reverse engineering and shape editing. However, existing methods often\nrely on limited domain-specific languages (DSLs) and small-scale datasets,\nrestricting their ability to model complex geometries and structures. To\naddress these challenges, we introduce MeshCoder, a novel framework that\nreconstructs complex 3D objects from point clouds into editable Blender Python\nscripts. We develop a comprehensive set of expressive Blender Python APIs\ncapable of synthesizing intricate geometries. Leveraging these APIs, we\nconstruct a large-scale paired object-code dataset, where the code for each\nobject is decomposed into distinct semantic parts. Subsequently, we train a\nmultimodal large language model (LLM) that translates 3D point cloud into\nexecutable Blender Python scripts. Our approach not only achieves superior\nperformance in shape-to-code reconstruction tasks but also facilitates\nintuitive geometric and topological editing through convenient code\nmodifications. Furthermore, our code-based representation enhances the\nreasoning capabilities of LLMs in 3D shape understanding tasks. Together, these\ncontributions establish MeshCoder as a powerful and flexible solution for\nprogrammatic 3D shape reconstruction and understanding.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63f2ec797ddf724fbcc75aee/Ol5wGXJbiZPqL_OGDddxv.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.14879.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63f2ec797ddf724fbcc75aee",
      "avatarUrl": "/avatars/e93432ad11da703d46fe5e594d69f8c0.svg",
      "fullname": "Zhaoyang Lyu",
      "name": "ZhaoyangLyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.14111",
      "authors": [
        {
          "_id": "68a6a2e59e4b49496aac6a93",
          "name": "Jiaqi Wei",
          "hidden": false
        },
        {
          "_id": "68a6a2e59e4b49496aac6a94",
          "name": "Yuejin Yang",
          "hidden": false
        },
        {
          "_id": "68a6a2e59e4b49496aac6a95",
          "name": "Xiang Zhang",
          "hidden": false
        },
        {
          "_id": "68a6a2e59e4b49496aac6a96",
          "name": "Yuhan Chen",
          "hidden": false
        },
        {
          "_id": "68a6a2e59e4b49496aac6a97",
          "name": "Xiang Zhuang",
          "hidden": false
        },
        {
          "_id": "68a6a2e59e4b49496aac6a98",
          "name": "Zhangyang Gao",
          "hidden": false
        },
        {
          "_id": "68a6a2e59e4b49496aac6a99",
          "name": "Dongzhan Zhou",
          "hidden": false
        },
        {
          "_id": "68a6a2e59e4b49496aac6a9a",
          "name": "Guangshuai Wang",
          "hidden": false
        },
        {
          "_id": "68a6a2e59e4b49496aac6a9b",
          "name": "Zhiqiang Gao",
          "hidden": false
        },
        {
          "_id": "68a6a2e59e4b49496aac6a9c",
          "name": "Juntai Cao",
          "hidden": false
        },
        {
          "_id": "68a6a2e59e4b49496aac6a9d",
          "name": "Zijie Qiu",
          "hidden": false
        },
        {
          "_id": "68a6a2e59e4b49496aac6a9e",
          "name": "Xuming He",
          "hidden": false
        },
        {
          "_id": "68a6a2e59e4b49496aac6a9f",
          "name": "Qiang Zhang",
          "hidden": false
        },
        {
          "_id": "68a6a2e59e4b49496aac6aa0",
          "name": "Chenyu You",
          "hidden": false
        },
        {
          "_id": "68a6a2e59e4b49496aac6aa1",
          "name": "Shuangjia Zheng",
          "hidden": false
        },
        {
          "_id": "68a6a2e59e4b49496aac6aa2",
          "name": "Ning Ding",
          "hidden": false
        },
        {
          "_id": "68a6a2e59e4b49496aac6aa3",
          "name": "Wanli Ouyang",
          "hidden": false
        },
        {
          "_id": "68a6a2e59e4b49496aac6aa4",
          "name": "Nanqing Dong",
          "hidden": false
        },
        {
          "_id": "68a6a2e59e4b49496aac6aa5",
          "name": "Yu Cheng",
          "hidden": false
        },
        {
          "_id": "68a6a2e59e4b49496aac6aa6",
          "name": "Siqi Sun",
          "hidden": false
        },
        {
          "_id": "68a6a2e59e4b49496aac6aa7",
          "name": "Lei Bai",
          "hidden": false
        },
        {
          "_id": "68a6a2e59e4b49496aac6aa8",
          "name": "Bowen Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-18T05:25:54.000Z",
      "submittedOnDailyAt": "2025-08-21T05:42:52.665Z",
      "title": "From AI for Science to Agentic Science: A Survey on Autonomous\n  Scientific Discovery",
      "submittedOnDailyBy": {
        "_id": "656553d89bf6665f10e3a92d",
        "avatarUrl": "/avatars/f55b09e249c48ef3fe484afa33a182ae.svg",
        "isPro": false,
        "fullname": "xiang wyatt zhang",
        "user": "Wyattz23",
        "type": "user"
      },
      "summary": "Artificial intelligence (AI) is reshaping scientific discovery, evolving from\nspecialized computational tools into autonomous research partners. We position\nAgentic Science as a pivotal stage within the broader AI for Science paradigm,\nwhere AI systems progress from partial assistance to full scientific agency.\nEnabled by large language models (LLMs), multimodal systems, and integrated\nresearch platforms, agentic AI shows capabilities in hypothesis generation,\nexperimental design, execution, analysis, and iterative refinement -- behaviors\nonce regarded as uniquely human. This survey provides a domain-oriented review\nof autonomous scientific discovery across life sciences, chemistry, materials\nscience, and physics. We unify three previously fragmented perspectives --\nprocess-oriented, autonomy-oriented, and mechanism-oriented -- through a\ncomprehensive framework that connects foundational capabilities, core\nprocesses, and domain-specific realizations. Building on this framework, we (i)\ntrace the evolution of AI for Science, (ii) identify five core capabilities\nunderpinning scientific agency, (iii) model discovery as a dynamic four-stage\nworkflow, (iv) review applications across the above domains, and (v) synthesize\nkey challenges and future opportunities. This work establishes a\ndomain-oriented synthesis of autonomous scientific discovery and positions\nAgentic Science as a structured paradigm for advancing AI-driven research.",
      "upvotes": 11,
      "discussionId": "68a6a2e59e4b49496aac6aa9",
      "ai_summary": "Agentic Science leverages large language models, multimodal systems, and integrated platforms to enable autonomous scientific discovery across various domains, encompassing hypothesis generation, experimental design, execution, analysis, and iterative refinement.",
      "ai_keywords": [
        "large language models",
        "multimodal systems",
        "integrated research platforms",
        "hypothesis generation",
        "experimental design",
        "scientific agency",
        "process-oriented",
        "autonomy-oriented",
        "mechanism-oriented",
        "domain-specific realizations",
        "dynamic four-stage workflow"
      ]
    },
    "publishedAt": "2025-08-18T01:25:54.000Z",
    "title": "From AI for Science to Agentic Science: A Survey on Autonomous\n  Scientific Discovery",
    "summary": "Artificial intelligence (AI) is reshaping scientific discovery, evolving from\nspecialized computational tools into autonomous research partners. We position\nAgentic Science as a pivotal stage within the broader AI for Science paradigm,\nwhere AI systems progress from partial assistance to full scientific agency.\nEnabled by large language models (LLMs), multimodal systems, and integrated\nresearch platforms, agentic AI shows capabilities in hypothesis generation,\nexperimental design, execution, analysis, and iterative refinement -- behaviors\nonce regarded as uniquely human. This survey provides a domain-oriented review\nof autonomous scientific discovery across life sciences, chemistry, materials\nscience, and physics. We unify three previously fragmented perspectives --\nprocess-oriented, autonomy-oriented, and mechanism-oriented -- through a\ncomprehensive framework that connects foundational capabilities, core\nprocesses, and domain-specific realizations. Building on this framework, we (i)\ntrace the evolution of AI for Science, (ii) identify five core capabilities\nunderpinning scientific agency, (iii) model discovery as a dynamic four-stage\nworkflow, (iv) review applications across the above domains, and (v) synthesize\nkey challenges and future opportunities. This work establishes a\ndomain-oriented synthesis of autonomous scientific discovery and positions\nAgentic Science as a structured paradigm for advancing AI-driven research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.14111.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "656553d89bf6665f10e3a92d",
      "avatarUrl": "/avatars/f55b09e249c48ef3fe484afa33a182ae.svg",
      "fullname": "xiang wyatt zhang",
      "name": "Wyattz23",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.14896",
      "authors": [
        {
          "_id": "68a69cf99e4b49496aac6a7f",
          "name": "Haokun Lin",
          "hidden": false
        },
        {
          "_id": "68a69cf99e4b49496aac6a80",
          "name": "Haobo Xu",
          "hidden": false
        },
        {
          "_id": "68a69cf99e4b49496aac6a81",
          "name": "Yichen Wu",
          "hidden": false
        },
        {
          "_id": "68a69cf99e4b49496aac6a82",
          "name": "Ziyu Guo",
          "hidden": false
        },
        {
          "_id": "68a69cf99e4b49496aac6a83",
          "name": "Renrui Zhang",
          "hidden": false
        },
        {
          "_id": "68a69cf99e4b49496aac6a84",
          "name": "Zhichao Lu",
          "hidden": false
        },
        {
          "_id": "68a69cf99e4b49496aac6a85",
          "name": "Ying Wei",
          "hidden": false
        },
        {
          "_id": "68a69cf99e4b49496aac6a86",
          "name": "Qingfu Zhang",
          "hidden": false
        },
        {
          "_id": "68a69cf99e4b49496aac6a87",
          "name": "Zhenan Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-20T17:59:51.000Z",
      "submittedOnDailyAt": "2025-08-21T02:44:16.778Z",
      "title": "Quantization Meets dLLMs: A Systematic Study of Post-training\n  Quantization for Diffusion LLMs",
      "submittedOnDailyBy": {
        "_id": "6841378dcb9938c7aed795be",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/WNI8gQVHdWyt0OvmRINDp.png",
        "isPro": false,
        "fullname": "Haokun Lin",
        "user": "Felix1023",
        "type": "user"
      },
      "summary": "Recent advances in diffusion large language models (dLLMs) have introduced a\npromising alternative to autoregressive (AR) LLMs for natural language\ngeneration tasks, leveraging full attention and denoising-based decoding\nstrategies. However, the deployment of these models on edge devices remains\nchallenging due to their massive parameter scale and high resource demands.\nWhile post-training quantization (PTQ) has emerged as a widely adopted\ntechnique for compressing AR LLMs, its applicability to dLLMs remains largely\nunexplored. In this work, we present the first systematic study on quantizing\ndiffusion-based language models. We begin by identifying the presence of\nactivation outliers, characterized by abnormally large activation values that\ndominate the dynamic range. These outliers pose a key challenge to low-bit\nquantization, as they make it difficult to preserve precision for the majority\nof values. More importantly, we implement state-of-the-art PTQ methods and\nconduct a comprehensive evaluation across multiple task types and model\nvariants. Our analysis is structured along four key dimensions: bit-width,\nquantization method, task category, and model type. Through this\nmulti-perspective evaluation, we offer practical insights into the quantization\nbehavior of dLLMs under different configurations. We hope our findings provide\na foundation for future research in efficient dLLM deployment. All codes and\nexperimental setups will be released to support the community.",
      "upvotes": 6,
      "discussionId": "68a69cf99e4b49496aac6a88",
      "ai_summary": "A systematic study on quantizing diffusion large language models identifies challenges and evaluates state-of-the-art methods across various configurations to improve deployment on edge devices.",
      "ai_keywords": [
        "diffusion large language models",
        "autoregressive LLMs",
        "full attention",
        "denoising-based decoding",
        "post-training quantization",
        "activation outliers",
        "low-bit quantization",
        "bit-width",
        "quantization method",
        "task category",
        "model type"
      ]
    },
    "publishedAt": "2025-08-20T13:59:51.000Z",
    "title": "Quantization Meets dLLMs: A Systematic Study of Post-training\n  Quantization for Diffusion LLMs",
    "summary": "Recent advances in diffusion large language models (dLLMs) have introduced a\npromising alternative to autoregressive (AR) LLMs for natural language\ngeneration tasks, leveraging full attention and denoising-based decoding\nstrategies. However, the deployment of these models on edge devices remains\nchallenging due to their massive parameter scale and high resource demands.\nWhile post-training quantization (PTQ) has emerged as a widely adopted\ntechnique for compressing AR LLMs, its applicability to dLLMs remains largely\nunexplored. In this work, we present the first systematic study on quantizing\ndiffusion-based language models. We begin by identifying the presence of\nactivation outliers, characterized by abnormally large activation values that\ndominate the dynamic range. These outliers pose a key challenge to low-bit\nquantization, as they make it difficult to preserve precision for the majority\nof values. More importantly, we implement state-of-the-art PTQ methods and\nconduct a comprehensive evaluation across multiple task types and model\nvariants. Our analysis is structured along four key dimensions: bit-width,\nquantization method, task category, and model type. Through this\nmulti-perspective evaluation, we offer practical insights into the quantization\nbehavior of dLLMs under different configurations. We hope our findings provide\na foundation for future research in efficient dLLM deployment. All codes and\nexperimental setups will be released to support the community.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.14896.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6841378dcb9938c7aed795be",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/WNI8gQVHdWyt0OvmRINDp.png",
      "fullname": "Haokun Lin",
      "name": "Felix1023",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.14160",
      "authors": [
        {
          "_id": "68a68f329e4b49496aac697f",
          "name": "Ronghao Dang",
          "hidden": false
        },
        {
          "_id": "68a68f329e4b49496aac6980",
          "name": "Yuqian Yuan",
          "hidden": false
        },
        {
          "_id": "68a68f329e4b49496aac6981",
          "name": "Yunxuan Mao",
          "hidden": false
        },
        {
          "_id": "68a68f329e4b49496aac6982",
          "name": "Kehan Li",
          "hidden": false
        },
        {
          "_id": "68a68f329e4b49496aac6983",
          "name": "Jiangpin Liu",
          "hidden": false
        },
        {
          "_id": "68a68f329e4b49496aac6984",
          "name": "Zhikai Wang",
          "hidden": false
        },
        {
          "_id": "68a68f329e4b49496aac6985",
          "name": "Xin Li",
          "hidden": false
        },
        {
          "_id": "68a68f329e4b49496aac6986",
          "name": "Fan Wang",
          "hidden": false
        },
        {
          "_id": "68a68f329e4b49496aac6987",
          "name": "Deli Zhao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64a3fe3dde901eb01df12398/za0ek08sv8tqTUwNe-5v2.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/64a3fe3dde901eb01df12398/lCUNSM9dYqB_AZSvP1cOz.png"
      ],
      "publishedAt": "2025-08-19T18:00:01.000Z",
      "submittedOnDailyAt": "2025-08-21T01:57:33.918Z",
      "title": "RynnEC: Bringing MLLMs into Embodied World",
      "submittedOnDailyBy": {
        "_id": "64a3fe3dde901eb01df12398",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a3fe3dde901eb01df12398/Js2bEx4rxKuEKVt5z9I2D.jpeg",
        "isPro": false,
        "fullname": "YuqianYuan",
        "user": "CircleRadon",
        "type": "user"
      },
      "summary": "We introduce RynnEC, a video multimodal large language model designed for\nembodied cognition. Built upon a general-purpose vision-language foundation\nmodel, RynnEC incorporates a region encoder and a mask decoder, enabling\nflexible region-level video interaction. Despite its compact architecture,\nRynnEC achieves state-of-the-art performance in object property understanding,\nobject segmentation, and spatial reasoning. Conceptually, it offers a\nregion-centric video paradigm for the brain of embodied agents, providing\nfine-grained perception of the physical world and enabling more precise\ninteractions. To mitigate the scarcity of annotated 3D datasets, we propose an\negocentric video based pipeline for generating embodied cognition data.\nFurthermore, we introduce RynnEC-Bench, a region-centered benchmark for\nevaluating embodied cognitive capabilities. We anticipate that RynnEC will\nadvance the development of general-purpose cognitive cores for embodied agents\nand facilitate generalization across diverse embodied tasks. The code, model\ncheckpoints, and benchmark are available at:\nhttps://github.com/alibaba-damo-academy/RynnEC",
      "upvotes": 6,
      "discussionId": "68a68f339e4b49496aac6988",
      "githubRepo": "https://github.com/alibaba-damo-academy/RynnEC",
      "ai_summary": "RynnEC, a video multimodal large language model with a region-centric approach, achieves state-of-the-art performance in object property understanding, segmentation, and spatial reasoning, using an egocentric video pipeline and a region-centered benchmark.",
      "ai_keywords": [
        "video multimodal large language model",
        "embodied cognition",
        "region encoder",
        "mask decoder",
        "region-level video interaction",
        "object property understanding",
        "object segmentation",
        "spatial reasoning",
        "egocentric video",
        "RynnEC-Bench",
        "embodied cognitive capabilities"
      ],
      "githubStars": 79
    },
    "publishedAt": "2025-08-19T14:00:01.000Z",
    "title": "RynnEC: Bringing MLLMs into Embodied World",
    "summary": "We introduce RynnEC, a video multimodal large language model designed for\nembodied cognition. Built upon a general-purpose vision-language foundation\nmodel, RynnEC incorporates a region encoder and a mask decoder, enabling\nflexible region-level video interaction. Despite its compact architecture,\nRynnEC achieves state-of-the-art performance in object property understanding,\nobject segmentation, and spatial reasoning. Conceptually, it offers a\nregion-centric video paradigm for the brain of embodied agents, providing\nfine-grained perception of the physical world and enabling more precise\ninteractions. To mitigate the scarcity of annotated 3D datasets, we propose an\negocentric video based pipeline for generating embodied cognition data.\nFurthermore, we introduce RynnEC-Bench, a region-centered benchmark for\nevaluating embodied cognitive capabilities. We anticipate that RynnEC will\nadvance the development of general-purpose cognitive cores for embodied agents\nand facilitate generalization across diverse embodied tasks. The code, model\ncheckpoints, and benchmark are available at:\nhttps://github.com/alibaba-damo-academy/RynnEC",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64a3fe3dde901eb01df12398/za0ek08sv8tqTUwNe-5v2.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/64a3fe3dde901eb01df12398/lCUNSM9dYqB_AZSvP1cOz.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.14160.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a3fe3dde901eb01df12398",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a3fe3dde901eb01df12398/Js2bEx4rxKuEKVt5z9I2D.jpeg",
      "fullname": "YuqianYuan",
      "name": "CircleRadon",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.14444",
      "authors": [
        {
          "_id": "68a69c709e4b49496aac69a9",
          "name": "NVIDIA",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69ab",
          "name": "Aarti Basant",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69ac",
          "name": "Abhijit Khairnar",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69ad",
          "name": "Abhijit Paithankar",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69ae",
          "name": "Abhinav Khattar",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69af",
          "name": "Adi Renduchintala",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69b0",
          "name": "Adithya Renduchintala",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69b1",
          "name": "Aditya Malte",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69b2",
          "name": "Akhiad Bercovich",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69b3",
          "name": "Akshay Hazare",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69b4",
          "name": "Alejandra Rico",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69b5",
          "name": "Aleksander Ficek",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69b6",
          "name": "Alex Kondratenko",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69b7",
          "name": "Alex Shaposhnikov",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69b8",
          "name": "Ali Taghibakhshi",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69b9",
          "name": "Amelia Barton",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69ba",
          "name": "Ameya Sunil Mahabaleshwarkar",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69bb",
          "name": "Amy Shen",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69bc",
          "name": "Andrew Tao",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69bd",
          "name": "Ann Guan",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69be",
          "name": "Anna Shors",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69bf",
          "name": "Anubhav Mandarwal",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69c0",
          "name": "Arham Mehta",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69c1",
          "name": "Arun Venkatesan",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69c2",
          "name": "Ashton Sharabiani",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69c3",
          "name": "Ashwath Aithal",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69c4",
          "name": "Ashwin Poojary",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69c5",
          "name": "Ayush Dattagupta",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69c6",
          "name": "Balaram Buddharaju",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69c7",
          "name": "Banghua Zhu",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69c8",
          "name": "Barnaby Simkin",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69c9",
          "name": "Bilal Kartal",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69ca",
          "name": "Bita Darvish Rouhani",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69cb",
          "name": "Bobby Chen",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69cc",
          "name": "Boris Ginsburg",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69cd",
          "name": "Brandon Norick",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69ce",
          "name": "Brian Yu",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69cf",
          "name": "Bryan Catanzaro",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69d0",
          "name": "Charles Wang",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69d1",
          "name": "Charlie Truong",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69d2",
          "name": "Chetan Mungekar",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69d3",
          "name": "Chintan Patel",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69d4",
          "name": "Chris Alexiuk",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69d5",
          "name": "Christian Munley",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69d6",
          "name": "Christopher Parisien",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69d7",
          "name": "Dan Su",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69d8",
          "name": "Daniel Afrimi",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69d9",
          "name": "Daniel Korzekwa",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69da",
          "name": "Daniel Rohrer",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69db",
          "name": "Daria Gitman",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69dc",
          "name": "David Mosallanezhad",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69dd",
          "name": "Deepak Narayanan",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69de",
          "name": "Dima Rekesh",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69df",
          "name": "Dina Yared",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69e0",
          "name": "Dmytro Pykhtar",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69e1",
          "name": "Dong Ahn",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69e2",
          "name": "Duncan Riach",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69e3",
          "name": "Eileen Long",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69e4",
          "name": "Elliott Ning",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69e5",
          "name": "Eric Chung",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69e6",
          "name": "Erick Galinkin",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69e7",
          "name": "Evelina Bakhturina",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69e8",
          "name": "Gargi Prasad",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69e9",
          "name": "Gerald Shen",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69ea",
          "name": "Haim Elisha",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69eb",
          "name": "Harsh Sharma",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69ec",
          "name": "Hayley Ross",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69ed",
          "name": "Helen Ngo",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69ee",
          "name": "Herman Sahota",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69ef",
          "name": "Hexin Wang",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69f0",
          "name": "Hoo Chang Shin",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69f1",
          "name": "Hua Huang",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69f2",
          "name": "Iain Cunningham",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69f3",
          "name": "Igor Gitman",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69f4",
          "name": "Ivan Moshkov",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69f5",
          "name": "Jaehun Jung",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69f6",
          "name": "Jan Kautz",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69f7",
          "name": "Jane Polak Scowcroft",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69f8",
          "name": "Jared Casper",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69f9",
          "name": "Jimmy Zhang",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69fa",
          "name": "Jinze Xue",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69fb",
          "name": "Jocelyn Huang",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69fc",
          "name": "Joey Conway",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69fd",
          "name": "John Kamalu",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69fe",
          "name": "Jonathan Cohen",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac69ff",
          "name": "Joseph Jennings",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a00",
          "name": "Julien Veron Vialard",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a01",
          "name": "Junkeun Yi",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a02",
          "name": "Jupinder Parmar",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a03",
          "name": "Kari Briski",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a04",
          "name": "Katherine Cheung",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a05",
          "name": "Katherine Luna",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a06",
          "name": "Keith Wyss",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a07",
          "name": "Keshav Santhanam",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a08",
          "name": "Kezhi Kong",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a09",
          "name": "Krzysztof Pawelec",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a0a",
          "name": "Kumar Anik",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a0b",
          "name": "Kunlun Li",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a0c",
          "name": "Kushan Ahmadian",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a0d",
          "name": "Lawrence McAfee",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a0e",
          "name": "Laya Sleiman",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a0f",
          "name": "Leon Derczynski",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a10",
          "name": "Luis Vega",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a11",
          "name": "Maer Rodrigues de Melo",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a12",
          "name": "Makesh Narsimhan Sreedhar",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a13",
          "name": "Marcin Chochowski",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a14",
          "name": "Mark Cai",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a15",
          "name": "Markus Kliegl",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a16",
          "name": "Marta Stepniewska-Dziubinska",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a17",
          "name": "Matvei Novikov",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a18",
          "name": "Mehrzad Samadi",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a19",
          "name": "Meredith Price",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a1a",
          "name": "Meriem Boubdir",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a1b",
          "name": "Michael Boone",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a1c",
          "name": "Michael Evans",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a1d",
          "name": "Michal Bien",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a1e",
          "name": "Michal Zawalski",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a1f",
          "name": "Miguel Martinez",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a20",
          "name": "Mike Chrzanowski",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a21",
          "name": "Mohammad Shoeybi",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a22",
          "name": "Mostofa Patwary",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a23",
          "name": "Namit Dhameja",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a24",
          "name": "Nave Assaf",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a25",
          "name": "Negar Habibi",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a26",
          "name": "Nidhi Bhatia",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a27",
          "name": "Nikki Pope",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a28",
          "name": "Nima Tajbakhsh",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a29",
          "name": "Nirmal Kumar Juluru",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a2a",
          "name": "Oleg Rybakov",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a2b",
          "name": "Oleksii Hrinchuk",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a2c",
          "name": "Oleksii Kuchaiev",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a2d",
          "name": "Oluwatobi Olabiyi",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a2e",
          "name": "Pablo Ribalta",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a2f",
          "name": "Padmavathy Subramanian",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a30",
          "name": "Parth Chadha",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a31",
          "name": "Pavlo Molchanov",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a32",
          "name": "Peter Dykas",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a33",
          "name": "Peter Jin",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a34",
          "name": "Piotr Bialecki",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a35",
          "name": "Piotr Januszewski",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a36",
          "name": "Pradeep Thalasta",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a37",
          "name": "Prashant Gaikwad",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a38",
          "name": "Prasoon Varshney",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a39",
          "name": "Pritam Gundecha",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a3a",
          "name": "Przemek Tredak",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a3b",
          "name": "Rabeeh Karimi Mahabadi",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a3c",
          "name": "Rajen Patel",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a3d",
          "name": "Ran El-Yaniv",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a3e",
          "name": "Ranjit Rajan",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a3f",
          "name": "Ria Cheruvu",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a40",
          "name": "Rima Shahbazyan",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a41",
          "name": "Ritika Borkar",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a42",
          "name": "Ritu Gala",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a43",
          "name": "Roger Waleffe",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a44",
          "name": "Ruoxi Zhang",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a45",
          "name": "Russell J. Hewett",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a46",
          "name": "Ryan Prenger",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a47",
          "name": "Sahil Jain",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a48",
          "name": "Samuel Kriman",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a49",
          "name": "Sanjeev Satheesh",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a4a",
          "name": "Saori Kaji",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a4b",
          "name": "Sarah Yurick",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a4c",
          "name": "Saurav Muralidharan",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a4d",
          "name": "Sean Narenthiran",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a4e",
          "name": "Seonmyeong Bak",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a4f",
          "name": "Sepehr Sameni",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a50",
          "name": "Seungju Han",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a51",
          "name": "Shanmugam Ramasamy",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a52",
          "name": "Shaona Ghosh",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a53",
          "name": "Sharath Turuvekere Sreenivas",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a54",
          "name": "Shelby Thomas",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a55",
          "name": "Shizhe Diao",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a56",
          "name": "Shreya Gopal",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a57",
          "name": "Shrimai Prabhumoye",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a58",
          "name": "Shubham Toshniwal",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a59",
          "name": "Shuoyang Ding",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a5a",
          "name": "Siddharth Singh",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a5b",
          "name": "Siddhartha Jain",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a5c",
          "name": "Somshubra Majumdar",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a5d",
          "name": "Stefania Alborghetti",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a5e",
          "name": "Syeda Nahida Akter",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a5f",
          "name": "Terry Kong",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a60",
          "name": "Tim Moon",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a61",
          "name": "Tomasz Hliwiak",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a62",
          "name": "Tomer Asida",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a63",
          "name": "Tony Wang",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a64",
          "name": "Twinkle Vashishth",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a65",
          "name": "Tyler Poon",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a66",
          "name": "Udi Karpas",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a67",
          "name": "Vahid Noroozi",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a68",
          "name": "Venkat Srinivasan",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a69",
          "name": "Vijay Korthikanti",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a6a",
          "name": "Vikram Fugro",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a6b",
          "name": "Vineeth Kalluru",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a6c",
          "name": "Vitaly Kurin",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a6d",
          "name": "Vitaly Lavrukhin",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a6e",
          "name": "Wasi Uddin Ahmad",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a6f",
          "name": "Wei Du",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a70",
          "name": "Wonmin Byeon",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a71",
          "name": "Ximing Lu",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a72",
          "name": "Xin Dong",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a73",
          "name": "Yashaswi Karnati",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a74",
          "name": "Yejin Choi",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a75",
          "name": "Yian Zhang",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a76",
          "name": "Ying Lin",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a77",
          "name": "Yonggan Fu",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a78",
          "name": "Yoshi Suhara",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a79",
          "name": "Zhen Dong",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a7a",
          "name": "Zhiyu Li",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a7b",
          "name": "Zhongbo Zhu",
          "hidden": false
        },
        {
          "_id": "68a69c709e4b49496aac6a7c",
          "name": "Zijia Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-20T06:00:57.000Z",
      "submittedOnDailyAt": "2025-08-21T02:41:40.962Z",
      "title": "NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid\n  Mamba-Transformer Reasoning Model",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We introduce Nemotron-Nano-9B-v2, a hybrid Mamba-Transformer language model\ndesigned to increase throughput for reasoning workloads while achieving\nstate-of-the-art accuracy compared to similarly-sized models.\nNemotron-Nano-9B-v2 builds on the Nemotron-H architecture, in which the\nmajority of the self-attention layers in the common Transformer architecture\nare replaced with Mamba-2 layers, to achieve improved inference speed when\ngenerating the long thinking traces needed for reasoning. We create\nNemotron-Nano-9B-v2 by first pre-training a 12-billion-parameter model\n(Nemotron-Nano-12B-v2-Base) on 20 trillion tokens using an FP8 training recipe.\nAfter aligning Nemotron-Nano-12B-v2-Base, we employ the Minitron strategy to\ncompress and distill the model with the goal of enabling inference on up to\n128k tokens on a single NVIDIA A10G GPU (22GiB of memory, bfloat16 precision).\nCompared to existing similarly-sized models (e.g., Qwen3-8B), we show that\nNemotron-Nano-9B-v2 achieves on-par or better accuracy on reasoning benchmarks\nwhile achieving up to 6x higher inference throughput in reasoning settings like\n8k input and 16k output tokens. We are releasing Nemotron-Nano-9B-v2,\nNemotron-Nano12B-v2-Base, and Nemotron-Nano-9B-v2-Base checkpoints along with\nthe majority of our pre- and post-training datasets on Hugging Face.",
      "upvotes": 3,
      "discussionId": "68a69c719e4b49496aac6a7d",
      "ai_summary": "Nemotron-Nano-9B-v2, a hybrid Mamba-Transformer model, enhances reasoning workload throughput and accuracy by replacing self-attention layers with Mamba-2 layers and using the Minitron strategy for compression.",
      "ai_keywords": [
        "Mamba-Transformer",
        "self-attention layers",
        "Mamba-2 layers",
        "Minitron strategy",
        "inference throughput",
        "reasoning benchmarks"
      ]
    },
    "publishedAt": "2025-08-20T02:00:57.000Z",
    "title": "NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid\n  Mamba-Transformer Reasoning Model",
    "summary": "We introduce Nemotron-Nano-9B-v2, a hybrid Mamba-Transformer language model\ndesigned to increase throughput for reasoning workloads while achieving\nstate-of-the-art accuracy compared to similarly-sized models.\nNemotron-Nano-9B-v2 builds on the Nemotron-H architecture, in which the\nmajority of the self-attention layers in the common Transformer architecture\nare replaced with Mamba-2 layers, to achieve improved inference speed when\ngenerating the long thinking traces needed for reasoning. We create\nNemotron-Nano-9B-v2 by first pre-training a 12-billion-parameter model\n(Nemotron-Nano-12B-v2-Base) on 20 trillion tokens using an FP8 training recipe.\nAfter aligning Nemotron-Nano-12B-v2-Base, we employ the Minitron strategy to\ncompress and distill the model with the goal of enabling inference on up to\n128k tokens on a single NVIDIA A10G GPU (22GiB of memory, bfloat16 precision).\nCompared to existing similarly-sized models (e.g., Qwen3-8B), we show that\nNemotron-Nano-9B-v2 achieves on-par or better accuracy on reasoning benchmarks\nwhile achieving up to 6x higher inference throughput in reasoning settings like\n8k input and 16k output tokens. We are releasing Nemotron-Nano-9B-v2,\nNemotron-Nano12B-v2-Base, and Nemotron-Nano-9B-v2-Base checkpoints along with\nthe majority of our pre- and post-training datasets on Hugging Face.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.14444.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 95
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.14460",
      "authors": [
        {
          "_id": "68a69c4a9e4b49496aac699d",
          "name": "Shuaijie She",
          "hidden": false
        },
        {
          "_id": "68a69c4a9e4b49496aac699e",
          "name": "Yu Bao",
          "hidden": false
        },
        {
          "_id": "68a69c4a9e4b49496aac699f",
          "name": "Yu Lu",
          "hidden": false
        },
        {
          "_id": "68a69c4a9e4b49496aac69a0",
          "name": "Lu Xu",
          "hidden": false
        },
        {
          "_id": "68a69c4a9e4b49496aac69a1",
          "name": "Tao Li",
          "hidden": false
        },
        {
          "_id": "68a69c4a9e4b49496aac69a2",
          "name": "Wenhao Zhu",
          "hidden": false
        },
        {
          "_id": "68a69c4a9e4b49496aac69a3",
          "name": "Shujian Huang",
          "hidden": false
        },
        {
          "_id": "68a69c4a9e4b49496aac69a4",
          "name": "Shanbo Cheng",
          "hidden": false
        },
        {
          "_id": "68a69c4a9e4b49496aac69a5",
          "name": "Lu Lu",
          "hidden": false
        },
        {
          "_id": "68a69c4a9e4b49496aac69a6",
          "name": "Yuxuan Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-20T06:31:18.000Z",
      "submittedOnDailyAt": "2025-08-21T02:41:01.130Z",
      "title": "DuPO: Enabling Reliable LLM Self-Verification via Dual Preference\n  Optimization",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We present DuPO, a dual learning-based preference optimization framework that\ngenerates annotation-free feedback via a generalized duality. DuPO addresses\ntwo key limitations: Reinforcement Learning with Verifiable Rewards (RLVR)'s\nreliance on costly labels and applicability restricted to verifiable tasks, and\ntraditional dual learning's restriction to strictly dual task pairs (e.g.,\ntranslation and back-translation). Specifically, DuPO decomposes a primal\ntask's input into known and unknown components, then constructs its dual task\nto reconstruct the unknown part using the primal output and known information\n(e.g., reversing math solutions to recover hidden variables), broadening\napplicability to non-invertible tasks. The quality of this reconstruction\nserves as a self-supervised reward to optimize the primal task, synergizing\nwith LLMs' ability to instantiate both tasks via a single model. Empirically,\nDuPO achieves substantial gains across diverse tasks: it enhances the average\ntranslation quality by 2.13 COMET over 756 directions, boosts the mathematical\nreasoning accuracy by an average of 6.4 points on three challenge benchmarks,\nand enhances performance by 9.3 points as an inference-time reranker (trading\ncomputation for accuracy). These results position DuPO as a scalable, general,\nand annotation-free paradigm for LLM optimization.",
      "upvotes": 2,
      "discussionId": "68a69c4b9e4b49496aac69a7",
      "ai_summary": "DuPO is a dual learning framework that generates annotation-free feedback using a generalized duality, enhancing performance across various tasks without relying on costly labels.",
      "ai_keywords": [
        "Reinforcement Learning with Verifiable Rewards",
        "dual learning",
        "primal task",
        "dual task",
        "self-supervised reward",
        "LLMs",
        "translation quality",
        "mathematical reasoning accuracy",
        "inference-time reranker"
      ]
    },
    "publishedAt": "2025-08-20T02:31:18.000Z",
    "title": "DuPO: Enabling Reliable LLM Self-Verification via Dual Preference\n  Optimization",
    "summary": "We present DuPO, a dual learning-based preference optimization framework that\ngenerates annotation-free feedback via a generalized duality. DuPO addresses\ntwo key limitations: Reinforcement Learning with Verifiable Rewards (RLVR)'s\nreliance on costly labels and applicability restricted to verifiable tasks, and\ntraditional dual learning's restriction to strictly dual task pairs (e.g.,\ntranslation and back-translation). Specifically, DuPO decomposes a primal\ntask's input into known and unknown components, then constructs its dual task\nto reconstruct the unknown part using the primal output and known information\n(e.g., reversing math solutions to recover hidden variables), broadening\napplicability to non-invertible tasks. The quality of this reconstruction\nserves as a self-supervised reward to optimize the primal task, synergizing\nwith LLMs' ability to instantiate both tasks via a single model. Empirically,\nDuPO achieves substantial gains across diverse tasks: it enhances the average\ntranslation quality by 2.13 COMET over 756 directions, boosts the mathematical\nreasoning accuracy by an average of 6.4 points on three challenge benchmarks,\nand enhances performance by 9.3 points as an inference-time reranker (trading\ncomputation for accuracy). These results position DuPO as a scalable, general,\nand annotation-free paradigm for LLM optimization.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.14460.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 95
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.14187",
      "authors": [
        {
          "_id": "68a69d2b9e4b49496aac6a8a",
          "name": "Md Ashiqur Rahman",
          "hidden": false
        },
        {
          "_id": "68a69d2b9e4b49496aac6a8b",
          "name": "Chiao-An Yang",
          "hidden": false
        },
        {
          "_id": "68a69d2b9e4b49496aac6a8c",
          "name": "Michael N. Cheng",
          "hidden": false
        },
        {
          "_id": "68a69d2b9e4b49496aac6a8d",
          "name": "Lim Jun Hao",
          "hidden": false
        },
        {
          "_id": "68a69d2b9e4b49496aac6a8e",
          "name": "Jeremiah Jiang",
          "hidden": false
        },
        {
          "_id": "68a69d2b9e4b49496aac6a8f",
          "name": "Teck-Yian Lim",
          "hidden": false
        },
        {
          "_id": "68a69d2b9e4b49496aac6a90",
          "name": "Raymond A. Yeh",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/661e07e02a8496916011c08a/_MhgNnRLFRElwm0SfWCZe.gif"
      ],
      "publishedAt": "2025-08-19T18:21:59.000Z",
      "submittedOnDailyAt": "2025-08-21T02:47:36.441Z",
      "title": "Local Scale Equivariance with Latent Deep Equilibrium Canonicalizer",
      "submittedOnDailyBy": {
        "_id": "661e07e02a8496916011c08a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/6vIkL4LJ0bLeJL99E20F_.jpeg",
        "isPro": false,
        "fullname": "Md Ashiqur Rahman",
        "user": "ashiq24",
        "type": "user"
      },
      "summary": "Scale variation is a fundamental challenge in computer vision. Objects of the\nsame class can have different sizes, and their perceived size is further\naffected by the distance from the camera. These variations are local to the\nobjects, i.e., different object sizes may change differently within the same\nimage. To effectively handle scale variations, we present a deep equilibrium\ncanonicalizer (DEC) to improve the local scale equivariance of a model. DEC can\nbe easily incorporated into existing network architectures and can be adapted\nto a pre-trained model. Notably, we show that on the competitive ImageNet\nbenchmark, DEC improves both model performance and local scale consistency\nacross four popular pre-trained deep-nets, e.g., ViT, DeiT, Swin, and BEiT. Our\ncode is available at https://github.com/ashiq24/local-scale-equivariance.",
      "upvotes": 1,
      "discussionId": "68a69d2b9e4b49496aac6a91",
      "projectPage": "https://ashiq24.github.io/local-scale-equivariance/",
      "githubRepo": "https://github.com/ashiq24/local-scale-equivariance",
      "ai_summary": "A deep equilibrium canonicalizer (DEC) enhances local scale equivariance in deep networks, improving performance and consistency on ImageNet.",
      "ai_keywords": [
        "deep equilibrium canonicalizer",
        "DEC",
        "local scale equivariance",
        "ImageNet",
        "ViT",
        "DeiT",
        "Swin",
        "BEiT"
      ],
      "githubStars": 0
    },
    "publishedAt": "2025-08-19T14:21:59.000Z",
    "title": "Local Scale Equivariance with Latent Deep Equilibrium Canonicalizer",
    "summary": "Scale variation is a fundamental challenge in computer vision. Objects of the\nsame class can have different sizes, and their perceived size is further\naffected by the distance from the camera. These variations are local to the\nobjects, i.e., different object sizes may change differently within the same\nimage. To effectively handle scale variations, we present a deep equilibrium\ncanonicalizer (DEC) to improve the local scale equivariance of a model. DEC can\nbe easily incorporated into existing network architectures and can be adapted\nto a pre-trained model. Notably, we show that on the competitive ImageNet\nbenchmark, DEC improves both model performance and local scale consistency\nacross four popular pre-trained deep-nets, e.g., ViT, DeiT, Swin, and BEiT. Our\ncode is available at https://github.com/ashiq24/local-scale-equivariance.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/661e07e02a8496916011c08a/_MhgNnRLFRElwm0SfWCZe.gif"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.14187.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "661e07e02a8496916011c08a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/6vIkL4LJ0bLeJL99E20F_.jpeg",
      "fullname": "Md Ashiqur Rahman",
      "name": "ashiq24",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.10137",
      "authors": [
        {
          "_id": "68a6b4809e4b49496aac6aca",
          "name": "Nghia Trung Ngo",
          "hidden": false
        },
        {
          "_id": "68a6b4809e4b49496aac6acb",
          "name": "Franck Dernoncourt",
          "hidden": false
        },
        {
          "_id": "68a6b4809e4b49496aac6acc",
          "name": "Thien Huu Nguyen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-13T18:59:02.000Z",
      "submittedOnDailyAt": "2025-08-21T04:24:27.752Z",
      "title": "mSCoRe: a Multilingual and Scalable Benchmark for Skill-based\n  Commonsense Reasoning",
      "submittedOnDailyBy": {
        "_id": "62c5947524171688a9feb992",
        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
        "isPro": false,
        "fullname": "Franck Dernoncourt",
        "user": "Franck-Dernoncourt",
        "type": "user"
      },
      "summary": "Recent advancements in reasoning-reinforced Large Language Models (LLMs) have\nshown remarkable capabilities in complex reasoning tasks. However, the\nmechanism underlying their utilization of different human reasoning skills\nremains poorly investigated, especially for multilingual commonsense reasoning\nthat involves everyday knowledge across different languages and cultures. To\naddress this gap, we propose a Multilingual and Scalable Benchmark for\nSkill-based Commonsense Reasoning (mSCoRe).\nOur benchmark incorporates three key components that are designed to\nsystematically evaluate LLM's reasoning capabilities, including: (1) a novel\ntaxonomy of reasoning skills that enables fine-grained analysis of models'\nreasoning processes, (2) a robust data synthesis pipeline tailored specifically\nfor commonsense reasoning evaluation, and (3) a complexity scaling framework\nallowing task difficulty to scale dynamically alongside future improvements in\nLLM abilities. Extensive experiments on eights state-of-the-art LLMs of varying\nsizes and training approaches demonstrate that mSCoRe remains\nsignificantly challenging for current models, particularly at higher complexity\nlevels. Our results reveal the limitations of such reasoning-reinforced models\nwhen confronted with nuanced multilingual general and cultural commonsense. We\nfurther provide detailed analysis on the models' reasoning processes,\nsuggesting future directions for improving multilingual commonsense reasoning\ncapabilities.",
      "upvotes": 1,
      "discussionId": "68a6b4889e4b49496aac6acd",
      "ai_summary": "A multilingual benchmark evaluates the reasoning skills of large language models across different languages and cultures, revealing their limitations in nuanced commonsense understanding.",
      "ai_keywords": [
        "Large Language Models",
        "multilingual commonsense reasoning",
        "reasoning skills",
        "data synthesis pipeline",
        "complexity scaling framework"
      ]
    },
    "publishedAt": "2025-08-13T14:59:02.000Z",
    "title": "mSCoRe: a Multilingual and Scalable Benchmark for Skill-based\n  Commonsense Reasoning",
    "summary": "Recent advancements in reasoning-reinforced Large Language Models (LLMs) have\nshown remarkable capabilities in complex reasoning tasks. However, the\nmechanism underlying their utilization of different human reasoning skills\nremains poorly investigated, especially for multilingual commonsense reasoning\nthat involves everyday knowledge across different languages and cultures. To\naddress this gap, we propose a Multilingual and Scalable Benchmark for\nSkill-based Commonsense Reasoning (mSCoRe).\nOur benchmark incorporates three key components that are designed to\nsystematically evaluate LLM's reasoning capabilities, including: (1) a novel\ntaxonomy of reasoning skills that enables fine-grained analysis of models'\nreasoning processes, (2) a robust data synthesis pipeline tailored specifically\nfor commonsense reasoning evaluation, and (3) a complexity scaling framework\nallowing task difficulty to scale dynamically alongside future improvements in\nLLM abilities. Extensive experiments on eights state-of-the-art LLMs of varying\nsizes and training approaches demonstrate that mSCoRe remains\nsignificantly challenging for current models, particularly at higher complexity\nlevels. Our results reveal the limitations of such reasoning-reinforced models\nwhen confronted with nuanced multilingual general and cultural commonsense. We\nfurther provide detailed analysis on the models' reasoning processes,\nsuggesting future directions for improving multilingual commonsense reasoning\ncapabilities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.10137.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c5947524171688a9feb992",
      "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
      "fullname": "Franck Dernoncourt",
      "name": "Franck-Dernoncourt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.13745",
      "authors": [
        {
          "_id": "68a5a213c4b3ea17d2d2cf47",
          "user": {
            "_id": "68a59cc87de3c48922820701",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/RXoYPK8yALphay3LFXvy-.png",
            "isPro": false,
            "fullname": "ShouxingMa",
            "user": "MrShouxingMa",
            "type": "user"
          },
          "name": "Shouxing Ma",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-20T19:13:28.696Z",
          "hidden": false
        },
        {
          "_id": "68a5a213c4b3ea17d2d2cf48",
          "name": "Yawen Zeng",
          "hidden": false
        },
        {
          "_id": "68a5a213c4b3ea17d2d2cf49",
          "name": "Shiqing Wu",
          "hidden": false
        },
        {
          "_id": "68a5a213c4b3ea17d2d2cf4a",
          "name": "Guandong Xu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-19T11:35:48.000Z",
      "submittedOnDailyAt": "2025-08-21T05:22:05.445Z",
      "title": "Refining Contrastive Learning and Homography Relations for Multi-Modal\n  Recommendation",
      "submittedOnDailyBy": {
        "_id": "68a59cc87de3c48922820701",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/RXoYPK8yALphay3LFXvy-.png",
        "isPro": false,
        "fullname": "ShouxingMa",
        "user": "MrShouxingMa",
        "type": "user"
      },
      "summary": "Multi-modal recommender system focuses on utilizing rich modal information (\ni.e., images and textual descriptions) of items to improve recommendation\nperformance. The current methods have achieved remarkable success with the\npowerful structure modeling capability of graph neural networks. However, these\nmethods are often hindered by sparse data in real-world scenarios. Although\ncontrastive learning and homography ( i.e., homogeneous graphs) are employed to\naddress the data sparsity challenge, existing methods still suffer two main\nlimitations: 1) Simple multi-modal feature contrasts fail to produce effective\nrepresentations, causing noisy modal-shared features and loss of valuable\ninformation in modal-unique features; 2) The lack of exploration of the\nhomograph relations between user interests and item co-occurrence results in\nincomplete mining of user-item interplay.\n  To address the above limitations, we propose a novel framework for\nREfining multi-modAl contRastive learning\nand hoMography relations (REARM). Specifically, we complement\nmulti-modal contrastive learning by employing meta-network and orthogonal\nconstraint strategies, which filter out noise in modal-shared features and\nretain recommendation-relevant information in modal-unique features. To mine\nhomogeneous relationships effectively, we integrate a newly constructed user\ninterest graph and an item co-occurrence graph with the existing user\nco-occurrence and item semantic graphs for graph learning. The extensive\nexperiments on three real-world datasets demonstrate the superiority of REARM\nto various state-of-the-art baselines. Our visualization further shows an\nimprovement made by REARM in distinguishing between modal-shared and\nmodal-unique features. Code is available\nhttps://github.com/MrShouxingMa/REARM{here}.",
      "upvotes": 0,
      "discussionId": "68a5a213c4b3ea17d2d2cf4b",
      "ai_summary": "A novel framework, REARM, enhances multi-modal recommender systems by refining contrastive learning and homography relations, improving feature representation and user-item interaction mining.",
      "ai_keywords": [
        "graph neural networks",
        "contrastive learning",
        "homography",
        "meta-network",
        "orthogonal constraint",
        "user interest graph",
        "item co-occurrence graph",
        "graph learning"
      ]
    },
    "publishedAt": "2025-08-19T07:35:48.000Z",
    "title": "Refining Contrastive Learning and Homography Relations for Multi-Modal\n  Recommendation",
    "summary": "Multi-modal recommender system focuses on utilizing rich modal information (\ni.e., images and textual descriptions) of items to improve recommendation\nperformance. The current methods have achieved remarkable success with the\npowerful structure modeling capability of graph neural networks. However, these\nmethods are often hindered by sparse data in real-world scenarios. Although\ncontrastive learning and homography ( i.e., homogeneous graphs) are employed to\naddress the data sparsity challenge, existing methods still suffer two main\nlimitations: 1) Simple multi-modal feature contrasts fail to produce effective\nrepresentations, causing noisy modal-shared features and loss of valuable\ninformation in modal-unique features; 2) The lack of exploration of the\nhomograph relations between user interests and item co-occurrence results in\nincomplete mining of user-item interplay.\n  To address the above limitations, we propose a novel framework for\nREfining multi-modAl contRastive learning\nand hoMography relations (REARM). Specifically, we complement\nmulti-modal contrastive learning by employing meta-network and orthogonal\nconstraint strategies, which filter out noise in modal-shared features and\nretain recommendation-relevant information in modal-unique features. To mine\nhomogeneous relationships effectively, we integrate a newly constructed user\ninterest graph and an item co-occurrence graph with the existing user\nco-occurrence and item semantic graphs for graph learning. The extensive\nexperiments on three real-world datasets demonstrate the superiority of REARM\nto various state-of-the-art baselines. Our visualization further shows an\nimprovement made by REARM in distinguishing between modal-shared and\nmodal-unique features. Code is available\nhttps://github.com/MrShouxingMa/REARM{here}.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.13745.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "68a59cc87de3c48922820701",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/RXoYPK8yALphay3LFXvy-.png",
      "fullname": "ShouxingMa",
      "name": "MrShouxingMa",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]