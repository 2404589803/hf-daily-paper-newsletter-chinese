[
  {
    "paper": {
      "id": "2507.01006",
      "authors": [
        {
          "_id": "68649318d59a9eda59024a6f",
          "name": "Wenyi Hong",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a70",
          "name": "Wenmeng Yu",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a71",
          "name": "Xiaotao Gu",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a72",
          "name": "Guo Wang",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a73",
          "name": "Guobing Gan",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a74",
          "name": "Haomiao Tang",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a75",
          "user": {
            "_id": "627626d42d26ac639e56f565",
            "avatarUrl": "/avatars/805c5f909f52656345b8bde486c9fa8f.svg",
            "isPro": false,
            "fullname": "Jiale Cheng",
            "user": "CCCCCC",
            "type": "user"
          },
          "name": "Jiale Cheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T07:39:50.429Z",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a76",
          "name": "Ji Qi",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a77",
          "name": "Junhui Ji",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a78",
          "name": "Lihang Pan",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a79",
          "name": "Shuaiqi Duan",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a7a",
          "name": "Weihan Wang",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a7b",
          "name": "Yan Wang",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a7c",
          "name": "Yean Cheng",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a7d",
          "name": "Zehai He",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a7e",
          "name": "Zhe Su",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a7f",
          "name": "Zhen Yang",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a80",
          "name": "Ziyang Pan",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a81",
          "name": "Aohan Zeng",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a82",
          "name": "Baoxu Wang",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a83",
          "name": "Boyan Shi",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a84",
          "name": "Changyu Pang",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a85",
          "name": "Chenhui Zhang",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a86",
          "name": "Da Yin",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a87",
          "name": "Fan Yang",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a88",
          "name": "Guoqing Chen",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a89",
          "name": "Jiazheng Xu",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a8a",
          "name": "Jiali Chen",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a8b",
          "name": "Jing Chen",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a8c",
          "name": "Jinhao Chen",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a8d",
          "name": "Jinghao Lin",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a8e",
          "name": "Jinjiang Wang",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a8f",
          "name": "Junjie Chen",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a90",
          "name": "Leqi Lei",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a91",
          "name": "Leyi Pan",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a92",
          "name": "Mingzhi Zhang",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a93",
          "name": "Qinkai Zheng",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a94",
          "name": "Sheng Yang",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a95",
          "name": "Shi Zhong",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a96",
          "name": "Shiyu Huang",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a97",
          "name": "Shuyuan Zhao",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a98",
          "name": "Siyan Xue",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a99",
          "user": {
            "_id": "648c48d8c0ddeee6df5b6d22",
            "avatarUrl": "/avatars/8706b0b16dfc332b96c91d3ced31bd0b.svg",
            "isPro": false,
            "fullname": "Shangqing Tu",
            "user": "tsq2000",
            "type": "user"
          },
          "name": "Shangqin Tu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T07:39:08.904Z",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a9a",
          "name": "Shengbiao Meng",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a9b",
          "name": "Tianshu Zhang",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a9c",
          "name": "Tianwei Luo",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a9d",
          "name": "Tianxiang Hao",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a9e",
          "name": "Tianle Gong",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024a9f",
          "name": "Wenkai Li",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024aa0",
          "name": "Wei Jia",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024aa1",
          "name": "Xin Lyu",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024aa2",
          "name": "Xuancheng Huang",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024aa3",
          "name": "Yanling Wang",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024aa4",
          "name": "Yadong Xue",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024aa5",
          "name": "Yanfeng Wang",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024aa6",
          "name": "Yifan An",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024aa7",
          "name": "Yifan Du",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024aa8",
          "user": {
            "_id": "642ebcefd09a9c63c2124bd2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642ebcefd09a9c63c2124bd2/HFSkoaxIM6nTq1CrY6jyF.jpeg",
            "isPro": false,
            "fullname": "Yiming Shi",
            "user": "Shiym",
            "type": "user"
          },
          "name": "Yiming Shi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T07:39:52.595Z",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024aa9",
          "name": "Yiheng Huang",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024aaa",
          "name": "Yilin Niu",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024aab",
          "name": "Yuan Wang",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024aac",
          "name": "Yuanchang Yue",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024aad",
          "name": "Yuchen Li",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024aae",
          "name": "Yutao Zhang",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024aaf",
          "name": "Yuxuan Zhang",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024ab0",
          "name": "Zhanxiao Du",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024ab1",
          "name": "Zhenyu Hou",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024ab2",
          "name": "Zhao Xue",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024ab3",
          "name": "Zhengxiao Du",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024ab4",
          "name": "Zihan Wang",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024ab5",
          "name": "Peng Zhang",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024ab6",
          "name": "Debing Liu",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024ab7",
          "name": "Bin Xu",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024ab8",
          "name": "Juanzi Li",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024ab9",
          "name": "Minlie Huang",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024aba",
          "name": "Yuxiao Dong",
          "hidden": false
        },
        {
          "_id": "68649318d59a9eda59024abb",
          "name": "Jie Tang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62ecd24cb8764c7738ef2793/zDkNTP70lk6UrrLAk_Sdt.jpeg"
      ],
      "publishedAt": "2025-07-01T17:55:04.000Z",
      "submittedOnDailyAt": "2025-07-02T03:07:41.410Z",
      "title": "GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable\n  Reinforcement Learning",
      "submittedOnDailyBy": {
        "_id": "62ecd24cb8764c7738ef2793",
        "avatarUrl": "/avatars/c1b80b5c55f9d652c1aaac7919e1fa32.svg",
        "isPro": false,
        "fullname": "Wenyi Hong",
        "user": "wenyi",
        "type": "user"
      },
      "summary": "We present GLM-4.1V-Thinking, a vision-language model (VLM) designed to\nadvance general-purpose multimodal reasoning. In this report, we share our key\nfindings in the development of the reasoning-centric training framework. We\nfirst develop a capable vision foundation model with significant potential\nthrough large-scale pre-training, which arguably sets the upper bound for the\nfinal performance. Reinforcement Learning with Curriculum Sampling (RLCS) then\nunlocks the full potential of the model, leading to comprehensive capability\nenhancement across a diverse range of tasks, including STEM problem solving,\nvideo understanding, content recognition, coding, grounding, GUI-based agents,\nand long document understanding, among others. To facilitate research in this\nfield, we open-source GLM-4.1V-9B-Thinking, which achieves state-of-the-art\nperformance among models of comparable size. In a comprehensive evaluation\nacross 28 public benchmarks, our model outperforms Qwen2.5-VL-7B on nearly all\ntasks and achieves comparable or even superior performance on 18 benchmarks\nrelative to the significantly larger Qwen2.5-VL-72B. Notably,\nGLM-4.1V-9B-Thinking also demonstrates competitive or superior performance\ncompared to closed-source models such as GPT-4o on challenging tasks including\nlong document understanding and STEM reasoning, further underscoring its strong\ncapabilities. Code, models and more information are released at\nhttps://github.com/THUDM/GLM-4.1V-Thinking.",
      "upvotes": 72,
      "discussionId": "68649319d59a9eda59024abc",
      "ai_summary": "A vision-language model, GLM-4.1V-Thinking, enhances general-purpose multimodal reasoning through large-scale pre-training and reinforcement learning, achieving state-of-the-art performance across various tasks.",
      "ai_keywords": [
        "vision-language model",
        "reasoning-centric training framework",
        "reinforcement learning",
        "curriculum sampling",
        "vision foundation model",
        "STEM problem solving",
        "video understanding",
        "content recognition",
        "coding",
        "grounding",
        "GUI-based agents",
        "long document understanding"
      ]
    },
    "publishedAt": "2025-07-01T13:55:04.000Z",
    "title": "GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable\n  Reinforcement Learning",
    "summary": "We present GLM-4.1V-Thinking, a vision-language model (VLM) designed to\nadvance general-purpose multimodal reasoning. In this report, we share our key\nfindings in the development of the reasoning-centric training framework. We\nfirst develop a capable vision foundation model with significant potential\nthrough large-scale pre-training, which arguably sets the upper bound for the\nfinal performance. Reinforcement Learning with Curriculum Sampling (RLCS) then\nunlocks the full potential of the model, leading to comprehensive capability\nenhancement across a diverse range of tasks, including STEM problem solving,\nvideo understanding, content recognition, coding, grounding, GUI-based agents,\nand long document understanding, among others. To facilitate research in this\nfield, we open-source GLM-4.1V-9B-Thinking, which achieves state-of-the-art\nperformance among models of comparable size. In a comprehensive evaluation\nacross 28 public benchmarks, our model outperforms Qwen2.5-VL-7B on nearly all\ntasks and achieves comparable or even superior performance on 18 benchmarks\nrelative to the significantly larger Qwen2.5-VL-72B. Notably,\nGLM-4.1V-9B-Thinking also demonstrates competitive or superior performance\ncompared to closed-source models such as GPT-4o on challenging tasks including\nlong document understanding and STEM reasoning, further underscoring its strong\ncapabilities. Code, models and more information are released at\nhttps://github.com/THUDM/GLM-4.1V-Thinking.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62ecd24cb8764c7738ef2793/zDkNTP70lk6UrrLAk_Sdt.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.01006.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62ecd24cb8764c7738ef2793",
      "avatarUrl": "/avatars/c1b80b5c55f9d652c1aaac7919e1fa32.svg",
      "fullname": "Wenyi Hong",
      "name": "wenyi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.23115",
      "authors": [
        {
          "_id": "68634959588cea0da970c8a5",
          "user": {
            "_id": "66add675c7a575aa0e03d5f3",
            "avatarUrl": "/avatars/b72b18130664c1de197c1f8df371aa70.svg",
            "isPro": true,
            "fullname": "Haonan Chen",
            "user": "Haon-Chen",
            "type": "user"
          },
          "name": "Haonan Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-01T10:13:35.715Z",
          "hidden": false
        },
        {
          "_id": "68634959588cea0da970c8a6",
          "user": {
            "_id": "648ba0d68e7f7a927675d4a3",
            "avatarUrl": "/avatars/d82ced93656e03d60c8b55010694f908.svg",
            "isPro": false,
            "fullname": "Hong Liu",
            "user": "hongliu9903",
            "type": "user"
          },
          "name": "Hong Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-01T10:13:40.207Z",
          "hidden": false
        },
        {
          "_id": "68634959588cea0da970c8a7",
          "user": {
            "_id": "64ed68ce60f6345da7014b38",
            "avatarUrl": "/avatars/adfc156482ef5570dc69329aa53975e6.svg",
            "isPro": false,
            "fullname": "Yuping Luo",
            "user": "roosephu",
            "type": "user"
          },
          "name": "Yuping Luo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-01T10:13:37.881Z",
          "hidden": false
        },
        {
          "_id": "68634959588cea0da970c8a8",
          "name": "Liang Wang",
          "hidden": false
        },
        {
          "_id": "68634959588cea0da970c8a9",
          "name": "Nan Yang",
          "hidden": false
        },
        {
          "_id": "68634959588cea0da970c8aa",
          "name": "Furu Wei",
          "hidden": false
        },
        {
          "_id": "68634959588cea0da970c8ab",
          "name": "Zhicheng Dou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-29T06:41:00.000Z",
      "submittedOnDailyAt": "2025-07-02T00:57:35.541Z",
      "title": "MoCa: Modality-aware Continual Pre-training Makes Better Bidirectional\n  Multimodal Embeddings",
      "submittedOnDailyBy": {
        "_id": "66add675c7a575aa0e03d5f3",
        "avatarUrl": "/avatars/b72b18130664c1de197c1f8df371aa70.svg",
        "isPro": true,
        "fullname": "Haonan Chen",
        "user": "Haon-Chen",
        "type": "user"
      },
      "summary": "Multimodal embedding models, built upon causal Vision Language Models (VLMs),\nhave shown promise in various tasks. However, current approaches face three key\nlimitations: the use of causal attention in VLM backbones is suboptimal for\nembedding tasks; scalability issues due to reliance on high-quality labeled\npaired data for contrastive learning; and limited diversity in training\nobjectives and data. To address these issues, we propose MoCa, a two-stage\nframework for transforming pre-trained VLMs into effective bidirectional\nmultimodal embedding models. The first stage, Modality-aware Continual\nPre-training, introduces a joint reconstruction objective that simultaneously\ndenoises interleaved text and image inputs, enhancing bidirectional\ncontext-aware reasoning. The second stage, Heterogeneous Contrastive\nFine-tuning, leverages diverse, semantically rich multimodal data beyond simple\nimage-caption pairs to enhance generalization and alignment. Our method\naddresses the stated limitations by introducing bidirectional attention through\ncontinual pre-training, scaling effectively with massive unlabeled datasets via\njoint reconstruction objectives, and utilizing diverse multimodal data for\nenhanced representation robustness. Experiments demonstrate that MoCa\nconsistently improves performance across MMEB and ViDoRe-v2 benchmarks,\nachieving new state-of-the-art results, and exhibits strong scalability with\nboth model size and training data on MMEB.",
      "upvotes": 26,
      "discussionId": "6863495a588cea0da970c8ac",
      "ai_summary": "MoCa, a two-stage framework, enhances pre-trained causal vision-language models for multimodal embedding by introducing bidirectional attention, scaling with unlabeled data, and diverse training objectives.",
      "ai_keywords": [
        "Vision Language Models",
        "VLMs",
        "bidirectional multimodal embedding models",
        "modality-aware continual pre-training",
        "joint reconstruction objective",
        "heterogeneous contrastive fine-tuning",
        "bidirectional attention",
        "massive unlabeled datasets",
        "MMEB",
        "ViDoRe-v2"
      ]
    },
    "publishedAt": "2025-06-29T02:41:00.000Z",
    "title": "MoCa: Modality-aware Continual Pre-training Makes Better Bidirectional\n  Multimodal Embeddings",
    "summary": "Multimodal embedding models, built upon causal Vision Language Models (VLMs),\nhave shown promise in various tasks. However, current approaches face three key\nlimitations: the use of causal attention in VLM backbones is suboptimal for\nembedding tasks; scalability issues due to reliance on high-quality labeled\npaired data for contrastive learning; and limited diversity in training\nobjectives and data. To address these issues, we propose MoCa, a two-stage\nframework for transforming pre-trained VLMs into effective bidirectional\nmultimodal embedding models. The first stage, Modality-aware Continual\nPre-training, introduces a joint reconstruction objective that simultaneously\ndenoises interleaved text and image inputs, enhancing bidirectional\ncontext-aware reasoning. The second stage, Heterogeneous Contrastive\nFine-tuning, leverages diverse, semantically rich multimodal data beyond simple\nimage-caption pairs to enhance generalization and alignment. Our method\naddresses the stated limitations by introducing bidirectional attention through\ncontinual pre-training, scaling effectively with massive unlabeled datasets via\njoint reconstruction objectives, and utilizing diverse multimodal data for\nenhanced representation robustness. Experiments demonstrate that MoCa\nconsistently improves performance across MMEB and ViDoRe-v2 benchmarks,\nachieving new state-of-the-art results, and exhibits strong scalability with\nboth model size and training data on MMEB.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.23115.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66add675c7a575aa0e03d5f3",
      "avatarUrl": "/avatars/b72b18130664c1de197c1f8df371aa70.svg",
      "fullname": "Haonan Chen",
      "name": "Haon-Chen",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.01001",
      "authors": [
        {
          "_id": "6864a4ddd59a9eda59024aea",
          "user": {
            "_id": "62f662bcc58915315c4eccea",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
            "isPro": true,
            "fullname": "Yilun Zhao",
            "user": "yilunzhao",
            "type": "user"
          },
          "name": "Yilun Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T07:38:54.448Z",
          "hidden": false
        },
        {
          "_id": "6864a4ddd59a9eda59024aeb",
          "user": {
            "_id": "676ce7767fff9075b5d526fa",
            "avatarUrl": "/avatars/bf6697163b91564a8d4b773d3f6420bf.svg",
            "isPro": false,
            "fullname": "Kaiyan Zhang",
            "user": "maxzky",
            "type": "user"
          },
          "name": "Kaiyan Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T07:38:52.477Z",
          "hidden": false
        },
        {
          "_id": "6864a4ddd59a9eda59024aec",
          "user": {
            "_id": "67492b9e347c3876f22b3684",
            "avatarUrl": "/avatars/0d80d23f7b10ce8bac689f6e8317a014.svg",
            "isPro": false,
            "fullname": "Tiansheng Hu",
            "user": "HughieHu",
            "type": "user"
          },
          "name": "Tiansheng Hu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T07:38:50.408Z",
          "hidden": false
        },
        {
          "_id": "6864a4ddd59a9eda59024aed",
          "name": "Sihong Wu",
          "hidden": false
        },
        {
          "_id": "6864a4ddd59a9eda59024aee",
          "name": "Ronan Le Bras",
          "hidden": false
        },
        {
          "_id": "6864a4ddd59a9eda59024aef",
          "name": "Taira Anderson",
          "hidden": false
        },
        {
          "_id": "6864a4ddd59a9eda59024af0",
          "name": "Jonathan Bragg",
          "hidden": false
        },
        {
          "_id": "6864a4ddd59a9eda59024af1",
          "name": "Joseph Chee Chang",
          "hidden": false
        },
        {
          "_id": "6864a4ddd59a9eda59024af2",
          "name": "Jesse Dodge",
          "hidden": false
        },
        {
          "_id": "6864a4ddd59a9eda59024af3",
          "name": "Matt Latzke",
          "hidden": false
        },
        {
          "_id": "6864a4ddd59a9eda59024af4",
          "name": "Yixin Liu",
          "hidden": false
        },
        {
          "_id": "6864a4ddd59a9eda59024af5",
          "name": "Charles McGrady",
          "hidden": false
        },
        {
          "_id": "6864a4ddd59a9eda59024af6",
          "name": "Xiangru Tang",
          "hidden": false
        },
        {
          "_id": "6864a4ddd59a9eda59024af7",
          "name": "Zihang Wang",
          "hidden": false
        },
        {
          "_id": "6864a4ddd59a9eda59024af8",
          "name": "Chen Zhao",
          "hidden": false
        },
        {
          "_id": "6864a4ddd59a9eda59024af9",
          "name": "Hannaneh Hajishirzi",
          "hidden": false
        },
        {
          "_id": "6864a4ddd59a9eda59024afa",
          "name": "Doug Downey",
          "hidden": false
        },
        {
          "_id": "6864a4ddd59a9eda59024afb",
          "name": "Arman Cohan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-01T17:51:59.000Z",
      "submittedOnDailyAt": "2025-07-02T01:48:24.934Z",
      "title": "SciArena: An Open Evaluation Platform for Foundation Models in\n  Scientific Literature Tasks",
      "submittedOnDailyBy": {
        "_id": "62f662bcc58915315c4eccea",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
        "isPro": true,
        "fullname": "Yilun Zhao",
        "user": "yilunzhao",
        "type": "user"
      },
      "summary": "We present SciArena, an open and collaborative platform for evaluating\nfoundation models on scientific literature tasks. Unlike traditional benchmarks\nfor scientific literature understanding and synthesis, SciArena engages the\nresearch community directly, following the Chatbot Arena evaluation approach of\ncommunity voting on model comparisons. By leveraging collective intelligence,\nSciArena offers a community-driven evaluation of model performance on\nopen-ended scientific tasks that demand literature-grounded, long-form\nresponses. The platform currently supports 23 open-source and proprietary\nfoundation models and has collected over 13,000 votes from trusted researchers\nacross diverse scientific domains. We analyze the data collected so far and\nconfirm that the submitted questions are diverse, aligned with real-world\nliterature needs, and that participating researchers demonstrate strong\nself-consistency and inter-annotator agreement in their evaluations. We discuss\nthe results and insights based on the model ranking leaderboard. To further\npromote research in building model-based automated evaluation systems for\nliterature tasks, we release SciArena-Eval, a meta-evaluation benchmark based\non our collected preference data. The benchmark measures the accuracy of models\nin judging answer quality by comparing their pairwise assessments with human\nvotes. Our experiments highlight the benchmark's challenges and emphasize the\nneed for more reliable automated evaluation methods.",
      "upvotes": 23,
      "discussionId": "6864a4ded59a9eda59024afc",
      "projectPage": "https://sciarena.allen.ai/",
      "ai_summary": "SciArena is a community-driven platform for evaluating foundation models on scientific literature tasks, using collective voter judgments to rank models and address the need for reliable automated evaluation.",
      "ai_keywords": [
        "Chatbot Arena",
        "collective intelligence",
        "open-ended scientific tasks",
        "literature-grounded",
        "long-form responses",
        "meta-evaluation benchmark",
        "automated evaluation systems"
      ]
    },
    "publishedAt": "2025-07-01T13:51:59.000Z",
    "title": "SciArena: An Open Evaluation Platform for Foundation Models in\n  Scientific Literature Tasks",
    "summary": "We present SciArena, an open and collaborative platform for evaluating\nfoundation models on scientific literature tasks. Unlike traditional benchmarks\nfor scientific literature understanding and synthesis, SciArena engages the\nresearch community directly, following the Chatbot Arena evaluation approach of\ncommunity voting on model comparisons. By leveraging collective intelligence,\nSciArena offers a community-driven evaluation of model performance on\nopen-ended scientific tasks that demand literature-grounded, long-form\nresponses. The platform currently supports 23 open-source and proprietary\nfoundation models and has collected over 13,000 votes from trusted researchers\nacross diverse scientific domains. We analyze the data collected so far and\nconfirm that the submitted questions are diverse, aligned with real-world\nliterature needs, and that participating researchers demonstrate strong\nself-consistency and inter-annotator agreement in their evaluations. We discuss\nthe results and insights based on the model ranking leaderboard. To further\npromote research in building model-based automated evaluation systems for\nliterature tasks, we release SciArena-Eval, a meta-evaluation benchmark based\non our collected preference data. The benchmark measures the accuracy of models\nin judging answer quality by comparing their pairwise assessments with human\nvotes. Our experiments highlight the benchmark's challenges and emphasize the\nneed for more reliable automated evaluation methods.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.01001.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62f662bcc58915315c4eccea",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
      "fullname": "Yilun Zhao",
      "name": "yilunzhao",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 13
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.00432",
      "authors": [
        {
          "_id": "686490e9d59a9eda59024a64",
          "name": "Maggie Huan",
          "hidden": false
        },
        {
          "_id": "686490e9d59a9eda59024a65",
          "name": "Yuetai Li",
          "hidden": false
        },
        {
          "_id": "686490e9d59a9eda59024a66",
          "user": {
            "_id": "64ab99dcb76bfd863eba64c1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ab99dcb76bfd863eba64c1/UBXwDPx17X-gl-SzBPvrc.jpeg",
            "isPro": false,
            "fullname": "TY.Zheng",
            "user": "aaabiao",
            "type": "user"
          },
          "name": "Tuney Zheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T07:39:59.058Z",
          "hidden": false
        },
        {
          "_id": "686490e9d59a9eda59024a67",
          "name": "Xiaoyu Xu",
          "hidden": false
        },
        {
          "_id": "686490e9d59a9eda59024a68",
          "name": "Seungone Kim",
          "hidden": false
        },
        {
          "_id": "686490e9d59a9eda59024a69",
          "name": "Minxin Du",
          "hidden": false
        },
        {
          "_id": "686490e9d59a9eda59024a6a",
          "name": "Radha Poovendran",
          "hidden": false
        },
        {
          "_id": "686490e9d59a9eda59024a6b",
          "name": "Graham Neubig",
          "hidden": false
        },
        {
          "_id": "686490e9d59a9eda59024a6c",
          "name": "Xiang Yue",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-01T05:23:05.000Z",
      "submittedOnDailyAt": "2025-07-02T00:24:45.275Z",
      "title": "Does Math Reasoning Improve General LLM Capabilities? Understanding\n  Transferability of LLM Reasoning",
      "submittedOnDailyBy": {
        "_id": "6230d750d93e84e233882dbc",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6230d750d93e84e233882dbc/4MGEekLW3oWzqeFWDWvIK.jpeg",
        "isPro": false,
        "fullname": "Xiang Yue",
        "user": "yuexiang96",
        "type": "user"
      },
      "summary": "Math reasoning has become the poster child of progress in large language\nmodels (LLMs), with new models rapidly surpassing human-level performance on\nbenchmarks like MATH and AIME. But as math leaderboards improve week by week,\nit is worth asking: do these gains reflect broader problem-solving ability or\njust narrow overfitting? To answer this question, we evaluate over 20\nopen-weight reasoning-tuned models across a broad suite of tasks, including\nmath, scientific QA, agent planning, coding, and standard\ninstruction-following. We surprisingly find that most models that succeed in\nmath fail to transfer their gains to other domains. To rigorously study this\nphenomenon, we conduct controlled experiments on Qwen3-14B models using\nmath-only data but different tuning methods. We find that reinforcement\nlearning (RL)-tuned models generalize well across domains, while supervised\nfine-tuning (SFT)-tuned models often forget general capabilities. Latent-space\nrepresentation and token-space distribution shift analyses reveal that SFT\ninduces substantial representation and output drift, while RL preserves\ngeneral-domain structure. Our results suggest a need to rethink standard\npost-training recipes, particularly the reliance on SFT-distilled data for\nadvancing reasoning models.",
      "upvotes": 20,
      "discussionId": "686490e9d59a9eda59024a6d",
      "githubRepo": "https://github.com/ReasoningTransfer/Transferability-of-LLM-Reasoning",
      "ai_summary": "Reinforcement learning-tuned models outperform supervised fine-tuned models in generalizing mathematical problem-solving abilities to other domains, indicating a need to re-evaluate training methods for reasoning models.",
      "ai_keywords": [
        "reinforcement learning",
        "supervised fine-tuning",
        "latent-space representation",
        "token-space distribution shift",
        "general-domain structure"
      ],
      "githubStars": 3
    },
    "publishedAt": "2025-07-01T01:23:05.000Z",
    "title": "Does Math Reasoning Improve General LLM Capabilities? Understanding\n  Transferability of LLM Reasoning",
    "summary": "Math reasoning has become the poster child of progress in large language\nmodels (LLMs), with new models rapidly surpassing human-level performance on\nbenchmarks like MATH and AIME. But as math leaderboards improve week by week,\nit is worth asking: do these gains reflect broader problem-solving ability or\njust narrow overfitting? To answer this question, we evaluate over 20\nopen-weight reasoning-tuned models across a broad suite of tasks, including\nmath, scientific QA, agent planning, coding, and standard\ninstruction-following. We surprisingly find that most models that succeed in\nmath fail to transfer their gains to other domains. To rigorously study this\nphenomenon, we conduct controlled experiments on Qwen3-14B models using\nmath-only data but different tuning methods. We find that reinforcement\nlearning (RL)-tuned models generalize well across domains, while supervised\nfine-tuning (SFT)-tuned models often forget general capabilities. Latent-space\nrepresentation and token-space distribution shift analyses reveal that SFT\ninduces substantial representation and output drift, while RL preserves\ngeneral-domain structure. Our results suggest a need to rethink standard\npost-training recipes, particularly the reliance on SFT-distilled data for\nadvancing reasoning models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.00432.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6230d750d93e84e233882dbc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6230d750d93e84e233882dbc/4MGEekLW3oWzqeFWDWvIK.jpeg",
      "fullname": "Xiang Yue",
      "name": "yuexiang96",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 36
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.19852",
      "authors": [
        {
          "_id": "685d790d61ef876fd500e925",
          "name": "Xingyang Li",
          "hidden": false
        },
        {
          "_id": "685d790d61ef876fd500e926",
          "user": {
            "_id": "63129589bbaa385279d1826e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63129589bbaa385279d1826e/0AUs3q4ngRZ-wXuY1jP9G.jpeg",
            "isPro": false,
            "fullname": "Muyang Li",
            "user": "Lmxyy",
            "type": "user"
          },
          "name": "Muyang Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-27T08:57:19.218Z",
          "hidden": false
        },
        {
          "_id": "685d790d61ef876fd500e927",
          "name": "Tianle Cai",
          "hidden": false
        },
        {
          "_id": "685d790d61ef876fd500e928",
          "name": "Haocheng Xi",
          "hidden": false
        },
        {
          "_id": "685d790d61ef876fd500e929",
          "name": "Shuo Yang",
          "hidden": false
        },
        {
          "_id": "685d790d61ef876fd500e92a",
          "name": "Yujun Lin",
          "hidden": false
        },
        {
          "_id": "685d790d61ef876fd500e92b",
          "name": "Lvmin Zhang",
          "hidden": false
        },
        {
          "_id": "685d790d61ef876fd500e92c",
          "name": "Songlin Yang",
          "hidden": false
        },
        {
          "_id": "685d790d61ef876fd500e92d",
          "name": "Jinbo Hu",
          "hidden": false
        },
        {
          "_id": "685d790d61ef876fd500e92e",
          "name": "Kelly Peng",
          "hidden": false
        },
        {
          "_id": "685d790d61ef876fd500e92f",
          "name": "Maneesh Agrawala",
          "hidden": false
        },
        {
          "_id": "685d790d61ef876fd500e930",
          "name": "Ion Stoica",
          "hidden": false
        },
        {
          "_id": "685d790d61ef876fd500e931",
          "name": "Kurt Keutzer",
          "hidden": false
        },
        {
          "_id": "685d790d61ef876fd500e932",
          "name": "Song Han",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63129589bbaa385279d1826e/X6ojt3M2m0VZGqlL3M-my.mp4"
      ],
      "publishedAt": "2025-06-24T17:59:59.000Z",
      "submittedOnDailyAt": "2025-07-02T02:22:22.027Z",
      "title": "Radial Attention: O(nlog n) Sparse Attention with Energy Decay for\n  Long Video Generation",
      "submittedOnDailyBy": {
        "_id": "63129589bbaa385279d1826e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63129589bbaa385279d1826e/0AUs3q4ngRZ-wXuY1jP9G.jpeg",
        "isPro": false,
        "fullname": "Muyang Li",
        "user": "Lmxyy",
        "type": "user"
      },
      "summary": "Recent advances in diffusion models have enabled high-quality video\ngeneration, but the additional temporal dimension significantly increases\ncomputational costs, making training and inference on long videos prohibitively\nexpensive. In this paper, we identify a phenomenon we term Spatiotemporal\nEnergy Decay in video diffusion models: post-softmax attention scores diminish\nas spatial and temporal distance between tokens increase, akin to the physical\ndecay of signal or waves over space and time in nature. Motivated by this, we\npropose Radial Attention, a scalable sparse attention mechanism with O(n log\nn) complexity that translates energy decay into exponentially decaying compute\ndensity, which is significantly more efficient than standard O(n^2) dense\nattention and more expressive than linear attention. Specifically, Radial\nAttention employs a simple, static attention mask where each token attends to\nspatially nearby tokens, with the attention window size shrinking with temporal\ndistance. Moreover, it allows pre-trained video diffusion models to extend\ntheir generation length with efficient LoRA-based fine-tuning. Extensive\nexperiments show that Radial Attention maintains video quality across\nWan2.1-14B, HunyuanVideo, and Mochi 1, achieving up to a 1.9times speedup\nover the original dense attention. With minimal tuning, it enables video\ngeneration up to 4times longer while reducing training costs by up to\n4.4times compared to direct fine-tuning and accelerating inference by up to\n3.7times compared to dense attention inference.",
      "upvotes": 11,
      "discussionId": "685d790d61ef876fd500e933",
      "projectPage": "https://hanlab.mit.edu/projects/radial-attention",
      "githubRepo": "https://github.com/mit-han-lab/radial-attention/",
      "ai_summary": "Radial Attention, a scalable sparse attention mechanism, improves efficiency and preserves video quality in diffusion models by leveraging spatiotemporal energy decay.",
      "ai_keywords": [
        "Spatiotemporal Energy Decay",
        "diffusion models",
        "radial attention",
        "attention scores",
        "dense attention",
        "linear attention",
        "attention mask",
        "LoRA-based fine-tuning",
        "video quality",
        "Wan2.1-14B",
        "HunyuanVideo",
        "Mochi 1"
      ],
      "githubStars": 185
    },
    "publishedAt": "2025-06-24T13:59:59.000Z",
    "title": "Radial Attention: O(nlog n) Sparse Attention with Energy Decay for\n  Long Video Generation",
    "summary": "Recent advances in diffusion models have enabled high-quality video\ngeneration, but the additional temporal dimension significantly increases\ncomputational costs, making training and inference on long videos prohibitively\nexpensive. In this paper, we identify a phenomenon we term Spatiotemporal\nEnergy Decay in video diffusion models: post-softmax attention scores diminish\nas spatial and temporal distance between tokens increase, akin to the physical\ndecay of signal or waves over space and time in nature. Motivated by this, we\npropose Radial Attention, a scalable sparse attention mechanism with O(n log\nn) complexity that translates energy decay into exponentially decaying compute\ndensity, which is significantly more efficient than standard O(n^2) dense\nattention and more expressive than linear attention. Specifically, Radial\nAttention employs a simple, static attention mask where each token attends to\nspatially nearby tokens, with the attention window size shrinking with temporal\ndistance. Moreover, it allows pre-trained video diffusion models to extend\ntheir generation length with efficient LoRA-based fine-tuning. Extensive\nexperiments show that Radial Attention maintains video quality across\nWan2.1-14B, HunyuanVideo, and Mochi 1, achieving up to a 1.9times speedup\nover the original dense attention. With minimal tuning, it enables video\ngeneration up to 4times longer while reducing training costs by up to\n4.4times compared to direct fine-tuning and accelerating inference by up to\n3.7times compared to dense attention inference.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63129589bbaa385279d1826e/X6ojt3M2m0VZGqlL3M-my.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19852.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63129589bbaa385279d1826e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63129589bbaa385279d1826e/0AUs3q4ngRZ-wXuY1jP9G.jpeg",
      "fullname": "Muyang Li",
      "name": "Lmxyy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.20639",
      "authors": [
        {
          "_id": "685d2773696820ba1f28f38c",
          "name": "Shansan Gong",
          "hidden": false
        },
        {
          "_id": "685d2773696820ba1f28f38d",
          "name": "Ruixiang Zhang",
          "hidden": false
        },
        {
          "_id": "685d2773696820ba1f28f38e",
          "name": "Huangjie Zheng",
          "hidden": false
        },
        {
          "_id": "685d2773696820ba1f28f38f",
          "name": "Jiatao Gu",
          "hidden": false
        },
        {
          "_id": "685d2773696820ba1f28f390",
          "name": "Navdeep Jaitly",
          "hidden": false
        },
        {
          "_id": "685d2773696820ba1f28f391",
          "name": "Lingpeng Kong",
          "hidden": false
        },
        {
          "_id": "685d2773696820ba1f28f392",
          "name": "Yizhe Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-25T17:35:47.000Z",
      "submittedOnDailyAt": "2025-07-02T01:51:58.159Z",
      "title": "DiffuCoder: Understanding and Improving Masked Diffusion Models for Code\n  Generation",
      "submittedOnDailyBy": {
        "_id": "628c83d186fc004b14e1ed48",
        "avatarUrl": "/avatars/05ff943a9b89b5f67c5bc254bf45b8f5.svg",
        "isPro": false,
        "fullname": "Shansan Gong",
        "user": "Sansa",
        "type": "user"
      },
      "summary": "Diffusion large language models (dLLMs) are compelling alternatives to\nautoregressive (AR) models because their denoising models operate over the\nentire sequence. The global planning and iterative refinement features of dLLMs\nare particularly useful for code generation. However, current training and\ninference mechanisms for dLLMs in coding are still under-explored. To demystify\nthe decoding behavior of dLLMs and unlock their potential for coding, we\nsystematically investigate their denoising processes and reinforcement learning\n(RL) methods. We train a 7B dLLM, DiffuCoder, on 130B tokens of code.\nUsing this model as a testbed, we analyze its decoding behavior, revealing how\nit differs from that of AR models: (1) dLLMs can decide how causal their\ngeneration should be without relying on semi-AR decoding, and (2) increasing\nthe sampling temperature diversifies not only token choices but also their\ngeneration order. This diversity creates a rich search space for RL rollouts.\nFor RL training, to reduce the variance of token log-likelihood estimates and\nmaintain training efficiency, we propose coupled-GRPO, a novel\nsampling scheme that constructs complementary mask noise for completions used\nin training. In our experiments, coupled-GRPO significantly improves\nDiffuCoder's performance on code generation benchmarks (+4.4\\% on EvalPlus) and\nreduces reliance on AR causal during decoding. Our work provides deeper insight\ninto the machinery of dLLM generation and offers an effective, diffusion-native\nRL training framework. https://github.com/apple/ml-diffucoder.",
      "upvotes": 10,
      "discussionId": "685d2774696820ba1f28f393",
      "githubRepo": "https://github.com/apple/ml-diffucoder",
      "ai_summary": "Diffusion large language models are applied to code generation, revealing their unique denoising processes and benefiting from a novel reinforcement learning sampling scheme.",
      "ai_keywords": [
        "diffusion large language models",
        "autoregressive models",
        "denoising models",
        "global planning",
        "iterative refinement",
        "code generation",
        "decoding behavior",
        "causal generation",
        "sampling temperature",
        "coupled-GRPO",
        "token log-likelihood estimates",
        "RL rollouts"
      ],
      "githubStars": 34
    },
    "publishedAt": "2025-06-25T13:35:47.000Z",
    "title": "DiffuCoder: Understanding and Improving Masked Diffusion Models for Code\n  Generation",
    "summary": "Diffusion large language models (dLLMs) are compelling alternatives to\nautoregressive (AR) models because their denoising models operate over the\nentire sequence. The global planning and iterative refinement features of dLLMs\nare particularly useful for code generation. However, current training and\ninference mechanisms for dLLMs in coding are still under-explored. To demystify\nthe decoding behavior of dLLMs and unlock their potential for coding, we\nsystematically investigate their denoising processes and reinforcement learning\n(RL) methods. We train a 7B dLLM, DiffuCoder, on 130B tokens of code.\nUsing this model as a testbed, we analyze its decoding behavior, revealing how\nit differs from that of AR models: (1) dLLMs can decide how causal their\ngeneration should be without relying on semi-AR decoding, and (2) increasing\nthe sampling temperature diversifies not only token choices but also their\ngeneration order. This diversity creates a rich search space for RL rollouts.\nFor RL training, to reduce the variance of token log-likelihood estimates and\nmaintain training efficiency, we propose coupled-GRPO, a novel\nsampling scheme that constructs complementary mask noise for completions used\nin training. In our experiments, coupled-GRPO significantly improves\nDiffuCoder's performance on code generation benchmarks (+4.4\\% on EvalPlus) and\nreduces reliance on AR causal during decoding. Our work provides deeper insight\ninto the machinery of dLLM generation and offers an effective, diffusion-native\nRL training framework. https://github.com/apple/ml-diffucoder.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.20639.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "628c83d186fc004b14e1ed48",
      "avatarUrl": "/avatars/05ff943a9b89b5f67c5bc254bf45b8f5.svg",
      "fullname": "Shansan Gong",
      "name": "Sansa",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.00951",
      "authors": [
        {
          "_id": "6864a0cdd59a9eda59024ad4",
          "name": "Rizwan Qureshi",
          "hidden": false
        },
        {
          "_id": "6864a0cdd59a9eda59024ad5",
          "name": "Ranjan Sapkota",
          "hidden": false
        },
        {
          "_id": "6864a0cdd59a9eda59024ad6",
          "name": "Abbas Shah",
          "hidden": false
        },
        {
          "_id": "6864a0cdd59a9eda59024ad7",
          "name": "Amgad Muneer",
          "hidden": false
        },
        {
          "_id": "6864a0cdd59a9eda59024ad8",
          "name": "Anas Zafar",
          "hidden": false
        },
        {
          "_id": "6864a0cdd59a9eda59024ad9",
          "name": "Ashmal Vayani",
          "hidden": false
        },
        {
          "_id": "6864a0cdd59a9eda59024ada",
          "name": "Maged Shoman",
          "hidden": false
        },
        {
          "_id": "6864a0cdd59a9eda59024adb",
          "name": "Abdelrahman B. M. Eldaly",
          "hidden": false
        },
        {
          "_id": "6864a0cdd59a9eda59024adc",
          "name": "Kai Zhang",
          "hidden": false
        },
        {
          "_id": "6864a0cdd59a9eda59024add",
          "name": "Ferhat Sadak",
          "hidden": false
        },
        {
          "_id": "6864a0cdd59a9eda59024ade",
          "user": {
            "_id": "5e466ca12d2efc729dc309ad",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1601989543860-5e466ca12d2efc729dc309ad.png",
            "isPro": false,
            "fullname": "shaina",
            "user": "shainaraza",
            "type": "user"
          },
          "name": "Shaina Raza",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T07:38:56.857Z",
          "hidden": false
        },
        {
          "_id": "6864a0cdd59a9eda59024adf",
          "name": "Xinqi Fan",
          "hidden": false
        },
        {
          "_id": "6864a0cdd59a9eda59024ae0",
          "name": "Ravid Shwartz-Ziv",
          "hidden": false
        },
        {
          "_id": "6864a0cdd59a9eda59024ae1",
          "name": "Hong Yan",
          "hidden": false
        },
        {
          "_id": "6864a0cdd59a9eda59024ae2",
          "name": "Vinjia Jain",
          "hidden": false
        },
        {
          "_id": "6864a0cdd59a9eda59024ae3",
          "user": {
            "_id": "63a4754927f1f64ed7238dac",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
            "isPro": false,
            "fullname": "Aman Chadha",
            "user": "amanchadha",
            "type": "user"
          },
          "name": "Aman Chadha",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T07:38:59.004Z",
          "hidden": false
        },
        {
          "_id": "6864a0cdd59a9eda59024ae4",
          "name": "Manoj Karkee",
          "hidden": false
        },
        {
          "_id": "6864a0cdd59a9eda59024ae5",
          "name": "Jia Wu",
          "hidden": false
        },
        {
          "_id": "6864a0cdd59a9eda59024ae6",
          "name": "Philip Torr",
          "hidden": false
        },
        {
          "_id": "6864a0cdd59a9eda59024ae7",
          "name": "Seyedali Mirjalili",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/67ddd80896ac367438d400a6/sRonQRwdqad3gk75XnJAO.png"
      ],
      "publishedAt": "2025-07-01T16:52:25.000Z",
      "submittedOnDailyAt": "2025-07-02T01:32:29.326Z",
      "title": "Thinking Beyond Tokens: From Brain-Inspired Intelligence to Cognitive\n  Foundations for Artificial General Intelligence and its Societal Impact",
      "submittedOnDailyBy": {
        "_id": "67ddd80896ac367438d400a6",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/C1NY6Nv5i0erwLnzCrTUM.png",
        "isPro": false,
        "fullname": "Ranjan Sapkota",
        "user": "RanjanSapkota",
        "type": "user"
      },
      "summary": "Can machines truly think, reason and act in domains like humans? This\nenduring question continues to shape the pursuit of Artificial General\nIntelligence (AGI). Despite the growing capabilities of models such as GPT-4.5,\nDeepSeek, Claude 3.5 Sonnet, Phi-4, and Grok 3, which exhibit multimodal\nfluency and partial reasoning, these systems remain fundamentally limited by\ntheir reliance on token-level prediction and lack of grounded agency. This\npaper offers a cross-disciplinary synthesis of AGI development, spanning\nartificial intelligence, cognitive neuroscience, psychology, generative models,\nand agent-based systems. We analyze the architectural and cognitive foundations\nof general intelligence, highlighting the role of modular reasoning, persistent\nmemory, and multi-agent coordination. In particular, we emphasize the rise of\nAgentic RAG frameworks that combine retrieval, planning, and dynamic tool use\nto enable more adaptive behavior. We discuss generalization strategies,\nincluding information compression, test-time adaptation, and training-free\nmethods, as critical pathways toward flexible, domain-agnostic intelligence.\nVision-Language Models (VLMs) are reexamined not just as perception modules but\nas evolving interfaces for embodied understanding and collaborative task\ncompletion. We also argue that true intelligence arises not from scale alone\nbut from the integration of memory and reasoning: an orchestration of modular,\ninteractive, and self-improving components where compression enables adaptive\nbehavior. Drawing on advances in neurosymbolic systems, reinforcement learning,\nand cognitive scaffolding, we explore how recent architectures begin to bridge\nthe gap between statistical learning and goal-directed cognition. Finally, we\nidentify key scientific, technical, and ethical challenges on the path to AGI.",
      "upvotes": 7,
      "discussionId": "6864a0ced59a9eda59024ae8",
      "ai_summary": "The paper synthesizes the interdisciplinary approach to achieving Artificial General Intelligence, emphasizing modular reasoning, memory, multi-agent coordination, and the integration of neurosymbolic systems and reinforcement learning to overcome current model limitations.",
      "ai_keywords": [
        "Artificial General Intelligence (AGI)",
        "Agentic RAG frameworks",
        "retrieval",
        "planning",
        "dynamic tool use",
        "generalization strategies",
        "information compression",
        "test-time adaptation",
        "training-free methods",
        "Vision-Language Models (VLMs)",
        "neurosymbolic systems",
        "reinforcement learning",
        "cognitive scaffolding"
      ]
    },
    "publishedAt": "2025-07-01T12:52:25.000Z",
    "title": "Thinking Beyond Tokens: From Brain-Inspired Intelligence to Cognitive\n  Foundations for Artificial General Intelligence and its Societal Impact",
    "summary": "Can machines truly think, reason and act in domains like humans? This\nenduring question continues to shape the pursuit of Artificial General\nIntelligence (AGI). Despite the growing capabilities of models such as GPT-4.5,\nDeepSeek, Claude 3.5 Sonnet, Phi-4, and Grok 3, which exhibit multimodal\nfluency and partial reasoning, these systems remain fundamentally limited by\ntheir reliance on token-level prediction and lack of grounded agency. This\npaper offers a cross-disciplinary synthesis of AGI development, spanning\nartificial intelligence, cognitive neuroscience, psychology, generative models,\nand agent-based systems. We analyze the architectural and cognitive foundations\nof general intelligence, highlighting the role of modular reasoning, persistent\nmemory, and multi-agent coordination. In particular, we emphasize the rise of\nAgentic RAG frameworks that combine retrieval, planning, and dynamic tool use\nto enable more adaptive behavior. We discuss generalization strategies,\nincluding information compression, test-time adaptation, and training-free\nmethods, as critical pathways toward flexible, domain-agnostic intelligence.\nVision-Language Models (VLMs) are reexamined not just as perception modules but\nas evolving interfaces for embodied understanding and collaborative task\ncompletion. We also argue that true intelligence arises not from scale alone\nbut from the integration of memory and reasoning: an orchestration of modular,\ninteractive, and self-improving components where compression enables adaptive\nbehavior. Drawing on advances in neurosymbolic systems, reinforcement learning,\nand cognitive scaffolding, we explore how recent architectures begin to bridge\nthe gap between statistical learning and goal-directed cognition. Finally, we\nidentify key scientific, technical, and ethical challenges on the path to AGI.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/67ddd80896ac367438d400a6/sRonQRwdqad3gk75XnJAO.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.00951.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67ddd80896ac367438d400a6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/C1NY6Nv5i0erwLnzCrTUM.png",
      "fullname": "Ranjan Sapkota",
      "name": "RanjanSapkota",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.21277",
      "authors": [
        {
          "_id": "68634a2c588cea0da970c8ae",
          "user": {
            "_id": "66a097801a26a2350395edc7",
            "avatarUrl": "/avatars/1e7e127cb7222df7d56e5bfda6bab519.svg",
            "isPro": false,
            "fullname": "Qize Yang",
            "user": "PhilipC",
            "type": "user"
          },
          "name": "Qize Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T07:40:53.951Z",
          "hidden": false
        },
        {
          "_id": "68634a2c588cea0da970c8af",
          "name": "Shimin Yao",
          "hidden": false
        },
        {
          "_id": "68634a2c588cea0da970c8b0",
          "name": "Weixuan Chen",
          "hidden": false
        },
        {
          "_id": "68634a2c588cea0da970c8b1",
          "user": {
            "_id": "67067633351e0c16a5c27497",
            "avatarUrl": "/avatars/356aa3431198c8931b820a714bcfb19d.svg",
            "isPro": false,
            "fullname": "Shenghao Fu",
            "user": "fushh7",
            "type": "user"
          },
          "name": "Shenghao Fu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T07:40:56.838Z",
          "hidden": false
        },
        {
          "_id": "68634a2c588cea0da970c8b2",
          "name": "Detao Bai",
          "hidden": false
        },
        {
          "_id": "68634a2c588cea0da970c8b3",
          "name": "Jiaxing Zhao",
          "hidden": false
        },
        {
          "_id": "68634a2c588cea0da970c8b4",
          "user": {
            "_id": "66ef2611fcc1c455f8dce832",
            "avatarUrl": "/avatars/c73ef2dfcd1e6ec8414a31226ad38e3b.svg",
            "isPro": false,
            "fullname": "Boyuan Sun",
            "user": "BBBBCHAN",
            "type": "user"
          },
          "name": "Boyuan Sun",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-01T10:13:33.669Z",
          "hidden": false
        },
        {
          "_id": "68634a2c588cea0da970c8b5",
          "name": "Bowen Yin",
          "hidden": false
        },
        {
          "_id": "68634a2c588cea0da970c8b6",
          "name": "Xihan Wei",
          "hidden": false
        },
        {
          "_id": "68634a2c588cea0da970c8b7",
          "name": "Jingren Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-26T14:01:03.000Z",
      "submittedOnDailyAt": "2025-07-02T00:57:00.933Z",
      "title": "HumanOmniV2: From Understanding to Omni-Modal Reasoning with Context",
      "submittedOnDailyBy": {
        "_id": "67067633351e0c16a5c27497",
        "avatarUrl": "/avatars/356aa3431198c8931b820a714bcfb19d.svg",
        "isPro": false,
        "fullname": "Shenghao Fu",
        "user": "fushh7",
        "type": "user"
      },
      "summary": "With the rapid evolution of multimodal large language models, the capacity to\ndeeply understand and interpret human intentions has emerged as a critical\ncapability, which demands detailed and thoughtful reasoning. In recent studies,\nReinforcement Learning (RL) has demonstrated potential in enhancing the\nreasoning capabilities of Large Language Models (LLMs). Nonetheless, the\nchallenges associated with adapting RL to multimodal data and formats remain\nlargely unaddressed. In this paper, we identify two issues in existing\nmultimodal reasoning models: insufficient global context understanding and\nshortcut problems. Insufficient context understanding can happen when a model\nmisinterprets multimodal context, resulting in incorrect answers. The shortcut\nproblem occurs when the model overlooks crucial clues in multimodal inputs,\ndirectly addressing the query without considering the multimodal information.\nTo tackle these issues, we emphasize the necessity for the model to reason with\na clear understanding of the global context within multimodal inputs. This\nglobal context understanding can effectively prevent the model from overlooking\nkey multimodal cues and ensure a thorough reasoning process. To ensure the\naccurate interpretation of multimodal context information, we implement a\ncontext reward judged by a large language model, alongside format and accuracy\nrewards. Additionally, to improve complex reasoning capability, we employ the\nLLM to assess the logical reward, determining whether the reasoning process\nsuccessfully integrates multimodal information with logical methods. We also\nintroduce a reasoning omni-modal benchmark, IntentBench, aimed at evaluating\nmodels in understanding complex human intentions and emotions. Our proposed\nmethod demonstrates advanced performance across multiple omni-modal benchmarks\ncompared to other open-source omni-modal models.",
      "upvotes": 7,
      "discussionId": "68634a2c588cea0da970c8b8",
      "githubRepo": "https://github.com/HumanMLLM/HumanOmniV2",
      "ai_summary": "A reinforcement learning-based approach enhances multimodal reasoning by addressing context understanding and shortcut problems, using context, format, accuracy, and logical rewards, and achieving superior performance on the IntentBench benchmark.",
      "ai_keywords": [
        "Reinforcement Learning",
        "Large Language Models",
        "multimodal reasoning",
        "global context understanding",
        "shortcut problem",
        "context reward",
        "logical reward",
        "IntentBench",
        "multimodal benchmark"
      ],
      "githubStars": 6
    },
    "publishedAt": "2025-06-26T10:01:03.000Z",
    "title": "HumanOmniV2: From Understanding to Omni-Modal Reasoning with Context",
    "summary": "With the rapid evolution of multimodal large language models, the capacity to\ndeeply understand and interpret human intentions has emerged as a critical\ncapability, which demands detailed and thoughtful reasoning. In recent studies,\nReinforcement Learning (RL) has demonstrated potential in enhancing the\nreasoning capabilities of Large Language Models (LLMs). Nonetheless, the\nchallenges associated with adapting RL to multimodal data and formats remain\nlargely unaddressed. In this paper, we identify two issues in existing\nmultimodal reasoning models: insufficient global context understanding and\nshortcut problems. Insufficient context understanding can happen when a model\nmisinterprets multimodal context, resulting in incorrect answers. The shortcut\nproblem occurs when the model overlooks crucial clues in multimodal inputs,\ndirectly addressing the query without considering the multimodal information.\nTo tackle these issues, we emphasize the necessity for the model to reason with\na clear understanding of the global context within multimodal inputs. This\nglobal context understanding can effectively prevent the model from overlooking\nkey multimodal cues and ensure a thorough reasoning process. To ensure the\naccurate interpretation of multimodal context information, we implement a\ncontext reward judged by a large language model, alongside format and accuracy\nrewards. Additionally, to improve complex reasoning capability, we employ the\nLLM to assess the logical reward, determining whether the reasoning process\nsuccessfully integrates multimodal information with logical methods. We also\nintroduce a reasoning omni-modal benchmark, IntentBench, aimed at evaluating\nmodels in understanding complex human intentions and emotions. Our proposed\nmethod demonstrates advanced performance across multiple omni-modal benchmarks\ncompared to other open-source omni-modal models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.21277.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67067633351e0c16a5c27497",
      "avatarUrl": "/avatars/356aa3431198c8931b820a714bcfb19d.svg",
      "fullname": "Shenghao Fu",
      "name": "fushh7",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.21545",
      "authors": [
        {
          "_id": "6864cbbad59a9eda59024b40",
          "name": "Yalun Dai",
          "hidden": false
        },
        {
          "_id": "6864cbbad59a9eda59024b41",
          "name": "Yangyu Huang",
          "hidden": false
        },
        {
          "_id": "6864cbbad59a9eda59024b42",
          "name": "Xin Zhang",
          "hidden": false
        },
        {
          "_id": "6864cbbad59a9eda59024b43",
          "name": "Wenshan Wu",
          "hidden": false
        },
        {
          "_id": "6864cbbad59a9eda59024b44",
          "name": "Chong Li",
          "hidden": false
        },
        {
          "_id": "6864cbbad59a9eda59024b45",
          "name": "Wenhui Lu",
          "hidden": false
        },
        {
          "_id": "6864cbbad59a9eda59024b46",
          "name": "Shijie Cao",
          "hidden": false
        },
        {
          "_id": "6864cbbad59a9eda59024b47",
          "name": "Li Dong",
          "hidden": false
        },
        {
          "_id": "6864cbbad59a9eda59024b48",
          "name": "Scarlett Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-26T17:59:07.000Z",
      "submittedOnDailyAt": "2025-07-02T04:34:17.870Z",
      "title": "Data Efficacy for Language Model Training",
      "submittedOnDailyBy": {
        "_id": "64d98ef7a4839890b25eb78b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d98ef7a4839890b25eb78b/215-CSVLl81z6CAq0ECWU.jpeg",
        "isPro": true,
        "fullname": "Fangyuan Yu",
        "user": "Ksgk-fy",
        "type": "user"
      },
      "summary": "Data is fundamental to the training of language models (LM). Recent research\nhas been dedicated to data efficiency, which aims to maximize performance by\nselecting a minimal or optimal subset of training data. Techniques such as data\nfiltering, sampling, and selection play a crucial role in this area. To\ncomplement it, we define Data Efficacy, which focuses on maximizing performance\nby optimizing the organization of training data and remains relatively\nunderexplored. This work introduces a general paradigm, DELT, for considering\ndata efficacy in LM training, which highlights the significance of training\ndata organization. DELT comprises three components: Data Scoring, Data\nSelection, and Data Ordering. Among these components, we design\nLearnability-Quality Scoring (LQS), as a new instance of Data Scoring, which\nconsiders both the learnability and quality of each data sample from the\ngradient consistency perspective. We also devise Folding Ordering (FO), as a\nnovel instance of Data Ordering, which addresses issues such as model\nforgetting and data distribution bias. Comprehensive experiments validate the\ndata efficacy in LM training, which demonstrates the following: Firstly,\nvarious instances of the proposed DELT enhance LM performance to varying\ndegrees without increasing the data scale and model size. Secondly, among these\ninstances, the combination of our proposed LQS for data scoring and Folding for\ndata ordering achieves the most significant improvement. Lastly, data efficacy\ncan be achieved together with data efficiency by applying data selection.\nTherefore, we believe that data efficacy is a promising foundational area in LM\ntraining.",
      "upvotes": 3,
      "discussionId": "6864cbbbd59a9eda59024b49",
      "ai_summary": "DELT, a paradigm for enhancing language model performance through data efficacy, consists of data scoring, selection, and ordering, demonstrating significant improvements without increasing data scale or model size.",
      "ai_keywords": [
        "data efficiency",
        "language models",
        "data efficacy",
        "DELT",
        "Data Scoring",
        "Data Selection",
        "Data Ordering",
        "Learnability-Quality Scoring",
        "LQS",
        "Folding Ordering",
        "model forgetting",
        "data distribution bias"
      ]
    },
    "publishedAt": "2025-06-26T13:59:07.000Z",
    "title": "Data Efficacy for Language Model Training",
    "summary": "Data is fundamental to the training of language models (LM). Recent research\nhas been dedicated to data efficiency, which aims to maximize performance by\nselecting a minimal or optimal subset of training data. Techniques such as data\nfiltering, sampling, and selection play a crucial role in this area. To\ncomplement it, we define Data Efficacy, which focuses on maximizing performance\nby optimizing the organization of training data and remains relatively\nunderexplored. This work introduces a general paradigm, DELT, for considering\ndata efficacy in LM training, which highlights the significance of training\ndata organization. DELT comprises three components: Data Scoring, Data\nSelection, and Data Ordering. Among these components, we design\nLearnability-Quality Scoring (LQS), as a new instance of Data Scoring, which\nconsiders both the learnability and quality of each data sample from the\ngradient consistency perspective. We also devise Folding Ordering (FO), as a\nnovel instance of Data Ordering, which addresses issues such as model\nforgetting and data distribution bias. Comprehensive experiments validate the\ndata efficacy in LM training, which demonstrates the following: Firstly,\nvarious instances of the proposed DELT enhance LM performance to varying\ndegrees without increasing the data scale and model size. Secondly, among these\ninstances, the combination of our proposed LQS for data scoring and Folding for\ndata ordering achieves the most significant improvement. Lastly, data efficacy\ncan be achieved together with data efficiency by applying data selection.\nTherefore, we believe that data efficacy is a promising foundational area in LM\ntraining.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.21545.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64d98ef7a4839890b25eb78b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d98ef7a4839890b25eb78b/215-CSVLl81z6CAq0ECWU.jpeg",
      "fullname": "Fangyuan Yu",
      "name": "Ksgk-fy",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.22960",
      "authors": [
        {
          "_id": "6864a811d59a9eda59024afe",
          "name": "Shreyas Dixit",
          "hidden": false
        },
        {
          "_id": "6864a811d59a9eda59024aff",
          "name": "Ashhar Aziz",
          "hidden": false
        },
        {
          "_id": "6864a811d59a9eda59024b00",
          "name": "Shashwat Bajpai",
          "hidden": false
        },
        {
          "_id": "6864a811d59a9eda59024b01",
          "name": "Vasu Sharma",
          "hidden": false
        },
        {
          "_id": "6864a811d59a9eda59024b02",
          "user": {
            "_id": "63a4754927f1f64ed7238dac",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
            "isPro": false,
            "fullname": "Aman Chadha",
            "user": "amanchadha",
            "type": "user"
          },
          "name": "Aman Chadha",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T07:38:48.204Z",
          "hidden": false
        },
        {
          "_id": "6864a811d59a9eda59024b03",
          "name": "Vinija Jain",
          "hidden": false
        },
        {
          "_id": "6864a811d59a9eda59024b04",
          "name": "Amitava Das",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-28T17:34:08.000Z",
      "submittedOnDailyAt": "2025-07-02T02:11:36.000Z",
      "title": "Peccavi: Visual Paraphrase Attack Safe and Distortion Free Image\n  Watermarking Technique for AI-Generated Images",
      "submittedOnDailyBy": {
        "_id": "63a4754927f1f64ed7238dac",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
        "isPro": false,
        "fullname": "Aman Chadha",
        "user": "amanchadha",
        "type": "user"
      },
      "summary": "A report by the European Union Law Enforcement Agency predicts that by 2026,\nup to 90 percent of online content could be synthetically generated, raising\nconcerns among policymakers, who cautioned that \"Generative AI could act as a\nforce multiplier for political disinformation. The combined effect of\ngenerative text, images, videos, and audio may surpass the influence of any\nsingle modality.\" In response, California's Bill AB 3211 mandates the\nwatermarking of AI-generated images, videos, and audio. However, concerns\nremain regarding the vulnerability of invisible watermarking techniques to\ntampering and the potential for malicious actors to bypass them entirely.\nGenerative AI-powered de-watermarking attacks, especially the newly introduced\nvisual paraphrase attack, have shown an ability to fully remove watermarks,\nresulting in a paraphrase of the original image. This paper introduces PECCAVI,\nthe first visual paraphrase attack-safe and distortion-free image watermarking\ntechnique. In visual paraphrase attacks, an image is altered while preserving\nits core semantic regions, termed Non-Melting Points (NMPs). PECCAVI\nstrategically embeds watermarks within these NMPs and employs multi-channel\nfrequency domain watermarking. It also incorporates noisy burnishing to counter\nreverse-engineering efforts aimed at locating NMPs to disrupt the embedded\nwatermark, thereby enhancing durability. PECCAVI is model-agnostic. All\nrelevant resources and codes will be open-sourced.",
      "upvotes": 1,
      "discussionId": "6864a811d59a9eda59024b05",
      "ai_summary": "PECCAVI is a robust image watermarking technique that is resistant to visual paraphrase attacks and distortions, utilizing NMPs and multi-channel frequency domain watermarking.",
      "ai_keywords": [
        "Generative AI",
        "visual paraphrase attack",
        "Non-Melting Points (NMPs)",
        "multi-channel frequency domain watermarking",
        "noisy burnishing",
        "model-agnostic"
      ]
    },
    "publishedAt": "2025-06-28T13:34:08.000Z",
    "title": "Peccavi: Visual Paraphrase Attack Safe and Distortion Free Image\n  Watermarking Technique for AI-Generated Images",
    "summary": "A report by the European Union Law Enforcement Agency predicts that by 2026,\nup to 90 percent of online content could be synthetically generated, raising\nconcerns among policymakers, who cautioned that \"Generative AI could act as a\nforce multiplier for political disinformation. The combined effect of\ngenerative text, images, videos, and audio may surpass the influence of any\nsingle modality.\" In response, California's Bill AB 3211 mandates the\nwatermarking of AI-generated images, videos, and audio. However, concerns\nremain regarding the vulnerability of invisible watermarking techniques to\ntampering and the potential for malicious actors to bypass them entirely.\nGenerative AI-powered de-watermarking attacks, especially the newly introduced\nvisual paraphrase attack, have shown an ability to fully remove watermarks,\nresulting in a paraphrase of the original image. This paper introduces PECCAVI,\nthe first visual paraphrase attack-safe and distortion-free image watermarking\ntechnique. In visual paraphrase attacks, an image is altered while preserving\nits core semantic regions, termed Non-Melting Points (NMPs). PECCAVI\nstrategically embeds watermarks within these NMPs and employs multi-channel\nfrequency domain watermarking. It also incorporates noisy burnishing to counter\nreverse-engineering efforts aimed at locating NMPs to disrupt the embedded\nwatermark, thereby enhancing durability. PECCAVI is model-agnostic. All\nrelevant resources and codes will be open-sourced.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.22960.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a4754927f1f64ed7238dac",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
      "fullname": "Aman Chadha",
      "name": "amanchadha",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  }
]