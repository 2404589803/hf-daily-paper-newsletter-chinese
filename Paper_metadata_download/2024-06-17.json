[
    {
        "paper": {
            "id": "2406.08973",
            "authors": [
                {
                    "_id": "666f1c8560142338147bfff3",
                    "user": {
                        "avatarUrl": "/avatars/fac3818da59913f8b9cca7575e1391a7.svg",
                        "isPro": false,
                        "fullname": "Alex",
                        "user": "Howuhh",
                        "type": "user"
                    },
                    "name": "Alexander Nikulin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-17T14:12:59.522Z",
                    "hidden": false
                },
                {
                    "_id": "666f1c8560142338147bfff4",
                    "user": {
                        "avatarUrl": "/avatars/fab89f503356f153834369f9564c6cda.svg",
                        "isPro": false,
                        "fullname": "Ilya",
                        "user": "suessmann",
                        "type": "user"
                    },
                    "name": "Ilya Zisman",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-17T16:15:22.758Z",
                    "hidden": false
                },
                {
                    "_id": "666f1c8560142338147bfff5",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/XR4dTb3BQ2wJ7bQ7FJwhx.jpeg",
                        "isPro": false,
                        "fullname": "Alex",
                        "user": "zzmtsvv",
                        "type": "user"
                    },
                    "name": "Alexey Zemtsov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-17T13:43:22.212Z",
                    "hidden": false
                },
                {
                    "_id": "666f1c8560142338147bfff6",
                    "name": "Viacheslav Sinii",
                    "hidden": false
                },
                {
                    "_id": "666f1c8560142338147bfff7",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d3711b37e2a376885b2ca7/DOcEfu-wNPijjK93jP2bn.jpeg",
                        "isPro": false,
                        "fullname": "Vladislav Kurenkov",
                        "user": "vkurenkov",
                        "type": "user"
                    },
                    "name": "Vladislav Kurenkov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-17T13:43:11.259Z",
                    "hidden": false
                },
                {
                    "_id": "666f1c8560142338147bfff8",
                    "name": "Sergey Kolesnikov",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-13T10:04:17.000Z",
            "title": "XLand-100B: A Large-Scale Multi-Task Dataset for In-Context\n  Reinforcement Learning",
            "summary": "Following the success of the in-context learning paradigm in large-scale\nlanguage and computer vision models, the recently emerging field of in-context\nreinforcement learning is experiencing a rapid growth. However, its development\nhas been held back by the lack of challenging benchmarks, as all the\nexperiments have been carried out in simple environments and on small-scale\ndatasets. We present XLand-100B, a large-scale dataset for in-context\nreinforcement learning based on the XLand-MiniGrid environment, as a first step\nto alleviate this problem. It contains complete learning histories for nearly\n30,000 different tasks, covering 100B transitions and 2.5B episodes. It\ntook 50,000 GPU hours to collect the dataset, which is beyond the reach of\nmost academic labs. Along with the dataset, we provide the utilities to\nreproduce or expand it even further. With this substantial effort, we aim to\ndemocratize research in the rapidly growing field of in-context reinforcement\nlearning and provide a solid foundation for further scaling. The code is\nopen-source and available under Apache 2.0 licence at\nhttps://github.com/dunno-lab/xland-minigrid-datasets.",
            "upvotes": 52
        },
        "publishedAt": "2024-06-17T06:23:52.336Z",
        "title": "XLand-100B: A Large-Scale Multi-Task Dataset for In-Context Reinforcement Learning",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.08973.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/fac3818da59913f8b9cca7575e1391a7.svg",
            "fullname": "Alex",
            "name": "Howuhh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.10210",
            "authors": [
                {
                    "_id": "666fe1d5b955b0e6551b19a8",
                    "user": {
                        "avatarUrl": "/avatars/cfdfd398d4600dad3a17c20424033a6b.svg",
                        "isPro": false,
                        "fullname": "Lital Binyamin",
                        "user": "Litalby",
                        "type": "user"
                    },
                    "name": "Lital Binyamin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-17T14:11:52.674Z",
                    "hidden": false
                },
                {
                    "_id": "666fe1d5b955b0e6551b19a9",
                    "user": {
                        "avatarUrl": "/avatars/4e47ab9ef1a4b9f59756a6d8c90a970f.svg",
                        "isPro": false,
                        "fullname": "Yoad Tewel",
                        "user": "YoadTew",
                        "type": "user"
                    },
                    "name": "Yoad Tewel",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-17T14:13:05.460Z",
                    "hidden": false
                },
                {
                    "_id": "666fe1d5b955b0e6551b19aa",
                    "user": {
                        "avatarUrl": "/avatars/ba42cf14b1ae558bd29e3ff51e59531a.svg",
                        "isPro": false,
                        "fullname": "Hilit Segev",
                        "user": "Hilit",
                        "type": "user"
                    },
                    "name": "Hilit Segev",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-17T14:13:11.770Z",
                    "hidden": false
                },
                {
                    "_id": "666fe1d5b955b0e6551b19ab",
                    "name": "Eran Hirsch",
                    "hidden": false
                },
                {
                    "_id": "666fe1d5b955b0e6551b19ac",
                    "user": {
                        "avatarUrl": "/avatars/91de4eb48f51bfd6e028c08ccfa98f8c.svg",
                        "isPro": false,
                        "fullname": "Royi Rassin",
                        "user": "Royir",
                        "type": "user"
                    },
                    "name": "Royi Rassin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-17T14:13:21.522Z",
                    "hidden": false
                },
                {
                    "_id": "666fe1d5b955b0e6551b19ad",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6493393f357b252af72196c5/EWSy18XRcMRa_4XMM3Fu-.jpeg",
                        "isPro": false,
                        "fullname": "Gal Chechik",
                        "user": "galchechik",
                        "type": "user"
                    },
                    "name": "Gal Chechik",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-17T14:13:27.138Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-14T17:46:08.000Z",
            "title": "Make It Count: Text-to-Image Generation with an Accurate Number of\n  Objects",
            "summary": "Despite the unprecedented success of text-to-image diffusion models,\ncontrolling the number of depicted objects using text is surprisingly hard.\nThis is important for various applications from technical documents, to\nchildren's books to illustrating cooking recipes. Generating object-correct\ncounts is fundamentally challenging because the generative model needs to keep\na sense of separate identity for every instance of the object, even if several\nobjects look identical or overlap, and then carry out a global computation\nimplicitly during generation. It is still unknown if such representations\nexist. To address count-correct generation, we first identify features within\nthe diffusion model that can carry the object identity information. We then use\nthem to separate and count instances of objects during the denoising process\nand detect over-generation and under-generation. We fix the latter by training\na model that predicts both the shape and location of a missing object, based on\nthe layout of existing ones, and show how it can be used to guide denoising\nwith correct object count. Our approach, CountGen, does not depend on external\nsource to determine object layout, but rather uses the prior from the diffusion\nmodel itself, creating prompt-dependent and seed-dependent layouts. Evaluated\non two benchmark datasets, we find that CountGen strongly outperforms the\ncount-accuracy of existing baselines.",
            "upvotes": 49
        },
        "publishedAt": "2024-06-17T09:10:51.957Z",
        "title": "Make It Count: Text-to-Image Generation with an Accurate Number of Objects",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.10210.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "/avatars/91de4eb48f51bfd6e028c08ccfa98f8c.svg",
            "fullname": "Royi Rassin",
            "name": "Royir",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.07230",
            "authors": [
                {
                    "_id": "6669285a759649b9b0a567c5",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/619507e7b74b6c591f794340/JbPDoy6Ko1V1-6oJJwFV8.jpeg",
                        "isPro": false,
                        "fullname": "Weiyun Wang",
                        "user": "Weiyun1025",
                        "type": "user"
                    },
                    "name": "Weiyun Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-17T13:52:35.011Z",
                    "hidden": false
                },
                {
                    "_id": "6669285a759649b9b0a567c6",
                    "name": "Shuibo Zhang",
                    "hidden": false
                },
                {
                    "_id": "6669285a759649b9b0a567c7",
                    "user": {
                        "avatarUrl": "/avatars/6f6204f9d9a7aca63c2b4599b3065977.svg",
                        "isPro": false,
                        "fullname": "yiming ren",
                        "user": "Flash128",
                        "type": "user"
                    },
                    "name": "Yiming Ren",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-17T13:52:50.031Z",
                    "hidden": false
                },
                {
                    "_id": "6669285a759649b9b0a567c8",
                    "user": {
                        "avatarUrl": "/avatars/e8b42301514094c8af6fa1e5fcb53119.svg",
                        "isPro": false,
                        "fullname": "Duan Yuchen",
                        "user": "duanyuchen",
                        "type": "user"
                    },
                    "name": "Yuchen Duan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-17T13:43:24.553Z",
                    "hidden": false
                },
                {
                    "_id": "6669285a759649b9b0a567c9",
                    "user": {
                        "avatarUrl": "/avatars/39d6c51cd1f15319b6c4efe9c72f1df2.svg",
                        "isPro": false,
                        "fullname": "Tiantong Li",
                        "user": "casually5101",
                        "type": "user"
                    },
                    "name": "Tiantong Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-17T13:52:55.709Z",
                    "hidden": false
                },
                {
                    "_id": "6669285a759649b9b0a567ca",
                    "name": "Shuo Liu",
                    "hidden": false
                },
                {
                    "_id": "6669285a759649b9b0a567cb",
                    "name": "Mengkang Hu",
                    "hidden": false
                },
                {
                    "_id": "6669285a759649b9b0a567cc",
                    "name": "Zhe Chen",
                    "hidden": false
                },
                {
                    "_id": "6669285a759649b9b0a567cd",
                    "user": {
                        "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
                        "isPro": false,
                        "fullname": "kaipeng",
                        "user": "kpzhang996",
                        "type": "user"
                    },
                    "name": "Kaipeng Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-17T07:33:57.728Z",
                    "hidden": false
                },
                {
                    "_id": "6669285a759649b9b0a567ce",
                    "user": {
                        "avatarUrl": "/avatars/dcf870695fd56b06ca03d82f831e9019.svg",
                        "isPro": false,
                        "fullname": "Lewei Lu",
                        "user": "luotto",
                        "type": "user"
                    },
                    "name": "Lewei Lu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-17T13:53:50.340Z",
                    "hidden": false
                },
                {
                    "_id": "6669285a759649b9b0a567cf",
                    "user": {
                        "avatarUrl": "/avatars/c387a75191005bcaa473091de5383a10.svg",
                        "isPro": false,
                        "fullname": "Xizhou Zhu",
                        "user": "Einsiedler",
                        "type": "user"
                    },
                    "name": "Xizhou Zhu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-17T13:53:56.048Z",
                    "hidden": false
                },
                {
                    "_id": "6669285a759649b9b0a567d0",
                    "user": {
                        "avatarUrl": "/avatars/30fd6b03b70af8018e8d5f99b89c68a5.svg",
                        "isPro": false,
                        "fullname": "luoping1121",
                        "user": "luoping",
                        "type": "user"
                    },
                    "name": "Ping Luo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-17T13:54:07.526Z",
                    "hidden": false
                },
                {
                    "_id": "6669285a759649b9b0a567d1",
                    "name": "Yu Qiao",
                    "hidden": false
                },
                {
                    "_id": "6669285a759649b9b0a567d2",
                    "name": "Jifeng Dai",
                    "hidden": false
                },
                {
                    "_id": "6669285a759649b9b0a567d3",
                    "user": {
                        "avatarUrl": "/avatars/5aa1a99468fa61d4b8b0e80b592c4e55.svg",
                        "isPro": false,
                        "fullname": "Wenqi Shao",
                        "user": "wqshao126",
                        "type": "user"
                    },
                    "name": "Wenqi Shao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-17T13:54:17.560Z",
                    "hidden": false
                },
                {
                    "_id": "6669285a759649b9b0a567d4",
                    "user": {
                        "avatarUrl": "/avatars/f42794fe25bffcd870a1bcee69b95298.svg",
                        "isPro": false,
                        "fullname": "wenhai.wang",
                        "user": "wangwhcore",
                        "type": "user"
                    },
                    "name": "Wenhai Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-17T13:54:39.048Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-11T13:09:16.000Z",
            "title": "Needle In A Multimodal Haystack",
            "summary": "With the rapid advancement of multimodal large language models (MLLMs), their\nevaluation has become increasingly comprehensive. However, understanding long\nmultimodal content, as a foundational ability for real-world applications,\nremains underexplored. In this work, we present Needle In A Multimodal Haystack\n(MM-NIAH), the first benchmark specifically designed to systematically evaluate\nthe capability of existing MLLMs to comprehend long multimodal documents. Our\nbenchmark includes three types of evaluation tasks: multimodal retrieval,\ncounting, and reasoning. In each task, the model is required to answer the\nquestions according to different key information scattered throughout the given\nmultimodal document. Evaluating the leading MLLMs on MM-NIAH, we observe that\nexisting models still have significant room for improvement on these tasks,\nespecially on vision-centric evaluation. We hope this work can provide a\nplatform for further research on long multimodal document comprehension and\ncontribute to the advancement of MLLMs. Code and benchmark are released at\nhttps://github.com/OpenGVLab/MM-NIAH.",
            "upvotes": 39
        },
        "publishedAt": "2024-06-17T04:32:27.531Z",
        "title": "Needle In A Multimodal Haystack",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.07230.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/619507e7b74b6c591f794340/JbPDoy6Ko1V1-6oJJwFV8.jpeg",
            "fullname": "Weiyun Wang",
            "name": "Weiyun1025",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.09961",
            "authors": [
                {
                    "_id": "666fdf3bc16d62cbb045cae0",
                    "user": {
                        "avatarUrl": "/avatars/36f36eea72a100ee56b37c6d185b47c0.svg",
                        "isPro": false,
                        "fullname": "Shi",
                        "user": "Chufan",
                        "type": "user"
                    },
                    "name": "Chufan Shi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-17T14:01:12.424Z",
                    "hidden": false
                },
                {
                    "_id": "666fdf3bc16d62cbb045cae1",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b75db417570fdff9b43fd9/-NcNL_QzuiAAvsBIQG3xo.png",
                        "isPro": false,
                        "fullname": "Cheng YANG",
                        "user": "Cheng-YANG",
                        "type": "user"
                    },
                    "name": "Cheng Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-17T07:31:03.951Z",
                    "hidden": false
                },
                {
                    "_id": "666fdf3bc16d62cbb045cae2",
                    "name": "Yaxin Liu",
                    "hidden": false
                },
                {
                    "_id": "666fdf3bc16d62cbb045cae3",
                    "name": "Bo Shui",
                    "hidden": false
                },
                {
                    "_id": "666fdf3bc16d62cbb045cae4",
                    "name": "Junjie Wang",
                    "hidden": false
                },
                {
                    "_id": "666fdf3bc16d62cbb045cae5",
                    "name": "Mohan Jing",
                    "hidden": false
                },
                {
                    "_id": "666fdf3bc16d62cbb045cae6",
                    "name": "Linran Xu",
                    "hidden": false
                },
                {
                    "_id": "666fdf3bc16d62cbb045cae7",
                    "name": "Xinyu Zhu",
                    "hidden": false
                },
                {
                    "_id": "666fdf3bc16d62cbb045cae8",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f71b23d3bdacb7eec43b1f/6RNVG9zug9tutjo5i8T4C.jpeg",
                        "isPro": false,
                        "fullname": "SihengLi",
                        "user": "SihengLi",
                        "type": "user"
                    },
                    "name": "Siheng Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-17T14:09:59.455Z",
                    "hidden": false
                },
                {
                    "_id": "666fdf3bc16d62cbb045cae9",
                    "name": "Yuxiang Zhang",
                    "hidden": false
                },
                {
                    "_id": "666fdf3bc16d62cbb045caea",
                    "user": {
                        "avatarUrl": "/avatars/6875e0cc1e4c13dce3ed45f9953e1ac4.svg",
                        "isPro": false,
                        "fullname": "gongye liu",
                        "user": "liuhuohuo",
                        "type": "user"
                    },
                    "name": "Gongye Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-17T14:10:43.252Z",
                    "hidden": false
                },
                {
                    "_id": "666fdf3bc16d62cbb045caeb",
                    "name": "Xiaomei Nie",
                    "hidden": false
                },
                {
                    "_id": "666fdf3bc16d62cbb045caec",
                    "name": "Deng Cai",
                    "hidden": false
                },
                {
                    "_id": "666fdf3bc16d62cbb045caed",
                    "user": {
                        "avatarUrl": "/avatars/44a3ad9e59318784ac531993b5f69f6b.svg",
                        "isPro": false,
                        "fullname": "Yujiu Yang",
                        "user": "Thu-redrobot",
                        "type": "user"
                    },
                    "name": "Yujiu Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-17T14:11:00.402Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-14T12:10:51.000Z",
            "title": "ChartMimic: Evaluating LMM's Cross-Modal Reasoning Capability via\n  Chart-to-Code Generation",
            "summary": "We introduce a new benchmark, ChartMimic, aimed at assessing the\nvisually-grounded code generation capabilities of large multimodal models\n(LMMs). ChartMimic utilizes information-intensive visual charts and textual\ninstructions as inputs, requiring LMMs to generate the corresponding code for\nchart rendering. ChartMimic includes 1,000 human-curated (figure, instruction,\ncode) triplets, which represent the authentic chart use cases found in\nscientific papers across various domains(e.g., Physics, Computer Science,\nEconomics, etc). These charts span 18 regular types and 4 advanced types,\ndiversifying into 191 subcategories. Furthermore, we propose multi-level\nevaluation metrics to provide an automatic and thorough assessment of the\noutput code and the rendered charts. Unlike existing code generation\nbenchmarks, ChartMimic places emphasis on evaluating LMMs' capacity to\nharmonize a blend of cognitive capabilities, encompassing visual understanding,\ncode generation, and cross-modal reasoning. The evaluation of 3 proprietary\nmodels and 11 open-weight models highlights the substantial challenges posed by\nChartMimic. Even the advanced GPT-4V, Claude-3-opus only achieve an average\nscore of 73.2 and 53.7, respectively, indicating significant room for\nimprovement. We anticipate that ChartMimic will inspire the development of\nLMMs, advancing the pursuit of artificial general intelligence.",
            "upvotes": 38
        },
        "publishedAt": "2024-06-17T06:08:18.341Z",
        "title": "ChartMimic: Evaluating LMM's Cross-Modal Reasoning Capability via Chart-to-Code Generation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.09961.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b75db417570fdff9b43fd9/-NcNL_QzuiAAvsBIQG3xo.png",
            "fullname": "Cheng YANG",
            "name": "Cheng-YANG",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.08418",
            "authors": [
                {
                    "_id": "666fd32c4aae2409910e249b",
                    "name": "Qingyun Li",
                    "hidden": false
                },
                {
                    "_id": "666fd32c4aae2409910e249c",
                    "name": "Zhe Chen",
                    "hidden": false
                },
                {
                    "_id": "666fd32c4aae2409910e249d",
                    "name": "Weiyun Wang",
                    "hidden": false
                },
                {
                    "_id": "666fd32c4aae2409910e249e",
                    "name": "Wenhai Wang",
                    "hidden": false
                },
                {
                    "_id": "666fd32c4aae2409910e249f",
                    "name": "Shenglong Ye",
                    "hidden": false
                },
                {
                    "_id": "666fd32c4aae2409910e24a0",
                    "name": "Zhenjiang Jin",
                    "hidden": false
                },
                {
                    "_id": "666fd32c4aae2409910e24a1",
                    "name": "Guanzhou Chen",
                    "hidden": false
                },
                {
                    "_id": "666fd32c4aae2409910e24a2",
                    "name": "Yinan He",
                    "hidden": false
                },
                {
                    "_id": "666fd32c4aae2409910e24a3",
                    "name": "Zhangwei Gao",
                    "hidden": false
                },
                {
                    "_id": "666fd32c4aae2409910e24a4",
                    "name": "Erfei Cui",
                    "hidden": false
                },
                {
                    "_id": "666fd32c4aae2409910e24a5",
                    "name": "Jiashuo Yu",
                    "hidden": false
                },
                {
                    "_id": "666fd32c4aae2409910e24a6",
                    "name": "Hao Tian",
                    "hidden": false
                },
                {
                    "_id": "666fd32c4aae2409910e24a7",
                    "name": "Jiasheng Zhou",
                    "hidden": false
                },
                {
                    "_id": "666fd32c4aae2409910e24a8",
                    "name": "Chao Xu",
                    "hidden": false
                },
                {
                    "_id": "666fd32c4aae2409910e24a9",
                    "name": "Bin Wang",
                    "hidden": false
                },
                {
                    "_id": "666fd32c4aae2409910e24aa",
                    "name": "Xingjian Wei",
                    "hidden": false
                },
                {
                    "_id": "666fd32c4aae2409910e24ab",
                    "name": "Wei Li",
                    "hidden": false
                },
                {
                    "_id": "666fd32c4aae2409910e24ac",
                    "name": "Wenjian Zhang",
                    "hidden": false
                },
                {
                    "_id": "666fd32c4aae2409910e24ad",
                    "name": "Bo Zhang",
                    "hidden": false
                },
                {
                    "_id": "666fd32c4aae2409910e24ae",
                    "name": "Pinlong Cai",
                    "hidden": false
                },
                {
                    "_id": "666fd32c4aae2409910e24af",
                    "name": "Licheng Wen",
                    "hidden": false
                },
                {
                    "_id": "666fd32c4aae2409910e24b0",
                    "name": "Xiangchao Yan",
                    "hidden": false
                },
                {
                    "_id": "666fd32c4aae2409910e24b1",
                    "name": "Zhenxiang Li",
                    "hidden": false
                },
                {
                    "_id": "666fd32c4aae2409910e24b2",
                    "name": "Pei Chu",
                    "hidden": false
                },
                {
                    "_id": "666fd32c4aae2409910e24b3",
                    "name": "Yi Wang",
                    "hidden": false
                },
                {
                    "_id": "666fd32c4aae2409910e24b4",
                    "name": "Min Dou",
                    "hidden": false
                },
                {
                    "_id": "666fd32c4aae2409910e24b5",
                    "name": "Changyao Tian",
                    "hidden": false
                },
                {
                    "_id": "666fd32c4aae2409910e24b6",
                    "name": "Xizhou Zhu",
                    "hidden": false
                },
                {
                    "_id": "666fd32c4aae2409910e24b7",
                    "name": "Lewei Lu",
                    "hidden": false
                },
                {
                    "_id": "666fd32c4aae2409910e24b8",
                    "name": "Yushi Chen",
                    "hidden": false
                },
                {
                    "_id": "666fd32c4aae2409910e24b9",
                    "name": "Junjun He",
                    "hidden": false
                },
                {
                    "_id": "666fd32c4aae2409910e24ba",
                    "name": "Zhongying Tu",
                    "hidden": false
                },
                {
                    "_id": "666fd32c4aae2409910e24bb",
                    "name": "Tong Lu",
                    "hidden": false
                },
                {
                    "_id": "666fd32c4aae2409910e24bc",
                    "name": "Yali Wang",
                    "hidden": false
                },
                {
                    "_id": "666fd32c4aae2409910e24bd",
                    "name": "Limin Wang",
                    "hidden": false
                },
                {
                    "_id": "666fd32c4aae2409910e24be",
                    "name": "Dahua Lin",
                    "hidden": false
                },
                {
                    "_id": "666fd32c4aae2409910e24bf",
                    "name": "Yu Qiao",
                    "hidden": false
                },
                {
                    "_id": "666fd32c4aae2409910e24c0",
                    "name": "Botian Shi",
                    "hidden": false
                },
                {
                    "_id": "666fd32c4aae2409910e24c1",
                    "name": "Conghui He",
                    "hidden": false
                },
                {
                    "_id": "666fd32c4aae2409910e24c2",
                    "name": "Jifeng Dai",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-12T17:01:04.000Z",
            "title": "OmniCorpus: A Unified Multimodal Corpus of 10 Billion-Level Images\n  Interleaved with Text",
            "summary": "Image-text interleaved data, consisting of multiple images and texts arranged\nin a natural document format, aligns with the presentation paradigm of internet\ndata and closely resembles human reading habits. Recent studies have shown that\nsuch data aids multimodal in-context learning and maintains the capabilities of\nlarge language models during multimodal fine-tuning. However, the limited scale\nand diversity of current image-text interleaved data restrict the development\nof multimodal large language models. In this paper, we introduce OmniCorpus, a\n10 billion-scale image-text interleaved dataset. Using an efficient data\nengine, we filter and extract large-scale high-quality documents, which contain\n8.6 billion images and 1,696 billion text tokens. Compared to counterparts\n(e.g., MMC4, OBELICS), our dataset 1) has 15 times larger scales while\nmaintaining good data quality; 2) features more diverse sources, including both\nEnglish and non-English websites as well as video-centric websites; 3) is more\nflexible, easily degradable from an image-text interleaved format to pure text\ncorpus and image-text pairs. Through comprehensive analysis and experiments, we\nvalidate the quality, usability, and effectiveness of the proposed dataset. We\nhope this could provide a solid data foundation for future multimodal model\nresearch. Code and data are released at\nhttps://github.com/OpenGVLab/OmniCorpus.",
            "upvotes": 19
        },
        "publishedAt": "2024-06-17T04:40:46.321Z",
        "title": "OmniCorpus: A Unified Multimodal Corpus of 10 Billion-Level Images Interleaved with Text",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.08418.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/619507e7b74b6c591f794340/JbPDoy6Ko1V1-6oJJwFV8.jpeg",
            "fullname": "Weiyun Wang",
            "name": "Weiyun1025",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.10149",
            "authors": [
                {
                    "_id": "666ffb662b9e452739047aa9",
                    "user": {
                        "avatarUrl": "/avatars/e4a9394057dc0813880e4a567293f34f.svg",
                        "isPro": false,
                        "fullname": "Yury Kuratov",
                        "user": "yurakuratov",
                        "type": "user"
                    },
                    "name": "Yuri Kuratov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-17T13:43:02.813Z",
                    "hidden": false
                },
                {
                    "_id": "666ffb662b9e452739047aaa",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c8b321cb2f1bf0e7c0f54b/JflXxMVnG9I0IB5YNyhXF.jpeg",
                        "isPro": false,
                        "fullname": "Aydar Bulatov",
                        "user": "booydar",
                        "type": "user"
                    },
                    "name": "Aydar Bulatov",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-17T14:13:46.699Z",
                    "hidden": false
                },
                {
                    "_id": "666ffb662b9e452739047aab",
                    "user": {
                        "avatarUrl": "/avatars/a50c7e1d22174f06fbaf99e8caca7c37.svg",
                        "isPro": false,
                        "fullname": "Petr Anokhin",
                        "user": "petranokhin",
                        "type": "user"
                    },
                    "name": "Petr Anokhin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-17T14:13:53.063Z",
                    "hidden": false
                },
                {
                    "_id": "666ffb662b9e452739047aac",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673636893395-noauth.png",
                        "isPro": false,
                        "fullname": "Ivan Rodkin",
                        "user": "irodkin",
                        "type": "user"
                    },
                    "name": "Ivan Rodkin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-17T14:13:59.696Z",
                    "hidden": false
                },
                {
                    "_id": "666ffb662b9e452739047aad",
                    "name": "Dmitry Sorokin",
                    "hidden": false
                },
                {
                    "_id": "666ffb662b9e452739047aae",
                    "user": {
                        "avatarUrl": "/avatars/7053c64142300fa446ba5870eb28c3ee.svg",
                        "isPro": false,
                        "fullname": "Artyom Sorokin",
                        "user": "griver",
                        "type": "user"
                    },
                    "name": "Artyom Sorokin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-17T14:14:09.088Z",
                    "hidden": false
                },
                {
                    "_id": "666ffb662b9e452739047aaf",
                    "user": {
                        "avatarUrl": "/avatars/c98ca8c9f9ed8509c2f1bb6aa994fd57.svg",
                        "isPro": false,
                        "fullname": "MIKHAIL BURTSEV",
                        "user": "mbur",
                        "type": "user"
                    },
                    "name": "Mikhail Burtsev",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-17T14:14:16.679Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-14T16:00:29.000Z",
            "title": "BABILong: Testing the Limits of LLMs with Long Context\n  Reasoning-in-a-Haystack",
            "summary": "In recent years, the input context sizes of large language models (LLMs) have\nincreased dramatically. However, existing evaluation methods have not kept\npace, failing to comprehensively assess the efficiency of models in handling\nlong contexts. To bridge this gap, we introduce the BABILong benchmark,\ndesigned to test language models' ability to reason across facts distributed in\nextremely long documents. BABILong includes a diverse set of 20 reasoning\ntasks, including fact chaining, simple induction, deduction, counting, and\nhandling lists/sets. These tasks are challenging on their own, and even more\ndemanding when the required facts are scattered across long natural text. Our\nevaluations show that popular LLMs effectively utilize only 10-20\\% of the\ncontext and their performance declines sharply with increased reasoning\ncomplexity. Among alternatives to in-context reasoning, Retrieval-Augmented\nGeneration methods achieve a modest 60\\% accuracy on single-fact question\nanswering, independent of context length. Among context extension methods, the\nhighest performance is demonstrated by recurrent memory transformers, enabling\nthe processing of lengths up to 11 million tokens. The BABILong benchmark is\nextendable to any length to support the evaluation of new upcoming models with\nincreased capabilities, and we provide splits up to 1 million token lengths.",
            "upvotes": 16
        },
        "publishedAt": "2024-06-17T08:15:02.180Z",
        "title": "BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.10149.png",
        "numComments": 3,
        "submittedBy": {
            "avatarUrl": "/avatars/e4a9394057dc0813880e4a567293f34f.svg",
            "fullname": "Yury Kuratov",
            "name": "yurakuratov",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.10118",
            "authors": [
                {
                    "_id": "6670475bf69ec2d1e9ce3c76",
                    "name": "Holy Lovenia",
                    "hidden": false
                },
                {
                    "_id": "6670475bf69ec2d1e9ce3c77",
                    "name": "Rahmad Mahendra",
                    "hidden": false
                },
                {
                    "_id": "6670475bf69ec2d1e9ce3c78",
                    "name": "Salsabil Maulana Akbar",
                    "hidden": false
                },
                {
                    "_id": "6670475bf69ec2d1e9ce3c79",
                    "name": "Lester James V. Miranda",
                    "hidden": false
                },
                {
                    "_id": "6670475bf69ec2d1e9ce3c7a",
                    "name": "Jennifer Santoso",
                    "hidden": false
                },
                {
                    "_id": "6670475bf69ec2d1e9ce3c7b",
                    "name": "Elyanah Aco",
                    "hidden": false
                },
                {
                    "_id": "6670475bf69ec2d1e9ce3c7c",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/615e04bb5aec20364f6a3ab8/VvfV00mJdCUr0MORk3w-N.png",
                        "isPro": false,
                        "fullname": "Akhdan Fadhilah",
                        "user": "akhdanfadh",
                        "type": "user"
                    },
                    "name": "Akhdan Fadhilah",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-17T16:28:37.955Z",
                    "hidden": false
                },
                {
                    "_id": "6670475bf69ec2d1e9ce3c7d",
                    "name": "Jonibek Mansurov",
                    "hidden": false
                },
                {
                    "_id": "6670475bf69ec2d1e9ce3c7e",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1644830238640-noauth.jpeg",
                        "isPro": true,
                        "fullname": "Joseph Imperial",
                        "user": "josephimperial",
                        "type": "user"
                    },
                    "name": "Joseph Marvin Imperial",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-17T16:15:02.811Z",
                    "hidden": false
                },
                {
                    "_id": "6670475bf69ec2d1e9ce3c7f",
                    "name": "Onno P. Kampman",
                    "hidden": false
                },
                {
                    "_id": "6670475bf69ec2d1e9ce3c80",
                    "name": "Joel Ruben Antony Moniz",
                    "hidden": false
                },
                {
                    "_id": "6670475bf69ec2d1e9ce3c81",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677144169806-63ddfced5ea8577c8d5fb421.jpeg",
                        "isPro": false,
                        "fullname": "Muhammad Ravi Shulthan Habibi",
                        "user": "muhammadravi251001",
                        "type": "user"
                    },
                    "name": "Muhammad Ravi Shulthan Habibi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-17T16:15:00.972Z",
                    "hidden": false
                },
                {
                    "_id": "6670475bf69ec2d1e9ce3c82",
                    "name": "Frederikus Hudi",
                    "hidden": false
                },
                {
                    "_id": "6670475bf69ec2d1e9ce3c83",
                    "name": "Railey Montalan",
                    "hidden": false
                },
                {
                    "_id": "6670475bf69ec2d1e9ce3c84",
                    "name": "Ryan Ignatius",
                    "hidden": false
                },
                {
                    "_id": "6670475bf69ec2d1e9ce3c85",
                    "name": "Joanito Agili Lopo",
                    "hidden": false
                },
                {
                    "_id": "6670475bf69ec2d1e9ce3c86",
                    "name": "William Nixon",
                    "hidden": false
                },
                {
                    "_id": "6670475bf69ec2d1e9ce3c87",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61e52be53d6dbb1da842316a/gx0WGPcOCClXPymoKglc4.jpeg",
                        "isPro": false,
                        "fullname": "Börje Karlsson",
                        "user": "tellarin",
                        "type": "user"
                    },
                    "name": "Börje F. Karlsson",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-17T16:15:05.238Z",
                    "hidden": false
                },
                {
                    "_id": "6670475bf69ec2d1e9ce3c88",
                    "name": "James Jaya",
                    "hidden": false
                },
                {
                    "_id": "6670475bf69ec2d1e9ce3c89",
                    "name": "Ryandito Diandaru",
                    "hidden": false
                },
                {
                    "_id": "6670475bf69ec2d1e9ce3c8a",
                    "name": "Yuze Gao",
                    "hidden": false
                },
                {
                    "_id": "6670475bf69ec2d1e9ce3c8b",
                    "name": "Patrick Amadeus",
                    "hidden": false
                },
                {
                    "_id": "6670475bf69ec2d1e9ce3c8c",
                    "name": "Bin Wang",
                    "hidden": false
                },
                {
                    "_id": "6670475bf69ec2d1e9ce3c8d",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5e89aea283fb7a4fc16792bf/h_hxnVJdufZHDU8bG-5Db.png",
                        "isPro": false,
                        "fullname": "Jan Christian Blaise Cruz",
                        "user": "jcblaise",
                        "type": "user"
                    },
                    "name": "Jan Christian Blaise Cruz",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-17T16:14:54.144Z",
                    "hidden": false
                },
                {
                    "_id": "6670475bf69ec2d1e9ce3c8e",
                    "name": "Chenxi Whitehouse",
                    "hidden": false
                },
                {
                    "_id": "6670475bf69ec2d1e9ce3c8f",
                    "name": "Ivan Halim Parmonangan",
                    "hidden": false
                },
                {
                    "_id": "6670475bf69ec2d1e9ce3c90",
                    "name": "Maria Khelli",
                    "hidden": false
                },
                {
                    "_id": "6670475bf69ec2d1e9ce3c91",
                    "name": "Wenyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "6670475bf69ec2d1e9ce3c92",
                    "name": "Lucky Susanto",
                    "hidden": false
                },
                {
                    "_id": "6670475bf69ec2d1e9ce3c93",
                    "name": "Reynard Adha Ryanda",
                    "hidden": false
                },
                {
                    "_id": "6670475bf69ec2d1e9ce3c94",
                    "name": "Sonny Lazuardi Hermawan",
                    "hidden": false
                },
                {
                    "_id": "6670475bf69ec2d1e9ce3c95",
                    "name": "Dan John Velasco",
                    "hidden": false
                },
                {
                    "_id": "6670475bf69ec2d1e9ce3c96",
                    "name": "Muhammad Dehan Al Kautsar",
                    "hidden": false
                },
                {
                    "_id": "6670475bf69ec2d1e9ce3c97",
                    "name": "Willy Fitra Hendria",
                    "hidden": false
                },
                {
                    "_id": "6670475bf69ec2d1e9ce3c98",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/618c42e4f35289d4bcab9cf1/wHmPhjRWUDdhyJMm8-jiJ.jpeg",
                        "isPro": false,
                        "fullname": "Yasmin Moslem",
                        "user": "ymoslem",
                        "type": "user"
                    },
                    "name": "Yasmin Moslem",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-17T16:14:56.447Z",
                    "hidden": false
                },
                {
                    "_id": "6670475bf69ec2d1e9ce3c99",
                    "name": "Noah Flynn",
                    "hidden": false
                },
                {
                    "_id": "6670475bf69ec2d1e9ce3c9a",
                    "name": "Muhammad Farid Adilazuarda",
                    "hidden": false
                },
                {
                    "_id": "6670475bf69ec2d1e9ce3c9b",
                    "name": "Haochen Li",
                    "hidden": false
                },
                {
                    "_id": "6670475bf69ec2d1e9ce3c9c",
                    "name": "Johanes Lee",
                    "hidden": false
                },
                {
                    "_id": "6670475bf69ec2d1e9ce3c9d",
                    "name": "R. Damanhuri",
                    "hidden": false
                },
                {
                    "_id": "6670475bf69ec2d1e9ce3c9e",
                    "name": "Shuo Sun",
                    "hidden": false
                },
                {
                    "_id": "6670475bf69ec2d1e9ce3c9f",
                    "name": "Muhammad Reza Qorib",
                    "hidden": false
                },
                {
                    "_id": "6670475bf69ec2d1e9ce3ca0",
                    "name": "Amirbek Djanibekov",
                    "hidden": false
                },
                {
                    "_id": "6670475bf69ec2d1e9ce3ca1",
                    "name": "Wei Qi Leong",
                    "hidden": false
                },
                {
                    "_id": "6670475bf69ec2d1e9ce3ca2",
                    "name": "Quyet V. Do",
                    "hidden": false
                },
                {
                    "_id": "6670475bf69ec2d1e9ce3ca3",
                    "name": "Niklas Muennighoff",
                    "hidden": false
                },
                {
                    "_id": "6670475bf69ec2d1e9ce3ca4",
                    "name": "Tanrada Pansuwan",
                    "hidden": false
                },
                {
                    "_id": "6670475bf69ec2d1e9ce3ca5",
                    "name": "Ilham Firdausi Putra",
                    "hidden": false
                },
                {
                    "_id": "6670475bf69ec2d1e9ce3ca6",
                    "name": "Yan Xu",
                    "hidden": false
                },
                {
                    "_id": "6670475bf69ec2d1e9ce3ca7",
                    "name": "Ngee Chia Tai",
                    "hidden": false
                },
                {
                    "_id": "6670475bf69ec2d1e9ce3ca8",
                    "name": "Ayu Purwarianti",
                    "hidden": false
                },
                {
                    "_id": "6670475bf69ec2d1e9ce3ca9",
                    "name": "Sebastian Ruder",
                    "hidden": false
                },
                {
                    "_id": "6670475bf69ec2d1e9ce3caa",
                    "name": "William Tjhi",
                    "hidden": false
                },
                {
                    "_id": "6670475bf69ec2d1e9ce3cab",
                    "name": "Peerat Limkonchotiwat",
                    "hidden": false
                },
                {
                    "_id": "6670475bf69ec2d1e9ce3cac",
                    "name": "Alham Fikri Aji",
                    "hidden": false
                },
                {
                    "_id": "6670475bf69ec2d1e9ce3cad",
                    "name": "Sedrick Keh",
                    "hidden": false
                },
                {
                    "_id": "6670475bf69ec2d1e9ce3cae",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1637813888895-5f5c4b20e56d546cd6233098.jpeg",
                        "isPro": false,
                        "fullname": "Genta Indra Winata",
                        "user": "gentaiscool",
                        "type": "user"
                    },
                    "name": "Genta Indra Winata",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-17T16:14:58.462Z",
                    "hidden": false
                },
                {
                    "_id": "6670475bf69ec2d1e9ce3caf",
                    "name": "Ruochen Zhang",
                    "hidden": false
                },
                {
                    "_id": "6670475bf69ec2d1e9ce3cb0",
                    "name": "Fajri Koto",
                    "hidden": false
                },
                {
                    "_id": "6670475bf69ec2d1e9ce3cb1",
                    "name": "Zheng-Xin Yong",
                    "hidden": false
                },
                {
                    "_id": "6670475bf69ec2d1e9ce3cb2",
                    "name": "Samuel Cahyawijaya",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-14T15:23:39.000Z",
            "title": "SEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for\n  Southeast Asian Languages",
            "summary": "Southeast Asia (SEA) is a region rich in linguistic diversity and cultural\nvariety, with over 1,300 indigenous languages and a population of 671 million\npeople. However, prevailing AI models suffer from a significant lack of\nrepresentation of texts, images, and audio datasets from SEA, compromising the\nquality of AI models for SEA languages. Evaluating models for SEA languages is\nchallenging due to the scarcity of high-quality datasets, compounded by the\ndominance of English training data, raising concerns about potential cultural\nmisrepresentation. To address these challenges, we introduce SEACrowd, a\ncollaborative initiative that consolidates a comprehensive resource hub that\nfills the resource gap by providing standardized corpora in nearly 1,000 SEA\nlanguages across three modalities. Through our SEACrowd benchmarks, we assess\nthe quality of AI models on 36 indigenous languages across 13 tasks, offering\nvaluable insights into the current AI landscape in SEA. Furthermore, we propose\nstrategies to facilitate greater AI advancements, maximizing potential utility\nand resource equity for the future of AI in SEA.",
            "upvotes": 15
        },
        "publishedAt": "2024-06-17T12:56:13.099Z",
        "title": "SEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.10118.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61e52be53d6dbb1da842316a/gx0WGPcOCClXPymoKglc4.jpeg",
            "fullname": "Börje Karlsson",
            "name": "tellarin",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.08451",
            "authors": [
                {
                    "_id": "666fd4c3cc1f54dfa9132919",
                    "user": {
                        "avatarUrl": "/avatars/5b9966742c5762e6e9026efc876f646a.svg",
                        "isPro": false,
                        "fullname": "Quanfeng Lu",
                        "user": "hflqf88888",
                        "type": "user"
                    },
                    "name": "Quanfeng Lu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-17T14:14:28.922Z",
                    "hidden": false
                },
                {
                    "_id": "666fd4c3cc1f54dfa913291a",
                    "user": {
                        "avatarUrl": "/avatars/5aa1a99468fa61d4b8b0e80b592c4e55.svg",
                        "isPro": false,
                        "fullname": "Wenqi Shao",
                        "user": "wqshao126",
                        "type": "user"
                    },
                    "name": "Wenqi Shao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-17T14:14:35.766Z",
                    "hidden": false
                },
                {
                    "_id": "666fd4c3cc1f54dfa913291b",
                    "user": {
                        "avatarUrl": "/avatars/a19a0cf6c3315f16f6adc8cbae8238be.svg",
                        "isPro": false,
                        "fullname": "Zitao Liu",
                        "user": "windlao",
                        "type": "user"
                    },
                    "name": "Zitao Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-17T14:14:41.384Z",
                    "hidden": false
                },
                {
                    "_id": "666fd4c3cc1f54dfa913291c",
                    "user": {
                        "avatarUrl": "/avatars/c7bef45efad6a0d911a720e2236fcba5.svg",
                        "isPro": false,
                        "fullname": "fanqing meng",
                        "user": "FanqingM",
                        "type": "user"
                    },
                    "name": "Fanqing Meng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-17T14:14:47.622Z",
                    "hidden": false
                },
                {
                    "_id": "666fd4c3cc1f54dfa913291d",
                    "user": {
                        "avatarUrl": "/avatars/79658c23555deadcfb097b6ebccbe4a6.svg",
                        "isPro": false,
                        "fullname": "Boxuan Li",
                        "user": "liboxuanhk",
                        "type": "user"
                    },
                    "name": "Boxuan Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-17T14:14:53.006Z",
                    "hidden": false
                },
                {
                    "_id": "666fd4c3cc1f54dfa913291e",
                    "name": "Botong Chen",
                    "hidden": false
                },
                {
                    "_id": "666fd4c3cc1f54dfa913291f",
                    "name": "Siyuan Huang",
                    "hidden": false
                },
                {
                    "_id": "666fd4c3cc1f54dfa9132920",
                    "user": {
                        "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
                        "isPro": false,
                        "fullname": "kaipeng",
                        "user": "kpzhang996",
                        "type": "user"
                    },
                    "name": "Kaipeng Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-17T07:31:41.126Z",
                    "hidden": false
                },
                {
                    "_id": "666fd4c3cc1f54dfa9132921",
                    "name": "Yu Qiao",
                    "hidden": false
                },
                {
                    "_id": "666fd4c3cc1f54dfa9132922",
                    "name": "Ping Luo",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-12T17:44:26.000Z",
            "title": "GUI Odyssey: A Comprehensive Dataset for Cross-App GUI Navigation on\n  Mobile Devices",
            "summary": "Smartphone users often navigate across multiple applications (apps) to\ncomplete tasks such as sharing content between social media platforms.\nAutonomous Graphical User Interface (GUI) navigation agents can enhance user\nexperience in communication, entertainment, and productivity by streamlining\nworkflows and reducing manual intervention. However, prior GUI agents often\ntrained with datasets comprising simple tasks that can be completed within a\nsingle app, leading to poor performance in cross-app navigation. To address\nthis problem, we introduce GUI Odyssey, a comprehensive dataset for training\nand evaluating cross-app navigation agents. GUI Odyssey consists of 7,735\nepisodes from 6 mobile devices, spanning 6 types of cross-app tasks, 201 apps,\nand 1.4K app combos. Leveraging GUI Odyssey, we developed OdysseyAgent, a\nmultimodal cross-app navigation agent by fine-tuning the Qwen-VL model with a\nhistory resampling module. Extensive experiments demonstrate OdysseyAgent's\nsuperior accuracy compared to existing models. For instance, OdysseyAgent\nsurpasses fine-tuned Qwen-VL and zero-shot GPT-4V by 1.44\\% and 55.49\\%\nin-domain accuracy, and 2.29\\% and 48.14\\% out-of-domain accuracy on average.\nThe dataset and code will be released in\nhttps://github.com/OpenGVLab/GUI-Odyssey.",
            "upvotes": 13
        },
        "publishedAt": "2024-06-17T04:59:14.569Z",
        "title": "GUI Odyssey: A Comprehensive Dataset for Cross-App GUI Navigation on Mobile Devices",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64b3fd42eec33e27dcc4c941/ee-ZZUFLwVr76LREw0RIE.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.08451.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/5aa1a99468fa61d4b8b0e80b592c4e55.svg",
            "fullname": "Wenqi Shao",
            "name": "wqshao126",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.10208",
            "authors": [
                {
                    "_id": "666fc0d80224abdc1997630f",
                    "name": "Zeyu Liu",
                    "hidden": false
                },
                {
                    "_id": "666fc0d80224abdc19976310",
                    "user": {
                        "avatarUrl": "/avatars/5d183d9c55043ccfc33cc58c86243182.svg",
                        "isPro": false,
                        "fullname": "Weicong Liang",
                        "user": "kxqt",
                        "type": "user"
                    },
                    "name": "Weicong Liang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-17T14:15:23.402Z",
                    "hidden": false
                },
                {
                    "_id": "666fc0d80224abdc19976311",
                    "name": "Yiming Zhao",
                    "hidden": false
                },
                {
                    "_id": "666fc0d80224abdc19976312",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/h7YdQe9wEr1TTJfP1ALhb.jpeg",
                        "isPro": false,
                        "fullname": "chen",
                        "user": "bohanChen",
                        "type": "user"
                    },
                    "name": "Bohan Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-17T14:15:33.769Z",
                    "hidden": false
                },
                {
                    "_id": "666fc0d80224abdc19976313",
                    "name": "Ji Li",
                    "hidden": false
                },
                {
                    "_id": "666fc0d80224abdc19976314",
                    "name": "Yuhui Yuan",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-14T17:44:09.000Z",
            "title": "Glyph-ByT5-v2: A Strong Aesthetic Baseline for Accurate Multilingual\n  Visual Text Rendering",
            "summary": "Recently, Glyph-ByT5 has achieved highly accurate visual text rendering\nperformance in graphic design images. However, it still focuses solely on\nEnglish and performs relatively poorly in terms of visual appeal. In this work,\nwe address these two fundamental limitations by presenting Glyph-ByT5-v2 and\nGlyph-SDXL-v2, which not only support accurate visual text rendering for 10\ndifferent languages but also achieve much better aesthetic quality. To achieve\nthis, we make the following contributions: (i) creating a high-quality\nmultilingual glyph-text and graphic design dataset consisting of more than 1\nmillion glyph-text pairs and 10 million graphic design image-text pairs\ncovering nine other languages, (ii) building a multilingual visual paragraph\nbenchmark consisting of 1,000 prompts, with 100 for each language, to assess\nmultilingual visual spelling accuracy, and (iii) leveraging the latest\nstep-aware preference learning approach to enhance the visual aesthetic\nquality. With the combination of these techniques, we deliver a powerful\ncustomized multilingual text encoder, Glyph-ByT5-v2, and a strong aesthetic\ngraphic generation model, Glyph-SDXL-v2, that can support accurate spelling in\n10 different languages. We perceive our work as a significant advancement,\nconsidering that the latest DALL-E3 and Ideogram 1.0 still struggle with the\nmultilingual visual text rendering task.",
            "upvotes": 11
        },
        "publishedAt": "2024-06-17T03:22:08.123Z",
        "title": "Glyph-ByT5-v2: A Strong Aesthetic Baseline for Accurate Multilingual Visual Text Rendering",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.10208.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646f69a6041e48e1c4728de3/U5OaW6PgsXTXnfG03xs9Q.png",
            "fullname": "GlyphByT5",
            "name": "GlyphByT5",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.09900",
            "authors": [
                {
                    "_id": "666ff529b07525f0bdec8aee",
                    "name": "Jie Wu",
                    "hidden": false
                },
                {
                    "_id": "666ff529b07525f0bdec8aef",
                    "user": {
                        "avatarUrl": "/avatars/d880f1c66945e91db7336f50f5dd7e41.svg",
                        "isPro": false,
                        "fullname": "zhu",
                        "user": "yufengzhu",
                        "type": "user"
                    },
                    "name": "Yufeng Zhu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-17T14:15:53.783Z",
                    "hidden": false
                },
                {
                    "_id": "666ff529b07525f0bdec8af0",
                    "name": "Lei Shen",
                    "hidden": false
                },
                {
                    "_id": "666ff529b07525f0bdec8af1",
                    "user": {
                        "avatarUrl": "/avatars/282f548c26c62660c9cd6b8d806ebe92.svg",
                        "isPro": false,
                        "fullname": "xuqing lu",
                        "user": "luxq",
                        "type": "user"
                    },
                    "name": "Xuqing Lu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-17T14:16:00.236Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-14T10:15:49.000Z",
            "title": "GEB-1.3B: Open Lightweight Large Language Model",
            "summary": "Recently developed large language models (LLMs) such as ChatGPT, Claude, and\nLlama have demonstrated impressive abilities, and even surpass human-level\nperformance in several tasks. Despite their success, the resource-intensive\ndemands of these models, requiring significant computational power for both\ntraining and inference, limit their deployment to high-performance servers.\nAdditionally, the extensive calculation requirements of the models often lead\nto increased latency in response times. With the increasing need for LLMs to\noperate efficiently on CPUs, research about lightweight models that are\noptimized for CPU inference has emerged. In this work, we introduce GEB-1.3B, a\nlightweight LLM trained on 550 billion tokens in both Chinese and English\nlanguages. We employ novel training techniques, including ROPE,\nGroup-Query-Attention, and FlashAttention-2, to accelerate training while\nmaintaining model performance. Additionally, we fine-tune the model using 10\nmillion samples of instruction data to enhance alignment. GEB-1.3B exhibits\noutstanding performance on general benchmarks such as MMLU, C-Eval, and CMMLU,\noutperforming comparative models such as MindLLM-1.3B and TinyLLaMA-1.1B.\nNotably, the FP32 version of GEB-1.3B achieves commendable inference times on\nCPUs, with ongoing efforts to further enhance speed through advanced\nquantization techniques. The release of GEB-1.3B as an open-source model marks\na significant contribution to the development of lightweight LLMs, promising to\nfoster further research and innovation in the field.",
            "upvotes": 5
        },
        "publishedAt": "2024-06-17T07:05:53.157Z",
        "title": "GEB-1.3B: Open Lightweight Large Language Model",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.09900.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1627505688463-60107b385ac3e86b3ea4fc34.jpeg",
            "fullname": "Daniel van Strien",
            "name": "davanstrien",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.07882",
            "authors": [
                {
                    "_id": "666fe01119b028dd91bc9dd6",
                    "name": "Yida Chen",
                    "hidden": false
                },
                {
                    "_id": "666fe01119b028dd91bc9dd7",
                    "name": "Aoyu Wu",
                    "hidden": false
                },
                {
                    "_id": "666fe01119b028dd91bc9dd8",
                    "name": "Trevor DePodesta",
                    "hidden": false
                },
                {
                    "_id": "666fe01119b028dd91bc9dd9",
                    "name": "Catherine Yeh",
                    "hidden": false
                },
                {
                    "_id": "666fe01119b028dd91bc9dda",
                    "name": "Kenneth Li",
                    "hidden": false
                },
                {
                    "_id": "666fe01119b028dd91bc9ddb",
                    "name": "Nicholas Castillo Marin",
                    "hidden": false
                },
                {
                    "_id": "666fe01119b028dd91bc9ddc",
                    "name": "Oam Patel",
                    "hidden": false
                },
                {
                    "_id": "666fe01119b028dd91bc9ddd",
                    "name": "Jan Riecke",
                    "hidden": false
                },
                {
                    "_id": "666fe01119b028dd91bc9dde",
                    "name": "Shivam Raval",
                    "hidden": false
                },
                {
                    "_id": "666fe01119b028dd91bc9ddf",
                    "name": "Olivia Seow",
                    "hidden": false
                },
                {
                    "_id": "666fe01119b028dd91bc9de0",
                    "name": "Martin Wattenberg",
                    "hidden": false
                },
                {
                    "_id": "666fe01119b028dd91bc9de1",
                    "name": "Fernanda Viégas",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-12T05:20:16.000Z",
            "title": "Designing a Dashboard for Transparency and Control of Conversational AI",
            "summary": "Conversational LLMs function as black box systems, leaving users guessing\nabout why they see the output they do. This lack of transparency is potentially\nproblematic, especially given concerns around bias and truthfulness. To address\nthis issue, we present an end-to-end prototype-connecting interpretability\ntechniques with user experience design-that seeks to make chatbots more\ntransparent. We begin by showing evidence that a prominent open-source LLM has\na \"user model\": examining the internal state of the system, we can extract data\nrelated to a user's age, gender, educational level, and socioeconomic status.\nNext, we describe the design of a dashboard that accompanies the chatbot\ninterface, displaying this user model in real time. The dashboard can also be\nused to control the user model and the system's behavior. Finally, we discuss a\nstudy in which users conversed with the instrumented system. Our results\nsuggest that users appreciate seeing internal states, which helped them expose\nbiased behavior and increased their sense of control. Participants also made\nvaluable suggestions that point to future directions for both design and\nmachine learning research. The project page and video demo of our TalkTuner\nsystem are available at https://bit.ly/talktuner-project-page",
            "upvotes": 4
        },
        "publishedAt": "2024-06-17T05:45:43.311Z",
        "title": "Designing a Dashboard for Transparency and Control of Conversational AI",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6312618c5e2531edb9ee9fb0/gzcTSgt09vUXn0Sg-cbRX.png",
            "https://cdn-uploads.huggingface.co/production/uploads/6312618c5e2531edb9ee9fb0/kxg87JYqMdqsZ4XjRyOsD.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.07882.png",
        "numComments": 3,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6312618c5e2531edb9ee9fb0/CxFdN-z8pTn8qCLcJ16bT.png",
            "fullname": "Yida Chen",
            "name": "YidaChen",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.08845",
            "authors": [
                {
                    "_id": "666fe04bc16d62cbb0462c4e",
                    "name": "Tianle Zhang",
                    "hidden": false
                },
                {
                    "_id": "666fe04bc16d62cbb0462c4f",
                    "name": "Langtian Ma",
                    "hidden": false
                },
                {
                    "_id": "666fe04bc16d62cbb0462c50",
                    "name": "Yuchen Yan",
                    "hidden": false
                },
                {
                    "_id": "666fe04bc16d62cbb0462c51",
                    "name": "Yuchen Zhang",
                    "hidden": false
                },
                {
                    "_id": "666fe04bc16d62cbb0462c52",
                    "name": "Kai Wang",
                    "hidden": false
                },
                {
                    "_id": "666fe04bc16d62cbb0462c53",
                    "name": "Yue Yang",
                    "hidden": false
                },
                {
                    "_id": "666fe04bc16d62cbb0462c54",
                    "name": "Ziyao Guo",
                    "hidden": false
                },
                {
                    "_id": "666fe04bc16d62cbb0462c55",
                    "name": "Wenqi Shao",
                    "hidden": false
                },
                {
                    "_id": "666fe04bc16d62cbb0462c56",
                    "name": "Yang You",
                    "hidden": false
                },
                {
                    "_id": "666fe04bc16d62cbb0462c57",
                    "name": "Yu Qiao",
                    "hidden": false
                },
                {
                    "_id": "666fe04bc16d62cbb0462c58",
                    "name": "Ping Luo",
                    "hidden": false
                },
                {
                    "_id": "666fe04bc16d62cbb0462c59",
                    "user": {
                        "avatarUrl": "/avatars/99a51adef5673b3ac1a8c02eb47759c4.svg",
                        "isPro": false,
                        "fullname": "KAIPENG ZHANG",
                        "user": "kpzhang",
                        "type": "user"
                    },
                    "name": "Kaipeng Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-17T14:16:41.682Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-13T06:09:22.000Z",
            "title": "Rethinking Human Evaluation Protocol for Text-to-Video Models: Enhancing\n  Reliability,Reproducibility, and Practicality",
            "summary": "Recent text-to-video (T2V) technology advancements, as demonstrated by models\nsuch as Gen2, Pika, and Sora, have significantly broadened its applicability\nand popularity. Despite these strides, evaluating these models poses\nsubstantial challenges. Primarily, due to the limitations inherent in automatic\nmetrics, manual evaluation is often considered a superior method for assessing\nT2V generation. However, existing manual evaluation protocols face\nreproducibility, reliability, and practicality issues. To address these\nchallenges, this paper introduces the Text-to-Video Human Evaluation (T2VHE)\nprotocol, a comprehensive and standardized protocol for T2V models. The T2VHE\nprotocol includes well-defined metrics, thorough annotator training, and an\neffective dynamic evaluation module. Experimental results demonstrate that this\nprotocol not only ensures high-quality annotations but can also reduce\nevaluation costs by nearly 50%. We will open-source the entire setup of the\nT2VHE protocol, including the complete protocol workflow, the dynamic\nevaluation component details, and the annotation interface code. This will help\ncommunities establish more sophisticated human assessment protocols.",
            "upvotes": 4
        },
        "publishedAt": "2024-06-17T05:36:17.600Z",
        "title": "Rethinking Human Evaluation Protocol for Text-to-Video Models: Enhancing Reliability,Reproducibility, and Practicality",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.08845.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/5aa1a99468fa61d4b8b0e80b592c4e55.svg",
            "fullname": "Wenqi Shao",
            "name": "wqshao126",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.06263",
            "authors": [
                {
                    "_id": "666840d3bb4e8a77538b1300",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61bf84c8ca59d6d196a1b4e8/L_NvUwlMYcye9X35z6f7e.jpeg",
                        "isPro": false,
                        "fullname": "Amir Hossein Kargaran",
                        "user": "kargaranamir",
                        "type": "user"
                    },
                    "name": "Amir Hossein Kargaran",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-11T13:26:27.595Z",
                    "hidden": true
                },
                {
                    "_id": "666840d3bb4e8a77538b1301",
                    "name": "François Yvon",
                    "hidden": false
                },
                {
                    "_id": "666840d3bb4e8a77538b1302",
                    "name": "Hinrich Schütze",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-10T13:44:29.000Z",
            "title": "MaskLID: Code-Switching Language Identification through Iterative\n  Masking",
            "summary": "We present MaskLID, a simple, yet effective, code-switching (CS) language\nidentification (LID) method. MaskLID does not require any training and is\ndesigned to complement current high-performance sentence-level LIDs.\nSentence-level LIDs are classifiers trained on monolingual texts to provide\nsingle labels, typically using a softmax layer to turn scores into\nprobabilities. However, in cases where a sentence is composed in both L1 and L2\nlanguages, the LID classifier often only returns the dominant label L1. To\naddress this limitation, MaskLID employs a strategy to mask text features\nassociated with L1, allowing the LID to classify the text as L2 in the next\nround. This method uses the LID itself to identify the features that require\nmasking and does not rely on any external resource. In this work, we explore\nthe use of MaskLID for two open-source LIDs (GlotLID and OpenLID), that are\nboth based on the FastText architecture. Code and demo are available at\nhttps://github.com/cisnlp/MaskLID.",
            "upvotes": 3
        },
        "publishedAt": "2024-06-17T06:26:57.103Z",
        "title": "MaskLID: Code-Switching Language Identification through Iterative Masking",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.06263.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61bf84c8ca59d6d196a1b4e8/L_NvUwlMYcye9X35z6f7e.jpeg",
            "fullname": "Amir Hossein Kargaran",
            "name": "kargaranamir",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.09559",
            "authors": [
                {
                    "_id": "666fc2679494795851c7e2d1",
                    "user": {
                        "avatarUrl": "/avatars/77f2ee0949560504e1643263fd7084da.svg",
                        "isPro": false,
                        "fullname": "Labs",
                        "user": "SankalpKJ",
                        "type": "user"
                    },
                    "name": "Sankalp KJ",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-17T16:28:48.049Z",
                    "hidden": false
                },
                {
                    "_id": "666fc2679494795851c7e2d2",
                    "name": "Vinija Jain",
                    "hidden": false
                },
                {
                    "_id": "666fc2679494795851c7e2d3",
                    "name": "Sreyoshi Bhaduri",
                    "hidden": false
                },
                {
                    "_id": "666fc2679494795851c7e2d4",
                    "name": "Tamoghna Roy",
                    "hidden": false
                },
                {
                    "_id": "666fc2679494795851c7e2d5",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
                        "isPro": false,
                        "fullname": "Aman Chadha",
                        "user": "amanchadha",
                        "type": "user"
                    },
                    "name": "Aman Chadha",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-17T07:31:44.640Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-13T19:55:20.000Z",
            "title": "Decoding the Diversity: A Review of the Indic AI Research Landscape",
            "summary": "This review paper provides a comprehensive overview of large language model\n(LLM) research directions within Indic languages. Indic languages are those\nspoken in the Indian subcontinent, including India, Pakistan, Bangladesh, Sri\nLanka, Nepal, and Bhutan, among others. These languages have a rich cultural\nand linguistic heritage and are spoken by over 1.5 billion people worldwide.\nWith the tremendous market potential and growing demand for natural language\nprocessing (NLP) based applications in diverse languages, generative\napplications for Indic languages pose unique challenges and opportunities for\nresearch. Our paper deep dives into the recent advancements in Indic generative\nmodeling, contributing with a taxonomy of research directions, tabulating 84\nrecent publications. Research directions surveyed in this paper include LLM\ndevelopment, fine-tuning existing LLMs, development of corpora, benchmarking\nand evaluation, as well as publications around specific techniques, tools, and\napplications. We found that researchers across the publications emphasize the\nchallenges associated with limited data availability, lack of standardization,\nand the peculiar linguistic complexities of Indic languages. This work aims to\nserve as a valuable resource for researchers and practitioners working in the\nfield of NLP, particularly those focused on Indic languages, and contributes to\nthe development of more accurate and efficient LLM applications for these\nlanguages.",
            "upvotes": 3
        },
        "publishedAt": "2024-06-17T03:54:15.058Z",
        "title": "Decoding the Diversity: A Review of the Indic AI Research Landscape",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.09559.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
            "fullname": "Aman Chadha",
            "name": "amanchadha",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.08920",
            "authors": [
                {
                    "_id": "66705dfbb0cc5418431113a8",
                    "name": "Swapnil Bhosale",
                    "hidden": false
                },
                {
                    "_id": "66705dfbb0cc5418431113a9",
                    "name": "Haosen Yang",
                    "hidden": false
                },
                {
                    "_id": "66705dfbb0cc5418431113aa",
                    "name": "Diptesh Kanojia",
                    "hidden": false
                },
                {
                    "_id": "66705dfbb0cc5418431113ab",
                    "name": "Jiankang Deng",
                    "hidden": false
                },
                {
                    "_id": "66705dfbb0cc5418431113ac",
                    "name": "Xiatian Zhu",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-13T08:34:12.000Z",
            "title": "AV-GS: Learning Material and Geometry Aware Priors for Novel View\n  Acoustic Synthesis",
            "summary": "Novel view acoustic synthesis (NVAS) aims to render binaural audio at any\ntarget viewpoint, given a mono audio emitted by a sound source at a 3D scene.\nExisting methods have proposed NeRF-based implicit models to exploit visual\ncues as a condition for synthesizing binaural audio. However, in addition to\nlow efficiency originating from heavy NeRF rendering, these methods all have a\nlimited ability of characterizing the entire scene environment such as room\ngeometry, material properties, and the spatial relation between the listener\nand sound source. To address these issues, we propose a novel Audio-Visual\nGaussian Splatting (AV-GS) model. To obtain a material-aware and geometry-aware\ncondition for audio synthesis, we learn an explicit point-based scene\nrepresentation with an audio-guidance parameter on locally initialized Gaussian\npoints, taking into account the space relation from the listener and sound\nsource. To make the visual scene model audio adaptive, we propose a point\ndensification and pruning strategy to optimally distribute the Gaussian points,\nwith the per-point contribution in sound propagation (e.g., more points needed\nfor texture-less wall surfaces as they affect sound path diversion). Extensive\nexperiments validate the superiority of our AV-GS over existing alternatives on\nthe real-world RWAS and simulation-based SoundSpaces datasets.",
            "upvotes": 2
        },
        "publishedAt": "2024-06-17T15:33:26.821Z",
        "title": "AV-GS: Learning Material and Geometry Aware Priors for Novel View Acoustic Synthesis",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64c281e30344289c376c0fa4/cevSFyPLzlSgXpAKDGPpy.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.08920.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/dae01f7054da8c9517e3808a6f10eb1a.svg",
            "fullname": "HY",
            "name": "happy0612",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.10227",
            "authors": [
                {
                    "_id": "666fbd99bc840e6748785265",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg",
                        "isPro": false,
                        "fullname": "Qinghong Lin",
                        "user": "KevinQHLin",
                        "type": "user"
                    },
                    "name": "Kevin Qinghong Lin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-17T16:15:20.666Z",
                    "hidden": false
                },
                {
                    "_id": "666fbd99bc840e6748785266",
                    "user": {
                        "avatarUrl": "/avatars/bfb7e0d730b7d03302799d5d2828d97d.svg",
                        "isPro": false,
                        "fullname": "Linjie Li",
                        "user": "linjieli222",
                        "type": "user"
                    },
                    "name": "Linjie Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-17T16:22:52.728Z",
                    "hidden": false
                },
                {
                    "_id": "666fbd99bc840e6748785267",
                    "user": {
                        "avatarUrl": "/avatars/46c004650ba15525ccf02d40c9314085.svg",
                        "isPro": false,
                        "fullname": "Difei Gao",
                        "user": "QuStar",
                        "type": "user"
                    },
                    "name": "Difei Gao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-17T16:22:59.802Z",
                    "hidden": false
                },
                {
                    "_id": "666fbd99bc840e6748785268",
                    "name": "Qinchen WU",
                    "hidden": false
                },
                {
                    "_id": "666fbd99bc840e6748785269",
                    "name": "Mingyi Yan",
                    "hidden": false
                },
                {
                    "_id": "666fbd99bc840e674878526a",
                    "user": {
                        "avatarUrl": "/avatars/fb36f69f03421c3a2a7f72ba0858fa60.svg",
                        "isPro": false,
                        "fullname": "Zhengyuan Yang",
                        "user": "zyang39",
                        "type": "user"
                    },
                    "name": "Zhengyuan Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-17T16:23:22.034Z",
                    "hidden": false
                },
                {
                    "_id": "666fbd99bc840e674878526b",
                    "user": {
                        "avatarUrl": "/avatars/a6f8d0573e678f79bc3c0b7897b818ce.svg",
                        "isPro": false,
                        "fullname": "Lijuan Wang",
                        "user": "Lijuan",
                        "type": "user"
                    },
                    "name": "Lijuan Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-17T16:23:28.369Z",
                    "hidden": false
                },
                {
                    "_id": "666fbd99bc840e674878526c",
                    "user": {
                        "avatarUrl": "/avatars/fa4fc03664803e02aede4d4c3d50b393.svg",
                        "isPro": false,
                        "fullname": "Mike Zheng Shou",
                        "user": "AnalMom",
                        "type": "user"
                    },
                    "name": "Mike Zheng Shou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-17T16:23:42.360Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-14T17:59:08.000Z",
            "title": "VideoGUI: A Benchmark for GUI Automation from Instructional Videos",
            "summary": "Graphical User Interface (GUI) automation holds significant promise for\nenhancing human productivity by assisting with computer tasks. Existing task\nformulations primarily focus on simple tasks that can be specified by a single,\nlanguage-only instruction, such as \"Insert a new slide.\" In this work, we\nintroduce VideoGUI, a novel multi-modal benchmark designed to evaluate GUI\nassistants on visual-centric GUI tasks. Sourced from high-quality web\ninstructional videos, our benchmark focuses on tasks involving professional and\nnovel software (e.g., Adobe Photoshop or Stable Diffusion WebUI) and complex\nactivities (e.g., video editing). VideoGUI evaluates GUI assistants through a\nhierarchical process, allowing for identification of the specific levels at\nwhich they may fail: (i) high-level planning: reconstruct procedural subtasks\nfrom visual conditions without language descriptions; (ii) middle-level\nplanning: generate sequences of precise action narrations based on visual state\n(i.e., screenshot) and goals; (iii) atomic action execution: perform specific\nactions such as accurately clicking designated elements. For each level, we\ndesign evaluation metrics across individual dimensions to provide clear\nsignals, such as individual performance in clicking, dragging, typing, and\nscrolling for atomic action execution. Our evaluation on VideoGUI reveals that\neven the SoTA large multimodal model GPT4o performs poorly on visual-centric\nGUI tasks, especially for high-level planning.",
            "upvotes": 2
        },
        "publishedAt": "2024-06-17T14:09:33.290Z",
        "title": "VideoGUI: A Benchmark for GUI Automation from Instructional Videos",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64440be5af034cdfd69ca3a7/3DUnWU3t2-KfVahhM-qaS.mp4",
            "https://cdn-uploads.huggingface.co/production/uploads/64440be5af034cdfd69ca3a7/5j3ghQDBXS_KupzNR_BQB.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.10227.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg",
            "fullname": "Qinghong Lin",
            "name": "KevinQHLin",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.10126",
            "authors": [
                {
                    "_id": "666fd85c3666df05b7b4f569",
                    "user": {
                        "avatarUrl": "/avatars/7a79a12d9f67100b0ea2dfb19dc695d0.svg",
                        "isPro": false,
                        "fullname": "Chen Hou",
                        "user": "LegolasS",
                        "type": "user"
                    },
                    "name": "Chen Hou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-17T07:31:15.600Z",
                    "hidden": false
                },
                {
                    "_id": "666fd85c3666df05b7b4f56a",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1668083049513-63160a1d29411a6864b0e585.jpeg",
                        "isPro": false,
                        "fullname": "Guoqiang Wei",
                        "user": "weigq",
                        "type": "user"
                    },
                    "name": "Guoqiang Wei",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-17T07:31:27.898Z",
                    "hidden": false
                },
                {
                    "_id": "666fd85c3666df05b7b4f56b",
                    "name": "Yan Zeng",
                    "hidden": false
                },
                {
                    "_id": "666fd85c3666df05b7b4f56c",
                    "name": "Zhibo Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-14T15:33:00.000Z",
            "title": "Training-free Camera Control for Video Generation",
            "summary": "We propose a training-free and robust solution to offer camera movement\ncontrol for off-the-shelf video diffusion models. Unlike previous work, our\nmethod does not require any supervised finetuning on camera-annotated datasets\nor self-supervised training via data augmentation. Instead, it can be plugged\nand played with most pretrained video diffusion models and generate camera\ncontrollable videos with a single image or text prompt as input. The\ninspiration of our work comes from the layout prior that intermediate latents\nhold towards generated results, thus rearranging noisy pixels in them will make\noutput content reallocated as well. As camera move could also be seen as a kind\nof pixel rearrangement caused by perspective change, videos could be\nreorganized following specific camera motion if their noisy latents change\naccordingly. Established on this, we propose our method CamTrol, which enables\nrobust camera control for video diffusion models. It is achieved by a two-stage\nprocess. First, we model image layout rearrangement through explicit camera\nmovement in 3D point cloud space. Second, we generate videos with camera motion\nusing layout prior of noisy latents formed by a series of rearranged images.\nExtensive experiments have demonstrated the robustness our method holds in\ncontrolling camera motion of generated videos. Furthermore, we show that our\nmethod can produce impressive results in generating 3D rotation videos with\ndynamic content. Project page at https://lifedecoder.github.io/CamTrol/.",
            "upvotes": 2
        },
        "publishedAt": "2024-06-17T12:22:38.932Z",
        "title": "Training-free Camera Control for Video Generation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.10126.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.10111",
            "authors": [
                {
                    "_id": "666fe9a81f975c1f86db1d51",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661052258980-619b7b1cab4c7b7f16a7d59e.png",
                        "isPro": false,
                        "fullname": "Tianyu He",
                        "user": "deeptimhe",
                        "type": "user"
                    },
                    "name": "Xiqian Yu",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2024-06-17T08:36:47.720Z",
                    "hidden": true
                },
                {
                    "_id": "666fe9a81f975c1f86db1d52",
                    "user": {
                        "avatarUrl": "/avatars/384b35ea59b55f8d454be9efb7cce47c.svg",
                        "isPro": false,
                        "fullname": "Hanxin ZHU",
                        "user": "ingzzzz",
                        "type": "user"
                    },
                    "name": "Hanxin Zhu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-17T16:20:13.935Z",
                    "hidden": false
                },
                {
                    "_id": "666fe9a81f975c1f86db1d53",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661052258980-619b7b1cab4c7b7f16a7d59e.png",
                        "isPro": false,
                        "fullname": "Tianyu He",
                        "user": "deeptimhe",
                        "type": "user"
                    },
                    "name": "Tianyu He",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-17T16:20:47.708Z",
                    "hidden": false
                },
                {
                    "_id": "666fe9a81f975c1f86db1d54",
                    "name": "Zhibo Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-14T15:19:21.000Z",
            "title": "GaussianSR: 3D Gaussian Super-Resolution with 2D Diffusion Priors",
            "summary": "Achieving high-resolution novel view synthesis (HRNVS) from low-resolution\ninput views is a challenging task due to the lack of high-resolution data.\nPrevious methods optimize high-resolution Neural Radiance Field (NeRF) from\nlow-resolution input views but suffer from slow rendering speed. In this work,\nwe base our method on 3D Gaussian Splatting (3DGS) due to its capability of\nproducing high-quality images at a faster rendering speed. To alleviate the\nshortage of data for higher-resolution synthesis, we propose to leverage\noff-the-shelf 2D diffusion priors by distilling the 2D knowledge into 3D with\nScore Distillation Sampling (SDS). Nevertheless, applying SDS directly to\nGaussian-based 3D super-resolution leads to undesirable and redundant 3D\nGaussian primitives, due to the randomness brought by generative priors. To\nmitigate this issue, we introduce two simple yet effective techniques to reduce\nstochastic disturbances introduced by SDS. Specifically, we 1) shrink the range\nof diffusion timestep in SDS with an annealing strategy; 2) randomly discard\nredundant Gaussian primitives during densification. Extensive experiments have\ndemonstrated that our proposed GaussainSR can attain high-quality results for\nHRNVS with only low-resolution inputs on both synthetic and real-world\ndatasets. Project page: https://chchnii.github.io/GaussianSR/",
            "upvotes": 2
        },
        "publishedAt": "2024-06-17T07:09:15.105Z",
        "title": "GaussianSR: 3D Gaussian Super-Resolution with 2D Diffusion Priors",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.10111.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661052258980-619b7b1cab4c7b7f16a7d59e.png",
            "fullname": "Tianyu He",
            "name": "deeptimhe",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    }
]