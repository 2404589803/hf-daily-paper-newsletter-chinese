[
    {
        "paper": {
            "id": "2410.23090",
            "authors": [
                {
                    "_id": "6722ddfb2b7a133fcb7a3978",
                    "user": {
                        "_id": "6530eba73572cd4fb78daf36",
                        "avatarUrl": "/avatars/4e6f0f07bb4e0ed9e3c58d49eaf78fe5.svg",
                        "isPro": false,
                        "fullname": "Yiruo Cheng",
                        "user": "ariya2357",
                        "type": "user"
                    },
                    "name": "Yiruo Cheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-31T09:54:45.558Z",
                    "hidden": false
                },
                {
                    "_id": "6722ddfb2b7a133fcb7a3979",
                    "name": "Kelong Mao",
                    "hidden": false
                },
                {
                    "_id": "6722ddfb2b7a133fcb7a397a",
                    "name": "Ziliang Zhao",
                    "hidden": false
                },
                {
                    "_id": "6722ddfb2b7a133fcb7a397b",
                    "user": {
                        "_id": "61cd4b833dd34ba1985e0753",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61cd4b833dd34ba1985e0753/BfHfrwotoMESpXZOHiIe4.png",
                        "isPro": false,
                        "fullname": "KABI",
                        "user": "dongguanting",
                        "type": "user"
                    },
                    "name": "Guanting Dong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-31T09:54:42.575Z",
                    "hidden": false
                },
                {
                    "_id": "6722ddfb2b7a133fcb7a397c",
                    "name": "Hongjin Qian",
                    "hidden": false
                },
                {
                    "_id": "6722ddfb2b7a133fcb7a397d",
                    "name": "Yongkang Wu",
                    "hidden": false
                },
                {
                    "_id": "6722ddfb2b7a133fcb7a397e",
                    "name": "Tetsuya Sakai",
                    "hidden": false
                },
                {
                    "_id": "6722ddfb2b7a133fcb7a397f",
                    "name": "Ji-Rong Wen",
                    "hidden": false
                },
                {
                    "_id": "6722ddfb2b7a133fcb7a3980",
                    "name": "Zhicheng Dou",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-30T15:06:32.000Z",
            "title": "CORAL: Benchmarking Multi-turn Conversational Retrieval-Augmentation\n  Generation",
            "summary": "Retrieval-Augmented Generation (RAG) has become a powerful paradigm for\nenhancing large language models (LLMs) through external knowledge retrieval.\nDespite its widespread attention, existing academic research predominantly\nfocuses on single-turn RAG, leaving a significant gap in addressing the\ncomplexities of multi-turn conversations found in real-world applications. To\nbridge this gap, we introduce CORAL, a large-scale benchmark designed to assess\nRAG systems in realistic multi-turn conversational settings. CORAL includes\ndiverse information-seeking conversations automatically derived from Wikipedia\nand tackles key challenges such as open-domain coverage, knowledge intensity,\nfree-form responses, and topic shifts. It supports three core tasks of\nconversational RAG: passage retrieval, response generation, and citation\nlabeling. We propose a unified framework to standardize various conversational\nRAG methods and conduct a comprehensive evaluation of these methods on CORAL,\ndemonstrating substantial opportunities for improving existing approaches.",
            "upvotes": 43,
            "discussionId": "6722ddfb2b7a133fcb7a39d0"
        },
        "publishedAt": "2024-10-31T01:48:44.889Z",
        "title": "CORAL: Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.23090.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61cd4b833dd34ba1985e0753/BfHfrwotoMESpXZOHiIe4.png",
            "fullname": "KABI",
            "name": "dongguanting",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 9
        }
    },
    {
        "paper": {
            "id": "2410.22391",
            "authors": [
                {
                    "_id": "67233b27548a7844da459d6d",
                    "name": "Thomas Schmied",
                    "hidden": false
                },
                {
                    "_id": "67233b27548a7844da459d6e",
                    "name": "Thomas Adler",
                    "hidden": false
                },
                {
                    "_id": "67233b27548a7844da459d6f",
                    "name": "Vihang Patil",
                    "hidden": false
                },
                {
                    "_id": "67233b27548a7844da459d70",
                    "name": "Maximilian Beck",
                    "hidden": false
                },
                {
                    "_id": "67233b27548a7844da459d71",
                    "name": "Korbinian Pöppel",
                    "hidden": false
                },
                {
                    "_id": "67233b27548a7844da459d72",
                    "name": "Johannes Brandstetter",
                    "hidden": false
                },
                {
                    "_id": "67233b27548a7844da459d73",
                    "name": "Günter Klambauer",
                    "hidden": false
                },
                {
                    "_id": "67233b27548a7844da459d74",
                    "name": "Razvan Pascanu",
                    "hidden": false
                },
                {
                    "_id": "67233b27548a7844da459d75",
                    "name": "Sepp Hochreiter",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-29T17:55:47.000Z",
            "title": "A Large Recurrent Action Model: xLSTM enables Fast Inference for\n  Robotics Tasks",
            "summary": "In recent years, there has been a trend in the field of Reinforcement\nLearning (RL) towards large action models trained offline on large-scale\ndatasets via sequence modeling. Existing models are primarily based on the\nTransformer architecture, which result in powerful agents. However, due to slow\ninference times, Transformer-based approaches are impractical for real-time\napplications, such as robotics. Recently, modern recurrent architectures, such\nas xLSTM and Mamba, have been proposed that exhibit parallelization benefits\nduring training similar to the Transformer architecture while offering fast\ninference. In this work, we study the aptitude of these modern recurrent\narchitectures for large action models. Consequently, we propose a Large\nRecurrent Action Model (LRAM) with an xLSTM at its core that comes with\nlinear-time inference complexity and natural sequence length extrapolation\nabilities. Experiments on 432 tasks from 6 domains show that LRAM compares\nfavorably to Transformers in terms of performance and speed.",
            "upvotes": 11,
            "discussionId": "67233b28548a7844da459e37"
        },
        "publishedAt": "2024-10-31T06:40:34.269Z",
        "title": "A Large Recurrent Action Model: xLSTM enables Fast Inference for Robotics Tasks",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.22391.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/9f0c832d5b51b659c7bb83074f02a648.svg",
            "fullname": "Thomas Schmied",
            "name": "thomasschmied",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.20779",
            "authors": [
                {
                    "_id": "672287ddb915ce8c01cf78d7",
                    "user": {
                        "_id": "60ef001bed64a34082bfa0dd",
                        "avatarUrl": "/avatars/78e4daeac169edbf4dc42fbed9b50d59.svg",
                        "isPro": false,
                        "fullname": "Omer Shubi",
                        "user": "scaperex",
                        "type": "user"
                    },
                    "name": "Omer Shubi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-31T09:54:50.442Z",
                    "hidden": false
                },
                {
                    "_id": "672287ddb915ce8c01cf78d8",
                    "name": "Cfir Avraham Hadar",
                    "hidden": false
                },
                {
                    "_id": "672287ddb915ce8c01cf78d9",
                    "name": "Yevgeni Berzak",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-28T06:40:03.000Z",
            "title": "Decoding Reading Goals from Eye Movements",
            "summary": "Readers can have different goals with respect to the text they are reading.\nCan these goals be decoded from the pattern of their eye movements over the\ntext? In this work, we examine for the first time whether it is possible to\ndecode two types of reading goals that are common in daily life: information\nseeking and ordinary reading. Using large scale eye-tracking data, we apply to\nthis task a wide range of state-of-the-art models for eye movements and text\nthat cover different architectural and data representation strategies, and\nfurther introduce a new model ensemble. We systematically evaluate these models\nat three levels of generalization: new textual item, new participant, and the\ncombination of both. We find that eye movements contain highly valuable signals\nfor this task. We further perform an error analysis which builds on prior\nempirical findings on differences between ordinary reading and information\nseeking and leverages rich textual annotations. This analysis reveals key\nproperties of textual items and participant eye movements that contribute to\nthe difficulty of the task.",
            "upvotes": 8,
            "discussionId": "672287deb915ce8c01cf790c"
        },
        "publishedAt": "2024-10-31T14:43:49.692Z",
        "title": "Decoding Reading Goals from Eye Movements",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.20779.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/78e4daeac169edbf4dc42fbed9b50d59.svg",
            "fullname": "Omer Shubi",
            "name": "scaperex",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        }
    },
    {
        "paper": {
            "id": "2410.23287",
            "authors": [
                {
                    "_id": "6722e182440fd7dab3f28195",
                    "user": {
                        "_id": "661dfccdd8427174a890910b",
                        "avatarUrl": "/avatars/acf4310a159013ae440edd30632ce543.svg",
                        "isPro": false,
                        "fullname": "Anurag Bagchi",
                        "user": "anuragba",
                        "type": "user"
                    },
                    "name": "Anurag Bagchi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-31T09:54:38.370Z",
                    "hidden": false
                },
                {
                    "_id": "6722e182440fd7dab3f28196",
                    "user": {
                        "_id": "65cedb3e6712e2451ee14ac2",
                        "avatarUrl": "/avatars/c34c8c71e7a45180bbf1684ad5b3f23c.svg",
                        "isPro": false,
                        "fullname": "Zhipeng Bao",
                        "user": "bzp15",
                        "type": "user"
                    },
                    "name": "Zhipeng Bao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-31T09:54:35.841Z",
                    "hidden": false
                },
                {
                    "_id": "6722e182440fd7dab3f28197",
                    "name": "Yu-Xiong Wang",
                    "hidden": false
                },
                {
                    "_id": "6722e182440fd7dab3f28198",
                    "user": {
                        "_id": "65aeab2201b6f61e24323339",
                        "avatarUrl": "/avatars/2c119173c0871f59df108f9bb4471954.svg",
                        "isPro": false,
                        "fullname": "Pavel Tokmakov",
                        "user": "ptokmakov",
                        "type": "user"
                    },
                    "name": "Pavel Tokmakov",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2024-10-31T01:57:00.271Z",
                    "hidden": false
                },
                {
                    "_id": "6722e182440fd7dab3f28199",
                    "name": "Martial Hebert",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-30T17:59:26.000Z",
            "title": "ReferEverything: Towards Segmenting Everything We Can Speak of in Videos",
            "summary": "We present REM, a framework for segmenting a wide range of concepts in video\nthat can be described through natural language. Our method capitalizes on\nvisual-language representations learned by video diffusion models on\nInternet-scale datasets. A key insight of our approach is preserving as much of\nthe generative model's original representation as possible, while fine-tuning\nit on narrow-domain Referral Object Segmentation datasets. As a result, our\nframework can accurately segment and track rare and unseen objects, despite\nbeing trained on object masks from a limited set of categories. Additionally,\nit can generalize to non-object dynamic concepts, such as waves crashing in the\nocean, as demonstrated in our newly introduced benchmark for Referral Video\nProcess Segmentation (Ref-VPS). Our experiments show that REM performs on par\nwith state-of-the-art approaches on in-domain datasets, like Ref-DAVIS, while\noutperforming them by up to twelve points in terms of region similarity on\nout-of-domain data, leveraging the power of Internet-scale pre-training.",
            "upvotes": 8,
            "discussionId": "6722e184440fd7dab3f28247"
        },
        "publishedAt": "2024-10-31T14:24:01.740Z",
        "title": "ReferEverything: Towards Segmenting Everything We Can Speak of in Videos",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.23287.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "/avatars/c34c8c71e7a45180bbf1684ad5b3f23c.svg",
            "fullname": "Zhipeng Bao",
            "name": "bzp15",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.22884",
            "authors": [
                {
                    "_id": "672340ae85f9fe93ec5b20be",
                    "name": "Itay Yona",
                    "hidden": false
                },
                {
                    "_id": "672340ae85f9fe93ec5b20bf",
                    "name": "Ilia Shumailov",
                    "hidden": false
                },
                {
                    "_id": "672340ae85f9fe93ec5b20c0",
                    "name": "Jamie Hayes",
                    "hidden": false
                },
                {
                    "_id": "672340ae85f9fe93ec5b20c1",
                    "name": "Nicholas Carlini",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-30T10:25:35.000Z",
            "title": "Stealing User Prompts from Mixture of Experts",
            "summary": "Mixture-of-Experts (MoE) models improve the efficiency and scalability of\ndense language models by routing each token to a small number of experts in\neach layer. In this paper, we show how an adversary that can arrange for their\nqueries to appear in the same batch of examples as a victim's queries can\nexploit Expert-Choice-Routing to fully disclose a victim's prompt. We\nsuccessfully demonstrate the effectiveness of this attack on a two-layer\nMixtral model, exploiting the tie-handling behavior of the torch.topk CUDA\nimplementation. Our results show that we can extract the entire prompt using\nO({VM}^2) queries (with vocabulary size V and prompt length M) or 100\nqueries on average per token in the setting we consider. This is the first\nattack to exploit architectural flaws for the purpose of extracting user\nprompts, introducing a new class of LLM vulnerabilities.",
            "upvotes": 5,
            "discussionId": "672340af85f9fe93ec5b2150"
        },
        "publishedAt": "2024-10-31T07:03:18.826Z",
        "title": "Stealing User Prompts from Mixture of Experts",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.22884.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/db428715dfd2239df2aeaaff1282323f.svg",
            "fullname": "i",
            "name": "iliashum",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.20050",
            "authors": [
                {
                    "_id": "6722e59578218659f678a559",
                    "name": "Lei Li",
                    "hidden": false
                },
                {
                    "_id": "6722e59578218659f678a55a",
                    "name": "Xiangxu Zhang",
                    "hidden": false
                },
                {
                    "_id": "6722e59578218659f678a55b",
                    "name": "Xiao Zhou",
                    "hidden": false
                },
                {
                    "_id": "6722e59578218659f678a55c",
                    "user": {
                        "_id": "64a38c590111d5ff6c3d5f2b",
                        "avatarUrl": "/avatars/ef13dc7ce243819bc0da9b04e778b432.svg",
                        "isPro": false,
                        "fullname": "zhengliu",
                        "user": "zl101",
                        "type": "user"
                    },
                    "name": "Zheng Liu",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2024-10-31T02:04:07.533Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-26T02:53:20.000Z",
            "title": "AutoMIR: Effective Zero-Shot Medical Information Retrieval without\n  Relevance Labels",
            "summary": "Medical information retrieval (MIR) is essential for retrieving relevant\nmedical knowledge from diverse sources, including electronic health records,\nscientific literature, and medical databases. However, achieving effective\nzero-shot dense retrieval in the medical domain poses substantial challenges\ndue to the lack of relevance-labeled data. In this paper, we introduce a novel\napproach called Self-Learning Hypothetical Document Embeddings (SL-HyDE) to\ntackle this issue. SL-HyDE leverages large language models (LLMs) as generators\nto generate hypothetical documents based on a given query. These generated\ndocuments encapsulate key medical context, guiding a dense retriever in\nidentifying the most relevant documents. The self-learning framework\nprogressively refines both pseudo-document generation and retrieval, utilizing\nunlabeled medical corpora without requiring any relevance-labeled data.\nAdditionally, we present the Chinese Medical Information Retrieval Benchmark\n(CMIRB), a comprehensive evaluation framework grounded in real-world medical\nscenarios, encompassing five tasks and ten datasets. By benchmarking ten models\non CMIRB, we establish a rigorous standard for evaluating medical information\nretrieval systems. Experimental results demonstrate that SL-HyDE significantly\nsurpasses existing methods in retrieval accuracy while showcasing strong\ngeneralization and scalability across various LLM and retriever configurations.\nCMIRB data and evaluation code are publicly available at:\nhttps://github.com/CMIRB-benchmark/CMIRB.",
            "upvotes": 5,
            "discussionId": "6722e59778218659f678a5c8"
        },
        "publishedAt": "2024-10-31T00:46:54.385Z",
        "title": "AutoMIR: Effective Zero-Shot Medical Information Retrieval without Relevance Labels",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.20050.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/bef422f5b0ea241996196b2bf0f9ca8d.svg",
            "fullname": "Lei Li",
            "name": "GriffinLei",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 3
        }
    },
    {
        "paper": {
            "id": "2410.23168",
            "authors": [
                {
                    "_id": "6722db7db23a243a937ee098",
                    "user": {
                        "_id": "65f43c3cc9940817caaf4434",
                        "avatarUrl": "/avatars/ecec2856ba7a7d3421a2071a0a88800b.svg",
                        "isPro": false,
                        "fullname": "Haiyang Wang",
                        "user": "Haiyang-W",
                        "type": "user"
                    },
                    "name": "Haiyang Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-31T14:06:06.262Z",
                    "hidden": false
                },
                {
                    "_id": "6722db7db23a243a937ee099",
                    "name": "Yue Fan",
                    "hidden": false
                },
                {
                    "_id": "6722db7db23a243a937ee09a",
                    "name": "Muhammad Ferjad Naeem",
                    "hidden": false
                },
                {
                    "_id": "6722db7db23a243a937ee09b",
                    "name": "Yongqin Xian",
                    "hidden": false
                },
                {
                    "_id": "6722db7db23a243a937ee09c",
                    "name": "Jan Eric Lenssen",
                    "hidden": false
                },
                {
                    "_id": "6722db7db23a243a937ee09d",
                    "name": "Liwei Wang",
                    "hidden": false
                },
                {
                    "_id": "6722db7db23a243a937ee09e",
                    "name": "Federico Tombari",
                    "hidden": false
                },
                {
                    "_id": "6722db7db23a243a937ee09f",
                    "name": "Bernt Schiele",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-30T16:19:00.000Z",
            "title": "TokenFormer: Rethinking Transformer Scaling with Tokenized Model\n  Parameters",
            "summary": "Transformers have become the predominant architecture in foundation models\ndue to their excellent performance across various domains. However, the\nsubstantial cost of scaling these models remains a significant concern. This\nproblem arises primarily from their dependence on a fixed number of parameters\nwithin linear projections. When architectural modifications (e.g., channel\ndimensions) are introduced, the entire model typically requires retraining from\nscratch. As model sizes continue growing, this strategy results in increasingly\nhigh computational costs and becomes unsustainable. To overcome this problem,\nwe introduce TokenFormer, a natively scalable architecture that leverages the\nattention mechanism not only for computations among input tokens but also for\ninteractions between tokens and model parameters, thereby enhancing\narchitectural flexibility. By treating model parameters as tokens, we replace\nall the linear projections in Transformers with our token-parameter attention\nlayer, where input tokens act as queries and model parameters as keys and\nvalues. This reformulation allows for progressive and efficient scaling without\nnecessitating retraining from scratch. Our model scales from 124M to 1.4B\nparameters by incrementally adding new key-value parameter pairs, achieving\nperformance comparable to Transformers trained from scratch while greatly\nreducing training costs. Code and models are available at\nhttps://github.com/Haiyang-W/TokenFormer.",
            "upvotes": 4,
            "discussionId": "6722db7eb23a243a937ee0e4"
        },
        "publishedAt": "2024-10-31T09:59:54.485Z",
        "title": "TokenFormer: Rethinking Transformer Scaling with Tokenized Model Parameters",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.23168.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/ecec2856ba7a7d3421a2071a0a88800b.svg",
            "fullname": "Haiyang Wang",
            "name": "Haiyang-W",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 2
        }
    },
    {
        "paper": {
            "id": "2410.22587",
            "authors": [
                {
                    "_id": "6722e83247de1a565f91c768",
                    "user": {
                        "_id": "6384d424ac61472e5e92d403",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6384d424ac61472e5e92d403/ECs_z-ULNlKbOMiXJdJNe.jpeg",
                        "isPro": false,
                        "fullname": "Catherine Arnett",
                        "user": "catherinearnett",
                        "type": "user"
                    },
                    "name": "Catherine Arnett",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-31T14:06:04.601Z",
                    "hidden": false
                },
                {
                    "_id": "6722e83247de1a565f91c769",
                    "user": {
                        "_id": "66214936126ebc0deb501a87",
                        "avatarUrl": "/avatars/e038e84f58c8577425df327a06b801d0.svg",
                        "isPro": false,
                        "fullname": "Eliot Jones",
                        "user": "eliotj",
                        "type": "user"
                    },
                    "name": "Eliot Jones",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2024-10-31T02:18:38.592Z",
                    "hidden": false
                },
                {
                    "_id": "6722e83247de1a565f91c76a",
                    "user": {
                        "_id": "6723a215f60c084a7dd016b8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/5z5Ap4AFQhCkMJz3oOXCU.png",
                        "isPro": false,
                        "fullname": "Prof. Ivan Yamshchikov",
                        "user": "ivan-the-bearable",
                        "type": "user"
                    },
                    "name": "Ivan P. Yamshchikov",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2024-10-31T15:30:07.401Z",
                    "hidden": false
                },
                {
                    "_id": "6722e83247de1a565f91c76b",
                    "name": "Pierre-Carl Langlais",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-29T23:00:05.000Z",
            "title": "Toxicity of the Commons: Curating Open-Source Pre-Training Data",
            "summary": "Open-source large language models are becoming increasingly available and\npopular among researchers and practitioners. While significant progress has\nbeen made on open-weight models, open training data is a practice yet to be\nadopted by the leading open-weight models creators. At the same time, there\nresearchers are working to make language models safer. We propose a data\ncuration pipeline to reduce harmful outputs by models trained on public domain\ndata. There are unique challenges to working with public domain data, as these\nsources differ from web text in both form and content. Many sources are\nhistorical documents and are the result of Optical Character Recognition (OCR).\nConsequently, current state-of-the-art approaches to toxicity filtering are\noften infeasible or inappropriate for open data models. In this paper, we\nintroduce a new fully open-source pipeline for open-data toxicity filtering.\nOur contributions are threefold. We create a custom training dataset,\nToxicCommons, which is composed of texts which have been classified across five\ndifferent dimensions (racial/origin-based, gender/sex-based, religious,\nability-based discrimination, and violence). We use this dataset to train a\ncustom classifier, Celadon, that can be used to detect toxic content in open\ndata more efficiently at a larger scale. Finally, we describe the balanced\napproach to content filtration that optimizes safety filtering with respect to\nthe filtered data available for training.",
            "upvotes": 2,
            "discussionId": "6722e83247de1a565f91c7a7"
        },
        "publishedAt": "2024-10-31T12:27:05.817Z",
        "title": "Toxicity of the Commons: Curating Open-Source Pre-Training Data",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.22587.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6384d424ac61472e5e92d403/ECs_z-ULNlKbOMiXJdJNe.jpeg",
            "fullname": "Catherine Arnett",
            "name": "catherinearnett",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 10
        }
    }
]