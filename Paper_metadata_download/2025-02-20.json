[
  {
    "paper": {
      "id": "2502.13923",
      "authors": [
        {
          "_id": "67b6b0688b56622e70b9e83e",
          "name": "Shuai Bai",
          "hidden": false
        },
        {
          "_id": "67b6b0688b56622e70b9e83f",
          "name": "Keqin Chen",
          "hidden": false
        },
        {
          "_id": "67b6b0688b56622e70b9e840",
          "name": "Xuejing Liu",
          "hidden": false
        },
        {
          "_id": "67b6b0688b56622e70b9e841",
          "name": "Jialin Wang",
          "hidden": false
        },
        {
          "_id": "67b6b0688b56622e70b9e842",
          "name": "Wenbin Ge",
          "hidden": false
        },
        {
          "_id": "67b6b0688b56622e70b9e843",
          "name": "Sibo Song",
          "hidden": false
        },
        {
          "_id": "67b6b0688b56622e70b9e844",
          "name": "Kai Dang",
          "hidden": false
        },
        {
          "_id": "67b6b0688b56622e70b9e845",
          "name": "Peng Wang",
          "hidden": false
        },
        {
          "_id": "67b6b0688b56622e70b9e846",
          "name": "Shijie Wang",
          "hidden": false
        },
        {
          "_id": "67b6b0688b56622e70b9e847",
          "name": "Jun Tang",
          "hidden": false
        },
        {
          "_id": "67b6b0688b56622e70b9e848",
          "name": "Humen Zhong",
          "hidden": false
        },
        {
          "_id": "67b6b0688b56622e70b9e849",
          "name": "Yuanzhi Zhu",
          "hidden": false
        },
        {
          "_id": "67b6b0688b56622e70b9e84a",
          "name": "Mingkun Yang",
          "hidden": false
        },
        {
          "_id": "67b6b0688b56622e70b9e84b",
          "name": "Zhaohai Li",
          "hidden": false
        },
        {
          "_id": "67b6b0688b56622e70b9e84c",
          "name": "Jianqiang Wan",
          "hidden": false
        },
        {
          "_id": "67b6b0688b56622e70b9e84d",
          "name": "Pengfei Wang",
          "hidden": false
        },
        {
          "_id": "67b6b0688b56622e70b9e84e",
          "name": "Wei Ding",
          "hidden": false
        },
        {
          "_id": "67b6b0688b56622e70b9e84f",
          "name": "Zheren Fu",
          "hidden": false
        },
        {
          "_id": "67b6b0688b56622e70b9e850",
          "name": "Yiheng Xu",
          "hidden": false
        },
        {
          "_id": "67b6b0688b56622e70b9e851",
          "name": "Jiabo Ye",
          "hidden": false
        },
        {
          "_id": "67b6b0688b56622e70b9e852",
          "name": "Xi Zhang",
          "hidden": false
        },
        {
          "_id": "67b6b0688b56622e70b9e853",
          "name": "Tianbao Xie",
          "hidden": false
        },
        {
          "_id": "67b6b0688b56622e70b9e854",
          "name": "Zesen Cheng",
          "hidden": false
        },
        {
          "_id": "67b6b0688b56622e70b9e855",
          "name": "Hang Zhang",
          "hidden": false
        },
        {
          "_id": "67b6b0688b56622e70b9e856",
          "name": "Zhibo Yang",
          "hidden": false
        },
        {
          "_id": "67b6b0688b56622e70b9e857",
          "name": "Haiyang Xu",
          "hidden": false
        },
        {
          "_id": "67b6b0688b56622e70b9e858",
          "name": "Junyang Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-19T18:00:14.000Z",
      "title": "Qwen2.5-VL Technical Report",
      "summary": "We introduce Qwen2.5-VL, the latest flagship model of Qwen vision-language\nseries, which demonstrates significant advancements in both foundational\ncapabilities and innovative functionalities. Qwen2.5-VL achieves a major leap\nforward in understanding and interacting with the world through enhanced visual\nrecognition, precise object localization, robust document parsing, and\nlong-video comprehension. A standout feature of Qwen2.5-VL is its ability to\nlocalize objects using bounding boxes or points accurately. It provides robust\nstructured data extraction from invoices, forms, and tables, as well as\ndetailed analysis of charts, diagrams, and layouts. To handle complex inputs,\nQwen2.5-VL introduces dynamic resolution processing and absolute time encoding,\nenabling it to process images of varying sizes and videos of extended durations\n(up to hours) with second-level event localization. This allows the model to\nnatively perceive spatial scales and temporal dynamics without relying on\ntraditional normalization techniques. By training a native dynamic-resolution\nVision Transformer (ViT) from scratch and incorporating Window Attention, we\nreduce computational overhead while maintaining native resolution. As a result,\nQwen2.5-VL excels not only in static image and document understanding but also\nas an interactive visual agent capable of reasoning, tool usage, and task\nexecution in real-world scenarios such as operating computers and mobile\ndevices. Qwen2.5-VL is available in three sizes, addressing diverse use cases\nfrom edge AI to high-performance computing. The flagship Qwen2.5-VL-72B model\nmatches state-of-the-art models like GPT-4o and Claude 3.5 Sonnet, particularly\nexcelling in document and diagram understanding. Additionally, Qwen2.5-VL\nmaintains robust linguistic performance, preserving the core language\ncompetencies of the Qwen2.5 LLM.",
      "upvotes": 32,
      "discussionId": "67b6b0688b56622e70b9e875"
    },
    "publishedAt": "2025-02-19T23:35:06.194Z",
    "title": "Qwen2.5-VL Technical Report",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13923.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63451cf0a05b51f7ded25505",
      "avatarUrl": "/avatars/dec4bbee4a82b773fc58dfc2dce9dbeb.svg",
      "fullname": "shuai bai",
      "name": "bluelike",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.13144",
      "authors": [
        {
          "_id": "67b55c7fba22c1ddbb8d5746",
          "user": {
            "_id": "6536187bd34e9f02b9df1c3b",
            "avatarUrl": "/avatars/0b34d62868b93053b0a05062a018b5bd.svg",
            "isPro": false,
            "fullname": "Hao Gao",
            "user": "Hao605",
            "type": "user"
          },
          "name": "Hao Gao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:00:48.944Z",
          "hidden": false
        },
        {
          "_id": "67b55c7fba22c1ddbb8d5747",
          "name": "Shaoyu Chen",
          "hidden": false
        },
        {
          "_id": "67b55c7fba22c1ddbb8d5748",
          "name": "Bo Jiang",
          "hidden": false
        },
        {
          "_id": "67b55c7fba22c1ddbb8d5749",
          "name": "Bencheng Liao",
          "hidden": false
        },
        {
          "_id": "67b55c7fba22c1ddbb8d574a",
          "name": "Yiang Shi",
          "hidden": false
        },
        {
          "_id": "67b55c7fba22c1ddbb8d574b",
          "name": "Xiaoyang Guo",
          "hidden": false
        },
        {
          "_id": "67b55c7fba22c1ddbb8d574c",
          "name": "Yuechuan Pu",
          "hidden": false
        },
        {
          "_id": "67b55c7fba22c1ddbb8d574d",
          "name": "Haoran Yin",
          "hidden": false
        },
        {
          "_id": "67b55c7fba22c1ddbb8d574e",
          "name": "Xiangyu Li",
          "hidden": false
        },
        {
          "_id": "67b55c7fba22c1ddbb8d574f",
          "name": "Xinbang Zhang",
          "hidden": false
        },
        {
          "_id": "67b55c7fba22c1ddbb8d5750",
          "name": "Ying Zhang",
          "hidden": false
        },
        {
          "_id": "67b55c7fba22c1ddbb8d5751",
          "name": "Wenyu Liu",
          "hidden": false
        },
        {
          "_id": "67b55c7fba22c1ddbb8d5752",
          "name": "Qian Zhang",
          "hidden": false
        },
        {
          "_id": "67b55c7fba22c1ddbb8d5753",
          "name": "Xinggang Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T18:59:21.000Z",
      "title": "RAD: Training an End-to-End Driving Policy via Large-Scale 3DGS-based\n  Reinforcement Learning",
      "summary": "Existing end-to-end autonomous driving (AD) algorithms typically follow the\nImitation Learning (IL) paradigm, which faces challenges such as causal\nconfusion and the open-loop gap. In this work, we establish a 3DGS-based\nclosed-loop Reinforcement Learning (RL) training paradigm. By leveraging 3DGS\ntechniques, we construct a photorealistic digital replica of the real physical\nworld, enabling the AD policy to extensively explore the state space and learn\nto handle out-of-distribution scenarios through large-scale trial and error. To\nenhance safety, we design specialized rewards that guide the policy to\neffectively respond to safety-critical events and understand real-world causal\nrelationships. For better alignment with human driving behavior, IL is\nincorporated into RL training as a regularization term. We introduce a\nclosed-loop evaluation benchmark consisting of diverse, previously unseen 3DGS\nenvironments. Compared to IL-based methods, RAD achieves stronger performance\nin most closed-loop metrics, especially 3x lower collision rate. Abundant\nclosed-loop results are presented at https://hgao-cv.github.io/RAD.",
      "upvotes": 22,
      "discussionId": "67b55c80ba22c1ddbb8d579c"
    },
    "publishedAt": "2025-02-19T22:13:49.764Z",
    "title": "RAD: Training an End-to-End Driving Policy via Large-Scale 3DGS-based Reinforcement Learning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13144.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6536187bd34e9f02b9df1c3b",
      "avatarUrl": "/avatars/0b34d62868b93053b0a05062a018b5bd.svg",
      "fullname": "Hao Gao",
      "name": "Hao605",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.13128",
      "authors": [
        {
          "_id": "67b6c696e9b901edeaf320d5",
          "name": "Zihan Liu",
          "hidden": false
        },
        {
          "_id": "67b6c696e9b901edeaf320d6",
          "name": "Shuangrui Ding",
          "hidden": false
        },
        {
          "_id": "67b6c696e9b901edeaf320d7",
          "name": "Zhixiong Zhang",
          "hidden": false
        },
        {
          "_id": "67b6c696e9b901edeaf320d8",
          "name": "Xiaoyi Dong",
          "hidden": false
        },
        {
          "_id": "67b6c696e9b901edeaf320d9",
          "name": "Pan Zhang",
          "hidden": false
        },
        {
          "_id": "67b6c696e9b901edeaf320da",
          "name": "Yuhang Zang",
          "hidden": false
        },
        {
          "_id": "67b6c696e9b901edeaf320db",
          "name": "Yuhang Cao",
          "hidden": false
        },
        {
          "_id": "67b6c696e9b901edeaf320dc",
          "name": "Dahua Lin",
          "hidden": false
        },
        {
          "_id": "67b6c696e9b901edeaf320dd",
          "name": "Jiaqi Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T18:52:21.000Z",
      "title": "SongGen: A Single Stage Auto-regressive Transformer for Text-to-Song\n  Generation",
      "summary": "Text-to-song generation, the task of creating vocals and accompaniment from\ntextual inputs, poses significant challenges due to domain complexity and data\nscarcity. Existing approaches often employ multi-stage generation procedures,\nresulting in cumbersome training and inference pipelines. In this paper, we\npropose SongGen, a fully open-source, single-stage auto-regressive transformer\ndesigned for controllable song generation. The proposed model facilitates\nfine-grained control over diverse musical attributes, including lyrics and\ntextual descriptions of instrumentation, genre, mood, and timbre, while also\noffering an optional three-second reference clip for voice cloning. Within a\nunified auto-regressive framework, SongGen supports two output modes: mixed\nmode, which generates a mixture of vocals and accompaniment directly, and\ndual-track mode, which synthesizes them separately for greater flexibility in\ndownstream applications. We explore diverse token pattern strategies for each\nmode, leading to notable improvements and valuable insights. Furthermore, we\ndesign an automated data preprocessing pipeline with effective quality control.\nTo foster community engagement and future research, we will release our model\nweights, training code, annotated data, and preprocessing pipeline. The\ngenerated samples are showcased on our project page at\nhttps://liuzh-19.github.io/SongGen/ , and the code will be available at\nhttps://github.com/LiuZH-19/SongGen .",
      "upvotes": 16,
      "discussionId": "67b6c698e9b901edeaf321a7"
    },
    "publishedAt": "2025-02-20T01:07:44.785Z",
    "title": "SongGen: A Single Stage Auto-regressive Transformer for Text-to-Song Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13128.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b4eec4faa3181a5eab9c46",
      "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
      "fullname": "Jiaqi Wang",
      "name": "myownskyW7",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 16
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.13922",
      "authors": [
        {
          "_id": "67b6948dbef24bad725b5d4b",
          "name": "Guanzheng Chen",
          "hidden": false
        },
        {
          "_id": "67b6948dbef24bad725b5d4c",
          "name": "Xin Li",
          "hidden": false
        },
        {
          "_id": "67b6948dbef24bad725b5d4d",
          "name": "Michael Qizhe Shieh",
          "hidden": false
        },
        {
          "_id": "67b6948dbef24bad725b5d4e",
          "name": "Lidong Bing",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-19T17:59:03.000Z",
      "title": "LongPO: Long Context Self-Evolution of Large Language Models through\n  Short-to-Long Preference Optimization",
      "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities\nthrough pretraining and alignment. However, superior short-context LLMs may\nunderperform in long-context scenarios due to insufficient long-context\nalignment. This alignment process remains challenging due to the impracticality\nof human annotation for extended contexts and the difficulty in balancing\nshort- and long-context performance. To address these challenges, we introduce\nLongPO, that enables short-context LLMs to self-evolve to excel on long-context\ntasks by internally transferring short-context capabilities. LongPO harnesses\nLLMs to learn from self-generated short-to-long preference data, comprising\npaired responses generated for identical instructions with long-context inputs\nand their compressed short-context counterparts, respectively. This preference\nreveals capabilities and potentials of LLMs cultivated during short-context\nalignment that may be diminished in under-aligned long-context scenarios.\nAdditionally, LongPO incorporates a short-to-long KL constraint to mitigate\nshort-context performance decline during long-context alignment. When applied\nto Mistral-7B-Instruct-v0.2 from 128K to 512K context lengths, LongPO fully\nretains short-context performance and largely outperforms naive SFT and DPO in\nboth long- and short-context tasks. Specifically, \\ourMethod-trained models can\nachieve results on long-context benchmarks comparable to, or even surpassing,\nthose of superior LLMs (e.g., GPT-4-128K) that involve extensive long-context\nannotation and larger parameter scales.",
      "upvotes": 14,
      "discussionId": "67b6948ebef24bad725b5d84"
    },
    "publishedAt": "2025-02-19T21:35:20.931Z",
    "title": "LongPO: Long Context Self-Evolution of Large Language Models through Short-to-Long Preference Optimization",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13922.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645475e2548f22be59847604",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645475e2548f22be59847604/EhSurrZ25u31qQ2TVXQXt.jpeg",
      "fullname": "Chen",
      "name": "Guanzheng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.13347",
      "authors": [
        {
          "_id": "67b6a7e83ef3656c48f149b9",
          "name": "Shi Yu",
          "hidden": false
        },
        {
          "_id": "67b6a7e83ef3656c48f149ba",
          "name": "Zhiyuan Liu",
          "hidden": false
        },
        {
          "_id": "67b6a7e83ef3656c48f149bb",
          "name": "Chenyan Xiong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-19T00:31:43.000Z",
      "title": "Craw4LLM: Efficient Web Crawling for LLM Pretraining",
      "summary": "Web crawl is a main source of large language models' (LLMs) pretraining data,\nbut the majority of crawled web pages are discarded in pretraining due to low\ndata quality. This paper presents Crawl4LLM, an efficient web crawling method\nthat explores the web graph based on the preference of LLM pretraining.\nSpecifically, it leverages the influence of a webpage in LLM pretraining as the\npriority score of the web crawler's scheduler, replacing the standard graph\nconnectivity based priority. Our experiments on a web graph containing 900\nmillion webpages from a commercial search engine's index demonstrate the\nefficiency of Crawl4LLM in obtaining high-quality pretraining data. With just\n21% URLs crawled, LLMs pretrained on Crawl4LLM data reach the same downstream\nperformances of previous crawls, significantly reducing the crawling waste and\nalleviating the burdens on websites. Our code is publicly available at\nhttps://github.com/cxcscmu/Crawl4LLM.",
      "upvotes": 13,
      "discussionId": "67b6a7e93ef3656c48f149f1"
    },
    "publishedAt": "2025-02-19T22:57:23.298Z",
    "title": "Craw4LLM: Efficient Web Crawling for LLM Pretraining",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13347.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6135eeeb5bc6ecdf86b60f0d",
      "avatarUrl": "/avatars/43cedcf20ab6b0801a662787400e1384.svg",
      "fullname": "Shi Yu",
      "name": "yushi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.13685",
      "authors": [
        {
          "_id": "67b6dc1ba7567156c6547880",
          "name": "Jusen Du",
          "hidden": false
        },
        {
          "_id": "67b6dc1ba7567156c6547881",
          "user": {
            "_id": "6246bb33da617c00b48e4d92",
            "avatarUrl": "/avatars/0304a9f6eb7f5dee4d933d03222f94e9.svg",
            "isPro": false,
            "fullname": "Weigao Sun",
            "user": "weigao266",
            "type": "user"
          },
          "name": "Weigao Sun",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-20T07:39:08.547Z",
          "hidden": false
        },
        {
          "_id": "67b6dc1ba7567156c6547882",
          "name": "Disen Lan",
          "hidden": false
        },
        {
          "_id": "67b6dc1ba7567156c6547883",
          "name": "Jiaxi Hu",
          "hidden": false
        },
        {
          "_id": "67b6dc1ba7567156c6547884",
          "name": "Yu Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-19T12:53:55.000Z",
      "title": "MoM: Linear Sequence Modeling with Mixture-of-Memories",
      "summary": "Linear sequence modeling methods, such as linear attention, state space\nmodeling, and linear RNNs, offer significant efficiency improvements by\nreducing the complexity of training and inference. However, these methods\ntypically compress the entire input sequence into a single fixed-size memory\nstate, which leads to suboptimal performance on recall-intensive downstream\ntasks. Drawing inspiration from neuroscience, particularly the brain's ability\nto maintain robust long-term memory while mitigating \"memory interference\", we\nintroduce a novel architecture called Mixture-of-Memories (MoM). MoM utilizes\nmultiple independent memory states, with a router network directing input\ntokens to specific memory states. This approach greatly enhances the overall\nmemory capacity while minimizing memory interference. As a result, MoM performs\nexceptionally well on recall-intensive tasks, surpassing existing linear\nsequence modeling techniques. Despite incorporating multiple memory states, the\ncomputation of each memory state remains linear in complexity, allowing MoM to\nretain the linear-complexity advantage during training, while\nconstant-complexity during inference. Our experimental results show that MoM\nsignificantly outperforms current linear sequence models on downstream language\ntasks, particularly recall-intensive tasks, and even achieves performance\ncomparable to Transformer models. The code is released at\nhttps://github.com/OpenSparseLLMs/MoM and is also released as a part of\nhttps://github.com/OpenSparseLLMs/Linear-MoE.",
      "upvotes": 11,
      "discussionId": "67b6dc1ca7567156c65478b8"
    },
    "publishedAt": "2025-02-20T02:40:09.567Z",
    "title": "MoM: Linear Sequence Modeling with Mixture-of-Memories",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13685.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6246bb33da617c00b48e4d92",
      "avatarUrl": "/avatars/0304a9f6eb7f5dee4d933d03222f94e9.svg",
      "fullname": "Weigao Sun",
      "name": "weigao266",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.13965",
      "authors": [
        {
          "_id": "67b6a3fa09841367596a1db5",
          "name": "Michael Luo",
          "hidden": false
        },
        {
          "_id": "67b6a3fa09841367596a1db6",
          "name": "Xiaoxiang Shi",
          "hidden": false
        },
        {
          "_id": "67b6a3fa09841367596a1db7",
          "name": "Colin Cai",
          "hidden": false
        },
        {
          "_id": "67b6a3fa09841367596a1db8",
          "name": "Tianjun Zhang",
          "hidden": false
        },
        {
          "_id": "67b6a3fa09841367596a1db9",
          "name": "Justin Wong",
          "hidden": false
        },
        {
          "_id": "67b6a3fa09841367596a1dba",
          "name": "Yichuan Wang",
          "hidden": false
        },
        {
          "_id": "67b6a3fa09841367596a1dbb",
          "name": "Chi Wang",
          "hidden": false
        },
        {
          "_id": "67b6a3fa09841367596a1dbc",
          "name": "Yanping Huang",
          "hidden": false
        },
        {
          "_id": "67b6a3fa09841367596a1dbd",
          "name": "Zhifeng Chen",
          "hidden": false
        },
        {
          "_id": "67b6a3fa09841367596a1dbe",
          "name": "Joseph E. Gonzalez",
          "hidden": false
        },
        {
          "_id": "67b6a3fa09841367596a1dbf",
          "name": "Ion Stoica",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-19T18:59:30.000Z",
      "title": "Autellix: An Efficient Serving Engine for LLM Agents as General Programs",
      "summary": "Large language model (LLM) applications are evolving beyond simple chatbots\ninto dynamic, general-purpose agentic programs, which scale LLM calls and\noutput tokens to help AI agents reason, explore, and solve complex tasks.\nHowever, existing LLM serving systems ignore dependencies between programs and\ncalls, missing significant opportunities for optimization. Our analysis reveals\nthat programs submitted to LLM serving engines experience long cumulative wait\ntimes, primarily due to head-of-line blocking at both the individual LLM\nrequest and the program. To address this, we introduce Autellix, an LLM serving\nsystem that treats programs as first-class citizens to minimize their\nend-to-end latencies. Autellix intercepts LLM calls submitted by programs,\nenriching schedulers with program-level context. We propose two scheduling\nalgorithms-for single-threaded and distributed programs-that preempt and\nprioritize LLM calls based on their programs' previously completed calls. Our\nevaluation demonstrates that across diverse LLMs and agentic workloads,\nAutellix improves throughput of programs by 4-15x at the same latency compared\nto state-of-the-art systems, such as vLLM.",
      "upvotes": 9,
      "discussionId": "67b6a3fb09841367596a1e06"
    },
    "publishedAt": "2025-02-19T22:42:06.502Z",
    "title": "Autellix: An Efficient Serving Engine for LLM Agents as General Programs",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13965.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "654037be97949fd2304aab7f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654037be97949fd2304aab7f/2cSME81gcwYa2OTeVlq5Q.jpeg",
      "fullname": "Michael Luo",
      "name": "michaelzhiluo",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.12143",
      "authors": [
        {
          "_id": "67b4d05a9f8a8ab661450397",
          "name": "Yuetai Li",
          "hidden": false
        },
        {
          "_id": "67b4d05a9f8a8ab661450398",
          "name": "Xiang Yue",
          "hidden": false
        },
        {
          "_id": "67b4d05a9f8a8ab661450399",
          "name": "Zhangchen Xu",
          "hidden": false
        },
        {
          "_id": "67b4d05a9f8a8ab66145039a",
          "name": "Fengqing Jiang",
          "hidden": false
        },
        {
          "_id": "67b4d05a9f8a8ab66145039b",
          "name": "Luyao Niu",
          "hidden": false
        },
        {
          "_id": "67b4d05a9f8a8ab66145039c",
          "name": "Bill Yuchen Lin",
          "hidden": false
        },
        {
          "_id": "67b4d05a9f8a8ab66145039d",
          "name": "Bhaskar Ramasubramanian",
          "hidden": false
        },
        {
          "_id": "67b4d05a9f8a8ab66145039e",
          "name": "Radha Poovendran",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-17T18:56:15.000Z",
      "title": "Small Models Struggle to Learn from Strong Reasoners",
      "summary": "Large language models (LLMs) excel in complex reasoning tasks, and distilling\ntheir reasoning capabilities into smaller models has shown promise. However, we\nuncover an interesting phenomenon, which we term the Small Model Learnability\nGap: small models (leq3B parameters) do not consistently benefit from long\nchain-of-thought (CoT) reasoning or distillation from larger models. Instead,\nthey perform better when fine-tuned on shorter, simpler reasoning chains that\nbetter align with their intrinsic learning capacity. To address this, we\npropose Mix Distillation, a simple yet effective strategy that balances\nreasoning complexity by combining long and short CoT examples or reasoning from\nboth larger and smaller models. Our experiments demonstrate that Mix\nDistillation significantly improves small model reasoning performance compared\nto training on either data alone. These findings highlight the limitations of\ndirect strong model distillation and underscore the importance of adapting\nreasoning complexity for effective reasoning capability transfer.",
      "upvotes": 9,
      "discussionId": "67b4d05b9f8a8ab6614503cb"
    },
    "publishedAt": "2025-02-19T21:38:13.468Z",
    "title": "Small Models Struggle to Learn from Strong Reasoners",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12143.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "653df1323479e9ebbe3eb6cc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653df1323479e9ebbe3eb6cc/K_g-r1iMRNKj99LXPuYF3.jpeg",
      "fullname": "Zhangchen Xu",
      "name": "flydust",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.13946",
      "authors": [
        {
          "_id": "67b6b416b4ad845374143c31",
          "name": "Chak Tou Leong",
          "hidden": false
        },
        {
          "_id": "67b6b416b4ad845374143c32",
          "name": "Qingyu Yin",
          "hidden": false
        },
        {
          "_id": "67b6b416b4ad845374143c33",
          "name": "Jian Wang",
          "hidden": false
        },
        {
          "_id": "67b6b416b4ad845374143c34",
          "name": "Wenjie Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-19T18:42:45.000Z",
      "title": "Why Safeguarded Ships Run Aground? Aligned Large Language Models' Safety\n  Mechanisms Tend to Be Anchored in The Template Region",
      "summary": "The safety alignment of large language models (LLMs) remains vulnerable, as\ntheir initial behavior can be easily jailbroken by even relatively simple\nattacks. Since infilling a fixed template between the input instruction and\ninitial model output is a common practice for existing LLMs, we hypothesize\nthat this template is a key factor behind their vulnerabilities: LLMs'\nsafety-related decision-making overly relies on the aggregated information from\nthe template region, which largely influences these models' safety behavior. We\nrefer to this issue as template-anchored safety alignment. In this paper, we\nconduct extensive experiments and verify that template-anchored safety\nalignment is widespread across various aligned LLMs. Our mechanistic analyses\ndemonstrate how it leads to models' susceptibility when encountering\ninference-time jailbreak attacks. Furthermore, we show that detaching safety\nmechanisms from the template region is promising in mitigating vulnerabilities\nto jailbreak attacks. We encourage future research to develop more robust\nsafety alignment techniques that reduce reliance on the template region.",
      "upvotes": 7,
      "discussionId": "67b6b416b4ad845374143c5b"
    },
    "publishedAt": "2025-02-19T23:54:57.669Z",
    "title": "Why Safeguarded Ships Run Aground? Aligned Large Language Models' Safety Mechanisms Tend to Be Anchored in The Template Region",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13946.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "631326d6289cf15634c52369",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/631326d6289cf15634c52369/lmPWGHLsQ36H556cqcXjT.jpeg",
      "fullname": "Cooper Leong",
      "name": "cooperleong00",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.13233",
      "authors": [
        {
          "_id": "67b689aeba514d2c2c969289",
          "name": "Yucheng Shi",
          "hidden": false
        },
        {
          "_id": "67b689aeba514d2c2c96928a",
          "name": "Tianze Yang",
          "hidden": false
        },
        {
          "_id": "67b689aeba514d2c2c96928b",
          "name": "Canyu Chen",
          "hidden": false
        },
        {
          "_id": "67b689aeba514d2c2c96928c",
          "name": "Quanzheng Li",
          "hidden": false
        },
        {
          "_id": "67b689aeba514d2c2c96928d",
          "name": "Tianming Liu",
          "hidden": false
        },
        {
          "_id": "67b689aeba514d2c2c96928e",
          "name": "Xiang Li",
          "hidden": false
        },
        {
          "_id": "67b689aeba514d2c2c96928f",
          "name": "Ninghao Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T19:12:15.000Z",
      "title": "SearchRAG: Can Search Engines Be Helpful for LLM-based Medical Question\n  Answering?",
      "summary": "Large Language Models (LLMs) have shown remarkable capabilities in general\ndomains but often struggle with tasks requiring specialized knowledge.\nConventional Retrieval-Augmented Generation (RAG) techniques typically retrieve\nexternal information from static knowledge bases, which can be outdated or\nincomplete, missing fine-grained clinical details essential for accurate\nmedical question answering. In this work, we propose SearchRAG, a novel\nframework that overcomes these limitations by leveraging real-time search\nengines. Our method employs synthetic query generation to convert complex\nmedical questions into search-engine-friendly queries and utilizes\nuncertainty-based knowledge selection to filter and incorporate the most\nrelevant and informative medical knowledge into the LLM's input. Experimental\nresults demonstrate that our method significantly improves response accuracy in\nmedical question answering tasks, particularly for complex questions requiring\ndetailed and up-to-date knowledge.",
      "upvotes": 6,
      "discussionId": "67b689aeba514d2c2c9692b9"
    },
    "publishedAt": "2025-02-19T22:27:22.403Z",
    "title": "SearchRAG: Can Search Engines Be Helpful for LLM-based Medical Question Answering?",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13233.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64beb6b6140491ca9f803ebf",
      "avatarUrl": "/avatars/0daa2e813a13668b8b708cd8c12763d9.svg",
      "fullname": "Yucheng SHi",
      "name": "YuchengShi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.13962",
      "authors": [
        {
          "_id": "67b691751f861500916ecd5d",
          "name": "William Jurayj",
          "hidden": false
        },
        {
          "_id": "67b691751f861500916ecd5e",
          "name": "Jeffrey Cheng",
          "hidden": false
        },
        {
          "_id": "67b691751f861500916ecd5f",
          "name": "Benjamin Van Durme",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-19T18:58:31.000Z",
      "title": "Is That Your Final Answer? Test-Time Scaling Improves Selective Question\n  Answering",
      "summary": "Scaling the test-time compute of large language models has demonstrated\nimpressive performance on reasoning benchmarks. However, existing evaluations\nof test-time scaling make the strong assumption that a reasoning system should\nalways give an answer to any question provided. This overlooks concerns about\nwhether a model is confident in its answer, and whether it is appropriate to\nalways provide a response. To address these concerns, we extract confidence\nscores during reasoning for thresholding model responses. We find that\nincreasing compute budget at inference time not only helps models answer more\nquestions correctly, but also increases confidence in correct responses. We\nthen extend the current paradigm of zero-risk responses during evaluation by\nconsidering settings with non-zero levels of response risk, and suggest a\nrecipe for reporting evaluations under these settings.",
      "upvotes": 5,
      "discussionId": "67b691761f861500916ecd8e"
    },
    "publishedAt": "2025-02-19T23:34:43.424Z",
    "title": "Is That Your Final Answer? Test-Time Scaling Improves Selective Question Answering",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13962.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6146
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.13943",
      "authors": [
        {
          "_id": "67b6a9a7c721bee91cac2888",
          "name": "Yuliang Liu",
          "hidden": false
        },
        {
          "_id": "67b6a9a7c721bee91cac2889",
          "name": "Junjie Lu",
          "hidden": false
        },
        {
          "_id": "67b6a9a7c721bee91cac288a",
          "name": "Zhaoling Chen",
          "hidden": false
        },
        {
          "_id": "67b6a9a7c721bee91cac288b",
          "name": "Chaofeng Qu",
          "hidden": false
        },
        {
          "_id": "67b6a9a7c721bee91cac288c",
          "name": "Jason Klein Liu",
          "hidden": false
        },
        {
          "_id": "67b6a9a7c721bee91cac288d",
          "name": "Chonghan Liu",
          "hidden": false
        },
        {
          "_id": "67b6a9a7c721bee91cac288e",
          "name": "Zefan Cai",
          "hidden": false
        },
        {
          "_id": "67b6a9a7c721bee91cac288f",
          "name": "Yunhui Xia",
          "hidden": false
        },
        {
          "_id": "67b6a9a7c721bee91cac2890",
          "name": "Li Zhao",
          "hidden": false
        },
        {
          "_id": "67b6a9a7c721bee91cac2891",
          "name": "Jiang Bian",
          "hidden": false
        },
        {
          "_id": "67b6a9a7c721bee91cac2892",
          "name": "Chuheng Zhang",
          "hidden": false
        },
        {
          "_id": "67b6a9a7c721bee91cac2893",
          "name": "Wei Shen",
          "hidden": false
        },
        {
          "_id": "67b6a9a7c721bee91cac2894",
          "name": "Zhouhan Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-19T18:35:55.000Z",
      "title": "AdaptiveStep: Automatically Dividing Reasoning Step through Model\n  Confidence",
      "summary": "Current approaches for training Process Reward Models (PRMs) often involve\nbreaking down responses into multiple reasoning steps using rule-based\ntechniques, such as using predefined placeholder tokens or setting the\nreasoning step's length into a fixed size. These approaches overlook the fact\nthat specific words do not typically mark true decision points in a text. To\naddress this, we propose AdaptiveStep, a method that divides reasoning steps\nbased on the model's confidence in predicting the next word. This division\nmethod provides more decision-making information at each step, enhancing\ndownstream tasks, such as reward model learning. Moreover, our method does not\nrequire manual annotation. We demonstrate its effectiveness through experiments\nwith AdaptiveStep-trained PRMs in mathematical reasoning and code generation\ntasks. Experimental results indicate that the outcome PRM achieves\nstate-of-the-art Best-of-N performance, surpassing greedy search strategy with\ntoken-level value-guided decoding, while also reducing construction costs by\nover 30% compared to existing open-source PRMs. In addition, we provide a\nthorough analysis and case study on the PRM's performance, transferability, and\ngeneralization capabilities.",
      "upvotes": 4,
      "discussionId": "67b6a9a8c721bee91cac28e7"
    },
    "publishedAt": "2025-02-19T23:07:01.367Z",
    "title": "AdaptiveStep: Automatically Dividing Reasoning Step through Model Confidence",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13943.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6529f79e802e3d1a4f8ec662",
      "avatarUrl": "/avatars/d05320c370a6497d8792ef5acb563dd5.svg",
      "fullname": "Yuliang Liu",
      "name": "yuliang03181",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.13173",
      "authors": [
        {
          "_id": "67b6b014f7e569081326494f",
          "name": "Wang Yang",
          "hidden": false
        },
        {
          "_id": "67b6b014f7e5690813264950",
          "name": "Hongye Jin",
          "hidden": false
        },
        {
          "_id": "67b6b014f7e5690813264951",
          "name": "Jingfeng Yang",
          "hidden": false
        },
        {
          "_id": "67b6b014f7e5690813264952",
          "name": "Vipin Chaudhary",
          "hidden": false
        },
        {
          "_id": "67b6b014f7e5690813264953",
          "name": "Xiaotian Han",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-17T19:56:21.000Z",
      "title": "Thinking Preference Optimization",
      "summary": "Supervised Fine-Tuning (SFT) has been a go-to and effective method for\nenhancing long chain-of-thought (CoT) reasoning in relatively small LLMs by\nfine-tuning them with long CoT responses from larger LLMs. To continually\nimprove reasoning abilities, we can either collect new high-quality long CoT\nreasoning SFT data or repeatedly train on existing SFT datasets. However,\nacquiring new long CoT SFT data is costly and limited, while repeated training\noften results in a performance plateau or decline. To further boost the\nperformance with the SFT data, we propose Thinking Preference Optimization\n(ThinkPO), a simple yet effective post-SFT method that enhances long CoT\nreasoning without requiring new long CoT responses. Instead, ThinkPO utilizes\nreadily available or easily obtainable short CoT reasoning responses as\nrejected answers and long CoT responses as chosen answers for the same\nquestion. It then applies direct preference optimization to encourage the model\nto favor longer reasoning outputs. Experiments show that ThinkPO further\nimproves the reasoning performance of SFT-ed models, e.g. it increases math\nreasoning accuracy of SFT-ed models by 8.6% and output length by 25.9%.\nNotably, ThinkPO is capable of continually boosting the performance of the\npublicly distilled SFT model, e.g., increasing the official\nDeepSeek-R1-Distill-Qwen-7B's performance on MATH500 from 87.4% to 91.2%.",
      "upvotes": 3,
      "discussionId": "67b6b015f7e56908132649a0"
    },
    "publishedAt": "2025-02-19T23:31:36.410Z",
    "title": "Thinking Preference Optimization",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13173.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6146
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.12638",
      "authors": [
        {
          "_id": "67b6acdb3a3df2f965e7af0b",
          "name": "Zhiyuan Liu",
          "hidden": false
        },
        {
          "_id": "67b6acdb3a3df2f965e7af0c",
          "name": "Yanchen Luo",
          "hidden": false
        },
        {
          "_id": "67b6acdb3a3df2f965e7af0d",
          "name": "Han Huang",
          "hidden": false
        },
        {
          "_id": "67b6acdb3a3df2f965e7af0e",
          "name": "Enzhi Zhang",
          "hidden": false
        },
        {
          "_id": "67b6acdb3a3df2f965e7af0f",
          "name": "Sihang Li",
          "hidden": false
        },
        {
          "_id": "67b6acdb3a3df2f965e7af10",
          "name": "Junfeng Fang",
          "hidden": false
        },
        {
          "_id": "67b6acdb3a3df2f965e7af11",
          "name": "Yaorui Shi",
          "hidden": false
        },
        {
          "_id": "67b6acdb3a3df2f965e7af12",
          "user": {
            "_id": "65fca775fa59bdf4737b1a84",
            "avatarUrl": "/avatars/a161b510bde8f57e7686cbb0b4aa6a52.svg",
            "isPro": false,
            "fullname": "Xiang Wang",
            "user": "xiangwang1223",
            "type": "user"
          },
          "name": "Xiang Wang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-20T04:17:33.860Z",
          "hidden": false
        },
        {
          "_id": "67b6acdb3a3df2f965e7af13",
          "name": "Kenji Kawaguchi",
          "hidden": false
        },
        {
          "_id": "67b6acdb3a3df2f965e7af14",
          "name": "Tat-Seng Chua",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T08:40:13.000Z",
      "title": "NExT-Mol: 3D Diffusion Meets 1D Language Modeling for 3D Molecule\n  Generation",
      "summary": "3D molecule generation is crucial for drug discovery and material design.\nWhile prior efforts focus on 3D diffusion models for their benefits in modeling\ncontinuous 3D conformers, they overlook the advantages of 1D SELFIES-based\nLanguage Models (LMs), which can generate 100% valid molecules and leverage the\nbillion-scale 1D molecule datasets. To combine these advantages for 3D molecule\ngeneration, we propose a foundation model -- NExT-Mol: 3D Diffusion Meets 1D\nLanguage Modeling for 3D Molecule Generation. NExT-Mol uses an extensively\npretrained molecule LM for 1D molecule generation, and subsequently predicts\nthe generated molecule's 3D conformers with a 3D diffusion model. We enhance\nNExT-Mol's performance by scaling up the LM's model size, refining the\ndiffusion neural architecture, and applying 1D to 3D transfer learning.\nNotably, our 1D molecule LM significantly outperforms baselines in\ndistributional similarity while ensuring validity, and our 3D diffusion model\nachieves leading performances in conformer prediction. Given these improvements\nin 1D and 3D modeling, NExT-Mol achieves a 26% relative improvement in 3D FCD\nfor de novo 3D generation on GEOM-DRUGS, and a 13% average relative gain for\nconditional 3D generation on QM9-2014. Our codes and pretrained checkpoints are\navailable at https://github.com/acharkq/NExT-Mol.",
      "upvotes": 3,
      "discussionId": "67b6acdd3a3df2f965e7af85"
    },
    "publishedAt": "2025-02-19T23:18:32.647Z",
    "title": "NExT-Mol: 3D Diffusion Meets 1D Language Modeling for 3D Molecule Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12638.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6310a3cd531cc21f9e06de6a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6310a3cd531cc21f9e06de6a/aTGMx3O41lUARK9s3dAik.jpeg",
      "fullname": "Zhiyuan Liu",
      "name": "acharkq",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.11995",
      "authors": [
        {
          "_id": "67b65bbe0d878eff1a6b111d",
          "name": "Siddhesh Pawar",
          "hidden": false
        },
        {
          "_id": "67b65bbe0d878eff1a6b111e",
          "name": "Arnav Arora",
          "hidden": false
        },
        {
          "_id": "67b65bbe0d878eff1a6b111f",
          "name": "Lucie-Aimée Kaffee",
          "hidden": false
        },
        {
          "_id": "67b65bbe0d878eff1a6b1120",
          "name": "Isabelle Augenstein",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-17T16:35:15.000Z",
      "title": "Presumed Cultural Identity: How Names Shape LLM Responses",
      "summary": "Names are deeply tied to human identity. They can serve as markers of\nindividuality, cultural heritage, and personal history. However, using names as\na core indicator of identity can lead to over-simplification of complex\nidentities. When interacting with LLMs, user names are an important point of\ninformation for personalisation. Names can enter chatbot conversations through\ndirect user input (requested by chatbots), as part of task contexts such as CV\nreviews, or as built-in memory features that store user information for\npersonalisation. We study biases associated with names by measuring cultural\npresumptions in the responses generated by LLMs when presented with common\nsuggestion-seeking queries, which might involve making assumptions about the\nuser. Our analyses demonstrate strong assumptions about cultural identity\nassociated with names present in LLM generations across multiple cultures. Our\nwork has implications for designing more nuanced personalisation systems that\navoid reinforcing stereotypes while maintaining meaningful customisation.",
      "upvotes": 0,
      "discussionId": "67b65bbf0d878eff1a6b1174"
    },
    "publishedAt": "2025-02-20T01:20:46.431Z",
    "title": "Presumed Cultural Identity: How Names Shape LLM Responses",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11995.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60c50f18754747f54fa37114",
      "avatarUrl": "/avatars/648ae58b81806dbd93a68546666047e3.svg",
      "fullname": "Siddhesh",
      "name": "sidicity",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]