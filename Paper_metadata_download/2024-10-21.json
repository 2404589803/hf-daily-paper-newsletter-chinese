[
    {
        "paper": {
            "id": "2410.14059",
            "authors": [
                {
                    "_id": "6715fc2cac8cb216a41877a5",
                    "user": {
                        "_id": "63f622c69cbd6730302783eb",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f622c69cbd6730302783eb/9cb96JVKiOm_JhF-shbFw.jpeg",
                        "isPro": false,
                        "fullname": "Yuzhe Yang",
                        "user": "TobyYang7",
                        "type": "user"
                    },
                    "name": "Yuzhe Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-21T07:40:43.520Z",
                    "hidden": false
                },
                {
                    "_id": "6715fc2cac8cb216a41877a6",
                    "user": {
                        "_id": "643c047326f177a3e41627b6",
                        "avatarUrl": "/avatars/ade75cebd049daf080ba80a80d516240.svg",
                        "isPro": false,
                        "fullname": "Yifei Zhang",
                        "user": "amstrongzyf",
                        "type": "user"
                    },
                    "name": "Yifei Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-21T07:40:45.362Z",
                    "hidden": false
                },
                {
                    "_id": "6715fc2cac8cb216a41877a7",
                    "name": "Yan Hu",
                    "hidden": false
                },
                {
                    "_id": "6715fc2cac8cb216a41877a8",
                    "name": "Yilin Guo",
                    "hidden": false
                },
                {
                    "_id": "6715fc2cac8cb216a41877a9",
                    "user": {
                        "_id": "651fdadbf998876ea7515265",
                        "avatarUrl": "/avatars/d43c4dc13764a0c43783c3b4e3fcda0e.svg",
                        "isPro": false,
                        "fullname": "ganruoli",
                        "user": "wittenberg",
                        "type": "user"
                    },
                    "name": "Ruoli Gan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-21T12:04:36.004Z",
                    "hidden": false
                },
                {
                    "_id": "6715fc2cac8cb216a41877aa",
                    "user": {
                        "_id": "65bd14e8ce846f8aa94db1d1",
                        "avatarUrl": "/avatars/76eaad15bf32eba75271f3dc315527c2.svg",
                        "isPro": false,
                        "fullname": "Yueru He",
                        "user": "Yueru1",
                        "type": "user"
                    },
                    "name": "Yueru He",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-21T12:04:42.030Z",
                    "hidden": false
                },
                {
                    "_id": "6715fc2cac8cb216a41877ab",
                    "user": {
                        "_id": "6628c6107751d297d7025a71",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6628c6107751d297d7025a71/S1rm5VIwV2Uxfv8GetKMU.jpeg",
                        "isPro": false,
                        "fullname": "Lei Mingcong",
                        "user": "SP4595",
                        "type": "user"
                    },
                    "name": "Mingcong Lei",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-21T12:05:00.431Z",
                    "hidden": false
                },
                {
                    "_id": "6715fc2cac8cb216a41877ac",
                    "name": "Xiao Zhang",
                    "hidden": false
                },
                {
                    "_id": "6715fc2cac8cb216a41877ad",
                    "user": {
                        "_id": "61f40f56b34a114b58db2fed",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1643384634931-noauth.png",
                        "isPro": false,
                        "fullname": "Haining Wang",
                        "user": "haining",
                        "type": "user"
                    },
                    "name": "Haining Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-21T12:05:09.237Z",
                    "hidden": false
                },
                {
                    "_id": "6715fc2cac8cb216a41877ae",
                    "user": {
                        "_id": "6479f4317c18dca75e9a9324",
                        "avatarUrl": "/avatars/9aa709230b057f57ee4415c04a622c63.svg",
                        "isPro": false,
                        "fullname": "Xie",
                        "user": "QianqianXie1994",
                        "type": "user"
                    },
                    "name": "Qianqian Xie",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-21T12:05:41.140Z",
                    "hidden": false
                },
                {
                    "_id": "6715fc2cac8cb216a41877af",
                    "user": {
                        "_id": "63b58ed5889aa6707f0bb0f4",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b58ed5889aa6707f0bb0f4/9-6SJBOLdqUoc2LrKsI6y.jpeg",
                        "isPro": true,
                        "fullname": "Jimin Huang",
                        "user": "jiminHuang",
                        "type": "user"
                    },
                    "name": "Jimin Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-21T12:06:03.105Z",
                    "hidden": false
                },
                {
                    "_id": "6715fc2cac8cb216a41877b0",
                    "name": "Honghai Yu",
                    "hidden": false
                },
                {
                    "_id": "6715fc2cac8cb216a41877b1",
                    "name": "Benyou Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-17T22:03:52.000Z",
            "title": "UCFE: A User-Centric Financial Expertise Benchmark for Large Language\n  Models",
            "summary": "This paper introduces the UCFE: User-Centric Financial Expertise benchmark,\nan innovative framework designed to evaluate the ability of large language\nmodels (LLMs) to handle complex real-world financial tasks. UCFE benchmark\nadopts a hybrid approach that combines human expert evaluations with dynamic,\ntask-specific interactions to simulate the complexities of evolving financial\nscenarios. Firstly, we conducted a user study involving 804 participants,\ncollecting their feedback on financial tasks. Secondly, based on this feedback,\nwe created our dataset that encompasses a wide range of user intents and\ninteractions. This dataset serves as the foundation for benchmarking 12 LLM\nservices using the LLM-as-Judge methodology. Our results show a significant\nalignment between benchmark scores and human preferences, with a Pearson\ncorrelation coefficient of 0.78, confirming the effectiveness of the UCFE\ndataset and our evaluation approach. UCFE benchmark not only reveals the\npotential of LLMs in the financial sector but also provides a robust framework\nfor assessing their performance and user satisfaction.The benchmark dataset and\nevaluation code are available.",
            "upvotes": 38,
            "discussionId": "6715fc2dac8cb216a41877db"
        },
        "publishedAt": "2024-10-21T05:46:41.625Z",
        "title": "UCFE: A User-Centric Financial Expertise Benchmark for Large Language Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.14059.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/ade75cebd049daf080ba80a80d516240.svg",
            "fullname": "Yifei Zhang",
            "name": "amstrongzyf",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.13232",
            "authors": [
                {
                    "_id": "6715f0338cb36b73b1dbf48d",
                    "user": {
                        "_id": "64c8f4cec547ed5243ebd0a8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c8f4cec547ed5243ebd0a8/MiOH5YbMg8Gh9KYlQsLmX.jpeg",
                        "isPro": false,
                        "fullname": "Hyungjoo Chae",
                        "user": "hyungjoochae",
                        "type": "user"
                    },
                    "name": "Hyungjoo Chae",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-21T10:26:53.304Z",
                    "hidden": false
                },
                {
                    "_id": "6715f0338cb36b73b1dbf48e",
                    "user": {
                        "_id": "6631e2b9fb55832ca41d78b1",
                        "avatarUrl": "/avatars/6115ac6dfb3cc6c8e6e94729f3d5a8c5.svg",
                        "isPro": false,
                        "fullname": "Namyoung Kim",
                        "user": "kimnamssya",
                        "type": "user"
                    },
                    "name": "Namyoung Kim",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-21T11:45:10.010Z",
                    "hidden": false
                },
                {
                    "_id": "6715f0338cb36b73b1dbf48f",
                    "user": {
                        "_id": "640ec2fd2f9c7b364d182735",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640ec2fd2f9c7b364d182735/G5WxQXYEOaVL39rAoOI3Y.jpeg",
                        "isPro": false,
                        "fullname": "kai tzu-iunn ong",
                        "user": "ktio",
                        "type": "user"
                    },
                    "name": "Kai Tzu-iunn Ong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-21T11:43:52.128Z",
                    "hidden": false
                },
                {
                    "_id": "6715f0338cb36b73b1dbf490",
                    "name": "Minju Gwak",
                    "hidden": false
                },
                {
                    "_id": "6715f0338cb36b73b1dbf491",
                    "user": {
                        "_id": "668d512d3d0bc7c4758a73cc",
                        "avatarUrl": "/avatars/054e4ed3c06f0464fb06b88fa7cec7f3.svg",
                        "isPro": false,
                        "fullname": "Gwanwoo Song",
                        "user": "Gwanwoo",
                        "type": "user"
                    },
                    "name": "Gwanwoo Song",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-21T11:43:39.177Z",
                    "hidden": false
                },
                {
                    "_id": "6715f0338cb36b73b1dbf492",
                    "user": {
                        "_id": "66429ceee4a7476619a5ab64",
                        "avatarUrl": "/avatars/43cb4f087f4f9dc972196047fdb339f4.svg",
                        "isPro": false,
                        "fullname": "Jihoon Kim",
                        "user": "jihoonkim25",
                        "type": "user"
                    },
                    "name": "Jihoon Kim",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-21T11:44:42.881Z",
                    "hidden": false
                },
                {
                    "_id": "6715f0338cb36b73b1dbf493",
                    "name": "Sunghwan Kim",
                    "hidden": false
                },
                {
                    "_id": "6715f0338cb36b73b1dbf494",
                    "user": {
                        "_id": "61d4bb8a34cff1c8e64b6a2a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61d4bb8a34cff1c8e64b6a2a/WG3OOjwTcbOxzuMb-4Fpw.png",
                        "isPro": false,
                        "fullname": "Dongha Lee",
                        "user": "donalee",
                        "type": "user"
                    },
                    "name": "Dongha Lee",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-21T12:14:10.468Z",
                    "hidden": false
                },
                {
                    "_id": "6715f0338cb36b73b1dbf495",
                    "user": {
                        "_id": "6419ad74060a651c415eec62",
                        "avatarUrl": "/avatars/e7b060d0a45e3af1c7439948dbd60e34.svg",
                        "isPro": false,
                        "fullname": "Jinyoung Yeom",
                        "user": "geen02",
                        "type": "user"
                    },
                    "name": "Jinyoung Yeo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-21T11:43:28.091Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-17T05:37:00.000Z",
            "title": "Web Agents with World Models: Learning and Leveraging Environment\n  Dynamics in Web Navigation",
            "summary": "Large language models (LLMs) have recently gained much attention in building\nautonomous agents. However, the performance of current LLM-based web agents in\nlong-horizon tasks is far from optimal, often yielding errors such as\nrepeatedly buying a non-refundable flight ticket. By contrast, humans can avoid\nsuch an irreversible mistake, as we have an awareness of the potential outcomes\n(e.g., losing money) of our actions, also known as the \"world model\". Motivated\nby this, our study first starts with preliminary analyses, confirming the\nabsence of world models in current LLMs (e.g., GPT-4o, Claude-3.5-Sonnet,\netc.). Then, we present a World-model-augmented (WMA) web agent, which\nsimulates the outcomes of its actions for better decision-making. To overcome\nthe challenges in training LLMs as world models predicting next observations,\nsuch as repeated elements across observations and long HTML inputs, we propose\na transition-focused observation abstraction, where the prediction objectives\nare free-form natural language descriptions exclusively highlighting important\nstate differences between time steps. Experiments on WebArena and Mind2Web show\nthat our world models improve agents' policy selection without training and\ndemonstrate our agents' cost- and time-efficiency compared to recent\ntree-search-based agents.",
            "upvotes": 27,
            "discussionId": "6715f0358cb36b73b1dbf519"
        },
        "publishedAt": "2024-10-21T04:41:07.414Z",
        "title": "Web Agents with World Models: Learning and Leveraging Environment Dynamics in Web Navigation",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64c8f4cec547ed5243ebd0a8/Q3cNBL09-ITpyL-jk5lHQ.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.13232.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c8f4cec547ed5243ebd0a8/MiOH5YbMg8Gh9KYlQsLmX.jpeg",
            "fullname": "Hyungjoo Chae",
            "name": "hyungjoochae",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.13370",
            "authors": [
                {
                    "_id": "671361859186942050bfeee7",
                    "user": {
                        "_id": "67136093d2e50f1e8c9fad52",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0q49MyGuav8lJ9CIeyLhu.png",
                        "isPro": false,
                        "fullname": "Donghao Zhou",
                        "user": "donghao-zhou",
                        "type": "user"
                    },
                    "name": "Donghao Zhou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-21T07:47:53.857Z",
                    "hidden": false
                },
                {
                    "_id": "671361859186942050bfeee8",
                    "user": {
                        "_id": "6395e7c311074713ad1fc29e",
                        "avatarUrl": "/avatars/6b1e61aaaed89af05daaf15f90747976.svg",
                        "isPro": false,
                        "fullname": "jiancheng huang",
                        "user": "jiancheng",
                        "type": "user"
                    },
                    "name": "Jiancheng Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-21T11:47:13.861Z",
                    "hidden": false
                },
                {
                    "_id": "671361859186942050bfeee9",
                    "user": {
                        "_id": "63fccdac93b993a4ebd7789a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63fccdac93b993a4ebd7789a/KRx8vpdoDjsZBRw0j8Vg8.jpeg",
                        "isPro": false,
                        "fullname": "Jinbin Bai",
                        "user": "BryanW",
                        "type": "user"
                    },
                    "name": "Jinbin Bai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-21T07:47:51.844Z",
                    "hidden": false
                },
                {
                    "_id": "671361859186942050bfeeea",
                    "user": {
                        "_id": "65f04e01a97811922fbafc86",
                        "avatarUrl": "/avatars/8bd37a4ce857d1ed5f3d39891ce1b89d.svg",
                        "isPro": false,
                        "fullname": "Jiaze Wang",
                        "user": "jzwangcuhk",
                        "type": "user"
                    },
                    "name": "Jiaze Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-21T11:52:48.671Z",
                    "hidden": false
                },
                {
                    "_id": "671361859186942050bfeeeb",
                    "name": "Hao Chen",
                    "hidden": false
                },
                {
                    "_id": "671361859186942050bfeeec",
                    "user": {
                        "_id": "6396829c4dffc883b735e22e",
                        "avatarUrl": "/avatars/c1785f212f50bfde1d202d245a6a5ca3.svg",
                        "isPro": false,
                        "fullname": "Guangyong Chen",
                        "user": "gychen",
                        "type": "user"
                    },
                    "name": "Guangyong Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-21T11:47:38.866Z",
                    "hidden": false
                },
                {
                    "_id": "671361859186942050bfeeed",
                    "name": "Xiaowei Hu",
                    "hidden": false
                },
                {
                    "_id": "671361859186942050bfeeee",
                    "name": "Pheng-Ann Heng",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-17T09:22:53.000Z",
            "title": "MagicTailor: Component-Controllable Personalization in Text-to-Image\n  Diffusion Models",
            "summary": "Recent advancements in text-to-image (T2I) diffusion models have enabled the\ncreation of high-quality images from text prompts, but they still struggle to\ngenerate images with precise control over specific visual concepts. Existing\napproaches can replicate a given concept by learning from reference images, yet\nthey lack the flexibility for fine-grained customization of the individual\ncomponent within the concept. In this paper, we introduce\ncomponent-controllable personalization, a novel task that pushes the boundaries\nof T2I models by allowing users to reconfigure specific components when\npersonalizing visual concepts. This task is particularly challenging due to two\nprimary obstacles: semantic pollution, where unwanted visual elements corrupt\nthe personalized concept, and semantic imbalance, which causes disproportionate\nlearning of the concept and component. To overcome these challenges, we design\nMagicTailor, an innovative framework that leverages Dynamic Masked Degradation\n(DM-Deg) to dynamically perturb undesired visual semantics and Dual-Stream\nBalancing (DS-Bal) to establish a balanced learning paradigm for desired visual\nsemantics. Extensive comparisons, ablations, and analyses demonstrate that\nMagicTailor not only excels in this challenging task but also holds significant\npromise for practical applications, paving the way for more nuanced and\ncreative image generation.",
            "upvotes": 23,
            "discussionId": "671361879186942050bfef20"
        },
        "publishedAt": "2024-10-21T01:06:41.801Z",
        "title": "MagicTailor: Component-Controllable Personalization in Text-to-Image Diffusion Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.13370.png",
        "numComments": 6,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63fccdac93b993a4ebd7789a/KRx8vpdoDjsZBRw0j8Vg8.jpeg",
            "fullname": "Jinbin Bai",
            "name": "BryanW",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.14669",
            "authors": [
                {
                    "_id": "6715c4518b7afb7724fa6aae",
                    "user": {
                        "_id": "65ea393b1032135623fe8037",
                        "avatarUrl": "/avatars/c5844e9cd43092cfb67543f5b4d7c0bf.svg",
                        "isPro": false,
                        "fullname": "BaiqiLi",
                        "user": "BaiqiL",
                        "type": "user"
                    },
                    "name": "Baiqi Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-21T07:40:56.291Z",
                    "hidden": false
                },
                {
                    "_id": "6715c4518b7afb7724fa6aaf",
                    "user": {
                        "_id": "64c170190bfb901b04399295",
                        "avatarUrl": "/avatars/c30ce7566ae3497ddc989ec8918d37cc.svg",
                        "isPro": false,
                        "fullname": "Zhiqiu Lin",
                        "user": "zhiqiulin",
                        "type": "user"
                    },
                    "name": "Zhiqiu Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-21T12:06:52.741Z",
                    "hidden": false
                },
                {
                    "_id": "6715c4518b7afb7724fa6ab0",
                    "user": {
                        "_id": "63b91e47f270ad02f613237b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673076442939-63b91e47f270ad02f613237b.jpeg",
                        "isPro": true,
                        "fullname": "WENXUAN PENG",
                        "user": "Lilymelon7",
                        "type": "user"
                    },
                    "name": "Wenxuan Peng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-21T12:07:22.881Z",
                    "hidden": false
                },
                {
                    "_id": "6715c4518b7afb7724fa6ab1",
                    "user": {
                        "_id": "62989cd743e990340bead77c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654168730944-noauth.png",
                        "isPro": false,
                        "fullname": "Jean de Dieu Nyandwi",
                        "user": "Nyandwi",
                        "type": "user"
                    },
                    "name": "Jean de Dieu Nyandwi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-21T11:42:11.775Z",
                    "hidden": false
                },
                {
                    "_id": "6715c4518b7afb7724fa6ab2",
                    "name": "Daniel Jiang",
                    "hidden": false
                },
                {
                    "_id": "6715c4518b7afb7724fa6ab3",
                    "user": {
                        "_id": "6442cce0e255a338677e59ff",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VheGtt37p7aYIodlt9N3n.jpeg",
                        "isPro": false,
                        "fullname": "Zixian Ma",
                        "user": "zixianma",
                        "type": "user"
                    },
                    "name": "Zixian Ma",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-21T12:07:32.845Z",
                    "hidden": false
                },
                {
                    "_id": "6715c4518b7afb7724fa6ab4",
                    "user": {
                        "_id": "63a4d079658851481f7e4394",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4d079658851481f7e4394/yptj_hJiaqHvEaqxLs0jT.jpeg",
                        "isPro": false,
                        "fullname": "Simran Khanuja",
                        "user": "skhanuja",
                        "type": "user"
                    },
                    "name": "Simran Khanuja",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-21T12:08:00.335Z",
                    "hidden": false
                },
                {
                    "_id": "6715c4518b7afb7724fa6ab5",
                    "user": {
                        "_id": "66429868ab89e3a3a85668b0",
                        "avatarUrl": "/avatars/170e0daa454838deee2bf946f7118651.svg",
                        "isPro": false,
                        "fullname": "Ranjay Krishna",
                        "user": "ranjaykrishna",
                        "type": "user"
                    },
                    "name": "Ranjay Krishna",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-21T12:08:07.227Z",
                    "hidden": false
                },
                {
                    "_id": "6715c4518b7afb7724fa6ab6",
                    "user": {
                        "_id": "60de14638bedd2315529d43f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1625166923504-noauth.png",
                        "isPro": false,
                        "fullname": "Graham Neubig",
                        "user": "gneubig",
                        "type": "user"
                    },
                    "name": "Graham Neubig",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-21T12:08:13.199Z",
                    "hidden": false
                },
                {
                    "_id": "6715c4518b7afb7724fa6ab7",
                    "user": {
                        "_id": "6337151b0267ebcf02640eb6",
                        "avatarUrl": "/avatars/14a723cafc5587043bdfb19304fc202d.svg",
                        "isPro": false,
                        "fullname": "Deva Ramanan",
                        "user": "devakramanan",
                        "type": "user"
                    },
                    "name": "Deva Ramanan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-21T12:08:21.627Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-18T17:58:21.000Z",
            "title": "NaturalBench: Evaluating Vision-Language Models on Natural Adversarial\n  Samples",
            "summary": "Vision-language models (VLMs) have made significant progress in recent\nvisual-question-answering (VQA) benchmarks that evaluate complex\nvisio-linguistic reasoning. However, are these models truly effective? In this\nwork, we show that VLMs still struggle with natural images and questions that\nhumans can easily answer, which we term natural adversarial samples. We also\nfind it surprisingly easy to generate these VQA samples from natural image-text\ncorpora using off-the-shelf models like CLIP and ChatGPT. We propose a\nsemi-automated approach to collect a new benchmark, NaturalBench, for reliably\nevaluating VLMs with 10,000 human-verified VQA samples. Crucially, we adopt a\nvision-centric design by pairing each question with two images that\nyield different answers, preventing blind solutions from answering without\nusing the images. This makes NaturalBench more challenging than previous\nbenchmarks that can be solved with commonsense priors. We evaluate 53\nstate-of-the-art VLMs on NaturalBench, showing that models like\nLLaVA-OneVision, Cambrian-1, Llama3.2-Vision, Molmo, Qwen2-VL, and even GPT-4o\nlag 50%-70% behind human performance (over 90%). We analyze why NaturalBench is\nhard from two angles: (1) Compositionality: Solving NaturalBench requires\ndiverse visio-linguistic skills, including understanding attribute bindings,\nobject relationships, and advanced reasoning like logic and counting. To this\nend, unlike prior work that uses a single tag per sample, we tag each\nNaturalBench sample with 1 to 8 skill tags for fine-grained evaluation. (2)\nBiases: NaturalBench exposes severe biases in VLMs, as models often choose the\nsame answer regardless of the image. Lastly, we apply our benchmark curation\nmethod to diverse data sources, including long captions (over 100 words) and\nnon-English languages like Chinese and Hindi, highlighting its potential for\ndynamic evaluations of VLMs.",
            "upvotes": 22,
            "discussionId": "6715c4538b7afb7724fa6b24"
        },
        "publishedAt": "2024-10-21T06:33:45.057Z",
        "title": "NaturalBench: Evaluating Vision-Language Models on Natural Adversarial Samples",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.14669.png",
        "numComments": 3,
        "submittedBy": {
            "avatarUrl": "/avatars/c5844e9cd43092cfb67543f5b4d7c0bf.svg",
            "fullname": "BaiqiLi",
            "name": "BaiqiL",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.13276",
            "authors": [
                {
                    "_id": "6715f1f89c2303f000b006b6",
                    "user": {
                        "_id": "661c96f48921f03a9dae04c3",
                        "avatarUrl": "/avatars/c486a45ffea7d1a8c72bd8512014b07e.svg",
                        "isPro": false,
                        "fullname": "Yizhao Gao",
                        "user": "Retromonic",
                        "type": "user"
                    },
                    "name": "Yizhao Gao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-21T07:40:49.287Z",
                    "hidden": false
                },
                {
                    "_id": "6715f1f89c2303f000b006b7",
                    "user": {
                        "_id": "66038f8e90c62bb38f06eb9f",
                        "avatarUrl": "/avatars/e67137b47f45f580bae7ea6472671359.svg",
                        "isPro": false,
                        "fullname": "Zhichen Zeng",
                        "user": "CharyZeng",
                        "type": "user"
                    },
                    "name": "Zhichen Zeng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-21T07:40:52.311Z",
                    "hidden": false
                },
                {
                    "_id": "6715f1f89c2303f000b006b8",
                    "user": {
                        "_id": "64551fcba13edf669cd84a88",
                        "avatarUrl": "/avatars/c3c9170d068757f610280518c2baf443.svg",
                        "isPro": false,
                        "fullname": "Dayou DU",
                        "user": "Daniel-Duda",
                        "type": "user"
                    },
                    "name": "Dayou Du",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-21T12:08:32.611Z",
                    "hidden": false
                },
                {
                    "_id": "6715f1f89c2303f000b006b9",
                    "user": {
                        "_id": "662852bf8bfc90408a97acdf",
                        "avatarUrl": "/avatars/008e6faf51e1be4f0c0d17008dfe2f49.svg",
                        "isPro": false,
                        "fullname": "cao",
                        "user": "tingcao",
                        "type": "user"
                    },
                    "name": "Shijie Cao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-21T12:08:50.992Z",
                    "hidden": false
                },
                {
                    "_id": "6715f1f89c2303f000b006ba",
                    "name": "Hayden Kwok-Hay So",
                    "hidden": false
                },
                {
                    "_id": "6715f1f89c2303f000b006bb",
                    "name": "Ting Cao",
                    "hidden": false
                },
                {
                    "_id": "6715f1f89c2303f000b006bc",
                    "name": "Fan Yang",
                    "hidden": false
                },
                {
                    "_id": "6715f1f89c2303f000b006bd",
                    "name": "Mao Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-17T07:07:09.000Z",
            "title": "SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs",
            "summary": "Attention is the cornerstone of modern Large Language Models (LLMs). Yet its\nquadratic complexity limits the efficiency and scalability of LLMs, especially\nfor those with a long-context window. A promising approach addressing this\nlimitation is to leverage the sparsity in attention. However, existing\nsparsity-based solutions predominantly rely on predefined patterns or\nheuristics to approximate sparsity. This practice falls short to fully capture\nthe dynamic nature of attention sparsity in language-based tasks. This paper\nargues that attention sparsity should be learned rather than predefined. To\nthis end, we design SeerAttention, a new Attention mechanism that augments the\nconventional attention with a learnable gate that adaptively selects\nsignificant blocks in an attention map and deems the rest blocks sparse. Such\nblock-level sparsity effectively balances accuracy and speedup. To enable\nefficient learning of the gating network, we develop a customized\nFlashAttention implementation that extracts the block-level ground truth of\nattention map with minimum overhead. SeerAttention not only applies to\npost-training, but also excels in long-context fine-tuning. Our results show\nthat at post-training stages, SeerAttention significantly outperforms\nstate-of-the-art static or heuristic-based sparse attention methods, while also\nbeing more versatile and flexible to adapt to varying context lengths and\nsparsity ratios. When applied to long-context fine-tuning with YaRN,\nSeerAttention can achieve a remarkable 90% sparsity ratio at a 32k context\nlength with minimal perplexity loss, offering a 5.67x speedup over\nFlashAttention-2.",
            "upvotes": 13,
            "discussionId": "6715f1f99c2303f000b006fb"
        },
        "publishedAt": "2024-10-21T05:11:11.210Z",
        "title": "SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/62c6cd336af791614d05c4fa/VJmaBHjfj-K9PtYKRFrbD.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.13276.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/474b4b4dbb664e0e449b4422f1b7d000.svg",
            "fullname": "Cao",
            "name": "Shijie",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.11190",
            "authors": [
                {
                    "_id": "6710b98b967512639d50a70e",
                    "user": {
                        "_id": "66c5d81a4061fd5907443787",
                        "avatarUrl": "/avatars/2e107195b1ff7d06bbc6c9bd4e5620cf.svg",
                        "isPro": false,
                        "fullname": "zhifei",
                        "user": "filicos",
                        "type": "user"
                    },
                    "name": "Zhifei Xie",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-18T13:28:54.691Z",
                    "hidden": false
                },
                {
                    "_id": "6710b98b967512639d50a70f",
                    "user": {
                        "_id": "66cdb20c6aee4c865fe9647e",
                        "avatarUrl": "/avatars/d3637e8278c4b14d0f03ebfbfd3d3711.svg",
                        "isPro": false,
                        "fullname": "Changqiao Wu",
                        "user": "gpt-omni",
                        "type": "user"
                    },
                    "name": "Changqiao Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-18T08:14:20.838Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-15T02:10:45.000Z",
            "title": "Mini-Omni2: Towards Open-source GPT-4o with Vision, Speech and Duplex\n  Capabilities",
            "summary": "GPT-4o, an all-encompassing model, represents a milestone in the development\nof large multi-modal language models. It can understand visual, auditory, and\ntextual modalities, directly output audio, and support flexible duplex\ninteraction. Models from the open-source community often achieve some\nfunctionalities of GPT-4o, such as visual understanding and voice chat.\nNevertheless, training a unified model that incorporates all modalities is\nchallenging due to the complexities of multi-modal data, intricate model\narchitectures, and training processes. In this paper, we introduce Mini-Omni2,\na visual-audio assistant capable of providing real-time, end-to-end voice\nresponses to visoin and audio queries. By integrating pretrained visual and\nauditory encoders, Mini-Omni2 maintains performance in individual modalities.\nWe propose a three-stage training process to align modalities, allowing the\nlanguage model to handle multi-modal inputs and outputs after training on a\nlimited dataset. For interaction, we introduce a command-based interruption\nmechanism, enabling more flexible interaction with users. To the best of our\nknowledge, Mini-Omni2 is one of the closest reproductions of GPT-4o, which have\nsimilar form of functionality, and we hope it can offer valuable insights for\nsubsequent research.",
            "upvotes": 10,
            "discussionId": "6710b98e967512639d50a7b0"
        },
        "publishedAt": "2024-10-21T13:21:46.486Z",
        "title": "Mini-Omni2: Towards Open-source GPT-4o with Vision, Speech and Duplex Capabilities",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.11190.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.13925",
            "authors": [
                {
                    "_id": "6715dc1d634b87d9f032c08a",
                    "name": "ZiDong Wang",
                    "hidden": false
                },
                {
                    "_id": "6715dc1d634b87d9f032c08b",
                    "user": {
                        "_id": "635626a8ec32331b227f407b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/635626a8ec32331b227f407b/KRkAEN3eXN_mYzzJQ_8dO.jpeg",
                        "isPro": false,
                        "fullname": "LuZeyu",
                        "user": "whlzy",
                        "type": "user"
                    },
                    "name": "Zeyu Lu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-21T16:13:35.701Z",
                    "hidden": false
                },
                {
                    "_id": "6715dc1d634b87d9f032c08c",
                    "name": "Di Huang",
                    "hidden": false
                },
                {
                    "_id": "6715dc1d634b87d9f032c08d",
                    "name": "Cai Zhou",
                    "hidden": false
                },
                {
                    "_id": "6715dc1d634b87d9f032c08e",
                    "name": "Wanli Ouyang",
                    "hidden": false
                },
                {
                    "_id": "6715dc1d634b87d9f032c08f",
                    "name": "and Lei Bai",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-17T15:51:49.000Z",
            "title": "FiTv2: Scalable and Improved Flexible Vision Transformer for Diffusion\n  Model",
            "summary": "Nature is infinitely resolution-free. In the context of this\nreality, existing diffusion models, such as Diffusion Transformers, often face\nchallenges when processing image resolutions outside of their trained domain.\nTo address this limitation, we conceptualize images as sequences of tokens with\ndynamic sizes, rather than traditional methods that perceive images as\nfixed-resolution grids. This perspective enables a flexible training strategy\nthat seamlessly accommodates various aspect ratios during both training and\ninference, thus promoting resolution generalization and eliminating biases\nintroduced by image cropping. On this basis, we present the Flexible\nVision Transformer (FiT), a transformer architecture specifically designed for\ngenerating images with unrestricted resolutions and aspect ratios. We\nfurther upgrade the FiT to FiTv2 with several innovative designs, includingthe\nQuery-Key vector normalization, the AdaLN-LoRA module, a rectified flow\nscheduler, and a Logit-Normal sampler. Enhanced by a meticulously adjusted\nnetwork structure, FiTv2 exhibits 2times convergence speed of FiT. When\nincorporating advanced training-free extrapolation techniques, FiTv2\ndemonstrates remarkable adaptability in both resolution extrapolation and\ndiverse resolution generation. Additionally, our exploration of the scalability\nof the FiTv2 model reveals that larger models exhibit better computational\nefficiency. Furthermore, we introduce an efficient post-training strategy to\nadapt a pre-trained model for the high-resolution generation. Comprehensive\nexperiments demonstrate the exceptional performance of FiTv2 across a broad\nrange of resolutions. We have released all the codes and models at\nhttps://github.com/whlzy/FiT to promote the exploration of diffusion\ntransformer models for arbitrary-resolution image generation.",
            "upvotes": 9,
            "discussionId": "6715dc1f634b87d9f032c199"
        },
        "publishedAt": "2024-10-21T13:46:40.121Z",
        "title": "FiTv2: Scalable and Improved Flexible Vision Transformer for Diffusion Model",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.13925.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/635626a8ec32331b227f407b/KRkAEN3eXN_mYzzJQ_8dO.jpeg",
            "fullname": "LuZeyu",
            "name": "whlzy",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.14677",
            "authors": [
                {
                    "_id": "6715f7ffef625f4d17fbe97a",
                    "name": "German Gritsai",
                    "hidden": false
                },
                {
                    "_id": "6715f7ffef625f4d17fbe97b",
                    "name": "Anastasia Voznyuk",
                    "hidden": false
                },
                {
                    "_id": "6715f7ffef625f4d17fbe97c",
                    "user": {
                        "_id": "670786e1b21dabc40ce5e702",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/670786e1b21dabc40ce5e702/Q_P5PoqmQr7b9tBuGZfls.jpeg",
                        "isPro": false,
                        "fullname": "Andrey Grabovoy",
                        "user": "andriygav",
                        "type": "user"
                    },
                    "name": "Andrey Grabovoy",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-21T07:40:47.194Z",
                    "hidden": false
                },
                {
                    "_id": "6715f7ffef625f4d17fbe97d",
                    "name": "Yury Chekhovich",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-18T17:59:57.000Z",
            "title": "Are AI Detectors Good Enough? A Survey on Quality of Datasets With\n  Machine-Generated Texts",
            "summary": "The rapid development of autoregressive Large Language Models (LLMs) has\nsignificantly improved the quality of generated texts, necessitating reliable\nmachine-generated text detectors. A huge number of detectors and collections\nwith AI fragments have emerged, and several detection methods even showed\nrecognition quality up to 99.9% according to the target metrics in such\ncollections. However, the quality of such detectors tends to drop dramatically\nin the wild, posing a question: Are detectors actually highly trustworthy or do\ntheir high benchmark scores come from the poor quality of evaluation datasets?\nIn this paper, we emphasise the need for robust and qualitative methods for\nevaluating generated data to be secure against bias and low generalising\nability of future model. We present a systematic review of datasets from\ncompetitions dedicated to AI-generated content detection and propose methods\nfor evaluating the quality of datasets containing AI-generated fragments. In\naddition, we discuss the possibility of using high-quality generated data to\nachieve two goals: improving the training of detection models and improving the\ntraining datasets themselves. Our contribution aims to facilitate a better\nunderstanding of the dynamics between human and machine text, which will\nultimately support the integrity of information in an increasingly automated\nworld.",
            "upvotes": 7,
            "discussionId": "6715f800ef625f4d17fbe9b2"
        },
        "publishedAt": "2024-10-21T05:20:23.073Z",
        "title": "Are AI Detectors Good Enough? A Survey on Quality of Datasets With Machine-Generated Texts",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.14677.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/670786e1b21dabc40ce5e702/Q_P5PoqmQr7b9tBuGZfls.jpeg",
            "fullname": "Andrey Grabovoy",
            "name": "andriygav",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.13674",
            "authors": [
                {
                    "_id": "6711d1dbff45f0171cde152c",
                    "name": "Yijun Liang",
                    "hidden": false
                },
                {
                    "_id": "6711d1dbff45f0171cde152d",
                    "name": "Shweta Bhardwaj",
                    "hidden": false
                },
                {
                    "_id": "6711d1dbff45f0171cde152e",
                    "user": {
                        "_id": "647f5af5b0e96764589f3b2a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
                        "isPro": false,
                        "fullname": "Tianyi Zhou",
                        "user": "zhoutianyi",
                        "type": "user"
                    },
                    "name": "Tianyi Zhou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-18T08:12:16.307Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-17T15:33:35.000Z",
            "title": "Diffusion Curriculum: Synthetic-to-Real Generative Curriculum Learning\n  via Image-Guided Diffusion",
            "summary": "Low-quality or scarce data has posed significant challenges for training deep\nneural networks in practice. While classical data augmentation cannot\ncontribute very different new data, diffusion models opens up a new door to\nbuild self-evolving AI by generating high-quality and diverse synthetic data\nthrough text-guided prompts. However, text-only guidance cannot control\nsynthetic images' proximity to the original images, resulting in\nout-of-distribution data detrimental to the model performance. To overcome the\nlimitation, we study image guidance to achieve a spectrum of interpolations\nbetween synthetic and real images. With stronger image guidance, the generated\nimages are similar to the training data but hard to learn. While with weaker\nimage guidance, the synthetic images will be easier for model but contribute to\na larger distribution gap with the original data. The generated full spectrum\nof data enables us to build a novel \"Diffusion Curriculum (DisCL)\". DisCL\nadjusts the image guidance level of image synthesis for each training stage: It\nidentifies and focuses on hard samples for the model and assesses the most\neffective guidance level of synthetic images to improve hard data learning. We\napply DisCL to two challenging tasks: long-tail (LT) classification and\nlearning from low-quality data. It focuses on lower-guidance images of\nhigh-quality to learn prototypical features as a warm-up of learning\nhigher-guidance images that might be weak on diversity or quality. Extensive\nexperiments showcase a gain of 2.7% and 2.1% in OOD and ID macro-accuracy when\napplying DisCL to iWildCam dataset. On ImageNet-LT, DisCL improves the base\nmodel's tail-class accuracy from 4.4% to 23.64% and leads to a 4.02%\nimprovement in all-class accuracy.",
            "upvotes": 7,
            "discussionId": "6711d1dfff45f0171cde1676"
        },
        "publishedAt": "2024-10-21T02:09:52.757Z",
        "title": "Diffusion Curriculum: Synthetic-to-Real Generative Curriculum Learning via Image-Guided Diffusion",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/XRehS55CZel0XujemVfoB.png",
            "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/EuKtMZtxSEfsVRYfC7fKu.png",
            "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/ERfZgYzy4_zT0cS3t3EsV.png",
            "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/7u0iY3GMeEWqG9jDyl3Ma.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.13674.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
            "fullname": "Tianyi Zhou",
            "name": "zhoutianyi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.13726",
            "authors": [
                {
                    "_id": "67147d177b711cff63c0d610",
                    "user": {
                        "_id": "67147c30476f2449e228cad1",
                        "avatarUrl": "/avatars/13976ebe52634c551d41b38da801e35d.svg",
                        "isPro": false,
                        "fullname": "Hanbo-Cheng",
                        "user": "Hanbo-Cheng",
                        "type": "user"
                    },
                    "name": "Hanbo Cheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-21T07:47:17.268Z",
                    "hidden": false
                },
                {
                    "_id": "67147d177b711cff63c0d611",
                    "user": {
                        "_id": "6716128fcf2b9210e5cab2db",
                        "avatarUrl": "/avatars/254fa3b1409b844b38e4fd738f638fa6.svg",
                        "isPro": false,
                        "fullname": "Limin-Lin",
                        "user": "Limin-Lin",
                        "type": "user"
                    },
                    "name": "Limin Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-21T12:15:07.918Z",
                    "hidden": false
                },
                {
                    "_id": "67147d177b711cff63c0d612",
                    "name": "Chenyu Liu",
                    "hidden": false
                },
                {
                    "_id": "67147d177b711cff63c0d613",
                    "user": {
                        "_id": "64946adc02a46148e275735c",
                        "avatarUrl": "/avatars/26a20b321f495aa7af1481ba2af392bd.svg",
                        "isPro": false,
                        "fullname": "Pengcheng Xia",
                        "user": "page-xia",
                        "type": "user"
                    },
                    "name": "Pengcheng Xia",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-21T12:15:13.738Z",
                    "hidden": false
                },
                {
                    "_id": "67147d177b711cff63c0d614",
                    "user": {
                        "_id": "64291be28136224fee045d36",
                        "avatarUrl": "/avatars/c0d155c053742478e8c6d6bad9e0bd4f.svg",
                        "isPro": false,
                        "fullname": "Pengfei Hu",
                        "user": "Bazhu",
                        "type": "user"
                    },
                    "name": "Pengfei Hu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-21T12:15:19.747Z",
                    "hidden": false
                },
                {
                    "_id": "67147d177b711cff63c0d615",
                    "name": "Jiefeng Ma",
                    "hidden": false
                },
                {
                    "_id": "67147d177b711cff63c0d616",
                    "user": {
                        "_id": "5e0d54a0fcf41d740b699660",
                        "avatarUrl": "/avatars/67dbb87baf7d75a4ec253cc706f819ce.svg",
                        "isPro": false,
                        "fullname": "Tim Du",
                        "user": "dujun",
                        "type": "user"
                    },
                    "name": "Jun Du",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-21T12:15:40.258Z",
                    "hidden": false
                },
                {
                    "_id": "67147d177b711cff63c0d617",
                    "name": "Jia Pan",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-17T16:32:36.000Z",
            "title": "DAWN: Dynamic Frame Avatar with Non-autoregressive Diffusion Framework\n  for Talking Head Video Generation",
            "summary": "Talking head generation intends to produce vivid and realistic talking head\nvideos from a single portrait and speech audio clip. Although significant\nprogress has been made in diffusion-based talking head generation, almost all\nmethods rely on autoregressive strategies, which suffer from limited context\nutilization beyond the current generation step, error accumulation, and slower\ngeneration speed. To address these challenges, we present DAWN (Dynamic frame\nAvatar With Non-autoregressive diffusion), a framework that enables all-at-once\ngeneration of dynamic-length video sequences. Specifically, it consists of two\nmain components: (1) audio-driven holistic facial dynamics generation in the\nlatent motion space, and (2) audio-driven head pose and blink generation.\nExtensive experiments demonstrate that our method generates authentic and vivid\nvideos with precise lip motions, and natural pose/blink movements.\nAdditionally, with a high generation speed, DAWN possesses strong extrapolation\ncapabilities, ensuring the stable production of high-quality long videos. These\nresults highlight the considerable promise and potential impact of DAWN in the\nfield of talking head video generation. Furthermore, we hope that DAWN sparks\nfurther exploration of non-autoregressive approaches in diffusion models. Our\ncode will be publicly at https://github.com/Hanbo-Cheng/DAWN-pytorch.",
            "upvotes": 5,
            "discussionId": "67147d187b711cff63c0d67b"
        },
        "publishedAt": "2024-10-21T06:31:06.670Z",
        "title": "DAWN: Dynamic Frame Avatar with Non-autoregressive Diffusion Framework for Talking Head Video Generation",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/67147c30476f2449e228cad1/cI8XLR9pqeBK4MOGvENUJ.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.13726.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/13976ebe52634c551d41b38da801e35d.svg",
            "fullname": "Hanbo-Cheng",
            "name": "Hanbo-Cheng",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.13782",
            "authors": [
                {
                    "_id": "6715fdfc6757313a2bca5806",
                    "name": "Xinyou Wang",
                    "hidden": false
                },
                {
                    "_id": "6715fdfc6757313a2bca5807",
                    "user": {
                        "_id": "6347faa36d7f1d10f5ecdc18",
                        "avatarUrl": "/avatars/0fea067fa7bc8c4a797268b68a868756.svg",
                        "isPro": false,
                        "fullname": "zzheng",
                        "user": "zzheng",
                        "type": "user"
                    },
                    "name": "Zaixiang Zheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-21T12:31:01.433Z",
                    "hidden": false
                },
                {
                    "_id": "6715fdfc6757313a2bca5808",
                    "name": "Fei Ye",
                    "hidden": false
                },
                {
                    "_id": "6715fdfc6757313a2bca5809",
                    "name": "Dongyu Xue",
                    "hidden": false
                },
                {
                    "_id": "6715fdfc6757313a2bca580a",
                    "name": "Shujian Huang",
                    "hidden": false
                },
                {
                    "_id": "6715fdfc6757313a2bca580b",
                    "user": {
                        "_id": "64c039128e2612254356bba5",
                        "avatarUrl": "/avatars/06cc76feebba0cc80ebb8f4ff86f6d9b.svg",
                        "isPro": false,
                        "fullname": "Quanquan Gu",
                        "user": "thughost",
                        "type": "user"
                    },
                    "name": "Quanquan Gu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-21T12:18:39.526Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-17T17:20:24.000Z",
            "title": "DPLM-2: A Multimodal Diffusion Protein Language Model",
            "summary": "Proteins are essential macromolecules defined by their amino acid sequences,\nwhich determine their three-dimensional structures and, consequently, their\nfunctions in all living organisms. Therefore, generative protein modeling\nnecessitates a multimodal approach to simultaneously model, understand, and\ngenerate both sequences and structures. However, existing methods typically use\nseparate models for each modality, limiting their ability to capture the\nintricate relationships between sequence and structure. This results in\nsuboptimal performance in tasks that requires joint understanding and\ngeneration of both modalities. In this paper, we introduce DPLM-2, a multimodal\nprotein foundation model that extends discrete diffusion protein language model\n(DPLM) to accommodate both sequences and structures. To enable structural\nlearning with the language model, 3D coordinates are converted to discrete\ntokens using a lookup-free quantization-based tokenizer. By training on both\nexperimental and high-quality synthetic structures, DPLM-2 learns the joint\ndistribution of sequence and structure, as well as their marginals and\nconditionals. We also implement an efficient warm-up strategy to exploit the\nconnection between large-scale evolutionary data and structural inductive\nbiases from pre-trained sequence-based protein language models. Empirical\nevaluation shows that DPLM-2 can simultaneously generate highly compatible\namino acid sequences and their corresponding 3D structures eliminating the need\nfor a two-stage generation approach. Moreover, DPLM-2 demonstrates competitive\nperformance in various conditional generation tasks, including folding, inverse\nfolding, and scaffolding with multimodal motif inputs, as well as providing\nstructure-aware representations for predictive tasks.",
            "upvotes": 5,
            "discussionId": "6715fe026757313a2bca598b"
        },
        "publishedAt": "2024-10-21T05:42:52.833Z",
        "title": "DPLM-2: A Multimodal Diffusion Protein Language Model",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.13782.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/z7OZ0L_yl8UFzWKAWebNW.jpeg",
            "fullname": "Cheng-Yen Hsieh",
            "name": "chengyenhsieh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.10812",
            "authors": [
                {
                    "_id": "670e47a7ae6e15b48591a91b",
                    "name": "Haotian Tang",
                    "hidden": false
                },
                {
                    "_id": "670e47a7ae6e15b48591a91c",
                    "name": "Yecheng Wu",
                    "hidden": false
                },
                {
                    "_id": "670e47a7ae6e15b48591a91d",
                    "name": "Shang Yang",
                    "hidden": false
                },
                {
                    "_id": "670e47a7ae6e15b48591a91e",
                    "name": "Enze Xie",
                    "hidden": false
                },
                {
                    "_id": "670e47a7ae6e15b48591a91f",
                    "name": "Junsong Chen",
                    "hidden": false
                },
                {
                    "_id": "670e47a7ae6e15b48591a920",
                    "name": "Junyu Chen",
                    "hidden": false
                },
                {
                    "_id": "670e47a7ae6e15b48591a921",
                    "name": "Zhuoyang Zhang",
                    "hidden": false
                },
                {
                    "_id": "670e47a7ae6e15b48591a922",
                    "name": "Han Cai",
                    "hidden": false
                },
                {
                    "_id": "670e47a7ae6e15b48591a923",
                    "name": "Yao Lu",
                    "hidden": false
                },
                {
                    "_id": "670e47a7ae6e15b48591a924",
                    "name": "Song Han",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-14T17:59:42.000Z",
            "title": "HART: Efficient Visual Generation with Hybrid Autoregressive Transformer",
            "summary": "We introduce Hybrid Autoregressive Transformer (HART), an autoregressive (AR)\nvisual generation model capable of directly generating 1024x1024 images,\nrivaling diffusion models in image generation quality. Existing AR models face\nlimitations due to the poor image reconstruction quality of their discrete\ntokenizers and the prohibitive training costs associated with generating 1024px\nimages. To address these challenges, we present the hybrid tokenizer, which\ndecomposes the continuous latents from the autoencoder into two components:\ndiscrete tokens representing the big picture and continuous tokens representing\nthe residual components that cannot be represented by the discrete tokens. The\ndiscrete component is modeled by a scalable-resolution discrete AR model, while\nthe continuous component is learned with a lightweight residual diffusion\nmodule with only 37M parameters. Compared with the discrete-only VAR tokenizer,\nour hybrid approach improves reconstruction FID from 2.11 to 0.30 on MJHQ-30K,\nleading to a 31% generation FID improvement from 7.85 to 5.38. HART also\noutperforms state-of-the-art diffusion models in both FID and CLIP score, with\n4.5-7.7x higher throughput and 6.9-13.4x lower MACs. Our code is open sourced\nat https://github.com/mit-han-lab/hart.",
            "upvotes": 3,
            "discussionId": "670e47abae6e15b48591aa44"
        },
        "publishedAt": "2024-10-21T13:15:53.507Z",
        "title": "HART: Efficient Visual Generation with Hybrid Autoregressive Transformer",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.10812.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.14470",
            "authors": [
                {
                    "_id": "6716249236c0c14b5b1dc6a5",
                    "user": {
                        "_id": "6266c07e7a1f5a1562c4113b",
                        "avatarUrl": "/avatars/46ac3f22fb7ad1a5f7da00a6b31cc8f0.svg",
                        "isPro": false,
                        "fullname": "Paul Gavrikov",
                        "user": "paulgavrikov",
                        "type": "user"
                    },
                    "name": "Paul Gavrikov",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-21T12:26:52.565Z",
                    "hidden": false
                },
                {
                    "_id": "6716249236c0c14b5b1dc6a6",
                    "user": {
                        "_id": "67162cabea37ca179f5965c9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/7lKl5w8ecex4eP2I89MPz.png",
                        "isPro": false,
                        "fullname": "Shashank Agnihotri",
                        "user": "shashankskagnihotri",
                        "type": "user"
                    },
                    "name": "Shashank Agnihotri",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-21T12:26:58.209Z",
                    "hidden": false
                },
                {
                    "_id": "6716249236c0c14b5b1dc6a7",
                    "name": "Margret Keuper",
                    "hidden": false
                },
                {
                    "_id": "6716249236c0c14b5b1dc6a8",
                    "name": "Janis Keuper",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-18T13:54:46.000Z",
            "title": "How Do Training Methods Influence the Utilization of Vision Models?",
            "summary": "Not all learnable parameters (e.g., weights) contribute equally to a neural\nnetwork's decision function. In fact, entire layers' parameters can sometimes\nbe reset to random values with little to no impact on the model's decisions. We\nrevisit earlier studies that examined how architecture and task complexity\ninfluence this phenomenon and ask: is this phenomenon also affected by how we\ntrain the model? We conducted experimental evaluations on a diverse set of\nImageNet-1k classification models to explore this, keeping the architecture and\ntraining data constant but varying the training pipeline. Our findings reveal\nthat the training method strongly influences which layers become critical to\nthe decision function for a given task. For example, improved training regimes\nand self-supervised training increase the importance of early layers while\nsignificantly under-utilizing deeper layers. In contrast, methods such as\nadversarial training display an opposite trend. Our preliminary results extend\nprevious findings, offering a more nuanced understanding of the inner mechanics\nof neural networks.\n  Code: https://github.com/paulgavrikov/layer_criticality",
            "upvotes": 3,
            "discussionId": "6716249336c0c14b5b1dc6e9"
        },
        "publishedAt": "2024-10-21T08:23:59.537Z",
        "title": "How Do Training Methods Influence the Utilization of Vision Models?",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.14470.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/46ac3f22fb7ad1a5f7da00a6b31cc8f0.svg",
            "fullname": "Paul Gavrikov",
            "name": "paulgavrikov",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.13828",
            "authors": [
                {
                    "_id": "6711dffcff45f0171ce2a393",
                    "user": {
                        "_id": "65256f6ec6a6a53e2e693fc7",
                        "avatarUrl": "/avatars/928f29207c196bbffd53bb3b4a3bfd17.svg",
                        "isPro": false,
                        "fullname": "Hui Yuan",
                        "user": "huiyuan23",
                        "type": "user"
                    },
                    "name": "Hui Yuan",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2024-10-18T04:11:40.965Z",
                    "hidden": false
                },
                {
                    "_id": "6711dffcff45f0171ce2a394",
                    "user": {
                        "_id": "6245285af59b8d262df3321b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6245285af59b8d262df3321b/dvy__dTf-miJ60IbveDg4.jpeg",
                        "isPro": false,
                        "fullname": "Yifan Zeng",
                        "user": "yokey",
                        "type": "user"
                    },
                    "name": "Yifan Zeng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-18T08:09:54.399Z",
                    "hidden": false
                },
                {
                    "_id": "6711dffcff45f0171ce2a395",
                    "name": "Yue Wu",
                    "hidden": false
                },
                {
                    "_id": "6711dffcff45f0171ce2a396",
                    "name": "Huazheng Wang",
                    "hidden": false
                },
                {
                    "_id": "6711dffcff45f0171ce2a397",
                    "user": {
                        "_id": "6599415e8c8ac79295e0b5e3",
                        "avatarUrl": "/avatars/85500bc8d2cd51444adcc19b1f8db313.svg",
                        "isPro": false,
                        "fullname": "Mengdi Wang",
                        "user": "Edify-Kd2024",
                        "type": "user"
                    },
                    "name": "Mengdi Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-21T12:17:19.140Z",
                    "hidden": false
                },
                {
                    "_id": "6711dffcff45f0171ce2a398",
                    "user": {
                        "_id": "62791751bba7c62cbab1076e",
                        "avatarUrl": "/avatars/9efb4f809d8961b47d3ae687f6e8dc2f.svg",
                        "isPro": false,
                        "fullname": "Liu Leqi",
                        "user": "leqiliu",
                        "type": "user"
                    },
                    "name": "Liu Leqi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-21T12:18:14.977Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-17T17:52:01.000Z",
            "title": "A Common Pitfall of Margin-based Language Model Alignment: Gradient\n  Entanglement",
            "summary": "Reinforcement Learning from Human Feedback (RLHF) has become the predominant\napproach for language model (LM) alignment. At its core, RLHF uses a\nmargin-based loss for preference optimization, specifying ideal LM behavior\nonly by the difference between preferred and dispreferred responses. In this\npaper, we identify a common pitfall of margin-based methods -- the\nunder-specification of ideal LM behavior on preferred and dispreferred\nresponses individually, which leads to two unintended consequences as the\nmargin increases: (1) The probability of dispreferred (e.g., unsafe) responses\nmay increase, resulting in potential safety alignment failures. (2) The\nprobability of preferred responses may decrease, even when those responses are\nideal. We demystify the reasons behind these problematic behaviors:\nmargin-based losses couple the change in the preferred probability to the\ngradient of the dispreferred one, and vice versa, often preventing the\npreferred probability from increasing while the dispreferred one decreases, and\nthus causing a synchronized increase or decrease in both probabilities. We term\nthis effect, inherent in margin-based objectives, gradient entanglement.\nFormally, we derive conditions for general margin-based alignment objectives\nunder which gradient entanglement becomes concerning: the inner product of the\ngradients of preferred and dispreferred log-probabilities is large relative to\nthe individual gradient norms. We theoretically investigate why such inner\nproducts can be large when aligning language models and empirically validate\nour findings. Empirical implications of our framework extend to explaining\nimportant differences in the training dynamics of various preference\noptimization algorithms, and suggesting potential algorithm designs to mitigate\nthe under-specification issue of margin-based methods and thereby improving\nlanguage model alignment.",
            "upvotes": 3,
            "discussionId": "6711dffcff45f0171ce2a3cb"
        },
        "publishedAt": "2024-10-21T05:50:33.155Z",
        "title": "A Common Pitfall of Margin-based Language Model Alignment: Gradient Entanglement",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.13828.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6245285af59b8d262df3321b/dvy__dTf-miJ60IbveDg4.jpeg",
            "fullname": "Yifan Zeng",
            "name": "yokey",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.12791",
            "authors": [
                {
                    "_id": "671219417b8d16aeed706386",
                    "user": {
                        "_id": "62b5cade593a2c49da706c8c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b5cade593a2c49da706c8c/xSfvuAtTocNyWSWWyx1tj.jpeg",
                        "isPro": false,
                        "fullname": "Ross Kristensen-McLachlan",
                        "user": "rdkm89",
                        "type": "user"
                    },
                    "name": "Ross Deans Kristensen-McLachlan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-21T12:27:23.866Z",
                    "hidden": false
                },
                {
                    "_id": "671219417b8d16aeed706387",
                    "user": {
                        "_id": "6580701a1fb6cfa0b46c61ac",
                        "avatarUrl": "/avatars/359b9aa336eed1504ee57969fbe3ec52.svg",
                        "isPro": false,
                        "fullname": "Rebecca M. M. Hicke",
                        "user": "rmmhicke",
                        "type": "user"
                    },
                    "name": "Rebecca M. M. Hicke",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-21T12:25:50.898Z",
                    "hidden": false
                },
                {
                    "_id": "671219417b8d16aeed706388",
                    "user": {
                        "_id": "62696cd3d1ac0cde59280dcf",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656940964610-62696cd3d1ac0cde59280dcf.jpeg",
                        "isPro": false,
                        "fullname": "Márton Kardos",
                        "user": "kardosdrur",
                        "type": "user"
                    },
                    "name": "Márton Kardos",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-21T07:48:09.117Z",
                    "hidden": false
                },
                {
                    "_id": "671219417b8d16aeed706389",
                    "name": "Mette Thunø",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-16T17:59:52.000Z",
            "title": "Context is Key(NMF): Modelling Topical Information Dynamics in Chinese\n  Diaspora Media",
            "summary": "Does the People's Republic of China (PRC) interfere with European elections\nthrough ethnic Chinese diaspora media? This question forms the basis of an\nongoing research project exploring how PRC narratives about European elections\nare represented in Chinese diaspora media, and thus the objectives of PRC news\nmedia manipulation. In order to study diaspora media efficiently and at scale,\nit is necessary to use techniques derived from quantitative text analysis, such\nas topic modelling. In this paper, we present a pipeline for studying\ninformation dynamics in Chinese media. Firstly, we present KeyNMF, a new\napproach to static and dynamic topic modelling using transformer-based\ncontextual embedding models. We provide benchmark evaluations to demonstrate\nthat our approach is competitive on a number of Chinese datasets and metrics.\nSecondly, we integrate KeyNMF with existing methods for describing information\ndynamics in complex systems. We apply this pipeline to data from five news\nsites, focusing on the period of time leading up to the 2024 European\nparliamentary elections. Our methods and results demonstrate the effectiveness\nof KeyNMF for studying information dynamics in Chinese media and lay groundwork\nfor further work addressing the broader research questions.",
            "upvotes": 3,
            "discussionId": "671219427b8d16aeed7063cb"
        },
        "publishedAt": "2024-10-21T05:00:34.259Z",
        "title": "Context is Key(NMF): Modelling Topical Information Dynamics in Chinese Diaspora Media",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.12791.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656940964610-62696cd3d1ac0cde59280dcf.jpeg",
            "fullname": "Márton Kardos",
            "name": "kardosdrur",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.14208",
            "authors": [
                {
                    "_id": "671664c4f262be795484c53c",
                    "name": "Xiaochuan Li",
                    "hidden": false
                },
                {
                    "_id": "671664c4f262be795484c53d",
                    "name": "Zichun Yu",
                    "hidden": false
                },
                {
                    "_id": "671664c4f262be795484c53e",
                    "name": "Chenyan Xiong",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-18T06:50:15.000Z",
            "title": "Montessori-Instruct: Generate Influential Training Data Tailored for\n  Student Learning",
            "summary": "Synthetic data has been widely used to train large language models, but their\ngenerative nature inevitably introduces noisy, non-informative, and misleading\nlearning signals. In this paper, we propose Montessori-Instruct, a novel data\nsynthesis framework that tailors the data synthesis ability of the teacher\nlanguage model toward the student language model's learning process.\nSpecifically, we utilize local data influence of synthetic training data points\non students to characterize students' learning preferences. Then, we train the\nteacher model with Direct Preference Optimization (DPO) to generate synthetic\ndata tailored toward student learning preferences. Experiments with\nLlama3-8B-Instruct (teacher) and Llama3-8B (student) on Alpaca Eval and\nMT-Bench demonstrate that Montessori-Instruct significantly outperforms\nstandard synthesis methods by 18.35\\% and 46.24\\% relatively. Our method also\nbeats data synthesized by a stronger teacher model, GPT-4o. Further analysis\nconfirms the benefits of teacher's learning to generate more influential\ntraining data in the student's improved learning, the advantages of local data\ninfluence in accurately measuring student preferences, and the robustness of\nMontessori-Instruct across different student models. Our code and data are\nopen-sourced at https://github.com/cxcscmu/Montessori-Instruct.",
            "upvotes": 1,
            "discussionId": "671664c4f262be795484c582"
        },
        "publishedAt": "2024-10-21T12:58:48.559Z",
        "title": "Montessori-Instruct: Generate Influential Training Data Tailored for Student Learning",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.14208.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/ba0eb4fc712a8b9b93ceb30d11859ec2.svg",
            "fullname": "Jason Lee",
            "name": "lixiaochuan2020",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.14596",
            "authors": [
                {
                    "_id": "67165dcc03d83371d444d2c2",
                    "name": "Elias Stengel-Eskin",
                    "hidden": false
                },
                {
                    "_id": "67165dcc03d83371d444d2c3",
                    "name": "Peter Hase",
                    "hidden": false
                },
                {
                    "_id": "67165dcc03d83371d444d2c4",
                    "name": "Mohit Bansal",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-18T16:49:36.000Z",
            "title": "Teaching Models to Balance Resisting and Accepting Persuasion",
            "summary": "Large language models (LLMs) are susceptible to persuasion, which can pose\nrisks when models are faced with an adversarial interlocutor. We take a first\nstep towards defending models against persuasion while also arguing that\ndefense against adversarial (i.e. negative) persuasion is only half of the\nequation: models should also be able to accept beneficial (i.e. positive)\npersuasion to improve their answers. We show that optimizing models for only\none side results in poor performance on the other. In order to balance positive\nand negative persuasion, we introduce Persuasion-Balanced Training (or PBT),\nwhich leverages multi-agent recursive dialogue trees to create data and trains\nmodels via preference optimization to accept persuasion when appropriate. PBT\nconsistently improves resistance to misinformation and resilience to being\nchallenged while also resulting in the best overall performance on holistic\ndata containing both positive and negative persuasion. Crucially, we show that\nPBT models are better teammates in multi-agent debates. We find that without\nPBT, pairs of stronger and weaker models have unstable performance, with the\norder in which the models present their answers determining whether the team\nobtains the stronger or weaker model's performance. PBT leads to better and\nmore stable results and less order dependence, with the stronger model\nconsistently pulling the weaker one up.",
            "upvotes": 1,
            "discussionId": "67165dcd03d83371d444d315"
        },
        "publishedAt": "2024-10-21T12:28:08.253Z",
        "title": "Teaching Models to Balance Resisting and Accepting Persuasion",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/61781c4caf41befe8ff060e8/syol95kLhp9golraZKGcM.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.14596.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/8871d7b046fc28cbc8638228da8e9737.svg",
            "fullname": "Elias Stengel-Eskin",
            "name": "esteng",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.13787",
            "authors": [
                {
                    "_id": "671630274dd96ad5eaed5ab8",
                    "name": "Felix J Binder",
                    "hidden": false
                },
                {
                    "_id": "671630274dd96ad5eaed5ab9",
                    "user": {
                        "_id": "63f4497e520c146189300f56",
                        "avatarUrl": "/avatars/4647c756e49d3c658567ec5498b2a044.svg",
                        "isPro": false,
                        "fullname": "James Chua",
                        "user": "thejaminator",
                        "type": "user"
                    },
                    "name": "James Chua",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-21T11:42:03.726Z",
                    "hidden": false
                },
                {
                    "_id": "671630274dd96ad5eaed5aba",
                    "name": "Tomek Korbak",
                    "hidden": false
                },
                {
                    "_id": "671630274dd96ad5eaed5abb",
                    "name": "Henry Sleight",
                    "hidden": false
                },
                {
                    "_id": "671630274dd96ad5eaed5abc",
                    "name": "John Hughes",
                    "hidden": false
                },
                {
                    "_id": "671630274dd96ad5eaed5abd",
                    "name": "Robert Long",
                    "hidden": false
                },
                {
                    "_id": "671630274dd96ad5eaed5abe",
                    "name": "Ethan Perez",
                    "hidden": false
                },
                {
                    "_id": "671630274dd96ad5eaed5abf",
                    "name": "Miles Turpin",
                    "hidden": false
                },
                {
                    "_id": "671630274dd96ad5eaed5ac0",
                    "name": "Owain Evans",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-17T17:24:10.000Z",
            "title": "Looking Inward: Language Models Can Learn About Themselves by\n  Introspection",
            "summary": "Humans acquire knowledge by observing the external world, but also by\nintrospection. Introspection gives a person privileged access to their current\nstate of mind (e.g., thoughts and feelings) that is not accessible to external\nobservers. Can LLMs introspect? We define introspection as acquiring knowledge\nthat is not contained in or derived from training data but instead originates\nfrom internal states. Such a capability could enhance model interpretability.\nInstead of painstakingly analyzing a model's internal workings, we could simply\nask the model about its beliefs, world models, and goals. More speculatively,\nan introspective model might self-report on whether it possesses certain\ninternal states such as subjective feelings or desires and this could inform us\nabout the moral status of these states. Such self-reports would not be entirely\ndictated by the model's training data.\n  We study introspection by finetuning LLMs to predict properties of their own\nbehavior in hypothetical scenarios. For example, \"Given the input P, would your\noutput favor the short- or long-term option?\" If a model M1 can introspect, it\nshould outperform a different model M2 in predicting M1's behavior even if M2\nis trained on M1's ground-truth behavior. The idea is that M1 has privileged\naccess to its own behavioral tendencies, and this enables it to predict itself\nbetter than M2 (even if M2 is generally stronger).\n  In experiments with GPT-4, GPT-4o, and Llama-3 models (each finetuned to\npredict itself), we find that the model M1 outperforms M2 in predicting itself,\nproviding evidence for introspection. Notably, M1 continues to predict its\nbehavior accurately even after we intentionally modify its ground-truth\nbehavior. However, while we successfully elicit introspection on simple tasks,\nwe are unsuccessful on more complex tasks or those requiring\nout-of-distribution generalization.",
            "upvotes": 1,
            "discussionId": "671630294dd96ad5eaed5b4f"
        },
        "publishedAt": "2024-10-21T11:27:51.996Z",
        "title": "Looking Inward: Language Models Can Learn About Themselves by Introspection",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.13787.png",
        "numComments": 10,
        "submittedBy": {
            "avatarUrl": "/avatars/4647c756e49d3c658567ec5498b2a044.svg",
            "fullname": "James Chua",
            "name": "thejaminator",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.11331",
            "authors": [
                {
                    "_id": "67161f7b092a4eca797430b0",
                    "user": {
                        "_id": "63d9e09f1cae35c27bf80cb2",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675223055197-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Syed Abdul Gaffar Shakhadri",
                        "user": "SyedAbdul",
                        "type": "user"
                    },
                    "name": "Syed Abdul Gaffar Shakhadri",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-21T16:13:33.968Z",
                    "hidden": false
                },
                {
                    "_id": "67161f7b092a4eca797430b1",
                    "name": "Kruthika KR",
                    "hidden": false
                },
                {
                    "_id": "67161f7b092a4eca797430b2",
                    "name": "Rakshit Aralimatti",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-15T06:59:44.000Z",
            "title": "SHAKTI: A 2.5 Billion Parameter Small Language Model Optimized for Edge\n  AI and Low-Resource Environments",
            "summary": "We introduce Shakti, a 2.5 billion parameter language model specifically\noptimized for resource-constrained environments such as edge devices, including\nsmartphones, wearables, and IoT systems. Shakti combines high-performance NLP\nwith optimized efficiency and precision, making it ideal for real-time AI\napplications where computational resources and memory are limited. With support\nfor vernacular languages and domain-specific tasks, Shakti excels in industries\nsuch as healthcare, finance, and customer service. Benchmark evaluations\ndemonstrate that Shakti performs competitively against larger models while\nmaintaining low latency and on-device efficiency, positioning it as a leading\nsolution for edge AI.",
            "upvotes": 0,
            "discussionId": "67161f7c092a4eca797430eb"
        },
        "publishedAt": "2024-10-21T14:19:02.341Z",
        "title": "SHAKTI: A 2.5 Billion Parameter Small Language Model Optimized for Edge AI and Low-Resource Environments",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.11331.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675223055197-noauth.jpeg",
            "fullname": "Syed Abdul Gaffar Shakhadri",
            "name": "SyedAbdul",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.14672",
            "authors": [
                {
                    "_id": "6715b0bc7dfe714b42e6fc6c",
                    "user": {
                        "_id": "63b67ab2877d94d5c04946e1",
                        "avatarUrl": "/avatars/56863b04863969b9aaa35194e25d1e82.svg",
                        "isPro": false,
                        "fullname": "Shaozhe Hao",
                        "user": "haoosz",
                        "type": "user"
                    },
                    "name": "Shaozhe Hao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-21T16:13:37.342Z",
                    "hidden": false
                },
                {
                    "_id": "6715b0bc7dfe714b42e6fc6d",
                    "name": "Xuantong Liu",
                    "hidden": false
                },
                {
                    "_id": "6715b0bc7dfe714b42e6fc6e",
                    "user": {
                        "_id": "6494483aa13255720397287a",
                        "avatarUrl": "/avatars/61ff2e0371df513194246cf6fbb2b78a.svg",
                        "isPro": false,
                        "fullname": "Xianbiao Qi",
                        "user": "qixianbiao",
                        "type": "user"
                    },
                    "name": "Xianbiao Qi",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2024-10-21T01:39:11.134Z",
                    "hidden": false
                },
                {
                    "_id": "6715b0bc7dfe714b42e6fc6f",
                    "name": "Shihao Zhao",
                    "hidden": false
                },
                {
                    "_id": "6715b0bc7dfe714b42e6fc70",
                    "name": "Bojia Zi",
                    "hidden": false
                },
                {
                    "_id": "6715b0bc7dfe714b42e6fc71",
                    "name": "Rong Xiao",
                    "hidden": false
                },
                {
                    "_id": "6715b0bc7dfe714b42e6fc72",
                    "name": "Kai Han",
                    "hidden": false
                },
                {
                    "_id": "6715b0bc7dfe714b42e6fc73",
                    "name": "Kwan-Yee K. Wong",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-18T17:59:04.000Z",
            "title": "BiGR: Harnessing Binary Latent Codes for Image Generation and Improved\n  Visual Representation Capabilities",
            "summary": "We introduce BiGR, a novel conditional image generation model using compact\nbinary latent codes for generative training, focusing on enhancing both\ngeneration and representation capabilities. BiGR is the first conditional\ngenerative model that unifies generation and discrimination within the same\nframework. BiGR features a binary tokenizer, a masked modeling mechanism, and a\nbinary transcoder for binary code prediction. Additionally, we introduce a\nnovel entropy-ordered sampling method to enable efficient image generation.\nExtensive experiments validate BiGR's superior performance in generation\nquality, as measured by FID-50k, and representation capabilities, as evidenced\nby linear-probe accuracy. Moreover, BiGR showcases zero-shot generalization\nacross various vision tasks, enabling applications such as image inpainting,\noutpainting, editing, interpolation, and enrichment, without the need for\nstructural modifications. Our findings suggest that BiGR unifies generative and\ndiscriminative tasks effectively, paving the way for further advancements in\nthe field.",
            "upvotes": 0,
            "discussionId": "6715b0bf7dfe714b42e6fd2f"
        },
        "publishedAt": "2024-10-21T13:05:25.714Z",
        "title": "BiGR: Harnessing Binary Latent Codes for Image Generation and Improved Visual Representation Capabilities",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.14672.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/56863b04863969b9aaa35194e25d1e82.svg",
            "fullname": "Shaozhe Hao",
            "name": "haoosz",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    }
]