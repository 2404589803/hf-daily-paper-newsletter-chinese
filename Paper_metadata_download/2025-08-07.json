[
  {
    "paper": {
      "id": "2508.04026",
      "authors": [
        {
          "_id": "68941e5d741a16f544fbceed",
          "name": "Shunyu Liu",
          "hidden": false
        },
        {
          "_id": "68941e5d741a16f544fbceee",
          "name": "Minghao Liu",
          "hidden": false
        },
        {
          "_id": "68941e5d741a16f544fbceef",
          "name": "Huichi Zhou",
          "hidden": false
        },
        {
          "_id": "68941e5d741a16f544fbcef0",
          "name": "Zhenyu Cui",
          "hidden": false
        },
        {
          "_id": "68941e5d741a16f544fbcef1",
          "name": "Yang Zhou",
          "hidden": false
        },
        {
          "_id": "68941e5d741a16f544fbcef2",
          "name": "Yuhao Zhou",
          "hidden": false
        },
        {
          "_id": "68941e5d741a16f544fbcef3",
          "name": "Wendong Fan",
          "hidden": false
        },
        {
          "_id": "68941e5d741a16f544fbcef4",
          "name": "Ge Zhang",
          "hidden": false
        },
        {
          "_id": "68941e5d741a16f544fbcef5",
          "name": "Jiajun Shi",
          "hidden": false
        },
        {
          "_id": "68941e5d741a16f544fbcef6",
          "name": "Weihao Xuan",
          "hidden": false
        },
        {
          "_id": "68941e5d741a16f544fbcef7",
          "name": "Jiaxing Huang",
          "hidden": false
        },
        {
          "_id": "68941e5d741a16f544fbcef8",
          "name": "Shuang Luo",
          "hidden": false
        },
        {
          "_id": "68941e5d741a16f544fbcef9",
          "name": "Fang Wu",
          "hidden": false
        },
        {
          "_id": "68941e5d741a16f544fbcefa",
          "name": "Heli Qi",
          "hidden": false
        },
        {
          "_id": "68941e5d741a16f544fbcefb",
          "name": "Qingcheng Zeng",
          "hidden": false
        },
        {
          "_id": "68941e5d741a16f544fbcefc",
          "name": "Ziqi Ren",
          "hidden": false
        },
        {
          "_id": "68941e5d741a16f544fbcefd",
          "name": "Jialiang Gao",
          "hidden": false
        },
        {
          "_id": "68941e5d741a16f544fbcefe",
          "name": "Jindi Lv",
          "hidden": false
        },
        {
          "_id": "68941e5d741a16f544fbceff",
          "name": "Junjie Wang",
          "hidden": false
        },
        {
          "_id": "68941e5d741a16f544fbcf00",
          "name": "Aosong Feng",
          "hidden": false
        },
        {
          "_id": "68941e5d741a16f544fbcf01",
          "name": "Heng Zhou",
          "hidden": false
        },
        {
          "_id": "68941e5d741a16f544fbcf02",
          "name": "Wangchunshu Zhou",
          "hidden": false
        },
        {
          "_id": "68941e5d741a16f544fbcf03",
          "name": "Zhenfei Yin",
          "hidden": false
        },
        {
          "_id": "68941e5d741a16f544fbcf04",
          "name": "Wenlong Zhang",
          "hidden": false
        },
        {
          "_id": "68941e5d741a16f544fbcf05",
          "name": "Guohao Li",
          "hidden": false
        },
        {
          "_id": "68941e5d741a16f544fbcf06",
          "name": "Wenhao Yu",
          "hidden": false
        },
        {
          "_id": "68941e5d741a16f544fbcf07",
          "name": "Irene Li",
          "hidden": false
        },
        {
          "_id": "68941e5d741a16f544fbcf08",
          "name": "Lei Ma",
          "hidden": false
        },
        {
          "_id": "68941e5d741a16f544fbcf09",
          "name": "Lei Bai",
          "hidden": false
        },
        {
          "_id": "68941e5d741a16f544fbcf0a",
          "name": "Qunshu Lin",
          "hidden": false
        },
        {
          "_id": "68941e5d741a16f544fbcf0b",
          "name": "Mingli Song",
          "hidden": false
        },
        {
          "_id": "68941e5d741a16f544fbcf0c",
          "name": "Dacheng Tao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-06T02:38:18.000Z",
      "submittedOnDailyAt": "2025-08-07T02:06:19.798Z",
      "title": "VeriGUI: Verifiable Long-Chain GUI Dataset",
      "submittedOnDailyBy": {
        "_id": "6713afea187a20dc579e121b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6713afea187a20dc579e121b/ELxVQLVF9ifuT-TCWPK22.jpeg",
        "isPro": false,
        "fullname": "Shunyu Liu",
        "user": "liushunyu",
        "type": "user"
      },
      "summary": "Recent studies have delved into constructing autonomous agents capable of\nperforming complex Graphical User Interface (GUI)-based computer tasks, with\nthe potential to revolutionize human-computer interaction. Despite encouraging\nresults, existing efforts mainly focus on short-term interactions and rely on\noutcome-only verification, thereby limiting their scalability in real-world GUI\napplications that demand long-horizon task decomposition and execution. In this\nwork, we introduce VeriGUI, a novel verifiable long-chain GUI dataset designed\nto facilitate the development and evaluation of generalist GUI agents operating\nin realistic computer environments. Our dataset emphasizes two critical\ndimensions: (1) long-chain complexity, with tasks decomposed into a sequence of\ninterdependent subtasks spanning hundreds of steps, explicitly designed to\nallow any subtask to serve as a valid starting point; and (2) subtask-level\nverifiability, which enables diverse exploration strategies within each\nsubtask, while ensuring that each subtask-level goal remains verifiable and\nconsistent. The dataset consists of GUI task trajectories across both desktop\nand web, annotated by human experts. Extensive experiments on VeriGUI using\nvarious agents with different foundation models reveal significant performance\ngaps in handling long-horizon tasks, highlighting the need for more robust\nplanning and decision-making capabilities in GUI agents.",
      "upvotes": 49,
      "discussionId": "68941e5e741a16f544fbcf0d",
      "githubRepo": "https://github.com/VeriGUI-Team/VeriGUI",
      "ai_summary": "VeriGUI is a novel dataset for evaluating GUI agents in long-horizon tasks, emphasizing long-chain complexity and subtask-level verifiability.",
      "ai_keywords": [
        "Graphical User Interface (GUI)",
        "GUI agents",
        "long-chain complexity",
        "subtask-level verifiability",
        "GUI task trajectories",
        "desktop",
        "web",
        "human experts",
        "long-horizon tasks",
        "robust planning",
        "decision-making capabilities"
      ],
      "githubStars": 48
    },
    "publishedAt": "2025-08-05T22:38:18.000Z",
    "title": "VeriGUI: Verifiable Long-Chain GUI Dataset",
    "summary": "Recent studies have delved into constructing autonomous agents capable of\nperforming complex Graphical User Interface (GUI)-based computer tasks, with\nthe potential to revolutionize human-computer interaction. Despite encouraging\nresults, existing efforts mainly focus on short-term interactions and rely on\noutcome-only verification, thereby limiting their scalability in real-world GUI\napplications that demand long-horizon task decomposition and execution. In this\nwork, we introduce VeriGUI, a novel verifiable long-chain GUI dataset designed\nto facilitate the development and evaluation of generalist GUI agents operating\nin realistic computer environments. Our dataset emphasizes two critical\ndimensions: (1) long-chain complexity, with tasks decomposed into a sequence of\ninterdependent subtasks spanning hundreds of steps, explicitly designed to\nallow any subtask to serve as a valid starting point; and (2) subtask-level\nverifiability, which enables diverse exploration strategies within each\nsubtask, while ensuring that each subtask-level goal remains verifiable and\nconsistent. The dataset consists of GUI task trajectories across both desktop\nand web, annotated by human experts. Extensive experiments on VeriGUI using\nvarious agents with different foundation models reveal significant performance\ngaps in handling long-horizon tasks, highlighting the need for more robust\nplanning and decision-making capabilities in GUI agents.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.04026.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "6713afea187a20dc579e121b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6713afea187a20dc579e121b/ELxVQLVF9ifuT-TCWPK22.jpeg",
      "fullname": "Shunyu Liu",
      "name": "liushunyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.04700",
      "authors": [
        {
          "_id": "68941f54741a16f544fbcf0f",
          "name": "Zeyi Sun",
          "hidden": false
        },
        {
          "_id": "68941f54741a16f544fbcf10",
          "name": "Ziyu Liu",
          "hidden": false
        },
        {
          "_id": "68941f54741a16f544fbcf11",
          "name": "Yuhang Zang",
          "hidden": false
        },
        {
          "_id": "68941f54741a16f544fbcf12",
          "name": "Yuhang Cao",
          "hidden": false
        },
        {
          "_id": "68941f54741a16f544fbcf13",
          "name": "Xiaoyi Dong",
          "hidden": false
        },
        {
          "_id": "68941f54741a16f544fbcf14",
          "name": "Tong Wu",
          "hidden": false
        },
        {
          "_id": "68941f54741a16f544fbcf15",
          "name": "Dahua Lin",
          "hidden": false
        },
        {
          "_id": "68941f54741a16f544fbcf16",
          "name": "Jiaqi Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-06T17:58:46.000Z",
      "submittedOnDailyAt": "2025-08-07T02:09:08.225Z",
      "title": "SEAgent: Self-Evolving Computer Use Agent with Autonomous Learning from\n  Experience",
      "submittedOnDailyBy": {
        "_id": "63fda3fced9eead590ff6918",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677566802735-noauth.jpeg",
        "isPro": false,
        "fullname": "Zeyi Sun",
        "user": "Zery",
        "type": "user"
      },
      "summary": "Repurposing large vision-language models (LVLMs) as computer use agents\n(CUAs) has led to substantial breakthroughs, primarily driven by human-labeled\ndata. However, these models often struggle with novel and specialized software,\nparticularly in scenarios lacking human annotations. To address this challenge,\nwe propose SEAgent, an agentic self-evolving framework enabling CUAs to\nautonomously evolve through interactions with unfamiliar software.\nSpecifically, SEAgent empowers computer-use agents to autonomously master novel\nsoftware environments via experiential learning, where agents explore new\nsoftware, learn through iterative trial-and-error, and progressively tackle\nauto-generated tasks organized from simple to complex. To achieve this goal, we\ndesign a World State Model for step-wise trajectory assessment, along with a\nCurriculum Generator that generates increasingly diverse and challenging tasks.\nThe agent's policy is updated through experiential learning, comprised of\nadversarial imitation of failure actions and Group Relative Policy Optimization\n(GRPO) on successful ones. Furthermore, we introduce a specialist-to-generalist\ntraining strategy that integrates individual experiential insights from\nspecialist agents, facilitating the development of a stronger generalist CUA\ncapable of continuous autonomous evolution. This unified agent ultimately\nachieves performance surpassing ensembles of individual specialist agents on\ntheir specialized software. We validate the effectiveness of SEAgent across\nfive novel software environments within OS-World. Our approach achieves a\nsignificant improvement of 23.2% in success rate, from 11.3% to 34.5%, over a\ncompetitive open-source CUA, i.e., UI-TARS.",
      "upvotes": 27,
      "discussionId": "68941f54741a16f544fbcf17",
      "projectPage": "https://github.com/SunzeY/SEAgent",
      "githubRepo": "https://github.com/SunzeY/SEAgent",
      "ai_summary": "SEAgent, an agentic self-evolving framework, enables computer-use agents to autonomously master novel software through experiential learning and a curriculum of tasks, achieving superior performance compared to existing methods.",
      "ai_keywords": [
        "vision-language models",
        "computer use agents",
        "SEAgent",
        "experiential learning",
        "World State Model",
        "Curriculum Generator",
        "adversarial imitation",
        "Group Relative Policy Optimization",
        "specialist-to-generalist training",
        "OS-World",
        "UI-TARS"
      ],
      "githubStars": 8
    },
    "publishedAt": "2025-08-06T13:58:46.000Z",
    "title": "SEAgent: Self-Evolving Computer Use Agent with Autonomous Learning from\n  Experience",
    "summary": "Repurposing large vision-language models (LVLMs) as computer use agents\n(CUAs) has led to substantial breakthroughs, primarily driven by human-labeled\ndata. However, these models often struggle with novel and specialized software,\nparticularly in scenarios lacking human annotations. To address this challenge,\nwe propose SEAgent, an agentic self-evolving framework enabling CUAs to\nautonomously evolve through interactions with unfamiliar software.\nSpecifically, SEAgent empowers computer-use agents to autonomously master novel\nsoftware environments via experiential learning, where agents explore new\nsoftware, learn through iterative trial-and-error, and progressively tackle\nauto-generated tasks organized from simple to complex. To achieve this goal, we\ndesign a World State Model for step-wise trajectory assessment, along with a\nCurriculum Generator that generates increasingly diverse and challenging tasks.\nThe agent's policy is updated through experiential learning, comprised of\nadversarial imitation of failure actions and Group Relative Policy Optimization\n(GRPO) on successful ones. Furthermore, we introduce a specialist-to-generalist\ntraining strategy that integrates individual experiential insights from\nspecialist agents, facilitating the development of a stronger generalist CUA\ncapable of continuous autonomous evolution. This unified agent ultimately\nachieves performance surpassing ensembles of individual specialist agents on\ntheir specialized software. We validate the effectiveness of SEAgent across\nfive novel software environments within OS-World. Our approach achieves a\nsignificant improvement of 23.2% in success rate, from 11.3% to 34.5%, over a\ncompetitive open-source CUA, i.e., UI-TARS.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.04700.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63fda3fced9eead590ff6918",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677566802735-noauth.jpeg",
      "fullname": "Zeyi Sun",
      "name": "Zery",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 20
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.01191",
      "authors": [
        {
          "_id": "689414b0741a16f544fbcec8",
          "name": "Chengshuai Zhao",
          "hidden": false
        },
        {
          "_id": "689414b0741a16f544fbcec9",
          "name": "Zhen Tan",
          "hidden": false
        },
        {
          "_id": "689414b0741a16f544fbceca",
          "name": "Pingchuan Ma",
          "hidden": false
        },
        {
          "_id": "689414b0741a16f544fbcecb",
          "name": "Dawei Li",
          "hidden": false
        },
        {
          "_id": "689414b0741a16f544fbcecc",
          "name": "Bohan Jiang",
          "hidden": false
        },
        {
          "_id": "689414b0741a16f544fbcecd",
          "name": "Yancheng Wang",
          "hidden": false
        },
        {
          "_id": "689414b0741a16f544fbcece",
          "name": "Yingzhen Yang",
          "hidden": false
        },
        {
          "_id": "689414b0741a16f544fbcecf",
          "name": "Huan Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-02T04:37:28.000Z",
      "submittedOnDailyAt": "2025-08-07T01:29:05.289Z",
      "title": "Is Chain-of-Thought Reasoning of LLMs a Mirage? A Data Distribution Lens",
      "submittedOnDailyBy": {
        "_id": "65b2fae679954e21ac426aec",
        "avatarUrl": "/avatars/c495c4ee9afb140a87ebc04602aa8c3e.svg",
        "isPro": false,
        "fullname": "Chengshuai Zhao",
        "user": "chengshuaizhao",
        "type": "user"
      },
      "summary": "Chain-of-Thought (CoT) prompting has been shown to improve Large Language\nModel (LLM) performance on various tasks. With this approach, LLMs appear to\nproduce human-like reasoning steps before providing answers (a.k.a., CoT\nreasoning), which often leads to the perception that they engage in deliberate\ninferential processes. However, some initial findings suggest that CoT\nreasoning may be more superficial than it appears, motivating us to explore\nfurther. In this paper, we study CoT reasoning via a data distribution lens and\ninvestigate if CoT reasoning reflects a structured inductive bias learned from\nin-distribution data, allowing the model to conditionally generate reasoning\npaths that approximate those seen during training. Thus, its effectiveness is\nfundamentally bounded by the degree of distribution discrepancy between the\ntraining data and the test queries. With this lens, we dissect CoT reasoning\nvia three dimensions: task, length, and format. To investigate each dimension,\nwe design DataAlchemy, an isolated and controlled environment to train LLMs\nfrom scratch and systematically probe them under various distribution\nconditions. Our results reveal that CoT reasoning is a brittle mirage that\nvanishes when it is pushed beyond training distributions. This work offers a\ndeeper understanding of why and when CoT reasoning fails, emphasizing the\nongoing challenge of achieving genuine and generalizable reasoning.",
      "upvotes": 24,
      "discussionId": "689414b0741a16f544fbced0",
      "githubRepo": "https://github.com/ChengshuaiZhao0/DataAlchemy",
      "ai_summary": "CoT reasoning in LLMs is found to be limited by the distribution discrepancy between training and test data, suggesting it is not a robust form of reasoning.",
      "ai_keywords": [
        "Chain-of-Thought",
        "Large Language Model",
        "CoT reasoning",
        "inductive bias",
        "DataAlchemy",
        "distribution discrepancy",
        "reasoning paths",
        "generalizable reasoning"
      ],
      "githubStars": 8
    },
    "publishedAt": "2025-08-02T00:37:28.000Z",
    "title": "Is Chain-of-Thought Reasoning of LLMs a Mirage? A Data Distribution Lens",
    "summary": "Chain-of-Thought (CoT) prompting has been shown to improve Large Language\nModel (LLM) performance on various tasks. With this approach, LLMs appear to\nproduce human-like reasoning steps before providing answers (a.k.a., CoT\nreasoning), which often leads to the perception that they engage in deliberate\ninferential processes. However, some initial findings suggest that CoT\nreasoning may be more superficial than it appears, motivating us to explore\nfurther. In this paper, we study CoT reasoning via a data distribution lens and\ninvestigate if CoT reasoning reflects a structured inductive bias learned from\nin-distribution data, allowing the model to conditionally generate reasoning\npaths that approximate those seen during training. Thus, its effectiveness is\nfundamentally bounded by the degree of distribution discrepancy between the\ntraining data and the test queries. With this lens, we dissect CoT reasoning\nvia three dimensions: task, length, and format. To investigate each dimension,\nwe design DataAlchemy, an isolated and controlled environment to train LLMs\nfrom scratch and systematically probe them under various distribution\nconditions. Our results reveal that CoT reasoning is a brittle mirage that\nvanishes when it is pushed beyond training distributions. This work offers a\ndeeper understanding of why and when CoT reasoning fails, emphasizing the\nongoing challenge of achieving genuine and generalizable reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.01191.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "65b2fae679954e21ac426aec",
      "avatarUrl": "/avatars/c495c4ee9afb140a87ebc04602aa8c3e.svg",
      "fullname": "Chengshuai Zhao",
      "name": "chengshuaizhao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.03159",
      "authors": [
        {
          "_id": "68942f2f741a16f544fbcf54",
          "name": "Jueon Park",
          "hidden": false
        },
        {
          "_id": "68942f2f741a16f544fbcf55",
          "name": "Yein Park",
          "hidden": false
        },
        {
          "_id": "68942f2f741a16f544fbcf56",
          "name": "Minju Song",
          "hidden": false
        },
        {
          "_id": "68942f2f741a16f544fbcf57",
          "name": "Soyon Park",
          "hidden": false
        },
        {
          "_id": "68942f2f741a16f544fbcf58",
          "name": "Donghyeon Lee",
          "hidden": false
        },
        {
          "_id": "68942f2f741a16f544fbcf59",
          "name": "Seungheun Baek",
          "hidden": false
        },
        {
          "_id": "68942f2f741a16f544fbcf5a",
          "name": "Jaewoo Kang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64e5c8e594aa0690321f6b29/_CtEK1w-EMh2t8mwZJWhe.jpeg"
      ],
      "publishedAt": "2025-08-05T07:04:44.000Z",
      "submittedOnDailyAt": "2025-08-07T03:18:00.323Z",
      "title": "CoTox: Chain-of-Thought-Based Molecular Toxicity Reasoning and\n  Prediction",
      "submittedOnDailyBy": {
        "_id": "64e5c8e594aa0690321f6b29",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/IW5LRzmPcAM-dri8taMN7.png",
        "isPro": false,
        "fullname": "Yein Park",
        "user": "P-YI",
        "type": "user"
      },
      "summary": "Drug toxicity remains a major challenge in pharmaceutical development. Recent\nmachine learning models have improved in silico toxicity prediction, but their\nreliance on annotated data and lack of interpretability limit their\napplicability. This limits their ability to capture organ-specific toxicities\ndriven by complex biological mechanisms. Large language models (LLMs) offer a\npromising alternative through step-by-step reasoning and integration of textual\ndata, yet prior approaches lack biological context and transparent rationale.\nTo address this issue, we propose CoTox, a novel framework that integrates LLM\nwith chain-of-thought (CoT) reasoning for multi-toxicity prediction. CoTox\ncombines chemical structure data, biological pathways, and gene ontology (GO)\nterms to generate interpretable toxicity predictions through step-by-step\nreasoning. Using GPT-4o, we show that CoTox outperforms both traditional\nmachine learning and deep learning model. We further examine its performance\nacross various LLMs to identify where CoTox is most effective. Additionally, we\nfind that representing chemical structures with IUPAC names, which are easier\nfor LLMs to understand than SMILES, enhances the model's reasoning ability and\nimproves predictive performance. To demonstrate its practical utility in drug\ndevelopment, we simulate the treatment of relevant cell types with drug and\nincorporated the resulting biological context into the CoTox framework. This\napproach allow CoTox to generate toxicity predictions aligned with\nphysiological responses, as shown in case study. This result highlights the\npotential of LLM-based frameworks to improve interpretability and support\nearly-stage drug safety assessment. The code and prompt used in this work are\navailable at https://github.com/dmis-lab/CoTox.",
      "upvotes": 17,
      "discussionId": "68942f2f741a16f544fbcf5b",
      "ai_summary": "CoTox, a framework integrating LLMs with chain-of-thought reasoning, enhances multi-toxicity prediction by incorporating chemical structure data, biological pathways, and gene ontology terms, improving interpretability and predictive performance in drug development.",
      "ai_keywords": [
        "large language models",
        "LLM",
        "chain-of-thought",
        "CoT reasoning",
        "multi-toxicity prediction",
        "chemical structure data",
        "biological pathways",
        "gene ontology",
        "GPT-4o",
        "IUPAC names",
        "SMILES",
        "toxicity predictions",
        "physiological responses"
      ]
    },
    "publishedAt": "2025-08-05T03:04:44.000Z",
    "title": "CoTox: Chain-of-Thought-Based Molecular Toxicity Reasoning and\n  Prediction",
    "summary": "Drug toxicity remains a major challenge in pharmaceutical development. Recent\nmachine learning models have improved in silico toxicity prediction, but their\nreliance on annotated data and lack of interpretability limit their\napplicability. This limits their ability to capture organ-specific toxicities\ndriven by complex biological mechanisms. Large language models (LLMs) offer a\npromising alternative through step-by-step reasoning and integration of textual\ndata, yet prior approaches lack biological context and transparent rationale.\nTo address this issue, we propose CoTox, a novel framework that integrates LLM\nwith chain-of-thought (CoT) reasoning for multi-toxicity prediction. CoTox\ncombines chemical structure data, biological pathways, and gene ontology (GO)\nterms to generate interpretable toxicity predictions through step-by-step\nreasoning. Using GPT-4o, we show that CoTox outperforms both traditional\nmachine learning and deep learning model. We further examine its performance\nacross various LLMs to identify where CoTox is most effective. Additionally, we\nfind that representing chemical structures with IUPAC names, which are easier\nfor LLMs to understand than SMILES, enhances the model's reasoning ability and\nimproves predictive performance. To demonstrate its practical utility in drug\ndevelopment, we simulate the treatment of relevant cell types with drug and\nincorporated the resulting biological context into the CoTox framework. This\napproach allow CoTox to generate toxicity predictions aligned with\nphysiological responses, as shown in case study. This result highlights the\npotential of LLM-based frameworks to improve interpretability and support\nearly-stage drug safety assessment. The code and prompt used in this work are\navailable at https://github.com/dmis-lab/CoTox.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64e5c8e594aa0690321f6b29/_CtEK1w-EMh2t8mwZJWhe.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.03159.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64e5c8e594aa0690321f6b29",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/IW5LRzmPcAM-dri8taMN7.png",
      "fullname": "Yein Park",
      "name": "P-YI",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.02694",
      "authors": [
        {
          "_id": "689385f0741a16f544fbcd8c",
          "name": "Ningning Wang",
          "hidden": false
        },
        {
          "_id": "689385f0741a16f544fbcd8d",
          "name": "Xavier Hu",
          "hidden": false
        },
        {
          "_id": "689385f0741a16f544fbcd8e",
          "name": "Pai Liu",
          "hidden": false
        },
        {
          "_id": "689385f0741a16f544fbcd8f",
          "name": "He Zhu",
          "hidden": false
        },
        {
          "_id": "689385f0741a16f544fbcd90",
          "name": "Yue Hou",
          "hidden": false
        },
        {
          "_id": "689385f0741a16f544fbcd91",
          "name": "Heyuan Huang",
          "hidden": false
        },
        {
          "_id": "689385f0741a16f544fbcd92",
          "name": "Shengyu Zhang",
          "hidden": false
        },
        {
          "_id": "689385f0741a16f544fbcd93",
          "name": "Jian Yang",
          "hidden": false
        },
        {
          "_id": "689385f0741a16f544fbcd94",
          "name": "Jiaheng Liu",
          "hidden": false
        },
        {
          "_id": "689385f0741a16f544fbcd95",
          "name": "Ge Zhang",
          "hidden": false
        },
        {
          "_id": "689385f0741a16f544fbcd96",
          "name": "Changwang Zhang",
          "hidden": false
        },
        {
          "_id": "689385f0741a16f544fbcd97",
          "name": "Jun Wang",
          "hidden": false
        },
        {
          "_id": "689385f0741a16f544fbcd98",
          "name": "Yuchen Eleanor Jiang",
          "hidden": false
        },
        {
          "_id": "689385f0741a16f544fbcd99",
          "name": "Wangchunshu Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-24T17:56:51.000Z",
      "submittedOnDailyAt": "2025-08-07T03:39:00.617Z",
      "title": "Efficient Agents: Building Effective Agents While Reducing Cost",
      "submittedOnDailyBy": {
        "_id": "6892b4459f604ad07412f117",
        "avatarUrl": "/avatars/fb9e54e1689b4add084e32d7e5dd2f16.svg",
        "isPro": false,
        "fullname": "Xavier Hu",
        "user": "xavier-hu",
        "type": "user"
      },
      "summary": "The remarkable capabilities of Large Language Model (LLM)-driven agents have\nenabled sophisticated systems to tackle complex, multi-step tasks, but their\nescalating costs threaten scalability and accessibility. This work presents the\nfirst systematic study of the efficiency-effectiveness trade-off in modern\nagent systems, addressing the critical need for cost-effective designs without\nsacrificing performance. We investigate three key questions: (1) How much\ncomplexity do agentic tasks inherently require? (2) When do additional modules\nyield diminishing returns? (3) How much efficiency can be gained through the\ndesign of efficient agent frameworks? Through an empirical analysis on the GAIA\nbenchmark, we evaluate the impact of LLM backbone selection, agent framework\ndesigns, and test-time scaling strategies. Using the cost-of-pass metric, we\nquantify the efficiency-performance trade-off across these dimensions. Our\nfindings inform the development of Efficient Agents , a novel agent framework\nthat has an optimal complexity to task requirements. Efficient Agents retains\n96.7% of the performance of OWL, one leading open-source agent framework, while\nreducing operational costs from 0.398 to 0.228, resulting in a 28.4%\nimprovement in cost-of-pass. Our work provides actionable insights for\ndesigning efficient, high-performing agent systems, advancing the accessibility\nand sustainability of AI-driven solutions.",
      "upvotes": 17,
      "discussionId": "689385f0741a16f544fbcd9a",
      "githubRepo": "https://github.com/OPPO-PersonalAI/OAgents",
      "ai_summary": "A study on the efficiency-effectiveness trade-off in LLM-driven agent systems identifies optimal agent framework design to reduce costs while maintaining performance.",
      "ai_keywords": [
        "Large Language Model",
        "LLM",
        "agent systems",
        "efficiency-effectiveness trade-off",
        "agentic tasks",
        "agent framework",
        "GAIA benchmark",
        "LLM backbone",
        "test-time scaling strategies",
        "cost-of-pass",
        "Efficient Agents",
        "OWL"
      ],
      "githubStars": 108
    },
    "publishedAt": "2025-07-24T13:56:51.000Z",
    "title": "Efficient Agents: Building Effective Agents While Reducing Cost",
    "summary": "The remarkable capabilities of Large Language Model (LLM)-driven agents have\nenabled sophisticated systems to tackle complex, multi-step tasks, but their\nescalating costs threaten scalability and accessibility. This work presents the\nfirst systematic study of the efficiency-effectiveness trade-off in modern\nagent systems, addressing the critical need for cost-effective designs without\nsacrificing performance. We investigate three key questions: (1) How much\ncomplexity do agentic tasks inherently require? (2) When do additional modules\nyield diminishing returns? (3) How much efficiency can be gained through the\ndesign of efficient agent frameworks? Through an empirical analysis on the GAIA\nbenchmark, we evaluate the impact of LLM backbone selection, agent framework\ndesigns, and test-time scaling strategies. Using the cost-of-pass metric, we\nquantify the efficiency-performance trade-off across these dimensions. Our\nfindings inform the development of Efficient Agents , a novel agent framework\nthat has an optimal complexity to task requirements. Efficient Agents retains\n96.7% of the performance of OWL, one leading open-source agent framework, while\nreducing operational costs from 0.398 to 0.228, resulting in a 28.4%\nimprovement in cost-of-pass. Our work provides actionable insights for\ndesigning efficient, high-performing agent systems, advancing the accessibility\nand sustainability of AI-driven solutions.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.02694.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6892b4459f604ad07412f117",
      "avatarUrl": "/avatars/fb9e54e1689b4add084e32d7e5dd2f16.svg",
      "fullname": "Xavier Hu",
      "name": "xavier-hu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.03680",
      "authors": [
        {
          "_id": "68940ca8741a16f544fbce6b",
          "name": "Xufang Luo",
          "hidden": false
        },
        {
          "_id": "68940ca8741a16f544fbce6c",
          "name": "Yuge Zhang",
          "hidden": false
        },
        {
          "_id": "68940ca8741a16f544fbce6d",
          "name": "Zhiyuan He",
          "hidden": false
        },
        {
          "_id": "68940ca8741a16f544fbce6e",
          "name": "Zilong Wang",
          "hidden": false
        },
        {
          "_id": "68940ca8741a16f544fbce6f",
          "name": "Siyun Zhao",
          "hidden": false
        },
        {
          "_id": "68940ca8741a16f544fbce70",
          "name": "Dongsheng Li",
          "hidden": false
        },
        {
          "_id": "68940ca8741a16f544fbce71",
          "name": "Luna K. Qiu",
          "hidden": false
        },
        {
          "_id": "68940ca8741a16f544fbce72",
          "name": "Yuqing Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-05T17:50:13.000Z",
      "submittedOnDailyAt": "2025-08-07T02:30:53.807Z",
      "title": "Agent Lightning: Train ANY AI Agents with Reinforcement Learning",
      "submittedOnDailyBy": {
        "_id": "66a1f912345b3106f47ce860",
        "avatarUrl": "/avatars/40177299c64e16703e7bfe83de0810be.svg",
        "isPro": false,
        "fullname": "Xufang Luo",
        "user": "daixufang",
        "type": "user"
      },
      "summary": "We present Agent Lightning, a flexible and extensible framework that enables\nReinforcement Learning (RL)-based training of Large Language Models (LLMs) for\nany AI agent. Unlike existing methods that tightly couple RL training with\nagent or rely on sequence concatenation with masking, Agent Lightning achieves\ncomplete decoupling between agent execution and training, allowing seamless\nintegration with existing agents developed via diverse ways (e.g., using\nframeworks like LangChain, OpenAI Agents SDK, AutoGen, and building from\nscratch) with almost ZERO code modifications. By formulating agent execution as\nMarkov decision process, we define an unified data interface and propose a\nhierarchical RL algorithm, LightningRL, which contains a credit assignment\nmodule, allowing us to decompose trajectories generated by ANY agents into\ntraining transition. This enables RL to handle complex interaction logic, such\nas multi-agent scenarios and dynamic workflows. For the system design, we\nintroduce a Training-Agent Disaggregation architecture, and brings agent\nobservability frameworks into agent runtime, providing a standardized agent\nfinetuning interface. Experiments across text-to-SQL, retrieval-augmented\ngeneration, and math tool-use tasks demonstrate stable, continuous\nimprovements, showcasing the framework's potential for real-world agent\ntraining and deployment.",
      "upvotes": 16,
      "discussionId": "68940ca8741a16f544fbce73",
      "projectPage": "https://www.microsoft.com/en-us/research/project/agent-lightning/",
      "githubRepo": "https://github.com/microsoft/agent-lightning",
      "ai_summary": "Agent Lightning is a flexible RL framework for training LLMs in various agents, using a hierarchical RL algorithm and decoupling execution from training to handle complex interactions.",
      "ai_keywords": [
        "Reinforcement Learning",
        "Large Language Models",
        "Markov decision process",
        "hierarchical RL algorithm",
        "credit assignment module",
        "Training-Agent Disaggregation architecture",
        "agent observability frameworks",
        "text-to-SQL",
        "retrieval-augmented generation",
        "math tool-use tasks"
      ],
      "githubStars": 125
    },
    "publishedAt": "2025-08-05T13:50:13.000Z",
    "title": "Agent Lightning: Train ANY AI Agents with Reinforcement Learning",
    "summary": "We present Agent Lightning, a flexible and extensible framework that enables\nReinforcement Learning (RL)-based training of Large Language Models (LLMs) for\nany AI agent. Unlike existing methods that tightly couple RL training with\nagent or rely on sequence concatenation with masking, Agent Lightning achieves\ncomplete decoupling between agent execution and training, allowing seamless\nintegration with existing agents developed via diverse ways (e.g., using\nframeworks like LangChain, OpenAI Agents SDK, AutoGen, and building from\nscratch) with almost ZERO code modifications. By formulating agent execution as\nMarkov decision process, we define an unified data interface and propose a\nhierarchical RL algorithm, LightningRL, which contains a credit assignment\nmodule, allowing us to decompose trajectories generated by ANY agents into\ntraining transition. This enables RL to handle complex interaction logic, such\nas multi-agent scenarios and dynamic workflows. For the system design, we\nintroduce a Training-Agent Disaggregation architecture, and brings agent\nobservability frameworks into agent runtime, providing a standardized agent\nfinetuning interface. Experiments across text-to-SQL, retrieval-augmented\ngeneration, and math tool-use tasks demonstrate stable, continuous\nimprovements, showcasing the framework's potential for real-world agent\ntraining and deployment.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.03680.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66a1f912345b3106f47ce860",
      "avatarUrl": "/avatars/40177299c64e16703e7bfe83de0810be.svg",
      "fullname": "Xufang Luo",
      "name": "daixufang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.03905",
      "authors": [
        {
          "_id": "6894156a741a16f544fbced2",
          "name": "Haofei Yu",
          "hidden": false
        },
        {
          "_id": "6894156a741a16f544fbced3",
          "name": "Zhengyang Qi",
          "hidden": false
        },
        {
          "_id": "6894156a741a16f544fbced4",
          "name": "Yining Zhao",
          "hidden": false
        },
        {
          "_id": "6894156a741a16f544fbced5",
          "name": "Kolby Nottingham",
          "hidden": false
        },
        {
          "_id": "6894156a741a16f544fbced6",
          "name": "Keyang Xuan",
          "hidden": false
        },
        {
          "_id": "6894156a741a16f544fbced7",
          "name": "Bodhisattwa Prasad Majumder",
          "hidden": false
        },
        {
          "_id": "6894156a741a16f544fbced8",
          "name": "Hao Zhu",
          "hidden": false
        },
        {
          "_id": "6894156a741a16f544fbced9",
          "name": "Paul Pu Liang",
          "hidden": false
        },
        {
          "_id": "6894156a741a16f544fbceda",
          "name": "Jiaxuan You",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-05T20:43:42.000Z",
      "submittedOnDailyAt": "2025-08-07T01:30:32.752Z",
      "title": "Sotopia-RL: Reward Design for Social Intelligence",
      "submittedOnDailyBy": {
        "_id": "636453547cf2c0b4f0a3ee1e",
        "avatarUrl": "/avatars/a453340b44d08eec2281ecbe5e993707.svg",
        "isPro": false,
        "fullname": "Haofei Yu",
        "user": "lwaekfjlk",
        "type": "user"
      },
      "summary": "Social intelligence has become a critical capability for large language\nmodels (LLMs), enabling them to engage effectively in real-world social tasks\nsuch as accommodation, persuasion, collaboration, and negotiation.\nReinforcement learning (RL) is a natural fit for training socially intelligent\nagents because it allows models to learn sophisticated strategies directly\nthrough social interactions. However, social interactions have two key\ncharacteristics that set barriers for RL training: (1) partial observability,\nwhere utterances have indirect and delayed effects that complicate credit\nassignment, and (2) multi-dimensionality, where behaviors such as\nrapport-building or knowledge-seeking contribute indirectly to goal\nachievement. These characteristics make Markov decision process (MDP)-based RL\nwith single-dimensional episode-level rewards inefficient and unstable. To\naddress these challenges, we propose Sotopia-RL, a novel framework that refines\ncoarse episode-level feedback into utterance-level, multi-dimensional rewards.\nUtterance-level credit assignment mitigates partial observability by\nattributing outcomes to individual utterances, while multi-dimensional rewards\ncapture the full richness of social interactions and reduce reward hacking.\nExperiments in Sotopia, an open-ended social learning environment, demonstrate\nthat Sotopia-RL achieves state-of-the-art social goal completion scores (7.17\non Sotopia-hard and 8.31 on Sotopia-full), significantly outperforming existing\napproaches. Ablation studies confirm the necessity of both utterance-level\ncredit assignment and multi-dimensional reward design for RL training. Our\nimplementation is publicly available at:\nhttps://github.com/sotopia-lab/sotopia-rl.",
      "upvotes": 11,
      "discussionId": "6894156b741a16f544fbcedb",
      "ai_summary": "Sotopia-RL, a novel reinforcement learning framework, enhances social intelligence in large language models by refining feedback into utterance-level, multi-dimensional rewards, improving performance in social tasks.",
      "ai_keywords": [
        "reinforcement learning",
        "socially intelligent agents",
        "partial observability",
        "multi-dimensionality",
        "Markov decision process",
        "episode-level rewards",
        "utterance-level credit assignment",
        "multi-dimensional rewards",
        "reward hacking",
        "social goal completion scores",
        "Sotopia",
        "Sotopia-RL"
      ]
    },
    "publishedAt": "2025-08-05T16:43:42.000Z",
    "title": "Sotopia-RL: Reward Design for Social Intelligence",
    "summary": "Social intelligence has become a critical capability for large language\nmodels (LLMs), enabling them to engage effectively in real-world social tasks\nsuch as accommodation, persuasion, collaboration, and negotiation.\nReinforcement learning (RL) is a natural fit for training socially intelligent\nagents because it allows models to learn sophisticated strategies directly\nthrough social interactions. However, social interactions have two key\ncharacteristics that set barriers for RL training: (1) partial observability,\nwhere utterances have indirect and delayed effects that complicate credit\nassignment, and (2) multi-dimensionality, where behaviors such as\nrapport-building or knowledge-seeking contribute indirectly to goal\nachievement. These characteristics make Markov decision process (MDP)-based RL\nwith single-dimensional episode-level rewards inefficient and unstable. To\naddress these challenges, we propose Sotopia-RL, a novel framework that refines\ncoarse episode-level feedback into utterance-level, multi-dimensional rewards.\nUtterance-level credit assignment mitigates partial observability by\nattributing outcomes to individual utterances, while multi-dimensional rewards\ncapture the full richness of social interactions and reduce reward hacking.\nExperiments in Sotopia, an open-ended social learning environment, demonstrate\nthat Sotopia-RL achieves state-of-the-art social goal completion scores (7.17\non Sotopia-hard and 8.31 on Sotopia-full), significantly outperforming existing\napproaches. Ablation studies confirm the necessity of both utterance-level\ncredit assignment and multi-dimensional reward design for RL training. Our\nimplementation is publicly available at:\nhttps://github.com/sotopia-lab/sotopia-rl.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.03905.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "636453547cf2c0b4f0a3ee1e",
      "avatarUrl": "/avatars/a453340b44d08eec2281ecbe5e993707.svg",
      "fullname": "Haofei Yu",
      "name": "lwaekfjlk",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.03560",
      "authors": [
        {
          "_id": "6892be3d8da45ffb0a2b240f",
          "user": {
            "_id": "6523ad818fc4884992051c8f",
            "avatarUrl": "/avatars/fe19eb397e7c121f78b370b456e542cf.svg",
            "isPro": false,
            "fullname": "Yi Gui",
            "user": "starmage520",
            "type": "user"
          },
          "name": "Yi Gui",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-06T19:17:22.322Z",
          "hidden": false
        },
        {
          "_id": "6892be3d8da45ffb0a2b2410",
          "name": "Zhen Li",
          "hidden": false
        },
        {
          "_id": "6892be3d8da45ffb0a2b2411",
          "name": "Zhongyi Zhang",
          "hidden": false
        },
        {
          "_id": "6892be3d8da45ffb0a2b2412",
          "name": "Guohao Wang",
          "hidden": false
        },
        {
          "_id": "6892be3d8da45ffb0a2b2413",
          "name": "Tianpeng Lv",
          "hidden": false
        },
        {
          "_id": "6892be3d8da45ffb0a2b2414",
          "name": "Gaoyang Jiang",
          "hidden": false
        },
        {
          "_id": "6892be3d8da45ffb0a2b2415",
          "name": "Yi Liu",
          "hidden": false
        },
        {
          "_id": "6892be3d8da45ffb0a2b2416",
          "name": "Dongping Chen",
          "hidden": false
        },
        {
          "_id": "6892be3d8da45ffb0a2b2417",
          "name": "Yao Wan",
          "hidden": false
        },
        {
          "_id": "6892be3d8da45ffb0a2b2418",
          "name": "Hongyu Zhang",
          "hidden": false
        },
        {
          "_id": "6892be3d8da45ffb0a2b2419",
          "name": "Wenbin Jiang",
          "hidden": false
        },
        {
          "_id": "6892be3d8da45ffb0a2b241a",
          "name": "Xuanhua Shi",
          "hidden": false
        },
        {
          "_id": "6892be3d8da45ffb0a2b241b",
          "name": "Hai Jin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-05T15:28:48.000Z",
      "submittedOnDailyAt": "2025-08-07T03:02:24.780Z",
      "title": "LaTCoder: Converting Webpage Design to Code with Layout-as-Thought",
      "submittedOnDailyBy": {
        "_id": "6523ad818fc4884992051c8f",
        "avatarUrl": "/avatars/fe19eb397e7c121f78b370b456e542cf.svg",
        "isPro": false,
        "fullname": "Yi Gui",
        "user": "starmage520",
        "type": "user"
      },
      "summary": "Converting webpage designs into code (design-to-code) plays a vital role in\nUser Interface (UI) development for front-end developers, bridging the gap\nbetween visual design and functional implementation. While recent Multimodal\nLarge Language Models (MLLMs) have shown significant potential in\ndesign-to-code tasks, they often fail to accurately preserve the layout during\ncode generation. To this end, we draw inspiration from the Chain-of-Thought\n(CoT) reasoning in human cognition and propose LaTCoder, a novel approach that\nenhances layout preservation in webpage design during code generation with\nLayout-as-Thought (LaT). Specifically, we first introduce a simple yet\nefficient algorithm to divide the webpage design into image blocks. Next, we\nprompt MLLMs using a CoTbased approach to generate code for each block.\nFinally, we apply two assembly strategies-absolute positioning and an\nMLLM-based method-followed by dynamic selection to determine the optimal\noutput. We evaluate the effectiveness of LaTCoder using multiple backbone MLLMs\n(i.e., DeepSeek-VL2, Gemini, and GPT-4o) on both a public benchmark and a newly\nintroduced, more challenging benchmark (CC-HARD) that features complex layouts.\nThe experimental results on automatic metrics demonstrate significant\nimprovements. Specifically, TreeBLEU scores increased by 66.67% and MAE\ndecreased by 38% when using DeepSeek-VL2, compared to direct prompting.\nMoreover, the human preference evaluation results indicate that annotators\nfavor the webpages generated by LaTCoder in over 60% of cases, providing strong\nevidence of the effectiveness of our method.",
      "upvotes": 9,
      "discussionId": "6892be3d8da45ffb0a2b241c",
      "ai_summary": "LaTCoder enhances layout preservation in design-to-code tasks by dividing webpage designs into blocks and using Chain-of-Thought reasoning with MLLMs, achieving significant improvements in metrics and human preference.",
      "ai_keywords": [
        "Multimodal Large Language Models",
        "Chain-of-Thought",
        "Layout-as-Thought",
        "image blocks",
        "absolute positioning",
        "TreeBLEU",
        "MAE"
      ]
    },
    "publishedAt": "2025-08-05T11:28:48.000Z",
    "title": "LaTCoder: Converting Webpage Design to Code with Layout-as-Thought",
    "summary": "Converting webpage designs into code (design-to-code) plays a vital role in\nUser Interface (UI) development for front-end developers, bridging the gap\nbetween visual design and functional implementation. While recent Multimodal\nLarge Language Models (MLLMs) have shown significant potential in\ndesign-to-code tasks, they often fail to accurately preserve the layout during\ncode generation. To this end, we draw inspiration from the Chain-of-Thought\n(CoT) reasoning in human cognition and propose LaTCoder, a novel approach that\nenhances layout preservation in webpage design during code generation with\nLayout-as-Thought (LaT). Specifically, we first introduce a simple yet\nefficient algorithm to divide the webpage design into image blocks. Next, we\nprompt MLLMs using a CoTbased approach to generate code for each block.\nFinally, we apply two assembly strategies-absolute positioning and an\nMLLM-based method-followed by dynamic selection to determine the optimal\noutput. We evaluate the effectiveness of LaTCoder using multiple backbone MLLMs\n(i.e., DeepSeek-VL2, Gemini, and GPT-4o) on both a public benchmark and a newly\nintroduced, more challenging benchmark (CC-HARD) that features complex layouts.\nThe experimental results on automatic metrics demonstrate significant\nimprovements. Specifically, TreeBLEU scores increased by 66.67% and MAE\ndecreased by 38% when using DeepSeek-VL2, compared to direct prompting.\nMoreover, the human preference evaluation results indicate that annotators\nfavor the webpages generated by LaTCoder in over 60% of cases, providing strong\nevidence of the effectiveness of our method.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.03560.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6523ad818fc4884992051c8f",
      "avatarUrl": "/avatars/fe19eb397e7c121f78b370b456e542cf.svg",
      "fullname": "Yi Gui",
      "name": "starmage520",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2508.01858",
      "authors": [
        {
          "_id": "689326568da45ffb0a2b2598",
          "user": {
            "_id": "67189ab6034e7dc93591fcfa",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67189ab6034e7dc93591fcfa/jeLp-_pdEDzGDsSyKuAks.png",
            "isPro": false,
            "fullname": "Eohan G",
            "user": "Gnonymous",
            "type": "user"
          },
          "name": "Yuhan Guo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-06T19:14:35.040Z",
          "hidden": false
        },
        {
          "_id": "689326568da45ffb0a2b2599",
          "name": "Cong Guo",
          "hidden": false
        },
        {
          "_id": "689326568da45ffb0a2b259a",
          "name": "Aiwen Sun",
          "hidden": false
        },
        {
          "_id": "689326568da45ffb0a2b259b",
          "name": "Hongliang He",
          "hidden": false
        },
        {
          "_id": "689326568da45ffb0a2b259c",
          "name": "Xinyu Yang",
          "hidden": false
        },
        {
          "_id": "689326568da45ffb0a2b259d",
          "name": "Yue Lu",
          "hidden": false
        },
        {
          "_id": "689326568da45ffb0a2b259e",
          "name": "Yingji Zhang",
          "hidden": false
        },
        {
          "_id": "689326568da45ffb0a2b259f",
          "name": "Xuntao Guo",
          "hidden": false
        },
        {
          "_id": "689326568da45ffb0a2b25a0",
          "name": "Dong Zhang",
          "hidden": false
        },
        {
          "_id": "689326568da45ffb0a2b25a1",
          "name": "Jianzhuang Liu",
          "hidden": false
        },
        {
          "_id": "689326568da45ffb0a2b25a2",
          "name": "Jiang Duan",
          "hidden": false
        },
        {
          "_id": "689326568da45ffb0a2b25a3",
          "name": "Yijia Xiao",
          "hidden": false
        },
        {
          "_id": "689326568da45ffb0a2b25a4",
          "name": "Liangjian Wen",
          "hidden": false
        },
        {
          "_id": "689326568da45ffb0a2b25a5",
          "name": "Hai-Ming Xu",
          "hidden": false
        },
        {
          "_id": "689326568da45ffb0a2b25a6",
          "name": "Yong Dai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-03T17:17:52.000Z",
      "submittedOnDailyAt": "2025-08-07T01:40:19.751Z",
      "title": "Web-CogReasoner: Towards Knowledge-Induced Cognitive Reasoning for Web\n  Agents",
      "submittedOnDailyBy": {
        "_id": "67189ab6034e7dc93591fcfa",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67189ab6034e7dc93591fcfa/jeLp-_pdEDzGDsSyKuAks.png",
        "isPro": false,
        "fullname": "Eohan G",
        "user": "Gnonymous",
        "type": "user"
      },
      "summary": "Multimodal large-scale models have significantly advanced the development of\nweb agents, enabling perception and interaction with digital environments akin\nto human cognition. In this paper, we argue that web agents must first acquire\nsufficient knowledge to effectively engage in cognitive reasoning. Therefore,\nwe decompose a web agent's capabilities into two essential stages: knowledge\ncontent learning and cognitive processes. To formalize this, we propose\nWeb-CogKnowledge Framework, categorizing knowledge as Factual, Conceptual, and\nProcedural. In this framework, knowledge content learning corresponds to the\nagent's processes of Memorizing and Understanding, which rely on the first two\nknowledge types, representing the \"what\" of learning. Conversely, cognitive\nprocesses correspond to Exploring, grounded in Procedural knowledge, defining\nthe \"how\" of reasoning and action. To facilitate knowledge acquisition, we\nconstruct the Web-CogDataset, a structured resource curated from 14 real-world\nwebsites, designed to systematically instill core knowledge necessary for web\nagent. This dataset serves as the agent's conceptual grounding-the \"nouns\" upon\nwhich comprehension is built-as well as the basis for learning how to reason\nand act. Building on this foundation, we operationalize these processes through\na novel knowledge-driven Chain-of-Thought (CoT) reasoning framework, developing\nand training our proposed agent, the Web-CogReasoner. Extensive experimentation\nreveals its significant superiority over existing models, especially in\ngeneralizing to unseen tasks where structured knowledge is decisive. To enable\nrigorous evaluation, we introduce the Web-CogBench, a comprehensive evaluation\nsuite designed to assess and compare agent performance across the delineated\nknowledge domains and cognitive capabilities. Our code and data is open sourced\nat https://github.com/Gnonymous/Web-CogReasoner",
      "upvotes": 9,
      "discussionId": "689326568da45ffb0a2b25a7",
      "projectPage": "https://eohan.me/Web-CogReasoner",
      "githubRepo": "https://github.com/Gnonymous/Web-CogReasoner",
      "ai_summary": "A framework for web agents decomposes their capabilities into knowledge content learning and cognitive processes, using a structured dataset and a novel reasoning framework to enhance generalization and performance.",
      "ai_keywords": [
        "Web-CogKnowledge Framework",
        "Factual knowledge",
        "Conceptual knowledge",
        "Procedural knowledge",
        "Memorizing",
        "Understanding",
        "Exploring",
        "Web-CogDataset",
        "Chain-of-Thought (CoT) reasoning",
        "Web-CogReasoner",
        "Web-CogBench"
      ],
      "githubStars": 3
    },
    "publishedAt": "2025-08-03T13:17:52.000Z",
    "title": "Web-CogReasoner: Towards Knowledge-Induced Cognitive Reasoning for Web\n  Agents",
    "summary": "Multimodal large-scale models have significantly advanced the development of\nweb agents, enabling perception and interaction with digital environments akin\nto human cognition. In this paper, we argue that web agents must first acquire\nsufficient knowledge to effectively engage in cognitive reasoning. Therefore,\nwe decompose a web agent's capabilities into two essential stages: knowledge\ncontent learning and cognitive processes. To formalize this, we propose\nWeb-CogKnowledge Framework, categorizing knowledge as Factual, Conceptual, and\nProcedural. In this framework, knowledge content learning corresponds to the\nagent's processes of Memorizing and Understanding, which rely on the first two\nknowledge types, representing the \"what\" of learning. Conversely, cognitive\nprocesses correspond to Exploring, grounded in Procedural knowledge, defining\nthe \"how\" of reasoning and action. To facilitate knowledge acquisition, we\nconstruct the Web-CogDataset, a structured resource curated from 14 real-world\nwebsites, designed to systematically instill core knowledge necessary for web\nagent. This dataset serves as the agent's conceptual grounding-the \"nouns\" upon\nwhich comprehension is built-as well as the basis for learning how to reason\nand act. Building on this foundation, we operationalize these processes through\na novel knowledge-driven Chain-of-Thought (CoT) reasoning framework, developing\nand training our proposed agent, the Web-CogReasoner. Extensive experimentation\nreveals its significant superiority over existing models, especially in\ngeneralizing to unseen tasks where structured knowledge is decisive. To enable\nrigorous evaluation, we introduce the Web-CogBench, a comprehensive evaluation\nsuite designed to assess and compare agent performance across the delineated\nknowledge domains and cognitive capabilities. Our code and data is open sourced\nat https://github.com/Gnonymous/Web-CogReasoner",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.01858.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67189ab6034e7dc93591fcfa",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67189ab6034e7dc93591fcfa/jeLp-_pdEDzGDsSyKuAks.png",
      "fullname": "Eohan G",
      "name": "Gnonymous",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2508.03789",
      "authors": [
        {
          "_id": "6894142c741a16f544fbcec2",
          "name": "Yuhang Ma",
          "hidden": false
        },
        {
          "_id": "6894142c741a16f544fbcec3",
          "name": "Xiaoshi Wu",
          "hidden": false
        },
        {
          "_id": "6894142c741a16f544fbcec4",
          "name": "Keqiang Sun",
          "hidden": false
        },
        {
          "_id": "6894142c741a16f544fbcec5",
          "name": "Hongsheng Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-05T17:17:13.000Z",
      "submittedOnDailyAt": "2025-08-07T01:20:19.742Z",
      "title": "HPSv3: Towards Wide-Spectrum Human Preference Score",
      "submittedOnDailyBy": {
        "_id": "645a46df5e6871b4b2d39bb8",
        "avatarUrl": "/avatars/5687fdca5bec77677377b57a06d64b9e.svg",
        "isPro": false,
        "fullname": "Yunhaoshui",
        "user": "xilanhua12138",
        "type": "user"
      },
      "summary": "Evaluating text-to-image generation models requires alignment with human\nperception, yet existing human-centric metrics are constrained by limited data\ncoverage, suboptimal feature extraction, and inefficient loss functions. To\naddress these challenges, we introduce Human Preference Score v3 (HPSv3). (1)\nWe release HPDv3, the first wide-spectrum human preference dataset integrating\n1.08M text-image pairs and 1.17M annotated pairwise comparisons from\nstate-of-the-art generative models and low to high-quality real-world images.\n(2) We introduce a VLM-based preference model trained using an\nuncertainty-aware ranking loss for fine-grained ranking. Besides, we propose\nChain-of-Human-Preference (CoHP), an iterative image refinement method that\nenhances quality without extra data, using HPSv3 to select the best image at\neach step. Extensive experiments demonstrate that HPSv3 serves as a robust\nmetric for wide-spectrum image evaluation, and CoHP offers an efficient and\nhuman-aligned approach to improve image generation quality. The code and\ndataset are available at the HPSv3 Homepage.",
      "upvotes": 8,
      "discussionId": "6894142d741a16f544fbcec6",
      "projectPage": "https://mizzenai.github.io/HPSv3.project/",
      "githubRepo": "https://github.com/MizzenAI/HPSv3",
      "ai_summary": "HPSv3, a human preference score using a wide-spectrum dataset and uncertainty-aware ranking loss, enhances text-to-image generation quality through iterative refinement.",
      "ai_keywords": [
        "human preference score",
        "HPDv3",
        "text-image pairs",
        "pairwise comparisons",
        "VLM-based preference model",
        "uncertainty-aware ranking loss",
        "Chain-of-Human-Preference",
        "CoHP",
        "image refinement",
        "image generation quality"
      ],
      "githubStars": 56
    },
    "publishedAt": "2025-08-05T13:17:13.000Z",
    "title": "HPSv3: Towards Wide-Spectrum Human Preference Score",
    "summary": "Evaluating text-to-image generation models requires alignment with human\nperception, yet existing human-centric metrics are constrained by limited data\ncoverage, suboptimal feature extraction, and inefficient loss functions. To\naddress these challenges, we introduce Human Preference Score v3 (HPSv3). (1)\nWe release HPDv3, the first wide-spectrum human preference dataset integrating\n1.08M text-image pairs and 1.17M annotated pairwise comparisons from\nstate-of-the-art generative models and low to high-quality real-world images.\n(2) We introduce a VLM-based preference model trained using an\nuncertainty-aware ranking loss for fine-grained ranking. Besides, we propose\nChain-of-Human-Preference (CoHP), an iterative image refinement method that\nenhances quality without extra data, using HPSv3 to select the best image at\neach step. Extensive experiments demonstrate that HPSv3 serves as a robust\nmetric for wide-spectrum image evaluation, and CoHP offers an efficient and\nhuman-aligned approach to improve image generation quality. The code and\ndataset are available at the HPSv3 Homepage.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.03789.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645a46df5e6871b4b2d39bb8",
      "avatarUrl": "/avatars/5687fdca5bec77677377b57a06d64b9e.svg",
      "fullname": "Yunhaoshui",
      "name": "xilanhua12138",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.02215",
      "authors": [
        {
          "_id": "6894106d741a16f544fbcea6",
          "name": "Yike Zhang",
          "hidden": false
        },
        {
          "_id": "6894106d741a16f544fbcea7",
          "name": "Zhiyuan He",
          "hidden": false
        },
        {
          "_id": "6894106d741a16f544fbcea8",
          "name": "Huiqiang Jiang",
          "hidden": false
        },
        {
          "_id": "6894106d741a16f544fbcea9",
          "name": "Chengruidong Zhang",
          "hidden": false
        },
        {
          "_id": "6894106d741a16f544fbceaa",
          "name": "Yuqing Yang",
          "hidden": false
        },
        {
          "_id": "6894106d741a16f544fbceab",
          "name": "Jianyong Wang",
          "hidden": false
        },
        {
          "_id": "6894106d741a16f544fbceac",
          "name": "Lili Qiu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-04T09:08:43.000Z",
      "submittedOnDailyAt": "2025-08-07T01:18:44.229Z",
      "title": "LeanK: Learnable K Cache Channel Pruning for Efficient Decoding",
      "submittedOnDailyBy": {
        "_id": "6541159afcbd1aa00682ec66",
        "avatarUrl": "/avatars/94fd9ce0ea534e5d84736c1a1eb949e7.svg",
        "isPro": false,
        "fullname": "zhangyik21",
        "user": "zhangyik21",
        "type": "user"
      },
      "summary": "Large language models (LLMs) enable long-context tasks but face efficiency\nchallenges due to the growing key-value (KV) cache. We propose LeanK, a\nlearning-based method that prunes unimportant key (K) cache channels by\nleveraging static channel sparsity. With a novel two-stage training process,\nLeanK learns channel-wise static mask that could satisfy specific sparsity\nratio and hardware alignment requirement. LeanK reduces GPU memory and\naccelerates decoding without sacrificing accuracy. Experiments demonstrate up\nto 70% K cache and 16%-18% V cache memory reduction. Custom decoding kernel\nenables 1.3x speedup for attention computation. We also provide insights into\nmodel channels and attention heads during long-context inference by analyzing\nthe learned importance distribution. Our code is available at\nhttps://aka.ms/LeanK.",
      "upvotes": 8,
      "discussionId": "6894106d741a16f544fbcead",
      "projectPage": "https://aka.ms/LeanK",
      "ai_summary": "LeanK, a learning-based method, prunes unimportant key cache channels in large language models to reduce memory usage and accelerate decoding without sacrificing accuracy.",
      "ai_keywords": [
        "large language models",
        "key-value cache",
        "channel sparsity",
        "two-stage training",
        "channel-wise static mask",
        "GPU memory",
        "decoding speedup",
        "attention computation",
        "long-context inference",
        "importance distribution"
      ]
    },
    "publishedAt": "2025-08-04T05:08:43.000Z",
    "title": "LeanK: Learnable K Cache Channel Pruning for Efficient Decoding",
    "summary": "Large language models (LLMs) enable long-context tasks but face efficiency\nchallenges due to the growing key-value (KV) cache. We propose LeanK, a\nlearning-based method that prunes unimportant key (K) cache channels by\nleveraging static channel sparsity. With a novel two-stage training process,\nLeanK learns channel-wise static mask that could satisfy specific sparsity\nratio and hardware alignment requirement. LeanK reduces GPU memory and\naccelerates decoding without sacrificing accuracy. Experiments demonstrate up\nto 70% K cache and 16%-18% V cache memory reduction. Custom decoding kernel\nenables 1.3x speedup for attention computation. We also provide insights into\nmodel channels and attention heads during long-context inference by analyzing\nthe learned importance distribution. Our code is available at\nhttps://aka.ms/LeanK.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.02215.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6541159afcbd1aa00682ec66",
      "avatarUrl": "/avatars/94fd9ce0ea534e5d84736c1a1eb949e7.svg",
      "fullname": "zhangyik21",
      "name": "zhangyik21",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.23785",
      "authors": [
        {
          "_id": "6892cf048da45ffb0a2b24b6",
          "user": {
            "_id": "6237f25f16004228e6c74e01",
            "avatarUrl": "/avatars/cc63ce464a25702c8155610d2a708595.svg",
            "isPro": false,
            "fullname": "Bowen Zhang",
            "user": "BwZhang",
            "type": "user"
          },
          "name": "Bowen Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-06T19:16:22.835Z",
          "hidden": false
        },
        {
          "_id": "6892cf048da45ffb0a2b24b7",
          "name": "Sicheng Xu",
          "hidden": false
        },
        {
          "_id": "6892cf048da45ffb0a2b24b8",
          "name": "Chuxin Wang",
          "hidden": false
        },
        {
          "_id": "6892cf048da45ffb0a2b24b9",
          "name": "Jiaolong Yang",
          "hidden": false
        },
        {
          "_id": "6892cf048da45ffb0a2b24ba",
          "name": "Feng Zhao",
          "hidden": false
        },
        {
          "_id": "6892cf048da45ffb0a2b24bb",
          "name": "Dong Chen",
          "hidden": false
        },
        {
          "_id": "6892cf048da45ffb0a2b24bc",
          "name": "Baining Guo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-31T17:59:51.000Z",
      "submittedOnDailyAt": "2025-08-07T01:26:55.033Z",
      "title": "Gaussian Variation Field Diffusion for High-fidelity Video-to-4D\n  Synthesis",
      "submittedOnDailyBy": {
        "_id": "6237f25f16004228e6c74e01",
        "avatarUrl": "/avatars/cc63ce464a25702c8155610d2a708595.svg",
        "isPro": false,
        "fullname": "Bowen Zhang",
        "user": "BwZhang",
        "type": "user"
      },
      "summary": "In this paper, we present a novel framework for video-to-4D generation that\ncreates high-quality dynamic 3D content from single video inputs. Direct 4D\ndiffusion modeling is extremely challenging due to costly data construction and\nthe high-dimensional nature of jointly representing 3D shape, appearance, and\nmotion. We address these challenges by introducing a Direct 4DMesh-to-GS\nVariation Field VAE that directly encodes canonical Gaussian Splats (GS) and\ntheir temporal variations from 3D animation data without per-instance fitting,\nand compresses high-dimensional animations into a compact latent space.\nBuilding upon this efficient representation, we train a Gaussian Variation\nField diffusion model with temporal-aware Diffusion Transformer conditioned on\ninput videos and canonical GS. Trained on carefully-curated animatable 3D\nobjects from the Objaverse dataset, our model demonstrates superior generation\nquality compared to existing methods. It also exhibits remarkable\ngeneralization to in-the-wild video inputs despite being trained exclusively on\nsynthetic data, paving the way for generating high-quality animated 3D content.\nProject page: https://gvfdiffusion.github.io/.",
      "upvotes": 7,
      "discussionId": "6892cf058da45ffb0a2b24bd",
      "projectPage": "https://gvfdiffusion.github.io/",
      "githubRepo": "https://github.com/ForeverFancy/gvfdiffusion",
      "ai_summary": "A novel framework uses a Direct 4DMesh-to-GS Variation Field VAE and Gaussian Variation Field diffusion model to generate high-quality dynamic 3D content from single video inputs, demonstrating superior quality and generalization.",
      "ai_keywords": [
        "Direct 4D diffusion modeling",
        "Gaussian Splats (GS)",
        "Direct 4DMesh-to-GS Variation Field VAE",
        "Gaussian Variation Field diffusion model",
        "Diffusion Transformer",
        "Objaverse dataset"
      ],
      "githubStars": 50
    },
    "publishedAt": "2025-07-31T13:59:51.000Z",
    "title": "Gaussian Variation Field Diffusion for High-fidelity Video-to-4D\n  Synthesis",
    "summary": "In this paper, we present a novel framework for video-to-4D generation that\ncreates high-quality dynamic 3D content from single video inputs. Direct 4D\ndiffusion modeling is extremely challenging due to costly data construction and\nthe high-dimensional nature of jointly representing 3D shape, appearance, and\nmotion. We address these challenges by introducing a Direct 4DMesh-to-GS\nVariation Field VAE that directly encodes canonical Gaussian Splats (GS) and\ntheir temporal variations from 3D animation data without per-instance fitting,\nand compresses high-dimensional animations into a compact latent space.\nBuilding upon this efficient representation, we train a Gaussian Variation\nField diffusion model with temporal-aware Diffusion Transformer conditioned on\ninput videos and canonical GS. Trained on carefully-curated animatable 3D\nobjects from the Objaverse dataset, our model demonstrates superior generation\nquality compared to existing methods. It also exhibits remarkable\ngeneralization to in-the-wild video inputs despite being trained exclusively on\nsynthetic data, paving the way for generating high-quality animated 3D content.\nProject page: https://gvfdiffusion.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.23785.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6237f25f16004228e6c74e01",
      "avatarUrl": "/avatars/cc63ce464a25702c8155610d2a708595.svg",
      "fullname": "Bowen Zhang",
      "name": "BwZhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2508.04664",
      "authors": [
        {
          "_id": "689402a1741a16f544fbce59",
          "name": "Mo Li",
          "hidden": false
        },
        {
          "_id": "689402a1741a16f544fbce5a",
          "name": "L. H. Xu",
          "hidden": false
        },
        {
          "_id": "689402a1741a16f544fbce5b",
          "name": "Qitai Tan",
          "hidden": false
        },
        {
          "_id": "689402a1741a16f544fbce5c",
          "name": "Ting Cao",
          "hidden": false
        },
        {
          "_id": "689402a1741a16f544fbce5d",
          "name": "Yunxin Liu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6576fe2b42ab083faea19841/Dl7QjjxqaNg8sBGAYF0RV.png"
      ],
      "publishedAt": "2025-08-06T17:32:58.000Z",
      "submittedOnDailyAt": "2025-08-07T01:37:06.645Z",
      "title": "Sculptor: Empowering LLMs with Cognitive Agency via Active Context\n  Management",
      "submittedOnDailyBy": {
        "_id": "6576fe2b42ab083faea19841",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/c91ZKOR2E0gL8iIVkvEUa.jpeg",
        "isPro": false,
        "fullname": "Mo Li",
        "user": "Mor-Li",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) suffer from significant performance degradation\nwhen processing long contexts due to proactive interference, where irrelevant\ninformation in earlier parts of the context disrupts reasoning and memory\nrecall. While most research focuses on external memory systems to augment LLMs'\ncapabilities, we propose a complementary approach: empowering LLMs with Active\nContext Management (ACM) tools to actively sculpt their internal working\nmemory. We introduce Sculptor, a framework that equips LLMs with three\ncategories of tools: (1) context fragmentation, (2) summary, hide, and restore,\nand (3) intelligent search. Our approach enables LLMs to proactively manage\ntheir attention and working memory, analogous to how humans selectively focus\non relevant information while filtering out distractions. Experimental\nevaluation on information-sparse benchmarks-PI-LLM (proactive interference) and\nNeedleBench Multi-Needle Reasoning-demonstrates that Sculptor significantly\nimproves performance even without specific training, leveraging LLMs' inherent\ntool calling generalization capabilities. By enabling Active Context\nManagement, Sculptor not only mitigates proactive interference but also\nprovides a cognitive foundation for more reliable reasoning across diverse\nlong-context tasks-highlighting that explicit context-control strategies,\nrather than merely larger token windows, are key to robustness at scale.",
      "upvotes": 6,
      "discussionId": "689402a1741a16f544fbce5e",
      "ai_summary": "Sculptor, a framework for Active Context Management, enhances LLM performance on long contexts by enabling proactive attention and memory control, reducing proactive interference and improving reasoning reliability.",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "proactive interference",
        "Active Context Management (ACM)",
        "context fragmentation",
        "summary",
        "hide",
        "restore",
        "intelligent search",
        "PI-LLM",
        "NeedleBench Multi-Needle Reasoning"
      ]
    },
    "publishedAt": "2025-08-06T13:32:58.000Z",
    "title": "Sculptor: Empowering LLMs with Cognitive Agency via Active Context\n  Management",
    "summary": "Large Language Models (LLMs) suffer from significant performance degradation\nwhen processing long contexts due to proactive interference, where irrelevant\ninformation in earlier parts of the context disrupts reasoning and memory\nrecall. While most research focuses on external memory systems to augment LLMs'\ncapabilities, we propose a complementary approach: empowering LLMs with Active\nContext Management (ACM) tools to actively sculpt their internal working\nmemory. We introduce Sculptor, a framework that equips LLMs with three\ncategories of tools: (1) context fragmentation, (2) summary, hide, and restore,\nand (3) intelligent search. Our approach enables LLMs to proactively manage\ntheir attention and working memory, analogous to how humans selectively focus\non relevant information while filtering out distractions. Experimental\nevaluation on information-sparse benchmarks-PI-LLM (proactive interference) and\nNeedleBench Multi-Needle Reasoning-demonstrates that Sculptor significantly\nimproves performance even without specific training, leveraging LLMs' inherent\ntool calling generalization capabilities. By enabling Active Context\nManagement, Sculptor not only mitigates proactive interference but also\nprovides a cognitive foundation for more reliable reasoning across diverse\nlong-context tasks-highlighting that explicit context-control strategies,\nrather than merely larger token windows, are key to robustness at scale.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6576fe2b42ab083faea19841/Dl7QjjxqaNg8sBGAYF0RV.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.04664.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6576fe2b42ab083faea19841",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/c91ZKOR2E0gL8iIVkvEUa.jpeg",
      "fullname": "Mo Li",
      "name": "Mor-Li",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.04586",
      "authors": [
        {
          "_id": "6893fe53741a16f544fbce21",
          "name": "Nuo Chen",
          "hidden": false
        },
        {
          "_id": "6893fe53741a16f544fbce22",
          "name": "Moming Duan",
          "hidden": false
        },
        {
          "_id": "6893fe53741a16f544fbce23",
          "name": "Andre Huikai Lin",
          "hidden": false
        },
        {
          "_id": "6893fe53741a16f544fbce24",
          "name": "Qian Wang",
          "hidden": false
        },
        {
          "_id": "6893fe53741a16f544fbce25",
          "name": "Jiaying Wu",
          "hidden": false
        },
        {
          "_id": "6893fe53741a16f544fbce26",
          "name": "Bingsheng He",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/641ac2207c21ab946bf036e8/yyBhKvXaRlW-2Z24Q0vJ1.gif"
      ],
      "publishedAt": "2025-08-06T16:08:27.000Z",
      "submittedOnDailyAt": "2025-08-07T01:14:52.604Z",
      "title": "Position: The Current AI Conference Model is Unsustainable! Diagnosing\n  the Crisis of Centralized AI Conference",
      "submittedOnDailyBy": {
        "_id": "641ac2207c21ab946bf036e8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/641ac2207c21ab946bf036e8/r6c9gpOrul0eC59d9e2Mo.png",
        "isPro": true,
        "fullname": "Nuo Chen",
        "user": "nuojohnchen",
        "type": "user"
      },
      "summary": "Artificial Intelligence (AI) conferences are essential for advancing\nresearch, sharing knowledge, and fostering academic community. However, their\nrapid expansion has rendered the centralized conference model increasingly\nunsustainable. This paper offers a data-driven diagnosis of a structural crisis\nthat threatens the foundational goals of scientific dissemination, equity, and\ncommunity well-being. We identify four key areas of strain: (1) scientifically,\nwith per-author publication rates more than doubling over the past decade to\nover 4.5 papers annually; (2) environmentally, with the carbon footprint of a\nsingle conference exceeding the daily emissions of its host city; (3)\npsychologically, with 71% of online community discourse reflecting negative\nsentiment and 35% referencing mental health concerns; and (4) logistically,\nwith attendance at top conferences such as NeurIPS 2024 beginning to outpace\nvenue capacity. These pressures point to a system that is misaligned with its\ncore mission. In response, we propose the Community-Federated Conference (CFC)\nmodel, which separates peer review, presentation, and networking into globally\ncoordinated but locally organized components, offering a more sustainable,\ninclusive, and resilient path forward for AI research.",
      "upvotes": 5,
      "discussionId": "6893fe53741a16f544fbce27",
      "ai_summary": "The paper diagnoses structural issues in AI conferences, including publication rates, carbon footprint, negative community sentiment, and logistical challenges, and proposes a Community-Federated Conference model to address these issues.",
      "ai_keywords": [
        ""
      ]
    },
    "publishedAt": "2025-08-06T12:08:27.000Z",
    "title": "Position: The Current AI Conference Model is Unsustainable! Diagnosing\n  the Crisis of Centralized AI Conference",
    "summary": "Artificial Intelligence (AI) conferences are essential for advancing\nresearch, sharing knowledge, and fostering academic community. However, their\nrapid expansion has rendered the centralized conference model increasingly\nunsustainable. This paper offers a data-driven diagnosis of a structural crisis\nthat threatens the foundational goals of scientific dissemination, equity, and\ncommunity well-being. We identify four key areas of strain: (1) scientifically,\nwith per-author publication rates more than doubling over the past decade to\nover 4.5 papers annually; (2) environmentally, with the carbon footprint of a\nsingle conference exceeding the daily emissions of its host city; (3)\npsychologically, with 71% of online community discourse reflecting negative\nsentiment and 35% referencing mental health concerns; and (4) logistically,\nwith attendance at top conferences such as NeurIPS 2024 beginning to outpace\nvenue capacity. These pressures point to a system that is misaligned with its\ncore mission. In response, we propose the Community-Federated Conference (CFC)\nmodel, which separates peer review, presentation, and networking into globally\ncoordinated but locally organized components, offering a more sustainable,\ninclusive, and resilient path forward for AI research.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/641ac2207c21ab946bf036e8/yyBhKvXaRlW-2Z24Q0vJ1.gif"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.04586.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "641ac2207c21ab946bf036e8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/641ac2207c21ab946bf036e8/r6c9gpOrul0eC59d9e2Mo.png",
      "fullname": "Nuo Chen",
      "name": "nuojohnchen",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.04295",
      "authors": [
        {
          "_id": "68941a73741a16f544fbcee3",
          "name": "Chaofan Wang",
          "hidden": false
        },
        {
          "_id": "68941a73741a16f544fbcee4",
          "name": "Tingrui Yu",
          "hidden": false
        },
        {
          "_id": "68941a73741a16f544fbcee5",
          "name": "Jie Wang",
          "hidden": false
        },
        {
          "_id": "68941a73741a16f544fbcee6",
          "name": "Dong Chen",
          "hidden": false
        },
        {
          "_id": "68941a73741a16f544fbcee7",
          "name": "Wenrui Zhang",
          "hidden": false
        },
        {
          "_id": "68941a73741a16f544fbcee8",
          "name": "Yuling Shi",
          "hidden": false
        },
        {
          "_id": "68941a73741a16f544fbcee9",
          "name": "Xiaodong Gu",
          "hidden": false
        },
        {
          "_id": "68941a73741a16f544fbceea",
          "name": "Beijun Shen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-06T10:31:23.000Z",
      "submittedOnDailyAt": "2025-08-07T02:57:51.348Z",
      "title": "EVOC2RUST: A Skeleton-guided Framework for Project-Level C-to-Rust\n  Translation",
      "submittedOnDailyBy": {
        "_id": "645b0c3ec35da9c7afd95421",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg",
        "isPro": false,
        "fullname": "Yuling",
        "user": "YerbaPage",
        "type": "user"
      },
      "summary": "Rust's compile-time safety guarantees make it ideal for safety-critical\nsystems, creating demand for translating legacy C codebases to Rust. While\nvarious approaches have emerged for this task, they face inherent trade-offs:\nrule-based solutions face challenges in meeting code safety and idiomaticity\nrequirements, while LLM-based solutions often fail to generate semantically\nequivalent Rust code, due to the heavy dependencies of modules across the\nentire codebase. Recent studies have revealed that both solutions are limited\nto small-scale programs. In this paper, we propose EvoC2Rust, an automated\nframework for converting entire C projects to equivalent Rust ones. EvoC2Rust\nemploys a skeleton-guided translation strategy for project-level translation.\nThe pipeline consists of three evolutionary stages: 1) it first decomposes the\nC project into functional modules, employs a feature-mapping-enhanced LLM to\ntransform definitions and macros and generates type-checked function stubs,\nwhich form a compilable Rust skeleton; 2) it then incrementally translates the\nfunction, replacing the corresponding stub placeholder; 3) finally, it repairs\ncompilation errors by integrating LLM and static analysis. Through evolutionary\naugmentation, EvoC2Rust combines the advantages of both rule-based and\nLLM-based solutions. Our evaluation on open-source benchmarks and six\nindustrial projects demonstrates EvoC2Rust's superior performance in\nproject-level C-to-Rust translation. On average, it achieves 17.24% and 14.32%\nimprovements in syntax and semantic accuracy over the LLM-based approaches,\nalong with a 96.79% higher code safety rate than the rule-based tools. At the\nmodule level, EvoC2Rust reaches 92.25% compilation and 89.53% test pass rates\non industrial projects, even for complex codebases and long functions.",
      "upvotes": 3,
      "discussionId": "68941a74741a16f544fbceeb",
      "ai_summary": "EvoC2Rust is an automated framework that translates entire C projects to Rust using a skeleton-guided approach, combining rule-based and LLM-based methods to improve syntax, semantics, and safety.",
      "ai_keywords": [
        "LLM",
        "feature-mapping-enhanced LLM",
        "static analysis",
        "evolutionary stages",
        "functional modules",
        "type-checked function stubs",
        "syntax accuracy",
        "semantic accuracy",
        "code safety",
        "compilation",
        "test pass rates"
      ]
    },
    "publishedAt": "2025-08-06T06:31:23.000Z",
    "title": "EVOC2RUST: A Skeleton-guided Framework for Project-Level C-to-Rust\n  Translation",
    "summary": "Rust's compile-time safety guarantees make it ideal for safety-critical\nsystems, creating demand for translating legacy C codebases to Rust. While\nvarious approaches have emerged for this task, they face inherent trade-offs:\nrule-based solutions face challenges in meeting code safety and idiomaticity\nrequirements, while LLM-based solutions often fail to generate semantically\nequivalent Rust code, due to the heavy dependencies of modules across the\nentire codebase. Recent studies have revealed that both solutions are limited\nto small-scale programs. In this paper, we propose EvoC2Rust, an automated\nframework for converting entire C projects to equivalent Rust ones. EvoC2Rust\nemploys a skeleton-guided translation strategy for project-level translation.\nThe pipeline consists of three evolutionary stages: 1) it first decomposes the\nC project into functional modules, employs a feature-mapping-enhanced LLM to\ntransform definitions and macros and generates type-checked function stubs,\nwhich form a compilable Rust skeleton; 2) it then incrementally translates the\nfunction, replacing the corresponding stub placeholder; 3) finally, it repairs\ncompilation errors by integrating LLM and static analysis. Through evolutionary\naugmentation, EvoC2Rust combines the advantages of both rule-based and\nLLM-based solutions. Our evaluation on open-source benchmarks and six\nindustrial projects demonstrates EvoC2Rust's superior performance in\nproject-level C-to-Rust translation. On average, it achieves 17.24% and 14.32%\nimprovements in syntax and semantic accuracy over the LLM-based approaches,\nalong with a 96.79% higher code safety rate than the rule-based tools. At the\nmodule level, EvoC2Rust reaches 92.25% compilation and 89.53% test pass rates\non industrial projects, even for complex codebases and long functions.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.04295.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645b0c3ec35da9c7afd95421",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg",
      "fullname": "Yuling",
      "name": "YerbaPage",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 276
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.01197",
      "authors": [
        {
          "_id": "6894158a741a16f544fbcedd",
          "name": "Zhan Shi",
          "hidden": false
        },
        {
          "_id": "6894158a741a16f544fbcede",
          "name": "Song Wang",
          "hidden": false
        },
        {
          "_id": "6894158a741a16f544fbcedf",
          "name": "Junbo Chen",
          "hidden": false
        },
        {
          "_id": "6894158a741a16f544fbcee0",
          "name": "Jianke Zhu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-02T05:05:50.000Z",
      "submittedOnDailyAt": "2025-08-07T01:25:50.658Z",
      "title": "A Coarse-to-Fine Approach to Multi-Modality 3D Occupancy Grounding",
      "submittedOnDailyBy": {
        "_id": "66863d26e2b71e3d09189ae9",
        "avatarUrl": "/avatars/3c0e6f30e053f2e622ae75e1dc43edba.svg",
        "isPro": false,
        "fullname": "Song Wang",
        "user": "songw-zju",
        "type": "user"
      },
      "summary": "Visual grounding aims to identify objects or regions in a scene based on\nnatural language descriptions, essential for spatially aware perception in\nautonomous driving. However, existing visual grounding tasks typically depend\non bounding boxes that often fail to capture fine-grained details. Not all\nvoxels within a bounding box are occupied, resulting in inaccurate object\nrepresentations. To address this, we introduce a benchmark for 3D occupancy\ngrounding in challenging outdoor scenes. Built on the nuScenes dataset, it\nintegrates natural language with voxel-level occupancy annotations, offering\nmore precise object perception compared to the traditional grounding task.\nMoreover, we propose GroundingOcc, an end-to-end model designed for 3D\noccupancy grounding through multi-modal learning. It combines visual, textual,\nand point cloud features to predict object location and occupancy information\nfrom coarse to fine. Specifically, GroundingOcc comprises a multimodal encoder\nfor feature extraction, an occupancy head for voxel-wise predictions, and a\ngrounding head to refine localization. Additionally, a 2D grounding module and\na depth estimation module enhance geometric understanding, thereby boosting\nmodel performance. Extensive experiments on the benchmark demonstrate that our\nmethod outperforms existing baselines on 3D occupancy grounding. The dataset is\navailable at https://github.com/RONINGOD/GroundingOcc.",
      "upvotes": 3,
      "discussionId": "6894158a741a16f544fbcee1",
      "githubRepo": "https://github.com/RONINGOD/GroundingOcc",
      "ai_summary": "A benchmark and model for 3D occupancy grounding using natural language and voxel-level annotations improve object perception in autonomous driving.",
      "ai_keywords": [
        "visual grounding",
        "3D occupancy grounding",
        "nuScenes dataset",
        "voxel-level occupancy",
        "multimodal learning",
        "multimodal encoder",
        "occupancy head",
        "grounding head",
        "2D grounding module",
        "depth estimation module"
      ],
      "githubStars": 2
    },
    "publishedAt": "2025-08-02T01:05:50.000Z",
    "title": "A Coarse-to-Fine Approach to Multi-Modality 3D Occupancy Grounding",
    "summary": "Visual grounding aims to identify objects or regions in a scene based on\nnatural language descriptions, essential for spatially aware perception in\nautonomous driving. However, existing visual grounding tasks typically depend\non bounding boxes that often fail to capture fine-grained details. Not all\nvoxels within a bounding box are occupied, resulting in inaccurate object\nrepresentations. To address this, we introduce a benchmark for 3D occupancy\ngrounding in challenging outdoor scenes. Built on the nuScenes dataset, it\nintegrates natural language with voxel-level occupancy annotations, offering\nmore precise object perception compared to the traditional grounding task.\nMoreover, we propose GroundingOcc, an end-to-end model designed for 3D\noccupancy grounding through multi-modal learning. It combines visual, textual,\nand point cloud features to predict object location and occupancy information\nfrom coarse to fine. Specifically, GroundingOcc comprises a multimodal encoder\nfor feature extraction, an occupancy head for voxel-wise predictions, and a\ngrounding head to refine localization. Additionally, a 2D grounding module and\na depth estimation module enhance geometric understanding, thereby boosting\nmodel performance. Extensive experiments on the benchmark demonstrate that our\nmethod outperforms existing baselines on 3D occupancy grounding. The dataset is\navailable at https://github.com/RONINGOD/GroundingOcc.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.01197.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66863d26e2b71e3d09189ae9",
      "avatarUrl": "/avatars/3c0e6f30e053f2e622ae75e1dc43edba.svg",
      "fullname": "Song Wang",
      "name": "songw-zju",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.01630",
      "authors": [
        {
          "_id": "68916f8df01a094725f8347f",
          "user": {
            "_id": "5fd5e18a90b6dc4633f6d292",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fd5e18a90b6dc4633f6d292/gZXHW5dd9R86AV9LMZ--y.png",
            "isPro": true,
            "fullname": "Maziyar Panahi",
            "user": "MaziyarPanahi",
            "type": "user"
          },
          "name": "Maziyar Panahi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-06T19:22:17.859Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-03T07:33:28.000Z",
      "submittedOnDailyAt": "2025-08-07T03:31:30.498Z",
      "title": "OpenMed NER: Open-Source, Domain-Adapted State-of-the-Art Transformers\n  for Biomedical NER Across 12 Public Datasets",
      "submittedOnDailyBy": {
        "_id": "5fd5e18a90b6dc4633f6d292",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fd5e18a90b6dc4633f6d292/gZXHW5dd9R86AV9LMZ--y.png",
        "isPro": true,
        "fullname": "Maziyar Panahi",
        "user": "MaziyarPanahi",
        "type": "user"
      },
      "summary": "Named-entity recognition (NER) is fundamental to extracting structured\ninformation from the >80% of healthcare data that resides in unstructured\nclinical notes and biomedical literature. Despite recent advances with large\nlanguage models, achieving state-of-the-art performance across diverse entity\ntypes while maintaining computational efficiency remains a significant\nchallenge. We introduce OpenMed NER, a suite of open-source, domain-adapted\ntransformer models that combine lightweight domain-adaptive pre-training (DAPT)\nwith parameter-efficient Low-Rank Adaptation (LoRA). Our approach performs\ncost-effective DAPT on a 350k-passage corpus compiled from ethically sourced,\npublicly available research repositories and de-identified clinical notes\n(PubMed, arXiv, and MIMIC-III) using DeBERTa-v3, PubMedBERT, and BioELECTRA\nbackbones. This is followed by task-specific fine-tuning with LoRA, which\nupdates less than 1.5% of model parameters. We evaluate our models on 12\nestablished biomedical NER benchmarks spanning chemicals, diseases, genes, and\nspecies. OpenMed NER achieves new state-of-the-art micro-F1 scores on 10 of\nthese 12 datasets, with substantial gains across diverse entity types. Our\nmodels advance the state-of-the-art on foundational disease and chemical\nbenchmarks (e.g., BC5CDR-Disease, +2.70 pp), while delivering even larger\nimprovements of over 5.3 and 9.7 percentage points on more specialized gene and\nclinical cell line corpora. This work demonstrates that strategically adapted\nopen-source models can surpass closed-source solutions. This performance is\nachieved with remarkable efficiency: training completes in under 12 hours on a\nsingle GPU with a low carbon footprint (< 1.2 kg CO2e), producing permissively\nlicensed, open-source checkpoints designed to help practitioners facilitate\ncompliance with emerging data protection and AI regulations, such as the EU AI\nAct.",
      "upvotes": 2,
      "discussionId": "68916f8df01a094725f83480",
      "projectPage": "https://huggingface.co/OpenMed",
      "ai_summary": "OpenMed NER, a suite of open-source transformer models using DAPT and LoRA, achieves state-of-the-art performance on diverse biomedical NER benchmarks with high efficiency and low computational cost.",
      "ai_keywords": [
        "transformer models",
        "lightweight domain-adaptive pre-training (DAPT)",
        "parameter-efficient Low-Rank Adaptation (LoRA)",
        "DeBERTa-v3",
        "PubMedBERT",
        "BioELECTRA",
        "micro-F1 scores",
        "BC5CDR-Disease",
        "gene",
        "clinical cell line corpora",
        "open-source checkpoints",
        "EU AI Act"
      ]
    },
    "publishedAt": "2025-08-03T03:33:28.000Z",
    "title": "OpenMed NER: Open-Source, Domain-Adapted State-of-the-Art Transformers\n  for Biomedical NER Across 12 Public Datasets",
    "summary": "Named-entity recognition (NER) is fundamental to extracting structured\ninformation from the >80% of healthcare data that resides in unstructured\nclinical notes and biomedical literature. Despite recent advances with large\nlanguage models, achieving state-of-the-art performance across diverse entity\ntypes while maintaining computational efficiency remains a significant\nchallenge. We introduce OpenMed NER, a suite of open-source, domain-adapted\ntransformer models that combine lightweight domain-adaptive pre-training (DAPT)\nwith parameter-efficient Low-Rank Adaptation (LoRA). Our approach performs\ncost-effective DAPT on a 350k-passage corpus compiled from ethically sourced,\npublicly available research repositories and de-identified clinical notes\n(PubMed, arXiv, and MIMIC-III) using DeBERTa-v3, PubMedBERT, and BioELECTRA\nbackbones. This is followed by task-specific fine-tuning with LoRA, which\nupdates less than 1.5% of model parameters. We evaluate our models on 12\nestablished biomedical NER benchmarks spanning chemicals, diseases, genes, and\nspecies. OpenMed NER achieves new state-of-the-art micro-F1 scores on 10 of\nthese 12 datasets, with substantial gains across diverse entity types. Our\nmodels advance the state-of-the-art on foundational disease and chemical\nbenchmarks (e.g., BC5CDR-Disease, +2.70 pp), while delivering even larger\nimprovements of over 5.3 and 9.7 percentage points on more specialized gene and\nclinical cell line corpora. This work demonstrates that strategically adapted\nopen-source models can surpass closed-source solutions. This performance is\nachieved with remarkable efficiency: training completes in under 12 hours on a\nsingle GPU with a low carbon footprint (< 1.2 kg CO2e), producing permissively\nlicensed, open-source checkpoints designed to help practitioners facilitate\ncompliance with emerging data protection and AI regulations, such as the EU AI\nAct.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.01630.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "5fd5e18a90b6dc4633f6d292",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fd5e18a90b6dc4633f6d292/gZXHW5dd9R86AV9LMZ--y.png",
      "fullname": "Maziyar Panahi",
      "name": "MaziyarPanahi",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3533
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2508.03448",
      "authors": [
        {
          "_id": "68943937741a16f544fbcf91",
          "name": "Jan Melechovsky",
          "hidden": false
        },
        {
          "_id": "68943937741a16f544fbcf92",
          "name": "Ambuj Mehrish",
          "hidden": false
        },
        {
          "_id": "68943937741a16f544fbcf93",
          "name": "Dorien Herremans",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-05T13:49:04.000Z",
      "submittedOnDailyAt": "2025-08-07T03:59:29.921Z",
      "title": "SonicMaster: Towards Controllable All-in-One Music Restoration and\n  Mastering",
      "submittedOnDailyBy": {
        "_id": "655431b2997379e9b0999d23",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655431b2997379e9b0999d23/OhHCPCXfOS61Z7CSewUCU.png",
        "isPro": false,
        "fullname": "Dorien Herremans",
        "user": "dorienh",
        "type": "user"
      },
      "summary": "Music recordings often suffer from audio quality issues such as excessive\nreverberation, distortion, clipping, tonal imbalances, and a narrowed stereo\nimage, especially when created in non-professional settings without specialized\nequipment or expertise. These problems are typically corrected using separate\nspecialized tools and manual adjustments. In this paper, we introduce\nSonicMaster, the first unified generative model for music restoration and\nmastering that addresses a broad spectrum of audio artifacts with text-based\ncontrol. SonicMaster is conditioned on natural language instructions to apply\ntargeted enhancements, or can operate in an automatic mode for general\nrestoration. To train this model, we construct the SonicMaster dataset, a large\ndataset of paired degraded and high-quality tracks by simulating common\ndegradation types with nineteen degradation functions belonging to five\nenhancements groups: equalization, dynamics, reverb, amplitude, and stereo. Our\napproach leverages a flow-matching generative training paradigm to learn an\naudio transformation that maps degraded inputs to their cleaned, mastered\nversions guided by text prompts. Objective audio quality metrics demonstrate\nthat SonicMaster significantly improves sound quality across all artifact\ncategories. Furthermore, subjective listening tests confirm that listeners\nprefer SonicMaster's enhanced outputs over the original degraded audio,\nhighlighting the effectiveness of our unified approach.",
      "upvotes": 1,
      "discussionId": "68943938741a16f544fbcf94",
      "githubRepo": "https://github.com/AMAAI-Lab/SonicMaster",
      "ai_summary": "SonicMaster, a unified generative model, improves music audio quality by addressing various artifacts using text-based control and a flow-matching generative training paradigm.",
      "ai_keywords": [
        "generative model",
        "music restoration",
        "mastering",
        "text-based control",
        "flow-matching",
        "audio transformation",
        "equalization",
        "dynamics",
        "reverb",
        "amplitude",
        "stereo"
      ],
      "githubStars": 6
    },
    "publishedAt": "2025-08-05T09:49:04.000Z",
    "title": "SonicMaster: Towards Controllable All-in-One Music Restoration and\n  Mastering",
    "summary": "Music recordings often suffer from audio quality issues such as excessive\nreverberation, distortion, clipping, tonal imbalances, and a narrowed stereo\nimage, especially when created in non-professional settings without specialized\nequipment or expertise. These problems are typically corrected using separate\nspecialized tools and manual adjustments. In this paper, we introduce\nSonicMaster, the first unified generative model for music restoration and\nmastering that addresses a broad spectrum of audio artifacts with text-based\ncontrol. SonicMaster is conditioned on natural language instructions to apply\ntargeted enhancements, or can operate in an automatic mode for general\nrestoration. To train this model, we construct the SonicMaster dataset, a large\ndataset of paired degraded and high-quality tracks by simulating common\ndegradation types with nineteen degradation functions belonging to five\nenhancements groups: equalization, dynamics, reverb, amplitude, and stereo. Our\napproach leverages a flow-matching generative training paradigm to learn an\naudio transformation that maps degraded inputs to their cleaned, mastered\nversions guided by text prompts. Objective audio quality metrics demonstrate\nthat SonicMaster significantly improves sound quality across all artifact\ncategories. Furthermore, subjective listening tests confirm that listeners\nprefer SonicMaster's enhanced outputs over the original degraded audio,\nhighlighting the effectiveness of our unified approach.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.03448.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "655431b2997379e9b0999d23",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655431b2997379e9b0999d23/OhHCPCXfOS61Z7CSewUCU.png",
      "fullname": "Dorien Herremans",
      "name": "dorienh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.02807",
      "authors": [
        {
          "_id": "68944b9e741a16f544fbcfec",
          "name": "Tongchun Zuo",
          "hidden": false
        },
        {
          "_id": "68944b9e741a16f544fbcfed",
          "name": "Zaiyu Huang",
          "hidden": false
        },
        {
          "_id": "68944b9e741a16f544fbcfee",
          "name": "Shuliang Ning",
          "hidden": false
        },
        {
          "_id": "68944b9e741a16f544fbcfef",
          "name": "Ente Lin",
          "hidden": false
        },
        {
          "_id": "68944b9e741a16f544fbcff0",
          "name": "Chao Liang",
          "hidden": false
        },
        {
          "_id": "68944b9e741a16f544fbcff1",
          "name": "Zerong Zheng",
          "hidden": false
        },
        {
          "_id": "68944b9e741a16f544fbcff2",
          "name": "Jianwen Jiang",
          "hidden": false
        },
        {
          "_id": "68944b9e741a16f544fbcff3",
          "name": "Yuan Zhang",
          "hidden": false
        },
        {
          "_id": "68944b9e741a16f544fbcff4",
          "name": "Mingyuan Gao",
          "hidden": false
        },
        {
          "_id": "68944b9e741a16f544fbcff5",
          "name": "Xin Dong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-04T18:27:55.000Z",
      "submittedOnDailyAt": "2025-08-07T05:16:54.059Z",
      "title": "DreamVVT: Mastering Realistic Video Virtual Try-On in the Wild via a\n  Stage-Wise Diffusion Transformer Framework",
      "submittedOnDailyBy": {
        "_id": "6358d973f53b06a81271d8d6",
        "avatarUrl": "/avatars/06b7e205a1b08d2419afc2f6273028d1.svg",
        "isPro": false,
        "fullname": "Ning",
        "user": "Shuliang",
        "type": "user"
      },
      "summary": "Video virtual try-on (VVT) technology has garnered considerable academic\ninterest owing to its promising applications in e-commerce advertising and\nentertainment. However, most existing end-to-end methods rely heavily on scarce\npaired garment-centric datasets and fail to effectively leverage priors of\nadvanced visual models and test-time inputs, making it challenging to\naccurately preserve fine-grained garment details and maintain temporal\nconsistency in unconstrained scenarios. To address these challenges, we propose\nDreamVVT, a carefully designed two-stage framework built upon Diffusion\nTransformers (DiTs), which is inherently capable of leveraging diverse unpaired\nhuman-centric data to enhance adaptability in real-world scenarios. To further\nleverage prior knowledge from pretrained models and test-time inputs, in the\nfirst stage, we sample representative frames from the input video and utilize a\nmulti-frame try-on model integrated with a vision-language model (VLM), to\nsynthesize high-fidelity and semantically consistent keyframe try-on images.\nThese images serve as complementary appearance guidance for subsequent video\ngeneration. In the second stage, skeleton maps together with\nfine-grained motion and appearance descriptions are extracted from the input\ncontent, and these along with the keyframe try-on images are then fed into a\npretrained video generation model enhanced with LoRA adapters. This ensures\nlong-term temporal coherence for unseen regions and enables highly plausible\ndynamic motions. Extensive quantitative and qualitative experiments demonstrate\nthat DreamVVT surpasses existing methods in preserving detailed garment content\nand temporal stability in real-world scenarios. Our project page\nhttps://virtu-lab.github.io/",
      "upvotes": 1,
      "discussionId": "68944b9e741a16f544fbcff6",
      "ai_summary": "DreamVVT, a two-stage framework using Diffusion Transformers and LoRA adapters, enhances video virtual try-on by leveraging unpaired human-centric data and pretrained models to preserve garment details and temporal consistency.",
      "ai_keywords": [
        "Diffusion Transformers",
        "DiTs",
        "multi-frame try-on model",
        "vision-language model",
        "VLM",
        "skeleton maps",
        "fine-grained motion",
        "appearance descriptions",
        "pretrained video generation model",
        "LoRA adapters",
        "long-term temporal coherence"
      ]
    },
    "publishedAt": "2025-08-04T14:27:55.000Z",
    "title": "DreamVVT: Mastering Realistic Video Virtual Try-On in the Wild via a\n  Stage-Wise Diffusion Transformer Framework",
    "summary": "Video virtual try-on (VVT) technology has garnered considerable academic\ninterest owing to its promising applications in e-commerce advertising and\nentertainment. However, most existing end-to-end methods rely heavily on scarce\npaired garment-centric datasets and fail to effectively leverage priors of\nadvanced visual models and test-time inputs, making it challenging to\naccurately preserve fine-grained garment details and maintain temporal\nconsistency in unconstrained scenarios. To address these challenges, we propose\nDreamVVT, a carefully designed two-stage framework built upon Diffusion\nTransformers (DiTs), which is inherently capable of leveraging diverse unpaired\nhuman-centric data to enhance adaptability in real-world scenarios. To further\nleverage prior knowledge from pretrained models and test-time inputs, in the\nfirst stage, we sample representative frames from the input video and utilize a\nmulti-frame try-on model integrated with a vision-language model (VLM), to\nsynthesize high-fidelity and semantically consistent keyframe try-on images.\nThese images serve as complementary appearance guidance for subsequent video\ngeneration. In the second stage, skeleton maps together with\nfine-grained motion and appearance descriptions are extracted from the input\ncontent, and these along with the keyframe try-on images are then fed into a\npretrained video generation model enhanced with LoRA adapters. This ensures\nlong-term temporal coherence for unseen regions and enables highly plausible\ndynamic motions. Extensive quantitative and qualitative experiments demonstrate\nthat DreamVVT surpasses existing methods in preserving detailed garment content\nand temporal stability in real-world scenarios. Our project page\nhttps://virtu-lab.github.io/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.02807.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6358d973f53b06a81271d8d6",
      "avatarUrl": "/avatars/06b7e205a1b08d2419afc2f6273028d1.svg",
      "fullname": "Ning",
      "name": "Shuliang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.00222",
      "authors": [
        {
          "_id": "68944a32741a16f544fbcfdc",
          "name": "Yihong Dong",
          "hidden": false
        },
        {
          "_id": "68944a32741a16f544fbcfdd",
          "name": "Xue Jiang",
          "hidden": false
        },
        {
          "_id": "68944a32741a16f544fbcfde",
          "name": "Yongding Tao",
          "hidden": false
        },
        {
          "_id": "68944a32741a16f544fbcfdf",
          "name": "Huanyu Liu",
          "hidden": false
        },
        {
          "_id": "68944a32741a16f544fbcfe0",
          "name": "Kechi Zhang",
          "hidden": false
        },
        {
          "_id": "68944a32741a16f544fbcfe1",
          "name": "Lili Mou",
          "hidden": false
        },
        {
          "_id": "68944a32741a16f544fbcfe2",
          "name": "Rongyu Cao",
          "hidden": false
        },
        {
          "_id": "68944a32741a16f544fbcfe3",
          "name": "Yingwei Ma",
          "hidden": false
        },
        {
          "_id": "68944a32741a16f544fbcfe4",
          "name": "Jue Chen",
          "hidden": false
        },
        {
          "_id": "68944a32741a16f544fbcfe5",
          "name": "Binhua Li",
          "hidden": false
        },
        {
          "_id": "68944a32741a16f544fbcfe6",
          "name": "Zhi Jin",
          "hidden": false
        },
        {
          "_id": "68944a32741a16f544fbcfe7",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "68944a32741a16f544fbcfe8",
          "name": "Yongbin Li",
          "hidden": false
        },
        {
          "_id": "68944a32741a16f544fbcfe9",
          "name": "Ge Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-31T23:55:29.000Z",
      "submittedOnDailyAt": "2025-08-07T05:10:17.088Z",
      "title": "RL-PLUS: Countering Capability Boundary Collapse of LLMs in\n  Reinforcement Learning with Hybrid-policy Optimization",
      "submittedOnDailyBy": {
        "_id": "62e0ef42edb0462c8d51818d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62e0ef42edb0462c8d51818d/3YM7DUynIWiiRFM6_enpg.jpeg",
        "isPro": false,
        "fullname": "Ting-En Lin",
        "user": "tnlin",
        "type": "user"
      },
      "summary": "Reinforcement Learning with Verifiable Reward (RLVR) has significantly\nadvanced the complex reasoning abilities of Large Language Models (LLMs).\nHowever, it struggles to break through the inherent capability boundaries of\nthe base LLM, due to its essentially on-policy strategy coupled with LLM's\nimmense action space and sparse reward. Critically, RLVR can lead to the\ncapability boundary collapse, narrowing the LLM's problem-solving scope. To\naddress this problem, we propose RL-PLUS, a novel hybrid-policy optimization\napproach for LLMs that synergizes internal exploitation with external data to\nachieve stronger reasoning capabilities and surpass the boundaries of base\nmodels. RL-PLUS integrates two core components, i.e., Multiple Importance\nSampling to address distributional mismatch from external data, and\nExploration-Based Advantage Function to guide the model towards high-value,\nunexplored reasoning paths. We provide both theoretical analysis and extensive\nexperiments to demonstrate the superiority and generalizability of our\napproach. Compared with existing RLVR methods, RL-PLUS achieves 1)\nstate-of-the-art performance on six math reasoning benchmarks; 2) superior\nperformance on six out-of-distribution reasoning tasks; 3) consistent and\nsignificant gains across diverse model families, with average relative\nimprovements up to 69.2\\%. Moreover, the analysis of Pass@k curves indicates\nthat RL-PLUS effectively resolves the capability boundary collapse problem.",
      "upvotes": 1,
      "discussionId": "68944a32741a16f544fbcfea",
      "ai_summary": "RL-PLUS, a hybrid-policy optimization approach, enhances LLM reasoning capabilities by integrating Multiple Importance Sampling and Exploration-Based Advantage Function, outperforming RLVR on various benchmarks and resolving capability boundary collapse.",
      "ai_keywords": [
        "Reinforcement Learning with Verifiable Reward (RLVR)",
        "Large Language Models (LLMs)",
        "hybrid-policy optimization",
        "Multiple Importance Sampling",
        "Exploration-Based Advantage Function",
        "capability boundary collapse",
        "math reasoning benchmarks",
        "out-of-distribution reasoning tasks"
      ]
    },
    "publishedAt": "2025-07-31T19:55:29.000Z",
    "title": "RL-PLUS: Countering Capability Boundary Collapse of LLMs in\n  Reinforcement Learning with Hybrid-policy Optimization",
    "summary": "Reinforcement Learning with Verifiable Reward (RLVR) has significantly\nadvanced the complex reasoning abilities of Large Language Models (LLMs).\nHowever, it struggles to break through the inherent capability boundaries of\nthe base LLM, due to its essentially on-policy strategy coupled with LLM's\nimmense action space and sparse reward. Critically, RLVR can lead to the\ncapability boundary collapse, narrowing the LLM's problem-solving scope. To\naddress this problem, we propose RL-PLUS, a novel hybrid-policy optimization\napproach for LLMs that synergizes internal exploitation with external data to\nachieve stronger reasoning capabilities and surpass the boundaries of base\nmodels. RL-PLUS integrates two core components, i.e., Multiple Importance\nSampling to address distributional mismatch from external data, and\nExploration-Based Advantage Function to guide the model towards high-value,\nunexplored reasoning paths. We provide both theoretical analysis and extensive\nexperiments to demonstrate the superiority and generalizability of our\napproach. Compared with existing RLVR methods, RL-PLUS achieves 1)\nstate-of-the-art performance on six math reasoning benchmarks; 2) superior\nperformance on six out-of-distribution reasoning tasks; 3) consistent and\nsignificant gains across diverse model families, with average relative\nimprovements up to 69.2\\%. Moreover, the analysis of Pass@k curves indicates\nthat RL-PLUS effectively resolves the capability boundary collapse problem.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.00222.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62e0ef42edb0462c8d51818d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62e0ef42edb0462c8d51818d/3YM7DUynIWiiRFM6_enpg.jpeg",
      "fullname": "Ting-En Lin",
      "name": "tnlin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.23313",
      "authors": [
        {
          "_id": "68905b400a411b3b8d28d6f2",
          "name": "Alfio Ferrara",
          "hidden": false
        },
        {
          "_id": "68905b400a411b3b8d28d6f3",
          "user": {
            "_id": "6458e5e8c16ecb4815de8aac",
            "avatarUrl": "/avatars/c33c1a67960cdbbc8a970d751dff07af.svg",
            "isPro": false,
            "fullname": "Sergio Picascia",
            "user": "sergiopicascia",
            "type": "user"
          },
          "name": "Sergio Picascia",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-06T19:25:00.992Z",
          "hidden": false
        },
        {
          "_id": "68905b400a411b3b8d28d6f4",
          "name": "Elisabetta Rocchetti",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-31T07:47:01.000Z",
      "submittedOnDailyAt": "2025-08-07T02:58:12.465Z",
      "title": "The Cow of Rembrandt - Analyzing Artistic Prompt Interpretation in\n  Text-to-Image Models",
      "submittedOnDailyBy": {
        "_id": "6458e5e8c16ecb4815de8aac",
        "avatarUrl": "/avatars/c33c1a67960cdbbc8a970d751dff07af.svg",
        "isPro": false,
        "fullname": "Sergio Picascia",
        "user": "sergiopicascia",
        "type": "user"
      },
      "summary": "Text-to-image diffusion models have demonstrated remarkable capabilities in\ngenerating artistic content by learning from billions of images, including\npopular artworks. However, the fundamental question of how these models\ninternally represent concepts, such as content and style in paintings, remains\nunexplored. Traditional computer vision assumes content and style are\northogonal, but diffusion models receive no explicit guidance about this\ndistinction during training. In this work, we investigate how transformer-based\ntext-to-image diffusion models encode content and style concepts when\ngenerating artworks. We leverage cross-attention heatmaps to attribute pixels\nin generated images to specific prompt tokens, enabling us to isolate image\nregions influenced by content-describing versus style-describing tokens. Our\nfindings reveal that diffusion models demonstrate varying degrees of\ncontent-style separation depending on the specific artistic prompt and style\nrequested. In many cases, content tokens primarily influence object-related\nregions while style tokens affect background and texture areas, suggesting an\nemergent understanding of the content-style distinction. These insights\ncontribute to our understanding of how large-scale generative models internally\nrepresent complex artistic concepts without explicit supervision. We share the\ncode and dataset, together with an exploratory tool for visualizing attention\nmaps at https://github.com/umilISLab/artistic-prompt-interpretation.",
      "upvotes": 1,
      "discussionId": "68905b400a411b3b8d28d6f5",
      "projectPage": "https://thecowofrembrandt.islab.di.unimi.it/",
      "githubRepo": "https://github.com/umilISLab/artistic-prompt-interpretation",
      "ai_summary": "Transformer-based text-to-image diffusion models show varying degrees of content-style separation in generated artworks, as revealed by cross-attention heatmaps.",
      "ai_keywords": [
        "text-to-image diffusion models",
        "transformer-based",
        "cross-attention heatmaps",
        "content-style separation",
        "artistic prompts",
        "attention maps"
      ],
      "githubStars": 1
    },
    "publishedAt": "2025-07-31T03:47:01.000Z",
    "title": "The Cow of Rembrandt - Analyzing Artistic Prompt Interpretation in\n  Text-to-Image Models",
    "summary": "Text-to-image diffusion models have demonstrated remarkable capabilities in\ngenerating artistic content by learning from billions of images, including\npopular artworks. However, the fundamental question of how these models\ninternally represent concepts, such as content and style in paintings, remains\nunexplored. Traditional computer vision assumes content and style are\northogonal, but diffusion models receive no explicit guidance about this\ndistinction during training. In this work, we investigate how transformer-based\ntext-to-image diffusion models encode content and style concepts when\ngenerating artworks. We leverage cross-attention heatmaps to attribute pixels\nin generated images to specific prompt tokens, enabling us to isolate image\nregions influenced by content-describing versus style-describing tokens. Our\nfindings reveal that diffusion models demonstrate varying degrees of\ncontent-style separation depending on the specific artistic prompt and style\nrequested. In many cases, content tokens primarily influence object-related\nregions while style tokens affect background and texture areas, suggesting an\nemergent understanding of the content-style distinction. These insights\ncontribute to our understanding of how large-scale generative models internally\nrepresent complex artistic concepts without explicit supervision. We share the\ncode and dataset, together with an exploratory tool for visualizing attention\nmaps at https://github.com/umilISLab/artistic-prompt-interpretation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.23313.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6458e5e8c16ecb4815de8aac",
      "avatarUrl": "/avatars/c33c1a67960cdbbc8a970d751dff07af.svg",
      "fullname": "Sergio Picascia",
      "name": "sergiopicascia",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]