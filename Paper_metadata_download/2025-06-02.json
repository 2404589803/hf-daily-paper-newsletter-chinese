[
  {
    "paper": {
      "id": "2505.24864",
      "authors": [
        {
          "_id": "683d2d05ae87a04bca311b22",
          "name": "Mingjie Liu",
          "hidden": false
        },
        {
          "_id": "683d2d05ae87a04bca311b23",
          "name": "Shizhe Diao",
          "hidden": false
        },
        {
          "_id": "683d2d05ae87a04bca311b24",
          "name": "Ximing Lu",
          "hidden": false
        },
        {
          "_id": "683d2d05ae87a04bca311b25",
          "name": "Jian Hu",
          "hidden": false
        },
        {
          "_id": "683d2d05ae87a04bca311b26",
          "name": "Xin Dong",
          "hidden": false
        },
        {
          "_id": "683d2d05ae87a04bca311b27",
          "name": "Yejin Choi",
          "hidden": false
        },
        {
          "_id": "683d2d05ae87a04bca311b28",
          "name": "Jan Kautz",
          "hidden": false
        },
        {
          "_id": "683d2d05ae87a04bca311b29",
          "name": "Yi Dong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T17:59:01.000Z",
      "submittedOnDailyAt": "2025-06-02T03:18:21.654Z",
      "title": "ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in\n  Large Language Models",
      "submittedOnDailyBy": {
        "_id": "633bd54b00732349209a18fe",
        "avatarUrl": "/avatars/150aa5b8d9bcca24366402932bf2d049.svg",
        "isPro": false,
        "fullname": "Shizhe Diao",
        "user": "shizhediao",
        "type": "user"
      },
      "summary": "Recent advances in reasoning-centric language models have highlighted\nreinforcement learning (RL) as a promising method for aligning models with\nverifiable rewards. However, it remains contentious whether RL truly expands a\nmodel's reasoning capabilities or merely amplifies high-reward outputs already\nlatent in the base model's distribution, and whether continually scaling up RL\ncompute reliably leads to improved reasoning performance. In this work, we\nchallenge prevailing assumptions by demonstrating that prolonged RL (ProRL)\ntraining can uncover novel reasoning strategies that are inaccessible to base\nmodels, even under extensive sampling. We introduce ProRL, a novel training\nmethodology that incorporates KL divergence control, reference policy\nresetting, and a diverse suite of tasks. Our empirical analysis reveals that\nRL-trained models consistently outperform base models across a wide range of\npass@k evaluations, including scenarios where base models fail entirely\nregardless of the number of attempts. We further show that reasoning boundary\nimprovements correlates strongly with task competence of base model and\ntraining duration, suggesting that RL can explore and populate new regions of\nsolution space over time. These findings offer new insights into the conditions\nunder which RL meaningfully expands reasoning boundaries in language models and\nestablish a foundation for future work on long-horizon RL for reasoning. We\nrelease model weights to support further research:\nhttps://huggingface.co/nvidia/Nemotron-Research-Reasoning-Qwen-1.5B",
      "upvotes": 35,
      "discussionId": "683d2d08ae87a04bca311bd4",
      "ai_summary": "Prolonged reinforcement learning training (ProRL) uncovers novel reasoning strategies in language models, outperforming base models and suggesting meaningful expansion of reasoning capabilities.",
      "ai_keywords": [
        "reinforcement learning",
        "RL",
        "ProRL",
        "KL divergence control",
        "reference policy resetting",
        "pass@k evaluations",
        "reasoning boundary improvements",
        "task competence",
        "long-horizon RL"
      ]
    },
    "publishedAt": "2025-05-30T13:59:01.000Z",
    "title": "ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in\n  Large Language Models",
    "summary": "Recent advances in reasoning-centric language models have highlighted\nreinforcement learning (RL) as a promising method for aligning models with\nverifiable rewards. However, it remains contentious whether RL truly expands a\nmodel's reasoning capabilities or merely amplifies high-reward outputs already\nlatent in the base model's distribution, and whether continually scaling up RL\ncompute reliably leads to improved reasoning performance. In this work, we\nchallenge prevailing assumptions by demonstrating that prolonged RL (ProRL)\ntraining can uncover novel reasoning strategies that are inaccessible to base\nmodels, even under extensive sampling. We introduce ProRL, a novel training\nmethodology that incorporates KL divergence control, reference policy\nresetting, and a diverse suite of tasks. Our empirical analysis reveals that\nRL-trained models consistently outperform base models across a wide range of\npass@k evaluations, including scenarios where base models fail entirely\nregardless of the number of attempts. We further show that reasoning boundary\nimprovements correlates strongly with task competence of base model and\ntraining duration, suggesting that RL can explore and populate new regions of\nsolution space over time. These findings offer new insights into the conditions\nunder which RL meaningfully expands reasoning boundaries in language models and\nestablish a foundation for future work on long-horizon RL for reasoning. We\nrelease model weights to support further research:\nhttps://huggingface.co/nvidia/Nemotron-Research-Reasoning-Qwen-1.5B",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24864.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "633bd54b00732349209a18fe",
      "avatarUrl": "/avatars/150aa5b8d9bcca24366402932bf2d049.svg",
      "fullname": "Shizhe Diao",
      "name": "shizhediao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.24863",
      "authors": [
        {
          "_id": "683d0b3de2a7d8d9778bd141",
          "user": {
            "_id": "6719bfd07c6e6c83a388aeae",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6719bfd07c6e6c83a388aeae/jHxryk04dzHo23TX5F5sz.png",
            "isPro": false,
            "fullname": "Junyu Zhang",
            "user": "jyzhang1208",
            "type": "user"
          },
          "name": "Junyu Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:40:59.716Z",
          "hidden": false
        },
        {
          "_id": "683d0b3de2a7d8d9778bd142",
          "user": {
            "_id": "6201fc5d91d53938a6432fbf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6201fc5d91d53938a6432fbf/VLs8ZYaZrop4KBpZn53fH.jpeg",
            "isPro": false,
            "fullname": "Runpei Dong",
            "user": "RunpeiDong",
            "type": "user"
          },
          "name": "Runpei Dong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:41:03.079Z",
          "hidden": false
        },
        {
          "_id": "683d0b3de2a7d8d9778bd143",
          "name": "Han Wang",
          "hidden": false
        },
        {
          "_id": "683d0b3de2a7d8d9778bd144",
          "name": "Xuying Ning",
          "hidden": false
        },
        {
          "_id": "683d0b3de2a7d8d9778bd145",
          "name": "Haoran Geng",
          "hidden": false
        },
        {
          "_id": "683d0b3de2a7d8d9778bd146",
          "name": "Peihao Li",
          "hidden": false
        },
        {
          "_id": "683d0b3de2a7d8d9778bd147",
          "name": "Xialin He",
          "hidden": false
        },
        {
          "_id": "683d0b3de2a7d8d9778bd148",
          "name": "Yutong Bai",
          "hidden": false
        },
        {
          "_id": "683d0b3de2a7d8d9778bd149",
          "name": "Jitendra Malik",
          "hidden": false
        },
        {
          "_id": "683d0b3de2a7d8d9778bd14a",
          "name": "Saurabh Gupta",
          "hidden": false
        },
        {
          "_id": "683d0b3de2a7d8d9778bd14b",
          "name": "Huan Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6201fc5d91d53938a6432fbf/dBNLCtnWBtBclw0ZZsYBU.png"
      ],
      "publishedAt": "2025-05-30T17:58:36.000Z",
      "submittedOnDailyAt": "2025-06-02T00:55:04.615Z",
      "title": "AlphaOne: Reasoning Models Thinking Slow and Fast at Test Time",
      "submittedOnDailyBy": {
        "_id": "6201fc5d91d53938a6432fbf",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6201fc5d91d53938a6432fbf/VLs8ZYaZrop4KBpZn53fH.jpeg",
        "isPro": false,
        "fullname": "Runpei Dong",
        "user": "RunpeiDong",
        "type": "user"
      },
      "summary": "This paper presents AlphaOne (alpha1), a universal framework for\nmodulating reasoning progress in large reasoning models (LRMs) at test time.\nalpha1 first introduces alpha moment, which represents the scaled\nthinking phase with a universal parameter alpha. Within this scaled\npre-alpha moment phase, it dynamically schedules slow thinking transitions\nby modeling the insertion of reasoning transition tokens as a Bernoulli\nstochastic process. After the alpha moment, alpha1 deterministically\nterminates slow thinking with the end-of-thinking token, thereby fostering fast\nreasoning and efficient answer generation. This approach unifies and\ngeneralizes existing monotonic scaling methods by enabling flexible and dense\nslow-to-fast reasoning modulation. Extensive empirical studies on various\nchallenging benchmarks across mathematical, coding, and scientific domains\ndemonstrate alpha1's superior reasoning capability and efficiency. Project\npage: https://alphaone-project.github.io/",
      "upvotes": 28,
      "discussionId": "683d0b3ee2a7d8d9778bd1ce",
      "projectPage": "https://alphaone-project.github.io/",
      "githubRepo": "https://github.com/ASTRAL-Group/AlphaOne",
      "ai_summary": "AlphaOne dynamically modulates reasoning in large models by introducing $\\alpha$ moment and Bernoulli process for slow thinking, improving efficiency and capability across diverse domains.",
      "ai_keywords": [
        "AlphaOne",
        "$\\alpha$ moment",
        "Bernoulli stochastic process",
        "large reasoning models",
        "reasoning transition tokens",
        "end-of-thinking token",
        "monotonic scaling methods",
        "fast reasoning",
        "efficient answer generation"
      ]
    },
    "publishedAt": "2025-05-30T13:58:36.000Z",
    "title": "AlphaOne: Reasoning Models Thinking Slow and Fast at Test Time",
    "summary": "This paper presents AlphaOne (alpha1), a universal framework for\nmodulating reasoning progress in large reasoning models (LRMs) at test time.\nalpha1 first introduces alpha moment, which represents the scaled\nthinking phase with a universal parameter alpha. Within this scaled\npre-alpha moment phase, it dynamically schedules slow thinking transitions\nby modeling the insertion of reasoning transition tokens as a Bernoulli\nstochastic process. After the alpha moment, alpha1 deterministically\nterminates slow thinking with the end-of-thinking token, thereby fostering fast\nreasoning and efficient answer generation. This approach unifies and\ngeneralizes existing monotonic scaling methods by enabling flexible and dense\nslow-to-fast reasoning modulation. Extensive empirical studies on various\nchallenging benchmarks across mathematical, coding, and scientific domains\ndemonstrate alpha1's superior reasoning capability and efficiency. Project\npage: https://alphaone-project.github.io/",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6201fc5d91d53938a6432fbf/dBNLCtnWBtBclw0ZZsYBU.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24863.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6201fc5d91d53938a6432fbf",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6201fc5d91d53938a6432fbf/VLs8ZYaZrop4KBpZn53fH.jpeg",
      "fullname": "Runpei Dong",
      "name": "RunpeiDong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.24098",
      "authors": [
        {
          "_id": "683d2cee5bdbb3803e42bc8a",
          "name": "Zhongmou He",
          "hidden": false
        },
        {
          "_id": "683d2cee5bdbb3803e42bc8b",
          "name": "Yee Man Choi",
          "hidden": false
        },
        {
          "_id": "683d2cee5bdbb3803e42bc8c",
          "name": "Kexun Zhang",
          "hidden": false
        },
        {
          "_id": "683d2cee5bdbb3803e42bc8d",
          "name": "Jiabao Ji",
          "hidden": false
        },
        {
          "_id": "683d2cee5bdbb3803e42bc8e",
          "user": {
            "_id": "65a374a59acab1998092a9bc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65a374a59acab1998092a9bc/M3s_7bSf9G-6b9nLg7N3Z.jpeg",
            "isPro": false,
            "fullname": "Antonio",
            "user": "JuntingZhou",
            "type": "user"
          },
          "name": "Junting Zhou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:40:34.926Z",
          "hidden": false
        },
        {
          "_id": "683d2cee5bdbb3803e42bc8f",
          "name": "Dejia Xu",
          "hidden": false
        },
        {
          "_id": "683d2cee5bdbb3803e42bc90",
          "name": "Ivan Bercovich",
          "hidden": false
        },
        {
          "_id": "683d2cee5bdbb3803e42bc91",
          "name": "Aidan Zhang",
          "hidden": false
        },
        {
          "_id": "683d2cee5bdbb3803e42bc92",
          "name": "Lei Li",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62ee423b4bebb4ab55c674b1/yE3pB5JGaOf-sdyDjpCk6.png"
      ],
      "publishedAt": "2025-05-30T01:00:34.000Z",
      "submittedOnDailyAt": "2025-06-02T03:20:27.903Z",
      "title": "HardTests: Synthesizing High-Quality Test Cases for LLM Coding",
      "submittedOnDailyBy": {
        "_id": "62ee423b4bebb4ab55c674b1",
        "avatarUrl": "/avatars/ce2797937e8225937fc84d6847d50077.svg",
        "isPro": false,
        "fullname": "Kexun Zhang",
        "user": "k1z",
        "type": "user"
      },
      "summary": "Verifiers play a crucial role in large language model (LLM) reasoning, needed\nby post-training techniques such as reinforcement learning. However, reliable\nverifiers are hard to get for difficult coding problems, because a\nwell-disguised wrong solution may only be detected by carefully human-written\nedge cases that are difficult to synthesize. To address this issue, we propose\nHARDTESTGEN, a pipeline for high-quality test synthesis using LLMs. With this\npipeline, we curate a comprehensive competitive programming dataset HARDTESTS\nwith 47k problems and synthetic high-quality tests. Compared with existing\ntests, HARDTESTGEN tests demonstrate precision that is 11.3 percentage points\nhigher and recall that is 17.5 percentage points higher when evaluating\nLLM-generated code. For harder problems, the improvement in precision can be as\nlarge as 40 points. HARDTESTS also proves to be more effective for model\ntraining, measured by downstream code generation performance. We will\nopen-source our dataset and synthesis pipeline at\nhttps://leililab.github.io/HardTests/.",
      "upvotes": 18,
      "discussionId": "683d2cef5bdbb3803e42bccc",
      "projectPage": "https://leililab.github.io/HardTests/",
      "ai_summary": "HARDTESTGEN creates a large, high-quality competitive programming dataset to enhance the precision and recall of verifiers in evaluating LLM-generated code.",
      "ai_keywords": [
        "LLM reasoning",
        "reinforcement learning",
        "verifiers",
        "test synthesis",
        "LLMs",
        "competitive programming",
        "synthetic tests",
        "precision",
        "recall",
        "code generation performance"
      ]
    },
    "publishedAt": "2025-05-29T21:00:34.000Z",
    "title": "HardTests: Synthesizing High-Quality Test Cases for LLM Coding",
    "summary": "Verifiers play a crucial role in large language model (LLM) reasoning, needed\nby post-training techniques such as reinforcement learning. However, reliable\nverifiers are hard to get for difficult coding problems, because a\nwell-disguised wrong solution may only be detected by carefully human-written\nedge cases that are difficult to synthesize. To address this issue, we propose\nHARDTESTGEN, a pipeline for high-quality test synthesis using LLMs. With this\npipeline, we curate a comprehensive competitive programming dataset HARDTESTS\nwith 47k problems and synthetic high-quality tests. Compared with existing\ntests, HARDTESTGEN tests demonstrate precision that is 11.3 percentage points\nhigher and recall that is 17.5 percentage points higher when evaluating\nLLM-generated code. For harder problems, the improvement in precision can be as\nlarge as 40 points. HARDTESTS also proves to be more effective for model\ntraining, measured by downstream code generation performance. We will\nopen-source our dataset and synthesis pipeline at\nhttps://leililab.github.io/HardTests/.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62ee423b4bebb4ab55c674b1/yE3pB5JGaOf-sdyDjpCk6.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24098.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62ee423b4bebb4ab55c674b1",
      "avatarUrl": "/avatars/ce2797937e8225937fc84d6847d50077.svg",
      "fullname": "Kexun Zhang",
      "name": "k1z",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.14752",
      "authors": [
        {
          "_id": "6832c2c8ba29b909f4013a6d",
          "user": {
            "_id": "67569b1860146dd8c9c8008f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67569b1860146dd8c9c8008f/f5Tz2yVTry4LGyQE2VC6-.jpeg",
            "isPro": false,
            "fullname": "Yihong Tang",
            "user": "HYTYH",
            "type": "user"
          },
          "name": "Yihong Tang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:12:00.941Z",
          "hidden": false
        },
        {
          "_id": "6832c2c8ba29b909f4013a6e",
          "name": "Menglin Kong",
          "hidden": false
        },
        {
          "_id": "6832c2c8ba29b909f4013a6f",
          "name": "Lijun Sun",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/67569b1860146dd8c9c8008f/VaA9NxCa0ncxzh03aZE8c.png",
        "https://cdn-uploads.huggingface.co/production/uploads/67569b1860146dd8c9c8008f/oEgXPOzTQVTDfuTp5Z575.png"
      ],
      "publishedAt": "2025-05-20T13:35:38.000Z",
      "submittedOnDailyAt": "2025-06-02T02:10:16.659Z",
      "title": "Large Language Models for Data Synthesis",
      "submittedOnDailyBy": {
        "_id": "67569b1860146dd8c9c8008f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67569b1860146dd8c9c8008f/f5Tz2yVTry4LGyQE2VC6-.jpeg",
        "isPro": false,
        "fullname": "Yihong Tang",
        "user": "HYTYH",
        "type": "user"
      },
      "summary": "Generating synthetic data that faithfully captures the statistical structure\nof real-world distributions is a fundamental challenge in data modeling.\nClassical approaches often depend on strong parametric assumptions or manual\nstructural design and struggle in high-dimensional or heterogeneous domains.\nRecent progress in Large Language Models (LLMs) reveals their potential as\nflexible, high-dimensional priors over real-world distributions. However, when\napplied to data synthesis, standard LLM-based sampling is inefficient,\nconstrained by fixed context limits, and fails to ensure statistical alignment.\nGiven this, we introduce LLMSynthor, a general framework for data synthesis\nthat transforms LLMs into structure-aware simulators guided by distributional\nfeedback. LLMSynthor treats the LLM as a nonparametric copula simulator for\nmodeling high-order dependencies and introduces LLM Proposal Sampling to\ngenerate grounded proposal distributions that improve sampling efficiency\nwithout requiring rejection. By minimizing discrepancies in the summary\nstatistics space, the iterative synthesis loop aligns real and synthetic data\nwhile gradually uncovering and refining the latent generative structure. We\nevaluate LLMSynthor in both controlled and real-world settings using\nheterogeneous datasets in privacy-sensitive domains (e.g., e-commerce,\npopulation, and mobility) that encompass both structured and unstructured\nformats. The synthetic data produced by LLMSynthor shows high statistical\nfidelity, practical utility, and cross-data adaptability, positioning it as a\nvaluable tool across economics, social science, urban studies, and beyond.",
      "upvotes": 16,
      "discussionId": "6832c2c9ba29b909f4013aea",
      "projectPage": "https://yihongt.github.io/llmsynthor_web/",
      "githubRepo": "https://github.com/YihongT/LLMSynthor",
      "ai_summary": "LLMSynthor enhances LLMs for efficient and statistically accurate data synthesis through distributional feedback and proposal sampling.",
      "ai_keywords": [
        "Large Language Models",
        "LLMSynthor",
        "nonparametric copula simulator",
        "LLM Proposal Sampling",
        "summary statistics space",
        "synthetic data",
        "statistical fidelity",
        "practical utility",
        "cross-data adaptability"
      ]
    },
    "publishedAt": "2025-05-20T09:35:38.000Z",
    "title": "Large Language Models for Data Synthesis",
    "summary": "Generating synthetic data that faithfully captures the statistical structure\nof real-world distributions is a fundamental challenge in data modeling.\nClassical approaches often depend on strong parametric assumptions or manual\nstructural design and struggle in high-dimensional or heterogeneous domains.\nRecent progress in Large Language Models (LLMs) reveals their potential as\nflexible, high-dimensional priors over real-world distributions. However, when\napplied to data synthesis, standard LLM-based sampling is inefficient,\nconstrained by fixed context limits, and fails to ensure statistical alignment.\nGiven this, we introduce LLMSynthor, a general framework for data synthesis\nthat transforms LLMs into structure-aware simulators guided by distributional\nfeedback. LLMSynthor treats the LLM as a nonparametric copula simulator for\nmodeling high-order dependencies and introduces LLM Proposal Sampling to\ngenerate grounded proposal distributions that improve sampling efficiency\nwithout requiring rejection. By minimizing discrepancies in the summary\nstatistics space, the iterative synthesis loop aligns real and synthetic data\nwhile gradually uncovering and refining the latent generative structure. We\nevaluate LLMSynthor in both controlled and real-world settings using\nheterogeneous datasets in privacy-sensitive domains (e.g., e-commerce,\npopulation, and mobility) that encompass both structured and unstructured\nformats. The synthetic data produced by LLMSynthor shows high statistical\nfidelity, practical utility, and cross-data adaptability, positioning it as a\nvaluable tool across economics, social science, urban studies, and beyond.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/67569b1860146dd8c9c8008f/VaA9NxCa0ncxzh03aZE8c.png",
      "https://cdn-uploads.huggingface.co/production/uploads/67569b1860146dd8c9c8008f/oEgXPOzTQVTDfuTp5Z575.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14752.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67569b1860146dd8c9c8008f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67569b1860146dd8c9c8008f/f5Tz2yVTry4LGyQE2VC6-.jpeg",
      "fullname": "Yihong Tang",
      "name": "HYTYH",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.24867",
      "authors": [
        {
          "_id": "683d3d6f3f97feb881155aef",
          "user": {
            "_id": "5df7ca7cda6d0311fd3d53f2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5df7ca7cda6d0311fd3d53f2/dtAoDSqgNxeO9AYg9V3na.jpeg",
            "isPro": false,
            "fullname": "Ujjwal Upadhyay",
            "user": "ujjwal9",
            "type": "user"
          },
          "name": "Ujjwal Upadhyay",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-02T05:58:12.617Z",
          "hidden": false
        },
        {
          "_id": "683d3d6f3f97feb881155af0",
          "user": {
            "_id": "65262a396b41932089fd7bae",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65262a396b41932089fd7bae/6YIEoAfJojuTW1UOKlwZT.png",
            "isPro": true,
            "fullname": "Mukul Ranjan",
            "user": "mukul54",
            "type": "user"
          },
          "name": "Mukul Ranjan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:40:23.895Z",
          "hidden": false
        },
        {
          "_id": "683d3d6f3f97feb881155af1",
          "name": "Zhiqiang Shen",
          "hidden": false
        },
        {
          "_id": "683d3d6f3f97feb881155af2",
          "name": "Mohamed Elhoseiny",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T17:59:12.000Z",
      "submittedOnDailyAt": "2025-06-02T04:31:40.253Z",
      "title": "Time Blindness: Why Video-Language Models Can't See What Humans Can?",
      "submittedOnDailyBy": {
        "_id": "65262a396b41932089fd7bae",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65262a396b41932089fd7bae/6YIEoAfJojuTW1UOKlwZT.png",
        "isPro": true,
        "fullname": "Mukul Ranjan",
        "user": "mukul54",
        "type": "user"
      },
      "summary": "Recent advances in vision-language models (VLMs) have made impressive strides\nin understanding spatio-temporal relationships in videos. However, when spatial\ninformation is obscured, these models struggle to capture purely temporal\npatterns. We introduce SpookyBench, a benchmark where information is\nencoded solely in temporal sequences of noise-like frames, mirroring natural\nphenomena from biological signaling to covert communication. Interestingly,\nwhile humans can recognize shapes, text, and patterns in these sequences with\nover 98% accuracy, state-of-the-art VLMs achieve 0% accuracy. This performance\ngap highlights a critical limitation: an over-reliance on frame-level spatial\nfeatures and an inability to extract meaning from temporal cues. Furthermore,\nwhen trained in data sets with low spatial signal-to-noise ratios (SNR),\ntemporal understanding of models degrades more rapidly than human perception,\nespecially in tasks requiring fine-grained temporal reasoning. Overcoming this\nlimitation will require novel architectures or training paradigms that decouple\nspatial dependencies from temporal processing. Our systematic analysis shows\nthat this issue persists across model scales and architectures. We release\nSpookyBench to catalyze research in temporal pattern recognition and bridge the\ngap between human and machine video understanding. Dataset and code has been\nmade available on our project website: https://timeblindness.github.io/.",
      "upvotes": 13,
      "discussionId": "683d3d743f97feb881155c56",
      "projectPage": "https://timeblindness.github.io",
      "githubRepo": "https://github.com/TimeBlindness/time-blindness",
      "ai_summary": "SpookyBench is a benchmark for temporal pattern recognition in videos that highlights the limitations of vision-language models in processing noise-like frames without spatial information.",
      "ai_keywords": [
        "vision-language models",
        "VLMs",
        "spatio-temporal relationships",
        "temporal sequences",
        "noise-like frames",
        "biological signaling",
        "covert communication",
        "frame-level spatial features",
        "temporal understanding",
        "data sets",
        "low spatial signal-to-noise ratios",
        "SNR",
        "temporal reasoning",
        "novel architectures",
        "training paradigms",
        "systematic analysis"
      ]
    },
    "publishedAt": "2025-05-30T13:59:12.000Z",
    "title": "Time Blindness: Why Video-Language Models Can't See What Humans Can?",
    "summary": "Recent advances in vision-language models (VLMs) have made impressive strides\nin understanding spatio-temporal relationships in videos. However, when spatial\ninformation is obscured, these models struggle to capture purely temporal\npatterns. We introduce SpookyBench, a benchmark where information is\nencoded solely in temporal sequences of noise-like frames, mirroring natural\nphenomena from biological signaling to covert communication. Interestingly,\nwhile humans can recognize shapes, text, and patterns in these sequences with\nover 98% accuracy, state-of-the-art VLMs achieve 0% accuracy. This performance\ngap highlights a critical limitation: an over-reliance on frame-level spatial\nfeatures and an inability to extract meaning from temporal cues. Furthermore,\nwhen trained in data sets with low spatial signal-to-noise ratios (SNR),\ntemporal understanding of models degrades more rapidly than human perception,\nespecially in tasks requiring fine-grained temporal reasoning. Overcoming this\nlimitation will require novel architectures or training paradigms that decouple\nspatial dependencies from temporal processing. Our systematic analysis shows\nthat this issue persists across model scales and architectures. We release\nSpookyBench to catalyze research in temporal pattern recognition and bridge the\ngap between human and machine video understanding. Dataset and code has been\nmade available on our project website: https://timeblindness.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24867.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65262a396b41932089fd7bae",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65262a396b41932089fd7bae/6YIEoAfJojuTW1UOKlwZT.png",
      "fullname": "Mukul Ranjan",
      "name": "mukul54",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.18842",
      "authors": [
        {
          "_id": "6839543d6451d371f9e834ec",
          "name": "Jiwan Chung",
          "hidden": false
        },
        {
          "_id": "6839543d6451d371f9e834ed",
          "user": {
            "_id": "646aecb04c1cd18b497a50ee",
            "avatarUrl": "/avatars/de15c724056f36a41cb4f375d05ed836.svg",
            "isPro": false,
            "fullname": "Junhyeok Kim",
            "user": "kjunh",
            "type": "user"
          },
          "name": "Junhyeok Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:46:37.442Z",
          "hidden": false
        },
        {
          "_id": "6839543d6451d371f9e834ee",
          "user": {
            "_id": "67021743e4d49b157afd8260",
            "avatarUrl": "/avatars/2a22a18cd45f6d115e8a3a5d1e477dcb.svg",
            "isPro": false,
            "fullname": "Siyeol Kim",
            "user": "siyeolkim",
            "type": "user"
          },
          "name": "Siyeol Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:46:34.334Z",
          "hidden": false
        },
        {
          "_id": "6839543d6451d371f9e834ef",
          "name": "Jaeyoung Lee",
          "hidden": false
        },
        {
          "_id": "6839543d6451d371f9e834f0",
          "name": "Min Soo Kim",
          "hidden": false
        },
        {
          "_id": "6839543d6451d371f9e834f1",
          "name": "Youngjae Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-24T19:30:47.000Z",
      "submittedOnDailyAt": "2025-06-02T02:58:04.513Z",
      "title": "Don't Look Only Once: Towards Multimodal Interactive Reasoning with\n  Selective Visual Revisitation",
      "submittedOnDailyBy": {
        "_id": "646aecb04c1cd18b497a50ee",
        "avatarUrl": "/avatars/de15c724056f36a41cb4f375d05ed836.svg",
        "isPro": false,
        "fullname": "Junhyeok Kim",
        "user": "kjunh",
        "type": "user"
      },
      "summary": "We present v1, a lightweight extension to Multimodal Large Language Models\n(MLLMs) that enables selective visual revisitation during inference. While\ncurrent MLLMs typically consume visual input only once and reason purely over\ninternal memory, v1 introduces a simple point-and-copy mechanism that allows\nthe model to dynamically retrieve relevant image regions throughout the\nreasoning process. This mechanism augments existing architectures with minimal\nmodifications, enabling contextual access to visual tokens based on the model's\nevolving hypotheses. To train this capability, we construct v1g, a dataset of\n300K multimodal reasoning traces with interleaved visual grounding annotations.\nExperiments on three multimodal mathematical reasoning benchmarks -- MathVista,\nMathVision, and MathVerse -- demonstrate that v1 consistently improves\nperformance over comparable baselines, particularly on tasks requiring\nfine-grained visual reference and multi-step reasoning. Our results suggest\nthat dynamic visual access is a promising direction for enhancing grounded\nmultimodal reasoning. Code, models, and data will be released to support future\nresearch.",
      "upvotes": 13,
      "discussionId": "6839543f6451d371f9e83544",
      "githubRepo": "https://github.com/jun297/v1",
      "ai_summary": "v1 enhances Multimodal Large Language Models by enabling selective and dynamic visual region retrieval during inference, improving performance on multimodal reasoning tasks.",
      "ai_keywords": [
        "Multimodal Large Language Models (MLLMs)",
        "point-and-copy mechanism",
        "visual tokens",
        "multimodal reasoning traces",
        "visual grounding annotations",
        "MathVista",
        "MathVision",
        "MathVerse",
        "grounded multimodal reasoning"
      ]
    },
    "publishedAt": "2025-05-24T15:30:47.000Z",
    "title": "Don't Look Only Once: Towards Multimodal Interactive Reasoning with\n  Selective Visual Revisitation",
    "summary": "We present v1, a lightweight extension to Multimodal Large Language Models\n(MLLMs) that enables selective visual revisitation during inference. While\ncurrent MLLMs typically consume visual input only once and reason purely over\ninternal memory, v1 introduces a simple point-and-copy mechanism that allows\nthe model to dynamically retrieve relevant image regions throughout the\nreasoning process. This mechanism augments existing architectures with minimal\nmodifications, enabling contextual access to visual tokens based on the model's\nevolving hypotheses. To train this capability, we construct v1g, a dataset of\n300K multimodal reasoning traces with interleaved visual grounding annotations.\nExperiments on three multimodal mathematical reasoning benchmarks -- MathVista,\nMathVision, and MathVerse -- demonstrate that v1 consistently improves\nperformance over comparable baselines, particularly on tasks requiring\nfine-grained visual reference and multi-step reasoning. Our results suggest\nthat dynamic visual access is a promising direction for enhancing grounded\nmultimodal reasoning. Code, models, and data will be released to support future\nresearch.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18842.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "646aecb04c1cd18b497a50ee",
      "avatarUrl": "/avatars/de15c724056f36a41cb4f375d05ed836.svg",
      "fullname": "Junhyeok Kim",
      "name": "kjunh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.24878",
      "authors": [
        {
          "_id": "683d160e51706d12b2c6f79f",
          "name": "Yaxin Luo",
          "hidden": false
        },
        {
          "_id": "683d160e51706d12b2c6f7a0",
          "name": "Zhaoyi Li",
          "hidden": false
        },
        {
          "_id": "683d160e51706d12b2c6f7a1",
          "name": "Jiacheng Liu",
          "hidden": false
        },
        {
          "_id": "683d160e51706d12b2c6f7a2",
          "user": {
            "_id": "683d2ac900c71614bab8ea02",
            "avatarUrl": "/avatars/7cb1a5c2c778774262a7d7cb6d309abe.svg",
            "isPro": false,
            "fullname": "Jiacheng Cui",
            "user": "jiachengcui888",
            "type": "user"
          },
          "name": "Jiacheng Cui",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:40:52.498Z",
          "hidden": false
        },
        {
          "_id": "683d160e51706d12b2c6f7a3",
          "name": "Xiaohan Zhao",
          "hidden": false
        },
        {
          "_id": "683d160e51706d12b2c6f7a4",
          "name": "Zhiqiang Shen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T17:59:55.000Z",
      "submittedOnDailyAt": "2025-06-02T01:40:24.093Z",
      "title": "Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and\n  Benchmarking Multimodal LLM Agents",
      "submittedOnDailyBy": {
        "_id": "653cb809b424289c5f384a02",
        "avatarUrl": "/avatars/a1565ab5ae51075c75d6857d64c426a8.svg",
        "isPro": true,
        "fullname": "YaxinLuo",
        "user": "YaxinLuo",
        "type": "user"
      },
      "summary": "CAPTCHAs have been a critical bottleneck for deploying web agents in\nreal-world applications, often blocking them from completing end-to-end\nautomation tasks. While modern multimodal LLM agents have demonstrated\nimpressive performance in static perception tasks, their ability to handle\ninteractive, multi-step reasoning challenges like CAPTCHAs is largely untested.\nTo address this gap, we introduce Open CaptchaWorld, the first web-based\nbenchmark and platform specifically designed to evaluate the visual reasoning\nand interaction capabilities of MLLM-powered agents through diverse and dynamic\nCAPTCHA puzzles. Our benchmark spans 20 modern CAPTCHA types, totaling 225\nCAPTCHAs, annotated with a new metric we propose: CAPTCHA Reasoning Depth,\nwhich quantifies the number of cognitive and motor steps required to solve each\npuzzle. Experimental results show that humans consistently achieve near-perfect\nscores, state-of-the-art MLLM agents struggle significantly, with success rates\nat most 40.0% by Browser-Use Openai-o3, far below human-level performance,\n93.3%. This highlights Open CaptchaWorld as a vital benchmark for diagnosing\nthe limits of current multimodal agents and guiding the development of more\nrobust multimodal reasoning systems. Code and Data are available at this https\nURL.",
      "upvotes": 10,
      "discussionId": "683d160f51706d12b2c6f7f4",
      "githubRepo": "https://github.com/MetaAgentX/OpenCaptchaWorld",
      "ai_summary": "Open CaptchaWorld benchmark evaluates MLLM-powered agents on diverse CAPTCHA puzzles, revealing significant performance gaps compared to humans.",
      "ai_keywords": [
        "multimodal LLM",
        "CAPTCHA",
        "visual reasoning",
        "interaction capabilities",
        "CAPTCHA Reasoning Depth",
        "Browser-Use Openai-o3"
      ]
    },
    "publishedAt": "2025-05-30T13:59:55.000Z",
    "title": "Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and\n  Benchmarking Multimodal LLM Agents",
    "summary": "CAPTCHAs have been a critical bottleneck for deploying web agents in\nreal-world applications, often blocking them from completing end-to-end\nautomation tasks. While modern multimodal LLM agents have demonstrated\nimpressive performance in static perception tasks, their ability to handle\ninteractive, multi-step reasoning challenges like CAPTCHAs is largely untested.\nTo address this gap, we introduce Open CaptchaWorld, the first web-based\nbenchmark and platform specifically designed to evaluate the visual reasoning\nand interaction capabilities of MLLM-powered agents through diverse and dynamic\nCAPTCHA puzzles. Our benchmark spans 20 modern CAPTCHA types, totaling 225\nCAPTCHAs, annotated with a new metric we propose: CAPTCHA Reasoning Depth,\nwhich quantifies the number of cognitive and motor steps required to solve each\npuzzle. Experimental results show that humans consistently achieve near-perfect\nscores, state-of-the-art MLLM agents struggle significantly, with success rates\nat most 40.0% by Browser-Use Openai-o3, far below human-level performance,\n93.3%. This highlights Open CaptchaWorld as a vital benchmark for diagnosing\nthe limits of current multimodal agents and guiding the development of more\nrobust multimodal reasoning systems. Code and Data are available at this https\nURL.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24878.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "653cb809b424289c5f384a02",
      "avatarUrl": "/avatars/a1565ab5ae51075c75d6857d64c426a8.svg",
      "fullname": "YaxinLuo",
      "name": "YaxinLuo",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.24521",
      "authors": [
        {
          "_id": "683d11d1495f0b58f2fd49a9",
          "name": "Yang-Tian Sun",
          "hidden": false
        },
        {
          "_id": "683d11d1495f0b58f2fd49aa",
          "name": "Xin Yu",
          "hidden": false
        },
        {
          "_id": "683d11d1495f0b58f2fd49ab",
          "name": "Zehuan Huang",
          "hidden": false
        },
        {
          "_id": "683d11d1495f0b58f2fd49ac",
          "name": "Yi-Hua Huang",
          "hidden": false
        },
        {
          "_id": "683d11d1495f0b58f2fd49ad",
          "name": "Yuan-Chen Guo",
          "hidden": false
        },
        {
          "_id": "683d11d1495f0b58f2fd49ae",
          "name": "Ziyi Yang",
          "hidden": false
        },
        {
          "_id": "683d11d1495f0b58f2fd49af",
          "name": "Yan-Pei Cao",
          "hidden": false
        },
        {
          "_id": "683d11d1495f0b58f2fd49b0",
          "name": "Xiaojuan Qi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T12:31:59.000Z",
      "submittedOnDailyAt": "2025-06-02T01:25:45.570Z",
      "title": "UniGeo: Taming Video Diffusion for Unified Consistent Geometry\n  Estimation",
      "submittedOnDailyBy": {
        "_id": "6375d136dee28348a9c63cbf",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6375d136dee28348a9c63cbf/gK465HBrQWIOZ-qtHb-Vh.jpeg",
        "isPro": false,
        "fullname": "zehuan-huang",
        "user": "huanngzh",
        "type": "user"
      },
      "summary": "Recently, methods leveraging diffusion model priors to assist monocular\ngeometric estimation (e.g., depth and normal) have gained significant attention\ndue to their strong generalization ability. However, most existing works focus\non estimating geometric properties within the camera coordinate system of\nindividual video frames, neglecting the inherent ability of diffusion models to\ndetermine inter-frame correspondence. In this work, we demonstrate that,\nthrough appropriate design and fine-tuning, the intrinsic consistency of video\ngeneration models can be effectively harnessed for consistent geometric\nestimation. Specifically, we 1) select geometric attributes in the global\ncoordinate system that share the same correspondence with video frames as the\nprediction targets, 2) introduce a novel and efficient conditioning method by\nreusing positional encodings, and 3) enhance performance through joint training\non multiple geometric attributes that share the same correspondence. Our\nresults achieve superior performance in predicting global geometric attributes\nin videos and can be directly applied to reconstruction tasks. Even when\ntrained solely on static video data, our approach exhibits the potential to\ngeneralize to dynamic video scenes.",
      "upvotes": 8,
      "discussionId": "683d11d3495f0b58f2fd4a95",
      "projectPage": "https://sunyangtian.github.io/UniGeo-web/",
      "githubRepo": "https://github.com/SunYangtian/UniGeo",
      "ai_summary": "Video generation models leveraging diffusion priors achieve superior global geometric attribute estimation and reconstructions, benefiting from inter-frame consistency and joint training on shared attributes.",
      "ai_keywords": [
        "diffusion models",
        "monocular geometric estimation",
        "depth",
        "normal",
        "camera coordinate system",
        "intrinsic consistency",
        "video generation models",
        "global coordinate system",
        "positional encodings",
        "joint training",
        "static video data",
        "dynamic video scenes"
      ]
    },
    "publishedAt": "2025-05-30T08:31:59.000Z",
    "title": "UniGeo: Taming Video Diffusion for Unified Consistent Geometry\n  Estimation",
    "summary": "Recently, methods leveraging diffusion model priors to assist monocular\ngeometric estimation (e.g., depth and normal) have gained significant attention\ndue to their strong generalization ability. However, most existing works focus\non estimating geometric properties within the camera coordinate system of\nindividual video frames, neglecting the inherent ability of diffusion models to\ndetermine inter-frame correspondence. In this work, we demonstrate that,\nthrough appropriate design and fine-tuning, the intrinsic consistency of video\ngeneration models can be effectively harnessed for consistent geometric\nestimation. Specifically, we 1) select geometric attributes in the global\ncoordinate system that share the same correspondence with video frames as the\nprediction targets, 2) introduce a novel and efficient conditioning method by\nreusing positional encodings, and 3) enhance performance through joint training\non multiple geometric attributes that share the same correspondence. Our\nresults achieve superior performance in predicting global geometric attributes\nin videos and can be directly applied to reconstruction tasks. Even when\ntrained solely on static video data, our approach exhibits the potential to\ngeneralize to dynamic video scenes.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24521.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6375d136dee28348a9c63cbf",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6375d136dee28348a9c63cbf/gK465HBrQWIOZ-qtHb-Vh.jpeg",
      "fullname": "zehuan-huang",
      "name": "huanngzh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 32
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.24196",
      "authors": [
        {
          "_id": "683d29da83edd521f116444c",
          "name": "Longze Chen",
          "hidden": false
        },
        {
          "_id": "683d29da83edd521f116444d",
          "name": "Renke Shan",
          "hidden": false
        },
        {
          "_id": "683d29da83edd521f116444e",
          "name": "Huiming Wang",
          "hidden": false
        },
        {
          "_id": "683d29da83edd521f116444f",
          "name": "Lu Wang",
          "hidden": false
        },
        {
          "_id": "683d29da83edd521f1164450",
          "name": "Ziqiang Liu",
          "hidden": false
        },
        {
          "_id": "683d29da83edd521f1164451",
          "name": "Run Luo",
          "hidden": false
        },
        {
          "_id": "683d29da83edd521f1164452",
          "name": "Jiawei Wang",
          "hidden": false
        },
        {
          "_id": "683d29da83edd521f1164453",
          "name": "Hamid Alinejad-Rokny",
          "hidden": false
        },
        {
          "_id": "683d29da83edd521f1164454",
          "name": "Min Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T04:15:06.000Z",
      "submittedOnDailyAt": "2025-06-02T03:23:19.536Z",
      "title": "CLaSp: In-Context Layer Skip for Self-Speculative Decoding",
      "submittedOnDailyBy": {
        "_id": "64c7b4d1c547ed5243c07b6c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c7b4d1c547ed5243c07b6c/h96CLBj6dcm01soK2UJzr.jpeg",
        "isPro": false,
        "fullname": "Longze Chen",
        "user": "lzchen2001",
        "type": "user"
      },
      "summary": "Speculative decoding (SD) is a promising method for accelerating the decoding\nprocess of Large Language Models (LLMs). The efficiency of SD primarily hinges\non the consistency between the draft model and the verify model. However,\nexisting drafting approaches typically require additional modules to be\ntrained, which can be challenging to implement and ensure compatibility across\nvarious LLMs. In this paper, we propose CLaSp, an in-context layer-skipping\nstrategy for self-speculative decoding. Unlike prior methods, CLaSp does not\nrequire additional drafting modules or extra training. Instead, it employs a\nplug-and-play mechanism by skipping intermediate layers of the verify model to\nconstruct a compressed draft model. Specifically, we develop a dynamic\nprogramming algorithm that optimizes the layer-skipping process by leveraging\nthe complete hidden states from the last verification stage as an objective.\nThis enables CLaSp to dynamically adjust its layer-skipping strategy after each\nverification stage, without relying on pre-optimized sets of skipped layers.\nExperimental results across diverse downstream tasks demonstrate that CLaSp\nachieves a speedup of 1.3x ~ 1.7x on LLaMA3 series models without altering the\noriginal distribution of the generated text.",
      "upvotes": 8,
      "discussionId": "683d29db83edd521f1164482",
      "ai_summary": "CLaSp, an in-context layer-skipping strategy for self-speculative decoding, accelerates Large Language Model decoding without additional modules or training, achieving a 1.3x to 1.7x speedup on LLaMA3 models.",
      "ai_keywords": [
        "speculative decoding",
        "large language models",
        "draft model",
        "verify model",
        "in-context layer-skipping",
        "dynamic programming algorithm",
        "hidden states",
        "verification stage"
      ]
    },
    "publishedAt": "2025-05-30T00:15:06.000Z",
    "title": "CLaSp: In-Context Layer Skip for Self-Speculative Decoding",
    "summary": "Speculative decoding (SD) is a promising method for accelerating the decoding\nprocess of Large Language Models (LLMs). The efficiency of SD primarily hinges\non the consistency between the draft model and the verify model. However,\nexisting drafting approaches typically require additional modules to be\ntrained, which can be challenging to implement and ensure compatibility across\nvarious LLMs. In this paper, we propose CLaSp, an in-context layer-skipping\nstrategy for self-speculative decoding. Unlike prior methods, CLaSp does not\nrequire additional drafting modules or extra training. Instead, it employs a\nplug-and-play mechanism by skipping intermediate layers of the verify model to\nconstruct a compressed draft model. Specifically, we develop a dynamic\nprogramming algorithm that optimizes the layer-skipping process by leveraging\nthe complete hidden states from the last verification stage as an objective.\nThis enables CLaSp to dynamically adjust its layer-skipping strategy after each\nverification stage, without relying on pre-optimized sets of skipped layers.\nExperimental results across diverse downstream tasks demonstrate that CLaSp\nachieves a speedup of 1.3x ~ 1.7x on LLaMA3 series models without altering the\noriginal distribution of the generated text.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24196.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "64c7b4d1c547ed5243c07b6c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c7b4d1c547ed5243c07b6c/h96CLBj6dcm01soK2UJzr.jpeg",
      "fullname": "Longze Chen",
      "name": "lzchen2001",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.23941",
      "authors": [
        {
          "_id": "683cf4405810d395f0a3788b",
          "name": "An Vo",
          "hidden": false
        },
        {
          "_id": "683cf4405810d395f0a3788c",
          "name": "Khai-Nguyen Nguyen",
          "hidden": false
        },
        {
          "_id": "683cf4405810d395f0a3788d",
          "user": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "name": "Mohammad Reza Taesiri",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-06-02T00:52:37.933Z",
          "hidden": false
        },
        {
          "_id": "683cf4405810d395f0a3788e",
          "name": "Vy Tuong Dang",
          "hidden": false
        },
        {
          "_id": "683cf4405810d395f0a3788f",
          "user": {
            "_id": "653194a4c8da3465f4701ad1",
            "avatarUrl": "/avatars/6682164fcaf1d339ce9ac82ba131af5e.svg",
            "isPro": true,
            "fullname": "Khai-Nguyen Nguyen",
            "user": "knguyennguyen",
            "type": "user"
          },
          "name": "Anh Totti Nguyen",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-02T00:45:56.803Z",
          "hidden": false
        },
        {
          "_id": "683cf4405810d395f0a37890",
          "name": "Daeyoung Kim",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T18:47:58.000Z",
      "submittedOnDailyAt": "2025-06-02T03:28:19.444Z",
      "title": "Vision Language Models are Biased",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Large language models (LLMs) memorize a vast amount of prior knowledge from\nthe Internet that help them on downstream tasks but also may notoriously sway\ntheir outputs towards wrong or biased answers. In this work, we test how the\nknowledge about popular subjects hurt the accuracy of vision language models\n(VLMs) on standard, objective visual tasks of counting and identification. We\nfind that state-of-the-art VLMs are strongly biased (e.g, unable to recognize a\nfourth stripe has been added to a 3-stripe Adidas logo) scoring an average of\n17.05% accuracy in counting (e.g., counting stripes in an Adidas-like logo)\nacross 7 diverse domains from animals, logos, chess, board games, optical\nillusions, to patterned grids. Insert text (e.g., \"Adidas\") describing the\nsubject name into the counterfactual image further decreases VLM accuracy. The\nbiases in VLMs are so strong that instructing them to double-check their\nresults or rely exclusively on image details to answer improves counting\naccuracy by only +2 points, on average. Our work presents an interesting\nfailure mode in VLMs and an automated framework for testing VLM biases. Code\nand data are available at: vlmsarebiased.github.io.",
      "upvotes": 8,
      "discussionId": "683cf4445810d395f0a37983",
      "projectPage": "https://vlmsarebiased.github.io/",
      "githubRepo": "https://github.com/anvo25/vlms-are-biased",
      "ai_summary": "Vision language models exhibit strong biases in counting and identification tasks, demonstrating a failure mode that persist even with additional instructions or context.",
      "ai_keywords": [
        "large language models",
        "vision language models",
        "downstream tasks",
        "popular subjects",
        "accuracy",
        "visual tasks",
        "counting",
        "identification",
        "biases",
        "counterfactual image",
        "automated framework"
      ]
    },
    "publishedAt": "2025-05-29T14:47:58.000Z",
    "title": "Vision Language Models are Biased",
    "summary": "Large language models (LLMs) memorize a vast amount of prior knowledge from\nthe Internet that help them on downstream tasks but also may notoriously sway\ntheir outputs towards wrong or biased answers. In this work, we test how the\nknowledge about popular subjects hurt the accuracy of vision language models\n(VLMs) on standard, objective visual tasks of counting and identification. We\nfind that state-of-the-art VLMs are strongly biased (e.g, unable to recognize a\nfourth stripe has been added to a 3-stripe Adidas logo) scoring an average of\n17.05% accuracy in counting (e.g., counting stripes in an Adidas-like logo)\nacross 7 diverse domains from animals, logos, chess, board games, optical\nillusions, to patterned grids. Insert text (e.g., \"Adidas\") describing the\nsubject name into the counterfactual image further decreases VLM accuracy. The\nbiases in VLMs are so strong that instructing them to double-check their\nresults or rely exclusively on image details to answer improves counting\naccuracy by only +2 points, on average. Our work presents an interesting\nfailure mode in VLMs and an automated framework for testing VLM biases. Code\nand data are available at: vlmsarebiased.github.io.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23941.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 84
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.24858",
      "authors": [
        {
          "_id": "683d2a3651706d12b2cc8ace",
          "name": "Gabrielle Kaili-May Liu",
          "hidden": false
        },
        {
          "_id": "683d2a3651706d12b2cc8acf",
          "name": "Gal Yona",
          "hidden": false
        },
        {
          "_id": "683d2a3651706d12b2cc8ad0",
          "name": "Avi Caciularu",
          "hidden": false
        },
        {
          "_id": "683d2a3651706d12b2cc8ad1",
          "name": "Idan Szpektor",
          "hidden": false
        },
        {
          "_id": "683d2a3651706d12b2cc8ad2",
          "name": "Tim G. J. Rudner",
          "hidden": false
        },
        {
          "_id": "683d2a3651706d12b2cc8ad3",
          "name": "Arman Cohan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T17:54:08.000Z",
      "submittedOnDailyAt": "2025-06-02T03:13:37.735Z",
      "title": "MetaFaith: Faithful Natural Language Uncertainty Expression in LLMs",
      "submittedOnDailyBy": {
        "_id": "64f1ca1d5b8a6a5d39d75771",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f1ca1d5b8a6a5d39d75771/Caq_Ahp7Qkm1nHyhBcztE.jpeg",
        "isPro": false,
        "fullname": "John Chih Liu",
        "user": "johncliu",
        "type": "user"
      },
      "summary": "A critical component in the trustworthiness of LLMs is reliable uncertainty\ncommunication, yet LLMs often use assertive language when conveying false\nclaims, leading to over-reliance and eroded trust. We present the first\nsystematic study of faithful confidence calibration of LLMs,\nbenchmarking models' ability to use linguistic expressions of uncertainty that\nfaithfully reflect their intrinsic uncertainty, across a\ncomprehensive array of models, datasets, and prompting strategies. Our results\ndemonstrate that LLMs largely fail at this task, and that existing\ninterventions are insufficient: standard prompt approaches provide only\nmarginal gains, and existing, factuality-based calibration techniques can even\nharm faithful calibration. To address this critical gap, we introduce\nMetaFaith, a novel prompt-based calibration approach inspired by human\nmetacognition. We show that MetaFaith robustly improves faithful calibration\nacross diverse models and task domains, enabling up to 61% improvement in\nfaithfulness and achieving an 83% win rate over original generations as judged\nby humans.",
      "upvotes": 6,
      "discussionId": "683d2a3751706d12b2cc8b0a",
      "ai_summary": "A study reveals that Large Language Models (LLMs) struggle with expressing uncertainty accurately and introduces MetaFaith, a prompt-based method that enhances their calibration significantly.",
      "ai_keywords": [
        "faithful confidence calibration",
        "linguistic expressions of uncertainty",
        "intrinsic uncertainty",
        "prompting strategies",
        "metacognition"
      ]
    },
    "publishedAt": "2025-05-30T13:54:08.000Z",
    "title": "MetaFaith: Faithful Natural Language Uncertainty Expression in LLMs",
    "summary": "A critical component in the trustworthiness of LLMs is reliable uncertainty\ncommunication, yet LLMs often use assertive language when conveying false\nclaims, leading to over-reliance and eroded trust. We present the first\nsystematic study of faithful confidence calibration of LLMs,\nbenchmarking models' ability to use linguistic expressions of uncertainty that\nfaithfully reflect their intrinsic uncertainty, across a\ncomprehensive array of models, datasets, and prompting strategies. Our results\ndemonstrate that LLMs largely fail at this task, and that existing\ninterventions are insufficient: standard prompt approaches provide only\nmarginal gains, and existing, factuality-based calibration techniques can even\nharm faithful calibration. To address this critical gap, we introduce\nMetaFaith, a novel prompt-based calibration approach inspired by human\nmetacognition. We show that MetaFaith robustly improves faithful calibration\nacross diverse models and task domains, enabling up to 61% improvement in\nfaithfulness and achieving an 83% win rate over original generations as judged\nby humans.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24858.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64f1ca1d5b8a6a5d39d75771",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f1ca1d5b8a6a5d39d75771/Caq_Ahp7Qkm1nHyhBcztE.jpeg",
      "fullname": "John Chih Liu",
      "name": "johncliu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.24850",
      "authors": [
        {
          "_id": "683d0ffbe41c42faceda19b2",
          "user": {
            "_id": "6587e5a4b2177de3967ff434",
            "avatarUrl": "/avatars/f2dfbc44eb2bff8d8d66d26db8539708.svg",
            "isPro": false,
            "fullname": "Shuyao Xu",
            "user": "Tim-Xu",
            "type": "user"
          },
          "name": "Shuyao Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:40:56.229Z",
          "hidden": false
        },
        {
          "_id": "683d0ffbe41c42faceda19b3",
          "name": "Cheng Peng",
          "hidden": false
        },
        {
          "_id": "683d0ffbe41c42faceda19b4",
          "name": "Jiangxuan Long",
          "hidden": false
        },
        {
          "_id": "683d0ffbe41c42faceda19b5",
          "name": "Weidi Xu",
          "hidden": false
        },
        {
          "_id": "683d0ffbe41c42faceda19b6",
          "name": "Wei Chu",
          "hidden": false
        },
        {
          "_id": "683d0ffbe41c42faceda19b7",
          "name": "Yuan Qi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T17:47:17.000Z",
      "submittedOnDailyAt": "2025-06-02T02:07:38.924Z",
      "title": "Harnessing Negative Signals: Reinforcement Distillation from Teacher\n  Data for LLM Reasoning",
      "submittedOnDailyBy": {
        "_id": "66e83ec5deb449d8d856e78d",
        "avatarUrl": "/avatars/c5e56be65fcacb3192ce10ba6d8f48e2.svg",
        "isPro": false,
        "fullname": "Tongyan Hu",
        "user": "entropyhu",
        "type": "user"
      },
      "summary": "Recent advances in model distillation demonstrate that data from advanced\nreasoning models (e.g., DeepSeek-R1, OpenAI's o1) can effectively transfer\ncomplex reasoning abilities to smaller, efficient student models. However,\nstandard practices employ rejection sampling, discarding incorrect reasoning\nexamples -- valuable, yet often underutilized data. This paper addresses the\ncritical question: How can both positive and negative distilled reasoning\ntraces be effectively leveraged to maximize LLM reasoning performance in an\noffline setting? To this end, We propose Reinforcement Distillation (REDI), a\ntwo-stage framework. Stage 1 learns from positive traces via Supervised\nFine-Tuning (SFT). Stage 2 further refines the model using both positive and\nnegative traces through our proposed REDI objective. This novel objective is a\nsimple, reference-free loss function that outperforms established methods like\nDPO and SimPO in this distillation context. Our empirical evaluations\ndemonstrate REDI's superiority over baseline Rejection Sampling SFT or SFT\ncombined with DPO/SimPO on mathematical reasoning tasks. Notably, the\nQwen-REDI-1.5B model, post-trained on just 131k positive and negative examples\nfrom the open Open-R1 dataset, achieves an 83.1% score on MATH-500 (pass@1).\nIts performance matches or surpasses that of DeepSeek-R1-Distill-Qwen-1.5B (a\nmodel post-trained on 800k proprietary data) across various mathematical\nreasoning benchmarks, establishing a new state-of-the-art for 1.5B models\npost-trained offline with openly available data.",
      "upvotes": 5,
      "discussionId": "683d0ffce41c42faceda19da",
      "githubRepo": "https://github.com/Tim-Siu/reinforcement-distillation",
      "ai_summary": "Reinforcement Distillation (REDI) leverages both positive and negative traces to enhance large language model reasoning performance offline, outperforming traditional methods and achieving state-of-the-art results with limited open data.",
      "ai_keywords": [
        "model distillation",
        "DeepSeek-R1",
        "OpenAI's o1",
        "Reinforcement Distillation (REDI)",
        "Supervised Fine-Tuning (SFT)",
        "REDI objective",
        "DPO",
        "SimPO",
        "mathematical reasoning tasks",
        "MATH-500",
        "Qwen-REDI-1.5B",
        "DeepSeek-R1-Distill-Qwen-1.5B"
      ]
    },
    "publishedAt": "2025-05-30T13:47:17.000Z",
    "title": "Harnessing Negative Signals: Reinforcement Distillation from Teacher\n  Data for LLM Reasoning",
    "summary": "Recent advances in model distillation demonstrate that data from advanced\nreasoning models (e.g., DeepSeek-R1, OpenAI's o1) can effectively transfer\ncomplex reasoning abilities to smaller, efficient student models. However,\nstandard practices employ rejection sampling, discarding incorrect reasoning\nexamples -- valuable, yet often underutilized data. This paper addresses the\ncritical question: How can both positive and negative distilled reasoning\ntraces be effectively leveraged to maximize LLM reasoning performance in an\noffline setting? To this end, We propose Reinforcement Distillation (REDI), a\ntwo-stage framework. Stage 1 learns from positive traces via Supervised\nFine-Tuning (SFT). Stage 2 further refines the model using both positive and\nnegative traces through our proposed REDI objective. This novel objective is a\nsimple, reference-free loss function that outperforms established methods like\nDPO and SimPO in this distillation context. Our empirical evaluations\ndemonstrate REDI's superiority over baseline Rejection Sampling SFT or SFT\ncombined with DPO/SimPO on mathematical reasoning tasks. Notably, the\nQwen-REDI-1.5B model, post-trained on just 131k positive and negative examples\nfrom the open Open-R1 dataset, achieves an 83.1% score on MATH-500 (pass@1).\nIts performance matches or surpasses that of DeepSeek-R1-Distill-Qwen-1.5B (a\nmodel post-trained on 800k proprietary data) across various mathematical\nreasoning benchmarks, establishing a new state-of-the-art for 1.5B models\npost-trained offline with openly available data.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24850.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "66e83ec5deb449d8d856e78d",
      "avatarUrl": "/avatars/c5e56be65fcacb3192ce10ba6d8f48e2.svg",
      "fullname": "Tongyan Hu",
      "name": "entropyhu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.24417",
      "authors": [
        {
          "_id": "683d0b6c5810d395f0a9a49e",
          "name": "Runnan Lu",
          "hidden": false
        },
        {
          "_id": "683d0b6c5810d395f0a9a49f",
          "name": "Yuxuan Zhang",
          "hidden": false
        },
        {
          "_id": "683d0b6c5810d395f0a9a4a0",
          "name": "Jailing Liu",
          "hidden": false
        },
        {
          "_id": "683d0b6c5810d395f0a9a4a1",
          "name": "Haifa Wang",
          "hidden": false
        },
        {
          "_id": "683d0b6c5810d395f0a9a4a2",
          "name": "Yiren Song",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T09:55:39.000Z",
      "submittedOnDailyAt": "2025-06-02T00:55:24.254Z",
      "title": "EasyText: Controllable Diffusion Transformer for Multilingual Text\n  Rendering",
      "submittedOnDailyBy": {
        "_id": "64311a95034ecbefddd141ef",
        "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
        "isPro": true,
        "fullname": "Yiren Song",
        "user": "yiren98",
        "type": "user"
      },
      "summary": "Generating accurate multilingual text with diffusion models has long been\ndesired but remains challenging. Recent methods have made progress in rendering\ntext in a single language, but rendering arbitrary languages is still an\nunexplored area. This paper introduces EasyText, a text rendering framework\nbased on DiT (Diffusion Transformer), which connects denoising latents with\nmultilingual character tokens encoded as character tokens. We propose character\npositioning encoding and position encoding interpolation techniques to achieve\ncontrollable and precise text rendering. Additionally, we construct a\nlarge-scale synthetic text image dataset with 1 million multilingual image-text\nannotations as well as a high-quality dataset of 20K annotated images, which\nare used for pretraining and fine-tuning respectively. Extensive experiments\nand evaluations demonstrate the effectiveness and advancement of our approach\nin multilingual text rendering, visual quality, and layout-aware text\nintegration.",
      "upvotes": 5,
      "discussionId": "683d0b6f5810d395f0a9a57b",
      "ai_summary": "The paper presents EasyText, a multilingual text rendering framework using DiT that enhances rendering precision and visual quality with large datasets.",
      "ai_keywords": [
        "DiT (Diffusion Transformer)",
        "denoising latents",
        "multilingual character tokens",
        "character positioning encoding",
        "position encoding interpolation",
        "synthetic text image dataset",
        "pretraining",
        "fine-tuning",
        "multilingual text rendering",
        "visual quality",
        "layout-aware text integration"
      ]
    },
    "publishedAt": "2025-05-30T05:55:39.000Z",
    "title": "EasyText: Controllable Diffusion Transformer for Multilingual Text\n  Rendering",
    "summary": "Generating accurate multilingual text with diffusion models has long been\ndesired but remains challenging. Recent methods have made progress in rendering\ntext in a single language, but rendering arbitrary languages is still an\nunexplored area. This paper introduces EasyText, a text rendering framework\nbased on DiT (Diffusion Transformer), which connects denoising latents with\nmultilingual character tokens encoded as character tokens. We propose character\npositioning encoding and position encoding interpolation techniques to achieve\ncontrollable and precise text rendering. Additionally, we construct a\nlarge-scale synthetic text image dataset with 1 million multilingual image-text\nannotations as well as a high-quality dataset of 20K annotated images, which\nare used for pretraining and fine-tuning respectively. Extensive experiments\nand evaluations demonstrate the effectiveness and advancement of our approach\nin multilingual text rendering, visual quality, and layout-aware text\nintegration.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24417.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64311a95034ecbefddd141ef",
      "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
      "fullname": "Yiren Song",
      "name": "yiren98",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 17
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.23926",
      "authors": [
        {
          "_id": "683d33be277ad05e5a672f79",
          "name": "Xuweiyi Chen",
          "hidden": false
        },
        {
          "_id": "683d33be277ad05e5a672f7a",
          "name": "Wentao Zhou",
          "hidden": false
        },
        {
          "_id": "683d33be277ad05e5a672f7b",
          "name": "Aruni RoyChowdhury",
          "hidden": false
        },
        {
          "_id": "683d33be277ad05e5a672f7c",
          "name": "Zezhou Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T18:21:47.000Z",
      "submittedOnDailyAt": "2025-06-02T03:49:08.677Z",
      "title": "Point-MoE: Towards Cross-Domain Generalization in 3D Semantic\n  Segmentation via Mixture-of-Experts",
      "submittedOnDailyBy": {
        "_id": "634632aaac1cb29fb2ac9f14",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634632aaac1cb29fb2ac9f14/nGZ2TzKOOcKMAR_NFYKkR.jpeg",
        "isPro": false,
        "fullname": "Xuweiyi Chen",
        "user": "Xuweiyi",
        "type": "user"
      },
      "summary": "While scaling laws have transformed natural language processing and computer\nvision, 3D point cloud understanding has yet to reach that stage. This can be\nattributed to both the comparatively smaller scale of 3D datasets, as well as\nthe disparate sources of the data itself. Point clouds are captured by diverse\nsensors (e.g., depth cameras, LiDAR) across varied domains (e.g., indoor,\noutdoor), each introducing unique scanning patterns, sampling densities, and\nsemantic biases. Such domain heterogeneity poses a major barrier towards\ntraining unified models at scale, especially under the realistic constraint\nthat domain labels are typically inaccessible at inference time. In this work,\nwe propose Point-MoE, a Mixture-of-Experts architecture designed to enable\nlarge-scale, cross-domain generalization in 3D perception. We show that\nstandard point cloud backbones degrade significantly in performance when\ntrained on mixed-domain data, whereas Point-MoE with a simple top-k routing\nstrategy can automatically specialize experts, even without access to domain\nlabels. Our experiments demonstrate that Point-MoE not only outperforms strong\nmulti-domain baselines but also generalizes better to unseen domains. This work\nhighlights a scalable path forward for 3D understanding: letting the model\ndiscover structure in diverse 3D data, rather than imposing it via manual\ncuration or domain supervision.",
      "upvotes": 3,
      "discussionId": "683d33c4277ad05e5a67310e",
      "ai_summary": "Point-MoE, a Mixture-of-Experts architecture, enables large-scale, cross-domain generalization in 3D perception by automatically specializing experts without domain labels.",
      "ai_keywords": [
        "Mixture-of-Experts",
        "Point-MoE",
        "point cloud backbones",
        "3D perception",
        "domain heterogeneity",
        "domain labels",
        "top-k routing",
        "multi-domain baselines"
      ]
    },
    "publishedAt": "2025-05-29T14:21:47.000Z",
    "title": "Point-MoE: Towards Cross-Domain Generalization in 3D Semantic\n  Segmentation via Mixture-of-Experts",
    "summary": "While scaling laws have transformed natural language processing and computer\nvision, 3D point cloud understanding has yet to reach that stage. This can be\nattributed to both the comparatively smaller scale of 3D datasets, as well as\nthe disparate sources of the data itself. Point clouds are captured by diverse\nsensors (e.g., depth cameras, LiDAR) across varied domains (e.g., indoor,\noutdoor), each introducing unique scanning patterns, sampling densities, and\nsemantic biases. Such domain heterogeneity poses a major barrier towards\ntraining unified models at scale, especially under the realistic constraint\nthat domain labels are typically inaccessible at inference time. In this work,\nwe propose Point-MoE, a Mixture-of-Experts architecture designed to enable\nlarge-scale, cross-domain generalization in 3D perception. We show that\nstandard point cloud backbones degrade significantly in performance when\ntrained on mixed-domain data, whereas Point-MoE with a simple top-k routing\nstrategy can automatically specialize experts, even without access to domain\nlabels. Our experiments demonstrate that Point-MoE not only outperforms strong\nmulti-domain baselines but also generalizes better to unseen domains. This work\nhighlights a scalable path forward for 3D understanding: letting the model\ndiscover structure in diverse 3D data, rather than imposing it via manual\ncuration or domain supervision.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23926.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "634632aaac1cb29fb2ac9f14",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634632aaac1cb29fb2ac9f14/nGZ2TzKOOcKMAR_NFYKkR.jpeg",
      "fullname": "Xuweiyi Chen",
      "name": "Xuweiyi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.24862",
      "authors": [
        {
          "_id": "683d54f364b44c0ccabb9e65",
          "name": "Cailin Zhuang",
          "hidden": false
        },
        {
          "_id": "683d54f364b44c0ccabb9e66",
          "name": "Ailin Huang",
          "hidden": false
        },
        {
          "_id": "683d54f364b44c0ccabb9e67",
          "user": {
            "_id": "64b914c8ace99c0723ad83a9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/B4gxNByeVY_xaOcjwiN1j.jpeg",
            "isPro": false,
            "fullname": "Wei Cheng",
            "user": "wchengad",
            "type": "user"
          },
          "name": "Wei Cheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:40:49.393Z",
          "hidden": false
        },
        {
          "_id": "683d54f364b44c0ccabb9e68",
          "name": "Jingwei Wu",
          "hidden": false
        },
        {
          "_id": "683d54f364b44c0ccabb9e69",
          "name": "Yaoqi Hu",
          "hidden": false
        },
        {
          "_id": "683d54f364b44c0ccabb9e6a",
          "name": "Jiaqi Liao",
          "hidden": false
        },
        {
          "_id": "683d54f364b44c0ccabb9e6b",
          "name": "Zhewei Huang",
          "hidden": false
        },
        {
          "_id": "683d54f364b44c0ccabb9e6c",
          "name": "Hongyuan Wang",
          "hidden": false
        },
        {
          "_id": "683d54f364b44c0ccabb9e6d",
          "name": "Xinyao Liao",
          "hidden": false
        },
        {
          "_id": "683d54f364b44c0ccabb9e6e",
          "name": "Weiwei Cai",
          "hidden": false
        },
        {
          "_id": "683d54f364b44c0ccabb9e6f",
          "name": "Hengyuan Xu",
          "hidden": false
        },
        {
          "_id": "683d54f364b44c0ccabb9e70",
          "name": "Xuanyang Zhang",
          "hidden": false
        },
        {
          "_id": "683d54f364b44c0ccabb9e71",
          "name": "Xianfang Zeng",
          "hidden": false
        },
        {
          "_id": "683d54f364b44c0ccabb9e72",
          "name": "Gang Yu",
          "hidden": false
        },
        {
          "_id": "683d54f364b44c0ccabb9e73",
          "name": "Chi Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64b914c8ace99c0723ad83a9/ZHmu_F8c4mbhiPtomY6H7.mp4"
      ],
      "publishedAt": "2025-05-30T17:58:21.000Z",
      "submittedOnDailyAt": "2025-06-02T06:09:52.296Z",
      "title": "ViStoryBench: Comprehensive Benchmark Suite for Story Visualization",
      "submittedOnDailyBy": {
        "_id": "64b914c8ace99c0723ad83a9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/B4gxNByeVY_xaOcjwiN1j.jpeg",
        "isPro": false,
        "fullname": "Wei Cheng",
        "user": "wchengad",
        "type": "user"
      },
      "summary": "Story visualization, which aims to generate a sequence of visually coherent\nimages aligning with a given narrative and reference images, has seen\nsignificant progress with recent advancements in generative models. To further\nenhance the performance of story visualization frameworks in real-world\nscenarios, we introduce a comprehensive evaluation benchmark, ViStoryBench. We\ncollect a diverse dataset encompassing various story types and artistic styles,\nensuring models are evaluated across multiple dimensions such as different\nplots (e.g., comedy, horror) and visual aesthetics (e.g., anime, 3D\nrenderings). ViStoryBench is carefully curated to balance narrative structures\nand visual elements, featuring stories with single and multiple protagonists to\ntest models' ability to maintain character consistency. Additionally, it\nincludes complex plots and intricate world-building to challenge models in\ngenerating accurate visuals. To ensure comprehensive comparisons, our benchmark\nincorporates a wide range of evaluation metrics assessing critical aspects.\nThis structured and multifaceted framework enables researchers to thoroughly\nidentify both the strengths and weaknesses of different models, fostering\ntargeted improvements.",
      "upvotes": 2,
      "discussionId": "683d54f764b44c0ccabb9f60",
      "projectPage": "https://vistorybench.github.io/",
      "githubRepo": "https://github.com/vistorybench/vistorybench"
    },
    "publishedAt": "2025-05-30T13:58:21.000Z",
    "title": "ViStoryBench: Comprehensive Benchmark Suite for Story Visualization",
    "summary": "Story visualization, which aims to generate a sequence of visually coherent\nimages aligning with a given narrative and reference images, has seen\nsignificant progress with recent advancements in generative models. To further\nenhance the performance of story visualization frameworks in real-world\nscenarios, we introduce a comprehensive evaluation benchmark, ViStoryBench. We\ncollect a diverse dataset encompassing various story types and artistic styles,\nensuring models are evaluated across multiple dimensions such as different\nplots (e.g., comedy, horror) and visual aesthetics (e.g., anime, 3D\nrenderings). ViStoryBench is carefully curated to balance narrative structures\nand visual elements, featuring stories with single and multiple protagonists to\ntest models' ability to maintain character consistency. Additionally, it\nincludes complex plots and intricate world-building to challenge models in\ngenerating accurate visuals. To ensure comprehensive comparisons, our benchmark\nincorporates a wide range of evaluation metrics assessing critical aspects.\nThis structured and multifaceted framework enables researchers to thoroughly\nidentify both the strengths and weaknesses of different models, fostering\ntargeted improvements.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64b914c8ace99c0723ad83a9/ZHmu_F8c4mbhiPtomY6H7.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24862.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b914c8ace99c0723ad83a9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/B4gxNByeVY_xaOcjwiN1j.jpeg",
      "fullname": "Wei Cheng",
      "name": "wchengad",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.24615",
      "authors": [
        {
          "_id": "683d4295c31058e5bf2e2b0b",
          "name": "Yan Liu",
          "hidden": false
        },
        {
          "_id": "683d4295c31058e5bf2e2b0c",
          "user": {
            "_id": "646a11791556443f24b582e9",
            "avatarUrl": "/avatars/b54d416ddca2f124492ba51bc4b95fd2.svg",
            "isPro": false,
            "fullname": "Zonglin Yang",
            "user": "ZonglinY",
            "type": "user"
          },
          "name": "Zonglin Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:40:15.714Z",
          "hidden": false
        },
        {
          "_id": "683d4295c31058e5bf2e2b0d",
          "name": "Soujanya Poria",
          "hidden": false
        },
        {
          "_id": "683d4295c31058e5bf2e2b0e",
          "name": "Thanh-Son Nguyen",
          "hidden": false
        },
        {
          "_id": "683d4295c31058e5bf2e2b0f",
          "name": "Erik Cambria",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T14:08:13.000Z",
      "submittedOnDailyAt": "2025-06-02T04:51:23.329Z",
      "title": "Harnessing Large Language Models for Scientific Novelty Detection",
      "submittedOnDailyBy": {
        "_id": "646a11791556443f24b582e9",
        "avatarUrl": "/avatars/b54d416ddca2f124492ba51bc4b95fd2.svg",
        "isPro": false,
        "fullname": "Zonglin Yang",
        "user": "ZonglinY",
        "type": "user"
      },
      "summary": "In an era of exponential scientific growth, identifying novel research ideas\nis crucial and challenging in academia. Despite potential, the lack of an\nappropriate benchmark dataset hinders the research of novelty detection. More\nimportantly, simply adopting existing NLP technologies, e.g., retrieving and\nthen cross-checking, is not a one-size-fits-all solution due to the gap between\ntextual similarity and idea conception. In this paper, we propose to harness\nlarge language models (LLMs) for scientific novelty detection (ND), associated\nwith two new datasets in marketing and NLP domains. To construct the\nconsiderate datasets for ND, we propose to extract closure sets of papers based\non their relationship, and then summarize their main ideas based on LLMs. To\ncapture idea conception, we propose to train a lightweight retriever by\ndistilling the idea-level knowledge from LLMs to align ideas with similar\nconception, enabling efficient and accurate idea retrieval for LLM novelty\ndetection. Experiments show our method consistently outperforms others on the\nproposed benchmark datasets for idea retrieval and ND tasks. Codes and data are\navailable at https://anonymous.4open.science/r/NoveltyDetection-10FB/.",
      "upvotes": 2,
      "discussionId": "683d4296c31058e5bf2e2b63",
      "ai_summary": "A method utilizing large language models to detect scientific novelty by distilling idea-level knowledge and constructing specialized datasets in marketing and NLP domains.",
      "ai_keywords": [
        "large language models",
        "scientific novelty detection",
        "closure sets",
        "idea retrieval",
        "idea conception",
        "lightweight retriever",
        "knowledge distillation"
      ]
    },
    "publishedAt": "2025-05-30T10:08:13.000Z",
    "title": "Harnessing Large Language Models for Scientific Novelty Detection",
    "summary": "In an era of exponential scientific growth, identifying novel research ideas\nis crucial and challenging in academia. Despite potential, the lack of an\nappropriate benchmark dataset hinders the research of novelty detection. More\nimportantly, simply adopting existing NLP technologies, e.g., retrieving and\nthen cross-checking, is not a one-size-fits-all solution due to the gap between\ntextual similarity and idea conception. In this paper, we propose to harness\nlarge language models (LLMs) for scientific novelty detection (ND), associated\nwith two new datasets in marketing and NLP domains. To construct the\nconsiderate datasets for ND, we propose to extract closure sets of papers based\non their relationship, and then summarize their main ideas based on LLMs. To\ncapture idea conception, we propose to train a lightweight retriever by\ndistilling the idea-level knowledge from LLMs to align ideas with similar\nconception, enabling efficient and accurate idea retrieval for LLM novelty\ndetection. Experiments show our method consistently outperforms others on the\nproposed benchmark datasets for idea retrieval and ND tasks. Codes and data are\navailable at https://anonymous.4open.science/r/NoveltyDetection-10FB/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24615.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "646a11791556443f24b582e9",
      "avatarUrl": "/avatars/b54d416ddca2f124492ba51bc4b95fd2.svg",
      "fullname": "Zonglin Yang",
      "name": "ZonglinY",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.24517",
      "authors": [
        {
          "_id": "683d3f3100c71614babecb8c",
          "user": {
            "_id": "64395702bb7ded0a0fee8889",
            "avatarUrl": "/avatars/afe8f9b6de358497b0db8a03f8a3a704.svg",
            "isPro": false,
            "fullname": "Yinqi Li",
            "user": "yinqi",
            "type": "user"
          },
          "name": "Yinqi Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:40:19.037Z",
          "hidden": false
        },
        {
          "_id": "683d3f3100c71614babecb8d",
          "name": "Jiahe Zhao",
          "hidden": false
        },
        {
          "_id": "683d3f3100c71614babecb8e",
          "name": "Hong Chang",
          "hidden": false
        },
        {
          "_id": "683d3f3100c71614babecb8f",
          "name": "Ruibing Hou",
          "hidden": false
        },
        {
          "_id": "683d3f3100c71614babecb90",
          "name": "Shiguang Shan",
          "hidden": false
        },
        {
          "_id": "683d3f3100c71614babecb91",
          "name": "Xilin Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T12:29:38.000Z",
      "submittedOnDailyAt": "2025-06-02T04:55:28.985Z",
      "title": "un^2CLIP: Improving CLIP's Visual Detail Capturing Ability via\n  Inverting unCLIP",
      "submittedOnDailyBy": {
        "_id": "64395702bb7ded0a0fee8889",
        "avatarUrl": "/avatars/afe8f9b6de358497b0db8a03f8a3a704.svg",
        "isPro": false,
        "fullname": "Yinqi Li",
        "user": "yinqi",
        "type": "user"
      },
      "summary": "Contrastive Language-Image Pre-training (CLIP) has become a foundation model\nand has been applied to various vision and multimodal tasks. However, recent\nworks indicate that CLIP falls short in distinguishing detailed differences in\nimages and shows suboptimal performance on dense-prediction and vision-centric\nmultimodal tasks. Therefore, this work focuses on improving existing CLIP\nmodels, aiming to capture as many visual details in images as possible. We find\nthat a specific type of generative models, unCLIP, provides a suitable\nframework for achieving our goal. Specifically, unCLIP trains an image\ngenerator conditioned on the CLIP image embedding. In other words, it inverts\nthe CLIP image encoder. Compared to discriminative models like CLIP, generative\nmodels are better at capturing image details because they are trained to learn\nthe data distribution of images. Additionally, the conditional input space of\nunCLIP aligns with CLIP's original image-text embedding space. Therefore, we\npropose to invert unCLIP (dubbed un^2CLIP) to improve the CLIP model. In this\nway, the improved image encoder can gain unCLIP's visual detail capturing\nability while preserving its alignment with the original text encoder\nsimultaneously. We evaluate our improved CLIP across various tasks to which\nCLIP has been applied, including the challenging MMVP-VLM benchmark, the\ndense-prediction open-vocabulary segmentation task, and multimodal large\nlanguage model tasks. Experiments show that un^2CLIP significantly improves\nthe original CLIP and previous CLIP improvement methods. Code and models will\nbe available at https://github.com/LiYinqi/un2CLIP.",
      "upvotes": 2,
      "discussionId": "683d3f3200c71614babecbe3",
      "githubRepo": "https://github.com/LiYinqi/un2CLIP",
      "ai_summary": "A generative model framework, unCLIP, is inverted to improve CLIP's ability to capture detailed visual information while maintaining text alignment.",
      "ai_keywords": [
        "Contrastive Language-Image Pre-training",
        "CLIP",
        "unCLIP",
        "image generator",
        "image encoding",
        "data distribution",
        "dense-prediction",
        "vision-centric",
        "multimodal",
        "open-vocabulary segmentation",
        "multimodal large language model",
        "MMVP-VLM benchmark"
      ]
    },
    "publishedAt": "2025-05-30T08:29:38.000Z",
    "title": "un^2CLIP: Improving CLIP's Visual Detail Capturing Ability via\n  Inverting unCLIP",
    "summary": "Contrastive Language-Image Pre-training (CLIP) has become a foundation model\nand has been applied to various vision and multimodal tasks. However, recent\nworks indicate that CLIP falls short in distinguishing detailed differences in\nimages and shows suboptimal performance on dense-prediction and vision-centric\nmultimodal tasks. Therefore, this work focuses on improving existing CLIP\nmodels, aiming to capture as many visual details in images as possible. We find\nthat a specific type of generative models, unCLIP, provides a suitable\nframework for achieving our goal. Specifically, unCLIP trains an image\ngenerator conditioned on the CLIP image embedding. In other words, it inverts\nthe CLIP image encoder. Compared to discriminative models like CLIP, generative\nmodels are better at capturing image details because they are trained to learn\nthe data distribution of images. Additionally, the conditional input space of\nunCLIP aligns with CLIP's original image-text embedding space. Therefore, we\npropose to invert unCLIP (dubbed un^2CLIP) to improve the CLIP model. In this\nway, the improved image encoder can gain unCLIP's visual detail capturing\nability while preserving its alignment with the original text encoder\nsimultaneously. We evaluate our improved CLIP across various tasks to which\nCLIP has been applied, including the challenging MMVP-VLM benchmark, the\ndense-prediction open-vocabulary segmentation task, and multimodal large\nlanguage model tasks. Experiments show that un^2CLIP significantly improves\nthe original CLIP and previous CLIP improvement methods. Code and models will\nbe available at https://github.com/LiYinqi/un2CLIP.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24517.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64395702bb7ded0a0fee8889",
      "avatarUrl": "/avatars/afe8f9b6de358497b0db8a03f8a3a704.svg",
      "fullname": "Yinqi Li",
      "name": "yinqi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.24293",
      "authors": [
        {
          "_id": "683d01ec446fd0c8ff323010",
          "user": {
            "_id": "6658f863ce1b283888625af3",
            "avatarUrl": "/avatars/e4b2c7df0f398eb68c0566031ceac99e.svg",
            "isPro": false,
            "fullname": "James Golden",
            "user": "jamesgolden1",
            "type": "user"
          },
          "name": "James R. Golden",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-06-02T02:38:10.635Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6658f863ce1b283888625af3/uulwAnV1EYXXSdsC-eDMW.png"
      ],
      "publishedAt": "2025-05-30T07:08:33.000Z",
      "submittedOnDailyAt": "2025-06-02T01:02:58.980Z",
      "title": "Large Language Models are Locally Linear Mappings",
      "submittedOnDailyBy": {
        "_id": "6658f863ce1b283888625af3",
        "avatarUrl": "/avatars/e4b2c7df0f398eb68c0566031ceac99e.svg",
        "isPro": false,
        "fullname": "James Golden",
        "user": "jamesgolden1",
        "type": "user"
      },
      "summary": "We demonstrate that the inference operations of several open-weight large\nlanguage models (LLMs) can be mapped to an exactly equivalent linear system for\nan input sequence without modifying the model weights or altering output\npredictions. Extending techniques from image diffusion models that exhibit\nlocal or piecewise linearity, we strategically alter the gradient computation\nwith respect to a given input sequence for a next-token prediction such that\nthe Jacobian of the model nearly exactly reproduces the forward prediction with\na linear system. We demonstrate this approach across models (Llama 3, Gemma 3,\nQwen 3, Phi 4, Mistral Ministral and OLMo 2, up to Llama 3.3 70B Q4) and show\nthrough the singular value decomposition of the detached Jacobian that these\nLLMs operate in extremely low-dimensional subspaces where many of the largest\nsingular vectors decode to concepts related to the most-likely output token.\nThis approach also allows us to examine the operation of each successive layer\n(and its attention and MLP components) as nearly-exact linear systems and\nobserve the emergence of semantic concepts. Despite their expressive power and\nglobal nonlinearity, modern LLMs can be interpreted through nearly-exact\nlocally linear decompositions that provide insights into their internal\nrepresentations and reveal interpretable semantic structures in the next-token\nprediction process.",
      "upvotes": 2,
      "discussionId": "683d01ee446fd0c8ff323087",
      "githubRepo": "https://github.com/jamesgolden1/llms-are-llms/",
      "ai_summary": "LLMs can be approximated as linear systems for inference, offering insights into their internal representations and semantic structures without altering the models or their predictions.",
      "ai_keywords": [
        "large language models (LLMs)",
        "inference operations",
        "linear system",
        "gradient computation",
        "Jacobian",
        "singular value decomposition",
        "low-dimensional subspaces",
        "semantic concepts",
        "attention components",
        "MLP components",
        "locally linear decompositions"
      ]
    },
    "publishedAt": "2025-05-30T03:08:33.000Z",
    "title": "Large Language Models are Locally Linear Mappings",
    "summary": "We demonstrate that the inference operations of several open-weight large\nlanguage models (LLMs) can be mapped to an exactly equivalent linear system for\nan input sequence without modifying the model weights or altering output\npredictions. Extending techniques from image diffusion models that exhibit\nlocal or piecewise linearity, we strategically alter the gradient computation\nwith respect to a given input sequence for a next-token prediction such that\nthe Jacobian of the model nearly exactly reproduces the forward prediction with\na linear system. We demonstrate this approach across models (Llama 3, Gemma 3,\nQwen 3, Phi 4, Mistral Ministral and OLMo 2, up to Llama 3.3 70B Q4) and show\nthrough the singular value decomposition of the detached Jacobian that these\nLLMs operate in extremely low-dimensional subspaces where many of the largest\nsingular vectors decode to concepts related to the most-likely output token.\nThis approach also allows us to examine the operation of each successive layer\n(and its attention and MLP components) as nearly-exact linear systems and\nobserve the emergence of semantic concepts. Despite their expressive power and\nglobal nonlinearity, modern LLMs can be interpreted through nearly-exact\nlocally linear decompositions that provide insights into their internal\nrepresentations and reveal interpretable semantic structures in the next-token\nprediction process.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6658f863ce1b283888625af3/uulwAnV1EYXXSdsC-eDMW.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24293.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6658f863ce1b283888625af3",
      "avatarUrl": "/avatars/e4b2c7df0f398eb68c0566031ceac99e.svg",
      "fullname": "James Golden",
      "name": "jamesgolden1",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23009",
      "authors": [
        {
          "_id": "683916c60df60182c0dee89d",
          "user": {
            "_id": "66958c29d4ca2767b9c41005",
            "avatarUrl": "/avatars/c81b65c1ad345c48c252773ea78b7607.svg",
            "isPro": true,
            "fullname": "Ruskin Raj Manku",
            "user": "ruskinmanku",
            "type": "user"
          },
          "name": "Ruskin Raj Manku",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:47:12.390Z",
          "hidden": false
        },
        {
          "_id": "683916c60df60182c0dee89e",
          "name": "Yuzhi Tang",
          "hidden": false
        },
        {
          "_id": "683916c60df60182c0dee89f",
          "name": "Xingjian Shi",
          "hidden": false
        },
        {
          "_id": "683916c60df60182c0dee8a0",
          "name": "Mu Li",
          "hidden": false
        },
        {
          "_id": "683916c60df60182c0dee8a1",
          "name": "Alex Smola",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T02:36:24.000Z",
      "submittedOnDailyAt": "2025-06-02T01:24:21.995Z",
      "title": "EmergentTTS-Eval: Evaluating TTS Models on Complex Prosodic,\n  Expressiveness, and Linguistic Challenges Using Model-as-a-Judge",
      "submittedOnDailyBy": {
        "_id": "66958c29d4ca2767b9c41005",
        "avatarUrl": "/avatars/c81b65c1ad345c48c252773ea78b7607.svg",
        "isPro": true,
        "fullname": "Ruskin Raj Manku",
        "user": "ruskinmanku",
        "type": "user"
      },
      "summary": "Text-to-Speech (TTS) benchmarks often fail to capture how well models handle\nnuanced and semantically complex text. Building on EmergentTTS, we\nintroduce EmergentTTS-Eval, a comprehensive benchmark covering six\nchallenging TTS scenarios: emotions, paralinguistics, foreign words, syntactic\ncomplexity, complex pronunciation (e.g. URLs, formulas), and questions.\nCrucially, our framework automates both test-case generation and evaluation,\nmaking the benchmark easily extensible. Starting from a small set of\nhuman-written seed prompts, we iteratively extend them using LLMs to target\nspecific structural, phonetic and prosodic challenges, resulting in 1,645\ndiverse test cases. Moreover, we employ a model-as-a-judge approach, using a\nLarge Audio Language Model (LALM) to assess the speech across multiple\ndimensions such as expressed emotion, prosodic, intonational, and pronunciation\naccuracy. We evaluate state-of-the-art open-source and proprietary TTS systems,\nsuch as 11Labs, Deepgram, and OpenAI's 4o-mini-TTS, on EmergentTTS-Eval,\ndemonstrating its ability to reveal fine-grained performance differences.\nResults show that the model-as-a-judge approach offers robust TTS assessment\nand a high correlation with human preferences. We open source the evaluation\nhttps://github.com/boson-ai/EmergentTTS-Eval-public{code} and the\nhttps://huggingface.co/datasets/bosonai/EmergentTTS-Eval{dataset}.",
      "upvotes": 2,
      "discussionId": "683916c70df60182c0dee8dc",
      "ai_summary": "A comprehensive TTS benchmark, EmergentTTS-Eval, automates test-case generation and evaluation using LLMs and LALM to assess nuanced and semantically complex text in speech outputs.",
      "ai_keywords": [
        "EmergentTTS-Eval",
        "LLMs",
        "Large Audio Language Model (LALM)",
        "expressed emotion",
        "prosodic",
        "intonational",
        "pronunciation accuracy",
        "TTS systems",
        "model-as-a-judge"
      ]
    },
    "publishedAt": "2025-05-28T22:36:24.000Z",
    "title": "EmergentTTS-Eval: Evaluating TTS Models on Complex Prosodic,\n  Expressiveness, and Linguistic Challenges Using Model-as-a-Judge",
    "summary": "Text-to-Speech (TTS) benchmarks often fail to capture how well models handle\nnuanced and semantically complex text. Building on EmergentTTS, we\nintroduce EmergentTTS-Eval, a comprehensive benchmark covering six\nchallenging TTS scenarios: emotions, paralinguistics, foreign words, syntactic\ncomplexity, complex pronunciation (e.g. URLs, formulas), and questions.\nCrucially, our framework automates both test-case generation and evaluation,\nmaking the benchmark easily extensible. Starting from a small set of\nhuman-written seed prompts, we iteratively extend them using LLMs to target\nspecific structural, phonetic and prosodic challenges, resulting in 1,645\ndiverse test cases. Moreover, we employ a model-as-a-judge approach, using a\nLarge Audio Language Model (LALM) to assess the speech across multiple\ndimensions such as expressed emotion, prosodic, intonational, and pronunciation\naccuracy. We evaluate state-of-the-art open-source and proprietary TTS systems,\nsuch as 11Labs, Deepgram, and OpenAI's 4o-mini-TTS, on EmergentTTS-Eval,\ndemonstrating its ability to reveal fine-grained performance differences.\nResults show that the model-as-a-judge approach offers robust TTS assessment\nand a high correlation with human preferences. We open source the evaluation\nhttps://github.com/boson-ai/EmergentTTS-Eval-public{code} and the\nhttps://huggingface.co/datasets/bosonai/EmergentTTS-Eval{dataset}.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23009.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66958c29d4ca2767b9c41005",
      "avatarUrl": "/avatars/c81b65c1ad345c48c252773ea78b7607.svg",
      "fullname": "Ruskin Raj Manku",
      "name": "ruskinmanku",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23844",
      "authors": [
        {
          "_id": "683d0ac47852d920b7dc3dc5",
          "name": "Zhenglun Kong",
          "hidden": false
        },
        {
          "_id": "683d0ac47852d920b7dc3dc6",
          "name": "Zheng Zhan",
          "hidden": false
        },
        {
          "_id": "683d0ac47852d920b7dc3dc7",
          "name": "Shiyue Hou",
          "hidden": false
        },
        {
          "_id": "683d0ac47852d920b7dc3dc8",
          "name": "Yifan Gong",
          "hidden": false
        },
        {
          "_id": "683d0ac47852d920b7dc3dc9",
          "name": "Xin Meng",
          "hidden": false
        },
        {
          "_id": "683d0ac47852d920b7dc3dca",
          "name": "Pengwei Sui",
          "hidden": false
        },
        {
          "_id": "683d0ac47852d920b7dc3dcb",
          "name": "Peiyan Dong",
          "hidden": false
        },
        {
          "_id": "683d0ac47852d920b7dc3dcc",
          "name": "Xuan Shen",
          "hidden": false
        },
        {
          "_id": "683d0ac47852d920b7dc3dcd",
          "name": "Zifeng Wang",
          "hidden": false
        },
        {
          "_id": "683d0ac47852d920b7dc3dce",
          "name": "Pu Zhao",
          "hidden": false
        },
        {
          "_id": "683d0ac47852d920b7dc3dcf",
          "name": "Hao Tang",
          "hidden": false
        },
        {
          "_id": "683d0ac47852d920b7dc3dd0",
          "name": "Stratis Ioannidis",
          "hidden": false
        },
        {
          "_id": "683d0ac47852d920b7dc3dd1",
          "name": "Yanzhi Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T16:24:50.000Z",
      "submittedOnDailyAt": "2025-06-02T00:52:55.266Z",
      "title": "Enabling Flexible Multi-LLM Integration for Scalable Knowledge\n  Aggregation",
      "submittedOnDailyBy": {
        "_id": "5f2c36551ebc8c6ede2f0e53",
        "avatarUrl": "/avatars/e3ddbd15f50b86958377b5fc2460a57e.svg",
        "isPro": false,
        "fullname": "Tony Kong",
        "user": "TonyK",
        "type": "user"
      },
      "summary": "Large language models (LLMs) have shown remarkable promise but remain\nchallenging to continually improve through traditional finetuning, particularly\nwhen integrating capabilities from other specialized LLMs. Popular methods like\nensemble and weight merging require substantial memory and struggle to adapt to\nchanging data environments. Recent efforts have transferred knowledge from\nmultiple LLMs into a single target model; however, they suffer from\ninterference and degraded performance among tasks, largely due to limited\nflexibility in candidate selection and training pipelines. To address these\nissues, we propose a framework that adaptively selects and aggregates knowledge\nfrom diverse LLMs to build a single, stronger model, avoiding the high memory\noverhead of ensemble and inflexible weight merging. Specifically, we design an\nadaptive selection network that identifies the most relevant source LLMs based\non their scores, thereby reducing knowledge interference. We further propose a\ndynamic weighted fusion strategy that accounts for the inherent strengths of\ncandidate LLMs, along with a feedback-driven loss function that prevents the\nselector from converging on a single subset of sources. Experimental results\ndemonstrate that our method can enable a more stable and scalable knowledge\naggregation process while reducing knowledge interference by up to 50% compared\nto existing approaches. Code is avaliable at\nhttps://github.com/ZLKong/LLM_Integration",
      "upvotes": 2,
      "discussionId": "683d0ac57852d920b7dc3e20",
      "projectPage": "https://github.com/ZLKong/LLM_Integration/tree/main",
      "githubRepo": "https://github.com/ZLKong/LLM_Integration/tree/main",
      "ai_summary": "A framework for adaptive selection and dynamic weighted fusion of knowledge from multiple LLMs reduces interference and improves scalability in knowledge aggregation.",
      "ai_keywords": [
        "large language models",
        "fine-tuning",
        "ensemble",
        "weight merging",
        "adaptive selection network",
        "dynamic weighted fusion",
        "feedback-driven loss function",
        "knowledge interference"
      ]
    },
    "publishedAt": "2025-05-28T12:24:50.000Z",
    "title": "Enabling Flexible Multi-LLM Integration for Scalable Knowledge\n  Aggregation",
    "summary": "Large language models (LLMs) have shown remarkable promise but remain\nchallenging to continually improve through traditional finetuning, particularly\nwhen integrating capabilities from other specialized LLMs. Popular methods like\nensemble and weight merging require substantial memory and struggle to adapt to\nchanging data environments. Recent efforts have transferred knowledge from\nmultiple LLMs into a single target model; however, they suffer from\ninterference and degraded performance among tasks, largely due to limited\nflexibility in candidate selection and training pipelines. To address these\nissues, we propose a framework that adaptively selects and aggregates knowledge\nfrom diverse LLMs to build a single, stronger model, avoiding the high memory\noverhead of ensemble and inflexible weight merging. Specifically, we design an\nadaptive selection network that identifies the most relevant source LLMs based\non their scores, thereby reducing knowledge interference. We further propose a\ndynamic weighted fusion strategy that accounts for the inherent strengths of\ncandidate LLMs, along with a feedback-driven loss function that prevents the\nselector from converging on a single subset of sources. Experimental results\ndemonstrate that our method can enable a more stable and scalable knowledge\naggregation process while reducing knowledge interference by up to 50% compared\nto existing approaches. Code is avaliable at\nhttps://github.com/ZLKong/LLM_Integration",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23844.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f2c36551ebc8c6ede2f0e53",
      "avatarUrl": "/avatars/e3ddbd15f50b86958377b5fc2460a57e.svg",
      "fullname": "Tony Kong",
      "name": "TonyK",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]