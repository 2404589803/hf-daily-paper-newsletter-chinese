[
  {
    "paper": {
      "id": "2601.05242",
      "authors": [
        {
          "_id": "69607a225b7998385e63952a",
          "name": "Shih-Yang Liu",
          "hidden": false
        },
        {
          "_id": "69607a225b7998385e63952b",
          "name": "Xin Dong",
          "hidden": false
        },
        {
          "_id": "69607a225b7998385e63952c",
          "name": "Ximing Lu",
          "hidden": false
        },
        {
          "_id": "69607a225b7998385e63952d",
          "name": "Shizhe Diao",
          "hidden": false
        },
        {
          "_id": "69607a225b7998385e63952e",
          "name": "Peter Belcak",
          "hidden": false
        },
        {
          "_id": "69607a225b7998385e63952f",
          "name": "Mingjie Liu",
          "hidden": false
        },
        {
          "_id": "69607a225b7998385e639530",
          "name": "Min-Hung Chen",
          "hidden": false
        },
        {
          "_id": "69607a225b7998385e639531",
          "name": "Hongxu Yin",
          "hidden": false
        },
        {
          "_id": "69607a225b7998385e639532",
          "name": "Yu-Chiang Frank Wang",
          "hidden": false
        },
        {
          "_id": "69607a225b7998385e639533",
          "name": "Kwang-Ting Cheng",
          "hidden": false
        },
        {
          "_id": "69607a225b7998385e639534",
          "name": "Yejin Choi",
          "hidden": false
        },
        {
          "_id": "69607a225b7998385e639535",
          "name": "Jan Kautz",
          "hidden": false
        },
        {
          "_id": "69607a225b7998385e639536",
          "name": "Pavlo Molchanov",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-08T18:59:24.000Z",
      "submittedOnDailyAt": "2026-01-09T01:16:50.715Z",
      "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization",
      "submittedOnDailyBy": {
        "_id": "62b58c68a1bae3c711c41321",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png",
        "isPro": false,
        "fullname": "LIU Shih-yang",
        "user": "sliuau",
        "type": "user"
      },
      "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.",
      "upvotes": 58,
      "discussionId": "69607a225b7998385e639537",
      "projectPage": "https://nvlabs.github.io/GDPO/",
      "githubRepo": "https://github.com/NVlabs/GDPO",
      "githubRepoAddedBy": "user",
      "ai_summary": "Multi-reward reinforcement learning suffers from reward normalization collapse in GRPO, which GDPO addresses by decoupling reward normalization for improved training stability and performance across reasoning tasks.",
      "ai_keywords": [
        "Reinforcement learning",
        "Group Relative Policy Optimization",
        "multi-reward setting",
        "policy optimization",
        "Group reward-Decoupled Normalization Policy Optimization",
        "reward normalization",
        "advantage values",
        "training stability",
        "multi-reward reinforcement learning"
      ],
      "githubStars": 8,
      "organization": {
        "_id": "60262b67268c201cdc8b7d43",
        "name": "nvidia",
        "fullname": "NVIDIA",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
      }
    },
    "publishedAt": "2026-01-08T13:59:24.000Z",
    "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization",
    "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05242.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62b58c68a1bae3c711c41321",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png",
      "fullname": "LIU Shih-yang",
      "name": "sliuau",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "60262b67268c201cdc8b7d43",
      "name": "nvidia",
      "fullname": "NVIDIA",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.05241",
      "authors": [
        {
          "_id": "6960775a5b7998385e6394ff",
          "name": "Boyang Wang",
          "hidden": false
        },
        {
          "_id": "6960775a5b7998385e639500",
          "name": "Haoran Zhang",
          "hidden": false
        },
        {
          "_id": "6960775a5b7998385e639501",
          "name": "Shujie Zhang",
          "hidden": false
        },
        {
          "_id": "6960775a5b7998385e639502",
          "name": "Jinkun Hao",
          "hidden": false
        },
        {
          "_id": "6960775a5b7998385e639503",
          "name": "Mingda Jia",
          "hidden": false
        },
        {
          "_id": "6960775a5b7998385e639504",
          "name": "Qi Lv",
          "hidden": false
        },
        {
          "_id": "6960775a5b7998385e639505",
          "name": "Yucheng Mao",
          "hidden": false
        },
        {
          "_id": "6960775a5b7998385e639506",
          "name": "Zhaoyang Lyu",
          "hidden": false
        },
        {
          "_id": "6960775a5b7998385e639507",
          "name": "Jia Zeng",
          "hidden": false
        },
        {
          "_id": "6960775a5b7998385e639508",
          "name": "Xudong Xu",
          "hidden": false
        },
        {
          "_id": "6960775a5b7998385e639509",
          "name": "Jiangmiao Pang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-08T18:59:22.000Z",
      "submittedOnDailyAt": "2026-01-09T02:54:13.651Z",
      "title": "RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation",
      "submittedOnDailyBy": {
        "_id": "64ed876a74d9b58eabc769a4",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ed876a74d9b58eabc769a4/K4bJVW0FlqRtAAxJBJifR.jpeg",
        "isPro": true,
        "fullname": "Boyang Wang",
        "user": "HikariDawn",
        "type": "user"
      },
      "summary": "The diversity, quantity, and quality of manipulation data are critical for training effective robot policies. However, due to hardware and physical setup constraints, collecting large-scale real-world manipulation data remains difficult to scale across diverse environments. Recent work uses text-prompt conditioned image diffusion models to augment manipulation data by altering the backgrounds and tabletop objects in the visual observations. However, these approaches often overlook the practical need for multi-view and temporally coherent observations required by state-of-the-art policy models. Further, text prompts alone cannot reliably specify the scene setup. To provide the diffusion model with explicit visual guidance, we introduce visual identity prompting, which supplies exemplar images as conditioning inputs to guide the generation of the desired scene setup. To this end, we also build a scalable pipeline to curate a visual identity pool from large robotics datasets. Using our augmented manipulation data to train downstream vision-language-action and visuomotor policy models yields consistent performance gains in both simulation and real-robot settings.",
      "upvotes": 16,
      "discussionId": "6960775a5b7998385e63950a",
      "projectPage": "https://robovip.github.io/RoboVIP/",
      "githubRepo": "https://github.com/RoboVIP/RoboVIP_VDM",
      "githubRepoAddedBy": "user",
      "ai_summary": "Visual identity prompting enhances manipulation data augmentation for robot policies by providing explicit visual guidance to diffusion models, improving policy performance in both simulation and real-world settings.",
      "ai_keywords": [
        "image diffusion models",
        "visual identity prompting",
        "manipulation data",
        "vision-language-action models",
        "visuomotor policy models",
        "visual identity pool"
      ],
      "githubStars": 1
    },
    "publishedAt": "2026-01-08T13:59:22.000Z",
    "title": "RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation",
    "summary": "The diversity, quantity, and quality of manipulation data are critical for training effective robot policies. However, due to hardware and physical setup constraints, collecting large-scale real-world manipulation data remains difficult to scale across diverse environments. Recent work uses text-prompt conditioned image diffusion models to augment manipulation data by altering the backgrounds and tabletop objects in the visual observations. However, these approaches often overlook the practical need for multi-view and temporally coherent observations required by state-of-the-art policy models. Further, text prompts alone cannot reliably specify the scene setup. To provide the diffusion model with explicit visual guidance, we introduce visual identity prompting, which supplies exemplar images as conditioning inputs to guide the generation of the desired scene setup. To this end, we also build a scalable pipeline to curate a visual identity pool from large robotics datasets. Using our augmented manipulation data to train downstream vision-language-action and visuomotor policy models yields consistent performance gains in both simulation and real-robot settings.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05241.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64ed876a74d9b58eabc769a4",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ed876a74d9b58eabc769a4/K4bJVW0FlqRtAAxJBJifR.jpeg",
      "fullname": "Boyang Wang",
      "name": "HikariDawn",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.05167",
      "authors": [
        {
          "_id": "69606c325b7998385e639481",
          "name": "Chengsong Huang",
          "hidden": false
        },
        {
          "_id": "69606c325b7998385e639482",
          "name": "Tong Zheng",
          "hidden": false
        },
        {
          "_id": "69606c325b7998385e639483",
          "name": "Langlin Huang",
          "hidden": false
        },
        {
          "_id": "69606c325b7998385e639484",
          "name": "Jinyuan Li",
          "hidden": false
        },
        {
          "_id": "69606c325b7998385e639485",
          "name": "Haolin Liu",
          "hidden": false
        },
        {
          "_id": "69606c325b7998385e639486",
          "name": "Jiaxin Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-08T17:56:16.000Z",
      "submittedOnDailyAt": "2026-01-09T00:17:45.320Z",
      "title": "RelayLLM: Efficient Reasoning via Collaborative Decoding",
      "submittedOnDailyBy": {
        "_id": "62ea79dd01ed9b0e8f61ccd3",
        "avatarUrl": "/avatars/70af83e0e267be39fcd5f23b85e2dafa.svg",
        "isPro": false,
        "fullname": "Chengsong Huang",
        "user": "ChengsongHuang",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) for complex reasoning is often hindered by high computational costs and latency, while resource-efficient Small Language Models (SLMs) typically lack the necessary reasoning capacity. Existing collaborative approaches, such as cascading or routing, operate at a coarse granularity by offloading entire queries to LLMs, resulting in significant computational waste when the SLM is capable of handling the majority of reasoning steps. To address this, we propose RelayLLM, a novel framework for efficient reasoning via token-level collaborative decoding. Unlike routers, RelayLLM empowers the SLM to act as an active controller that dynamically invokes the LLM only for critical tokens via a special command, effectively \"relaying\" the generation process. We introduce a two-stage training framework, including warm-up and Group Relative Policy Optimization (GRPO) to teach the model to balance independence with strategic help-seeking. Empirical results across six benchmarks demonstrate that RelayLLM achieves an average accuracy of 49.52%, effectively bridging the performance gap between the two models. Notably, this is achieved by invoking the LLM for only 1.07% of the total generated tokens, offering a 98.2% cost reduction compared to performance-matched random routers.",
      "upvotes": 14,
      "discussionId": "69606c325b7998385e639487",
      "githubRepo": "https://github.com/Chengsong-Huang/RelayLLM",
      "githubRepoAddedBy": "user",
      "ai_summary": "RelayLLM enables efficient collaborative reasoning between small and large language models through token-level dynamic invocation, achieving high accuracy with minimal computational overhead.",
      "ai_keywords": [
        "Large Language Models",
        "Small Language Models",
        "collaborative decoding",
        "token-level collaboration",
        "Group Relative Policy Optimization",
        "policy optimization",
        "dynamic invocation",
        "computational efficiency",
        "reasoning capacity"
      ],
      "githubStars": 2
    },
    "publishedAt": "2026-01-08T12:56:16.000Z",
    "title": "RelayLLM: Efficient Reasoning via Collaborative Decoding",
    "summary": "Large Language Models (LLMs) for complex reasoning is often hindered by high computational costs and latency, while resource-efficient Small Language Models (SLMs) typically lack the necessary reasoning capacity. Existing collaborative approaches, such as cascading or routing, operate at a coarse granularity by offloading entire queries to LLMs, resulting in significant computational waste when the SLM is capable of handling the majority of reasoning steps. To address this, we propose RelayLLM, a novel framework for efficient reasoning via token-level collaborative decoding. Unlike routers, RelayLLM empowers the SLM to act as an active controller that dynamically invokes the LLM only for critical tokens via a special command, effectively \"relaying\" the generation process. We introduce a two-stage training framework, including warm-up and Group Relative Policy Optimization (GRPO) to teach the model to balance independence with strategic help-seeking. Empirical results across six benchmarks demonstrate that RelayLLM achieves an average accuracy of 49.52%, effectively bridging the performance gap between the two models. Notably, this is achieved by invoking the LLM for only 1.07% of the total generated tokens, offering a 98.2% cost reduction compared to performance-matched random routers.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05167.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62ea79dd01ed9b0e8f61ccd3",
      "avatarUrl": "/avatars/70af83e0e267be39fcd5f23b85e2dafa.svg",
      "fullname": "Chengsong Huang",
      "name": "ChengsongHuang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.04890",
      "authors": [
        {
          "_id": "69608e7c5b7998385e639583",
          "name": "Maksim Velikanov",
          "hidden": false
        },
        {
          "_id": "69608e7c5b7998385e639584",
          "name": "Ilyas Chahed",
          "hidden": false
        },
        {
          "_id": "69608e7c5b7998385e639585",
          "name": "Jingwei Zuo",
          "hidden": false
        },
        {
          "_id": "69608e7c5b7998385e639586",
          "name": "Dhia Eddine Rhaiem",
          "hidden": false
        },
        {
          "_id": "69608e7c5b7998385e639587",
          "name": "Younes Belkada",
          "hidden": false
        },
        {
          "_id": "69608e7c5b7998385e639588",
          "name": "Hakim Hacid",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-08T12:41:49.000Z",
      "submittedOnDailyAt": "2026-01-09T02:55:50.938Z",
      "title": "Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers",
      "submittedOnDailyBy": {
        "_id": "6460c3811db65f878513bcaf",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6460c3811db65f878513bcaf/CRdJ8lXixDku3k8Rm5Stn.jpeg",
        "isPro": false,
        "fullname": "Jingwei Zuo",
        "user": "JingweiZuo",
        "type": "user"
      },
      "summary": "Applying weight decay (WD) to matrix layers is standard practice in large-language-model pretraining. Prior work suggests that stochastic gradient noise induces a Brownian-like expansion of the weight matrices W, whose growth is counteracted by WD, leading to a WD-noise equilibrium with a certain weight norm ||W||. In this work, we view the equilibrium norm as a harmful artifact of the training procedure, and address it by introducing learnable multipliers to learn the optimal scale. First, we attach a learnable scalar multiplier to W and confirm that the WD-noise equilibrium norm is suboptimal: the learned scale adapts to data and improves performance. We then argue that individual row and column norms are similarly constrained, and free their scale by introducing learnable per-row and per-column multipliers. Our method can be viewed as a learnable, more expressive generalization of muP multipliers. It outperforms a well-tuned muP baseline, reduces the computational overhead of multiplier tuning, and surfaces practical questions such as forward-pass symmetries and the width-scaling of the learned multipliers. Finally, we validate learnable multipliers with both Adam and Muon optimizers, where it shows improvement in downstream evaluations matching the improvement of the switching from Adam to Muon.",
      "upvotes": 12,
      "discussionId": "69608e7c5b7998385e639589",
      "projectPage": "https://tiiuae.github.io/Falcon-H1/",
      "githubRepo": "https://github.com/tiiuae/falcon-h1",
      "githubRepoAddedBy": "user",
      "ai_summary": "Learnable multipliers are introduced to address weight decay-induced normalization artifacts in large language model training, outperforming traditional methods while reducing computational overhead.",
      "ai_keywords": [
        "weight decay",
        "stochastic gradient noise",
        "Brownian-like expansion",
        "WD-noise equilibrium",
        "learnable multipliers",
        "matrix layers",
        "weight norm",
        "muP multipliers",
        "Adam optimizer",
        "Muon optimizer"
      ],
      "githubStars": 96,
      "organization": {
        "_id": "6448cad23adf50d86406b0a3",
        "name": "tiiuae",
        "fullname": "Technology Innovation Institute",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61a8d1aac664736898ffc84f/AT6cAB5ZNwCcqFMal71WD.jpeg"
      }
    },
    "publishedAt": "2026-01-08T07:41:49.000Z",
    "title": "Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers",
    "summary": "Applying weight decay (WD) to matrix layers is standard practice in large-language-model pretraining. Prior work suggests that stochastic gradient noise induces a Brownian-like expansion of the weight matrices W, whose growth is counteracted by WD, leading to a WD-noise equilibrium with a certain weight norm ||W||. In this work, we view the equilibrium norm as a harmful artifact of the training procedure, and address it by introducing learnable multipliers to learn the optimal scale. First, we attach a learnable scalar multiplier to W and confirm that the WD-noise equilibrium norm is suboptimal: the learned scale adapts to data and improves performance. We then argue that individual row and column norms are similarly constrained, and free their scale by introducing learnable per-row and per-column multipliers. Our method can be viewed as a learnable, more expressive generalization of muP multipliers. It outperforms a well-tuned muP baseline, reduces the computational overhead of multiplier tuning, and surfaces practical questions such as forward-pass symmetries and the width-scaling of the learned multipliers. Finally, we validate learnable multipliers with both Adam and Muon optimizers, where it shows improvement in downstream evaluations matching the improvement of the switching from Adam to Muon.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04890.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6460c3811db65f878513bcaf",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6460c3811db65f878513bcaf/CRdJ8lXixDku3k8Rm5Stn.jpeg",
      "fullname": "Jingwei Zuo",
      "name": "JingweiZuo",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 36,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "6448cad23adf50d86406b0a3",
      "name": "tiiuae",
      "fullname": "Technology Innovation Institute",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61a8d1aac664736898ffc84f/AT6cAB5ZNwCcqFMal71WD.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.05249",
      "authors": [
        {
          "_id": "6960a8ce5b7998385e639615",
          "name": "Yuan-Kang Lee",
          "hidden": false
        },
        {
          "_id": "6960a8ce5b7998385e639616",
          "name": "Kuan-Lin Chen",
          "hidden": false
        },
        {
          "_id": "6960a8ce5b7998385e639617",
          "name": "Chia-Che Chang",
          "hidden": false
        },
        {
          "_id": "6960a8ce5b7998385e639618",
          "name": "Yu-Lun Liu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/-GwLalmxp_ctrLFDuUq73.webp"
      ],
      "publishedAt": "2026-01-08T18:59:55.000Z",
      "submittedOnDailyAt": "2026-01-09T04:35:57.571Z",
      "title": "RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes",
      "submittedOnDailyBy": {
        "_id": "6459d5da3b6fafd9664807ab",
        "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
        "isPro": false,
        "fullname": "Yu-Lun Liu",
        "user": "yulunliu",
        "type": "user"
      },
      "summary": "Nighttime color constancy remains a challenging problem in computational photography due to low-light noise and complex illumination conditions. We present RL-AWB, a novel framework combining statistical methods with deep reinforcement learning for nighttime white balance. Our method begins with a statistical algorithm tailored for nighttime scenes, integrating salient gray pixel detection with novel illumination estimation. Building on this foundation, we develop the first deep reinforcement learning approach for color constancy that leverages the statistical algorithm as its core, mimicking professional AWB tuning experts by dynamically optimizing parameters for each image. To facilitate cross-sensor evaluation, we introduce the first multi-sensor nighttime dataset. Experiment results demonstrate that our method achieves superior generalization capability across low-light and well-illuminated images. Project page: https://ntuneillee.github.io/research/rl-awb/",
      "upvotes": 8,
      "discussionId": "6960a8ce5b7998385e639619",
      "projectPage": "https://ntuneillee.github.io/research/rl-awb/",
      "ai_summary": "A novel nighttime color constancy framework combines statistical methods with deep reinforcement learning to improve white balance adjustment under low-light conditions.",
      "ai_keywords": [
        "deep reinforcement learning",
        "color constancy",
        "white balance",
        "statistical algorithms",
        "illumination estimation",
        "multi-sensor dataset"
      ]
    },
    "publishedAt": "2026-01-08T13:59:55.000Z",
    "title": "RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes",
    "summary": "Nighttime color constancy remains a challenging problem in computational photography due to low-light noise and complex illumination conditions. We present RL-AWB, a novel framework combining statistical methods with deep reinforcement learning for nighttime white balance. Our method begins with a statistical algorithm tailored for nighttime scenes, integrating salient gray pixel detection with novel illumination estimation. Building on this foundation, we develop the first deep reinforcement learning approach for color constancy that leverages the statistical algorithm as its core, mimicking professional AWB tuning experts by dynamically optimizing parameters for each image. To facilitate cross-sensor evaluation, we introduce the first multi-sensor nighttime dataset. Experiment results demonstrate that our method achieves superior generalization capability across low-light and well-illuminated images. Project page: https://ntuneillee.github.io/research/rl-awb/",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/-GwLalmxp_ctrLFDuUq73.webp"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05249.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6459d5da3b6fafd9664807ab",
      "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
      "fullname": "Yu-Lun Liu",
      "name": "yulunliu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.05175",
      "authors": [
        {
          "_id": "69606c015b7998385e639468",
          "name": "Shuming Liu",
          "hidden": false
        },
        {
          "_id": "69606c015b7998385e639469",
          "name": "Mingchen Zhuge",
          "hidden": false
        },
        {
          "_id": "69606c015b7998385e63946a",
          "name": "Changsheng Zhao",
          "hidden": false
        },
        {
          "_id": "69606c015b7998385e63946b",
          "name": "Jun Chen",
          "hidden": false
        },
        {
          "_id": "69606c015b7998385e63946c",
          "name": "Lemeng Wu",
          "hidden": false
        },
        {
          "_id": "69606c015b7998385e63946d",
          "name": "Zechun Liu",
          "hidden": false
        },
        {
          "_id": "69606c015b7998385e63946e",
          "name": "Chenchen Zhu",
          "hidden": false
        },
        {
          "_id": "69606c015b7998385e63946f",
          "name": "Zhipeng Cai",
          "hidden": false
        },
        {
          "_id": "69606c015b7998385e639470",
          "name": "Chong Zhou",
          "hidden": false
        },
        {
          "_id": "69606c015b7998385e639471",
          "name": "Haozhe Liu",
          "hidden": false
        },
        {
          "_id": "69606c015b7998385e639472",
          "name": "Ernie Chang",
          "hidden": false
        },
        {
          "_id": "69606c015b7998385e639473",
          "name": "Saksham Suri",
          "hidden": false
        },
        {
          "_id": "69606c015b7998385e639474",
          "name": "Hongyu Xu",
          "hidden": false
        },
        {
          "_id": "69606c015b7998385e639475",
          "name": "Qi Qian",
          "hidden": false
        },
        {
          "_id": "69606c015b7998385e639476",
          "name": "Wei Wen",
          "hidden": false
        },
        {
          "_id": "69606c015b7998385e639477",
          "name": "Balakrishnan Varadarajan",
          "hidden": false
        },
        {
          "_id": "69606c015b7998385e639478",
          "name": "Zhuang Liu",
          "hidden": false
        },
        {
          "_id": "69606c015b7998385e639479",
          "name": "Hu Xu",
          "hidden": false
        },
        {
          "_id": "69606c015b7998385e63947a",
          "name": "Florian Bordes",
          "hidden": false
        },
        {
          "_id": "69606c015b7998385e63947b",
          "name": "Raghuraman Krishnamoorthi",
          "hidden": false
        },
        {
          "_id": "69606c015b7998385e63947c",
          "name": "Bernard Ghanem",
          "hidden": false
        },
        {
          "_id": "69606c015b7998385e63947d",
          "name": "Vikas Chandra",
          "hidden": false
        },
        {
          "_id": "69606c015b7998385e63947e",
          "name": "Yunyang Xiong",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/ukFTFnGpjfjCtH8xLwHyV.png"
      ],
      "publishedAt": "2026-01-08T18:00:59.000Z",
      "submittedOnDailyAt": "2026-01-09T00:16:39.681Z",
      "title": "VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Chain-of-thought (CoT) reasoning has emerged as a powerful tool for multimodal large language models on video understanding tasks. However, its necessity and advantages over direct answering remain underexplored. In this paper, we first demonstrate that for RL-trained video models, direct answering often matches or even surpasses CoT performance, despite CoT producing step-by-step analyses at a higher computational cost. Motivated by this, we propose VideoAuto-R1, a video understanding framework that adopts a reason-when-necessary strategy. During training, our approach follows a Thinking Once, Answering Twice paradigm: the model first generates an initial answer, then performs reasoning, and finally outputs a reviewed answer. Both answers are supervised via verifiable rewards. During inference, the model uses the confidence score of the initial answer to determine whether to proceed with reasoning. Across video QA and grounding benchmarks, VideoAuto-R1 achieves state-of-the-art accuracy with significantly improved efficiency, reducing the average response length by ~3.3x, e.g., from 149 to just 44 tokens. Moreover, we observe a low rate of thinking-mode activation on perception-oriented tasks, but a higher rate on reasoning-intensive tasks. This suggests that explicit language-based reasoning is generally beneficial but not always necessary.",
      "upvotes": 7,
      "discussionId": "69606c015b7998385e63947f",
      "projectPage": "https://ivul-kaust.github.io/projects/videoauto-r1/",
      "ai_summary": "VideoAuto-R1 framework employs a reason-when-necessary strategy for video understanding, using a Thinking Once, Answering Twice training paradigm with verifiable rewards and confidence-based reasoning activation during inference.",
      "ai_keywords": [
        "Chain-of-thought reasoning",
        "multimodal large language models",
        "video understanding",
        "RL-trained video models",
        "VideoAuto-R1",
        "Thinking Once Answering Twice",
        "verifiable rewards",
        "confidence score",
        "perception-oriented tasks",
        "reasoning-intensive tasks"
      ],
      "organization": {
        "_id": "5e63d8713071d5be688861b8",
        "name": "facebook",
        "fullname": "AI at Meta",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"
      }
    },
    "publishedAt": "2026-01-08T13:00:59.000Z",
    "title": "VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice",
    "summary": "Chain-of-thought (CoT) reasoning has emerged as a powerful tool for multimodal large language models on video understanding tasks. However, its necessity and advantages over direct answering remain underexplored. In this paper, we first demonstrate that for RL-trained video models, direct answering often matches or even surpasses CoT performance, despite CoT producing step-by-step analyses at a higher computational cost. Motivated by this, we propose VideoAuto-R1, a video understanding framework that adopts a reason-when-necessary strategy. During training, our approach follows a Thinking Once, Answering Twice paradigm: the model first generates an initial answer, then performs reasoning, and finally outputs a reviewed answer. Both answers are supervised via verifiable rewards. During inference, the model uses the confidence score of the initial answer to determine whether to proceed with reasoning. Across video QA and grounding benchmarks, VideoAuto-R1 achieves state-of-the-art accuracy with significantly improved efficiency, reducing the average response length by ~3.3x, e.g., from 149 to just 44 tokens. Moreover, we observe a low rate of thinking-mode activation on perception-oriented tasks, but a higher rate on reasoning-intensive tasks. This suggests that explicit language-based reasoning is generally beneficial but not always necessary.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/ukFTFnGpjfjCtH8xLwHyV.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05175.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 204,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "5e63d8713071d5be688861b8",
      "name": "facebook",
      "fullname": "AI at Meta",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.04767",
      "authors": [
        {
          "_id": "69609fe35b7998385e6395ed",
          "name": "Zefang Zong",
          "hidden": false
        },
        {
          "_id": "69609fe35b7998385e6395ee",
          "name": "Dingwei Chen",
          "hidden": false
        },
        {
          "_id": "69609fe35b7998385e6395ef",
          "name": "Yang Li",
          "hidden": false
        },
        {
          "_id": "69609fe35b7998385e6395f0",
          "name": "Qi Yi",
          "hidden": false
        },
        {
          "_id": "69609fe35b7998385e6395f1",
          "name": "Bo Zhou",
          "hidden": false
        },
        {
          "_id": "69609fe35b7998385e6395f2",
          "name": "Chengming Li",
          "hidden": false
        },
        {
          "_id": "69609fe35b7998385e6395f3",
          "name": "Bo Qian",
          "hidden": false
        },
        {
          "_id": "69609fe35b7998385e6395f4",
          "name": "Peng Chen",
          "hidden": false
        },
        {
          "_id": "69609fe35b7998385e6395f5",
          "name": "Jie Jiang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-08T09:35:49.000Z",
      "submittedOnDailyAt": "2026-01-09T04:20:12.513Z",
      "title": "AT^2PO: Agentic Turn-based Policy Optimization via Tree Search",
      "submittedOnDailyBy": {
        "_id": "64feba7efa64465422ce3003",
        "avatarUrl": "/avatars/abdc7a3748f6a4e15ebc6aa8d616c87d.svg",
        "isPro": false,
        "fullname": "zongzefang",
        "user": "zzfoutofspace",
        "type": "user"
      },
      "summary": "LLM agents have emerged as powerful systems for tackling multi-turn tasks by interleaving internal reasoning and external tool interactions. Agentic Reinforcement Learning has recently drawn significant research attention as a critical post-training paradigm to further refine these capabilities. In this paper, we present AT^2PO (Agentic Turn-based Policy Optimization via Tree Search), a unified framework for multi-turn agentic RL that addresses three core challenges: limited exploration diversity, sparse credit assignment, and misaligned policy optimization. AT^2PO introduces a turn-level tree structure that jointly enables Entropy-Guided Tree Expansion for strategic exploration and Turn-wise Credit Assignment for fine-grained reward propagation from sparse outcomes. Complementing this, we propose Agentic Turn-based Policy Optimization, a turn-level learning objective that aligns policy updates with the natural decision granularity of agentic interactions. ATPO is orthogonal to tree search and can be readily integrated into any multi-turn RL pipeline. Experiments across seven benchmarks demonstrate consistent improvements over the state-of-the-art baseline by up to 1.84 percentage points in average, with ablation studies validating the effectiveness of each component. Our code is available at https://github.com/zzfoutofspace/ATPO.",
      "upvotes": 7,
      "discussionId": "69609fe35b7998385e6395f6",
      "githubRepo": "https://github.com/zzfoutofspace/ATPO",
      "githubRepoAddedBy": "user",
      "ai_summary": "ATÂ²PO is a unified framework for multi-turn agentic reinforcement learning that improves exploration diversity, credit assignment, and policy optimization through tree search and turn-level learning objectives.",
      "ai_keywords": [
        "Agentic Reinforcement Learning",
        "tree search",
        "Entropy-Guided Tree Expansion",
        "Turn-wise Credit Assignment",
        "Agentic Turn-based Policy Optimization",
        "multi-turn tasks",
        "policy optimization",
        "reward propagation",
        "turn-level learning objective"
      ],
      "githubStars": 0,
      "organization": {
        "_id": "66543b6e420092799d2f625c",
        "name": "tencent",
        "fullname": "Tencent",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
      }
    },
    "publishedAt": "2026-01-08T04:35:49.000Z",
    "title": "AT^2PO: Agentic Turn-based Policy Optimization via Tree Search",
    "summary": "LLM agents have emerged as powerful systems for tackling multi-turn tasks by interleaving internal reasoning and external tool interactions. Agentic Reinforcement Learning has recently drawn significant research attention as a critical post-training paradigm to further refine these capabilities. In this paper, we present AT^2PO (Agentic Turn-based Policy Optimization via Tree Search), a unified framework for multi-turn agentic RL that addresses three core challenges: limited exploration diversity, sparse credit assignment, and misaligned policy optimization. AT^2PO introduces a turn-level tree structure that jointly enables Entropy-Guided Tree Expansion for strategic exploration and Turn-wise Credit Assignment for fine-grained reward propagation from sparse outcomes. Complementing this, we propose Agentic Turn-based Policy Optimization, a turn-level learning objective that aligns policy updates with the natural decision granularity of agentic interactions. ATPO is orthogonal to tree search and can be readily integrated into any multi-turn RL pipeline. Experiments across seven benchmarks demonstrate consistent improvements over the state-of-the-art baseline by up to 1.84 percentage points in average, with ablation studies validating the effectiveness of each component. Our code is available at https://github.com/zzfoutofspace/ATPO.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04767.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64feba7efa64465422ce3003",
      "avatarUrl": "/avatars/abdc7a3748f6a4e15ebc6aa8d616c87d.svg",
      "fullname": "zongzefang",
      "name": "zzfoutofspace",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "66543b6e420092799d2f625c",
      "name": "tencent",
      "fullname": "Tencent",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.05138",
      "authors": [
        {
          "_id": "69606cd45b7998385e639489",
          "name": "Sixiao Zheng",
          "hidden": false
        },
        {
          "_id": "69606cd45b7998385e63948a",
          "name": "Minghao Yin",
          "hidden": false
        },
        {
          "_id": "69606cd45b7998385e63948b",
          "name": "Wenbo Hu",
          "hidden": false
        },
        {
          "_id": "69606cd45b7998385e63948c",
          "name": "Xiaoyu Li",
          "hidden": false
        },
        {
          "_id": "69606cd45b7998385e63948d",
          "name": "Ying Shan",
          "hidden": false
        },
        {
          "_id": "69606cd45b7998385e63948e",
          "name": "Yanwei Fu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/vfMPtMKg_t5C1-6WqzYur.mp4"
      ],
      "publishedAt": "2026-01-08T17:28:52.000Z",
      "submittedOnDailyAt": "2026-01-09T00:21:14.757Z",
      "title": "VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Video world models aim to simulate dynamic, real-world environments, yet existing methods struggle to provide unified and precise control over camera and multi-object motion, as videos inherently operate dynamics in the projected 2D image plane. To bridge this gap, we introduce VerseCrafter, a 4D-aware video world model that enables explicit and coherent control over both camera and object dynamics within a unified 4D geometric world state. Our approach is centered on a novel 4D Geometric Control representation, which encodes the world state through a static background point cloud and per-object 3D Gaussian trajectories. This representation captures not only an object's path but also its probabilistic 3D occupancy over time, offering a flexible, category-agnostic alternative to rigid bounding boxes or parametric models. These 4D controls are rendered into conditioning signals for a pretrained video diffusion model, enabling the generation of high-fidelity, view-consistent videos that precisely adhere to the specified dynamics. Unfortunately, another major challenge lies in the scarcity of large-scale training data with explicit 4D annotations. We address this by developing an automatic data engine that extracts the required 4D controls from in-the-wild videos, allowing us to train our model on a massive and diverse dataset.",
      "upvotes": 6,
      "discussionId": "69606cd45b7998385e63948f",
      "ai_summary": "VerseCrafter is a 4D-aware video world model that enables unified control over camera and object dynamics through 4D geometric control representation and video diffusion models.",
      "ai_keywords": [
        "video world models",
        "4D geometric control",
        "point cloud",
        "3D Gaussian trajectories",
        "video diffusion model",
        "view-consistent videos",
        "automatic data engine",
        "in-the-wild videos"
      ]
    },
    "publishedAt": "2026-01-08T12:28:52.000Z",
    "title": "VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control",
    "summary": "Video world models aim to simulate dynamic, real-world environments, yet existing methods struggle to provide unified and precise control over camera and multi-object motion, as videos inherently operate dynamics in the projected 2D image plane. To bridge this gap, we introduce VerseCrafter, a 4D-aware video world model that enables explicit and coherent control over both camera and object dynamics within a unified 4D geometric world state. Our approach is centered on a novel 4D Geometric Control representation, which encodes the world state through a static background point cloud and per-object 3D Gaussian trajectories. This representation captures not only an object's path but also its probabilistic 3D occupancy over time, offering a flexible, category-agnostic alternative to rigid bounding boxes or parametric models. These 4D controls are rendered into conditioning signals for a pretrained video diffusion model, enabling the generation of high-fidelity, view-consistent videos that precisely adhere to the specified dynamics. Unfortunately, another major challenge lies in the scarcity of large-scale training data with explicit 4D annotations. We address this by developing an automatic data engine that extracts the required 4D controls from in-the-wild videos, allowing us to train our model on a massive and diverse dataset.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/vfMPtMKg_t5C1-6WqzYur.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05138.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 204,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.03425",
      "authors": [
        {
          "_id": "69606e2d5b7998385e6394a1",
          "name": "Yan Wang",
          "hidden": false
        },
        {
          "_id": "69606e2d5b7998385e6394a2",
          "name": "Yitao Xu",
          "hidden": false
        },
        {
          "_id": "69606e2d5b7998385e6394a3",
          "name": "Nanhan Shen",
          "hidden": false
        },
        {
          "_id": "69606e2d5b7998385e6394a4",
          "name": "Jinyan Su",
          "hidden": false
        },
        {
          "_id": "69606e2d5b7998385e6394a5",
          "name": "Jimin Huang",
          "hidden": false
        },
        {
          "_id": "69606e2d5b7998385e6394a6",
          "name": "Zining Zhu",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-06T21:29:45.000Z",
      "submittedOnDailyAt": "2026-01-09T00:30:00.167Z",
      "title": "The Illusion of Specialization: Unveiling the Domain-Invariant \"Standing Committee\" in Mixture-of-Experts Models",
      "submittedOnDailyBy": {
        "_id": "65d76cc5b9b7b8bf88faa916",
        "avatarUrl": "/avatars/d95232cd0c307efab6197ade1a66190b.svg",
        "isPro": true,
        "fullname": "Yan Wang",
        "user": "YanAdjeNole",
        "type": "user"
      },
      "summary": "Mixture of Experts models are widely assumed to achieve domain specialization through sparse routing. In this work, we question this assumption by introducing COMMITTEEAUDIT, a post hoc framework that analyzes routing behavior at the level of expert groups rather than individual experts. Across three representative models and the MMLU benchmark, we uncover a domain-invariant Standing Committee. This is a compact coalition of routed experts that consistently captures the majority of routing mass across domains, layers, and routing budgets, even when architectures already include shared experts. Qualitative analysis further shows that Standing Committees anchor reasoning structure and syntax, while peripheral experts handle domain-specific knowledge. These findings reveal a strong structural bias toward centralized computation, suggesting that specialization in Mixture of Experts models is far less pervasive than commonly believed. This inherent bias also indicates that current training objectives, such as load-balancing losses that enforce uniform expert utilization, may be working against the model's natural optimization path, thereby limiting training efficiency and performance.",
      "upvotes": 6,
      "discussionId": "69606e2d5b7998385e6394a7",
      "ai_summary": "Research challenges the assumption of domain specialization in Mixture of Experts models by identifying a persistent central committee of experts that dominates routing behavior across different domains and architectures.",
      "ai_keywords": [
        "Mixture of Experts",
        "sparse routing",
        "COMMITTEEAUDIT",
        "domain specialization",
        "Standing Committee",
        "expert groups",
        "routing behavior",
        "MMLU benchmark",
        "load-balancing losses",
        "expert utilization"
      ],
      "organization": {
        "_id": "658f4413674349122c0708e9",
        "name": "TheFinAI",
        "fullname": "The Fin AI",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63b58ed5889aa6707f0bb0f4/ZK5nQKw34W3-eH3p4NAYc.jpeg"
      }
    },
    "publishedAt": "2026-01-06T16:29:45.000Z",
    "title": "The Illusion of Specialization: Unveiling the Domain-Invariant \"Standing Committee\" in Mixture-of-Experts Models",
    "summary": "Mixture of Experts models are widely assumed to achieve domain specialization through sparse routing. In this work, we question this assumption by introducing COMMITTEEAUDIT, a post hoc framework that analyzes routing behavior at the level of expert groups rather than individual experts. Across three representative models and the MMLU benchmark, we uncover a domain-invariant Standing Committee. This is a compact coalition of routed experts that consistently captures the majority of routing mass across domains, layers, and routing budgets, even when architectures already include shared experts. Qualitative analysis further shows that Standing Committees anchor reasoning structure and syntax, while peripheral experts handle domain-specific knowledge. These findings reveal a strong structural bias toward centralized computation, suggesting that specialization in Mixture of Experts models is far less pervasive than commonly believed. This inherent bias also indicates that current training objectives, such as load-balancing losses that enforce uniform expert utilization, may be working against the model's natural optimization path, thereby limiting training efficiency and performance.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03425.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65d76cc5b9b7b8bf88faa916",
      "avatarUrl": "/avatars/d95232cd0c307efab6197ade1a66190b.svg",
      "fullname": "Yan Wang",
      "name": "YanAdjeNole",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "658f4413674349122c0708e9",
      "name": "TheFinAI",
      "fullname": "The Fin AI",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63b58ed5889aa6707f0bb0f4/ZK5nQKw34W3-eH3p4NAYc.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.05239",
      "authors": [
        {
          "_id": "69607a775b7998385e639541",
          "name": "Xiao Fu",
          "hidden": false
        },
        {
          "_id": "69607a775b7998385e639542",
          "name": "Shitao Tang",
          "hidden": false
        },
        {
          "_id": "69607a775b7998385e639543",
          "name": "Min Shi",
          "hidden": false
        },
        {
          "_id": "69607a775b7998385e639544",
          "name": "Xian Liu",
          "hidden": false
        },
        {
          "_id": "69607a775b7998385e639545",
          "name": "Jinwei Gu",
          "hidden": false
        },
        {
          "_id": "69607a775b7998385e639546",
          "name": "Ming-Yu Liu",
          "hidden": false
        },
        {
          "_id": "69607a775b7998385e639547",
          "name": "Dahua Lin",
          "hidden": false
        },
        {
          "_id": "69607a775b7998385e639548",
          "name": "Chen-Hsuan Lin",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/65DnWFYt3ib6IDLweuVtz.mp4"
      ],
      "publishedAt": "2026-01-08T18:58:32.000Z",
      "submittedOnDailyAt": "2026-01-09T01:19:02.879Z",
      "title": "Plenoptic Video Generation",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Camera-controlled generative video re-rendering methods, such as ReCamMaster, have achieved remarkable progress. However, despite their success in single-view setting, these works often struggle to maintain consistency across multi-view scenarios. Ensuring spatio-temporal coherence in hallucinated regions remains challenging due to the inherent stochasticity of generative models. To address it, we introduce PlenopticDreamer, a framework that synchronizes generative hallucinations to maintain spatio-temporal memory. The core idea is to train a multi-in-single-out video-conditioned model in an autoregressive manner, aided by a camera-guided video retrieval strategy that adaptively selects salient videos from previous generations as conditional inputs. In addition, Our training incorporates progressive context-scaling to improve convergence, self-conditioning to enhance robustness against long-range visual degradation caused by error accumulation, and a long-video conditioning mechanism to support extended video generation. Extensive experiments on the Basic and Agibot benchmarks demonstrate that PlenopticDreamer achieves state-of-the-art video re-rendering, delivering superior view synchronization, high-fidelity visuals, accurate camera control, and diverse view transformations (e.g., third-person to third-person, and head-view to gripper-view in robotic manipulation). Project page: https://research.nvidia.com/labs/dir/plenopticdreamer/",
      "upvotes": 3,
      "discussionId": "69607a775b7998385e639549",
      "projectPage": "https://research.nvidia.com/labs/dir/plenopticdreamer/",
      "ai_summary": "PlenopticDreamer enables consistent multi-view video re-rendering through synchronized generative hallucinations, leveraging camera-guided retrieval and progressive training mechanisms for improved temporal coherence and visual fidelity.",
      "ai_keywords": [
        "generative video re-rendering",
        "spatio-temporal coherence",
        "autoregressive training",
        "camera-guided video retrieval",
        "progressive context-scaling",
        "self-conditioning",
        "long-video conditioning",
        "multi-in-single-out model",
        "video-conditioned model"
      ],
      "organization": {
        "_id": "60262b67268c201cdc8b7d43",
        "name": "nvidia",
        "fullname": "NVIDIA",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
      }
    },
    "publishedAt": "2026-01-08T13:58:32.000Z",
    "title": "Plenoptic Video Generation",
    "summary": "Camera-controlled generative video re-rendering methods, such as ReCamMaster, have achieved remarkable progress. However, despite their success in single-view setting, these works often struggle to maintain consistency across multi-view scenarios. Ensuring spatio-temporal coherence in hallucinated regions remains challenging due to the inherent stochasticity of generative models. To address it, we introduce PlenopticDreamer, a framework that synchronizes generative hallucinations to maintain spatio-temporal memory. The core idea is to train a multi-in-single-out video-conditioned model in an autoregressive manner, aided by a camera-guided video retrieval strategy that adaptively selects salient videos from previous generations as conditional inputs. In addition, Our training incorporates progressive context-scaling to improve convergence, self-conditioning to enhance robustness against long-range visual degradation caused by error accumulation, and a long-video conditioning mechanism to support extended video generation. Extensive experiments on the Basic and Agibot benchmarks demonstrate that PlenopticDreamer achieves state-of-the-art video re-rendering, delivering superior view synchronization, high-fidelity visuals, accurate camera control, and diverse view transformations (e.g., third-person to third-person, and head-view to gripper-view in robotic manipulation). Project page: https://research.nvidia.com/labs/dir/plenopticdreamer/",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/65DnWFYt3ib6IDLweuVtz.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05239.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 204,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "60262b67268c201cdc8b7d43",
      "name": "nvidia",
      "fullname": "NVIDIA",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.05163",
      "authors": [
        {
          "_id": "696071945b7998385e6394b7",
          "name": "Qintong Zhang",
          "hidden": false
        },
        {
          "_id": "696071945b7998385e6394b8",
          "name": "Xinjie Lv",
          "hidden": false
        },
        {
          "_id": "696071945b7998385e6394b9",
          "name": "Jialong Wu",
          "hidden": false
        },
        {
          "_id": "696071945b7998385e6394ba",
          "name": "Baixuan Li",
          "hidden": false
        },
        {
          "_id": "696071945b7998385e6394bb",
          "name": "Zhengwei Tao",
          "hidden": false
        },
        {
          "_id": "696071945b7998385e6394bc",
          "name": "Guochen Yan",
          "hidden": false
        },
        {
          "_id": "696071945b7998385e6394bd",
          "name": "Huanyao Zhang",
          "hidden": false
        },
        {
          "_id": "696071945b7998385e6394be",
          "name": "Bin Wang",
          "hidden": false
        },
        {
          "_id": "696071945b7998385e6394bf",
          "name": "Jiahao Xu",
          "hidden": false
        },
        {
          "_id": "696071945b7998385e6394c0",
          "name": "Haitao Mi",
          "hidden": false
        },
        {
          "_id": "696071945b7998385e6394c1",
          "name": "Wentao Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-08T17:54:32.000Z",
      "submittedOnDailyAt": "2026-01-09T00:41:06.549Z",
      "title": "DocDancer: Towards Agentic Document-Grounded Information Seeking",
      "submittedOnDailyBy": {
        "_id": "644a4fbc2166258fccc664bc",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
        "isPro": false,
        "fullname": "Jialong Wu",
        "user": "callanwu",
        "type": "user"
      },
      "summary": "Document Question Answering (DocQA) focuses on answering questions grounded in given documents, yet existing DocQA agents lack effective tool utilization and largely rely on closed-source models. In this work, we introduce DocDancer, an end-to-end trained open-source Doc agent. We formulate DocQA as an information-seeking problem and propose a tool-driven agent framework that explicitly models document exploration and comprehension. To enable end-to-end training of such agents, we introduce an Exploration-then-Synthesis data synthesis pipeline that addresses the scarcity of high-quality training data for DocQA. Training on the synthesized data, the trained models on two long-context document understanding benchmarks, MMLongBench-Doc and DocBench, show their effectiveness. Further analysis provides valuable insights for the agentic tool design and synthetic data.",
      "upvotes": 3,
      "discussionId": "696071945b7998385e6394c2",
      "ai_summary": "DocDancer is an end-to-end trained open-source document question answering agent that formulates the task as an information-seeking problem and uses a tool-driven framework with exploration and synthesis for training.",
      "ai_keywords": [
        "document question answering",
        "information-seeking problem",
        "tool-driven agent framework",
        "document exploration",
        "document comprehension",
        "end-to-end training",
        "data synthesis pipeline",
        "long-context document understanding",
        "MMLongBench-Doc",
        "DocBench"
      ],
      "organization": {
        "_id": "61dcd8e344f59573371b5cb6",
        "name": "PekingUniversity",
        "fullname": "Peking University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
      }
    },
    "publishedAt": "2026-01-08T12:54:32.000Z",
    "title": "DocDancer: Towards Agentic Document-Grounded Information Seeking",
    "summary": "Document Question Answering (DocQA) focuses on answering questions grounded in given documents, yet existing DocQA agents lack effective tool utilization and largely rely on closed-source models. In this work, we introduce DocDancer, an end-to-end trained open-source Doc agent. We formulate DocQA as an information-seeking problem and propose a tool-driven agent framework that explicitly models document exploration and comprehension. To enable end-to-end training of such agents, we introduce an Exploration-then-Synthesis data synthesis pipeline that addresses the scarcity of high-quality training data for DocQA. Training on the synthesized data, the trained models on two long-context document understanding benchmarks, MMLongBench-Doc and DocBench, show their effectiveness. Further analysis provides valuable insights for the agentic tool design and synthetic data.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05163.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "644a4fbc2166258fccc664bc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
      "fullname": "Jialong Wu",
      "name": "callanwu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 32,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "61dcd8e344f59573371b5cb6",
      "name": "PekingUniversity",
      "fullname": "Peking University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.05111",
      "authors": [
        {
          "_id": "69606e265b7998385e639497",
          "name": "Runyang You",
          "hidden": false
        },
        {
          "_id": "69606e265b7998385e639498",
          "name": "Hongru Cai",
          "hidden": false
        },
        {
          "_id": "69606e265b7998385e639499",
          "name": "Caiqi Zhang",
          "hidden": false
        },
        {
          "_id": "69606e265b7998385e63949a",
          "name": "Qiancheng Xu",
          "hidden": false
        },
        {
          "_id": "69606e265b7998385e63949b",
          "name": "Meng Liu",
          "hidden": false
        },
        {
          "_id": "69606e265b7998385e63949c",
          "name": "Tiezheng Yu",
          "hidden": false
        },
        {
          "_id": "69606e265b7998385e63949d",
          "name": "Yongqi Li",
          "hidden": false
        },
        {
          "_id": "69606e265b7998385e63949e",
          "name": "Wenjie Li",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-08T16:58:10.000Z",
      "submittedOnDailyAt": "2026-01-09T00:25:38.567Z",
      "title": "Agent-as-a-Judge",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "LLM-as-a-Judge has revolutionized AI evaluation by leveraging large language models for scalable assessments. However, as evaluands become increasingly complex, specialized, and multi-step, the reliability of LLM-as-a-Judge has become constrained by inherent biases, shallow single-pass reasoning, and the inability to verify assessments against real-world observations. This has catalyzed the transition to Agent-as-a-Judge, where agentic judges employ planning, tool-augmented verification, multi-agent collaboration, and persistent memory to enable more robust, verifiable, and nuanced evaluations. Despite the rapid proliferation of agentic evaluation systems, the field lacks a unified framework to navigate this shifting landscape. To bridge this gap, we present the first comprehensive survey tracing this evolution. Specifically, we identify key dimensions that characterize this paradigm shift and establish a developmental taxonomy. We organize core methodologies and survey applications across general and professional domains. Furthermore, we analyze frontier challenges and identify promising research directions, ultimately providing a clear roadmap for the next generation of agentic evaluation.",
      "upvotes": 3,
      "discussionId": "69606e265b7998385e63949f",
      "ai_summary": "Large language models face limitations in evaluating complex, multi-step tasks, prompting the development of agent-based evaluation systems that utilize planning, tool-augmented verification, and multi-agent collaboration for more robust assessments.",
      "ai_keywords": [
        "LLM-as-a-Judge",
        "Agent-as-a-Judge",
        "agentic judges",
        "planning",
        "tool-augmented verification",
        "multi-agent collaboration",
        "persistent memory",
        "evaluation systems",
        "agentic evaluation"
      ]
    },
    "publishedAt": "2026-01-08T11:58:10.000Z",
    "title": "Agent-as-a-Judge",
    "summary": "LLM-as-a-Judge has revolutionized AI evaluation by leveraging large language models for scalable assessments. However, as evaluands become increasingly complex, specialized, and multi-step, the reliability of LLM-as-a-Judge has become constrained by inherent biases, shallow single-pass reasoning, and the inability to verify assessments against real-world observations. This has catalyzed the transition to Agent-as-a-Judge, where agentic judges employ planning, tool-augmented verification, multi-agent collaboration, and persistent memory to enable more robust, verifiable, and nuanced evaluations. Despite the rapid proliferation of agentic evaluation systems, the field lacks a unified framework to navigate this shifting landscape. To bridge this gap, we present the first comprehensive survey tracing this evolution. Specifically, we identify key dimensions that characterize this paradigm shift and establish a developmental taxonomy. We organize core methodologies and survey applications across general and professional domains. Furthermore, we analyze frontier challenges and identify promising research directions, ultimately providing a clear roadmap for the next generation of agentic evaluation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05111.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 204,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.05124",
      "authors": [
        {
          "_id": "69608d7e5b7998385e639573",
          "name": "Runze He",
          "hidden": false
        },
        {
          "_id": "69608d7e5b7998385e639574",
          "name": "Yiji Cheng",
          "hidden": false
        },
        {
          "_id": "69608d7e5b7998385e639575",
          "name": "Tiankai Hang",
          "hidden": false
        },
        {
          "_id": "69608d7e5b7998385e639576",
          "name": "Zhimin Li",
          "hidden": false
        },
        {
          "_id": "69608d7e5b7998385e639577",
          "name": "Yu Xu",
          "hidden": false
        },
        {
          "_id": "69608d7e5b7998385e639578",
          "name": "Zijin Yin",
          "hidden": false
        },
        {
          "_id": "69608d7e5b7998385e639579",
          "name": "Shiyi Zhang",
          "hidden": false
        },
        {
          "_id": "69608d7e5b7998385e63957a",
          "name": "Wenxun Dai",
          "hidden": false
        },
        {
          "_id": "69608d7e5b7998385e63957b",
          "name": "Penghui Du",
          "hidden": false
        },
        {
          "_id": "69608d7e5b7998385e63957c",
          "name": "Ao Ma",
          "hidden": false
        },
        {
          "_id": "69608d7e5b7998385e63957d",
          "name": "Chunyu Wang",
          "hidden": false
        },
        {
          "_id": "69608d7e5b7998385e63957e",
          "name": "Qinglin Lu",
          "hidden": false
        },
        {
          "_id": "69608d7e5b7998385e63957f",
          "name": "Jizhong Han",
          "hidden": false
        },
        {
          "_id": "69608d7e5b7998385e639580",
          "name": "Jiao Dai",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-08T17:13:00.000Z",
      "submittedOnDailyAt": "2026-01-09T03:17:37.094Z",
      "title": "Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing",
      "submittedOnDailyBy": {
        "_id": "6428fd124fe87caede856311",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/5OrNPwZkxu3Dm1IInCxML.jpeg",
        "isPro": false,
        "fullname": "Xianghao Kong",
        "user": "refkxh",
        "type": "user"
      },
      "summary": "In-context image generation and editing (ICGE) enables users to specify visual concepts through interleaved image-text prompts, demanding precise understanding and faithful execution of user intent. Although recent unified multimodal models exhibit promising understanding capabilities, these strengths often fail to transfer effectively to image generation. We introduce Re-Align, a unified framework that bridges the gap between understanding and generation through structured reasoning-guided alignment. At its core lies the In-Context Chain-of-Thought (IC-CoT), a structured reasoning paradigm that decouples semantic guidance and reference association, providing clear textual target and mitigating confusion among reference images. Furthermore, Re-Align introduces an effective RL training scheme that leverages a surrogate reward to measure the alignment between structured reasoning text and the generated image, thereby improving the model's overall performance on ICGE tasks. Extensive experiments verify that Re-Align outperforms competitive methods of comparable model scale and resources on both in-context image generation and editing tasks.",
      "upvotes": 2,
      "discussionId": "69608d7f5b7998385e639581",
      "projectPage": "https://hrz2000.github.io/realign/",
      "githubRepo": "https://github.com/hrz2000/realign",
      "githubRepoAddedBy": "user",
      "ai_summary": "Re-Align addresses the gap between understanding and generation in in-context image generation and editing through structured reasoning-guided alignment and reinforcement learning training.",
      "ai_keywords": [
        "In-Context Chain-of-Thought",
        "structured reasoning",
        "reinforcement learning",
        "surrogate reward",
        "multimodal models",
        "image generation",
        "image editing"
      ],
      "githubStars": 0
    },
    "publishedAt": "2026-01-08T12:13:00.000Z",
    "title": "Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing",
    "summary": "In-context image generation and editing (ICGE) enables users to specify visual concepts through interleaved image-text prompts, demanding precise understanding and faithful execution of user intent. Although recent unified multimodal models exhibit promising understanding capabilities, these strengths often fail to transfer effectively to image generation. We introduce Re-Align, a unified framework that bridges the gap between understanding and generation through structured reasoning-guided alignment. At its core lies the In-Context Chain-of-Thought (IC-CoT), a structured reasoning paradigm that decouples semantic guidance and reference association, providing clear textual target and mitigating confusion among reference images. Furthermore, Re-Align introduces an effective RL training scheme that leverages a surrogate reward to measure the alignment between structured reasoning text and the generated image, thereby improving the model's overall performance on ICGE tasks. Extensive experiments verify that Re-Align outperforms competitive methods of comparable model scale and resources on both in-context image generation and editing tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05124.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6428fd124fe87caede856311",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/5OrNPwZkxu3Dm1IInCxML.jpeg",
      "fullname": "Xianghao Kong",
      "name": "refkxh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.03362",
      "authors": [
        {
          "_id": "695fa6e22450f142afb3c507",
          "name": "Xiang Zhang",
          "hidden": false
        },
        {
          "_id": "695fa6e22450f142afb3c508",
          "name": "Yang Zhang",
          "hidden": false
        },
        {
          "_id": "695fa6e22450f142afb3c509",
          "name": "Lukas Mehl",
          "hidden": false
        },
        {
          "_id": "695fa6e22450f142afb3c50a",
          "name": "Markus Gross",
          "hidden": false
        },
        {
          "_id": "695fa6e22450f142afb3c50b",
          "name": "Christopher Schroers",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-06T19:02:34.000Z",
      "submittedOnDailyAt": "2026-01-09T03:38:18.551Z",
      "title": "Guardians of the Hair: Rescuing Soft Boundaries in Depth, Stereo, and Novel Views",
      "submittedOnDailyBy": {
        "_id": "657dc1576dc01435cd9029d8",
        "avatarUrl": "/avatars/3bba11ac7659fce61aeaedf40e2057a8.svg",
        "isPro": false,
        "fullname": "Xiang Zhang",
        "user": "XiangZ",
        "type": "user"
      },
      "summary": "Soft boundaries, like thin hairs, are commonly observed in natural and computer-generated imagery, but they remain challenging for 3D vision due to the ambiguous mixing of foreground and background cues. This paper introduces Guardians of the Hair (HairGuard), a framework designed to recover fine-grained soft boundary details in 3D vision tasks. Specifically, we first propose a novel data curation pipeline that leverages image matting datasets for training and design a depth fixer network to automatically identify soft boundary regions. With a gated residual module, the depth fixer refines depth precisely around soft boundaries while maintaining global depth quality, allowing plug-and-play integration with state-of-the-art depth models. For view synthesis, we perform depth-based forward warping to retain high-fidelity textures, followed by a generative scene painter that fills disoccluded regions and eliminates redundant background artifacts within soft boundaries. Finally, a color fuser adaptively combines warped and inpainted results to produce novel views with consistent geometry and fine-grained details. Extensive experiments demonstrate that HairGuard achieves state-of-the-art performance across monocular depth estimation, stereo image/video conversion, and novel view synthesis, with significant improvements in soft boundary regions.",
      "upvotes": 2,
      "discussionId": "695fa6e22450f142afb3c50c",
      "ai_summary": "HairGuard is a framework for recovering fine-grained soft boundary details in 3D vision tasks through specialized depth refinement and view synthesis techniques.",
      "ai_keywords": [
        "depth fixer network",
        "gated residual module",
        "depth-based forward warping",
        "generative scene painter",
        "color fuser",
        "monocular depth estimation",
        "stereo image conversion",
        "novel view synthesis"
      ],
      "organization": {
        "_id": "63263d7db8e57aab1a778773",
        "name": "ethz",
        "fullname": "ETH Zurich",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/xMcrQI8Yx8o697uhiCcoA.jpeg"
      }
    },
    "publishedAt": "2026-01-06T14:02:34.000Z",
    "title": "Guardians of the Hair: Rescuing Soft Boundaries in Depth, Stereo, and Novel Views",
    "summary": "Soft boundaries, like thin hairs, are commonly observed in natural and computer-generated imagery, but they remain challenging for 3D vision due to the ambiguous mixing of foreground and background cues. This paper introduces Guardians of the Hair (HairGuard), a framework designed to recover fine-grained soft boundary details in 3D vision tasks. Specifically, we first propose a novel data curation pipeline that leverages image matting datasets for training and design a depth fixer network to automatically identify soft boundary regions. With a gated residual module, the depth fixer refines depth precisely around soft boundaries while maintaining global depth quality, allowing plug-and-play integration with state-of-the-art depth models. For view synthesis, we perform depth-based forward warping to retain high-fidelity textures, followed by a generative scene painter that fills disoccluded regions and eliminates redundant background artifacts within soft boundaries. Finally, a color fuser adaptively combines warped and inpainted results to produce novel views with consistent geometry and fine-grained details. Extensive experiments demonstrate that HairGuard achieves state-of-the-art performance across monocular depth estimation, stereo image/video conversion, and novel view synthesis, with significant improvements in soft boundary regions.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03362.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "657dc1576dc01435cd9029d8",
      "avatarUrl": "/avatars/3bba11ac7659fce61aeaedf40e2057a8.svg",
      "fullname": "Xiang Zhang",
      "name": "XiangZ",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "63263d7db8e57aab1a778773",
      "name": "ethz",
      "fullname": "ETH Zurich",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/xMcrQI8Yx8o697uhiCcoA.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.05106",
      "authors": [
        {
          "_id": "696073d35b7998385e6394c4",
          "name": "Nuoya Xiong",
          "hidden": false
        },
        {
          "_id": "696073d35b7998385e6394c5",
          "name": "Yuhang Zhou",
          "hidden": false
        },
        {
          "_id": "696073d35b7998385e6394c6",
          "name": "Hanqing Zeng",
          "hidden": false
        },
        {
          "_id": "696073d35b7998385e6394c7",
          "name": "Zhaorun Chen",
          "hidden": false
        },
        {
          "_id": "696073d35b7998385e6394c8",
          "name": "Furong Huang",
          "hidden": false
        },
        {
          "_id": "696073d35b7998385e6394c9",
          "name": "Shuchao Bi",
          "hidden": false
        },
        {
          "_id": "696073d35b7998385e6394ca",
          "name": "Lizhu Zhang",
          "hidden": false
        },
        {
          "_id": "696073d35b7998385e6394cb",
          "name": "Zhuokai Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-08T16:53:16.000Z",
      "submittedOnDailyAt": "2026-01-09T00:49:52.202Z",
      "title": "Token-Level LLM Collaboration via FusionRoute",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Large language models (LLMs) exhibit strengths across diverse domains. However, achieving strong performance across these domains with a single general-purpose model typically requires scaling to sizes that are prohibitively expensive to train and deploy. On the other hand, while smaller domain-specialized models are much more efficient, they struggle to generalize beyond their training distributions. To address this dilemma, we propose FusionRoute, a robust and effective token-level multi-LLM collaboration framework in which a lightweight router simultaneously (i) selects the most suitable expert at each decoding step and (ii) contributes a complementary logit that refines or corrects the selected expert's next-token distribution via logit addition. Unlike existing token-level collaboration methods that rely solely on fixed expert outputs, we provide a theoretical analysis showing that pure expert-only routing is fundamentally limited: unless strong global coverage assumptions hold, it cannot in general realize the optimal decoding policy. By augmenting expert selection with a trainable complementary generator, FusionRoute expands the effective policy class and enables recovery of optimal value functions under mild conditions. Empirically, across both Llama-3 and Gemma-2 families and diverse benchmarks spanning mathematical reasoning, code generation, and instruction following, FusionRoute outperforms both sequence- and token-level collaboration, model merging, and direct fine-tuning, while remaining competitive with domain experts on their respective tasks.",
      "upvotes": 1,
      "discussionId": "696073d45b7998385e6394cc",
      "ai_summary": "FusionRoute is a token-level multi-LLM collaboration framework that uses a lightweight router to select optimal experts and add complementary logits, outperforming existing methods in diverse tasks while maintaining efficiency.",
      "ai_keywords": [
        "large language models",
        "token-level collaboration",
        "multi-LLM collaboration",
        "lightweight router",
        "expert selection",
        "logit addition",
        "complementary generator",
        "optimal decoding policy",
        "model merging",
        "direct fine-tuning"
      ]
    },
    "publishedAt": "2026-01-08T11:53:16.000Z",
    "title": "Token-Level LLM Collaboration via FusionRoute",
    "summary": "Large language models (LLMs) exhibit strengths across diverse domains. However, achieving strong performance across these domains with a single general-purpose model typically requires scaling to sizes that are prohibitively expensive to train and deploy. On the other hand, while smaller domain-specialized models are much more efficient, they struggle to generalize beyond their training distributions. To address this dilemma, we propose FusionRoute, a robust and effective token-level multi-LLM collaboration framework in which a lightweight router simultaneously (i) selects the most suitable expert at each decoding step and (ii) contributes a complementary logit that refines or corrects the selected expert's next-token distribution via logit addition. Unlike existing token-level collaboration methods that rely solely on fixed expert outputs, we provide a theoretical analysis showing that pure expert-only routing is fundamentally limited: unless strong global coverage assumptions hold, it cannot in general realize the optimal decoding policy. By augmenting expert selection with a trainable complementary generator, FusionRoute expands the effective policy class and enables recovery of optimal value functions under mild conditions. Empirically, across both Llama-3 and Gemma-2 families and diverse benchmarks spanning mathematical reasoning, code generation, and instruction following, FusionRoute outperforms both sequence- and token-level collaboration, model merging, and direct fine-tuning, while remaining competitive with domain experts on their respective tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05106.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 204,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.02016",
      "authors": [
        {
          "_id": "6960a4995b7998385e639609",
          "name": "Matthias Bartolo",
          "hidden": false
        },
        {
          "_id": "6960a4995b7998385e63960a",
          "name": "Dylan Seychell",
          "hidden": false
        },
        {
          "_id": "6960a4995b7998385e63960b",
          "name": "Gabriel Hili",
          "hidden": false
        },
        {
          "_id": "6960a4995b7998385e63960c",
          "name": "Matthew Montebello",
          "hidden": false
        },
        {
          "_id": "6960a4995b7998385e63960d",
          "name": "Carl James Debono",
          "hidden": false
        },
        {
          "_id": "6960a4995b7998385e63960e",
          "name": "Saviour Formosa",
          "hidden": false
        },
        {
          "_id": "6960a4995b7998385e63960f",
          "name": "Konstantinos Makantasis",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-05T11:24:34.000Z",
      "submittedOnDailyAt": "2026-01-09T04:20:37.776Z",
      "title": "Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach",
      "submittedOnDailyBy": {
        "_id": "664ae70790135abe9ba76c28",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664ae70790135abe9ba76c28/S9j90H0hjdJwf4x1kXIV0.png",
        "isPro": false,
        "fullname": "Matthias Bartolo",
        "user": "mbar0075",
        "type": "user"
      },
      "summary": "This paper investigates the integration of the Learning Using Privileged Information (LUPI) paradigm in object detection to exploit fine-grained, descriptive information available during training but not at inference. We introduce a general, model-agnostic methodology for injecting privileged information-such as bounding box masks, saliency maps, and depth cues-into deep learning-based object detectors through a teacher-student architecture. Experiments are conducted across five state-of-the-art object detection models and multiple public benchmarks, including UAV-based litter detection datasets and Pascal VOC 2012, to assess the impact on accuracy, generalization, and computational efficiency. Our results demonstrate that LUPI-trained students consistently outperform their baseline counterparts, achieving significant boosts in detection accuracy with no increase in inference complexity or model size. Performance improvements are especially marked for medium and large objects, while ablation studies reveal that intermediate weighting of teacher guidance optimally balances learning from privileged and standard inputs. The findings affirm that the LUPI framework provides an effective and practical strategy for advancing object detection systems in both resource-constrained and real-world settings.",
      "upvotes": 1,
      "discussionId": "6960a4995b7998385e639610",
      "ai_summary": "Learning Using Privileged Information paradigm enhances object detection accuracy by integrating additional training-time information through teacher-student architectures without increasing inference complexity.",
      "ai_keywords": [
        "Learning Using Privileged Information",
        "object detection",
        "teacher-student architecture",
        "bounding box masks",
        "saliency maps",
        "depth cues",
        "model-agnostic methodology",
        "UAV-based litter detection",
        "Pascal VOC 2012",
        "ablation studies",
        "intermediate weighting"
      ]
    },
    "publishedAt": "2026-01-05T06:24:34.000Z",
    "title": "Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach",
    "summary": "This paper investigates the integration of the Learning Using Privileged Information (LUPI) paradigm in object detection to exploit fine-grained, descriptive information available during training but not at inference. We introduce a general, model-agnostic methodology for injecting privileged information-such as bounding box masks, saliency maps, and depth cues-into deep learning-based object detectors through a teacher-student architecture. Experiments are conducted across five state-of-the-art object detection models and multiple public benchmarks, including UAV-based litter detection datasets and Pascal VOC 2012, to assess the impact on accuracy, generalization, and computational efficiency. Our results demonstrate that LUPI-trained students consistently outperform their baseline counterparts, achieving significant boosts in detection accuracy with no increase in inference complexity or model size. Performance improvements are especially marked for medium and large objects, while ablation studies reveal that intermediate weighting of teacher guidance optimally balances learning from privileged and standard inputs. The findings affirm that the LUPI framework provides an effective and practical strategy for advancing object detection systems in both resource-constrained and real-world settings.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02016.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "664ae70790135abe9ba76c28",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664ae70790135abe9ba76c28/S9j90H0hjdJwf4x1kXIV0.png",
      "fullname": "Matthias Bartolo",
      "name": "mbar0075",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.03559",
      "authors": [
        {
          "_id": "695f278c5fa3847525c41d76",
          "name": "Shidong Cao",
          "hidden": false
        },
        {
          "_id": "695f278c5fa3847525c41d77",
          "name": "Hongzhan Lin",
          "hidden": false
        },
        {
          "_id": "695f278c5fa3847525c41d78",
          "name": "Yuxuan Gu",
          "hidden": false
        },
        {
          "_id": "695f278c5fa3847525c41d79",
          "name": "Ziyang Luo",
          "hidden": false
        },
        {
          "_id": "695f278c5fa3847525c41d7a",
          "name": "Jing Ma",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-07T03:58:42.000Z",
      "submittedOnDailyAt": "2026-01-09T00:10:48.964Z",
      "title": "DiffCoT: Diffusion-styled Chain-of-Thought Reasoning in LLMs",
      "submittedOnDailyBy": {
        "_id": "6499466c7d1edf7cb612a9a6",
        "avatarUrl": "/avatars/c2e18594aa0879db8226f2a04496fb0b.svg",
        "isPro": false,
        "fullname": "Hongzhan Lin",
        "user": "danielhzlin",
        "type": "user"
      },
      "summary": "Chain-of-Thought (CoT) reasoning improves multi-step mathematical problem solving in large language models but remains vulnerable to exposure bias and error accumulation, as early mistakes propagate irreversibly through autoregressive decoding. In this work, we propose DiffCoT, a diffusion-styled CoT framework that reformulates CoT reasoning as an iterative denoising process. DiffCoT integrates diffusion principles at the reasoning-step level via a sliding-window mechanism, enabling unified generation and retrospective correction of intermediate steps while preserving token-level autoregression. To maintain causal consistency, we further introduce a causal diffusion noise schedule that respects the temporal structure of reasoning chains. Extensive experiments on three multi-step CoT reasoning benchmarks across diverse model backbones demonstrate that DiffCoT consistently outperforms existing CoT preference optimization methods, yielding improved robustness and error-correction capability in CoT reasoning.",
      "upvotes": 0,
      "discussionId": "695f278c5fa3847525c41d7b",
      "ai_summary": "DiffCoT reformulates chain-of-thought reasoning as an iterative denoising process using diffusion principles, enabling unified generation and correction of intermediate steps while maintaining causal consistency.",
      "ai_keywords": [
        "chain-of-thought",
        "diffusion models",
        "denoising process",
        "autoregressive decoding",
        "exposure bias",
        "error accumulation",
        "sliding-window mechanism",
        "causal consistency",
        "causal diffusion noise schedule",
        "reasoning chains",
        "CoT preference optimization"
      ]
    },
    "publishedAt": "2026-01-06T22:58:42.000Z",
    "title": "DiffCoT: Diffusion-styled Chain-of-Thought Reasoning in LLMs",
    "summary": "Chain-of-Thought (CoT) reasoning improves multi-step mathematical problem solving in large language models but remains vulnerable to exposure bias and error accumulation, as early mistakes propagate irreversibly through autoregressive decoding. In this work, we propose DiffCoT, a diffusion-styled CoT framework that reformulates CoT reasoning as an iterative denoising process. DiffCoT integrates diffusion principles at the reasoning-step level via a sliding-window mechanism, enabling unified generation and retrospective correction of intermediate steps while preserving token-level autoregression. To maintain causal consistency, we further introduce a causal diffusion noise schedule that respects the temporal structure of reasoning chains. Extensive experiments on three multi-step CoT reasoning benchmarks across diverse model backbones demonstrate that DiffCoT consistently outperforms existing CoT preference optimization methods, yielding improved robustness and error-correction capability in CoT reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03559.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6499466c7d1edf7cb612a9a6",
      "avatarUrl": "/avatars/c2e18594aa0879db8226f2a04496fb0b.svg",
      "fullname": "Hongzhan Lin",
      "name": "danielhzlin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.01887",
      "authors": [
        {
          "_id": "696077705b7998385e63950c",
          "name": "Jiawen Zhang",
          "hidden": false
        },
        {
          "_id": "696077705b7998385e63950d",
          "name": "Lipeng He",
          "hidden": false
        },
        {
          "_id": "696077705b7998385e63950e",
          "name": "Kejia Chen",
          "hidden": false
        },
        {
          "_id": "696077705b7998385e63950f",
          "name": "Jian Lou",
          "hidden": false
        },
        {
          "_id": "696077705b7998385e639510",
          "name": "Jian Liu",
          "hidden": false
        },
        {
          "_id": "696077705b7998385e639511",
          "name": "Xiaohu Yang",
          "hidden": false
        },
        {
          "_id": "696077705b7998385e639512",
          "name": "Ruoxi Jia",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-05T08:26:34.000Z",
      "submittedOnDailyAt": "2026-01-09T01:06:01.987Z",
      "title": "Safety at One Shot: Patching Fine-Tuned LLMs with A Single Instance",
      "submittedOnDailyBy": {
        "_id": "6433707307bad11484af1d2a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6433707307bad11484af1d2a/w5zB-zstJzY561n6q7m4D.jpeg",
        "isPro": false,
        "fullname": "Lipeng (Tony) He",
        "user": "ttttonyhe",
        "type": "user"
      },
      "summary": "Fine-tuning safety-aligned large language models (LLMs) can substantially compromise their safety. Previous approaches require many safety samples or calibration sets, which not only incur significant computational overhead during realignment but also lead to noticeable degradation in model utility. Contrary to this belief, we show that safety alignment can be fully recovered with only a single safety example, without sacrificing utility and at minimal cost. Remarkably, this recovery is effective regardless of the number of harmful examples used in fine-tuning or the size of the underlying model, and convergence is achieved within just a few epochs. Furthermore, we uncover the low-rank structure of the safety gradient, which explains why such efficient correction is possible. We validate our findings across five safety-aligned LLMs and multiple datasets, demonstrating the generality of our approach.",
      "upvotes": 0,
      "discussionId": "696077705b7998385e639513",
      "ai_summary": "Safety alignment of large language models can be fully recovered with a single safety example, maintaining utility and achieving convergence in few epochs through identified low-rank gradient structures.",
      "ai_keywords": [
        "large language models",
        "fine-tuning",
        "safety alignment",
        "harmful examples",
        "low-rank structure",
        "safety gradient",
        "convergence"
      ]
    },
    "publishedAt": "2026-01-05T03:26:34.000Z",
    "title": "Safety at One Shot: Patching Fine-Tuned LLMs with A Single Instance",
    "summary": "Fine-tuning safety-aligned large language models (LLMs) can substantially compromise their safety. Previous approaches require many safety samples or calibration sets, which not only incur significant computational overhead during realignment but also lead to noticeable degradation in model utility. Contrary to this belief, we show that safety alignment can be fully recovered with only a single safety example, without sacrificing utility and at minimal cost. Remarkably, this recovery is effective regardless of the number of harmful examples used in fine-tuning or the size of the underlying model, and convergence is achieved within just a few epochs. Furthermore, we uncover the low-rank structure of the safety gradient, which explains why such efficient correction is possible. We validate our findings across five safety-aligned LLMs and multiple datasets, demonstrating the generality of our approach.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.01887.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6433707307bad11484af1d2a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6433707307bad11484af1d2a/w5zB-zstJzY561n6q7m4D.jpeg",
      "fullname": "Lipeng (Tony) He",
      "name": "ttttonyhe",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.23628",
      "authors": [
        {
          "_id": "6960865d5b7998385e639560",
          "name": "Shu Pu",
          "hidden": false
        },
        {
          "_id": "6960865d5b7998385e639561",
          "name": "Boya Zeng",
          "hidden": false
        },
        {
          "_id": "6960865d5b7998385e639562",
          "name": "Kaichen Zhou",
          "hidden": false
        },
        {
          "_id": "6960865d5b7998385e639563",
          "name": "Mengyu Wang",
          "hidden": false
        },
        {
          "_id": "6960865d5b7998385e639564",
          "name": "Zhuang Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-29T17:39:21.000Z",
      "submittedOnDailyAt": "2026-01-09T02:11:30.755Z",
      "title": "Memorization in 3D Shape Generation: An Empirical Study",
      "submittedOnDailyBy": {
        "_id": "663ba53b96c98cfb8ded1c4f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/6-ipDyZbMYj_lfVurLjvh.png",
        "isPro": true,
        "fullname": "Shu Pu",
        "user": "pudashi",
        "type": "user"
      },
      "summary": "Generative models are increasingly used in 3D vision to synthesize novel shapes, yet it remains unclear whether their generation relies on memorizing training shapes. Understanding their memorization could help prevent training data leakage and improve the diversity of generated results. In this paper, we design an evaluation framework to quantify memorization in 3D generative models and study the influence of different data and modeling designs on memorization. We first apply our framework to quantify memorization in existing methods. Next, through controlled experiments with a latent vector-set (Vecset) diffusion model, we find that, on the data side, memorization depends on data modality, and increases with data diversity and finer-grained conditioning; on the modeling side, it peaks at a moderate guidance scale and can be mitigated by longer Vecsets and simple rotation augmentation. Together, our framework and analysis provide an empirical understanding of memorization in 3D generative models and suggest simple yet effective strategies to reduce it without degrading generation quality. Our code is available at https://github.com/zlab-princeton/3d_mem.",
      "upvotes": 0,
      "discussionId": "6960865e5b7998385e639565",
      "ai_summary": "Researchers develop a framework to measure memorization in 3D generative models and identify factors affecting it, finding that data modality and model design parameters influence how much training data is remembered during generation.",
      "ai_keywords": [
        "generative models",
        "3D vision",
        "memorization",
        "latent vector-set",
        "diffusion model",
        "data modality",
        "data diversity",
        "conditioning",
        "guidance scale",
        "Vecset",
        "rotation augmentation"
      ],
      "organization": {
        "_id": "6735d51c08a190b1caea1f29",
        "name": "PrincetonUniversity",
        "fullname": "Princeton University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6735d43222d14a01ae63fb6d/N7pHYiJcM_zqOnp5MWfo3.png"
      }
    },
    "publishedAt": "2025-12-29T12:39:21.000Z",
    "title": "Memorization in 3D Shape Generation: An Empirical Study",
    "summary": "Generative models are increasingly used in 3D vision to synthesize novel shapes, yet it remains unclear whether their generation relies on memorizing training shapes. Understanding their memorization could help prevent training data leakage and improve the diversity of generated results. In this paper, we design an evaluation framework to quantify memorization in 3D generative models and study the influence of different data and modeling designs on memorization. We first apply our framework to quantify memorization in existing methods. Next, through controlled experiments with a latent vector-set (Vecset) diffusion model, we find that, on the data side, memorization depends on data modality, and increases with data diversity and finer-grained conditioning; on the modeling side, it peaks at a moderate guidance scale and can be mitigated by longer Vecsets and simple rotation augmentation. Together, our framework and analysis provide an empirical understanding of memorization in 3D generative models and suggest simple yet effective strategies to reduce it without degrading generation quality. Our code is available at https://github.com/zlab-princeton/3d_mem.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23628.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "663ba53b96c98cfb8ded1c4f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/6-ipDyZbMYj_lfVurLjvh.png",
      "fullname": "Shu Pu",
      "name": "pudashi",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "6735d51c08a190b1caea1f29",
      "name": "PrincetonUniversity",
      "fullname": "Princeton University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6735d43222d14a01ae63fb6d/N7pHYiJcM_zqOnp5MWfo3.png"
    },
    "isAuthorParticipating": false
  }
]