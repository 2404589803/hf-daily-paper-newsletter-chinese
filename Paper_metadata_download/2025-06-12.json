[
  {
    "paper": {
      "id": "2506.09113",
      "authors": [
        {
          "_id": "684a3b0a9b38e1e5a33a683f",
          "name": "Yu Gao",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6840",
          "name": "Haoyuan Guo",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6841",
          "name": "Tuyen Hoang",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6842",
          "name": "Weilin Huang",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6843",
          "name": "Lu Jiang",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6844",
          "name": "Fangyuan Kong",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6845",
          "name": "Huixia Li",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6846",
          "name": "Jiashi Li",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6847",
          "name": "Liang Li",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6848",
          "name": "Xiaojie Li",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6849",
          "name": "Xunsong Li",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a684a",
          "name": "Yifu Li",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a684b",
          "name": "Shanchuan Lin",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a684c",
          "name": "Zhijie Lin",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a684d",
          "name": "Jiawei Liu",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a684e",
          "name": "Shu Liu",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a684f",
          "name": "Xiaonan Nie",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6850",
          "name": "Zhiwu Qing",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6851",
          "name": "Yuxi Ren",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6852",
          "name": "Li Sun",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6853",
          "name": "Zhi Tian",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6854",
          "name": "Rui Wang",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6855",
          "name": "Sen Wang",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6856",
          "name": "Guoqiang Wei",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6857",
          "name": "Guohong Wu",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6858",
          "name": "Jie Wu",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6859",
          "name": "Ruiqi Xia",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a685a",
          "name": "Fei Xiao",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a685b",
          "name": "Xuefeng Xiao",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a685c",
          "name": "Jiangqiao Yan",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a685d",
          "name": "Ceyuan Yang",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a685e",
          "name": "Jianchao Yang",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a685f",
          "name": "Runkai Yang",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6860",
          "name": "Tao Yang",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6861",
          "name": "Yihang Yang",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6862",
          "name": "Zilyu Ye",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6863",
          "name": "Xuejiao Zeng",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6864",
          "name": "Yan Zeng",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6865",
          "name": "Heng Zhang",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6866",
          "name": "Yang Zhao",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6867",
          "name": "Xiaozheng Zheng",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6868",
          "name": "Peihao Zhu",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6869",
          "name": "Jiaxin Zou",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a686a",
          "name": "Feilong Zuo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-10T17:56:11.000Z",
      "submittedOnDailyAt": "2025-06-12T01:08:56.090Z",
      "title": "Seedance 1.0: Exploring the Boundaries of Video Generation Models",
      "submittedOnDailyBy": {
        "_id": "6381c5d63680a7cf34e08ca9",
        "avatarUrl": "/avatars/731467e2d80d0ae163c4a00a9e3ff9e5.svg",
        "isPro": false,
        "fullname": "wujie10558@gmail.com",
        "user": "wujie10",
        "type": "user"
      },
      "summary": "Notable breakthroughs in diffusion modeling have propelled rapid improvements\nin video generation, yet current foundational model still face critical\nchallenges in simultaneously balancing prompt following, motion plausibility,\nand visual quality. In this report, we introduce Seedance 1.0, a\nhigh-performance and inference-efficient video foundation generation model that\nintegrates several core technical improvements: (i) multi-source data curation\naugmented with precision and meaningful video captioning, enabling\ncomprehensive learning across diverse scenarios; (ii) an efficient architecture\ndesign with proposed training paradigm, which allows for natively supporting\nmulti-shot generation and jointly learning of both text-to-video and\nimage-to-video tasks. (iii) carefully-optimized post-training approaches\nleveraging fine-grained supervised fine-tuning, and video-specific RLHF with\nmulti-dimensional reward mechanisms for comprehensive performance improvements;\n(iv) excellent model acceleration achieving ~10x inference speedup through\nmulti-stage distillation strategies and system-level optimizations. Seedance\n1.0 can generate a 5-second video at 1080p resolution only with 41.4 seconds\n(NVIDIA-L20). Compared to state-of-the-art video generation models, Seedance\n1.0 stands out with high-quality and fast video generation having superior\nspatiotemporal fluidity with structural stability, precise instruction\nadherence in complex multi-subject contexts, native multi-shot narrative\ncoherence with consistent subject representation.",
      "upvotes": 24,
      "discussionId": "684a3b0b9b38e1e5a33a686b",
      "projectPage": "https://seed.bytedance.com/seedance",
      "ai_summary": "Seedance 1.0 offers high-performance video generation by integrating advanced data curation, efficient architecture, post-training optimization, and model acceleration, resulting in superior quality and speed.",
      "ai_keywords": [
        "diffusion modeling",
        "multi-source data curation",
        "precision and meaningful video captioning",
        "efficient architecture",
        "training paradigm",
        "multi-shot generation",
        "text-to-video",
        "image-to-video",
        "fine-grained supervised fine-tuning",
        "video-specific RLHF",
        "multi-dimensional reward mechanisms",
        "multi-stage distillation strategies",
        "model acceleration",
        "spatiotemporal fluidity",
        "structural stability",
        "instruction adherence",
        "multi-shot narrative coherence",
        "consistent subject representation"
      ]
    },
    "publishedAt": "2025-06-10T13:56:11.000Z",
    "title": "Seedance 1.0: Exploring the Boundaries of Video Generation Models",
    "summary": "Notable breakthroughs in diffusion modeling have propelled rapid improvements\nin video generation, yet current foundational model still face critical\nchallenges in simultaneously balancing prompt following, motion plausibility,\nand visual quality. In this report, we introduce Seedance 1.0, a\nhigh-performance and inference-efficient video foundation generation model that\nintegrates several core technical improvements: (i) multi-source data curation\naugmented with precision and meaningful video captioning, enabling\ncomprehensive learning across diverse scenarios; (ii) an efficient architecture\ndesign with proposed training paradigm, which allows for natively supporting\nmulti-shot generation and jointly learning of both text-to-video and\nimage-to-video tasks. (iii) carefully-optimized post-training approaches\nleveraging fine-grained supervised fine-tuning, and video-specific RLHF with\nmulti-dimensional reward mechanisms for comprehensive performance improvements;\n(iv) excellent model acceleration achieving ~10x inference speedup through\nmulti-stage distillation strategies and system-level optimizations. Seedance\n1.0 can generate a 5-second video at 1080p resolution only with 41.4 seconds\n(NVIDIA-L20). Compared to state-of-the-art video generation models, Seedance\n1.0 stands out with high-quality and fast video generation having superior\nspatiotemporal fluidity with structural stability, precise instruction\nadherence in complex multi-subject contexts, native multi-shot narrative\ncoherence with consistent subject representation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09113.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6381c5d63680a7cf34e08ca9",
      "avatarUrl": "/avatars/731467e2d80d0ae163c4a00a9e3ff9e5.svg",
      "fullname": "wujie10558@gmail.com",
      "name": "wujie10",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.09995",
      "authors": [
        {
          "_id": "684a39639b38e1e5a33a6837",
          "name": "Yuanpeng Tu",
          "hidden": false
        },
        {
          "_id": "684a39639b38e1e5a33a6838",
          "name": "Hao Luo",
          "hidden": false
        },
        {
          "_id": "684a39639b38e1e5a33a6839",
          "name": "Xi Chen",
          "hidden": false
        },
        {
          "_id": "684a39639b38e1e5a33a683a",
          "name": "Xiang Bai",
          "hidden": false
        },
        {
          "_id": "684a39639b38e1e5a33a683b",
          "name": "Fan Wang",
          "hidden": false
        },
        {
          "_id": "684a39639b38e1e5a33a683c",
          "name": "Hengshuang Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-11T17:59:53.000Z",
      "submittedOnDailyAt": "2025-06-12T00:50:19.796Z",
      "title": "PlayerOne: Egocentric World Simulator",
      "submittedOnDailyBy": {
        "_id": "644a1b6401e18bf93a6f45c1",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644a1b6401e18bf93a6f45c1/P0i_CgCrIzOS2tYRlxoE9.png",
        "isPro": false,
        "fullname": "xichen",
        "user": "xichenhku",
        "type": "user"
      },
      "summary": "We introduce PlayerOne, the first egocentric realistic world simulator,\nfacilitating immersive and unrestricted exploration within vividly dynamic\nenvironments. Given an egocentric scene image from the user, PlayerOne can\naccurately construct the corresponding world and generate egocentric videos\nthat are strictly aligned with the real scene human motion of the user captured\nby an exocentric camera. PlayerOne is trained in a coarse-to-fine pipeline that\nfirst performs pretraining on large-scale egocentric text-video pairs for\ncoarse-level egocentric understanding, followed by finetuning on synchronous\nmotion-video data extracted from egocentric-exocentric video datasets with our\nautomatic construction pipeline. Besides, considering the varying importance of\ndifferent components, we design a part-disentangled motion injection scheme,\nenabling precise control of part-level movements. In addition, we devise a\njoint reconstruction framework that progressively models both the 4D scene and\nvideo frames, ensuring scene consistency in the long-form video generation.\nExperimental results demonstrate its great generalization ability in precise\ncontrol of varying human movements and worldconsistent modeling of diverse\nscenarios. It marks the first endeavor into egocentric real-world simulation\nand can pave the way for the community to delve into fresh frontiers of world\nmodeling and its diverse applications.",
      "upvotes": 19,
      "discussionId": "684a39639b38e1e5a33a683d",
      "projectPage": "https://playerone-hku.github.io/",
      "ai_summary": "PlayerOne is an egocentric realistic world simulator that constructs and generates videos from user-captured images, using a coarse-to-fine training pipeline and advanced motion injection and reconstruction frameworks.",
      "ai_keywords": [
        "egocentric realistic world simulator",
        "coarse-to-fine pipeline",
        "pretraining",
        "finetuning",
        "synchronous motion-video data",
        "automatic construction pipeline",
        "part-disentangled motion injection",
        "joint reconstruction framework",
        "4D scene",
        "video frames",
        "scene consistency",
        "long-form video generation",
        "worldconsistent modeling"
      ]
    },
    "publishedAt": "2025-06-11T13:59:53.000Z",
    "title": "PlayerOne: Egocentric World Simulator",
    "summary": "We introduce PlayerOne, the first egocentric realistic world simulator,\nfacilitating immersive and unrestricted exploration within vividly dynamic\nenvironments. Given an egocentric scene image from the user, PlayerOne can\naccurately construct the corresponding world and generate egocentric videos\nthat are strictly aligned with the real scene human motion of the user captured\nby an exocentric camera. PlayerOne is trained in a coarse-to-fine pipeline that\nfirst performs pretraining on large-scale egocentric text-video pairs for\ncoarse-level egocentric understanding, followed by finetuning on synchronous\nmotion-video data extracted from egocentric-exocentric video datasets with our\nautomatic construction pipeline. Besides, considering the varying importance of\ndifferent components, we design a part-disentangled motion injection scheme,\nenabling precise control of part-level movements. In addition, we devise a\njoint reconstruction framework that progressively models both the 4D scene and\nvideo frames, ensuring scene consistency in the long-form video generation.\nExperimental results demonstrate its great generalization ability in precise\ncontrol of varying human movements and worldconsistent modeling of diverse\nscenarios. It marks the first endeavor into egocentric real-world simulation\nand can pave the way for the community to delve into fresh frontiers of world\nmodeling and its diverse applications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09995.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "644a1b6401e18bf93a6f45c1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644a1b6401e18bf93a6f45c1/P0i_CgCrIzOS2tYRlxoE9.png",
      "fullname": "xichen",
      "name": "xichenhku",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 42
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.09350",
      "authors": [
        {
          "_id": "684a79ca9b38e1e5a33a68bf",
          "name": "Shanchuan Lin",
          "hidden": false
        },
        {
          "_id": "684a79ca9b38e1e5a33a68c0",
          "name": "Ceyuan Yang",
          "hidden": false
        },
        {
          "_id": "684a79ca9b38e1e5a33a68c1",
          "name": "Hao He",
          "hidden": false
        },
        {
          "_id": "684a79ca9b38e1e5a33a68c2",
          "name": "Jianwen Jiang",
          "hidden": false
        },
        {
          "_id": "684a79ca9b38e1e5a33a68c3",
          "name": "Yuxi Ren",
          "hidden": false
        },
        {
          "_id": "684a79ca9b38e1e5a33a68c4",
          "name": "Xin Xia",
          "hidden": false
        },
        {
          "_id": "684a79ca9b38e1e5a33a68c5",
          "name": "Yang Zhao",
          "hidden": false
        },
        {
          "_id": "684a79ca9b38e1e5a33a68c6",
          "name": "Xuefeng Xiao",
          "hidden": false
        },
        {
          "_id": "684a79ca9b38e1e5a33a68c7",
          "name": "Lu Jiang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-11T03:04:23.000Z",
      "submittedOnDailyAt": "2025-06-12T05:25:53.654Z",
      "title": "Autoregressive Adversarial Post-Training for Real-Time Interactive Video\n  Generation",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": true,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Existing large-scale video generation models are computationally intensive,\npreventing adoption in real-time and interactive applications. In this work, we\npropose autoregressive adversarial post-training (AAPT) to transform a\npre-trained latent video diffusion model into a real-time, interactive video\ngenerator. Our model autoregressively generates a latent frame at a time using\na single neural function evaluation (1NFE). The model can stream the result to\nthe user in real time and receive interactive responses as controls to generate\nthe next latent frame. Unlike existing approaches, our method explores\nadversarial training as an effective paradigm for autoregressive generation.\nThis not only allows us to design an architecture that is more efficient for\none-step generation while fully utilizing the KV cache, but also enables\ntraining the model in a student-forcing manner that proves to be effective in\nreducing error accumulation during long video generation. Our experiments\ndemonstrate that our 8B model achieves real-time, 24fps, streaming video\ngeneration at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to\na minute long (1440 frames). Visit our research website at\nhttps://seaweed-apt.com/2",
      "upvotes": 15,
      "discussionId": "684a79ca9b38e1e5a33a68c8",
      "projectPage": "https://seaweed-apt.com/2",
      "ai_summary": "Autoregressive adversarial post-training transforms pre-trained latent video diffusion models into real-time, interactive video generators with reduced computational requirements.",
      "ai_keywords": [
        "autoregressive adversarial post-training",
        "latent video diffusion model",
        "autoregressive generation",
        "neural function evaluation",
        "KV cache",
        "student-forcing",
        "real-time video generation",
        "24fps",
        "736x416 resolution",
        "1280x720 resolution",
        "H100"
      ]
    },
    "publishedAt": "2025-06-10T23:04:23.000Z",
    "title": "Autoregressive Adversarial Post-Training for Real-Time Interactive Video\n  Generation",
    "summary": "Existing large-scale video generation models are computationally intensive,\npreventing adoption in real-time and interactive applications. In this work, we\npropose autoregressive adversarial post-training (AAPT) to transform a\npre-trained latent video diffusion model into a real-time, interactive video\ngenerator. Our model autoregressively generates a latent frame at a time using\na single neural function evaluation (1NFE). The model can stream the result to\nthe user in real time and receive interactive responses as controls to generate\nthe next latent frame. Unlike existing approaches, our method explores\nadversarial training as an effective paradigm for autoregressive generation.\nThis not only allows us to design an architecture that is more efficient for\none-step generation while fully utilizing the KV cache, but also enables\ntraining the model in a student-forcing manner that proves to be effective in\nreducing error accumulation during long video generation. Our experiments\ndemonstrate that our 8B model achieves real-time, 24fps, streaming video\ngeneration at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to\na minute long (1440 frames). Visit our research website at\nhttps://seaweed-apt.com/2",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09350.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": true,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7089
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.09790",
      "authors": [
        {
          "_id": "684a33989b38e1e5a33a6804",
          "name": "Zhenran Xu",
          "hidden": false
        },
        {
          "_id": "684a33989b38e1e5a33a6805",
          "name": "Yiyu Wang",
          "hidden": false
        },
        {
          "_id": "684a33989b38e1e5a33a6806",
          "name": "Xue Yang",
          "hidden": false
        },
        {
          "_id": "684a33989b38e1e5a33a6807",
          "name": "Longyue Wang",
          "hidden": false
        },
        {
          "_id": "684a33989b38e1e5a33a6808",
          "name": "Weihua Luo",
          "hidden": false
        },
        {
          "_id": "684a33989b38e1e5a33a6809",
          "name": "Kaifu Zhang",
          "hidden": false
        },
        {
          "_id": "684a33989b38e1e5a33a680a",
          "name": "Baotian Hu",
          "hidden": false
        },
        {
          "_id": "684a33989b38e1e5a33a680b",
          "name": "Min Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-11T14:35:15.000Z",
      "submittedOnDailyAt": "2025-06-12T00:38:07.422Z",
      "title": "ComfyUI-R1: Exploring Reasoning Models for Workflow Generation",
      "submittedOnDailyBy": {
        "_id": "639c379cdb7c5f35004066cb",
        "avatarUrl": "/avatars/3e435506ee85aa7d2d0ec2174a07462f.svg",
        "isPro": false,
        "fullname": "Zhenran Xu",
        "user": "imryanxu",
        "type": "user"
      },
      "summary": "AI-generated content has evolved from monolithic models to modular workflows,\nparticularly on platforms like ComfyUI, enabling customization in creative\npipelines. However, crafting effective workflows requires great expertise to\norchestrate numerous specialized components, presenting a steep learning curve\nfor users. To address this challenge, we introduce ComfyUI-R1, the first large\nreasoning model for automated workflow generation. Starting with our curated\ndataset of 4K workflows, we construct long chain-of-thought (CoT) reasoning\ndata, including node selection, workflow planning, and code-level workflow\nrepresentation. ComfyUI-R1 is trained through a two-stage framework: (1) CoT\nfine-tuning for cold start, adapting models to the ComfyUI domain; (2)\nreinforcement learning for incentivizing reasoning capability, guided by a\nfine-grained rule-metric hybrid reward, ensuring format validity, structural\nintegrity, and node-level fidelity. Experiments show that our 7B-parameter\nmodel achieves a 97\\% format validity rate, along with high pass rate,\nnode-level and graph-level F1 scores, significantly surpassing prior\nstate-of-the-art methods that employ leading closed-source models such as\nGPT-4o and Claude series. Further analysis highlights the critical role of the\nreasoning process and the advantage of transforming workflows into code.\nQualitative comparison reveals our strength in synthesizing intricate workflows\nwith diverse nodes, underscoring the potential of long CoT reasoning in AI art\ncreation.",
      "upvotes": 12,
      "discussionId": "684a33989b38e1e5a33a680c",
      "projectPage": "https://github.com/AIDC-AI/ComfyUI-Copilot",
      "githubRepo": "https://github.com/AIDC-AI/ComfyUI-Copilot",
      "ai_summary": "ComfyUI-R1, a large reasoning model for automated workflow generation, demonstrates superior performance in creating AI art workflows through long chain-of-thought reasoning and reinforcement learning.",
      "ai_keywords": [
        "modular workflows",
        "ComfyUI",
        "large reasoning model",
        "automated workflow generation",
        "chain-of-thought (CoT) reasoning",
        "node selection",
        "workflow planning",
        "code-level workflow representation",
        "CoT fine-tuning",
        "reinforcement learning",
        "fine-grained rule-metric hybrid reward",
        "format validity",
        "structural integrity",
        "node-level fidelity",
        "GPT-4o",
        "Claude series",
        "pass rate",
        "node-level F1 scores",
        "graph-level F1 scores",
        "intricate workflows",
        "diverse nodes",
        "qualitative comparison",
        "AI art creation"
      ]
    },
    "publishedAt": "2025-06-11T10:35:15.000Z",
    "title": "ComfyUI-R1: Exploring Reasoning Models for Workflow Generation",
    "summary": "AI-generated content has evolved from monolithic models to modular workflows,\nparticularly on platforms like ComfyUI, enabling customization in creative\npipelines. However, crafting effective workflows requires great expertise to\norchestrate numerous specialized components, presenting a steep learning curve\nfor users. To address this challenge, we introduce ComfyUI-R1, the first large\nreasoning model for automated workflow generation. Starting with our curated\ndataset of 4K workflows, we construct long chain-of-thought (CoT) reasoning\ndata, including node selection, workflow planning, and code-level workflow\nrepresentation. ComfyUI-R1 is trained through a two-stage framework: (1) CoT\nfine-tuning for cold start, adapting models to the ComfyUI domain; (2)\nreinforcement learning for incentivizing reasoning capability, guided by a\nfine-grained rule-metric hybrid reward, ensuring format validity, structural\nintegrity, and node-level fidelity. Experiments show that our 7B-parameter\nmodel achieves a 97\\% format validity rate, along with high pass rate,\nnode-level and graph-level F1 scores, significantly surpassing prior\nstate-of-the-art methods that employ leading closed-source models such as\nGPT-4o and Claude series. Further analysis highlights the critical role of the\nreasoning process and the advantage of transforming workflows into code.\nQualitative comparison reveals our strength in synthesizing intricate workflows\nwith diverse nodes, underscoring the potential of long CoT reasoning in AI art\ncreation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09790.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "639c379cdb7c5f35004066cb",
      "avatarUrl": "/avatars/3e435506ee85aa7d2d0ec2174a07462f.svg",
      "fullname": "Zhenran Xu",
      "name": "imryanxu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.09003",
      "authors": [
        {
          "_id": "6848eed742e4f9106973f2cf",
          "name": "Lei Zhang",
          "hidden": false
        },
        {
          "_id": "6848eed742e4f9106973f2d0",
          "name": "Jiaxi Yang",
          "hidden": false
        },
        {
          "_id": "6848eed742e4f9106973f2d1",
          "name": "Min Yang",
          "hidden": false
        },
        {
          "_id": "6848eed742e4f9106973f2d2",
          "name": "Jian Yang",
          "hidden": false
        },
        {
          "_id": "6848eed742e4f9106973f2d3",
          "name": "Mouxiang Chen",
          "hidden": false
        },
        {
          "_id": "6848eed742e4f9106973f2d4",
          "name": "Jiajun Zhang",
          "hidden": false
        },
        {
          "_id": "6848eed742e4f9106973f2d5",
          "name": "Zeyu Cui",
          "hidden": false
        },
        {
          "_id": "6848eed742e4f9106973f2d6",
          "name": "Binyuan Hui",
          "hidden": false
        },
        {
          "_id": "6848eed742e4f9106973f2d7",
          "name": "Junyang Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-10T17:23:33.000Z",
      "submittedOnDailyAt": "2025-06-12T00:17:18.390Z",
      "title": "SWE-Flow: Synthesizing Software Engineering Data in a Test-Driven Manner",
      "submittedOnDailyBy": {
        "_id": "64c38871f9cd765462fa1a17",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c38871f9cd765462fa1a17/yuIlVcqeDlQVKsUF8uEl3.jpeg",
        "isPro": false,
        "fullname": "Lei Zhang",
        "user": "Lemoncoke",
        "type": "user"
      },
      "summary": "We introduce **SWE-Flow**, a novel data synthesis framework grounded in\nTest-Driven Development (TDD). Unlike existing software engineering data that\nrely on human-submitted issues, **SWE-Flow** automatically infers incremental\ndevelopment steps directly from unit tests, which inherently encapsulate\nhigh-level requirements. The core of **SWE-Flow** is the construction of a\nRuntime Dependency Graph (RDG), which precisely captures function interactions,\nenabling the generation of a structured, step-by-step *development schedule*.\nAt each step, **SWE-Flow** produces a partial codebase, the corresponding unit\ntests, and the necessary code modifications, resulting in fully verifiable TDD\ntasks. With this approach, we generated 16,061 training instances and 2,020\ntest instances from real-world GitHub projects, creating the **SWE-Flow-Eval**\nbenchmark. Our experiments show that fine-tuning open model on this dataset\nsignificantly improves performance in TDD-based coding. To facilitate further\nresearch, we release all code, datasets, models, and Docker images at\n[Github](https://github.com/Hambaobao/SWE-Flow).",
      "upvotes": 12,
      "discussionId": "6848eed842e4f9106973f2d8",
      "githubRepo": "https://github.com/Hambaobao/SWE-Flow",
      "ai_summary": "A novel data synthesis framework, SWE-Flow, uses unit tests to automatically infer development steps and generate a structured schedule for Test-Driven Development (TDD), significantly improving the performance of open models fine-tuned on real-world projects.",
      "ai_keywords": [
        "Test-Driven Development (TDD)",
        "Runtime Dependency Graph (RDG)",
        "SWE-Flow",
        "unit tests",
        "development schedule",
        "SWE-Flow-Eval",
        "fine-tuning",
        "open model"
      ]
    },
    "publishedAt": "2025-06-10T13:23:33.000Z",
    "title": "SWE-Flow: Synthesizing Software Engineering Data in a Test-Driven Manner",
    "summary": "We introduce **SWE-Flow**, a novel data synthesis framework grounded in\nTest-Driven Development (TDD). Unlike existing software engineering data that\nrely on human-submitted issues, **SWE-Flow** automatically infers incremental\ndevelopment steps directly from unit tests, which inherently encapsulate\nhigh-level requirements. The core of **SWE-Flow** is the construction of a\nRuntime Dependency Graph (RDG), which precisely captures function interactions,\nenabling the generation of a structured, step-by-step *development schedule*.\nAt each step, **SWE-Flow** produces a partial codebase, the corresponding unit\ntests, and the necessary code modifications, resulting in fully verifiable TDD\ntasks. With this approach, we generated 16,061 training instances and 2,020\ntest instances from real-world GitHub projects, creating the **SWE-Flow-Eval**\nbenchmark. Our experiments show that fine-tuning open model on this dataset\nsignificantly improves performance in TDD-based coding. To facilitate further\nresearch, we release all code, datasets, models, and Docker images at\n[Github](https://github.com/Hambaobao/SWE-Flow).",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09003.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64c38871f9cd765462fa1a17",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c38871f9cd765462fa1a17/yuIlVcqeDlQVKsUF8uEl3.jpeg",
      "fullname": "Lei Zhang",
      "name": "Lemoncoke",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.08889",
      "authors": [
        {
          "_id": "684a39599b38e1e5a33a6822",
          "name": "Yizhao Gao",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a6823",
          "name": "Shuming Guo",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a6824",
          "name": "Shijie Cao",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a6825",
          "name": "Yuqing Xia",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a6826",
          "name": "Yu Cheng",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a6827",
          "name": "Lei Wang",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a6828",
          "name": "Lingxiao Ma",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a6829",
          "name": "Yutao Sun",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a682a",
          "name": "Tianzhu Ye",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a682b",
          "name": "Li Dong",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a682c",
          "name": "Hayden Kwok-Hay So",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a682d",
          "name": "Yu Hua",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a682e",
          "name": "Ting Cao",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a682f",
          "name": "Fan Yang",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a6830",
          "name": "Mao Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-10T15:17:26.000Z",
      "submittedOnDailyAt": "2025-06-12T00:54:43.454Z",
      "title": "SeerAttention-R: Sparse Attention Adaptation for Long Reasoning",
      "submittedOnDailyBy": {
        "_id": "661c96f48921f03a9dae04c3",
        "avatarUrl": "/avatars/c486a45ffea7d1a8c72bd8512014b07e.svg",
        "isPro": false,
        "fullname": "Yizhao Gao",
        "user": "LongMountain",
        "type": "user"
      },
      "summary": "We introduce SeerAttention-R, a sparse attention framework specifically\ntailored for the long decoding of reasoning models. Extended from\nSeerAttention, SeerAttention-R retains the design of learning attention\nsparsity through a self-distilled gating mechanism, while removing query\npooling to accommodate auto-regressive decoding. With a lightweight plug-in\ngating, SeerAttention-R is flexible and can be easily integrated into existing\npretrained model without modifying the original parameters. We demonstrate that\nSeerAttention-R, trained on just 0.4B tokens, maintains near-lossless reasoning\naccuracy with 4K token budget in AIME benchmark under large sparse attention\nblock sizes (64/128). Using TileLang, we develop a highly optimized sparse\ndecoding kernel that achieves near-theoretical speedups of up to 9x over\nFlashAttention-3 on H100 GPU at 90% sparsity. Code is available at:\nhttps://github.com/microsoft/SeerAttention.",
      "upvotes": 10,
      "discussionId": "684a39599b38e1e5a33a6833",
      "githubRepo": "https://github.com/microsoft/SeerAttention",
      "ai_summary": "SeerAttention-R is a sparse attention framework for reasoning models that maintains high accuracy and achieves significant speedups through optimized sparse decoding kernels.",
      "ai_keywords": [
        "sparse attention",
        "reasoning models",
        "self-distilled gating mechanism",
        "query pooling",
        "lightweight plug-in gating",
        "AIME benchmark",
        "TileLang",
        "sparse decoding kernel",
        "FlashAttention-3",
        "H100 GPU"
      ]
    },
    "publishedAt": "2025-06-10T11:17:26.000Z",
    "title": "SeerAttention-R: Sparse Attention Adaptation for Long Reasoning",
    "summary": "We introduce SeerAttention-R, a sparse attention framework specifically\ntailored for the long decoding of reasoning models. Extended from\nSeerAttention, SeerAttention-R retains the design of learning attention\nsparsity through a self-distilled gating mechanism, while removing query\npooling to accommodate auto-regressive decoding. With a lightweight plug-in\ngating, SeerAttention-R is flexible and can be easily integrated into existing\npretrained model without modifying the original parameters. We demonstrate that\nSeerAttention-R, trained on just 0.4B tokens, maintains near-lossless reasoning\naccuracy with 4K token budget in AIME benchmark under large sparse attention\nblock sizes (64/128). Using TileLang, we develop a highly optimized sparse\ndecoding kernel that achieves near-theoretical speedups of up to 9x over\nFlashAttention-3 on H100 GPU at 90% sparsity. Code is available at:\nhttps://github.com/microsoft/SeerAttention.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08889.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "661c96f48921f03a9dae04c3",
      "avatarUrl": "/avatars/c486a45ffea7d1a8c72bd8512014b07e.svg",
      "fullname": "Yizhao Gao",
      "name": "LongMountain",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.09984",
      "authors": [
        {
          "_id": "684a49fa9b38e1e5a33a6884",
          "name": "Zhenzhi Wang",
          "hidden": false
        },
        {
          "_id": "684a49fa9b38e1e5a33a6885",
          "name": "Jiaqi Yang",
          "hidden": false
        },
        {
          "_id": "684a49fa9b38e1e5a33a6886",
          "name": "Jianwen Jiang",
          "hidden": false
        },
        {
          "_id": "684a49fa9b38e1e5a33a6887",
          "name": "Chao Liang",
          "hidden": false
        },
        {
          "_id": "684a49fa9b38e1e5a33a6888",
          "name": "Gaojie Lin",
          "hidden": false
        },
        {
          "_id": "684a49fa9b38e1e5a33a6889",
          "name": "Zerong Zheng",
          "hidden": false
        },
        {
          "_id": "684a49fa9b38e1e5a33a688a",
          "name": "Ceyuan Yang",
          "hidden": false
        },
        {
          "_id": "684a49fa9b38e1e5a33a688b",
          "name": "Dahua Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-11T17:57:09.000Z",
      "submittedOnDailyAt": "2025-06-12T02:02:32.983Z",
      "title": "InterActHuman: Multi-Concept Human Animation with Layout-Aligned Audio\n  Conditions",
      "submittedOnDailyBy": {
        "_id": "6519346a186bc3b6997c1aaf",
        "avatarUrl": "/avatars/8981bd278962da50f9bbfb92c2abe2bf.svg",
        "isPro": false,
        "fullname": "Zhenzhi Wang",
        "user": "zhenzhiwang",
        "type": "user"
      },
      "summary": "End-to-end human animation with rich multi-modal conditions, e.g., text,\nimage and audio has achieved remarkable advancements in recent years. However,\nmost existing methods could only animate a single subject and inject conditions\nin a global manner, ignoring scenarios that multiple concepts could appears in\nthe same video with rich human-human interactions and human-object\ninteractions. Such global assumption prevents precise and per-identity control\nof multiple concepts including humans and objects, therefore hinders\napplications. In this work, we discard the single-entity assumption and\nintroduce a novel framework that enforces strong, region-specific binding of\nconditions from modalities to each identity's spatiotemporal footprint. Given\nreference images of multiple concepts, our method could automatically infer\nlayout information by leveraging a mask predictor to match appearance cues\nbetween the denoised video and each reference appearance. Furthermore, we\ninject local audio condition into its corresponding region to ensure\nlayout-aligned modality matching in a iterative manner. This design enables the\nhigh-quality generation of controllable multi-concept human-centric videos.\nEmpirical results and ablation studies validate the effectiveness of our\nexplicit layout control for multi-modal conditions compared to implicit\ncounterparts and other existing methods.",
      "upvotes": 5,
      "discussionId": "684a49fa9b38e1e5a33a688c",
      "ai_summary": "A new framework enables precise, per-identity control of multiple concepts in end-to-end human animation by enforcing region-specific binding of multi-modal conditions.",
      "ai_keywords": [
        "human animation",
        "multi-modal conditions",
        "mask predictor",
        "denoised video",
        "layout information",
        "local audio condition",
        "controllable multi-concept videos",
        "explicit layout control"
      ]
    },
    "publishedAt": "2025-06-11T13:57:09.000Z",
    "title": "InterActHuman: Multi-Concept Human Animation with Layout-Aligned Audio\n  Conditions",
    "summary": "End-to-end human animation with rich multi-modal conditions, e.g., text,\nimage and audio has achieved remarkable advancements in recent years. However,\nmost existing methods could only animate a single subject and inject conditions\nin a global manner, ignoring scenarios that multiple concepts could appears in\nthe same video with rich human-human interactions and human-object\ninteractions. Such global assumption prevents precise and per-identity control\nof multiple concepts including humans and objects, therefore hinders\napplications. In this work, we discard the single-entity assumption and\nintroduce a novel framework that enforces strong, region-specific binding of\nconditions from modalities to each identity's spatiotemporal footprint. Given\nreference images of multiple concepts, our method could automatically infer\nlayout information by leveraging a mask predictor to match appearance cues\nbetween the denoised video and each reference appearance. Furthermore, we\ninject local audio condition into its corresponding region to ensure\nlayout-aligned modality matching in a iterative manner. This design enables the\nhigh-quality generation of controllable multi-concept human-centric videos.\nEmpirical results and ablation studies validate the effectiveness of our\nexplicit layout control for multi-modal conditions compared to implicit\ncounterparts and other existing methods.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09984.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6519346a186bc3b6997c1aaf",
      "avatarUrl": "/avatars/8981bd278962da50f9bbfb92c2abe2bf.svg",
      "fullname": "Zhenzhi Wang",
      "name": "zhenzhiwang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.09937",
      "authors": [
        {
          "_id": "684a3d639b38e1e5a33a686d",
          "name": "Qiao Gu",
          "hidden": false
        },
        {
          "_id": "684a3d639b38e1e5a33a686e",
          "name": "Yuanliang Ju",
          "hidden": false
        },
        {
          "_id": "684a3d639b38e1e5a33a686f",
          "name": "Shengxiang Sun",
          "hidden": false
        },
        {
          "_id": "684a3d639b38e1e5a33a6870",
          "name": "Igor Gilitschenski",
          "hidden": false
        },
        {
          "_id": "684a3d639b38e1e5a33a6871",
          "name": "Haruki Nishimura",
          "hidden": false
        },
        {
          "_id": "684a3d639b38e1e5a33a6872",
          "name": "Masha Itkina",
          "hidden": false
        },
        {
          "_id": "684a3d639b38e1e5a33a6873",
          "name": "Florian Shkurti",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-11T16:59:13.000Z",
      "submittedOnDailyAt": "2025-06-12T01:08:05.216Z",
      "title": "SAFE: Multitask Failure Detection for Vision-Language-Action Models",
      "submittedOnDailyBy": {
        "_id": "63d1df92f7f31a66a2d7292c",
        "avatarUrl": "/avatars/eb3339c1f2c82742b518b8a7f142e99a.svg",
        "isPro": false,
        "fullname": "Qiao Gu",
        "user": "guqiao",
        "type": "user"
      },
      "summary": "While vision-language-action models (VLAs) have shown promising robotic\nbehaviors across a diverse set of manipulation tasks, they achieve limited\nsuccess rates when deployed on novel tasks out-of-the-box. To allow these\npolicies to safely interact with their environments, we need a failure detector\nthat gives a timely alert such that the robot can stop, backtrack, or ask for\nhelp. However, existing failure detectors are trained and tested only on one or\na few specific tasks, while VLAs require the detector to generalize and detect\nfailures also in unseen tasks and novel environments. In this paper, we\nintroduce the multitask failure detection problem and propose SAFE, a failure\ndetector for generalist robot policies such as VLAs. We analyze the VLA feature\nspace and find that VLAs have sufficient high-level knowledge about task\nsuccess and failure, which is generic across different tasks. Based on this\ninsight, we design SAFE to learn from VLA internal features and predict a\nsingle scalar indicating the likelihood of task failure. SAFE is trained on\nboth successful and failed rollouts, and is evaluated on unseen tasks. SAFE is\ncompatible with different policy architectures. We test it on OpenVLA, pi_0,\nand pi_0-FAST in both simulated and real-world environments extensively. We\ncompare SAFE with diverse baselines and show that SAFE achieves\nstate-of-the-art failure detection performance and the best trade-off between\naccuracy and detection time using conformal prediction. More qualitative\nresults can be found at https://vla-safe.github.io/.",
      "upvotes": 2,
      "discussionId": "684a3d639b38e1e5a33a6874",
      "ai_summary": "SAFE is a failure detector for vision-language-action models that generalizes to unseen tasks by learning from high-level internal features of the models.",
      "ai_keywords": [
        "vision-language-action models",
        "VLAs",
        "multitask failure detection",
        "failure detector",
        "feature space",
        "high-level knowledge",
        "scalar prediction",
        "rollout",
        "conformal prediction"
      ]
    },
    "publishedAt": "2025-06-11T12:59:13.000Z",
    "title": "SAFE: Multitask Failure Detection for Vision-Language-Action Models",
    "summary": "While vision-language-action models (VLAs) have shown promising robotic\nbehaviors across a diverse set of manipulation tasks, they achieve limited\nsuccess rates when deployed on novel tasks out-of-the-box. To allow these\npolicies to safely interact with their environments, we need a failure detector\nthat gives a timely alert such that the robot can stop, backtrack, or ask for\nhelp. However, existing failure detectors are trained and tested only on one or\na few specific tasks, while VLAs require the detector to generalize and detect\nfailures also in unseen tasks and novel environments. In this paper, we\nintroduce the multitask failure detection problem and propose SAFE, a failure\ndetector for generalist robot policies such as VLAs. We analyze the VLA feature\nspace and find that VLAs have sufficient high-level knowledge about task\nsuccess and failure, which is generic across different tasks. Based on this\ninsight, we design SAFE to learn from VLA internal features and predict a\nsingle scalar indicating the likelihood of task failure. SAFE is trained on\nboth successful and failed rollouts, and is evaluated on unseen tasks. SAFE is\ncompatible with different policy architectures. We test it on OpenVLA, pi_0,\nand pi_0-FAST in both simulated and real-world environments extensively. We\ncompare SAFE with diverse baselines and show that SAFE achieves\nstate-of-the-art failure detection performance and the best trade-off between\naccuracy and detection time using conformal prediction. More qualitative\nresults can be found at https://vla-safe.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09937.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63d1df92f7f31a66a2d7292c",
      "avatarUrl": "/avatars/eb3339c1f2c82742b518b8a7f142e99a.svg",
      "fullname": "Qiao Gu",
      "name": "guqiao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.08001",
      "authors": [
        {
          "_id": "684a649f9b38e1e5a33a6895",
          "name": "Zeju Qiu",
          "hidden": false
        },
        {
          "_id": "684a649f9b38e1e5a33a6896",
          "name": "Simon Buchholz",
          "hidden": false
        },
        {
          "_id": "684a649f9b38e1e5a33a6897",
          "name": "Tim Z. Xiao",
          "hidden": false
        },
        {
          "_id": "684a649f9b38e1e5a33a6898",
          "name": "Maximilian Dax",
          "hidden": false
        },
        {
          "_id": "684a649f9b38e1e5a33a6899",
          "name": "Bernhard Schlkopf",
          "hidden": false
        },
        {
          "_id": "684a649f9b38e1e5a33a689a",
          "name": "Weiyang Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-09T17:59:34.000Z",
      "submittedOnDailyAt": "2025-06-12T03:55:47.829Z",
      "title": "Reparameterized LLM Training via Orthogonal Equivalence Transformation",
      "submittedOnDailyBy": {
        "_id": "648905d1a15c43c791d4381f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648905d1a15c43c791d4381f/GpqGBzsLiMHX0gWZEz3qn.jpeg",
        "isPro": false,
        "fullname": "Weiyang Liu",
        "user": "wy1iu",
        "type": "user"
      },
      "summary": "While large language models (LLMs) are driving the rapid advancement of\nartificial intelligence, effectively and reliably training these large models\nremains one of the field's most significant challenges. To address this\nchallenge, we propose POET, a novel reParameterized training algorithm that\nuses Orthogonal Equivalence Transformation to optimize neurons. Specifically,\nPOET reparameterizes each neuron with two learnable orthogonal matrices and a\nfixed random weight matrix. Because of its provable preservation of spectral\nproperties of weight matrices, POET can stably optimize the objective function\nwith improved generalization. We further develop efficient approximations that\nmake POET flexible and scalable for training large-scale neural networks.\nExtensive experiments validate the effectiveness and scalability of POET in\ntraining LLMs.",
      "upvotes": 1,
      "discussionId": "684a649f9b38e1e5a33a689b",
      "projectPage": "https://spherelab.ai/poet/",
      "githubRepo": "https://github.com/Sphere-AI-Lab/poet",
      "ai_summary": "POET is a reParameterized training algorithm using Orthogonal Equivalence Transformation to optimize neurons in large language models, ensuring stable training and improved generalization.",
      "ai_keywords": [
        "POET",
        "reParameterized training algorithm",
        "Orthogonal Equivalence Transformation",
        "orthogonal matrices",
        "spectral properties",
        "weight matrices",
        "large language models",
        "generalization",
        "efficient approximations",
        "training",
        "scalability"
      ]
    },
    "publishedAt": "2025-06-09T13:59:34.000Z",
    "title": "Reparameterized LLM Training via Orthogonal Equivalence Transformation",
    "summary": "While large language models (LLMs) are driving the rapid advancement of\nartificial intelligence, effectively and reliably training these large models\nremains one of the field's most significant challenges. To address this\nchallenge, we propose POET, a novel reParameterized training algorithm that\nuses Orthogonal Equivalence Transformation to optimize neurons. Specifically,\nPOET reparameterizes each neuron with two learnable orthogonal matrices and a\nfixed random weight matrix. Because of its provable preservation of spectral\nproperties of weight matrices, POET can stably optimize the objective function\nwith improved generalization. We further develop efficient approximations that\nmake POET flexible and scalable for training large-scale neural networks.\nExtensive experiments validate the effectiveness and scalability of POET in\ntraining LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08001.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648905d1a15c43c791d4381f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648905d1a15c43c791d4381f/GpqGBzsLiMHX0gWZEz3qn.jpeg",
      "fullname": "Weiyang Liu",
      "name": "wy1iu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.09007",
      "authors": [
        {
          "_id": "6849516c42e4f9106973f4d1",
          "name": "Sophia Tang",
          "hidden": false
        },
        {
          "_id": "6849516c42e4f9106973f4d2",
          "name": "Yinuo Zhang",
          "hidden": false
        },
        {
          "_id": "6849516c42e4f9106973f4d3",
          "name": "Alexander Tong",
          "hidden": false
        },
        {
          "_id": "6849516c42e4f9106973f4d4",
          "name": "Pranam Chatterjee",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-10T17:29:48.000Z",
      "submittedOnDailyAt": "2025-06-12T01:32:31.298Z",
      "title": "Branched Schrdinger Bridge Matching",
      "submittedOnDailyBy": {
        "_id": "64cd5b3f0494187a9e8b7c69",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eo9HiGbvCxHQZ-QJB1Y_o.jpeg",
        "isPro": false,
        "fullname": "Pranam Chatterjee",
        "user": "pranamanam",
        "type": "user"
      },
      "summary": "Predicting the intermediate trajectories between an initial and target\ndistribution is a central problem in generative modeling. Existing approaches,\nsuch as flow matching and Schr\\\"odinger Bridge Matching, effectively learn\nmappings between two distributions by modeling a single stochastic path.\nHowever, these methods are inherently limited to unimodal transitions and\ncannot capture branched or divergent evolution from a common origin to multiple\ndistinct outcomes. To address this, we introduce Branched Schr\\\"odinger Bridge\nMatching (BranchSBM), a novel framework that learns branched Schr\\\"odinger\nbridges. BranchSBM parameterizes multiple time-dependent velocity fields and\ngrowth processes, enabling the representation of population-level divergence\ninto multiple terminal distributions. We show that BranchSBM is not only more\nexpressive but also essential for tasks involving multi-path surface\nnavigation, modeling cell fate bifurcations from homogeneous progenitor states,\nand simulating diverging cellular responses to perturbations.",
      "upvotes": 0,
      "discussionId": "6849516d42e4f9106973f4d5",
      "ai_summary": "BranchSBM, a novel generative modeling framework, extends Schr\\\"odinger Bridge Matching to model branched stochastic paths and multi-path evolution from a single initial distribution to multiple outcomes.",
      "ai_keywords": [
        "flow matching",
        "Schr\\\"odinger Bridge Matching",
        "Branched Schr\\\"odinger Bridge Matching",
        "BranchSBM",
        "time-dependent velocity fields",
        "growth processes",
        "multi-path surface navigation",
        "cell fate bifurcations",
        "cellular responses to perturbations"
      ]
    },
    "publishedAt": "2025-06-10T13:29:48.000Z",
    "title": "Branched Schrdinger Bridge Matching",
    "summary": "Predicting the intermediate trajectories between an initial and target\ndistribution is a central problem in generative modeling. Existing approaches,\nsuch as flow matching and Schr\\\"odinger Bridge Matching, effectively learn\nmappings between two distributions by modeling a single stochastic path.\nHowever, these methods are inherently limited to unimodal transitions and\ncannot capture branched or divergent evolution from a common origin to multiple\ndistinct outcomes. To address this, we introduce Branched Schr\\\"odinger Bridge\nMatching (BranchSBM), a novel framework that learns branched Schr\\\"odinger\nbridges. BranchSBM parameterizes multiple time-dependent velocity fields and\ngrowth processes, enabling the representation of population-level divergence\ninto multiple terminal distributions. We show that BranchSBM is not only more\nexpressive but also essential for tasks involving multi-path surface\nnavigation, modeling cell fate bifurcations from homogeneous progenitor states,\nand simulating diverging cellular responses to perturbations.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09007.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64cd5b3f0494187a9e8b7c69",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eo9HiGbvCxHQZ-QJB1Y_o.jpeg",
      "fullname": "Pranam Chatterjee",
      "name": "pranamanam",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  }
]