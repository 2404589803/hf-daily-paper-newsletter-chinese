[
    "{'paper': {'id': '2412.13147', 'authors': [{'_id': '67623bb9d44ba09e9119fe12', 'user': {'_id': '643d26979347842571bc9613', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/3heFf7h3jbhhJWJ4JfGfh.jpeg', 'isPro': False, 'fullname': 'Junnan Liu', 'user': 'jnanliu', 'type': 'user'}, 'name': 'Junnan Liu', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-18T12:55:23.904Z', 'hidden': False}, {'_id': '67623bb9d44ba09e9119fe13', 'name': 'Hongwei Liu', 'hidden': False}, {'_id': '67623bb9d44ba09e9119fe14', 'user': {'_id': '64f58f279eaf9d8fb746cd0e', 'avatarUrl': '/avatars/227db72e089026012694ff16cdf102d0.svg', 'isPro': False, 'fullname': 'LinchenXiao', 'user': 'LinchenXiao', 'type': 'user'}, 'name': 'Linchen Xiao', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-18T12:55:51.678Z', 'hidden': False}, {'_id': '67623bb9d44ba09e9119fe15', 'name': 'Ziyi Wang', 'hidden': False}, {'_id': '67623bb9d44ba09e9119fe16', 'user': {'_id': '63fd691794cc8f815d50c112', 'avatarUrl': '/avatars/87305d1cbfcc717e910ccdfaf0568f80.svg', 'isPro': False, 'fullname': 'liu', 'user': 'Harold-lkk', 'type': 'user'}, 'name': 'Kuikun Liu', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-18T09:52:14.336Z', 'hidden': False}, {'_id': '67623bb9d44ba09e9119fe17', 'user': {'_id': '650ab54e23196fb2d86b486b', 'avatarUrl': '/avatars/e0506393589695b553ec9ee3fe99b93a.svg', 'isPro': False, 'fullname': 'SongYang Gao', 'user': 'Wizardcoast', 'type': 'user'}, 'name': 'Songyang Gao', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-18T12:56:48.903Z', 'hidden': False}, {'_id': '67623bb9d44ba09e9119fe18', 'user': {'_id': '64e8505321540e1da3226b54', 'avatarUrl': '/avatars/18958b8406d1ce492b54c1c839f18c54.svg', 'isPro': False, 'fullname': 'Wenwei Zhang', 'user': 'ZwwWayne', 'type': 'user'}, 'name': 'Wenwei Zhang', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-18T12:56:56.862Z', 'hidden': False}, {'_id': '67623bb9d44ba09e9119fe19', 'user': {'_id': '630716d11801ecc7d2595021', 'avatarUrl': '/avatars/2d36a880ce4a3cf7efc5ff3987dbeaf3.svg', 'isPro': False, 'fullname': 'Songyang Zhang', 'user': 'zsytony', 'type': 'user'}, 'name': 'Songyang Zhang', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-18T09:52:35.610Z', 'hidden': False}, {'_id': '67623bb9d44ba09e9119fe1a', 'name': 'Kai Chen', 'hidden': False}], 'publishedAt': '2024-12-17T18:12:47.000Z', 'title': 'Are Your LLMs Capable of Stable Reasoning?', 'summary': 'The rapid advancement of Large Language Models (LLMs) has demonstrated\\nremarkable progress in complex reasoning tasks. However, a significant\\ndiscrepancy persists between benchmark performances and real-world\\napplications. We identify this gap as primarily stemming from current\\nevaluation protocols and metrics, which inadequately capture the full spectrum\\nof LLM capabilities, particularly in complex reasoning tasks where both\\naccuracy and consistency are crucial. This work makes two key contributions.\\nFirst, we introduce G-Pass@k, a novel evaluation metric that provides a\\ncontinuous assessment of model performance across multiple sampling attempts,\\nquantifying both the model\\'s peak performance potential and its stability.\\nSecond, we present LiveMathBench, a dynamic benchmark comprising challenging,\\ncontemporary mathematical problems designed to minimize data leakage risks\\nduring evaluation. Through extensive experiments using G-Pass@k on\\nstate-of-the-art LLMs with LiveMathBench, we provide comprehensive insights\\ninto both their maximum capabilities and operational consistency. Our findings\\nreveal substantial room for improvement in LLMs\\' \"realistic\" reasoning\\ncapabilities, highlighting the need for more robust evaluation methods. The\\nbenchmark and detailed results are available at:\\nhttps://github.com/open-compass/GPassK.', 'upvotes': 55, 'discussionId': '67623bb9d44ba09e9119fe50'}, 'publishedAt': '2024-12-17T22:05:04.201Z', 'title': 'Are Your LLMs Capable of Stable Reasoning?', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.13147.png', 'numComments': 1, 'submittedBy': {'_id': '630716d11801ecc7d2595021', 'avatarUrl': '/avatars/2d36a880ce4a3cf7efc5ff3987dbeaf3.svg', 'fullname': 'Songyang Zhang', 'name': 'zsytony', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 13}}",
    "{'paper': {'id': '2412.12606', 'authors': [{'_id': '676240aeb57a82f47d1a81f2', 'name': 'YiFan Zhang', 'hidden': False}, {'_id': '676240aeb57a82f47d1a81f3', 'name': 'Shanglin Lei', 'hidden': False}, {'_id': '676240aeb57a82f47d1a81f4', 'name': 'Runqi Qiao', 'hidden': False}, {'_id': '676240aeb57a82f47d1a81f5', 'name': 'Zhuoma GongQue', 'hidden': False}, {'_id': '676240aeb57a82f47d1a81f6', 'name': 'Xiaoshuai Song', 'hidden': False}, {'_id': '676240aeb57a82f47d1a81f7', 'user': {'_id': '61cd4b833dd34ba1985e0753', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/61cd4b833dd34ba1985e0753/BfHfrwotoMESpXZOHiIe4.png', 'isPro': False, 'fullname': 'KABI', 'user': 'dongguanting', 'type': 'user'}, 'name': 'Guanting Dong', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-18T12:53:40.027Z', 'hidden': False}, {'_id': '676240aeb57a82f47d1a81f8', 'user': {'_id': '66615dd1c8bbe034b4b82029', 'avatarUrl': '/avatars/7ef1ddf2fa356efbeb2b3aa12284a1f0.svg', 'isPro': False, 'fullname': 'Zhang YiFan', 'user': 'zhangyifan666', 'type': 'user'}, 'name': 'Qiuna Tan', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-18T14:32:33.953Z', 'hidden': False}, {'_id': '676240aeb57a82f47d1a81f9', 'name': 'Zhe Wei', 'hidden': False}, {'_id': '676240aeb57a82f47d1a81fa', 'user': {'_id': '6513aae6330c55fdc5462ca8', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/EDhpiTqCBMNPmMGrOKcvY.jpeg', 'isPro': False, 'fullname': 'pq-yang', 'user': 'PeiqingYang', 'type': 'user'}, 'name': 'Peiqing Yang', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-18T12:52:52.896Z', 'hidden': False}, {'_id': '676240aeb57a82f47d1a81fb', 'name': 'Ye Tian', 'hidden': False}, {'_id': '676240aeb57a82f47d1a81fc', 'user': {'_id': '64d4b5808b65d477e68f2fba', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/upUtVjLXZ9lAulngeFh_i.jpeg', 'isPro': False, 'fullname': 'Xue Yadong', 'user': 'ataraxy3', 'type': 'user'}, 'name': 'Yadong Xue', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-18T12:52:43.539Z', 'hidden': False}, {'_id': '676240aeb57a82f47d1a81fd', 'name': 'Xiaofei Wang', 'hidden': False}, {'_id': '676240aeb57a82f47d1a81fe', 'name': 'Honggang Zhang', 'hidden': False}], 'publishedAt': '2024-12-17T07:06:10.000Z', 'title': 'Multi-Dimensional Insights: Benchmarking Real-World Personalization in\\n  Large Multimodal Models', 'summary': \"The rapidly developing field of large multimodal models (LMMs) has led to the\\nemergence of diverse models with remarkable capabilities. However, existing\\nbenchmarks fail to comprehensively, objectively and accurately evaluate whether\\nLMMs align with the diverse needs of humans in real-world scenarios. To bridge\\nthis gap, we propose the Multi-Dimensional Insights (MDI) benchmark, which\\nincludes over 500 images covering six common scenarios of human life. Notably,\\nthe MDI-Benchmark offers two significant advantages over existing evaluations:\\n(1) Each image is accompanied by two types of questions: simple questions to\\nassess the model's understanding of the image, and complex questions to\\nevaluate the model's ability to analyze and reason beyond basic content. (2)\\nRecognizing that people of different age groups have varying needs and\\nperspectives when faced with the same scenario, our benchmark stratifies\\nquestions into three age categories: young people, middle-aged people, and\\nolder people. This design allows for a detailed assessment of LMMs'\\ncapabilities in meeting the preferences and needs of different age groups. With\\nMDI-Benchmark, the strong model like GPT-4o achieve 79% accuracy on age-related\\ntasks, indicating that existing LMMs still have considerable room for\\nimprovement in addressing real-world applications. Looking ahead, we anticipate\\nthat the MDI-Benchmark will open new pathways for aligning real-world\\npersonalization in LMMs. The MDI-Benchmark data and evaluation code are\\navailable at https://mdi-benchmark.github.io/\", 'upvotes': 28, 'discussionId': '676240b0b57a82f47d1a8254'}, 'publishedAt': '2024-12-17T22:50:03.244Z', 'title': 'Multi-Dimensional Insights: Benchmarking Real-World Personalization in Large Multimodal Models', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.12606.png', 'numComments': 2, 'submittedBy': {'_id': '6683a05e74fb1736a4b7c934', 'avatarUrl': '/avatars/35292253cf1882080be353c4b8c9ca3b.svg', 'fullname': 'QRQ', 'name': 'RichardQRQ', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 1}}",
    "{'paper': {'id': '2412.13018', 'authors': [{'_id': '67625f697fa2dfcfa4a26eb1', 'user': {'_id': '66d81fe818e470c7312c0f2a', 'avatarUrl': '/avatars/27dd2331c1193220d6ef5bc153419a12.svg', 'isPro': False, 'fullname': 'ShutingWang', 'user': 'ShootingWong', 'type': 'user'}, 'name': 'Shuting Wang', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-18T09:51:53.983Z', 'hidden': False}, {'_id': '67625f697fa2dfcfa4a26eb2', 'user': {'_id': '62e52483a944e2a56cd2c6ca', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/62e52483a944e2a56cd2c6ca/pG44O-1qD00q5CEJMMyFQ.jpeg', 'isPro': False, 'fullname': 'Jiejun Tan', 'user': 'zstanjj', 'type': 'user'}, 'name': 'Jiejun Tan', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-18T12:38:31.592Z', 'hidden': False}, {'_id': '67625f697fa2dfcfa4a26eb3', 'user': {'_id': '66f0bf59e9d50ec57febf751', 'avatarUrl': '/avatars/be97941e60064e5dd806c6fe9db3c537.svg', 'isPro': False, 'fullname': 'Zhicheng Dou', 'user': 'douzc', 'type': 'user'}, 'name': 'Zhicheng Dou', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-18T12:52:00.291Z', 'hidden': False}, {'_id': '67625f697fa2dfcfa4a26eb4', 'user': {'_id': '64b8c89052b7353d8c6a1013', 'avatarUrl': '/avatars/cd59fffe81f6b07b4519540b8ff3d95f.svg', 'isPro': False, 'fullname': 'Ji-Rong Wen', 'user': 'jrwen', 'type': 'user'}, 'name': 'Ji-Rong Wen', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-18T12:52:06.433Z', 'hidden': False}], 'publishedAt': '2024-12-17T15:38:42.000Z', 'title': 'OmniEval: An Omnidirectional and Automatic RAG Evaluation Benchmark in\\n  Financial Domain', 'summary': 'As a typical and practical application of Large Language Models (LLMs),\\nRetrieval-Augmented Generation (RAG) techniques have gained extensive\\nattention, particularly in vertical domains where LLMs may lack domain-specific\\nknowledge. In this paper, we introduce an omnidirectional and automatic RAG\\nbenchmark, OmniEval, in the financial domain. Our benchmark is characterized by\\nits multi-dimensional evaluation framework, including (1) a matrix-based RAG\\nscenario evaluation system that categorizes queries into five task classes and\\n16 financial topics, leading to a structured assessment of diverse query\\nscenarios; (2) a multi-dimensional evaluation data generation approach, which\\ncombines GPT-4-based automatic generation and human annotation, achieving an\\n87.47\\\\% acceptance ratio in human evaluations on generated instances; (3) a\\nmulti-stage evaluation system that evaluates both retrieval and generation\\nperformance, result in a comprehensive evaluation on the RAG pipeline; and (4)\\nrobust evaluation metrics derived from rule-based and LLM-based ones, enhancing\\nthe reliability of assessments through manual annotations and supervised\\nfine-tuning of an LLM evaluator. Our experiments demonstrate the\\ncomprehensiveness of OmniEval, which includes extensive test datasets and\\nhighlights the performance variations of RAG systems across diverse topics and\\ntasks, revealing significant opportunities for RAG models to improve their\\ncapabilities in vertical domains. We open source the code of our benchmark in\\nhttps://github.com/RUC-NLPIR/OmniEval{https://github.com/RUC-NLPIR/OmniEval}.', 'upvotes': 27, 'discussionId': '67625f6a7fa2dfcfa4a26eef'}, 'publishedAt': '2024-12-18T00:39:03.356Z', 'title': 'OmniEval: An Omnidirectional and Automatic RAG Evaluation Benchmark in Financial Domain', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.13018.png', 'numComments': 1, 'submittedBy': {'_id': '62e52483a944e2a56cd2c6ca', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/62e52483a944e2a56cd2c6ca/pG44O-1qD00q5CEJMMyFQ.jpeg', 'fullname': 'Jiejun Tan', 'name': 'zstanjj', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 6}}",
    "{'paper': {'id': '2412.13171', 'authors': [{'_id': '6762f21ea3eed649f9889a1c', 'user': {'_id': '65f28eebf5cf26fe0632ce67', 'avatarUrl': '/avatars/cd970cfe4215374c82d47df57ac30795.svg', 'isPro': False, 'fullname': 'Jeffrey Cheng', 'user': 'nexync', 'type': 'user'}, 'name': 'Jeffrey Cheng', 'status': 'extracted_pending', 'statusLastChangedAt': '2024-12-18T16:02:39.080Z', 'hidden': False}, {'_id': '6762f21ea3eed649f9889a1d', 'name': 'Benjamin Van Durme', 'hidden': False}], 'publishedAt': '2024-12-17T18:50:33.000Z', 'title': 'Compressed Chain of Thought: Efficient Reasoning Through Dense\\n  Representations', 'summary': 'Chain-of-thought (CoT) decoding enables language models to improve reasoning\\nperformance at the cost of high generation latency in decoding. Recent\\nproposals have explored variants of contemplation tokens, a term we introduce\\nthat refers to special tokens used during inference to allow for extra\\ncomputation. Prior work has considered fixed-length sequences drawn from a\\ndiscrete set of embeddings as contemplation tokens. Here we propose Compressed\\nChain-of-Thought (CCoT), a framework to generate contentful and continuous\\ncontemplation tokens of variable sequence length. The generated contemplation\\ntokens are compressed representations of explicit reasoning chains, and our\\nmethod can be applied to off-the-shelf decoder language models. Through\\nexperiments, we illustrate how CCoT enables additional reasoning over dense\\ncontentful representations to achieve corresponding improvements in accuracy.\\nMoreover, the reasoning improvements can be adaptively modified on demand by\\ncontrolling the number of contemplation tokens generated.', 'upvotes': 8, 'discussionId': '6762f21fa3eed649f9889a49'}, 'publishedAt': '2024-12-18T11:03:46.400Z', 'title': 'Compressed Chain of Thought: Efficient Reasoning Through Dense Representations', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.13171.png', 'numComments': 1, 'submittedBy': {'_id': '65f28eebf5cf26fe0632ce67', 'avatarUrl': '/avatars/cd970cfe4215374c82d47df57ac30795.svg', 'fullname': 'Jeffrey Cheng', 'name': 'nexync', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}}",
    "{'paper': {'id': '2412.12276', 'authors': [{'_id': '67623311673b665026a73ee9', 'user': {'_id': '6520d6db2a16045c092b3b36', 'avatarUrl': '/avatars/dab34f141a1aef39d00c789ff85e729f.svg', 'isPro': False, 'fullname': 'Seungwook Han', 'user': 'hanseungwook', 'type': 'user'}, 'name': 'Seungwook Han', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-18T12:51:02.455Z', 'hidden': False}, {'_id': '67623311673b665026a73eea', 'user': {'_id': '65e37f6890a87c0f26d345da', 'avatarUrl': '/avatars/884f645b2adde7ff3fe8689ecd5a2535.svg', 'isPro': False, 'fullname': 'Jinyeop Song', 'user': 'Jinyeop', 'type': 'user'}, 'name': 'Jinyeop Song', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-18T12:51:08.296Z', 'hidden': False}, {'_id': '67623311673b665026a73eeb', 'name': 'Jeff Gore', 'hidden': False}, {'_id': '67623311673b665026a73eec', 'name': 'Pulkit Agrawal', 'hidden': False}], 'publishedAt': '2024-12-16T19:00:18.000Z', 'title': 'Emergence of Abstractions: Concept Encoding and Decoding Mechanism for\\n  In-Context Learning in Transformers', 'summary': 'Humans distill complex experiences into fundamental abstractions that enable\\nrapid learning and adaptation. Similarly, autoregressive transformers exhibit\\nadaptive learning through in-context learning (ICL), which begs the question of\\nhow. In this paper, we propose concept encoding-decoding mechanism to\\nexplain ICL by studying how transformers form and use internal abstractions in\\ntheir representations. On synthetic ICL tasks, we analyze the training dynamics\\nof a small transformer and report the coupled emergence of concept encoding and\\ndecoding. As the model learns to encode different latent concepts (e.g.,\\n``Finding the first noun in a sentence.\") into distinct, separable\\nrepresentations, it concureently builds conditional decoding algorithms and\\nimprove its ICL performance. We validate the existence of this mechanism across\\npretrained models of varying scales (Gemma-2 2B/9B/27B, Llama-3.1 8B/70B).\\nFurther, through mechanistic interventions and controlled finetuning, we\\ndemonstrate that the quality of concept encoding is causally related and\\npredictive of ICL performance. Our empirical insights shed light into better\\nunderstanding the success and failure modes of large language models via their\\nrepresentations.', 'upvotes': 5, 'discussionId': '67623312673b665026a73f27'}, 'publishedAt': '2024-12-18T01:04:36.583Z', 'title': 'Emergence of Abstractions: Concept Encoding and Decoding Mechanism for In-Context Learning in Transformers', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.12276.png', 'numComments': 1, 'submittedBy': {'_id': '6520d6db2a16045c092b3b36', 'avatarUrl': '/avatars/dab34f141a1aef39d00c789ff85e729f.svg', 'fullname': 'Seungwook Han', 'name': 'hanseungwook', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}}",
    "{'paper': {'id': '2412.13180', 'authors': [{'_id': '6762382699046e8f14ef8319', 'name': 'Mark Endo', 'hidden': False}, {'_id': '6762382699046e8f14ef831a', 'name': 'Xiaohan Wang', 'hidden': False}, {'_id': '6762382699046e8f14ef831b', 'name': 'Serena Yeung-Levy', 'hidden': False}], 'publishedAt': '2024-12-17T18:56:50.000Z', 'title': 'Feather the Throttle: Revisiting Visual Token Pruning for\\n  Vision-Language Model Acceleration', 'summary': \"Recent works on accelerating Vision-Language Models show that strong\\nperformance can be maintained across a variety of vision-language tasks despite\\nhighly compressing visual information. In this work, we examine the popular\\nacceleration approach of early pruning of visual tokens inside the language\\nmodel and find that its strong performance across many tasks is not due to an\\nexceptional ability to compress visual information, but rather the benchmarks'\\nlimited ability to assess fine-grained visual capabilities. Namely, we\\ndemonstrate a core issue with the acceleration approach where most tokens\\ntowards the top of the image are pruned away. Yet, this issue is only reflected\\nin performance for a small subset of tasks such as localization. For the other\\nevaluated tasks, strong performance is maintained with the flawed pruning\\nstrategy. Noting the limited visual capabilities of the studied acceleration\\ntechnique, we propose FEATHER (Fast and Effective Acceleration wiTH Ensemble\\ncRiteria), a straightforward approach that (1) resolves the identified issue\\nwith early-layer pruning, (2) incorporates uniform sampling to ensure coverage\\nacross all image regions, and (3) applies pruning in two stages to allow the\\ncriteria to become more effective at a later layer while still achieving\\nsignificant speedup through early-layer pruning. With comparable computational\\nsavings, we find that FEATHER has more than 5times performance improvement\\non the vision-centric localization benchmarks compared to the original\\nacceleration approach.\", 'upvotes': 2, 'discussionId': '6762382799046e8f14ef83bd'}, 'publishedAt': '2024-12-18T10:50:49.638Z', 'title': 'Feather the Throttle: Revisiting Visual Token Pruning for Vision-Language Model Acceleration', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/65c4287b6b3715a9cf28ded9/5agCZ8GMvq9j1yxHcC07o.mp4'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.13180.png', 'numComments': 1, 'submittedBy': {'_id': '65c4287b6b3715a9cf28ded9', 'avatarUrl': '/avatars/2af7d2c64665cb5283398084628c1701.svg', 'fullname': 'Mark Endo', 'name': 'markendo', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}}",
    "{'paper': {'id': '2412.13194', 'authors': [{'_id': '6762d6291d6d92e318dfb0f4', 'name': 'Yifei Zhou', 'hidden': False}, {'_id': '6762d6291d6d92e318dfb0f5', 'user': {'_id': '662c11c4d64b8a3bee132130', 'avatarUrl': '/avatars/6c669c1d175bfc582e93f02c95b18f1e.svg', 'isPro': False, 'fullname': 'Qianlan Yang', 'user': 'yanQval', 'type': 'user'}, 'name': 'Qianlan Yang', 'status': 'extracted_pending', 'statusLastChangedAt': '2024-12-18T14:03:27.735Z', 'hidden': False}, {'_id': '6762d6291d6d92e318dfb0f6', 'name': 'Kaixiang Lin', 'hidden': False}, {'_id': '6762d6291d6d92e318dfb0f7', 'name': 'Min Bai', 'hidden': False}, {'_id': '6762d6291d6d92e318dfb0f8', 'name': 'Xiong Zhou', 'hidden': False}, {'_id': '6762d6291d6d92e318dfb0f9', 'name': 'Yu-Xiong Wang', 'hidden': False}, {'_id': '6762d6291d6d92e318dfb0fa', 'name': 'Sergey Levine', 'hidden': False}, {'_id': '6762d6291d6d92e318dfb0fb', 'name': 'Erran Li', 'hidden': False}], 'publishedAt': '2024-12-17T18:59:50.000Z', 'title': 'Proposer-Agent-Evaluator(PAE): Autonomous Skill Discovery For Foundation\\n  Model Internet Agents', 'summary': \"The vision of a broadly capable and goal-directed agent, such as an\\nInternet-browsing agent in the digital world and a household humanoid in the\\nphysical world, has rapidly advanced, thanks to the generalization capability\\nof foundation models. Such a generalist agent needs to have a large and diverse\\nskill repertoire, such as finding directions between two travel locations and\\nbuying specific items from the Internet. If each skill needs to be specified\\nmanually through a fixed set of human-annotated instructions, the agent's skill\\nrepertoire will necessarily be limited due to the quantity and diversity of\\nhuman-annotated instructions. In this work, we address this challenge by\\nproposing Proposer-Agent-Evaluator, an effective learning system that enables\\nfoundation model agents to autonomously discover and practice skills in the\\nwild. At the heart of PAE is a context-aware task proposer that autonomously\\nproposes tasks for the agent to practice with context information of the\\nenvironment such as user demos or even just the name of the website itself for\\nInternet-browsing agents. Then, the agent policy attempts those tasks with\\nthoughts and actual grounded operations in the real world with resulting\\ntrajectories evaluated by an autonomous VLM-based success evaluator. The\\nsuccess evaluation serves as the reward signal for the agent to refine its\\npolicies through RL. We validate PAE on challenging vision-based web\\nnavigation, using both real-world and self-hosted websites from WebVoyager and\\nWebArena.To the best of our knowledge, this work represents the first effective\\nlearning system to apply autonomous task proposal with RL for agents that\\ngeneralizes real-world human-annotated benchmarks with SOTA performances. Our\\nopen-source checkpoints and code can be found in https://yanqval.github.io/PAE/\", 'upvotes': 2, 'discussionId': '6762d62f1d6d92e318dfb28e'}, 'publishedAt': '2024-12-18T09:13:53.632Z', 'title': 'Proposer-Agent-Evaluator(PAE): Autonomous Skill Discovery For Foundation Model Internet Agents', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/64fd0229e0dc35986bd3c0e5/f_3fo8ba60Uw6V814LBWb.mp4'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.13194.png', 'numComments': 1, 'submittedBy': {'_id': '64fd0229e0dc35986bd3c0e5', 'avatarUrl': '/avatars/94f5698f9104dad7288edb4460026fd8.svg', 'fullname': 'Yifei Zhou', 'name': 'yifeizhou', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 1}}",
    "{'paper': {'id': '2412.10704', 'authors': [{'_id': '6762d715220f276ceedda74d', 'name': 'Manan Suri', 'hidden': False}, {'_id': '6762d715220f276ceedda74e', 'user': {'_id': '65c16444d4c3b8dff2f0d78d', 'avatarUrl': '/avatars/4ed764c1657bd260d2a12ba61c111062.svg', 'isPro': False, 'fullname': 'Puneet Mathur', 'user': 'puneetm', 'type': 'user'}, 'name': 'Puneet Mathur', 'status': 'extracted_pending', 'statusLastChangedAt': '2024-12-18T14:07:19.812Z', 'hidden': False}, {'_id': '6762d715220f276ceedda74f', 'user': {'_id': '62c5947524171688a9feb992', 'avatarUrl': '/avatars/5a151713b9eae8dc566f5957acee3475.svg', 'isPro': False, 'fullname': 'Franck Dernoncourt', 'user': 'Franck-Dernoncourt', 'type': 'user'}, 'name': 'Franck Dernoncourt', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-18T14:32:28.183Z', 'hidden': False}, {'_id': '6762d715220f276ceedda750', 'name': 'Kanika Goswami', 'hidden': False}, {'_id': '6762d715220f276ceedda751', 'name': 'Ryan A. Rossi', 'hidden': False}, {'_id': '6762d715220f276ceedda752', 'name': 'Dinesh Manocha', 'hidden': False}], 'publishedAt': '2024-12-14T06:24:55.000Z', 'title': 'VisDoM: Multi-Document QA with Visually Rich Elements Using Multimodal\\n  Retrieval-Augmented Generation', 'summary': 'Understanding information from a collection of multiple documents,\\nparticularly those with visually rich elements, is important for\\ndocument-grounded question answering. This paper introduces VisDoMBench, the\\nfirst comprehensive benchmark designed to evaluate QA systems in multi-document\\nsettings with rich multimodal content, including tables, charts, and\\npresentation slides. We propose VisDoMRAG, a novel multimodal Retrieval\\nAugmented Generation (RAG) approach that simultaneously utilizes visual and\\ntextual RAG, combining robust visual retrieval capabilities with sophisticated\\nlinguistic reasoning. VisDoMRAG employs a multi-step reasoning process\\nencompassing evidence curation and chain-of-thought reasoning for concurrent\\ntextual and visual RAG pipelines. A key novelty of VisDoMRAG is its\\nconsistency-constrained modality fusion mechanism, which aligns the reasoning\\nprocesses across modalities at inference time to produce a coherent final\\nanswer. This leads to enhanced accuracy in scenarios where critical information\\nis distributed across modalities and improved answer verifiability through\\nimplicit context attribution. Through extensive experiments involving\\nopen-source and proprietary large language models, we benchmark\\nstate-of-the-art document QA methods on VisDoMBench. Extensive results show\\nthat VisDoMRAG outperforms unimodal and long-context LLM baselines for\\nend-to-end multimodal document QA by 12-20%.', 'upvotes': 2, 'discussionId': '6762d717220f276ceedda807'}, 'publishedAt': '2024-12-18T09:07:27.719Z', 'title': 'VisDoM: Multi-Document QA with Visually Rich Elements Using Multimodal Retrieval-Augmented Generation', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.10704.png', 'numComments': 1, 'submittedBy': {'_id': '62c5947524171688a9feb992', 'avatarUrl': '/avatars/5a151713b9eae8dc566f5957acee3475.svg', 'fullname': 'Franck Dernoncourt', 'name': 'Franck-Dernoncourt', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 3}}",
    "{'paper': {'id': '2412.12527', 'authors': [{'_id': '6762627c1b66b28261b4bf71', 'user': {'_id': '6077e727048132eef27894b0', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/R49vipboidpLkGra_h1z5.png', 'isPro': False, 'fullname': 'Hyuhng Joon Kim', 'user': 'heyjoonkim', 'type': 'user'}, 'name': 'Hyuhng Joon Kim', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-18T09:51:18.410Z', 'hidden': False}, {'_id': '6762627c1b66b28261b4bf72', 'name': 'Youna Kim', 'hidden': False}, {'_id': '6762627c1b66b28261b4bf73', 'name': 'Sang-goo Lee', 'hidden': False}, {'_id': '6762627c1b66b28261b4bf74', 'name': 'Taeuk Kim', 'hidden': False}], 'publishedAt': '2024-12-17T04:38:08.000Z', 'title': 'When to Speak, When to Abstain: Contrastive Decoding with Abstention', 'summary': \"Large Language Models (LLMs) demonstrate exceptional performance across\\ndiverse tasks by leveraging both pre-trained knowledge (i.e., parametric\\nknowledge) and external knowledge (i.e., contextual knowledge). While\\nsubstantial efforts have been made to leverage both forms of knowledge,\\nscenarios in which the model lacks any relevant knowledge remain underexplored.\\nSuch limitations can result in issues like hallucination, causing reduced\\nreliability and potential risks in high-stakes applications. To address such\\nlimitations, this paper extends the task scope to encompass cases where the\\nuser's request cannot be fulfilled due to the lack of relevant knowledge. To\\nthis end, we introduce Contrastive Decoding with Abstention (CDA), a\\ntraining-free decoding method that empowers LLMs to generate responses when\\nrelevant knowledge is available and to abstain otherwise. CDA evaluates the\\nrelevance of each knowledge for a given query, adaptively determining which\\nknowledge to prioritize or which to completely ignore. Extensive experiments\\nwith four LLMs on three question-answering datasets demonstrate that CDA can\\neffectively perform accurate generation and abstention simultaneously. These\\nfindings highlight CDA's potential to broaden the applicability of LLMs,\\nenhancing reliability and preserving user trust.\", 'upvotes': 1, 'discussionId': '6762627e1b66b28261b4bff5'}, 'publishedAt': '2024-12-18T08:51:03.377Z', 'title': 'When to Speak, When to Abstain: Contrastive Decoding with Abstention', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.12527.png', 'numComments': 1, 'submittedBy': {'_id': '6077e727048132eef27894b0', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/R49vipboidpLkGra_h1z5.png', 'fullname': 'Hyuhng Joon Kim', 'name': 'heyjoonkim', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}}",
    "{'paper': {'id': '2412.12877', 'authors': [{'_id': '67628a706562d8e0d34a6ed6', 'user': {'_id': '669ffd8bc8cb5058061229cf', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/SIGrFQfuTTh1dMSh5um1P.png', 'isPro': False, 'fullname': 'Samuel Teodoro', 'user': 'TheRota', 'type': 'user'}, 'name': 'Samuel Teodoro', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-18T12:39:54.778Z', 'hidden': False}, {'_id': '67628a706562d8e0d34a6ed7', 'user': {'_id': '63bbeda2141c7d395c467084', 'avatarUrl': '/avatars/fcaa0e4a95796f2ac7000991f1aa19e5.svg', 'isPro': False, 'fullname': 'Agus Gunawan', 'user': 'agusgun', 'type': 'user'}, 'name': 'Agus Gunawan', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-18T12:39:38.581Z', 'hidden': False}, {'_id': '67628a706562d8e0d34a6ed8', 'name': 'Soo Ye Kim', 'hidden': False}, {'_id': '67628a706562d8e0d34a6ed9', 'user': {'_id': '6576b99d58ce19fa1e33eb1d', 'avatarUrl': '/avatars/b533e776aa3d95d722b46ef0cd381acd.svg', 'isPro': False, 'fullname': 'Jihyong Oh', 'user': 'ozbro', 'type': 'user'}, 'name': 'Jihyong Oh', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-18T12:38:29.484Z', 'hidden': False}, {'_id': '67628a706562d8e0d34a6eda', 'name': 'Munchurl Kim', 'hidden': False}], 'publishedAt': '2024-12-17T13:00:04.000Z', 'title': 'MIVE: New Design and Benchmark for Multi-Instance Video Editing', 'summary': 'Recent AI-based video editing has enabled users to edit videos through simple\\ntext prompts, significantly simplifying the editing process. However, recent\\nzero-shot video editing techniques primarily focus on global or single-object\\nedits, which can lead to unintended changes in other parts of the video. When\\nmultiple objects require localized edits, existing methods face challenges,\\nsuch as unfaithful editing, editing leakage, and lack of suitable evaluation\\ndatasets and metrics. To overcome these limitations, we propose a zero-shot\\nMulti-Instance Video Editing\\nframework, called MIVE. MIVE is a general-purpose mask-based framework, not\\ndedicated to specific objects (e.g., people). MIVE introduces two key modules:\\n(i) Disentangled Multi-instance Sampling (DMS) to prevent editing leakage and\\n(ii) Instance-centric Probability Redistribution (IPR) to ensure precise\\nlocalization and faithful editing. Additionally, we present our new MIVE\\nDataset featuring diverse video scenarios and introduce the Cross-Instance\\nAccuracy (CIA) Score to evaluate editing leakage in multi-instance video\\nediting tasks. Our extensive qualitative, quantitative, and user study\\nevaluations demonstrate that MIVE significantly outperforms recent\\nstate-of-the-art methods in terms of editing faithfulness, accuracy, and\\nleakage prevention, setting a new benchmark for multi-instance video editing.\\nThe project page is available at https://kaist-viclab.github.io/mive-site/', 'upvotes': 1, 'discussionId': '67628a746562d8e0d34a6f9b'}, 'publishedAt': '2024-12-18T05:18:37.724Z', 'title': 'MIVE: New Design and Benchmark for Multi-Instance Video Editing', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/6576b99d58ce19fa1e33eb1d/pJ8yty4mHoU2g6n0iT8Sd.mp4'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.12877.png', 'numComments': 1, 'submittedBy': {'_id': '6576b99d58ce19fa1e33eb1d', 'avatarUrl': '/avatars/b533e776aa3d95d722b46ef0cd381acd.svg', 'fullname': 'Jihyong Oh', 'name': 'ozbro', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}}",
    "{'paper': {'id': '2412.11713', 'authors': [{'_id': '6762f1ae42436c7f70b76849', 'name': 'Xuanming Zhang', 'hidden': False}, {'_id': '6762f1ae42436c7f70b7684a', 'name': 'Yuxuan Chen', 'hidden': False}, {'_id': '6762f1ae42436c7f70b7684b', 'name': 'Yiming Zheng', 'hidden': False}, {'_id': '6762f1ae42436c7f70b7684c', 'name': 'Zhexin Zhang', 'hidden': False}, {'_id': '6762f1ae42436c7f70b7684d', 'name': 'Yuan Yuan', 'hidden': False}, {'_id': '6762f1ae42436c7f70b7684e', 'name': 'Minlie Huang', 'hidden': False}], 'publishedAt': '2024-12-16T12:35:29.000Z', 'title': 'Seeker: Towards Exception Safety Code Generation with Intermediate\\n  Language Agents Framework', 'summary': 'In real world software development, improper or missing exception handling\\ncan severely impact the robustness and reliability of code. Exception handling\\nmechanisms require developers to detect, capture, and manage exceptions\\naccording to high standards, but many developers struggle with these tasks,\\nleading to fragile code. This problem is particularly evident in open-source\\nprojects and impacts the overall quality of the software ecosystem. To address\\nthis challenge, we explore the use of large language models (LLMs) to improve\\nexception handling in code. Through extensive analysis, we identify three key\\nissues: Insensitive Detection of Fragile Code, Inaccurate Capture of Exception\\nBlock, and Distorted Handling Solution. These problems are widespread across\\nreal world repositories, suggesting that robust exception handling practices\\nare often overlooked or mishandled. In response, we propose Seeker, a\\nmulti-agent framework inspired by expert developer strategies for exception\\nhandling. Seeker uses agents: Scanner, Detector, Predator, Ranker, and Handler\\nto assist LLMs in detecting, capturing, and resolving exceptions more\\neffectively. Our work is the first systematic study on leveraging LLMs to\\nenhance exception handling practices in real development scenarios, providing\\nvaluable insights for future improvements in code reliability.', 'upvotes': 0, 'discussionId': '6762f1b042436c7f70b768e2'}, 'publishedAt': '2024-12-18T11:02:16.856Z', 'title': 'Seeker: Towards Exception Safety Code Generation with Intermediate Language Agents Framework', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.11713.png', 'numComments': 1, 'submittedBy': {'_id': '65fc5109899083a2aad987c5', 'avatarUrl': '/avatars/289dbb8128746d931118cff6f6871a45.svg', 'fullname': 'XUANMING ZHANG', 'name': 'XUANMINGZHANG', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 1}}"
]