[
  {
    "paper": {
      "id": "2602.05843",
      "authors": [
        {
          "_id": "698567834ad556f294b7ec03",
          "user": {
            "_id": "64e6cf78ecce34cb442dc889",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e6cf78ecce34cb442dc889/qVZFiUEpBpSkmH8SQeinm.jpeg",
            "isPro": false,
            "fullname": "Fangzhi Xu",
            "user": "xufangzhi",
            "type": "user"
          },
          "name": "Fangzhi Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-06T18:51:15.682Z",
          "hidden": false
        },
        {
          "_id": "698567834ad556f294b7ec04",
          "name": "Hang Yan",
          "hidden": false
        },
        {
          "_id": "698567834ad556f294b7ec05",
          "name": "Qiushi Sun",
          "hidden": false
        },
        {
          "_id": "698567834ad556f294b7ec06",
          "name": "Jinyang Wu",
          "hidden": false
        },
        {
          "_id": "698567834ad556f294b7ec07",
          "name": "Zixian Huang",
          "hidden": false
        },
        {
          "_id": "698567834ad556f294b7ec08",
          "name": "Muye Huang",
          "hidden": false
        },
        {
          "_id": "698567834ad556f294b7ec09",
          "name": "Jingyang Gong",
          "hidden": false
        },
        {
          "_id": "698567834ad556f294b7ec0a",
          "name": "Zichen Ding",
          "hidden": false
        },
        {
          "_id": "698567834ad556f294b7ec0b",
          "name": "Kanzhi Cheng",
          "hidden": false
        },
        {
          "_id": "698567834ad556f294b7ec0c",
          "name": "Yian Wang",
          "hidden": false
        },
        {
          "_id": "698567834ad556f294b7ec0d",
          "name": "Xinyu Che",
          "hidden": false
        },
        {
          "_id": "698567834ad556f294b7ec0e",
          "name": "Zeyi Sun",
          "hidden": false
        },
        {
          "_id": "698567834ad556f294b7ec0f",
          "user": {
            "_id": "658be7fe135580745c510323",
            "avatarUrl": "/avatars/830e5cec4565efdc23226a86a0fcef0e.svg",
            "isPro": false,
            "fullname": "Jian Zhang",
            "user": "VentureZJ",
            "type": "user"
          },
          "name": "Jian Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-06T18:51:08.819Z",
          "hidden": false
        },
        {
          "_id": "698567834ad556f294b7ec10",
          "name": "Zhangyue Yin",
          "hidden": false
        },
        {
          "_id": "698567834ad556f294b7ec11",
          "name": "Haoran Luo",
          "hidden": false
        },
        {
          "_id": "698567834ad556f294b7ec12",
          "name": "Xuanjing Huang",
          "hidden": false
        },
        {
          "_id": "698567834ad556f294b7ec13",
          "name": "Ben Kao",
          "hidden": false
        },
        {
          "_id": "698567834ad556f294b7ec14",
          "name": "Jun Liu",
          "hidden": false
        },
        {
          "_id": "698567834ad556f294b7ec15",
          "name": "Qika Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-05T16:31:43.000Z",
      "submittedOnDailyAt": "2026-02-09T03:14:11.152Z",
      "title": "OdysseyArena: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions",
      "submittedOnDailyBy": {
        "_id": "64e6cf78ecce34cb442dc889",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e6cf78ecce34cb442dc889/qVZFiUEpBpSkmH8SQeinm.jpeg",
        "isPro": false,
        "fullname": "Fangzhi Xu",
        "user": "xufangzhi",
        "type": "user"
      },
      "summary": "The rapid advancement of Large Language Models (LLMs) has catalyzed the development of autonomous agents capable of navigating complex environments. However, existing evaluations primarily adopt a deductive paradigm, where agents execute tasks based on explicitly provided rules and static goals, often within limited planning horizons. Crucially, this neglects the inductive necessity for agents to discover latent transition laws from experience autonomously, which is the cornerstone for enabling agentic foresight and sustaining strategic coherence. To bridge this gap, we introduce OdysseyArena, which re-centers agent evaluation on long-horizon, active, and inductive interactions. We formalize and instantiate four primitives, translating abstract transition dynamics into concrete interactive environments. Building upon this, we establish OdysseyArena-Lite for standardized benchmarking, providing a set of 120 tasks to measure an agent's inductive efficiency and long-horizon discovery. Pushing further, we introduce OdysseyArena-Challenge to stress-test agent stability across extreme interaction horizons (e.g., > 200 steps). Extensive experiments on 15+ leading LLMs reveal that even frontier models exhibit a deficiency in inductive scenarios, identifying a critical bottleneck in the pursuit of autonomous discovery in complex environments. Our code and data are available at https://github.com/xufangzhi/Odyssey-Arena",
      "upvotes": 42,
      "discussionId": "698567834ad556f294b7ec16",
      "projectPage": "https://yayayacc.github.io/Odyssey-Home/",
      "githubRepo": "https://github.com/xufangzhi/Odyssey-Arena",
      "githubRepoAddedBy": "user",
      "ai_summary": "OdysseyArena presents a new framework for evaluating large language models on long-horizon, inductive agent tasks that emphasize autonomous discovery of environmental transition laws.",
      "ai_keywords": [
        "Large Language Models",
        "autonomous agents",
        "inductive reasoning",
        "long-horizon planning",
        "agent evaluation",
        "transition laws",
        "OdysseyArena",
        "OdysseyArena-Lite",
        "OdysseyArena-Challenge"
      ],
      "githubStars": 19
    },
    "publishedAt": "2026-02-05T11:31:43.000Z",
    "title": "OdysseyArena: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions",
    "summary": "The rapid advancement of Large Language Models (LLMs) has catalyzed the development of autonomous agents capable of navigating complex environments. However, existing evaluations primarily adopt a deductive paradigm, where agents execute tasks based on explicitly provided rules and static goals, often within limited planning horizons. Crucially, this neglects the inductive necessity for agents to discover latent transition laws from experience autonomously, which is the cornerstone for enabling agentic foresight and sustaining strategic coherence. To bridge this gap, we introduce OdysseyArena, which re-centers agent evaluation on long-horizon, active, and inductive interactions. We formalize and instantiate four primitives, translating abstract transition dynamics into concrete interactive environments. Building upon this, we establish OdysseyArena-Lite for standardized benchmarking, providing a set of 120 tasks to measure an agent's inductive efficiency and long-horizon discovery. Pushing further, we introduce OdysseyArena-Challenge to stress-test agent stability across extreme interaction horizons (e.g., > 200 steps). Extensive experiments on 15+ leading LLMs reveal that even frontier models exhibit a deficiency in inductive scenarios, identifying a critical bottleneck in the pursuit of autonomous discovery in complex environments. Our code and data are available at https://github.com/xufangzhi/Odyssey-Arena",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.05843.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64e6cf78ecce34cb442dc889",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e6cf78ecce34cb442dc889/qVZFiUEpBpSkmH8SQeinm.jpeg",
      "fullname": "Fangzhi Xu",
      "name": "xufangzhi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 18,
      "isUserFollowing": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2602.06570",
      "authors": [
        {
          "_id": "69895518beecc443208d2680",
          "name": "Baichuan-M3 Team",
          "hidden": false
        },
        {
          "_id": "69895518beecc443208d2682",
          "name": "Chengfeng Dou",
          "hidden": false
        },
        {
          "_id": "69895518beecc443208d2683",
          "name": "Fan Yang",
          "hidden": false
        },
        {
          "_id": "69895518beecc443208d2684",
          "name": "Fei Li",
          "hidden": false
        },
        {
          "_id": "69895518beecc443208d2685",
          "name": "Jiyuan Jia",
          "hidden": false
        },
        {
          "_id": "69895518beecc443208d2686",
          "name": "Qiang Ju",
          "hidden": false
        },
        {
          "_id": "69895518beecc443208d2687",
          "name": "Shuai Wang",
          "hidden": false
        },
        {
          "_id": "69895518beecc443208d2688",
          "name": "Tianpeng Li",
          "hidden": false
        },
        {
          "_id": "69895518beecc443208d2689",
          "name": "Xiangrong Zeng",
          "hidden": false
        },
        {
          "_id": "69895518beecc443208d268a",
          "name": "Yijie Zhou",
          "hidden": false
        },
        {
          "_id": "69895518beecc443208d268b",
          "name": "Hongda Zhang",
          "hidden": false
        },
        {
          "_id": "69895518beecc443208d268c",
          "name": "Jinyang Tai",
          "hidden": false
        },
        {
          "_id": "69895518beecc443208d268d",
          "name": "Linzhuang Sun",
          "hidden": false
        },
        {
          "_id": "69895518beecc443208d268e",
          "name": "Peidong Guo",
          "hidden": false
        },
        {
          "_id": "69895518beecc443208d268f",
          "name": "Yichuan Mo",
          "hidden": false
        },
        {
          "_id": "69895518beecc443208d2690",
          "name": "Xiaochuan Wang",
          "hidden": false
        },
        {
          "_id": "69895518beecc443208d2691",
          "name": "Hengfu Cui",
          "hidden": false
        },
        {
          "_id": "69895518beecc443208d2692",
          "name": "Zhishou Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-06T10:08:59.000Z",
      "submittedOnDailyAt": "2026-02-09T03:00:35.501Z",
      "title": "Baichuan-M3: Modeling Clinical Inquiry for Reliable Medical Decision-Making",
      "submittedOnDailyBy": {
        "_id": "641c45c921964f8f6d451d16",
        "avatarUrl": "/avatars/da06cc603f8f9ee46ddb7dc72aae5bec.svg",
        "isPro": false,
        "fullname": "FanYang",
        "user": "fairyang",
        "type": "user"
      },
      "summary": "We introduce Baichuan-M3, a medical-enhanced large language model engineered to shift the paradigm from passive question-answering to active, clinical-grade decision support. Addressing the limitations of existing systems in open-ended consultations, Baichuan-M3 utilizes a specialized training pipeline to model the systematic workflow of a physician. Key capabilities include: (i) proactive information acquisition to resolve ambiguity; (ii) long-horizon reasoning that unifies scattered evidence into coherent diagnoses; and (iii) adaptive hallucination suppression to ensure factual reliability. Empirical evaluations demonstrate that Baichuan-M3 achieves state-of-the-art results on HealthBench, the newly introduced HealthBench-Hallu and ScanBench, significantly outperforming GPT-5.2 in clinical inquiry, advisory and safety. The models are publicly available at https://huggingface.co/collections/baichuan-inc/baichuan-m3.",
      "upvotes": 38,
      "discussionId": "69895518beecc443208d2693",
      "githubRepo": "https://github.com/baichuan-inc/Baichuan-M3-235B",
      "githubRepoAddedBy": "user",
      "ai_summary": "Baichuan-M3 is a medical-enhanced large language model designed for clinical decision support with capabilities in proactive information gathering, long-horizon reasoning, and hallucination suppression.",
      "ai_keywords": [
        "large language model",
        "clinical decision support",
        "proactive information acquisition",
        "long-horizon reasoning",
        "hallucination suppression",
        "HealthBench",
        "HealthBench-Hallu",
        "ScanBench"
      ],
      "githubStars": 186,
      "organization": {
        "_id": "648457d38cf0b32b0ba0a913",
        "name": "baichuan-inc",
        "fullname": "Baichuan Intelligent Technology",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/640dd3e0c364a086c6322ad2/acwcllU0PQz4Bg3gchhYo.png"
      }
    },
    "publishedAt": "2026-02-06T05:08:59.000Z",
    "title": "Baichuan-M3: Modeling Clinical Inquiry for Reliable Medical Decision-Making",
    "summary": "We introduce Baichuan-M3, a medical-enhanced large language model engineered to shift the paradigm from passive question-answering to active, clinical-grade decision support. Addressing the limitations of existing systems in open-ended consultations, Baichuan-M3 utilizes a specialized training pipeline to model the systematic workflow of a physician. Key capabilities include: (i) proactive information acquisition to resolve ambiguity; (ii) long-horizon reasoning that unifies scattered evidence into coherent diagnoses; and (iii) adaptive hallucination suppression to ensure factual reliability. Empirical evaluations demonstrate that Baichuan-M3 achieves state-of-the-art results on HealthBench, the newly introduced HealthBench-Hallu and ScanBench, significantly outperforming GPT-5.2 in clinical inquiry, advisory and safety. The models are publicly available at https://huggingface.co/collections/baichuan-inc/baichuan-m3.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06570.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "641c45c921964f8f6d451d16",
      "avatarUrl": "/avatars/da06cc603f8f9ee46ddb7dc72aae5bec.svg",
      "fullname": "FanYang",
      "name": "fairyang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "648457d38cf0b32b0ba0a913",
      "name": "baichuan-inc",
      "fullname": "Baichuan Intelligent Technology",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/640dd3e0c364a086c6322ad2/acwcllU0PQz4Bg3gchhYo.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.03392",
      "authors": [
        {
          "_id": "69858ae34ad556f294b7ec93",
          "user": {
            "_id": "652f7bf41ad13fee8c407247",
            "avatarUrl": "/avatars/5c7a74a9edf748025bffeeba97a61505.svg",
            "isPro": false,
            "fullname": "Shumin",
            "user": "Mystery",
            "type": "user"
          },
          "name": "Shumin Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-06T18:50:45.887Z",
          "hidden": false
        },
        {
          "_id": "69858ae34ad556f294b7ec94",
          "name": "Yuexiang Xie",
          "hidden": false
        },
        {
          "_id": "69858ae34ad556f294b7ec95",
          "name": "Wenhao Zhang",
          "hidden": false
        },
        {
          "_id": "69858ae34ad556f294b7ec96",
          "name": "Yuchang Sun",
          "hidden": false
        },
        {
          "_id": "69858ae34ad556f294b7ec97",
          "name": "Yanxi Chen",
          "hidden": false
        },
        {
          "_id": "69858ae34ad556f294b7ec98",
          "name": "Yaliang Li",
          "hidden": false
        },
        {
          "_id": "69858ae34ad556f294b7ec99",
          "name": "Yanyong Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-03T11:14:58.000Z",
      "submittedOnDailyAt": "2026-02-09T00:31:59.901Z",
      "title": "On the Entropy Dynamics in Reinforcement Fine-Tuning of Large Language Models",
      "submittedOnDailyBy": {
        "_id": "652f7bf41ad13fee8c407247",
        "avatarUrl": "/avatars/5c7a74a9edf748025bffeeba97a61505.svg",
        "isPro": false,
        "fullname": "Shumin",
        "user": "Mystery",
        "type": "user"
      },
      "summary": "Entropy serves as a critical metric for measuring the diversity of outputs generated by large language models (LLMs), providing valuable insights into their exploration capabilities. While recent studies increasingly focus on monitoring and adjusting entropy to better balance exploration and exploitation in reinforcement fine-tuning (RFT), a principled understanding of entropy dynamics during this process is yet to be thoroughly investigated. In this paper, we establish a theoretical framework for analyzing the entropy dynamics during the RFT process, which begins with a discriminant expression that quantifies entropy change under a single logit update. This foundation enables the derivation of a first-order expression for entropy change, which can be further extended to the update formula of Group Relative Policy Optimization (GRPO). The corollaries and insights drawn from the theoretical analysis inspire the design of entropy control methods, and also offer a unified lens for interpreting various entropy-based methods in existing studies. We provide empirical evidence to support the main conclusions of our analysis and demonstrate the effectiveness of the derived entropy-discriminator clipping methods. This study yields novel insights into RFT training dynamics, providing theoretical support and practical strategies for optimizing the exploration-exploitation balance during LLM fine-tuning.",
      "upvotes": 38,
      "discussionId": "69858ae34ad556f294b7ec9a",
      "githubRepo": "https://github.com/agentscope-ai/Trinity-RFT",
      "githubRepoAddedBy": "user",
      "ai_summary": "The paper establishes a theoretical framework for analyzing entropy dynamics in reinforcement fine-tuning of large language models, deriving expressions for entropy change and proposing entropy control methods based on discriminant analysis.",
      "ai_keywords": [
        "entropy",
        "large language models",
        "reinforcement fine-tuning",
        "RFT",
        "logit update",
        "Group Relative Policy Optimization",
        "GRPO",
        "entropy-discriminator clipping",
        "exploration-exploitation balance"
      ],
      "githubStars": 520
    },
    "publishedAt": "2026-02-03T06:14:58.000Z",
    "title": "On the Entropy Dynamics in Reinforcement Fine-Tuning of Large Language Models",
    "summary": "Entropy serves as a critical metric for measuring the diversity of outputs generated by large language models (LLMs), providing valuable insights into their exploration capabilities. While recent studies increasingly focus on monitoring and adjusting entropy to better balance exploration and exploitation in reinforcement fine-tuning (RFT), a principled understanding of entropy dynamics during this process is yet to be thoroughly investigated. In this paper, we establish a theoretical framework for analyzing the entropy dynamics during the RFT process, which begins with a discriminant expression that quantifies entropy change under a single logit update. This foundation enables the derivation of a first-order expression for entropy change, which can be further extended to the update formula of Group Relative Policy Optimization (GRPO). The corollaries and insights drawn from the theoretical analysis inspire the design of entropy control methods, and also offer a unified lens for interpreting various entropy-based methods in existing studies. We provide empirical evidence to support the main conclusions of our analysis and demonstrate the effectiveness of the derived entropy-discriminator clipping methods. This study yields novel insights into RFT training dynamics, providing theoretical support and practical strategies for optimizing the exploration-exploitation balance during LLM fine-tuning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.03392.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "652f7bf41ad13fee8c407247",
      "avatarUrl": "/avatars/5c7a74a9edf748025bffeeba97a61505.svg",
      "fullname": "Shumin",
      "name": "Mystery",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.01734",
      "authors": [
        {
          "_id": "6987609dbeecc443208d2375",
          "name": "Lianhai Ren",
          "hidden": false
        },
        {
          "_id": "6987609dbeecc443208d2376",
          "name": "Yucheng Ding",
          "hidden": false
        },
        {
          "_id": "6987609dbeecc443208d2377",
          "name": "Xiao Liu",
          "hidden": false
        },
        {
          "_id": "6987609dbeecc443208d2378",
          "name": "Qianxiao Li",
          "hidden": false
        },
        {
          "_id": "6987609dbeecc443208d2379",
          "name": "Peng Cheng",
          "hidden": false
        },
        {
          "_id": "6987609dbeecc443208d237a",
          "name": "Yeyun Gong",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-02T07:18:45.000Z",
      "submittedOnDailyAt": "2026-02-09T02:17:53.967Z",
      "title": "MSign: An Optimizer Preventing Training Instability in Large Language Models via Stable Rank Restoration",
      "submittedOnDailyBy": {
        "_id": "63fb6e281b4b1bd4e7ffc5be",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63fb6e281b4b1bd4e7ffc5be/aiRu_bulgnxvEMrjipGoQ.jpeg",
        "isPro": false,
        "fullname": "Xiao Liu",
        "user": "lx865712528",
        "type": "user"
      },
      "summary": "Training instability remains a critical challenge in large language model (LLM) pretraining, often manifesting as sudden gradient explosions that waste significant computational resources. We study training failures in a 5M-parameter NanoGPT model scaled via μP, identifying two key phenomena preceding collapse: (1) rapid decline in weight matrix stable rank (ratio of squared Frobenius norm to squared spectral norm), and (2) increasing alignment between adjacent layer Jacobians. We prove theoretically that these two conditions jointly cause exponential gradient norm growth with network depth. To break this instability mechanism, we propose MSign, a new optimizer that periodically applies matrix sign operations to restore stable rank. Experiments on models from 5M to 3B parameters demonstrate that MSign effectively prevents training failures with a computational overhead of less than 7.0%.",
      "upvotes": 21,
      "discussionId": "6987609dbeecc443208d237b",
      "ai_summary": "Training instability in large language models is linked to weight matrix stable rank decline and Jacobian alignment, which MSign addresses through matrix sign operations to prevent gradient explosions.",
      "ai_keywords": [
        "large language model",
        "pretraining",
        "gradient explosions",
        "weight matrix stable rank",
        "Frobenius norm",
        "spectral norm",
        "Jacobian",
        "matrix sign operations",
        "optimizer"
      ],
      "organization": {
        "_id": "5e6485f787403103f9f1055e",
        "name": "microsoft",
        "fullname": "Microsoft",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"
      }
    },
    "publishedAt": "2026-02-02T02:18:45.000Z",
    "title": "MSign: An Optimizer Preventing Training Instability in Large Language Models via Stable Rank Restoration",
    "summary": "Training instability remains a critical challenge in large language model (LLM) pretraining, often manifesting as sudden gradient explosions that waste significant computational resources. We study training failures in a 5M-parameter NanoGPT model scaled via μP, identifying two key phenomena preceding collapse: (1) rapid decline in weight matrix stable rank (ratio of squared Frobenius norm to squared spectral norm), and (2) increasing alignment between adjacent layer Jacobians. We prove theoretically that these two conditions jointly cause exponential gradient norm growth with network depth. To break this instability mechanism, we propose MSign, a new optimizer that periodically applies matrix sign operations to restore stable rank. Experiments on models from 5M to 3B parameters demonstrate that MSign effectively prevents training failures with a computational overhead of less than 7.0%.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.01734.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63fb6e281b4b1bd4e7ffc5be",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63fb6e281b4b1bd4e7ffc5be/aiRu_bulgnxvEMrjipGoQ.jpeg",
      "fullname": "Xiao Liu",
      "name": "lx865712528",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "5e6485f787403103f9f1055e",
      "name": "microsoft",
      "fullname": "Microsoft",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.06717",
      "authors": [
        {
          "_id": "69898989beecc443208d2741",
          "name": "Daniil Plyusov",
          "hidden": false
        },
        {
          "_id": "69898989beecc443208d2742",
          "user": {
            "_id": "62897fce5d9e25c10e4f319d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62897fce5d9e25c10e4f319d/bMlfAyzkNNZlkQ5mCW6Vc.jpeg",
            "isPro": false,
            "fullname": "Alexey Gorbatovski",
            "user": "Myashka",
            "type": "user"
          },
          "name": "Alexey Gorbatovski",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-09T08:27:53.815Z",
          "hidden": false
        },
        {
          "_id": "69898989beecc443208d2743",
          "name": "Boris Shaposhnikov",
          "hidden": false
        },
        {
          "_id": "69898989beecc443208d2744",
          "name": "Viacheslav Sinii",
          "hidden": false
        },
        {
          "_id": "69898989beecc443208d2745",
          "name": "Alexey Malakhov",
          "hidden": false
        },
        {
          "_id": "69898989beecc443208d2746",
          "user": {
            "_id": "62a9c8edc19f92ae443ab37f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669110208492-62a9c8edc19f92ae443ab37f.png",
            "isPro": false,
            "fullname": "Daniil Gavrilov",
            "user": "kefirski",
            "type": "user"
          },
          "name": "Daniil Gavrilov",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-09T08:27:57.059Z",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-06T14:07:30.000Z",
      "submittedOnDailyAt": "2026-02-09T04:48:51.744Z",
      "title": "F-GRPO: Don't Let Your Policy Learn the Obvious and Forget the Rare",
      "submittedOnDailyBy": {
        "_id": "62897fce5d9e25c10e4f319d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62897fce5d9e25c10e4f319d/bMlfAyzkNNZlkQ5mCW6Vc.jpeg",
        "isPro": false,
        "fullname": "Alexey Gorbatovski",
        "user": "Myashka",
        "type": "user"
      },
      "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) is commonly based on group sampling to estimate advantages and stabilize policy updates. In practice, large group sizes are not feasible due to computational limits, which biases learning toward trajectories that are already likely. Smaller groups often miss rare-correct trajectories while still containing mixed rewards, concentrating probability on common solutions. We derive the probability that updates miss rare-correct modes as a function of group size, showing non-monotonic behavior, and characterize how updates redistribute mass within the correct set, revealing that unsampled-correct mass can shrink even as total correct mass grows. Motivated by this analysis, we propose a difficulty-aware advantage scaling coefficient, inspired by Focal loss, that down-weights updates on high-success prompts. The lightweight modification can be directly integrated into any group-relative RLVR algorithm such as GRPO, DAPO, and CISPO. On Qwen2.5-7B across in-domain and out-of-domain benchmarks, our method improves pass@256 from 64.1 rightarrow 70.3 (GRPO), 69.3 rightarrow 72.5 (DAPO), and 73.2 rightarrow 76.8 (CISPO), while preserving or improving pass@1, without increasing group size or computational cost.",
      "upvotes": 14,
      "discussionId": "69898989beecc443208d2747",
      "ai_summary": "RLVR methods using group sampling suffer from bias toward likely trajectories and missed rare-correct ones; a difficulty-aware advantage scaling technique improves performance on benchmarks without increasing computational cost.",
      "ai_keywords": [
        "reinforcement learning",
        "verifiable rewards",
        "group sampling",
        "advantage estimation",
        "policy updates",
        "Focal loss",
        "GRPO",
        "DAPO",
        "CISPO",
        "pass@k metrics"
      ],
      "organization": {
        "_id": "675861e944dbb69c2673c71c",
        "name": "t-tech",
        "fullname": "T-Tech",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/674ea07d320a043daeb2d98b/IwSCMolFY4Otk7sFXzWhi.jpeg"
      }
    },
    "publishedAt": "2026-02-06T09:07:30.000Z",
    "title": "F-GRPO: Don't Let Your Policy Learn the Obvious and Forget the Rare",
    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) is commonly based on group sampling to estimate advantages and stabilize policy updates. In practice, large group sizes are not feasible due to computational limits, which biases learning toward trajectories that are already likely. Smaller groups often miss rare-correct trajectories while still containing mixed rewards, concentrating probability on common solutions. We derive the probability that updates miss rare-correct modes as a function of group size, showing non-monotonic behavior, and characterize how updates redistribute mass within the correct set, revealing that unsampled-correct mass can shrink even as total correct mass grows. Motivated by this analysis, we propose a difficulty-aware advantage scaling coefficient, inspired by Focal loss, that down-weights updates on high-success prompts. The lightweight modification can be directly integrated into any group-relative RLVR algorithm such as GRPO, DAPO, and CISPO. On Qwen2.5-7B across in-domain and out-of-domain benchmarks, our method improves pass@256 from 64.1 rightarrow 70.3 (GRPO), 69.3 rightarrow 72.5 (DAPO), and 73.2 rightarrow 76.8 (CISPO), while preserving or improving pass@1, without increasing group size or computational cost.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06717.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62897fce5d9e25c10e4f319d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62897fce5d9e25c10e4f319d/bMlfAyzkNNZlkQ5mCW6Vc.jpeg",
      "fullname": "Alexey Gorbatovski",
      "name": "Myashka",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "675861e944dbb69c2673c71c",
      "name": "t-tech",
      "fullname": "T-Tech",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/674ea07d320a043daeb2d98b/IwSCMolFY4Otk7sFXzWhi.jpeg"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2602.06291",
      "authors": [
        {
          "_id": "698964b8beecc443208d26d8",
          "name": "Guijin Son",
          "hidden": false
        },
        {
          "_id": "698964b8beecc443208d26d9",
          "name": "Donghun Yang",
          "hidden": false
        },
        {
          "_id": "698964b8beecc443208d26da",
          "name": "Hitesh Laxmichand Patel",
          "hidden": false
        },
        {
          "_id": "698964b8beecc443208d26db",
          "name": "Hyunwoo Ko",
          "hidden": false
        },
        {
          "_id": "698964b8beecc443208d26dc",
          "name": "Amit Agarwal",
          "hidden": false
        },
        {
          "_id": "698964b8beecc443208d26dd",
          "name": "Sunghee Ahn",
          "hidden": false
        },
        {
          "_id": "698964b8beecc443208d26de",
          "name": "Kyong-Ha Lee",
          "hidden": false
        },
        {
          "_id": "698964b8beecc443208d26df",
          "name": "Youngjae Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-06T01:10:28.000Z",
      "submittedOnDailyAt": "2026-02-09T02:08:27.389Z",
      "title": "Judging What We Cannot Solve: A Consequence-Based Approach for Oracle-Free Evaluation of Research-Level Math",
      "submittedOnDailyBy": {
        "_id": "60d3e619b8448e1785bbda2a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60d3e619b8448e1785bbda2a/q2re5u1HNwsCCyIMtid_I.jpeg",
        "isPro": true,
        "fullname": "GUIJIN SON",
        "user": "amphora",
        "type": "user"
      },
      "summary": "Recent progress in reasoning models suggests that generating plausible attempts for research-level mathematics may be within reach, but verification remains a bottleneck, consuming scarce expert time. We hypothesize that a meaningful solution should contain enough method-level information that, when applied to a neighborhood of related questions, it should yield better downstream performance than incorrect solutions. Building on this idea, we propose Consequence-Based Utility, an oracle-free evaluator that scores each candidate by testing its value as an in-context exemplar in solving related yet verifiable questions. Our approach is evaluated on an original set of research-level math problems, each paired with one expert-written solution and nine LLM-generated solutions. Notably, Consequence-Based Utility consistently outperforms reward models, generative reward models, and LLM judges on ranking quality. Specifically, for GPT-OSS-120B, it improves Acc@1 from 67.2 to 76.3 and AUC from 71.4 to 79.6, with similarly large AUC gains on GPT-OSS-20B (69.0 to 79.2). Furthermore, compared to LLM-Judges, it also exhibits a larger solver-evaluator gap, maintaining a stronger correct-wrong separation even on instances where the underlying solver often fails to solve.",
      "upvotes": 14,
      "discussionId": "698964b8beecc443208d26e0",
      "ai_summary": "Consequence-Based Utility evaluates mathematical solutions by testing their effectiveness as exemplars for related problems, outperforming reward models and LLM judges in ranking quality and correct-wrong separation.",
      "ai_keywords": [
        "reasoning models",
        "research-level mathematics",
        "verification",
        "oracle-free evaluator",
        "in-context exemplar",
        "reward models",
        "generative reward models",
        "LLM judges",
        "Acc@1",
        "AUC",
        "solver-evaluator gap"
      ]
    },
    "publishedAt": "2026-02-05T20:10:28.000Z",
    "title": "Judging What We Cannot Solve: A Consequence-Based Approach for Oracle-Free Evaluation of Research-Level Math",
    "summary": "Recent progress in reasoning models suggests that generating plausible attempts for research-level mathematics may be within reach, but verification remains a bottleneck, consuming scarce expert time. We hypothesize that a meaningful solution should contain enough method-level information that, when applied to a neighborhood of related questions, it should yield better downstream performance than incorrect solutions. Building on this idea, we propose Consequence-Based Utility, an oracle-free evaluator that scores each candidate by testing its value as an in-context exemplar in solving related yet verifiable questions. Our approach is evaluated on an original set of research-level math problems, each paired with one expert-written solution and nine LLM-generated solutions. Notably, Consequence-Based Utility consistently outperforms reward models, generative reward models, and LLM judges on ranking quality. Specifically, for GPT-OSS-120B, it improves Acc@1 from 67.2 to 76.3 and AUC from 71.4 to 79.6, with similarly large AUC gains on GPT-OSS-20B (69.0 to 79.2). Furthermore, compared to LLM-Judges, it also exhibits a larger solver-evaluator gap, maintaining a stronger correct-wrong separation even on instances where the underlying solver often fails to solve.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06291.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60d3e619b8448e1785bbda2a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60d3e619b8448e1785bbda2a/q2re5u1HNwsCCyIMtid_I.jpeg",
      "fullname": "GUIJIN SON",
      "name": "amphora",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 76,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.05940",
      "authors": [
        {
          "_id": "6985662b4ad556f294b7ebf8",
          "user": {
            "_id": "68356f5db243fb809813a715",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68356f5db243fb809813a715/grhHvANfDRp75rMJxWlQo.jpeg",
            "isPro": false,
            "fullname": "LiuJunxiao",
            "user": "master-lan",
            "type": "user"
          },
          "name": "Junxiao Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-06T18:51:11.681Z",
          "hidden": false
        },
        {
          "_id": "6985662b4ad556f294b7ebf9",
          "name": "Zhijun Wang",
          "hidden": false
        },
        {
          "_id": "6985662b4ad556f294b7ebfa",
          "name": "Yixiao Li",
          "hidden": false
        },
        {
          "_id": "6985662b4ad556f294b7ebfb",
          "name": "Zhejian Lai",
          "hidden": false
        },
        {
          "_id": "6985662b4ad556f294b7ebfc",
          "name": "Liqian Huang",
          "hidden": false
        },
        {
          "_id": "6985662b4ad556f294b7ebfd",
          "name": "Xin Huang",
          "hidden": false
        },
        {
          "_id": "6985662b4ad556f294b7ebfe",
          "name": "Xue Han",
          "hidden": false
        },
        {
          "_id": "6985662b4ad556f294b7ebff",
          "name": "Junlan Feng",
          "hidden": false
        },
        {
          "_id": "6985662b4ad556f294b7ec00",
          "name": "Shujian Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-05T17:55:09.000Z",
      "submittedOnDailyAt": "2026-02-09T01:14:36.127Z",
      "title": "Self-Improving Multilingual Long Reasoning via Translation-Reasoning Integrated Training",
      "submittedOnDailyBy": {
        "_id": "68356f5db243fb809813a715",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68356f5db243fb809813a715/grhHvANfDRp75rMJxWlQo.jpeg",
        "isPro": false,
        "fullname": "LiuJunxiao",
        "user": "master-lan",
        "type": "user"
      },
      "summary": "Long reasoning models often struggle in multilingual settings: they tend to reason in English for non-English questions; when constrained to reasoning in the question language, accuracies drop substantially. The struggle is caused by the limited abilities for both multilingual question understanding and multilingual reasoning. To address both problems, we propose TRIT (Translation-Reasoning Integrated Training), a self-improving framework that integrates the training of translation into multilingual reasoning. Without external feedback or additional multilingual data, our method jointly enhances multilingual question understanding and response generation. On MMATH, our method outperforms multiple baselines by an average of 7 percentage points, improving both answer correctness and language consistency. Further analysis reveals that integrating translation training improves cross-lingual question alignment by over 10 percentage points and enhances translation quality for both mathematical questions and general-domain text, with gains up to 8.4 COMET points on FLORES-200.",
      "upvotes": 14,
      "discussionId": "6985662b4ad556f294b7ec01",
      "ai_summary": "TRIT framework improves multilingual reasoning by jointly training translation and reasoning components, enhancing question understanding and response generation across languages.",
      "ai_keywords": [
        "multilingual reasoning",
        "translation reasoning integrated training",
        "cross-lingual question alignment",
        "COMET",
        "FLORES-200",
        "MMATH"
      ]
    },
    "publishedAt": "2026-02-05T12:55:09.000Z",
    "title": "Self-Improving Multilingual Long Reasoning via Translation-Reasoning Integrated Training",
    "summary": "Long reasoning models often struggle in multilingual settings: they tend to reason in English for non-English questions; when constrained to reasoning in the question language, accuracies drop substantially. The struggle is caused by the limited abilities for both multilingual question understanding and multilingual reasoning. To address both problems, we propose TRIT (Translation-Reasoning Integrated Training), a self-improving framework that integrates the training of translation into multilingual reasoning. Without external feedback or additional multilingual data, our method jointly enhances multilingual question understanding and response generation. On MMATH, our method outperforms multiple baselines by an average of 7 percentage points, improving both answer correctness and language consistency. Further analysis reveals that integrating translation training improves cross-lingual question alignment by over 10 percentage points and enhances translation quality for both mathematical questions and general-domain text, with gains up to 8.4 COMET points on FLORES-200.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.05940.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "68356f5db243fb809813a715",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68356f5db243fb809813a715/grhHvANfDRp75rMJxWlQo.jpeg",
      "fullname": "LiuJunxiao",
      "name": "master-lan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2602.06391",
      "authors": [
        {
          "_id": "69894bc6beecc443208d25c0",
          "name": "Zhongyin Zhao",
          "hidden": false
        },
        {
          "_id": "69894bc6beecc443208d25c1",
          "name": "Yuan Liu",
          "hidden": false
        },
        {
          "_id": "69894bc6beecc443208d25c2",
          "name": "Yikun Liu",
          "hidden": false
        },
        {
          "_id": "69894bc6beecc443208d25c3",
          "name": "Haicheng Wang",
          "hidden": false
        },
        {
          "_id": "69894bc6beecc443208d25c4",
          "name": "Le Tian",
          "hidden": false
        },
        {
          "_id": "69894bc6beecc443208d25c5",
          "name": "Xiao Zhou",
          "hidden": false
        },
        {
          "_id": "69894bc6beecc443208d25c6",
          "name": "Yangxiu You",
          "hidden": false
        },
        {
          "_id": "69894bc6beecc443208d25c7",
          "name": "Zilin Yu",
          "hidden": false
        },
        {
          "_id": "69894bc6beecc443208d25c8",
          "name": "Yang Yu",
          "hidden": false
        },
        {
          "_id": "69894bc6beecc443208d25c9",
          "name": "Jie Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-06T05:14:11.000Z",
      "submittedOnDailyAt": "2026-02-09T00:30:58.259Z",
      "title": "POINTS-GUI-G: GUI-Grounding Journey",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "The rapid advancement of vision-language models has catalyzed the emergence of GUI agents, which hold immense potential for automating complex tasks, from online shopping to flight booking, thereby alleviating the burden of repetitive digital workflows. As a foundational capability, GUI grounding is typically established as a prerequisite for end-to-end task execution. It enables models to precisely locate interface elements, such as text and icons, to perform accurate operations like clicking and typing. Unlike prior works that fine-tune models already possessing strong spatial awareness (e.g., Qwen3-VL), we aim to master the full technical pipeline by starting from a base model with minimal grounding ability, such as POINTS-1.5. We introduce POINTS-GUI-G-8B, which achieves state-of-the-art performance with scores of 59.9 on ScreenSpot-Pro, 66.0 on OSWorld-G, 95.7 on ScreenSpot-v2, and 49.9 on UI-Vision. Our model's success is driven by three key factors: (1) Refined Data Engineering, involving the unification of diverse open-source datasets format alongside sophisticated strategies for augmentation, filtering, and difficulty grading; (2) Improved Training Strategies, including continuous fine-tuning of the vision encoder to enhance perceptual accuracy and maintaining resolution consistency between training and inference; and (3) Reinforcement Learning (RL) with Verifiable Rewards. While RL is traditionally used to bolster reasoning, we demonstrate that it significantly improves precision in the perception-intensive GUI grounding task. Furthermore, GUI grounding provides a natural advantage for RL, as rewards are easily verifiable and highly accurate.",
      "upvotes": 11,
      "discussionId": "69894bc6beecc443208d25ca",
      "githubRepo": "https://github.com/Tencent/POINTS-GUI",
      "githubRepoAddedBy": "user",
      "ai_summary": "GUI agents for automated digital tasks rely on vision-language models with enhanced grounding capabilities, achieved through refined data engineering, improved training strategies, and reinforcement learning with verifiable rewards.",
      "ai_keywords": [
        "vision-language models",
        "GUI agents",
        "GUI grounding",
        "ScreenSpot-Pro",
        "OSWorld-G",
        "ScreenSpot-v2",
        "UI-Vision",
        "data engineering",
        "training strategies",
        "reinforcement learning",
        "verifiable rewards"
      ],
      "githubStars": 13
    },
    "publishedAt": "2026-02-06T00:14:11.000Z",
    "title": "POINTS-GUI-G: GUI-Grounding Journey",
    "summary": "The rapid advancement of vision-language models has catalyzed the emergence of GUI agents, which hold immense potential for automating complex tasks, from online shopping to flight booking, thereby alleviating the burden of repetitive digital workflows. As a foundational capability, GUI grounding is typically established as a prerequisite for end-to-end task execution. It enables models to precisely locate interface elements, such as text and icons, to perform accurate operations like clicking and typing. Unlike prior works that fine-tune models already possessing strong spatial awareness (e.g., Qwen3-VL), we aim to master the full technical pipeline by starting from a base model with minimal grounding ability, such as POINTS-1.5. We introduce POINTS-GUI-G-8B, which achieves state-of-the-art performance with scores of 59.9 on ScreenSpot-Pro, 66.0 on OSWorld-G, 95.7 on ScreenSpot-v2, and 49.9 on UI-Vision. Our model's success is driven by three key factors: (1) Refined Data Engineering, involving the unification of diverse open-source datasets format alongside sophisticated strategies for augmentation, filtering, and difficulty grading; (2) Improved Training Strategies, including continuous fine-tuning of the vision encoder to enhance perceptual accuracy and maintaining resolution consistency between training and inference; and (3) Reinforcement Learning (RL) with Verifiable Rewards. While RL is traditionally used to bolster reasoning, we demonstrate that it significantly improves precision in the perception-intensive GUI grounding task. Furthermore, GUI grounding provides a natural advantage for RL, as rewards are easily verifiable and highly accurate.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06391.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 230,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.06949",
      "authors": [
        {
          "_id": "69894c74beecc443208d25db",
          "name": "Shenyuan Gao",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25dc",
          "name": "William Liang",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25dd",
          "name": "Kaiyuan Zheng",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25de",
          "name": "Ayaan Malik",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25df",
          "name": "Seonghyeon Ye",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25e0",
          "name": "Sihyun Yu",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25e1",
          "name": "Wei-Cheng Tseng",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25e2",
          "name": "Yuzhu Dong",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25e3",
          "name": "Kaichun Mo",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25e4",
          "name": "Chen-Hsuan Lin",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25e5",
          "name": "Qianli Ma",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25e6",
          "name": "Seungjun Nah",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25e7",
          "name": "Loic Magne",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25e8",
          "name": "Jiannan Xiang",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25e9",
          "name": "Yuqi Xie",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25ea",
          "name": "Ruijie Zheng",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25eb",
          "name": "Dantong Niu",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25ec",
          "name": "You Liang Tan",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25ed",
          "name": "K. R. Zentner",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25ee",
          "name": "George Kurian",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25ef",
          "name": "Suneel Indupuru",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25f0",
          "name": "Pooya Jannaty",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25f1",
          "name": "Jinwei Gu",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25f2",
          "name": "Jun Zhang",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25f3",
          "name": "Jitendra Malik",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25f4",
          "name": "Pieter Abbeel",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25f5",
          "name": "Ming-Yu Liu",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25f6",
          "name": "Yuke Zhu",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25f7",
          "name": "Joel Jang",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25f8",
          "name": "Linxi \"Jim\" Fan",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/MN-A84kxkw1l1lyftyRTR.mp4"
      ],
      "publishedAt": "2026-02-06T18:49:43.000Z",
      "submittedOnDailyAt": "2026-02-09T00:32:34.350Z",
      "title": "DreamDojo: A Generalist Robot World Model from Large-Scale Human Videos",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Being able to simulate the outcomes of actions in varied environments will revolutionize the development of generalist agents at scale. However, modeling these world dynamics, especially for dexterous robotics tasks, poses significant challenges due to limited data coverage and scarce action labels. As an endeavor towards this end, we introduce DreamDojo, a foundation world model that learns diverse interactions and dexterous controls from 44k hours of egocentric human videos. Our data mixture represents the largest video dataset to date for world model pretraining, spanning a wide range of daily scenarios with diverse objects and skills. To address the scarcity of action labels, we introduce continuous latent actions as unified proxy actions, enhancing interaction knowledge transfer from unlabeled videos. After post-training on small-scale target robot data, DreamDojo demonstrates a strong understanding of physics and precise action controllability. We also devise a distillation pipeline that accelerates DreamDojo to a real-time speed of 10.81 FPS and further improves context consistency. Our work enables several important applications based on generative world models, including live teleoperation, policy evaluation, and model-based planning. Systematic evaluation on multiple challenging out-of-distribution (OOD) benchmarks verifies the significance of our method for simulating open-world, contact-rich tasks, paving the way for general-purpose robot world models.",
      "upvotes": 10,
      "discussionId": "69894c74beecc443208d25f9",
      "projectPage": "https://dreamdojo-world.github.io/",
      "ai_summary": "DreamDojo is a foundation world model trained on 44k hours of egocentric human videos that enables efficient simulation of dexterous robotic tasks through continuous latent actions and real-time distillation.",
      "ai_keywords": [
        "world model",
        "egocentric videos",
        "continuous latent actions",
        "action labels",
        "distillation pipeline",
        "real-time speed",
        "teleoperation",
        "policy evaluation",
        "model-based planning",
        "out-of-distribution benchmarks"
      ],
      "organization": {
        "_id": "60262b67268c201cdc8b7d43",
        "name": "nvidia",
        "fullname": "NVIDIA",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
      }
    },
    "publishedAt": "2026-02-06T13:49:43.000Z",
    "title": "DreamDojo: A Generalist Robot World Model from Large-Scale Human Videos",
    "summary": "Being able to simulate the outcomes of actions in varied environments will revolutionize the development of generalist agents at scale. However, modeling these world dynamics, especially for dexterous robotics tasks, poses significant challenges due to limited data coverage and scarce action labels. As an endeavor towards this end, we introduce DreamDojo, a foundation world model that learns diverse interactions and dexterous controls from 44k hours of egocentric human videos. Our data mixture represents the largest video dataset to date for world model pretraining, spanning a wide range of daily scenarios with diverse objects and skills. To address the scarcity of action labels, we introduce continuous latent actions as unified proxy actions, enhancing interaction knowledge transfer from unlabeled videos. After post-training on small-scale target robot data, DreamDojo demonstrates a strong understanding of physics and precise action controllability. We also devise a distillation pipeline that accelerates DreamDojo to a real-time speed of 10.81 FPS and further improves context consistency. Our work enables several important applications based on generative world models, including live teleoperation, policy evaluation, and model-based planning. Systematic evaluation on multiple challenging out-of-distribution (OOD) benchmarks verifies the significance of our method for simulating open-world, contact-rich tasks, paving the way for general-purpose robot world models.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/MN-A84kxkw1l1lyftyRTR.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06949.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 230,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "60262b67268c201cdc8b7d43",
      "name": "nvidia",
      "fullname": "NVIDIA",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.05281",
      "authors": [
        {
          "_id": "69889d19beecc443208d24b2",
          "name": "Pengyi Li",
          "hidden": false
        },
        {
          "_id": "69889d19beecc443208d24b3",
          "name": "Elizaveta Goncharova",
          "hidden": false
        },
        {
          "_id": "69889d19beecc443208d24b4",
          "name": "Andrey Kuznetsov",
          "hidden": false
        },
        {
          "_id": "69889d19beecc443208d24b5",
          "name": "Ivan Oseledets",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-05T04:06:55.000Z",
      "submittedOnDailyAt": "2026-02-09T01:25:49.518Z",
      "title": "Back to Basics: Revisiting Exploration in Reinforcement Learning for LLM Reasoning via Generative Probabilities",
      "submittedOnDailyBy": {
        "_id": "6734e315c1aadce903f73aea",
        "avatarUrl": "/avatars/95d95c49419372debc201cb63c354b86.svg",
        "isPro": false,
        "fullname": "Li Pengyi",
        "user": "LiPengyi29",
        "type": "user"
      },
      "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an indispensable paradigm for enhancing reasoning in Large Language Models (LLMs). However, standard policy optimization methods, such as Group Relative Policy Optimization (GRPO), often converge to low-entropy policies, leading to severe mode collapse and limited output diversity. We analyze this issue from the perspective of sampling probability dynamics, identifying that the standard objective disproportionately reinforces the highest-likelihood paths, thereby suppressing valid alternative reasoning chains. To address this, we propose a novel Advantage Re-weighting Mechanism (ARM) designed to equilibrate the confidence levels across all correct responses. By incorporating Prompt Perplexity and Answer Confidence into the advantage estimation, our method dynamically reshapes the reward signal to attenuate the gradient updates of over-confident reasoning paths, while redistributing probability mass toward under-explored correct solutions. Empirical results demonstrate that our approach significantly enhances generative diversity and response entropy while maintaining competitive accuracy, effectively achieving a superior trade-off between exploration and exploitation in reasoning tasks. Empirical results on Qwen2.5 and DeepSeek models across mathematical and coding benchmarks show that ProGRPO significantly mitigates entropy collapse. Specifically, on Qwen2.5-7B, our method outperforms GRPO by 5.7% in Pass@1 and, notably, by 13.9% in Pass@32, highlighting its superior capability in generating diverse correct reasoning paths.",
      "upvotes": 10,
      "discussionId": "69889d19beecc443208d24b6",
      "ai_summary": "A novel reinforcement learning approach called ARM addresses entropy collapse in LLM reasoning by equilibrating confidence levels across correct responses through dynamic reward shaping.",
      "ai_keywords": [
        "Reinforcement Learning with Verifiable Rewards",
        "policy optimization",
        "Group Relative Policy Optimization",
        "advantage estimation",
        "Prompt Perplexity",
        "Answer Confidence",
        "entropy collapse",
        "generative diversity",
        "response entropy",
        "exploration",
        "exploitation",
        "reasoning tasks",
        "Pass@1",
        "Pass@32"
      ]
    },
    "publishedAt": "2026-02-04T23:06:55.000Z",
    "title": "Back to Basics: Revisiting Exploration in Reinforcement Learning for LLM Reasoning via Generative Probabilities",
    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an indispensable paradigm for enhancing reasoning in Large Language Models (LLMs). However, standard policy optimization methods, such as Group Relative Policy Optimization (GRPO), often converge to low-entropy policies, leading to severe mode collapse and limited output diversity. We analyze this issue from the perspective of sampling probability dynamics, identifying that the standard objective disproportionately reinforces the highest-likelihood paths, thereby suppressing valid alternative reasoning chains. To address this, we propose a novel Advantage Re-weighting Mechanism (ARM) designed to equilibrate the confidence levels across all correct responses. By incorporating Prompt Perplexity and Answer Confidence into the advantage estimation, our method dynamically reshapes the reward signal to attenuate the gradient updates of over-confident reasoning paths, while redistributing probability mass toward under-explored correct solutions. Empirical results demonstrate that our approach significantly enhances generative diversity and response entropy while maintaining competitive accuracy, effectively achieving a superior trade-off between exploration and exploitation in reasoning tasks. Empirical results on Qwen2.5 and DeepSeek models across mathematical and coding benchmarks show that ProGRPO significantly mitigates entropy collapse. Specifically, on Qwen2.5-7B, our method outperforms GRPO by 5.7% in Pass@1 and, notably, by 13.9% in Pass@32, highlighting its superior capability in generating diverse correct reasoning paths.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.05281.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6734e315c1aadce903f73aea",
      "avatarUrl": "/avatars/95d95c49419372debc201cb63c354b86.svg",
      "fullname": "Li Pengyi",
      "name": "LiPengyi29",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.06075",
      "authors": [
        {
          "_id": "698949ccbeecc443208d25a3",
          "name": "Guangyi Liu",
          "hidden": false
        },
        {
          "_id": "698949ccbeecc443208d25a4",
          "name": "Pengxiang Zhao",
          "hidden": false
        },
        {
          "_id": "698949ccbeecc443208d25a5",
          "name": "Yaozhen Liang",
          "hidden": false
        },
        {
          "_id": "698949ccbeecc443208d25a6",
          "name": "Qinyi Luo",
          "hidden": false
        },
        {
          "_id": "698949ccbeecc443208d25a7",
          "name": "Shunye Tang",
          "hidden": false
        },
        {
          "_id": "698949ccbeecc443208d25a8",
          "name": "Yuxiang Chai",
          "hidden": false
        },
        {
          "_id": "698949ccbeecc443208d25a9",
          "name": "Weifeng Lin",
          "hidden": false
        },
        {
          "_id": "698949ccbeecc443208d25aa",
          "name": "Han Xiao",
          "hidden": false
        },
        {
          "_id": "698949ccbeecc443208d25ab",
          "name": "WenHao Wang",
          "hidden": false
        },
        {
          "_id": "698949ccbeecc443208d25ac",
          "name": "Siheng Chen",
          "hidden": false
        },
        {
          "_id": "698949ccbeecc443208d25ad",
          "name": "Zhengxi Lu",
          "hidden": false
        },
        {
          "_id": "698949ccbeecc443208d25ae",
          "name": "Gao Wu",
          "hidden": false
        },
        {
          "_id": "698949ccbeecc443208d25af",
          "name": "Hao Wang",
          "hidden": false
        },
        {
          "_id": "698949ccbeecc443208d25b0",
          "name": "Liang Liu",
          "hidden": false
        },
        {
          "_id": "698949ccbeecc443208d25b1",
          "name": "Yong Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-03T17:01:59.000Z",
      "submittedOnDailyAt": "2026-02-09T00:13:26.013Z",
      "title": "MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments",
      "submittedOnDailyBy": {
        "_id": "64d761b98ebc40443831f82a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d761b98ebc40443831f82a/DHBOtOstiFp2-lDY6b9gb.png",
        "isPro": false,
        "fullname": "Guangyi Liu",
        "user": "lgy0404",
        "type": "user"
      },
      "summary": "Current mobile GUI agent benchmarks systematically fail to assess memory capabilities, with only 5.2-11.8% memory-related tasks and no cross-session learning evaluation. We introduce MemGUI-Bench, a comprehensive memory-centric benchmark with pass@k and staged LLM-as-judge evaluation. Our contributions include: (1) a systematic memory taxonomy analyzing 11 agents across 5 architectures; (2) 128 tasks across 26 applications where 89.8% challenge memory through cross-temporal and cross-spatial retention; (3) MemGUI-Eval, an automated pipeline with Progressive Scrutiny and 7 hierarchical metrics; and (4) RQ-driven assessment of 11 state-of-the-art agents. Our experiments reveal significant memory deficits across all evaluated systems, identify 5 distinct failure modes, and synthesize 5 actionable design implications. All resources including code, benchmark, and evaluation results will be \\textit{fully open-sourced and continuously maintained} at https://lgy0404.github.io/MemGUI-Bench/.",
      "upvotes": 10,
      "discussionId": "698949ccbeecc443208d25b2",
      "projectPage": "https://lgy0404.github.io/MemGUI-Bench/",
      "githubRepo": "https://github.com/lgy0404/MemGUI-Bench",
      "githubRepoAddedBy": "user",
      "ai_summary": "A comprehensive memory-focused benchmark for mobile GUI agents reveals significant memory capability gaps and provides systematic evaluation methods and design insights.",
      "ai_keywords": [
        "memory-centric benchmark",
        "LLM-as-judge evaluation",
        "memory taxonomy",
        "cross-temporal retention",
        "cross-spatial retention",
        "automated pipeline",
        "Progressive Scrutiny",
        "hierarchical metrics",
        "state-of-the-art agents",
        "failure modes",
        "design implications"
      ],
      "githubStars": 5,
      "organization": {
        "_id": "61bac2af530e5c78d7b99667",
        "name": "zju",
        "fullname": "Zhejiang University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"
      }
    },
    "publishedAt": "2026-02-03T12:01:59.000Z",
    "title": "MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments",
    "summary": "Current mobile GUI agent benchmarks systematically fail to assess memory capabilities, with only 5.2-11.8% memory-related tasks and no cross-session learning evaluation. We introduce MemGUI-Bench, a comprehensive memory-centric benchmark with pass@k and staged LLM-as-judge evaluation. Our contributions include: (1) a systematic memory taxonomy analyzing 11 agents across 5 architectures; (2) 128 tasks across 26 applications where 89.8% challenge memory through cross-temporal and cross-spatial retention; (3) MemGUI-Eval, an automated pipeline with Progressive Scrutiny and 7 hierarchical metrics; and (4) RQ-driven assessment of 11 state-of-the-art agents. Our experiments reveal significant memory deficits across all evaluated systems, identify 5 distinct failure modes, and synthesize 5 actionable design implications. All resources including code, benchmark, and evaluation results will be \\textit{fully open-sourced and continuously maintained} at https://lgy0404.github.io/MemGUI-Bench/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06075.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "64d761b98ebc40443831f82a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d761b98ebc40443831f82a/DHBOtOstiFp2-lDY6b9gb.png",
      "fullname": "Guangyi Liu",
      "name": "lgy0404",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "61bac2af530e5c78d7b99667",
      "name": "zju",
      "fullname": "Zhejiang University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.06960",
      "authors": [
        {
          "_id": "69894ae3beecc443208d25b4",
          "name": "Yuchen Yan",
          "hidden": false
        },
        {
          "_id": "69894ae3beecc443208d25b5",
          "name": "Liang Jiang",
          "hidden": false
        },
        {
          "_id": "69894ae3beecc443208d25b6",
          "name": "Jin Jiang",
          "hidden": false
        },
        {
          "_id": "69894ae3beecc443208d25b7",
          "name": "Shuaicheng Li",
          "hidden": false
        },
        {
          "_id": "69894ae3beecc443208d25b8",
          "name": "Zujie Wen",
          "hidden": false
        },
        {
          "_id": "69894ae3beecc443208d25b9",
          "name": "Zhiqiang Zhang",
          "hidden": false
        },
        {
          "_id": "69894ae3beecc443208d25ba",
          "name": "Jun Zhou",
          "hidden": false
        },
        {
          "_id": "69894ae3beecc443208d25bb",
          "name": "Jian Shao",
          "hidden": false
        },
        {
          "_id": "69894ae3beecc443208d25bc",
          "name": "Yueting Zhuang",
          "hidden": false
        },
        {
          "_id": "69894ae3beecc443208d25bd",
          "name": "Yongliang Shen",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-06T18:59:27.000Z",
      "submittedOnDailyAt": "2026-02-09T00:18:12.522Z",
      "title": "InftyThink+: Effective and Efficient Infinite-Horizon Reasoning via Reinforcement Learning",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Large reasoning models achieve strong performance by scaling inference-time chain-of-thought, but this paradigm suffers from quadratic cost, context length limits, and degraded reasoning due to lost-in-the-middle effects. Iterative reasoning mitigates these issues by periodically summarizing intermediate thoughts, yet existing methods rely on supervised learning or fixed heuristics and fail to optimize when to summarize, what to preserve, and how to resume reasoning. We propose InftyThink+, an end-to-end reinforcement learning framework that optimizes the entire iterative reasoning trajectory, building on model-controlled iteration boundaries and explicit summarization. InftyThink+ adopts a two-stage training scheme with supervised cold-start followed by trajectory-level reinforcement learning, enabling the model to learn strategic summarization and continuation decisions. Experiments on DeepSeek-R1-Distill-Qwen-1.5B show that InftyThink+ improves accuracy by 21% on AIME24 and outperforms conventional long chain-of-thought reinforcement learning by a clear margin, while also generalizing better to out-of-distribution benchmarks. Moreover, InftyThink+ significantly reduces inference latency and accelerates reinforcement learning training, demonstrating improved reasoning efficiency alongside stronger performance.",
      "upvotes": 5,
      "discussionId": "69894ae3beecc443208d25be",
      "projectPage": "https://zju-real.github.io/InftyThink-Plus/",
      "githubRepo": "https://github.com/ZJU-REAL/InftyThink-Plus",
      "githubRepoAddedBy": "user",
      "ai_summary": "InftyThink+ uses reinforcement learning to optimize iterative reasoning processes, improving accuracy and efficiency in large language models.",
      "ai_keywords": [
        "chain-of-thought",
        "iterative reasoning",
        "reinforcement learning",
        "trajectory-level reinforcement learning",
        "summarization",
        "reasoning efficiency",
        "inference latency"
      ],
      "githubStars": 3
    },
    "publishedAt": "2026-02-06T13:59:27.000Z",
    "title": "InftyThink+: Effective and Efficient Infinite-Horizon Reasoning via Reinforcement Learning",
    "summary": "Large reasoning models achieve strong performance by scaling inference-time chain-of-thought, but this paradigm suffers from quadratic cost, context length limits, and degraded reasoning due to lost-in-the-middle effects. Iterative reasoning mitigates these issues by periodically summarizing intermediate thoughts, yet existing methods rely on supervised learning or fixed heuristics and fail to optimize when to summarize, what to preserve, and how to resume reasoning. We propose InftyThink+, an end-to-end reinforcement learning framework that optimizes the entire iterative reasoning trajectory, building on model-controlled iteration boundaries and explicit summarization. InftyThink+ adopts a two-stage training scheme with supervised cold-start followed by trajectory-level reinforcement learning, enabling the model to learn strategic summarization and continuation decisions. Experiments on DeepSeek-R1-Distill-Qwen-1.5B show that InftyThink+ improves accuracy by 21% on AIME24 and outperforms conventional long chain-of-thought reinforcement learning by a clear margin, while also generalizing better to out-of-distribution benchmarks. Moreover, InftyThink+ significantly reduces inference latency and accelerates reinforcement learning training, demonstrating improved reasoning efficiency alongside stronger performance.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06960.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 230,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.06663",
      "authors": [
        {
          "_id": "698959b9beecc443208d26c2",
          "name": "Junxian Li",
          "hidden": false
        },
        {
          "_id": "698959b9beecc443208d26c3",
          "name": "Kai Liu",
          "hidden": false
        },
        {
          "_id": "698959b9beecc443208d26c4",
          "name": "Leyang Chen",
          "hidden": false
        },
        {
          "_id": "698959b9beecc443208d26c5",
          "name": "Weida Wang",
          "hidden": false
        },
        {
          "_id": "698959b9beecc443208d26c6",
          "name": "Zhixin Wang",
          "hidden": false
        },
        {
          "_id": "698959b9beecc443208d26c7",
          "name": "Jiaqi Xu",
          "hidden": false
        },
        {
          "_id": "698959b9beecc443208d26c8",
          "name": "Fan Li",
          "hidden": false
        },
        {
          "_id": "698959b9beecc443208d26c9",
          "name": "Renjing Pei",
          "hidden": false
        },
        {
          "_id": "698959b9beecc443208d26ca",
          "name": "Linghe Kong",
          "hidden": false
        },
        {
          "_id": "698959b9beecc443208d26cb",
          "name": "Yulun Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-06T12:47:16.000Z",
      "submittedOnDailyAt": "2026-02-09T01:24:35.879Z",
      "title": "PlanViz: Evaluating Planning-Oriented Image Generation and Editing for Computer-Use Tasks",
      "submittedOnDailyBy": {
        "_id": "656ae4088fb1ddf0d5ec9ac5",
        "avatarUrl": "/avatars/e38468d2c0274f3c0f5732f30a2e3436.svg",
        "isPro": false,
        "fullname": "Junxian Li",
        "user": "Duke-de-Artois",
        "type": "user"
      },
      "summary": "Unified multimodal models (UMMs) have shown impressive capabilities in generating natural images and supporting multimodal reasoning. However, their potential in supporting computer-use planning tasks, which are closely related to our lives, remain underexplored. Image generation and editing in computer-use tasks require capabilities like spatial reasoning and procedural understanding, and it is still unknown whether UMMs have these capabilities to finish these tasks or not. Therefore, we propose PlanViz, a new benchmark designed to evaluate image generation and editing for computer-use tasks. To achieve the goal of our evaluation, we focus on sub-tasks which frequently involve in daily life and require planning steps. Specifically, three new sub-tasks are designed: route planning, work diagramming, and web&UI displaying. We address challenges in data quality ensuring by curating human-annotated questions and reference images, and a quality control process. For challenges of comprehensive and exact evaluation, a task-adaptive score, PlanScore, is proposed. The score helps understanding the correctness, visual quality and efficiency of generated images. Through experiments, we highlight key limitations and opportunities for future research on this topic.",
      "upvotes": 5,
      "discussionId": "698959b9beecc443208d26cc",
      "projectPage": "https://github.com/lijunxian111/PlanViz",
      "githubRepo": "https://github.com/lijunxian111/PlanViz",
      "githubRepoAddedBy": "user",
      "ai_summary": "PlanViz benchmark evaluates unified multimodal models' capabilities in computer-use planning tasks through route planning, work diagramming, and web&UI displaying sub-tasks with a task-adaptive scoring system.",
      "ai_keywords": [
        "unified multimodal models",
        "image generation",
        "image editing",
        "spatial reasoning",
        "procedural understanding",
        "computer-use planning",
        "PlanViz",
        "route planning",
        "work diagramming",
        "web&UI displaying",
        "PlanScore"
      ],
      "githubStars": 15
    },
    "publishedAt": "2026-02-06T07:47:16.000Z",
    "title": "PlanViz: Evaluating Planning-Oriented Image Generation and Editing for Computer-Use Tasks",
    "summary": "Unified multimodal models (UMMs) have shown impressive capabilities in generating natural images and supporting multimodal reasoning. However, their potential in supporting computer-use planning tasks, which are closely related to our lives, remain underexplored. Image generation and editing in computer-use tasks require capabilities like spatial reasoning and procedural understanding, and it is still unknown whether UMMs have these capabilities to finish these tasks or not. Therefore, we propose PlanViz, a new benchmark designed to evaluate image generation and editing for computer-use tasks. To achieve the goal of our evaluation, we focus on sub-tasks which frequently involve in daily life and require planning steps. Specifically, three new sub-tasks are designed: route planning, work diagramming, and web&UI displaying. We address challenges in data quality ensuring by curating human-annotated questions and reference images, and a quality control process. For challenges of comprehensive and exact evaluation, a task-adaptive score, PlanScore, is proposed. The score helps understanding the correctness, visual quality and efficiency of generated images. Through experiments, we highlight key limitations and opportunities for future research on this topic.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06663.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "656ae4088fb1ddf0d5ec9ac5",
      "avatarUrl": "/avatars/e38468d2c0274f3c0f5732f30a2e3436.svg",
      "fullname": "Junxian Li",
      "name": "Duke-de-Artois",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.05367",
      "authors": [
        {
          "_id": "6985919b4ad556f294b7ec9c",
          "name": "Youngcheon You",
          "hidden": false
        },
        {
          "_id": "6985919b4ad556f294b7ec9d",
          "name": "Banseok Lee",
          "hidden": false
        },
        {
          "_id": "6985919b4ad556f294b7ec9e",
          "name": "Minseop Choi",
          "hidden": false
        },
        {
          "_id": "6985919b4ad556f294b7ec9f",
          "name": "Seonyoung Kim",
          "hidden": false
        },
        {
          "_id": "6985919b4ad556f294b7eca0",
          "user": {
            "_id": "6670d2ec92412fd464eac919",
            "avatarUrl": "/avatars/f76013e72d19b12feddd80f3a4b5d71f.svg",
            "isPro": false,
            "fullname": "Hyochan Chong",
            "user": "d7chong",
            "type": "user"
          },
          "name": "Hyochan Chong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-06T18:50:42.521Z",
          "hidden": false
        },
        {
          "_id": "6985919b4ad556f294b7eca1",
          "name": "Changdong Kim",
          "hidden": false
        },
        {
          "_id": "6985919b4ad556f294b7eca2",
          "name": "Youngmin Kim",
          "hidden": false
        },
        {
          "_id": "6985919b4ad556f294b7eca3",
          "name": "Dongkyu Kim",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-05T06:41:11.000Z",
      "submittedOnDailyAt": "2026-02-09T02:54:59.622Z",
      "title": "RaBiT: Residual-Aware Binarization Training for Accurate and Efficient LLMs",
      "submittedOnDailyBy": {
        "_id": "6670d2ec92412fd464eac919",
        "avatarUrl": "/avatars/f76013e72d19b12feddd80f3a4b5d71f.svg",
        "isPro": false,
        "fullname": "Hyochan Chong",
        "user": "d7chong",
        "type": "user"
      },
      "summary": "Efficient deployment of large language models (LLMs) requires extreme quantization, forcing a critical trade-off between low-bit efficiency and performance. Residual binarization enables hardware-friendly, matmul-free inference by stacking binary (pm1) layers, but is plagued by pathological feature co-adaptation. We identify a key failure mode, which we term inter-path adaptation: during quantization-aware training (QAT), parallel residual binary paths learn redundant features, degrading the error-compensation structure and limiting the expressive capacity of the model. While prior work relies on heuristic workarounds (e.g., path freezing) that constrain the solution space, we propose RaBiT, a novel quantization framework that resolves co-adaptation by algorithmically enforcing a residual hierarchy. Its core mechanism sequentially derives each binary path from a single shared full-precision weight, which ensures that every path corrects the error of the preceding one. This process is stabilized by a robust initialization that prioritizes functional preservation over mere weight approximation. RaBiT redefines the 2-bit accuracy-efficiency frontier: it achieves state-of-the-art performance, rivals even hardware-intensive Vector Quantization (VQ) methods, and delivers a 4.49times inference speed-up over full-precision models on an RTX 4090.",
      "upvotes": 5,
      "discussionId": "6985919b4ad556f294b7eca4",
      "ai_summary": "Residual binarization framework RaBiT addresses feature co-adaptation in quantized LLMs through hierarchical path derivation and robust initialization, achieving superior accuracy-efficiency trade-offs.",
      "ai_keywords": [
        "residual binarization",
        "quantization-aware training",
        "QAT",
        "inter-path adaptation",
        "residual hierarchy",
        "binary layers",
        "error-compensation structure",
        "RaBiT",
        "Vector Quantization",
        "VQ",
        "inference speed-up",
        "RTX 4090"
      ],
      "organization": {
        "_id": "686df54910a52f2c2cf03c06",
        "name": "SamsungResearch",
        "fullname": "Samsung Research",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60ffc3e62403168abcae811d/lBrkzrpjrJ8k-3CGLKRLr.jpeg"
      }
    },
    "publishedAt": "2026-02-05T01:41:11.000Z",
    "title": "RaBiT: Residual-Aware Binarization Training for Accurate and Efficient LLMs",
    "summary": "Efficient deployment of large language models (LLMs) requires extreme quantization, forcing a critical trade-off between low-bit efficiency and performance. Residual binarization enables hardware-friendly, matmul-free inference by stacking binary (pm1) layers, but is plagued by pathological feature co-adaptation. We identify a key failure mode, which we term inter-path adaptation: during quantization-aware training (QAT), parallel residual binary paths learn redundant features, degrading the error-compensation structure and limiting the expressive capacity of the model. While prior work relies on heuristic workarounds (e.g., path freezing) that constrain the solution space, we propose RaBiT, a novel quantization framework that resolves co-adaptation by algorithmically enforcing a residual hierarchy. Its core mechanism sequentially derives each binary path from a single shared full-precision weight, which ensures that every path corrects the error of the preceding one. This process is stabilized by a robust initialization that prioritizes functional preservation over mere weight approximation. RaBiT redefines the 2-bit accuracy-efficiency frontier: it achieves state-of-the-art performance, rivals even hardware-intensive Vector Quantization (VQ) methods, and delivers a 4.49times inference speed-up over full-precision models on an RTX 4090.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.05367.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6670d2ec92412fd464eac919",
      "avatarUrl": "/avatars/f76013e72d19b12feddd80f3a4b5d71f.svg",
      "fullname": "Hyochan Chong",
      "name": "d7chong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "686df54910a52f2c2cf03c06",
      "name": "SamsungResearch",
      "fullname": "Samsung Research",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60ffc3e62403168abcae811d/lBrkzrpjrJ8k-3CGLKRLr.jpeg"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2602.05711",
      "authors": [
        {
          "_id": "69884b90beecc443208d2475",
          "name": "Jingze Shi",
          "hidden": false
        },
        {
          "_id": "69884b90beecc443208d2476",
          "name": "Zhangyang Peng",
          "hidden": false
        },
        {
          "_id": "69884b90beecc443208d2477",
          "name": "Yizhang Zhu",
          "hidden": false
        },
        {
          "_id": "69884b90beecc443208d2478",
          "name": "Yifan Wu",
          "hidden": false
        },
        {
          "_id": "69884b90beecc443208d2479",
          "name": "Guang Liu",
          "hidden": false
        },
        {
          "_id": "69884b90beecc443208d247a",
          "name": "Yuyu Luo",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-05T14:37:32.000Z",
      "submittedOnDailyAt": "2026-02-09T00:30:00.301Z",
      "title": "OmniMoE: An Efficient MoE by Orchestrating Atomic Experts at Scale",
      "submittedOnDailyBy": {
        "_id": "673ab3647afcea17eb4378fd",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/673ab3647afcea17eb4378fd/YQB6zSH1LPxBMUYayIURi.png",
        "isPro": false,
        "fullname": "Loser Cheems",
        "user": "JingzeShi",
        "type": "user"
      },
      "summary": "Mixture-of-Experts (MoE) architectures are evolving towards finer granularity to improve parameter efficiency. However, existing MoE designs face an inherent trade-off between the granularity of expert specialization and hardware execution efficiency. We propose OmniMoE, a system-algorithm co-designed framework that pushes expert granularity to its logical extreme. OmniMoE introduces vector-level Atomic Experts, enabling scalable routing and execution within a single MoE layer, while retaining a shared dense MLP branch for general-purpose processing. Although this atomic design maximizes capacity, it poses severe challenges for routing complexity and memory access. To address these, OmniMoE adopts a system-algorithm co-design: (i) a Cartesian Product Router that decomposes the massive index space to reduce routing complexity from O(N) to O(sqrt(N)); and (ii) Expert-Centric Scheduling that inverts the execution order to turn scattered, memory-bound lookups into efficient dense matrix operations. Validated on seven benchmarks, OmniMoE (with 1.7B active parameters) achieves 50.9% zero-shot accuracy across seven benchmarks, outperforming coarse-grained (e.g., DeepSeekMoE) and fine-grained (e.g., PEER) baselines. Crucially, OmniMoE reduces inference latency from 73ms to 6.7ms (a 10.9-fold speedup) compared to PEER, demonstrating that massive-scale fine-grained MoE can be fast and accurate. Our code is open-sourced at https://github.com/flash-algo/omni-moe.",
      "upvotes": 4,
      "discussionId": "69884b90beecc443208d247b",
      "githubRepo": "https://github.com/flash-algo/omni-moe",
      "githubRepoAddedBy": "user",
      "ai_summary": "OmniMoE presents a system-algorithm co-designed framework that achieves fine-grained expert specialization in Mixture-of-Experts architectures through vector-level atomic experts and optimized routing and scheduling mechanisms.",
      "ai_keywords": [
        "Mixture-of-Experts",
        "expert specialization",
        "atomic experts",
        "routing complexity",
        "memory access",
        "Cartesian Product Router",
        "Expert-Centric Scheduling",
        "inference latency",
        "zero-shot accuracy"
      ],
      "githubStars": 46,
      "organization": {
        "_id": "61be9739d2f9358e24ca0a4f",
        "name": "BAAI",
        "fullname": "Beijing Academy of Artificial Intelligence",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664511063789-632c234f42c386ebd2710434.png"
      }
    },
    "publishedAt": "2026-02-05T09:37:32.000Z",
    "title": "OmniMoE: An Efficient MoE by Orchestrating Atomic Experts at Scale",
    "summary": "Mixture-of-Experts (MoE) architectures are evolving towards finer granularity to improve parameter efficiency. However, existing MoE designs face an inherent trade-off between the granularity of expert specialization and hardware execution efficiency. We propose OmniMoE, a system-algorithm co-designed framework that pushes expert granularity to its logical extreme. OmniMoE introduces vector-level Atomic Experts, enabling scalable routing and execution within a single MoE layer, while retaining a shared dense MLP branch for general-purpose processing. Although this atomic design maximizes capacity, it poses severe challenges for routing complexity and memory access. To address these, OmniMoE adopts a system-algorithm co-design: (i) a Cartesian Product Router that decomposes the massive index space to reduce routing complexity from O(N) to O(sqrt(N)); and (ii) Expert-Centric Scheduling that inverts the execution order to turn scattered, memory-bound lookups into efficient dense matrix operations. Validated on seven benchmarks, OmniMoE (with 1.7B active parameters) achieves 50.9% zero-shot accuracy across seven benchmarks, outperforming coarse-grained (e.g., DeepSeekMoE) and fine-grained (e.g., PEER) baselines. Crucially, OmniMoE reduces inference latency from 73ms to 6.7ms (a 10.9-fold speedup) compared to PEER, demonstrating that massive-scale fine-grained MoE can be fast and accurate. Our code is open-sourced at https://github.com/flash-algo/omni-moe.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.05711.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "673ab3647afcea17eb4378fd",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/673ab3647afcea17eb4378fd/YQB6zSH1LPxBMUYayIURi.png",
      "fullname": "Loser Cheems",
      "name": "JingzeShi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 43,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "61be9739d2f9358e24ca0a4f",
      "name": "BAAI",
      "fullname": "Beijing Academy of Artificial Intelligence",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664511063789-632c234f42c386ebd2710434.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.06854",
      "authors": [
        {
          "_id": "69896b56beecc443208d26ee",
          "name": "Mingqian Feng",
          "hidden": false
        },
        {
          "_id": "69896b56beecc443208d26ef",
          "name": "Xiaodong Liu",
          "hidden": false
        },
        {
          "_id": "69896b56beecc443208d26f0",
          "name": "Weiwei Yang",
          "hidden": false
        },
        {
          "_id": "69896b56beecc443208d26f1",
          "name": "Jialin Song",
          "hidden": false
        },
        {
          "_id": "69896b56beecc443208d26f2",
          "name": "Xuekai Zhu",
          "hidden": false
        },
        {
          "_id": "69896b56beecc443208d26f3",
          "name": "Chenliang Xu",
          "hidden": false
        },
        {
          "_id": "69896b56beecc443208d26f4",
          "name": "Jianfeng Gao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/66332a98e39731d65a4b7e45/a_3LkBK51Y0fO2Pmf2uaq.png",
        "https://cdn-uploads.huggingface.co/production/uploads/66332a98e39731d65a4b7e45/vr5XDSyOSXw6D2WZ05pvn.png",
        "https://cdn-uploads.huggingface.co/production/uploads/66332a98e39731d65a4b7e45/YVFcTdCSbiqE-ww4JPpJb.png",
        "https://cdn-uploads.huggingface.co/production/uploads/66332a98e39731d65a4b7e45/HiZycfFyN1JUsteSVVPE0.png"
      ],
      "publishedAt": "2026-02-06T16:44:57.000Z",
      "submittedOnDailyAt": "2026-02-09T02:38:48.605Z",
      "title": "SEMA: Simple yet Effective Learning for Multi-Turn Jailbreak Attacks",
      "submittedOnDailyBy": {
        "_id": "66332a98e39731d65a4b7e45",
        "avatarUrl": "/avatars/193e9ee3ac7488960229d6edddb9d1e9.svg",
        "isPro": true,
        "fullname": "Mingqian Feng",
        "user": "fmmarkmq",
        "type": "user"
      },
      "summary": "Multi-turn jailbreaks capture the real threat model for safety-aligned chatbots, where single-turn attacks are merely a special case. Yet existing approaches break under exploration complexity and intent drift. We propose SEMA, a simple yet effective framework that trains a multi-turn attacker without relying on any existing strategies or external data. SEMA comprises two stages. Prefilling self-tuning enables usable rollouts by fine-tuning on non-refusal, well-structured, multi-turn adversarial prompts that are self-generated with a minimal prefix, thereby stabilizing subsequent learning. Reinforcement learning with intent-drift-aware reward trains the attacker to elicit valid multi-turn adversarial prompts while maintaining the same harmful objective. We anchor harmful intent in multi-turn jailbreaks via an intent-drift-aware reward that combines intent alignment, compliance risk, and level of detail. Our open-loop attack regime avoids dependence on victim feedback, unifies single- and multi-turn settings, and reduces exploration complexity. Across multiple datasets, victim models, and jailbreak judges, our method achieves state-of-the-art (SOTA) attack success rates (ASR), outperforming all single-turn baselines, manually scripted and template-driven multi-turn baselines, as well as our SFT (Supervised Fine-Tuning) and DPO (Direct Preference Optimization) variants. For instance, SEMA performs an average 80.1% ASR@1 across three closed-source and open-source victim models on AdvBench, 33.9% over SOTA. The approach is compact, reproducible, and transfers across targets, providing a stronger and more realistic stress test for large language model (LLM) safety and enabling automatic redteaming to expose and localize failure modes. Our code is available at: https://github.com/fmmarkmq/SEMA.",
      "upvotes": 3,
      "discussionId": "69896b56beecc443208d26f5",
      "ai_summary": "A novel framework called SEMA is introduced that effectively trains multi-turn attackers for large language models without relying on existing strategies or external data, achieving state-of-the-art attack success rates while being compact, reproducible, and transferable across different models and datasets.",
      "ai_keywords": [
        "multi-turn jailbreaks",
        "reinforcement learning",
        "intent-drift-aware reward",
        "supervised fine-tuning",
        "direct preference optimization",
        "attack success rates",
        "adversarial prompts",
        "victim models",
        "jailbreak judges",
        "open-loop attack regime"
      ],
      "organization": {
        "_id": "5e6485f787403103f9f1055e",
        "name": "microsoft",
        "fullname": "Microsoft",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"
      }
    },
    "publishedAt": "2026-02-06T11:44:57.000Z",
    "title": "SEMA: Simple yet Effective Learning for Multi-Turn Jailbreak Attacks",
    "summary": "Multi-turn jailbreaks capture the real threat model for safety-aligned chatbots, where single-turn attacks are merely a special case. Yet existing approaches break under exploration complexity and intent drift. We propose SEMA, a simple yet effective framework that trains a multi-turn attacker without relying on any existing strategies or external data. SEMA comprises two stages. Prefilling self-tuning enables usable rollouts by fine-tuning on non-refusal, well-structured, multi-turn adversarial prompts that are self-generated with a minimal prefix, thereby stabilizing subsequent learning. Reinforcement learning with intent-drift-aware reward trains the attacker to elicit valid multi-turn adversarial prompts while maintaining the same harmful objective. We anchor harmful intent in multi-turn jailbreaks via an intent-drift-aware reward that combines intent alignment, compliance risk, and level of detail. Our open-loop attack regime avoids dependence on victim feedback, unifies single- and multi-turn settings, and reduces exploration complexity. Across multiple datasets, victim models, and jailbreak judges, our method achieves state-of-the-art (SOTA) attack success rates (ASR), outperforming all single-turn baselines, manually scripted and template-driven multi-turn baselines, as well as our SFT (Supervised Fine-Tuning) and DPO (Direct Preference Optimization) variants. For instance, SEMA performs an average 80.1% ASR@1 across three closed-source and open-source victim models on AdvBench, 33.9% over SOTA. The approach is compact, reproducible, and transfers across targets, providing a stronger and more realistic stress test for large language model (LLM) safety and enabling automatic redteaming to expose and localize failure modes. Our code is available at: https://github.com/fmmarkmq/SEMA.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/66332a98e39731d65a4b7e45/a_3LkBK51Y0fO2Pmf2uaq.png",
      "https://cdn-uploads.huggingface.co/production/uploads/66332a98e39731d65a4b7e45/vr5XDSyOSXw6D2WZ05pvn.png",
      "https://cdn-uploads.huggingface.co/production/uploads/66332a98e39731d65a4b7e45/YVFcTdCSbiqE-ww4JPpJb.png",
      "https://cdn-uploads.huggingface.co/production/uploads/66332a98e39731d65a4b7e45/HiZycfFyN1JUsteSVVPE0.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06854.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66332a98e39731d65a4b7e45",
      "avatarUrl": "/avatars/193e9ee3ac7488960229d6edddb9d1e9.svg",
      "fullname": "Mingqian Feng",
      "name": "fmmarkmq",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "5e6485f787403103f9f1055e",
      "name": "microsoft",
      "fullname": "Microsoft",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.05027",
      "authors": [
        {
          "_id": "69871a872d626112378ad69f",
          "name": "Georgii Aparin",
          "hidden": false
        },
        {
          "_id": "69871a872d626112378ad6a0",
          "name": "Tasnima Sadekova",
          "hidden": false
        },
        {
          "_id": "69871a872d626112378ad6a1",
          "name": "Alexey Rukhovich",
          "hidden": false
        },
        {
          "_id": "69871a872d626112378ad6a2",
          "name": "Assel Yermekova",
          "hidden": false
        },
        {
          "_id": "69871a872d626112378ad6a3",
          "name": "Laida Kushnareva",
          "hidden": false
        },
        {
          "_id": "69871a872d626112378ad6a4",
          "name": "Vadim Popov",
          "hidden": false
        },
        {
          "_id": "69871a872d626112378ad6a5",
          "name": "Kristian Kuznetsov",
          "hidden": false
        },
        {
          "_id": "69871a872d626112378ad6a6",
          "name": "Irina Piontkovskaya",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-04T20:29:16.000Z",
      "submittedOnDailyAt": "2026-02-09T05:28:19.089Z",
      "title": "AudioSAE: Towards Understanding of Audio-Processing Models with Sparse AutoEncoders",
      "submittedOnDailyBy": {
        "_id": "636254dc2691058b19d9276a",
        "avatarUrl": "/avatars/36eb0e27e0e321fb0ac513f0d4d67c95.svg",
        "isPro": false,
        "fullname": "Kushnareva",
        "user": "Kushnareva",
        "type": "user"
      },
      "summary": "Sparse Autoencoders (SAEs) are powerful tools for interpreting neural representations, yet their use in audio remains underexplored. We train SAEs across all encoder layers of Whisper and HuBERT, provide an extensive evaluation of their stability, interpretability, and show their practical utility. Over 50% of the features remain consistent across random seeds, and reconstruction quality is preserved. SAE features capture general acoustic and semantic information as well as specific events, including environmental noises and paralinguistic sounds (e.g. laughter, whispering) and disentangle them effectively, requiring removal of only 19-27% of features to erase a concept. Feature steering reduces Whisper's false speech detections by 70% with negligible WER increase, demonstrating real-world applicability. Finally, we find SAE features correlated with human EEG activity during speech perception, indicating alignment with human neural processing. The code and checkpoints are available at https://github.com/audiosae/audiosae_demo.",
      "upvotes": 3,
      "discussionId": "69871a872d626112378ad6a7",
      "ai_summary": "Sparse Autoencoders trained on Whisper and HuBERT models demonstrate stable feature extraction and effective disentanglement of acoustic and semantic information, showing practical applications in audio processing and correlation with human neural activity.",
      "ai_keywords": [
        "Sparse Autoencoders",
        "encoder layers",
        "Whisper",
        "HuBERT",
        "feature steering",
        "false speech detections",
        "WER",
        "EEG activity",
        "speech perception"
      ],
      "organization": {
        "_id": "5f83c275f0801648bf88454a",
        "name": "huawei-noah",
        "fullname": "HUAWEI Noah's Ark Lab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1602470452594-5f83c19ff0801648bf884549.png"
      }
    },
    "publishedAt": "2026-02-04T15:29:16.000Z",
    "title": "AudioSAE: Towards Understanding of Audio-Processing Models with Sparse AutoEncoders",
    "summary": "Sparse Autoencoders (SAEs) are powerful tools for interpreting neural representations, yet their use in audio remains underexplored. We train SAEs across all encoder layers of Whisper and HuBERT, provide an extensive evaluation of their stability, interpretability, and show their practical utility. Over 50% of the features remain consistent across random seeds, and reconstruction quality is preserved. SAE features capture general acoustic and semantic information as well as specific events, including environmental noises and paralinguistic sounds (e.g. laughter, whispering) and disentangle them effectively, requiring removal of only 19-27% of features to erase a concept. Feature steering reduces Whisper's false speech detections by 70% with negligible WER increase, demonstrating real-world applicability. Finally, we find SAE features correlated with human EEG activity during speech perception, indicating alignment with human neural processing. The code and checkpoints are available at https://github.com/audiosae/audiosae_demo.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.05027.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "636254dc2691058b19d9276a",
      "avatarUrl": "/avatars/36eb0e27e0e321fb0ac513f0d4d67c95.svg",
      "fullname": "Kushnareva",
      "name": "Kushnareva",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "5f83c275f0801648bf88454a",
      "name": "huawei-noah",
      "fullname": "HUAWEI Noah's Ark Lab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1602470452594-5f83c19ff0801648bf884549.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.06554",
      "authors": [
        {
          "_id": "69896f0dbeecc443208d26f7",
          "name": "Tianyi Hu",
          "hidden": false
        },
        {
          "_id": "69896f0dbeecc443208d26f8",
          "name": "Qingxu Fu",
          "hidden": false
        },
        {
          "_id": "69896f0dbeecc443208d26f9",
          "name": "Yanxi Chen",
          "hidden": false
        },
        {
          "_id": "69896f0dbeecc443208d26fa",
          "name": "Zhaoyang Liu",
          "hidden": false
        },
        {
          "_id": "69896f0dbeecc443208d26fb",
          "name": "Bolin Ding",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-06T09:57:23.000Z",
      "submittedOnDailyAt": "2026-02-09T02:52:23.412Z",
      "title": "SeeUPO: Sequence-Level Agentic-RL with Convergence Guarantees",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Reinforcement learning (RL) has emerged as the predominant paradigm for training large language model (LLM)-based AI agents. However, existing backbone RL algorithms lack verified convergence guarantees in agentic scenarios, especially in multi-turn settings, which can lead to training instability and failure to converge to optimal policies.\n  In this paper, we systematically analyze how different combinations of policy update mechanisms and advantage estimation methods affect convergence properties in single/multi-turn scenarios. We find that REINFORCE with Group Relative Advantage Estimation (GRAE) can converge to the globally optimal under undiscounted conditions, but the combination of PPO & GRAE breaks PPO's original monotonic improvement property. Furthermore, we demonstrate that mainstream backbone RL algorithms cannot simultaneously achieve both critic-free and convergence guarantees in multi-turn scenarios.\n  To address this, we propose SeeUPO (Sequence-level Sequential Update Policy Optimization), a critic-free approach with convergence guarantees for multi-turn interactions. SeeUPO models multi-turn interaction as sequentially executed multi-agent bandit problems. Through turn-by-turn sequential policy updates in reverse execution order, it ensures monotonic improvement and convergence to global optimal solution via backward induction.\n  Experiments on AppWorld and BFCL v4 demonstrate SeeUPO's substantial improvements over existing backbone algorithms: relative gains of 43.3%-54.6% on Qwen3-14B and 24.1%-41.9% on Qwen2.5-14B (averaged across benchmarks), along with superior training stability.",
      "upvotes": 2,
      "discussionId": "69896f0dbeecc443208d26fc",
      "ai_summary": "SeeUPO is a critic-free reinforcement learning method that ensures convergence guarantees in multi-turn agent interactions by modeling sequential decision-making as multi-agent bandit problems and using backward induction for policy updates.",
      "ai_keywords": [
        "reinforcement learning",
        "policy update mechanisms",
        "advantage estimation",
        "convergence guarantees",
        "multi-turn scenarios",
        "REINFORCE",
        "Group Relative Advantage Estimation",
        "PPO",
        "critic-free",
        "sequential policy updates",
        "backward induction",
        "multi-agent bandit problems"
      ],
      "organization": {
        "_id": "6925b20fed452d1567c012d3",
        "name": "Tongyi-MAI",
        "fullname": "Tongyi-MAI",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64379d79fac5ea753f1c10f3/fxHO6QoYjdv9_LTyiUD3g.jpeg"
      }
    },
    "publishedAt": "2026-02-06T04:57:23.000Z",
    "title": "SeeUPO: Sequence-Level Agentic-RL with Convergence Guarantees",
    "summary": "Reinforcement learning (RL) has emerged as the predominant paradigm for training large language model (LLM)-based AI agents. However, existing backbone RL algorithms lack verified convergence guarantees in agentic scenarios, especially in multi-turn settings, which can lead to training instability and failure to converge to optimal policies.\n  In this paper, we systematically analyze how different combinations of policy update mechanisms and advantage estimation methods affect convergence properties in single/multi-turn scenarios. We find that REINFORCE with Group Relative Advantage Estimation (GRAE) can converge to the globally optimal under undiscounted conditions, but the combination of PPO & GRAE breaks PPO's original monotonic improvement property. Furthermore, we demonstrate that mainstream backbone RL algorithms cannot simultaneously achieve both critic-free and convergence guarantees in multi-turn scenarios.\n  To address this, we propose SeeUPO (Sequence-level Sequential Update Policy Optimization), a critic-free approach with convergence guarantees for multi-turn interactions. SeeUPO models multi-turn interaction as sequentially executed multi-agent bandit problems. Through turn-by-turn sequential policy updates in reverse execution order, it ensures monotonic improvement and convergence to global optimal solution via backward induction.\n  Experiments on AppWorld and BFCL v4 demonstrate SeeUPO's substantial improvements over existing backbone algorithms: relative gains of 43.3%-54.6% on Qwen3-14B and 24.1%-41.9% on Qwen2.5-14B (averaged across benchmarks), along with superior training stability.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06554.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 230,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "6925b20fed452d1567c012d3",
      "name": "Tongyi-MAI",
      "fullname": "Tongyi-MAI",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64379d79fac5ea753f1c10f3/fxHO6QoYjdv9_LTyiUD3g.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.06471",
      "authors": [
        {
          "_id": "6989492abeecc443208d259d",
          "name": "Feng-Ting Liao",
          "hidden": false
        },
        {
          "_id": "6989492abeecc443208d259e",
          "name": "Meng-Hsi Chen",
          "hidden": false
        },
        {
          "_id": "6989492abeecc443208d259f",
          "name": "Guan-Ting Yi",
          "hidden": false
        },
        {
          "_id": "6989492abeecc443208d25a0",
          "name": "Da-shan Shiu",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-06T07:55:30.000Z",
      "submittedOnDailyAt": "2026-02-09T00:11:07.040Z",
      "title": "Revisiting the Shape Convention of Transformer Language Models",
      "submittedOnDailyBy": {
        "_id": "643fb7332397d8eef5b844cd",
        "avatarUrl": "/avatars/e403a19fc13e478d5929c67028230b0e.svg",
        "isPro": false,
        "fullname": "Feng-Ting Liao",
        "user": "FengTing",
        "type": "user"
      },
      "summary": "Dense Transformer language models have largely adhered to one consistent architectural shape: each layer consists of an attention module followed by a feed-forward network (FFN) with a narrow-wide-narrow MLP, allocating most parameters to the MLP at expansion ratios between 2 and 4. Motivated by recent results that residual wide-narrow-wide (hourglass) MLPs offer superior function approximation capabilities, we revisit the long-standing MLP shape convention in Transformer, challenging the necessity of the narrow-wide-narrow design. To study this, we develop a Transformer variant that replaces the conventional FFN with a deeper hourglass-shaped FFN, comprising a stack of hourglass sub-MLPs connected by residual pathways. We posit that a deeper but lighter hourglass FFN can serve as a competitive alternative to the conventional FFN, and that parameters saved by using a lighter hourglass FFN can be more effectively utilized, such as by enlarging model hidden dimensions under fixed budgets. We confirm these through empirical validations across model scales: hourglass FFNs outperform conventional FFNs up to 400M and achieve comparable performance at larger scales to 1B parameters; hourglass FFN variants with reduced FFN and increased attention parameters show consistent improvements over conventional configurations at matched budgets. Together, these findings shed new light on recent work and prompt a rethinking of the narrow-wide-narrow MLP convention and the balance between attention and FFN towards efficient and expressive modern language models.",
      "upvotes": 1,
      "discussionId": "6989492abeecc443208d25a1",
      "ai_summary": "Replacing conventional feed-forward networks with hourglass-shaped MLPs in Transformers improves model efficiency and performance by enabling better parameter utilization and competitive scaling.",
      "ai_keywords": [
        "Transformer",
        "feed-forward network",
        "MLP",
        "attention module",
        "residual pathways",
        "hourglass MLP",
        "narrow-wide-narrow",
        "model scaling",
        "parameter efficiency"
      ],
      "organization": {
        "_id": "6388e08c5a3d2a335624705b",
        "name": "MediaTek-Research",
        "fullname": "MediaTek Research",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1669948083850-6213410828005421265b27d3.jpeg"
      }
    },
    "publishedAt": "2026-02-06T02:55:30.000Z",
    "title": "Revisiting the Shape Convention of Transformer Language Models",
    "summary": "Dense Transformer language models have largely adhered to one consistent architectural shape: each layer consists of an attention module followed by a feed-forward network (FFN) with a narrow-wide-narrow MLP, allocating most parameters to the MLP at expansion ratios between 2 and 4. Motivated by recent results that residual wide-narrow-wide (hourglass) MLPs offer superior function approximation capabilities, we revisit the long-standing MLP shape convention in Transformer, challenging the necessity of the narrow-wide-narrow design. To study this, we develop a Transformer variant that replaces the conventional FFN with a deeper hourglass-shaped FFN, comprising a stack of hourglass sub-MLPs connected by residual pathways. We posit that a deeper but lighter hourglass FFN can serve as a competitive alternative to the conventional FFN, and that parameters saved by using a lighter hourglass FFN can be more effectively utilized, such as by enlarging model hidden dimensions under fixed budgets. We confirm these through empirical validations across model scales: hourglass FFNs outperform conventional FFNs up to 400M and achieve comparable performance at larger scales to 1B parameters; hourglass FFN variants with reduced FFN and increased attention parameters show consistent improvements over conventional configurations at matched budgets. Together, these findings shed new light on recent work and prompt a rethinking of the narrow-wide-narrow MLP convention and the balance between attention and FFN towards efficient and expressive modern language models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06471.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643fb7332397d8eef5b844cd",
      "avatarUrl": "/avatars/e403a19fc13e478d5929c67028230b0e.svg",
      "fullname": "Feng-Ting Liao",
      "name": "FengTing",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "6388e08c5a3d2a335624705b",
      "name": "MediaTek-Research",
      "fullname": "MediaTek Research",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1669948083850-6213410828005421265b27d3.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.06139",
      "authors": [
        {
          "_id": "69894d1dbeecc443208d25fb",
          "name": "Ashish Seth",
          "hidden": false
        },
        {
          "_id": "69894d1dbeecc443208d25fc",
          "name": "Xinhao Mei",
          "hidden": false
        },
        {
          "_id": "69894d1dbeecc443208d25fd",
          "name": "Changsheng Zhao",
          "hidden": false
        },
        {
          "_id": "69894d1dbeecc443208d25fe",
          "name": "Varun Nagaraja",
          "hidden": false
        },
        {
          "_id": "69894d1dbeecc443208d25ff",
          "name": "Ernie Chang",
          "hidden": false
        },
        {
          "_id": "69894d1dbeecc443208d2600",
          "name": "Gregory P. Meyer",
          "hidden": false
        },
        {
          "_id": "69894d1dbeecc443208d2601",
          "name": "Gael Le Lan",
          "hidden": false
        },
        {
          "_id": "69894d1dbeecc443208d2602",
          "name": "Yunyang Xiong",
          "hidden": false
        },
        {
          "_id": "69894d1dbeecc443208d2603",
          "name": "Vikas Chandra",
          "hidden": false
        },
        {
          "_id": "69894d1dbeecc443208d2604",
          "name": "Yangyang Shi",
          "hidden": false
        },
        {
          "_id": "69894d1dbeecc443208d2605",
          "name": "Dinesh Manocha",
          "hidden": false
        },
        {
          "_id": "69894d1dbeecc443208d2606",
          "name": "Zhipeng Cai",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-05T19:16:55.000Z",
      "submittedOnDailyAt": "2026-02-09T00:44:50.275Z",
      "title": "EgoAVU: Egocentric Audio-Visual Understanding",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Understanding egocentric videos plays a vital role for embodied intelligence. Recent multi-modal large language models (MLLMs) can accept both visual and audio inputs. However, due to the challenge of obtaining text labels with coherent joint-modality information, whether MLLMs can jointly understand both modalities in egocentric videos remains under-explored. To address this problem, we introduce EgoAVU, a scalable data engine to automatically generate egocentric audio-visual narrations, questions, and answers. EgoAVU enriches human narrations with multimodal context and generates audio-visual narrations through cross-modal correlation modeling. Token-based video filtering and modular, graph-based curation ensure both data diversity and quality. Leveraging EgoAVU, we construct EgoAVU-Instruct, a large-scale training dataset of 3M samples, and EgoAVU-Bench, a manually verified evaluation split covering diverse tasks. EgoAVU-Bench clearly reveals the limitations of existing MLLMs: they bias heavily toward visual signals, often neglecting audio cues or failing to correspond audio with the visual source. Finetuning MLLMs on EgoAVU-Instruct effectively addresses this issue, enabling up to 113% performance improvement on EgoAVU-Bench. Such benefits also transfer to other benchmarks such as EgoTempo and EgoIllusion, achieving up to 28% relative performance gain. Code will be released to the community.",
      "upvotes": 1,
      "discussionId": "69894d1dbeecc443208d2607",
      "ai_summary": "Multi-modal large language models struggle to jointly understand audio and visual signals in egocentric videos, but a new scalable data engine and dataset significantly improve their performance through targeted fine-tuning.",
      "ai_keywords": [
        "multi-modal large language models",
        "egocentric videos",
        "audio-visual narrations",
        "cross-modal correlation modeling",
        "token-based video filtering",
        "modular curation",
        "graph-based curation",
        "EgoAVU-Instruct",
        "EgoAVU-Bench",
        "EgoTempo",
        "EgoIllusion"
      ]
    },
    "publishedAt": "2026-02-05T14:16:55.000Z",
    "title": "EgoAVU: Egocentric Audio-Visual Understanding",
    "summary": "Understanding egocentric videos plays a vital role for embodied intelligence. Recent multi-modal large language models (MLLMs) can accept both visual and audio inputs. However, due to the challenge of obtaining text labels with coherent joint-modality information, whether MLLMs can jointly understand both modalities in egocentric videos remains under-explored. To address this problem, we introduce EgoAVU, a scalable data engine to automatically generate egocentric audio-visual narrations, questions, and answers. EgoAVU enriches human narrations with multimodal context and generates audio-visual narrations through cross-modal correlation modeling. Token-based video filtering and modular, graph-based curation ensure both data diversity and quality. Leveraging EgoAVU, we construct EgoAVU-Instruct, a large-scale training dataset of 3M samples, and EgoAVU-Bench, a manually verified evaluation split covering diverse tasks. EgoAVU-Bench clearly reveals the limitations of existing MLLMs: they bias heavily toward visual signals, often neglecting audio cues or failing to correspond audio with the visual source. Finetuning MLLMs on EgoAVU-Instruct effectively addresses this issue, enabling up to 113% performance improvement on EgoAVU-Bench. Such benefits also transfer to other benchmarks such as EgoTempo and EgoIllusion, achieving up to 28% relative performance gain. Code will be released to the community.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06139.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 230,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.06129",
      "authors": [
        {
          "_id": "69899348beecc443208d2778",
          "user": {
            "_id": "67abad973d3f1b93ddcab1f0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/-pzH6sZ-ktqRUnsvqCh1-.jpeg",
            "isPro": false,
            "fullname": "Olaf Laitinen-Fredriksson, PhD, M.Sc.",
            "user": "olaflaitinen",
            "type": "user"
          },
          "name": "Olaf Yunus Laitinen Imanov",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-09T08:27:38.562Z",
          "hidden": false
        },
        {
          "_id": "69899348beecc443208d2779",
          "name": "Derya Umut Kulali",
          "hidden": false
        },
        {
          "_id": "69899348beecc443208d277a",
          "name": "Taner Yilmaz",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-05T19:01:56.000Z",
      "submittedOnDailyAt": "2026-02-09T05:27:44.623Z",
      "title": "Urban Spatio-Temporal Foundation Models for Climate-Resilient Housing: Scaling Diffusion Transformers for Disaster Risk Prediction",
      "submittedOnDailyBy": {
        "_id": "67abad973d3f1b93ddcab1f0",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/-pzH6sZ-ktqRUnsvqCh1-.jpeg",
        "isPro": false,
        "fullname": "Olaf Laitinen-Fredriksson, PhD, M.Sc.",
        "user": "olaflaitinen",
        "type": "user"
      },
      "summary": "Climate hazards increasingly disrupt urban transportation and emergency-response operations by damaging housing stock, degrading infrastructure, and reducing network accessibility. This paper presents Skjold-DiT, a diffusion-transformer framework that integrates heterogeneous spatio-temporal urban data to forecast building-level climate-risk indicators while explicitly incorporating transportation-network structure and accessibility signals relevant to intelligent vehicles (e.g., emergency reachability and evacuation-route constraints). Concretely, Skjold-DiT enables hazard-conditioned routing constraints by producing calibrated, uncertainty-aware accessibility layers (reachability, travel-time inflation, and route redundancy) that can be consumed by intelligent-vehicle routing and emergency dispatch systems. Skjold-DiT combines: (1) Fjell-Prompt, a prompt-based conditioning interface designed to support cross-city transfer; (2) Norrland-Fusion, a cross-modal attention mechanism unifying hazard maps/imagery, building attributes, demographics, and transportation infrastructure into a shared latent representation; and (3) Valkyrie-Forecast, a counterfactual simulator for generating probabilistic risk trajectories under intervention prompts. We introduce the Baltic-Caspian Urban Resilience (BCUR) dataset with 847,392 building-level observations across six cities, including multi-hazard annotations (e.g., flood and heat indicators) and transportation accessibility features. Experiments evaluate prediction quality, cross-city generalization, calibration, and downstream transportation-relevant outcomes, including reachability and hazard-conditioned travel times under counterfactual interventions.",
      "upvotes": 1,
      "discussionId": "69899349beecc443208d277b",
      "ai_summary": "A diffusion-transformer framework integrates spatio-temporal urban data to predict building-level climate risks while incorporating transportation network structures for emergency response applications.",
      "ai_keywords": [
        "diffusion-transformer framework",
        "spatio-temporal urban data",
        "climate-risk indicators",
        "transportation-network structure",
        "accessibility signals",
        "intelligent vehicles",
        "hazard-conditioned routing",
        "uncertainty-aware accessibility layers",
        "prompt-based conditioning",
        "cross-modal attention mechanism",
        "counterfactual simulator",
        "probabilistic risk trajectories",
        "cross-city transfer",
        "shared latent representation"
      ]
    },
    "publishedAt": "2026-02-05T14:01:56.000Z",
    "title": "Urban Spatio-Temporal Foundation Models for Climate-Resilient Housing: Scaling Diffusion Transformers for Disaster Risk Prediction",
    "summary": "Climate hazards increasingly disrupt urban transportation and emergency-response operations by damaging housing stock, degrading infrastructure, and reducing network accessibility. This paper presents Skjold-DiT, a diffusion-transformer framework that integrates heterogeneous spatio-temporal urban data to forecast building-level climate-risk indicators while explicitly incorporating transportation-network structure and accessibility signals relevant to intelligent vehicles (e.g., emergency reachability and evacuation-route constraints). Concretely, Skjold-DiT enables hazard-conditioned routing constraints by producing calibrated, uncertainty-aware accessibility layers (reachability, travel-time inflation, and route redundancy) that can be consumed by intelligent-vehicle routing and emergency dispatch systems. Skjold-DiT combines: (1) Fjell-Prompt, a prompt-based conditioning interface designed to support cross-city transfer; (2) Norrland-Fusion, a cross-modal attention mechanism unifying hazard maps/imagery, building attributes, demographics, and transportation infrastructure into a shared latent representation; and (3) Valkyrie-Forecast, a counterfactual simulator for generating probabilistic risk trajectories under intervention prompts. We introduce the Baltic-Caspian Urban Resilience (BCUR) dataset with 847,392 building-level observations across six cities, including multi-hazard annotations (e.g., flood and heat indicators) and transportation accessibility features. Experiments evaluate prediction quality, cross-city generalization, calibration, and downstream transportation-relevant outcomes, including reachability and hazard-conditioned travel times under counterfactual interventions.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06129.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67abad973d3f1b93ddcab1f0",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/-pzH6sZ-ktqRUnsvqCh1-.jpeg",
      "fullname": "Olaf Laitinen-Fredriksson, PhD, M.Sc.",
      "name": "olaflaitinen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9,
      "isUserFollowing": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2602.04454",
      "authors": [
        {
          "_id": "6985bb7b4ad556f294b7ed95",
          "user": {
            "_id": "66c98c27fafc0fc87c280749",
            "avatarUrl": "/avatars/c71db3bee0fcd9aabcc38fd871d1cb75.svg",
            "isPro": false,
            "fullname": "Tianming Liang",
            "user": "liangtm",
            "type": "user"
          },
          "name": "Tianming Liang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-06T18:50:11.951Z",
          "hidden": false
        },
        {
          "_id": "6985bb7b4ad556f294b7ed96",
          "name": "Qirui Du",
          "hidden": false
        },
        {
          "_id": "6985bb7b4ad556f294b7ed97",
          "name": "Jian-Fang Hu",
          "hidden": false
        },
        {
          "_id": "6985bb7b4ad556f294b7ed98",
          "name": "Haichao Jiang",
          "hidden": false
        },
        {
          "_id": "6985bb7b4ad556f294b7ed99",
          "name": "Zicheng Lin",
          "hidden": false
        },
        {
          "_id": "6985bb7b4ad556f294b7ed9a",
          "name": "Wei-Shi Zheng",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-04T11:33:16.000Z",
      "submittedOnDailyAt": "2026-02-09T05:01:54.003Z",
      "title": "Seg-ReSearch: Segmentation with Interleaved Reasoning and External Search",
      "submittedOnDailyBy": {
        "_id": "66c98c27fafc0fc87c280749",
        "avatarUrl": "/avatars/c71db3bee0fcd9aabcc38fd871d1cb75.svg",
        "isPro": false,
        "fullname": "Tianming Liang",
        "user": "liangtm",
        "type": "user"
      },
      "summary": "Segmentation based on language has been a popular topic in computer vision. While recent advances in multimodal large language models (MLLMs) have endowed segmentation systems with reasoning capabilities, these efforts remain confined by the frozen internal knowledge of MLLMs, which limits their potential for real-world scenarios that involve up-to-date information or domain-specific concepts. In this work, we propose Seg-ReSearch, a novel segmentation paradigm that overcomes the knowledge bottleneck of existing approaches. By enabling interleaved reasoning and external search, Seg-ReSearch empowers segmentation systems to handle dynamic, open-world queries that extend beyond the frozen knowledge of MLLMs. To effectively train this capability, we introduce a hierarchical reward design that harmonizes initial guidance with progressive incentives, mitigating the dilemma between sparse outcome signals and rigid step-wise supervision. For evaluation, we construct OK-VOS, a challenging benchmark that explicitly requires outside knowledge for video object segmentation. Experiments on OK-VOS and two existing reasoning segmentation benchmarks demonstrate that our Seg-ReSearch improves state-of-the-art approaches by a substantial margin. Code and data will be released at https://github.com/iSEE-Laboratory/Seg-ReSearch.",
      "upvotes": 1,
      "discussionId": "6985bb7b4ad556f294b7ed9b",
      "githubRepo": "https://github.com/iSEE-Laboratory/Seg-ReSearch",
      "githubRepoAddedBy": "user",
      "ai_summary": "Seg-ReSearch introduces a novel segmentation approach that combines interleaved reasoning with external search to overcome limitations of frozen MLLM knowledge, using hierarchical reward design for training and demonstrating superior performance on video object segmentation benchmarks.",
      "ai_keywords": [
        "multimodal large language models",
        "segmentation",
        "interleaved reasoning",
        "external search",
        "knowledge bottleneck",
        "hierarchical reward design",
        "video object segmentation",
        "OK-VOS benchmark"
      ],
      "githubStars": 10,
      "organization": {
        "_id": "6884742647f9088912113d8d",
        "name": "iSEE-Laboratory",
        "fullname": "iSEE-Laboratory",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67067633351e0c16a5c27497/ldEqoVV-aKBa-UJWJnXwk.jpeg"
      }
    },
    "publishedAt": "2026-02-04T06:33:16.000Z",
    "title": "Seg-ReSearch: Segmentation with Interleaved Reasoning and External Search",
    "summary": "Segmentation based on language has been a popular topic in computer vision. While recent advances in multimodal large language models (MLLMs) have endowed segmentation systems with reasoning capabilities, these efforts remain confined by the frozen internal knowledge of MLLMs, which limits their potential for real-world scenarios that involve up-to-date information or domain-specific concepts. In this work, we propose Seg-ReSearch, a novel segmentation paradigm that overcomes the knowledge bottleneck of existing approaches. By enabling interleaved reasoning and external search, Seg-ReSearch empowers segmentation systems to handle dynamic, open-world queries that extend beyond the frozen knowledge of MLLMs. To effectively train this capability, we introduce a hierarchical reward design that harmonizes initial guidance with progressive incentives, mitigating the dilemma between sparse outcome signals and rigid step-wise supervision. For evaluation, we construct OK-VOS, a challenging benchmark that explicitly requires outside knowledge for video object segmentation. Experiments on OK-VOS and two existing reasoning segmentation benchmarks demonstrate that our Seg-ReSearch improves state-of-the-art approaches by a substantial margin. Code and data will be released at https://github.com/iSEE-Laboratory/Seg-ReSearch.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.04454.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66c98c27fafc0fc87c280749",
      "avatarUrl": "/avatars/c71db3bee0fcd9aabcc38fd871d1cb75.svg",
      "fullname": "Tianming Liang",
      "name": "liangtm",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "6884742647f9088912113d8d",
      "name": "iSEE-Laboratory",
      "fullname": "iSEE-Laboratory",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67067633351e0c16a5c27497/ldEqoVV-aKBa-UJWJnXwk.jpeg"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2602.06079",
      "authors": [
        {
          "_id": "69895559beecc443208d26a2",
          "name": "Liangyu Wang",
          "hidden": false
        },
        {
          "_id": "69895559beecc443208d26a3",
          "name": "Siqi Zhang",
          "hidden": false
        },
        {
          "_id": "69895559beecc443208d26a4",
          "name": "Junjie Wang",
          "hidden": false
        },
        {
          "_id": "69895559beecc443208d26a5",
          "name": "Yiming Dong",
          "hidden": false
        },
        {
          "_id": "69895559beecc443208d26a6",
          "name": "Bo Zheng",
          "hidden": false
        },
        {
          "_id": "69895559beecc443208d26a7",
          "name": "Zihan Qiu",
          "hidden": false
        },
        {
          "_id": "69895559beecc443208d26a8",
          "name": "Shengkun Tang",
          "hidden": false
        },
        {
          "_id": "69895559beecc443208d26a9",
          "name": "Di Wang",
          "hidden": false
        },
        {
          "_id": "69895559beecc443208d26aa",
          "name": "Rui Men",
          "hidden": false
        },
        {
          "_id": "69895559beecc443208d26ab",
          "name": "Dayiheng Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-04T07:38:24.000Z",
      "submittedOnDailyAt": "2026-02-09T05:54:16.367Z",
      "title": "Canzona: A Unified, Asynchronous, and Load-Balanced Framework for Distributed Matrix-based Optimizers",
      "submittedOnDailyBy": {
        "_id": "66224a84afbc88c1e4881ad7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66224a84afbc88c1e4881ad7/fGDiIiqhTBQri3khSqNcU.jpeg",
        "isPro": false,
        "fullname": "Liangyu Wang",
        "user": "ly4096",
        "type": "user"
      },
      "summary": "The scaling of Large Language Models (LLMs) drives interest in matrix-based optimizers (e.g., Shampoo, Muon, SOAP) for their convergence efficiency; yet their requirement for holistic updates conflicts with the tensor fragmentation in distributed frameworks like Megatron. Existing solutions are suboptimal: synchronous approaches suffer from computational redundancy, while layer-wise partitioning fails to reconcile this conflict without violating the geometric constraints of efficient communication primitives. To bridge this gap, we propose Canzona, a Unified, Asynchronous, and Load-Balanced framework that decouples logical optimizer assignment from physical parameter distribution. For Data Parallelism, we introduce an alpha-Balanced Static Partitioning strategy that respects atomicity while neutralizing the load imbalance. For Tensor Parallelism, we design an Asynchronous Compute pipeline utilizing Micro-Group Scheduling to batch fragmented updates and hide reconstruction overhead. Extensive evaluations on the Qwen3 model family (up to 32B parameters) on 256 GPUs demonstrate that our approach preserves the efficiency of established parallel architectures, achieving a 1.57x speedup in end-to-end iteration time and reducing optimizer step latency by 5.8x compared to the baseline.",
      "upvotes": 1,
      "discussionId": "6989555abeecc443208d26ac",
      "ai_summary": "Canzona presents a unified asynchronous framework that addresses the conflict between matrix-based optimizers and distributed tensor fragmentation in LLM training, improving efficiency and reducing latency.",
      "ai_keywords": [
        "Large Language Models",
        "matrix-based optimizers",
        "Shampoo",
        "Muon",
        "SOAP",
        "distributed frameworks",
        "Megatron",
        "synchronous approaches",
        "layer-wise partitioning",
        "geometric constraints",
        "Canzona",
        "Data Parallelism",
        "alpha-Balanced Static Partitioning",
        "tensor fragmentation",
        "Tensor Parallelism",
        "Asynchronous Compute pipeline",
        "Micro-Group Scheduling",
        "optimizer step latency",
        "end-to-end iteration time"
      ],
      "organization": {
        "_id": "64c8b5837fe12ecd0a7e92eb",
        "name": "Qwen",
        "fullname": "Qwen",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"
      }
    },
    "publishedAt": "2026-02-04T02:38:24.000Z",
    "title": "Canzona: A Unified, Asynchronous, and Load-Balanced Framework for Distributed Matrix-based Optimizers",
    "summary": "The scaling of Large Language Models (LLMs) drives interest in matrix-based optimizers (e.g., Shampoo, Muon, SOAP) for their convergence efficiency; yet their requirement for holistic updates conflicts with the tensor fragmentation in distributed frameworks like Megatron. Existing solutions are suboptimal: synchronous approaches suffer from computational redundancy, while layer-wise partitioning fails to reconcile this conflict without violating the geometric constraints of efficient communication primitives. To bridge this gap, we propose Canzona, a Unified, Asynchronous, and Load-Balanced framework that decouples logical optimizer assignment from physical parameter distribution. For Data Parallelism, we introduce an alpha-Balanced Static Partitioning strategy that respects atomicity while neutralizing the load imbalance. For Tensor Parallelism, we design an Asynchronous Compute pipeline utilizing Micro-Group Scheduling to batch fragmented updates and hide reconstruction overhead. Extensive evaluations on the Qwen3 model family (up to 32B parameters) on 256 GPUs demonstrate that our approach preserves the efficiency of established parallel architectures, achieving a 1.57x speedup in end-to-end iteration time and reducing optimizer step latency by 5.8x compared to the baseline.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06079.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66224a84afbc88c1e4881ad7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66224a84afbc88c1e4881ad7/fGDiIiqhTBQri3khSqNcU.jpeg",
      "fullname": "Liangyu Wang",
      "name": "ly4096",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "64c8b5837fe12ecd0a7e92eb",
      "name": "Qwen",
      "fullname": "Qwen",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.23039",
      "authors": [
        {
          "_id": "6985bff8e387e657cdfcbce2",
          "user": {
            "_id": "696c8dc1c083124a6e6bc0f4",
            "avatarUrl": "/avatars/833e1cef1f077d2882669db63c259011.svg",
            "isPro": false,
            "fullname": "YIZHI LIU",
            "user": "leon0923",
            "type": "user"
          },
          "name": "Yizhi Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-06T18:50:09.883Z",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-30T14:47:18.000Z",
      "submittedOnDailyAt": "2026-02-09T03:17:27.263Z",
      "title": "Avoiding Premature Collapse: Adaptive Annealing for Entropy-Regularized Structural Inference",
      "submittedOnDailyBy": {
        "_id": "696c8dc1c083124a6e6bc0f4",
        "avatarUrl": "/avatars/833e1cef1f077d2882669db63c259011.svg",
        "isPro": false,
        "fullname": "YIZHI LIU",
        "user": "leon0923",
        "type": "user"
      },
      "summary": "Differentiable matching layers and residual connection paradigms, often implemented via entropy-regularized Optimal Transport (OT), serve as critical mechanisms in structural prediction and architectural scaling. However, recovering discrete permutations or maintaining identity mappings via annealing εto 0 is notoriously unstable. In this work, we identify a fundamental mechanism for this failure: Premature Mode Collapse. By analyzing the non-normal dynamics of the Sinkhorn fixed-point map, we reveal a theoretical thermodynamic speed limit: standard exponential cooling outpaces the contraction rate of the inference operator, which degrades as O(1/ε). To address this, we propose Efficient Piecewise Hybrid Adaptive Stability Control (EPH-ASC), an adaptive scheduling algorithm that monitors the stability of the inference process. We demonstrate that EPH-ASC is essential for stabilizing Manifold-Constrained Hyper-Connections (mHC) during large-scale training on the FineWeb-Edu dataset, effectively preventing late-stage gradient explosions by enforcing a linear stability law.",
      "upvotes": 1,
      "discussionId": "6985bff9e387e657cdfcbce3",
      "ai_summary": "Researchers identify and address premature mode collapse in optimal transport-based structural prediction models through an adaptive stability control algorithm that prevents gradient explosions during large-scale training.",
      "ai_keywords": [
        "Optimal Transport",
        "entropy-regularized Optimal Transport",
        "Sinkhorn fixed-point map",
        "mode collapse",
        "adaptive scheduling algorithm",
        "manifold-constrained hyper-connections",
        "gradient explosions",
        "stability analysis"
      ]
    },
    "publishedAt": "2026-01-30T09:47:18.000Z",
    "title": "Avoiding Premature Collapse: Adaptive Annealing for Entropy-Regularized Structural Inference",
    "summary": "Differentiable matching layers and residual connection paradigms, often implemented via entropy-regularized Optimal Transport (OT), serve as critical mechanisms in structural prediction and architectural scaling. However, recovering discrete permutations or maintaining identity mappings via annealing εto 0 is notoriously unstable. In this work, we identify a fundamental mechanism for this failure: Premature Mode Collapse. By analyzing the non-normal dynamics of the Sinkhorn fixed-point map, we reveal a theoretical thermodynamic speed limit: standard exponential cooling outpaces the contraction rate of the inference operator, which degrades as O(1/ε). To address this, we propose Efficient Piecewise Hybrid Adaptive Stability Control (EPH-ASC), an adaptive scheduling algorithm that monitors the stability of the inference process. We demonstrate that EPH-ASC is essential for stabilizing Manifold-Constrained Hyper-Connections (mHC) during large-scale training on the FineWeb-Edu dataset, effectively preventing late-stage gradient explosions by enforcing a linear stability law.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.23039.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "696c8dc1c083124a6e6bc0f4",
      "avatarUrl": "/avatars/833e1cef1f077d2882669db63c259011.svg",
      "fullname": "YIZHI LIU",
      "name": "leon0923",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2602.06883",
      "authors": [
        {
          "_id": "698989dfbeecc443208d2749",
          "user": {
            "_id": "6661f79ac36ae4c83f3213e4",
            "avatarUrl": "/avatars/2e48f052fdf37b5b06d101a6a3232eea.svg",
            "isPro": false,
            "fullname": "Ambroise Odonnat",
            "user": "ambroiseodt",
            "type": "user"
          },
          "name": "Ambroise Odonnat",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-09T08:27:50.665Z",
          "hidden": false
        },
        {
          "_id": "698989dfbeecc443208d274a",
          "name": "Laetitia Chapel",
          "hidden": false
        },
        {
          "_id": "698989dfbeecc443208d274b",
          "name": "Romain Tavenard",
          "hidden": false
        },
        {
          "_id": "698989dfbeecc443208d274c",
          "name": "Ievgen Redko",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-06T17:12:22.000Z",
      "submittedOnDailyAt": "2026-02-09T04:59:09.364Z",
      "title": "Vision Transformer Finetuning Benefits from Non-Smooth Components",
      "submittedOnDailyBy": {
        "_id": "6661f79ac36ae4c83f3213e4",
        "avatarUrl": "/avatars/2e48f052fdf37b5b06d101a6a3232eea.svg",
        "isPro": false,
        "fullname": "Ambroise Odonnat",
        "user": "ambroiseodt",
        "type": "user"
      },
      "summary": "The smoothness of the transformer architecture has been extensively studied in the context of generalization, training stability, and adversarial robustness. However, its role in transfer learning remains poorly understood. In this paper, we analyze the ability of vision transformer components to adapt their outputs to changes in inputs, or, in other words, their plasticity. Defined as an average rate of change, it captures the sensitivity to input perturbation; in particular, a high plasticity implies low smoothness. We demonstrate through theoretical analysis and comprehensive experiments that this perspective provides principled guidance in choosing the components to prioritize during adaptation. A key takeaway for practitioners is that the high plasticity of the attention modules and feedforward layers consistently leads to better finetuning performance. Our findings depart from the prevailing assumption that smoothness is desirable, offering a novel perspective on the functional properties of transformers. The code is available at https://github.com/ambroiseodt/vit-plasticity.",
      "upvotes": 0,
      "discussionId": "698989e0beecc443208d274d",
      "githubRepo": "https://github.com/ambroiseodt/vit-plasticity",
      "githubRepoAddedBy": "user",
      "ai_summary": "Vision transformer components exhibit varying plasticity levels that correlate with finetuning performance, challenging the assumption that smoothness is always beneficial.",
      "ai_keywords": [
        "vision transformer",
        "attention modules",
        "feedforward layers",
        "plasticity",
        "smoothness",
        "generalization",
        "training stability",
        "adversarial robustness",
        "transfer learning",
        "finetuning performance"
      ],
      "githubStars": 2
    },
    "publishedAt": "2026-02-06T12:12:22.000Z",
    "title": "Vision Transformer Finetuning Benefits from Non-Smooth Components",
    "summary": "The smoothness of the transformer architecture has been extensively studied in the context of generalization, training stability, and adversarial robustness. However, its role in transfer learning remains poorly understood. In this paper, we analyze the ability of vision transformer components to adapt their outputs to changes in inputs, or, in other words, their plasticity. Defined as an average rate of change, it captures the sensitivity to input perturbation; in particular, a high plasticity implies low smoothness. We demonstrate through theoretical analysis and comprehensive experiments that this perspective provides principled guidance in choosing the components to prioritize during adaptation. A key takeaway for practitioners is that the high plasticity of the attention modules and feedforward layers consistently leads to better finetuning performance. Our findings depart from the prevailing assumption that smoothness is desirable, offering a novel perspective on the functional properties of transformers. The code is available at https://github.com/ambroiseodt/vit-plasticity.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06883.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6661f79ac36ae4c83f3213e4",
      "avatarUrl": "/avatars/2e48f052fdf37b5b06d101a6a3232eea.svg",
      "fullname": "Ambroise Odonnat",
      "name": "ambroiseodt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4,
      "isUserFollowing": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2602.01064",
      "authors": [
        {
          "_id": "69895b69beecc443208d26ce",
          "name": "Ruihan Jin",
          "hidden": false
        },
        {
          "_id": "69895b69beecc443208d26cf",
          "name": "Pengpeng Shao",
          "hidden": false
        },
        {
          "_id": "69895b69beecc443208d26d0",
          "name": "Zhengqi Wen",
          "hidden": false
        },
        {
          "_id": "69895b69beecc443208d26d1",
          "name": "Jinyang Wu",
          "hidden": false
        },
        {
          "_id": "69895b69beecc443208d26d2",
          "name": "Mingkuan Feng",
          "hidden": false
        },
        {
          "_id": "69895b69beecc443208d26d3",
          "name": "Shuo Yang",
          "hidden": false
        },
        {
          "_id": "69895b69beecc443208d26d4",
          "name": "Chu Yuan Zhang",
          "hidden": false
        },
        {
          "_id": "69895b69beecc443208d26d5",
          "name": "Jianhua Tao",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-01T07:19:57.000Z",
      "submittedOnDailyAt": "2026-02-09T01:29:12.554Z",
      "title": "Exploring Knowledge Purification in Multi-Teacher Knowledge Distillation for LLMs",
      "submittedOnDailyBy": {
        "_id": "6747de57f8cab58c22ec94a2",
        "avatarUrl": "/avatars/5bae0341862fac24564781c0fa32aac5.svg",
        "isPro": false,
        "fullname": "Jinyang Wu",
        "user": "Jinyang23",
        "type": "user"
      },
      "summary": "Knowledge distillation has emerged as a pivotal technique for transferring knowledge from stronger large language models (LLMs) to smaller, more efficient models. However, traditional distillation approaches face challenges related to knowledge conflicts and high resource demands, particularly when leveraging multiple teacher models. In this paper, we introduce the concept of Knowledge Purification, which consolidates the rationales from multiple teacher LLMs into a single rationale, thereby mitigating conflicts and enhancing efficiency. To investigate the effectiveness of knowledge purification, we further propose five purification methods from various perspectives. Our experiments demonstrate that these methods not only improve the performance of the distilled model but also effectively alleviate knowledge conflicts. Moreover, router-based methods exhibit robust generalization capabilities, underscoring the potential of innovative purification techniques in optimizing multi-teacher distillation and facilitating the practical deployment of powerful yet lightweight models.",
      "upvotes": 0,
      "discussionId": "69895b6abeecc443208d26d6",
      "ai_summary": "Knowledge purification techniques consolidate rationales from multiple teacher LLMs to reduce conflicts and improve efficiency in distillation processes.",
      "ai_keywords": [
        "knowledge distillation",
        "large language models",
        "teacher models",
        "knowledge conflicts",
        "knowledge purification",
        "router-based methods"
      ]
    },
    "publishedAt": "2026-02-01T02:19:57.000Z",
    "title": "Exploring Knowledge Purification in Multi-Teacher Knowledge Distillation for LLMs",
    "summary": "Knowledge distillation has emerged as a pivotal technique for transferring knowledge from stronger large language models (LLMs) to smaller, more efficient models. However, traditional distillation approaches face challenges related to knowledge conflicts and high resource demands, particularly when leveraging multiple teacher models. In this paper, we introduce the concept of Knowledge Purification, which consolidates the rationales from multiple teacher LLMs into a single rationale, thereby mitigating conflicts and enhancing efficiency. To investigate the effectiveness of knowledge purification, we further propose five purification methods from various perspectives. Our experiments demonstrate that these methods not only improve the performance of the distilled model but also effectively alleviate knowledge conflicts. Moreover, router-based methods exhibit robust generalization capabilities, underscoring the potential of innovative purification techniques in optimizing multi-teacher distillation and facilitating the practical deployment of powerful yet lightweight models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.01064.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6747de57f8cab58c22ec94a2",
      "avatarUrl": "/avatars/5bae0341862fac24564781c0fa32aac5.svg",
      "fullname": "Jinyang Wu",
      "name": "Jinyang23",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  }
]