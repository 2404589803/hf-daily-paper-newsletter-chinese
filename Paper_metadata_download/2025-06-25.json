[
  {
    "paper": {
      "id": "2506.19851",
      "authors": [
        {
          "_id": "685b5a46d2ee4fac76521dce",
          "name": "Zehuan Huang",
          "hidden": false
        },
        {
          "_id": "685b5a46d2ee4fac76521dcf",
          "name": "Haoran Feng",
          "hidden": false
        },
        {
          "_id": "685b5a46d2ee4fac76521dd0",
          "name": "Yangtian Sun",
          "hidden": false
        },
        {
          "_id": "685b5a46d2ee4fac76521dd1",
          "name": "Yuanchen Guo",
          "hidden": false
        },
        {
          "_id": "685b5a46d2ee4fac76521dd2",
          "name": "Yanpei Cao",
          "hidden": false
        },
        {
          "_id": "685b5a46d2ee4fac76521dd3",
          "name": "Lu Sheng",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64a96a375a69e2ca889abdff/ttfXuQOJQmOqnO-_-F1op.mp4"
      ],
      "publishedAt": "2025-06-24T17:59:58.000Z",
      "submittedOnDailyAt": "2025-06-25T01:12:40.364Z",
      "title": "AnimaX: Animating the Inanimate in 3D with Joint Video-Pose Diffusion\n  Models",
      "submittedOnDailyBy": {
        "_id": "64a96a375a69e2ca889abdff",
        "avatarUrl": "/avatars/f288c66ace09d907f132a79a740a3701.svg",
        "isPro": false,
        "fullname": "fanhongxing",
        "user": "fanhongxing",
        "type": "user"
      },
      "summary": "We present AnimaX, a feed-forward 3D animation framework that bridges the\nmotion priors of video diffusion models with the controllable structure of\nskeleton-based animation. Traditional motion synthesis methods are either\nrestricted to fixed skeletal topologies or require costly optimization in\nhigh-dimensional deformation spaces. In contrast, AnimaX effectively transfers\nvideo-based motion knowledge to the 3D domain, supporting diverse articulated\nmeshes with arbitrary skeletons. Our method represents 3D motion as multi-view,\nmulti-frame 2D pose maps, and enables joint video-pose diffusion conditioned on\ntemplate renderings and a textual motion prompt. We introduce shared positional\nencodings and modality-aware embeddings to ensure spatial-temporal alignment\nbetween video and pose sequences, effectively transferring video priors to\nmotion generation task. The resulting multi-view pose sequences are\ntriangulated into 3D joint positions and converted into mesh animation via\ninverse kinematics. Trained on a newly curated dataset of 160,000 rigged\nsequences, AnimaX achieves state-of-the-art results on VBench in\ngeneralization, motion fidelity, and efficiency, offering a scalable solution\nfor category-agnostic 3D animation. Project page:\nhttps://anima-x.github.io/{https://anima-x.github.io/}.",
      "upvotes": 21,
      "discussionId": "685b5a47d2ee4fac76521dd4",
      "projectPage": "https://anima-x.github.io/",
      "githubRepo": "https://github.com/anima-x/anima-x",
      "ai_summary": "AnimaX creates multi-skeleton 3D animations by blending video diffusion model priors with skeleton-based control, using joint video-pose diffusion and shared positional encodings.",
      "ai_keywords": [
        "feed-forward 3D animation framework",
        "video diffusion models",
        "skeleton-based animation",
        "motion synthesis",
        "high-dimensional deformation spaces",
        "2D pose maps",
        "joint video-pose diffusion",
        "template renderings",
        "textual motion prompt",
        "shared positional encodings",
        "modality-aware embeddings",
        "spatial-temporal alignment",
        "inverse kinematics",
        "VBench",
        "category-agnostic 3D animation"
      ],
      "githubStars": 20
    },
    "publishedAt": "2025-06-24T13:59:58.000Z",
    "title": "AnimaX: Animating the Inanimate in 3D with Joint Video-Pose Diffusion\n  Models",
    "summary": "We present AnimaX, a feed-forward 3D animation framework that bridges the\nmotion priors of video diffusion models with the controllable structure of\nskeleton-based animation. Traditional motion synthesis methods are either\nrestricted to fixed skeletal topologies or require costly optimization in\nhigh-dimensional deformation spaces. In contrast, AnimaX effectively transfers\nvideo-based motion knowledge to the 3D domain, supporting diverse articulated\nmeshes with arbitrary skeletons. Our method represents 3D motion as multi-view,\nmulti-frame 2D pose maps, and enables joint video-pose diffusion conditioned on\ntemplate renderings and a textual motion prompt. We introduce shared positional\nencodings and modality-aware embeddings to ensure spatial-temporal alignment\nbetween video and pose sequences, effectively transferring video priors to\nmotion generation task. The resulting multi-view pose sequences are\ntriangulated into 3D joint positions and converted into mesh animation via\ninverse kinematics. Trained on a newly curated dataset of 160,000 rigged\nsequences, AnimaX achieves state-of-the-art results on VBench in\ngeneralization, motion fidelity, and efficiency, offering a scalable solution\nfor category-agnostic 3D animation. Project page:\nhttps://anima-x.github.io/{https://anima-x.github.io/}.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64a96a375a69e2ca889abdff/ttfXuQOJQmOqnO-_-F1op.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19851.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a96a375a69e2ca889abdff",
      "avatarUrl": "/avatars/f288c66ace09d907f132a79a740a3701.svg",
      "fullname": "fanhongxing",
      "name": "fanhongxing",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.16141",
      "authors": [
        {
          "_id": "6858b1fac0c8e29df8ea3c18",
          "name": "Yi Chen",
          "hidden": false
        },
        {
          "_id": "6858b1fac0c8e29df8ea3c19",
          "name": "Yuying Ge",
          "hidden": false
        },
        {
          "_id": "6858b1fac0c8e29df8ea3c1a",
          "name": "Rui Wang",
          "hidden": false
        },
        {
          "_id": "6858b1fac0c8e29df8ea3c1b",
          "name": "Yixiao Ge",
          "hidden": false
        },
        {
          "_id": "6858b1fac0c8e29df8ea3c1c",
          "name": "Junhao Cheng",
          "hidden": false
        },
        {
          "_id": "6858b1fac0c8e29df8ea3c1d",
          "name": "Ying Shan",
          "hidden": false
        },
        {
          "_id": "6858b1fac0c8e29df8ea3c1e",
          "name": "Xihui Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-19T08:49:13.000Z",
      "submittedOnDailyAt": "2025-06-25T01:50:33.428Z",
      "title": "GRPO-CARE: Consistency-Aware Reinforcement Learning for Multimodal\n  Reasoning",
      "submittedOnDailyBy": {
        "_id": "60d045c4778bafd0fbcfa3f5",
        "avatarUrl": "/avatars/0cc0c2739c1934430ea09df7e9668c80.svg",
        "isPro": false,
        "fullname": "Yi Chen",
        "user": "ChenYi99",
        "type": "user"
      },
      "summary": "Recent reinforcement learning approaches, such as outcome-supervised GRPO,\nhave advanced Chain-of-Thought reasoning in large language models (LLMs), yet\ntheir adaptation to multimodal LLMs (MLLMs) is unexplored. To address the lack\nof rigorous evaluation for MLLM post-training methods, we introduce\nSEED-Bench-R1, a benchmark with complex real-world videos requiring balanced\nperception and reasoning. It offers a large training set and evaluates\ngeneralization across three escalating challenges: in-distribution,\ncross-environment, and cross-environment-task scenarios. Using SEED-Bench-R1,\nwe find that standard GRPO, while improving answer accuracy, often reduces\nlogical coherence between reasoning steps and answers, with only a 57.9%\nconsistency rate. This stems from reward signals focusing solely on final\nanswers, encouraging shortcuts, and strict KL penalties limiting exploration.To\naddress this, we propose GRPO-CARE, a consistency-aware RL framework optimizing\nboth answer correctness and reasoning coherence without explicit supervision.\nGRPO-CARE introduces a two-tiered reward: (1) a base reward for answer\ncorrectness, and (2) an adaptive consistency bonus, computed by comparing the\nmodel's reasoning-to-answer likelihood (via a slowly-evolving reference model)\nagainst group peers.This dual mechanism amplifies rewards for reasoning paths\nthat are both correct and logically consistent. Replacing KL penalties with\nthis adaptive bonus, GRPO-CARE outperforms standard GRPO on SEED-Bench-R1,\nachieving a 6.7% performance gain on the hardest evaluation level and a 24.5%\nimprovement in consistency. It also shows strong transferability, improving\nmodel performance across diverse video understanding benchmarks. Our work\ncontributes a systematically designed benchmark and a generalizable\npost-training framework, advancing the development of more interpretable and\nrobust MLLMs.",
      "upvotes": 19,
      "discussionId": "6858b1fac0c8e29df8ea3c1f",
      "githubRepo": "https://github.com/TencentARC/GRPO-CARE",
      "ai_summary": "GRPO-CARE, a reinforcement learning framework optimizing for consistency and correctness, outperforms standard GRPO on a new video understanding benchmark, SEED-Bench-R1, improving both performance and logical coherence in multimodal large language models.",
      "ai_keywords": [
        "reinforcement learning",
        "outcome-supervised GRPO",
        "Chain-of-Thought reasoning",
        "large language models",
        "multimodal large language models",
        "SEED-Bench-R1",
        "in-distribution",
        "cross-environment",
        "cross-environment-task",
        "logical coherence",
        "reasoning steps",
        "answer accuracy",
        "reward signals",
        "shortcuts",
        "KL penalties",
        "exploration",
        "consistency-aware RL framework",
        "two-tiered reward",
        "reasoning-to-answer likelihood",
        "adaptive consistency bonus",
        "video understanding benchmarks",
        "transferability",
        "interpretable models",
        "robust models"
      ],
      "githubStars": 19
    },
    "publishedAt": "2025-06-19T04:49:13.000Z",
    "title": "GRPO-CARE: Consistency-Aware Reinforcement Learning for Multimodal\n  Reasoning",
    "summary": "Recent reinforcement learning approaches, such as outcome-supervised GRPO,\nhave advanced Chain-of-Thought reasoning in large language models (LLMs), yet\ntheir adaptation to multimodal LLMs (MLLMs) is unexplored. To address the lack\nof rigorous evaluation for MLLM post-training methods, we introduce\nSEED-Bench-R1, a benchmark with complex real-world videos requiring balanced\nperception and reasoning. It offers a large training set and evaluates\ngeneralization across three escalating challenges: in-distribution,\ncross-environment, and cross-environment-task scenarios. Using SEED-Bench-R1,\nwe find that standard GRPO, while improving answer accuracy, often reduces\nlogical coherence between reasoning steps and answers, with only a 57.9%\nconsistency rate. This stems from reward signals focusing solely on final\nanswers, encouraging shortcuts, and strict KL penalties limiting exploration.To\naddress this, we propose GRPO-CARE, a consistency-aware RL framework optimizing\nboth answer correctness and reasoning coherence without explicit supervision.\nGRPO-CARE introduces a two-tiered reward: (1) a base reward for answer\ncorrectness, and (2) an adaptive consistency bonus, computed by comparing the\nmodel's reasoning-to-answer likelihood (via a slowly-evolving reference model)\nagainst group peers.This dual mechanism amplifies rewards for reasoning paths\nthat are both correct and logically consistent. Replacing KL penalties with\nthis adaptive bonus, GRPO-CARE outperforms standard GRPO on SEED-Bench-R1,\nachieving a 6.7% performance gain on the hardest evaluation level and a 24.5%\nimprovement in consistency. It also shows strong transferability, improving\nmodel performance across diverse video understanding benchmarks. Our work\ncontributes a systematically designed benchmark and a generalizable\npost-training framework, advancing the development of more interpretable and\nrobust MLLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.16141.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60d045c4778bafd0fbcfa3f5",
      "avatarUrl": "/avatars/0cc0c2739c1934430ea09df7e9668c80.svg",
      "fullname": "Yi Chen",
      "name": "ChenYi99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.19848",
      "authors": [
        {
          "_id": "685b7cc2d2ee4fac76521e83",
          "name": "Long Xing",
          "hidden": false
        },
        {
          "_id": "685b7cc2d2ee4fac76521e84",
          "name": "Qidong Huang",
          "hidden": false
        },
        {
          "_id": "685b7cc2d2ee4fac76521e85",
          "name": "Xiaoyi Dong",
          "hidden": false
        },
        {
          "_id": "685b7cc2d2ee4fac76521e86",
          "name": "Pan Zhang",
          "hidden": false
        },
        {
          "_id": "685b7cc2d2ee4fac76521e87",
          "name": "Yuhang Zang",
          "hidden": false
        },
        {
          "_id": "685b7cc2d2ee4fac76521e88",
          "name": "Yuhang Cao",
          "hidden": false
        },
        {
          "_id": "685b7cc2d2ee4fac76521e89",
          "name": "Jinsong Li",
          "hidden": false
        },
        {
          "_id": "685b7cc2d2ee4fac76521e8a",
          "name": "Shuangrui Ding",
          "hidden": false
        },
        {
          "_id": "685b7cc2d2ee4fac76521e8b",
          "name": "Weiming Zhang",
          "hidden": false
        },
        {
          "_id": "685b7cc2d2ee4fac76521e8c",
          "name": "Nenghai Yu",
          "hidden": false
        },
        {
          "_id": "685b7cc2d2ee4fac76521e8d",
          "name": "Jiaqi Wang",
          "hidden": false
        },
        {
          "_id": "685b7cc2d2ee4fac76521e8e",
          "name": "Feng Wu",
          "hidden": false
        },
        {
          "_id": "685b7cc2d2ee4fac76521e8f",
          "name": "Dahua Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-24T17:59:55.000Z",
      "submittedOnDailyAt": "2025-06-25T03:07:04.508Z",
      "title": "ScaleCap: Inference-Time Scalable Image Captioning via Dual-Modality\n  Debiasing",
      "submittedOnDailyBy": {
        "_id": "64b4eec4faa3181a5eab9c46",
        "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
        "isPro": true,
        "fullname": "Jiaqi Wang",
        "user": "myownskyW7",
        "type": "user"
      },
      "summary": "This paper presents ScaleCap, an inference-time scalable image captioning\nstrategy that generates comprehensive and detailed image captions. The key\nchallenges of high-quality image captioning lie in the inherent biases of\nLVLMs: multimodal bias resulting in imbalanced descriptive granularity,\noffering detailed accounts of some elements while merely skimming over others;\nlinguistic bias leading to hallucinated descriptions of non-existent objects.\nTo address these issues, we propose a scalable debiased captioning strategy,\nwhich continuously enriches and calibrates the caption with increased inference\nbudget. Specifically, we propose two novel components: heuristic question\nanswering and contrastive sentence rating. The former generates\ncontent-specific questions based on the image and answers them to progressively\ninject relevant information into the caption. The latter employs sentence-level\noffline contrastive decoding to effectively identify and eliminate\nhallucinations caused by linguistic biases. With increased inference cost, more\nheuristic questions are raised by ScaleCap to progressively capture additional\nvisual details, generating captions that are more accurate, balanced, and\ninformative. Extensive modality alignment experiments demonstrate the\neffectiveness of ScaleCap. Annotating 450K images with ScaleCap and using them\nfor LVLM pretraining leads to consistent performance gains across 11 widely\nused benchmarks. Furthermore, ScaleCap showcases superb richness and fidelity\nof generated captions with two additional tasks: replacing images with captions\nin VQA task, and reconstructing images from captions to assess semantic\ncoverage. Code is available at https://github.com/Cooperx521/ScaleCap.",
      "upvotes": 18,
      "discussionId": "685b7cc2d2ee4fac76521e90",
      "githubRepo": "https://github.com/Cooperx521/ScaleCap",
      "ai_summary": "ScaleCap enhances image captioning by iteratively enriching and calibrating captions using heuristic question answering and contrastive sentence rating, addressing multimodal and linguistic biases to improve accuracy, balance, and informativeness.",
      "ai_keywords": [
        "LVLMs",
        "multimodal bias",
        "linguistic bias",
        "heuristic question answering",
        "contrastive sentence rating",
        "VQA task"
      ],
      "githubStars": 9
    },
    "publishedAt": "2025-06-24T13:59:55.000Z",
    "title": "ScaleCap: Inference-Time Scalable Image Captioning via Dual-Modality\n  Debiasing",
    "summary": "This paper presents ScaleCap, an inference-time scalable image captioning\nstrategy that generates comprehensive and detailed image captions. The key\nchallenges of high-quality image captioning lie in the inherent biases of\nLVLMs: multimodal bias resulting in imbalanced descriptive granularity,\noffering detailed accounts of some elements while merely skimming over others;\nlinguistic bias leading to hallucinated descriptions of non-existent objects.\nTo address these issues, we propose a scalable debiased captioning strategy,\nwhich continuously enriches and calibrates the caption with increased inference\nbudget. Specifically, we propose two novel components: heuristic question\nanswering and contrastive sentence rating. The former generates\ncontent-specific questions based on the image and answers them to progressively\ninject relevant information into the caption. The latter employs sentence-level\noffline contrastive decoding to effectively identify and eliminate\nhallucinations caused by linguistic biases. With increased inference cost, more\nheuristic questions are raised by ScaleCap to progressively capture additional\nvisual details, generating captions that are more accurate, balanced, and\ninformative. Extensive modality alignment experiments demonstrate the\neffectiveness of ScaleCap. Annotating 450K images with ScaleCap and using them\nfor LVLM pretraining leads to consistent performance gains across 11 widely\nused benchmarks. Furthermore, ScaleCap showcases superb richness and fidelity\nof generated captions with two additional tasks: replacing images with captions\nin VQA task, and reconstructing images from captions to assess semantic\ncoverage. Code is available at https://github.com/Cooperx521/ScaleCap.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19848.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b4eec4faa3181a5eab9c46",
      "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
      "fullname": "Jiaqi Wang",
      "name": "myownskyW7",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 20
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.19290",
      "authors": [
        {
          "_id": "685b6640d2ee4fac76521e42",
          "name": "Liang Zeng",
          "hidden": false
        },
        {
          "_id": "685b6640d2ee4fac76521e43",
          "name": "Yongcong Li",
          "hidden": false
        },
        {
          "_id": "685b6640d2ee4fac76521e44",
          "name": "Yuzhen Xiao",
          "hidden": false
        },
        {
          "_id": "685b6640d2ee4fac76521e45",
          "name": "Changshi Li",
          "hidden": false
        },
        {
          "_id": "685b6640d2ee4fac76521e46",
          "name": "Chris Yuhao Liu",
          "hidden": false
        },
        {
          "_id": "685b6640d2ee4fac76521e47",
          "name": "Rui Yan",
          "hidden": false
        },
        {
          "_id": "685b6640d2ee4fac76521e48",
          "name": "Tianwen Wei",
          "hidden": false
        },
        {
          "_id": "685b6640d2ee4fac76521e49",
          "name": "Jujie He",
          "hidden": false
        },
        {
          "_id": "685b6640d2ee4fac76521e4a",
          "name": "Xuchen Song",
          "hidden": false
        },
        {
          "_id": "685b6640d2ee4fac76521e4b",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "685b6640d2ee4fac76521e4c",
          "name": "Yahui Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-24T03:53:36.000Z",
      "submittedOnDailyAt": "2025-06-25T01:35:02.603Z",
      "title": "Skywork-SWE: Unveiling Data Scaling Laws for Software Engineering in\n  LLMs",
      "submittedOnDailyBy": {
        "_id": "6621efe1a6eec3ad03e38759",
        "avatarUrl": "/avatars/c35acce69f244ec0833dffd53eedf6a3.svg",
        "isPro": false,
        "fullname": "Liang Zeng",
        "user": "zengliangcs",
        "type": "user"
      },
      "summary": "Software engineering (SWE) has recently emerged as a crucial testbed for\nnext-generation LLM agents, demanding inherent capabilities in two critical\ndimensions: sustained iterative problem-solving (e.g., >50 interaction rounds)\nand long-context dependency resolution (e.g., >32k tokens). However, the data\ncuration process in SWE remains notoriously time-consuming, as it heavily\nrelies on manual annotation for code file filtering and the setup of dedicated\nruntime environments to execute and validate unit tests. Consequently, most\nexisting datasets are limited to only a few thousand GitHub-sourced instances.\nTo this end, we propose an incremental, automated data-curation pipeline that\nsystematically scales both the volume and diversity of SWE datasets. Our\ndataset comprises 10,169 real-world Python task instances from 2,531 distinct\nGitHub repositories, each accompanied by a task specified in natural language\nand a dedicated runtime-environment image for automated unit-test validation.\nWe have carefully curated over 8,000 successfully runtime-validated training\ntrajectories from our proposed SWE dataset. When fine-tuning the Skywork-SWE\nmodel on these trajectories, we uncover a striking data scaling phenomenon: the\ntrained model's performance for software engineering capabilities in LLMs\ncontinues to improve as the data size increases, showing no signs of\nsaturation. Notably, our Skywork-SWE model achieves 38.0% pass@1 accuracy on\nthe SWE-bench Verified benchmark without using verifiers or multiple rollouts,\nestablishing a new state-of-the-art (SOTA) among the Qwen2.5-Coder-32B-based\nLLMs built on the OpenHands agent framework. Furthermore, with the\nincorporation of test-time scaling techniques, the performance further improves\nto 47.0% accuracy, surpassing the previous SOTA results for sub-32B parameter\nmodels. We release the Skywork-SWE-32B model checkpoint to accelerate future\nresearch.",
      "upvotes": 9,
      "discussionId": "685b6641d2ee4fac76521e4d",
      "projectPage": "https://quixotic-sting-239.notion.site/eb17f379610040ceb54da5d5d24065bd",
      "ai_summary": "An automated data-curation pipeline for software engineering improves large language model performance on SWE tasks, achieving state-of-the-art results with and without test-time scaling techniques.",
      "ai_keywords": [
        "LLM agents",
        "iterative problem-solving",
        "long-context dependency resolution",
        "code file filtering",
        "unit tests",
        "runtime environments",
        "data-curation pipeline",
        "software engineering capabilities",
        "Skywork-SWE model",
        "SWE-bench Verified",
        "pass@1 accuracy",
        "OpenHands agent framework",
        "test-time scaling techniques",
        "parameter-efficient fine-tuning"
      ]
    },
    "publishedAt": "2025-06-23T23:53:36.000Z",
    "title": "Skywork-SWE: Unveiling Data Scaling Laws for Software Engineering in\n  LLMs",
    "summary": "Software engineering (SWE) has recently emerged as a crucial testbed for\nnext-generation LLM agents, demanding inherent capabilities in two critical\ndimensions: sustained iterative problem-solving (e.g., >50 interaction rounds)\nand long-context dependency resolution (e.g., >32k tokens). However, the data\ncuration process in SWE remains notoriously time-consuming, as it heavily\nrelies on manual annotation for code file filtering and the setup of dedicated\nruntime environments to execute and validate unit tests. Consequently, most\nexisting datasets are limited to only a few thousand GitHub-sourced instances.\nTo this end, we propose an incremental, automated data-curation pipeline that\nsystematically scales both the volume and diversity of SWE datasets. Our\ndataset comprises 10,169 real-world Python task instances from 2,531 distinct\nGitHub repositories, each accompanied by a task specified in natural language\nand a dedicated runtime-environment image for automated unit-test validation.\nWe have carefully curated over 8,000 successfully runtime-validated training\ntrajectories from our proposed SWE dataset. When fine-tuning the Skywork-SWE\nmodel on these trajectories, we uncover a striking data scaling phenomenon: the\ntrained model's performance for software engineering capabilities in LLMs\ncontinues to improve as the data size increases, showing no signs of\nsaturation. Notably, our Skywork-SWE model achieves 38.0% pass@1 accuracy on\nthe SWE-bench Verified benchmark without using verifiers or multiple rollouts,\nestablishing a new state-of-the-art (SOTA) among the Qwen2.5-Coder-32B-based\nLLMs built on the OpenHands agent framework. Furthermore, with the\nincorporation of test-time scaling techniques, the performance further improves\nto 47.0% accuracy, surpassing the previous SOTA results for sub-32B parameter\nmodels. We release the Skywork-SWE-32B model checkpoint to accelerate future\nresearch.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19290.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6621efe1a6eec3ad03e38759",
      "avatarUrl": "/avatars/c35acce69f244ec0833dffd53eedf6a3.svg",
      "fullname": "Liang Zeng",
      "name": "zengliangcs",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.19767",
      "authors": [
        {
          "_id": "685b5791d2ee4fac76521dc2",
          "name": "Yuqian Fu",
          "hidden": false
        },
        {
          "_id": "685b5791d2ee4fac76521dc3",
          "name": "Tinghong Chen",
          "hidden": false
        },
        {
          "_id": "685b5791d2ee4fac76521dc4",
          "name": "Jiajun Chai",
          "hidden": false
        },
        {
          "_id": "685b5791d2ee4fac76521dc5",
          "name": "Xihuai Wang",
          "hidden": false
        },
        {
          "_id": "685b5791d2ee4fac76521dc6",
          "name": "Songjun Tu",
          "hidden": false
        },
        {
          "_id": "685b5791d2ee4fac76521dc7",
          "name": "Guojun Yin",
          "hidden": false
        },
        {
          "_id": "685b5791d2ee4fac76521dc8",
          "name": "Wei Lin",
          "hidden": false
        },
        {
          "_id": "685b5791d2ee4fac76521dc9",
          "name": "Qichao Zhang",
          "hidden": false
        },
        {
          "_id": "685b5791d2ee4fac76521dca",
          "name": "Yuanheng Zhu",
          "hidden": false
        },
        {
          "_id": "685b5791d2ee4fac76521dcb",
          "name": "Dongbin Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-24T16:31:37.000Z",
      "submittedOnDailyAt": "2025-06-25T01:33:02.160Z",
      "title": "SRFT: A Single-Stage Method with Supervised and Reinforcement\n  Fine-Tuning for Reasoning",
      "submittedOnDailyBy": {
        "_id": "66e14f4142ceed655c731966",
        "avatarUrl": "/avatars/d708562349913b89c8c4cf384628f82a.svg",
        "isPro": false,
        "fullname": "SONGJUN TU",
        "user": "SONGJUNTU",
        "type": "user"
      },
      "summary": "Large language models (LLMs) have achieved remarkable progress in reasoning\ntasks, yet the optimal integration of Supervised Fine-Tuning (SFT) and\nReinforcement Learning (RL) remains a fundamental challenge. Through\ncomprehensive analysis of token distributions, learning dynamics, and\nintegration mechanisms from entropy-based perspectives, we reveal key\ndifferences between these paradigms: SFT induces coarse-grained global changes\nto LLM policy distributions, while RL performs fine-grained selective\noptimizations, with entropy serving as a critical indicator of training\neffectiveness. Building on these observations, we propose Supervised\nReinforcement Fine-Tuning (SRFT), a single-stage method that unifies both\nfine-tuning paradigms through entropy-aware weighting mechanisms. Our approach\nsimultaneously applies SFT and RL to directly optimize the LLM using\ndemonstrations and self-exploration rollouts rather than through two-stage\nsequential methods. Extensive experiments show that SRFT achieves 59.1% average\naccuracy, outperforming zero-RL methods by 9.0% on five mathematical reasoning\nbenchmarks and 10.9% on three out-of-distribution benchmarks.",
      "upvotes": 7,
      "discussionId": "685b5792d2ee4fac76521dcc",
      "projectPage": "https://anonymous.4open.science/w/SRFT2025",
      "ai_summary": " Supervised Reinforcement Fine-Tuning (SRFT) integrates Supervised Fine-Tuning and Reinforcement Learning through entropy-aware weighting to achieve high accuracy in language model optimization.",
      "ai_keywords": [
        "Large language models",
        "Supervised Fine-Tuning",
        "Reinforcement Learning",
        "token distributions",
        "learning dynamics",
        "entropy",
        "Supervised Reinforcement Fine-Tuning"
      ]
    },
    "publishedAt": "2025-06-24T12:31:37.000Z",
    "title": "SRFT: A Single-Stage Method with Supervised and Reinforcement\n  Fine-Tuning for Reasoning",
    "summary": "Large language models (LLMs) have achieved remarkable progress in reasoning\ntasks, yet the optimal integration of Supervised Fine-Tuning (SFT) and\nReinforcement Learning (RL) remains a fundamental challenge. Through\ncomprehensive analysis of token distributions, learning dynamics, and\nintegration mechanisms from entropy-based perspectives, we reveal key\ndifferences between these paradigms: SFT induces coarse-grained global changes\nto LLM policy distributions, while RL performs fine-grained selective\noptimizations, with entropy serving as a critical indicator of training\neffectiveness. Building on these observations, we propose Supervised\nReinforcement Fine-Tuning (SRFT), a single-stage method that unifies both\nfine-tuning paradigms through entropy-aware weighting mechanisms. Our approach\nsimultaneously applies SFT and RL to directly optimize the LLM using\ndemonstrations and self-exploration rollouts rather than through two-stage\nsequential methods. Extensive experiments show that SRFT achieves 59.1% average\naccuracy, outperforming zero-RL methods by 9.0% on five mathematical reasoning\nbenchmarks and 10.9% on three out-of-distribution benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19767.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66e14f4142ceed655c731966",
      "avatarUrl": "/avatars/d708562349913b89c8c4cf384628f82a.svg",
      "fullname": "SONGJUN TU",
      "name": "SONGJUNTU",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.19838",
      "authors": [
        {
          "_id": "685b5e05d2ee4fac76521ddd",
          "name": "Liangbin Xie",
          "hidden": false
        },
        {
          "_id": "685b5e05d2ee4fac76521dde",
          "name": "Yu Li",
          "hidden": false
        },
        {
          "_id": "685b5e05d2ee4fac76521ddf",
          "name": "Shian Du",
          "hidden": false
        },
        {
          "_id": "685b5e05d2ee4fac76521de0",
          "name": "Menghan Xia",
          "hidden": false
        },
        {
          "_id": "685b5e05d2ee4fac76521de1",
          "name": "Xintao Wang",
          "hidden": false
        },
        {
          "_id": "685b5e05d2ee4fac76521de2",
          "name": "Fanghua Yu",
          "hidden": false
        },
        {
          "_id": "685b5e05d2ee4fac76521de3",
          "name": "Ziyan Chen",
          "hidden": false
        },
        {
          "_id": "685b5e05d2ee4fac76521de4",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "685b5e05d2ee4fac76521de5",
          "name": "Jiantao Zhou",
          "hidden": false
        },
        {
          "_id": "685b5e05d2ee4fac76521de6",
          "name": "Chao Dong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-24T17:57:26.000Z",
      "submittedOnDailyAt": "2025-06-25T00:55:41.694Z",
      "title": "SimpleGVR: A Simple Baseline for Latent-Cascaded Video Super-Resolution",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": true,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Latent diffusion models have emerged as a leading paradigm for efficient\nvideo generation. However, as user expectations shift toward higher-resolution\noutputs, relying solely on latent computation becomes inadequate. A promising\napproach involves decoupling the process into two stages: semantic content\ngeneration and detail synthesis. The former employs a computationally intensive\nbase model at lower resolutions, while the latter leverages a lightweight\ncascaded video super-resolution (VSR) model to achieve high-resolution output.\nIn this work, we focus on studying key design principles for latter cascaded\nVSR models, which are underexplored currently. First, we propose two\ndegradation strategies to generate training pairs that better mimic the output\ncharacteristics of the base model, ensuring alignment between the VSR model and\nits upstream generator. Second, we provide critical insights into VSR model\nbehavior through systematic analysis of (1) timestep sampling strategies, (2)\nnoise augmentation effects on low-resolution (LR) inputs. These findings\ndirectly inform our architectural and training innovations. Finally, we\nintroduce interleaving temporal unit and sparse local attention to achieve\nefficient training and inference, drastically reducing computational overhead.\nExtensive experiments demonstrate the superiority of our framework over\nexisting methods, with ablation studies confirming the efficacy of each design\nchoice. Our work establishes a simple yet effective baseline for cascaded video\nsuper-resolution generation, offering practical insights to guide future\nadvancements in efficient cascaded synthesis systems.",
      "upvotes": 6,
      "discussionId": "685b5e05d2ee4fac76521de7",
      "ai_summary": "Researchers propose design principles for cascaded video super-resolution models to improve high-resolution video generation by introduces degradation strategies, timestep sampling, noise augmentation, and interleaving temporal units with sparse local attention.",
      "ai_keywords": [
        "latent diffusion models",
        "video generation",
        "cascaded video super-resolution",
        "VSR",
        "degradation strategies",
        "timestep sampling",
        "noise augmentation",
        "interleaving temporal unit",
        "sparse local attention"
      ]
    },
    "publishedAt": "2025-06-24T13:57:26.000Z",
    "title": "SimpleGVR: A Simple Baseline for Latent-Cascaded Video Super-Resolution",
    "summary": "Latent diffusion models have emerged as a leading paradigm for efficient\nvideo generation. However, as user expectations shift toward higher-resolution\noutputs, relying solely on latent computation becomes inadequate. A promising\napproach involves decoupling the process into two stages: semantic content\ngeneration and detail synthesis. The former employs a computationally intensive\nbase model at lower resolutions, while the latter leverages a lightweight\ncascaded video super-resolution (VSR) model to achieve high-resolution output.\nIn this work, we focus on studying key design principles for latter cascaded\nVSR models, which are underexplored currently. First, we propose two\ndegradation strategies to generate training pairs that better mimic the output\ncharacteristics of the base model, ensuring alignment between the VSR model and\nits upstream generator. Second, we provide critical insights into VSR model\nbehavior through systematic analysis of (1) timestep sampling strategies, (2)\nnoise augmentation effects on low-resolution (LR) inputs. These findings\ndirectly inform our architectural and training innovations. Finally, we\nintroduce interleaving temporal unit and sparse local attention to achieve\nefficient training and inference, drastically reducing computational overhead.\nExtensive experiments demonstrate the superiority of our framework over\nexisting methods, with ablation studies confirming the efficacy of each design\nchoice. Our work establishes a simple yet effective baseline for cascaded video\nsuper-resolution generation, offering practical insights to guide future\nadvancements in efficient cascaded synthesis systems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19838.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": true,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7182
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.19794",
      "authors": [
        {
          "_id": "685b75d0d2ee4fac76521e70",
          "name": "Yuqi Zhu",
          "hidden": false
        },
        {
          "_id": "685b75d0d2ee4fac76521e71",
          "name": "Yi Zhong",
          "hidden": false
        },
        {
          "_id": "685b75d0d2ee4fac76521e72",
          "name": "Jintian Zhang",
          "hidden": false
        },
        {
          "_id": "685b75d0d2ee4fac76521e73",
          "name": "Ziheng Zhang",
          "hidden": false
        },
        {
          "_id": "685b75d0d2ee4fac76521e74",
          "name": "Shuofei Qiao",
          "hidden": false
        },
        {
          "_id": "685b75d0d2ee4fac76521e75",
          "name": "Yujie Luo",
          "hidden": false
        },
        {
          "_id": "685b75d0d2ee4fac76521e76",
          "name": "Lun Du",
          "hidden": false
        },
        {
          "_id": "685b75d0d2ee4fac76521e77",
          "name": "Da Zheng",
          "hidden": false
        },
        {
          "_id": "685b75d0d2ee4fac76521e78",
          "name": "Huajun Chen",
          "hidden": false
        },
        {
          "_id": "685b75d0d2ee4fac76521e79",
          "name": "Ningyu Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-24T17:04:23.000Z",
      "submittedOnDailyAt": "2025-06-25T02:37:00.536Z",
      "title": "Why Do Open-Source LLMs Struggle with Data Analysis? A Systematic\n  Empirical Study",
      "submittedOnDailyBy": {
        "_id": "620b3bbb0668e435407c8d0a",
        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
        "isPro": false,
        "fullname": "Ningyu Zhang",
        "user": "Ningyu",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) hold promise in automating data analysis tasks,\nyet open-source models face significant limitations in these kinds of\nreasoning-intensive scenarios. In this work, we investigate strategies to\nenhance the data analysis capabilities of open-source LLMs. By curating a seed\ndataset of diverse, realistic scenarios, we evaluate models across three\ndimensions: data understanding, code generation, and strategic planning. Our\nanalysis reveals three key findings: (1) Strategic planning quality serves as\nthe primary determinant of model performance; (2) Interaction design and task\ncomplexity significantly influence reasoning capabilities; (3) Data quality\ndemonstrates a greater impact than diversity in achieving optimal performance.\nWe leverage these insights to develop a data synthesis methodology,\ndemonstrating significant improvements in open-source LLMs' analytical\nreasoning capabilities.",
      "upvotes": 5,
      "discussionId": "685b75d1d2ee4fac76521e7a",
      "ai_summary": "Enhancements to open-source large language models' data analysis capabilities through strategic planning, interaction design, and data quality improvements were identified and applied.",
      "ai_keywords": [
        "Large Language Models",
        "data analysis",
        "data understanding",
        "code generation",
        "strategic planning",
        "interaction design",
        "task complexity",
        "data quality",
        "data synthesis methodology"
      ]
    },
    "publishedAt": "2025-06-24T13:04:23.000Z",
    "title": "Why Do Open-Source LLMs Struggle with Data Analysis? A Systematic\n  Empirical Study",
    "summary": "Large Language Models (LLMs) hold promise in automating data analysis tasks,\nyet open-source models face significant limitations in these kinds of\nreasoning-intensive scenarios. In this work, we investigate strategies to\nenhance the data analysis capabilities of open-source LLMs. By curating a seed\ndataset of diverse, realistic scenarios, we evaluate models across three\ndimensions: data understanding, code generation, and strategic planning. Our\nanalysis reveals three key findings: (1) Strategic planning quality serves as\nthe primary determinant of model performance; (2) Interaction design and task\ncomplexity significantly influence reasoning capabilities; (3) Data quality\ndemonstrates a greater impact than diversity in achieving optimal performance.\nWe leverage these insights to develop a data synthesis methodology,\ndemonstrating significant improvements in open-source LLMs' analytical\nreasoning capabilities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19794.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620b3bbb0668e435407c8d0a",
      "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
      "fullname": "Ningyu Zhang",
      "name": "Ningyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 24
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.18843",
      "authors": [
        {
          "_id": "685a06460e4ad7e2197584c0",
          "user": {
            "_id": "6179f36a2a4e9edab3a95798",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6179f36a2a4e9edab3a95798/0mmFY5lFzPC5k6_GSdmYQ.jpeg",
            "isPro": false,
            "fullname": "Heng-Jui Chang",
            "user": "vectominist",
            "type": "user"
          },
          "name": "Heng-Jui Chang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-24T08:08:49.104Z",
          "hidden": false
        },
        {
          "_id": "685a06460e4ad7e2197584c1",
          "name": "Saurabhchand Bhati",
          "hidden": false
        },
        {
          "_id": "685a06460e4ad7e2197584c2",
          "name": "James Glass",
          "hidden": false
        },
        {
          "_id": "685a06460e4ad7e2197584c3",
          "name": "Alexander H. Liu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6179f36a2a4e9edab3a95798/QPPxw2hyPglQF_q3uWSyk.png"
      ],
      "publishedAt": "2025-06-23T17:02:00.000Z",
      "submittedOnDailyAt": "2025-06-25T02:21:09.630Z",
      "title": "USAD: Universal Speech and Audio Representation via Distillation",
      "submittedOnDailyBy": {
        "_id": "6179f36a2a4e9edab3a95798",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6179f36a2a4e9edab3a95798/0mmFY5lFzPC5k6_GSdmYQ.jpeg",
        "isPro": false,
        "fullname": "Heng-Jui Chang",
        "user": "vectominist",
        "type": "user"
      },
      "summary": "Self-supervised learning (SSL) has revolutionized audio representations, yet\nmodels often remain domain-specific, focusing on either speech or non-speech\ntasks. In this work, we present Universal Speech and Audio Distillation (USAD),\na unified approach to audio representation learning that integrates diverse\naudio types - speech, sound, and music - into a single model. USAD employs\nefficient layer-to-layer distillation from domain-specific SSL models to train\na student on a comprehensive audio dataset. USAD offers competitive performance\nacross various benchmarks and datasets, including frame and instance-level\nspeech processing tasks, audio tagging, and sound classification, achieving\nnear state-of-the-art results with a single encoder on SUPERB and HEAR\nbenchmarks.",
      "upvotes": 5,
      "discussionId": "685a06470e4ad7e2197584c4",
      "ai_summary": "USAD integrates diverse audio types using efficient layer-to-layer distillation from domain-specific models, achieving competitive performance across various benchmarks with a single encoder.",
      "ai_keywords": [
        "self-supervised learning",
        "universal speech and audio distillation",
        "domain-specific models",
        "layer-to-layer distillation",
        "frame and instance-level speech processing",
        "audio tagging",
        "sound classification",
        "encoder",
        "SUPERB benchmarks",
        "HEAR benchmarks"
      ]
    },
    "publishedAt": "2025-06-23T13:02:00.000Z",
    "title": "USAD: Universal Speech and Audio Representation via Distillation",
    "summary": "Self-supervised learning (SSL) has revolutionized audio representations, yet\nmodels often remain domain-specific, focusing on either speech or non-speech\ntasks. In this work, we present Universal Speech and Audio Distillation (USAD),\na unified approach to audio representation learning that integrates diverse\naudio types - speech, sound, and music - into a single model. USAD employs\nefficient layer-to-layer distillation from domain-specific SSL models to train\na student on a comprehensive audio dataset. USAD offers competitive performance\nacross various benchmarks and datasets, including frame and instance-level\nspeech processing tasks, audio tagging, and sound classification, achieving\nnear state-of-the-art results with a single encoder on SUPERB and HEAR\nbenchmarks.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6179f36a2a4e9edab3a95798/QPPxw2hyPglQF_q3uWSyk.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18843.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6179f36a2a4e9edab3a95798",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6179f36a2a4e9edab3a95798/0mmFY5lFzPC5k6_GSdmYQ.jpeg",
      "fullname": "Heng-Jui Chang",
      "name": "vectominist",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.19807",
      "authors": [
        {
          "_id": "685b75edd2ee4fac76521e7c",
          "name": "Baochang Ren",
          "hidden": false
        },
        {
          "_id": "685b75edd2ee4fac76521e7d",
          "name": "Shuofei Qiao",
          "hidden": false
        },
        {
          "_id": "685b75edd2ee4fac76521e7e",
          "name": "Wenhao Yu",
          "hidden": false
        },
        {
          "_id": "685b75edd2ee4fac76521e7f",
          "name": "Huajun Chen",
          "hidden": false
        },
        {
          "_id": "685b75edd2ee4fac76521e80",
          "name": "Ningyu Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-24T17:17:17.000Z",
      "submittedOnDailyAt": "2025-06-25T02:37:40.331Z",
      "title": "KnowRL: Exploring Knowledgeable Reinforcement Learning for Factuality",
      "submittedOnDailyBy": {
        "_id": "620b3bbb0668e435407c8d0a",
        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
        "isPro": false,
        "fullname": "Ningyu Zhang",
        "user": "Ningyu",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs), particularly slow-thinking models, often\nexhibit severe hallucination, outputting incorrect content due to an inability\nto accurately recognize knowledge boundaries during reasoning. While\nReinforcement Learning (RL) can enhance complex reasoning abilities, its\noutcome-oriented reward mechanism often lacks factual supervision over the\nthinking process, further exacerbating the hallucination problem. To address\nthe high hallucination in slow-thinking models, we propose Knowledge-enhanced\nRL, KnowRL. KnowRL guides models to perform fact-based slow thinking by\nintegrating a factuality reward, based on knowledge verification, into the RL\ntraining process, helping them recognize their knowledge boundaries. KnowRL\nguides models to perform fact-based slow thinking by integrating a factuality\nreward, based on knowledge verification, into the RL training process, helping\nthem recognize their knowledge boundaries. This targeted factual input during\nRL training enables the model to learn and internalize fact-based reasoning\nstrategies. By directly rewarding adherence to facts within the reasoning\nsteps, KnowRL fosters a more reliable thinking process. Experimental results on\nthree hallucination evaluation datasets and two reasoning evaluation datasets\ndemonstrate that KnowRL effectively mitigates hallucinations in slow-thinking\nmodels while maintaining their original strong reasoning capabilities. Our code\nis available at https://github.com/zjunlp/KnowRL.",
      "upvotes": 4,
      "discussionId": "685b75edd2ee4fac76521e81",
      "ai_summary": "KnowRL, a knowledge-enhanced reinforcement learning approach, reduces hallucinations in slow-thinking large language models by incorporating factuality rewards based on knowledge verification during training.",
      "ai_keywords": [
        "Large Language Models",
        "slow-thinking models",
        "hallucination",
        "Reinforcement Learning",
        "KnowRL",
        "factuality reward",
        "knowledge verification",
        "reasoning",
        "reasoning capabilities"
      ]
    },
    "publishedAt": "2025-06-24T13:17:17.000Z",
    "title": "KnowRL: Exploring Knowledgeable Reinforcement Learning for Factuality",
    "summary": "Large Language Models (LLMs), particularly slow-thinking models, often\nexhibit severe hallucination, outputting incorrect content due to an inability\nto accurately recognize knowledge boundaries during reasoning. While\nReinforcement Learning (RL) can enhance complex reasoning abilities, its\noutcome-oriented reward mechanism often lacks factual supervision over the\nthinking process, further exacerbating the hallucination problem. To address\nthe high hallucination in slow-thinking models, we propose Knowledge-enhanced\nRL, KnowRL. KnowRL guides models to perform fact-based slow thinking by\nintegrating a factuality reward, based on knowledge verification, into the RL\ntraining process, helping them recognize their knowledge boundaries. KnowRL\nguides models to perform fact-based slow thinking by integrating a factuality\nreward, based on knowledge verification, into the RL training process, helping\nthem recognize their knowledge boundaries. This targeted factual input during\nRL training enables the model to learn and internalize fact-based reasoning\nstrategies. By directly rewarding adherence to facts within the reasoning\nsteps, KnowRL fosters a more reliable thinking process. Experimental results on\nthree hallucination evaluation datasets and two reasoning evaluation datasets\ndemonstrate that KnowRL effectively mitigates hallucinations in slow-thinking\nmodels while maintaining their original strong reasoning capabilities. Our code\nis available at https://github.com/zjunlp/KnowRL.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19807.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620b3bbb0668e435407c8d0a",
      "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
      "fullname": "Ningyu Zhang",
      "name": "Ningyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 24
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.19713",
      "authors": [
        {
          "_id": "685b9a5dd2ee4fac76521ecc",
          "name": "Seyedmorteza Sadat",
          "hidden": false
        },
        {
          "_id": "685b9a5dd2ee4fac76521ecd",
          "name": "Tobias Vontobel",
          "hidden": false
        },
        {
          "_id": "685b9a5dd2ee4fac76521ece",
          "name": "Farnood Salehi",
          "hidden": false
        },
        {
          "_id": "685b9a5dd2ee4fac76521ecf",
          "name": "Romann M. Weber",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-24T15:19:42.000Z",
      "submittedOnDailyAt": "2025-06-25T05:16:23.771Z",
      "title": "Guidance in the Frequency Domain Enables High-Fidelity Sampling at Low\n  CFG Scales",
      "submittedOnDailyBy": {
        "_id": "63b4b02a103617b0a5b0ee2e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b4b02a103617b0a5b0ee2e/LasBC-pCnPJ6XuCt6qqcp.jpeg",
        "isPro": false,
        "fullname": "Seyedmorteza Sadat",
        "user": "msadat97",
        "type": "user"
      },
      "summary": "Classifier-free guidance (CFG) has become an essential component of modern\nconditional diffusion models. Although highly effective in practice, the\nunderlying mechanisms by which CFG enhances quality, detail, and prompt\nalignment are not fully understood. We present a novel perspective on CFG by\nanalyzing its effects in the frequency domain, showing that low and high\nfrequencies have distinct impacts on generation quality. Specifically,\nlow-frequency guidance governs global structure and condition alignment, while\nhigh-frequency guidance mainly enhances visual fidelity. However, applying a\nuniform scale across all frequencies -- as is done in standard CFG -- leads to\noversaturation and reduced diversity at high scales and degraded visual quality\nat low scales. Based on these insights, we propose frequency-decoupled guidance\n(FDG), an effective approach that decomposes CFG into low- and high-frequency\ncomponents and applies separate guidance strengths to each component. FDG\nimproves image quality at low guidance scales and avoids the drawbacks of high\nCFG scales by design. Through extensive experiments across multiple datasets\nand models, we demonstrate that FDG consistently enhances sample fidelity while\npreserving diversity, leading to improved FID and recall compared to CFG,\nestablishing our method as a plug-and-play alternative to standard\nclassifier-free guidance.",
      "upvotes": 3,
      "discussionId": "685b9a5ed2ee4fac76521ed0",
      "ai_summary": "Frequency-decoupled guidance (FDG) enhances image quality and diversity by separately controlling low- and high-frequency guidance components in diffusion models, outperforming standard classifier-free guidance.",
      "ai_keywords": [
        "classifier-free guidance",
        "conditional diffusion models",
        "frequency domain",
        "low-frequency guidance",
        "high-frequency guidance",
        "frequency-decoupled guidance",
        "FID",
        "recall"
      ]
    },
    "publishedAt": "2025-06-24T11:19:42.000Z",
    "title": "Guidance in the Frequency Domain Enables High-Fidelity Sampling at Low\n  CFG Scales",
    "summary": "Classifier-free guidance (CFG) has become an essential component of modern\nconditional diffusion models. Although highly effective in practice, the\nunderlying mechanisms by which CFG enhances quality, detail, and prompt\nalignment are not fully understood. We present a novel perspective on CFG by\nanalyzing its effects in the frequency domain, showing that low and high\nfrequencies have distinct impacts on generation quality. Specifically,\nlow-frequency guidance governs global structure and condition alignment, while\nhigh-frequency guidance mainly enhances visual fidelity. However, applying a\nuniform scale across all frequencies -- as is done in standard CFG -- leads to\noversaturation and reduced diversity at high scales and degraded visual quality\nat low scales. Based on these insights, we propose frequency-decoupled guidance\n(FDG), an effective approach that decomposes CFG into low- and high-frequency\ncomponents and applies separate guidance strengths to each component. FDG\nimproves image quality at low guidance scales and avoids the drawbacks of high\nCFG scales by design. Through extensive experiments across multiple datasets\nand models, we demonstrate that FDG consistently enhances sample fidelity while\npreserving diversity, leading to improved FID and recall compared to CFG,\nestablishing our method as a plug-and-play alternative to standard\nclassifier-free guidance.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19713.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63b4b02a103617b0a5b0ee2e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b4b02a103617b0a5b0ee2e/LasBC-pCnPJ6XuCt6qqcp.jpeg",
      "fullname": "Seyedmorteza Sadat",
      "name": "msadat97",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.18951",
      "authors": [
        {
          "_id": "685ba757d2ee4fac76521f47",
          "name": "Jinyang Li",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f48",
          "name": "Xiaolong Li",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f49",
          "name": "Ge Qu",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f4a",
          "name": "Per Jacobsson",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f4b",
          "name": "Bowen Qin",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f4c",
          "name": "Binyuan Hui",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f4d",
          "name": "Shuzheng Si",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f4e",
          "name": "Nan Huo",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f4f",
          "name": "Xiaohan Xu",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f50",
          "name": "Yue Zhang",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f51",
          "name": "Ziwei Tang",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f52",
          "name": "Yuanshuai Li",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f53",
          "name": "Florensia Widjaja",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f54",
          "name": "Xintong Zhu",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f55",
          "name": "Feige Zhou",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f56",
          "name": "Yongfeng Huang",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f57",
          "name": "Yannis Papakonstantinou",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f58",
          "name": "Fatma Ozcan",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f59",
          "name": "Chenhao Ma",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f5a",
          "name": "Reynold Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-23T09:41:37.000Z",
      "submittedOnDailyAt": "2025-06-25T06:09:19.766Z",
      "title": "SWE-SQL: Illuminating LLM Pathways to Solve User SQL Issues in\n  Real-World Applications",
      "submittedOnDailyBy": {
        "_id": "6419435385030eca6ac94701",
        "avatarUrl": "/avatars/c49ff1991739de49ec98c8310ab21e46.svg",
        "isPro": false,
        "fullname": "Ge Qu",
        "user": "gq2138",
        "type": "user"
      },
      "summary": "Resolution of complex SQL issues persists as a significant bottleneck in\nreal-world database applications. Current Large Language Models (LLMs), while\nadept at text-to-SQL translation, have not been rigorously evaluated on the\nmore challenging task of debugging SQL issues. To address this gap, we\nintroduce BIRD-CRITIC, a new SQL issue debugging benchmark comprising 530\nPostgreSQL tasks (BIRD-CRITIC-PG) and 570 multi-dialect tasks\n(BIRD-CRITIC-Multi), distilled from authentic user issues and replayed within\nnew environments to facilitate rigorous evaluation. Baseline evaluations\nunderscore the task's complexity, with the leading reasoning model O3-Mini\nachieving only 38.87% success rate on BIRD-CRITIC-PG and 33.33% on\nBIRD-CRITIC-Multi. Meanwhile, advancing open-source models for database tasks\nis crucial for empowering local development while safeguarding data privacy.\nTherefore, we present Six-Gym (Sql-fIX-Gym), a training environment for\nelevating open-source model capabilities for SQL issue debugging. This\nenvironment leverages SQL-Rewind strategy, which automatically generates\nexecutable issue-solution datasets by reverse-engineering issues from verified\nSQLs. However, popular trajectory-based fine-tuning methods do not explore\nsubstantial supervisory signals. We further propose f-Plan Boosting, which\nextracts high-level debugging plans from SQL solutions, enabling teacher LLMs\nto produce 73.7% more successful trajectories for training. We integrate these\ncomponents into an open-source agent, Bird-Fixer. Based on Qwen-2.5-Coder-14B,\nBird-Fixer achieves 38.11% success rate on BIRD-CRITIC-PG and 29.65% on\nBIRD-CRITIC-Multi, surpassing leading proprietary models such as\nClaude-3.7-Sonnet and GPT-4.1, marking a significant step toward democratizing\nsophisticated SQL-debugging capabilities. The leaderboard and source code are\navailable: https://bird-critic.github.io/",
      "upvotes": 3,
      "discussionId": "685ba758d2ee4fac76521f5b",
      "ai_summary": "A new benchmark and training environment for debugging SQL issues using advanced open-source models significantly improves their performance compared to proprietary solutions.",
      "ai_keywords": [
        "BIRD-CRITIC",
        "BIRD-CRITIC-PG",
        "BIRD-CRITIC-Multi",
        "PostgreSQL",
        "Six-Gym (Sql-fIX-Gym)",
        "SQL-Rewind",
        "f-Plan Boosting",
        "Bird-Fixer",
        "Qwen-2.5-Coder-14B",
        "Claude-3.7-Sonnet",
        "GPT-4.1"
      ]
    },
    "publishedAt": "2025-06-23T05:41:37.000Z",
    "title": "SWE-SQL: Illuminating LLM Pathways to Solve User SQL Issues in\n  Real-World Applications",
    "summary": "Resolution of complex SQL issues persists as a significant bottleneck in\nreal-world database applications. Current Large Language Models (LLMs), while\nadept at text-to-SQL translation, have not been rigorously evaluated on the\nmore challenging task of debugging SQL issues. To address this gap, we\nintroduce BIRD-CRITIC, a new SQL issue debugging benchmark comprising 530\nPostgreSQL tasks (BIRD-CRITIC-PG) and 570 multi-dialect tasks\n(BIRD-CRITIC-Multi), distilled from authentic user issues and replayed within\nnew environments to facilitate rigorous evaluation. Baseline evaluations\nunderscore the task's complexity, with the leading reasoning model O3-Mini\nachieving only 38.87% success rate on BIRD-CRITIC-PG and 33.33% on\nBIRD-CRITIC-Multi. Meanwhile, advancing open-source models for database tasks\nis crucial for empowering local development while safeguarding data privacy.\nTherefore, we present Six-Gym (Sql-fIX-Gym), a training environment for\nelevating open-source model capabilities for SQL issue debugging. This\nenvironment leverages SQL-Rewind strategy, which automatically generates\nexecutable issue-solution datasets by reverse-engineering issues from verified\nSQLs. However, popular trajectory-based fine-tuning methods do not explore\nsubstantial supervisory signals. We further propose f-Plan Boosting, which\nextracts high-level debugging plans from SQL solutions, enabling teacher LLMs\nto produce 73.7% more successful trajectories for training. We integrate these\ncomponents into an open-source agent, Bird-Fixer. Based on Qwen-2.5-Coder-14B,\nBird-Fixer achieves 38.11% success rate on BIRD-CRITIC-PG and 29.65% on\nBIRD-CRITIC-Multi, surpassing leading proprietary models such as\nClaude-3.7-Sonnet and GPT-4.1, marking a significant step toward democratizing\nsophisticated SQL-debugging capabilities. The leaderboard and source code are\navailable: https://bird-critic.github.io/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18951.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6419435385030eca6ac94701",
      "avatarUrl": "/avatars/c49ff1991739de49ec98c8310ab21e46.svg",
      "fullname": "Ge Qu",
      "name": "gq2138",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.17612",
      "authors": [
        {
          "_id": "685b7538d2ee4fac76521e63",
          "name": "Yunlong Lin",
          "hidden": false
        },
        {
          "_id": "685b7538d2ee4fac76521e64",
          "name": "Zixu Lin",
          "hidden": false
        },
        {
          "_id": "685b7538d2ee4fac76521e65",
          "name": "Kunjie Lin",
          "hidden": false
        },
        {
          "_id": "685b7538d2ee4fac76521e66",
          "name": "Jinbin Bai",
          "hidden": false
        },
        {
          "_id": "685b7538d2ee4fac76521e67",
          "name": "Panwang Pan",
          "hidden": false
        },
        {
          "_id": "685b7538d2ee4fac76521e68",
          "name": "Chenxin Li",
          "hidden": false
        },
        {
          "_id": "685b7538d2ee4fac76521e69",
          "name": "Haoyu Chen",
          "hidden": false
        },
        {
          "_id": "685b7538d2ee4fac76521e6a",
          "name": "Zhongdao Wang",
          "hidden": false
        },
        {
          "_id": "685b7538d2ee4fac76521e6b",
          "name": "Xinghao Ding",
          "hidden": false
        },
        {
          "_id": "685b7538d2ee4fac76521e6c",
          "name": "Wenbo Li",
          "hidden": false
        },
        {
          "_id": "685b7538d2ee4fac76521e6d",
          "name": "Shuicheng Yan",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64ecb174f22081b4ac7ca397/PCTU9EbA4XUz8eKeLbxVC.gif"
      ],
      "publishedAt": "2025-06-21T06:36:00.000Z",
      "submittedOnDailyAt": "2025-06-25T02:43:05.885Z",
      "title": "JarvisArt: Liberating Human Artistic Creativity via an Intelligent Photo\n  Retouching Agent",
      "submittedOnDailyBy": {
        "_id": "64ecb174f22081b4ac7ca397",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ecb174f22081b4ac7ca397/PiAPtD_rbuhGOqfE6ZSIu.jpeg",
        "isPro": true,
        "fullname": "Yunlong Lin",
        "user": "LYL1015",
        "type": "user"
      },
      "summary": "Photo retouching has become integral to contemporary visual storytelling,\nenabling users to capture aesthetics and express creativity. While professional\ntools such as Adobe Lightroom offer powerful capabilities, they demand\nsubstantial expertise and manual effort. In contrast, existing AI-based\nsolutions provide automation but often suffer from limited adjustability and\npoor generalization, failing to meet diverse and personalized editing needs. To\nbridge this gap, we introduce JarvisArt, a multi-modal large language model\n(MLLM)-driven agent that understands user intent, mimics the reasoning process\nof professional artists, and intelligently coordinates over 200 retouching\ntools within Lightroom. JarvisArt undergoes a two-stage training process: an\ninitial Chain-of-Thought supervised fine-tuning to establish basic reasoning\nand tool-use skills, followed by Group Relative Policy Optimization for\nRetouching (GRPO-R) to further enhance its decision-making and tool\nproficiency. We also propose the Agent-to-Lightroom Protocol to facilitate\nseamless integration with Lightroom. To evaluate performance, we develop\nMMArt-Bench, a novel benchmark constructed from real-world user edits.\nJarvisArt demonstrates user-friendly interaction, superior generalization, and\nfine-grained control over both global and local adjustments, paving a new\navenue for intelligent photo retouching. Notably, it outperforms GPT-4o with a\n60% improvement in average pixel-level metrics on MMArt-Bench for content\nfidelity, while maintaining comparable instruction-following capabilities.\nProject Page: https://jarvisart.vercel.app/.",
      "upvotes": 3,
      "discussionId": "685b7539d2ee4fac76521e6e",
      "projectPage": "https://jarvisart.vercel.app/",
      "githubRepo": "https://github.com/LYL1015/JarvisArt",
      "ai_summary": "JarvisArt, an MLLM-driven agent, achieves superior photo retouching by understanding user intent and coordinating multiple retouching tools in Lightroom, outperforming GPT-4o on a novel benchmark.",
      "ai_keywords": [
        "multi-modal large language model",
        "Chain-of-Thought supervised fine-tuning",
        "Group Relative Policy Optimization",
        "Agent-to-Lightroom Protocol",
        "MMArt-Bench",
        "global adjustments",
        "local adjustments",
        "content fidelity",
        "instruction-following capabilities"
      ],
      "githubStars": 3
    },
    "publishedAt": "2025-06-21T02:36:00.000Z",
    "title": "JarvisArt: Liberating Human Artistic Creativity via an Intelligent Photo\n  Retouching Agent",
    "summary": "Photo retouching has become integral to contemporary visual storytelling,\nenabling users to capture aesthetics and express creativity. While professional\ntools such as Adobe Lightroom offer powerful capabilities, they demand\nsubstantial expertise and manual effort. In contrast, existing AI-based\nsolutions provide automation but often suffer from limited adjustability and\npoor generalization, failing to meet diverse and personalized editing needs. To\nbridge this gap, we introduce JarvisArt, a multi-modal large language model\n(MLLM)-driven agent that understands user intent, mimics the reasoning process\nof professional artists, and intelligently coordinates over 200 retouching\ntools within Lightroom. JarvisArt undergoes a two-stage training process: an\ninitial Chain-of-Thought supervised fine-tuning to establish basic reasoning\nand tool-use skills, followed by Group Relative Policy Optimization for\nRetouching (GRPO-R) to further enhance its decision-making and tool\nproficiency. We also propose the Agent-to-Lightroom Protocol to facilitate\nseamless integration with Lightroom. To evaluate performance, we develop\nMMArt-Bench, a novel benchmark constructed from real-world user edits.\nJarvisArt demonstrates user-friendly interaction, superior generalization, and\nfine-grained control over both global and local adjustments, paving a new\navenue for intelligent photo retouching. Notably, it outperforms GPT-4o with a\n60% improvement in average pixel-level metrics on MMArt-Bench for content\nfidelity, while maintaining comparable instruction-following capabilities.\nProject Page: https://jarvisart.vercel.app/.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64ecb174f22081b4ac7ca397/PCTU9EbA4XUz8eKeLbxVC.gif"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.17612.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64ecb174f22081b4ac7ca397",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ecb174f22081b4ac7ca397/PiAPtD_rbuhGOqfE6ZSIu.jpeg",
      "fullname": "Yunlong Lin",
      "name": "LYL1015",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.14012",
      "authors": [
        {
          "_id": "685b863bd2ee4fac76521e92",
          "name": "Amr Mohamed",
          "hidden": false
        },
        {
          "_id": "685b863bd2ee4fac76521e93",
          "name": "Yang Zhang",
          "hidden": false
        },
        {
          "_id": "685b863bd2ee4fac76521e94",
          "name": "Michalis Vazirgiannis",
          "hidden": false
        },
        {
          "_id": "685b863bd2ee4fac76521e95",
          "name": "Guokan Shang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-16T21:19:27.000Z",
      "submittedOnDailyAt": "2025-06-25T03:51:26.828Z",
      "title": "Lost in the Mix: Evaluating LLM Understanding of Code-Switched Text",
      "submittedOnDailyBy": {
        "_id": "655efd24afee0e00788bb589",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655efd24afee0e00788bb589/22guLxIWNybbJR3jI-c4w.jpeg",
        "isPro": false,
        "fullname": "Amr Mohamed",
        "user": "amr-mohamed",
        "type": "user"
      },
      "summary": "Code-switching (CSW) is the act of alternating between two or more languages\nwithin a single discourse. This phenomenon is widespread in multilingual\ncommunities, and increasingly prevalent in online content, where users\nnaturally mix languages in everyday communication. As a result, Large Language\nModels (LLMs), now central to content processing and generation, are frequently\nexposed to code-switched inputs. Given their widespread use, it is crucial to\nunderstand how LLMs process and reason about such mixed-language text. This\npaper presents a systematic evaluation of LLM comprehension under\ncode-switching by generating CSW variants of established reasoning and\ncomprehension benchmarks. While degradation is evident when foreign tokens\ndisrupt English textx2013even under linguistic\nconstraintsx2013embedding English into other languages often\nimproves comprehension. Though prompting yields mixed results, fine-tuning\noffers a more stable path to degradation mitigation.",
      "upvotes": 3,
      "discussionId": "685b863bd2ee4fac76521e96",
      "ai_summary": "LLMs' comprehension and reasoning skills are evaluated under code-switching conditions, revealing that embedding English into other languages can improve understanding, while prompts and fine-tuning affect degradation mitigation differently.",
      "ai_keywords": [
        "Large Language Models",
        "code-switching",
        "CSW",
        "reasoning benchmarks",
        "comprehension benchmarks",
        "foreign tokens",
        "embedding",
        "fine-tuning"
      ]
    },
    "publishedAt": "2025-06-16T17:19:27.000Z",
    "title": "Lost in the Mix: Evaluating LLM Understanding of Code-Switched Text",
    "summary": "Code-switching (CSW) is the act of alternating between two or more languages\nwithin a single discourse. This phenomenon is widespread in multilingual\ncommunities, and increasingly prevalent in online content, where users\nnaturally mix languages in everyday communication. As a result, Large Language\nModels (LLMs), now central to content processing and generation, are frequently\nexposed to code-switched inputs. Given their widespread use, it is crucial to\nunderstand how LLMs process and reason about such mixed-language text. This\npaper presents a systematic evaluation of LLM comprehension under\ncode-switching by generating CSW variants of established reasoning and\ncomprehension benchmarks. While degradation is evident when foreign tokens\ndisrupt English textx2013even under linguistic\nconstraintsx2013embedding English into other languages often\nimproves comprehension. Though prompting yields mixed results, fine-tuning\noffers a more stable path to degradation mitigation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14012.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "655efd24afee0e00788bb589",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655efd24afee0e00788bb589/22guLxIWNybbJR3jI-c4w.jpeg",
      "fullname": "Amr Mohamed",
      "name": "amr-mohamed",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.19850",
      "authors": [
        {
          "_id": "685b63c2d2ee4fac76521dee",
          "name": "Yuqi Wang",
          "hidden": false
        },
        {
          "_id": "685b63c2d2ee4fac76521def",
          "name": "Xinghang Li",
          "hidden": false
        },
        {
          "_id": "685b63c2d2ee4fac76521df0",
          "name": "Wenxuan Wang",
          "hidden": false
        },
        {
          "_id": "685b63c2d2ee4fac76521df1",
          "name": "Junbo Zhang",
          "hidden": false
        },
        {
          "_id": "685b63c2d2ee4fac76521df2",
          "name": "Yingyan Li",
          "hidden": false
        },
        {
          "_id": "685b63c2d2ee4fac76521df3",
          "name": "Yuntao Chen",
          "hidden": false
        },
        {
          "_id": "685b63c2d2ee4fac76521df4",
          "name": "Xinlong Wang",
          "hidden": false
        },
        {
          "_id": "685b63c2d2ee4fac76521df5",
          "name": "Zhaoxiang Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-24T17:59:57.000Z",
      "submittedOnDailyAt": "2025-06-25T06:01:30.093Z",
      "title": "Unified Vision-Language-Action Model",
      "submittedOnDailyBy": {
        "_id": "649fe21d59c1ae90dbfacf91",
        "avatarUrl": "/avatars/7f77fa77113e80cb45406927e2386387.svg",
        "isPro": false,
        "fullname": "Wang Yuqi",
        "user": "Yuqi1997",
        "type": "user"
      },
      "summary": "Vision-language-action models (VLAs) have garnered significant attention for\ntheir potential in advancing robotic manipulation. However, previous approaches\npredominantly rely on the general comprehension capabilities of vision-language\nmodels (VLMs) to generate action signals, often overlooking the rich temporal\nand causal structure embedded in visual observations. In this paper, we present\nUniVLA, a unified and native multimodal VLA model that autoregressively models\nvision, language, and action signals as discrete token sequences. This\nformulation enables flexible multimodal tasks learning, particularly from\nlarge-scale video data. By incorporating world modeling during post-training,\nUniVLA captures causal dynamics from videos, facilitating effective transfer to\ndownstream policy learning--especially for long-horizon tasks. Our approach\nsets new state-of-the-art results across several widely used simulation\nbenchmarks, including CALVIN, LIBERO, and Simplenv-Bridge, significantly\nsurpassing previous methods. For example, UniVLA achieves 95.5% average success\nrate on LIBERO benchmark, surpassing pi0-FAST's 85.5%. We further demonstrate\nits broad applicability on real-world ALOHA manipulation and autonomous\ndriving.",
      "upvotes": 1,
      "discussionId": "685b63c3d2ee4fac76521df6",
      "ai_summary": "UniVLA is a multimodal VLA model that autoregressively processes vision, language, and action as token sequences, incorporating world modeling for effective long-horizon policy learning and achieving state-of-the-art results across simulation and real-world benchmarks.",
      "ai_keywords": [
        "vision-language-action models",
        "VLAs",
        "vision-language models",
        "VLMs",
        "autoregressive models",
        "discrete token sequences",
        "multimodal tasks learning",
        "world modeling",
        "causal dynamics",
        "policy learning",
        "simulation benchmarks",
        "CALVIN",
        "LIBERO",
        "Simplenv-Bridge",
        "ALOHA manipulation",
        "autonomous driving"
      ]
    },
    "publishedAt": "2025-06-24T13:59:57.000Z",
    "title": "Unified Vision-Language-Action Model",
    "summary": "Vision-language-action models (VLAs) have garnered significant attention for\ntheir potential in advancing robotic manipulation. However, previous approaches\npredominantly rely on the general comprehension capabilities of vision-language\nmodels (VLMs) to generate action signals, often overlooking the rich temporal\nand causal structure embedded in visual observations. In this paper, we present\nUniVLA, a unified and native multimodal VLA model that autoregressively models\nvision, language, and action signals as discrete token sequences. This\nformulation enables flexible multimodal tasks learning, particularly from\nlarge-scale video data. By incorporating world modeling during post-training,\nUniVLA captures causal dynamics from videos, facilitating effective transfer to\ndownstream policy learning--especially for long-horizon tasks. Our approach\nsets new state-of-the-art results across several widely used simulation\nbenchmarks, including CALVIN, LIBERO, and Simplenv-Bridge, significantly\nsurpassing previous methods. For example, UniVLA achieves 95.5% average success\nrate on LIBERO benchmark, surpassing pi0-FAST's 85.5%. We further demonstrate\nits broad applicability on real-world ALOHA manipulation and autonomous\ndriving.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19850.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649fe21d59c1ae90dbfacf91",
      "avatarUrl": "/avatars/7f77fa77113e80cb45406927e2386387.svg",
      "fullname": "Wang Yuqi",
      "name": "Yuqi1997",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  }
]