[
    {
        "paper": {
            "id": "2411.08147",
            "authors": [
                {
                    "_id": "673593ee8d2bfc6159c2f994",
                    "user": {
                        "_id": "66d45a8de5837f38ce3b73f7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66d45a8de5837f38ce3b73f7/5MQusIK5fD6W9kJwdXIlR.jpeg",
                        "isPro": false,
                        "fullname": "SihengLi",
                        "user": "Siheng99",
                        "type": "user"
                    },
                    "name": "Siheng Li",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2024-11-14T06:09:11.650Z",
                    "hidden": false
                },
                {
                    "_id": "673593ee8d2bfc6159c2f995",
                    "name": "Cheng Yang",
                    "hidden": false
                },
                {
                    "_id": "673593ee8d2bfc6159c2f996",
                    "name": "Zesen Cheng",
                    "hidden": false
                },
                {
                    "_id": "673593ee8d2bfc6159c2f997",
                    "name": "Lemao Liu",
                    "hidden": false
                },
                {
                    "_id": "673593ee8d2bfc6159c2f998",
                    "name": "Mo Yu",
                    "hidden": false
                },
                {
                    "_id": "673593ee8d2bfc6159c2f999",
                    "name": "Yujiu Yang",
                    "hidden": false
                },
                {
                    "_id": "673593ee8d2bfc6159c2f99a",
                    "name": "Wai Lam",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-12T19:53:00.000Z",
            "title": "Large Language Models Can Self-Improve in Long-context Reasoning",
            "summary": "Large language models (LLMs) have achieved substantial progress in processing\nlong contexts but still struggle with long-context reasoning. Existing\napproaches typically involve fine-tuning LLMs with synthetic data, which\ndepends on annotations from human experts or advanced models like GPT-4, thus\nrestricting further advancements. To address this issue, we investigate the\npotential for LLMs to self-improve in long-context reasoning and propose \\ours,\nan approach specifically designed for this purpose. This approach is\nstraightforward: we sample multiple outputs for each question, score them with\nMinimum Bayes Risk, and then apply supervised fine-tuning or preference\noptimization based on these outputs. Extensive experiments on several leading\nLLMs demonstrate the effectiveness of \\ours, with an absolute improvement of\n4.2 points for Llama-3.1-8B-Instruct. Furthermore, \\ours achieves superior\nperformance compared to prior approaches that depend on data produced by human\nexperts or advanced models. We anticipate that this work will open new avenues\nfor self-improvement techniques in long-context scenarios, which are essential\nfor the continual advancement of LLMs.",
            "upvotes": 36,
            "discussionId": "673593ef8d2bfc6159c2f9f6"
        },
        "publishedAt": "2024-11-14T05:16:18.902Z",
        "title": "Large Language Models Can Self-Improve in Long-context Reasoning",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.08147.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66d45a8de5837f38ce3b73f7/5MQusIK5fD6W9kJwdXIlR.jpeg",
            "fullname": "SihengLi",
            "name": "Siheng99",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 3
        }
    },
    {
        "paper": {
            "id": "2411.08380",
            "authors": [
                {
                    "_id": "67357e302a3c30eac4c75625",
                    "name": "Xiaofeng Wang",
                    "hidden": false
                },
                {
                    "_id": "67357e302a3c30eac4c75626",
                    "name": "Kang Zhao",
                    "hidden": false
                },
                {
                    "_id": "67357e302a3c30eac4c75627",
                    "name": "Feng Liu",
                    "hidden": false
                },
                {
                    "_id": "67357e302a3c30eac4c75628",
                    "name": "Jiayu Wang",
                    "hidden": false
                },
                {
                    "_id": "67357e302a3c30eac4c75629",
                    "name": "Guosheng Zhao",
                    "hidden": false
                },
                {
                    "_id": "67357e302a3c30eac4c7562a",
                    "name": "Xiaoyi Bao",
                    "hidden": false
                },
                {
                    "_id": "67357e302a3c30eac4c7562b",
                    "name": "Zheng Zhu",
                    "hidden": false
                },
                {
                    "_id": "67357e302a3c30eac4c7562c",
                    "name": "Yingya Zhang",
                    "hidden": false
                },
                {
                    "_id": "67357e302a3c30eac4c7562d",
                    "name": "Xingang Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-13T07:05:40.000Z",
            "title": "EgoVid-5M: A Large-Scale Video-Action Dataset for Egocentric Video\n  Generation",
            "summary": "Video generation has emerged as a promising tool for world simulation,\nleveraging visual data to replicate real-world environments. Within this\ncontext, egocentric video generation, which centers on the human perspective,\nholds significant potential for enhancing applications in virtual reality,\naugmented reality, and gaming. However, the generation of egocentric videos\npresents substantial challenges due to the dynamic nature of egocentric\nviewpoints, the intricate diversity of actions, and the complex variety of\nscenes encountered. Existing datasets are inadequate for addressing these\nchallenges effectively. To bridge this gap, we present EgoVid-5M, the first\nhigh-quality dataset specifically curated for egocentric video generation.\nEgoVid-5M encompasses 5 million egocentric video clips and is enriched with\ndetailed action annotations, including fine-grained kinematic control and\nhigh-level textual descriptions. To ensure the integrity and usability of the\ndataset, we implement a sophisticated data cleaning pipeline designed to\nmaintain frame consistency, action coherence, and motion smoothness under\negocentric conditions. Furthermore, we introduce EgoDreamer, which is capable\nof generating egocentric videos driven simultaneously by action descriptions\nand kinematic control signals. The EgoVid-5M dataset, associated action\nannotations, and all data cleansing metadata will be released for the\nadvancement of research in egocentric video generation.",
            "upvotes": 12,
            "discussionId": "67357e332a3c30eac4c75725"
        },
        "publishedAt": "2024-11-14T03:13:55.434Z",
        "title": "EgoVid-5M: A Large-Scale Video-Action Dataset for Egocentric Video Generation",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6426616ea5ec4a5cbc535634/k1Q0SDqMX284n2OZ_mp6c.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.08380.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6426616ea5ec4a5cbc535634/hH6JsxnXeakH3mBTeNNmO.jpeg",
            "fullname": "JeffWang",
            "name": "Jeff-Wang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2411.07618",
            "authors": [
                {
                    "_id": "673575eee12f354dfc1ea6f8",
                    "name": "Qingyu Yin",
                    "hidden": false
                },
                {
                    "_id": "673575eee12f354dfc1ea6f9",
                    "name": "Chak Tou Leong",
                    "hidden": false
                },
                {
                    "_id": "673575eee12f354dfc1ea6fa",
                    "name": "Hongbo Zhang",
                    "hidden": false
                },
                {
                    "_id": "673575eee12f354dfc1ea6fb",
                    "name": "Minjun Zhu",
                    "hidden": false
                },
                {
                    "_id": "673575eee12f354dfc1ea6fc",
                    "name": "Hanqi Yan",
                    "hidden": false
                },
                {
                    "_id": "673575eee12f354dfc1ea6fd",
                    "name": "Qiang Zhang",
                    "hidden": false
                },
                {
                    "_id": "673575eee12f354dfc1ea6fe",
                    "name": "Yulan He",
                    "hidden": false
                },
                {
                    "_id": "673575eee12f354dfc1ea6ff",
                    "name": "Wenjie Li",
                    "hidden": false
                },
                {
                    "_id": "673575eee12f354dfc1ea700",
                    "name": "Jun Wang",
                    "hidden": false
                },
                {
                    "_id": "673575eee12f354dfc1ea701",
                    "name": "Yue Zhang",
                    "hidden": false
                },
                {
                    "_id": "673575eee12f354dfc1ea702",
                    "name": "Linyi Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-12T07:54:13.000Z",
            "title": "Direct Preference Optimization Using Sparse Feature-Level Constraints",
            "summary": "The alignment of large language models (LLMs) with human preferences remains\na key challenge. While post-training techniques like Reinforcement Learning\nfrom Human Feedback (RLHF) and Direct Preference Optimization (DPO) have\nachieved notable success, they often introduce computational inefficiencies and\ntraining instability. In this paper, we propose Feature-level constrained\nPreference Optimization (FPO), a novel method designed to simplify the\nalignment process while ensuring stability. FPO leverages pre-trained Sparse\nAutoencoders (SAEs) and introduces feature-level constraints, allowing for\nefficient, sparsity-enforced alignment. Our approach enjoys efficiency by using\nsparse features activated in a well-trained sparse autoencoder and the quality\nof sequential KL divergence by using the feature-level offline reference.\nExperimental results on benchmark datasets demonstrate that FPO achieves a\n5.08% absolute improvement in win rate with much lower computational cost\ncompared to state-of-the-art baselines, making it a promising solution for\nefficient and controllable LLM alignments.",
            "upvotes": 9,
            "discussionId": "673575efe12f354dfc1ea740"
        },
        "publishedAt": "2024-11-14T02:31:38.090Z",
        "title": "Direct Preference Optimization Using Sparse Feature-Level Constraints",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.07618.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "/avatars/73cc9e6db6db86793787750776b57c63.svg",
            "fullname": "Linyi Yang",
            "name": "linyiyang2023",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2411.08868",
            "authors": [
                {
                    "_id": "6735b1a8bee2d791dc7f9367",
                    "name": "Wissam Antoun",
                    "hidden": false
                },
                {
                    "_id": "6735b1a8bee2d791dc7f9368",
                    "name": "Francis Kulumba",
                    "hidden": false
                },
                {
                    "_id": "6735b1a8bee2d791dc7f9369",
                    "name": "Rian Touchent",
                    "hidden": false
                },
                {
                    "_id": "6735b1a8bee2d791dc7f936a",
                    "name": "Éric de la Clergerie",
                    "hidden": false
                },
                {
                    "_id": "6735b1a8bee2d791dc7f936b",
                    "name": "Benoît Sagot",
                    "hidden": false
                },
                {
                    "_id": "6735b1a8bee2d791dc7f936c",
                    "name": "Djamé Seddah",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-13T18:49:35.000Z",
            "title": "CamemBERT 2.0: A Smarter French Language Model Aged to Perfection",
            "summary": "French language models, such as CamemBERT, have been widely adopted across\nindustries for natural language processing (NLP) tasks, with models like\nCamemBERT seeing over 4 million downloads per month. However, these models face\nchallenges due to temporal concept drift, where outdated training data leads to\na decline in performance, especially when encountering new topics and\nterminology. This issue emphasizes the need for updated models that reflect\ncurrent linguistic trends. In this paper, we introduce two new versions of the\nCamemBERT base model-CamemBERTav2 and CamemBERTv2-designed to address these\nchallenges. CamemBERTav2 is based on the DeBERTaV3 architecture and makes use\nof the Replaced Token Detection (RTD) objective for better contextual\nunderstanding, while CamemBERTv2 is built on RoBERTa, which uses the Masked\nLanguage Modeling (MLM) objective. Both models are trained on a significantly\nlarger and more recent dataset with longer context length and an updated\ntokenizer that enhances tokenization performance for French. We evaluate the\nperformance of these models on both general-domain NLP tasks and\ndomain-specific applications, such as medical field tasks, demonstrating their\nversatility and effectiveness across a range of use cases. Our results show\nthat these updated models vastly outperform their predecessors, making them\nvaluable tools for modern NLP systems. All our new models, as well as\nintermediate checkpoints, are made openly available on Huggingface.",
            "upvotes": 8,
            "discussionId": "6735b1a8bee2d791dc7f93ae"
        },
        "publishedAt": "2024-11-14T06:49:03.423Z",
        "title": "CamemBERT 2.0: A Smarter French Language Model Aged to Perfection",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.08868.png",
        "numComments": 3,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1584020801691-noauth.jpeg",
            "fullname": "Stefan Schweter",
            "name": "stefan-it",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isMod": false,
            "followerCount": 1836
        }
    },
    {
        "paper": {
            "id": "2411.08790",
            "authors": [
                {
                    "_id": "6735c8b06d4b6c741016a221",
                    "name": "Harry Mayne",
                    "hidden": false
                },
                {
                    "_id": "6735c8b06d4b6c741016a222",
                    "name": "Yushi Yang",
                    "hidden": false
                },
                {
                    "_id": "6735c8b06d4b6c741016a223",
                    "name": "Adam Mahdi",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-13T17:16:48.000Z",
            "title": "Can sparse autoencoders be used to decompose and interpret steering\n  vectors?",
            "summary": "Steering vectors are a promising approach to control the behaviour of large\nlanguage models. However, their underlying mechanisms remain poorly understood.\nWhile sparse autoencoders (SAEs) may offer a potential method to interpret\nsteering vectors, recent findings show that SAE-reconstructed vectors often\nlack the steering properties of the original vectors. This paper investigates\nwhy directly applying SAEs to steering vectors yields misleading\ndecompositions, identifying two reasons: (1) steering vectors fall outside the\ninput distribution for which SAEs are designed, and (2) steering vectors can\nhave meaningful negative projections in feature directions, which SAEs are not\ndesigned to accommodate. These limitations hinder the direct use of SAEs for\ninterpreting steering vectors.",
            "upvotes": 4,
            "discussionId": "6735c8b16d4b6c741016a24d"
        },
        "publishedAt": "2024-11-14T08:24:21.876Z",
        "title": "Can sparse autoencoders be used to decompose and interpret steering vectors?",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.08790.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/aaaff49b6abb5f9351159006b7755d25.svg",
            "fullname": "YY",
            "name": "yy0514",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2411.08307",
            "authors": [
                {
                    "_id": "6736129a7506985f27806752",
                    "name": "Yungang Yi",
                    "hidden": false
                },
                {
                    "_id": "6736129a7506985f27806753",
                    "name": "Weihua Li",
                    "hidden": false
                },
                {
                    "_id": "6736129a7506985f27806754",
                    "name": "Matthew Kuo",
                    "hidden": false
                },
                {
                    "_id": "6736129a7506985f27806755",
                    "name": "Quan Bai",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-13T03:14:10.000Z",
            "title": "PerceiverS: A Multi-Scale Perceiver with Effective Segmentation for\n  Long-Term Expressive Symbolic Music Generation",
            "summary": "Music generation has progressed significantly, especially in the domain of\naudio generation. However, generating symbolic music that is both\nlong-structured and expressive remains a significant challenge. In this paper,\nwe propose PerceiverS (Segmentation and Scale), a novel architecture designed\nto address this issue by leveraging both Effective Segmentation and Multi-Scale\nattention mechanisms. Our approach enhances symbolic music generation by\nsimultaneously learning long-term structural dependencies and short-term\nexpressive details. By combining cross-attention and self-attention in a\nMulti-Scale setting, PerceiverS captures long-range musical structure while\npreserving performance nuances. The proposed model, evaluated on datasets like\nMaestro, demonstrates improvements in generating coherent and diverse music\nwith both structural consistency and expressive variation. The project demos\nand the generated music samples can be accessed through the link:\nhttps://perceivers.github.io.",
            "upvotes": 0,
            "discussionId": "6736129b7506985f27806792"
        },
        "publishedAt": "2024-11-14T13:39:25.019Z",
        "title": "PerceiverS: A Multi-Scale Perceiver with Effective Segmentation for Long-Term Expressive Symbolic Music Generation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.08307.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 5127
        }
    },
    {
        "paper": {
            "id": "2411.08328",
            "authors": [
                {
                    "_id": "6736121f12b0cbbb1232b84f",
                    "name": "Qiang Zhou",
                    "hidden": false
                },
                {
                    "_id": "6736121f12b0cbbb1232b850",
                    "name": "Shaofeng Zhang",
                    "hidden": false
                },
                {
                    "_id": "6736121f12b0cbbb1232b851",
                    "name": "Nianzu Yang",
                    "hidden": false
                },
                {
                    "_id": "6736121f12b0cbbb1232b852",
                    "name": "Ye Qian",
                    "hidden": false
                },
                {
                    "_id": "6736121f12b0cbbb1232b853",
                    "name": "Hao Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-13T04:20:45.000Z",
            "title": "Motion Control for Enhanced Complex Action Video Generation",
            "summary": "Existing text-to-video (T2V) models often struggle with generating videos\nwith sufficiently pronounced or complex actions. A key limitation lies in the\ntext prompt's inability to precisely convey intricate motion details. To\naddress this, we propose a novel framework, MVideo, designed to produce\nlong-duration videos with precise, fluid actions. MVideo overcomes the\nlimitations of text prompts by incorporating mask sequences as an additional\nmotion condition input, providing a clearer, more accurate representation of\nintended actions. Leveraging foundational vision models such as GroundingDINO\nand SAM2, MVideo automatically generates mask sequences, enhancing both\nefficiency and robustness. Our results demonstrate that, after training, MVideo\neffectively aligns text prompts with motion conditions to produce videos that\nsimultaneously meet both criteria. This dual control mechanism allows for more\ndynamic video generation by enabling alterations to either the text prompt or\nmotion condition independently, or both in tandem. Furthermore, MVideo supports\nmotion condition editing and composition, facilitating the generation of videos\nwith more complex actions. MVideo thus advances T2V motion generation, setting\na strong benchmark for improved action depiction in current video diffusion\nmodels. Our project page is available at https://mvideo-v1.github.io/.",
            "upvotes": 0,
            "discussionId": "6736122512b0cbbb1232bacb"
        },
        "publishedAt": "2024-11-14T13:37:26.637Z",
        "title": "Motion Control for Enhanced Complex Action Video Generation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.08328.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 5127
        }
    }
]