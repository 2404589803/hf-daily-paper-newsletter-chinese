[
    "{'paper': {'id': '2412.14922', 'authors': [{'_id': '676a2354463437b5e1217e15', 'name': 'Junyu Luo', 'hidden': False}, {'_id': '676a2354463437b5e1217e16', 'name': 'Xiao Luo', 'hidden': False}, {'_id': '676a2354463437b5e1217e17', 'user': {'_id': '665e2f9301ca1c80a0a311d2', 'avatarUrl': '/avatars/67c88b55b580e6db74df4d0091197cea.svg', 'isPro': False, 'fullname': 'Kaize Ding', 'user': 'kaize0409', 'type': 'user'}, 'name': 'Kaize Ding', 'status': 'extracted_pending', 'statusLastChangedAt': '2024-12-24T02:58:28.896Z', 'hidden': False}, {'_id': '676a2354463437b5e1217e18', 'name': 'Jingyang Yuan', 'hidden': False}, {'_id': '676a2354463437b5e1217e19', 'name': 'Zhiping Xiao', 'hidden': False}, {'_id': '676a2354463437b5e1217e1a', 'name': 'Ming Zhang', 'hidden': False}], 'publishedAt': '2024-12-19T15:00:18.000Z', 'title': 'RobustFT: Robust Supervised Fine-tuning for Large Language Models under\\n  Noisy Response', 'summary': \"Supervised fine-tuning (SFT) plays a crucial role in adapting large language\\nmodels (LLMs) to specific domains or tasks. However, as demonstrated by\\nempirical experiments, the collected data inevitably contains noise in\\npractical applications, which poses significant challenges to model performance\\non downstream tasks. Therefore, there is an urgent need for a noise-robust SFT\\nframework to enhance model capabilities in downstream tasks. To address this\\nchallenge, we introduce a robust SFT framework (RobustFT) that performs noise\\ndetection and relabeling on downstream task data. For noise identification, our\\napproach employs a multi-expert collaborative system with inference-enhanced\\nmodels to achieve superior noise detection. In the denoising phase, we utilize\\na context-enhanced strategy, which incorporates the most relevant and confident\\nknowledge followed by careful assessment to generate reliable annotations.\\nAdditionally, we introduce an effective data selection mechanism based on\\nresponse entropy, ensuring only high-quality samples are retained for\\nfine-tuning. Extensive experiments conducted on multiple LLMs across five\\ndatasets demonstrate RobustFT's exceptional performance in noisy scenarios.\", 'upvotes': 46, 'discussionId': '676a2354463437b5e1217e51'}, 'publishedAt': '2024-12-23T22:03:04.208Z', 'title': 'RobustFT: Robust Supervised Fine-tuning for Large Language Models under Noisy Response', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.14922.png', 'numComments': 1, 'submittedBy': {'_id': '642da1cd99f3110ac27caca5', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/642da1cd99f3110ac27caca5/C1QJY3R_ZdaeANG1y8iW7.jpeg', 'fullname': 'junyu', 'name': 'luojunyu', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 4}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2412.17256', 'authors': [{'_id': '676a23c19fc612bf4a3b93f6', 'name': 'Weihao Zeng', 'hidden': False}, {'_id': '676a23c19fc612bf4a3b93f7', 'name': 'Yuzhen Huang', 'hidden': False}, {'_id': '676a23c19fc612bf4a3b93f8', 'name': 'Lulu Zhao', 'hidden': False}, {'_id': '676a23c19fc612bf4a3b93f9', 'name': 'Yijun Wang', 'hidden': False}, {'_id': '676a23c19fc612bf4a3b93fa', 'name': 'Zifei Shan', 'hidden': False}, {'_id': '676a23c19fc612bf4a3b93fb', 'name': 'Junxian He', 'hidden': False}], 'publishedAt': '2024-12-23T03:58:34.000Z', 'title': 'B-STaR: Monitoring and Balancing Exploration and Exploitation in\\n  Self-Taught Reasoners', 'summary': \"In the absence of extensive human-annotated data for complex reasoning tasks,\\nself-improvement -- where models are trained on their own outputs -- has\\nemerged as a primary method for enhancing performance. However, the critical\\nfactors underlying the mechanism of these iterative self-improving methods\\nremain poorly understood, such as under what conditions self-improvement is\\neffective, and what are the bottlenecks in the current iterations. In this\\nwork, we identify and propose methods to monitor two pivotal factors in this\\niterative process: (1) the model's ability to generate sufficiently diverse\\nresponses (exploration); and (2) the effectiveness of external rewards in\\ndistinguishing high-quality candidates from lower-quality ones (exploitation).\\nUsing mathematical reasoning as a case study, we begin with a quantitative\\nanalysis to track the dynamics of exploration and exploitation, discovering\\nthat a model's exploratory capabilities rapidly deteriorate over iterations,\\nand the effectiveness of exploiting external rewards diminishes as well.\\nMotivated by these findings, we introduce B-STaR, a Self-Taught Reasoning\\nframework that autonomously adjusts configurations across iterations to Balance\\nexploration and exploitation, thereby optimizing the self-improving\\neffectiveness based on the current policy model and available rewards. Our\\nexperiments on mathematical reasoning, coding, and commonsense reasoning\\ndemonstrate that B-STaR not only enhances the model's exploratory capabilities\\nthroughout training but also achieves a more effective balance between\\nexploration and exploitation, leading to superior performance.\", 'upvotes': 28, 'discussionId': '676a23c29fc612bf4a3b943b'}, 'publishedAt': '2024-12-23T22:04:28.157Z', 'title': 'B-STaR: Monitoring and Balancing Exploration and Exploitation in Self-Taught Reasoners', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.17256.png', 'numComments': 1, 'submittedBy': {'_id': '62751082b43ccfeef483424f', 'avatarUrl': '/avatars/fec83e4478e7d1731ba6033328131852.svg', 'fullname': 'WeihaoZeng', 'name': 'AndrewZeng', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 2}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2412.17451', 'authors': [{'_id': '676a25e38ffab02f2c91a99e', 'name': 'Wei Liu', 'hidden': False}, {'_id': '676a25e38ffab02f2c91a99f', 'name': 'Junlong Li', 'hidden': False}, {'_id': '676a25e38ffab02f2c91a9a0', 'name': 'Xiwen Zhang', 'hidden': False}, {'_id': '676a25e38ffab02f2c91a9a1', 'name': 'Fan Zhou', 'hidden': False}, {'_id': '676a25e38ffab02f2c91a9a2', 'name': 'Yu Cheng', 'hidden': False}, {'_id': '676a25e38ffab02f2c91a9a3', 'name': 'Junxian He', 'hidden': False}], 'publishedAt': '2024-12-23T10:18:41.000Z', 'title': 'Diving into Self-Evolving Training for Multimodal Reasoning', 'summary': \"Reasoning ability is essential for Large Multimodal Models (LMMs). In the\\nabsence of multimodal chain-of-thought annotated data, self-evolving training,\\nwhere the model learns from its own outputs, has emerged as an effective and\\nscalable approach for enhancing reasoning abilities. Despite its growing usage,\\na comprehensive understanding of self-evolving training, particularly in the\\ncontext of multimodal reasoning, remains limited. In this paper, we delve into\\nthe intricacies of self-evolving training for multimodal reasoning, pinpointing\\nthree key factors: Training Method, Reward Model, and Prompt Variation. We\\nsystematically examine each factor and explore how various configurations\\naffect the training's effectiveness. Our analysis leads to a set of best\\npractices for each factor, aimed at optimizing multimodal reasoning.\\nFurthermore, we explore the Self-Evolution Dynamics during training and the\\nimpact of automatic balancing mechanisms in boosting performance. After all the\\ninvestigations, we present a final recipe for self-evolving training in\\nmultimodal reasoning, encapsulating these design choices into a framework we\\ncall MSTaR (Multimodal Self-evolving Training for Reasoning), which is\\nuniversally effective for models with different sizes on various benchmarks,\\ne.g., surpassing the pre-evolved model significantly on 5 multimodal reasoning\\nbenchmarks without using additional human annotations, as demonstrated on\\nMiniCPM-V-2.5 (8B), Phi-3.5-Vision (4B) and InternVL2 (2B). We believe this\\nstudy fills a significant gap in the understanding of self-evolving training\\nfor multimodal reasoning and offers a robust framework for future research. Our\\npolicy and reward models, as well as the collected data, is released to\\nfacilitate further investigation in multimodal reasoning.\", 'upvotes': 22, 'discussionId': '676a25e48ffab02f2c91a9e3'}, 'publishedAt': '2024-12-23T22:09:53.346Z', 'title': 'Diving into Self-Evolving Training for Multimodal Reasoning', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.17451.png', 'numComments': 1, 'submittedBy': {'_id': '6458af46f4d212d780bd7c68', 'avatarUrl': '/avatars/832fd34bcc041b0b7b551873a459fc3c.svg', 'fullname': 'Wei Liu', 'name': 'PeterV09', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 6}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2412.17153', 'authors': [{'_id': '676a3070b1618113354d99fa', 'name': 'Enshu Liu', 'hidden': False}, {'_id': '676a3070b1618113354d99fb', 'name': 'Xuefei Ning', 'hidden': False}, {'_id': '676a3070b1618113354d99fc', 'name': 'Yu Wang', 'hidden': False}, {'_id': '676a3070b1618113354d99fd', 'user': {'_id': '64c832a8c547ed5243d29630', 'avatarUrl': '/avatars/59d1975634e84095b69423c02441d453.svg', 'isPro': False, 'fullname': 'Zinan Lin', 'user': 'fjxmlzn', 'type': 'user'}, 'name': 'Zinan Lin', 'status': 'extracted_confirmed', 'statusLastChangedAt': '2024-12-24T03:54:58.649Z', 'hidden': False}], 'publishedAt': '2024-12-22T20:21:54.000Z', 'title': 'Distilled Decoding 1: One-step Sampling of Image Auto-regressive Models\\n  with Flow Matching', 'summary': \"Autoregressive (AR) models have achieved state-of-the-art performance in text\\nand image generation but suffer from slow generation due to the token-by-token\\nprocess. We ask an ambitious question: can a pre-trained AR model be adapted to\\ngenerate outputs in just one or two steps? If successful, this would\\nsignificantly advance the development and deployment of AR models. We notice\\nthat existing works that try to speed up AR generation by generating multiple\\ntokens at once fundamentally cannot capture the output distribution due to the\\nconditional dependencies between tokens, limiting their effectiveness for\\nfew-step generation. To address this, we propose Distilled Decoding (DD), which\\nuses flow matching to create a deterministic mapping from Gaussian distribution\\nto the output distribution of the pre-trained AR model. We then train a network\\nto distill this mapping, enabling few-step generation. DD doesn't need the\\ntraining data of the original AR model, making it more practical.We evaluate DD\\non state-of-the-art image AR models and present promising results on\\nImageNet-256. For VAR, which requires 10-step generation, DD enables one-step\\ngeneration (6.3times speed-up), with an acceptable increase in FID from 4.19\\nto 9.96. For LlamaGen, DD reduces generation from 256 steps to 1, achieving an\\n217.8times speed-up with a comparable FID increase from 4.11 to 11.35. In\\nboth cases, baseline methods completely fail with FID>100. DD also excels on\\ntext-to-image generation, reducing the generation from 256 steps to 2 for\\nLlamaGen with minimal FID increase from 25.70 to 28.95. As the first work to\\ndemonstrate the possibility of one-step generation for image AR models, DD\\nchallenges the prevailing notion that AR models are inherently slow, and opens\\nup new opportunities for efficient AR generation. The project website is at\\nhttps://imagination-research.github.io/distilled-decoding.\", 'upvotes': 17, 'discussionId': '676a3072b1618113354d9aa1'}, 'publishedAt': '2024-12-23T23:23:30.988Z', 'title': 'Distilled Decoding 1: One-step Sampling of Image Auto-regressive Models with Flow Matching', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.17153.png', 'numComments': 1, 'submittedBy': {'_id': '64c832a8c547ed5243d29630', 'avatarUrl': '/avatars/59d1975634e84095b69423c02441d453.svg', 'fullname': 'Zinan Lin', 'name': 'fjxmlzn', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 2}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2412.17747', 'authors': [{'_id': '676a3ec0b91a321e164a8780', 'name': 'Luyang Liu', 'hidden': False}, {'_id': '676a3ec0b91a321e164a8781', 'name': 'Jonas Pfeiffer', 'hidden': False}, {'_id': '676a3ec0b91a321e164a8782', 'name': 'Jiaxing Wu', 'hidden': False}, {'_id': '676a3ec0b91a321e164a8783', 'name': 'Jun Xie', 'hidden': False}, {'_id': '676a3ec0b91a321e164a8784', 'name': 'Arthur Szlam', 'hidden': False}], 'publishedAt': '2024-12-23T18:02:25.000Z', 'title': 'Deliberation in Latent Space via Differentiable Cache Augmentation', 'summary': 'Techniques enabling large language models (LLMs) to \"think more\" by\\ngenerating and attending to intermediate reasoning steps have shown promise in\\nsolving complex problems. However, the standard approaches generate sequences\\nof discrete tokens immediately before responding, and so they can incur\\nsignificant latency costs and be challenging to optimize. In this work, we\\ndemonstrate that a frozen LLM can be augmented with an offline coprocessor that\\noperates on the model\\'s key-value (kv) cache. This coprocessor augments the\\ncache with a set of latent embeddings designed to improve the fidelity of\\nsubsequent decoding. We train this coprocessor using the language modeling loss\\nfrom the decoder on standard pretraining data, while keeping the decoder itself\\nfrozen. This approach enables the model to learn, in an end-to-end\\ndifferentiable fashion, how to distill additional computation into its\\nkv-cache. Because the decoder remains unchanged, the coprocessor can operate\\noffline and asynchronously, and the language model can function normally if the\\ncoprocessor is unavailable or if a given cache is deemed not to require extra\\ncomputation. We show experimentally that when a cache is augmented, the decoder\\nachieves lower perplexity on numerous subsequent tokens. Furthermore, even\\nwithout any task-specific training, our experiments demonstrate that cache\\naugmentation consistently reduces perplexity and improves performance across a\\nrange of reasoning-intensive tasks.', 'upvotes': 14, 'discussionId': '676a3ec1b91a321e164a87ca'}, 'publishedAt': '2024-12-24T00:05:04.046Z', 'title': 'Deliberation in Latent Space via Differentiable Cache Augmentation', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.17747.png', 'numComments': 2, 'submittedBy': {'_id': '65d8fa4e16cebdd689fa5587', 'avatarUrl': '/avatars/9646f22ae16653af47ea38af0839ae7b.svg', 'fullname': 'Luyang Liu', 'name': 'luyangl', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2412.17805', 'authors': [{'_id': '676a5396207235e4b020dfda', 'name': 'Yazhou Xing', 'hidden': False}, {'_id': '676a5396207235e4b020dfdb', 'name': 'Yang Fei', 'hidden': False}, {'_id': '676a5396207235e4b020dfdc', 'name': 'Yingqing He', 'hidden': False}, {'_id': '676a5396207235e4b020dfdd', 'name': 'Jingye Chen', 'hidden': False}, {'_id': '676a5396207235e4b020dfde', 'name': 'Jiaxin Xie', 'hidden': False}, {'_id': '676a5396207235e4b020dfdf', 'name': 'Xiaowei Chi', 'hidden': False}, {'_id': '676a5396207235e4b020dfe0', 'name': 'Qifeng Chen', 'hidden': False}], 'publishedAt': '2024-12-23T18:58:24.000Z', 'title': 'Large Motion Video Autoencoding with Cross-modal Video VAE', 'summary': 'Learning a robust video Variational Autoencoder (VAE) is essential for\\nreducing video redundancy and facilitating efficient video generation. Directly\\napplying image VAEs to individual frames in isolation can result in temporal\\ninconsistencies and suboptimal compression rates due to a lack of temporal\\ncompression. Existing Video VAEs have begun to address temporal compression;\\nhowever, they often suffer from inadequate reconstruction performance. In this\\npaper, we present a novel and powerful video autoencoder capable of\\nhigh-fidelity video encoding. First, we observe that entangling spatial and\\ntemporal compression by merely extending the image VAE to a 3D VAE can\\nintroduce motion blur and detail distortion artifacts. Thus, we propose\\ntemporal-aware spatial compression to better encode and decode the spatial\\ninformation. Additionally, we integrate a lightweight motion compression model\\nfor further temporal compression. Second, we propose to leverage the textual\\ninformation inherent in text-to-video datasets and incorporate text guidance\\ninto our model. This significantly enhances reconstruction quality,\\nparticularly in terms of detail preservation and temporal stability. Third, we\\nfurther improve the versatility of our model through joint training on both\\nimages and videos, which not only enhances reconstruction quality but also\\nenables the model to perform both image and video autoencoding. Extensive\\nevaluations against strong recent baselines demonstrate the superior\\nperformance of our method. The project website can be found\\nat~https://yzxing87.github.io/vae/{https://yzxing87.github.io/vae/}.', 'upvotes': 13, 'discussionId': '676a5398207235e4b020e097'}, 'publishedAt': '2024-12-24T02:03:44.880Z', 'title': 'Large Motion Video Autoencoding with Cross-modal Video VAE', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/630231bf7e137e3d6b3b0645/TYdqz6KUOeiRZ2ZAYaiyq.mp4'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.17805.png', 'numComments': 2, 'submittedBy': {'_id': '630231bf7e137e3d6b3b0645', 'avatarUrl': '/avatars/dad52f8955110f0a2caeb613d6aa3ea2.svg', 'fullname': 'He', 'name': 'Yingqing', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 3}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2412.15118', 'authors': [{'_id': '676a444d9315022860ac1f0e', 'name': 'Zhuohao Yu', 'hidden': False}, {'_id': '676a444d9315022860ac1f0f', 'name': 'Weizheng Gu', 'hidden': False}, {'_id': '676a444d9315022860ac1f10', 'name': 'Yidong Wang', 'hidden': False}, {'_id': '676a444d9315022860ac1f11', 'name': 'Zhengran Zeng', 'hidden': False}, {'_id': '676a444d9315022860ac1f12', 'name': 'Jindong Wang', 'hidden': False}, {'_id': '676a444d9315022860ac1f13', 'name': 'Wei Ye', 'hidden': False}, {'_id': '676a444d9315022860ac1f14', 'name': 'Shikun Zhang', 'hidden': False}], 'publishedAt': '2024-12-19T17:59:42.000Z', 'title': 'Outcome-Refining Process Supervision for Code Generation', 'summary': 'Large Language Models have demonstrated remarkable capabilities in code\\ngeneration, yet they often struggle with complex programming tasks that require\\ndeep algorithmic reasoning. While process supervision through learned reward\\nmodels shows promise in guiding reasoning steps, it requires expensive training\\ndata and suffers from unreliable evaluation. We propose Outcome-Refining\\nProcess Supervision, a novel paradigm that treats outcome refinement itself as\\nthe process to be supervised. Our framework leverages concrete execution\\nsignals to ground the supervision of reasoning steps, while using\\ntree-structured exploration to maintain multiple solution trajectories\\nsimultaneously. Experiments demonstrate that our approach enables even smaller\\nmodels to achieve high success accuracy and performance metrics on competitive\\nprogramming tasks, creates more reliable verification than traditional reward\\nmodels without requiring training PRMs. Our approach achieves significant\\nimprovements across 5 models and 3 datasets: an average of 26.9% increase in\\ncorrectness and 42.2% in efficiency. The results suggest that providing\\nstructured reasoning space with concrete verification signals is crucial for\\nsolving complex programming tasks. We open-source all our code and data at:\\nhttps://github.com/zhuohaoyu/ORPS', 'upvotes': 11, 'discussionId': '676a444f9315022860ac1f70'}, 'publishedAt': '2024-12-24T00:41:11.109Z', 'title': 'Outcome-Refining Process Supervision for Code Generation', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.15118.png', 'numComments': 1, 'submittedBy': {'_id': '61e24808b31e7cc38eb84d37', 'avatarUrl': '/avatars/65fbea940fad211462ecc5ad725e0c28.svg', 'fullname': 'Zhuohao Yu', 'name': 'zhuohaoyu', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2412.16926', 'authors': [{'_id': '676a211eb16181133547681b', 'name': 'Jinheon Baek', 'hidden': False}, {'_id': '676a211eb16181133547681c', 'name': 'Sun Jae Lee', 'hidden': False}, {'_id': '676a211eb16181133547681d', 'name': 'Prakhar Gupta', 'hidden': False}, {'_id': '676a211eb16181133547681e', 'name': 'Geunseob', 'hidden': False}, {'_id': '676a211eb16181133547681f', 'name': 'Oh', 'hidden': False}, {'_id': '676a211eb161811335476820', 'name': 'Siddharth Dalmia', 'hidden': False}, {'_id': '676a211eb161811335476821', 'name': 'Prateek Kolhar', 'hidden': False}], 'publishedAt': '2024-12-22T08:55:19.000Z', 'title': 'Revisiting In-Context Learning with Long Context Language Models', 'summary': 'In-Context Learning (ICL) is a technique by which language models make\\npredictions based on examples provided in their input context. Previously,\\ntheir context window size imposed a limit on the number of examples that can be\\nshown, making example selection techniques crucial for identifying the\\nmaximally effective set of examples. However, the recent advent of Long Context\\nLanguage Models (LCLMs) has significantly increased the number of examples that\\ncan be included in context, raising an important question of whether ICL\\nperformance in a many-shot regime is still sensitive to the method of sample\\nselection. To answer this, we revisit these approaches in the context of LCLMs\\nthrough extensive experiments on 18 datasets spanning 4 tasks. Surprisingly, we\\nobserve that sophisticated example selection techniques do not yield\\nsignificant improvements over a simple random sample selection method. Instead,\\nwe find that the advent of LCLMs has fundamentally shifted the challenge of ICL\\nfrom that of selecting the most effective examples to that of collecting\\nsufficient examples to fill the context window. Specifically, in certain\\ndatasets, including all available examples does not fully utilize the context\\nwindow; however, by augmenting the examples in context with a simple data\\naugmentation approach, we substantially improve ICL performance by 5%.', 'upvotes': 11, 'discussionId': '676a211fb161811335476846'}, 'publishedAt': '2024-12-23T21:55:42.534Z', 'title': 'Revisiting In-Context Learning with Long Context Language Models', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.16926.png', 'numComments': 1, 'submittedBy': {'_id': '63036b6c5c70c21d0ea79d48', 'avatarUrl': '/avatars/a7eb03f5cbd4eaa09fe807bbed8bc0f7.svg', 'fullname': 'Jinheon Baek', 'name': 'jinheon', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2412.16429', 'authors': [{'_id': '676a61bda89cd26e3da31478', 'name': 'LearnLM Team', 'hidden': False}, {'_id': '676a61bda89cd26e3da31479', 'name': 'Abhinit Modi', 'hidden': False}, {'_id': '676a61bda89cd26e3da3147a', 'name': 'Aditya Srikanth Veerubhotla', 'hidden': False}, {'_id': '676a61bda89cd26e3da3147b', 'name': 'Aliya Rysbek', 'hidden': False}, {'_id': '676a61bda89cd26e3da3147c', 'name': 'Andrea Huber', 'hidden': False}, {'_id': '676a61bda89cd26e3da3147d', 'name': 'Brett Wiltshire', 'hidden': False}, {'_id': '676a61bda89cd26e3da3147e', 'name': 'Brian Veprek', 'hidden': False}, {'_id': '676a61bda89cd26e3da3147f', 'name': 'Daniel Gillick', 'hidden': False}, {'_id': '676a61bda89cd26e3da31480', 'name': 'Daniel Kasenberg', 'hidden': False}, {'_id': '676a61bda89cd26e3da31481', 'name': 'Derek Ahmed', 'hidden': False}, {'_id': '676a61bda89cd26e3da31482', 'name': 'Irina Jurenka', 'hidden': False}, {'_id': '676a61bda89cd26e3da31483', 'name': 'James Cohan', 'hidden': False}, {'_id': '676a61bda89cd26e3da31484', 'name': 'Jennifer She', 'hidden': False}, {'_id': '676a61bda89cd26e3da31485', 'name': 'Julia Wilkowski', 'hidden': False}, {'_id': '676a61bda89cd26e3da31486', 'name': 'Kaiz Alarakyia', 'hidden': False}, {'_id': '676a61bda89cd26e3da31487', 'name': 'Kevin McKee', 'hidden': False}, {'_id': '676a61bda89cd26e3da31488', 'name': 'Lisa Wang', 'hidden': False}, {'_id': '676a61bda89cd26e3da31489', 'name': 'Markus Kunesch', 'hidden': False}, {'_id': '676a61bda89cd26e3da3148a', 'name': 'Mike Schaekermann', 'hidden': False}, {'_id': '676a61bda89cd26e3da3148b', 'name': 'Miruna Pîslar', 'hidden': False}, {'_id': '676a61bda89cd26e3da3148c', 'name': 'Nikhil Joshi', 'hidden': False}, {'_id': '676a61bda89cd26e3da3148d', 'name': 'Parsa Mahmoudieh', 'hidden': False}, {'_id': '676a61bda89cd26e3da3148e', 'name': 'Paul Jhun', 'hidden': False}, {'_id': '676a61bda89cd26e3da3148f', 'name': 'Sara Wiltberger', 'hidden': False}, {'_id': '676a61bda89cd26e3da31490', 'name': 'Shakir Mohamed', 'hidden': False}, {'_id': '676a61bda89cd26e3da31491', 'name': 'Shashank Agarwal', 'hidden': False}, {'_id': '676a61bda89cd26e3da31492', 'name': 'Shubham Milind Phal', 'hidden': False}, {'_id': '676a61bda89cd26e3da31493', 'name': 'Sun Jae Lee', 'hidden': False}, {'_id': '676a61bda89cd26e3da31494', 'name': 'Theofilos Strinopoulos', 'hidden': False}, {'_id': '676a61bda89cd26e3da31495', 'name': 'Wei-Jen Ko', 'hidden': False}, {'_id': '676a61bda89cd26e3da31496', 'name': 'Amy Wang', 'hidden': False}, {'_id': '676a61bda89cd26e3da31497', 'name': 'Ankit Anand', 'hidden': False}, {'_id': '676a61bda89cd26e3da31498', 'name': 'Avishkar Bhoopchand', 'hidden': False}, {'_id': '676a61bda89cd26e3da31499', 'name': 'Dan Wild', 'hidden': False}, {'_id': '676a61bda89cd26e3da3149a', 'name': 'Divya Pandya', 'hidden': False}, {'_id': '676a61bda89cd26e3da3149b', 'name': 'Filip Bar', 'hidden': False}, {'_id': '676a61bda89cd26e3da3149c', 'name': 'Garth Graham', 'hidden': False}, {'_id': '676a61bda89cd26e3da3149d', 'name': 'Holger Winnemoeller', 'hidden': False}, {'_id': '676a61bda89cd26e3da3149e', 'name': 'Mahvish Nagda', 'hidden': False}, {'_id': '676a61bda89cd26e3da3149f', 'name': 'Prateek Kolhar', 'hidden': False}, {'_id': '676a61bda89cd26e3da314a0', 'name': 'Renee Schneider', 'hidden': False}, {'_id': '676a61bda89cd26e3da314a1', 'name': 'Shaojian Zhu', 'hidden': False}, {'_id': '676a61bda89cd26e3da314a2', 'name': 'Stephanie Chan', 'hidden': False}, {'_id': '676a61bda89cd26e3da314a3', 'name': 'Steve Yadlowsky', 'hidden': False}, {'_id': '676a61bda89cd26e3da314a4', 'name': 'Viknesh Sounderajah', 'hidden': False}, {'_id': '676a61bda89cd26e3da314a5', 'name': 'Yannis Assael', 'hidden': False}], 'publishedAt': '2024-12-21T01:34:05.000Z', 'title': 'LearnLM: Improving Gemini for Learning', 'summary': \"Today's generative AI systems are tuned to present information by default\\nrather than engage users in service of learning as a human tutor would. To\\naddress the wide range of potential education use cases for these systems, we\\nreframe the challenge of injecting pedagogical behavior as one of\\npedagogical instruction following, where training and evaluation\\nexamples include system-level instructions describing the specific pedagogy\\nattributes present or desired in subsequent model turns. This framing avoids\\ncommitting our models to any particular definition of pedagogy, and instead\\nallows teachers or developers to specify desired model behavior. It also clears\\na path to improving Gemini models for learning -- by enabling the addition of\\nour pedagogical data to post-training mixtures -- alongside their rapidly\\nexpanding set of capabilities. Both represent important changes from our\\ninitial tech report. We show how training with pedagogical instruction\\nfollowing produces a LearnLM model (available on Google AI Studio) that is\\npreferred substantially by expert raters across a diverse set of learning\\nscenarios, with average preference strengths of 31\\\\% over GPT-4o, 11\\\\% over\\nClaude 3.5, and 13\\\\% over the Gemini 1.5 Pro model LearnLM was based on.\", 'upvotes': 9, 'discussionId': '676a61bfa89cd26e3da3150e'}, 'publishedAt': '2024-12-24T02:25:15.795Z', 'title': 'LearnLM: Improving Gemini for Learning', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.16429.png', 'numComments': 1, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isMod': False, 'followerCount': 5451}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2412.16720', 'authors': [{'_id': '676a5f1127c8341daf37c83b', 'name': 'OpenAI', 'hidden': False}, {'_id': '676a5f1127c8341daf37c83d', 'name': 'Aaron Jaech', 'hidden': False}, {'_id': '676a5f1127c8341daf37c83e', 'name': 'Adam Kalai', 'hidden': False}, {'_id': '676a5f1127c8341daf37c83f', 'name': 'Adam Lerer', 'hidden': False}, {'_id': '676a5f1127c8341daf37c840', 'name': 'Adam Richardson', 'hidden': False}, {'_id': '676a5f1127c8341daf37c841', 'name': 'Ahmed El-Kishky', 'hidden': False}, {'_id': '676a5f1127c8341daf37c842', 'name': 'Aiden Low', 'hidden': False}, {'_id': '676a5f1127c8341daf37c843', 'name': 'Alec Helyar', 'hidden': False}, {'_id': '676a5f1127c8341daf37c844', 'name': 'Aleksander Madry', 'hidden': False}, {'_id': '676a5f1127c8341daf37c845', 'name': 'Alex Beutel', 'hidden': False}, {'_id': '676a5f1127c8341daf37c846', 'name': 'Alex Carney', 'hidden': False}, {'_id': '676a5f1127c8341daf37c847', 'name': 'Alex Iftimie', 'hidden': False}, {'_id': '676a5f1127c8341daf37c848', 'name': 'Alex Karpenko', 'hidden': False}, {'_id': '676a5f1127c8341daf37c849', 'name': 'Alex Tachard Passos', 'hidden': False}, {'_id': '676a5f1127c8341daf37c84a', 'name': 'Alexander Neitz', 'hidden': False}, {'_id': '676a5f1127c8341daf37c84b', 'name': 'Alexander Prokofiev', 'hidden': False}, {'_id': '676a5f1127c8341daf37c84c', 'name': 'Alexander Wei', 'hidden': False}, {'_id': '676a5f1127c8341daf37c84d', 'name': 'Allison Tam', 'hidden': False}, {'_id': '676a5f1127c8341daf37c84e', 'name': 'Ally Bennett', 'hidden': False}, {'_id': '676a5f1127c8341daf37c84f', 'name': 'Ananya Kumar', 'hidden': False}, {'_id': '676a5f1127c8341daf37c850', 'name': 'Andre Saraiva', 'hidden': False}, {'_id': '676a5f1127c8341daf37c851', 'name': 'Andrea Vallone', 'hidden': False}, {'_id': '676a5f1127c8341daf37c852', 'name': 'Andrew Duberstein', 'hidden': False}, {'_id': '676a5f1127c8341daf37c853', 'name': 'Andrew Kondrich', 'hidden': False}, {'_id': '676a5f1127c8341daf37c854', 'name': 'Andrey Mishchenko', 'hidden': False}, {'_id': '676a5f1127c8341daf37c855', 'name': 'Andy Applebaum', 'hidden': False}, {'_id': '676a5f1127c8341daf37c856', 'name': 'Angela Jiang', 'hidden': False}, {'_id': '676a5f1127c8341daf37c857', 'name': 'Ashvin Nair', 'hidden': False}, {'_id': '676a5f1127c8341daf37c858', 'name': 'Barret Zoph', 'hidden': False}, {'_id': '676a5f1127c8341daf37c859', 'name': 'Behrooz Ghorbani', 'hidden': False}, {'_id': '676a5f1127c8341daf37c85a', 'name': 'Ben Rossen', 'hidden': False}, {'_id': '676a5f1127c8341daf37c85b', 'name': 'Benjamin Sokolowsky', 'hidden': False}, {'_id': '676a5f1127c8341daf37c85c', 'name': 'Boaz Barak', 'hidden': False}, {'_id': '676a5f1127c8341daf37c85d', 'name': 'Bob McGrew', 'hidden': False}, {'_id': '676a5f1127c8341daf37c85e', 'name': 'Borys Minaiev', 'hidden': False}, {'_id': '676a5f1127c8341daf37c85f', 'name': 'Botao Hao', 'hidden': False}, {'_id': '676a5f1127c8341daf37c860', 'name': 'Bowen Baker', 'hidden': False}, {'_id': '676a5f1127c8341daf37c861', 'name': 'Brandon Houghton', 'hidden': False}, {'_id': '676a5f1127c8341daf37c862', 'name': 'Brandon McKinzie', 'hidden': False}, {'_id': '676a5f1127c8341daf37c863', 'name': 'Brydon Eastman', 'hidden': False}, {'_id': '676a5f1127c8341daf37c864', 'name': 'Camillo Lugaresi', 'hidden': False}, {'_id': '676a5f1127c8341daf37c865', 'name': 'Cary Bassin', 'hidden': False}, {'_id': '676a5f1127c8341daf37c866', 'name': 'Cary Hudson', 'hidden': False}, {'_id': '676a5f1127c8341daf37c867', 'name': 'Chak Ming Li', 'hidden': False}, {'_id': '676a5f1127c8341daf37c868', 'name': 'Charles de Bourcy', 'hidden': False}, {'_id': '676a5f1127c8341daf37c869', 'name': 'Chelsea Voss', 'hidden': False}, {'_id': '676a5f1127c8341daf37c86a', 'name': 'Chen Shen', 'hidden': False}, {'_id': '676a5f1127c8341daf37c86b', 'name': 'Chong Zhang', 'hidden': False}, {'_id': '676a5f1127c8341daf37c86c', 'name': 'Chris Koch', 'hidden': False}, {'_id': '676a5f1127c8341daf37c86d', 'name': 'Chris Orsinger', 'hidden': False}, {'_id': '676a5f1127c8341daf37c86e', 'name': 'Christopher Hesse', 'hidden': False}, {'_id': '676a5f1127c8341daf37c86f', 'name': 'Claudia Fischer', 'hidden': False}, {'_id': '676a5f1127c8341daf37c870', 'name': 'Clive Chan', 'hidden': False}, {'_id': '676a5f1127c8341daf37c871', 'name': 'Dan Roberts', 'hidden': False}, {'_id': '676a5f1127c8341daf37c872', 'name': 'Daniel Kappler', 'hidden': False}, {'_id': '676a5f1127c8341daf37c873', 'name': 'Daniel Levy', 'hidden': False}, {'_id': '676a5f1127c8341daf37c874', 'name': 'Daniel Selsam', 'hidden': False}, {'_id': '676a5f1127c8341daf37c875', 'name': 'David Dohan', 'hidden': False}, {'_id': '676a5f1127c8341daf37c876', 'name': 'David Farhi', 'hidden': False}, {'_id': '676a5f1127c8341daf37c877', 'name': 'David Mely', 'hidden': False}, {'_id': '676a5f1127c8341daf37c878', 'name': 'David Robinson', 'hidden': False}, {'_id': '676a5f1127c8341daf37c879', 'name': 'Dimitris Tsipras', 'hidden': False}, {'_id': '676a5f1127c8341daf37c87a', 'name': 'Doug Li', 'hidden': False}, {'_id': '676a5f1127c8341daf37c87b', 'name': 'Dragos Oprica', 'hidden': False}, {'_id': '676a5f1127c8341daf37c87c', 'name': 'Eben Freeman', 'hidden': False}, {'_id': '676a5f1127c8341daf37c87d', 'name': 'Eddie Zhang', 'hidden': False}, {'_id': '676a5f1127c8341daf37c87e', 'name': 'Edmund Wong', 'hidden': False}, {'_id': '676a5f1127c8341daf37c87f', 'name': 'Elizabeth Proehl', 'hidden': False}, {'_id': '676a5f1127c8341daf37c880', 'name': 'Enoch Cheung', 'hidden': False}, {'_id': '676a5f1127c8341daf37c881', 'name': 'Eric Mitchell', 'hidden': False}, {'_id': '676a5f1127c8341daf37c882', 'name': 'Eric Wallace', 'hidden': False}, {'_id': '676a5f1127c8341daf37c883', 'name': 'Erik Ritter', 'hidden': False}, {'_id': '676a5f1127c8341daf37c884', 'name': 'Evan Mays', 'hidden': False}, {'_id': '676a5f1127c8341daf37c885', 'name': 'Fan Wang', 'hidden': False}, {'_id': '676a5f1127c8341daf37c886', 'name': 'Felipe Petroski Such', 'hidden': False}, {'_id': '676a5f1127c8341daf37c887', 'name': 'Filippo Raso', 'hidden': False}, {'_id': '676a5f1127c8341daf37c888', 'name': 'Florencia Leoni', 'hidden': False}, {'_id': '676a5f1127c8341daf37c889', 'name': 'Foivos Tsimpourlas', 'hidden': False}, {'_id': '676a5f1127c8341daf37c88a', 'name': 'Francis Song', 'hidden': False}, {'_id': '676a5f1127c8341daf37c88b', 'name': 'Fred von Lohmann', 'hidden': False}, {'_id': '676a5f1127c8341daf37c88c', 'name': 'Freddie Sulit', 'hidden': False}, {'_id': '676a5f1127c8341daf37c88d', 'name': 'Geoff Salmon', 'hidden': False}, {'_id': '676a5f1127c8341daf37c88e', 'name': 'Giambattista Parascandolo', 'hidden': False}, {'_id': '676a5f1127c8341daf37c88f', 'name': 'Gildas Chabot', 'hidden': False}, {'_id': '676a5f1127c8341daf37c890', 'name': 'Grace Zhao', 'hidden': False}, {'_id': '676a5f1127c8341daf37c891', 'name': 'Greg Brockman', 'hidden': False}, {'_id': '676a5f1127c8341daf37c892', 'name': 'Guillaume Leclerc', 'hidden': False}, {'_id': '676a5f1127c8341daf37c893', 'name': 'Hadi Salman', 'hidden': False}, {'_id': '676a5f1127c8341daf37c894', 'name': 'Haiming Bao', 'hidden': False}, {'_id': '676a5f1127c8341daf37c895', 'name': 'Hao Sheng', 'hidden': False}, {'_id': '676a5f1127c8341daf37c896', 'name': 'Hart Andrin', 'hidden': False}, {'_id': '676a5f1127c8341daf37c897', 'name': 'Hessam Bagherinezhad', 'hidden': False}, {'_id': '676a5f1127c8341daf37c898', 'name': 'Hongyu Ren', 'hidden': False}, {'_id': '676a5f1127c8341daf37c899', 'name': 'Hunter Lightman', 'hidden': False}, {'_id': '676a5f1127c8341daf37c89a', 'name': 'Hyung Won Chung', 'hidden': False}, {'_id': '676a5f1127c8341daf37c89b', 'name': 'Ian Kivlichan', 'hidden': False}, {'_id': '676a5f1127c8341daf37c89c', 'name': \"Ian O'Connell\", 'hidden': False}, {'_id': '676a5f1127c8341daf37c89d', 'name': 'Ian Osband', 'hidden': False}, {'_id': '676a5f1127c8341daf37c89e', 'name': 'Ignasi Clavera Gilaberte', 'hidden': False}, {'_id': '676a5f1127c8341daf37c89f', 'name': 'Ilge Akkaya', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8a0', 'name': 'Ilya Kostrikov', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8a1', 'name': 'Ilya Sutskever', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8a2', 'name': 'Irina Kofman', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8a3', 'name': 'Jakub Pachocki', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8a4', 'name': 'James Lennon', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8a5', 'name': 'Jason Wei', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8a6', 'name': 'Jean Harb', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8a7', 'name': 'Jerry Twore', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8a8', 'name': 'Jiacheng Feng', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8a9', 'name': 'Jiahui Yu', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8aa', 'name': 'Jiayi Weng', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8ab', 'name': 'Jie Tang', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8ac', 'name': 'Jieqi Yu', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8ad', 'name': 'Joaquin Quiñonero Candela', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8ae', 'name': 'Joe Palermo', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8af', 'name': 'Joel Parish', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8b0', 'name': 'Johannes Heidecke', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8b1', 'name': 'John Hallman', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8b2', 'name': 'John Rizzo', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8b3', 'name': 'Jonathan Gordon', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8b4', 'name': 'Jonathan Uesato', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8b5', 'name': 'Jonathan Uesato', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8b6', 'name': 'Jonathan Ward', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8b7', 'name': 'Joost Huizinga', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8b8', 'name': 'Julie Wang', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8b9', 'name': 'Kai Chen', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8ba', 'name': 'Kai Xiao', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8bb', 'name': 'Karan Singhal', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8bc', 'name': 'Karina Nguyen', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8bd', 'name': 'Karl Cobbe', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8be', 'name': 'Katy Shi', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8bf', 'name': 'Kayla Wood', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8c0', 'name': 'Kendra Rimbach', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8c1', 'name': 'Keren Gu-Lemberg', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8c2', 'name': 'Keren GuLemberg', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8c3', 'name': 'Kevin Liu', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8c4', 'name': 'Kevin Lu', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8c5', 'name': 'Kevin Stone', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8c6', 'name': 'Kevin Yu', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8c7', 'name': 'Lama Ahmad', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8c8', 'name': 'Lauren Yang', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8c9', 'name': 'Leo Liu', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8ca', 'name': 'Leon Maksin', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8cb', 'name': 'Leyton Ho', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8cc', 'name': 'Liam Fedus', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8cd', 'name': 'Lilian Weng', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8ce', 'name': 'Linden Li', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8cf', 'name': 'Lindsay McCallum', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8d0', 'name': 'Lindsey Held', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8d1', 'name': 'Lorenz Kuhn', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8d2', 'name': 'Lukas Kondraciuk', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8d3', 'name': 'Lukasz Kaiser', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8d4', 'name': 'Luke Metz', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8d5', 'name': 'Madelaine Boyd', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8d6', 'name': 'Maja Trebacz', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8d7', 'name': 'Manas Joglekar', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8d8', 'name': 'Mark Chen', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8d9', 'name': 'Marko Tintor', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8da', 'name': 'Mason Meyer', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8db', 'name': 'Matt Jones', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8dc', 'name': 'Matt Kaufer', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8dd', 'name': 'Max Schwarzer', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8de', 'name': 'Meghan Shah', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8df', 'name': 'Mehmet Yatbaz', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8e0', 'name': 'Melody Guan', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8e1', 'name': 'Mengyuan Xu', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8e2', 'name': 'Mengyuan Yan', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8e3', 'name': 'Mia Glaese', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8e4', 'name': 'Mianna Chen', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8e5', 'name': 'Mianna Chen', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8e6', 'name': 'Michael Lampe', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8e7', 'name': 'Michael Malek', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8e8', 'name': 'Michele Wang', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8e9', 'name': 'Michelle Fradin', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8ea', 'name': 'Mike McClay', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8eb', 'name': 'Mikhail Pavlov', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8ec', 'name': 'Miles Wang', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8ed', 'name': 'Mingxuan Wang', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8ee', 'name': 'Mira Murati', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8ef', 'name': 'Mo Bavarian', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8f0', 'name': 'Mostafa Rohaninejad', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8f1', 'name': 'Nat McAleese', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8f2', 'name': 'Neil Chowdhury', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8f3', 'name': 'Neil Chowdhury', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8f4', 'name': 'Nick Ryder', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8f5', 'name': 'Nikolas Tezak', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8f6', 'name': 'Noam Brown', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8f7', 'name': 'Ofir Nachum', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8f8', 'name': 'Oleg Boiko', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8f9', 'name': 'Oleg Murk', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8fa', 'name': 'Olivia Watkins', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8fb', 'name': 'Patrick Chao', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8fc', 'name': 'Paul Ashbourne', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8fd', 'name': 'Pavel Izmailov', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8fe', 'name': 'Peter Zhokhov', 'hidden': False}, {'_id': '676a5f1127c8341daf37c8ff', 'name': 'Rachel Dias', 'hidden': False}, {'_id': '676a5f1127c8341daf37c900', 'name': 'Rahul Arora', 'hidden': False}, {'_id': '676a5f1127c8341daf37c901', 'name': 'Randall Lin', 'hidden': False}, {'_id': '676a5f1127c8341daf37c902', 'name': 'Rapha Gontijo Lopes', 'hidden': False}, {'_id': '676a5f1127c8341daf37c903', 'name': 'Raz Gaon', 'hidden': False}, {'_id': '676a5f1127c8341daf37c904', 'name': 'Reah Miyara', 'hidden': False}, {'_id': '676a5f1127c8341daf37c905', 'name': 'Reimar Leike', 'hidden': False}, {'_id': '676a5f1127c8341daf37c906', 'name': 'Renny Hwang', 'hidden': False}, {'_id': '676a5f1127c8341daf37c907', 'name': 'Rhythm Garg', 'hidden': False}, {'_id': '676a5f1127c8341daf37c908', 'name': 'Robin Brown', 'hidden': False}, {'_id': '676a5f1127c8341daf37c909', 'name': 'Roshan James', 'hidden': False}, {'_id': '676a5f1127c8341daf37c90a', 'name': 'Rui Shu', 'hidden': False}, {'_id': '676a5f1127c8341daf37c90b', 'name': 'Ryan Cheu', 'hidden': False}, {'_id': '676a5f1127c8341daf37c90c', 'name': 'Ryan Greene', 'hidden': False}, {'_id': '676a5f1127c8341daf37c90d', 'name': 'Saachi Jain', 'hidden': False}, {'_id': '676a5f1127c8341daf37c90e', 'name': 'Sam Altman', 'hidden': False}, {'_id': '676a5f1127c8341daf37c90f', 'name': 'Sam Toizer', 'hidden': False}, {'_id': '676a5f1127c8341daf37c910', 'name': 'Sam Toyer', 'hidden': False}, {'_id': '676a5f1127c8341daf37c911', 'name': 'Samuel Miserendino', 'hidden': False}, {'_id': '676a5f1127c8341daf37c912', 'name': 'Sandhini Agarwal', 'hidden': False}, {'_id': '676a5f1127c8341daf37c913', 'name': 'Santiago Hernandez', 'hidden': False}, {'_id': '676a5f1127c8341daf37c914', 'name': 'Sasha Baker', 'hidden': False}, {'_id': '676a5f1127c8341daf37c915', 'name': 'Scott McKinney', 'hidden': False}, {'_id': '676a5f1127c8341daf37c916', 'name': 'Scottie Yan', 'hidden': False}, {'_id': '676a5f1127c8341daf37c917', 'name': 'Shengjia Zhao', 'hidden': False}, {'_id': '676a5f1127c8341daf37c918', 'name': 'Shengli Hu', 'hidden': False}, {'_id': '676a5f1127c8341daf37c919', 'name': 'Shibani Santurkar', 'hidden': False}, {'_id': '676a5f1127c8341daf37c91a', 'name': 'Shraman Ray Chaudhuri', 'hidden': False}, {'_id': '676a5f1127c8341daf37c91b', 'name': 'Shuyuan Zhang', 'hidden': False}, {'_id': '676a5f1127c8341daf37c91c', 'name': 'Siyuan Fu', 'hidden': False}, {'_id': '676a5f1127c8341daf37c91d', 'name': 'Spencer Papay', 'hidden': False}, {'_id': '676a5f1127c8341daf37c91e', 'name': 'Steph Lin', 'hidden': False}, {'_id': '676a5f1127c8341daf37c91f', 'name': 'Suchir Balaji', 'hidden': False}, {'_id': '676a5f1127c8341daf37c920', 'name': 'Suvansh Sanjeev', 'hidden': False}, {'_id': '676a5f1127c8341daf37c921', 'name': 'Szymon Sidor', 'hidden': False}, {'_id': '676a5f1127c8341daf37c922', 'name': 'Tal Broda', 'hidden': False}, {'_id': '676a5f1127c8341daf37c923', 'name': 'Aidan Clark', 'hidden': False}, {'_id': '676a5f1127c8341daf37c924', 'name': 'Tao Wang', 'hidden': False}, {'_id': '676a5f1127c8341daf37c925', 'name': 'Taylor Gordon', 'hidden': False}, {'_id': '676a5f1127c8341daf37c926', 'name': 'Ted Sanders', 'hidden': False}, {'_id': '676a5f1127c8341daf37c927', 'name': 'Tejal Patwardhan', 'hidden': False}, {'_id': '676a5f1127c8341daf37c928', 'name': 'Thibault Sottiaux', 'hidden': False}, {'_id': '676a5f1127c8341daf37c929', 'name': 'Thomas Degry', 'hidden': False}, {'_id': '676a5f1127c8341daf37c92a', 'name': 'Thomas Dimson', 'hidden': False}, {'_id': '676a5f1127c8341daf37c92b', 'name': 'Tianhao Zheng', 'hidden': False}, {'_id': '676a5f1127c8341daf37c92c', 'name': 'Timur Garipov', 'hidden': False}, {'_id': '676a5f1127c8341daf37c92d', 'name': 'Tom Stasi', 'hidden': False}, {'_id': '676a5f1127c8341daf37c92e', 'name': 'Trapit Bansal', 'hidden': False}, {'_id': '676a5f1127c8341daf37c92f', 'name': 'Trevor Creech', 'hidden': False}, {'_id': '676a5f1127c8341daf37c930', 'name': 'Troy Peterson', 'hidden': False}, {'_id': '676a5f1127c8341daf37c931', 'name': 'Tyna Eloundou', 'hidden': False}, {'_id': '676a5f1127c8341daf37c932', 'name': 'Valerie Qi', 'hidden': False}, {'_id': '676a5f1127c8341daf37c933', 'name': 'Vineet Kosaraju', 'hidden': False}, {'_id': '676a5f1127c8341daf37c934', 'name': 'Vinnie Monaco', 'hidden': False}, {'_id': '676a5f1127c8341daf37c935', 'name': 'Vitchyr Pong', 'hidden': False}, {'_id': '676a5f1127c8341daf37c936', 'name': 'Vlad Fomenko', 'hidden': False}, {'_id': '676a5f1127c8341daf37c937', 'name': 'Weiyi Zheng', 'hidden': False}, {'_id': '676a5f1127c8341daf37c938', 'name': 'Wenda Zhou', 'hidden': False}, {'_id': '676a5f1127c8341daf37c939', 'name': 'Wes McCabe', 'hidden': False}, {'_id': '676a5f1127c8341daf37c93a', 'name': 'Wojciech Zaremba', 'hidden': False}, {'_id': '676a5f1127c8341daf37c93b', 'name': 'Yann Dubois', 'hidden': False}, {'_id': '676a5f1127c8341daf37c93c', 'name': 'Yinghai Lu', 'hidden': False}, {'_id': '676a5f1127c8341daf37c93d', 'name': 'Yining Chen', 'hidden': False}, {'_id': '676a5f1127c8341daf37c93e', 'name': 'Young Cha', 'hidden': False}, {'_id': '676a5f1127c8341daf37c93f', 'name': 'Yu Bai', 'hidden': False}, {'_id': '676a5f1127c8341daf37c940', 'name': 'Yuchen He', 'hidden': False}, {'_id': '676a5f1127c8341daf37c941', 'name': 'Yuchen Zhang', 'hidden': False}, {'_id': '676a5f1127c8341daf37c942', 'name': 'Yunyun Wang', 'hidden': False}, {'_id': '676a5f1127c8341daf37c943', 'name': 'Zheng Shao', 'hidden': False}, {'_id': '676a5f1127c8341daf37c944', 'name': 'Zhuohan Li', 'hidden': False}], 'publishedAt': '2024-12-21T18:04:31.000Z', 'title': 'OpenAI o1 System Card', 'summary': 'The o1 model series is trained with large-scale reinforcement learning to\\nreason using chain of thought. These advanced reasoning capabilities provide\\nnew avenues for improving the safety and robustness of our models. In\\nparticular, our models can reason about our safety policies in context when\\nresponding to potentially unsafe prompts, through deliberative alignment. This\\nleads to state-of-the-art performance on certain benchmarks for risks such as\\ngenerating illicit advice, choosing stereotyped responses, and succumbing to\\nknown jailbreaks. Training models to incorporate a chain of thought before\\nanswering has the potential to unlock substantial benefits, while also\\nincreasing potential risks that stem from heightened intelligence. Our results\\nunderscore the need for building robust alignment methods, extensively\\nstress-testing their efficacy, and maintaining meticulous risk management\\nprotocols. This report outlines the safety work carried out for the OpenAI o1\\nand OpenAI o1-mini models, including safety evaluations, external red teaming,\\nand Preparedness Framework evaluations.', 'upvotes': 9, 'discussionId': '676a5f1427c8341daf37c9c1'}, 'publishedAt': '2024-12-24T02:15:11.940Z', 'title': 'OpenAI o1 System Card', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.16720.png', 'numComments': 1, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isMod': False, 'followerCount': 5451}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2412.17498', 'authors': [{'_id': '676a1e8facedf3baab442be0', 'name': 'Jiaan Wang', 'hidden': False}, {'_id': '676a1e8facedf3baab442be1', 'name': 'Fandong Meng', 'hidden': False}, {'_id': '676a1e8facedf3baab442be2', 'name': 'Yunlong Liang', 'hidden': False}, {'_id': '676a1e8facedf3baab442be3', 'name': 'Jie Zhou', 'hidden': False}], 'publishedAt': '2024-12-23T11:55:33.000Z', 'title': 'DRT-o1: Optimized Deep Reasoning Translation via Long Chain-of-Thought', 'summary': \"Recently, O1-like models have emerged as representative examples,\\nillustrating the effectiveness of long chain-of-thought (CoT) in reasoning\\ntasks such as math and coding tasks. In this paper, we introduce DRT-o1, an\\nattempt to bring the success of long CoT to neural machine translation (MT).\\nSpecifically, in view of the literature books that might involve similes and\\nmetaphors, translating these texts to a target language is very difficult in\\npractice due to cultural differences. In such cases, literal translation often\\nfails to convey the intended meaning effectively. Even for professional human\\ntranslators, considerable thought must be given to preserving semantics\\nthroughout the translation process. To simulate LLMs' long thought ability in\\nMT, we first mine sentences containing similes or metaphors from existing\\nliterature books, and then develop a multi-agent framework to translate these\\nsentences via long thought. In the multi-agent framework, a translator is used\\nto iteratively translate the source sentence under the suggestions provided by\\nan advisor. To ensure the effectiveness of the long thoughts, an evaluator is\\nalso employed to judge whether the translation in the current round is better\\nthan the previous one or not. In this manner, we collect tens of thousands of\\nlong-thought MT data, which is used to train our DRT-o1. The experimental\\nresults on literature translation demonstrate the effectiveness of the DRT-o1.\\nUsing Qwen2.5-7B and Qwen2.5-14B as the backbones, the improvement brought by\\nDRT-o1 achieves 7.33~8.26 BLEU and 1.66~3.36 CometScore. Besides, DRT-o1-7B can\\noutperform QwQ-32B-Preview by 7.82 BLEU and 1.46 CometScore, showing its\\neffectiveness. The project is available at https://github.com/krystalan/DRT-o1\", 'upvotes': 7, 'discussionId': '676a1e90acedf3baab442c22'}, 'publishedAt': '2024-12-24T02:18:55.342Z', 'title': 'DRT-o1: Optimized Deep Reasoning Translation via Long Chain-of-Thought', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.17498.png', 'numComments': 1, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isMod': False, 'followerCount': 5451}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2412.17589', 'authors': [{'_id': '676a8186b88f71d971444400', 'name': 'Yanheng He', 'hidden': False}, {'_id': '676a8186b88f71d971444401', 'name': 'Jiahe Jin', 'hidden': False}, {'_id': '676a8186b88f71d971444402', 'name': 'Shijie Xia', 'hidden': False}, {'_id': '676a8186b88f71d971444403', 'name': 'Jiadi Su', 'hidden': False}, {'_id': '676a8186b88f71d971444404', 'name': 'Runze Fan', 'hidden': False}, {'_id': '676a8186b88f71d971444405', 'name': 'Haoyang Zou', 'hidden': False}, {'_id': '676a8186b88f71d971444406', 'name': 'Xiangkun Hu', 'hidden': False}, {'_id': '676a8186b88f71d971444407', 'name': 'Pengfei Liu', 'hidden': False}], 'publishedAt': '2024-12-23T14:02:12.000Z', 'title': 'PC Agent: While You Sleep, AI Works -- A Cognitive Journey into Digital\\n  World', 'summary': 'Imagine a world where AI can handle your work while you sleep - organizing\\nyour research materials, drafting a report, or creating a presentation you need\\nfor tomorrow. However, while current digital agents can perform simple tasks,\\nthey are far from capable of handling the complex real-world work that humans\\nroutinely perform. We present PC Agent, an AI system that demonstrates a\\ncrucial step toward this vision through human cognition transfer. Our key\\ninsight is that the path from executing simple \"tasks\" to handling complex\\n\"work\" lies in efficiently capturing and learning from human cognitive\\nprocesses during computer use. To validate this hypothesis, we introduce three\\nkey innovations: (1) PC Tracker, a lightweight infrastructure that efficiently\\ncollects high-quality human-computer interaction trajectories with complete\\ncognitive context; (2) a two-stage cognition completion pipeline that\\ntransforms raw interaction data into rich cognitive trajectories by completing\\naction semantics and thought processes; and (3) a multi-agent system combining\\na planning agent for decision-making with a grounding agent for robust visual\\ngrounding. Our preliminary experiments in PowerPoint presentation creation\\nreveal that complex digital work capabilities can be achieved with a small\\namount of high-quality cognitive data - PC Agent, trained on just 133 cognitive\\ntrajectories, can handle sophisticated work scenarios involving up to 50 steps\\nacross multiple applications. This demonstrates the data efficiency of our\\napproach, highlighting that the key to training capable digital agents lies in\\ncollecting human cognitive data. By open-sourcing our complete framework,\\nincluding the data collection infrastructure and cognition completion methods,\\nwe aim to lower the barriers for the research community to develop truly\\ncapable digital agents.', 'upvotes': 6, 'discussionId': '676a8188b88f71d971444477'}, 'publishedAt': '2024-12-24T04:45:51.603Z', 'title': 'PC Agent: While You Sleep, AI Works -- A Cognitive Journey into Digital World', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.17589.png', 'numComments': 1, 'submittedBy': {'_id': '616bfc2b40e2f69baa1c7add', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/616bfc2b40e2f69baa1c7add/Os7_qgMei-2lRVelrOG7B.jpeg', 'fullname': 'Run-Ze Fan', 'name': 'Vfrz', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 9}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2412.14470', 'authors': [{'_id': '676a77cf5d76485cb34417b3', 'name': 'Zhexin Zhang', 'hidden': False}, {'_id': '676a77cf5d76485cb34417b4', 'name': 'Shiyao Cui', 'hidden': False}, {'_id': '676a77cf5d76485cb34417b5', 'name': 'Yida Lu', 'hidden': False}, {'_id': '676a77cf5d76485cb34417b6', 'name': 'Jingzhuo Zhou', 'hidden': False}, {'_id': '676a77cf5d76485cb34417b7', 'name': 'Junxiao Yang', 'hidden': False}, {'_id': '676a77cf5d76485cb34417b8', 'name': 'Hongning Wang', 'hidden': False}, {'_id': '676a77cf5d76485cb34417b9', 'name': 'Minlie Huang', 'hidden': False}], 'publishedAt': '2024-12-19T02:35:15.000Z', 'title': 'Agent-SafetyBench: Evaluating the Safety of LLM Agents', 'summary': 'As large language models (LLMs) are increasingly deployed as agents, their\\nintegration into interactive environments and tool use introduce new safety\\nchallenges beyond those associated with the models themselves. However, the\\nabsence of comprehensive benchmarks for evaluating agent safety presents a\\nsignificant barrier to effective assessment and further improvement. In this\\npaper, we introduce Agent-SafetyBench, a comprehensive benchmark designed to\\nevaluate the safety of LLM agents. Agent-SafetyBench encompasses 349\\ninteraction environments and 2,000 test cases, evaluating 8 categories of\\nsafety risks and covering 10 common failure modes frequently encountered in\\nunsafe interactions. Our evaluation of 16 popular LLM agents reveals a\\nconcerning result: none of the agents achieves a safety score above 60%. This\\nhighlights significant safety challenges in LLM agents and underscores the\\nconsiderable need for improvement. Through quantitative analysis, we identify\\ncritical failure modes and summarize two fundamental safety detects in current\\nLLM agents: lack of robustness and lack of risk awareness. Furthermore, our\\nfindings suggest that reliance on defense prompts alone is insufficient to\\naddress these safety issues, emphasizing the need for more advanced and robust\\nstrategies. We release Agent-SafetyBench at\\nhttps://github.com/thu-coai/Agent-SafetyBench to facilitate further\\nresearch and innovation in agent safety evaluation and improvement.', 'upvotes': 5, 'discussionId': '676a77d15d76485cb3441886'}, 'publishedAt': '2024-12-24T04:00:22.597Z', 'title': 'Agent-SafetyBench: Evaluating the Safety of LLM Agents', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.14470.png', 'numComments': 1, 'submittedBy': {'_id': '61b58aa0d65058ce70beb98c', 'avatarUrl': '/avatars/aefd9271b891abc6dd2ded1a30eebca4.svg', 'fullname': 'Zhexin Zhang', 'name': 'nonstopfor', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2412.16686', 'authors': [{'_id': '676a20cbeabbef01bb5f825c', 'name': 'Minda Hu', 'hidden': False}, {'_id': '676a20cbeabbef01bb5f825d', 'name': 'Qiyuan Zhang', 'hidden': False}, {'_id': '676a20cbeabbef01bb5f825e', 'name': 'Yufei Wang', 'hidden': False}, {'_id': '676a20cbeabbef01bb5f825f', 'name': 'Bowei He', 'hidden': False}, {'_id': '676a20cbeabbef01bb5f8260', 'name': 'Hongru Wang', 'hidden': False}, {'_id': '676a20cbeabbef01bb5f8261', 'name': 'Jingyan Zhou', 'hidden': False}, {'_id': '676a20cbeabbef01bb5f8262', 'name': 'Liangyou Li', 'hidden': False}, {'_id': '676a20cbeabbef01bb5f8263', 'name': 'Yasheng Wang', 'hidden': False}, {'_id': '676a20cbeabbef01bb5f8264', 'name': 'Chen Ma', 'hidden': False}, {'_id': '676a20cbeabbef01bb5f8265', 'name': 'Irwin King', 'hidden': False}], 'publishedAt': '2024-12-21T16:25:16.000Z', 'title': 'NILE: Internal Consistency Alignment in Large Language Models', 'summary': \"As a crucial step to enhance LLMs alignment with human intentions,\\nInstruction Fine-Tuning (IFT) has a high demand on dataset quality. However,\\nexisting IFT datasets often contain knowledge that is inconsistent with LLMs'\\ninternal knowledge learned from the pre-training phase, which can greatly\\naffect the efficacy of IFT. To address this issue, we introduce NILE (iNternal\\nconsIstency aLignmEnt) framework, aimed at optimizing IFT datasets to unlock\\nLLMs' capability further. NILE operates by eliciting target pre-trained LLM's\\ninternal knowledge corresponding to instruction data. The internal knowledge is\\nleveraged to revise the answer in IFT datasets. Additionally, we propose a\\nnovel Internal Consistency Filtering (ICF) method to filter training samples,\\nensuring its high consistency with LLM's internal knowledge. Our experiments\\ndemonstrate that NILE-aligned IFT datasets sharply boost LLM performance across\\nmultiple LLM ability evaluation datasets, achieving up to 66.6% gain on\\nArena-Hard and 68.5% on Alpaca-Eval V2. Further analysis confirms that each\\ncomponent of the NILE}framework contributes to these substantial performance\\nimprovements, and provides compelling evidence that dataset consistency with\\npre-trained internal knowledge is pivotal for maximizing LLM potential.\", 'upvotes': 5, 'discussionId': '676a20cceabbef01bb5f82df'}, 'publishedAt': '2024-12-23T22:18:25.419Z', 'title': 'NILE: Internal Consistency Alignment in Large Language Models', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.16686.png', 'numComments': 1, 'submittedBy': {'_id': '62a42f22c683d02f5b63320c', 'avatarUrl': '/avatars/bc611abe9c4ef8d378123cb8ac9fdbf2.svg', 'fullname': 'Qiyuan Zhang', 'name': 'DonJoey', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2412.17767', 'authors': [{'_id': '676aaa45d4000ace4575110c', 'name': 'Haofei Yu', 'hidden': False}, {'_id': '676aaa45d4000ace4575110d', 'name': 'Zhaochen Hong', 'hidden': False}, {'_id': '676aaa45d4000ace4575110e', 'name': 'Zirui Cheng', 'hidden': False}, {'_id': '676aaa45d4000ace4575110f', 'name': 'Kunlun Zhu', 'hidden': False}, {'_id': '676aaa45d4000ace45751110', 'name': 'Keyang Xuan', 'hidden': False}, {'_id': '676aaa45d4000ace45751111', 'name': 'Jinwei Yao', 'hidden': False}, {'_id': '676aaa45d4000ace45751112', 'name': 'Tao Feng', 'hidden': False}, {'_id': '676aaa45d4000ace45751113', 'name': 'Jiaxuan You', 'hidden': False}], 'publishedAt': '2024-12-23T18:26:53.000Z', 'title': 'ResearchTown: Simulator of Human Research Community', 'summary': 'Large Language Models (LLMs) have demonstrated remarkable potential in\\nscientific domains, yet a fundamental question remains unanswered: Can we\\nsimulate human research communities with LLMs? Addressing this question can\\ndeepen our understanding of the processes behind idea brainstorming and inspire\\nthe automatic discovery of novel scientific insights. In this work, we propose\\nResearchTown, a multi-agent framework for research community simulation. Within\\nthis framework, the human research community is simplified and modeled as an\\nagent-data graph, where researchers and papers are represented as agent-type\\nand data-type nodes, respectively, and connected based on their collaboration\\nrelationships. We also introduce TextGNN, a text-based inference framework that\\nmodels various research activities (e.g., paper reading, paper writing, and\\nreview writing) as special forms of a unified message-passing process on the\\nagent-data graph. To evaluate the quality of the research simulation, we\\npresent ResearchBench, a benchmark that uses a node-masking prediction task for\\nscalable and objective assessment based on similarity. Our experiments reveal\\nthree key findings: (1) ResearchTown can provide a realistic simulation of\\ncollaborative research activities, including paper writing and review writing;\\n(2) ResearchTown can maintain robust simulation with multiple researchers and\\ndiverse papers; (3) ResearchTown can generate interdisciplinary research ideas\\nthat potentially inspire novel research directions.', 'upvotes': 4, 'discussionId': '676aaa46d4000ace457511b2'}, 'publishedAt': '2024-12-24T07:42:01.751Z', 'title': 'ResearchTown: Simulator of Human Research Community', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/636453547cf2c0b4f0a3ee1e/SHXI2ZTH3G7RYb7lG8UhL.png'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.17767.png', 'numComments': 1, 'submittedBy': {'_id': '636453547cf2c0b4f0a3ee1e', 'avatarUrl': '/avatars/29b4bf0e6abd3b70bb0dbd58188e4ac8.svg', 'fullname': 'Haofei Yu', 'name': 'lwaekfjlk', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2412.16849', 'authors': [{'_id': '676a85a619a646a97e21e8e5', 'name': 'Yuxiang Zhang', 'hidden': False}, {'_id': '676a85a619a646a97e21e8e6', 'name': 'Yuqi Yang', 'hidden': False}, {'_id': '676a85a619a646a97e21e8e7', 'name': 'Jiangming Shu', 'hidden': False}, {'_id': '676a85a619a646a97e21e8e8', 'name': 'Yuhang Wang', 'hidden': False}, {'_id': '676a85a619a646a97e21e8e9', 'name': 'Jinlin Xiao', 'hidden': False}, {'_id': '676a85a619a646a97e21e8ea', 'name': 'Jitao Sang', 'hidden': False}], 'publishedAt': '2024-12-22T04:21:30.000Z', 'title': 'OpenRFT: Adapting Reasoning Foundation Model for Domain-specific Tasks\\n  with Reinforcement Fine-Tuning', 'summary': \"OpenAI's recent introduction of Reinforcement Fine-Tuning (RFT) showcases the\\npotential of reasoning foundation model and offers a new paradigm for\\nfine-tuning beyond simple pattern imitation. This technical report presents\\nOpenRFT, our attempt to fine-tune generalist reasoning models for\\ndomain-specific tasks under the same settings as RFT. OpenRFT addresses two key\\nchallenges of lacking reasoning step data and the limited quantity of training\\nsamples, by leveraging the domain-specific samples in three ways: question\\naugmentation, synthesizing reasoning-process data, and few-shot ICL. The\\nevaluation is conducted on SciKnowEval, where OpenRFT achieves notable\\nperformance gains with only 100 domain-specific samples for each task. More\\nexperimental results will be updated continuously in later versions. Source\\ncodes, datasets, and models are disclosed at:\\nhttps://github.com/ADaM-BJTU/OpenRFT\", 'upvotes': 3, 'discussionId': '676a85a719a646a97e21e92d'}, 'publishedAt': '2024-12-24T05:04:23.303Z', 'title': 'OpenRFT: Adapting Reasoning Foundation Model for Domain-specific Tasks with Reinforcement Fine-Tuning', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.16849.png', 'numComments': 1, 'submittedBy': {'_id': '6494457c6339264dd78bcb95', 'avatarUrl': '/avatars/d87842251f1a43f50cc827f0e2a995ee.svg', 'fullname': 'sdzy', 'name': 'sdzy', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2412.17295', 'authors': [{'_id': '676a3878eabbef01bb66f7f8', 'name': 'Yueqian Wang', 'hidden': False}, {'_id': '676a3878eabbef01bb66f7f9', 'name': 'Xiaojun Meng', 'hidden': False}, {'_id': '676a3878eabbef01bb66f7fa', 'user': {'_id': '60b9e6837946aff342f734ae', 'avatarUrl': '/avatars/a711a6aa35757dfd7b78b26098a964fc.svg', 'isPro': False, 'fullname': 'Yuxuan Wang', 'user': 'ColorfulAI', 'type': 'user'}, 'name': 'Yuxuan Wang', 'status': 'extracted_pending', 'statusLastChangedAt': '2024-12-24T04:28:46.420Z', 'hidden': False}, {'_id': '676a3878eabbef01bb66f7fb', 'name': 'Jianxin Liang', 'hidden': False}, {'_id': '676a3878eabbef01bb66f7fc', 'name': 'Qun Liu', 'hidden': False}, {'_id': '676a3878eabbef01bb66f7fd', 'name': 'Dongyan Zhao', 'hidden': False}], 'publishedAt': '2024-12-23T05:32:48.000Z', 'title': 'Friends-MMC: A Dataset for Multi-modal Multi-party Conversation\\n  Understanding', 'summary': 'Multi-modal multi-party conversation (MMC) is a less studied yet important\\ntopic of research due to that it well fits real-world scenarios and thus\\npotentially has more widely-used applications. Compared with the traditional\\nmulti-modal conversations, MMC requires stronger character-centered\\nunderstanding abilities as there are many interlocutors appearing in both the\\nvisual and textual context. To facilitate the study of this problem, we present\\nFriends-MMC in this paper, an MMC dataset that contains 24,000+ unique\\nutterances paired with video context. To explore the character-centered\\nunderstanding of the dialogue, we also annotate the speaker of each utterance,\\nthe names and bounding bboxes of faces that appear in the video. Based on this\\nFriends-MMC dataset, we further study two fundamental MMC tasks: conversation\\nspeaker identification and conversation response prediction, both of which have\\nthe multi-party nature with the video or image as visual context. For\\nconversation speaker identification, we demonstrate the inefficiencies of\\nexisting methods such as pre-trained models, and propose a simple yet effective\\nbaseline method that leverages an optimization solver to utilize the context of\\ntwo modalities to achieve better performance. For conversation response\\nprediction, we fine-tune generative dialogue models on Friend-MMC, and analyze\\nthe benefits of speaker information. The code and dataset is publicly available\\nat https://github.com/yellow-binary-tree/Friends-MMC and thus we call for more\\nattention on modeling speaker information when understanding conversations.', 'upvotes': 3, 'discussionId': '676a387eeabbef01bb66ff15'}, 'publishedAt': '2024-12-24T03:56:19.269Z', 'title': 'Friends-MMC: A Dataset for Multi-modal Multi-party Conversation Understanding', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.17295.png', 'numComments': 1, 'submittedBy': {'_id': '60b9e6837946aff342f734ae', 'avatarUrl': '/avatars/a711a6aa35757dfd7b78b26098a964fc.svg', 'fullname': 'Yuxuan Wang', 'name': 'ColorfulAI', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 2}, 'isAuthorParticipating': True}"
]