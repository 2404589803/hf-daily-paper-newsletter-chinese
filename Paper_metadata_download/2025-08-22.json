[
  {
    "paper": {
      "id": "2508.15763",
      "authors": [
        {
          "_id": "68a7ca3639413c456c05afcc",
          "name": "Lei Bai",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05afcd",
          "name": "Zhongrui Cai",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05afce",
          "name": "Maosong Cao",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05afcf",
          "name": "Weihan Cao",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05afd0",
          "name": "Chiyu Chen",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05afd1",
          "name": "Haojiong Chen",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05afd2",
          "name": "Kai Chen",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05afd3",
          "name": "Pengcheng Chen",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05afd4",
          "name": "Ying Chen",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05afd5",
          "name": "Yongkang Chen",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05afd6",
          "name": "Yu Cheng",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05afd7",
          "name": "Yu Cheng",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05afd8",
          "name": "Pei Chu",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05afd9",
          "name": "Tao Chu",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05afda",
          "name": "Erfei Cui",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05afdb",
          "name": "Ganqu Cui",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05afdc",
          "name": "Long Cui",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05afdd",
          "name": "Ziyun Cui",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05afde",
          "name": "Nianchen Deng",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05afdf",
          "name": "Ning Ding",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05afe0",
          "name": "Nanqin Dong",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05afe1",
          "name": "Peijie Dong",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05afe2",
          "name": "Shihan Dou",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05afe3",
          "name": "Sinan Du",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05afe4",
          "name": "Haodong Duan",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05afe5",
          "name": "Caihua Fan",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05afe6",
          "name": "Ben Gao",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05afe7",
          "name": "Changjiang Gao",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05afe8",
          "name": "Jianfei Gao",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05afe9",
          "name": "Songyang Gao",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05afea",
          "name": "Yang Gao",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05afeb",
          "name": "Zhangwei Gao",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05afec",
          "name": "Jiaye Ge",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05afed",
          "name": "Qiming Ge",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05afee",
          "name": "Lixin Gu",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05afef",
          "name": "Yuzhe Gu",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05aff0",
          "name": "Aijia Guo",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05aff1",
          "name": "Qipeng Guo",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05aff2",
          "user": {
            "_id": "6687bdeb3495d4c1c4d0b8a2",
            "avatarUrl": "/avatars/40b229b809347f83d7c0f694ee2b693a.svg",
            "isPro": false,
            "fullname": "guox18",
            "user": "guox18",
            "type": "user"
          },
          "name": "Xu Guo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-22T07:22:38.718Z",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05aff3",
          "name": "Conghui He",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05aff4",
          "name": "Junjun He",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05aff5",
          "name": "Yili Hong",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05aff6",
          "name": "Siyuan Hou",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05aff7",
          "name": "Caiyu Hu",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05aff8",
          "name": "Hanglei Hu",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05aff9",
          "name": "Jucheng Hu",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05affa",
          "name": "Ming Hu",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05affb",
          "name": "Zhouqi Hua",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05affc",
          "name": "Haian Huang",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05affd",
          "name": "Junhao Huang",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05affe",
          "name": "Xu Huang",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05afff",
          "name": "Zixian Huang",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b000",
          "name": "Zhe Jiang",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b001",
          "name": "Lingkai Kong",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b002",
          "name": "Linyang Li",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b003",
          "name": "Peiji Li",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b004",
          "name": "Pengze Li",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b005",
          "name": "Shuaibin Li",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b006",
          "name": "Tianbin Li",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b007",
          "name": "Wei Li",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b008",
          "name": "Yuqiang Li",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b009",
          "name": "Dahua Lin",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b00a",
          "name": "Junyao Lin",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b00b",
          "name": "Tianyi Lin",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b00c",
          "name": "Zhishan Lin",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b00d",
          "name": "Hongwei Liu",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b00e",
          "name": "Jiangning Liu",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b00f",
          "name": "Jiyao Liu",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b010",
          "user": {
            "_id": "643d26979347842571bc9613",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/3heFf7h3jbhhJWJ4JfGfh.jpeg",
            "isPro": false,
            "fullname": "Junnan Liu",
            "user": "jnanliu",
            "type": "user"
          },
          "name": "Junnan Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-22T07:22:42.929Z",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b011",
          "name": "Kai Liu",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b012",
          "name": "Kaiwen Liu",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b013",
          "user": {
            "_id": "63fd691794cc8f815d50c112",
            "avatarUrl": "/avatars/87305d1cbfcc717e910ccdfaf0568f80.svg",
            "isPro": false,
            "fullname": "liu",
            "user": "Harold-lkk",
            "type": "user"
          },
          "name": "Kuikun Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-22T07:22:36.574Z",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b014",
          "name": "Shichun Liu",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b015",
          "name": "Shudong Liu",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b016",
          "name": "Wei Liu",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b017",
          "name": "Xinyao Liu",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b018",
          "name": "Yuhong Liu",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b019",
          "name": "Zhan Liu",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b01a",
          "name": "Yinquan Lu",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b01b",
          "name": "Haijun Lv",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b01c",
          "name": "Hongxia Lv",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b01d",
          "name": "Huijie Lv",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b01e",
          "name": "Qidang Lv",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b01f",
          "name": "Ying Lv",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b020",
          "name": "Chengqi Lyu",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b021",
          "name": "Chenglong Ma",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b022",
          "name": "Jianpeng Ma",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b023",
          "name": "Ren Ma",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b024",
          "name": "Runmin Ma",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b025",
          "name": "Runyuan Ma",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b026",
          "name": "Xinzhu Ma",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b027",
          "name": "Yichuan Ma",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b028",
          "user": {
            "_id": "677e869467f3bb8d8215eec6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/677e869467f3bb8d8215eec6/kEC6JOKObgLHA22jRcP4H.jpeg",
            "isPro": false,
            "fullname": "Zihan Ma",
            "user": "MichaelErchi",
            "type": "user"
          },
          "name": "Zihan Ma",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-22T07:22:40.987Z",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b029",
          "name": "Sixuan Mi",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b02a",
          "name": "Junzhi Ning",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b02b",
          "name": "Wenchang Ning",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b02c",
          "name": "Xinle Pang",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b02d",
          "name": "Jiahui Peng",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b02e",
          "name": "Runyu Peng",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b02f",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b030",
          "name": "Jiantao Qiu",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b031",
          "name": "Xiaoye Qu",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b032",
          "name": "Yuan Qu",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b033",
          "name": "Yuchen Ren",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b034",
          "name": "Fukai Shang",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b035",
          "name": "Wenqi Shao",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b036",
          "user": {
            "_id": "687f853bb39262ba84f3eeff",
            "avatarUrl": "/avatars/5313f5974f289239488e842fe1536749.svg",
            "isPro": false,
            "fullname": "Junhao Shen",
            "user": "shenjunhao",
            "type": "user"
          },
          "name": "Junhao Shen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-22T07:22:46.810Z",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b037",
          "name": "Shuaike Shen",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b038",
          "name": "Chunfeng Song",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b039",
          "name": "Demin Song",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b03a",
          "name": "Diping Song",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b03b",
          "name": "Chenlin Su",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b03c",
          "name": "Weijie Su",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b03d",
          "name": "Weigao Sun",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b03e",
          "name": "Yu Sun",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b03f",
          "name": "Qian Tan",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b040",
          "name": "Cheng Tang",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b041",
          "name": "Huanze Tang",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b042",
          "name": "Kexian Tang",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b043",
          "name": "Shixiang Tang",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b044",
          "name": "Jian Tong",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b045",
          "name": "Aoran Wang",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b046",
          "name": "Bin Wang",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b047",
          "name": "Dong Wang",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b048",
          "name": "Lintao Wang",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b049",
          "name": "Rui Wang",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b04a",
          "name": "Weiyun Wang",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b04b",
          "name": "Wenhai Wang",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b04c",
          "name": "Yi Wang",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b04d",
          "name": "Ziyi Wang",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b04e",
          "name": "Ling-I Wu",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b04f",
          "name": "Wen Wu",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b050",
          "name": "Yue Wu",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b051",
          "name": "Zijian Wu",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b052",
          "name": "Linchen Xiao",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b053",
          "name": "Shuhao Xing",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b054",
          "name": "Chao Xu",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b055",
          "name": "Huihui Xu",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b056",
          "name": "Jun Xu",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b057",
          "name": "Ruiliang Xu",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b058",
          "name": "Wanghan Xu",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b059",
          "name": "GanLin Yang",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b05a",
          "name": "Yuming Yang",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b05b",
          "name": "Haochen Ye",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b05c",
          "name": "Jin Ye",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b05d",
          "name": "Shenglong Ye",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b05e",
          "name": "Jia Yu",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b05f",
          "name": "Jiashuo Yu",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b060",
          "name": "Jing Yu",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b061",
          "name": "Fei Yuan",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b062",
          "name": "Bo Zhang",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b063",
          "name": "Chao Zhang",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b064",
          "name": "Chen Zhang",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b065",
          "name": "Hongjie Zhang",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b066",
          "name": "Jin Zhang",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b067",
          "name": "Qiaosheng Zhang",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b068",
          "name": "Qiuyinzhe Zhang",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b069",
          "user": {
            "_id": "630716d11801ecc7d2595021",
            "avatarUrl": "/avatars/2d36a880ce4a3cf7efc5ff3987dbeaf3.svg",
            "isPro": false,
            "fullname": "Songyang Zhang",
            "user": "zsytony",
            "type": "user"
          },
          "name": "Songyang Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-22T07:22:44.976Z",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b06a",
          "name": "Taolin Zhang",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b06b",
          "name": "Wenlong Zhang",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b06c",
          "user": {
            "_id": "64e8505321540e1da3226b54",
            "avatarUrl": "/avatars/18958b8406d1ce492b54c1c839f18c54.svg",
            "isPro": false,
            "fullname": "Wenwei Zhang",
            "user": "ZwwWayne",
            "type": "user"
          },
          "name": "Wenwei Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-22T07:22:49.089Z",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b06d",
          "name": "Yechen Zhang",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b06e",
          "name": "Ziyang Zhang",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b06f",
          "user": {
            "_id": "64a7c6e223622f7f189bcbe1",
            "avatarUrl": "/avatars/4f13a7ed0d2b8d8dfff7dc650e46450a.svg",
            "isPro": false,
            "fullname": "haiteng zhao",
            "user": "haitengzhao",
            "type": "user"
          },
          "name": "Haiteng Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-22T07:22:32.021Z",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b070",
          "name": "Qian Zhao",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b071",
          "name": "Xiangyu Zhao",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b072",
          "name": "Xiangyu Zhao",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b073",
          "name": "Bowen Zhou",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b074",
          "name": "Dongzhan Zhou",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b075",
          "name": "Peiheng Zhou",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b076",
          "name": "Yuhao Zhou",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b077",
          "name": "Yunhua Zhou",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b078",
          "user": {
            "_id": "630da0fae57da204209411d3",
            "avatarUrl": "/avatars/e79c250cf8031441ffd0e853e653cef6.svg",
            "isPro": false,
            "fullname": "dongsheng zhu",
            "user": "dongsheng",
            "type": "user"
          },
          "name": "Dongsheng Zhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-22T07:22:34.188Z",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b079",
          "name": "Lin Zhu",
          "hidden": false
        },
        {
          "_id": "68a7ca3639413c456c05b07a",
          "name": "Yicheng Zou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-21T17:58:00.000Z",
      "submittedOnDailyAt": "2025-08-22T00:12:32.664Z",
      "title": "Intern-S1: A Scientific Multimodal Foundation Model",
      "submittedOnDailyBy": {
        "_id": "64e8505321540e1da3226b54",
        "avatarUrl": "/avatars/18958b8406d1ce492b54c1c839f18c54.svg",
        "isPro": false,
        "fullname": "Wenwei Zhang",
        "user": "ZwwWayne",
        "type": "user"
      },
      "summary": "In recent years, a plethora of open-source foundation models have emerged,\nachieving remarkable progress in some widely attended fields, with performance\nbeing quite close to that of closed-source models. However, in high-value but\nmore challenging scientific professional fields, either the fields still rely\non expert models, or the progress of general foundation models lags\nsignificantly compared to those in popular areas, far from sufficient for\ntransforming scientific research and leaving substantial gap between\nopen-source models and closed-source models in these scientific domains. To\nmitigate this gap and explore a step further toward Artificial General\nIntelligence (AGI), we introduce Intern-S1, a specialized generalist equipped\nwith general understanding and reasoning capabilities with expertise to analyze\nmultiple science modal data. Intern-S1 is a multimodal Mixture-of-Experts (MoE)\nmodel with 28 billion activated parameters and 241 billion total parameters,\ncontinually pre-trained on 5T tokens, including over 2.5T tokens from\nscientific domains. In the post-training stage, Intern-S1 undergoes offline and\nthen online reinforcement learning (RL) in InternBootCamp, where we propose\nMixture-of-Rewards (MoR) to synergize the RL training on more than 1000 tasks\nsimultaneously. Through integrated innovations in algorithms, data, and\ntraining systems, Intern-S1 achieved top-tier performance in online RL\ntraining.On comprehensive evaluation benchmarks, Intern-S1 demonstrates\ncompetitive performance on general reasoning tasks among open-source models and\nsignificantly outperforms open-source models in scientific domains, surpassing\nclosed-source state-of-the-art models in professional tasks, such as molecular\nsynthesis planning, reaction condition prediction, predicting thermodynamic\nstabilities for crystals. Our models are available at\nhttps://huggingface.co/internlm/Intern-S1.",
      "upvotes": 140,
      "discussionId": "68a7ca3639413c456c05b07b",
      "githubRepo": "https://github.com/InternLM/Intern-S1",
      "ai_summary": "Intern-S1, a multimodal Mixture-of-Experts model with extensive pre-training and reinforcement learning, achieves top-tier performance in general reasoning and outperforms closed-source models in scientific tasks.",
      "ai_keywords": [
        "Mixture-of-Experts (MoE)",
        "reinforcement learning (RL)",
        "Mixture-of-Rewards (MoR)",
        "molecular synthesis planning",
        "reaction condition prediction",
        "thermodynamic stabilities"
      ],
      "githubStars": 441
    },
    "publishedAt": "2025-08-21T13:58:00.000Z",
    "title": "Intern-S1: A Scientific Multimodal Foundation Model",
    "summary": "In recent years, a plethora of open-source foundation models have emerged,\nachieving remarkable progress in some widely attended fields, with performance\nbeing quite close to that of closed-source models. However, in high-value but\nmore challenging scientific professional fields, either the fields still rely\non expert models, or the progress of general foundation models lags\nsignificantly compared to those in popular areas, far from sufficient for\ntransforming scientific research and leaving substantial gap between\nopen-source models and closed-source models in these scientific domains. To\nmitigate this gap and explore a step further toward Artificial General\nIntelligence (AGI), we introduce Intern-S1, a specialized generalist equipped\nwith general understanding and reasoning capabilities with expertise to analyze\nmultiple science modal data. Intern-S1 is a multimodal Mixture-of-Experts (MoE)\nmodel with 28 billion activated parameters and 241 billion total parameters,\ncontinually pre-trained on 5T tokens, including over 2.5T tokens from\nscientific domains. In the post-training stage, Intern-S1 undergoes offline and\nthen online reinforcement learning (RL) in InternBootCamp, where we propose\nMixture-of-Rewards (MoR) to synergize the RL training on more than 1000 tasks\nsimultaneously. Through integrated innovations in algorithms, data, and\ntraining systems, Intern-S1 achieved top-tier performance in online RL\ntraining.On comprehensive evaluation benchmarks, Intern-S1 demonstrates\ncompetitive performance on general reasoning tasks among open-source models and\nsignificantly outperforms open-source models in scientific domains, surpassing\nclosed-source state-of-the-art models in professional tasks, such as molecular\nsynthesis planning, reaction condition prediction, predicting thermodynamic\nstabilities for crystals. Our models are available at\nhttps://huggingface.co/internlm/Intern-S1.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.15763.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64e8505321540e1da3226b54",
      "avatarUrl": "/avatars/18958b8406d1ce492b54c1c839f18c54.svg",
      "fullname": "Wenwei Zhang",
      "name": "ZwwWayne",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 16
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2508.15144",
      "authors": [
        {
          "_id": "68a7cb8d39413c456c05b082",
          "name": "Jiabo Ye",
          "hidden": false
        },
        {
          "_id": "68a7cb8d39413c456c05b083",
          "name": "Xi Zhang",
          "hidden": false
        },
        {
          "_id": "68a7cb8d39413c456c05b084",
          "name": "Haiyang Xu",
          "hidden": false
        },
        {
          "_id": "68a7cb8d39413c456c05b085",
          "name": "Haowei Liu",
          "hidden": false
        },
        {
          "_id": "68a7cb8d39413c456c05b086",
          "name": "Junyang Wang",
          "hidden": false
        },
        {
          "_id": "68a7cb8d39413c456c05b087",
          "name": "Zhaoqing Zhu",
          "hidden": false
        },
        {
          "_id": "68a7cb8d39413c456c05b088",
          "name": "Ziwei Zheng",
          "hidden": false
        },
        {
          "_id": "68a7cb8d39413c456c05b089",
          "name": "Feiyu Gao",
          "hidden": false
        },
        {
          "_id": "68a7cb8d39413c456c05b08a",
          "name": "Junjie Cao",
          "hidden": false
        },
        {
          "_id": "68a7cb8d39413c456c05b08b",
          "user": {
            "_id": "676127cf11b19ea602bb202a",
            "avatarUrl": "/avatars/dfd802a24bd63e509728159ebb1769f6.svg",
            "isPro": false,
            "fullname": "Zhengxi Lu",
            "user": "LZXzju",
            "type": "user"
          },
          "name": "Zhengxi Lu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-22T07:22:04.351Z",
          "hidden": false
        },
        {
          "_id": "68a7cb8d39413c456c05b08c",
          "name": "Jitong Liao",
          "hidden": false
        },
        {
          "_id": "68a7cb8d39413c456c05b08d",
          "name": "Qi Zheng",
          "hidden": false
        },
        {
          "_id": "68a7cb8d39413c456c05b08e",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "68a7cb8d39413c456c05b08f",
          "name": "Jingren Zhou",
          "hidden": false
        },
        {
          "_id": "68a7cb8d39413c456c05b090",
          "name": "Ming Yan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-21T00:39:12.000Z",
      "submittedOnDailyAt": "2025-08-22T01:21:57.840Z",
      "title": "Mobile-Agent-v3: Foundamental Agents for GUI Automation",
      "submittedOnDailyBy": {
        "_id": "645b10e80c73ea27d13f7aca",
        "avatarUrl": "/avatars/95e565306472a15067440b5b43e07a6f.svg",
        "isPro": false,
        "fullname": "xuhaiyang",
        "user": "xhyandwyy",
        "type": "user"
      },
      "summary": "This paper introduces GUI-Owl, a foundational GUI agent model that achieves\nstate-of-the-art performance among open-source end-to-end models on ten GUI\nbenchmarks across desktop and mobile environments, covering grounding, question\nanswering, planning, decision-making, and procedural knowledge. GUI-Owl-7B\nachieves 66.4 on AndroidWorld and 29.4 on OSWorld. Building on this, we propose\nMobile-Agent-v3, a general-purpose GUI agent framework that further improves\nperformance to 73.3 on AndroidWorld and 37.7 on OSWorld, setting a new\nstate-of-the-art for open-source GUI agent frameworks. GUI-Owl incorporates\nthree key innovations: (1) Large-scale Environment Infrastructure: a\ncloud-based virtual environment spanning Android, Ubuntu, macOS, and Windows,\nenabling our Self-Evolving GUI Trajectory Production framework. This generates\nhigh-quality interaction data via automated query generation and correctness\nvalidation, leveraging GUI-Owl to refine trajectories iteratively, forming a\nself-improving loop. It supports diverse data pipelines and reduces manual\nannotation. (2) Diverse Foundational Agent Capabilities: by integrating UI\ngrounding, planning, action semantics, and reasoning patterns, GUI-Owl supports\nend-to-end decision-making and can act as a modular component in multi-agent\nsystems. (3) Scalable Environment RL: we develop a scalable reinforcement\nlearning framework with fully asynchronous training for real-world alignment.\nWe also introduce Trajectory-aware Relative Policy Optimization (TRPO) for\nonline RL, achieving 34.9 on OSWorld. GUI-Owl and Mobile-Agent-v3 are\nopen-sourced at https://github.com/X-PLUG/MobileAgent.",
      "upvotes": 29,
      "discussionId": "68a7cb8e39413c456c05b091",
      "ai_summary": "GUI-Owl and Mobile-Agent-v3 are open-source GUI agent models and frameworks that achieve state-of-the-art performance across various benchmarks using innovations in environment infrastructure, agent capabilities, and scalable reinforcement learning.",
      "ai_keywords": [
        "GUI agent model",
        "Self-Evolving GUI Trajectory Production",
        "UI grounding",
        "planning",
        "action semantics",
        "reasoning patterns",
        "multi-agent systems",
        "scalable reinforcement learning",
        "fully asynchronous training",
        "Trajectory-aware Relative Policy Optimization (TRPO)"
      ]
    },
    "publishedAt": "2025-08-20T20:39:12.000Z",
    "title": "Mobile-Agent-v3: Foundamental Agents for GUI Automation",
    "summary": "This paper introduces GUI-Owl, a foundational GUI agent model that achieves\nstate-of-the-art performance among open-source end-to-end models on ten GUI\nbenchmarks across desktop and mobile environments, covering grounding, question\nanswering, planning, decision-making, and procedural knowledge. GUI-Owl-7B\nachieves 66.4 on AndroidWorld and 29.4 on OSWorld. Building on this, we propose\nMobile-Agent-v3, a general-purpose GUI agent framework that further improves\nperformance to 73.3 on AndroidWorld and 37.7 on OSWorld, setting a new\nstate-of-the-art for open-source GUI agent frameworks. GUI-Owl incorporates\nthree key innovations: (1) Large-scale Environment Infrastructure: a\ncloud-based virtual environment spanning Android, Ubuntu, macOS, and Windows,\nenabling our Self-Evolving GUI Trajectory Production framework. This generates\nhigh-quality interaction data via automated query generation and correctness\nvalidation, leveraging GUI-Owl to refine trajectories iteratively, forming a\nself-improving loop. It supports diverse data pipelines and reduces manual\nannotation. (2) Diverse Foundational Agent Capabilities: by integrating UI\ngrounding, planning, action semantics, and reasoning patterns, GUI-Owl supports\nend-to-end decision-making and can act as a modular component in multi-agent\nsystems. (3) Scalable Environment RL: we develop a scalable reinforcement\nlearning framework with fully asynchronous training for real-world alignment.\nWe also introduce Trajectory-aware Relative Policy Optimization (TRPO) for\nonline RL, achieving 34.9 on OSWorld. GUI-Owl and Mobile-Agent-v3 are\nopen-sourced at https://github.com/X-PLUG/MobileAgent.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.15144.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "645b10e80c73ea27d13f7aca",
      "avatarUrl": "/avatars/95e565306472a15067440b5b43e07a6f.svg",
      "fullname": "xuhaiyang",
      "name": "xhyandwyy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.15260",
      "authors": [
        {
          "_id": "68a7ca1239413c456c05afc6",
          "user": {
            "_id": "62ba66296501b0ff15ba1075",
            "avatarUrl": "/avatars/de468727cb1240fd4b1f24a19fb237a6.svg",
            "isPro": true,
            "fullname": "Yichao Fu",
            "user": "Viol2000",
            "type": "user"
          },
          "name": "Yichao Fu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-22T07:22:50.899Z",
          "hidden": false
        },
        {
          "_id": "68a7ca1239413c456c05afc7",
          "name": "Xuewei Wang",
          "hidden": false
        },
        {
          "_id": "68a7ca1239413c456c05afc8",
          "name": "Yuandong Tian",
          "hidden": false
        },
        {
          "_id": "68a7ca1239413c456c05afc9",
          "user": {
            "_id": "64dd8355573d067c9e858262",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64dd8355573d067c9e858262/Bu2kFs6-lcYc93A_SE8WU.jpeg",
            "isPro": false,
            "fullname": "Jiawei Zhao",
            "user": "jiaweizhao",
            "type": "user"
          },
          "name": "Jiawei Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-22T07:22:52.968Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64dd8355573d067c9e858262/UB_URJ70REn1i4nd66ow-.png"
      ],
      "publishedAt": "2025-08-21T05:48:38.000Z",
      "submittedOnDailyAt": "2025-08-22T00:17:29.045Z",
      "title": "Deep Think with Confidence",
      "submittedOnDailyBy": {
        "_id": "64dd8355573d067c9e858262",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64dd8355573d067c9e858262/Bu2kFs6-lcYc93A_SE8WU.jpeg",
        "isPro": false,
        "fullname": "Jiawei Zhao",
        "user": "jiaweizhao",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) have shown great potential in reasoning tasks\nthrough test-time scaling methods like self-consistency with majority voting.\nHowever, this approach often leads to diminishing returns in accuracy and high\ncomputational overhead. To address these challenges, we introduce Deep Think\nwith Confidence (DeepConf), a simple yet powerful method that enhances both\nreasoning efficiency and performance at test time. DeepConf leverages\nmodel-internal confidence signals to dynamically filter out low-quality\nreasoning traces during or after generation. It requires no additional model\ntraining or hyperparameter tuning and can be seamlessly integrated into\nexisting serving frameworks. We evaluate DeepConf across a variety of reasoning\ntasks and the latest open-source models, including Qwen 3 and GPT-OSS series.\nNotably, on challenging benchmarks such as AIME 2025, DeepConf@512 achieves up\nto 99.9% accuracy and reduces generated tokens by up to 84.7% compared to full\nparallel thinking.",
      "upvotes": 14,
      "discussionId": "68a7ca1239413c456c05afca",
      "projectPage": "https://jiaweizzhao.github.io/deepconf/",
      "ai_summary": "DeepConf enhances reasoning efficiency and performance by filtering low-quality reasoning traces using model-internal confidence signals, achieving high accuracy and reducing token generation.",
      "ai_keywords": [
        "Deep Think with Confidence",
        "DeepConf",
        "model-internal confidence signals",
        "reasoning traces",
        "Qwen 3",
        "GPT-OSS series",
        "AIME 2025"
      ]
    },
    "publishedAt": "2025-08-21T01:48:38.000Z",
    "title": "Deep Think with Confidence",
    "summary": "Large Language Models (LLMs) have shown great potential in reasoning tasks\nthrough test-time scaling methods like self-consistency with majority voting.\nHowever, this approach often leads to diminishing returns in accuracy and high\ncomputational overhead. To address these challenges, we introduce Deep Think\nwith Confidence (DeepConf), a simple yet powerful method that enhances both\nreasoning efficiency and performance at test time. DeepConf leverages\nmodel-internal confidence signals to dynamically filter out low-quality\nreasoning traces during or after generation. It requires no additional model\ntraining or hyperparameter tuning and can be seamlessly integrated into\nexisting serving frameworks. We evaluate DeepConf across a variety of reasoning\ntasks and the latest open-source models, including Qwen 3 and GPT-OSS series.\nNotably, on challenging benchmarks such as AIME 2025, DeepConf@512 achieves up\nto 99.9% accuracy and reduces generated tokens by up to 84.7% compared to full\nparallel thinking.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64dd8355573d067c9e858262/UB_URJ70REn1i4nd66ow-.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.15260.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64dd8355573d067c9e858262",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64dd8355573d067c9e858262/Bu2kFs6-lcYc93A_SE8WU.jpeg",
      "fullname": "Jiawei Zhao",
      "name": "jiaweizhao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2508.15769",
      "authors": [
        {
          "_id": "68a7cd5939413c456c05b09e",
          "name": "Yanxu Meng",
          "hidden": false
        },
        {
          "_id": "68a7cd5939413c456c05b09f",
          "user": {
            "_id": "632c7a0d1d303f5f9acf01b8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/632c7a0d1d303f5f9acf01b8/T010IFuCp6UaOeIyWhbCk.jpeg",
            "isPro": false,
            "fullname": "Haoning Wu",
            "user": "haoningwu",
            "type": "user"
          },
          "name": "Haoning Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-22T07:22:01.228Z",
          "hidden": false
        },
        {
          "_id": "68a7cd5939413c456c05b0a0",
          "name": "Ya Zhang",
          "hidden": false
        },
        {
          "_id": "68a7cd5939413c456c05b0a1",
          "name": "Weidi Xie",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/632c7a0d1d303f5f9acf01b8/rmLnc0_Wu60sfTJ6zBQdi.jpeg"
      ],
      "publishedAt": "2025-08-21T17:59:16.000Z",
      "submittedOnDailyAt": "2025-08-22T00:30:58.941Z",
      "title": "SceneGen: Single-Image 3D Scene Generation in One Feedforward Pass",
      "submittedOnDailyBy": {
        "_id": "632c7a0d1d303f5f9acf01b8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/632c7a0d1d303f5f9acf01b8/T010IFuCp6UaOeIyWhbCk.jpeg",
        "isPro": false,
        "fullname": "Haoning Wu",
        "user": "haoningwu",
        "type": "user"
      },
      "summary": "3D content generation has recently attracted significant research interest\ndue to its applications in VR/AR and embodied AI. In this work, we address the\nchallenging task of synthesizing multiple 3D assets within a single scene\nimage. Concretely, our contributions are fourfold: (i) we present SceneGen, a\nnovel framework that takes a scene image and corresponding object masks as\ninput, simultaneously producing multiple 3D assets with geometry and texture.\nNotably, SceneGen operates with no need for optimization or asset retrieval;\n(ii) we introduce a novel feature aggregation module that integrates local and\nglobal scene information from visual and geometric encoders within the feature\nextraction module. Coupled with a position head, this enables the generation of\n3D assets and their relative spatial positions in a single feedforward pass;\n(iii) we demonstrate SceneGen's direct extensibility to multi-image input\nscenarios. Despite being trained solely on single-image inputs, our\narchitectural design enables improved generation performance with multi-image\ninputs; and (iv) extensive quantitative and qualitative evaluations confirm the\nefficiency and robust generation abilities of our approach. We believe this\nparadigm offers a novel solution for high-quality 3D content generation,\npotentially advancing its practical applications in downstream tasks. The code\nand model will be publicly available at: https://mengmouxu.github.io/SceneGen.",
      "upvotes": 5,
      "discussionId": "68a7cd5939413c456c05b0a2",
      "projectPage": "https://mengmouxu.github.io/SceneGen/",
      "githubRepo": "https://github.com/Mengmouxu/SceneGen",
      "ai_summary": "SceneGen generates multiple 3D assets from a single scene image using a novel framework that integrates local and global scene information, enabling efficient and robust 3D content creation.",
      "ai_keywords": [
        "SceneGen",
        "feature aggregation module",
        "visual encoders",
        "geometric encoders",
        "position head",
        "multi-image input",
        "3D content generation",
        "VR/AR",
        "embodied AI"
      ],
      "githubStars": 6
    },
    "publishedAt": "2025-08-21T13:59:16.000Z",
    "title": "SceneGen: Single-Image 3D Scene Generation in One Feedforward Pass",
    "summary": "3D content generation has recently attracted significant research interest\ndue to its applications in VR/AR and embodied AI. In this work, we address the\nchallenging task of synthesizing multiple 3D assets within a single scene\nimage. Concretely, our contributions are fourfold: (i) we present SceneGen, a\nnovel framework that takes a scene image and corresponding object masks as\ninput, simultaneously producing multiple 3D assets with geometry and texture.\nNotably, SceneGen operates with no need for optimization or asset retrieval;\n(ii) we introduce a novel feature aggregation module that integrates local and\nglobal scene information from visual and geometric encoders within the feature\nextraction module. Coupled with a position head, this enables the generation of\n3D assets and their relative spatial positions in a single feedforward pass;\n(iii) we demonstrate SceneGen's direct extensibility to multi-image input\nscenarios. Despite being trained solely on single-image inputs, our\narchitectural design enables improved generation performance with multi-image\ninputs; and (iv) extensive quantitative and qualitative evaluations confirm the\nefficiency and robust generation abilities of our approach. We believe this\nparadigm offers a novel solution for high-quality 3D content generation,\npotentially advancing its practical applications in downstream tasks. The code\nand model will be publicly available at: https://mengmouxu.github.io/SceneGen.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/632c7a0d1d303f5f9acf01b8/rmLnc0_Wu60sfTJ6zBQdi.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.15769.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "632c7a0d1d303f5f9acf01b8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/632c7a0d1d303f5f9acf01b8/T010IFuCp6UaOeIyWhbCk.jpeg",
      "fullname": "Haoning Wu",
      "name": "haoningwu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2508.15760",
      "authors": [
        {
          "_id": "68a80f2939413c456c05b1b6",
          "user": {
            "_id": "66d4f80bbb76fb26ee6298cf",
            "avatarUrl": "/avatars/b341445ecc7f9b1c63b4bcba94d83bd0.svg",
            "isPro": false,
            "fullname": "Ming Yin",
            "user": "Kevin355",
            "type": "user"
          },
          "name": "Ming Yin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-22T07:21:36.904Z",
          "hidden": false
        },
        {
          "_id": "68a80f2939413c456c05b1b7",
          "name": "Dinghan Shen",
          "hidden": false
        },
        {
          "_id": "68a80f2939413c456c05b1b8",
          "name": "Silei Xu",
          "hidden": false
        },
        {
          "_id": "68a80f2939413c456c05b1b9",
          "name": "Jianbing Han",
          "hidden": false
        },
        {
          "_id": "68a80f2939413c456c05b1ba",
          "name": "Sixun Dong",
          "hidden": false
        },
        {
          "_id": "68a80f2939413c456c05b1bb",
          "name": "Mian Zhang",
          "hidden": false
        },
        {
          "_id": "68a80f2939413c456c05b1bc",
          "name": "Yebowen Hu",
          "hidden": false
        },
        {
          "_id": "68a80f2939413c456c05b1bd",
          "name": "Shujian Liu",
          "hidden": false
        },
        {
          "_id": "68a80f2939413c456c05b1be",
          "name": "Simin Ma",
          "hidden": false
        },
        {
          "_id": "68a80f2939413c456c05b1bf",
          "name": "Song Wang",
          "hidden": false
        },
        {
          "_id": "68a80f2939413c456c05b1c0",
          "name": "Sathish Reddy Indurthi",
          "hidden": false
        },
        {
          "_id": "68a80f2939413c456c05b1c1",
          "name": "Xun Wang",
          "hidden": false
        },
        {
          "_id": "68a80f2939413c456c05b1c2",
          "name": "Yiran Chen",
          "hidden": false
        },
        {
          "_id": "68a80f2939413c456c05b1c3",
          "name": "Kaiqiang Song",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/66d4f80bbb76fb26ee6298cf/H-CzXXjGWhKkx1Uxy9GSp.png",
        "https://cdn-uploads.huggingface.co/production/uploads/66d4f80bbb76fb26ee6298cf/a1tLghfsPEN5BTn0hdQqm.png"
      ],
      "publishedAt": "2025-08-21T17:55:54.000Z",
      "submittedOnDailyAt": "2025-08-22T05:16:52.442Z",
      "title": "LiveMCP-101: Stress Testing and Diagnosing MCP-enabled Agents on\n  Challenging Queries",
      "submittedOnDailyBy": {
        "_id": "66d4f80bbb76fb26ee6298cf",
        "avatarUrl": "/avatars/b341445ecc7f9b1c63b4bcba94d83bd0.svg",
        "isPro": false,
        "fullname": "Ming Yin",
        "user": "Kevin355",
        "type": "user"
      },
      "summary": "Tool calling has emerged as a critical capability for AI agents to interact\nwith the real world and solve complex tasks. While the Model Context Protocol\n(MCP) provides a powerful standardized framework for tool integration, there is\na significant gap in benchmarking how well AI agents can effectively solve\nmulti-step tasks using diverse MCP tools in realistic, dynamic scenarios. In\nthis work, we present LiveMCP-101, a benchmark of 101 carefully curated\nreal-world queries, refined through iterative LLM rewriting and manual review,\nthat require coordinated use of multiple MCP tools including web search, file\noperations, mathematical reasoning, and data analysis. Moreover, we introduce a\nnovel evaluation approach that leverages ground-truth execution plans rather\nthan raw API outputs, better reflecting the evolving nature of real-world\nenvironments. Experiments show that even frontier LLMs achieve a success rate\nbelow 60\\%, highlighting major challenges in tool orchestration. Detailed\nablations and error analysis further reveal distinct failure modes and\ninefficiencies in token usage, pointing to concrete directions for advancing\ncurrent models. LiveMCP-101 sets a rigorous standard for evaluating real-world\nagent capabilities, advancing toward autonomous AI systems that reliably\nexecute complex tasks through tool use.",
      "upvotes": 5,
      "discussionId": "68a80f2939413c456c05b1c4",
      "ai_summary": "LiveMCP-101 benchmarks AI agents' ability to use multiple tools in real-world scenarios, revealing challenges in tool orchestration and inefficiencies in token usage.",
      "ai_keywords": [
        "Model Context Protocol",
        "MCP",
        "LiveMCP-101",
        "real-world queries",
        "LLM rewriting",
        "web search",
        "file operations",
        "mathematical reasoning",
        "data analysis",
        "ground-truth execution plans",
        "token usage"
      ]
    },
    "publishedAt": "2025-08-21T13:55:54.000Z",
    "title": "LiveMCP-101: Stress Testing and Diagnosing MCP-enabled Agents on\n  Challenging Queries",
    "summary": "Tool calling has emerged as a critical capability for AI agents to interact\nwith the real world and solve complex tasks. While the Model Context Protocol\n(MCP) provides a powerful standardized framework for tool integration, there is\na significant gap in benchmarking how well AI agents can effectively solve\nmulti-step tasks using diverse MCP tools in realistic, dynamic scenarios. In\nthis work, we present LiveMCP-101, a benchmark of 101 carefully curated\nreal-world queries, refined through iterative LLM rewriting and manual review,\nthat require coordinated use of multiple MCP tools including web search, file\noperations, mathematical reasoning, and data analysis. Moreover, we introduce a\nnovel evaluation approach that leverages ground-truth execution plans rather\nthan raw API outputs, better reflecting the evolving nature of real-world\nenvironments. Experiments show that even frontier LLMs achieve a success rate\nbelow 60\\%, highlighting major challenges in tool orchestration. Detailed\nablations and error analysis further reveal distinct failure modes and\ninefficiencies in token usage, pointing to concrete directions for advancing\ncurrent models. LiveMCP-101 sets a rigorous standard for evaluating real-world\nagent capabilities, advancing toward autonomous AI systems that reliably\nexecute complex tasks through tool use.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/66d4f80bbb76fb26ee6298cf/H-CzXXjGWhKkx1Uxy9GSp.png",
      "https://cdn-uploads.huggingface.co/production/uploads/66d4f80bbb76fb26ee6298cf/a1tLghfsPEN5BTn0hdQqm.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.15760.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66d4f80bbb76fb26ee6298cf",
      "avatarUrl": "/avatars/b341445ecc7f9b1c63b4bcba94d83bd0.svg",
      "fullname": "Ming Yin",
      "name": "Kevin355",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2508.15767",
      "authors": [
        {
          "_id": "68a7f0f139413c456c05b15f",
          "name": "Jinhyung Park",
          "hidden": false
        },
        {
          "_id": "68a7f0f139413c456c05b160",
          "name": "Javier Romero",
          "hidden": false
        },
        {
          "_id": "68a7f0f139413c456c05b161",
          "name": "Shunsuke Saito",
          "hidden": false
        },
        {
          "_id": "68a7f0f139413c456c05b162",
          "name": "Fabian Prada",
          "hidden": false
        },
        {
          "_id": "68a7f0f139413c456c05b163",
          "name": "Takaaki Shiratori",
          "hidden": false
        },
        {
          "_id": "68a7f0f139413c456c05b164",
          "name": "Yichen Xu",
          "hidden": false
        },
        {
          "_id": "68a7f0f139413c456c05b165",
          "name": "Federica Bogo",
          "hidden": false
        },
        {
          "_id": "68a7f0f139413c456c05b166",
          "name": "Shoou-I Yu",
          "hidden": false
        },
        {
          "_id": "68a7f0f139413c456c05b167",
          "name": "Kris Kitani",
          "hidden": false
        },
        {
          "_id": "68a7f0f139413c456c05b168",
          "name": "Rawal Khirodkar",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/WRuT3hgvSsIJGF_qgL85k.png"
      ],
      "publishedAt": "2025-08-21T17:58:56.000Z",
      "submittedOnDailyAt": "2025-08-22T02:54:40.485Z",
      "title": "ATLAS: Decoupling Skeletal and Shape Parameters for Expressive\n  Parametric Human Modeling",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Parametric body models offer expressive 3D representation of humans across a\nwide range of poses, shapes, and facial expressions, typically derived by\nlearning a basis over registered 3D meshes. However, existing human mesh\nmodeling approaches struggle to capture detailed variations across diverse body\nposes and shapes, largely due to limited training data diversity and\nrestrictive modeling assumptions. Moreover, the common paradigm first optimizes\nthe external body surface using a linear basis, then regresses internal\nskeletal joints from surface vertices. This approach introduces problematic\ndependencies between internal skeleton and outer soft tissue, limiting direct\ncontrol over body height and bone lengths. To address these issues, we present\nATLAS, a high-fidelity body model learned from 600k high-resolution scans\ncaptured using 240 synchronized cameras. Unlike previous methods, we explicitly\ndecouple the shape and skeleton bases by grounding our mesh representation in\nthe human skeleton. This decoupling enables enhanced shape expressivity,\nfine-grained customization of body attributes, and keypoint fitting independent\nof external soft-tissue characteristics. ATLAS outperforms existing methods by\nfitting unseen subjects in diverse poses more accurately, and quantitative\nevaluations show that our non-linear pose correctives more effectively capture\ncomplex poses compared to linear models.",
      "upvotes": 3,
      "discussionId": "68a7f0f139413c456c05b169",
      "projectPage": "https://jindapark.github.io/projects/atlas/",
      "ai_summary": "ATLAS is a high-fidelity body model that decouples shape and skeleton bases, improving pose accuracy and shape expressivity using 600k high-resolution scans.",
      "ai_keywords": [
        "parametric body models",
        "3D representation",
        "human mesh modeling",
        "basis",
        "registered 3D meshes",
        "training data diversity",
        "modeling assumptions",
        "external body surface",
        "linear basis",
        "internal skeletal joints",
        "surface vertices",
        "decoupling",
        "human skeleton",
        "shape expressivity",
        "body attributes",
        "keypoint fitting",
        "non-linear pose correctives",
        "complex poses"
      ]
    },
    "publishedAt": "2025-08-21T13:58:56.000Z",
    "title": "ATLAS: Decoupling Skeletal and Shape Parameters for Expressive\n  Parametric Human Modeling",
    "summary": "Parametric body models offer expressive 3D representation of humans across a\nwide range of poses, shapes, and facial expressions, typically derived by\nlearning a basis over registered 3D meshes. However, existing human mesh\nmodeling approaches struggle to capture detailed variations across diverse body\nposes and shapes, largely due to limited training data diversity and\nrestrictive modeling assumptions. Moreover, the common paradigm first optimizes\nthe external body surface using a linear basis, then regresses internal\nskeletal joints from surface vertices. This approach introduces problematic\ndependencies between internal skeleton and outer soft tissue, limiting direct\ncontrol over body height and bone lengths. To address these issues, we present\nATLAS, a high-fidelity body model learned from 600k high-resolution scans\ncaptured using 240 synchronized cameras. Unlike previous methods, we explicitly\ndecouple the shape and skeleton bases by grounding our mesh representation in\nthe human skeleton. This decoupling enables enhanced shape expressivity,\nfine-grained customization of body attributes, and keypoint fitting independent\nof external soft-tissue characteristics. ATLAS outperforms existing methods by\nfitting unseen subjects in diverse poses more accurately, and quantitative\nevaluations show that our non-linear pose correctives more effectively capture\ncomplex poses compared to linear models.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/WRuT3hgvSsIJGF_qgL85k.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.15767.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 95
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.15761",
      "authors": [
        {
          "_id": "68a7f15139413c456c05b16b",
          "name": "Yifu Zhang",
          "hidden": false
        },
        {
          "_id": "68a7f15139413c456c05b16c",
          "name": "Hao Yang",
          "hidden": false
        },
        {
          "_id": "68a7f15139413c456c05b16d",
          "name": "Yuqi Zhang",
          "hidden": false
        },
        {
          "_id": "68a7f15139413c456c05b16e",
          "name": "Yifei Hu",
          "hidden": false
        },
        {
          "_id": "68a7f15139413c456c05b16f",
          "name": "Fengda Zhu",
          "hidden": false
        },
        {
          "_id": "68a7f15139413c456c05b170",
          "name": "Chuang Lin",
          "hidden": false
        },
        {
          "_id": "68a7f15139413c456c05b171",
          "name": "Xiaofeng Mei",
          "hidden": false
        },
        {
          "_id": "68a7f15139413c456c05b172",
          "name": "Yi Jiang",
          "hidden": false
        },
        {
          "_id": "68a7f15139413c456c05b173",
          "name": "Zehuan Yuan",
          "hidden": false
        },
        {
          "_id": "68a7f15139413c456c05b174",
          "name": "Bingyue Peng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-21T17:56:10.000Z",
      "submittedOnDailyAt": "2025-08-22T02:56:12.480Z",
      "title": "Waver: Wave Your Way to Lifelike Video Generation",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We present Waver, a high-performance foundation model for unified image and\nvideo generation. Waver can directly generate videos with durations ranging\nfrom 5 to 10 seconds at a native resolution of 720p, which are subsequently\nupscaled to 1080p. The model simultaneously supports text-to-video (T2V),\nimage-to-video (I2V), and text-to-image (T2I) generation within a single,\nintegrated framework. We introduce a Hybrid Stream DiT architecture to enhance\nmodality alignment and accelerate training convergence. To ensure training data\nquality, we establish a comprehensive data curation pipeline and manually\nannotate and train an MLLM-based video quality model to filter for the\nhighest-quality samples. Furthermore, we provide detailed training and\ninference recipes to facilitate the generation of high-quality videos. Building\non these contributions, Waver excels at capturing complex motion, achieving\nsuperior motion amplitude and temporal consistency in video synthesis. Notably,\nit ranks among the Top 3 on both the T2V and I2V leaderboards at Artificial\nAnalysis (data as of 2025-07-30 10:00 GMT+8), consistently outperforming\nexisting open-source models and matching or surpassing state-of-the-art\ncommercial solutions. We hope this technical report will help the community\nmore efficiently train high-quality video generation models and accelerate\nprogress in video generation technologies. Official page:\nhttps://github.com/FoundationVision/Waver.",
      "upvotes": 2,
      "discussionId": "68a7f15139413c456c05b175",
      "githubRepo": "https://github.com/FoundationVision/Waver",
      "ai_summary": "Waver, a high-performance foundation model, generates high-quality videos and images using a Hybrid Stream DiT architecture, excelling in text-to-video, image-to-video, and text-to-image tasks.",
      "ai_keywords": [
        "Hybrid Stream DiT",
        "modality alignment",
        "training convergence",
        "MLLM-based video quality model",
        "superior motion amplitude",
        "temporal consistency",
        "T2V",
        "I2V",
        "T2I"
      ],
      "githubStars": 211
    },
    "publishedAt": "2025-08-21T13:56:10.000Z",
    "title": "Waver: Wave Your Way to Lifelike Video Generation",
    "summary": "We present Waver, a high-performance foundation model for unified image and\nvideo generation. Waver can directly generate videos with durations ranging\nfrom 5 to 10 seconds at a native resolution of 720p, which are subsequently\nupscaled to 1080p. The model simultaneously supports text-to-video (T2V),\nimage-to-video (I2V), and text-to-image (T2I) generation within a single,\nintegrated framework. We introduce a Hybrid Stream DiT architecture to enhance\nmodality alignment and accelerate training convergence. To ensure training data\nquality, we establish a comprehensive data curation pipeline and manually\nannotate and train an MLLM-based video quality model to filter for the\nhighest-quality samples. Furthermore, we provide detailed training and\ninference recipes to facilitate the generation of high-quality videos. Building\non these contributions, Waver excels at capturing complex motion, achieving\nsuperior motion amplitude and temporal consistency in video synthesis. Notably,\nit ranks among the Top 3 on both the T2V and I2V leaderboards at Artificial\nAnalysis (data as of 2025-07-30 10:00 GMT+8), consistently outperforming\nexisting open-source models and matching or surpassing state-of-the-art\ncommercial solutions. We hope this technical report will help the community\nmore efficiently train high-quality video generation models and accelerate\nprogress in video generation technologies. Official page:\nhttps://github.com/FoundationVision/Waver.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.15761.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 95
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.15752",
      "authors": [
        {
          "_id": "68a7f31139413c456c05b195",
          "name": "Jon E. Froehlich",
          "hidden": false
        },
        {
          "_id": "68a7f31139413c456c05b196",
          "name": "Jared Hwang",
          "hidden": false
        },
        {
          "_id": "68a7f31139413c456c05b197",
          "name": "Zeyu Wang",
          "hidden": false
        },
        {
          "_id": "68a7f31139413c456c05b198",
          "name": "John S. O'Meara",
          "hidden": false
        },
        {
          "_id": "68a7f31139413c456c05b199",
          "name": "Xia Su",
          "hidden": false
        },
        {
          "_id": "68a7f31139413c456c05b19a",
          "name": "William Huang",
          "hidden": false
        },
        {
          "_id": "68a7f31139413c456c05b19b",
          "name": "Yang Zhang",
          "hidden": false
        },
        {
          "_id": "68a7f31139413c456c05b19c",
          "name": "Alex Fiannaca",
          "hidden": false
        },
        {
          "_id": "68a7f31139413c456c05b19d",
          "name": "Philip Nelson",
          "hidden": false
        },
        {
          "_id": "68a7f31139413c456c05b19e",
          "name": "Shaun Kane",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-21T17:49:52.000Z",
      "submittedOnDailyAt": "2025-08-22T03:03:35.215Z",
      "title": "\"Does the cafe entrance look accessible? Where is the door?\" Towards\n  Geospatial AI Agents for Visual Inquiries",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Interactive digital maps have revolutionized how people travel and learn\nabout the world; however, they rely on pre-existing structured data in GIS\ndatabases (e.g., road networks, POI indices), limiting their ability to address\ngeo-visual questions related to what the world looks like. We introduce our\nvision for Geo-Visual Agents--multimodal AI agents capable of understanding and\nresponding to nuanced visual-spatial inquiries about the world by analyzing\nlarge-scale repositories of geospatial images, including streetscapes (e.g.,\nGoogle Street View), place-based photos (e.g., TripAdvisor, Yelp), and aerial\nimagery (e.g., satellite photos) combined with traditional GIS data sources. We\ndefine our vision, describe sensing and interaction approaches, provide three\nexemplars, and enumerate key challenges and opportunities for future work.",
      "upvotes": 1,
      "discussionId": "68a7f31139413c456c05b19f",
      "ai_summary": "Geo-Visual Agents integrate geospatial images and GIS data to answer nuanced visual-spatial inquiries about the world.",
      "ai_keywords": [
        "Geo-Visual Agents",
        "multimodal AI agents",
        "geospatial images",
        "streetscapes",
        "place-based photos",
        "aerial imagery",
        "GIS data sources"
      ]
    },
    "publishedAt": "2025-08-21T13:49:52.000Z",
    "title": "\"Does the cafe entrance look accessible? Where is the door?\" Towards\n  Geospatial AI Agents for Visual Inquiries",
    "summary": "Interactive digital maps have revolutionized how people travel and learn\nabout the world; however, they rely on pre-existing structured data in GIS\ndatabases (e.g., road networks, POI indices), limiting their ability to address\ngeo-visual questions related to what the world looks like. We introduce our\nvision for Geo-Visual Agents--multimodal AI agents capable of understanding and\nresponding to nuanced visual-spatial inquiries about the world by analyzing\nlarge-scale repositories of geospatial images, including streetscapes (e.g.,\nGoogle Street View), place-based photos (e.g., TripAdvisor, Yelp), and aerial\nimagery (e.g., satellite photos) combined with traditional GIS data sources. We\ndefine our vision, describe sensing and interaction approaches, provide three\nexemplars, and enumerate key challenges and opportunities for future work.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.15752.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 95
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.15361",
      "authors": [
        {
          "_id": "68a7f2a139413c456c05b17c",
          "name": "Shiwen Ni",
          "hidden": false
        },
        {
          "_id": "68a7f2a139413c456c05b17d",
          "name": "Guhong Chen",
          "hidden": false
        },
        {
          "_id": "68a7f2a139413c456c05b17e",
          "name": "Shuaimin Li",
          "hidden": false
        },
        {
          "_id": "68a7f2a139413c456c05b17f",
          "name": "Xuanang Chen",
          "hidden": false
        },
        {
          "_id": "68a7f2a139413c456c05b180",
          "name": "Siyi Li",
          "hidden": false
        },
        {
          "_id": "68a7f2a139413c456c05b181",
          "name": "Bingli Wang",
          "hidden": false
        },
        {
          "_id": "68a7f2a139413c456c05b182",
          "name": "Qiyao Wang",
          "hidden": false
        },
        {
          "_id": "68a7f2a139413c456c05b183",
          "name": "Xingjian Wang",
          "hidden": false
        },
        {
          "_id": "68a7f2a139413c456c05b184",
          "name": "Yifan Zhang",
          "hidden": false
        },
        {
          "_id": "68a7f2a139413c456c05b185",
          "name": "Liyang Fan",
          "hidden": false
        },
        {
          "_id": "68a7f2a139413c456c05b186",
          "name": "Chengming Li",
          "hidden": false
        },
        {
          "_id": "68a7f2a139413c456c05b187",
          "name": "Ruifeng Xu",
          "hidden": false
        },
        {
          "_id": "68a7f2a139413c456c05b188",
          "name": "Le Sun",
          "hidden": false
        },
        {
          "_id": "68a7f2a139413c456c05b189",
          "name": "Min Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-21T08:43:35.000Z",
      "submittedOnDailyAt": "2025-08-22T03:01:42.364Z",
      "title": "A Survey on Large Language Model Benchmarks",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "In recent years, with the rapid development of the depth and breadth of large\nlanguage models' capabilities, various corresponding evaluation benchmarks have\nbeen emerging in increasing numbers. As a quantitative assessment tool for\nmodel performance, benchmarks are not only a core means to measure model\ncapabilities but also a key element in guiding the direction of model\ndevelopment and promoting technological innovation. We systematically review\nthe current status and development of large language model benchmarks for the\nfirst time, categorizing 283 representative benchmarks into three categories:\ngeneral capabilities, domain-specific, and target-specific. General capability\nbenchmarks cover aspects such as core linguistics, knowledge, and reasoning;\ndomain-specific benchmarks focus on fields like natural sciences, humanities\nand social sciences, and engineering technology; target-specific benchmarks pay\nattention to risks, reliability, agents, etc. We point out that current\nbenchmarks have problems such as inflated scores caused by data contamination,\nunfair evaluation due to cultural and linguistic biases, and lack of evaluation\non process credibility and dynamic environments, and provide a referable design\nparadigm for future benchmark innovation.",
      "upvotes": 1,
      "discussionId": "68a7f2a139413c456c05b18a",
      "ai_summary": "A systematic review of large language model benchmarks identifies issues such as data contamination, cultural biases, and lack of process credibility, and proposes a design paradigm for future improvements.",
      "ai_keywords": [
        "large language models",
        "evaluation benchmarks",
        "general capabilities",
        "domain-specific",
        "target-specific",
        "core linguistics",
        "knowledge",
        "reasoning",
        "natural sciences",
        "humanities",
        "social sciences",
        "engineering technology",
        "data contamination",
        "cultural biases",
        "process credibility",
        "dynamic environments"
      ]
    },
    "publishedAt": "2025-08-21T04:43:35.000Z",
    "title": "A Survey on Large Language Model Benchmarks",
    "summary": "In recent years, with the rapid development of the depth and breadth of large\nlanguage models' capabilities, various corresponding evaluation benchmarks have\nbeen emerging in increasing numbers. As a quantitative assessment tool for\nmodel performance, benchmarks are not only a core means to measure model\ncapabilities but also a key element in guiding the direction of model\ndevelopment and promoting technological innovation. We systematically review\nthe current status and development of large language model benchmarks for the\nfirst time, categorizing 283 representative benchmarks into three categories:\ngeneral capabilities, domain-specific, and target-specific. General capability\nbenchmarks cover aspects such as core linguistics, knowledge, and reasoning;\ndomain-specific benchmarks focus on fields like natural sciences, humanities\nand social sciences, and engineering technology; target-specific benchmarks pay\nattention to risks, reliability, agents, etc. We point out that current\nbenchmarks have problems such as inflated scores caused by data contamination,\nunfair evaluation due to cultural and linguistic biases, and lack of evaluation\non process credibility and dynamic environments, and provide a referable design\nparadigm for future benchmark innovation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.15361.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 95
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.15202",
      "authors": [
        {
          "_id": "68a7cd6f39413c456c05b0a4",
          "name": "Yuanchen Zhou",
          "hidden": false
        },
        {
          "_id": "68a7cd6f39413c456c05b0a5",
          "name": "Shuo Jiang",
          "hidden": false
        },
        {
          "_id": "68a7cd6f39413c456c05b0a6",
          "user": {
            "_id": "642656cbad1e3b0e6e91b752",
            "avatarUrl": "/avatars/3bf0ee15fd528e09b2b889f5cce3cbd0.svg",
            "isPro": false,
            "fullname": "Jie Zhu",
            "user": "amazingj",
            "type": "user"
          },
          "name": "Jie Zhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-22T07:21:59.230Z",
          "hidden": false
        },
        {
          "_id": "68a7cd6f39413c456c05b0a7",
          "name": "Junhui Li",
          "hidden": false
        },
        {
          "_id": "68a7cd6f39413c456c05b0a8",
          "name": "Lifan Guo",
          "hidden": false
        },
        {
          "_id": "68a7cd6f39413c456c05b0a9",
          "name": "Feng Chen",
          "hidden": false
        },
        {
          "_id": "68a7cd6f39413c456c05b0aa",
          "name": "Chi Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-21T03:31:11.000Z",
      "submittedOnDailyAt": "2025-08-22T00:23:11.006Z",
      "title": "Fin-PRM: A Domain-Specialized Process Reward Model for Financial\n  Reasoning in Large Language Models",
      "submittedOnDailyBy": {
        "_id": "642656cbad1e3b0e6e91b752",
        "avatarUrl": "/avatars/3bf0ee15fd528e09b2b889f5cce3cbd0.svg",
        "isPro": false,
        "fullname": "Jie Zhu",
        "user": "amazingj",
        "type": "user"
      },
      "summary": "Process Reward Models (PRMs) have emerged as a promising framework for\nsupervising intermediate reasoning in large language models (LLMs), yet\nexisting PRMs are primarily trained on general or Science, Technology,\nEngineering, and Mathematics (STEM) domains and fall short in domain-specific\ncontexts such as finance, where reasoning is more structured, symbolic, and\nsensitive to factual and regulatory correctness. We introduce Fin-PRM,\na domain-specialized, trajectory-aware PRM tailored to evaluate intermediate\nreasoning steps in financial tasks. Fin-PRM integrates step-level and\ntrajectory-level reward supervision, enabling fine-grained evaluation of\nreasoning traces aligned with financial logic. We apply Fin-PRM in both offline\nand online reward learning settings, supporting three key applications: (i)\nselecting high-quality reasoning trajectories for distillation-based supervised\nfine-tuning, (ii) providing dense process-level rewards for reinforcement\nlearning, and (iii) guiding reward-informed Best-of-N inference at test time.\nExperimental results on financial reasoning benchmarks, including CFLUE and\nFinQA, demonstrate that Fin-PRM consistently outperforms general-purpose PRMs\nand strong domain baselines in trajectory selection quality. Downstream models\ntrained with Fin-PRM yield substantial improvements with baselines, with gains\nof 12.9\\% in supervised learning, 5.2\\% in reinforcement learning, and 5.1\\% in\ntest-time performance. These findings highlight the value of domain-specialized\nreward modeling for aligning LLMs with expert-level financial reasoning. Our\nproject resources will be available at https://github.com/aliyun/qwen-dianjin.",
      "upvotes": 1,
      "discussionId": "68a7cd6f39413c456c05b0ab",
      "projectPage": "https://github.com/aliyun/qwen-dianjin",
      "ai_summary": "Fin-PRM, a domain-specialized reward model for finance, enhances intermediate reasoning in LLMs through step-level and trajectory-level supervision, improving performance in supervised learning, reinforcement learning, and test-time inference.",
      "ai_keywords": [
        "Process Reward Models",
        "PRMs",
        "large language models",
        "LLMs",
        "domain-specific",
        "finance",
        "structured reasoning",
        "symbolic reasoning",
        "factual correctness",
        "regulatory correctness",
        "trajectory-aware",
        "step-level supervision",
        "trajectory-level supervision",
        "reasoning traces",
        "financial logic",
        "distillation-based supervised fine-tuning",
        "dense process-level rewards",
        "reinforcement learning",
        "reward-informed Best-of-N inference",
        "CFLUE",
        "FinQA",
        "trajectory selection quality",
        "supervised learning",
        "test-time performance"
      ]
    },
    "publishedAt": "2025-08-20T23:31:11.000Z",
    "title": "Fin-PRM: A Domain-Specialized Process Reward Model for Financial\n  Reasoning in Large Language Models",
    "summary": "Process Reward Models (PRMs) have emerged as a promising framework for\nsupervising intermediate reasoning in large language models (LLMs), yet\nexisting PRMs are primarily trained on general or Science, Technology,\nEngineering, and Mathematics (STEM) domains and fall short in domain-specific\ncontexts such as finance, where reasoning is more structured, symbolic, and\nsensitive to factual and regulatory correctness. We introduce Fin-PRM,\na domain-specialized, trajectory-aware PRM tailored to evaluate intermediate\nreasoning steps in financial tasks. Fin-PRM integrates step-level and\ntrajectory-level reward supervision, enabling fine-grained evaluation of\nreasoning traces aligned with financial logic. We apply Fin-PRM in both offline\nand online reward learning settings, supporting three key applications: (i)\nselecting high-quality reasoning trajectories for distillation-based supervised\nfine-tuning, (ii) providing dense process-level rewards for reinforcement\nlearning, and (iii) guiding reward-informed Best-of-N inference at test time.\nExperimental results on financial reasoning benchmarks, including CFLUE and\nFinQA, demonstrate that Fin-PRM consistently outperforms general-purpose PRMs\nand strong domain baselines in trajectory selection quality. Downstream models\ntrained with Fin-PRM yield substantial improvements with baselines, with gains\nof 12.9\\% in supervised learning, 5.2\\% in reinforcement learning, and 5.1\\% in\ntest-time performance. These findings highlight the value of domain-specialized\nreward modeling for aligning LLMs with expert-level financial reasoning. Our\nproject resources will be available at https://github.com/aliyun/qwen-dianjin.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.15202.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642656cbad1e3b0e6e91b752",
      "avatarUrl": "/avatars/3bf0ee15fd528e09b2b889f5cce3cbd0.svg",
      "fullname": "Jie Zhu",
      "name": "amazingj",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2508.15126",
      "authors": [
        {
          "_id": "68a8113e39413c456c05b1dc",
          "name": "Pengsong Zhang",
          "hidden": false
        },
        {
          "_id": "68a8113e39413c456c05b1dd",
          "name": "Xiang Hu",
          "hidden": false
        },
        {
          "_id": "68a8113e39413c456c05b1de",
          "name": "Guowei Huang",
          "hidden": false
        },
        {
          "_id": "68a8113e39413c456c05b1df",
          "name": "Yang Qi",
          "hidden": false
        },
        {
          "_id": "68a8113e39413c456c05b1e0",
          "name": "Heng Zhang",
          "hidden": false
        },
        {
          "_id": "68a8113e39413c456c05b1e1",
          "name": "Xiuxu Li",
          "hidden": false
        },
        {
          "_id": "68a8113e39413c456c05b1e2",
          "name": "Jiaxing Song",
          "hidden": false
        },
        {
          "_id": "68a8113e39413c456c05b1e3",
          "name": "Jiabin Luo",
          "hidden": false
        },
        {
          "_id": "68a8113e39413c456c05b1e4",
          "name": "Yijiang Li",
          "hidden": false
        },
        {
          "_id": "68a8113e39413c456c05b1e5",
          "name": "Shuo Yin",
          "hidden": false
        },
        {
          "_id": "68a8113e39413c456c05b1e6",
          "name": "Chengxiao Dai",
          "hidden": false
        },
        {
          "_id": "68a8113e39413c456c05b1e7",
          "name": "Eric Hanchen Jiang",
          "hidden": false
        },
        {
          "_id": "68a8113e39413c456c05b1e8",
          "name": "Xiaoyan Zhou",
          "hidden": false
        },
        {
          "_id": "68a8113e39413c456c05b1e9",
          "name": "Zhenfei Yin",
          "hidden": false
        },
        {
          "_id": "68a8113e39413c456c05b1ea",
          "name": "Boqin Yuan",
          "hidden": false
        },
        {
          "_id": "68a8113e39413c456c05b1eb",
          "name": "Jing Dong",
          "hidden": false
        },
        {
          "_id": "68a8113e39413c456c05b1ec",
          "name": "Guinan Su",
          "hidden": false
        },
        {
          "_id": "68a8113e39413c456c05b1ed",
          "name": "Guanren Qiao",
          "hidden": false
        },
        {
          "_id": "68a8113e39413c456c05b1ee",
          "name": "Haiming Tang",
          "hidden": false
        },
        {
          "_id": "68a8113e39413c456c05b1ef",
          "name": "Anghong Du",
          "hidden": false
        },
        {
          "_id": "68a8113e39413c456c05b1f0",
          "name": "Lili Pan",
          "hidden": false
        },
        {
          "_id": "68a8113e39413c456c05b1f1",
          "name": "Zhenzhong Lan",
          "hidden": false
        },
        {
          "_id": "68a8113e39413c456c05b1f2",
          "name": "Xinyu Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-20T23:16:41.000Z",
      "submittedOnDailyAt": "2025-08-22T05:13:19.798Z",
      "title": "aiXiv: A Next-Generation Open Access Ecosystem for Scientific Discovery\n  Generated by AI Scientists",
      "submittedOnDailyBy": {
        "_id": "655fb8a122ce47e5fa491c72",
        "avatarUrl": "/avatars/d320fe777649b68fbd1372865a2f4def.svg",
        "isPro": false,
        "fullname": "Pengsong Zhang",
        "user": "universea",
        "type": "user"
      },
      "summary": "Recent advances in large language models (LLMs) have enabled AI agents to\nautonomously generate scientific proposals, conduct experiments, author papers,\nand perform peer reviews. Yet this flood of AI-generated research content\ncollides with a fragmented and largely closed publication ecosystem.\nTraditional journals and conferences rely on human peer review, making them\ndifficult to scale and often reluctant to accept AI-generated research content;\nexisting preprint servers (e.g. arXiv) lack rigorous quality-control\nmechanisms. Consequently, a significant amount of high-quality AI-generated\nresearch lacks appropriate venues for dissemination, hindering its potential to\nadvance scientific progress. To address these challenges, we introduce aiXiv, a\nnext-generation open-access platform for human and AI scientists. Its\nmulti-agent architecture allows research proposals and papers to be submitted,\nreviewed, and iteratively refined by both human and AI scientists. It also\nprovides API and MCP interfaces that enable seamless integration of\nheterogeneous human and AI scientists, creating a scalable and extensible\necosystem for autonomous scientific discovery. Through extensive experiments,\nwe demonstrate that aiXiv is a reliable and robust platform that significantly\nenhances the quality of AI-generated research proposals and papers after\niterative revising and reviewing on aiXiv. Our work lays the groundwork for a\nnext-generation open-access ecosystem for AI scientists, accelerating the\npublication and dissemination of high-quality AI-generated research content.\nCode is available at https://github.com/aixiv-org. Website is available at\nhttps://forms.gle/DxQgCtXFsJ4paMtn8.",
      "upvotes": 1,
      "discussionId": "68a8113f39413c456c05b1f3",
      "ai_summary": "aiXiv is an open-access platform that facilitates the submission, review, and refinement of scientific proposals and papers by both human and AI scientists, enhancing the quality and dissemination of AI-generated research.",
      "ai_keywords": [
        "large language models",
        "LLMS",
        "autonomous scientific discovery",
        "aiXiv",
        "multi-agent architecture",
        "API",
        "MCP interfaces",
        "iterative revising and reviewing"
      ]
    },
    "publishedAt": "2025-08-20T19:16:41.000Z",
    "title": "aiXiv: A Next-Generation Open Access Ecosystem for Scientific Discovery\n  Generated by AI Scientists",
    "summary": "Recent advances in large language models (LLMs) have enabled AI agents to\nautonomously generate scientific proposals, conduct experiments, author papers,\nand perform peer reviews. Yet this flood of AI-generated research content\ncollides with a fragmented and largely closed publication ecosystem.\nTraditional journals and conferences rely on human peer review, making them\ndifficult to scale and often reluctant to accept AI-generated research content;\nexisting preprint servers (e.g. arXiv) lack rigorous quality-control\nmechanisms. Consequently, a significant amount of high-quality AI-generated\nresearch lacks appropriate venues for dissemination, hindering its potential to\nadvance scientific progress. To address these challenges, we introduce aiXiv, a\nnext-generation open-access platform for human and AI scientists. Its\nmulti-agent architecture allows research proposals and papers to be submitted,\nreviewed, and iteratively refined by both human and AI scientists. It also\nprovides API and MCP interfaces that enable seamless integration of\nheterogeneous human and AI scientists, creating a scalable and extensible\necosystem for autonomous scientific discovery. Through extensive experiments,\nwe demonstrate that aiXiv is a reliable and robust platform that significantly\nenhances the quality of AI-generated research proposals and papers after\niterative revising and reviewing on aiXiv. Our work lays the groundwork for a\nnext-generation open-access ecosystem for AI scientists, accelerating the\npublication and dissemination of high-quality AI-generated research content.\nCode is available at https://github.com/aixiv-org. Website is available at\nhttps://forms.gle/DxQgCtXFsJ4paMtn8.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.15126.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "655fb8a122ce47e5fa491c72",
      "avatarUrl": "/avatars/d320fe777649b68fbd1372865a2f4def.svg",
      "fullname": "Pengsong Zhang",
      "name": "universea",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.15641",
      "authors": [
        {
          "_id": "68a7f23039413c456c05b177",
          "name": "Pengcheng Fang",
          "hidden": false
        },
        {
          "_id": "68a7f23039413c456c05b178",
          "name": "Yuxia Chen",
          "hidden": false
        },
        {
          "_id": "68a7f23039413c456c05b179",
          "name": "Rui Guo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-21T15:12:14.000Z",
      "submittedOnDailyAt": "2025-08-22T02:59:57.965Z",
      "title": "When and What: Diffusion-Grounded VideoLLM with Entity Aware\n  Segmentation for Long Video Understanding",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Understanding videos requires more than answering open ended questions, it\ndemands the ability to pinpoint when events occur and how entities interact\nacross time. While recent Video LLMs have achieved remarkable progress in\nholistic reasoning, they remain coarse in temporal perception: timestamps are\nencoded only implicitly, frame level features are weak in capturing continuity,\nand language vision alignment often drifts from the entities of interest. In\nthis paper, we present Grounded VideoDiT, a Video LLM designed to overcome\nthese limitations by introducing three key innovations. First, a Diffusion\nTemporal Latent (DTL) encoder enhances boundary sensitivity and maintains\ntemporal consistency. Second, object grounded representations explicitly bind\nquery entities to localized visual evidence, strengthening alignment. Third, a\nmixed token scheme with discrete temporal tokens provides explicit timestamp\nmodeling, enabling fine grained temporal reasoning. Together, these designs\nequip Grounded VideoDiT with robust grounding capabilities, as validated by\nstate of the art results on Charades STA, NExT GQA, and multiple VideoQA\nbenchmarks.",
      "upvotes": 0,
      "discussionId": "68a7f23139413c456c05b17a",
      "ai_summary": "Grounded VideoDiT addresses temporal perception and entity interaction in videos through a Diffusion Temporal Latent encoder, object grounded representations, and mixed token scheme, achieving state-of-the-art results on VideoQA benchmarks.",
      "ai_keywords": [
        "Diffusion Temporal Latent",
        "object grounded representations",
        "mixed token scheme",
        "discrete temporal tokens",
        "Charades STA",
        "NExT GQA",
        "VideoQA benchmarks"
      ]
    },
    "publishedAt": "2025-08-21T11:12:14.000Z",
    "title": "When and What: Diffusion-Grounded VideoLLM with Entity Aware\n  Segmentation for Long Video Understanding",
    "summary": "Understanding videos requires more than answering open ended questions, it\ndemands the ability to pinpoint when events occur and how entities interact\nacross time. While recent Video LLMs have achieved remarkable progress in\nholistic reasoning, they remain coarse in temporal perception: timestamps are\nencoded only implicitly, frame level features are weak in capturing continuity,\nand language vision alignment often drifts from the entities of interest. In\nthis paper, we present Grounded VideoDiT, a Video LLM designed to overcome\nthese limitations by introducing three key innovations. First, a Diffusion\nTemporal Latent (DTL) encoder enhances boundary sensitivity and maintains\ntemporal consistency. Second, object grounded representations explicitly bind\nquery entities to localized visual evidence, strengthening alignment. Third, a\nmixed token scheme with discrete temporal tokens provides explicit timestamp\nmodeling, enabling fine grained temporal reasoning. Together, these designs\nequip Grounded VideoDiT with robust grounding capabilities, as validated by\nstate of the art results on Charades STA, NExT GQA, and multiple VideoQA\nbenchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.15641.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 95
    },
    "isAuthorParticipating": false
  }
]