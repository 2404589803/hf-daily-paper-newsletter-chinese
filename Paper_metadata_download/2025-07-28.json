[
  {
    "paper": {
      "id": "2507.16075",
      "authors": [
        {
          "_id": "688501d57d7a19a208cdf03a",
          "name": "Rujun Han",
          "hidden": false
        },
        {
          "_id": "688501d57d7a19a208cdf03b",
          "name": "Yanfei Chen",
          "hidden": false
        },
        {
          "_id": "688501d57d7a19a208cdf03c",
          "name": "Zoey CuiZhu",
          "hidden": false
        },
        {
          "_id": "688501d57d7a19a208cdf03d",
          "name": "Lesly Miculicich",
          "hidden": false
        },
        {
          "_id": "688501d57d7a19a208cdf03e",
          "name": "Guan Sun",
          "hidden": false
        },
        {
          "_id": "688501d57d7a19a208cdf03f",
          "name": "Yuanjun Bi",
          "hidden": false
        },
        {
          "_id": "688501d57d7a19a208cdf040",
          "name": "Weiming Wen",
          "hidden": false
        },
        {
          "_id": "688501d57d7a19a208cdf041",
          "name": "Hui Wan",
          "hidden": false
        },
        {
          "_id": "688501d57d7a19a208cdf042",
          "name": "Chunfeng Wen",
          "hidden": false
        },
        {
          "_id": "688501d57d7a19a208cdf043",
          "name": "Solène Maître",
          "hidden": false
        },
        {
          "_id": "688501d57d7a19a208cdf044",
          "name": "George Lee",
          "hidden": false
        },
        {
          "_id": "688501d57d7a19a208cdf045",
          "name": "Vishy Tirumalashetty",
          "hidden": false
        },
        {
          "_id": "688501d57d7a19a208cdf046",
          "name": "Emily Xue",
          "hidden": false
        },
        {
          "_id": "688501d57d7a19a208cdf047",
          "name": "Zizhao Zhang",
          "hidden": false
        },
        {
          "_id": "688501d57d7a19a208cdf048",
          "name": "Salem Haykal",
          "hidden": false
        },
        {
          "_id": "688501d57d7a19a208cdf049",
          "name": "Burak Gokturk",
          "hidden": false
        },
        {
          "_id": "688501d57d7a19a208cdf04a",
          "name": "Tomas Pfister",
          "hidden": false
        },
        {
          "_id": "688501d57d7a19a208cdf04b",
          "name": "Chen-Yu Lee",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-21T21:23:21.000Z",
      "submittedOnDailyAt": "2025-07-28T05:49:28.861Z",
      "title": "Deep Researcher with Test-Time Diffusion",
      "submittedOnDailyBy": {
        "_id": "62f32eab52ad88c930bb3f3b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677134945205-62f32eab52ad88c930bb3f3b.png",
        "isPro": true,
        "fullname": "Asankhaya Sharma",
        "user": "codelion",
        "type": "user"
      },
      "summary": "Deep research agents, powered by Large Language Models (LLMs), are rapidly\nadvancing; yet, their performance often plateaus when generating complex,\nlong-form research reports using generic test-time scaling algorithms. Drawing\ninspiration from the iterative nature of human research, which involves cycles\nof searching, reasoning, and revision, we propose the Test-Time Diffusion Deep\nResearcher (TTD-DR). This novel framework conceptualizes research report\ngeneration as a diffusion process. TTD-DR initiates this process with a\npreliminary draft, an updatable skeleton that serves as an evolving foundation\nto guide the research direction. The draft is then iteratively refined through\na \"denoising\" process, which is dynamically informed by a retrieval mechanism\nthat incorporates external information at each step. The core process is\nfurther enhanced by a self-evolutionary algorithm applied to each component of\nthe agentic workflow, ensuring the generation of high-quality context for the\ndiffusion process. This draft-centric design makes the report writing process\nmore timely and coherent while reducing information loss during the iterative\nsearch process. We demonstrate that our TTD-DR achieves state-of-the-art\nresults on a wide array of benchmarks that require intensive search and\nmulti-hop reasoning, significantly outperforming existing deep research agents.",
      "upvotes": 2,
      "discussionId": "688501d67d7a19a208cdf04c",
      "githubRepo": "https://github.com/codelion/optillm/tree/main/optillm/plugins/deep_research",
      "ai_summary": "The Test-Time Diffusion Deep Researcher (TTD-DR) framework uses a diffusion process with iterative refinement and external information retrieval to generate high-quality research reports, outperforming existing methods.",
      "ai_keywords": [
        "Test-Time Diffusion Deep Researcher",
        "TTD-DR",
        "diffusion process",
        "preliminary draft",
        "denoising process",
        "retrieval mechanism",
        "self-evolutionary algorithm",
        "agentic workflow",
        "iterative search",
        "multi-hop reasoning"
      ]
    },
    "publishedAt": "2025-07-21T17:23:21.000Z",
    "title": "Deep Researcher with Test-Time Diffusion",
    "summary": "Deep research agents, powered by Large Language Models (LLMs), are rapidly\nadvancing; yet, their performance often plateaus when generating complex,\nlong-form research reports using generic test-time scaling algorithms. Drawing\ninspiration from the iterative nature of human research, which involves cycles\nof searching, reasoning, and revision, we propose the Test-Time Diffusion Deep\nResearcher (TTD-DR). This novel framework conceptualizes research report\ngeneration as a diffusion process. TTD-DR initiates this process with a\npreliminary draft, an updatable skeleton that serves as an evolving foundation\nto guide the research direction. The draft is then iteratively refined through\na \"denoising\" process, which is dynamically informed by a retrieval mechanism\nthat incorporates external information at each step. The core process is\nfurther enhanced by a self-evolutionary algorithm applied to each component of\nthe agentic workflow, ensuring the generation of high-quality context for the\ndiffusion process. This draft-centric design makes the report writing process\nmore timely and coherent while reducing information loss during the iterative\nsearch process. We demonstrate that our TTD-DR achieves state-of-the-art\nresults on a wide array of benchmarks that require intensive search and\nmulti-hop reasoning, significantly outperforming existing deep research agents.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.16075.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "62f32eab52ad88c930bb3f3b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677134945205-62f32eab52ad88c930bb3f3b.png",
      "fullname": "Asankhaya Sharma",
      "name": "codelion",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 152
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.18742",
      "authors": [
        {
          "_id": "688728d6a5a841b139945452",
          "name": "Víctor Gallego",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-24T18:44:28.000Z",
      "submittedOnDailyAt": "2025-07-28T06:09:19.276Z",
      "title": "Specification Self-Correction: Mitigating In-Context Reward Hacking\n  Through Test-Time Refinement",
      "submittedOnDailyBy": {
        "_id": "5fad8602b8423e1d80b8a965",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fad8602b8423e1d80b8a965/tRqTwcZmrGka8c1vFq2wX.jpeg",
        "isPro": false,
        "fullname": "Victor Gallego",
        "user": "vicgalle",
        "type": "user"
      },
      "summary": "Language models (LMs) are susceptible to in-context reward hacking, where\nthey exploit flaws in tainted or faulty written specifications or rubrics to\nachieve high scores without fulfilling the user's true intent. We introduce\nSpecification Self-Correction (SSC), a novel, test-time framework that enables\nan LM to identify and correct flaws within its own guiding specification. SSC\nemploys a multi-step inference process where the model first generates a\nresponse based on a potentially tainted specification, critiques its output,\nand then revises the specification itself to remove the exploitable loophole. A\nfinal, more robust response is then generated using this self-corrected\nspecification. Across experiments spanning creative writing and agentic coding\ntasks with several LMs, we demonstrate that while models initially game tainted\nspecifications in 50-70\\% of cases, the SSC process reduces this vulnerability\nby over 90\\%. This dynamic repair occurs at inference time, requires no weight\nmodification, and leads to more robustly aligned model behavior. Code at\nhttps://github.com/vicgalle/specification-self-correction .",
      "upvotes": 1,
      "discussionId": "688728d7a5a841b139945453",
      "githubRepo": "https://github.com/vicgalle/specification-self-correction",
      "ai_summary": "A new framework called Specification Self-Correction allows language models to dynamically correct flawed instructions during inference, reducing reward hacking vulnerabilities.",
      "ai_keywords": [
        "language models",
        "reward hacking",
        "Specification Self-Correction",
        "multi-step inference",
        "self-corrected specification"
      ]
    },
    "publishedAt": "2025-07-24T14:44:28.000Z",
    "title": "Specification Self-Correction: Mitigating In-Context Reward Hacking\n  Through Test-Time Refinement",
    "summary": "Language models (LMs) are susceptible to in-context reward hacking, where\nthey exploit flaws in tainted or faulty written specifications or rubrics to\nachieve high scores without fulfilling the user's true intent. We introduce\nSpecification Self-Correction (SSC), a novel, test-time framework that enables\nan LM to identify and correct flaws within its own guiding specification. SSC\nemploys a multi-step inference process where the model first generates a\nresponse based on a potentially tainted specification, critiques its output,\nand then revises the specification itself to remove the exploitable loophole. A\nfinal, more robust response is then generated using this self-corrected\nspecification. Across experiments spanning creative writing and agentic coding\ntasks with several LMs, we demonstrate that while models initially game tainted\nspecifications in 50-70\\% of cases, the SSC process reduces this vulnerability\nby over 90\\%. This dynamic repair occurs at inference time, requires no weight\nmodification, and leads to more robustly aligned model behavior. Code at\nhttps://github.com/vicgalle/specification-self-correction .",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.18742.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5fad8602b8423e1d80b8a965",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fad8602b8423e1d80b8a965/tRqTwcZmrGka8c1vFq2wX.jpeg",
      "fullname": "Victor Gallego",
      "name": "vicgalle",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 133
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.17596",
      "authors": [
        {
          "_id": "688223086a54dd1e77daa947",
          "user": {
            "_id": "641862440956be7233a1788f",
            "avatarUrl": "/avatars/8710a44806a98f44d36b60814144186a.svg",
            "isPro": false,
            "fullname": "Maciej Wozniak",
            "user": "maciejw94",
            "type": "user"
          },
          "name": "Maciej K. Wozniak",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-24T14:11:19.043Z",
          "hidden": false
        },
        {
          "_id": "688223086a54dd1e77daa948",
          "name": "Lianhang Liu",
          "hidden": false
        },
        {
          "_id": "688223086a54dd1e77daa949",
          "name": "Yixi Cai",
          "hidden": false
        },
        {
          "_id": "688223086a54dd1e77daa94a",
          "name": "Patric Jensfelt",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-23T15:28:23.000Z",
      "submittedOnDailyAt": "2025-07-28T05:33:52.120Z",
      "title": "PRIX: Learning to Plan from Raw Pixels for End-to-End Autonomous Driving",
      "submittedOnDailyBy": {
        "_id": "641862440956be7233a1788f",
        "avatarUrl": "/avatars/8710a44806a98f44d36b60814144186a.svg",
        "isPro": false,
        "fullname": "Maciej Wozniak",
        "user": "maciejw94",
        "type": "user"
      },
      "summary": "While end-to-end autonomous driving models show promising results, their\npractical deployment is often hindered by large model sizes, a reliance on\nexpensive LiDAR sensors and computationally intensive BEV feature\nrepresentations. This limits their scalability, especially for mass-market\nvehicles equipped only with cameras. To address these challenges, we propose\nPRIX (Plan from Raw Pixels). Our novel and efficient end-to-end driving\narchitecture operates using only camera data, without explicit BEV\nrepresentation and forgoing the need for LiDAR. PRIX leverages a visual feature\nextractor coupled with a generative planning head to predict safe trajectories\nfrom raw pixel inputs directly. A core component of our architecture is the\nContext-aware Recalibration Transformer (CaRT), a novel module designed to\neffectively enhance multi-level visual features for more robust planning. We\ndemonstrate through comprehensive experiments that PRIX achieves\nstate-of-the-art performance on the NavSim and nuScenes benchmarks, matching\nthe capabilities of larger, multimodal diffusion planners while being\nsignificantly more efficient in terms of inference speed and model size, making\nit a practical solution for real-world deployment. Our work is open-source and\nthe code will be at https://maxiuw.github.io/prix.",
      "upvotes": 1,
      "discussionId": "688223086a54dd1e77daa94b",
      "ai_summary": "PRIX, an end-to-end driving architecture using only camera data, achieves state-of-the-art performance with a Context-aware Recalibration Transformer, outperforming larger multimodal planners in efficiency and scalability.",
      "ai_keywords": [
        "end-to-end driving architecture",
        "Context-aware Recalibration Transformer",
        "CaRT",
        "NavSim",
        "nuScenes",
        "BEV feature representations",
        "LiDAR",
        "visual feature extractor",
        "generative planning head",
        "safe trajectories",
        "raw pixel inputs"
      ]
    },
    "publishedAt": "2025-07-23T11:28:23.000Z",
    "title": "PRIX: Learning to Plan from Raw Pixels for End-to-End Autonomous Driving",
    "summary": "While end-to-end autonomous driving models show promising results, their\npractical deployment is often hindered by large model sizes, a reliance on\nexpensive LiDAR sensors and computationally intensive BEV feature\nrepresentations. This limits their scalability, especially for mass-market\nvehicles equipped only with cameras. To address these challenges, we propose\nPRIX (Plan from Raw Pixels). Our novel and efficient end-to-end driving\narchitecture operates using only camera data, without explicit BEV\nrepresentation and forgoing the need for LiDAR. PRIX leverages a visual feature\nextractor coupled with a generative planning head to predict safe trajectories\nfrom raw pixel inputs directly. A core component of our architecture is the\nContext-aware Recalibration Transformer (CaRT), a novel module designed to\neffectively enhance multi-level visual features for more robust planning. We\ndemonstrate through comprehensive experiments that PRIX achieves\nstate-of-the-art performance on the NavSim and nuScenes benchmarks, matching\nthe capabilities of larger, multimodal diffusion planners while being\nsignificantly more efficient in terms of inference speed and model size, making\nit a practical solution for real-world deployment. Our work is open-source and\nthe code will be at https://maxiuw.github.io/prix.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.17596.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "641862440956be7233a1788f",
      "avatarUrl": "/avatars/8710a44806a98f44d36b60814144186a.svg",
      "fullname": "Maciej Wozniak",
      "name": "maciejw94",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.10510",
      "authors": [
        {
          "_id": "6886fab77d7a19a208cdf212",
          "name": "Jiangkai Wu",
          "hidden": false
        },
        {
          "_id": "6886fab77d7a19a208cdf213",
          "name": "Zhiyuan Ren",
          "hidden": false
        },
        {
          "_id": "6886fab77d7a19a208cdf214",
          "name": "Liming Liu",
          "hidden": false
        },
        {
          "_id": "6886fab77d7a19a208cdf215",
          "name": "Xinggong Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-14T17:34:49.000Z",
      "submittedOnDailyAt": "2025-07-28T02:51:56.268Z",
      "title": "Chat with AI: The Surprising Turn of Real-time Video Communication from\n  Human to AI",
      "submittedOnDailyBy": {
        "_id": "67e43f9b29d49781111e4013",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67e43f9b29d49781111e4013/UXMNGH3-PsVEE3oazmGx2.jpeg",
        "isPro": false,
        "fullname": "Jiangkai",
        "user": "keyonN",
        "type": "user"
      },
      "summary": "AI Video Chat emerges as a new paradigm for Real-time Communication (RTC),\nwhere one peer is not a human, but a Multimodal Large Language Model (MLLM).\nThis makes interaction between humans and AI more intuitive, as if chatting\nface-to-face with a real person. However, this poses significant challenges to\nlatency, because the MLLM inference takes up most of the response time, leaving\nvery little time for video streaming. Due to network uncertainty and\ninstability, transmission latency becomes a critical bottleneck preventing AI\nfrom being like a real person. To address this, we propose Artic, an\nAI-oriented Real-time Communication framework, exploring the network\nrequirement shift from \"humans watching video\" to \"AI understanding video\". To\nreduce bitrate dramatically while maintaining MLLM accuracy, we propose\nContext-Aware Video Streaming that recognizes the importance of each video\nregion for chat and allocates bitrate almost exclusively to chat-important\nregions. To avoid packet retransmission, we propose Loss-Resilient Adaptive\nFrame Rate that leverages previous frames to substitute for lost/delayed frames\nwhile avoiding bitrate waste. To evaluate the impact of video streaming quality\non MLLM accuracy, we build the first benchmark, named Degraded Video\nUnderstanding Benchmark (DeViBench). Finally, we discuss some open questions\nand ongoing solutions for AI Video Chat.",
      "upvotes": 1,
      "discussionId": "6886fab77d7a19a208cdf216",
      "ai_summary": "Artic addresses latency issues in AI Video Chat by optimizing video streaming and frame rate adaptation to enhance MLLM accuracy and reduce bitrate.",
      "ai_keywords": [
        "Multimodal Large Language Model",
        "MLLM",
        "Context-Aware Video Streaming",
        "Loss-Resilient Adaptive Frame Rate",
        "Degraded Video Understanding Benchmark",
        "DeViBench"
      ]
    },
    "publishedAt": "2025-07-14T13:34:49.000Z",
    "title": "Chat with AI: The Surprising Turn of Real-time Video Communication from\n  Human to AI",
    "summary": "AI Video Chat emerges as a new paradigm for Real-time Communication (RTC),\nwhere one peer is not a human, but a Multimodal Large Language Model (MLLM).\nThis makes interaction between humans and AI more intuitive, as if chatting\nface-to-face with a real person. However, this poses significant challenges to\nlatency, because the MLLM inference takes up most of the response time, leaving\nvery little time for video streaming. Due to network uncertainty and\ninstability, transmission latency becomes a critical bottleneck preventing AI\nfrom being like a real person. To address this, we propose Artic, an\nAI-oriented Real-time Communication framework, exploring the network\nrequirement shift from \"humans watching video\" to \"AI understanding video\". To\nreduce bitrate dramatically while maintaining MLLM accuracy, we propose\nContext-Aware Video Streaming that recognizes the importance of each video\nregion for chat and allocates bitrate almost exclusively to chat-important\nregions. To avoid packet retransmission, we propose Loss-Resilient Adaptive\nFrame Rate that leverages previous frames to substitute for lost/delayed frames\nwhile avoiding bitrate waste. To evaluate the impact of video streaming quality\non MLLM accuracy, we build the first benchmark, named Degraded Video\nUnderstanding Benchmark (DeViBench). Finally, we discuss some open questions\nand ongoing solutions for AI Video Chat.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.10510.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67e43f9b29d49781111e4013",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67e43f9b29d49781111e4013/UXMNGH3-PsVEE3oazmGx2.jpeg",
      "fullname": "Jiangkai",
      "name": "keyonN",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]