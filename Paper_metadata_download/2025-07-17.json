[
  {
    "paper": {
      "id": "2507.09477",
      "authors": [
        {
          "_id": "68787030001546c83aa4f9ae",
          "name": "Yangning Li",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9af",
          "name": "Weizhi Zhang",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9b0",
          "name": "Yuyao Yang",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9b1",
          "name": "Wei-Chieh Huang",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9b2",
          "name": "Yaozu Wu",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9b3",
          "name": "Junyu Luo",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9b4",
          "name": "Yuanchen Bei",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9b5",
          "name": "Henry Peng Zou",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9b6",
          "name": "Xiao Luo",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9b7",
          "name": "Yusheng Zhao",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9b8",
          "name": "Chunkit Chan",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9b9",
          "name": "Yankai Chen",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9ba",
          "name": "Zhongfen Deng",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9bb",
          "name": "Yinghui Li",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9bc",
          "name": "Hai-Tao Zheng",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9bd",
          "name": "Dongyuan Li",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9be",
          "name": "Renhe Jiang",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9bf",
          "name": "Ming Zhang",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9c0",
          "name": "Yangqiu Song",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9c1",
          "name": "Philip S. Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-13T03:29:41.000Z",
      "submittedOnDailyAt": "2025-07-17T02:27:52.541Z",
      "title": "Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning\n  Systems in LLMs",
      "submittedOnDailyBy": {
        "_id": "6667e801fd95ddf66cac84ff",
        "avatarUrl": "/avatars/b75f6253160c81f558e6b40ed2ca1755.svg",
        "isPro": false,
        "fullname": "Weizhi Zhang",
        "user": "WZDavid",
        "type": "user"
      },
      "summary": "Retrieval-Augmented Generation (RAG) lifts the factuality of Large Language\nModels (LLMs) by injecting external knowledge, yet it falls short on problems\nthat demand multi-step inference; conversely, purely reasoning-oriented\napproaches often hallucinate or mis-ground facts. This survey synthesizes both\nstrands under a unified reasoning-retrieval perspective. We first map how\nadvanced reasoning optimizes each stage of RAG (Reasoning-Enhanced RAG). Then,\nwe show how retrieved knowledge of different type supply missing premises and\nexpand context for complex inference (RAG-Enhanced Reasoning). Finally, we\nspotlight emerging Synergized RAG-Reasoning frameworks, where (agentic) LLMs\niteratively interleave search and reasoning to achieve state-of-the-art\nperformance across knowledge-intensive benchmarks. We categorize methods,\ndatasets, and open challenges, and outline research avenues toward deeper\nRAG-Reasoning systems that are more effective, multimodally-adaptive,\ntrustworthy, and human-centric. The collection is available at\nhttps://github.com/DavidZWZ/Awesome-RAG-Reasoning.",
      "upvotes": 25,
      "discussionId": "68787031001546c83aa4f9c2",
      "githubRepo": "https://github.com/DavidZWZ/Awesome-RAG-Reasoning",
      "ai_summary": "This survey integrates reasoning and retrieval in Large Language Models to improve factuality and multi-step inference, highlighting Synergized RAG-Reasoning frameworks and outlining future research directions.",
      "ai_keywords": [
        "Retrieval-Augmented Generation",
        "Large Language Models",
        "Reasoning-Enhanced RAG",
        "RAG-Enhanced Reasoning",
        "Synergized RAG-Reasoning",
        "knowledge-intensive benchmarks",
        "multimodally-adaptive",
        "trustworthy",
        "human-centric"
      ],
      "githubStars": 38
    },
    "publishedAt": "2025-07-12T23:29:41.000Z",
    "title": "Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning\n  Systems in LLMs",
    "summary": "Retrieval-Augmented Generation (RAG) lifts the factuality of Large Language\nModels (LLMs) by injecting external knowledge, yet it falls short on problems\nthat demand multi-step inference; conversely, purely reasoning-oriented\napproaches often hallucinate or mis-ground facts. This survey synthesizes both\nstrands under a unified reasoning-retrieval perspective. We first map how\nadvanced reasoning optimizes each stage of RAG (Reasoning-Enhanced RAG). Then,\nwe show how retrieved knowledge of different type supply missing premises and\nexpand context for complex inference (RAG-Enhanced Reasoning). Finally, we\nspotlight emerging Synergized RAG-Reasoning frameworks, where (agentic) LLMs\niteratively interleave search and reasoning to achieve state-of-the-art\nperformance across knowledge-intensive benchmarks. We categorize methods,\ndatasets, and open challenges, and outline research avenues toward deeper\nRAG-Reasoning systems that are more effective, multimodally-adaptive,\ntrustworthy, and human-centric. The collection is available at\nhttps://github.com/DavidZWZ/Awesome-RAG-Reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.09477.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6667e801fd95ddf66cac84ff",
      "avatarUrl": "/avatars/b75f6253160c81f558e6b40ed2ca1755.svg",
      "fullname": "Weizhi Zhang",
      "name": "WZDavid",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.12465",
      "authors": [
        {
          "_id": "6878635e001546c83aa4f979",
          "name": "Ziang Cao",
          "hidden": false
        },
        {
          "_id": "6878635e001546c83aa4f97a",
          "name": "Zhaoxi Chen",
          "hidden": false
        },
        {
          "_id": "6878635e001546c83aa4f97b",
          "name": "Linag Pan",
          "hidden": false
        },
        {
          "_id": "6878635e001546c83aa4f97c",
          "name": "Ziwei Liu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/65af6f6b52e1b2aae437af2e/nrL7wGaZ1Z5FZWuu0GTbg.mp4"
      ],
      "publishedAt": "2025-07-16T17:59:35.000Z",
      "submittedOnDailyAt": "2025-07-17T01:26:11.001Z",
      "title": "PhysX: Physical-Grounded 3D Asset Generation",
      "submittedOnDailyBy": {
        "_id": "65af6f6b52e1b2aae437af2e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65af6f6b52e1b2aae437af2e/sFC98zLL_ZPS9fvZFi01W.jpeg",
        "isPro": false,
        "fullname": "Ziang Cao",
        "user": "Caoza",
        "type": "user"
      },
      "summary": "3D modeling is moving from virtual to physical. Existing 3D generation\nprimarily emphasizes geometries and textures while neglecting physical-grounded\nmodeling. Consequently, despite the rapid development of 3D generative models,\nthe synthesized 3D assets often overlook rich and important physical\nproperties, hampering their real-world application in physical domains like\nsimulation and embodied AI. As an initial attempt to address this challenge, we\npropose PhysX, an end-to-end paradigm for physical-grounded 3D asset\ngeneration. 1) To bridge the critical gap in physics-annotated 3D datasets, we\npresent PhysXNet - the first physics-grounded 3D dataset systematically\nannotated across five foundational dimensions: absolute scale, material,\naffordance, kinematics, and function description. In particular, we devise a\nscalable human-in-the-loop annotation pipeline based on vision-language models,\nwhich enables efficient creation of physics-first assets from raw 3D assets.2)\nFurthermore, we propose PhysXGen, a feed-forward framework for\nphysics-grounded image-to-3D asset generation, injecting physical knowledge\ninto the pre-trained 3D structural space. Specifically, PhysXGen employs a\ndual-branch architecture to explicitly model the latent correlations between 3D\nstructures and physical properties, thereby producing 3D assets with plausible\nphysical predictions while preserving the native geometry quality. Extensive\nexperiments validate the superior performance and promising generalization\ncapability of our framework. All the code, data, and models will be released to\nfacilitate future research in generative physical AI.",
      "upvotes": 16,
      "discussionId": "6878635e001546c83aa4f97d",
      "projectPage": "https://physx-3d.github.io/",
      "githubRepo": "https://github.com/ziangcao0312/PhysX",
      "ai_summary": "PhysX addresses the lack of physical properties in 3D generative models by introducing PhysXNet, a physics-annotated dataset, and PhysXGen, a framework that integrates physical knowledge into 3D asset generation.",
      "ai_keywords": [
        "physics-grounded 3D asset generation",
        "PhysXNet",
        "physics-annotated 3D datasets",
        "vision-language models",
        "human-in-the-loop annotation pipeline",
        "PhysXGen",
        "feed-forward framework",
        "dual-branch architecture",
        "latent correlations",
        "physical predictions",
        "geometry quality"
      ],
      "githubStars": 20
    },
    "publishedAt": "2025-07-16T13:59:35.000Z",
    "title": "PhysX: Physical-Grounded 3D Asset Generation",
    "summary": "3D modeling is moving from virtual to physical. Existing 3D generation\nprimarily emphasizes geometries and textures while neglecting physical-grounded\nmodeling. Consequently, despite the rapid development of 3D generative models,\nthe synthesized 3D assets often overlook rich and important physical\nproperties, hampering their real-world application in physical domains like\nsimulation and embodied AI. As an initial attempt to address this challenge, we\npropose PhysX, an end-to-end paradigm for physical-grounded 3D asset\ngeneration. 1) To bridge the critical gap in physics-annotated 3D datasets, we\npresent PhysXNet - the first physics-grounded 3D dataset systematically\nannotated across five foundational dimensions: absolute scale, material,\naffordance, kinematics, and function description. In particular, we devise a\nscalable human-in-the-loop annotation pipeline based on vision-language models,\nwhich enables efficient creation of physics-first assets from raw 3D assets.2)\nFurthermore, we propose PhysXGen, a feed-forward framework for\nphysics-grounded image-to-3D asset generation, injecting physical knowledge\ninto the pre-trained 3D structural space. Specifically, PhysXGen employs a\ndual-branch architecture to explicitly model the latent correlations between 3D\nstructures and physical properties, thereby producing 3D assets with plausible\nphysical predictions while preserving the native geometry quality. Extensive\nexperiments validate the superior performance and promising generalization\ncapability of our framework. All the code, data, and models will be released to\nfacilitate future research in generative physical AI.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/65af6f6b52e1b2aae437af2e/nrL7wGaZ1Z5FZWuu0GTbg.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.12465.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65af6f6b52e1b2aae437af2e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65af6f6b52e1b2aae437af2e/sFC98zLL_ZPS9fvZFi01W.jpeg",
      "fullname": "Ziang Cao",
      "name": "Caoza",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.12463",
      "authors": [
        {
          "_id": "68788789001546c83aa4f9e4",
          "name": "Renjie Li",
          "hidden": false
        },
        {
          "_id": "68788789001546c83aa4f9e5",
          "name": "Ruijie Ye",
          "hidden": false
        },
        {
          "_id": "68788789001546c83aa4f9e6",
          "name": "Mingyang Wu",
          "hidden": false
        },
        {
          "_id": "68788789001546c83aa4f9e7",
          "name": "Hao Frank Yang",
          "hidden": false
        },
        {
          "_id": "68788789001546c83aa4f9e8",
          "name": "Zhiwen Fan",
          "hidden": false
        },
        {
          "_id": "68788789001546c83aa4f9e9",
          "name": "Hezhen Hu",
          "hidden": false
        },
        {
          "_id": "68788789001546c83aa4f9ea",
          "name": "Zhengzhong Tu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62548d5fef3debb2ddf91217/ARblfCBCEmNaS2zzbTJLm.jpeg"
      ],
      "publishedAt": "2025-07-16T17:59:30.000Z",
      "submittedOnDailyAt": "2025-07-17T03:48:41.381Z",
      "title": "MMHU: A Massive-Scale Multimodal Benchmark for Human Behavior\n  Understanding",
      "submittedOnDailyBy": {
        "_id": "62548d5fef3debb2ddf91217",
        "avatarUrl": "/avatars/14975b45568f9c399c92c3986b6ce83e.svg",
        "isPro": false,
        "fullname": "Zhengzhong Tu",
        "user": "vztu",
        "type": "user"
      },
      "summary": "Humans are integral components of the transportation ecosystem, and\nunderstanding their behaviors is crucial to facilitating the development of\nsafe driving systems. Although recent progress has explored various aspects of\nhuman behaviorx2014such as motion, trajectories, and\nintentionx2014a comprehensive benchmark for evaluating human\nbehavior understanding in autonomous driving remains unavailable. In this work,\nwe propose MMHU, a large-scale benchmark for human behavior analysis\nfeaturing rich annotations, such as human motion and trajectories, text\ndescription for human motions, human intention, and critical behavior labels\nrelevant to driving safety. Our dataset encompasses 57k human motion clips and\n1.73M frames gathered from diverse sources, including established driving\ndatasets such as Waymo, in-the-wild videos from YouTube, and self-collected\ndata. A human-in-the-loop annotation pipeline is developed to generate rich\nbehavior captions. We provide a thorough dataset analysis and benchmark\nmultiple tasksx2014ranging from motion prediction to motion\ngeneration and human behavior question answeringx2014thereby\noffering a broad evaluation suite. Project page :\nhttps://MMHU-Benchmark.github.io.",
      "upvotes": 8,
      "discussionId": "6878878a001546c83aa4f9eb",
      "projectPage": "https://mmhu-benchmark.github.io/",
      "ai_summary": "A large-scale benchmark, MMHU, is proposed for human behavior analysis in autonomous driving, featuring rich annotations and diverse data sources, and benchmarking multiple tasks including motion prediction and behavior question answering.",
      "ai_keywords": [
        "human behavior analysis",
        "motion prediction",
        "motion generation",
        "human behavior question answering",
        "human-in-the-loop annotation",
        "Waymo",
        "YouTube",
        "self-collected data"
      ]
    },
    "publishedAt": "2025-07-16T13:59:30.000Z",
    "title": "MMHU: A Massive-Scale Multimodal Benchmark for Human Behavior\n  Understanding",
    "summary": "Humans are integral components of the transportation ecosystem, and\nunderstanding their behaviors is crucial to facilitating the development of\nsafe driving systems. Although recent progress has explored various aspects of\nhuman behaviorx2014such as motion, trajectories, and\nintentionx2014a comprehensive benchmark for evaluating human\nbehavior understanding in autonomous driving remains unavailable. In this work,\nwe propose MMHU, a large-scale benchmark for human behavior analysis\nfeaturing rich annotations, such as human motion and trajectories, text\ndescription for human motions, human intention, and critical behavior labels\nrelevant to driving safety. Our dataset encompasses 57k human motion clips and\n1.73M frames gathered from diverse sources, including established driving\ndatasets such as Waymo, in-the-wild videos from YouTube, and self-collected\ndata. A human-in-the-loop annotation pipeline is developed to generate rich\nbehavior captions. We provide a thorough dataset analysis and benchmark\nmultiple tasksx2014ranging from motion prediction to motion\ngeneration and human behavior question answeringx2014thereby\noffering a broad evaluation suite. Project page :\nhttps://MMHU-Benchmark.github.io.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62548d5fef3debb2ddf91217/ARblfCBCEmNaS2zzbTJLm.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.12463.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62548d5fef3debb2ddf91217",
      "avatarUrl": "/avatars/14975b45568f9c399c92c3986b6ce83e.svg",
      "fullname": "Zhengzhong Tu",
      "name": "vztu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.11527",
      "authors": [
        {
          "_id": "68785ee1001546c83aa4f967",
          "name": "Yinsheng Li",
          "hidden": false
        },
        {
          "_id": "68785ee1001546c83aa4f968",
          "name": "Zhen Dong",
          "hidden": false
        },
        {
          "_id": "68785ee1001546c83aa4f969",
          "name": "Yi Shao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/643ba2f725681c3afaa8f05e/2T1jxRrhqMGVLESbEDwJT.png"
      ],
      "publishedAt": "2025-07-15T17:56:04.000Z",
      "submittedOnDailyAt": "2025-07-17T01:30:58.515Z",
      "title": "DrafterBench: Benchmarking Large Language Models for Tasks Automation in\n  Civil Engineering",
      "submittedOnDailyBy": {
        "_id": "643ba2f725681c3afaa8f05e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643ba2f725681c3afaa8f05e/2RnOdmbBM8WHYhiTGG-Cd.jpeg",
        "isPro": false,
        "fullname": "Zhen Dong",
        "user": "zhendongucb",
        "type": "user"
      },
      "summary": "Large Language Model (LLM) agents have shown great potential for solving\nreal-world problems and promise to be a solution for tasks automation in\nindustry. However, more benchmarks are needed to systematically evaluate\nautomation agents from an industrial perspective, for example, in Civil\nEngineering. Therefore, we propose DrafterBench for the comprehensive\nevaluation of LLM agents in the context of technical drawing revision, a\nrepresentation task in civil engineering. DrafterBench contains twelve types of\ntasks summarized from real-world drawing files, with 46 customized\nfunctions/tools and 1920 tasks in total. DrafterBench is an open-source\nbenchmark to rigorously test AI agents' proficiency in interpreting intricate\nand long-context instructions, leveraging prior knowledge, and adapting to\ndynamic instruction quality via implicit policy awareness. The toolkit\ncomprehensively assesses distinct capabilities in structured data\ncomprehension, function execution, instruction following, and critical\nreasoning. DrafterBench offers detailed analysis of task accuracy and error\nstatistics, aiming to provide deeper insight into agent capabilities and\nidentify improvement targets for integrating LLMs in engineering applications.\nOur benchmark is available at https://github.com/Eason-Li-AIS/DrafterBench,\nwith the test set hosted at\nhttps://huggingface.co/datasets/Eason666/DrafterBench.",
      "upvotes": 8,
      "discussionId": "68785ee1001546c83aa4f96a",
      "githubRepo": "https://github.com/Eason-Li-AIS/DrafterBench",
      "ai_summary": "DrafterBench is an open-source benchmark for evaluating LLM agents in technical drawing revision, assessing their capabilities in structured data comprehension, function execution, instruction following, and critical reasoning.",
      "ai_keywords": [
        "Large Language Model (LLM)",
        "technical drawing revision",
        "structured data comprehension",
        "function execution",
        "instruction following",
        "critical reasoning",
        "benchmark",
        "open-source",
        "implicit policy awareness"
      ],
      "githubStars": 29
    },
    "publishedAt": "2025-07-15T13:56:04.000Z",
    "title": "DrafterBench: Benchmarking Large Language Models for Tasks Automation in\n  Civil Engineering",
    "summary": "Large Language Model (LLM) agents have shown great potential for solving\nreal-world problems and promise to be a solution for tasks automation in\nindustry. However, more benchmarks are needed to systematically evaluate\nautomation agents from an industrial perspective, for example, in Civil\nEngineering. Therefore, we propose DrafterBench for the comprehensive\nevaluation of LLM agents in the context of technical drawing revision, a\nrepresentation task in civil engineering. DrafterBench contains twelve types of\ntasks summarized from real-world drawing files, with 46 customized\nfunctions/tools and 1920 tasks in total. DrafterBench is an open-source\nbenchmark to rigorously test AI agents' proficiency in interpreting intricate\nand long-context instructions, leveraging prior knowledge, and adapting to\ndynamic instruction quality via implicit policy awareness. The toolkit\ncomprehensively assesses distinct capabilities in structured data\ncomprehension, function execution, instruction following, and critical\nreasoning. DrafterBench offers detailed analysis of task accuracy and error\nstatistics, aiming to provide deeper insight into agent capabilities and\nidentify improvement targets for integrating LLMs in engineering applications.\nOur benchmark is available at https://github.com/Eason-Li-AIS/DrafterBench,\nwith the test set hosted at\nhttps://huggingface.co/datasets/Eason666/DrafterBench.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/643ba2f725681c3afaa8f05e/2T1jxRrhqMGVLESbEDwJT.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.11527.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643ba2f725681c3afaa8f05e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643ba2f725681c3afaa8f05e/2RnOdmbBM8WHYhiTGG-Cd.jpeg",
      "fullname": "Zhen Dong",
      "name": "zhendongucb",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.12415",
      "authors": [
        {
          "_id": "68788b9b001546c83aa4f9ed",
          "name": "Xinyi He",
          "hidden": false
        },
        {
          "_id": "68788b9b001546c83aa4f9ee",
          "name": "Qian Liu",
          "hidden": false
        },
        {
          "_id": "68788b9b001546c83aa4f9ef",
          "name": "Mingzhe Du",
          "hidden": false
        },
        {
          "_id": "68788b9b001546c83aa4f9f0",
          "name": "Lin Yan",
          "hidden": false
        },
        {
          "_id": "68788b9b001546c83aa4f9f1",
          "name": "Zhijie Fan",
          "hidden": false
        },
        {
          "_id": "68788b9b001546c83aa4f9f2",
          "name": "Yiming Huang",
          "hidden": false
        },
        {
          "_id": "68788b9b001546c83aa4f9f3",
          "name": "Zejian Yuan",
          "hidden": false
        },
        {
          "_id": "68788b9b001546c83aa4f9f4",
          "name": "Zejun Ma",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/612ee6a7b960e78c6d2319d4/_XkK-c8Xm-G1Ui5AwtGdu.png"
      ],
      "publishedAt": "2025-07-16T17:05:17.000Z",
      "submittedOnDailyAt": "2025-07-17T04:10:30.408Z",
      "title": "SWE-Perf: Can Language Models Optimize Code Performance on Real-World\n  Repositories?",
      "submittedOnDailyBy": {
        "_id": "612ee6a7b960e78c6d2319d4",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/612ee6a7b960e78c6d2319d4/2Hu9BaAyXbyh1vt0v1Qui.jpeg",
        "isPro": false,
        "fullname": "Qian Liu",
        "user": "SivilTaram",
        "type": "user"
      },
      "summary": "Code performance optimization is paramount in real-world software engineering\nand critical for production-level systems. While Large Language Models (LLMs)\nhave demonstrated impressive capabilities in code generation and bug fixing,\ntheir proficiency in enhancing code performance at the repository level remains\nlargely unexplored. To address this gap, we introduce SWE-Perf, the first\nbenchmark specifically designed to systematically evaluate LLMs on code\nperformance optimization tasks within authentic repository contexts. SWE-Perf\ncomprises 140 carefully curated instances, each derived from\nperformance-improving pull requests from popular GitHub repositories. Each\nbenchmark instance includes the relevant codebase, target functions,\nperformance-related tests, expert-authored patches, and executable\nenvironments. Through a comprehensive evaluation of representative methods that\nspan file-level and repo-level approaches (e.g., Agentless and OpenHands), we\nreveal a substantial capability gap between existing LLMs and expert-level\noptimization performance, highlighting critical research opportunities in this\nemerging field.",
      "upvotes": 7,
      "discussionId": "68788b9b001546c83aa4f9f5",
      "projectPage": "https://swe-perf.github.io/",
      "githubRepo": "https://github.com/SWE-Perf/SWE-Perf",
      "ai_summary": "SWE-Perf is a benchmark for evaluating Large Language Models in code performance optimization using real-world repository data.",
      "ai_keywords": [
        "Large Language Models",
        "code performance optimization",
        "benchmark",
        "performance-improving pull requests",
        "codebase",
        "target functions",
        "performance-related tests",
        "expert-authored patches",
        "executable environments",
        "Agentless",
        "OpenHands"
      ],
      "githubStars": 4
    },
    "publishedAt": "2025-07-16T13:05:17.000Z",
    "title": "SWE-Perf: Can Language Models Optimize Code Performance on Real-World\n  Repositories?",
    "summary": "Code performance optimization is paramount in real-world software engineering\nand critical for production-level systems. While Large Language Models (LLMs)\nhave demonstrated impressive capabilities in code generation and bug fixing,\ntheir proficiency in enhancing code performance at the repository level remains\nlargely unexplored. To address this gap, we introduce SWE-Perf, the first\nbenchmark specifically designed to systematically evaluate LLMs on code\nperformance optimization tasks within authentic repository contexts. SWE-Perf\ncomprises 140 carefully curated instances, each derived from\nperformance-improving pull requests from popular GitHub repositories. Each\nbenchmark instance includes the relevant codebase, target functions,\nperformance-related tests, expert-authored patches, and executable\nenvironments. Through a comprehensive evaluation of representative methods that\nspan file-level and repo-level approaches (e.g., Agentless and OpenHands), we\nreveal a substantial capability gap between existing LLMs and expert-level\noptimization performance, highlighting critical research opportunities in this\nemerging field.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/612ee6a7b960e78c6d2319d4/_XkK-c8Xm-G1Ui5AwtGdu.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.12415.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "612ee6a7b960e78c6d2319d4",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/612ee6a7b960e78c6d2319d4/2Hu9BaAyXbyh1vt0v1Qui.jpeg",
      "fullname": "Qian Liu",
      "name": "SivilTaram",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 85
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.11949",
      "authors": [
        {
          "_id": "687872c3001546c83aa4f9cf",
          "name": "Shuyang Xu",
          "hidden": false
        },
        {
          "_id": "687872c3001546c83aa4f9d0",
          "name": "Zhiyang Dou",
          "hidden": false
        },
        {
          "_id": "687872c3001546c83aa4f9d1",
          "name": "Mingyi Shi",
          "hidden": false
        },
        {
          "_id": "687872c3001546c83aa4f9d2",
          "name": "Liang Pan",
          "hidden": false
        },
        {
          "_id": "687872c3001546c83aa4f9d3",
          "name": "Leo Ho",
          "hidden": false
        },
        {
          "_id": "687872c3001546c83aa4f9d4",
          "name": "Jingbo Wang",
          "hidden": false
        },
        {
          "_id": "687872c3001546c83aa4f9d5",
          "name": "Yuan Liu",
          "hidden": false
        },
        {
          "_id": "687872c3001546c83aa4f9d6",
          "name": "Cheng Lin",
          "hidden": false
        },
        {
          "_id": "687872c3001546c83aa4f9d7",
          "name": "Yuexin Ma",
          "hidden": false
        },
        {
          "_id": "687872c3001546c83aa4f9d8",
          "name": "Wenping Wang",
          "hidden": false
        },
        {
          "_id": "687872c3001546c83aa4f9d9",
          "name": "Taku Komura",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/645223fb01d7bd9555ea399a/KoPBJkGwyft7hfVmM8ikZ.mp4"
      ],
      "publishedAt": "2025-07-16T06:33:11.000Z",
      "submittedOnDailyAt": "2025-07-17T02:48:15.112Z",
      "title": "MOSPA: Human Motion Generation Driven by Spatial Audio",
      "submittedOnDailyBy": {
        "_id": "645223fb01d7bd9555ea399a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645223fb01d7bd9555ea399a/fVR7XmGg6pMSRKx9sEdvT.png",
        "isPro": false,
        "fullname": "Zhiyang Dou",
        "user": "frankzydou",
        "type": "user"
      },
      "summary": "Enabling virtual humans to dynamically and realistically respond to diverse\nauditory stimuli remains a key challenge in character animation, demanding the\nintegration of perceptual modeling and motion synthesis. Despite its\nsignificance, this task remains largely unexplored. Most previous works have\nprimarily focused on mapping modalities like speech, audio, and music to\ngenerate human motion. As of yet, these models typically overlook the impact of\nspatial features encoded in spatial audio signals on human motion. To bridge\nthis gap and enable high-quality modeling of human movements in response to\nspatial audio, we introduce the first comprehensive Spatial Audio-Driven Human\nMotion (SAM) dataset, which contains diverse and high-quality spatial audio and\nmotion data. For benchmarking, we develop a simple yet effective\ndiffusion-based generative framework for human MOtion generation driven by\nSPatial Audio, termed MOSPA, which faithfully captures the relationship between\nbody motion and spatial audio through an effective fusion mechanism. Once\ntrained, MOSPA could generate diverse realistic human motions conditioned on\nvarying spatial audio inputs. We perform a thorough investigation of the\nproposed dataset and conduct extensive experiments for benchmarking, where our\nmethod achieves state-of-the-art performance on this task. Our model and\ndataset will be open-sourced upon acceptance. Please refer to our supplementary\nvideo for more details.",
      "upvotes": 7,
      "discussionId": "687872c9001546c83aa4f9da",
      "ai_summary": "A diffusion-based generative framework, MOSPA, is introduced to model human motion in response to spatial audio, achieving state-of-the-art performance using the newly created SAM dataset.",
      "ai_keywords": [
        "diffusion-based generative framework",
        "spatial audio",
        "human motion",
        "SAM dataset",
        "MOSPA",
        "spatial features",
        "perceptual modeling",
        "motion synthesis"
      ]
    },
    "publishedAt": "2025-07-16T02:33:11.000Z",
    "title": "MOSPA: Human Motion Generation Driven by Spatial Audio",
    "summary": "Enabling virtual humans to dynamically and realistically respond to diverse\nauditory stimuli remains a key challenge in character animation, demanding the\nintegration of perceptual modeling and motion synthesis. Despite its\nsignificance, this task remains largely unexplored. Most previous works have\nprimarily focused on mapping modalities like speech, audio, and music to\ngenerate human motion. As of yet, these models typically overlook the impact of\nspatial features encoded in spatial audio signals on human motion. To bridge\nthis gap and enable high-quality modeling of human movements in response to\nspatial audio, we introduce the first comprehensive Spatial Audio-Driven Human\nMotion (SAM) dataset, which contains diverse and high-quality spatial audio and\nmotion data. For benchmarking, we develop a simple yet effective\ndiffusion-based generative framework for human MOtion generation driven by\nSPatial Audio, termed MOSPA, which faithfully captures the relationship between\nbody motion and spatial audio through an effective fusion mechanism. Once\ntrained, MOSPA could generate diverse realistic human motions conditioned on\nvarying spatial audio inputs. We perform a thorough investigation of the\nproposed dataset and conduct extensive experiments for benchmarking, where our\nmethod achieves state-of-the-art performance on this task. Our model and\ndataset will be open-sourced upon acceptance. Please refer to our supplementary\nvideo for more details.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/645223fb01d7bd9555ea399a/KoPBJkGwyft7hfVmM8ikZ.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.11949.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645223fb01d7bd9555ea399a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645223fb01d7bd9555ea399a/fVR7XmGg6pMSRKx9sEdvT.png",
      "fullname": "Zhiyang Dou",
      "name": "frankzydou",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.09025",
      "authors": [
        {
          "_id": "68786e45001546c83aa4f9a0",
          "name": "Chien Van Nguyen",
          "hidden": false
        },
        {
          "_id": "68786e45001546c83aa4f9a1",
          "name": "Ruiyi Zhang",
          "hidden": false
        },
        {
          "_id": "68786e45001546c83aa4f9a2",
          "name": "Hanieh Deilamsalehy",
          "hidden": false
        },
        {
          "_id": "68786e45001546c83aa4f9a3",
          "name": "Puneet Mathur",
          "hidden": false
        },
        {
          "_id": "68786e45001546c83aa4f9a4",
          "name": "Viet Dac Lai",
          "hidden": false
        },
        {
          "_id": "68786e45001546c83aa4f9a5",
          "name": "Haoliang Wang",
          "hidden": false
        },
        {
          "_id": "68786e45001546c83aa4f9a6",
          "name": "Jayakumar Subramanian",
          "hidden": false
        },
        {
          "_id": "68786e45001546c83aa4f9a7",
          "name": "Ryan A. Rossi",
          "hidden": false
        },
        {
          "_id": "68786e45001546c83aa4f9a8",
          "name": "Trung Bui",
          "hidden": false
        },
        {
          "_id": "68786e45001546c83aa4f9a9",
          "name": "Nikos Vlassis",
          "hidden": false
        },
        {
          "_id": "68786e45001546c83aa4f9aa",
          "name": "Franck Dernoncourt",
          "hidden": false
        },
        {
          "_id": "68786e45001546c83aa4f9ab",
          "name": "Thien Huu Nguyen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-11T21:19:18.000Z",
      "submittedOnDailyAt": "2025-07-17T02:00:25.332Z",
      "title": "Lizard: An Efficient Linearization Framework for Large Language Models",
      "submittedOnDailyBy": {
        "_id": "62c5947524171688a9feb992",
        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
        "isPro": false,
        "fullname": "Franck Dernoncourt",
        "user": "Franck-Dernoncourt",
        "type": "user"
      },
      "summary": "We propose Lizard, a linearization framework that transforms pretrained\nTransformer-based Large Language Models (LLMs) into flexible, subquadratic\narchitectures for infinite-context generation. Transformer-based LLMs face\nsignificant memory and computational bottlenecks as context lengths increase,\ndue to the quadratic complexity of softmax attention and the growing key-value\n(KV) cache. Lizard addresses these limitations by introducing a subquadratic\nattention mechanism that closely approximates softmax attention while\npreserving the output quality. Unlike previous linearization methods, which are\noften limited by fixed model structures and therefore exclude gating\nmechanisms, Lizard incorporates a gating module inspired by recent\nstate-of-the-art linear models. This enables adaptive memory control, supports\nconstant-memory inference, offers strong length generalization, and allows more\nflexible model design. Lizard combines gated linear attention for global\ncontext compression with sliding window attention enhanced by meta memory,\nforming a hybrid mechanism that captures both long-range dependencies and\nfine-grained local interactions. Moreover, we introduce a hardware-aware\nalgorithm that accelerates the training speed of our models. Extensive\nexperiments show that Lizard achieves near-lossless recovery of the teacher\nmodel's performance across standard language modeling tasks, while\nsignificantly outperforming previous linearization methods. On the 5-shot MMLU\nbenchmark, Lizard improves over prior models by 18 points and shows significant\nimprovements on associative recall tasks.",
      "upvotes": 2,
      "discussionId": "68786e45001546c83aa4f9ac",
      "ai_summary": "Lizard is a linearization framework that transforms Transformer-based LLMs into subquadratic architectures for efficient infinite-context generation, using a hybrid attention mechanism and hardware-aware training.",
      "ai_keywords": [
        "Transformer-based LLMs",
        "subquadratic architectures",
        "softmax attention",
        "key-value (KV) cache",
        "subquadratic attention mechanism",
        "gating module",
        "adaptive memory control",
        "constant-memory inference",
        "length generalization",
        "gated linear attention",
        "sliding window attention",
        "meta memory",
        "hardware-aware algorithm",
        "MMLU benchmark",
        "associative recall tasks"
      ]
    },
    "publishedAt": "2025-07-11T17:19:18.000Z",
    "title": "Lizard: An Efficient Linearization Framework for Large Language Models",
    "summary": "We propose Lizard, a linearization framework that transforms pretrained\nTransformer-based Large Language Models (LLMs) into flexible, subquadratic\narchitectures for infinite-context generation. Transformer-based LLMs face\nsignificant memory and computational bottlenecks as context lengths increase,\ndue to the quadratic complexity of softmax attention and the growing key-value\n(KV) cache. Lizard addresses these limitations by introducing a subquadratic\nattention mechanism that closely approximates softmax attention while\npreserving the output quality. Unlike previous linearization methods, which are\noften limited by fixed model structures and therefore exclude gating\nmechanisms, Lizard incorporates a gating module inspired by recent\nstate-of-the-art linear models. This enables adaptive memory control, supports\nconstant-memory inference, offers strong length generalization, and allows more\nflexible model design. Lizard combines gated linear attention for global\ncontext compression with sliding window attention enhanced by meta memory,\nforming a hybrid mechanism that captures both long-range dependencies and\nfine-grained local interactions. Moreover, we introduce a hardware-aware\nalgorithm that accelerates the training speed of our models. Extensive\nexperiments show that Lizard achieves near-lossless recovery of the teacher\nmodel's performance across standard language modeling tasks, while\nsignificantly outperforming previous linearization methods. On the 5-shot MMLU\nbenchmark, Lizard improves over prior models by 18 points and shows significant\nimprovements on associative recall tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.09025.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c5947524171688a9feb992",
      "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
      "fullname": "Franck Dernoncourt",
      "name": "Franck-Dernoncourt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.02857",
      "authors": [
        {
          "_id": "68788cd9001546c83aa4f9f7",
          "name": "Ziye Li",
          "hidden": false
        },
        {
          "_id": "68788cd9001546c83aa4f9f8",
          "name": "Hao Luo",
          "hidden": false
        },
        {
          "_id": "68788cd9001546c83aa4f9f9",
          "name": "Xincheng Shuai",
          "hidden": false
        },
        {
          "_id": "68788cd9001546c83aa4f9fa",
          "name": "Henghui Ding",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-03T17:59:02.000Z",
      "submittedOnDailyAt": "2025-07-17T04:11:55.501Z",
      "title": "AnyI2V: Animating Any Conditional Image with Motion Control",
      "submittedOnDailyBy": {
        "_id": "67ff29ecbf6889a333c69c7a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/fYIJFtF0uciB-1wb0lmTY.png",
        "isPro": false,
        "fullname": "Henghui Ding",
        "user": "HenghuiDing",
        "type": "user"
      },
      "summary": "Recent advancements in video generation, particularly in diffusion models,\nhave driven notable progress in text-to-video (T2V) and image-to-video (I2V)\nsynthesis. However, challenges remain in effectively integrating dynamic motion\nsignals and flexible spatial constraints. Existing T2V methods typically rely\non text prompts, which inherently lack precise control over the spatial layout\nof generated content. In contrast, I2V methods are limited by their dependence\non real images, which restricts the editability of the synthesized content.\nAlthough some methods incorporate ControlNet to introduce image-based\nconditioning, they often lack explicit motion control and require\ncomputationally expensive training. To address these limitations, we propose\nAnyI2V, a training-free framework that animates any conditional images with\nuser-defined motion trajectories. AnyI2V supports a broader range of modalities\nas the conditional image, including data types such as meshes and point clouds\nthat are not supported by ControlNet, enabling more flexible and versatile\nvideo generation. Additionally, it supports mixed conditional inputs and\nenables style transfer and editing via LoRA and text prompts. Extensive\nexperiments demonstrate that the proposed AnyI2V achieves superior performance\nand provides a new perspective in spatial- and motion-controlled video\ngeneration. Code is available at https://henghuiding.com/AnyI2V/.",
      "upvotes": 2,
      "discussionId": "68788cda001546c83aa4f9fb",
      "projectPage": "https://henghuiding.com/AnyI2V/",
      "githubRepo": "https://github.com/FudanCVL/AnyI2V",
      "ai_summary": "AnyI2V is a training-free framework that animates conditional images with user-defined motion trajectories, supporting various data types and enabling flexible video generation.",
      "ai_keywords": [
        "diffusion models",
        "text-to-video",
        "image-to-video",
        "ControlNet",
        "motion trajectories",
        "conditional images",
        "meshes",
        "point clouds",
        "mixed conditional inputs",
        "style transfer",
        "LoRA",
        "text prompts"
      ],
      "githubStars": 83
    },
    "publishedAt": "2025-07-03T13:59:02.000Z",
    "title": "AnyI2V: Animating Any Conditional Image with Motion Control",
    "summary": "Recent advancements in video generation, particularly in diffusion models,\nhave driven notable progress in text-to-video (T2V) and image-to-video (I2V)\nsynthesis. However, challenges remain in effectively integrating dynamic motion\nsignals and flexible spatial constraints. Existing T2V methods typically rely\non text prompts, which inherently lack precise control over the spatial layout\nof generated content. In contrast, I2V methods are limited by their dependence\non real images, which restricts the editability of the synthesized content.\nAlthough some methods incorporate ControlNet to introduce image-based\nconditioning, they often lack explicit motion control and require\ncomputationally expensive training. To address these limitations, we propose\nAnyI2V, a training-free framework that animates any conditional images with\nuser-defined motion trajectories. AnyI2V supports a broader range of modalities\nas the conditional image, including data types such as meshes and point clouds\nthat are not supported by ControlNet, enabling more flexible and versatile\nvideo generation. Additionally, it supports mixed conditional inputs and\nenables style transfer and editing via LoRA and text prompts. Extensive\nexperiments demonstrate that the proposed AnyI2V achieves superior performance\nand provides a new perspective in spatial- and motion-controlled video\ngeneration. Code is available at https://henghuiding.com/AnyI2V/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.02857.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67ff29ecbf6889a333c69c7a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/fYIJFtF0uciB-1wb0lmTY.png",
      "fullname": "Henghui Ding",
      "name": "HenghuiDing",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]