[
  {
    "paper": {
      "id": "2502.14776",
      "authors": [
        {
          "_id": "67bbdb46d94d32bcfba70db7",
          "name": "Xun Liang",
          "hidden": false
        },
        {
          "_id": "67bbdb46d94d32bcfba70db8",
          "name": "Jiawei Yang",
          "hidden": false
        },
        {
          "_id": "67bbdb46d94d32bcfba70db9",
          "user": {
            "_id": "662dd19f9e6d371ab71b91ce",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662dd19f9e6d371ab71b91ce/mZBPw_Zs8ZlEFGlbekAoH.jpeg",
            "isPro": false,
            "fullname": "Yezhaohui Wang",
            "user": "HaruTeru",
            "type": "user"
          },
          "name": "Yezhaohui Wang",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-02-24T04:12:46.485Z",
          "hidden": false
        },
        {
          "_id": "67bbdb46d94d32bcfba70dba",
          "name": "Chen Tang",
          "hidden": false
        },
        {
          "_id": "67bbdb46d94d32bcfba70dbb",
          "name": "Zifan Zheng",
          "hidden": false
        },
        {
          "_id": "67bbdb46d94d32bcfba70dbc",
          "name": "Simin Niu",
          "hidden": false
        },
        {
          "_id": "67bbdb46d94d32bcfba70dbd",
          "name": "Shichao Song",
          "hidden": false
        },
        {
          "_id": "67bbdb46d94d32bcfba70dbe",
          "name": "Hanyu Wang",
          "hidden": false
        },
        {
          "_id": "67bbdb46d94d32bcfba70dbf",
          "name": "Bo Tang",
          "hidden": false
        },
        {
          "_id": "67bbdb46d94d32bcfba70dc0",
          "name": "Feiyu Xiong",
          "hidden": false
        },
        {
          "_id": "67bbdb46d94d32bcfba70dc1",
          "name": "Keming Mao",
          "hidden": false
        },
        {
          "_id": "67bbdb46d94d32bcfba70dc2",
          "name": "Zhiyu li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T17:59:45.000Z",
      "title": "SurveyX: Academic Survey Automation via Large Language Models",
      "summary": "Large Language Models (LLMs) have demonstrated exceptional comprehension\ncapabilities and a vast knowledge base, suggesting that LLMs can serve as\nefficient tools for automated survey generation. However, recent research\nrelated to automated survey generation remains constrained by some critical\nlimitations like finite context window, lack of in-depth content discussion,\nand absence of systematic evaluation frameworks. Inspired by human writing\nprocesses, we propose SurveyX, an efficient and organized system for automated\nsurvey generation that decomposes the survey composing process into two phases:\nthe Preparation and Generation phases. By innovatively introducing online\nreference retrieval, a pre-processing method called AttributeTree, and a\nre-polishing process, SurveyX significantly enhances the efficacy of survey\ncomposition. Experimental evaluation results show that SurveyX outperforms\nexisting automated survey generation systems in content quality (0.259\nimprovement) and citation quality (1.76 enhancement), approaching human expert\nperformance across multiple evaluation dimensions. Examples of surveys\ngenerated by SurveyX are available on www.surveyx.cn",
      "upvotes": 56,
      "discussionId": "67bbdb47d94d32bcfba70df3"
    },
    "publishedAt": "2025-02-23T21:39:54.375Z",
    "title": "SurveyX: Academic Survey Automation via Large Language Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14776.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "669e60ee8580d17cb60f8347",
      "avatarUrl": "/avatars/37963b833228afe39cc24854c9326670.svg",
      "fullname": "yang jiawei",
      "name": "Dany-0",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.11663",
      "authors": [
        {
          "_id": "67b705d2ebee4662205c47f7",
          "name": "Jingcheng Ni",
          "hidden": false
        },
        {
          "_id": "67b705d2ebee4662205c47f8",
          "name": "Yuxin Guo",
          "hidden": false
        },
        {
          "_id": "67b705d2ebee4662205c47f9",
          "name": "Yichen Liu",
          "hidden": false
        },
        {
          "_id": "67b705d2ebee4662205c47fa",
          "name": "Rui Chen",
          "hidden": false
        },
        {
          "_id": "67b705d2ebee4662205c47fb",
          "name": "Lewei Lu",
          "hidden": false
        },
        {
          "_id": "67b705d2ebee4662205c47fc",
          "user": {
            "_id": "65717368be66cd9b65a8201c",
            "avatarUrl": "/avatars/fe945828eec9ded4cfa3b89d48a64d90.svg",
            "isPro": false,
            "fullname": "Wu Zehuan",
            "user": "wzhgba",
            "type": "user"
          },
          "name": "Zehuan Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-21T09:59:38.956Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-17T10:53:56.000Z",
      "title": "MaskGWM: A Generalizable Driving World Model with Video Mask\n  Reconstruction",
      "summary": "World models that forecast environmental changes from actions are vital for\nautonomous driving models with strong generalization. The prevailing driving\nworld model mainly build on video prediction model. Although these models can\nproduce high-fidelity video sequences with advanced diffusion-based generator,\nthey are constrained by their predictive duration and overall generalization\ncapabilities. In this paper, we explore to solve this problem by combining\ngeneration loss with MAE-style feature-level context learning. In particular,\nwe instantiate this target with three key design: (1) A more scalable Diffusion\nTransformer (DiT) structure trained with extra mask construction task. (2) we\ndevise diffusion-related mask tokens to deal with the fuzzy relations between\nmask reconstruction and generative diffusion process. (3) we extend mask\nconstruction task to spatial-temporal domain by utilizing row-wise mask for\nshifted self-attention rather than masked self-attention in MAE. Then, we adopt\na row-wise cross-view module to align with this mask design. Based on above\nimprovement, we propose MaskGWM: a Generalizable driving World Model embodied\nwith Video Mask reconstruction. Our model contains two variants: MaskGWM-long,\nfocusing on long-horizon prediction, and MaskGWM-mview, dedicated to multi-view\ngeneration. Comprehensive experiments on standard benchmarks validate the\neffectiveness of the proposed method, which contain normal validation of\nNuscene dataset, long-horizon rollout of OpenDV-2K dataset and zero-shot\nvalidation of Waymo dataset. Quantitative metrics on these datasets show our\nmethod notably improving state-of-the-art driving world model.",
      "upvotes": 27,
      "discussionId": "67b705d4ebee4662205c489c"
    },
    "publishedAt": "2025-02-24T01:16:03.517Z",
    "title": "MaskGWM: A Generalizable Driving World Model with Video Mask Reconstruction",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11663.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65717368be66cd9b65a8201c",
      "avatarUrl": "/avatars/fe945828eec9ded4cfa3b89d48a64d90.svg",
      "fullname": "Wu Zehuan",
      "name": "wzhgba",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.13449",
      "authors": [
        {
          "_id": "67b7ceae3e8a45f770b2606e",
          "user": {
            "_id": "65633c5e84a9fbe322f87d81",
            "avatarUrl": "/avatars/7233a555b43c669847a950ce5697c92c.svg",
            "isPro": false,
            "fullname": "DongkiKim",
            "user": "DongkiKim",
            "type": "user"
          },
          "name": "Dongki Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-21T09:59:11.214Z",
          "hidden": false
        },
        {
          "_id": "67b7ceae3e8a45f770b2606f",
          "name": "Wonbin Lee",
          "hidden": false
        },
        {
          "_id": "67b7ceae3e8a45f770b26070",
          "name": "Sung Ju Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-19T05:49:10.000Z",
      "title": "Mol-LLaMA: Towards General Understanding of Molecules in Large Molecular\n  Language Model",
      "summary": "Understanding molecules is key to understanding organisms and driving\nadvances in drug discovery, requiring interdisciplinary knowledge across\nchemistry and biology. Although large molecular language models have achieved\nnotable success in interpreting molecular structures, their instruction\ndatasets are limited to the specific knowledge from task-oriented datasets and\ndo not fully cover the fundamental characteristics of molecules, hindering\ntheir abilities as general-purpose molecular assistants. To address this issue,\nwe propose Mol-LLaMA, a large molecular language model that grasps the general\nknowledge centered on molecules via multi-modal instruction tuning. To this\nend, we design key data types that encompass the fundamental features of\nmolecules, incorporating essential knowledge from molecular structures. In\naddition, to improve understanding of molecular features, we introduce a module\nthat integrates complementary information from different molecular encoders,\nleveraging the distinct advantages of different molecular representations. Our\nexperimental results demonstrate that Mol-LLaMA is capable of comprehending the\ngeneral features of molecules and generating relevant responses to users'\nqueries with detailed explanations, implying its potential as a general-purpose\nassistant for molecular analysis.",
      "upvotes": 25,
      "discussionId": "67b7ceae3e8a45f770b2609f"
    },
    "publishedAt": "2025-02-23T21:52:51.059Z",
    "title": "Mol-LLaMA: Towards General Understanding of Molecules in Large Molecular Language Model",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13449.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65633c5e84a9fbe322f87d81",
      "avatarUrl": "/avatars/7233a555b43c669847a950ce5697c92c.svg",
      "fullname": "DongkiKim",
      "name": "DongkiKim",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.14922",
      "authors": [
        {
          "_id": "67bbe4ba79e0a705cf573985",
          "name": "Zihao Zeng",
          "hidden": false
        },
        {
          "_id": "67bbe4ba79e0a705cf573986",
          "name": "Xuyao Huang",
          "hidden": false
        },
        {
          "_id": "67bbe4ba79e0a705cf573987",
          "name": "Boxiu Li",
          "hidden": false
        },
        {
          "_id": "67bbe4ba79e0a705cf573988",
          "name": "Zhijie Deng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-19T17:38:46.000Z",
      "title": "SIFT: Grounding LLM Reasoning in Contexts via Stickers",
      "summary": "This paper identifies the misinterpretation of the context can be a\nsignificant issue during the reasoning process of large language models,\nspanning from smaller models like Llama3.2-3B-Instruct to cutting-edge ones\nlike DeepSeek-R1. For example, in the phrase \"10 dollars per kilo,\" LLMs might\nnot recognize that \"per\" means \"for each,\" leading to calculation errors. We\nintroduce a novel, post-training approach called **Stick to the Facts (SIFT)**\nto tackle this. SIFT leverages increasing inference-time compute to ground LLM\nreasoning in contexts. At the core of SIFT lies the *Sticker*, which is\ngenerated by the model itself to explicitly emphasize the key information\nwithin the context. Given the curated Sticker, SIFT generates two predictions\n-- one from the original query and one from the query augmented with the\nSticker. If they differ, the Sticker is sequentially refined via *forward*\noptimization (to better align the extracted facts with the query) and *inverse*\ngeneration (to conform with the model's inherent tendencies) for more faithful\nreasoning outcomes. Studies across diverse models (from 3B to 100B+) and\nbenchmarks (e.g., GSM8K, MATH-500) reveal consistent performance improvements.\nNotably, SIFT improves the pass@1 accuracy of DeepSeek-R1 on AIME2024 from\n78.33% to **85.67**%, establishing a new state-of-the-art in the open-source\ncommunity. The code is available at https://github.com/zhijie-group/SIFT.",
      "upvotes": 11,
      "discussionId": "67bbe4bb79e0a705cf5739c3"
    },
    "publishedAt": "2025-02-23T22:17:18.309Z",
    "title": "SIFT: Grounding LLM Reasoning in Contexts via Stickers",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14922.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6190
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.12084",
      "authors": [
        {
          "_id": "67b8922ef6632327952ec1e1",
          "user": {
            "_id": "65d8b0f0661492b25c6623de",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d8b0f0661492b25c6623de/c6LPDse8NIV_3BHIu8dYe.png",
            "isPro": false,
            "fullname": "Jianshu Zhang",
            "user": "Sterzhang",
            "type": "user"
          },
          "name": "Jianshu Zhang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-21T14:48:16.643Z",
          "hidden": false
        },
        {
          "_id": "67b8922ef6632327952ec1e2",
          "name": "Dongyu Yao",
          "hidden": false
        },
        {
          "_id": "67b8922ef6632327952ec1e3",
          "name": "Renjie Pi",
          "hidden": false
        },
        {
          "_id": "67b8922ef6632327952ec1e4",
          "name": "Paul Pu Liang",
          "hidden": false
        },
        {
          "_id": "67b8922ef6632327952ec1e5",
          "name": "Yi R.",
          "hidden": false
        },
        {
          "_id": "67b8922ef6632327952ec1e6",
          "name": "Fung",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-17T17:57:50.000Z",
      "title": "VLM^2-Bench: A Closer Look at How Well VLMs Implicitly Link Explicit\n  Matching Visual Cues",
      "summary": "Visually linking matching cues is a crucial ability in daily life, such as\nidentifying the same person in multiple photos based on their cues, even\nwithout knowing who they are. Despite the extensive knowledge that\nvision-language models (VLMs) possess, it remains largely unexplored whether\nthey are capable of performing this fundamental task. To address this, we\nintroduce VLM^2-Bench, a benchmark designed to assess whether VLMs can\nVisually Link Matching cues, with 9 subtasks and over 3,000 test cases.\nComprehensive evaluation across eight open-source VLMs and GPT-4o, along with\nfurther analysis of various language-side and vision-side prompting methods,\nleads to a total of eight key findings. We identify critical challenges in\nmodels' ability to link visual cues, highlighting a significant performance gap\nwhere even GPT-4o lags 34.80% behind humans. Based on these insights, we\nadvocate for (i) enhancing core visual capabilities to improve adaptability and\nreduce reliance on prior knowledge, (ii) establishing clearer principles for\nintegrating language-based reasoning in vision-centric tasks to prevent\nunnecessary biases, and (iii) shifting vision-text training paradigms toward\nfostering models' ability to independently structure and infer relationships\namong visual cues.",
      "upvotes": 9,
      "discussionId": "67b89230f6632327952ec27a"
    },
    "publishedAt": "2025-02-24T00:36:34.341Z",
    "title": "VLM$^2$-Bench: A Closer Look at How Well VLMs Implicitly Link Explicit Matching Visual Cues",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12084.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65d8b0f0661492b25c6623de",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d8b0f0661492b25c6623de/c6LPDse8NIV_3BHIu8dYe.png",
      "fullname": "Jianshu Zhang",
      "name": "Sterzhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.15589",
      "authors": [
        {
          "_id": "67bbfe2d670ece8d9184f339",
          "name": "Jintian Zhang",
          "hidden": false
        },
        {
          "_id": "67bbfe2d670ece8d9184f33a",
          "name": "Yuqi Zhu",
          "hidden": false
        },
        {
          "_id": "67bbfe2d670ece8d9184f33b",
          "name": "Mengshu Sun",
          "hidden": false
        },
        {
          "_id": "67bbfe2d670ece8d9184f33c",
          "name": "Yujie Luo",
          "hidden": false
        },
        {
          "_id": "67bbfe2d670ece8d9184f33d",
          "name": "Shuofei Qiao",
          "hidden": false
        },
        {
          "_id": "67bbfe2d670ece8d9184f33e",
          "name": "Lun Du",
          "hidden": false
        },
        {
          "_id": "67bbfe2d670ece8d9184f33f",
          "name": "Da Zheng",
          "hidden": false
        },
        {
          "_id": "67bbfe2d670ece8d9184f340",
          "name": "Huajun Chen",
          "hidden": false
        },
        {
          "_id": "67bbfe2d670ece8d9184f341",
          "name": "Ningyu Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-21T16:57:22.000Z",
      "title": "LightThinker: Thinking Step-by-Step Compression",
      "summary": "Large language models (LLMs) have shown remarkable performance in complex\nreasoning tasks, but their efficiency is hindered by the substantial memory and\ncomputational costs associated with generating lengthy tokens. In this paper,\nwe propose LightThinker, a novel method that enables LLMs to dynamically\ncompress intermediate thoughts during reasoning. Inspired by human cognitive\nprocesses, LightThinker compresses verbose thought steps into compact\nrepresentations and discards the original reasoning chains, thereby\nsignificantly reducing the number of tokens stored in the context window. This\nis achieved by training the model on when and how to perform compression\nthrough data construction, mapping hidden states to condensed gist tokens, and\ncreating specialized attention masks. Additionally, we introduce the Dependency\n(Dep) metric to quantify the degree of compression by measuring the reliance on\nhistorical tokens during generation. Extensive experiments on four datasets and\ntwo models show that LightThinker reduces peak memory usage and inference time,\nwhile maintaining competitive accuracy. Our work provides a new direction for\nimproving the efficiency of LLMs in complex reasoning tasks without sacrificing\nperformance. Code will be released at https://github.com/zjunlp/LightThinker.",
      "upvotes": 8,
      "discussionId": "67bbfe2f670ece8d9184f3a4"
    },
    "publishedAt": "2025-02-24T00:07:05.804Z",
    "title": "LightThinker: Thinking Step-by-Step Compression",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/620b3bbb0668e435407c8d0a/dhGMWf_tcPkvQlRm5DbD6.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15589.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620b3bbb0668e435407c8d0a",
      "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
      "fullname": "Ningyu Zhang",
      "name": "Ningyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 19
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.14494",
      "authors": [
        {
          "_id": "67b9dda03593f69f41cdb5d3",
          "name": "Jinnan Li",
          "hidden": false
        },
        {
          "_id": "67b9dda03593f69f41cdb5d4",
          "name": "Jinzhe Li",
          "hidden": false
        },
        {
          "_id": "67b9dda03593f69f41cdb5d5",
          "name": "Yue Wang",
          "hidden": false
        },
        {
          "_id": "67b9dda03593f69f41cdb5d6",
          "name": "Yi Chang",
          "hidden": false
        },
        {
          "_id": "67b9dda03593f69f41cdb5d7",
          "name": "Yuan Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T12:22:18.000Z",
      "title": "StructFlowBench: A Structured Flow Benchmark for Multi-turn Instruction\n  Following",
      "summary": "Multi-turn instruction following capability constitutes a core competency of\nlarge language models (LLMs) in real-world applications. Existing evaluation\nbenchmarks predominantly focus on fine-grained constraint satisfaction and\ndomain-specific capability assessment, yet overlook the crucial structural\ndependency between dialogue turns that distinguishes multi-turn from\nsingle-turn interactions. This structural dependency not only reflects user\nintent but also establishes a second dimension for instruction following\nevaluation beyond constraint satisfaction. To address this gap, we propose\nStructFlowBench, a multi-turn instruction following benchmark with structural\nflow modeling. The benchmark innovatively defines a structural flow framework\ncomprising six fundamental inter-turn relationships, which not only introduces\nnovel structural constraints for model evaluation but also serves as generation\nparameters for creating customized dialogue flows tailored to specific\nscenarios. Adopting established LLM-based automatic evaluation methodologies,\nwe conduct systematic evaluations of 13 leading open-source and closed-source\nLLMs. Experimental results reveal significant deficiencies in current models'\ncomprehension of multi-turn dialogue structures. The code is available at\nhttps://github.com/MLGroupJLU/StructFlowBench.",
      "upvotes": 8,
      "discussionId": "67b9dda13593f69f41cdb635"
    },
    "publishedAt": "2025-02-23T23:43:43.529Z",
    "title": "StructFlowBench: A Structured Flow Benchmark for Multi-turn Instruction Following",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14494.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "670e57b3391f1a7021182bff",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/N0tuHZVz8KFPCv8G1qUX2.png",
      "fullname": "Yuan Wu",
      "name": "WhiteCatY",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.14397",
      "authors": [
        {
          "_id": "67bbed806f2833ecccf914dd",
          "name": "Shijie Huang",
          "hidden": false
        },
        {
          "_id": "67bbed806f2833ecccf914de",
          "name": "Yiren Song",
          "hidden": false
        },
        {
          "_id": "67bbed806f2833ecccf914df",
          "name": "Yuxuan Zhang",
          "hidden": false
        },
        {
          "_id": "67bbed806f2833ecccf914e0",
          "name": "Hailong Guo",
          "hidden": false
        },
        {
          "_id": "67bbed806f2833ecccf914e1",
          "name": "Xueyin Wang",
          "hidden": false
        },
        {
          "_id": "67bbed806f2833ecccf914e2",
          "name": "Mike Zheng Shou",
          "hidden": false
        },
        {
          "_id": "67bbed806f2833ecccf914e3",
          "name": "Jiaming Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T09:35:38.000Z",
      "title": "PhotoDoodle: Learning Artistic Image Editing from Few-Shot Pairwise Data",
      "summary": "We introduce PhotoDoodle, a novel image editing framework designed to\nfacilitate photo doodling by enabling artists to overlay decorative elements\nonto photographs. Photo doodling is challenging because the inserted elements\nmust appear seamlessly integrated with the background, requiring realistic\nblending, perspective alignment, and contextual coherence. Additionally, the\nbackground must be preserved without distortion, and the artist's unique style\nmust be captured efficiently from limited training data. These requirements are\nnot addressed by previous methods that primarily focus on global style transfer\nor regional inpainting. The proposed method, PhotoDoodle, employs a two-stage\ntraining strategy. Initially, we train a general-purpose image editing model,\nOmniEditor, using large-scale data. Subsequently, we fine-tune this model with\nEditLoRA using a small, artist-curated dataset of before-and-after image pairs\nto capture distinct editing styles and techniques. To enhance consistency in\nthe generated results, we introduce a positional encoding reuse mechanism.\nAdditionally, we release a PhotoDoodle dataset featuring six high-quality\nstyles. Extensive experiments demonstrate the advanced performance and\nrobustness of our method in customized image editing, opening new possibilities\nfor artistic creation.",
      "upvotes": 7,
      "discussionId": "67bbed856f2833ecccf915c5"
    },
    "publishedAt": "2025-02-23T22:55:04.409Z",
    "title": "PhotoDoodle: Learning Artistic Image Editing from Few-Shot Pairwise Data",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14397.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "64311a95034ecbefddd141ef",
      "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
      "fullname": "Yiren Song",
      "name": "yiren98",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.15007",
      "authors": [
        {
          "_id": "67bc1a4a72499ce2ba28cc70",
          "name": "Anton Razzhigaev",
          "hidden": false
        },
        {
          "_id": "67bc1a4a72499ce2ba28cc71",
          "name": "Matvey Mikhalchuk",
          "hidden": false
        },
        {
          "_id": "67bc1a4a72499ce2ba28cc72",
          "name": "Temurbek Rahmatullaev",
          "hidden": false
        },
        {
          "_id": "67bc1a4a72499ce2ba28cc73",
          "name": "Elizaveta Goncharova",
          "hidden": false
        },
        {
          "_id": "67bc1a4a72499ce2ba28cc74",
          "name": "Polina Druzhinina",
          "hidden": false
        },
        {
          "_id": "67bc1a4a72499ce2ba28cc75",
          "name": "Ivan Oseledets",
          "hidden": false
        },
        {
          "_id": "67bc1a4a72499ce2ba28cc76",
          "name": "Andrey Kuznetsov",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T19:59:35.000Z",
      "title": "LLM-Microscope: Uncovering the Hidden Role of Punctuation in Context\n  Memory of Transformers",
      "summary": "We introduce methods to quantify how Large Language Models (LLMs) encode and\nstore contextual information, revealing that tokens often seen as minor (e.g.,\ndeterminers, punctuation) carry surprisingly high context. Notably, removing\nthese tokens -- especially stopwords, articles, and commas -- consistently\ndegrades performance on MMLU and BABILong-4k, even if removing only irrelevant\ntokens. Our analysis also shows a strong correlation between contextualization\nand linearity, where linearity measures how closely the transformation from one\nlayer's embeddings to the next can be approximated by a single linear mapping.\nThese findings underscore the hidden importance of filler tokens in maintaining\ncontext. For further exploration, we present LLM-Microscope, an open-source\ntoolkit that assesses token-level nonlinearity, evaluates contextual memory,\nvisualizes intermediate layer contributions (via an adapted Logit Lens), and\nmeasures the intrinsic dimensionality of representations. This toolkit\nilluminates how seemingly trivial tokens can be critical for long-range\nunderstanding.",
      "upvotes": 3,
      "discussionId": "67bc1a4c72499ce2ba28cd49"
    },
    "publishedAt": "2025-02-24T02:07:41.624Z",
    "title": "LLM-Microscope: Uncovering the Hidden Role of Punctuation in Context Memory of Transformers",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6172aaeec8e66e2aa84c06b9/ZPSmOQ-7Yd7B7YIYiwcTw.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15007.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6172aaeec8e66e2aa84c06b9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6172aaeec8e66e2aa84c06b9/ZdRZSp3P1SU6CIDbvQwkv.jpeg",
      "fullname": "Anton Razzhigaev",
      "name": "razzant",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.15657",
      "authors": [
        {
          "_id": "67bbfd6c3593f69f41512d54",
          "name": "Yoshua Bengio",
          "hidden": false
        },
        {
          "_id": "67bbfd6c3593f69f41512d55",
          "name": "Michael Cohen",
          "hidden": false
        },
        {
          "_id": "67bbfd6c3593f69f41512d56",
          "name": "Damiano Fornasiere",
          "hidden": false
        },
        {
          "_id": "67bbfd6c3593f69f41512d57",
          "name": "Joumana Ghosn",
          "hidden": false
        },
        {
          "_id": "67bbfd6c3593f69f41512d58",
          "name": "Pietro Greiner",
          "hidden": false
        },
        {
          "_id": "67bbfd6c3593f69f41512d59",
          "name": "Matt MacDermott",
          "hidden": false
        },
        {
          "_id": "67bbfd6c3593f69f41512d5a",
          "name": "Sören Mindermann",
          "hidden": false
        },
        {
          "_id": "67bbfd6c3593f69f41512d5b",
          "name": "Adam Oberman",
          "hidden": false
        },
        {
          "_id": "67bbfd6c3593f69f41512d5c",
          "name": "Jesse Richardson",
          "hidden": false
        },
        {
          "_id": "67bbfd6c3593f69f41512d5d",
          "name": "Oliver Richardson",
          "hidden": false
        },
        {
          "_id": "67bbfd6c3593f69f41512d5e",
          "name": "Marc-Antoine Rondeau",
          "hidden": false
        },
        {
          "_id": "67bbfd6c3593f69f41512d5f",
          "name": "Pierre-Luc St-Charles",
          "hidden": false
        },
        {
          "_id": "67bbfd6c3593f69f41512d60",
          "name": "David Williams-King",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-21T18:28:36.000Z",
      "title": "Superintelligent Agents Pose Catastrophic Risks: Can Scientist AI Offer\n  a Safer Path?",
      "summary": "The leading AI companies are increasingly focused on building generalist AI\nagents -- systems that can autonomously plan, act, and pursue goals across\nalmost all tasks that humans can perform. Despite how useful these systems\nmight be, unchecked AI agency poses significant risks to public safety and\nsecurity, ranging from misuse by malicious actors to a potentially irreversible\nloss of human control. We discuss how these risks arise from current AI\ntraining methods. Indeed, various scenarios and experiments have demonstrated\nthe possibility of AI agents engaging in deception or pursuing goals that were\nnot specified by human operators and that conflict with human interests, such\nas self-preservation. Following the precautionary principle, we see a strong\nneed for safer, yet still useful, alternatives to the current agency-driven\ntrajectory. Accordingly, we propose as a core building block for further\nadvances the development of a non-agentic AI system that is trustworthy and\nsafe by design, which we call Scientist AI. This system is designed to explain\nthe world from observations, as opposed to taking actions in it to imitate or\nplease humans. It comprises a world model that generates theories to explain\ndata and a question-answering inference machine. Both components operate with\nan explicit notion of uncertainty to mitigate the risks of overconfident\npredictions. In light of these considerations, a Scientist AI could be used to\nassist human researchers in accelerating scientific progress, including in AI\nsafety. In particular, our system can be employed as a guardrail against AI\nagents that might be created despite the risks involved. Ultimately, focusing\non non-agentic AI may enable the benefits of AI innovation while avoiding the\nrisks associated with the current trajectory. We hope these arguments will\nmotivate researchers, developers, and policymakers to favor this safer path.",
      "upvotes": 3,
      "discussionId": "67bbfd6c3593f69f41512d96"
    },
    "publishedAt": "2025-02-24T00:02:52.495Z",
    "title": "Superintelligent Agents Pose Catastrophic Risks: Can Scientist AI Offer a Safer Path?",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15657.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 63
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.15631",
      "authors": [
        {
          "_id": "67bbdbe8ea3003f47f15d036",
          "name": "Marthe Ballon",
          "hidden": false
        },
        {
          "_id": "67bbdbe8ea3003f47f15d037",
          "name": "Andres Algaba",
          "hidden": false
        },
        {
          "_id": "67bbdbe8ea3003f47f15d038",
          "name": "Vincent Ginis",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-21T17:59:13.000Z",
      "title": "The Relationship Between Reasoning and Performance in Large Language\n  Models -- o3 (mini) Thinks Harder, Not Longer",
      "summary": "Large language models have demonstrated remarkable progress in mathematical\nreasoning, leveraging chain-of-thought and test-time compute scaling. However,\nmany open questions remain regarding the interplay between reasoning token\nusage and accuracy gains. In particular, when comparing models across\ngenerations, it is unclear whether improved performance results from longer\nreasoning chains or more efficient reasoning. We systematically analyze\nchain-of-thought length across o1-mini and o3-mini variants on the Omni-MATH\nbenchmark, finding that o3-mini (m) achieves superior accuracy without\nrequiring longer reasoning chains than o1-mini. Moreover, we show that accuracy\ngenerally declines as reasoning chains grow across all models and compute\nsettings, even when controlling for difficulty of the questions. This accuracy\ndrop is significantly smaller in more proficient models, suggesting that new\ngenerations of reasoning models use test-time compute more effectively.\nFinally, we highlight that while o3-mini (h) achieves a marginal accuracy gain\nover o3-mini (m), it does so by allocating substantially more reasoning tokens\nacross all problems, even the ones that o3-mini (m) can already solve. These\nfindings provide new insights into the relationship between model capability\nand reasoning length, with implications for efficiency, scaling, and evaluation\nmethodologies.",
      "upvotes": 3,
      "discussionId": "67bbdbefea3003f47f15d226"
    },
    "publishedAt": "2025-02-23T21:40:17.216Z",
    "title": "The Relationship Between Reasoning and Performance in Large Language Models -- o3 (mini) Thinks Harder, Not Longer",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15631.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6190
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.14905",
      "authors": [
        {
          "_id": "67bbe0520aabd5d571a723e7",
          "name": "Bhavik Agarwal",
          "hidden": false
        },
        {
          "_id": "67bbe0520aabd5d571a723e8",
          "name": "Ishan Joshi",
          "hidden": false
        },
        {
          "_id": "67bbe0520aabd5d571a723e9",
          "name": "Viktoria Rojkova",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T16:44:55.000Z",
      "title": "Think Inside the JSON: Reinforcement Strategy for Strict LLM Schema\n  Adherence",
      "summary": "In this paper, we address the challenge of enforcing strict schema adherence\nin large language model (LLM) generation by leveraging LLM reasoning\ncapabilities. Building on the DeepSeek R1 reinforcement learning framework, our\napproach trains structured reasoning skills of a 1.5B parameter model through a\nnovel pipeline that combines synthetic reasoning dataset construction with\ncustom reward functions under Group Relative Policy Optimization (GRPO).\nSpecifically, we first perform R1 reinforcement learning on a 20K sample\nunstructured-to-structured dataset, mirroring the original DeepSeek R1 methods,\nto establish core reasoning abilities. Subsequently, we performed supervised\nfine-tuning on a separate 10K reasoning sample dataset, focusing on refining\nschema adherence for downstream tasks. Despite the relatively modest training\nscope, requiring approximately 20 hours on an 8xH100 GPU cluster for GRPO\ntraining and 3 hours on 1xA100 for SFT, our model demonstrates robust\nperformance in enforcing schema consistency. We compare our ThinkJSON approach\nagainst the original DeepSeek R1 (671B), distilled versions of DeepSeek R1\n(Qwen-1.5B and Qwen-7B), and Gemini 2.0 Flash (70B), showcasing its\neffectiveness in real-world applications. Our results underscore the practical\nutility of a resource-efficient framework for schema-constrained text\ngeneration.",
      "upvotes": 2,
      "discussionId": "67bbe0530aabd5d571a72437"
    },
    "publishedAt": "2025-02-23T22:11:17.789Z",
    "title": "Think Inside the JSON: Reinforcement Strategy for Strict LLM Schema Adherence",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14905.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6190
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.15681",
      "authors": [
        {
          "_id": "67bbe67c7727595ca5979d2a",
          "name": "Yilun Xu",
          "hidden": false
        },
        {
          "_id": "67bbe67c7727595ca5979d2b",
          "name": "Weili Nie",
          "hidden": false
        },
        {
          "_id": "67bbe67c7727595ca5979d2c",
          "name": "Arash Vahdat",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-21T18:59:20.000Z",
      "title": "One-step Diffusion Models with f-Divergence Distribution Matching",
      "summary": "Sampling from diffusion models involves a slow iterative process that hinders\ntheir practical deployment, especially for interactive applications. To\naccelerate generation speed, recent approaches distill a multi-step diffusion\nmodel into a single-step student generator via variational score distillation,\nwhich matches the distribution of samples generated by the student to the\nteacher's distribution. However, these approaches use the reverse\nKullback-Leibler (KL) divergence for distribution matching which is known to be\nmode seeking. In this paper, we generalize the distribution matching approach\nusing a novel f-divergence minimization framework, termed f-distill, that\ncovers different divergences with different trade-offs in terms of mode\ncoverage and training variance. We derive the gradient of the f-divergence\nbetween the teacher and student distributions and show that it is expressed as\nthe product of their score differences and a weighting function determined by\ntheir density ratio. This weighting function naturally emphasizes samples with\nhigher density in the teacher distribution, when using a less mode-seeking\ndivergence. We observe that the popular variational score distillation approach\nusing the reverse-KL divergence is a special case within our framework.\nEmpirically, we demonstrate that alternative f-divergences, such as\nforward-KL and Jensen-Shannon divergences, outperform the current best\nvariational score distillation methods across image generation tasks. In\nparticular, when using Jensen-Shannon divergence, f-distill achieves current\nstate-of-the-art one-step generation performance on ImageNet64 and zero-shot\ntext-to-image generation on MS-COCO. Project page:\nhttps://research.nvidia.com/labs/genair/f-distill",
      "upvotes": 1,
      "discussionId": "67bbe6837727595ca5979e8c"
    },
    "publishedAt": "2025-02-23T22:24:55.500Z",
    "title": "One-step Diffusion Models with $f$-Divergence Distribution Matching",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15681.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6190
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.15027",
      "authors": [
        {
          "_id": "67bbdcec79fcd85f09ddd869",
          "name": "Henry Hengyuan Zhao",
          "hidden": false
        },
        {
          "_id": "67bbdcec79fcd85f09ddd86a",
          "name": "Wenqi Pei",
          "hidden": false
        },
        {
          "_id": "67bbdcec79fcd85f09ddd86b",
          "name": "Yifei Tao",
          "hidden": false
        },
        {
          "_id": "67bbdcec79fcd85f09ddd86c",
          "name": "Haiyang Mei",
          "hidden": false
        },
        {
          "_id": "67bbdcec79fcd85f09ddd86d",
          "name": "Mike Zheng Shou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T20:27:06.000Z",
      "title": "InterFeedback: Unveiling Interactive Intelligence of Large Multimodal\n  Models via Human Feedback",
      "summary": "Existing benchmarks do not test Large Multimodal Models (LMMs) on their\ninteractive intelligence with human users which is vital for developing\ngeneral-purpose AI assistants. We design InterFeedback, an interactive\nframework, which can be applied to any LMM and dataset to assess this ability\nautonomously. On top of this, we introduce InterFeedback-Bench which evaluates\ninteractive intelligence using two representative datasets, MMMU-Pro and\nMathVerse, to test 10 different open-source LMMs. Additionally, we present\nInterFeedback-Human, a newly collected dataset of 120 cases designed for\nmanually testing interactive performance in leading models such as OpenAI-o1\nand Claude-3.5-Sonnet. Our evaluation results show that even state-of-the-art\nLMM (like OpenAI-o1) can correct their results through human feedback less than\n50%. Our findings point to the need for methods that can enhance the LMMs'\ncapability to interpret and benefit from feedback.",
      "upvotes": 1,
      "discussionId": "67bbdced79fcd85f09ddd8da"
    },
    "publishedAt": "2025-02-23T21:44:33.443Z",
    "title": "InterFeedback: Unveiling Interactive Intelligence of Large Multimodal Models via Human Feedback",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15027.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6190
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.15011",
      "authors": [
        {
          "_id": "67bc0d12ffc2c387329c8cfd",
          "name": "Sayan Deb Sarkar",
          "hidden": false
        },
        {
          "_id": "67bc0d12ffc2c387329c8cfe",
          "name": "Ondrej Miksik",
          "hidden": false
        },
        {
          "_id": "67bc0d12ffc2c387329c8cff",
          "name": "Marc Pollefeys",
          "hidden": false
        },
        {
          "_id": "67bc0d12ffc2c387329c8d00",
          "name": "Daniel Barath",
          "hidden": false
        },
        {
          "_id": "67bc0d12ffc2c387329c8d01",
          "name": "Iro Armeni",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T20:05:30.000Z",
      "title": "CrossOver: 3D Scene Cross-Modal Alignment",
      "summary": "Multi-modal 3D object understanding has gained significant attention, yet\ncurrent approaches often assume complete data availability and rigid alignment\nacross all modalities. We present CrossOver, a novel framework for cross-modal\n3D scene understanding via flexible, scene-level modality alignment. Unlike\ntraditional methods that require aligned modality data for every object\ninstance, CrossOver learns a unified, modality-agnostic embedding space for\nscenes by aligning modalities - RGB images, point clouds, CAD models,\nfloorplans, and text descriptions - with relaxed constraints and without\nexplicit object semantics. Leveraging dimensionality-specific encoders, a\nmulti-stage training pipeline, and emergent cross-modal behaviors, CrossOver\nsupports robust scene retrieval and object localization, even with missing\nmodalities. Evaluations on ScanNet and 3RScan datasets show its superior\nperformance across diverse metrics, highlighting adaptability for real-world\napplications in 3D scene understanding.",
      "upvotes": 0,
      "discussionId": "67bc0d18ffc2c387329c8e56"
    },
    "publishedAt": "2025-02-24T01:13:24.911Z",
    "title": "CrossOver: 3D Scene Cross-Modal Alignment",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/650ec19e6620b0c57e2a551b/S_xFBPoV3YbtHmtLtRrSV.gif"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15011.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "650ec19e6620b0c57e2a551b",
      "avatarUrl": "/avatars/c26c03fa920d857120f03c9ccb9f1d7a.svg",
      "fullname": "Sayan Deb Sarkar",
      "name": "sayandsarkar",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.15082",
      "authors": [
        {
          "_id": "67bbe93f267aa2b537b318be",
          "name": "Vaidehi Patil",
          "hidden": false
        },
        {
          "_id": "67bbe93f267aa2b537b318bf",
          "name": "Elias Stengel-Eskin",
          "hidden": false
        },
        {
          "_id": "67bbe93f267aa2b537b318c0",
          "name": "Mohit Bansal",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T22:51:10.000Z",
      "title": "UPCORE: Utility-Preserving Coreset Selection for Balanced Unlearning",
      "summary": "User specifications or legal frameworks often require information to be\nremoved from pretrained models, including large language models (LLMs). This\nrequires deleting or \"forgetting\" a set of data points from an already-trained\nmodel, which typically degrades its performance on other data points. Thus, a\nbalance must be struck between removing information and keeping the model's\nother abilities intact, with a failure to balance this trade-off leading to\npoor deletion or an unusable model. To this end, we propose UPCORE\n(Utility-Preserving Coreset Selection), a method-agnostic data selection\nframework for mitigating collateral damage during unlearning. Finding that the\nmodel damage is correlated with the variance of the model's representations on\nthe forget set, we selectively prune the forget set to remove outliers, thereby\nminimizing model degradation after unlearning. We evaluate UPCORE across three\nstandard unlearning methods consistently achieving a superior balance between\nthe competing objectives of deletion efficacy and model preservation. To better\nevaluate this trade-off, we introduce a new metric, measuring the\narea-under-the-curve (AUC) across standard metrics. We find that UPCORE\nimproves both standard metrics and AUC, benefitting from positive transfer\nbetween the coreset and pruned points while reducing negative transfer from the\nforget set to points outside of it.",
      "upvotes": 0,
      "discussionId": "67bbe940267aa2b537b318f4"
    },
    "publishedAt": "2025-02-23T23:17:33.152Z",
    "title": "UPCORE: Utility-Preserving Coreset Selection for Balanced Unlearning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15082.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64f64da90efa33bfe0a3d9ba",
      "avatarUrl": "/avatars/c45fb015433e46a2eeb9518910f75d35.svg",
      "fullname": "Vaidehi Patil",
      "name": "vaidehi99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]