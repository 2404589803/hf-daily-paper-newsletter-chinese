[
    "{'paper': {'id': '2501.02976', 'authors': [{'_id': '677c986c1b9a7499c3644ef0', 'user': {'_id': '660a7ecf14cfe4973e0acfe1', 'avatarUrl': '/avatars/e488058397f2b7a617515a4f721a9a00.svg', 'isPro': False, 'fullname': 'Rui Xie', 'user': 'SherryX', 'type': 'user'}, 'name': 'Rui Xie', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-07T08:41:59.722Z', 'hidden': False}, {'_id': '677c986c1b9a7499c3644ef1', 'user': {'_id': '6548f92f5a65d02afb4a3021', 'avatarUrl': '/avatars/6bde44107ea0d1986356563f0738d2b8.svg', 'isPro': False, 'fullname': 'Yinhong Liu', 'user': 'Solitude-liu', 'type': 'user'}, 'name': 'Yinhong Liu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-07T13:16:18.989Z', 'hidden': False}, {'_id': '677c986c1b9a7499c3644ef2', 'user': {'_id': '667e5d480f66814a82e56420', 'avatarUrl': '/avatars/33fd797c7690959fed7449666a2502bf.svg', 'isPro': False, 'fullname': 'Zhou', 'user': 'LeonZhouph', 'type': 'user'}, 'name': 'Penghao Zhou', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-07T13:02:04.376Z', 'hidden': False}, {'_id': '677c986c1b9a7499c3644ef3', 'user': {'_id': '660103ec4ae78d4ded4633fc', 'avatarUrl': '/avatars/efce106d70f5d092bf44d0638aa49984.svg', 'isPro': False, 'fullname': 'CHEN Zhao', 'user': 'chenzhao', 'type': 'user'}, 'name': 'Chen Zhao', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-07T09:57:59.285Z', 'hidden': False}, {'_id': '677c986c1b9a7499c3644ef4', 'name': 'Jun Zhou', 'hidden': False}, {'_id': '677c986c1b9a7499c3644ef5', 'name': 'Kai Zhang', 'hidden': False}, {'_id': '677c986c1b9a7499c3644ef6', 'name': 'Zhenyu Zhang', 'hidden': False}, {'_id': '677c986c1b9a7499c3644ef7', 'name': 'Jian Yang', 'hidden': False}, {'_id': '677c986c1b9a7499c3644ef8', 'user': {'_id': '6421183b69a2c2933882d652', 'avatarUrl': '/avatars/66813a8fa22915087cccd4dbfb945ca7.svg', 'isPro': False, 'fullname': 'Zhenheng Yang', 'user': 'zhenheny', 'type': 'user'}, 'name': 'Zhenheng Yang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-07T09:58:14.804Z', 'hidden': False}, {'_id': '677c986c1b9a7499c3644ef9', 'user': {'_id': '65734004769f3ee9bde1af10', 'avatarUrl': '/avatars/d6310ed861972fd691687d8f47413f33.svg', 'isPro': False, 'fullname': 'Ying Tai', 'user': 'yingtai', 'type': 'user'}, 'name': 'Ying Tai', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-07T09:58:21.644Z', 'hidden': False}], 'publishedAt': '2025-01-06T12:36:21.000Z', 'title': 'STAR: Spatial-Temporal Augmentation with Text-to-Video Models for\\n  Real-World Video Super-Resolution', 'summary': 'Image diffusion models have been adapted for real-world video\\nsuper-resolution to tackle over-smoothing issues in GAN-based methods. However,\\nthese models struggle to maintain temporal consistency, as they are trained on\\nstatic images, limiting their ability to capture temporal dynamics effectively.\\nIntegrating text-to-video (T2V) models into video super-resolution for improved\\ntemporal modeling is straightforward. However, two key challenges remain:\\nartifacts introduced by complex degradations in real-world scenarios, and\\ncompromised fidelity due to the strong generative capacity of powerful T2V\\nmodels (e.g., CogVideoX-5B). To enhance the spatio-temporal quality of\\nrestored videos, we introduce~\\\\name\\n(Spatial-Temporal Augmentation with T2V models for\\nReal-world video super-resolution), a novel approach that leverages\\nT2V models for real-world video super-resolution, achieving realistic spatial\\ndetails and robust temporal consistency. Specifically, we introduce a Local\\nInformation Enhancement Module (LIEM) before the global attention block to\\nenrich local details and mitigate degradation artifacts. Moreover, we propose a\\nDynamic Frequency (DF) Loss to reinforce fidelity, guiding the model to focus\\non different frequency components across diffusion steps. Extensive experiments\\ndemonstrate~\\\\name~outperforms state-of-the-art methods on both\\nsynthetic and real-world datasets.', 'upvotes': 35, 'discussionId': '677c986e1b9a7499c3644fb5'}, 'publishedAt': '2025-01-06T22:01:52.386Z', 'title': 'STAR: Spatial-Temporal Augmentation with Text-to-Video Models for Real-World Video Super-Resolution', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.02976.png', 'numComments': 1, 'submittedBy': {'_id': '660a7ecf14cfe4973e0acfe1', 'avatarUrl': '/avatars/e488058397f2b7a617515a4f721a9a00.svg', 'fullname': 'Rui Xie', 'name': 'SherryX', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2501.03226', 'authors': [{'_id': '677cdcd50604b68871999e0f', 'user': {'_id': '64b93578ee257c3a4cfceed1', 'avatarUrl': '/avatars/e6188562254f75a09b4048b800860016.svg', 'isPro': False, 'fullname': 'Beichen Zhang', 'user': 'BeichenZhang', 'type': 'user'}, 'name': 'Beichen Zhang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-07T09:37:59.472Z', 'hidden': False}, {'_id': '677cdcd50604b68871999e10', 'name': 'Yuhong Liu', 'hidden': False}, {'_id': '677cdcd50604b68871999e11', 'name': 'Xiaoyi Dong', 'hidden': False}, {'_id': '677cdcd50604b68871999e12', 'user': {'_id': '63859cf3b2906edaf83af9f0', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/63859cf3b2906edaf83af9f0/iUQm5FAomzqYi6fkqIn9F.jpeg', 'isPro': False, 'fullname': 'Yuhang Zang', 'user': 'yuhangzang', 'type': 'user'}, 'name': 'Yuhang Zang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-07T08:41:16.697Z', 'hidden': False}, {'_id': '677cdcd50604b68871999e13', 'name': 'Pan Zhang', 'hidden': False}, {'_id': '677cdcd50604b68871999e14', 'user': {'_id': '63ee1379190ddd6214efd73a', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1676546883247-noauth.png', 'isPro': False, 'fullname': 'HAODONG DUAN', 'user': 'KennyUTC', 'type': 'user'}, 'name': 'Haodong Duan', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-07T08:57:58.362Z', 'hidden': False}, {'_id': '677cdcd50604b68871999e15', 'user': {'_id': '65000bef18830fabea469fdd', 'avatarUrl': '/avatars/b320c77dfad039d9f9c54127f610d44f.svg', 'isPro': False, 'fullname': 'Cao Yuhang', 'user': 'yhcao', 'type': 'user'}, 'name': 'Yuhang Cao', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-07T08:57:51.041Z', 'hidden': False}, {'_id': '677cdcd50604b68871999e16', 'user': {'_id': '636317ed80c1a705a6eff396', 'avatarUrl': '/avatars/3db090e101b916d9256d0d3e043db71d.svg', 'isPro': False, 'fullname': 'Dahua Lin', 'user': 'lindahua', 'type': 'user'}, 'name': 'Dahua Lin', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-07T08:56:41.557Z', 'hidden': False}, {'_id': '677cdcd50604b68871999e17', 'user': {'_id': '64b4eec4faa3181a5eab9c46', 'avatarUrl': '/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg', 'isPro': True, 'fullname': 'Jiaqi Wang', 'user': 'myownskyW7', 'type': 'user'}, 'name': 'Jiaqi Wang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-07T09:37:28.234Z', 'hidden': False}], 'publishedAt': '2025-01-06T18:59:13.000Z', 'title': 'BoostStep: Boosting mathematical capability of Large Language Models via\\n  improved single-step reasoning', 'summary': \"Cutting-edge large language models (LLMs) demonstrate promising performance\\nin solving complex math problems with a divide-and-conquer pipeline and the\\nassistance of in-context learning (ICL) examples. However, their potential for\\nimprovement is limited by two critical problems within their ICL examples:\\ngranularity-mismatch and the ensuing negative-effect noise problem.\\nSpecifically, the LLMs are capable of the dividing process yet mostly failed by\\ninaccurate reasoning within a few conquer steps, while the ICL examples\\nretrieved in question-grained sometimes lack relevant steps for a specific\\nchallenging reasoning step. Further, this disconnect may hinder the correct\\nreasoning due to its irrelevance. To this end, we focus on improving the\\nreasoning quality within each step and present BoostStep. BoostStep aligns the\\ngranularity between the retrieving and reasoning on step grained, and provides\\nhighly related ICL examples for each reasoning step with a novel `first-try'\\nstrategy. BoostStep provides more relevant examples than the coarse\\nquestion-grained strategy, enhancing the model reasoning quality within each\\nstep steadily. BoostStep is a general and robust reasoning-enhancing method\\nthat not only improves standalone reasoning performance but also integrates\\nseamlessly with Monte Carlo Tree Search methods (MCTS) to refine both candidate\\ngeneration and decision-making. Quantitatively, it improves GPT-4o and\\nQwen2.5-Math-72B by 3.6\\\\% and 2.0\\\\% respectively on various mathematical\\nbenchmarks, and 7.5\\\\% gain combined with MCTS.\", 'upvotes': 20, 'discussionId': '677cdcd60604b68871999e7b'}, 'publishedAt': '2025-01-07T02:52:35.633Z', 'title': 'BoostStep: Boosting mathematical capability of Large Language Models via improved single-step reasoning', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.03226.png', 'numComments': 1, 'submittedBy': {'_id': '64b4eec4faa3181a5eab9c46', 'avatarUrl': '/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg', 'fullname': 'Jiaqi Wang', 'name': 'myownskyW7', 'type': 'user', 'isPro': True, 'isHf': False, 'isMod': False, 'followerCount': 13}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2501.03218', 'authors': [{'_id': '677cdc004804091804c654c6', 'name': 'Rui Qian', 'hidden': False}, {'_id': '677cdc004804091804c654c7', 'user': {'_id': '65a7c0335e79abfa2ec30c52', 'avatarUrl': '/avatars/2f62f83f9c5c4cc9444571f067cd85b7.svg', 'isPro': True, 'fullname': 'Shuangrui Ding', 'user': 'Mar2Ding', 'type': 'user'}, 'name': 'Shuangrui Ding', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-07T09:47:38.104Z', 'hidden': False}, {'_id': '677cdc004804091804c654c8', 'name': 'Xiaoyi Dong', 'hidden': False}, {'_id': '677cdc004804091804c654c9', 'name': 'Pan Zhang', 'hidden': False}, {'_id': '677cdc004804091804c654ca', 'user': {'_id': '63859cf3b2906edaf83af9f0', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/63859cf3b2906edaf83af9f0/iUQm5FAomzqYi6fkqIn9F.jpeg', 'isPro': False, 'fullname': 'Yuhang Zang', 'user': 'yuhangzang', 'type': 'user'}, 'name': 'Yuhang Zang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-07T08:41:19.447Z', 'hidden': False}, {'_id': '677cdc004804091804c654cb', 'user': {'_id': '65000bef18830fabea469fdd', 'avatarUrl': '/avatars/b320c77dfad039d9f9c54127f610d44f.svg', 'isPro': False, 'fullname': 'Cao Yuhang', 'user': 'yhcao', 'type': 'user'}, 'name': 'Yuhang Cao', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-07T09:47:16.734Z', 'hidden': False}, {'_id': '677cdc004804091804c654cc', 'user': {'_id': '636317ed80c1a705a6eff396', 'avatarUrl': '/avatars/3db090e101b916d9256d0d3e043db71d.svg', 'isPro': False, 'fullname': 'Dahua Lin', 'user': 'lindahua', 'type': 'user'}, 'name': 'Dahua Lin', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-07T09:47:00.823Z', 'hidden': False}, {'_id': '677cdc004804091804c654cd', 'user': {'_id': '64b4eec4faa3181a5eab9c46', 'avatarUrl': '/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg', 'isPro': True, 'fullname': 'Jiaqi Wang', 'user': 'myownskyW7', 'type': 'user'}, 'name': 'Jiaqi Wang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-07T09:46:54.181Z', 'hidden': False}], 'publishedAt': '2025-01-06T18:55:10.000Z', 'title': 'Dispider: Enabling Video LLMs with Active Real-Time Interaction via\\n  Disentangled Perception, Decision, and Reaction', 'summary': 'Active Real-time interaction with video LLMs introduces a new paradigm for\\nhuman-computer interaction, where the model not only understands user intent\\nbut also responds while continuously processing streaming video on the fly.\\nUnlike offline video LLMs, which analyze the entire video before answering\\nquestions, active real-time interaction requires three capabilities: 1)\\nPerception: real-time video monitoring and interaction capturing. 2) Decision:\\nraising proactive interaction in proper situations, 3) Reaction: continuous\\ninteraction with users. However, inherent conflicts exist among the desired\\ncapabilities. The Decision and Reaction require a contrary Perception scale and\\ngrain, and the autoregressive decoding blocks the real-time Perception and\\nDecision during the Reaction. To unify the conflicted capabilities within a\\nharmonious system, we present Dispider, a system that disentangles Perception,\\nDecision, and Reaction. Dispider features a lightweight proactive streaming\\nvideo processing module that tracks the video stream and identifies optimal\\nmoments for interaction. Once the interaction is triggered, an asynchronous\\ninteraction module provides detailed responses, while the processing module\\ncontinues to monitor the video in the meantime. Our disentangled and\\nasynchronous design ensures timely, contextually accurate, and computationally\\nefficient responses, making Dispider ideal for active real-time interaction for\\nlong-duration video streams. Experiments show that Dispider not only maintains\\nstrong performance in conventional video QA tasks, but also significantly\\nsurpasses previous online models in streaming scenario responses, thereby\\nvalidating the effectiveness of our architecture. The code and model are\\nreleased at https://github.com/Mark12Ding/Dispider.', 'upvotes': 19, 'discussionId': '677cdc014804091804c6552e'}, 'publishedAt': '2025-01-07T02:47:54.643Z', 'title': 'Dispider: Enabling Video LLMs with Active Real-Time Interaction via Disentangled Perception, Decision, and Reaction', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.03218.png', 'numComments': 1, 'submittedBy': {'_id': '64b4eec4faa3181a5eab9c46', 'avatarUrl': '/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg', 'fullname': 'Jiaqi Wang', 'name': 'myownskyW7', 'type': 'user', 'isPro': True, 'isHf': False, 'isMod': False, 'followerCount': 13}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2501.02157', 'authors': [{'_id': '677c9728ae2cce31b6aa977b', 'user': {'_id': '65dd26cf5012ec503f0137d1', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/jPhOKOrd9axHhUfa7SFic.jpeg', 'isPro': False, 'fullname': 'Steven Au', 'user': 'StevenAu', 'type': 'user'}, 'name': 'Steven Au', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-07T09:48:43.716Z', 'hidden': False}, {'_id': '677c9728ae2cce31b6aa977c', 'user': {'_id': '63888721f67ad3caa9d2701d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1669891869519-noauth.png', 'isPro': False, 'fullname': 'Cameron Dimacali', 'user': 'Tobilee', 'type': 'user'}, 'name': 'Cameron J. Dimacali', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-07T09:49:00.825Z', 'hidden': False}, {'_id': '677c9728ae2cce31b6aa977d', 'user': {'_id': '6561171fe0a7720b6ae89483', 'avatarUrl': '/avatars/34e3b8f41084811b9a5f8022b698f3a9.svg', 'isPro': False, 'fullname': 'Ojasmitha Pedirappagari', 'user': 'Ojasmitha17', 'type': 'user'}, 'name': 'Ojasmitha Pedirappagari', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-07T09:49:08.187Z', 'hidden': False}, {'_id': '677c9728ae2cce31b6aa977e', 'user': {'_id': '65f9ecb65ff1f80a69598e33', 'avatarUrl': '/avatars/e0dd617533d9d7d6167a8dc4383ae08e.svg', 'isPro': False, 'fullname': 'Namyong Park', 'user': 'namyongp', 'type': 'user'}, 'name': 'Namyong Park', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-07T09:49:14.438Z', 'hidden': False}, {'_id': '677c9728ae2cce31b6aa977f', 'user': {'_id': '62c5947524171688a9feb992', 'avatarUrl': '/avatars/5a151713b9eae8dc566f5957acee3475.svg', 'isPro': False, 'fullname': 'Franck Dernoncourt', 'user': 'Franck-Dernoncourt', 'type': 'user'}, 'name': 'Franck Dernoncourt', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-07T08:42:05.220Z', 'hidden': False}, {'_id': '677c9728ae2cce31b6aa9780', 'name': 'Yu Wang', 'hidden': False}, {'_id': '677c9728ae2cce31b6aa9781', 'user': {'_id': '64c3d7543e30498cca75acaf', 'avatarUrl': '/avatars/9608497e2b4d0a57cff69843fd09a077.svg', 'isPro': False, 'fullname': 'Nikos Kanakaris', 'user': 'nkanak', 'type': 'user'}, 'name': 'Nikos Kanakaris', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-07T08:42:01.570Z', 'hidden': False}, {'_id': '677c9728ae2cce31b6aa9782', 'name': 'Hanieh Deilamsalehy', 'hidden': False}, {'_id': '677c9728ae2cce31b6aa9783', 'user': {'_id': '62a3ab83e4dd6252344d27cd', 'avatarUrl': '/avatars/7ca8510f70a58dc207b104240e30c35c.svg', 'isPro': False, 'fullname': 'Ryan A. Rossi', 'user': 'ryanrossi', 'type': 'user'}, 'name': 'Ryan A. Rossi', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-07T09:49:50.928Z', 'hidden': False}, {'_id': '677c9728ae2cce31b6aa9784', 'user': {'_id': '663e588ccca86c8371a913d9', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/663e588ccca86c8371a913d9/wgVGw5U77cqwtjovhsXyR.jpeg', 'isPro': False, 'fullname': 'Nesreen Ahmed', 'user': 'nkahmed', 'type': 'user'}, 'name': 'Nesreen K. Ahmed', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-07T08:42:03.376Z', 'hidden': False}], 'publishedAt': '2025-01-04T01:46:49.000Z', 'title': 'Personalized Graph-Based Retrieval for Large Language Models', 'summary': 'As large language models (LLMs) evolve, their ability to deliver personalized\\nand context-aware responses offers transformative potential for improving user\\nexperiences. Existing personalization approaches, however, often rely solely on\\nuser history to augment the prompt, limiting their effectiveness in generating\\ntailored outputs, especially in cold-start scenarios with sparse data. To\\naddress these limitations, we propose Personalized Graph-based\\nRetrieval-Augmented Generation (PGraphRAG), a framework that leverages\\nuser-centric knowledge graphs to enrich personalization. By directly\\nintegrating structured user knowledge into the retrieval process and augmenting\\nprompts with user-relevant context, PGraphRAG enhances contextual understanding\\nand output quality. We also introduce the Personalized Graph-based Benchmark\\nfor Text Generation, designed to evaluate personalized text generation tasks in\\nreal-world settings where user history is sparse or unavailable. Experimental\\nresults show that PGraphRAG significantly outperforms state-of-the-art\\npersonalization methods across diverse tasks, demonstrating the unique\\nadvantages of graph-based retrieval for personalization.', 'upvotes': 15, 'discussionId': '677c9729ae2cce31b6aa97c2'}, 'publishedAt': '2025-01-06T21:53:31.716Z', 'title': 'Personalized Graph-Based Retrieval for Large Language Models', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.02157.png', 'numComments': 1, 'submittedBy': {'_id': '62c5947524171688a9feb992', 'avatarUrl': '/avatars/5a151713b9eae8dc566f5957acee3475.svg', 'fullname': 'Franck Dernoncourt', 'name': 'Franck-Dernoncourt', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 6}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2501.02497', 'authors': [{'_id': '677ca7db0c9718b04a426305', 'user': {'_id': '63412a43a7582111c3f1cadd', 'avatarUrl': '/avatars/033e809a7aa3d9c4b1326e9195290f65.svg', 'isPro': False, 'fullname': 'Yixin Ji', 'user': 'Yisam', 'type': 'user'}, 'name': 'Yixin Ji', 'status': 'extracted_confirmed', 'statusLastChangedAt': '2025-01-07T04:53:11.414Z', 'hidden': False}, {'_id': '677ca7db0c9718b04a426306', 'user': {'_id': '6670e285b0c03c4e9d6e0985', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/uCZHm4gKSHZ2b0hpHWgZv.jpeg', 'isPro': False, 'fullname': 'Juntao Li', 'user': 'douvleplus', 'type': 'user'}, 'name': 'Juntao Li', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-07T09:51:35.155Z', 'hidden': False}, {'_id': '677ca7db0c9718b04a426307', 'user': {'_id': '6239888e7fef05b7bdd5fcff', 'avatarUrl': '/avatars/54fcc756b8c0936b6bb410c6e0e02d75.svg', 'isPro': False, 'fullname': 'Hai Ye', 'user': 'oceanpty', 'type': 'user'}, 'name': 'Hai Ye', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-07T13:02:02.646Z', 'hidden': False}, {'_id': '677ca7db0c9718b04a426308', 'name': 'Kaixin Wu', 'hidden': False}, {'_id': '677ca7db0c9718b04a426309', 'name': 'Jia Xu', 'hidden': False}, {'_id': '677ca7db0c9718b04a42630a', 'name': 'Linjian Mo', 'hidden': False}, {'_id': '677ca7db0c9718b04a42630b', 'name': 'Min Zhang', 'hidden': False}], 'publishedAt': '2025-01-05T10:24:20.000Z', 'title': 'Test-time Computing: from System-1 Thinking to System-2 Thinking', 'summary': \"The remarkable performance of the o1 model in complex reasoning demonstrates\\nthat test-time computing scaling can further unlock the model's potential,\\nenabling powerful System-2 thinking. However, there is still a lack of\\ncomprehensive surveys for test-time computing scaling. We trace the concept of\\ntest-time computing back to System-1 models. In System-1 models, test-time\\ncomputing addresses distribution shifts and improves robustness and\\ngeneralization through parameter updating, input modification, representation\\nediting, and output calibration. In System-2 models, it enhances the model's\\nreasoning ability to solve complex problems through repeated sampling,\\nself-correction, and tree search. We organize this survey according to the\\ntrend of System-1 to System-2 thinking, highlighting the key role of test-time\\ncomputing in the transition from System-1 models to weak System-2 models, and\\nthen to strong System-2 models. We also point out a few possible future\\ndirections.\", 'upvotes': 13, 'discussionId': '677ca7dc0c9718b04a426342'}, 'publishedAt': '2025-01-06T23:05:27.984Z', 'title': 'Test-time Computing: from System-1 Thinking to System-2 Thinking', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.02497.png', 'numComments': 1, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isMod': False, 'followerCount': 5581}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2501.02045', 'authors': [{'_id': '677ca28f1fbb93b90f3d2482', 'user': {'_id': '66197a8afeb55cbe39e50ae8', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/pcSS1TsCoHxRcAEkcMNm0.png', 'isPro': False, 'fullname': 'Ollie Liu', 'user': 'oliu-io', 'type': 'user'}, 'name': 'Ollie Liu', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-07T09:48:10.277Z', 'hidden': False}, {'_id': '677ca28f1fbb93b90f3d2483', 'name': 'Sami Jaghouar', 'hidden': False}, {'_id': '677ca28f1fbb93b90f3d2484', 'user': {'_id': '606ae9ab0392350f35a22e37', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1617619292851-noauth.jpeg', 'isPro': False, 'fullname': 'Johannes Hagemann', 'user': 'Johannes', 'type': 'user'}, 'name': 'Johannes Hagemann', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-07T09:48:26.938Z', 'hidden': False}, {'_id': '677ca28f1fbb93b90f3d2485', 'user': {'_id': '67469d6a8407f929491dce06', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/67469d6a8407f929491dce06/ZJ17ItOIG6X6yqqo4FQMp.jpeg', 'isPro': False, 'fullname': 'Shangshang Wang', 'user': 'upup-ashton-wang', 'type': 'user'}, 'name': 'Shangshang Wang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-07T08:41:44.794Z', 'hidden': False}, {'_id': '677ca28f1fbb93b90f3d2486', 'user': {'_id': '66197a8afeb55cbe39e50ae8', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/pcSS1TsCoHxRcAEkcMNm0.png', 'isPro': False, 'fullname': 'Ollie Liu', 'user': 'oliu-io', 'type': 'user'}, 'name': 'Jason Wiemels', 'status': 'extracted_confirmed', 'statusLastChangedAt': '2025-01-07T03:44:48.441Z', 'hidden': False}, {'_id': '677ca28f1fbb93b90f3d2487', 'name': 'Jeff Kaufman', 'hidden': False}, {'_id': '677ca28f1fbb93b90f3d2488', 'user': {'_id': '644bf65522d211df6444a7f4', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/644bf65522d211df6444a7f4/k_ddZdQDg2fzhwjI1EXyx.jpeg', 'isPro': False, 'fullname': 'Willie Neiswanger', 'user': 'willieneis', 'type': 'user'}, 'name': 'Willie Neiswanger', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-07T08:41:42.587Z', 'hidden': False}], 'publishedAt': '2025-01-03T18:44:43.000Z', 'title': 'METAGENE-1: Metagenomic Foundation Model for Pandemic Monitoring', 'summary': 'We pretrain METAGENE-1, a 7-billion-parameter autoregressive transformer\\nmodel, which we refer to as a metagenomic foundation model, on a novel corpus\\nof diverse metagenomic DNA and RNA sequences comprising over 1.5 trillion base\\npairs. This dataset is sourced from a large collection of human wastewater\\nsamples, processed and sequenced using deep metagenomic (next-generation)\\nsequencing methods. Unlike genomic models that focus on individual genomes or\\ncurated sets of specific species, the aim of METAGENE-1 is to capture the full\\ndistribution of genomic information present within this wastewater, to aid in\\ntasks relevant to pandemic monitoring and pathogen detection. We carry out\\nbyte-pair encoding (BPE) tokenization on our dataset, tailored for metagenomic\\nsequences, and then pretrain our model. In this paper, we first detail the\\npretraining dataset, tokenization strategy, and model architecture,\\nhighlighting the considerations and design choices that enable the effective\\nmodeling of metagenomic data. We then show results of pretraining this model on\\nour metagenomic dataset, providing details about our losses, system metrics,\\nand training stability over the course of pretraining. Finally, we demonstrate\\nthe performance of METAGENE-1, which achieves state-of-the-art results on a set\\nof genomic benchmarks and new evaluations focused on human-pathogen detection\\nand genomic sequence embedding, showcasing its potential for public health\\napplications in pandemic monitoring, biosurveillance, and early detection of\\nemerging health threats.', 'upvotes': 11, 'discussionId': '677ca2901fbb93b90f3d24db'}, 'publishedAt': '2025-01-06T22:44:23.327Z', 'title': 'METAGENE-1: Metagenomic Foundation Model for Pandemic Monitoring', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.02045.png', 'numComments': 1, 'submittedBy': {'_id': '66197a8afeb55cbe39e50ae8', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/pcSS1TsCoHxRcAEkcMNm0.png', 'fullname': 'Ollie Liu', 'name': 'oliu-io', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 6}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2501.02690', 'authors': [{'_id': '677cbd8ef7134ae168acc1fc', 'user': {'_id': '65b8b840a81930c593073370', 'avatarUrl': '/avatars/31b399d3da33e8c02f9e06a16dd85779.svg', 'isPro': False, 'fullname': 'Weikang BIAN', 'user': 'wkbian', 'type': 'user'}, 'name': 'Weikang Bian', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-07T13:01:59.093Z', 'hidden': False}, {'_id': '677cbd8ef7134ae168acc1fd', 'name': 'Zhaoyang Huang', 'hidden': False}, {'_id': '677cbd8ef7134ae168acc1fe', 'name': 'Xiaoyu Shi', 'hidden': False}, {'_id': '677cbd8ef7134ae168acc1ff', 'name': 'Yijin Li', 'hidden': False}, {'_id': '677cbd8ef7134ae168acc200', 'user': {'_id': '63e9e92f20c109718713f5eb', 'avatarUrl': '/avatars/9ff312e854d803e1a2e9e685a21d12f8.svg', 'isPro': False, 'fullname': 'Fu-Yun Wang', 'user': 'wangfuyun', 'type': 'user'}, 'name': 'Fu-Yun Wang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-07T08:41:34.222Z', 'hidden': False}, {'_id': '677cbd8ef7134ae168acc201', 'name': 'Hongsheng Li', 'hidden': False}], 'publishedAt': '2025-01-05T23:55:33.000Z', 'title': 'GS-DiT: Advancing Video Generation with Pseudo 4D Gaussian Fields\\n  through Efficient Dense 3D Point Tracking', 'summary': '4D video control is essential in video generation as it enables the use of\\nsophisticated lens techniques, such as multi-camera shooting and dolly zoom,\\nwhich are currently unsupported by existing methods. Training a video Diffusion\\nTransformer (DiT) directly to control 4D content requires expensive multi-view\\nvideos. Inspired by Monocular Dynamic novel View Synthesis (MDVS) that\\noptimizes a 4D representation and renders videos according to different 4D\\nelements, such as camera pose and object motion editing, we bring pseudo 4D\\nGaussian fields to video generation. Specifically, we propose a novel framework\\nthat constructs a pseudo 4D Gaussian field with dense 3D point tracking and\\nrenders the Gaussian field for all video frames. Then we finetune a pretrained\\nDiT to generate videos following the guidance of the rendered video, dubbed as\\nGS-DiT. To boost the training of the GS-DiT, we also propose an efficient Dense\\n3D Point Tracking (D3D-PT) method for the pseudo 4D Gaussian field\\nconstruction. Our D3D-PT outperforms SpatialTracker, the state-of-the-art\\nsparse 3D point tracking method, in accuracy and accelerates the inference\\nspeed by two orders of magnitude. During the inference stage, GS-DiT can\\ngenerate videos with the same dynamic content while adhering to different\\ncamera parameters, addressing a significant limitation of current video\\ngeneration models. GS-DiT demonstrates strong generalization capabilities and\\nextends the 4D controllability of Gaussian splatting to video generation beyond\\njust camera poses. It supports advanced cinematic effects through the\\nmanipulation of the Gaussian field and camera intrinsics, making it a powerful\\ntool for creative video production. Demos are available at\\nhttps://wkbian.github.io/Projects/GS-DiT/.', 'upvotes': 10, 'discussionId': '677cbd90f7134ae168acc294'}, 'publishedAt': '2025-01-07T00:44:52.790Z', 'title': 'GS-DiT: Advancing Video Generation with Pseudo 4D Gaussian Fields through Efficient Dense 3D Point Tracking', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.02690.png', 'numComments': 1, 'submittedBy': {'_id': '63e9e92f20c109718713f5eb', 'avatarUrl': '/avatars/9ff312e854d803e1a2e9e685a21d12f8.svg', 'fullname': 'Fu-Yun Wang', 'name': 'wangfuyun', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 857}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2501.03059', 'authors': [{'_id': '677cd67175191fdc79cbcc04', 'user': {'_id': '646d239f4220471ca0c6471c', 'avatarUrl': '/avatars/a48f6a085b8a69fa0a29847fc5ae9065.svg', 'isPro': False, 'fullname': 'Guy Yariv', 'user': 'GuyYariv', 'type': 'user'}, 'name': 'Guy Yariv', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-07T09:37:35.128Z', 'hidden': False}, {'_id': '677cd67175191fdc79cbcc05', 'name': 'Yuval Kirstain', 'hidden': False}, {'_id': '677cd67175191fdc79cbcc06', 'user': {'_id': '64d8cb34505306fcd2fb89c3', 'avatarUrl': '/avatars/4ae9ffe5d494a7d53bc519ff8e402b3e.svg', 'isPro': False, 'fullname': 'Amit Zohar', 'user': 'amitz', 'type': 'user'}, 'name': 'Amit Zohar', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-07T13:17:40.162Z', 'hidden': False}, {'_id': '677cd67175191fdc79cbcc07', 'user': {'_id': '63468e6ca6f101c7f9049132', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1665662111525-63468e6ca6f101c7f9049132.png', 'isPro': False, 'fullname': 'Shelly Sheynin', 'user': 'shellysheynin', 'type': 'user'}, 'name': 'Shelly Sheynin', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-07T13:17:46.692Z', 'hidden': False}, {'_id': '677cd67175191fdc79cbcc08', 'name': 'Yaniv Taigman', 'hidden': False}, {'_id': '677cd67175191fdc79cbcc09', 'user': {'_id': '6481e135578646b5c2386728', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6481e135578646b5c2386728/SPva4iNw0pORiCXD45cx9.jpeg', 'isPro': False, 'fullname': 'Yossi Adi', 'user': 'adiyoss', 'type': 'user'}, 'name': 'Yossi Adi', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-07T13:17:56.992Z', 'hidden': False}, {'_id': '677cd67175191fdc79cbcc0a', 'user': {'_id': '6345a9b9a8c2ff9f1377faab', 'avatarUrl': '/avatars/a5f2b999ef8b967b2af9f41afcd9d475.svg', 'isPro': False, 'fullname': 'Sagie Benaim', 'user': 'sagiebenaim', 'type': 'user'}, 'name': 'Sagie Benaim', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-07T13:18:03.458Z', 'hidden': False}, {'_id': '677cd67175191fdc79cbcc0b', 'user': {'_id': '6304d514dae2eb7d08413d62', 'avatarUrl': '/avatars/02a571bc791b78d3993d9a0484b70a29.svg', 'isPro': False, 'fullname': 'Adam Polyak', 'user': 'adampo', 'type': 'user'}, 'name': 'Adam Polyak', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-07T13:18:10.411Z', 'hidden': False}], 'publishedAt': '2025-01-06T14:49:26.000Z', 'title': 'Through-The-Mask: Mask-based Motion Trajectories for Image-to-Video\\n  Generation', 'summary': \"We consider the task of Image-to-Video (I2V) generation, which involves\\ntransforming static images into realistic video sequences based on a textual\\ndescription. While recent advancements produce photorealistic outputs, they\\nfrequently struggle to create videos with accurate and consistent object\\nmotion, especially in multi-object scenarios. To address these limitations, we\\npropose a two-stage compositional framework that decomposes I2V generation\\ninto: (i) An explicit intermediate representation generation stage, followed by\\n(ii) A video generation stage that is conditioned on this representation. Our\\nkey innovation is the introduction of a mask-based motion trajectory as an\\nintermediate representation, that captures both semantic object information and\\nmotion, enabling an expressive but compact representation of motion and\\nsemantics. To incorporate the learned representation in the second stage, we\\nutilize object-level attention objectives. Specifically, we consider a spatial,\\nper-object, masked-cross attention objective, integrating object-specific\\nprompts into corresponding latent space regions and a masked spatio-temporal\\nself-attention objective, ensuring frame-to-frame consistency for each object.\\nWe evaluate our method on challenging benchmarks with multi-object and\\nhigh-motion scenarios and empirically demonstrate that the proposed method\\nachieves state-of-the-art results in temporal coherence, motion realism, and\\ntext-prompt faithfulness. Additionally, we introduce \\\\benchmark, a new\\nchallenging benchmark for single-object and multi-object I2V generation, and\\ndemonstrate our method's superiority on this benchmark. Project page is\\navailable at https://guyyariv.github.io/TTM/.\", 'upvotes': 9, 'discussionId': '677cd67375191fdc79cbcc76'}, 'publishedAt': '2025-01-07T02:24:26.904Z', 'title': 'Through-The-Mask: Mask-based Motion Trajectories for Image-to-Video Generation', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.03059.png', 'numComments': 1, 'submittedBy': {'_id': '646d239f4220471ca0c6471c', 'avatarUrl': '/avatars/a48f6a085b8a69fa0a29847fc5ae9065.svg', 'fullname': 'Guy Yariv', 'name': 'GuyYariv', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2501.03006', 'authors': [{'_id': '677c92224a867d8292b0a4d6', 'user': {'_id': '6340333a733f9eef46913dc8', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6340333a733f9eef46913dc8/BwhKgkqbuuxWsJv2s_JEH.png', 'isPro': True, 'fullname': 'luozhou wang', 'user': 'wileewang', 'type': 'user'}, 'name': 'Luozhou Wang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-07T09:50:03.830Z', 'hidden': False}, {'_id': '677c92224a867d8292b0a4d7', 'name': 'Yijun Li', 'hidden': False}, {'_id': '677c92224a867d8292b0a4d8', 'user': {'_id': '634a91c53a0cd2d4985dc98a', 'avatarUrl': '/avatars/fe680e683761c347cfa4bc9273b67b0d.svg', 'isPro': False, 'fullname': 'ZhiFei Chen', 'user': 'zhifeichen097', 'type': 'user'}, 'name': 'Zhifei Chen', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-07T09:50:28.496Z', 'hidden': False}, {'_id': '677c92224a867d8292b0a4d9', 'name': 'Jui-Hsien Wang', 'hidden': False}, {'_id': '677c92224a867d8292b0a4da', 'name': 'Zhifei Zhang', 'hidden': False}, {'_id': '677c92224a867d8292b0a4db', 'user': {'_id': '646c333136505117e22f08eb', 'avatarUrl': '/avatars/6b720565b3916a6f7d1bb7a73cd99a33.svg', 'isPro': False, 'fullname': 'He Zhang', 'user': 'HeZhang', 'type': 'user'}, 'name': 'He Zhang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-07T09:50:58.619Z', 'hidden': False}, {'_id': '677c92224a867d8292b0a4dc', 'name': 'Zhe Lin', 'hidden': False}, {'_id': '677c92224a867d8292b0a4dd', 'user': {'_id': '655cba1d87b67834000590e8', 'avatarUrl': '/avatars/3bd43b7c9351f65b8f38f4c8237a0146.svg', 'isPro': False, 'fullname': 'Yingcong Chen', 'user': 'yingcongchen', 'type': 'user'}, 'name': 'Yingcong Chen', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-07T09:51:09.776Z', 'hidden': False}], 'publishedAt': '2025-01-06T13:32:16.000Z', 'title': 'TransPixar: Advancing Text-to-Video Generation with Transparency', 'summary': 'Text-to-video generative models have made significant strides, enabling\\ndiverse applications in entertainment, advertising, and education. However,\\ngenerating RGBA video, which includes alpha channels for transparency, remains\\na challenge due to limited datasets and the difficulty of adapting existing\\nmodels. Alpha channels are crucial for visual effects (VFX), allowing\\ntransparent elements like smoke and reflections to blend seamlessly into\\nscenes. We introduce TransPixar, a method to extend pretrained video models for\\nRGBA generation while retaining the original RGB capabilities. TransPixar\\nleverages a diffusion transformer (DiT) architecture, incorporating\\nalpha-specific tokens and using LoRA-based fine-tuning to jointly generate RGB\\nand alpha channels with high consistency. By optimizing attention mechanisms,\\nTransPixar preserves the strengths of the original RGB model and achieves\\nstrong alignment between RGB and alpha channels despite limited training data.\\nOur approach effectively generates diverse and consistent RGBA videos,\\nadvancing the possibilities for VFX and interactive content creation.', 'upvotes': 8, 'discussionId': '677c92274a867d8292b0a5aa'}, 'publishedAt': '2025-01-06T21:32:48.629Z', 'title': 'TransPixar: Advancing Text-to-Video Generation with Transparency', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.03006.png', 'numComments': 1, 'submittedBy': {'_id': '6340333a733f9eef46913dc8', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6340333a733f9eef46913dc8/BwhKgkqbuuxWsJv2s_JEH.png', 'fullname': 'luozhou wang', 'name': 'wileewang', 'type': 'user', 'isPro': True, 'isHf': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2501.01790', 'authors': [{'_id': '677c162919230f94bdcc45e2', 'user': {'_id': '617ba1820e4237bd1731b867', 'avatarUrl': '/avatars/f9de06363e64bddd7dc977e96e85df8a.svg', 'isPro': False, 'fullname': 'zhengcong fei', 'user': 'onion', 'type': 'user'}, 'name': 'Zhengcong Fei', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-07T10:01:30.625Z', 'hidden': False}, {'_id': '677c162919230f94bdcc45e3', 'user': {'_id': '65dc3a850af7e21ba40e939f', 'avatarUrl': '/avatars/e129c64617675edd05d4317d39604318.svg', 'isPro': False, 'fullname': 'Li', 'user': 'Debang', 'type': 'user'}, 'name': 'Debang Li', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-07T10:01:40.288Z', 'hidden': False}, {'_id': '677c162919230f94bdcc45e4', 'user': {'_id': '65bef422fdb8d33cefeaccc3', 'avatarUrl': '/avatars/d40b0d7dda21fa1a68c291d11bc357ec.svg', 'isPro': False, 'fullname': 'Qiu Di', 'user': 'dddq', 'type': 'user'}, 'name': 'Di Qiu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-07T13:15:36.627Z', 'hidden': False}, {'_id': '677c162919230f94bdcc45e5', 'user': {'_id': '6419e0b3ed725fef6444f53a', 'avatarUrl': '/avatars/4cda142fb1f0b45107afce9422db282b.svg', 'isPro': False, 'fullname': 'Yu Changqian', 'user': 'Changqian', 'type': 'user'}, 'name': 'Changqian Yu', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-07T10:01:50.216Z', 'hidden': False}, {'_id': '677c162919230f94bdcc45e6', 'user': {'_id': '634672bfb7b4e71c7f45360f', 'avatarUrl': '/avatars/4b646fc3e271be90b9ec619d42ce3e99.svg', 'isPro': False, 'fullname': 'Fan Mingyuan', 'user': 'MichaelFan', 'type': 'user'}, 'name': 'Mingyuan Fan', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-07T10:02:01.415Z', 'hidden': False}], 'publishedAt': '2025-01-03T12:45:22.000Z', 'title': 'Ingredients: Blending Custom Photos with Video Diffusion Transformers', 'summary': 'This paper presents a powerful framework to customize video creations by\\nincorporating multiple specific identity (ID) photos, with video diffusion\\nTransformers, referred to as Ingredients. Generally, our method\\nconsists of three primary modules: (i) a facial extractor that\\ncaptures versatile and precise facial features for each human ID from both\\nglobal and local perspectives; (ii) a multi-scale projector that maps\\nface embeddings into the contextual space of image query in video diffusion\\ntransformers; (iii) an ID router that dynamically combines and\\nallocates multiple ID embedding to the corresponding space-time regions.\\nLeveraging a meticulously curated text-video dataset and a multi-stage training\\nprotocol, Ingredients demonstrates superior performance in turning\\ncustom photos into dynamic and personalized video content. Qualitative\\nevaluations highlight the advantages of proposed method, positioning it as a\\nsignificant advancement toward more effective generative video control tools in\\nTransformer-based architecture, compared to existing methods. The data, code,\\nand model weights are publicly available at:\\nhttps://github.com/feizc/Ingredients.', 'upvotes': 6, 'discussionId': '677c162b19230f94bdcc4705'}, 'publishedAt': '2025-01-06T23:05:17.500Z', 'title': 'Ingredients: Blending Custom Photos with Video Diffusion Transformers', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.01790.png', 'numComments': 1, 'submittedBy': {'_id': '63468720dd6d90d82ccf3450', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg', 'fullname': 'YSH', 'name': 'BestWishYsh', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 26}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2501.02576', 'authors': [{'_id': '677d15386ddea8749283f9c1', 'user': {'_id': '666fbf760de9ee884b99db29', 'avatarUrl': '/avatars/9a7ecedc703e92d1ebf34ca0bd35e41b.svg', 'isPro': False, 'fullname': 'Ziyang Song', 'user': 'zysong212', 'type': 'user'}, 'name': 'Ziyang Song', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-07T13:23:41.821Z', 'hidden': False}, {'_id': '677d15386ddea8749283f9c2', 'user': {'_id': '668600d61de6c00f43c5665b', 'avatarUrl': '/avatars/04dceea51ae26a069403a21f293c0f2f.svg', 'isPro': False, 'fullname': 'Zerong.Wang', 'user': 'Zerong007', 'type': 'user'}, 'name': 'Zerong Wang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-07T13:23:02.681Z', 'hidden': False}, {'_id': '677d15386ddea8749283f9c3', 'user': {'_id': '6493236b70d925ae8050a1bf', 'avatarUrl': '/avatars/b16069de1445cfa8608567175deaa2ae.svg', 'isPro': False, 'fullname': 'Bo Li', 'user': 'BoLi-aisecure', 'type': 'user'}, 'name': 'Bo Li', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-07T13:22:55.281Z', 'hidden': False}, {'_id': '677d15386ddea8749283f9c4', 'name': 'Hao Zhang', 'hidden': False}, {'_id': '677d15386ddea8749283f9c5', 'user': {'_id': '6697ac8427e4e21a3a92da27', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6697ac8427e4e21a3a92da27/9vn07-1_BBDk9zfDtDpcG.png', 'isPro': False, 'fullname': 'Ruijie Zhu', 'user': 'RuijieZhu', 'type': 'user'}, 'name': 'Ruijie Zhu', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-07T13:22:26.716Z', 'hidden': False}, {'_id': '677d15386ddea8749283f9c6', 'name': 'Li Liu', 'hidden': False}, {'_id': '677d15386ddea8749283f9c7', 'user': {'_id': '6423efbdb77cc3daf8429755', 'avatarUrl': '/avatars/a5b480713b4dd1dae8191545cb4c6f94.svg', 'isPro': False, 'fullname': 'Peng-Tao Jiang', 'user': 'ptjiang', 'type': 'user'}, 'name': 'Peng-Tao Jiang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-07T13:22:04.674Z', 'hidden': False}, {'_id': '677d15386ddea8749283f9c8', 'user': {'_id': '662b8d0083de3e26a6d9f1d1', 'avatarUrl': '/avatars/d9566ba1881f86413f5de10a45f24673.svg', 'isPro': False, 'fullname': 'Tianzhu Zhang', 'user': 'ztz1989', 'type': 'user'}, 'name': 'Tianzhu Zhang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-07T13:21:57.347Z', 'hidden': False}], 'publishedAt': '2025-01-05T15:18:32.000Z', 'title': 'DepthMaster: Taming Diffusion Models for Monocular Depth Estimation', 'summary': \"Monocular depth estimation within the diffusion-denoising paradigm\\ndemonstrates impressive generalization ability but suffers from low inference\\nspeed. Recent methods adopt a single-step deterministic paradigm to improve\\ninference efficiency while maintaining comparable performance. However, they\\noverlook the gap between generative and discriminative features, leading to\\nsuboptimal results. In this work, we propose DepthMaster, a single-step\\ndiffusion model designed to adapt generative features for the discriminative\\ndepth estimation task. First, to mitigate overfitting to texture details\\nintroduced by generative features, we propose a Feature Alignment module, which\\nincorporates high-quality semantic features to enhance the denoising network's\\nrepresentation capability. Second, to address the lack of fine-grained details\\nin the single-step deterministic framework, we propose a Fourier Enhancement\\nmodule to adaptively balance low-frequency structure and high-frequency\\ndetails. We adopt a two-stage training strategy to fully leverage the potential\\nof the two modules. In the first stage, we focus on learning the global scene\\nstructure with the Feature Alignment module, while in the second stage, we\\nexploit the Fourier Enhancement module to improve the visual quality. Through\\nthese efforts, our model achieves state-of-the-art performance in terms of\\ngeneralization and detail preservation, outperforming other diffusion-based\\nmethods across various datasets. Our project page can be found at\\nhttps://indu1ge.github.io/DepthMaster_page.\", 'upvotes': 5, 'discussionId': '677d15396ddea8749283fa0b'}, 'publishedAt': '2025-01-07T06:53:36.849Z', 'title': 'DepthMaster: Taming Diffusion Models for Monocular Depth Estimation', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.02576.png', 'numComments': 1, 'submittedBy': {'_id': '6697ac8427e4e21a3a92da27', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6697ac8427e4e21a3a92da27/9vn07-1_BBDk9zfDtDpcG.png', 'fullname': 'Ruijie Zhu', 'name': 'RuijieZhu', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2501.01830', 'authors': [{'_id': '677c948a6f370093aa2a764e', 'name': 'Yanjiang Liu', 'hidden': False}, {'_id': '677c948a6f370093aa2a764f', 'name': 'Shuhen Zhou', 'hidden': False}, {'_id': '677c948a6f370093aa2a7650', 'user': {'_id': '6216496a9b34d2fb49144599', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6216496a9b34d2fb49144599/41CKA_h1Ffj3RzVabSAkm.jpeg', 'isPro': False, 'fullname': 'Yaojie Lu', 'user': 'luyaojie', 'type': 'user'}, 'name': 'Yaojie Lu', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-07T13:24:42.703Z', 'hidden': False}, {'_id': '677c948a6f370093aa2a7651', 'name': 'Huijia Zhu', 'hidden': False}, {'_id': '677c948a6f370093aa2a7652', 'user': {'_id': '65817e9ee77395a0c87c9d30', 'avatarUrl': '/avatars/f19f3c10e97df7dac8d3a58f41275c4c.svg', 'isPro': False, 'fullname': 'weiqiang wang', 'user': 'weyl', 'type': 'user'}, 'name': 'Weiqiang Wang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-07T13:24:28.891Z', 'hidden': False}, {'_id': '677c948a6f370093aa2a7653', 'user': {'_id': '6711c702f858a456b4b9f3a4', 'avatarUrl': '/avatars/178e9567c3111ab22717c3c0dd003a6a.svg', 'isPro': False, 'fullname': 'Hongyu  Lin', 'user': 'sanmusunrise', 'type': 'user'}, 'name': 'Hongyu Lin', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-07T13:24:22.844Z', 'hidden': False}, {'_id': '677c948a6f370093aa2a7654', 'name': 'Ben He', 'hidden': False}, {'_id': '677c948a6f370093aa2a7655', 'user': {'_id': '65e99a77e71555ed193609cf', 'avatarUrl': '/avatars/38ceb127883944677665da967d17dd18.svg', 'isPro': False, 'fullname': 'Xianpei Han', 'user': 'xphan', 'type': 'user'}, 'name': 'Xianpei Han', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-07T13:24:12.963Z', 'hidden': False}, {'_id': '677c948a6f370093aa2a7656', 'name': 'Le Sun', 'hidden': False}], 'publishedAt': '2025-01-03T14:30:14.000Z', 'title': 'Auto-RT: Automatic Jailbreak Strategy Exploration for Red-Teaming Large\\n  Language Models', 'summary': 'Automated red-teaming has become a crucial approach for uncovering\\nvulnerabilities in large language models (LLMs). However, most existing methods\\nfocus on isolated safety flaws, limiting their ability to adapt to dynamic\\ndefenses and uncover complex vulnerabilities efficiently. To address this\\nchallenge, we propose Auto-RT, a reinforcement learning framework that\\nautomatically explores and optimizes complex attack strategies to effectively\\nuncover security vulnerabilities through malicious queries. Specifically, we\\nintroduce two key mechanisms to reduce exploration complexity and improve\\nstrategy optimization: 1) Early-terminated Exploration, which accelerate\\nexploration by focusing on high-potential attack strategies; and 2) Progressive\\nReward Tracking algorithm with intermediate downgrade models, which dynamically\\nrefine the search trajectory toward successful vulnerability exploitation.\\nExtensive experiments across diverse LLMs demonstrate that, by significantly\\nimproving exploration efficiency and automatically optimizing attack\\nstrategies, Auto-RT detects a boarder range of vulnerabilities, achieving a\\nfaster detection speed and 16.63\\\\% higher success rates compared to existing\\nmethods.', 'upvotes': 4, 'discussionId': '677c948c6f370093aa2a76e2'}, 'publishedAt': '2025-01-07T00:06:57.011Z', 'title': 'Auto-RT: Automatic Jailbreak Strategy Exploration for Red-Teaming Large Language Models', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.01830.png', 'numComments': 1, 'submittedBy': {'_id': '643b62ac065961b2252abb7a', 'avatarUrl': '/avatars/c7fb4d11f0d795a52bdc771c04a69a20.svg', 'fullname': 'zuijiang', 'name': 'zuijiang', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 2}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2501.02506', 'authors': [{'_id': '677cab009909f23ac4dc1d0e', 'user': {'_id': '66384be673c2c55f2ded89fa', 'avatarUrl': '/avatars/1d8721074f0f51fab405f81474f2035f.svg', 'isPro': False, 'fullname': 'Junjie Ye', 'user': 'Junjie-Ye', 'type': 'user'}, 'name': 'Junjie Ye', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-07T13:02:00.881Z', 'hidden': False}, {'_id': '677cab009909f23ac4dc1d0f', 'name': 'Zhengyin Du', 'hidden': False}, {'_id': '677cab009909f23ac4dc1d10', 'name': 'Xuesong Yao', 'hidden': False}, {'_id': '677cab009909f23ac4dc1d11', 'user': {'_id': '6511f4fdc915724a65380f78', 'avatarUrl': '/avatars/b7cf645e0a035e3a3683ee1fd01380ed.svg', 'isPro': False, 'fullname': 'Weijian Lin', 'user': 'maverick1994', 'type': 'user'}, 'name': 'Weijian Lin', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-07T08:42:22.956Z', 'hidden': False}, {'_id': '677cab009909f23ac4dc1d12', 'name': 'Yufei Xu', 'hidden': False}, {'_id': '677cab009909f23ac4dc1d13', 'name': 'Zehui Chen', 'hidden': False}, {'_id': '677cab009909f23ac4dc1d14', 'name': 'Zaiyuan Wang', 'hidden': False}, {'_id': '677cab009909f23ac4dc1d15', 'name': 'Sining Zhu', 'hidden': False}, {'_id': '677cab009909f23ac4dc1d16', 'user': {'_id': '653a6e5cae155b92bae77b74', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/653a6e5cae155b92bae77b74/TA5FWKAUsB249ux4MzD_R.jpeg', 'isPro': False, 'fullname': 'Zhiheng Xi', 'user': 'WooooDyy', 'type': 'user'}, 'name': 'Zhiheng Xi', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-07T13:26:27.253Z', 'hidden': False}, {'_id': '677cab009909f23ac4dc1d17', 'user': {'_id': '62d62b333bf5e059f7d2b286', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1668513815771-62d62b333bf5e059f7d2b286.jpeg', 'isPro': False, 'fullname': 'Siyu Yuan', 'user': 'siyuyuan', 'type': 'user'}, 'name': 'Siyu Yuan', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-07T13:26:42.842Z', 'hidden': False}, {'_id': '677cab009909f23ac4dc1d18', 'user': {'_id': '65b71b9e8bb894c96cc7707b', 'avatarUrl': '/avatars/78e2a833054e89bcc34ddc6e789677d5.svg', 'isPro': False, 'fullname': 'Tao Gui', 'user': 'guixiaotao', 'type': 'user'}, 'name': 'Tao Gui', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-07T13:26:50.647Z', 'hidden': False}, {'_id': '677cab009909f23ac4dc1d19', 'name': 'Qi Zhang', 'hidden': False}, {'_id': '677cab009909f23ac4dc1d1a', 'name': 'Xuanjing Huang', 'hidden': False}, {'_id': '677cab009909f23ac4dc1d1b', 'name': 'Jiechao Chen', 'hidden': False}], 'publishedAt': '2025-01-05T11:06:55.000Z', 'title': 'ToolHop: A Query-Driven Benchmark for Evaluating Large Language Models\\n  in Multi-Hop Tool Use', 'summary': 'Effective evaluation of multi-hop tool use is critical for analyzing the\\nunderstanding, reasoning, and function-calling capabilities of large language\\nmodels (LLMs). However, progress has been hindered by a lack of reliable\\nevaluation datasets. To address this, we present ToolHop, a dataset comprising\\n995 user queries and 3,912 associated tools, specifically designed for rigorous\\nevaluation of multi-hop tool use. ToolHop ensures diverse queries, meaningful\\ninterdependencies, locally executable tools, detailed feedback, and verifiable\\nanswers through a novel query-driven data construction approach that includes\\ntool creation, document refinement, and code generation. We evaluate 14 LLMs\\nacross five model families (i.e., LLaMA3.1, Qwen2.5, Gemini1.5, Claude3.5, and\\nGPT), uncovering significant challenges in handling multi-hop tool-use\\nscenarios. The leading model, GPT-4o, achieves an accuracy of 49.04%,\\nunderscoring substantial room for improvement. Further analysis reveals\\nvariations in tool-use strategies for various families, offering actionable\\ninsights to guide the development of more effective approaches. Code and data\\ncan be found in https://huggingface.co/bytedance-research/ToolHop.', 'upvotes': 4, 'discussionId': '677cab029909f23ac4dc1d60'}, 'publishedAt': '2025-01-06T23:18:40.598Z', 'title': 'ToolHop: A Query-Driven Benchmark for Evaluating Large Language Models in Multi-Hop Tool Use', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.02506.png', 'numComments': 2, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isMod': False, 'followerCount': 5581}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2501.02423', 'authors': [{'_id': '677d1abbac2db4c22667f64a', 'name': 'Xingwu Sun', 'hidden': False}, {'_id': '677d1abbac2db4c22667f64b', 'user': {'_id': '64e32fb7e5bd41dd056bc943', 'avatarUrl': '/avatars/fbe84d0641dfdd665cffb1aad464e8a0.svg', 'isPro': False, 'fullname': 'Shuaipeng Li', 'user': 'unlimblue', 'type': 'user'}, 'name': 'Shuaipeng Li', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-07T13:27:47.517Z', 'hidden': False}, {'_id': '677d1abbac2db4c22667f64c', 'user': {'_id': '6622443b9b0614a760dd8123', 'avatarUrl': '/avatars/acb6c1c9c429af1112530dcf76a8e420.svg', 'isPro': False, 'fullname': 'Ruobing Xie', 'user': 'Ruobing-Xie', 'type': 'user'}, 'name': 'Ruobing Xie', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-07T13:01:47.778Z', 'hidden': False}, {'_id': '677d1abbac2db4c22667f64d', 'user': {'_id': '615c2f8416284d83200e1105', 'avatarUrl': '/avatars/8e0c89925f714de58f0fc77265acd76f.svg', 'isPro': False, 'fullname': 'han weidong', 'user': 'dongdong2021', 'type': 'user'}, 'name': 'Weidong Han', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-07T13:27:58.222Z', 'hidden': False}, {'_id': '677d1abbac2db4c22667f64e', 'name': 'Kan Wu', 'hidden': False}, {'_id': '677d1abbac2db4c22667f64f', 'name': 'Zhen Yang', 'hidden': False}, {'_id': '677d1abbac2db4c22667f650', 'name': 'Yixing Li', 'hidden': False}, {'_id': '677d1abbac2db4c22667f651', 'name': 'An Wang', 'hidden': False}, {'_id': '677d1abbac2db4c22667f652', 'name': 'Shuai Li', 'hidden': False}, {'_id': '677d1abbac2db4c22667f653', 'name': 'Jinbao Xue', 'hidden': False}, {'_id': '677d1abbac2db4c22667f654', 'name': 'Yu Cheng', 'hidden': False}, {'_id': '677d1abbac2db4c22667f655', 'name': 'Yangyu Tao', 'hidden': False}, {'_id': '677d1abbac2db4c22667f656', 'user': {'_id': '6728117b71b9baba45c25c35', 'avatarUrl': '/avatars/5e4603f00a426c6c41e3e6fef5fa0362.svg', 'isPro': False, 'fullname': 'zhanhui kang', 'user': 'kangzhanhui', 'type': 'user'}, 'name': 'Zhanhui Kang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-07T13:30:50.613Z', 'hidden': False}, {'_id': '677d1abbac2db4c22667f657', 'name': 'Chengzhong Xu', 'hidden': False}, {'_id': '677d1abbac2db4c22667f658', 'name': 'Di Wang', 'hidden': False}, {'_id': '677d1abbac2db4c22667f659', 'name': 'Jie Jiang', 'hidden': False}], 'publishedAt': '2025-01-05T02:30:41.000Z', 'title': 'Scaling Laws for Floating Point Quantization Training', 'summary': 'Low-precision training is considered an effective strategy for reducing both\\ntraining and downstream inference costs. Previous scaling laws for precision\\nmainly focus on integer quantization, which pay less attention to the\\nconstituents in floating-point quantization and thus cannot well fit the LLM\\nlosses in this scenario. In contrast, while floating-point quantization\\ntraining is more commonly implemented in production, the research on it has\\nbeen relatively superficial. In this paper, we thoroughly explore the effects\\nof floating-point quantization targets, exponent bits, mantissa bits, and the\\ncalculation granularity of the scaling factor in floating-point quantization\\ntraining performance of LLM models. While presenting an accurate floating-point\\nquantization unified scaling law, we also provide valuable suggestions for the\\ncommunity: (1) Exponent bits contribute slightly more to the model performance\\nthan mantissa bits. We provide the optimal exponent-mantissa bit ratio for\\ndifferent bit numbers, which is available for future reference by hardware\\nmanufacturers; (2) We discover the formation of the critical data size in\\nlow-precision LLM training. Too much training data exceeding the critical data\\nsize will inversely bring in degradation of LLM performance; (3) The optimal\\nfloating-point quantization precision is directly proportional to the\\ncomputational power, but within a wide computational power range, we estimate\\nthat the best cost-performance precision lies between 4-8 bits.', 'upvotes': 3, 'discussionId': '677d1abcac2db4c22667f6a5'}, 'publishedAt': '2025-01-07T07:19:33.353Z', 'title': 'Scaling Laws for Floating Point Quantization Training', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.02423.png', 'numComments': 1, 'submittedBy': {'_id': '6622443b9b0614a760dd8123', 'avatarUrl': '/avatars/acb6c1c9c429af1112530dcf76a8e420.svg', 'fullname': 'Ruobing Xie', 'name': 'Ruobing-Xie', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2501.02832', 'authors': [{'_id': '677cbd43ce51bc60df2c94db', 'user': {'_id': '63d9e09f1cae35c27bf80cb2', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1675223055197-noauth.jpeg', 'isPro': True, 'fullname': 'Syed Abdul Gaffar Shakhadri', 'user': 'SyedAbdul', 'type': 'user'}, 'name': 'Syed Abdul Gaffar Shakhadri', 'status': 'extracted_pending', 'statusLastChangedAt': '2025-01-07T05:36:04.622Z', 'hidden': False}, {'_id': '677cbd43ce51bc60df2c94dc', 'user': {'_id': '5fb7ae48e6ae537272bdeb3c', 'avatarUrl': '/avatars/e5d01cb428f4b22161e0d17895a5c678.svg', 'isPro': False, 'fullname': 'Kruthika', 'user': 'kruthika', 'type': 'user'}, 'name': 'Kruthika KR', 'status': 'extracted_pending', 'statusLastChangedAt': '2025-01-07T05:36:04.622Z', 'hidden': False}, {'_id': '677cbd43ce51bc60df2c94dd', 'user': {'_id': '666a75f9f0d87d9c3b278693', 'avatarUrl': '/avatars/657f2273bff0e136c93f10ad72ce87d8.svg', 'isPro': False, 'fullname': 'Kartik Basavaraj Angadi', 'user': 'Kartik-angadi', 'type': 'user'}, 'name': 'Kartik Basavaraj Angadi', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-07T08:41:40.099Z', 'hidden': False}], 'publishedAt': '2025-01-06T08:16:06.000Z', 'title': 'Samba-asr state-of-the-art speech recognition leveraging structured\\n  state-space models', 'summary': 'We propose Samba ASR, the first state-of-the-art Automatic Speech Recognition\\n(ASR) model leveraging the novel Mamba architecture as both encoder and\\ndecoder, built on the foundation of state-space models (SSMs). Unlike\\ntransformer-based ASR models, which rely on self-attention mechanisms to\\ncapture dependencies, Samba ASR effectively models both local and global\\ntemporal dependencies using efficient state-space dynamics, achieving\\nremarkable performance gains. By addressing the limitations of transformers,\\nsuch as quadratic scaling with input length and difficulty in handling\\nlong-range dependencies, Samba ASR achieves superior accuracy and efficiency.\\n  Experimental results demonstrate that Samba ASR surpasses existing\\nopen-source transformer-based ASR models across various standard benchmarks,\\nestablishing it as the new state of the art in ASR. Extensive evaluations on\\nbenchmark datasets show significant improvements in Word Error Rate (WER), with\\ncompetitive performance even in low-resource scenarios. Furthermore, the\\ncomputational efficiency and parameter optimization of the Mamba architecture\\nmake Samba ASR a scalable and robust solution for diverse ASR tasks.\\n  Our contributions include:\\n  A new Samba ASR architecture demonstrating the superiority of SSMs over\\ntransformer-based models for speech sequence processing. A comprehensive\\nevaluation on public benchmarks showcasing state-of-the-art performance. An\\nanalysis of computational efficiency, robustness to noise, and sequence\\ngeneralization. This work highlights the viability of Mamba SSMs as a\\ntransformer-free alternative for efficient and accurate ASR. By leveraging\\nstate-space modeling advancements, Samba ASR sets a new benchmark for ASR\\nperformance and future research.', 'upvotes': 3, 'discussionId': '677cbd44ce51bc60df2c9526'}, 'publishedAt': '2025-01-07T00:36:33.823Z', 'title': 'Samba-asr state-of-the-art speech recognition leveraging structured state-space models', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.02832.png', 'numComments': 1, 'submittedBy': {'_id': '63d9e09f1cae35c27bf80cb2', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1675223055197-noauth.jpeg', 'fullname': 'Syed Abdul Gaffar Shakhadri', 'name': 'SyedAbdul', 'type': 'user', 'isPro': True, 'isHf': False, 'isMod': False, 'followerCount': 4}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2501.00912', 'authors': [{'_id': '677d530bbc194dfe381d3e0b', 'name': 'Jiaxin Ge', 'hidden': False}, {'_id': '677d530bbc194dfe381d3e0c', 'name': 'Zora Zhiruo Wang', 'hidden': False}, {'_id': '677d530bbc194dfe381d3e0d', 'name': 'Xuhui Zhou', 'hidden': False}, {'_id': '677d530bbc194dfe381d3e0e', 'name': 'Yi-Hao Peng', 'hidden': False}, {'_id': '677d530bbc194dfe381d3e0f', 'name': 'Sanjay Subramanian', 'hidden': False}, {'_id': '677d530bbc194dfe381d3e10', 'name': 'Qinyue Tan', 'hidden': False}, {'_id': '677d530bbc194dfe381d3e11', 'name': 'Maarten Sap', 'hidden': False}, {'_id': '677d530bbc194dfe381d3e12', 'name': 'Alane Suhr', 'hidden': False}, {'_id': '677d530bbc194dfe381d3e13', 'name': 'Daniel Fried', 'hidden': False}, {'_id': '677d530bbc194dfe381d3e14', 'name': 'Graham Neubig', 'hidden': False}, {'_id': '677d530bbc194dfe381d3e15', 'name': 'Trevor Darrell', 'hidden': False}], 'publishedAt': '2025-01-01T18:09:32.000Z', 'title': 'AutoPresent: Designing Structured Visuals from Scratch', 'summary': \"Designing structured visuals such as presentation slides is essential for\\ncommunicative needs, necessitating both content creation and visual planning\\nskills. In this work, we tackle the challenge of automated slide generation,\\nwhere models produce slide presentations from natural language (NL)\\ninstructions. We first introduce the SlidesBench benchmark, the first benchmark\\nfor slide generation with 7k training and 585 testing examples derived from 310\\nslide decks across 10 domains. SlidesBench supports evaluations that are\\n(i)reference-based to measure similarity to a target slide, and\\n(ii)reference-free to measure the design quality of generated slides alone. We\\nbenchmark end-to-end image generation and program generation methods with a\\nvariety of models, and find that programmatic methods produce higher-quality\\nslides in user-interactable formats. Built on the success of program\\ngeneration, we create AutoPresent, an 8B Llama-based model trained on 7k pairs\\nof instructions paired with code for slide generation, and achieve results\\ncomparable to the closed-source model GPT-4o. We further explore iterative\\ndesign refinement where the model is tasked to self-refine its own output, and\\nwe found that this process improves the slide's quality. We hope that our work\\nwill provide a basis for future work on generating structured visuals.\", 'upvotes': 2, 'discussionId': '677d530cbc194dfe381d3e6f'}, 'publishedAt': '2025-01-07T11:17:05.176Z', 'title': 'AutoPresent: Designing Structured Visuals from Scratch', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.00912.png', 'numComments': 1, 'submittedBy': {'_id': '6356c77dd18276171735985f', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6356c77dd18276171735985f/HYsRr1G13xgwVUoou7nFQ.jpeg', 'fullname': 'Zora Zhiruo Wang', 'name': 'zorazrw', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 5}, 'isAuthorParticipating': False}"
]