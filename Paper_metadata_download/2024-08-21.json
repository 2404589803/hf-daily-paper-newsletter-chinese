[
    {
        "paper": {
            "id": "2408.09174",
            "authors": [
                {
                    "_id": "66c5940c83672a810a1d9861",
                    "name": "Xianjie Wu",
                    "hidden": false
                },
                {
                    "_id": "66c5940c83672a810a1d9862",
                    "name": "Jian Yang",
                    "hidden": false
                },
                {
                    "_id": "66c5940c83672a810a1d9863",
                    "name": "Linzheng Chai",
                    "hidden": false
                },
                {
                    "_id": "66c5940c83672a810a1d9864",
                    "name": "Ge Zhang",
                    "hidden": false
                },
                {
                    "_id": "66c5940c83672a810a1d9865",
                    "name": "Jiaheng Liu",
                    "hidden": false
                },
                {
                    "_id": "66c5940c83672a810a1d9866",
                    "name": "Xinrun Du",
                    "hidden": false
                },
                {
                    "_id": "66c5940c83672a810a1d9867",
                    "name": "Di Liang",
                    "hidden": false
                },
                {
                    "_id": "66c5940c83672a810a1d9868",
                    "name": "Daixin Shu",
                    "hidden": false
                },
                {
                    "_id": "66c5940c83672a810a1d9869",
                    "name": "Xianfu Cheng",
                    "hidden": false
                },
                {
                    "_id": "66c5940c83672a810a1d986a",
                    "name": "Tianzhen Sun",
                    "hidden": false
                },
                {
                    "_id": "66c5940c83672a810a1d986b",
                    "name": "Guanglin Niu",
                    "hidden": false
                },
                {
                    "_id": "66c5940c83672a810a1d986c",
                    "name": "Tongliang Li",
                    "hidden": false
                },
                {
                    "_id": "66c5940c83672a810a1d986d",
                    "name": "Zhoujun Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-08-17T11:40:10.000Z",
            "title": "TableBench: A Comprehensive and Complex Benchmark for Table Question\n  Answering",
            "summary": "Recent advancements in Large Language Models (LLMs) have markedly enhanced\nthe interpretation and processing of tabular data, introducing previously\nunimaginable capabilities. Despite these achievements, LLMs still encounter\nsignificant challenges when applied in industrial scenarios, particularly due\nto the increased complexity of reasoning required with real-world tabular data,\nunderscoring a notable disparity between academic benchmarks and practical\napplications. To address this discrepancy, we conduct a detailed investigation\ninto the application of tabular data in industrial scenarios and propose a\ncomprehensive and complex benchmark TableBench, including 18 fields within four\nmajor categories of table question answering (TableQA) capabilities.\nFurthermore, we introduce TableLLM, trained on our meticulously constructed\ntraining set TableInstruct, achieving comparable performance with GPT-3.5.\nMassive experiments conducted on TableBench indicate that both open-source and\nproprietary LLMs still have significant room for improvement to meet real-world\ndemands, where the most advanced model, GPT-4, achieves only a modest score\ncompared to humans.",
            "upvotes": 32,
            "discussionId": "66c5940c83672a810a1d98ac"
        },
        "publishedAt": "2024-08-21T05:45:38.406Z",
        "title": "TableBench: A Comprehensive and Complex Benchmark for Table Question Answering",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.09174.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
            "fullname": "Ge Zhang",
            "name": "zhangysk",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2408.10914",
            "authors": [
                {
                    "_id": "66c556dd50f505c5451ce8e1",
                    "name": "Viraat Aryabumi",
                    "hidden": false
                },
                {
                    "_id": "66c556dd50f505c5451ce8e2",
                    "name": "Yixuan Su",
                    "hidden": false
                },
                {
                    "_id": "66c556dd50f505c5451ce8e3",
                    "name": "Raymond Ma",
                    "hidden": false
                },
                {
                    "_id": "66c556dd50f505c5451ce8e4",
                    "name": "Adrien Morisot",
                    "hidden": false
                },
                {
                    "_id": "66c556dd50f505c5451ce8e5",
                    "name": "Ivan Zhang",
                    "hidden": false
                },
                {
                    "_id": "66c556dd50f505c5451ce8e6",
                    "name": "Acyr Locatelli",
                    "hidden": false
                },
                {
                    "_id": "66c556dd50f505c5451ce8e7",
                    "name": "Marzieh Fadaee",
                    "hidden": false
                },
                {
                    "_id": "66c556dd50f505c5451ce8e8",
                    "name": "Ahmet Üstün",
                    "hidden": false
                },
                {
                    "_id": "66c556dd50f505c5451ce8e9",
                    "name": "Sara Hooker",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-08-20T14:58:13.000Z",
            "title": "To Code, or Not To Code? Exploring Impact of Code in Pre-training",
            "summary": "Including code in the pre-training data mixture, even for models not\nspecifically designed for code, has become a common practice in LLMs\npre-training. While there has been anecdotal consensus among practitioners that\ncode data plays a vital role in general LLMs' performance, there is only\nlimited work analyzing the precise impact of code on non-code tasks. In this\nwork, we systematically investigate the impact of code data on general\nperformance. We ask \"what is the impact of code data used in pre-training on a\nlarge variety of downstream tasks beyond code generation\". We conduct extensive\nablations and evaluate across a broad range of natural language reasoning\ntasks, world knowledge tasks, code benchmarks, and LLM-as-a-judge win-rates for\nmodels with sizes ranging from 470M to 2.8B parameters. Across settings, we\nfind a consistent results that code is a critical building block for\ngeneralization far beyond coding tasks and improvements to code quality have an\noutsized impact across all tasks. In particular, compared to text-only\npre-training, the addition of code results in up to relative increase of 8.2%\nin natural language (NL) reasoning, 4.2% in world knowledge, 6.6% improvement\nin generative win-rates, and a 12x boost in code performance respectively. Our\nwork suggests investments in code quality and preserving code during\npre-training have positive impacts.",
            "upvotes": 22,
            "discussionId": "66c556de50f505c5451ce947"
        },
        "publishedAt": "2024-08-21T01:26:01.884Z",
        "title": "To Code, or Not To Code? Exploring Impact of Code in Pre-training",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.10914.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2408.11039",
            "authors": [
                {
                    "_id": "66c556167690afd2f4c47e12",
                    "name": "Chunting Zhou",
                    "hidden": false
                },
                {
                    "_id": "66c556167690afd2f4c47e13",
                    "name": "Lili Yu",
                    "hidden": false
                },
                {
                    "_id": "66c556167690afd2f4c47e14",
                    "name": "Arun Babu",
                    "hidden": false
                },
                {
                    "_id": "66c556167690afd2f4c47e15",
                    "name": "Kushal Tirumala",
                    "hidden": false
                },
                {
                    "_id": "66c556167690afd2f4c47e16",
                    "name": "Michihiro Yasunaga",
                    "hidden": false
                },
                {
                    "_id": "66c556167690afd2f4c47e17",
                    "name": "Leonid Shamis",
                    "hidden": false
                },
                {
                    "_id": "66c556167690afd2f4c47e18",
                    "name": "Jacob Kahn",
                    "hidden": false
                },
                {
                    "_id": "66c556167690afd2f4c47e19",
                    "name": "Xuezhe Ma",
                    "hidden": false
                },
                {
                    "_id": "66c556167690afd2f4c47e1a",
                    "name": "Luke Zettlemoyer",
                    "hidden": false
                },
                {
                    "_id": "66c556167690afd2f4c47e1b",
                    "name": "Omer Levy",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-08-20T17:48:20.000Z",
            "title": "Transfusion: Predict the Next Token and Diffuse Images with One\n  Multi-Modal Model",
            "summary": "We introduce Transfusion, a recipe for training a multi-modal model over\ndiscrete and continuous data. Transfusion combines the language modeling loss\nfunction (next token prediction) with diffusion to train a single transformer\nover mixed-modality sequences. We pretrain multiple Transfusion models up to 7B\nparameters from scratch on a mixture of text and image data, establishing\nscaling laws with respect to a variety of uni- and cross-modal benchmarks. Our\nexperiments show that Transfusion scales significantly better than quantizing\nimages and training a language model over discrete image tokens. By introducing\nmodality-specific encoding and decoding layers, we can further improve the\nperformance of Transfusion models, and even compress each image to just 16\npatches. We further demonstrate that scaling our Transfusion recipe to 7B\nparameters and 2T multi-modal tokens produces a model that can generate images\nand text on a par with similar scale diffusion models and language models,\nreaping the benefits of both worlds.",
            "upvotes": 20,
            "discussionId": "66c556197690afd2f4c47e82"
        },
        "publishedAt": "2024-08-21T01:21:40.567Z",
        "title": "Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.11039.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2408.11001",
            "authors": [
                {
                    "_id": "66c54a9336a75deef8a188e9",
                    "name": "Haoning Wu",
                    "hidden": false
                },
                {
                    "_id": "66c54a9336a75deef8a188ea",
                    "name": "Shaocheng Shen",
                    "hidden": false
                },
                {
                    "_id": "66c54a9336a75deef8a188eb",
                    "name": "Qiang Hu",
                    "hidden": false
                },
                {
                    "_id": "66c54a9336a75deef8a188ec",
                    "name": "Xiaoyun Zhang",
                    "hidden": false
                },
                {
                    "_id": "66c54a9336a75deef8a188ed",
                    "name": "Ya Zhang",
                    "hidden": false
                },
                {
                    "_id": "66c54a9336a75deef8a188ee",
                    "name": "Yanfeng Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-08-20T16:53:34.000Z",
            "title": "MegaFusion: Extend Diffusion Models towards Higher-resolution Image\n  Generation without Further Tuning",
            "summary": "Diffusion models have emerged as frontrunners in text-to-image generation for\ntheir impressive capabilities. Nonetheless, their fixed image resolution during\ntraining often leads to challenges in high-resolution image generation, such as\nsemantic inaccuracies and object replication. This paper introduces MegaFusion,\na novel approach that extends existing diffusion-based text-to-image generation\nmodels towards efficient higher-resolution generation without additional\nfine-tuning or extra adaptation. Specifically, we employ an innovative truncate\nand relay strategy to bridge the denoising processes across different\nresolutions, allowing for high-resolution image generation in a coarse-to-fine\nmanner. Moreover, by integrating dilated convolutions and noise re-scheduling,\nwe further adapt the model's priors for higher resolution. The versatility and\nefficacy of MegaFusion make it universally applicable to both latent-space and\npixel-space diffusion models, along with other derivative models. Extensive\nexperiments confirm that MegaFusion significantly boosts the capability of\nexisting models to produce images of megapixels and various aspect ratios,\nwhile only requiring about 40% of the original computational cost.",
            "upvotes": 7,
            "discussionId": "66c54a9536a75deef8a1896c"
        },
        "publishedAt": "2024-08-21T00:45:01.594Z",
        "title": "MegaFusion: Extend Diffusion Models towards Higher-resolution Image Generation without Further Tuning",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.11001.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/632c7a0d1d303f5f9acf01b8/T010IFuCp6UaOeIyWhbCk.jpeg",
            "fullname": "HaoningWu",
            "name": "haoningwu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2408.11054",
            "authors": [
                {
                    "_id": "66c5b3eb2d2fcb957116b49e",
                    "name": "Valentinos Pariza",
                    "hidden": false
                },
                {
                    "_id": "66c5b3eb2d2fcb957116b49f",
                    "name": "Mohammadreza Salehi",
                    "hidden": false
                },
                {
                    "_id": "66c5b3eb2d2fcb957116b4a0",
                    "name": "Gertjan Burghouts",
                    "hidden": false
                },
                {
                    "_id": "66c5b3eb2d2fcb957116b4a1",
                    "name": "Francesco Locatello",
                    "hidden": false
                },
                {
                    "_id": "66c5b3eb2d2fcb957116b4a2",
                    "name": "Yuki M. Asano",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-08-20T17:58:59.000Z",
            "title": "NeCo: Improving DINOv2's spatial representations in 19 GPU hours with\n  Patch Neighbor Consistency",
            "summary": "We propose sorting patch representations across views as a novel\nself-supervised learning signal to improve pretrained representations. To this\nend, we introduce NeCo: Patch Neighbor Consistency, a novel training loss that\nenforces patch-level nearest neighbor consistency across a student and teacher\nmodel, relative to reference batches. Our method leverages a differentiable\nsorting method applied on top of pretrained representations, such as\nDINOv2-registers to bootstrap the learning signal and further improve upon\nthem. This dense post-pretraining leads to superior performance across various\nmodels and datasets, despite requiring only 19 hours on a single GPU. We\ndemonstrate that this method generates high-quality dense feature encoders and\nestablish several new state-of-the-art results: +5.5% and + 6% for\nnon-parametric in-context semantic segmentation on ADE20k and Pascal VOC, and\n+7.2% and +5.7% for linear segmentation evaluations on COCO-Things and -Stuff.",
            "upvotes": 6,
            "discussionId": "66c5b3f12d2fcb957116b776"
        },
        "publishedAt": "2024-08-21T08:01:50.798Z",
        "title": "NeCo: Improving DINOv2's spatial representations in 19 GPU hours with Patch Neighbor Consistency",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.11054.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/44c3c1a917c92899a6707ff52e45b7d0.svg",
            "fullname": "Yuki Asano",
            "name": "yukimasano",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2408.11049",
            "authors": [
                {
                    "_id": "66c557a6e9d46e06d5d498d6",
                    "name": "Jian Chen",
                    "hidden": false
                },
                {
                    "_id": "66c557a6e9d46e06d5d498d7",
                    "name": "Vashisth Tiwari",
                    "hidden": false
                },
                {
                    "_id": "66c557a6e9d46e06d5d498d8",
                    "name": "Ranajoy Sadhukhan",
                    "hidden": false
                },
                {
                    "_id": "66c557a6e9d46e06d5d498d9",
                    "name": "Zhuoming Chen",
                    "hidden": false
                },
                {
                    "_id": "66c557a6e9d46e06d5d498da",
                    "name": "Jinyuan Shi",
                    "hidden": false
                },
                {
                    "_id": "66c557a6e9d46e06d5d498db",
                    "name": "Ian En-Hsu Yen",
                    "hidden": false
                },
                {
                    "_id": "66c557a6e9d46e06d5d498dc",
                    "name": "Beidi Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-08-20T17:57:31.000Z",
            "title": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding",
            "summary": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency without sacrificing performance but the\nconventional wisdom suggests that its efficacy is limited to small batch sizes.\nIn MagicDec, we show that surprisingly SD can achieve speedup even for a high\nthroughput inference regime for moderate to long sequences. More interestingly,\nan intelligent drafting strategy can achieve better speedup with increasing\nbatch size based on our rigorous analysis. MagicDec first identifies the\nbottleneck shifts with increasing batch size and sequence length, and uses\nthese insights to deploy speculative decoding more effectively for high\nthroughput inference. Then, it leverages draft models with sparse KV cache to\naddress the KV bottleneck that scales with both sequence length and batch size.",
            "upvotes": 6,
            "discussionId": "66c557a7e9d46e06d5d49947"
        },
        "publishedAt": "2024-08-21T01:27:52.117Z",
        "title": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context Generation with Speculative Decoding",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.11049.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2408.10701",
            "authors": [
                {
                    "_id": "66c59d26ed6dabdeafc2a897",
                    "name": "Tej Deep Pala",
                    "hidden": false
                },
                {
                    "_id": "66c59d26ed6dabdeafc2a898",
                    "name": "Vernon Y. H. Toh",
                    "hidden": false
                },
                {
                    "_id": "66c59d26ed6dabdeafc2a899",
                    "name": "Rishabh Bhardwaj",
                    "hidden": false
                },
                {
                    "_id": "66c59d26ed6dabdeafc2a89a",
                    "name": "Soujanya Poria",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-08-20T09:58:01.000Z",
            "title": "Ferret: Faster and Effective Automated Red Teaming with Reward-Based\n  Scoring Technique",
            "summary": "In today's era, where large language models (LLMs) are integrated into\nnumerous real-world applications, ensuring their safety and robustness is\ncrucial for responsible AI usage. Automated red-teaming methods play a key role\nin this process by generating adversarial attacks to identify and mitigate\npotential vulnerabilities in these models. However, existing methods often\nstruggle with slow performance, limited categorical diversity, and high\nresource demands. While Rainbow Teaming, a recent approach, addresses the\ndiversity challenge by framing adversarial prompt generation as a\nquality-diversity search, it remains slow and requires a large fine-tuned\nmutator for optimal performance. To overcome these limitations, we propose\nFerret, a novel approach that builds upon Rainbow Teaming by generating\nmultiple adversarial prompt mutations per iteration and using a scoring\nfunction to rank and select the most effective adversarial prompt. We explore\nvarious scoring functions, including reward models, Llama Guard, and\nLLM-as-a-judge, to rank adversarial mutations based on their potential harm to\nimprove the efficiency of the search for harmful mutations. Our results\ndemonstrate that Ferret, utilizing a reward model as a scoring function,\nimproves the overall attack success rate (ASR) to 95%, which is 46% higher than\nRainbow Teaming. Additionally, Ferret reduces the time needed to achieve a 90%\nASR by 15.2% compared to the baseline and generates adversarial prompts that\nare transferable i.e. effective on other LLMs of larger size. Our codes are\navailable at https://github.com/declare-lab/ferret.",
            "upvotes": 4,
            "discussionId": "66c59d26ed6dabdeafc2a8cd"
        },
        "publishedAt": "2024-08-21T06:26:24.713Z",
        "title": "Ferret: Faster and Effective Automated Red Teaming with Reward-Based Scoring Technique",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.10701.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/626b626405fe1cb65725aca1/aa-Lata46I3fXOmMetvXH.jpeg",
            "fullname": "Soujanya Poria",
            "name": "soujanyaporia",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2408.10764",
            "authors": [
                {
                    "_id": "66c5786109ea098de4af206e",
                    "name": "Chenhan Yuan",
                    "hidden": false
                },
                {
                    "_id": "66c5786109ea098de4af206f",
                    "name": "Fei Huang",
                    "hidden": false
                },
                {
                    "_id": "66c5786109ea098de4af2070",
                    "name": "Ru Peng",
                    "hidden": false
                },
                {
                    "_id": "66c5786109ea098de4af2071",
                    "name": "Keming Lu",
                    "hidden": false
                },
                {
                    "_id": "66c5786109ea098de4af2072",
                    "name": "Bowen Yu",
                    "hidden": false
                },
                {
                    "_id": "66c5786109ea098de4af2073",
                    "name": "Chang Zhou",
                    "hidden": false
                },
                {
                    "_id": "66c5786109ea098de4af2074",
                    "name": "Jingren Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-08-20T12:00:35.000Z",
            "title": "Predicting Rewards Alongside Tokens: Non-disruptive Parameter Insertion\n  for Efficient Inference Intervention in Large Language Model",
            "summary": "Transformer-based large language models (LLMs) exhibit limitations such as\ngenerating unsafe responses, unreliable reasoning, etc. Existing inference\nintervention approaches attempt to mitigate these issues by finetuning\nadditional models to produce calibration signals (such as rewards) that guide\nthe LLM's decoding process. However, this solution introduces substantial time\nand space overhead due to the separate models required. This work proposes\nNon-disruptive parameters insertion (Otter), inserting extra parameters into\nthe transformer architecture to predict calibration signals along with the\noriginal LLM output. Otter offers state-of-the-art performance on multiple\ndemanding tasks while saving up to 86.5\\% extra space and 98.5\\% extra time.\nFurthermore, Otter seamlessly integrates with existing inference engines,\nrequiring only a one-line code change, and the original model response remains\naccessible after the parameter insertion. Our code is publicly available at\nhttps://github.com/chenhan97/Otter",
            "upvotes": 4,
            "discussionId": "66c5786209ea098de4af20a8"
        },
        "publishedAt": "2024-08-21T03:47:43.603Z",
        "title": "Predicting Rewards Alongside Tokens: Non-disruptive Parameter Insertion for Efficient Inference Intervention in Large Language Model",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.10764.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/e8c9025ef24cec958c87a1008bb54fd7.svg",
            "fullname": "Keming Lu",
            "name": "keminglu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2408.10998",
            "authors": [
                {
                    "_id": "66c5552f1fa603b9d4eff4b7",
                    "name": "Dennis Fedorishin",
                    "hidden": false
                },
                {
                    "_id": "66c5552f1fa603b9d4eff4b8",
                    "name": "Lie Lu",
                    "hidden": false
                },
                {
                    "_id": "66c5552f1fa603b9d4eff4b9",
                    "name": "Srirangaraj Setlur",
                    "hidden": false
                },
                {
                    "_id": "66c5552f1fa603b9d4eff4ba",
                    "name": "Venu Govindaraju",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-08-20T16:46:54.000Z",
            "title": "Audio Match Cutting: Finding and Creating Matching Audio Transitions in\n  Movies and Videos",
            "summary": "A \"match cut\" is a common video editing technique where a pair of shots that\nhave a similar composition transition fluidly from one to another. Although\nmatch cuts are often visual, certain match cuts involve the fluid transition of\naudio, where sounds from different sources merge into one indistinguishable\ntransition between two shots. In this paper, we explore the ability to\nautomatically find and create \"audio match cuts\" within videos and movies. We\ncreate a self-supervised audio representation for audio match cutting and\ndevelop a coarse-to-fine audio match pipeline that recommends matching shots\nand creates the blended audio. We further annotate a dataset for the proposed\naudio match cut task and compare the ability of multiple audio representations\nto find audio match cut candidates. Finally, we evaluate multiple methods to\nblend two matching audio candidates with the goal of creating a smooth\ntransition. Project page and examples are available at:\nhttps://denfed.github.io/audiomatchcut/",
            "upvotes": 4,
            "discussionId": "66c555311fa603b9d4eff522"
        },
        "publishedAt": "2024-08-21T01:17:17.363Z",
        "title": "Audio Match Cutting: Finding and Creating Matching Audio Transitions in Movies and Videos",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.10998.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2408.09574",
            "authors": [
                {
                    "_id": "66c5a926e8f01fe34ec01d7d",
                    "user": {
                        "_id": "6575167c983403462a417aa2",
                        "avatarUrl": "/avatars/da05451d86e469fe87aee328103a5fd7.svg",
                        "isPro": false,
                        "fullname": "Thorsten  Hellert",
                        "user": "thellert",
                        "type": "user"
                    },
                    "name": "Thorsten Hellert",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2024-08-21T08:45:27.429Z",
                    "hidden": false
                },
                {
                    "_id": "66c5a926e8f01fe34ec01d7e",
                    "name": "João Montenegro",
                    "hidden": false
                },
                {
                    "_id": "66c5a926e8f01fe34ec01d7f",
                    "name": "Andrea Pollastro",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-08-18T19:18:12.000Z",
            "title": "PhysBERT: A Text Embedding Model for Physics Scientific Literature",
            "summary": "The specialized language and complex concepts in physics pose significant\nchallenges for information extraction through Natural Language Processing\n(NLP). Central to effective NLP applications is the text embedding model, which\nconverts text into dense vector representations for efficient information\nretrieval and semantic analysis. In this work, we introduce PhysBERT, the first\nphysics-specific text embedding model. Pre-trained on a curated corpus of 1.2\nmillion arXiv physics papers and fine-tuned with supervised data, PhysBERT\noutperforms leading general-purpose models on physics-specific tasks including\nthe effectiveness in fine-tuning for specific physics subdomains.",
            "upvotes": 3,
            "discussionId": "66c5a927e8f01fe34ec01da8"
        },
        "publishedAt": "2024-08-21T07:21:10.583Z",
        "title": "PhysBERT: A Text Embedding Model for Physics Scientific Literature",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.09574.png",
        "numComments": 0,
        "submittedBy": {
            "avatarUrl": "/avatars/1208629f14f010dbc2cd94f3c30f9baf.svg",
            "fullname": "JB D.",
            "name": "IAMJB",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2408.10446",
            "authors": [
                {
                    "_id": "66c59839960ea4e66c672b9b",
                    "name": "Niyar R Barman",
                    "hidden": false
                },
                {
                    "_id": "66c59839960ea4e66c672b9c",
                    "name": "Krish Sharma",
                    "hidden": false
                },
                {
                    "_id": "66c59839960ea4e66c672b9d",
                    "name": "Ashhar Aziz",
                    "hidden": false
                },
                {
                    "_id": "66c59839960ea4e66c672b9e",
                    "name": "Shashwat Bajpai",
                    "hidden": false
                },
                {
                    "_id": "66c59839960ea4e66c672b9f",
                    "name": "Shwetangshu Biswas",
                    "hidden": false
                },
                {
                    "_id": "66c59839960ea4e66c672ba0",
                    "name": "Vasu Sharma",
                    "hidden": false
                },
                {
                    "_id": "66c59839960ea4e66c672ba1",
                    "name": "Vinija Jain",
                    "hidden": false
                },
                {
                    "_id": "66c59839960ea4e66c672ba2",
                    "name": "Aman Chadha",
                    "hidden": false
                },
                {
                    "_id": "66c59839960ea4e66c672ba3",
                    "name": "Amit Sheth",
                    "hidden": false
                },
                {
                    "_id": "66c59839960ea4e66c672ba4",
                    "name": "Amitava Das",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-08-19T22:58:30.000Z",
            "title": "The Brittleness of AI-Generated Image Watermarking Techniques: Examining\n  Their Robustness Against Visual Paraphrasing Attacks",
            "summary": "The rapid advancement of text-to-image generation systems, exemplified by\nmodels like Stable Diffusion, Midjourney, Imagen, and DALL-E, has heightened\nconcerns about their potential misuse. In response, companies like Meta and\nGoogle have intensified their efforts to implement watermarking techniques on\nAI-generated images to curb the circulation of potentially misleading visuals.\nHowever, in this paper, we argue that current image watermarking methods are\nfragile and susceptible to being circumvented through visual paraphrase\nattacks. The proposed visual paraphraser operates in two steps. First, it\ngenerates a caption for the given image using KOSMOS-2, one of the latest\nstate-of-the-art image captioning systems. Second, it passes both the original\nimage and the generated caption to an image-to-image diffusion system. During\nthe denoising step of the diffusion pipeline, the system generates a visually\nsimilar image that is guided by the text caption. The resulting image is a\nvisual paraphrase and is free of any watermarks. Our empirical findings\ndemonstrate that visual paraphrase attacks can effectively remove watermarks\nfrom images. This paper provides a critical assessment, empirically revealing\nthe vulnerability of existing watermarking techniques to visual paraphrase\nattacks. While we do not propose solutions to this issue, this paper serves as\na call to action for the scientific community to prioritize the development of\nmore robust watermarking techniques. Our first-of-its-kind visual paraphrase\ndataset and accompanying code are publicly available.",
            "upvotes": 3,
            "discussionId": "66c5983e960ea4e66c672d64"
        },
        "publishedAt": "2024-08-21T06:10:29.889Z",
        "title": "The Brittleness of AI-Generated Image Watermarking Techniques: Examining Their Robustness Against Visual Paraphrasing Attacks",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63a4754927f1f64ed7238dac/Ha97QUlQgNdb_k92Afey7.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.10446.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
            "fullname": "Aman Chadha",
            "name": "amanchadha",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2408.10487",
            "authors": [
                {
                    "_id": "66c550cc3496dcc9c2769d7a",
                    "name": "Xiao Wang",
                    "hidden": false
                },
                {
                    "_id": "66c550cc3496dcc9c2769d7b",
                    "name": "Chao wang",
                    "hidden": false
                },
                {
                    "_id": "66c550cc3496dcc9c2769d7c",
                    "name": "Shiao Wang",
                    "hidden": false
                },
                {
                    "_id": "66c550cc3496dcc9c2769d7d",
                    "name": "Xixi Wang",
                    "hidden": false
                },
                {
                    "_id": "66c550cc3496dcc9c2769d7e",
                    "name": "Zhicheng Zhao",
                    "hidden": false
                },
                {
                    "_id": "66c550cc3496dcc9c2769d7f",
                    "name": "Lin Zhu",
                    "hidden": false
                },
                {
                    "_id": "66c550cc3496dcc9c2769d80",
                    "name": "Bo Jiang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-08-20T02:01:17.000Z",
            "title": "MambaEVT: Event Stream based Visual Object Tracking using State Space\n  Model",
            "summary": "Event camera-based visual tracking has drawn more and more attention in\nrecent years due to the unique imaging principle and advantages of low energy\nconsumption, high dynamic range, and dense temporal resolution. Current\nevent-based tracking algorithms are gradually hitting their performance\nbottlenecks, due to the utilization of vision Transformer and the static\ntemplate for target object localization. In this paper, we propose a novel\nMamba-based visual tracking framework that adopts the state space model with\nlinear complexity as a backbone network. The search regions and target template\nare fed into the vision Mamba network for simultaneous feature extraction and\ninteraction. The output tokens of search regions will be fed into the tracking\nhead for target localization. More importantly, we consider introducing a\ndynamic template update strategy into the tracking framework using the Memory\nMamba network. By considering the diversity of samples in the target template\nlibrary and making appropriate adjustments to the template memory module, a\nmore effective dynamic template can be integrated. The effective combination of\ndynamic and static templates allows our Mamba-based tracking algorithm to\nachieve a good balance between accuracy and computational cost on multiple\nlarge-scale datasets, including EventVOT, VisEvent, and FE240hz. The source\ncode will be released on https://github.com/Event-AHU/MambaEVT",
            "upvotes": 3,
            "discussionId": "66c550d13496dcc9c2769f21"
        },
        "publishedAt": "2024-08-21T00:58:38.199Z",
        "title": "MambaEVT: Event Stream based Visual Object Tracking using State Space Model",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.10487.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2408.11048",
            "authors": [
                {
                    "_id": "66c5593fb666ae525586a3ea",
                    "name": "Yi Zhao",
                    "hidden": false
                },
                {
                    "_id": "66c5593fb666ae525586a3eb",
                    "name": "Le Chen",
                    "hidden": false
                },
                {
                    "_id": "66c5593fb666ae525586a3ec",
                    "name": "Jan Schneider",
                    "hidden": false
                },
                {
                    "_id": "66c5593fb666ae525586a3ed",
                    "name": "Quankai Gao",
                    "hidden": false
                },
                {
                    "_id": "66c5593fb666ae525586a3ee",
                    "name": "Juho Kannala",
                    "hidden": false
                },
                {
                    "_id": "66c5593fb666ae525586a3ef",
                    "name": "Bernhard Schölkopf",
                    "hidden": false
                },
                {
                    "_id": "66c5593fb666ae525586a3f0",
                    "name": "Joni Pajarinen",
                    "hidden": false
                },
                {
                    "_id": "66c5593fb666ae525586a3f1",
                    "name": "Dieter Büchler",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-08-20T17:56:52.000Z",
            "title": "RP1M: A Large-Scale Motion Dataset for Piano Playing with Bi-Manual\n  Dexterous Robot Hands",
            "summary": "It has been a long-standing research goal to endow robot hands with\nhuman-level dexterity. Bi-manual robot piano playing constitutes a task that\ncombines challenges from dynamic tasks, such as generating fast while precise\nmotions, with slower but contact-rich manipulation problems. Although\nreinforcement learning based approaches have shown promising results in\nsingle-task performance, these methods struggle in a multi-song setting. Our\nwork aims to close this gap and, thereby, enable imitation learning approaches\nfor robot piano playing at scale. To this end, we introduce the Robot Piano 1\nMillion (RP1M) dataset, containing bi-manual robot piano playing motion data of\nmore than one million trajectories. We formulate finger placements as an\noptimal transport problem, thus, enabling automatic annotation of vast amounts\nof unlabeled songs. Benchmarking existing imitation learning approaches shows\nthat such approaches reach state-of-the-art robot piano playing performance by\nleveraging RP1M.",
            "upvotes": 2,
            "discussionId": "66c55940b666ae525586a429"
        },
        "publishedAt": "2024-08-21T01:34:36.959Z",
        "title": "RP1M: A Large-Scale Motion Dataset for Piano Playing with Bi-Manual Dexterous Robot Hands",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.11048.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2408.10906",
            "authors": [
                {
                    "_id": "66c55822e9d46e06d5d4ca88",
                    "name": "Qi Ma",
                    "hidden": false
                },
                {
                    "_id": "66c55822e9d46e06d5d4ca89",
                    "name": "Yue Li",
                    "hidden": false
                },
                {
                    "_id": "66c55822e9d46e06d5d4ca8a",
                    "name": "Bin Ren",
                    "hidden": false
                },
                {
                    "_id": "66c55822e9d46e06d5d4ca8b",
                    "name": "Nicu Sebe",
                    "hidden": false
                },
                {
                    "_id": "66c55822e9d46e06d5d4ca8c",
                    "name": "Ender Konukoglu",
                    "hidden": false
                },
                {
                    "_id": "66c55822e9d46e06d5d4ca8d",
                    "name": "Theo Gevers",
                    "hidden": false
                },
                {
                    "_id": "66c55822e9d46e06d5d4ca8e",
                    "name": "Luc Van Gool",
                    "hidden": false
                },
                {
                    "_id": "66c55822e9d46e06d5d4ca8f",
                    "name": "Danda Pani Paudel",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-08-20T14:49:14.000Z",
            "title": "ShapeSplat: A Large-scale Dataset of Gaussian Splats and Their\n  Self-Supervised Pretraining",
            "summary": "3D Gaussian Splatting (3DGS) has become the de facto method of 3D\nrepresentation in many vision tasks. This calls for the 3D understanding\ndirectly in this representation space. To facilitate the research in this\ndirection, we first build a large-scale dataset of 3DGS using the commonly used\nShapeNet and ModelNet datasets. Our dataset ShapeSplat consists of 65K objects\nfrom 87 unique categories, whose labels are in accordance with the respective\ndatasets. The creation of this dataset utilized the compute equivalent of 2 GPU\nyears on a TITAN XP GPU.\n  We utilize our dataset for unsupervised pretraining and supervised finetuning\nfor classification and segmentation tasks. To this end, we introduce\n\\textit{Gaussian-MAE}, which highlights the unique benefits of\nrepresentation learning from Gaussian parameters. Through exhaustive\nexperiments, we provide several valuable insights. In particular, we show that\n(1) the distribution of the optimized GS centroids significantly differs from\nthe uniformly sampled point cloud (used for initialization) counterpart; (2)\nthis change in distribution results in degradation in classification but\nimprovement in segmentation tasks when using only the centroids; (3) to\nleverage additional Gaussian parameters, we propose Gaussian feature grouping\nin a normalized feature space, along with splats pooling layer, offering a\ntailored solution to effectively group and embed similar Gaussians, which leads\nto notable improvement in finetuning tasks.",
            "upvotes": 2,
            "discussionId": "66c55824e9d46e06d5d4cb73"
        },
        "publishedAt": "2024-08-21T01:29:58.143Z",
        "title": "ShapeSplat: A Large-scale Dataset of Gaussian Splats and Their Self-Supervised Pretraining",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.10906.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2408.10088",
            "authors": [
                {
                    "_id": "66c5a729a1b181697e714b5e",
                    "name": "Shiqi Wang",
                    "hidden": false
                },
                {
                    "_id": "66c5a729a1b181697e714b5f",
                    "name": "Zhouye Zhao",
                    "hidden": false
                },
                {
                    "_id": "66c5a729a1b181697e714b60",
                    "name": "Yuhang Xie",
                    "hidden": false
                },
                {
                    "_id": "66c5a729a1b181697e714b61",
                    "name": "Mingchuan Ma",
                    "hidden": false
                },
                {
                    "_id": "66c5a729a1b181697e714b62",
                    "name": "Zirui Chen",
                    "hidden": false
                },
                {
                    "_id": "66c5a729a1b181697e714b63",
                    "name": "Zeyu Wang",
                    "hidden": false
                },
                {
                    "_id": "66c5a729a1b181697e714b64",
                    "name": "Bohao Su",
                    "hidden": false
                },
                {
                    "_id": "66c5a729a1b181697e714b65",
                    "name": "Wenrui Xu",
                    "hidden": false
                },
                {
                    "_id": "66c5a729a1b181697e714b66",
                    "name": "Tianyi Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-08-19T15:29:56.000Z",
            "title": "Recent Surge in Public Interest in Transportation: Sentiment Analysis of\n  Baidu Apollo Go Using Weibo Data",
            "summary": "Urban mobility and transportation systems have been profoundly transformed by\nthe advancement of autonomous vehicle technologies. Baidu Apollo Go, a pioneer\nrobotaxi service from the Chinese tech giant Baidu, has recently been widely\ndeployed in major cities like Beijing and Wuhan, sparking increased\nconversation and offering a glimpse into the future of urban mobility.\n  This study investigates public attitudes towards Apollo Go across China using\nSentiment Analysis with a hybrid BERT model on 36,096 Weibo posts from January\nto July 2024. The analysis shows that 89.56\\% of posts related to Apollo Go are\nclustered in July. From January to July, public sentiment was mostly positive,\nbut negative comments began to rise after it became a hot topic on July 21.\n  Spatial analysis indicates a strong correlation between provinces with high\ndiscussion intensity and those where Apollo Go operates. Initially, Hubei and\nGuangdong dominated online posting volume, but by July, Guangdong, Beijing, and\ninternational regions had overtaken Hubei. Attitudes varied significantly among\nprovinces, with Xinjiang and Qinghai showing optimism and Tibet and Gansu\nexpressing concerns about the impact on traditional taxi services.\n  Sentiment analysis revealed that positive comments focused on technology\napplications and personal experiences, while negative comments centered on job\ndisplacement and safety concerns. In summary, this study highlights the\ndivergence in public perceptions of autonomous ride-hailing services, providing\nvaluable insights for planners, policymakers, and service providers. The model\nis published on Hugging Face at\nhttps://huggingface.co/wsqstar/bert-finetuned-weibo-luobokuaipao and the\nrepository on GitHub at https://github.com/GIStudio/trb2024.",
            "upvotes": 1,
            "discussionId": "66c5a72da1b181697e714c3b"
        },
        "publishedAt": "2024-08-21T07:07:34.334Z",
        "title": "Recent Surge in Public Interest in Transportation: Sentiment Analysis of Baidu Apollo Go Using Weibo Data",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.10088.png",
        "numComments": 0,
        "submittedBy": {
            "avatarUrl": "/avatars/1208629f14f010dbc2cd94f3c30f9baf.svg",
            "fullname": "JB D.",
            "name": "IAMJB",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    }
]