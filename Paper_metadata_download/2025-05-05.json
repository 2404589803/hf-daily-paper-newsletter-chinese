[
  {
    "paper": {
      "id": "2505.01079",
      "authors": [
        {
          "_id": "68183d9ae65ec5d5716c6d94",
          "user": {
            "_id": "636b20591340f879a2eb98d0",
            "avatarUrl": "/avatars/4fc5cb13f916bcbc842ccf387bd5f6c0.svg",
            "isPro": false,
            "fullname": "Daneul Kim",
            "user": "carpedkm",
            "type": "user"
          },
          "name": "Daneul Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-05T07:30:39.678Z",
          "hidden": false
        },
        {
          "_id": "68183d9ae65ec5d5716c6d95",
          "name": "Jaeah Lee",
          "hidden": false
        },
        {
          "_id": "68183d9ae65ec5d5716c6d96",
          "name": "Jaesik Park",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-02T07:36:49.000Z",
      "submittedOnDailyAt": "2025-05-05T02:56:01.277Z",
      "title": "Improving Editability in Image Generation with Layer-wise Memory",
      "submittedOnDailyBy": {
        "_id": "636b20591340f879a2eb98d0",
        "avatarUrl": "/avatars/4fc5cb13f916bcbc842ccf387bd5f6c0.svg",
        "isPro": false,
        "fullname": "Daneul Kim",
        "user": "carpedkm",
        "type": "user"
      },
      "summary": "Most real-world image editing tasks require multiple sequential edits to\nachieve desired results. Current editing approaches, primarily designed for\nsingle-object modifications, struggle with sequential editing: especially with\nmaintaining previous edits along with adapting new objects naturally into the\nexisting content. These limitations significantly hinder complex editing\nscenarios where multiple objects need to be modified while preserving their\ncontextual relationships. We address this fundamental challenge through two key\nproposals: enabling rough mask inputs that preserve existing content while\nnaturally integrating new elements and supporting consistent editing across\nmultiple modifications. Our framework achieves this through layer-wise memory,\nwhich stores latent representations and prompt embeddings from previous edits.\nWe propose Background Consistency Guidance that leverages memorized latents to\nmaintain scene coherence and Multi-Query Disentanglement in cross-attention\nthat ensures natural adaptation to existing content. To evaluate our method, we\npresent a new benchmark dataset incorporating semantic alignment metrics and\ninteractive editing scenarios. Through comprehensive experiments, we\ndemonstrate superior performance in iterative image editing tasks with minimal\nuser effort, requiring only rough masks while maintaining high-quality results\nthroughout multiple editing steps.",
      "upvotes": 9,
      "discussionId": "68183d9de65ec5d5716c6e78",
      "projectPage": "https://carpedkm.github.io/projects/improving_edit/index.html",
      "githubRepo": "https://github.com/carpedkm/improving-editability",
      "ai_keywords": [
        "layer-wise memory",
        "latent representations",
        "prompt embeddings",
        "Background Consistency Guidance",
        "Multi-Query Disentanglement",
        "cross-attention",
        "semantic alignment metrics",
        "interactive editing scenarios",
        "iterative image editing"
      ]
    },
    "publishedAt": "2025-05-02T03:36:49.000Z",
    "title": "Improving Editability in Image Generation with Layer-wise Memory",
    "summary": "Most real-world image editing tasks require multiple sequential edits to\nachieve desired results. Current editing approaches, primarily designed for\nsingle-object modifications, struggle with sequential editing: especially with\nmaintaining previous edits along with adapting new objects naturally into the\nexisting content. These limitations significantly hinder complex editing\nscenarios where multiple objects need to be modified while preserving their\ncontextual relationships. We address this fundamental challenge through two key\nproposals: enabling rough mask inputs that preserve existing content while\nnaturally integrating new elements and supporting consistent editing across\nmultiple modifications. Our framework achieves this through layer-wise memory,\nwhich stores latent representations and prompt embeddings from previous edits.\nWe propose Background Consistency Guidance that leverages memorized latents to\nmaintain scene coherence and Multi-Query Disentanglement in cross-attention\nthat ensures natural adaptation to existing content. To evaluate our method, we\npresent a new benchmark dataset incorporating semantic alignment metrics and\ninteractive editing scenarios. Through comprehensive experiments, we\ndemonstrate superior performance in iterative image editing tasks with minimal\nuser effort, requiring only rough masks while maintaining high-quality results\nthroughout multiple editing steps.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.01079.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "636b20591340f879a2eb98d0",
      "avatarUrl": "/avatars/4fc5cb13f916bcbc842ccf387bd5f6c0.svg",
      "fullname": "Daneul Kim",
      "name": "carpedkm",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.20438",
      "authors": [
        {
          "_id": "6814e35a19162d7749852c4b",
          "user": {
            "_id": "633d4630e1aec4b8b33ad5b8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633d4630e1aec4b8b33ad5b8/owFqnyCMW8yuo9VcjA1hx.jpeg",
            "isPro": false,
            "fullname": "Ziyang Xu",
            "user": "Uyoung",
            "type": "user"
          },
          "name": "Ziyang Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-04T10:04:40.638Z",
          "hidden": false
        },
        {
          "_id": "6814e35a19162d7749852c4c",
          "name": "Kangsheng Duan",
          "hidden": false
        },
        {
          "_id": "6814e35a19162d7749852c4d",
          "user": {
            "_id": "627a34dac488a8ce15a2dc4a",
            "avatarUrl": "/avatars/61aecef507dea6620fe5574493f83595.svg",
            "isPro": false,
            "fullname": "ShenXiaolei",
            "user": "SmileTAT",
            "type": "user"
          },
          "name": "Xiaolei Shen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-05T07:32:16.185Z",
          "hidden": false
        },
        {
          "_id": "6814e35a19162d7749852c4e",
          "name": "Zhifeng Ding",
          "hidden": false
        },
        {
          "_id": "6814e35a19162d7749852c4f",
          "user": {
            "_id": "66c2e7fc934e2f07753542ac",
            "avatarUrl": "/avatars/f6fa3f94435cf1c1d06daa6c925d07d0.svg",
            "isPro": false,
            "fullname": "LWY",
            "user": "wenyuliu",
            "type": "user"
          },
          "name": "Wenyu Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-05T07:32:31.210Z",
          "hidden": false
        },
        {
          "_id": "6814e35a19162d7749852c50",
          "name": "Xiaohu Ruan",
          "hidden": false
        },
        {
          "_id": "6814e35a19162d7749852c51",
          "user": {
            "_id": "65389a669c474315d7425f96",
            "avatarUrl": "/avatars/2fa3828ca489cfe1948129a0eccf264f.svg",
            "isPro": false,
            "fullname": "chenxiaoxin",
            "user": "steelozazala",
            "type": "user"
          },
          "name": "Xiaoxin Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-05T07:32:55.012Z",
          "hidden": false
        },
        {
          "_id": "6814e35a19162d7749852c52",
          "user": {
            "_id": "62600de6d47e3dbae32ce1ce",
            "avatarUrl": "/avatars/a536417cfec6e10ac415091bd1829426.svg",
            "isPro": false,
            "fullname": "Xinggang Wang",
            "user": "xinggangw",
            "type": "user"
          },
          "name": "Xinggang Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-05T07:33:01.396Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-29T05:28:36.000Z",
      "submittedOnDailyAt": "2025-05-05T05:42:06.841Z",
      "title": "PixelHacker: Image Inpainting with Structural and Semantic Consistency",
      "submittedOnDailyBy": {
        "_id": "633d4630e1aec4b8b33ad5b8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633d4630e1aec4b8b33ad5b8/owFqnyCMW8yuo9VcjA1hx.jpeg",
        "isPro": false,
        "fullname": "Ziyang Xu",
        "user": "Uyoung",
        "type": "user"
      },
      "summary": "Image inpainting is a fundamental research area between image editing and\nimage generation. Recent state-of-the-art (SOTA) methods have explored novel\nattention mechanisms, lightweight architectures, and context-aware modeling,\ndemonstrating impressive performance. However, they often struggle with complex\nstructure (e.g., texture, shape, spatial relations) and semantics (e.g., color\nconsistency, object restoration, and logical correctness), leading to artifacts\nand inappropriate generation. To address this challenge, we design a simple yet\neffective inpainting paradigm called latent categories guidance, and further\npropose a diffusion-based model named PixelHacker. Specifically, we first\nconstruct a large dataset containing 14 million image-mask pairs by annotating\nforeground and background (potential 116 and 21 categories, respectively).\nThen, we encode potential foreground and background representations separately\nthrough two fixed-size embeddings, and intermittently inject these features\ninto the denoising process via linear attention. Finally, by pre-training on\nour dataset and fine-tuning on open-source benchmarks, we obtain PixelHacker.\nExtensive experiments show that PixelHacker comprehensively outperforms the\nSOTA on a wide range of datasets (Places2, CelebA-HQ, and FFHQ) and exhibits\nremarkable consistency in both structure and semantics. Project page at\nhttps://hustvl.github.io/PixelHacker.",
      "upvotes": 8,
      "discussionId": "6814e35c19162d7749852caa",
      "projectPage": "https://hustvl.github.io/PixelHacker",
      "githubRepo": "https://github.com/hustvl/PixelHacker",
      "ai_keywords": [
        "latent categories guidance",
        "diffusion-based model",
        "PixelHacker",
        "image-mask pairs",
        "fixed-size embeddings",
        "linear attention",
        "pre-training",
        "fine-tuning",
        "Places2",
        "CelebA-HQ",
        "FFHQ"
      ]
    },
    "publishedAt": "2025-04-29T01:28:36.000Z",
    "title": "PixelHacker: Image Inpainting with Structural and Semantic Consistency",
    "summary": "Image inpainting is a fundamental research area between image editing and\nimage generation. Recent state-of-the-art (SOTA) methods have explored novel\nattention mechanisms, lightweight architectures, and context-aware modeling,\ndemonstrating impressive performance. However, they often struggle with complex\nstructure (e.g., texture, shape, spatial relations) and semantics (e.g., color\nconsistency, object restoration, and logical correctness), leading to artifacts\nand inappropriate generation. To address this challenge, we design a simple yet\neffective inpainting paradigm called latent categories guidance, and further\npropose a diffusion-based model named PixelHacker. Specifically, we first\nconstruct a large dataset containing 14 million image-mask pairs by annotating\nforeground and background (potential 116 and 21 categories, respectively).\nThen, we encode potential foreground and background representations separately\nthrough two fixed-size embeddings, and intermittently inject these features\ninto the denoising process via linear attention. Finally, by pre-training on\nour dataset and fine-tuning on open-source benchmarks, we obtain PixelHacker.\nExtensive experiments show that PixelHacker comprehensively outperforms the\nSOTA on a wide range of datasets (Places2, CelebA-HQ, and FFHQ) and exhibits\nremarkable consistency in both structure and semantics. Project page at\nhttps://hustvl.github.io/PixelHacker.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20438.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "633d4630e1aec4b8b33ad5b8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633d4630e1aec4b8b33ad5b8/owFqnyCMW8yuo9VcjA1hx.jpeg",
      "fullname": "Ziyang Xu",
      "name": "Uyoung",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.00023",
      "authors": [
        {
          "_id": "6818413000ee590453feaf66",
          "user": {
            "_id": "65169ea3fbfe82f36fc6655c",
            "avatarUrl": "/avatars/01714ad316a2e06488246e4fe7dcdb52.svg",
            "isPro": false,
            "fullname": "Hyun Ji Lee",
            "user": "hyunjilee",
            "type": "user"
          },
          "name": "Hyunji Lee",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-05T07:34:41.006Z",
          "hidden": false
        },
        {
          "_id": "6818413000ee590453feaf67",
          "user": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "isPro": false,
            "fullname": "Franck Dernoncourt",
            "user": "Franck-Dernoncourt",
            "type": "user"
          },
          "name": "Franck Dernoncourt",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-05T07:30:36.382Z",
          "hidden": false
        },
        {
          "_id": "6818413000ee590453feaf68",
          "name": "Trung Bui",
          "hidden": false
        },
        {
          "_id": "6818413000ee590453feaf69",
          "user": {
            "_id": "6690ef3db70d356ed3e05cb0",
            "avatarUrl": "/avatars/530f3a0bd7b93e1e7f385c2708335728.svg",
            "isPro": false,
            "fullname": "yoon seung-hyun",
            "user": "aifactoryysh",
            "type": "user"
          },
          "name": "Seunghyun Yoon",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-05T07:35:01.378Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-25T02:40:48.000Z",
      "submittedOnDailyAt": "2025-05-05T03:10:26.249Z",
      "title": "CORG: Generating Answers from Complex, Interrelated Contexts",
      "submittedOnDailyBy": {
        "_id": "62c5947524171688a9feb992",
        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
        "isPro": false,
        "fullname": "Franck Dernoncourt",
        "user": "Franck-Dernoncourt",
        "type": "user"
      },
      "summary": "In a real-world corpus, knowledge frequently recurs across documents but\noften contains inconsistencies due to ambiguous naming, outdated information,\nor errors, leading to complex interrelationships between contexts. Previous\nresearch has shown that language models struggle with these complexities,\ntypically focusing on single factors in isolation. We classify these\nrelationships into four types: distracting, ambiguous, counterfactual, and\nduplicated. Our analysis reveals that no single approach effectively addresses\nall these interrelationships simultaneously. Therefore, we introduce Context\nOrganizer (CORG), a framework that organizes multiple contexts into\nindependently processed groups. This design allows the model to efficiently\nfind all relevant answers while ensuring disambiguation. CORG consists of three\nkey components: a graph constructor, a reranker, and an aggregator. Our results\ndemonstrate that CORG balances performance and efficiency effectively,\noutperforming existing grouping methods and achieving comparable results to\nmore computationally intensive, single-context approaches.",
      "upvotes": 3,
      "discussionId": "6818413100ee590453feaf97",
      "ai_keywords": [
        "graph constructor",
        "reranker",
        "aggregator",
        "Context Organizer (CORG)"
      ]
    },
    "publishedAt": "2025-04-24T22:40:48.000Z",
    "title": "CORG: Generating Answers from Complex, Interrelated Contexts",
    "summary": "In a real-world corpus, knowledge frequently recurs across documents but\noften contains inconsistencies due to ambiguous naming, outdated information,\nor errors, leading to complex interrelationships between contexts. Previous\nresearch has shown that language models struggle with these complexities,\ntypically focusing on single factors in isolation. We classify these\nrelationships into four types: distracting, ambiguous, counterfactual, and\nduplicated. Our analysis reveals that no single approach effectively addresses\nall these interrelationships simultaneously. Therefore, we introduce Context\nOrganizer (CORG), a framework that organizes multiple contexts into\nindependently processed groups. This design allows the model to efficiently\nfind all relevant answers while ensuring disambiguation. CORG consists of three\nkey components: a graph constructor, a reranker, and an aggregator. Our results\ndemonstrate that CORG balances performance and efficiency effectively,\noutperforming existing grouping methods and achieving comparable results to\nmore computationally intensive, single-context approaches.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.00023.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c5947524171688a9feb992",
      "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
      "fullname": "Franck Dernoncourt",
      "name": "Franck-Dernoncourt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.00174",
      "authors": [
        {
          "_id": "681594746a80babbe28775ba",
          "user": {
            "_id": "66225f7100352aeea584d02a",
            "avatarUrl": "/avatars/ca13f59bebf73d03a63a935f628aea5c.svg",
            "isPro": false,
            "fullname": "Ilan Strauss",
            "user": "strauss-NYC",
            "type": "user"
          },
          "name": "Ilan Strauss",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-04T10:04:25.496Z",
          "hidden": false
        },
        {
          "_id": "681594746a80babbe28775bb",
          "user": {
            "_id": "67535114d2a628475a0e7a6e",
            "avatarUrl": "/avatars/d7eb574c026817bbc204843c96f1caa6.svg",
            "isPro": false,
            "fullname": "Isobel Moure",
            "user": "isobelmoure",
            "type": "user"
          },
          "name": "Isobel Moure",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-05T07:35:12.655Z",
          "hidden": false
        },
        {
          "_id": "681594746a80babbe28775bc",
          "name": "Tim O'Reilly",
          "hidden": false
        },
        {
          "_id": "681594746a80babbe28775bd",
          "user": {
            "_id": "64582e94f8bdc3512d1ee940",
            "avatarUrl": "/avatars/e0eb72f06c7da58cb569198540484ab1.svg",
            "isPro": false,
            "fullname": "Sruly Rosenblat",
            "user": "sruly",
            "type": "user"
          },
          "name": "Sruly Rosenblat",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-05T07:35:24.305Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-30T20:44:42.000Z",
      "submittedOnDailyAt": "2025-05-05T02:50:33.572Z",
      "title": "Real-World Gaps in AI Governance Research",
      "submittedOnDailyBy": {
        "_id": "66225f7100352aeea584d02a",
        "avatarUrl": "/avatars/ca13f59bebf73d03a63a935f628aea5c.svg",
        "isPro": false,
        "fullname": "Ilan Strauss",
        "user": "strauss-NYC",
        "type": "user"
      },
      "summary": "Drawing on 1,178 safety and reliability papers from 9,439 generative AI\npapers (January 2020 - March 2025), we compare research outputs of leading AI\ncompanies (Anthropic, Google DeepMind, Meta, Microsoft, and OpenAI) and AI\nuniversities (CMU, MIT, NYU, Stanford, UC Berkeley, and University of\nWashington). We find that corporate AI research increasingly concentrates on\npre-deployment areas -- model alignment and testing & evaluation -- while\nattention to deployment-stage issues such as model bias has waned. Significant\nresearch gaps exist in high-risk deployment domains, including healthcare,\nfinance, misinformation, persuasive and addictive features, hallucinations, and\ncopyright. Without improved observability into deployed AI, growing corporate\nconcentration could deepen knowledge deficits. We recommend expanding external\nresearcher access to deployment data and systematic observability of in-market\nAI behaviors.",
      "upvotes": 2,
      "discussionId": "681594746a80babbe28775e5"
    },
    "publishedAt": "2025-04-30T16:44:42.000Z",
    "title": "Real-World Gaps in AI Governance Research",
    "summary": "Drawing on 1,178 safety and reliability papers from 9,439 generative AI\npapers (January 2020 - March 2025), we compare research outputs of leading AI\ncompanies (Anthropic, Google DeepMind, Meta, Microsoft, and OpenAI) and AI\nuniversities (CMU, MIT, NYU, Stanford, UC Berkeley, and University of\nWashington). We find that corporate AI research increasingly concentrates on\npre-deployment areas -- model alignment and testing & evaluation -- while\nattention to deployment-stage issues such as model bias has waned. Significant\nresearch gaps exist in high-risk deployment domains, including healthcare,\nfinance, misinformation, persuasive and addictive features, hallucinations, and\ncopyright. Without improved observability into deployed AI, growing corporate\nconcentration could deepen knowledge deficits. We recommend expanding external\nresearcher access to deployment data and systematic observability of in-market\nAI behaviors.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.00174.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66225f7100352aeea584d02a",
      "avatarUrl": "/avatars/ca13f59bebf73d03a63a935f628aea5c.svg",
      "fullname": "Ilan Strauss",
      "name": "strauss-NYC",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.20859",
      "authors": [
        {
          "_id": "681713aec075e49c1b22500e",
          "user": {
            "_id": "630d180f3dc31beba6f061c3",
            "avatarUrl": "/avatars/7cf70bff453b6e64fcac52f45c6b3730.svg",
            "isPro": false,
            "fullname": "guy hadad",
            "user": "guyhadad01",
            "type": "user"
          },
          "name": "Guy Hadad",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-04T07:14:56.074Z",
          "hidden": false
        },
        {
          "_id": "681713aec075e49c1b22500f",
          "name": "Haggai Roitman",
          "hidden": false
        },
        {
          "_id": "681713aec075e49c1b225010",
          "user": {
            "_id": "638f42e4c4444c6ca8715a06",
            "avatarUrl": "/avatars/aae741d00ed1f5ead516c07543e59f3e.svg",
            "isPro": false,
            "fullname": "yotam eshel",
            "user": "yeshel",
            "type": "user"
          },
          "name": "Yotam Eshel",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-04T07:13:50.951Z",
          "hidden": false
        },
        {
          "_id": "681713aec075e49c1b225011",
          "user": {
            "_id": "63aace84785b8279fe30b5f9",
            "avatarUrl": "/avatars/7b4793f6f0a0a8b608d0395c0e92a7eb.svg",
            "isPro": false,
            "fullname": "Bracha Shapira",
            "user": "Bshapira",
            "type": "user"
          },
          "name": "Bracha Shapira",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-05T07:35:41.141Z",
          "hidden": false
        },
        {
          "_id": "681713aec075e49c1b225012",
          "user": {
            "_id": "64141f0365f4b23aa99507a4",
            "avatarUrl": "/avatars/46d599acaa0f492139949dba0f00e030.svg",
            "isPro": false,
            "fullname": "Lior Rokach",
            "user": "liorrokach",
            "type": "user"
          },
          "name": "Lior Rokach",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-05T07:35:47.274Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-29T15:33:20.000Z",
      "submittedOnDailyAt": "2025-05-05T03:47:16.624Z",
      "title": "X-Cross: Dynamic Integration of Language Models for Cross-Domain\n  Sequential Recommendation",
      "submittedOnDailyBy": {
        "_id": "630d180f3dc31beba6f061c3",
        "avatarUrl": "/avatars/7cf70bff453b6e64fcac52f45c6b3730.svg",
        "isPro": false,
        "fullname": "guy hadad",
        "user": "guyhadad01",
        "type": "user"
      },
      "summary": "As new products are emerging daily, recommendation systems are required to\nquickly adapt to possible new domains without needing extensive retraining.\nThis work presents ``X-Cross'' -- a novel cross-domain\nsequential-recommendation model that recommends products in new domains by\nintegrating several domain-specific language models; each model is fine-tuned\nwith low-rank adapters (LoRA). Given a recommendation prompt, operating layer\nby layer, X-Cross dynamically refines the representation of each source\nlanguage model by integrating knowledge from all other models. These refined\nrepresentations are propagated from one layer to the next, leveraging the\nactivations from each domain adapter to ensure domain-specific nuances are\npreserved while enabling adaptability across domains. Using Amazon datasets for\nsequential recommendation, X-Cross achieves performance comparable to a model\nthat is fine-tuned with LoRA, while using only 25% of the additional\nparameters. In cross-domain tasks, such as adapting from Toys domain to Tools,\nElectronics or Sports, X-Cross demonstrates robust performance, while requiring\nabout 50%-75% less fine-tuning data than LoRA to make fine-tuning effective.\nFurthermore, X-Cross achieves significant improvement in accuracy over\nalternative cross-domain baselines. Overall, X-Cross enables scalable and\nadaptive cross-domain recommendations, reducing computational overhead and\nproviding an efficient solution for data-constrained environments.",
      "upvotes": 1,
      "discussionId": "681713aec075e49c1b22503e",
      "ai_keywords": [
        "cross-domain sequential-recommendation",
        "domain-specific language models",
        "low-rank adapters (LoRA)",
        "recommendation prompt",
        "activations",
        "domain-specific nuances",
        "cross-domain tasks",
        "computational overhead",
        "data-constrained environments"
      ]
    },
    "publishedAt": "2025-04-29T11:33:20.000Z",
    "title": "X-Cross: Dynamic Integration of Language Models for Cross-Domain\n  Sequential Recommendation",
    "summary": "As new products are emerging daily, recommendation systems are required to\nquickly adapt to possible new domains without needing extensive retraining.\nThis work presents ``X-Cross'' -- a novel cross-domain\nsequential-recommendation model that recommends products in new domains by\nintegrating several domain-specific language models; each model is fine-tuned\nwith low-rank adapters (LoRA). Given a recommendation prompt, operating layer\nby layer, X-Cross dynamically refines the representation of each source\nlanguage model by integrating knowledge from all other models. These refined\nrepresentations are propagated from one layer to the next, leveraging the\nactivations from each domain adapter to ensure domain-specific nuances are\npreserved while enabling adaptability across domains. Using Amazon datasets for\nsequential recommendation, X-Cross achieves performance comparable to a model\nthat is fine-tuned with LoRA, while using only 25% of the additional\nparameters. In cross-domain tasks, such as adapting from Toys domain to Tools,\nElectronics or Sports, X-Cross demonstrates robust performance, while requiring\nabout 50%-75% less fine-tuning data than LoRA to make fine-tuning effective.\nFurthermore, X-Cross achieves significant improvement in accuracy over\nalternative cross-domain baselines. Overall, X-Cross enables scalable and\nadaptive cross-domain recommendations, reducing computational overhead and\nproviding an efficient solution for data-constrained environments.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20859.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "630d180f3dc31beba6f061c3",
      "avatarUrl": "/avatars/7cf70bff453b6e64fcac52f45c6b3730.svg",
      "fullname": "guy hadad",
      "name": "guyhadad01",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.00562",
      "authors": [
        {
          "_id": "68184c3fb727bc3cb7301e15",
          "user": {
            "_id": "668e100b97171f3399e07f5d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/pgzatAZut-uoXOBOwgiFB.jpeg",
            "isPro": false,
            "fullname": "Yue Meng",
            "user": "yuemithucsd",
            "type": "user"
          },
          "name": "Yue Meng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-05T07:36:14.168Z",
          "hidden": false
        },
        {
          "_id": "68184c3fb727bc3cb7301e16",
          "name": "Chuchu Fan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-01T14:40:07.000Z",
      "submittedOnDailyAt": "2025-05-05T03:58:02.420Z",
      "title": "TeLoGraF: Temporal Logic Planning via Graph-encoded Flow Matching",
      "submittedOnDailyBy": {
        "_id": "668e100b97171f3399e07f5d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/pgzatAZut-uoXOBOwgiFB.jpeg",
        "isPro": false,
        "fullname": "Yue Meng",
        "user": "yuemithucsd",
        "type": "user"
      },
      "summary": "Learning to solve complex tasks with signal temporal logic (STL)\nspecifications is crucial to many real-world applications. However, most\nprevious works only consider fixed or parametrized STL specifications due to\nthe lack of a diverse STL dataset and encoders to effectively extract temporal\nlogic information for downstream tasks. In this paper, we propose TeLoGraF,\nTemporal Logic Graph-encoded Flow, which utilizes Graph Neural Networks (GNN)\nencoder and flow-matching to learn solutions for general STL specifications. We\nidentify four commonly used STL templates and collect a total of 200K\nspecifications with paired demonstrations. We conduct extensive experiments in\nfive simulation environments ranging from simple dynamical models in the 2D\nspace to high-dimensional 7DoF Franka Panda robot arm and Ant quadruped\nnavigation. Results show that our method outperforms other baselines in the STL\nsatisfaction rate. Compared to classical STL planning algorithms, our approach\nis 10-100X faster in inference and can work on any system dynamics. Besides, we\nshow our graph-encoding method's capability to solve complex STLs and\nrobustness to out-distribution STL specifications. Code is available at\nhttps://github.com/mengyuest/TeLoGraF",
      "upvotes": 0,
      "discussionId": "68184c41b727bc3cb7301e6c",
      "ai_keywords": [
        "TeLoGraF",
        "Temporal Logic Graph-encoded Flow",
        "Graph Neural Networks (GNN)",
        "flow-matching",
        "STL specifications",
        "STL templates",
        "STL satisfaction rate",
        "dynamical models",
        "Franka Panda robot arm",
        "Ant quadruped navigation",
        "classical STL planning algorithms"
      ]
    },
    "publishedAt": "2025-05-01T10:40:07.000Z",
    "title": "TeLoGraF: Temporal Logic Planning via Graph-encoded Flow Matching",
    "summary": "Learning to solve complex tasks with signal temporal logic (STL)\nspecifications is crucial to many real-world applications. However, most\nprevious works only consider fixed or parametrized STL specifications due to\nthe lack of a diverse STL dataset and encoders to effectively extract temporal\nlogic information for downstream tasks. In this paper, we propose TeLoGraF,\nTemporal Logic Graph-encoded Flow, which utilizes Graph Neural Networks (GNN)\nencoder and flow-matching to learn solutions for general STL specifications. We\nidentify four commonly used STL templates and collect a total of 200K\nspecifications with paired demonstrations. We conduct extensive experiments in\nfive simulation environments ranging from simple dynamical models in the 2D\nspace to high-dimensional 7DoF Franka Panda robot arm and Ant quadruped\nnavigation. Results show that our method outperforms other baselines in the STL\nsatisfaction rate. Compared to classical STL planning algorithms, our approach\nis 10-100X faster in inference and can work on any system dynamics. Besides, we\nshow our graph-encoding method's capability to solve complex STLs and\nrobustness to out-distribution STL specifications. Code is available at\nhttps://github.com/mengyuest/TeLoGraF",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.00562.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "668e100b97171f3399e07f5d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/pgzatAZut-uoXOBOwgiFB.jpeg",
      "fullname": "Yue Meng",
      "name": "yuemithucsd",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]