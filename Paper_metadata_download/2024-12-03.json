[
    {
        "paper": {
            "id": "2412.01824",
            "authors": [
                {
                    "_id": "674e8f89ec384b629bbc0f60",
                    "user": {
                        "_id": "63fda3fced9eead590ff6918",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677566802735-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Zeyi Sun",
                        "user": "Zery",
                        "type": "user"
                    },
                    "name": "Zeyi Sun",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-03T11:14:44.696Z",
                    "hidden": false
                },
                {
                    "_id": "674e8f89ec384b629bbc0f61",
                    "name": "Ziyang Chu",
                    "hidden": false
                },
                {
                    "_id": "674e8f89ec384b629bbc0f62",
                    "name": "Pan Zhang",
                    "hidden": false
                },
                {
                    "_id": "674e8f89ec384b629bbc0f63",
                    "user": {
                        "_id": "62fae9328e137d7c4b896498",
                        "avatarUrl": "/avatars/1bda39dec585c099417cc9daa9f53c42.svg",
                        "isPro": false,
                        "fullname": "Tong Wu",
                        "user": "tongwu2020",
                        "type": "user"
                    },
                    "name": "Tong Wu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-03T11:27:25.489Z",
                    "hidden": false
                },
                {
                    "_id": "674e8f89ec384b629bbc0f64",
                    "name": "Xiaoyi Dong",
                    "hidden": false
                },
                {
                    "_id": "674e8f89ec384b629bbc0f65",
                    "user": {
                        "_id": "63859cf3b2906edaf83af9f0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63859cf3b2906edaf83af9f0/iUQm5FAomzqYi6fkqIn9F.jpeg",
                        "isPro": false,
                        "fullname": "Yuhang Zang",
                        "user": "yuhangzang",
                        "type": "user"
                    },
                    "name": "Yuhang Zang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-03T11:27:44.758Z",
                    "hidden": false
                },
                {
                    "_id": "674e8f89ec384b629bbc0f66",
                    "user": {
                        "_id": "66209605f44be495f123b055",
                        "avatarUrl": "/avatars/b72081a0383a709899f9ba80b4211690.svg",
                        "isPro": false,
                        "fullname": "Yuanjun Xiong",
                        "user": "TheYJ",
                        "type": "user"
                    },
                    "name": "Yuanjun Xiong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-03T11:27:54.048Z",
                    "hidden": false
                },
                {
                    "_id": "674e8f89ec384b629bbc0f67",
                    "user": {
                        "_id": "636317ed80c1a705a6eff396",
                        "avatarUrl": "/avatars/3db090e101b916d9256d0d3e043db71d.svg",
                        "isPro": false,
                        "fullname": "Dahua Lin",
                        "user": "lindahua",
                        "type": "user"
                    },
                    "name": "Dahua Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-03T11:28:00.539Z",
                    "hidden": false
                },
                {
                    "_id": "674e8f89ec384b629bbc0f68",
                    "name": "Jiaqi Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-02T18:59:26.000Z",
            "title": "X-Prompt: Towards Universal In-Context Image Generation in\n  Auto-Regressive Vision Language Foundation Models",
            "summary": "In-context generation is a key component of large language models' (LLMs)\nopen-task generalization capability. By leveraging a few examples as context,\nLLMs can perform both in-domain and out-of-domain tasks. Recent advancements in\nauto-regressive vision-language models (VLMs) built upon LLMs have showcased\nimpressive performance in text-to-image generation. However, the potential of\nin-context learning for general image generation tasks remains largely\nunexplored. To address this, we introduce X-Prompt, a purely auto-regressive\nlarge-vision language model designed to deliver competitive performance across\na wide range of both seen and unseen image generation tasks, all within a\nunified in-context learning framework. X-Prompt incorporates a specialized\ndesign that efficiently compresses valuable features from in-context examples,\nsupporting longer in-context token sequences and improving its ability to\ngeneralize to unseen tasks. A unified training task for both text and image\nprediction enables X-Prompt to handle general image generation with enhanced\ntask awareness from in-context examples. Extensive experiments validate the\nmodel's performance across diverse seen image generation tasks and its capacity\nto generalize to previously unseen tasks.",
            "upvotes": 35,
            "discussionId": "674e8f8cec384b629bbc1072"
        },
        "publishedAt": "2024-12-03T00:00:06.179Z",
        "title": "X-Prompt: Towards Universal In-Context Image Generation in Auto-Regressive Vision Language Foundation Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.01824.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "63fda3fced9eead590ff6918",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677566802735-noauth.jpeg",
            "fullname": "Zeyi Sun",
            "name": "Zery",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 13
        }
    },
    {
        "paper": {
            "id": "2412.00131",
            "authors": [
                {
                    "_id": "674e8961ef38dd7bfb7d1add",
                    "user": {
                        "_id": "6367a8175bb06007ea099b8f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6367a8175bb06007ea099b8f/IjG7HyWyWRlVt_XwRbxRW.jpeg",
                        "isPro": false,
                        "fullname": "linbin",
                        "user": "LanguageBind",
                        "type": "user"
                    },
                    "name": "Bin Lin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-03T11:15:00.619Z",
                    "hidden": false
                },
                {
                    "_id": "674e8961ef38dd7bfb7d1ade",
                    "user": {
                        "_id": "646de6402fd5a8eb8c518aa6",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646de6402fd5a8eb8c518aa6/HYWb8-fT1kTm-ROBr1-0X.jpeg",
                        "isPro": false,
                        "fullname": "yunyangge",
                        "user": "yunyangge",
                        "type": "user"
                    },
                    "name": "Yunyang Ge",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-03T13:49:41.403Z",
                    "hidden": false
                },
                {
                    "_id": "674e8961ef38dd7bfb7d1adf",
                    "user": {
                        "_id": "645b4026f9d4ec91fdd516ce",
                        "avatarUrl": "/avatars/fb6556ab973764af5c44980f4e8fcc71.svg",
                        "isPro": false,
                        "fullname": "Xinhua Cheng",
                        "user": "cxh0519",
                        "type": "user"
                    },
                    "name": "Xinhua Cheng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-03T13:49:48.039Z",
                    "hidden": false
                },
                {
                    "_id": "674e8961ef38dd7bfb7d1ae0",
                    "name": "Zongjian Li",
                    "hidden": false
                },
                {
                    "_id": "674e8961ef38dd7bfb7d1ae1",
                    "name": "Bin Zhu",
                    "hidden": false
                },
                {
                    "_id": "674e8961ef38dd7bfb7d1ae2",
                    "name": "Shaodong Wang",
                    "hidden": false
                },
                {
                    "_id": "674e8961ef38dd7bfb7d1ae3",
                    "name": "Xianyi He",
                    "hidden": false
                },
                {
                    "_id": "674e8961ef38dd7bfb7d1ae4",
                    "name": "Yang Ye",
                    "hidden": false
                },
                {
                    "_id": "674e8961ef38dd7bfb7d1ae5",
                    "user": {
                        "_id": "63468720dd6d90d82ccf3450",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
                        "isPro": false,
                        "fullname": "YSH",
                        "user": "BestWishYsh",
                        "type": "user"
                    },
                    "name": "Shenghai Yuan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-03T11:14:50.136Z",
                    "hidden": false
                },
                {
                    "_id": "674e8961ef38dd7bfb7d1ae6",
                    "user": {
                        "_id": "65c0588569429d85dc594a85",
                        "avatarUrl": "/avatars/822203d982519b999247a53235e30c39.svg",
                        "isPro": false,
                        "fullname": "liuhan",
                        "user": "LiuhanChen",
                        "type": "user"
                    },
                    "name": "Liuhan Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-03T13:51:17.509Z",
                    "hidden": false
                },
                {
                    "_id": "674e8961ef38dd7bfb7d1ae7",
                    "name": "Tanghui Jia",
                    "hidden": false
                },
                {
                    "_id": "674e8961ef38dd7bfb7d1ae8",
                    "user": {
                        "_id": "64979385e486365ca6b3c030",
                        "avatarUrl": "/avatars/589344a9db41873d78452560cca3d234.svg",
                        "isPro": false,
                        "fullname": "Junwu Zhang",
                        "user": "junwuzhang",
                        "type": "user"
                    },
                    "name": "Junwu Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-03T13:51:03.908Z",
                    "hidden": false
                },
                {
                    "_id": "674e8961ef38dd7bfb7d1ae9",
                    "user": {
                        "_id": "63c09aa88d1175e3399e906e",
                        "avatarUrl": "/avatars/a1b8a89e83275f049f7bf75af9086d84.svg",
                        "isPro": false,
                        "fullname": "Zhenyu Tang",
                        "user": "royjames",
                        "type": "user"
                    },
                    "name": "Zhenyu Tang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-03T13:51:24.244Z",
                    "hidden": false
                },
                {
                    "_id": "674e8961ef38dd7bfb7d1aea",
                    "name": "Yatian Pang",
                    "hidden": false
                },
                {
                    "_id": "674e8961ef38dd7bfb7d1aeb",
                    "name": "Bin She",
                    "hidden": false
                },
                {
                    "_id": "674e8961ef38dd7bfb7d1aec",
                    "name": "Cen Yan",
                    "hidden": false
                },
                {
                    "_id": "674e8961ef38dd7bfb7d1aed",
                    "name": "Zhiheng Hu",
                    "hidden": false
                },
                {
                    "_id": "674e8961ef38dd7bfb7d1aee",
                    "name": "Xiaoyi Dong",
                    "hidden": false
                },
                {
                    "_id": "674e8961ef38dd7bfb7d1aef",
                    "user": {
                        "_id": "64b02ec0e5000ae8a572ced5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b02ec0e5000ae8a572ced5/6ifLntBU2ICQK7SW8WxKU.png",
                        "isPro": false,
                        "fullname": "Lin Chen",
                        "user": "Lin-Chen",
                        "type": "user"
                    },
                    "name": "Lin Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-03T11:14:52.541Z",
                    "hidden": false
                },
                {
                    "_id": "674e8961ef38dd7bfb7d1af0",
                    "name": "Zhang Pan",
                    "hidden": false
                },
                {
                    "_id": "674e8961ef38dd7bfb7d1af1",
                    "name": "Xing Zhou",
                    "hidden": false
                },
                {
                    "_id": "674e8961ef38dd7bfb7d1af2",
                    "name": "Shaoling Dong",
                    "hidden": false
                },
                {
                    "_id": "674e8961ef38dd7bfb7d1af3",
                    "name": "Yonghong Tian",
                    "hidden": false
                },
                {
                    "_id": "674e8961ef38dd7bfb7d1af4",
                    "name": "Li Yuan",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-28T14:07:45.000Z",
            "title": "Open-Sora Plan: Open-Source Large Video Generation Model",
            "summary": "We introduce Open-Sora Plan, an open-source project that aims to contribute a\nlarge generation model for generating desired high-resolution videos with long\ndurations based on various user inputs. Our project comprises multiple\ncomponents for the entire video generation process, including a Wavelet-Flow\nVariational Autoencoder, a Joint Image-Video Skiparse Denoiser, and various\ncondition controllers. Moreover, many assistant strategies for efficient\ntraining and inference are designed, and a multi-dimensional data curation\npipeline is proposed for obtaining desired high-quality data. Benefiting from\nefficient thoughts, our Open-Sora Plan achieves impressive video generation\nresults in both qualitative and quantitative evaluations. We hope our careful\ndesign and practical experience can inspire the video generation research\ncommunity. All our codes and model weights are publicly available at\nhttps://github.com/PKU-YuanGroup/Open-Sora-Plan.",
            "upvotes": 18,
            "discussionId": "674e8963ef38dd7bfb7d1ba9"
        },
        "publishedAt": "2024-12-02T23:32:47.983Z",
        "title": "Open-Sora Plan: Open-Source Large Video Generation Model",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.00131.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6367a8175bb06007ea099b8f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6367a8175bb06007ea099b8f/IjG7HyWyWRlVt_XwRbxRW.jpeg",
            "fullname": "linbin",
            "name": "LanguageBind",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 150
        }
    },
    {
        "paper": {
            "id": "2411.18499",
            "authors": [
                {
                    "_id": "6749af6061f0c91e2ea76a1e",
                    "user": {
                        "_id": "65df481e530333731ea24617",
                        "avatarUrl": "/avatars/3ed41f5d7d0489193807b5e6260f16c9.svg",
                        "isPro": false,
                        "fullname": "ZHOU PENGFEI",
                        "user": "lyuukuu",
                        "type": "user"
                    },
                    "name": "Pengfei Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-03T11:29:04.250Z",
                    "hidden": false
                },
                {
                    "_id": "6749af6061f0c91e2ea76a1f",
                    "user": {
                        "_id": "6527ecfce41334bdec702350",
                        "avatarUrl": "/avatars/a79a2adcfb2caf46cf32657e440a17d9.svg",
                        "isPro": false,
                        "fullname": "xiaopengqiu",
                        "user": "xiaopengpeng",
                        "type": "user"
                    },
                    "name": "Xiaopeng Peng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-03T11:29:16.477Z",
                    "hidden": false
                },
                {
                    "_id": "6749af6061f0c91e2ea76a20",
                    "name": "Jiajun Song",
                    "hidden": false
                },
                {
                    "_id": "6749af6061f0c91e2ea76a21",
                    "name": "Chuanhao Li",
                    "hidden": false
                },
                {
                    "_id": "6749af6061f0c91e2ea76a22",
                    "user": {
                        "_id": "646dee28174cc96d50951991",
                        "avatarUrl": "/avatars/17d88a24c905e9819268b27037015a35.svg",
                        "isPro": false,
                        "fullname": "xu",
                        "user": "xuzhaopan",
                        "type": "user"
                    },
                    "name": "Zhaopan Xu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-03T12:00:31.371Z",
                    "hidden": false
                },
                {
                    "_id": "6749af6061f0c91e2ea76a23",
                    "name": "Yue Yang",
                    "hidden": false
                },
                {
                    "_id": "6749af6061f0c91e2ea76a24",
                    "name": "Ziyao Guo",
                    "hidden": false
                },
                {
                    "_id": "6749af6061f0c91e2ea76a25",
                    "name": "Hao Zhang",
                    "hidden": false
                },
                {
                    "_id": "6749af6061f0c91e2ea76a26",
                    "name": "Yuqi Lin",
                    "hidden": false
                },
                {
                    "_id": "6749af6061f0c91e2ea76a27",
                    "user": {
                        "_id": "65a88c3d26598b995531fff1",
                        "avatarUrl": "/avatars/b1a524857d8572d0405476661b434160.svg",
                        "isPro": false,
                        "fullname": "Yefei He",
                        "user": "yefly",
                        "type": "user"
                    },
                    "name": "Yefei He",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-03T11:30:18.425Z",
                    "hidden": false
                },
                {
                    "_id": "6749af6061f0c91e2ea76a28",
                    "user": {
                        "_id": "661639c4dd9284ce5d361f0f",
                        "avatarUrl": "/avatars/9a0f8f725d7b90c30dfdb64313761155.svg",
                        "isPro": true,
                        "fullname": "Lirui Zhao",
                        "user": "LiruiZhao",
                        "type": "user"
                    },
                    "name": "Lirui Zhao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-03T11:30:12.376Z",
                    "hidden": false
                },
                {
                    "_id": "6749af6061f0c91e2ea76a29",
                    "name": "Shuo Liu",
                    "hidden": false
                },
                {
                    "_id": "6749af6061f0c91e2ea76a2a",
                    "name": "Tianhua Li",
                    "hidden": false
                },
                {
                    "_id": "6749af6061f0c91e2ea76a2b",
                    "name": "Yuxuan Xie",
                    "hidden": false
                },
                {
                    "_id": "6749af6061f0c91e2ea76a2c",
                    "name": "Xiaojun Chang",
                    "hidden": false
                },
                {
                    "_id": "6749af6061f0c91e2ea76a2d",
                    "name": "Yu Qiao",
                    "hidden": false
                },
                {
                    "_id": "6749af6061f0c91e2ea76a2e",
                    "user": {
                        "_id": "64b3fd42eec33e27dcc4c941",
                        "avatarUrl": "/avatars/5aa1a99468fa61d4b8b0e80b592c4e55.svg",
                        "isPro": false,
                        "fullname": "Wenqi Shao",
                        "user": "wqshao126",
                        "type": "user"
                    },
                    "name": "Wenqi Shao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-03T11:29:33.418Z",
                    "hidden": false
                },
                {
                    "_id": "6749af6061f0c91e2ea76a2f",
                    "user": {
                        "_id": "63527f4e7d071f23d085ad45",
                        "avatarUrl": "/avatars/99a51adef5673b3ac1a8c02eb47759c4.svg",
                        "isPro": false,
                        "fullname": "KAIPENG ZHANG",
                        "user": "kpzhang",
                        "type": "user"
                    },
                    "name": "Kaipeng Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-03T11:29:27.230Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-27T16:39:04.000Z",
            "title": "GATE OpenING: A Comprehensive Benchmark for Judging Open-ended\n  Interleaved Image-Text Generation",
            "summary": "Multimodal Large Language Models (MLLMs) have made significant strides in\nvisual understanding and generation tasks. However, generating interleaved\nimage-text content remains a challenge, which requires integrated multimodal\nunderstanding and generation abilities. While the progress in unified models\noffers new solutions, existing benchmarks are insufficient for evaluating these\nmethods due to data size and diversity limitations. To bridge this gap, we\nintroduce GATE OpenING (OpenING), a comprehensive benchmark comprising 5,400\nhigh-quality human-annotated instances across 56 real-world tasks. OpenING\ncovers diverse daily scenarios such as travel guide, design, and brainstorming,\noffering a robust platform for challenging interleaved generation methods. In\naddition, we present IntJudge, a judge model for evaluating open-ended\nmultimodal generation methods. Trained with a novel data pipeline, our IntJudge\nachieves an agreement rate of 82. 42% with human judgments, outperforming\nGPT-based evaluators by 11.34%. Extensive experiments on OpenING reveal that\ncurrent interleaved generation methods still have substantial room for\nimprovement. Key findings on interleaved image-text generation are further\npresented to guide the development of next-generation models. The OpenING is\nopen-sourced at https://opening.github.io.",
            "upvotes": 16,
            "discussionId": "6749af6561f0c91e2ea76c2b"
        },
        "publishedAt": "2024-12-03T01:24:34.336Z",
        "title": "GATE OpenING: A Comprehensive Benchmark for Judging Open-ended Interleaved Image-Text Generation",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/65f1713552c38a91e0a445e8/ZTSddFqkCuk_G2zAwQQqP.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.18499.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "65f1713552c38a91e0a445e8",
            "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
            "fullname": "kaipeng",
            "name": "kpzhang996",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        }
    },
    {
        "paper": {
            "id": "2412.01819",
            "authors": [
                {
                    "_id": "674eb16e236b0a3fa92fdacd",
                    "user": {
                        "_id": "6153161e0f96aa6c4d14cbf0",
                        "avatarUrl": "/avatars/e708193a493b850cf266ad6fb8a8beb7.svg",
                        "isPro": false,
                        "fullname": "Anton Voronov",
                        "user": "AntonVoronov",
                        "type": "user"
                    },
                    "name": "Anton Voronov",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-03T13:48:25.474Z",
                    "hidden": false
                },
                {
                    "_id": "674eb16e236b0a3fa92fdace",
                    "user": {
                        "_id": "629cf0475a13ba8233dd18c9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654452258405-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Denis Kuznedelev",
                        "user": "SpiridonSunRotator",
                        "type": "user"
                    },
                    "name": "Denis Kuznedelev",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-03T09:56:35.421Z",
                    "hidden": false
                },
                {
                    "_id": "674eb16e236b0a3fa92fdacf",
                    "user": {
                        "_id": "628b8e83ac304a69264a8c9c",
                        "avatarUrl": "/avatars/a51b630e8d01224d5d8d5edff0a69d30.svg",
                        "isPro": false,
                        "fullname": "Mikhail Khoroshikh",
                        "user": "michellemoorre",
                        "type": "user"
                    },
                    "name": "Mikhail Khoroshikh",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-03T13:48:31.974Z",
                    "hidden": false
                },
                {
                    "_id": "674eb16e236b0a3fa92fdad0",
                    "user": {
                        "_id": "624c4c51be275a1baf50a19b",
                        "avatarUrl": "/avatars/aa527fff70d096955962e1964a6840c8.svg",
                        "isPro": false,
                        "fullname": "Valentin Khrulkov",
                        "user": "mathemage",
                        "type": "user"
                    },
                    "name": "Valentin Khrulkov",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-03T13:48:37.650Z",
                    "hidden": false
                },
                {
                    "_id": "674eb16e236b0a3fa92fdad1",
                    "user": {
                        "_id": "62b6cc49752323892323bc04",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b6cc49752323892323bc04/gGBld1KJIP9AIpd81L3PC.jpeg",
                        "isPro": true,
                        "fullname": "Dmitry Baranchuk",
                        "user": "dbaranchuk",
                        "type": "user"
                    },
                    "name": "Dmitry Baranchuk",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-03T13:48:43.760Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-02T18:57:41.000Z",
            "title": "Switti: Designing Scale-Wise Transformers for Text-to-Image Synthesis",
            "summary": "This work presents Switti, a scale-wise transformer for text-to-image\ngeneration. Starting from existing next-scale prediction AR models, we first\nexplore them for T2I generation and propose architectural modifications to\nimprove their convergence and overall performance. We then observe that\nself-attention maps of our pretrained scale-wise AR model exhibit weak\ndependence on preceding scales. Based on this insight, we propose a non-AR\ncounterpart facilitating {sim}11% faster sampling and lower memory usage\nwhile also achieving slightly better generation quality.Furthermore, we reveal\nthat classifier-free guidance at high-resolution scales is often unnecessary\nand can even degrade performance. %may be not only unnecessary but potentially\ndetrimental. By disabling guidance at these scales, we achieve an additional\nsampling acceleration of {sim}20% and improve the generation of\nfine-grained details. Extensive human preference studies and automated\nevaluations show that Switti outperforms existing T2I AR models and competes\nwith state-of-the-art T2I diffusion models while being up to 7{times}\nfaster.",
            "upvotes": 15,
            "discussionId": "674eb174236b0a3fa92fdc0c"
        },
        "publishedAt": "2024-12-03T03:00:18.570Z",
        "title": "Switti: Designing Scale-Wise Transformers for Text-to-Image Synthesis",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.01819.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "62b6cc49752323892323bc04",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b6cc49752323892323bc04/gGBld1KJIP9AIpd81L3PC.jpeg",
            "fullname": "Dmitry Baranchuk",
            "name": "dbaranchuk",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isMod": false,
            "followerCount": 3
        }
    },
    {
        "paper": {
            "id": "2411.18671",
            "authors": [
                {
                    "_id": "674e74e2b3b82188d2e57535",
                    "user": {
                        "_id": "64c23a7639f458b91a27d0c4",
                        "avatarUrl": "/avatars/ab9059be0aa475ff45e0cbdd8a2e7e51.svg",
                        "isPro": false,
                        "fullname": "QuJinyuan",
                        "user": "JinyuanQu",
                        "type": "user"
                    },
                    "name": "Jinyuan Qu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-03T14:32:00.661Z",
                    "hidden": false
                },
                {
                    "_id": "674e74e2b3b82188d2e57536",
                    "user": {
                        "_id": "6499b0184936457997180c90",
                        "avatarUrl": "/avatars/b8be7bfabf746639e30330f5f623f560.svg",
                        "isPro": false,
                        "fullname": "Hongyang Li",
                        "user": "compileme",
                        "type": "user"
                    },
                    "name": "Hongyang Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-03T14:32:23.009Z",
                    "hidden": false
                },
                {
                    "_id": "674e74e2b3b82188d2e57537",
                    "user": {
                        "_id": "638b13c0c1d591879698f4e2",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638b13c0c1d591879698f4e2/X8X4EWMXuzhBpG62wO2xS.jpeg",
                        "isPro": false,
                        "fullname": "Shilong Liu",
                        "user": "ShilongLiu",
                        "type": "user"
                    },
                    "name": "Shilong Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-03T14:32:29.516Z",
                    "hidden": false
                },
                {
                    "_id": "674e74e2b3b82188d2e57538",
                    "name": "Tianhe Ren",
                    "hidden": false
                },
                {
                    "_id": "674e74e2b3b82188d2e57539",
                    "name": "Zhaoyang Zeng",
                    "hidden": false
                },
                {
                    "_id": "674e74e2b3b82188d2e5753a",
                    "name": "Lei Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-27T17:37:22.000Z",
            "title": "TAPTRv3: Spatial and Temporal Context Foster Robust Tracking of Any\n  Point in Long Video",
            "summary": "In this paper, we present TAPTRv3, which is built upon TAPTRv2 to improve its\npoint tracking robustness in long videos. TAPTRv2 is a simple DETR-like\nframework that can accurately track any point in real-world videos without\nrequiring cost-volume. TAPTRv3 improves TAPTRv2 by addressing its shortage in\nquerying high quality features from long videos, where the target tracking\npoints normally undergo increasing variation over time. In TAPTRv3, we propose\nto utilize both spatial and temporal context to bring better feature querying\nalong the spatial and temporal dimensions for more robust tracking in long\nvideos. For better spatial feature querying, we present Context-aware\nCross-Attention (CCA), which leverages surrounding spatial context to enhance\nthe quality of attention scores when querying image features. For better\ntemporal feature querying, we introduce Visibility-aware Long-Temporal\nAttention (VLTA) to conduct temporal attention to all past frames while\nconsidering their corresponding visibilities, which effectively addresses the\nfeature drifting problem in TAPTRv2 brought by its RNN-like long-temporal\nmodeling. TAPTRv3 surpasses TAPTRv2 by a large margin on most of the\nchallenging datasets and obtains state-of-the-art performance. Even when\ncompared with methods trained with large-scale extra internal data, TAPTRv3 is\nstill competitive.",
            "upvotes": 13,
            "discussionId": "674e74e5b3b82188d2e575e0"
        },
        "publishedAt": "2024-12-02T22:07:57.570Z",
        "title": "TAPTRv3: Spatial and Temporal Context Foster Robust Tracking of Any Point in Long Video",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6543de2aee7bbb595295c179/vEseBI1fZ3ZHd2e7yQq4c.mp4",
            "https://cdn-uploads.huggingface.co/production/uploads/6543de2aee7bbb595295c179/-_URx-NgttqEh6u-w4vqt.mp4",
            "https://cdn-uploads.huggingface.co/production/uploads/6543de2aee7bbb595295c179/M3ltT5jM-eL1FqyoH7JFe.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.18671.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6543de2aee7bbb595295c179",
            "avatarUrl": "/avatars/8d8a410900562fe9ce0b5bbd4ae6c55a.svg",
            "fullname": "HYeung Lee",
            "name": "HYeungLee",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2412.00927",
            "authors": [
                {
                    "_id": "674e99abe91289226e0428b4",
                    "user": {
                        "_id": "64405a9d518271b0d1beea38",
                        "avatarUrl": "/avatars/b702474588fd7090773320422417a582.svg",
                        "isPro": false,
                        "fullname": "Weiming Ren",
                        "user": "wren93",
                        "type": "user"
                    },
                    "name": "Weiming Ren",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-03T11:14:38.615Z",
                    "hidden": false
                },
                {
                    "_id": "674e99abe91289226e0428b5",
                    "user": {
                        "_id": "639005861c5a6237272557d1",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1670382915881-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Huan Yang",
                        "user": "hyang0511",
                        "type": "user"
                    },
                    "name": "Huan Yang",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2024-12-03T05:39:57.190Z",
                    "hidden": false
                },
                {
                    "_id": "674e99abe91289226e0428b6",
                    "name": "Jie Min",
                    "hidden": false
                },
                {
                    "_id": "674e99abe91289226e0428b7",
                    "name": "Cong Wei",
                    "hidden": false
                },
                {
                    "_id": "674e99abe91289226e0428b8",
                    "user": {
                        "_id": "6313a86154e6e5d9f0f94e04",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
                        "isPro": false,
                        "fullname": "Wenhu Chen",
                        "user": "wenhu",
                        "type": "user"
                    },
                    "name": "Wenhu Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-03T15:22:40.605Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-01T18:27:28.000Z",
            "title": "VISTA: Enhancing Long-Duration and High-Resolution Video Understanding\n  by Video Spatiotemporal Augmentation",
            "summary": "Current large multimodal models (LMMs) face significant challenges in\nprocessing and comprehending long-duration or high-resolution videos, which is\nmainly due to the lack of high-quality datasets. To address this issue from a\ndata-centric perspective, we propose VISTA, a simple yet effective Video\nSpatiotemporal Augmentation framework that synthesizes long-duration and\nhigh-resolution video instruction-following pairs from existing video-caption\ndatasets. VISTA spatially and temporally combines videos to create new\nsynthetic videos with extended durations and enhanced resolutions, and\nsubsequently produces question-answer pairs pertaining to these newly\nsynthesized videos. Based on this paradigm, we develop seven video augmentation\nmethods and curate VISTA-400K, a video instruction-following dataset aimed at\nenhancing long-duration and high-resolution video understanding. Finetuning\nvarious video LMMs on our data resulted in an average improvement of 3.3%\nacross four challenging benchmarks for long-video understanding. Furthermore,\nwe introduce the first comprehensive high-resolution video understanding\nbenchmark HRVideoBench, on which our finetuned models achieve a 6.5%\nperformance gain. These results highlight the effectiveness of our framework.",
            "upvotes": 12,
            "discussionId": "674e99ade91289226e042920"
        },
        "publishedAt": "2024-12-03T00:41:01.960Z",
        "title": "VISTA: Enhancing Long-Duration and High-Resolution Video Understanding by Video Spatiotemporal Augmentation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.00927.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64405a9d518271b0d1beea38",
            "avatarUrl": "/avatars/b702474588fd7090773320422417a582.svg",
            "fullname": "Weiming Ren",
            "name": "wren93",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 3
        }
    },
    {
        "paper": {
            "id": "2412.00154",
            "authors": [
                {
                    "_id": "674e99013209f1fbb76bbec6",
                    "user": {
                        "_id": "6433526c2bfb2b0ec75bf7eb",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/9LLLoJCYAzIS-GW3D1_4t.jpeg",
                        "isPro": false,
                        "fullname": "Yuxiang Zhang",
                        "user": "YuxiangZhang-BIT",
                        "type": "user"
                    },
                    "name": "Yuxiang Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-03T15:17:09.665Z",
                    "hidden": false
                },
                {
                    "_id": "674e99013209f1fbb76bbec7",
                    "user": {
                        "_id": "665ace40ddba0f825f3a4941",
                        "avatarUrl": "/avatars/ed7e209e14b5ae495aa7f4baec980b38.svg",
                        "isPro": false,
                        "fullname": "Shangxi Wu",
                        "user": "KirinNg",
                        "type": "user"
                    },
                    "name": "Shangxi Wu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-03T15:16:33.713Z",
                    "hidden": false
                },
                {
                    "_id": "674e99013209f1fbb76bbec8",
                    "user": {
                        "_id": "65b5b586d2ae98fd67c76d6b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/RR_mNkUAG4zP902WLgeYv.jpeg",
                        "isPro": false,
                        "fullname": "yuqi yang",
                        "user": "tzteyang",
                        "type": "user"
                    },
                    "name": "Yuqi Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-03T15:16:27.807Z",
                    "hidden": false
                },
                {
                    "_id": "674e99013209f1fbb76bbec9",
                    "user": {
                        "_id": "6540f059733c1ce6a6fcca88",
                        "avatarUrl": "/avatars/b8fa6b383caeb1fe79e163bac90d5cba.svg",
                        "isPro": false,
                        "fullname": "ShuJiangMing",
                        "user": "SanJiaoMao",
                        "type": "user"
                    },
                    "name": "Jiangming Shu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-03T15:16:20.077Z",
                    "hidden": false
                },
                {
                    "_id": "674e99013209f1fbb76bbeca",
                    "name": "Jinlin Xiao",
                    "hidden": false
                },
                {
                    "_id": "674e99013209f1fbb76bbecb",
                    "name": "Chao Kong",
                    "hidden": false
                },
                {
                    "_id": "674e99013209f1fbb76bbecc",
                    "name": "Jitao Sang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-29T07:19:56.000Z",
            "title": "o1-Coder: an o1 Replication for Coding",
            "summary": "The technical report introduces O1-CODER, an attempt to replicate OpenAI's o1\nmodel with a focus on coding tasks. It integrates reinforcement learning (RL)\nand Monte Carlo Tree Search (MCTS) to enhance the model's System-2 thinking\ncapabilities. The framework includes training a Test Case Generator (TCG) for\nstandardized code testing, using MCTS to generate code data with reasoning\nprocesses, and iteratively fine-tuning the policy model to initially produce\npseudocode, followed by the generation of the full code. The report also\naddresses the opportunities and challenges in deploying o1-like models in\nreal-world applications, suggesting transitioning to the System-2 paradigm and\nhighlighting the imperative for environment state updates. Updated model\nprogress and experimental results will be reported in subsequent versions. All\nsource code, curated datasets, as well as the derived models will be disclosed\nat https://github.com/ADaM-BJTU/O1-CODER .",
            "upvotes": 12,
            "discussionId": "674e99023209f1fbb76bbf06"
        },
        "publishedAt": "2024-12-03T00:38:02.500Z",
        "title": "o1-Coder: an o1 Replication for Coding",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.00154.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 5287
        }
    },
    {
        "paper": {
            "id": "2412.00174",
            "authors": [
                {
                    "_id": "674eadabef38dd7bfb8863cf",
                    "name": "Jianping Jiang",
                    "hidden": false
                },
                {
                    "_id": "674eadabef38dd7bfb8863d0",
                    "name": "Weiye Xiao",
                    "hidden": false
                },
                {
                    "_id": "674eadabef38dd7bfb8863d1",
                    "user": {
                        "_id": "6391fc80077fee9fbca29790",
                        "avatarUrl": "/avatars/53f89c6de4e91cdcaaa52a295a5f4ea4.svg",
                        "isPro": false,
                        "fullname": "Zhengyu Lin",
                        "user": "hayashiLin",
                        "type": "user"
                    },
                    "name": "Zhengyu Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-03T15:18:17.310Z",
                    "hidden": false
                },
                {
                    "_id": "674eadabef38dd7bfb8863d2",
                    "name": "Huaizhong Zhang",
                    "hidden": false
                },
                {
                    "_id": "674eadabef38dd7bfb8863d3",
                    "user": {
                        "_id": "66652269d22ee88501a25264",
                        "avatarUrl": "/avatars/5680b90d20e179be4e56ceabb3aa145d.svg",
                        "isPro": false,
                        "fullname": "Tianxiang Ren",
                        "user": "rentxray",
                        "type": "user"
                    },
                    "name": "Tianxiang Ren",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-03T15:18:45.694Z",
                    "hidden": false
                },
                {
                    "_id": "674eadabef38dd7bfb8863d4",
                    "name": "Yang Gao",
                    "hidden": false
                },
                {
                    "_id": "674eadabef38dd7bfb8863d5",
                    "name": "Zhiqian Lin",
                    "hidden": false
                },
                {
                    "_id": "674eadabef38dd7bfb8863d6",
                    "user": {
                        "_id": "652d06833b5997ed71ce5c46",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/xZTXEcnEogEmBm_ledJQr.jpeg",
                        "isPro": false,
                        "fullname": "Zhongang Cai",
                        "user": "caizhongang",
                        "type": "user"
                    },
                    "name": "Zhongang Cai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-03T09:56:37.337Z",
                    "hidden": false
                },
                {
                    "_id": "674eadabef38dd7bfb8863d7",
                    "user": {
                        "_id": "6626a471430a124253f197c8",
                        "avatarUrl": "/avatars/f5747fdbe495d1296fed9d16d8c95857.svg",
                        "isPro": false,
                        "fullname": "yl-1993",
                        "user": "yl-1993",
                        "type": "user"
                    },
                    "name": "Lei Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-03T14:40:45.662Z",
                    "hidden": false
                },
                {
                    "_id": "674eadabef38dd7bfb8863d8",
                    "user": {
                        "_id": "62ab1ac1d48b4d8b048a3473",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656826685333-62ab1ac1d48b4d8b048a3473.png",
                        "isPro": false,
                        "fullname": "Ziwei Liu",
                        "user": "liuziwei7",
                        "type": "user"
                    },
                    "name": "Ziwei Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-03T15:20:46.734Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-29T18:53:40.000Z",
            "title": "SOLAMI: Social Vision-Language-Action Modeling for Immersive Interaction\n  with 3D Autonomous Characters",
            "summary": "Human beings are social animals. How to equip 3D autonomous characters with\nsimilar social intelligence that can perceive, understand and interact with\nhumans remains an open yet foundamental problem. In this paper, we introduce\nSOLAMI, the first end-to-end Social vision-Language-Action (VLA) Modeling\nframework for Immersive interaction with 3D autonomous characters.\nSpecifically, SOLAMI builds 3D autonomous characters from three aspects: (1)\nSocial VLA Architecture: We propose a unified social VLA framework to generate\nmultimodal response (speech and motion) based on the user's multimodal input to\ndrive the character for social interaction. (2) Interactive Multimodal Data: We\npresent SynMSI, a synthetic multimodal social interaction dataset generated by\nan automatic pipeline using only existing motion datasets to address the issue\nof data scarcity. (3) Immersive VR Interface: We develop a VR interface that\nenables users to immersively interact with these characters driven by various\narchitectures. Extensive quantitative experiments and user studies demonstrate\nthat our framework leads to more precise and natural character responses (in\nboth speech and motion) that align with user expectations with lower latency.",
            "upvotes": 10,
            "discussionId": "674eadb0ef38dd7bfb886566"
        },
        "publishedAt": "2024-12-03T02:06:02.087Z",
        "title": "SOLAMI: Social Vision-Language-Action Modeling for Immersive Interaction with 3D Autonomous Characters",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/652d06833b5997ed71ce5c46/LDCHfm1pC_AZWIZw1CD_d.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.00174.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "652d06833b5997ed71ce5c46",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/xZTXEcnEogEmBm_ledJQr.jpeg",
            "fullname": "Zhongang Cai",
            "name": "caizhongang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 11
        }
    },
    {
        "paper": {
            "id": "2412.01199",
            "authors": [
                {
                    "_id": "674e94d36e8d5661331c9344",
                    "user": {
                        "_id": "646a1939c37ca1e12308fe81",
                        "avatarUrl": "/avatars/752e9d86018e7d33ad8bcd741203fd86.svg",
                        "isPro": false,
                        "fullname": "Gongfan Fang",
                        "user": "Vinnnf",
                        "type": "user"
                    },
                    "name": "Gongfan Fang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-03T15:22:25.108Z",
                    "hidden": false
                },
                {
                    "_id": "674e94d36e8d5661331c9345",
                    "name": "Kunjun Li",
                    "hidden": false
                },
                {
                    "_id": "674e94d36e8d5661331c9346",
                    "user": {
                        "_id": "64396ebc21221ac7411852b3",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64396ebc21221ac7411852b3/SR0dC8N0bdj9tZFxYPpSf.jpeg",
                        "isPro": false,
                        "fullname": "Xinyin Ma",
                        "user": "horseee",
                        "type": "user"
                    },
                    "name": "Xinyin Ma",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-03T15:22:04.107Z",
                    "hidden": false
                },
                {
                    "_id": "674e94d36e8d5661331c9347",
                    "user": {
                        "_id": "63fc03a50aab060792ffef39",
                        "avatarUrl": "/avatars/9d5b1bb2a41928e08176b703935133ab.svg",
                        "isPro": false,
                        "fullname": "Wangxinchao",
                        "user": "wxcTest",
                        "type": "user"
                    },
                    "name": "Xinchao Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-03T15:21:57.737Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-02T07:05:39.000Z",
            "title": "TinyFusion: Diffusion Transformers Learned Shallow",
            "summary": "Diffusion Transformers have demonstrated remarkable capabilities in image\ngeneration but often come with excessive parameterization, resulting in\nconsiderable inference overhead in real-world applications. In this work, we\npresent TinyFusion, a depth pruning method designed to remove redundant layers\nfrom diffusion transformers via end-to-end learning. The core principle of our\napproach is to create a pruned model with high recoverability, allowing it to\nregain strong performance after fine-tuning. To accomplish this, we introduce a\ndifferentiable sampling technique to make pruning learnable, paired with a\nco-optimized parameter to simulate future fine-tuning. While prior works focus\non minimizing loss or error after pruning, our method explicitly models and\noptimizes the post-fine-tuning performance of pruned models. Experimental\nresults indicate that this learnable paradigm offers substantial benefits for\nlayer pruning of diffusion transformers, surpassing existing importance-based\nand error-based methods. Additionally, TinyFusion exhibits strong\ngeneralization across diverse architectures, such as DiTs, MARs, and SiTs.\nExperiments with DiT-XL show that TinyFusion can craft a shallow diffusion\ntransformer at less than 7% of the pre-training cost, achieving a 2times\nspeedup with an FID score of 2.86, outperforming competitors with comparable\nefficiency. Code is available at https://github.com/VainF/TinyFusion.",
            "upvotes": 10,
            "discussionId": "674e94d56e8d5661331c93a2"
        },
        "publishedAt": "2024-12-03T00:20:02.318Z",
        "title": "TinyFusion: Diffusion Transformers Learned Shallow",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.01199.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64396ebc21221ac7411852b3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64396ebc21221ac7411852b3/SR0dC8N0bdj9tZFxYPpSf.jpeg",
            "fullname": "Xinyin Ma",
            "name": "horseee",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        }
    },
    {
        "paper": {
            "id": "2412.01822",
            "authors": [
                {
                    "_id": "674ed11caeb5b3706525f555",
                    "user": {
                        "_id": "657152eb12f162153b50ec9d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657152eb12f162153b50ec9d/qnldHP35PclV0pDz_05q8.jpeg",
                        "isPro": false,
                        "fullname": "Byung-Kwan Lee",
                        "user": "BK-Lee",
                        "type": "user"
                    },
                    "name": "Byung-Kwan Lee",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-03T09:56:32.986Z",
                    "hidden": false
                },
                {
                    "_id": "674ed11caeb5b3706525f556",
                    "user": {
                        "_id": "65b33e5f7cd0069ad648c4e8",
                        "avatarUrl": "/avatars/1a746ea535cffa92ea08006e05ea414a.svg",
                        "isPro": false,
                        "fullname": "Ryo Hachiuma",
                        "user": "rhachiuma",
                        "type": "user"
                    },
                    "name": "Ryo Hachiuma",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-03T15:21:03.967Z",
                    "hidden": false
                },
                {
                    "_id": "674ed11caeb5b3706525f557",
                    "name": "Yu-Chiang Frank Wang",
                    "hidden": false
                },
                {
                    "_id": "674ed11caeb5b3706525f558",
                    "user": {
                        "_id": "660603529e3555d648e3baf1",
                        "avatarUrl": "/avatars/0f54479afcfc19df00b25d5aedb4cf67.svg",
                        "isPro": false,
                        "fullname": "Yong Man Ro",
                        "user": "dwightro",
                        "type": "user"
                    },
                    "name": "Yong Man Ro",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-03T15:21:17.371Z",
                    "hidden": false
                },
                {
                    "_id": "674ed11caeb5b3706525f559",
                    "name": "Yueh-Hua Wu",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-02T18:58:25.000Z",
            "title": "VLsI: Verbalized Layers-to-Interactions from Large to Small Vision\n  Language Models",
            "summary": "The recent surge in high-quality visual instruction tuning samples from\nclosed-source vision-language models (VLMs) such as GPT-4V has accelerated the\nrelease of open-source VLMs across various model sizes. However, scaling VLMs\nto improve performance using larger models brings significant computational\nchallenges, especially for deployment on resource-constrained devices like\nmobile platforms and robots. To address this, we propose VLsI: Verbalized\nLayers-to-Interactions, a new VLM family in 2B and 7B model sizes, which\nprioritizes efficiency without compromising accuracy. VLsI leverages a unique,\nlayer-wise distillation process, introducing intermediate \"verbalizers\" that\nmap features from each layer to natural language space, allowing smaller VLMs\nto flexibly align with the reasoning processes of larger VLMs. This approach\nmitigates the training instability often encountered in output imitation and\ngoes beyond typical final-layer tuning by aligning the small VLMs' layer-wise\nprogression with that of the large ones. We validate VLsI across ten\nchallenging vision-language benchmarks, achieving notable performance gains\n(11.0% for 2B and 17.4% for 7B) over GPT-4V without the need for model scaling,\nmerging, or architectural changes.",
            "upvotes": 9,
            "discussionId": "674ed11daeb5b3706525f5d0"
        },
        "publishedAt": "2024-12-03T04:36:50.024Z",
        "title": "VLsI: Verbalized Layers-to-Interactions from Large to Small Vision Language Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.01822.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "657152eb12f162153b50ec9d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657152eb12f162153b50ec9d/qnldHP35PclV0pDz_05q8.jpeg",
            "fullname": "Byung-Kwan Lee",
            "name": "BK-Lee",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 50
        }
    },
    {
        "paper": {
            "id": "2412.00568",
            "authors": [
                {
                    "_id": "674f356ea03fd00bab16cdc2",
                    "name": "Ruben Ohana",
                    "hidden": false
                },
                {
                    "_id": "674f356ea03fd00bab16cdc3",
                    "name": "Michael McCabe",
                    "hidden": false
                },
                {
                    "_id": "674f356ea03fd00bab16cdc4",
                    "name": "Lucas Meyer",
                    "hidden": false
                },
                {
                    "_id": "674f356ea03fd00bab16cdc5",
                    "name": "Rudy Morel",
                    "hidden": false
                },
                {
                    "_id": "674f356ea03fd00bab16cdc6",
                    "name": "Fruzsina J. Agocs",
                    "hidden": false
                },
                {
                    "_id": "674f356ea03fd00bab16cdc7",
                    "name": "Miguel Beneitez",
                    "hidden": false
                },
                {
                    "_id": "674f356ea03fd00bab16cdc8",
                    "name": "Marsha Berger",
                    "hidden": false
                },
                {
                    "_id": "674f356ea03fd00bab16cdc9",
                    "name": "Blakesley Burkhart",
                    "hidden": false
                },
                {
                    "_id": "674f356ea03fd00bab16cdca",
                    "name": "Stuart B. Dalziel",
                    "hidden": false
                },
                {
                    "_id": "674f356ea03fd00bab16cdcb",
                    "name": "Drummond B. Fielding",
                    "hidden": false
                },
                {
                    "_id": "674f356ea03fd00bab16cdcc",
                    "name": "Daniel Fortunato",
                    "hidden": false
                },
                {
                    "_id": "674f356ea03fd00bab16cdcd",
                    "name": "Jared A. Goldberg",
                    "hidden": false
                },
                {
                    "_id": "674f356ea03fd00bab16cdce",
                    "name": "Keiya Hirashima",
                    "hidden": false
                },
                {
                    "_id": "674f356ea03fd00bab16cdcf",
                    "name": "Yan-Fei Jiang",
                    "hidden": false
                },
                {
                    "_id": "674f356ea03fd00bab16cdd0",
                    "name": "Rich R. Kerswell",
                    "hidden": false
                },
                {
                    "_id": "674f356ea03fd00bab16cdd1",
                    "name": "Suryanarayana Maddu",
                    "hidden": false
                },
                {
                    "_id": "674f356ea03fd00bab16cdd2",
                    "name": "Jonah Miller",
                    "hidden": false
                },
                {
                    "_id": "674f356ea03fd00bab16cdd3",
                    "name": "Payel Mukhopadhyay",
                    "hidden": false
                },
                {
                    "_id": "674f356ea03fd00bab16cdd4",
                    "name": "Stefan S. Nixon",
                    "hidden": false
                },
                {
                    "_id": "674f356ea03fd00bab16cdd5",
                    "name": "Jeff Shen",
                    "hidden": false
                },
                {
                    "_id": "674f356ea03fd00bab16cdd6",
                    "name": "Romain Watteaux",
                    "hidden": false
                },
                {
                    "_id": "674f356ea03fd00bab16cdd7",
                    "name": "Bruno Régaldo-Saint Blancard",
                    "hidden": false
                },
                {
                    "_id": "674f356ea03fd00bab16cdd8",
                    "name": "François Rozet",
                    "hidden": false
                },
                {
                    "_id": "674f356ea03fd00bab16cdd9",
                    "name": "Liam H. Parker",
                    "hidden": false
                },
                {
                    "_id": "674f356ea03fd00bab16cdda",
                    "name": "Miles Cranmer",
                    "hidden": false
                },
                {
                    "_id": "674f356ea03fd00bab16cddb",
                    "name": "Shirley Ho",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-30T19:42:14.000Z",
            "title": "The Well: a Large-Scale Collection of Diverse Physics Simulations for\n  Machine Learning",
            "summary": "Machine learning based surrogate models offer researchers powerful tools for\naccelerating simulation-based workflows. However, as standard datasets in this\nspace often cover small classes of physical behavior, it can be difficult to\nevaluate the efficacy of new approaches. To address this gap, we introduce the\nWell: a large-scale collection of datasets containing numerical simulations of\na wide variety of spatiotemporal physical systems. The Well draws from domain\nexperts and numerical software developers to provide 15TB of data across 16\ndatasets covering diverse domains such as biological systems, fluid dynamics,\nacoustic scattering, as well as magneto-hydrodynamic simulations of\nextra-galactic fluids or supernova explosions. These datasets can be used\nindividually or as part of a broader benchmark suite. To facilitate usage of\nthe Well, we provide a unified PyTorch interface for training and evaluating\nmodels. We demonstrate the function of this library by introducing example\nbaselines that highlight the new challenges posed by the complex dynamics of\nthe Well. The code and data is available at\nhttps://github.com/PolymathicAI/the_well.",
            "upvotes": 8,
            "discussionId": "674f3570a03fd00bab16ce66"
        },
        "publishedAt": "2024-12-03T11:46:06.868Z",
        "title": "The Well: a Large-Scale Collection of Diverse Physics Simulations for Machine Learning",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/630495adeb6d777a838d27b4/f-17m7Z_1gB7kHabYSn6K.gif"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.00568.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "630495adeb6d777a838d27b4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/630495adeb6d777a838d27b4/qCtQ8t37vYDOsQukDznTZ.jpeg",
            "fullname": "Ruben Ohana",
            "name": "rubenohana",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 3
        }
    },
    {
        "paper": {
            "id": "2412.00100",
            "authors": [
                {
                    "_id": "674e89d1ec384b629bba5e61",
                    "user": {
                        "_id": "622d2ff38d04fd29a9ccf1a7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/622d2ff38d04fd29a9ccf1a7/ORpGrlU8Lm_oSEVftcZEK.jpeg",
                        "isPro": false,
                        "fullname": "Maitreya Patel",
                        "user": "mpatel57",
                        "type": "user"
                    },
                    "name": "Maitreya Patel",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-03T11:14:47.767Z",
                    "hidden": false
                },
                {
                    "_id": "674e89d1ec384b629bba5e62",
                    "name": "Song Wen",
                    "hidden": false
                },
                {
                    "_id": "674e89d1ec384b629bba5e63",
                    "name": "Dimitris N. Metaxas",
                    "hidden": false
                },
                {
                    "_id": "674e89d1ec384b629bba5e64",
                    "name": "Yezhou Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-27T19:04:40.000Z",
            "title": "Steering Rectified Flow Models in the Vector Field for Controlled Image\n  Generation",
            "summary": "Diffusion models (DMs) excel in photorealism, image editing, and solving\ninverse problems, aided by classifier-free guidance and image inversion\ntechniques. However, rectified flow models (RFMs) remain underexplored for\nthese tasks. Existing DM-based methods often require additional training, lack\ngeneralization to pretrained latent models, underperform, and demand\nsignificant computational resources due to extensive backpropagation through\nODE solvers and inversion processes. In this work, we first develop a\ntheoretical and empirical understanding of the vector field dynamics of RFMs in\nefficiently guiding the denoising trajectory. Our findings reveal that we can\nnavigate the vector field in a deterministic and gradient-free manner.\nUtilizing this property, we propose FlowChef, which leverages the vector field\nto steer the denoising trajectory for controlled image generation tasks,\nfacilitated by gradient skipping. FlowChef is a unified framework for\ncontrolled image generation that, for the first time, simultaneously addresses\nclassifier guidance, linear inverse problems, and image editing without the\nneed for extra training, inversion, or intensive backpropagation. Finally, we\nperform extensive evaluations and show that FlowChef significantly outperforms\nbaselines in terms of performance, memory, and time requirements, achieving new\nstate-of-the-art results. Project Page: https://flowchef.github.io.",
            "upvotes": 8,
            "discussionId": "674e89d3ec384b629bba5eee"
        },
        "publishedAt": "2024-12-02T23:35:11.432Z",
        "title": "Steering Rectified Flow Models in the Vector Field for Controlled Image Generation",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/622d2ff38d04fd29a9ccf1a7/Iq85wFgXULr8mWDxLmv7H.qt"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.00100.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "622d2ff38d04fd29a9ccf1a7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/622d2ff38d04fd29a9ccf1a7/ORpGrlU8Lm_oSEVftcZEK.jpeg",
            "fullname": "Maitreya Patel",
            "name": "mpatel57",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 4
        }
    },
    {
        "paper": {
            "id": "2411.18933",
            "authors": [
                {
                    "_id": "674e953b1ff3268f006ab9ea",
                    "user": {
                        "_id": "65304b62e7535baecd85d080",
                        "avatarUrl": "/avatars/6e546c7d1414bd92c5a7c8d8c404de92.svg",
                        "isPro": false,
                        "fullname": "Yunyang Xiong",
                        "user": "yunyangx",
                        "type": "user"
                    },
                    "name": "Yunyang Xiong",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2024-12-03T13:05:39.894Z",
                    "hidden": false
                },
                {
                    "_id": "674e953b1ff3268f006ab9eb",
                    "name": "Chong Zhou",
                    "hidden": false
                },
                {
                    "_id": "674e953b1ff3268f006ab9ec",
                    "user": {
                        "_id": "6350675859bfa9a85d40c28b",
                        "avatarUrl": "/avatars/13fdc2b86f7f8191903bd65e6112acc8.svg",
                        "isPro": false,
                        "fullname": "Xiaoyu Xiang",
                        "user": "mukosame",
                        "type": "user"
                    },
                    "name": "Xiaoyu Xiang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-03T16:15:41.566Z",
                    "hidden": false
                },
                {
                    "_id": "674e953b1ff3268f006ab9ed",
                    "name": "Lemeng Wu",
                    "hidden": false
                },
                {
                    "_id": "674e953b1ff3268f006ab9ee",
                    "user": {
                        "_id": "66f6e80e7db9927533cbd3e1",
                        "avatarUrl": "/avatars/f169a3cb4f48781e38cc111a35741d2d.svg",
                        "isPro": false,
                        "fullname": "Chenchen Zhu",
                        "user": "zcckernel",
                        "type": "user"
                    },
                    "name": "Chenchen Zhu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-03T16:15:27.374Z",
                    "hidden": false
                },
                {
                    "_id": "674e953b1ff3268f006ab9ef",
                    "user": {
                        "_id": "660f893bae89429c07a32cdb",
                        "avatarUrl": "/avatars/27442b1dab58114cfe10220c040c1156.svg",
                        "isPro": false,
                        "fullname": "Zechun Liu",
                        "user": "zechunliu",
                        "type": "user"
                    },
                    "name": "Zechun Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-03T16:15:32.959Z",
                    "hidden": false
                },
                {
                    "_id": "674e953b1ff3268f006ab9f0",
                    "user": {
                        "_id": "645417e98617183806210cfc",
                        "avatarUrl": "/avatars/7a3ba5b0c38878db51da7091fed15f99.svg",
                        "isPro": false,
                        "fullname": "Saksham Suri",
                        "user": "sakshams",
                        "type": "user"
                    },
                    "name": "Saksham Suri",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-03T16:15:50.251Z",
                    "hidden": false
                },
                {
                    "_id": "674e953b1ff3268f006ab9f1",
                    "user": {
                        "_id": "6570032714fa8cfccd39c6ad",
                        "avatarUrl": "/avatars/f4b6140e21db5d18241dcc9b2e94ae33.svg",
                        "isPro": false,
                        "fullname": "Balakrishnan Varadarajan",
                        "user": "balakv",
                        "type": "user"
                    },
                    "name": "Balakrishnan Varadarajan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-03T16:14:54.717Z",
                    "hidden": false
                },
                {
                    "_id": "674e953b1ff3268f006ab9f2",
                    "name": "Ramya Akula",
                    "hidden": false
                },
                {
                    "_id": "674e953b1ff3268f006ab9f3",
                    "user": {
                        "_id": "5f1753f4925b9863e28ad4de",
                        "avatarUrl": "/avatars/e56d25a40f2c0e4e6696b9738877fce8.svg",
                        "isPro": false,
                        "fullname": "Forrest Iandola",
                        "user": "forresti",
                        "type": "user"
                    },
                    "name": "Forrest Iandola",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-03T16:15:07.365Z",
                    "hidden": false
                },
                {
                    "_id": "674e953b1ff3268f006ab9f4",
                    "name": "Raghuraman Krishnamoorthi",
                    "hidden": false
                },
                {
                    "_id": "674e953b1ff3268f006ab9f5",
                    "name": "Bilge Soran",
                    "hidden": false
                },
                {
                    "_id": "674e953b1ff3268f006ab9f6",
                    "user": {
                        "_id": "6566681f7b5ed0735812af32",
                        "avatarUrl": "/avatars/424905abb2973954b0850c592743b6fb.svg",
                        "isPro": false,
                        "fullname": "Vikas Chandra",
                        "user": "vchandra",
                        "type": "user"
                    },
                    "name": "Vikas Chandra",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-03T16:16:17.253Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-28T05:52:10.000Z",
            "title": "Efficient Track Anything",
            "summary": "Segment Anything Model 2 (SAM 2) has emerged as a powerful tool for video\nobject segmentation and tracking anything. Key components of SAM 2 that drive\nthe impressive video object segmentation performance include a large multistage\nimage encoder for frame feature extraction and a memory mechanism that stores\nmemory contexts from past frames to help current frame segmentation. The high\ncomputation complexity of multistage image encoder and memory module has\nlimited its applications in real-world tasks, e.g., video object segmentation\non mobile devices. To address this limitation, we propose EfficientTAMs,\nlightweight track anything models that produce high-quality results with low\nlatency and model size. Our idea is based on revisiting the plain,\nnonhierarchical Vision Transformer (ViT) as an image encoder for video object\nsegmentation, and introducing an efficient memory module, which reduces the\ncomplexity for both frame feature extraction and memory computation for current\nframe segmentation. We take vanilla lightweight ViTs and efficient memory\nmodule to build EfficientTAMs, and train the models on SA-1B and SA-V datasets\nfor video object segmentation and track anything tasks. We evaluate on multiple\nvideo segmentation benchmarks including semi-supervised VOS and promptable\nvideo segmentation, and find that our proposed EfficientTAM with vanilla ViT\nperform comparably to SAM 2 model (HieraB+SAM 2) with ~2x speedup on A100 and\n~2.4x parameter reduction. On segment anything image tasks, our EfficientTAMs\nalso perform favorably over original SAM with ~20x speedup on A100 and ~20x\nparameter reduction. On mobile devices such as iPhone 15 Pro Max, our\nEfficientTAMs can run at ~10 FPS for performing video object segmentation with\nreasonable quality, highlighting the capability of small models for on-device\nvideo object segmentation applications.",
            "upvotes": 7,
            "discussionId": "674e953d1ff3268f006aba9c"
        },
        "publishedAt": "2024-12-03T00:21:14.387Z",
        "title": "Efficient Track Anything",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.18933.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 5287
        }
    },
    {
        "paper": {
            "id": "2412.01064",
            "authors": [
                {
                    "_id": "674e9ca6068c8178f4815e4c",
                    "name": "Taekyung Ki",
                    "hidden": false
                },
                {
                    "_id": "674e9ca6068c8178f4815e4d",
                    "user": {
                        "_id": "663cb73c749d810d11ab7d32",
                        "avatarUrl": "/avatars/c95a9e07e23e16e3f362066f8478908e.svg",
                        "isPro": false,
                        "fullname": "dongchan min",
                        "user": "kevin0417",
                        "type": "user"
                    },
                    "name": "Dongchan Min",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-03T15:24:09.885Z",
                    "hidden": false
                },
                {
                    "_id": "674e9ca6068c8178f4815e4e",
                    "name": "Gyoungsu Chae",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-02T02:50:07.000Z",
            "title": "FLOAT: Generative Motion Latent Flow Matching for Audio-driven Talking\n  Portrait",
            "summary": "With the rapid advancement of diffusion-based generative models, portrait\nimage animation has achieved remarkable results. However, it still faces\nchallenges in temporally consistent video generation and fast sampling due to\nits iterative sampling nature. This paper presents FLOAT, an audio-driven\ntalking portrait video generation method based on flow matching generative\nmodel. We shift the generative modeling from the pixel-based latent space to a\nlearned motion latent space, enabling efficient design of temporally consistent\nmotion. To achieve this, we introduce a transformer-based vector field\npredictor with a simple yet effective frame-wise conditioning mechanism.\nAdditionally, our method supports speech-driven emotion enhancement, enabling a\nnatural incorporation of expressive motions. Extensive experiments demonstrate\nthat our method outperforms state-of-the-art audio-driven talking portrait\nmethods in terms of visual quality, motion fidelity, and efficiency.",
            "upvotes": 6,
            "discussionId": "674e9ca8068c8178f4815ec9"
        },
        "publishedAt": "2024-12-03T00:53:05.450Z",
        "title": "FLOAT: Generative Motion Latent Flow Matching for Audio-driven Talking Portrait",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.01064.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 5287
        }
    },
    {
        "paper": {
            "id": "2412.01316",
            "authors": [
                {
                    "_id": "674e81bbc5490201063a1c71",
                    "user": {
                        "_id": "62f1fcbee2c9b4f83b4a6276",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f1fcbee2c9b4f83b4a6276/Vb648pHEskN7U0rZouYha.jpeg",
                        "isPro": false,
                        "fullname": "Xin Yan",
                        "user": "Cakeyan",
                        "type": "user"
                    },
                    "name": "Xin Yan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-03T11:15:08.535Z",
                    "hidden": false
                },
                {
                    "_id": "674e81bbc5490201063a1c72",
                    "name": "Yuxuan Cai",
                    "hidden": false
                },
                {
                    "_id": "674e81bbc5490201063a1c73",
                    "name": "Qiuyue Wang",
                    "hidden": false
                },
                {
                    "_id": "674e81bbc5490201063a1c74",
                    "name": "Yuan Zhou",
                    "hidden": false
                },
                {
                    "_id": "674e81bbc5490201063a1c75",
                    "user": {
                        "_id": "639005861c5a6237272557d1",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1670382915881-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Huan Yang",
                        "user": "hyang0511",
                        "type": "user"
                    },
                    "name": "Wenhao Huang",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2024-12-03T03:57:51.296Z",
                    "hidden": false
                },
                {
                    "_id": "674e81bbc5490201063a1c76",
                    "name": "Huan Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-02T09:32:36.000Z",
            "title": "Long Video Diffusion Generation with Segmented Cross-Attention and\n  Content-Rich Video Data Curation",
            "summary": "We introduce Presto, a novel video diffusion model designed to generate\n15-second videos with long-range coherence and rich content. Extending video\ngeneration methods to maintain scenario diversity over long durations presents\nsignificant challenges. To address this, we propose a Segmented Cross-Attention\n(SCA) strategy, which splits hidden states into segments along the temporal\ndimension, allowing each segment to cross-attend to a corresponding\nsub-caption. SCA requires no additional parameters, enabling seamless\nincorporation into current DiT-based architectures. To facilitate high-quality\nlong video generation, we build the LongTake-HD dataset, consisting of 261k\ncontent-rich videos with scenario coherence, annotated with an overall video\ncaption and five progressive sub-captions. Experiments show that our Presto\nachieves 78.5% on the VBench Semantic Score and 100% on the Dynamic Degree,\noutperforming existing state-of-the-art video generation methods. This\ndemonstrates that our proposed Presto significantly enhances content richness,\nmaintains long-range coherence, and captures intricate textual details. More\ndetails are displayed on our project page: https://presto-video.github.io/.",
            "upvotes": 6,
            "discussionId": "674e81bfc5490201063a1e12"
        },
        "publishedAt": "2024-12-02T22:58:14.166Z",
        "title": "Long Video Diffusion Generation with Segmented Cross-Attention and Content-Rich Video Data Curation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.01316.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "62f1fcbee2c9b4f83b4a6276",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f1fcbee2c9b4f83b4a6276/Vb648pHEskN7U0rZouYha.jpeg",
            "fullname": "Xin Yan",
            "name": "Cakeyan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        }
    },
    {
        "paper": {
            "id": "2411.17459",
            "authors": [
                {
                    "_id": "674941ae421f757855b3d3d9",
                    "user": {
                        "_id": "64495f0616425b79ffeebb73",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64495f0616425b79ffeebb73/KAwrRqiAiXzV7arOFVQQZ.jpeg",
                        "isPro": false,
                        "fullname": "Zong-jian Li",
                        "user": "ZongjianLi",
                        "type": "user"
                    },
                    "name": "Zongjian Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-03T15:23:21.964Z",
                    "hidden": false
                },
                {
                    "_id": "674941ae421f757855b3d3da",
                    "user": {
                        "_id": "6367a8175bb06007ea099b8f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6367a8175bb06007ea099b8f/IjG7HyWyWRlVt_XwRbxRW.jpeg",
                        "isPro": false,
                        "fullname": "linbin",
                        "user": "LanguageBind",
                        "type": "user"
                    },
                    "name": "Bin Lin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-03T11:16:29.796Z",
                    "hidden": false
                },
                {
                    "_id": "674941ae421f757855b3d3db",
                    "name": "Yang Ye",
                    "hidden": false
                },
                {
                    "_id": "674941ae421f757855b3d3dc",
                    "user": {
                        "_id": "65c0588569429d85dc594a85",
                        "avatarUrl": "/avatars/822203d982519b999247a53235e30c39.svg",
                        "isPro": false,
                        "fullname": "liuhan",
                        "user": "LiuhanChen",
                        "type": "user"
                    },
                    "name": "Liuhan Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-03T15:23:30.042Z",
                    "hidden": false
                },
                {
                    "_id": "674941ae421f757855b3d3dd",
                    "user": {
                        "_id": "645b4026f9d4ec91fdd516ce",
                        "avatarUrl": "/avatars/fb6556ab973764af5c44980f4e8fcc71.svg",
                        "isPro": false,
                        "fullname": "Xinhua Cheng",
                        "user": "cxh0519",
                        "type": "user"
                    },
                    "name": "Xinhua Cheng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-03T15:23:38.006Z",
                    "hidden": false
                },
                {
                    "_id": "674941ae421f757855b3d3de",
                    "user": {
                        "_id": "63468720dd6d90d82ccf3450",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
                        "isPro": false,
                        "fullname": "YSH",
                        "user": "BestWishYsh",
                        "type": "user"
                    },
                    "name": "Shenghai Yuan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-03T11:16:31.845Z",
                    "hidden": false
                },
                {
                    "_id": "674941ae421f757855b3d3df",
                    "user": {
                        "_id": "614030bd8cbdb613b82f36a8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1631596713749-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Li Yuan",
                        "user": "LiYuan",
                        "type": "user"
                    },
                    "name": "Li Yuan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-03T15:23:44.798Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-26T14:23:53.000Z",
            "title": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent\n  Video Diffusion Model",
            "summary": "Video Variational Autoencoder (VAE) encodes videos into a low-dimensional\nlatent space, becoming a key component of most Latent Video Diffusion Models\n(LVDMs) to reduce model training costs. However, as the resolution and duration\nof generated videos increase, the encoding cost of Video VAEs becomes a\nlimiting bottleneck in training LVDMs. Moreover, the block-wise inference\nmethod adopted by most LVDMs can lead to discontinuities of latent space when\nprocessing long-duration videos. The key to addressing the computational\nbottleneck lies in decomposing videos into distinct components and efficiently\nencoding the critical information. Wavelet transform can decompose videos into\nmultiple frequency-domain components and improve the efficiency significantly,\nwe thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages\nmulti-level wavelet transform to facilitate low-frequency energy flow into\nlatent representation. Furthermore, we introduce a method called Causal Cache,\nwhich maintains the integrity of latent space during block-wise inference.\nCompared to state-of-the-art video VAEs, WF-VAE demonstrates superior\nperformance in both PSNR and LPIPS metrics, achieving 2x higher throughput and\n4x lower memory consumption while maintaining competitive reconstruction\nquality. Our code and models are available at\nhttps://github.com/PKU-YuanGroup/WF-VAE.",
            "upvotes": 6,
            "discussionId": "674941af421f757855b3d44a"
        },
        "publishedAt": "2024-12-02T21:47:39.755Z",
        "title": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent Video Diffusion Model",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.17459.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "63468720dd6d90d82ccf3450",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
            "fullname": "YSH",
            "name": "BestWishYsh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 19
        }
    },
    {
        "paper": {
            "id": "2411.19939",
            "authors": [
                {
                    "_id": "674d36af4f75f81717c47767",
                    "user": {
                        "_id": "6372813520a58a5e14c596a3",
                        "avatarUrl": "/avatars/9135151259db3e5b9c8969e1d00c949d.svg",
                        "isPro": false,
                        "fullname": "XuHao Hu",
                        "user": "Foreshhh",
                        "type": "user"
                    },
                    "name": "Xuhao Hu",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2024-12-02T04:27:20.801Z",
                    "hidden": false
                },
                {
                    "_id": "674d36af4f75f81717c47768",
                    "user": {
                        "_id": "657fe7a8504da7f6f30a2832",
                        "avatarUrl": "/avatars/65987e3cba449b5d250616510ee11f33.svg",
                        "isPro": false,
                        "fullname": "Dongrui Liu",
                        "user": "Max9803",
                        "type": "user"
                    },
                    "name": "Dongrui Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-03T11:25:48.997Z",
                    "hidden": false
                },
                {
                    "_id": "674d36af4f75f81717c47769",
                    "user": {
                        "_id": "653f1ef4aabbf15fc76a259c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653f1ef4aabbf15fc76a259c/dZa0CJlk8pXTwJ9dFnNFN.jpeg",
                        "isPro": false,
                        "fullname": "LLLeo Li",
                        "user": "LLLeo612",
                        "type": "user"
                    },
                    "name": "Hao Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-03T11:16:13.054Z",
                    "hidden": false
                },
                {
                    "_id": "674d36af4f75f81717c4776a",
                    "name": "Xuanjing Huang",
                    "hidden": false
                },
                {
                    "_id": "674d36af4f75f81717c4776b",
                    "name": "Jing Shao",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-29T18:56:37.000Z",
            "title": "VLSBench: Unveiling Visual Leakage in Multimodal Safety",
            "summary": "Safety concerns of Multimodal large language models (MLLMs) have gradually\nbecome an important problem in various applications. Surprisingly, previous\nworks indicate a counter-intuitive phenomenon that using textual unlearning to\nalign MLLMs achieves comparable safety performances with MLLMs trained with\nimage-text pairs. To explain such a counter-intuitive phenomenon, we discover a\nvisual safety information leakage (VSIL) problem in existing multimodal safety\nbenchmarks, i.e., the potentially risky and sensitive content in the image has\nbeen revealed in the textual query. In this way, MLLMs can easily refuse these\nsensitive text-image queries according to textual queries. However, image-text\npairs without VSIL are common in real-world scenarios and are overlooked by\nexisting multimodal safety benchmarks. To this end, we construct multimodal\nvisual leakless safety benchmark (VLSBench) preventing visual safety leakage\nfrom image to textual query with 2.4k image-text pairs. Experimental results\nindicate that VLSBench poses a significant challenge to both open-source and\nclose-source MLLMs, including LLaVA, Qwen2-VL, Llama3.2-Vision, and GPT-4o.\nThis study demonstrates that textual alignment is enough for multimodal safety\nscenarios with VSIL, while multimodal alignment is a more promising solution\nfor multimodal safety scenarios without VSIL. Please see our code and data at:\nhttp://hxhcreate.github.io/VLSBench",
            "upvotes": 4,
            "discussionId": "674d36b14f75f81717c477e8"
        },
        "publishedAt": "2024-12-03T02:52:46.387Z",
        "title": "VLSBench: Unveiling Visual Leakage in Multimodal Safety",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.19939.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6372813520a58a5e14c596a3",
            "avatarUrl": "/avatars/9135151259db3e5b9c8969e1d00c949d.svg",
            "fullname": "XuHao Hu",
            "name": "Foreshhh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        }
    },
    {
        "paper": {
            "id": "2412.00947",
            "authors": [
                {
                    "_id": "674e87d63efd09e5cf7de76e",
                    "user": {
                        "_id": "60591896102f61b42f65ae37",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60591896102f61b42f65ae37/PSHcLoEeNnwvzAPOd6-6i.jpeg",
                        "isPro": false,
                        "fullname": "Ryo Kamoi",
                        "user": "ryokamoi",
                        "type": "user"
                    },
                    "name": "Ryo Kamoi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-03T11:15:03.159Z",
                    "hidden": false
                },
                {
                    "_id": "674e87d63efd09e5cf7de76f",
                    "user": {
                        "_id": "6226742d7551f131b4eba59b",
                        "avatarUrl": "/avatars/252f1fb7a82d34df4855241b5c8145ce.svg",
                        "isPro": false,
                        "fullname": "Yusen Zhang",
                        "user": "chatc",
                        "type": "user"
                    },
                    "name": "Yusen Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-03T16:13:40.934Z",
                    "hidden": false
                },
                {
                    "_id": "674e87d63efd09e5cf7de770",
                    "user": {
                        "_id": "63c707ec656e7822e23af13c",
                        "avatarUrl": "/avatars/50d2cfe1e55fe8bd2e0f09e2ca5715ea.svg",
                        "isPro": false,
                        "fullname": "Sarkar Snigdha Sarathi Das",
                        "user": "sarathismg",
                        "type": "user"
                    },
                    "name": "Sarkar Snigdha Sarathi Das",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-03T16:13:46.969Z",
                    "hidden": false
                },
                {
                    "_id": "674e87d63efd09e5cf7de771",
                    "name": "Ranran Haoran Zhang",
                    "hidden": false
                },
                {
                    "_id": "674e87d63efd09e5cf7de772",
                    "name": "Rui Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-01T19:46:22.000Z",
            "title": "VisOnlyQA: Large Vision Language Models Still Struggle with Visual\n  Perception of Geometric Information",
            "summary": "Errors in understanding visual information in images (i.e., visual perception\nerrors) remain a major source of mistakes in Large Vision Language Models\n(LVLMs). While further analysis is essential, there is a deficiency in datasets\nfor evaluating the visual perception of LVLMs. In this work, we introduce\nVisOnlyQA, a new dataset designed to directly evaluate the visual perception\ncapabilities of LVLMs on questions about geometric and numerical information in\nscientific figures. Our dataset enables us to analyze the visual perception of\nLVLMs for fine-grained visual information, independent of other capabilities\nsuch as reasoning. The evaluation set of VisOnlyQA includes 1,200\nmultiple-choice questions in 12 tasks on four categories of figures. We also\nprovide synthetic training data consisting of 70k instances. Our experiments on\nVisOnlyQA highlight the following findings: (i) 20 LVLMs we evaluate, including\nGPT-4o and Gemini 1.5 Pro, work poorly on the visual perception tasks in\nVisOnlyQA, while human performance is nearly perfect. (ii) Fine-tuning on\nsynthetic training data demonstrates the potential for enhancing the visual\nperception of LVLMs, but observed improvements are limited to certain tasks and\nspecific models. (iii) Stronger language models improve the visual perception\nof LVLMs. In summary, our experiments suggest that both training data and model\narchitectures should be improved to enhance the visual perception capabilities\nof LVLMs. The datasets, code, and model responses are provided at\nhttps://github.com/psunlpgroup/VisOnlyQA.",
            "upvotes": 4,
            "discussionId": "674e87d83efd09e5cf7de7c3"
        },
        "publishedAt": "2024-12-02T23:25:24.252Z",
        "title": "VisOnlyQA: Large Vision Language Models Still Struggle with Visual Perception of Geometric Information",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.00947.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "60591896102f61b42f65ae37",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60591896102f61b42f65ae37/PSHcLoEeNnwvzAPOd6-6i.jpeg",
            "fullname": "Ryo Kamoi",
            "name": "ryokamoi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 4
        }
    },
    {
        "paper": {
            "id": "2412.01800",
            "authors": [
                {
                    "_id": "674edab4e8df509dc90ab774",
                    "name": "Meng Cao",
                    "hidden": false
                },
                {
                    "_id": "674edab4e8df509dc90ab775",
                    "name": "Haoran Tang",
                    "hidden": false
                },
                {
                    "_id": "674edab4e8df509dc90ab776",
                    "user": {
                        "_id": "6713de8eb8c4b8d06fc44418",
                        "avatarUrl": "/avatars/7753bd37363632e03180d7c3048b59f7.svg",
                        "isPro": false,
                        "fullname": "Haoze ZHAO",
                        "user": "ZHZ2002",
                        "type": "user"
                    },
                    "name": "Haoze Zhao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-03T16:12:38.116Z",
                    "hidden": false
                },
                {
                    "_id": "674edab4e8df509dc90ab777",
                    "user": {
                        "_id": "646b43deb1202bc77c1024a4",
                        "avatarUrl": "/avatars/cf791574ab986bac274e7fbcf04e2a59.svg",
                        "isPro": false,
                        "fullname": "hangyu guo",
                        "user": "Rosiness",
                        "type": "user"
                    },
                    "name": "Hangyu Guo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-03T16:12:50.388Z",
                    "hidden": false
                },
                {
                    "_id": "674edab4e8df509dc90ab778",
                    "user": {
                        "_id": "65377c30e48353201e6fdda0",
                        "avatarUrl": "/avatars/a8f803b6f2e598eaee9c52c0d2ddfc16.svg",
                        "isPro": false,
                        "fullname": "Jiaheng Liu",
                        "user": "CheeryLJH",
                        "type": "user"
                    },
                    "name": "Jiaheng Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-03T16:12:56.177Z",
                    "hidden": false
                },
                {
                    "_id": "674edab4e8df509dc90ab779",
                    "user": {
                        "_id": "638efcf4c67af472d316d424",
                        "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
                        "isPro": false,
                        "fullname": "Ge Zhang",
                        "user": "zhangysk",
                        "type": "user"
                    },
                    "name": "Ge Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-03T11:14:40.537Z",
                    "hidden": false
                },
                {
                    "_id": "674edab4e8df509dc90ab77a",
                    "user": {
                        "_id": "650f0e3988cdfe73a864b9c5",
                        "avatarUrl": "/avatars/ce74b96743a99819a762c23b4f2204fd.svg",
                        "isPro": false,
                        "fullname": "Ruyang Liu",
                        "user": "farewellthree",
                        "type": "user"
                    },
                    "name": "Ruyang Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-03T16:13:02.101Z",
                    "hidden": false
                },
                {
                    "_id": "674edab4e8df509dc90ab77b",
                    "user": {
                        "_id": "66a850167747a5de8ec191c0",
                        "avatarUrl": "/avatars/c47b1e4ba6299f185887f281c3e24dc6.svg",
                        "isPro": false,
                        "fullname": "Qiang Sun",
                        "user": "sunisrisingnow",
                        "type": "user"
                    },
                    "name": "Qiang Sun",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-03T16:13:10.028Z",
                    "hidden": false
                },
                {
                    "_id": "674edab4e8df509dc90ab77c",
                    "name": "Ian Reid",
                    "hidden": false
                },
                {
                    "_id": "674edab4e8df509dc90ab77d",
                    "name": "Xiaodan Liang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-02T18:47:25.000Z",
            "title": "PhysGame: Uncovering Physical Commonsense Violations in Gameplay Videos",
            "summary": "Recent advancements in video-based large language models (Video LLMs) have\nwitnessed the emergence of diverse capabilities to reason and interpret dynamic\nvisual content. Among them, gameplay videos stand out as a distinctive data\nsource, often containing glitches that defy physics commonsense. This\ncharacteristic renders them an effective benchmark for assessing the\nunder-explored capability of physical commonsense understanding in video LLMs.\nIn this paper, we propose PhysGame as a pioneering benchmark to evaluate\nphysical commonsense violations in gameplay videos. PhysGame comprises 880\nvideos associated with glitches spanning four fundamental domains (i.e.,\nmechanics, kinematics, optics, and material properties) and across 12 distinct\nphysical commonsense. Through extensively evaluating various state-ofthe-art\nvideo LLMs, our findings reveal that the performance of current open-source\nvideo LLMs significantly lags behind that of proprietary counterparts. To\nbridge this gap, we curate an instruction tuning dataset PhysInstruct with\n140,057 question-answering pairs to facilitate physical commonsense learning.\nIn addition, we also propose a preference optimization dataset PhysDPO with\n34,358 training pairs, where the dis-preferred responses are generated\nconditioned on misleading titles (i.e., meta information hacking), fewer frames\n(i.e., temporal hacking) and lower spatial resolutions (i.e., spatial hacking).\nBased on the suite of datasets, we propose PhysVLM as a physical\nknowledge-enhanced video LLM. Extensive experiments on both physical-oriented\nbenchmark PhysGame and general video understanding benchmarks demonstrate the\nstate-ofthe-art performance of PhysVLM.",
            "upvotes": 3,
            "discussionId": "674edab5e8df509dc90ab847"
        },
        "publishedAt": "2024-12-03T05:17:28.519Z",
        "title": "PhysGame: Uncovering Physical Commonsense Violations in Gameplay Videos",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.01800.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "638efcf4c67af472d316d424",
            "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
            "fullname": "Ge Zhang",
            "name": "zhangysk",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 35
        }
    },
    {
        "paper": {
            "id": "2412.01250",
            "authors": [
                {
                    "_id": "674ec96f8b4bb24d65e3e782",
                    "user": {
                        "_id": "63ab07a60ed3c32528492782",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ab07a60ed3c32528492782/gLDYEMu39AEWtpy9el-92.jpeg",
                        "isPro": false,
                        "fullname": "Francesco Taioli",
                        "user": "ftaioli",
                        "type": "user"
                    },
                    "name": "Francesco Taioli",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2024-12-03T09:05:34.387Z",
                    "hidden": false
                },
                {
                    "_id": "674ec96f8b4bb24d65e3e783",
                    "user": {
                        "_id": "6621462611c923d051d62072",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6621462611c923d051d62072/9MEUEZhR-LhrK4XCUck-r.png",
                        "isPro": false,
                        "fullname": "Edoardo",
                        "user": "e-zorzi",
                        "type": "user"
                    },
                    "name": "Edoardo Zorzi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-03T12:00:11.637Z",
                    "hidden": false
                },
                {
                    "_id": "674ec96f8b4bb24d65e3e784",
                    "name": "Gianni Franchi",
                    "hidden": false
                },
                {
                    "_id": "674ec96f8b4bb24d65e3e785",
                    "name": "Alberto Castellini",
                    "hidden": false
                },
                {
                    "_id": "674ec96f8b4bb24d65e3e786",
                    "name": "Alessandro Farinelli",
                    "hidden": false
                },
                {
                    "_id": "674ec96f8b4bb24d65e3e787",
                    "name": "Marco Cristani",
                    "hidden": false
                },
                {
                    "_id": "674ec96f8b4bb24d65e3e788",
                    "name": "Yiming Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-02T08:16:38.000Z",
            "title": "Collaborative Instance Navigation: Leveraging Agent Self-Dialogue to\n  Minimize User Input",
            "summary": "Existing embodied instance goal navigation tasks, driven by natural language,\nassume human users to provide complete and nuanced instance descriptions prior\nto the navigation, which can be impractical in the real world as human\ninstructions might be brief and ambiguous. To bridge this gap, we propose a new\ntask, Collaborative Instance Navigation (CoIN), with dynamic agent-human\ninteraction during navigation to actively resolve uncertainties about the\ntarget instance in natural, template-free, open-ended dialogues. To address\nCoIN, we propose a novel method, Agent-user Interaction with UncerTainty\nAwareness (AIUTA), leveraging the perception capability of Vision Language\nModels (VLMs) and the capability of Large Language Models (LLMs). First, upon\nobject detection, a Self-Questioner model initiates a self-dialogue to obtain a\ncomplete and accurate observation description, while a novel uncertainty\nestimation technique mitigates inaccurate VLM perception. Then, an Interaction\nTrigger module determines whether to ask a question to the user, continue or\nhalt navigation, minimizing user input. For evaluation, we introduce\nCoIN-Bench, a benchmark supporting both real and simulated humans. AIUTA\nachieves competitive performance in instance navigation against\nstate-of-the-art methods, demonstrating great flexibility in handling user\ninputs.",
            "upvotes": 3,
            "discussionId": "674ec9708b4bb24d65e3e7d7"
        },
        "publishedAt": "2024-12-03T04:08:01.157Z",
        "title": "Collaborative Instance Navigation: Leveraging Agent Self-Dialogue to Minimize User Input",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63ab07a60ed3c32528492782/a1dd5XQglzfixms6ZQmww.png",
            "https://cdn-uploads.huggingface.co/production/uploads/63ab07a60ed3c32528492782/2SSrVFaiXZ2liM-LuemEU.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.01250.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "63ab07a60ed3c32528492782",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ab07a60ed3c32528492782/gLDYEMu39AEWtpy9el-92.jpeg",
            "fullname": "Francesco Taioli",
            "name": "ftaioli",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2411.19799",
            "authors": [
                {
                    "_id": "674d75a01e1ca74a41c21237",
                    "user": {
                        "_id": "62a0803e3e7d1dda046d475a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62a0803e3e7d1dda046d475a/Oi1qHSaX7_PU1pS8qaXKv.jpeg",
                        "isPro": false,
                        "fullname": "Angelika Romanou",
                        "user": "angelika",
                        "type": "user"
                    },
                    "name": "Angelika Romanou",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2024-12-02T09:05:32.078Z",
                    "hidden": false
                },
                {
                    "_id": "674d75a01e1ca74a41c21238",
                    "name": "Negar Foroutan",
                    "hidden": false
                },
                {
                    "_id": "674d75a01e1ca74a41c21239",
                    "name": "Anna Sotnikova",
                    "hidden": false
                },
                {
                    "_id": "674d75a01e1ca74a41c2123a",
                    "name": "Zeming Chen",
                    "hidden": false
                },
                {
                    "_id": "674d75a01e1ca74a41c2123b",
                    "name": "Sree Harsha Nelaturu",
                    "hidden": false
                },
                {
                    "_id": "674d75a01e1ca74a41c2123c",
                    "name": "Shivalika Singh",
                    "hidden": false
                },
                {
                    "_id": "674d75a01e1ca74a41c2123d",
                    "name": "Rishabh Maheshwary",
                    "hidden": false
                },
                {
                    "_id": "674d75a01e1ca74a41c2123e",
                    "name": "Micol Altomare",
                    "hidden": false
                },
                {
                    "_id": "674d75a01e1ca74a41c2123f",
                    "name": "Mohamed A. Haggag",
                    "hidden": false
                },
                {
                    "_id": "674d75a01e1ca74a41c21240",
                    "name": "Snegha A",
                    "hidden": false
                },
                {
                    "_id": "674d75a01e1ca74a41c21241",
                    "name": "Alfonso Amayuelas",
                    "hidden": false
                },
                {
                    "_id": "674d75a01e1ca74a41c21242",
                    "name": "Azril Hafizi Amirudin",
                    "hidden": false
                },
                {
                    "_id": "674d75a01e1ca74a41c21243",
                    "name": "Viraat Aryabumi",
                    "hidden": false
                },
                {
                    "_id": "674d75a01e1ca74a41c21244",
                    "name": "Danylo Boiko",
                    "hidden": false
                },
                {
                    "_id": "674d75a01e1ca74a41c21245",
                    "name": "Michael Chang",
                    "hidden": false
                },
                {
                    "_id": "674d75a01e1ca74a41c21246",
                    "name": "Jenny Chim",
                    "hidden": false
                },
                {
                    "_id": "674d75a01e1ca74a41c21247",
                    "name": "Gal Cohen",
                    "hidden": false
                },
                {
                    "_id": "674d75a01e1ca74a41c21248",
                    "name": "Aditya Kumar Dalmia",
                    "hidden": false
                },
                {
                    "_id": "674d75a01e1ca74a41c21249",
                    "name": "Abraham Diress",
                    "hidden": false
                },
                {
                    "_id": "674d75a01e1ca74a41c2124a",
                    "name": "Sharad Duwal",
                    "hidden": false
                },
                {
                    "_id": "674d75a01e1ca74a41c2124b",
                    "name": "Daniil Dzenhaliou",
                    "hidden": false
                },
                {
                    "_id": "674d75a01e1ca74a41c2124c",
                    "name": "Daniel Fernando Erazo Florez",
                    "hidden": false
                },
                {
                    "_id": "674d75a01e1ca74a41c2124d",
                    "name": "Fabian Farestam",
                    "hidden": false
                },
                {
                    "_id": "674d75a01e1ca74a41c2124e",
                    "name": "Joseph Marvin Imperial",
                    "hidden": false
                },
                {
                    "_id": "674d75a01e1ca74a41c2124f",
                    "user": {
                        "_id": "5e4b943a37cb5b49818287b5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5e4b943a37cb5b49818287b5/Sd8QOGFY-_B1z4UlFTuPf.jpeg",
                        "isPro": false,
                        "fullname": "Shayekh Bin Islam",
                        "user": "shayekh",
                        "type": "user"
                    },
                    "name": "Shayekh Bin Islam",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-02T17:31:54.464Z",
                    "hidden": false
                },
                {
                    "_id": "674d75a01e1ca74a41c21250",
                    "name": "Perttu Isotalo",
                    "hidden": false
                },
                {
                    "_id": "674d75a01e1ca74a41c21251",
                    "name": "Maral Jabbarishiviari",
                    "hidden": false
                },
                {
                    "_id": "674d75a01e1ca74a41c21252",
                    "name": "Börje F. Karlsson",
                    "hidden": false
                },
                {
                    "_id": "674d75a01e1ca74a41c21253",
                    "name": "Eldar Khalilov",
                    "hidden": false
                },
                {
                    "_id": "674d75a01e1ca74a41c21254",
                    "name": "Christopher Klamm",
                    "hidden": false
                },
                {
                    "_id": "674d75a01e1ca74a41c21255",
                    "name": "Fajri Koto",
                    "hidden": false
                },
                {
                    "_id": "674d75a01e1ca74a41c21256",
                    "name": "Dominik Krzemiński",
                    "hidden": false
                },
                {
                    "_id": "674d75a01e1ca74a41c21257",
                    "name": "Gabriel Adriano de Melo",
                    "hidden": false
                },
                {
                    "_id": "674d75a01e1ca74a41c21258",
                    "name": "Syrielle Montariol",
                    "hidden": false
                },
                {
                    "_id": "674d75a01e1ca74a41c21259",
                    "name": "Yiyang Nan",
                    "hidden": false
                },
                {
                    "_id": "674d75a01e1ca74a41c2125a",
                    "name": "Joel Niklaus",
                    "hidden": false
                },
                {
                    "_id": "674d75a01e1ca74a41c2125b",
                    "name": "Jekaterina Novikova",
                    "hidden": false
                },
                {
                    "_id": "674d75a01e1ca74a41c2125c",
                    "name": "Johan Samir Obando Ceron",
                    "hidden": false
                },
                {
                    "_id": "674d75a01e1ca74a41c2125d",
                    "name": "Debjit Paul",
                    "hidden": false
                },
                {
                    "_id": "674d75a01e1ca74a41c2125e",
                    "name": "Esther Ploeger",
                    "hidden": false
                },
                {
                    "_id": "674d75a01e1ca74a41c2125f",
                    "user": {
                        "_id": "66c578770a22b2f9ab575847",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c578770a22b2f9ab575847/-zfNho1DR3yZHDazq669-.png",
                        "isPro": false,
                        "fullname": "Jebish Purbey",
                        "user": "jebish7",
                        "type": "user"
                    },
                    "name": "Jebish Purbey",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-03T11:16:06.831Z",
                    "hidden": false
                },
                {
                    "_id": "674d75a01e1ca74a41c21260",
                    "name": "Swati Rajwal",
                    "hidden": false
                },
                {
                    "_id": "674d75a01e1ca74a41c21261",
                    "name": "Selvan Sunitha Ravi",
                    "hidden": false
                },
                {
                    "_id": "674d75a01e1ca74a41c21262",
                    "name": "Sara Rydell",
                    "hidden": false
                },
                {
                    "_id": "674d75a01e1ca74a41c21263",
                    "name": "Roshan Santhosh",
                    "hidden": false
                },
                {
                    "_id": "674d75a01e1ca74a41c21264",
                    "name": "Drishti Sharma",
                    "hidden": false
                },
                {
                    "_id": "674d75a01e1ca74a41c21265",
                    "name": "Marjana Prifti Skenduli",
                    "hidden": false
                },
                {
                    "_id": "674d75a01e1ca74a41c21266",
                    "name": "Arshia Soltani Moakhar",
                    "hidden": false
                },
                {
                    "_id": "674d75a01e1ca74a41c21267",
                    "name": "Bardia Soltani Moakhar",
                    "hidden": false
                },
                {
                    "_id": "674d75a01e1ca74a41c21268",
                    "name": "Ran Tamir",
                    "hidden": false
                },
                {
                    "_id": "674d75a01e1ca74a41c21269",
                    "name": "Ayush Kumar Tarun",
                    "hidden": false
                },
                {
                    "_id": "674d75a01e1ca74a41c2126a",
                    "name": "Azmine Toushik Wasi",
                    "hidden": false
                },
                {
                    "_id": "674d75a01e1ca74a41c2126b",
                    "name": "Thenuka Ovin Weerasinghe",
                    "hidden": false
                },
                {
                    "_id": "674d75a01e1ca74a41c2126c",
                    "name": "Serhan Yilmaz",
                    "hidden": false
                },
                {
                    "_id": "674d75a01e1ca74a41c2126d",
                    "user": {
                        "_id": "60d33fbbd7b174177faabd4f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60d33fbbd7b174177faabd4f/pfyv_xj2B2m2N4F4sT9zJ.jpeg",
                        "isPro": true,
                        "fullname": "Mike Zhang",
                        "user": "jjzha",
                        "type": "user"
                    },
                    "name": "Mike Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-02T08:56:04.648Z",
                    "hidden": false
                },
                {
                    "_id": "674d75a01e1ca74a41c2126e",
                    "name": "Imanol Schlag",
                    "hidden": false
                },
                {
                    "_id": "674d75a01e1ca74a41c2126f",
                    "user": {
                        "_id": "6441042d5d600fb0951a5f99",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6441042d5d600fb0951a5f99/4CbOaYcEz99BtVAQvnGTn.jpeg",
                        "isPro": false,
                        "fullname": "Marzieh Fadaee",
                        "user": "MarziehFadaee",
                        "type": "user"
                    },
                    "name": "Marzieh Fadaee",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-03T14:40:43.732Z",
                    "hidden": false
                },
                {
                    "_id": "674d75a01e1ca74a41c21270",
                    "name": "Sara Hooker",
                    "hidden": false
                },
                {
                    "_id": "674d75a01e1ca74a41c21271",
                    "user": {
                        "_id": "654fb1c67490049d621516fb",
                        "avatarUrl": "/avatars/0f88177da236b1bce4a4a58fd0246884.svg",
                        "isPro": false,
                        "fullname": "Antoine Bosselut",
                        "user": "atcbosselut",
                        "type": "user"
                    },
                    "name": "Antoine Bosselut",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2024-12-02T08:57:36.004Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-29T16:03:14.000Z",
            "title": "INCLUDE: Evaluating Multilingual Language Understanding with Regional\n  Knowledge",
            "summary": "The performance differential of large language models (LLM) between languages\nhinders their effective deployment in many regions, inhibiting the potential\neconomic and societal value of generative AI tools in many communities.\nHowever, the development of functional LLMs in many languages (\\ie,\nmultilingual LLMs) is bottlenecked by the lack of high-quality evaluation\nresources in languages other than English. Moreover, current practices in\nmultilingual benchmark construction often translate English resources, ignoring\nthe regional and cultural knowledge of the environments in which multilingual\nsystems would be used. In this work, we construct an evaluation suite of\n197,243 QA pairs from local exam sources to measure the capabilities of\nmultilingual LLMs in a variety of regional contexts. Our novel resource,\nINCLUDE, is a comprehensive knowledge- and reasoning-centric benchmark across\n44 written languages that evaluates multilingual LLMs for performance in the\nactual language environments where they would be deployed.",
            "upvotes": 3,
            "discussionId": "674d75a21e1ca74a41c212ea"
        },
        "publishedAt": "2024-12-03T02:33:52.997Z",
        "title": "INCLUDE: Evaluating Multilingual Language Understanding with Regional Knowledge",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.19799.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "654fb1c67490049d621516fb",
            "avatarUrl": "/avatars/0f88177da236b1bce4a4a58fd0246884.svg",
            "fullname": "Antoine Bosselut",
            "name": "atcbosselut",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 5
        }
    },
    {
        "paper": {
            "id": "2412.00176",
            "authors": [
                {
                    "_id": "674ea7dcb311aeec3f329ae5",
                    "user": {
                        "_id": "65c30a5ffc592f2e565ea69f",
                        "avatarUrl": "/avatars/5b7b42fb96cc88dbeca530fedf3c2571.svg",
                        "isPro": false,
                        "fullname": "Hui Ren",
                        "user": "rhfeiyang",
                        "type": "user"
                    },
                    "name": "Hui Ren",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-03T09:56:40.473Z",
                    "hidden": false
                },
                {
                    "_id": "674ea7dcb311aeec3f329ae6",
                    "user": {
                        "_id": "60ef074b89938e400b470da8",
                        "avatarUrl": "/avatars/c914070df46fb2dce9067d0eafc88238.svg",
                        "isPro": false,
                        "fullname": "Joanna Materzynska",
                        "user": "jomat",
                        "type": "user"
                    },
                    "name": "Joanna Materzynska",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-03T15:25:28.758Z",
                    "hidden": false
                },
                {
                    "_id": "674ea7dcb311aeec3f329ae7",
                    "user": {
                        "_id": "636daf1b56c0762cfda074b5",
                        "avatarUrl": "/avatars/f44be5eb110acfa2efbd09de6b416239.svg",
                        "isPro": false,
                        "fullname": "Rohit Gandikota",
                        "user": "RohitGandikota",
                        "type": "user"
                    },
                    "name": "Rohit Gandikota",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-03T16:04:26.266Z",
                    "hidden": false
                },
                {
                    "_id": "674ea7dcb311aeec3f329ae8",
                    "user": {
                        "_id": "6214d6c01e35c843d42d1f77",
                        "avatarUrl": "/avatars/ac208cd180b4f3ed1ec367e581facfcf.svg",
                        "isPro": false,
                        "fullname": "David Bau",
                        "user": "davidbau",
                        "type": "user"
                    },
                    "name": "David Bau",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-03T16:04:19.664Z",
                    "hidden": false
                },
                {
                    "_id": "674ea7dcb311aeec3f329ae9",
                    "user": {
                        "_id": "6744c6ec6ec99a37d4ba9235",
                        "avatarUrl": "/avatars/a5384b63bb615192f6fa157c6ea89e92.svg",
                        "isPro": false,
                        "fullname": "Antonio",
                        "user": "Antoniotorralbaborruel",
                        "type": "user"
                    },
                    "name": "Antonio Torralba",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-03T16:04:13.407Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-29T18:59:01.000Z",
            "title": "Art-Free Generative Models: Art Creation Without Graphic Art Knowledge",
            "summary": "We explore the question: \"How much prior art knowledge is needed to create\nart?\" To investigate this, we propose a text-to-image generation model trained\nwithout access to art-related content. We then introduce a simple yet effective\nmethod to learn an art adapter using only a few examples of selected artistic\nstyles. Our experiments show that art generated using our method is perceived\nby users as comparable to art produced by models trained on large, art-rich\ndatasets. Finally, through data attribution techniques, we illustrate how\nexamples from both artistic and non-artistic datasets contributed to the\ncreation of new artistic styles.",
            "upvotes": 2,
            "discussionId": "674ea7e1b311aeec3f329bf2"
        },
        "publishedAt": "2024-12-03T09:35:37.693Z",
        "title": "Art-Free Generative Models: Art Creation Without Graphic Art Knowledge",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.00176.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60ef074b89938e400b470da8",
            "avatarUrl": "/avatars/c914070df46fb2dce9067d0eafc88238.svg",
            "fullname": "Joanna Materzynska",
            "name": "jomat",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 2
        }
    },
    {
        "paper": {
            "id": "2411.19477",
            "authors": [
                {
                    "_id": "674d483a1c5984922be0ef47",
                    "user": {
                        "_id": "6576f9f4654561a1b345610b",
                        "avatarUrl": "/avatars/f801f551640caa70368fcc26a0f51d27.svg",
                        "isPro": false,
                        "fullname": "Yanxi Chen",
                        "user": "yanxi-chen",
                        "type": "user"
                    },
                    "name": "Yanxi Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-03T11:16:09.629Z",
                    "hidden": false
                },
                {
                    "_id": "674d483a1c5984922be0ef48",
                    "user": {
                        "_id": "64101b4ca92fedb0e8559b91",
                        "avatarUrl": "/avatars/c7453a5641a94d04a75de9b7d4272bc2.svg",
                        "isPro": false,
                        "fullname": "Xuchen Pan",
                        "user": "panxuchen",
                        "type": "user"
                    },
                    "name": "Xuchen Pan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-03T15:25:01.972Z",
                    "hidden": false
                },
                {
                    "_id": "674d483a1c5984922be0ef49",
                    "name": "Yaliang Li",
                    "hidden": false
                },
                {
                    "_id": "674d483a1c5984922be0ef4a",
                    "name": "Bolin Ding",
                    "hidden": false
                },
                {
                    "_id": "674d483a1c5984922be0ef4b",
                    "user": {
                        "_id": "602f88f5e8149a962412a667",
                        "avatarUrl": "/avatars/b78f0e583df8e5d5e3365934fe5f4900.svg",
                        "isPro": false,
                        "fullname": "Zhou",
                        "user": "Jingren",
                        "type": "user"
                    },
                    "name": "Jingren Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-03T15:24:55.377Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-29T05:29:47.000Z",
            "title": "A Simple and Provable Scaling Law for the Test-Time Compute of Large\n  Language Models",
            "summary": "We propose a general two-stage algorithm that enjoys a provable scaling law\nfor the test-time compute of large language models (LLMs). Given an input\nproblem, the proposed algorithm first generates N candidate solutions, and\nthen chooses the best one via a multiple-round knockout tournament where each\npair of candidates are compared for K times and only the winners move on to\nthe next round. In a minimalistic implementation, both stages can be executed\nwith a black-box LLM alone and nothing else (e.g., no external verifier or\nreward model), and a total of N times (K + 1) highly parallelizable LLM\ncalls are needed for solving an input problem. Assuming that a generated\ncandidate solution is correct with probability p_{gen} > 0 and a\ncomparison between a pair of correct and incorrect solutions identifies the\nright winner with probability p_{comp} > 0.5 (i.e., better than a\nrandom guess), we prove theoretically that the failure probability of the\nproposed algorithm decays to zero exponentially with respect to N and K:\n$P(final output is incorrect) le (1 - p_{gen})^N +\nlceil log_2 N rceil e^{-2 K (p_{comp} - 0.5)^2}.$ Our empirical\nresults with the challenging MMLU-Pro benchmark validate the technical\nassumptions, as well as the efficacy of the proposed algorithm and the gains\nfrom scaling up its test-time compute.",
            "upvotes": 1,
            "discussionId": "674d483b1c5984922be0ef6d"
        },
        "publishedAt": "2024-12-03T00:46:03.917Z",
        "title": "A Simple and Provable Scaling Law for the Test-Time Compute of Large Language Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.19477.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6576f9f4654561a1b345610b",
            "avatarUrl": "/avatars/f801f551640caa70368fcc26a0f51d27.svg",
            "fullname": "Yanxi Chen",
            "name": "yanxi-chen",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2412.01821",
            "authors": [
                {
                    "_id": "674f26532836633b518dd279",
                    "user": {
                        "_id": "64da979f3a7ab21ea7d499c3",
                        "avatarUrl": "/avatars/84fd959092e5a5d3b29720f6f7464b25.svg",
                        "isPro": false,
                        "fullname": "Zhang",
                        "user": "QihangZhang",
                        "type": "user"
                    },
                    "name": "Qihang Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-03T16:05:47.586Z",
                    "hidden": false
                },
                {
                    "_id": "674f26532836633b518dd27a",
                    "name": "Shuangfei Zhai",
                    "hidden": false
                },
                {
                    "_id": "674f26532836633b518dd27b",
                    "user": {
                        "_id": "65dd58aa29b761fff4bd3a4c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/SD7Vh0i5HwJJsQxXpSTFa.jpeg",
                        "isPro": false,
                        "fullname": "Miguel Angel Bautista",
                        "user": "FlippyCode",
                        "type": "user"
                    },
                    "name": "Miguel Angel Bautista",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-03T16:05:28.650Z",
                    "hidden": false
                },
                {
                    "_id": "674f26532836633b518dd27c",
                    "name": "Kevin Miao",
                    "hidden": false
                },
                {
                    "_id": "674f26532836633b518dd27d",
                    "user": {
                        "_id": "6541636c4f939214d3a0c742",
                        "avatarUrl": "/avatars/c9507d6ec2b81681461837c8261703c7.svg",
                        "isPro": false,
                        "fullname": "Alexander Toshev",
                        "user": "toshev",
                        "type": "user"
                    },
                    "name": "Alexander Toshev",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-03T16:04:58.423Z",
                    "hidden": false
                },
                {
                    "_id": "674f26532836633b518dd27e",
                    "name": "Joshua Susskind",
                    "hidden": false
                },
                {
                    "_id": "674f26532836633b518dd27f",
                    "user": {
                        "_id": "6164e72d73996c363c52e66d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1634002684894-noauth.png",
                        "isPro": false,
                        "fullname": "Jiatao Gu",
                        "user": "thomagram",
                        "type": "user"
                    },
                    "name": "Jiatao Gu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-03T16:04:43.809Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-02T18:58:23.000Z",
            "title": "World-consistent Video Diffusion with Explicit 3D Modeling",
            "summary": "Recent advancements in diffusion models have set new benchmarks in image and\nvideo generation, enabling realistic visual synthesis across single- and\nmulti-frame contexts. However, these models still struggle with efficiently and\nexplicitly generating 3D-consistent content. To address this, we propose\nWorld-consistent Video Diffusion (WVD), a novel framework that incorporates\nexplicit 3D supervision using XYZ images, which encode global 3D coordinates\nfor each image pixel. More specifically, we train a diffusion transformer to\nlearn the joint distribution of RGB and XYZ frames. This approach supports\nmulti-task adaptability via a flexible inpainting strategy. For example, WVD\ncan estimate XYZ frames from ground-truth RGB or generate novel RGB frames\nusing XYZ projections along a specified camera trajectory. In doing so, WVD\nunifies tasks like single-image-to-3D generation, multi-view stereo, and\ncamera-controlled video generation. Our approach demonstrates competitive\nperformance across multiple benchmarks, providing a scalable solution for\n3D-consistent video and image generation with a single pretrained model.",
            "upvotes": 0,
            "discussionId": "674f26562836633b518dd339"
        },
        "publishedAt": "2024-12-03T10:40:24.296Z",
        "title": "World-consistent Video Diffusion with Explicit 3D Modeling",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.01821.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "63047acc41387c7f11756284",
            "avatarUrl": "/avatars/a184fca1ee2d98b81e1917aecc3cdca7.svg",
            "fullname": "i3h",
            "name": "qihang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2412.01408",
            "authors": [
                {
                    "_id": "674ef0ea044f82a94169d30d",
                    "user": {
                        "_id": "62d923892010d671bc9e67cc",
                        "avatarUrl": "/avatars/6b3d7c741755615f337aa909d1a53d55.svg",
                        "isPro": false,
                        "fullname": "Aditya Narayan Sankaran",
                        "user": "callmesan",
                        "type": "user"
                    },
                    "name": "Aditya Narayan Sankaran",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-03T12:00:13.468Z",
                    "hidden": false
                },
                {
                    "_id": "674ef0ea044f82a94169d30e",
                    "name": "Reza Farahbaksh",
                    "hidden": false
                },
                {
                    "_id": "674ef0ea044f82a94169d30f",
                    "name": "Noel Crespi",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-02T11:51:19.000Z",
            "title": "Towards Cross-Lingual Audio Abuse Detection in Low-Resource Settings\n  with Few-Shot Learning",
            "summary": "Online abusive content detection, particularly in low-resource settings and\nwithin the audio modality, remains underexplored. We investigate the potential\nof pre-trained audio representations for detecting abusive language in\nlow-resource languages, in this case, in Indian languages using Few Shot\nLearning (FSL). Leveraging powerful representations from models such as Wav2Vec\nand Whisper, we explore cross-lingual abuse detection using the ADIMA dataset\nwith FSL. Our approach integrates these representations within the\nModel-Agnostic Meta-Learning (MAML) framework to classify abusive language in\n10 languages. We experiment with various shot sizes (50-200) evaluating the\nimpact of limited data on performance. Additionally, a feature visualization\nstudy was conducted to better understand model behaviour. This study highlights\nthe generalization ability of pre-trained models in low-resource scenarios and\noffers valuable insights into detecting abusive language in multilingual\ncontexts.",
            "upvotes": 0,
            "discussionId": "674ef0eb044f82a94169d35f"
        },
        "publishedAt": "2024-12-03T06:52:47.644Z",
        "title": "Towards Cross-Lingual Audio Abuse Detection in Low-Resource Settings with Few-Shot Learning",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.01408.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "62d923892010d671bc9e67cc",
            "avatarUrl": "/avatars/6b3d7c741755615f337aa909d1a53d55.svg",
            "fullname": "Aditya Narayan Sankaran",
            "name": "callmesan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        }
    }
]