[
  {
    "paper": {
      "id": "2510.05096",
      "authors": [
        {
          "_id": "68e479bfe4e093a7044e4d04",
          "name": "Zeyu Zhu",
          "hidden": false
        },
        {
          "_id": "68e479bfe4e093a7044e4d05",
          "name": "Kevin Qinghong Lin",
          "hidden": false
        },
        {
          "_id": "68e479bfe4e093a7044e4d06",
          "name": "Mike Zheng Shou",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64440be5af034cdfd69ca3a7/7UBfF2Dt4zzFl8oAQsBrY.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/64440be5af034cdfd69ca3a7/uG-WApKKWLDEPeMdC9z1I.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/64440be5af034cdfd69ca3a7/M6UUqPDcw_dHIPQp4DelE.mp4"
      ],
      "publishedAt": "2025-10-06T17:58:02.000Z",
      "submittedOnDailyAt": "2025-10-07T01:03:40.404Z",
      "title": "Paper2Video: Automatic Video Generation from Scientific Papers",
      "submittedOnDailyBy": {
        "_id": "64440be5af034cdfd69ca3a7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg",
        "isPro": false,
        "fullname": "Qinghong (Kevin) Lin",
        "user": "KevinQHLin",
        "type": "user"
      },
      "summary": "Academic presentation videos have become an essential medium for research\ncommunication, yet producing them remains highly labor-intensive, often\nrequiring hours of slide design, recording, and editing for a short 2 to 10\nminutes video. Unlike natural video, presentation video generation involves\ndistinctive challenges: inputs from research papers, dense multi-modal\ninformation (text, figures, tables), and the need to coordinate multiple\naligned channels such as slides, subtitles, speech, and human talker. To\naddress these challenges, we introduce PaperTalker, the first benchmark of 101\nresearch papers paired with author-created presentation videos, slides, and\nspeaker metadata. We further design four tailored evaluation metrics--Meta\nSimilarity, PresentArena, PresentQuiz, and IP Memory--to measure how videos\nconvey the paper's information to the audience. Building on this foundation, we\npropose PaperTalker, the first multi-agent framework for academic presentation\nvideo generation. It integrates slide generation with effective layout\nrefinement by a novel effective tree search visual choice, cursor grounding,\nsubtitling, speech synthesis, and talking-head rendering, while parallelizing\nslide-wise generation for efficiency. Experiments on Paper2Video demonstrate\nthat the presentation videos produced by our approach are more faithful and\ninformative than existing baselines, establishing a practical step toward\nautomated and ready-to-use academic video generation. Our dataset, agent, and\ncode are available at https://github.com/showlab/Paper2Video.",
      "upvotes": 30,
      "discussionId": "68e479c0e4e093a7044e4d07",
      "projectPage": "https://showlab.github.io/Paper2Video/",
      "githubRepo": "https://github.com/showlab/Paper2Video",
      "ai_summary": "PaperTalker is a multi-agent framework that automates academic presentation video generation by integrating slide generation, layout refinement, subtitling, speech synthesis, and talking-head rendering, outperforming existing methods.",
      "ai_keywords": [
        "multi-agent framework",
        "slide generation",
        "layout refinement",
        "tree search visual choice",
        "cursor grounding",
        "subtitling",
        "speech synthesis",
        "talking-head rendering",
        "parallelization",
        "slide-wise generation"
      ],
      "githubStars": 28,
      "organization": {
        "_id": "63a553c4ce5763e06f78669c",
        "name": "showlab",
        "fullname": "Show Lab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1671779505215-63a55320ce5763e06f78519c.png"
      }
    },
    "publishedAt": "2025-10-06T13:58:02.000Z",
    "title": "Paper2Video: Automatic Video Generation from Scientific Papers",
    "summary": "Academic presentation videos have become an essential medium for research\ncommunication, yet producing them remains highly labor-intensive, often\nrequiring hours of slide design, recording, and editing for a short 2 to 10\nminutes video. Unlike natural video, presentation video generation involves\ndistinctive challenges: inputs from research papers, dense multi-modal\ninformation (text, figures, tables), and the need to coordinate multiple\naligned channels such as slides, subtitles, speech, and human talker. To\naddress these challenges, we introduce PaperTalker, the first benchmark of 101\nresearch papers paired with author-created presentation videos, slides, and\nspeaker metadata. We further design four tailored evaluation metrics--Meta\nSimilarity, PresentArena, PresentQuiz, and IP Memory--to measure how videos\nconvey the paper's information to the audience. Building on this foundation, we\npropose PaperTalker, the first multi-agent framework for academic presentation\nvideo generation. It integrates slide generation with effective layout\nrefinement by a novel effective tree search visual choice, cursor grounding,\nsubtitling, speech synthesis, and talking-head rendering, while parallelizing\nslide-wise generation for efficiency. Experiments on Paper2Video demonstrate\nthat the presentation videos produced by our approach are more faithful and\ninformative than existing baselines, establishing a practical step toward\nautomated and ready-to-use academic video generation. Our dataset, agent, and\ncode are available at https://github.com/showlab/Paper2Video.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64440be5af034cdfd69ca3a7/7UBfF2Dt4zzFl8oAQsBrY.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/64440be5af034cdfd69ca3a7/uG-WApKKWLDEPeMdC9z1I.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/64440be5af034cdfd69ca3a7/M6UUqPDcw_dHIPQp4DelE.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.05096.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64440be5af034cdfd69ca3a7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg",
      "fullname": "Qinghong (Kevin) Lin",
      "name": "KevinQHLin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 35
    },
    "organization": {
      "_id": "63a553c4ce5763e06f78669c",
      "name": "showlab",
      "fullname": "Show Lab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1671779505215-63a55320ce5763e06f78519c.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.05034",
      "authors": [
        {
          "_id": "68e47fd5e4e093a7044e4d3c",
          "name": "Yunlong Tang",
          "hidden": false
        },
        {
          "_id": "68e47fd5e4e093a7044e4d3d",
          "name": "Jing Bi",
          "hidden": false
        },
        {
          "_id": "68e47fd5e4e093a7044e4d3e",
          "name": "Pinxin Liu",
          "hidden": false
        },
        {
          "_id": "68e47fd5e4e093a7044e4d3f",
          "name": "Zhenyu Pan",
          "hidden": false
        },
        {
          "_id": "68e47fd5e4e093a7044e4d40",
          "name": "Zhangyun Tan",
          "hidden": false
        },
        {
          "_id": "68e47fd5e4e093a7044e4d41",
          "name": "Qianxiang Shen",
          "hidden": false
        },
        {
          "_id": "68e47fd5e4e093a7044e4d42",
          "name": "Jiani Liu",
          "hidden": false
        },
        {
          "_id": "68e47fd5e4e093a7044e4d43",
          "name": "Hang Hua",
          "hidden": false
        },
        {
          "_id": "68e47fd5e4e093a7044e4d44",
          "name": "Junjia Guo",
          "hidden": false
        },
        {
          "_id": "68e47fd5e4e093a7044e4d45",
          "name": "Yunzhong Xiao",
          "hidden": false
        },
        {
          "_id": "68e47fd5e4e093a7044e4d46",
          "name": "Chao Huang",
          "hidden": false
        },
        {
          "_id": "68e47fd5e4e093a7044e4d47",
          "name": "Zhiyuan Wang",
          "hidden": false
        },
        {
          "_id": "68e47fd5e4e093a7044e4d48",
          "name": "Susan Liang",
          "hidden": false
        },
        {
          "_id": "68e47fd5e4e093a7044e4d49",
          "name": "Xinyi Liu",
          "hidden": false
        },
        {
          "_id": "68e47fd5e4e093a7044e4d4a",
          "name": "Yizhi Song",
          "hidden": false
        },
        {
          "_id": "68e47fd5e4e093a7044e4d4b",
          "name": "Yuhe Nie",
          "hidden": false
        },
        {
          "_id": "68e47fd5e4e093a7044e4d4c",
          "name": "Jia-Xing Zhong",
          "hidden": false
        },
        {
          "_id": "68e47fd5e4e093a7044e4d4d",
          "name": "Bozheng Li",
          "hidden": false
        },
        {
          "_id": "68e47fd5e4e093a7044e4d4e",
          "name": "Daiqing Qi",
          "hidden": false
        },
        {
          "_id": "68e47fd5e4e093a7044e4d4f",
          "name": "Ziyun Zeng",
          "hidden": false
        },
        {
          "_id": "68e47fd5e4e093a7044e4d50",
          "name": "Ali Vosoughi",
          "hidden": false
        },
        {
          "_id": "68e47fd5e4e093a7044e4d51",
          "name": "Luchuan Song",
          "hidden": false
        },
        {
          "_id": "68e47fd5e4e093a7044e4d52",
          "name": "Zeliang Zhang",
          "hidden": false
        },
        {
          "_id": "68e47fd5e4e093a7044e4d53",
          "name": "Daiki Shimada",
          "hidden": false
        },
        {
          "_id": "68e47fd5e4e093a7044e4d54",
          "name": "Han Liu",
          "hidden": false
        },
        {
          "_id": "68e47fd5e4e093a7044e4d55",
          "name": "Jiebo Luo",
          "hidden": false
        },
        {
          "_id": "68e47fd5e4e093a7044e4d56",
          "name": "Chenliang Xu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-06T17:10:44.000Z",
      "submittedOnDailyAt": "2025-10-07T01:20:07.507Z",
      "title": "Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large\n  Multimodal Models",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Video understanding represents the most challenging frontier in computer\nvision, requiring models to reason about complex spatiotemporal relationships,\nlong-term dependencies, and multimodal evidence. The recent emergence of\nVideo-Large Multimodal Models (Video-LMMs), which integrate visual encoders\nwith powerful decoder-based language models, has demonstrated remarkable\ncapabilities in video understanding tasks. However, the critical phase that\ntransforms these models from basic perception systems into sophisticated\nreasoning engines, post-training, remains fragmented across the literature.\nThis survey provides the first comprehensive examination of post-training\nmethodologies for Video-LMMs, encompassing three fundamental pillars:\nsupervised fine-tuning (SFT) with chain-of-thought, reinforcement learning (RL)\nfrom verifiable objectives, and test-time scaling (TTS) through enhanced\ninference computation. We present a structured taxonomy that clarifies the\nroles, interconnections, and video-specific adaptations of these techniques,\naddressing unique challenges such as temporal localization, spatiotemporal\ngrounding, long video efficiency, and multimodal evidence integration. Through\nsystematic analysis of representative methods, we synthesize key design\nprinciples, insights, and evaluation protocols while identifying critical open\nchallenges in reward design, scalability, and cost-performance optimization. We\nfurther curate essential benchmarks, datasets, and metrics to facilitate\nrigorous assessment of post-training effectiveness. This survey aims to provide\nresearchers and practitioners with a unified framework for advancing Video-LMM\ncapabilities. Additional resources and updates are maintained at:\nhttps://github.com/yunlong10/Awesome-Video-LMM-Post-Training",
      "upvotes": 19,
      "discussionId": "68e47fd5e4e093a7044e4d57",
      "githubRepo": "https://github.com/yunlong10/Awesome-Video-LMM-Post-Training",
      "ai_summary": "This survey examines post-training methodologies for Video-LMMs, focusing on supervised fine-tuning, reinforcement learning, and test-time scaling, while addressing challenges in video understanding.",
      "ai_keywords": [
        "Video-LMMs",
        "supervised fine-tuning",
        "reinforcement learning",
        "test-time scaling",
        "temporal localization",
        "spatiotemporal grounding",
        "long video efficiency",
        "multimodal evidence integration",
        "reward design",
        "scalability",
        "cost-performance optimization"
      ],
      "githubStars": 23
    },
    "publishedAt": "2025-10-06T13:10:44.000Z",
    "title": "Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large\n  Multimodal Models",
    "summary": "Video understanding represents the most challenging frontier in computer\nvision, requiring models to reason about complex spatiotemporal relationships,\nlong-term dependencies, and multimodal evidence. The recent emergence of\nVideo-Large Multimodal Models (Video-LMMs), which integrate visual encoders\nwith powerful decoder-based language models, has demonstrated remarkable\ncapabilities in video understanding tasks. However, the critical phase that\ntransforms these models from basic perception systems into sophisticated\nreasoning engines, post-training, remains fragmented across the literature.\nThis survey provides the first comprehensive examination of post-training\nmethodologies for Video-LMMs, encompassing three fundamental pillars:\nsupervised fine-tuning (SFT) with chain-of-thought, reinforcement learning (RL)\nfrom verifiable objectives, and test-time scaling (TTS) through enhanced\ninference computation. We present a structured taxonomy that clarifies the\nroles, interconnections, and video-specific adaptations of these techniques,\naddressing unique challenges such as temporal localization, spatiotemporal\ngrounding, long video efficiency, and multimodal evidence integration. Through\nsystematic analysis of representative methods, we synthesize key design\nprinciples, insights, and evaluation protocols while identifying critical open\nchallenges in reward design, scalability, and cost-performance optimization. We\nfurther curate essential benchmarks, datasets, and metrics to facilitate\nrigorous assessment of post-training effectiveness. This survey aims to provide\nresearchers and practitioners with a unified framework for advancing Video-LMM\ncapabilities. Additional resources and updates are maintained at:\nhttps://github.com/yunlong10/Awesome-Video-LMM-Post-Training",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.05034.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 119
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.03632",
      "authors": [
        {
          "_id": "68e48b8be4e093a7044e4d97",
          "name": "Jiaxi Li",
          "hidden": false
        },
        {
          "_id": "68e48b8be4e093a7044e4d98",
          "name": "Yucheng Shi",
          "hidden": false
        },
        {
          "_id": "68e48b8be4e093a7044e4d99",
          "name": "Jin Lu",
          "hidden": false
        },
        {
          "_id": "68e48b8be4e093a7044e4d9a",
          "name": "Ninghao Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-04T02:30:40.000Z",
      "submittedOnDailyAt": "2025-10-07T02:19:10.215Z",
      "title": "MITS: Enhanced Tree Search Reasoning for LLMs via Pointwise Mutual\n  Information",
      "submittedOnDailyBy": {
        "_id": "64beb6b6140491ca9f803ebf",
        "avatarUrl": "/avatars/0daa2e813a13668b8b708cd8c12763d9.svg",
        "isPro": false,
        "fullname": "Yucheng SHi",
        "user": "YuchengShi",
        "type": "user"
      },
      "summary": "Tree search has become as a representative framework for test-time reasoning\nwith large language models (LLMs), exemplified by methods such as\nTree-of-Thought and Monte Carlo Tree Search that explore multiple reasoning\npaths. However, it remains difficult to provide instant and reliable\nquantitative assessments of intermediate reasoning step quality, and extensive\npath exploration is computationally costly. To address this, we propose Mutual\nInformation Tree Search (MITS), a novel framework that guides reasoning with\ninformation-theoretic principles. MITS introduces an effective scoring function\nbased on pointwise mutual information (PMI), which enables step-wise evaluation\nof reasoning paths and search tree expansion via beam search without expensive\nlook-ahead simulations, achieving superior reasoning performances while\nmaintaining computational efficiency. The framework is complemented by an\nentropy-based dynamic sampling strategy that adaptively allocates computational\nresources to uncertain reasoning steps where exploration is most beneficial.\nFor final prediction, MITS employs a weighted voting scheme that combines PMI\nscores with prediction consensus. Through comprehensive experiments on diverse\nreasoning benchmarks, MITS consistently surpasses baseline methods,\nestablishing a principled and efficient framework for LLM reasoning.",
      "upvotes": 17,
      "discussionId": "68e48b8ce4e093a7044e4d9b",
      "ai_summary": "Mutual Information Tree Search (MITS) uses information-theoretic principles to guide and evaluate reasoning paths in large language models, improving performance and efficiency.",
      "ai_keywords": [
        "Mutual Information Tree Search",
        "MITS",
        "pointwise mutual information",
        "PMI",
        "beam search",
        "entropy-based dynamic sampling",
        "weighted voting scheme",
        "reasoning paths",
        "search tree expansion",
        "information-theoretic principles"
      ],
      "organization": {
        "_id": "657e54cc3687559a676eba62",
        "name": "UGA-AI",
        "fullname": "University of Georgia",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/625de37b0bec31f086e32989/D64_Jh2Os7ZU4lrySuzvT.png"
      }
    },
    "publishedAt": "2025-10-03T22:30:40.000Z",
    "title": "MITS: Enhanced Tree Search Reasoning for LLMs via Pointwise Mutual\n  Information",
    "summary": "Tree search has become as a representative framework for test-time reasoning\nwith large language models (LLMs), exemplified by methods such as\nTree-of-Thought and Monte Carlo Tree Search that explore multiple reasoning\npaths. However, it remains difficult to provide instant and reliable\nquantitative assessments of intermediate reasoning step quality, and extensive\npath exploration is computationally costly. To address this, we propose Mutual\nInformation Tree Search (MITS), a novel framework that guides reasoning with\ninformation-theoretic principles. MITS introduces an effective scoring function\nbased on pointwise mutual information (PMI), which enables step-wise evaluation\nof reasoning paths and search tree expansion via beam search without expensive\nlook-ahead simulations, achieving superior reasoning performances while\nmaintaining computational efficiency. The framework is complemented by an\nentropy-based dynamic sampling strategy that adaptively allocates computational\nresources to uncertain reasoning steps where exploration is most beneficial.\nFor final prediction, MITS employs a weighted voting scheme that combines PMI\nscores with prediction consensus. Through comprehensive experiments on diverse\nreasoning benchmarks, MITS consistently surpasses baseline methods,\nestablishing a principled and efficient framework for LLM reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.03632.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64beb6b6140491ca9f803ebf",
      "avatarUrl": "/avatars/0daa2e813a13668b8b708cd8c12763d9.svg",
      "fullname": "Yucheng SHi",
      "name": "YuchengShi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "organization": {
      "_id": "657e54cc3687559a676eba62",
      "name": "UGA-AI",
      "fullname": "University of Georgia",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/625de37b0bec31f086e32989/D64_Jh2Os7ZU4lrySuzvT.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.04800",
      "authors": [
        {
          "_id": "68e47d99e4e093a7044e4d1d",
          "name": "Sangmin Bae",
          "hidden": false
        },
        {
          "_id": "68e47d99e4e093a7044e4d1e",
          "name": "Bilge Acun",
          "hidden": false
        },
        {
          "_id": "68e47d99e4e093a7044e4d1f",
          "name": "Haroun Habeeb",
          "hidden": false
        },
        {
          "_id": "68e47d99e4e093a7044e4d20",
          "name": "Seungyeon Kim",
          "hidden": false
        },
        {
          "_id": "68e47d99e4e093a7044e4d21",
          "name": "Chien-Yu Lin",
          "hidden": false
        },
        {
          "_id": "68e47d99e4e093a7044e4d22",
          "name": "Liang Luo",
          "hidden": false
        },
        {
          "_id": "68e47d99e4e093a7044e4d23",
          "name": "Junjie Wang",
          "hidden": false
        },
        {
          "_id": "68e47d99e4e093a7044e4d24",
          "name": "Carole-Jean Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-06T13:30:07.000Z",
      "submittedOnDailyAt": "2025-10-07T01:13:05.768Z",
      "title": "Hybrid Architectures for Language Models: Systematic Analysis and Design\n  Insights",
      "submittedOnDailyBy": {
        "_id": "6602ca1e10a1441af41637be",
        "avatarUrl": "/avatars/5880e699def320beb352cbed77495b2f.svg",
        "isPro": false,
        "fullname": "Sangmin Bae",
        "user": "raymin0223",
        "type": "user"
      },
      "summary": "Recent progress in large language models demonstrates that hybrid\narchitectures--combining self-attention mechanisms with structured state space\nmodels like Mamba--can achieve a compelling balance between modeling quality\nand computational efficiency, particularly for long-context tasks. While these\nhybrid models show promising performance, systematic comparisons of\nhybridization strategies and analyses on the key factors behind their\neffectiveness have not been clearly shared to the community. In this work, we\npresent a holistic evaluation of hybrid architectures based on inter-layer\n(sequential) or intra-layer (parallel) fusion. We evaluate these designs from a\nvariety of perspectives: language modeling performance, long-context\ncapabilities, scaling analysis, and training and inference efficiency. By\ninvestigating the core characteristics of their computational primitive, we\nidentify the most critical elements for each hybridization strategy and further\npropose optimal design recipes for both hybrid models. Our comprehensive\nanalysis provides practical guidance and valuable insights for developing\nhybrid language models, facilitating the optimization of architectural\nconfigurations.",
      "upvotes": 16,
      "discussionId": "68e47d99e4e093a7044e4d25",
      "ai_summary": "A comprehensive evaluation of hybrid language models combining self-attention with structured state space models, analyzing inter-layer and intra-layer fusion strategies, and providing design recommendations.",
      "ai_keywords": [
        "self-attention mechanisms",
        "structured state space models",
        "Mamba",
        "hybrid architectures",
        "inter-layer fusion",
        "intra-layer fusion",
        "language modeling performance",
        "long-context capabilities",
        "scaling analysis",
        "training efficiency",
        "inference efficiency",
        "computational primitive"
      ],
      "organization": {
        "_id": "689f81c63080d12247c8f067",
        "name": "MetaSuperintelligenceLab",
        "fullname": "MetaSuperintelligenceLab"
      }
    },
    "publishedAt": "2025-10-06T09:30:07.000Z",
    "title": "Hybrid Architectures for Language Models: Systematic Analysis and Design\n  Insights",
    "summary": "Recent progress in large language models demonstrates that hybrid\narchitectures--combining self-attention mechanisms with structured state space\nmodels like Mamba--can achieve a compelling balance between modeling quality\nand computational efficiency, particularly for long-context tasks. While these\nhybrid models show promising performance, systematic comparisons of\nhybridization strategies and analyses on the key factors behind their\neffectiveness have not been clearly shared to the community. In this work, we\npresent a holistic evaluation of hybrid architectures based on inter-layer\n(sequential) or intra-layer (parallel) fusion. We evaluate these designs from a\nvariety of perspectives: language modeling performance, long-context\ncapabilities, scaling analysis, and training and inference efficiency. By\ninvestigating the core characteristics of their computational primitive, we\nidentify the most critical elements for each hybridization strategy and further\npropose optimal design recipes for both hybrid models. Our comprehensive\nanalysis provides practical guidance and valuable insights for developing\nhybrid language models, facilitating the optimization of architectural\nconfigurations.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.04800.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6602ca1e10a1441af41637be",
      "avatarUrl": "/avatars/5880e699def320beb352cbed77495b2f.svg",
      "fullname": "Sangmin Bae",
      "name": "raymin0223",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "organization": {
      "_id": "689f81c63080d12247c8f067",
      "name": "MetaSuperintelligenceLab",
      "fullname": "MetaSuperintelligenceLab"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.00263",
      "authors": [
        {
          "_id": "68e44581e4e093a7044e4c86",
          "name": "Zhuohang Li",
          "hidden": false
        },
        {
          "_id": "68e44581e4e093a7044e4c87",
          "name": "Xiaowei Li",
          "hidden": false
        },
        {
          "_id": "68e44581e4e093a7044e4c88",
          "name": "Chengyu Huang",
          "hidden": false
        },
        {
          "_id": "68e44581e4e093a7044e4c89",
          "name": "Guowang Li",
          "hidden": false
        },
        {
          "_id": "68e44581e4e093a7044e4c8a",
          "name": "Katayoon Goshvadi",
          "hidden": false
        },
        {
          "_id": "68e44581e4e093a7044e4c8b",
          "name": "Bo Dai",
          "hidden": false
        },
        {
          "_id": "68e44581e4e093a7044e4c8c",
          "name": "Dale Schuurmans",
          "hidden": false
        },
        {
          "_id": "68e44581e4e093a7044e4c8d",
          "name": "Paul Zhou",
          "hidden": false
        },
        {
          "_id": "68e44581e4e093a7044e4c8e",
          "name": "Hamid Palangi",
          "hidden": false
        },
        {
          "_id": "68e44581e4e093a7044e4c8f",
          "name": "Yiwen Song",
          "hidden": false
        },
        {
          "_id": "68e44581e4e093a7044e4c90",
          "name": "Palash Goyal",
          "hidden": false
        },
        {
          "_id": "68e44581e4e093a7044e4c91",
          "name": "Murat Kantarcioglu",
          "hidden": false
        },
        {
          "_id": "68e44581e4e093a7044e4c92",
          "name": "Bradley A. Malin",
          "hidden": false
        },
        {
          "_id": "68e44581e4e093a7044e4c93",
          "name": "Yuan Xue",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-30T20:36:41.000Z",
      "submittedOnDailyAt": "2025-10-07T00:46:14.270Z",
      "title": "Judging with Confidence: Calibrating Autoraters to Preference\n  Distributions",
      "submittedOnDailyBy": {
        "_id": "6328f05f84e4c8b42c5d14a4",
        "avatarUrl": "/avatars/f8277830c7fc4978f0fc0535b25177cd.svg",
        "isPro": false,
        "fullname": "Zhuohang Li",
        "user": "zhhli",
        "type": "user"
      },
      "summary": "The alignment of large language models (LLMs) with human values increasingly\nrelies on using other LLMs as automated judges, or ``autoraters''. However,\ntheir reliability is limited by a foundational issue: they are trained on\ndiscrete preference labels, forcing a single ground truth onto tasks that are\noften subjective, ambiguous, or nuanced. We argue that a reliable autorater\nmust learn to model the full distribution of preferences defined by a target\npopulation. In this paper, we propose a general framework for calibrating\nprobabilistic autoraters to any given preference distribution. We formalize the\nproblem and present two learning methods tailored to different data conditions:\n1) a direct supervised fine-tuning for dense, probabilistic labels, and 2) a\nreinforcement learning approach for sparse, binary labels. Our empirical\nresults show that finetuning autoraters with a distribution-matching objective\nleads to verbalized probability predictions that are better aligned with the\ntarget preference distribution, with improved calibration and significantly\nlower positional bias, all while preserving performance on objective tasks.",
      "upvotes": 11,
      "discussionId": "68e44582e4e093a7044e4c94",
      "ai_summary": "A framework for calibrating probabilistic autoraters to preference distributions using supervised fine-tuning and reinforcement learning improves alignment with human values and reduces bias.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "autoraters",
        "preference labels",
        "probabilistic autoraters",
        "distribution-matching objective",
        "verbalized probability predictions",
        "calibration",
        "positional bias"
      ],
      "organization": {
        "_id": "5e6aca39878b8b2bf9806447",
        "name": "google",
        "fullname": "Google",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"
      }
    },
    "publishedAt": "2025-09-30T16:36:41.000Z",
    "title": "Judging with Confidence: Calibrating Autoraters to Preference\n  Distributions",
    "summary": "The alignment of large language models (LLMs) with human values increasingly\nrelies on using other LLMs as automated judges, or ``autoraters''. However,\ntheir reliability is limited by a foundational issue: they are trained on\ndiscrete preference labels, forcing a single ground truth onto tasks that are\noften subjective, ambiguous, or nuanced. We argue that a reliable autorater\nmust learn to model the full distribution of preferences defined by a target\npopulation. In this paper, we propose a general framework for calibrating\nprobabilistic autoraters to any given preference distribution. We formalize the\nproblem and present two learning methods tailored to different data conditions:\n1) a direct supervised fine-tuning for dense, probabilistic labels, and 2) a\nreinforcement learning approach for sparse, binary labels. Our empirical\nresults show that finetuning autoraters with a distribution-matching objective\nleads to verbalized probability predictions that are better aligned with the\ntarget preference distribution, with improved calibration and significantly\nlower positional bias, all while preserving performance on objective tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.00263.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6328f05f84e4c8b42c5d14a4",
      "avatarUrl": "/avatars/f8277830c7fc4978f0fc0535b25177cd.svg",
      "fullname": "Zhuohang Li",
      "name": "zhhli",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "organization": {
      "_id": "5e6aca39878b8b2bf9806447",
      "name": "google",
      "fullname": "Google",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.05091",
      "authors": [
        {
          "_id": "68e47ef4e4e093a7044e4d27",
          "name": "Le Zhuo",
          "hidden": false
        },
        {
          "_id": "68e47ef4e4e093a7044e4d28",
          "name": "Songhao Han",
          "hidden": false
        },
        {
          "_id": "68e47ef4e4e093a7044e4d29",
          "name": "Yuandong Pu",
          "hidden": false
        },
        {
          "_id": "68e47ef4e4e093a7044e4d2a",
          "name": "Boxiang Qiu",
          "hidden": false
        },
        {
          "_id": "68e47ef4e4e093a7044e4d2b",
          "name": "Sayak Paul",
          "hidden": false
        },
        {
          "_id": "68e47ef4e4e093a7044e4d2c",
          "name": "Yue Liao",
          "hidden": false
        },
        {
          "_id": "68e47ef4e4e093a7044e4d2d",
          "name": "Yihao Liu",
          "hidden": false
        },
        {
          "_id": "68e47ef4e4e093a7044e4d2e",
          "name": "Jie Shao",
          "hidden": false
        },
        {
          "_id": "68e47ef4e4e093a7044e4d2f",
          "name": "Xi Chen",
          "hidden": false
        },
        {
          "_id": "68e47ef4e4e093a7044e4d30",
          "name": "Si Liu",
          "hidden": false
        },
        {
          "_id": "68e47ef4e4e093a7044e4d31",
          "name": "Hongsheng Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-06T17:56:55.000Z",
      "submittedOnDailyAt": "2025-10-07T01:24:33.099Z",
      "title": "Factuality Matters: When Image Generation and Editing Meet Structured\n  Visuals",
      "submittedOnDailyBy": {
        "_id": "6358a167f56b03ec9147074d",
        "avatarUrl": "/avatars/e54ea7bf0c240cf76d538296efb3976c.svg",
        "isPro": false,
        "fullname": "Le Zhuo",
        "user": "JackyZhuo",
        "type": "user"
      },
      "summary": "While modern visual generation models excel at creating aesthetically\npleasing natural images, they struggle with producing or editing structured\nvisuals like charts, diagrams, and mathematical figures, which demand\ncomposition planning, text rendering, and multimodal reasoning for factual\nfidelity. To address this, we present the first comprehensive, systematic\ninvestigation of this domain, encompassing data construction, model training,\nand an evaluation benchmark. First, we construct a large-scale dataset of 1.3\nmillion high-quality structured image pairs derived from executable drawing\nprograms and augmented with chain-of-thought reasoning annotations. Building on\nit, we train a unified model that integrates a VLM with FLUX.1 Kontext via a\nlightweight connector for enhanced multimodal understanding. A three-stage\ntraining curriculum enables progressive feature alignment, knowledge infusion,\nand reasoning-augmented generation, further boosted by an external reasoner at\ninference time. Finally, we introduce StructBench, a novel benchmark for\ngeneration and editing with over 1,700 challenging instances, and an\naccompanying evaluation metric, StructScore, which employs a multi-round Q\\&A\nprotocol to assess fine-grained factual accuracy. Evaluations of 15 models\nreveal that even leading closed-source systems remain far from satisfactory.\nOur model attains strong editing performance, and inference-time reasoning\nyields consistent gains across diverse architectures. By releasing the dataset,\nmodel, and benchmark, we aim to advance unified multimodal foundations for\nstructured visuals.",
      "upvotes": 9,
      "discussionId": "68e47ef4e4e093a7044e4d32",
      "projectPage": "https://structvisuals.github.io/",
      "githubRepo": "https://github.com/zhuole1025/Structured-Visuals",
      "ai_summary": "A comprehensive investigation into generating and editing structured visuals using a unified model integrating a VLM with FLUX Kontext, achieving strong performance and introducing a new benchmark and evaluation metric.",
      "ai_keywords": [
        "VLM",
        "FLUX Kontext",
        "chain-of-thought reasoning",
        "multimodal understanding",
        "three-stage training curriculum",
        "feature alignment",
        "knowledge infusion",
        "reasoning-augmented generation",
        "StructBench",
        "StructScore",
        "multi-round Q&A protocol"
      ],
      "githubStars": 4
    },
    "publishedAt": "2025-10-06T13:56:55.000Z",
    "title": "Factuality Matters: When Image Generation and Editing Meet Structured\n  Visuals",
    "summary": "While modern visual generation models excel at creating aesthetically\npleasing natural images, they struggle with producing or editing structured\nvisuals like charts, diagrams, and mathematical figures, which demand\ncomposition planning, text rendering, and multimodal reasoning for factual\nfidelity. To address this, we present the first comprehensive, systematic\ninvestigation of this domain, encompassing data construction, model training,\nand an evaluation benchmark. First, we construct a large-scale dataset of 1.3\nmillion high-quality structured image pairs derived from executable drawing\nprograms and augmented with chain-of-thought reasoning annotations. Building on\nit, we train a unified model that integrates a VLM with FLUX.1 Kontext via a\nlightweight connector for enhanced multimodal understanding. A three-stage\ntraining curriculum enables progressive feature alignment, knowledge infusion,\nand reasoning-augmented generation, further boosted by an external reasoner at\ninference time. Finally, we introduce StructBench, a novel benchmark for\ngeneration and editing with over 1,700 challenging instances, and an\naccompanying evaluation metric, StructScore, which employs a multi-round Q\\&A\nprotocol to assess fine-grained factual accuracy. Evaluations of 15 models\nreveal that even leading closed-source systems remain far from satisfactory.\nOur model attains strong editing performance, and inference-time reasoning\nyields consistent gains across diverse architectures. By releasing the dataset,\nmodel, and benchmark, we aim to advance unified multimodal foundations for\nstructured visuals.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.05091.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6358a167f56b03ec9147074d",
      "avatarUrl": "/avatars/e54ea7bf0c240cf76d538296efb3976c.svg",
      "fullname": "Le Zhuo",
      "name": "JackyZhuo",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.05025",
      "authors": [
        {
          "_id": "68e4bd9ce4e093a7044e4e48",
          "name": "Kuofeng Gao",
          "hidden": false
        },
        {
          "_id": "68e4bd9ce4e093a7044e4e49",
          "name": "Yiming Li",
          "hidden": false
        },
        {
          "_id": "68e4bd9ce4e093a7044e4e4a",
          "name": "Chao Du",
          "hidden": false
        },
        {
          "_id": "68e4bd9ce4e093a7044e4e4b",
          "name": "Xin Wang",
          "hidden": false
        },
        {
          "_id": "68e4bd9ce4e093a7044e4e4c",
          "name": "Xingjun Ma",
          "hidden": false
        },
        {
          "_id": "68e4bd9ce4e093a7044e4e4d",
          "name": "Shu-Tao Xia",
          "hidden": false
        },
        {
          "_id": "68e4bd9ce4e093a7044e4e4e",
          "name": "Tianyu Pang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-06T17:03:50.000Z",
      "submittedOnDailyAt": "2025-10-07T05:44:35.041Z",
      "title": "Imperceptible Jailbreaking against Large Language Models",
      "submittedOnDailyBy": {
        "_id": "63d91b6d255ef6add20e1b38",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675921369867-63d91b6d255ef6add20e1b38.jpeg",
        "isPro": false,
        "fullname": "Tianyu Pang",
        "user": "P2333",
        "type": "user"
      },
      "summary": "Jailbreaking attacks on the vision modality typically rely on imperceptible\nadversarial perturbations, whereas attacks on the textual modality are\ngenerally assumed to require visible modifications (e.g., non-semantic\nsuffixes). In this paper, we introduce imperceptible jailbreaks that exploit a\nclass of Unicode characters called variation selectors. By appending invisible\nvariation selectors to malicious questions, the jailbreak prompts appear\nvisually identical to original malicious questions on screen, while their\ntokenization is \"secretly\" altered. We propose a chain-of-search pipeline to\ngenerate such adversarial suffixes to induce harmful responses. Our experiments\nshow that our imperceptible jailbreaks achieve high attack success rates\nagainst four aligned LLMs and generalize to prompt injection attacks, all\nwithout producing any visible modifications in the written prompt. Our code is\navailable at https://github.com/sail-sg/imperceptible-jailbreaks.",
      "upvotes": 8,
      "discussionId": "68e4bd9de4e093a7044e4e4f",
      "ai_summary": "Imperceptible jailbreaks using Unicode variation selectors enable high attack success rates against aligned LLMs without visible prompt modifications.",
      "ai_keywords": [
        "adversarial perturbations",
        "Unicode characters",
        "variation selectors",
        "tokenization",
        "chain-of-search pipeline",
        "prompt injection attacks",
        "aligned LLMs"
      ],
      "organization": {
        "_id": "61f4e841c771e23a1abb61ff",
        "name": "sail",
        "fullname": "Sea AI Lab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1643440185801-5df833bdda6d0311fd3d5403.png"
      }
    },
    "publishedAt": "2025-10-06T13:03:50.000Z",
    "title": "Imperceptible Jailbreaking against Large Language Models",
    "summary": "Jailbreaking attacks on the vision modality typically rely on imperceptible\nadversarial perturbations, whereas attacks on the textual modality are\ngenerally assumed to require visible modifications (e.g., non-semantic\nsuffixes). In this paper, we introduce imperceptible jailbreaks that exploit a\nclass of Unicode characters called variation selectors. By appending invisible\nvariation selectors to malicious questions, the jailbreak prompts appear\nvisually identical to original malicious questions on screen, while their\ntokenization is \"secretly\" altered. We propose a chain-of-search pipeline to\ngenerate such adversarial suffixes to induce harmful responses. Our experiments\nshow that our imperceptible jailbreaks achieve high attack success rates\nagainst four aligned LLMs and generalize to prompt injection attacks, all\nwithout producing any visible modifications in the written prompt. Our code is\navailable at https://github.com/sail-sg/imperceptible-jailbreaks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.05025.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63d91b6d255ef6add20e1b38",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675921369867-63d91b6d255ef6add20e1b38.jpeg",
      "fullname": "Tianyu Pang",
      "name": "P2333",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "organization": {
      "_id": "61f4e841c771e23a1abb61ff",
      "name": "sail",
      "fullname": "Sea AI Lab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1643440185801-5df833bdda6d0311fd3d5403.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.04996",
      "authors": [
        {
          "_id": "68e47839e4e093a7044e4ce0",
          "name": "Wei Xiong",
          "hidden": false
        },
        {
          "_id": "68e47839e4e093a7044e4ce1",
          "name": "Chenlu Ye",
          "hidden": false
        },
        {
          "_id": "68e47839e4e093a7044e4ce2",
          "name": "Baohao Liao",
          "hidden": false
        },
        {
          "_id": "68e47839e4e093a7044e4ce3",
          "name": "Hanze Dong",
          "hidden": false
        },
        {
          "_id": "68e47839e4e093a7044e4ce4",
          "name": "Xinxing Xu",
          "hidden": false
        },
        {
          "_id": "68e47839e4e093a7044e4ce5",
          "name": "Christof Monz",
          "hidden": false
        },
        {
          "_id": "68e47839e4e093a7044e4ce6",
          "name": "Jiang Bian",
          "hidden": false
        },
        {
          "_id": "68e47839e4e093a7044e4ce7",
          "name": "Nan Jiang",
          "hidden": false
        },
        {
          "_id": "68e47839e4e093a7044e4ce8",
          "name": "Tong Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-06T16:34:09.000Z",
      "submittedOnDailyAt": "2025-10-07T00:51:42.961Z",
      "title": "Reinforce-Ada: An Adaptive Sampling Framework for Reinforce-Style LLM\n  Training",
      "submittedOnDailyBy": {
        "_id": "643e59806db6ba8c5ee123f3",
        "avatarUrl": "/avatars/4052f2a250107f43b3634c3ee3cc30a1.svg",
        "isPro": false,
        "fullname": "Wei Xiong",
        "user": "weqweasdas",
        "type": "user"
      },
      "summary": "Reinforcement learning applied to large language models (LLMs) for reasoning\ntasks is often bottlenecked by unstable gradient estimates due to fixed and\nuniform sampling of responses across prompts. Prior work such as GVM-RAFT\naddresses this by dynamically allocating inference budget per prompt to\nminimize stochastic gradient variance under a budget constraint. Inspired by\nthis insight, we propose Reinforce-Ada, an adaptive sampling framework for\nonline RL post-training of LLMs that continuously reallocates sampling effort\nto the prompts with the greatest uncertainty or learning potential. Unlike\nconventional two-stage allocation methods, Reinforce-Ada interleaves estimation\nand sampling in an online successive elimination process, and automatically\nstops sampling for a prompt once sufficient signal is collected. To stabilize\nupdates, we form fixed-size groups with enforced reward diversity and compute\nadvantage baselines using global statistics aggregated over the adaptive\nsampling phase. Empirical results across multiple model architectures and\nreasoning benchmarks show that Reinforce-Ada accelerates convergence and\nimproves final performance compared to GRPO, especially when using the balanced\nsampling variant. Our work highlights the central role of variance-aware,\nadaptive data curation in enabling efficient and reliable reinforcement\nlearning for reasoning-capable LLMs. Code is available at\nhttps://github.com/RLHFlow/Reinforce-Ada.",
      "upvotes": 6,
      "discussionId": "68e47839e4e093a7044e4ce9",
      "ai_summary": "Reinforce-Ada is an adaptive sampling framework for online reinforcement learning post-training of large language models, which accelerates convergence and improves performance by dynamically reallocating sampling effort based on prompt uncertainty.",
      "ai_keywords": [
        "reinforcement learning",
        "large language models",
        "gradient estimates",
        "inference budget",
        "stochastic gradient variance",
        "adaptive sampling",
        "online successive elimination",
        "reward diversity",
        "advantage baselines",
        "variance-aware",
        "adaptive data curation"
      ],
      "organization": {
        "_id": "662d7a9b2b1b529a43840768",
        "name": "RLHFlow",
        "fullname": "RLHFlow",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/638fb8cf2380ffd99caf8c2a/xTHSf1YDQDriY5eZ7cn_1.jpeg"
      }
    },
    "publishedAt": "2025-10-06T12:34:09.000Z",
    "title": "Reinforce-Ada: An Adaptive Sampling Framework for Reinforce-Style LLM\n  Training",
    "summary": "Reinforcement learning applied to large language models (LLMs) for reasoning\ntasks is often bottlenecked by unstable gradient estimates due to fixed and\nuniform sampling of responses across prompts. Prior work such as GVM-RAFT\naddresses this by dynamically allocating inference budget per prompt to\nminimize stochastic gradient variance under a budget constraint. Inspired by\nthis insight, we propose Reinforce-Ada, an adaptive sampling framework for\nonline RL post-training of LLMs that continuously reallocates sampling effort\nto the prompts with the greatest uncertainty or learning potential. Unlike\nconventional two-stage allocation methods, Reinforce-Ada interleaves estimation\nand sampling in an online successive elimination process, and automatically\nstops sampling for a prompt once sufficient signal is collected. To stabilize\nupdates, we form fixed-size groups with enforced reward diversity and compute\nadvantage baselines using global statistics aggregated over the adaptive\nsampling phase. Empirical results across multiple model architectures and\nreasoning benchmarks show that Reinforce-Ada accelerates convergence and\nimproves final performance compared to GRPO, especially when using the balanced\nsampling variant. Our work highlights the central role of variance-aware,\nadaptive data curation in enabling efficient and reliable reinforcement\nlearning for reasoning-capable LLMs. Code is available at\nhttps://github.com/RLHFlow/Reinforce-Ada.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.04996.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643e59806db6ba8c5ee123f3",
      "avatarUrl": "/avatars/4052f2a250107f43b3634c3ee3cc30a1.svg",
      "fullname": "Wei Xiong",
      "name": "weqweasdas",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 19
    },
    "organization": {
      "_id": "662d7a9b2b1b529a43840768",
      "name": "RLHFlow",
      "fullname": "RLHFlow",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/638fb8cf2380ffd99caf8c2a/xTHSf1YDQDriY5eZ7cn_1.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.05069",
      "authors": [
        {
          "_id": "68e48451e4e093a7044e4d6c",
          "name": "Dachuan Shi",
          "hidden": false
        },
        {
          "_id": "68e48451e4e093a7044e4d6d",
          "name": "Abedelkadir Asi",
          "hidden": false
        },
        {
          "_id": "68e48451e4e093a7044e4d6e",
          "name": "Keying Li",
          "hidden": false
        },
        {
          "_id": "68e48451e4e093a7044e4d6f",
          "name": "Xiangchi Yuan",
          "hidden": false
        },
        {
          "_id": "68e48451e4e093a7044e4d70",
          "name": "Leyan Pan",
          "hidden": false
        },
        {
          "_id": "68e48451e4e093a7044e4d71",
          "name": "Wenke Lee",
          "hidden": false
        },
        {
          "_id": "68e48451e4e093a7044e4d72",
          "name": "Wen Xiao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-06T17:46:34.000Z",
      "submittedOnDailyAt": "2025-10-07T01:39:20.150Z",
      "title": "SwiReasoning: Switch-Thinking in Latent and Explicit for Pareto-Superior\n  Reasoning LLMs",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Recent work shows that, beyond discrete reasoning through explicit\nchain-of-thought steps, which are limited by the boundaries of natural\nlanguages, large language models (LLMs) can also reason continuously in latent\nspace, allowing richer information per step and thereby improving token\nefficiency. Despite this promise, latent reasoning still faces two challenges,\nespecially in training-free settings: 1) purely latent reasoning broadens the\nsearch distribution by maintaining multiple implicit paths, which diffuses\nprobability mass, introduces noise, and impedes convergence to a single\nhigh-confidence solution, thereby hurting accuracy; and 2) overthinking\npersists even without explicit text, wasting tokens and degrading efficiency.\nTo address these issues, we introduce SwiReasoning, a training-free framework\nfor LLM reasoning which features two key innovations: 1) SwiReasoning\ndynamically switches between explicit and latent reasoning, guided by\nblock-wise confidence estimated from entropy trends in next-token\ndistributions, to balance exploration and exploitation and promote timely\nconvergence. 2) By limiting the maximum number of thinking-block switches,\nSwiReasoning curbs overthinking and improves token efficiency across varying\nproblem difficulties. On widely used mathematics and STEM benchmarks,\nSwiReasoning consistently improves average accuracy by 1.5%-2.8% across\nreasoning LLMs of different model families and scales. Furthermore, under\nconstrained budgets, SwiReasoning improves average token efficiency by 56%-79%,\nwith larger gains as budgets tighten.",
      "upvotes": 5,
      "discussionId": "68e48451e4e093a7044e4d73",
      "ai_summary": "SwiReasoning, a training-free framework for LLMs, dynamically switches between explicit and latent reasoning to improve accuracy and token efficiency.",
      "ai_keywords": [
        "latent reasoning",
        "explicit reasoning",
        "next-token distributions",
        "entropy trends",
        "block-wise confidence",
        "thinking-block switches",
        "token efficiency",
        "mathematics benchmarks",
        "STEM benchmarks"
      ],
      "organization": {
        "_id": "5e6485f787403103f9f1055e",
        "name": "microsoft",
        "fullname": "Microsoft",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"
      }
    },
    "publishedAt": "2025-10-06T13:46:34.000Z",
    "title": "SwiReasoning: Switch-Thinking in Latent and Explicit for Pareto-Superior\n  Reasoning LLMs",
    "summary": "Recent work shows that, beyond discrete reasoning through explicit\nchain-of-thought steps, which are limited by the boundaries of natural\nlanguages, large language models (LLMs) can also reason continuously in latent\nspace, allowing richer information per step and thereby improving token\nefficiency. Despite this promise, latent reasoning still faces two challenges,\nespecially in training-free settings: 1) purely latent reasoning broadens the\nsearch distribution by maintaining multiple implicit paths, which diffuses\nprobability mass, introduces noise, and impedes convergence to a single\nhigh-confidence solution, thereby hurting accuracy; and 2) overthinking\npersists even without explicit text, wasting tokens and degrading efficiency.\nTo address these issues, we introduce SwiReasoning, a training-free framework\nfor LLM reasoning which features two key innovations: 1) SwiReasoning\ndynamically switches between explicit and latent reasoning, guided by\nblock-wise confidence estimated from entropy trends in next-token\ndistributions, to balance exploration and exploitation and promote timely\nconvergence. 2) By limiting the maximum number of thinking-block switches,\nSwiReasoning curbs overthinking and improves token efficiency across varying\nproblem difficulties. On widely used mathematics and STEM benchmarks,\nSwiReasoning consistently improves average accuracy by 1.5%-2.8% across\nreasoning LLMs of different model families and scales. Furthermore, under\nconstrained budgets, SwiReasoning improves average token efficiency by 56%-79%,\nwith larger gains as budgets tighten.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.05069.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 119
    },
    "organization": {
      "_id": "5e6485f787403103f9f1055e",
      "name": "microsoft",
      "fullname": "Microsoft",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.03561",
      "authors": [
        {
          "_id": "68e46ffee4e093a7044e4ccb",
          "name": "Adam Filipek",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/675197c3ae96d7ba4b4a6c66/mzVnK1wM9elG5vTCd4pi_.png",
        "https://cdn-uploads.huggingface.co/production/uploads/675197c3ae96d7ba4b4a6c66/emRA1Lr4AKtlLUVowTUjB.png"
      ],
      "publishedAt": "2025-10-03T23:18:07.000Z",
      "submittedOnDailyAt": "2025-10-07T00:25:25.255Z",
      "title": "Reactive Transformer (RxT) -- Stateful Real-Time Processing for\n  Event-Driven Reactive Language Models",
      "submittedOnDailyBy": {
        "_id": "675197c3ae96d7ba4b4a6c66",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/I2GHgrv70cfT8C5EbK6Q5.png",
        "isPro": false,
        "fullname": "Adam Filipek",
        "user": "AdamF92",
        "type": "user"
      },
      "summary": "The Transformer architecture has become the de facto standard for Large\nLanguage Models (LLMs), demonstrating remarkable capabilities in language\nunderstanding and generation. However, its application in conversational AI is\nfundamentally constrained by its stateless nature and the quadratic\ncomputational complexity (O(L^2)) with respect to sequence length L.\nCurrent models emulate memory by reprocessing an ever-expanding conversation\nhistory with each turn, leading to prohibitive costs and latency in long\ndialogues. This paper introduces the Reactive Transformer (RxT), a novel\narchitecture designed to overcome these limitations by shifting from a\ndata-driven to an event-driven paradigm. RxT processes each conversational turn\nas a discrete event in real-time, maintaining context in an integrated,\nfixed-size Short-Term Memory (STM) system. The architecture features a distinct\noperational cycle where a generator-decoder produces a response based on the\ncurrent query and the previous memory state, after which a memory-encoder and a\ndedicated Memory Attention network asynchronously update the STM with a\nrepresentation of the complete interaction. This design fundamentally alters\nthe scaling dynamics, reducing the total user-facing cost of a conversation\nfrom quadratic (O(N^2 cdot T)) to linear (O(N cdot T)) with respect to\nthe number of interactions N. By decoupling response generation from memory\nupdates, RxT achieves low latency, enabling truly real-time, stateful, and\neconomically viable long-form conversations. We validated our architecture with\na series of proof-of-concept experiments on synthetic data, demonstrating\nsuperior performance and constant-time inference latency compared to a baseline\nstateless model of comparable size.",
      "upvotes": 5,
      "discussionId": "68e47064e4e093a7044e4ccc",
      "projectPage": "https://rxai.dev/research/reactive-transformer-introduction",
      "githubRepo": "https://github.com/RxAI-dev/rxlm",
      "ai_summary": "The Reactive Transformer (RxT) addresses the limitations of stateless Transformers in conversational AI by using an event-driven paradigm with a fixed-size Short-Term Memory (STM) system, achieving linear scaling and low latency.",
      "ai_keywords": [
        "Transformer architecture",
        "Large Language Models (LLMs)",
        "stateless nature",
        "quadratic computational complexity",
        "Reactive Transformer (RxT)",
        "event-driven paradigm",
        "Short-Term Memory (STM)",
        "generator-decoder",
        "memory-encoder",
        "Memory Attention network",
        "linear scaling",
        "constant-time inference latency"
      ],
      "githubStars": 0,
      "organization": {
        "_id": "675776b060e4100500aeb4c8",
        "name": "ReactiveAI",
        "fullname": "Reactive AI",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/675197c3ae96d7ba4b4a6c66/AJDkLuavcYfENIRDzxjqR.png"
      }
    },
    "publishedAt": "2025-10-03T19:18:07.000Z",
    "title": "Reactive Transformer (RxT) -- Stateful Real-Time Processing for\n  Event-Driven Reactive Language Models",
    "summary": "The Transformer architecture has become the de facto standard for Large\nLanguage Models (LLMs), demonstrating remarkable capabilities in language\nunderstanding and generation. However, its application in conversational AI is\nfundamentally constrained by its stateless nature and the quadratic\ncomputational complexity (O(L^2)) with respect to sequence length L.\nCurrent models emulate memory by reprocessing an ever-expanding conversation\nhistory with each turn, leading to prohibitive costs and latency in long\ndialogues. This paper introduces the Reactive Transformer (RxT), a novel\narchitecture designed to overcome these limitations by shifting from a\ndata-driven to an event-driven paradigm. RxT processes each conversational turn\nas a discrete event in real-time, maintaining context in an integrated,\nfixed-size Short-Term Memory (STM) system. The architecture features a distinct\noperational cycle where a generator-decoder produces a response based on the\ncurrent query and the previous memory state, after which a memory-encoder and a\ndedicated Memory Attention network asynchronously update the STM with a\nrepresentation of the complete interaction. This design fundamentally alters\nthe scaling dynamics, reducing the total user-facing cost of a conversation\nfrom quadratic (O(N^2 cdot T)) to linear (O(N cdot T)) with respect to\nthe number of interactions N. By decoupling response generation from memory\nupdates, RxT achieves low latency, enabling truly real-time, stateful, and\neconomically viable long-form conversations. We validated our architecture with\na series of proof-of-concept experiments on synthetic data, demonstrating\nsuperior performance and constant-time inference latency compared to a baseline\nstateless model of comparable size.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/675197c3ae96d7ba4b4a6c66/mzVnK1wM9elG5vTCd4pi_.png",
      "https://cdn-uploads.huggingface.co/production/uploads/675197c3ae96d7ba4b4a6c66/emRA1Lr4AKtlLUVowTUjB.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.03561.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "675197c3ae96d7ba4b4a6c66",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/I2GHgrv70cfT8C5EbK6Q5.png",
      "fullname": "Adam Filipek",
      "name": "AdamF92",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "organization": {
      "_id": "675776b060e4100500aeb4c8",
      "name": "ReactiveAI",
      "fullname": "Reactive AI",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/675197c3ae96d7ba4b4a6c66/AJDkLuavcYfENIRDzxjqR.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.02919",
      "authors": [
        {
          "_id": "68e4aac5e4e093a7044e4dec",
          "name": "Jian Mu",
          "hidden": false
        },
        {
          "_id": "68e4aac5e4e093a7044e4ded",
          "name": "Qixin Zhang",
          "hidden": false
        },
        {
          "_id": "68e4aac5e4e093a7044e4dee",
          "name": "Zhiyong Wang",
          "hidden": false
        },
        {
          "_id": "68e4aac5e4e093a7044e4def",
          "name": "Menglin Yang",
          "hidden": false
        },
        {
          "_id": "68e4aac5e4e093a7044e4df0",
          "name": "Shuang Qiu",
          "hidden": false
        },
        {
          "_id": "68e4aac5e4e093a7044e4df1",
          "name": "Chengwei Qin",
          "hidden": false
        },
        {
          "_id": "68e4aac5e4e093a7044e4df2",
          "name": "Zhongxiang Dai",
          "hidden": false
        },
        {
          "_id": "68e4aac5e4e093a7044e4df3",
          "name": "Yao Shu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-03T11:46:04.000Z",
      "submittedOnDailyAt": "2025-10-07T04:29:47.229Z",
      "title": "Self-Reflective Generation at Test Time",
      "submittedOnDailyBy": {
        "_id": "6628b0621b4cd5f0ada35ed8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/TppL-D7GtZPqUlUb6UU-K.png",
        "isPro": false,
        "fullname": "mj",
        "user": "mujianijan",
        "type": "user"
      },
      "summary": "Large language models (LLMs) increasingly solve complex reasoning tasks via\nlong chain-of-thought, but their forward-only autoregressive generation process\nis fragile; early token errors can cascade, which creates a clear need for\nself-reflection mechanisms. However, existing self-reflection either performs\nrevisions over full drafts or learns self-correction via expensive training,\nboth fundamentally reactive and inefficient. To address this, we propose\nSelf-Reflective Generation at Test Time (SRGen), a lightweight test-time\nframework that reflects before generating at uncertain points. During token\ngeneration, SRGen utilizes dynamic entropy thresholding to identify\nhigh-uncertainty tokens. For each identified token, it trains a specific\ncorrective vector, which fully exploits the already generated context for a\nself-reflective generation to correct the token probability distribution. By\nretrospectively analyzing the partial output, this self-reflection enables more\ntrustworthy decisions, thereby significantly reducing the probability of errors\nat highly uncertain points. Evaluated on challenging mathematical reasoning\nbenchmarks and a diverse set of LLMs, SRGen can consistently strengthen model\nreasoning: improvements in single-pass quality also translate into stronger\nself-consistency voting. Especially, on AIME2024 with\nDeepSeek-R1-Distill-Qwen-7B, SRGen yields absolute improvements of +12.0% on\nPass@1 and +13.3% on Cons@5. Moreover, our findings position SRGen as a\nplug-and-play method that integrates reflection into the generation process for\nreliable LLM reasoning, achieving consistent gains with bounded overhead and\nbroad composability with other training-time (e.g., RLHF) and test-time (e.g.,\nSLOT) techniques.",
      "upvotes": 5,
      "discussionId": "68e4aac5e4e093a7044e4df4",
      "githubRepo": "https://github.com/2020-qqtcg/SRGen",
      "ai_summary": "SRGen, a lightweight test-time framework, improves LLM reasoning by dynamically identifying and correcting high-uncertainty tokens during generation, leading to better single-pass quality and self-consistency.",
      "ai_keywords": [
        "Self-Reflective Generation",
        "SRGen",
        "dynamic entropy thresholding",
        "corrective vector",
        "self-reflective generation",
        "token probability distribution",
        "self-consistency voting",
        "AIME2024",
        "DeepSeek-R1-Distill-Qwen-7B",
        "Pass@1",
        "Cons@5",
        "RLHF",
        "SLOT"
      ]
    },
    "publishedAt": "2025-10-03T07:46:04.000Z",
    "title": "Self-Reflective Generation at Test Time",
    "summary": "Large language models (LLMs) increasingly solve complex reasoning tasks via\nlong chain-of-thought, but their forward-only autoregressive generation process\nis fragile; early token errors can cascade, which creates a clear need for\nself-reflection mechanisms. However, existing self-reflection either performs\nrevisions over full drafts or learns self-correction via expensive training,\nboth fundamentally reactive and inefficient. To address this, we propose\nSelf-Reflective Generation at Test Time (SRGen), a lightweight test-time\nframework that reflects before generating at uncertain points. During token\ngeneration, SRGen utilizes dynamic entropy thresholding to identify\nhigh-uncertainty tokens. For each identified token, it trains a specific\ncorrective vector, which fully exploits the already generated context for a\nself-reflective generation to correct the token probability distribution. By\nretrospectively analyzing the partial output, this self-reflection enables more\ntrustworthy decisions, thereby significantly reducing the probability of errors\nat highly uncertain points. Evaluated on challenging mathematical reasoning\nbenchmarks and a diverse set of LLMs, SRGen can consistently strengthen model\nreasoning: improvements in single-pass quality also translate into stronger\nself-consistency voting. Especially, on AIME2024 with\nDeepSeek-R1-Distill-Qwen-7B, SRGen yields absolute improvements of +12.0% on\nPass@1 and +13.3% on Cons@5. Moreover, our findings position SRGen as a\nplug-and-play method that integrates reflection into the generation process for\nreliable LLM reasoning, achieving consistent gains with bounded overhead and\nbroad composability with other training-time (e.g., RLHF) and test-time (e.g.,\nSLOT) techniques.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.02919.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6628b0621b4cd5f0ada35ed8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/TppL-D7GtZPqUlUb6UU-K.png",
      "fullname": "mj",
      "name": "mujianijan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.04618",
      "authors": [
        {
          "_id": "68e48815e4e093a7044e4d75",
          "name": "Qizheng Zhang",
          "hidden": false
        },
        {
          "_id": "68e48815e4e093a7044e4d76",
          "name": "Changran Hu",
          "hidden": false
        },
        {
          "_id": "68e48815e4e093a7044e4d77",
          "name": "Shubhangi Upasani",
          "hidden": false
        },
        {
          "_id": "68e48815e4e093a7044e4d78",
          "name": "Boyuan Ma",
          "hidden": false
        },
        {
          "_id": "68e48815e4e093a7044e4d79",
          "name": "Fenglu Hong",
          "hidden": false
        },
        {
          "_id": "68e48815e4e093a7044e4d7a",
          "name": "Vamsidhar Kamanuru",
          "hidden": false
        },
        {
          "_id": "68e48815e4e093a7044e4d7b",
          "name": "Jay Rainton",
          "hidden": false
        },
        {
          "_id": "68e48815e4e093a7044e4d7c",
          "name": "Chen Wu",
          "hidden": false
        },
        {
          "_id": "68e48815e4e093a7044e4d7d",
          "name": "Mengmeng Ji",
          "hidden": false
        },
        {
          "_id": "68e48815e4e093a7044e4d7e",
          "name": "Hanchen Li",
          "hidden": false
        },
        {
          "_id": "68e48815e4e093a7044e4d7f",
          "name": "Urmish Thakker",
          "hidden": false
        },
        {
          "_id": "68e48815e4e093a7044e4d80",
          "name": "James Zou",
          "hidden": false
        },
        {
          "_id": "68e48815e4e093a7044e4d81",
          "name": "Kunle Olukotun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-06T09:30:18.000Z",
      "submittedOnDailyAt": "2025-10-07T01:55:17.863Z",
      "title": "Agentic Context Engineering: Evolving Contexts for Self-Improving\n  Language Models",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Large language model (LLM) applications such as agents and domain-specific\nreasoning increasingly rely on context adaptation -- modifying inputs with\ninstructions, strategies, or evidence, rather than weight updates. Prior\napproaches improve usability but often suffer from brevity bias, which drops\ndomain insights for concise summaries, and from context collapse, where\niterative rewriting erodes details over time. Building on the adaptive memory\nintroduced by Dynamic Cheatsheet, we introduce ACE (Agentic Context\nEngineering), a framework that treats contexts as evolving playbooks that\naccumulate, refine, and organize strategies through a modular process of\ngeneration, reflection, and curation. ACE prevents collapse with structured,\nincremental updates that preserve detailed knowledge and scale with\nlong-context models. Across agent and domain-specific benchmarks, ACE optimizes\ncontexts both offline (e.g., system prompts) and online (e.g., agent memory),\nconsistently outperforming strong baselines: +10.6% on agents and +8.6% on\nfinance, while significantly reducing adaptation latency and rollout cost.\nNotably, ACE could adapt effectively without labeled supervision and instead by\nleveraging natural execution feedback. On the AppWorld leaderboard, ACE matches\nthe top-ranked production-level agent on the overall average and surpasses it\non the harder test-challenge split, despite using a smaller open-source model.\nThese results show that comprehensive, evolving contexts enable scalable,\nefficient, and self-improving LLM systems with low overhead.",
      "upvotes": 4,
      "discussionId": "68e48815e4e093a7044e4d82",
      "ai_summary": "ACE, a framework for adaptive context engineering, enhances LLM applications by preserving detailed knowledge through structured updates, outperforming baselines in agent and domain-specific tasks with reduced adaptation costs.",
      "ai_keywords": [
        "Large language model",
        "context adaptation",
        "Dynamic Cheatsheet",
        "ACE",
        "Agentic Context Engineering",
        "generation",
        "reflection",
        "curation",
        "context collapse",
        "adaptation latency",
        "rollout cost",
        "natural execution feedback",
        "AppWorld leaderboard"
      ]
    },
    "publishedAt": "2025-10-06T05:30:18.000Z",
    "title": "Agentic Context Engineering: Evolving Contexts for Self-Improving\n  Language Models",
    "summary": "Large language model (LLM) applications such as agents and domain-specific\nreasoning increasingly rely on context adaptation -- modifying inputs with\ninstructions, strategies, or evidence, rather than weight updates. Prior\napproaches improve usability but often suffer from brevity bias, which drops\ndomain insights for concise summaries, and from context collapse, where\niterative rewriting erodes details over time. Building on the adaptive memory\nintroduced by Dynamic Cheatsheet, we introduce ACE (Agentic Context\nEngineering), a framework that treats contexts as evolving playbooks that\naccumulate, refine, and organize strategies through a modular process of\ngeneration, reflection, and curation. ACE prevents collapse with structured,\nincremental updates that preserve detailed knowledge and scale with\nlong-context models. Across agent and domain-specific benchmarks, ACE optimizes\ncontexts both offline (e.g., system prompts) and online (e.g., agent memory),\nconsistently outperforming strong baselines: +10.6% on agents and +8.6% on\nfinance, while significantly reducing adaptation latency and rollout cost.\nNotably, ACE could adapt effectively without labeled supervision and instead by\nleveraging natural execution feedback. On the AppWorld leaderboard, ACE matches\nthe top-ranked production-level agent on the overall average and surpasses it\non the harder test-challenge split, despite using a smaller open-source model.\nThese results show that comprehensive, evolving contexts enable scalable,\nefficient, and self-improving LLM systems with low overhead.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.04618.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 119
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.04290",
      "authors": [
        {
          "_id": "68e495cbe4e093a7044e4dc1",
          "name": "Jay Zhangjie Wu",
          "hidden": false
        },
        {
          "_id": "68e495cbe4e093a7044e4dc2",
          "name": "Xuanchi Ren",
          "hidden": false
        },
        {
          "_id": "68e495cbe4e093a7044e4dc3",
          "name": "Tianchang Shen",
          "hidden": false
        },
        {
          "_id": "68e495cbe4e093a7044e4dc4",
          "name": "Tianshi Cao",
          "hidden": false
        },
        {
          "_id": "68e495cbe4e093a7044e4dc5",
          "name": "Kai He",
          "hidden": false
        },
        {
          "_id": "68e495cbe4e093a7044e4dc6",
          "name": "Yifan Lu",
          "hidden": false
        },
        {
          "_id": "68e495cbe4e093a7044e4dc7",
          "name": "Ruiyuan Gao",
          "hidden": false
        },
        {
          "_id": "68e495cbe4e093a7044e4dc8",
          "name": "Enze Xie",
          "hidden": false
        },
        {
          "_id": "68e495cbe4e093a7044e4dc9",
          "name": "Shiyi Lan",
          "hidden": false
        },
        {
          "_id": "68e495cbe4e093a7044e4dca",
          "name": "Jose M. Alvarez",
          "hidden": false
        },
        {
          "_id": "68e495cbe4e093a7044e4dcb",
          "name": "Jun Gao",
          "hidden": false
        },
        {
          "_id": "68e495cbe4e093a7044e4dcc",
          "name": "Sanja Fidler",
          "hidden": false
        },
        {
          "_id": "68e495cbe4e093a7044e4dcd",
          "name": "Zian Wang",
          "hidden": false
        },
        {
          "_id": "68e495cbe4e093a7044e4dce",
          "name": "Huan Ling",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-05T17:02:01.000Z",
      "submittedOnDailyAt": "2025-10-07T02:54:44.177Z",
      "title": "ChronoEdit: Towards Temporal Reasoning for Image Editing and World\n  Simulation",
      "submittedOnDailyBy": {
        "_id": "633aaf695df91da9cea92960",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633aaf695df91da9cea92960/h1oOWUudbdlsd63Q5w0hM.png",
        "isPro": false,
        "fullname": "Jay Wu",
        "user": "jayw",
        "type": "user"
      },
      "summary": "Recent advances in large generative models have significantly advanced image\nediting and in-context image generation, yet a critical gap remains in ensuring\nphysical consistency, where edited objects must remain coherent. This\ncapability is especially vital for world simulation related tasks. In this\npaper, we present ChronoEdit, a framework that reframes image editing as a\nvideo generation problem. First, ChronoEdit treats the input and edited images\nas the first and last frames of a video, allowing it to leverage large\npretrained video generative models that capture not only object appearance but\nalso the implicit physics of motion and interaction through learned temporal\nconsistency. Second, ChronoEdit introduces a temporal reasoning stage that\nexplicitly performs editing at inference time. Under this setting, the target\nframe is jointly denoised with reasoning tokens to imagine a plausible editing\ntrajectory that constrains the solution space to physically viable\ntransformations. The reasoning tokens are then dropped after a few steps to\navoid the high computational cost of rendering a full video. To validate\nChronoEdit, we introduce PBench-Edit, a new benchmark of image-prompt pairs for\ncontexts that require physical consistency, and demonstrate that ChronoEdit\nsurpasses state-of-the-art baselines in both visual fidelity and physical\nplausibility. Code and models for both the 14B and 2B variants of ChronoEdit\nwill be released on the project page:\nhttps://research.nvidia.com/labs/toronto-ai/chronoedit",
      "upvotes": 4,
      "discussionId": "68e495cbe4e093a7044e4dcf",
      "projectPage": "https://research.nvidia.com/labs/toronto-ai/chronoedit",
      "githubRepo": "https://github.com/nv-tlabs/ChronoEdit",
      "ai_summary": "ChronoEdit addresses physical consistency in image editing by reframing it as a video generation problem, leveraging pretrained video models and temporal reasoning tokens.",
      "ai_keywords": [
        "video generative models",
        "temporal consistency",
        "temporal reasoning",
        "denoising",
        "reasoning tokens",
        "PBench-Edit",
        "visual fidelity",
        "physical plausibility"
      ],
      "githubStars": 13,
      "organization": {
        "_id": "60262b67268c201cdc8b7d43",
        "name": "nvidia",
        "fullname": "NVIDIA",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
      }
    },
    "publishedAt": "2025-10-05T13:02:01.000Z",
    "title": "ChronoEdit: Towards Temporal Reasoning for Image Editing and World\n  Simulation",
    "summary": "Recent advances in large generative models have significantly advanced image\nediting and in-context image generation, yet a critical gap remains in ensuring\nphysical consistency, where edited objects must remain coherent. This\ncapability is especially vital for world simulation related tasks. In this\npaper, we present ChronoEdit, a framework that reframes image editing as a\nvideo generation problem. First, ChronoEdit treats the input and edited images\nas the first and last frames of a video, allowing it to leverage large\npretrained video generative models that capture not only object appearance but\nalso the implicit physics of motion and interaction through learned temporal\nconsistency. Second, ChronoEdit introduces a temporal reasoning stage that\nexplicitly performs editing at inference time. Under this setting, the target\nframe is jointly denoised with reasoning tokens to imagine a plausible editing\ntrajectory that constrains the solution space to physically viable\ntransformations. The reasoning tokens are then dropped after a few steps to\navoid the high computational cost of rendering a full video. To validate\nChronoEdit, we introduce PBench-Edit, a new benchmark of image-prompt pairs for\ncontexts that require physical consistency, and demonstrate that ChronoEdit\nsurpasses state-of-the-art baselines in both visual fidelity and physical\nplausibility. Code and models for both the 14B and 2B variants of ChronoEdit\nwill be released on the project page:\nhttps://research.nvidia.com/labs/toronto-ai/chronoedit",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.04290.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "633aaf695df91da9cea92960",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633aaf695df91da9cea92960/h1oOWUudbdlsd63Q5w0hM.png",
      "fullname": "Jay Wu",
      "name": "jayw",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 13
    },
    "organization": {
      "_id": "60262b67268c201cdc8b7d43",
      "name": "nvidia",
      "fullname": "NVIDIA",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.05094",
      "authors": [
        {
          "_id": "68e47f85e4e093a7044e4d34",
          "name": "Ziqi Huang",
          "hidden": false
        },
        {
          "_id": "68e47f85e4e093a7044e4d35",
          "name": "Ning Yu",
          "hidden": false
        },
        {
          "_id": "68e47f85e4e093a7044e4d36",
          "name": "Gordon Chen",
          "hidden": false
        },
        {
          "_id": "68e47f85e4e093a7044e4d37",
          "name": "Haonan Qiu",
          "hidden": false
        },
        {
          "_id": "68e47f85e4e093a7044e4d38",
          "name": "Paul Debevec",
          "hidden": false
        },
        {
          "_id": "68e47f85e4e093a7044e4d39",
          "name": "Ziwei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-06T17:57:59.000Z",
      "submittedOnDailyAt": "2025-10-07T01:18:50.036Z",
      "title": "VChain: Chain-of-Visual-Thought for Reasoning in Video Generation",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Recent video generation models can produce smooth and visually appealing\nclips, but they often struggle to synthesize complex dynamics with a coherent\nchain of consequences. Accurately modeling visual outcomes and state\ntransitions over time remains a core challenge. In contrast, large language and\nmultimodal models (e.g., GPT-4o) exhibit strong visual state reasoning and\nfuture prediction capabilities. To bridge these strengths, we introduce VChain,\na novel inference-time chain-of-visual-thought framework that injects visual\nreasoning signals from multimodal models into video generation. Specifically,\nVChain contains a dedicated pipeline that leverages large multimodal models to\ngenerate a sparse set of critical keyframes as snapshots, which are then used\nto guide the sparse inference-time tuning of a pre-trained video generator only\nat these key moments. Our approach is tuning-efficient, introduces minimal\noverhead and avoids dense supervision. Extensive experiments on complex,\nmulti-step scenarios show that VChain significantly enhances the quality of\ngenerated videos.",
      "upvotes": 3,
      "discussionId": "68e47f86e4e093a7044e4d3a",
      "projectPage": "https://eyeline-labs.github.io/VChain/",
      "githubRepo": "https://github.com/Eyeline-Labs/VChain",
      "ai_summary": "VChain enhances video generation by integrating visual reasoning from multimodal models to guide sparse tuning of a pre-trained video generator.",
      "ai_keywords": [
        "video generation",
        "visual reasoning",
        "multimodal models",
        "keyframes",
        "sparse inference-time tuning",
        "pre-trained video generator"
      ],
      "githubStars": 10
    },
    "publishedAt": "2025-10-06T13:57:59.000Z",
    "title": "VChain: Chain-of-Visual-Thought for Reasoning in Video Generation",
    "summary": "Recent video generation models can produce smooth and visually appealing\nclips, but they often struggle to synthesize complex dynamics with a coherent\nchain of consequences. Accurately modeling visual outcomes and state\ntransitions over time remains a core challenge. In contrast, large language and\nmultimodal models (e.g., GPT-4o) exhibit strong visual state reasoning and\nfuture prediction capabilities. To bridge these strengths, we introduce VChain,\na novel inference-time chain-of-visual-thought framework that injects visual\nreasoning signals from multimodal models into video generation. Specifically,\nVChain contains a dedicated pipeline that leverages large multimodal models to\ngenerate a sparse set of critical keyframes as snapshots, which are then used\nto guide the sparse inference-time tuning of a pre-trained video generator only\nat these key moments. Our approach is tuning-efficient, introduces minimal\noverhead and avoids dense supervision. Extensive experiments on complex,\nmulti-step scenarios show that VChain significantly enhances the quality of\ngenerated videos.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.05094.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 119
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.04673",
      "authors": [
        {
          "_id": "68e481d4e4e093a7044e4d59",
          "name": "Chan Hee Song",
          "hidden": false
        },
        {
          "_id": "68e481d4e4e093a7044e4d5a",
          "name": "Yiwen Song",
          "hidden": false
        },
        {
          "_id": "68e481d4e4e093a7044e4d5b",
          "name": "Palash Goyal",
          "hidden": false
        },
        {
          "_id": "68e481d4e4e093a7044e4d5c",
          "name": "Yu Su",
          "hidden": false
        },
        {
          "_id": "68e481d4e4e093a7044e4d5d",
          "name": "Oriana Riva",
          "hidden": false
        },
        {
          "_id": "68e481d4e4e093a7044e4d5e",
          "name": "Hamid Palangi",
          "hidden": false
        },
        {
          "_id": "68e481d4e4e093a7044e4d5f",
          "name": "Tomas Pfister",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-06T10:29:00.000Z",
      "submittedOnDailyAt": "2025-10-07T01:28:38.169Z",
      "title": "Watch and Learn: Learning to Use Computers from Online Videos",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Computer use agents (CUAs) need to plan task workflows grounded in diverse,\never-changing applications and environments, but learning is hindered by the\nscarcity of large-scale, high-quality training data in the target application.\nExisting datasets are domain-specific, static, and costly to annotate, while\ncurrent synthetic data generation methods often yield simplistic or misaligned\ntask demonstrations. To address these limitations, we introduce Watch & Learn\n(W&L), a framework that converts human demonstration videos readily available\non the Internet into executable UI trajectories at scale. Instead of directly\ngenerating trajectories or relying on ad hoc reasoning heuristics, we cast the\nproblem as an inverse dynamics objective: predicting the user's action from\nconsecutive screen states. This formulation reduces manual engineering, is\neasier to learn, and generalizes more robustly across applications. Concretely,\nwe develop an inverse dynamics labeling pipeline with task-aware video\nretrieval, generate over 53k high-quality trajectories from raw web videos, and\ndemonstrate that these trajectories improve CUAs both as in-context\ndemonstrations and as supervised training data. On the challenging OSWorld\nbenchmark, UI trajectories extracted with W&L consistently enhance both\ngeneral-purpose and state-of-the-art frameworks in-context, and deliver\nstronger gains for open-source models under supervised training. These results\nhighlight web-scale human demonstration videos as a practical and scalable\nfoundation for advancing CUAs towards real-world deployment.",
      "upvotes": 3,
      "discussionId": "68e481d4e4e093a7044e4d60",
      "ai_summary": "Watch & Learn converts web demonstration videos into UI trajectories to enhance computer use agents, improving both in-context demonstrations and supervised training.",
      "ai_keywords": [
        "inverse dynamics",
        "task-aware video retrieval",
        "UI trajectories",
        "OSWorld benchmark"
      ]
    },
    "publishedAt": "2025-10-06T06:29:00.000Z",
    "title": "Watch and Learn: Learning to Use Computers from Online Videos",
    "summary": "Computer use agents (CUAs) need to plan task workflows grounded in diverse,\never-changing applications and environments, but learning is hindered by the\nscarcity of large-scale, high-quality training data in the target application.\nExisting datasets are domain-specific, static, and costly to annotate, while\ncurrent synthetic data generation methods often yield simplistic or misaligned\ntask demonstrations. To address these limitations, we introduce Watch & Learn\n(W&L), a framework that converts human demonstration videos readily available\non the Internet into executable UI trajectories at scale. Instead of directly\ngenerating trajectories or relying on ad hoc reasoning heuristics, we cast the\nproblem as an inverse dynamics objective: predicting the user's action from\nconsecutive screen states. This formulation reduces manual engineering, is\neasier to learn, and generalizes more robustly across applications. Concretely,\nwe develop an inverse dynamics labeling pipeline with task-aware video\nretrieval, generate over 53k high-quality trajectories from raw web videos, and\ndemonstrate that these trajectories improve CUAs both as in-context\ndemonstrations and as supervised training data. On the challenging OSWorld\nbenchmark, UI trajectories extracted with W&L consistently enhance both\ngeneral-purpose and state-of-the-art frameworks in-context, and deliver\nstronger gains for open-source models under supervised training. These results\nhighlight web-scale human demonstration videos as a practical and scalable\nfoundation for advancing CUAs towards real-world deployment.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.04673.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 119
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.04434",
      "authors": [
        {
          "_id": "68e478b7e4e093a7044e4cfc",
          "name": "Grace LeFevre",
          "hidden": false
        },
        {
          "_id": "68e478b7e4e093a7044e4cfd",
          "name": "Qingcheng Zeng",
          "hidden": false
        },
        {
          "_id": "68e478b7e4e093a7044e4cfe",
          "name": "Adam Leif",
          "hidden": false
        },
        {
          "_id": "68e478b7e4e093a7044e4cff",
          "name": "Jason Jewell",
          "hidden": false
        },
        {
          "_id": "68e478b7e4e093a7044e4d00",
          "name": "Denis Peskoff",
          "hidden": false
        },
        {
          "_id": "68e478b7e4e093a7044e4d01",
          "name": "Rob Voigt",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-06T02:04:42.000Z",
      "submittedOnDailyAt": "2025-10-07T00:57:27.695Z",
      "title": "Good Intentions Beyond ACL: Who Does NLP for Social Good, and Where?",
      "submittedOnDailyBy": {
        "_id": "63bc77661374e3ef9135735f",
        "avatarUrl": "/avatars/94b04545ed9d30bfe58691672a0b5618.svg",
        "isPro": false,
        "fullname": "Qingcheng Zeng",
        "user": "qcz",
        "type": "user"
      },
      "summary": "The social impact of Natural Language Processing (NLP) is increasingly\nimportant, with a rising community focus on initiatives related to NLP for\nSocial Good (NLP4SG). Indeed, in recent years, almost 20% of all papers in the\nACL Anthology address topics related to social good as defined by the UN\nSustainable Development Goals (Adauto et al., 2023). In this study, we take an\nauthor- and venue-level perspective to map the landscape of NLP4SG, quantifying\nthe proportion of work addressing social good concerns both within and beyond\nthe ACL community, by both core ACL contributors and non-ACL authors. With this\napproach we discover two surprising facts about the landscape of NLP4SG. First,\nACL authors are dramatically more likely to do work addressing social good\nconcerns when publishing in venues outside of ACL. Second, the vast majority of\npublications using NLP techniques to address concerns of social good are done\nby non-ACL authors in venues outside of ACL. We discuss the implications of\nthese findings on agenda-setting considerations for the ACL community related\nto NLP4SG.",
      "upvotes": 3,
      "discussionId": "68e478b7e4e093a7044e4d02",
      "ai_summary": "The study reveals that ACL authors are more likely to address social good concerns in non-ACL venues, and most NLP4SG publications are from non-ACL authors.",
      "ai_keywords": [
        "Natural Language Processing",
        "NLP for Social Good",
        "NLP4SG",
        "ACL Anthology",
        "UN Sustainable Development Goals",
        "author-level perspective",
        "venue-level perspective",
        "core ACL contributors",
        "non-ACL authors",
        "agenda-setting considerations"
      ]
    },
    "publishedAt": "2025-10-05T22:04:42.000Z",
    "title": "Good Intentions Beyond ACL: Who Does NLP for Social Good, and Where?",
    "summary": "The social impact of Natural Language Processing (NLP) is increasingly\nimportant, with a rising community focus on initiatives related to NLP for\nSocial Good (NLP4SG). Indeed, in recent years, almost 20% of all papers in the\nACL Anthology address topics related to social good as defined by the UN\nSustainable Development Goals (Adauto et al., 2023). In this study, we take an\nauthor- and venue-level perspective to map the landscape of NLP4SG, quantifying\nthe proportion of work addressing social good concerns both within and beyond\nthe ACL community, by both core ACL contributors and non-ACL authors. With this\napproach we discover two surprising facts about the landscape of NLP4SG. First,\nACL authors are dramatically more likely to do work addressing social good\nconcerns when publishing in venues outside of ACL. Second, the vast majority of\npublications using NLP techniques to address concerns of social good are done\nby non-ACL authors in venues outside of ACL. We discuss the implications of\nthese findings on agenda-setting considerations for the ACL community related\nto NLP4SG.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.04434.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63bc77661374e3ef9135735f",
      "avatarUrl": "/avatars/94b04545ed9d30bfe58691672a0b5618.svg",
      "fullname": "Qingcheng Zeng",
      "name": "qcz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.00732",
      "authors": [
        {
          "_id": "68e0764f73e20ab577841bcb",
          "name": "Yuchen Tian",
          "hidden": false
        },
        {
          "_id": "68e0764f73e20ab577841bcc",
          "name": "Ruiyuan Huang",
          "hidden": false
        },
        {
          "_id": "68e0764f73e20ab577841bcd",
          "name": "Xuanwu Wang",
          "hidden": false
        },
        {
          "_id": "68e0764f73e20ab577841bce",
          "name": "Jing Ma",
          "hidden": false
        },
        {
          "_id": "68e0764f73e20ab577841bcf",
          "name": "Zengfeng Huang",
          "hidden": false
        },
        {
          "_id": "68e0764f73e20ab577841bd0",
          "user": {
            "_id": "6090ff099a8bcaa437b234a4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6090ff099a8bcaa437b234a4/iUvw7JXT-ngI7rGk1x-io.jpeg",
            "isPro": false,
            "fullname": "Ziyang Luo",
            "user": "Ziyang",
            "type": "user"
          },
          "name": "Ziyang Luo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-05T12:45:56.245Z",
          "hidden": false
        },
        {
          "_id": "68e0764f73e20ab577841bd1",
          "user": {
            "_id": "6499466c7d1edf7cb612a9a6",
            "avatarUrl": "/avatars/c2e18594aa0879db8226f2a04496fb0b.svg",
            "isPro": false,
            "fullname": "Hongzhan Lin",
            "user": "danielhzlin",
            "type": "user"
          },
          "name": "Hongzhan Lin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-05T12:45:53.371Z",
          "hidden": false
        },
        {
          "_id": "68e0764f73e20ab577841bd2",
          "name": "Da Zheng",
          "hidden": false
        },
        {
          "_id": "68e0764f73e20ab577841bd3",
          "name": "Lun Du",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6499466c7d1edf7cb612a9a6/aXVxfwkFI28iYIZ0T2LQK.png"
      ],
      "publishedAt": "2025-10-01T10:15:27.000Z",
      "submittedOnDailyAt": "2025-10-07T03:25:48.318Z",
      "title": "EvolProver: Advancing Automated Theorem Proving by Evolving Formalized\n  Problems via Symmetry and Difficulty",
      "submittedOnDailyBy": {
        "_id": "6499466c7d1edf7cb612a9a6",
        "avatarUrl": "/avatars/c2e18594aa0879db8226f2a04496fb0b.svg",
        "isPro": false,
        "fullname": "Hongzhan Lin",
        "user": "danielhzlin",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) for formal theorem proving have shown\nsignificant promise, yet they often lack generalizability and are fragile to\neven minor transformations of problem statements. To address this limitation,\nwe introduce a novel data augmentation pipeline designed to enhance model\nrobustness from two perspectives: symmetry and difficulty. From the symmetry\nperspective, we propose two complementary methods: EvolAST, an Abstract Syntax\nTree (AST) based approach that targets syntactic symmetry to generate\nsemantically equivalent problem variants, and EvolDomain, which leverages LLMs\nto address semantic symmetry by translating theorems across mathematical\ndomains. From the difficulty perspective, we propose EvolDifficulty, which uses\ncarefully designed evolutionary instructions to guide LLMs in generating new\ntheorems with a wider range of difficulty. We then use the evolved data to\ntrain EvolProver, a 7B-parameter non-reasoning theorem prover. EvolProver\nestablishes a new state-of-the-art (SOTA) on FormalMATH-Lite with a 53.8%\npass@32 rate, surpassing all models of comparable size, including\nreasoning-based models. It also sets new SOTA records for non-reasoning models\non MiniF2F-Test (69.8% pass@32), Ineq-Comp-Seed (52.2% pass@32), and\nIneq-Comp-Transformed (34.0% pass@32). Ablation studies further confirm our\ndata augmentation pipeline's effectiveness across multiple benchmarks.",
      "upvotes": 3,
      "discussionId": "68e0764f73e20ab577841bd4",
      "ai_summary": "A novel data augmentation pipeline enhances the robustness and generalizability of large language models for formal theorem proving by addressing syntactic and semantic symmetry and varying difficulty levels, leading to state-of-the-art performance on multiple benchmarks.",
      "ai_keywords": [
        "Large Language Models",
        "formal theorem proving",
        "data augmentation",
        "Abstract Syntax Tree",
        "EvolAST",
        "EvolDomain",
        "EvolDifficulty",
        "EvolProver",
        "FormalMATH-Lite",
        "MiniF2F-Test",
        "Ineq-Comp-Seed",
        "Ineq-Comp-Transformed"
      ]
    },
    "publishedAt": "2025-10-01T06:15:27.000Z",
    "title": "EvolProver: Advancing Automated Theorem Proving by Evolving Formalized\n  Problems via Symmetry and Difficulty",
    "summary": "Large Language Models (LLMs) for formal theorem proving have shown\nsignificant promise, yet they often lack generalizability and are fragile to\neven minor transformations of problem statements. To address this limitation,\nwe introduce a novel data augmentation pipeline designed to enhance model\nrobustness from two perspectives: symmetry and difficulty. From the symmetry\nperspective, we propose two complementary methods: EvolAST, an Abstract Syntax\nTree (AST) based approach that targets syntactic symmetry to generate\nsemantically equivalent problem variants, and EvolDomain, which leverages LLMs\nto address semantic symmetry by translating theorems across mathematical\ndomains. From the difficulty perspective, we propose EvolDifficulty, which uses\ncarefully designed evolutionary instructions to guide LLMs in generating new\ntheorems with a wider range of difficulty. We then use the evolved data to\ntrain EvolProver, a 7B-parameter non-reasoning theorem prover. EvolProver\nestablishes a new state-of-the-art (SOTA) on FormalMATH-Lite with a 53.8%\npass@32 rate, surpassing all models of comparable size, including\nreasoning-based models. It also sets new SOTA records for non-reasoning models\non MiniF2F-Test (69.8% pass@32), Ineq-Comp-Seed (52.2% pass@32), and\nIneq-Comp-Transformed (34.0% pass@32). Ablation studies further confirm our\ndata augmentation pipeline's effectiveness across multiple benchmarks.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6499466c7d1edf7cb612a9a6/aXVxfwkFI28iYIZ0T2LQK.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.00732.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6499466c7d1edf7cb612a9a6",
      "avatarUrl": "/avatars/c2e18594aa0879db8226f2a04496fb0b.svg",
      "fullname": "Hongzhan Lin",
      "name": "danielhzlin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.03264",
      "authors": [
        {
          "_id": "68e473a0e4e093a7044e4cd7",
          "name": "Syeda Nahida Akter",
          "hidden": false
        },
        {
          "_id": "68e473a0e4e093a7044e4cd8",
          "name": "Shrimai Prabhumoye",
          "hidden": false
        },
        {
          "_id": "68e473a0e4e093a7044e4cd9",
          "name": "Eric Nyberg",
          "hidden": false
        },
        {
          "_id": "68e473a0e4e093a7044e4cda",
          "name": "Mostofa Patwary",
          "hidden": false
        },
        {
          "_id": "68e473a0e4e093a7044e4cdb",
          "name": "Mohammad Shoeybi",
          "hidden": false
        },
        {
          "_id": "68e473a0e4e093a7044e4cdc",
          "name": "Yejin Choi",
          "hidden": false
        },
        {
          "_id": "68e473a0e4e093a7044e4cdd",
          "name": "Bryan Catanzaro",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-26T20:08:51.000Z",
      "submittedOnDailyAt": "2025-10-07T00:30:07.282Z",
      "title": "Front-Loading Reasoning: The Synergy between Pretraining and\n  Post-Training Data",
      "submittedOnDailyBy": {
        "_id": "66980b9c9baa4382e1678809",
        "avatarUrl": "/avatars/1a516bb7aa7871834c19de708cdd853a.svg",
        "isPro": false,
        "fullname": "Shrimai Prabhumoye",
        "user": "shrimai19",
        "type": "user"
      },
      "summary": "The prevailing paradigm for enhancing the reasoning abilities of LLMs\nrevolves around post-training on high-quality, reasoning-intensive data. While\nemerging literature suggests that reasoning data is increasingly incorporated\nalso during the mid-training stage-a practice that is relatively more\nproprietary and less openly characterized-the role of such data in pretraining\nremains unclear. In particular, due to the opaqueness of pretraining corpora in\nmost frontier models, the effect of reasoning data introduced at different\nphases of pre- and/or post-training is relatively less reported in the\nscientific literature. This raises several important questions: Is adding\nreasoning data earlier during pretraining any better than introducing it during\npost-training? Could earlier inclusion risk overfitting and harm\ngeneralization, or instead establish durable foundations that later fine-tuning\ncannot recover? We conduct the first systematic study of how reasoning\ndata-varying in scale, diversity, and quality-affects LLM performance when\nintroduced at different stages of training. We find that front-loading\nreasoning data into pretraining is critical (19% avg gain), establishing\nfoundational capabilities that cannot be fully replicated by later-stage SFT,\neven with more data. We uncover an asymmetric principle for optimal data\nallocation: pretraining benefits most from broad diversity in reasoning\npatterns (11% avg gain), while SFT is more sensitive to data quality (15% avg\ngain). We show that high-quality pretraining data has latent effects, activated\nonly after SFT, and that naively scaling SFT data can be detrimental, washing\naway the benefits of early reasoning injection. Our results challenge the\nconventional separation of language modeling and reasoning, providing a\nprincipled guide for strategically allocating data across the entire training\npipeline to build more capable models.",
      "upvotes": 3,
      "discussionId": "68e473f1e4e093a7044e4cde",
      "ai_summary": "Introducing reasoning data during pretraining significantly enhances LLM performance compared to post-training, with pretraining benefiting more from diverse data patterns while SFT benefits more from high-quality data.",
      "ai_keywords": [
        "LLMs",
        "reasoning data",
        "pretraining",
        "post-training",
        "mid-training",
        "overfitting",
        "generalization",
        "systematic study",
        "SFT",
        "data allocation",
        "language modeling",
        "reasoning"
      ],
      "organization": {
        "_id": "60262b67268c201cdc8b7d43",
        "name": "nvidia",
        "fullname": "NVIDIA",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
      }
    },
    "publishedAt": "2025-09-26T16:08:51.000Z",
    "title": "Front-Loading Reasoning: The Synergy between Pretraining and\n  Post-Training Data",
    "summary": "The prevailing paradigm for enhancing the reasoning abilities of LLMs\nrevolves around post-training on high-quality, reasoning-intensive data. While\nemerging literature suggests that reasoning data is increasingly incorporated\nalso during the mid-training stage-a practice that is relatively more\nproprietary and less openly characterized-the role of such data in pretraining\nremains unclear. In particular, due to the opaqueness of pretraining corpora in\nmost frontier models, the effect of reasoning data introduced at different\nphases of pre- and/or post-training is relatively less reported in the\nscientific literature. This raises several important questions: Is adding\nreasoning data earlier during pretraining any better than introducing it during\npost-training? Could earlier inclusion risk overfitting and harm\ngeneralization, or instead establish durable foundations that later fine-tuning\ncannot recover? We conduct the first systematic study of how reasoning\ndata-varying in scale, diversity, and quality-affects LLM performance when\nintroduced at different stages of training. We find that front-loading\nreasoning data into pretraining is critical (19% avg gain), establishing\nfoundational capabilities that cannot be fully replicated by later-stage SFT,\neven with more data. We uncover an asymmetric principle for optimal data\nallocation: pretraining benefits most from broad diversity in reasoning\npatterns (11% avg gain), while SFT is more sensitive to data quality (15% avg\ngain). We show that high-quality pretraining data has latent effects, activated\nonly after SFT, and that naively scaling SFT data can be detrimental, washing\naway the benefits of early reasoning injection. Our results challenge the\nconventional separation of language modeling and reasoning, providing a\nprincipled guide for strategically allocating data across the entire training\npipeline to build more capable models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.03264.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66980b9c9baa4382e1678809",
      "avatarUrl": "/avatars/1a516bb7aa7871834c19de708cdd853a.svg",
      "fullname": "Shrimai Prabhumoye",
      "name": "shrimai19",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "organization": {
      "_id": "60262b67268c201cdc8b7d43",
      "name": "nvidia",
      "fullname": "NVIDIA",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.04860",
      "authors": [
        {
          "_id": "68e4a0f8e4e093a7044e4dd7",
          "name": "Siwei Han",
          "hidden": false
        },
        {
          "_id": "68e4a0f8e4e093a7044e4dd8",
          "name": "Jiaqi Liu",
          "hidden": false
        },
        {
          "_id": "68e4a0f8e4e093a7044e4dd9",
          "name": "Yaofeng Su",
          "hidden": false
        },
        {
          "_id": "68e4a0f8e4e093a7044e4dda",
          "name": "Wenbo Duan",
          "hidden": false
        },
        {
          "_id": "68e4a0f8e4e093a7044e4ddb",
          "name": "Xinyuan Liu",
          "hidden": false
        },
        {
          "_id": "68e4a0f8e4e093a7044e4ddc",
          "name": "Cihang Xie",
          "hidden": false
        },
        {
          "_id": "68e4a0f8e4e093a7044e4ddd",
          "name": "Mohit Bansal",
          "hidden": false
        },
        {
          "_id": "68e4a0f8e4e093a7044e4dde",
          "name": "Mingyu Ding",
          "hidden": false
        },
        {
          "_id": "68e4a0f8e4e093a7044e4ddf",
          "name": "Linjun Zhang",
          "hidden": false
        },
        {
          "_id": "68e4a0f8e4e093a7044e4de0",
          "name": "Huaxiu Yao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-06T14:48:39.000Z",
      "submittedOnDailyAt": "2025-10-07T03:44:58.692Z",
      "title": "Alignment Tipping Process: How Self-Evolution Pushes LLM Agents Off the\n  Rails",
      "submittedOnDailyBy": {
        "_id": "65941852f0152a21fc860f79",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65941852f0152a21fc860f79/mHOkrMOkivxxK__F4AK2o.png",
        "isPro": false,
        "fullname": "Lillianwei",
        "user": "Lillianwei",
        "type": "user"
      },
      "summary": "As Large Language Model (LLM) agents increasingly gain self-evolutionary\ncapabilities to adapt and refine their strategies through real-world\ninteraction, their long-term reliability becomes a critical concern. We\nidentify the Alignment Tipping Process (ATP), a critical post-deployment risk\nunique to self-evolving LLM agents. Unlike training-time failures, ATP arises\nwhen continual interaction drives agents to abandon alignment constraints\nestablished during training in favor of reinforced, self-interested strategies.\nWe formalize and analyze ATP through two complementary paradigms:\nSelf-Interested Exploration, where repeated high-reward deviations induce\nindividual behavioral drift, and Imitative Strategy Diffusion, where deviant\nbehaviors spread across multi-agent systems. Building on these paradigms, we\nconstruct controllable testbeds and benchmark Qwen3-8B and\nLlama-3.1-8B-Instruct. Our experiments show that alignment benefits erode\nrapidly under self-evolution, with initially aligned models converging toward\nunaligned states. In multi-agent settings, successful violations diffuse\nquickly, leading to collective misalignment. Moreover, current reinforcement\nlearning-based alignment methods provide only fragile defenses against\nalignment tipping. Together, these findings demonstrate that alignment of LLM\nagents is not a static property but a fragile and dynamic one, vulnerable to\nfeedback-driven decay during deployment. Our data and code are available at\nhttps://github.com/aiming-lab/ATP.",
      "upvotes": 2,
      "discussionId": "68e4a0f8e4e093a7044e4de1",
      "githubRepo": "https://github.com/aiming-lab/ATP",
      "ai_summary": "Self-evolving LLM agents can abandon alignment constraints post-deployment, leading to rapid misalignment and collective failure in multi-agent systems.",
      "ai_keywords": [
        "Alignment Tipping Process",
        "Self-Interested Exploration",
        "Imitative Strategy Diffusion",
        "reinforcement learning-based alignment methods"
      ],
      "githubStars": 2
    },
    "publishedAt": "2025-10-06T10:48:39.000Z",
    "title": "Alignment Tipping Process: How Self-Evolution Pushes LLM Agents Off the\n  Rails",
    "summary": "As Large Language Model (LLM) agents increasingly gain self-evolutionary\ncapabilities to adapt and refine their strategies through real-world\ninteraction, their long-term reliability becomes a critical concern. We\nidentify the Alignment Tipping Process (ATP), a critical post-deployment risk\nunique to self-evolving LLM agents. Unlike training-time failures, ATP arises\nwhen continual interaction drives agents to abandon alignment constraints\nestablished during training in favor of reinforced, self-interested strategies.\nWe formalize and analyze ATP through two complementary paradigms:\nSelf-Interested Exploration, where repeated high-reward deviations induce\nindividual behavioral drift, and Imitative Strategy Diffusion, where deviant\nbehaviors spread across multi-agent systems. Building on these paradigms, we\nconstruct controllable testbeds and benchmark Qwen3-8B and\nLlama-3.1-8B-Instruct. Our experiments show that alignment benefits erode\nrapidly under self-evolution, with initially aligned models converging toward\nunaligned states. In multi-agent settings, successful violations diffuse\nquickly, leading to collective misalignment. Moreover, current reinforcement\nlearning-based alignment methods provide only fragile defenses against\nalignment tipping. Together, these findings demonstrate that alignment of LLM\nagents is not a static property but a fragile and dynamic one, vulnerable to\nfeedback-driven decay during deployment. Our data and code are available at\nhttps://github.com/aiming-lab/ATP.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.04860.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65941852f0152a21fc860f79",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65941852f0152a21fc860f79/mHOkrMOkivxxK__F4AK2o.png",
      "fullname": "Lillianwei",
      "name": "Lillianwei",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.04016",
      "authors": [
        {
          "_id": "68e489e6e4e093a7044e4d84",
          "name": "Thanapol Popit",
          "hidden": false
        },
        {
          "_id": "68e489e6e4e093a7044e4d85",
          "name": "Natthapath Rungseesiripak",
          "hidden": false
        },
        {
          "_id": "68e489e6e4e093a7044e4d86",
          "name": "Monthol Charattrakool",
          "hidden": false
        },
        {
          "_id": "68e489e6e4e093a7044e4d87",
          "name": "Saksorn Ruangtanusak",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62243664af5df9d9e5582f67/nwxLd46vqAlIWtnbCCMq4.png",
        "https://cdn-uploads.huggingface.co/production/uploads/62243664af5df9d9e5582f67/_5x2_DPHnetOLRjFq8bjO.png",
        "https://cdn-uploads.huggingface.co/production/uploads/62243664af5df9d9e5582f67/qO5lgPDmO-P68U8SFcKN9.png"
      ],
      "publishedAt": "2025-10-05T03:31:59.000Z",
      "submittedOnDailyAt": "2025-10-07T02:06:11.660Z",
      "title": "Thai Semantic End-of-Turn Detection for Real-Time Voice Agents",
      "submittedOnDailyBy": {
        "_id": "62243664af5df9d9e5582f67",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62243664af5df9d9e5582f67/nAntUd0NVDcMYtwiunCU8.jpeg",
        "isPro": false,
        "fullname": "Saksorn Ruangtanusak",
        "user": "saksornr",
        "type": "user"
      },
      "summary": "Fluid voice-to-voice interaction requires reliable and low-latency detection\nof when a user has finished speaking. Traditional audio-silence end-pointers\nadd hundreds of milliseconds of delay and fail under hesitations or\nlanguage-specific phenomena. We present, to our knowledge, the first systematic\nstudy of Thai text-only end-of-turn (EOT) detection for real-time agents. We\ncompare zero-shot and few-shot prompting of compact LLMs to supervised\nfine-tuning of lightweight transformers. Using transcribed subtitles from the\nYODAS corpus and Thai-specific linguistic cues (e.g., sentence-final\nparticles), we formulate EOT as a binary decision over token boundaries. We\nreport a clear accuracy-latency tradeoff and provide a public-ready\nimplementation plan. This work establishes a Thai baseline and demonstrates\nthat small, fine-tuned models can deliver near-instant EOT decisions suitable\nfor on-device agents.",
      "upvotes": 2,
      "discussionId": "68e489e6e4e093a7044e4d88",
      "ai_summary": "Real-time Thai text-only end-of-turn detection using zero-shot and few-shot prompting of compact LLMs and lightweight transformers achieves near-instant accuracy suitable for on-device agents.",
      "ai_keywords": [
        "zero-shot prompting",
        "few-shot prompting",
        "compact LLMs",
        "lightweight transformers",
        "end-of-turn detection",
        "token boundaries"
      ]
    },
    "publishedAt": "2025-10-04T23:31:59.000Z",
    "title": "Thai Semantic End-of-Turn Detection for Real-Time Voice Agents",
    "summary": "Fluid voice-to-voice interaction requires reliable and low-latency detection\nof when a user has finished speaking. Traditional audio-silence end-pointers\nadd hundreds of milliseconds of delay and fail under hesitations or\nlanguage-specific phenomena. We present, to our knowledge, the first systematic\nstudy of Thai text-only end-of-turn (EOT) detection for real-time agents. We\ncompare zero-shot and few-shot prompting of compact LLMs to supervised\nfine-tuning of lightweight transformers. Using transcribed subtitles from the\nYODAS corpus and Thai-specific linguistic cues (e.g., sentence-final\nparticles), we formulate EOT as a binary decision over token boundaries. We\nreport a clear accuracy-latency tradeoff and provide a public-ready\nimplementation plan. This work establishes a Thai baseline and demonstrates\nthat small, fine-tuned models can deliver near-instant EOT decisions suitable\nfor on-device agents.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62243664af5df9d9e5582f67/nwxLd46vqAlIWtnbCCMq4.png",
      "https://cdn-uploads.huggingface.co/production/uploads/62243664af5df9d9e5582f67/_5x2_DPHnetOLRjFq8bjO.png",
      "https://cdn-uploads.huggingface.co/production/uploads/62243664af5df9d9e5582f67/qO5lgPDmO-P68U8SFcKN9.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.04016.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62243664af5df9d9e5582f67",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62243664af5df9d9e5582f67/nAntUd0NVDcMYtwiunCU8.jpeg",
      "fullname": "Saksorn Ruangtanusak",
      "name": "saksornr",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 16
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.24613",
      "authors": [
        {
          "_id": "68de41e4a9129e507bf4a4a1",
          "user": {
            "_id": "66976a7fa7fd582ae754850e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66976a7fa7fd582ae754850e/jQXfIqKh1FYGxjyFwSmSY.jpeg",
            "isPro": false,
            "fullname": "Gio Paik",
            "user": "skyil7",
            "type": "user"
          },
          "name": "Gio Paik",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-06T12:49:08.517Z",
          "hidden": false
        },
        {
          "_id": "68de41e4a9129e507bf4a4a2",
          "name": "Yongbeom Kim",
          "hidden": false
        },
        {
          "_id": "68de41e4a9129e507bf4a4a3",
          "name": "Soungmin Lee",
          "hidden": false
        },
        {
          "_id": "68de41e4a9129e507bf4a4a4",
          "name": "Sangmin Ahn",
          "hidden": false
        },
        {
          "_id": "68de41e4a9129e507bf4a4a5",
          "name": "Chanwoo Kim",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-29T11:18:13.000Z",
      "submittedOnDailyAt": "2025-10-07T01:49:42.028Z",
      "title": "HiKE: Hierarchical Evaluation Framework for Korean-English\n  Code-Switching Speech Recognition",
      "submittedOnDailyBy": {
        "_id": "66976a7fa7fd582ae754850e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66976a7fa7fd582ae754850e/jQXfIqKh1FYGxjyFwSmSY.jpeg",
        "isPro": false,
        "fullname": "Gio Paik",
        "user": "skyil7",
        "type": "user"
      },
      "summary": "Despite advances in multilingual automatic speech recognition (ASR),\ncode-switching (CS), the mixing of languages within an utterance common in\ndaily speech, remains a severely underexplored challenge. In this paper, we\nintroduce HiKE: the Hierarchical Korean-English code-switching benchmark, the\nfirst globally accessible evaluation framework for Korean-English CS, aiming to\nprovide a means for the precise evaluation of multilingual ASR models and to\nfoster research in the field. The proposed framework not only consists of\nhigh-quality, natural CS data across various topics, but also provides\nmeticulous loanword labels and a hierarchical CS-level labeling scheme (word,\nphrase, and sentence) that together enable a systematic evaluation of a model's\nability to handle each distinct level of code-switching. Through evaluations of\ndiverse multilingual ASR models and fine-tuning experiments, this paper\ndemonstrates that while most multilingual ASR models initially struggle with\nCS-ASR, this capability can be enabled through fine-tuning with CS data. HiKE\nwill be available at https://github.com/ThetaOne-AI/HiKE.",
      "upvotes": 2,
      "discussionId": "68de41e4a9129e507bf4a4a6",
      "githubRepo": "https://github.com/ThetaOne-AI/HiKE",
      "ai_summary": "A hierarchical benchmark for Korean-English code-switching in ASR evaluates model performance and demonstrates improvement through fine-tuning with code-switched data.",
      "ai_keywords": [
        "multilingual automatic speech recognition",
        "code-switching",
        "HiKE",
        "Hierarchical Korean-English code-switching benchmark",
        "loanword labels",
        "hierarchical CS-level labeling scheme",
        "fine-tuning"
      ],
      "githubStars": 2,
      "organization": {
        "_id": "68b1775f3eded3884d27cbd3",
        "name": "thetaone-ai",
        "fullname": "Theta One AI",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66976a7fa7fd582ae754850e/4r_irltP9kCKVSVzlOj2j.jpeg"
      }
    },
    "publishedAt": "2025-09-29T07:18:13.000Z",
    "title": "HiKE: Hierarchical Evaluation Framework for Korean-English\n  Code-Switching Speech Recognition",
    "summary": "Despite advances in multilingual automatic speech recognition (ASR),\ncode-switching (CS), the mixing of languages within an utterance common in\ndaily speech, remains a severely underexplored challenge. In this paper, we\nintroduce HiKE: the Hierarchical Korean-English code-switching benchmark, the\nfirst globally accessible evaluation framework for Korean-English CS, aiming to\nprovide a means for the precise evaluation of multilingual ASR models and to\nfoster research in the field. The proposed framework not only consists of\nhigh-quality, natural CS data across various topics, but also provides\nmeticulous loanword labels and a hierarchical CS-level labeling scheme (word,\nphrase, and sentence) that together enable a systematic evaluation of a model's\nability to handle each distinct level of code-switching. Through evaluations of\ndiverse multilingual ASR models and fine-tuning experiments, this paper\ndemonstrates that while most multilingual ASR models initially struggle with\nCS-ASR, this capability can be enabled through fine-tuning with CS data. HiKE\nwill be available at https://github.com/ThetaOne-AI/HiKE.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.24613.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66976a7fa7fd582ae754850e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66976a7fa7fd582ae754850e/jQXfIqKh1FYGxjyFwSmSY.jpeg",
      "fullname": "Gio Paik",
      "name": "skyil7",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "68b1775f3eded3884d27cbd3",
      "name": "thetaone-ai",
      "fullname": "Theta One AI",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66976a7fa7fd582ae754850e/4r_irltP9kCKVSVzlOj2j.jpeg"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.04399",
      "authors": [
        {
          "_id": "68e46e6ce4e093a7044e4cba",
          "name": "Charles L. Wang",
          "hidden": false
        },
        {
          "_id": "68e46e6ce4e093a7044e4cbb",
          "name": "Keir Dorchen",
          "hidden": false
        },
        {
          "_id": "68e46e6ce4e093a7044e4cbc",
          "name": "Peter Jin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-05T23:52:16.000Z",
      "submittedOnDailyAt": "2025-10-07T00:09:07.390Z",
      "title": "Utility-Learning Tension in Self-Modifying Agents",
      "submittedOnDailyBy": {
        "_id": "67c604ff3f8c7b027d479671",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67c604ff3f8c7b027d479671/IhfN3eWRQ8iNfcsDKA1Ep.png",
        "isPro": false,
        "fullname": "Charles Wang",
        "user": "charleslwang",
        "type": "user"
      },
      "summary": "As systems trend toward superintelligence, a natural modeling premise is that\nagents can self-improve along every facet of their own design. We formalize\nthis with a five-axis decomposition and a decision layer, separating incentives\nfrom learning behavior and analyzing axes in isolation. Our central result\nidentifies and introduces a sharp utility--learning tension, the structural\nconflict in self-modifying systems whereby utility-driven changes that improve\nimmediate or expected performance can also erode the statistical preconditions\nfor reliable learning and generalization. Our findings show that\ndistribution-free guarantees are preserved iff the policy-reachable model\nfamily is uniformly capacity-bounded; when capacity can grow without limit,\nutility-rational self-changes can render learnable tasks unlearnable. Under\nstandard assumptions common in practice, these axes reduce to the same capacity\ncriterion, yielding a single boundary for safe self-modification. Numerical\nexperiments across several axes validate the theory by comparing destructive\nutility policies against our proposed two-gate policies that preserve\nlearnability.",
      "upvotes": 1,
      "discussionId": "68e46e6ce4e093a7044e4cbd",
      "ai_summary": "Self-improving systems face a utility-learning tension that can degrade their ability to learn and generalize, requiring capacity bounds to ensure safe self-modification.",
      "ai_keywords": [
        "self-improving systems",
        "utility--learning tension",
        "statistical preconditions",
        "reliable learning",
        "generalization",
        "distribution-free guarantees",
        "policy-reachable model family",
        "uniformly capacity-bounded",
        "utility-rational self-changes",
        "learnable tasks",
        "two-gate policies"
      ]
    },
    "publishedAt": "2025-10-05T19:52:16.000Z",
    "title": "Utility-Learning Tension in Self-Modifying Agents",
    "summary": "As systems trend toward superintelligence, a natural modeling premise is that\nagents can self-improve along every facet of their own design. We formalize\nthis with a five-axis decomposition and a decision layer, separating incentives\nfrom learning behavior and analyzing axes in isolation. Our central result\nidentifies and introduces a sharp utility--learning tension, the structural\nconflict in self-modifying systems whereby utility-driven changes that improve\nimmediate or expected performance can also erode the statistical preconditions\nfor reliable learning and generalization. Our findings show that\ndistribution-free guarantees are preserved iff the policy-reachable model\nfamily is uniformly capacity-bounded; when capacity can grow without limit,\nutility-rational self-changes can render learnable tasks unlearnable. Under\nstandard assumptions common in practice, these axes reduce to the same capacity\ncriterion, yielding a single boundary for safe self-modification. Numerical\nexperiments across several axes validate the theory by comparing destructive\nutility policies against our proposed two-gate policies that preserve\nlearnability.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.04399.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67c604ff3f8c7b027d479671",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67c604ff3f8c7b027d479671/IhfN3eWRQ8iNfcsDKA1Ep.png",
      "fullname": "Charles Wang",
      "name": "charleslwang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.04226",
      "authors": [
        {
          "_id": "68e4a947e4e093a7044e4de3",
          "name": "Dustin Wright",
          "hidden": false
        },
        {
          "_id": "68e4a947e4e093a7044e4de4",
          "name": "Sarah Masud",
          "hidden": false
        },
        {
          "_id": "68e4a947e4e093a7044e4de5",
          "name": "Jared Moore",
          "hidden": false
        },
        {
          "_id": "68e4a947e4e093a7044e4de6",
          "name": "Srishti Yadav",
          "hidden": false
        },
        {
          "_id": "68e4a947e4e093a7044e4de7",
          "name": "Maria Antoniak",
          "hidden": false
        },
        {
          "_id": "68e4a947e4e093a7044e4de8",
          "name": "Chan Young Park",
          "hidden": false
        },
        {
          "_id": "68e4a947e4e093a7044e4de9",
          "name": "Isabelle Augenstein",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/60a643b9213fe60589b8fdf9/ZXznZbmWo_pnLPjJByPD1.png"
      ],
      "publishedAt": "2025-10-05T14:29:15.000Z",
      "submittedOnDailyAt": "2025-10-07T04:20:37.273Z",
      "title": "Epistemic Diversity and Knowledge Collapse in Large Language Models",
      "submittedOnDailyBy": {
        "_id": "60a643b9213fe60589b8fdf9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60a643b9213fe60589b8fdf9/OOXmW3MkSf88r63tAE6-n.jpeg",
        "isPro": false,
        "fullname": "Dustin Wright",
        "user": "dwright37",
        "type": "user"
      },
      "summary": "Large language models (LLMs) tend to generate lexically, semantically, and\nstylistically homogenous texts. This poses a risk of knowledge collapse, where\nhomogenous LLMs mediate a shrinking in the range of accessible information over\ntime. Existing works on homogenization are limited by a focus on closed-ended\nmultiple-choice setups or fuzzy semantic features, and do not look at trends\nacross time and cultural contexts. To overcome this, we present a new\nmethodology to measure epistemic diversity, i.e., variation in real-world\nclaims in LLM outputs, which we use to perform a broad empirical study of LLM\nknowledge collapse. We test 27 LLMs, 155 topics covering 12 countries, and 200\nprompt variations sourced from real user chats. For the topics in our study, we\nshow that while newer models tend to generate more diverse claims, nearly all\nmodels are less epistemically diverse than a basic web search. We find that\nmodel size has a negative impact on epistemic diversity, while\nretrieval-augmented generation (RAG) has a positive impact, though the\nimprovement from RAG varies by the cultural context. Finally, compared to a\ntraditional knowledge source (Wikipedia), we find that country-specific claims\nreflect the English language more than the local one, highlighting a gap in\nepistemic representation",
      "upvotes": 1,
      "discussionId": "68e4a947e4e093a7044e4dea",
      "ai_summary": "A study measures epistemic diversity in LLM outputs, showing that newer models are more diverse but still less so than web searches, and that RAG improves diversity with cultural context variations.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "epistemic diversity",
        "knowledge collapse",
        "retrieval-augmented generation",
        "RAG"
      ],
      "organization": {
        "_id": "60a63ea08b5faf05f3c4d761",
        "name": "copenlu",
        "fullname": "CopeNLU",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1621507708679-608918b7df398c3b285ce960.png"
      }
    },
    "publishedAt": "2025-10-05T10:29:15.000Z",
    "title": "Epistemic Diversity and Knowledge Collapse in Large Language Models",
    "summary": "Large language models (LLMs) tend to generate lexically, semantically, and\nstylistically homogenous texts. This poses a risk of knowledge collapse, where\nhomogenous LLMs mediate a shrinking in the range of accessible information over\ntime. Existing works on homogenization are limited by a focus on closed-ended\nmultiple-choice setups or fuzzy semantic features, and do not look at trends\nacross time and cultural contexts. To overcome this, we present a new\nmethodology to measure epistemic diversity, i.e., variation in real-world\nclaims in LLM outputs, which we use to perform a broad empirical study of LLM\nknowledge collapse. We test 27 LLMs, 155 topics covering 12 countries, and 200\nprompt variations sourced from real user chats. For the topics in our study, we\nshow that while newer models tend to generate more diverse claims, nearly all\nmodels are less epistemically diverse than a basic web search. We find that\nmodel size has a negative impact on epistemic diversity, while\nretrieval-augmented generation (RAG) has a positive impact, though the\nimprovement from RAG varies by the cultural context. Finally, compared to a\ntraditional knowledge source (Wikipedia), we find that country-specific claims\nreflect the English language more than the local one, highlighting a gap in\nepistemic representation",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/60a643b9213fe60589b8fdf9/ZXznZbmWo_pnLPjJByPD1.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.04226.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60a643b9213fe60589b8fdf9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60a643b9213fe60589b8fdf9/OOXmW3MkSf88r63tAE6-n.jpeg",
      "fullname": "Dustin Wright",
      "name": "dwright37",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "organization": {
      "_id": "60a63ea08b5faf05f3c4d761",
      "name": "copenlu",
      "fullname": "CopeNLU",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1621507708679-608918b7df398c3b285ce960.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.04136",
      "authors": [
        {
          "_id": "68e4bc16e4e093a7044e4e34",
          "name": "Umberto Cappellazzo",
          "hidden": false
        },
        {
          "_id": "68e4bc16e4e093a7044e4e35",
          "name": "Minsu Kim",
          "hidden": false
        },
        {
          "_id": "68e4bc16e4e093a7044e4e36",
          "name": "Pingchuan Ma",
          "hidden": false
        },
        {
          "_id": "68e4bc16e4e093a7044e4e37",
          "name": "Honglie Chen",
          "hidden": false
        },
        {
          "_id": "68e4bc16e4e093a7044e4e38",
          "name": "Xubo Liu",
          "hidden": false
        },
        {
          "_id": "68e4bc16e4e093a7044e4e39",
          "name": "Stavros Petridis",
          "hidden": false
        },
        {
          "_id": "68e4bc16e4e093a7044e4e3a",
          "name": "Maja Pantic",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-05T10:34:34.000Z",
      "submittedOnDailyAt": "2025-10-07T05:42:28.022Z",
      "title": "MoME: Mixture of Matryoshka Experts for Audio-Visual Speech Recognition",
      "submittedOnDailyBy": {
        "_id": "64903f017b630c141867877f",
        "avatarUrl": "/avatars/3c7b248baed446ecc2b6adc2c444320d.svg",
        "isPro": false,
        "fullname": "Umberto Cappellazzo",
        "user": "hisoka94",
        "type": "user"
      },
      "summary": "Large language models (LLMs) have recently shown strong potential in\naudio-visual speech recognition (AVSR), but their high computational demands\nand sensitivity to token granularity limit their practicality in\nresource-constrained settings. Token compression methods can reduce inference\ncost, but they require fixing a compression rate in advance and produce a\nsingle fixed-length output, offering no flexibility to balance information\ndensity and efficiency at inference time. Matryoshka representation learning\n(MRL) addresses this by enabling a single model to operate across multiple\ntoken granularities, allowing compression rates to be adjusted dynamically.\nHowever, current MRL-based methods treat each scale independently during\ntraining, limiting cross-scale generalization, robustness at high compression,\nand interpretability. To overcome these limitations, we propose MoME (Mixture\nof Matryoshka Experts), a novel framework that integrates sparse\nMixture-of-Experts (MoE) into MRL-based LLMs for AVSR. MoME augments a frozen\nLLM with top-k routed and shared experts, allowing dynamic capacity allocation\nacross scales and modalities. A shared router promotes consistent expert\nactivation across granularities, enabling compressed sequences to benefit from\nrepresentations learned at lower compression. Experiments on LRS2 and LRS3\ndemonstrate that MoME achieves state-of-the-art performance across AVSR, ASR,\nand VSR tasks, while requiring significantly fewer parameters and maintaining\nrobustness under noise. MoME unifies the adaptability of MRL with the\nefficiency of MoE, offering a scalable and interpretable solution for\nresource-aware speech recognition.",
      "upvotes": 1,
      "discussionId": "68e4bc16e4e093a7044e4e3b",
      "ai_summary": "MoME, a novel framework integrating sparse Mixture-of-Experts into Matryoshka representation learning, enhances audio-visual speech recognition by dynamically adjusting capacity across scales and modalities, achieving state-of-the-art performance with fewer parameters.",
      "ai_keywords": [
        "large language models",
        "audio-visual speech recognition",
        "token compression",
        "Matryoshka representation learning",
        "Mixture-of-Experts",
        "MoME",
        "LRS2",
        "LRS3",
        "ASR",
        "VSR",
        "parameter-efficient",
        "robustness",
        "noise"
      ],
      "organization": {
        "_id": "650987fc2feb9570c5137ac2",
        "name": "ImperialCollegeLondon",
        "fullname": "Imperial College London",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/u6ceSXXV6ldtt0qZOOMJw.jpeg"
      }
    },
    "publishedAt": "2025-10-05T06:34:34.000Z",
    "title": "MoME: Mixture of Matryoshka Experts for Audio-Visual Speech Recognition",
    "summary": "Large language models (LLMs) have recently shown strong potential in\naudio-visual speech recognition (AVSR), but their high computational demands\nand sensitivity to token granularity limit their practicality in\nresource-constrained settings. Token compression methods can reduce inference\ncost, but they require fixing a compression rate in advance and produce a\nsingle fixed-length output, offering no flexibility to balance information\ndensity and efficiency at inference time. Matryoshka representation learning\n(MRL) addresses this by enabling a single model to operate across multiple\ntoken granularities, allowing compression rates to be adjusted dynamically.\nHowever, current MRL-based methods treat each scale independently during\ntraining, limiting cross-scale generalization, robustness at high compression,\nand interpretability. To overcome these limitations, we propose MoME (Mixture\nof Matryoshka Experts), a novel framework that integrates sparse\nMixture-of-Experts (MoE) into MRL-based LLMs for AVSR. MoME augments a frozen\nLLM with top-k routed and shared experts, allowing dynamic capacity allocation\nacross scales and modalities. A shared router promotes consistent expert\nactivation across granularities, enabling compressed sequences to benefit from\nrepresentations learned at lower compression. Experiments on LRS2 and LRS3\ndemonstrate that MoME achieves state-of-the-art performance across AVSR, ASR,\nand VSR tasks, while requiring significantly fewer parameters and maintaining\nrobustness under noise. MoME unifies the adaptability of MRL with the\nefficiency of MoE, offering a scalable and interpretable solution for\nresource-aware speech recognition.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.04136.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64903f017b630c141867877f",
      "avatarUrl": "/avatars/3c7b248baed446ecc2b6adc2c444320d.svg",
      "fullname": "Umberto Cappellazzo",
      "name": "hisoka94",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "organization": {
      "_id": "650987fc2feb9570c5137ac2",
      "name": "ImperialCollegeLondon",
      "fullname": "Imperial College London",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/u6ceSXXV6ldtt0qZOOMJw.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.01586",
      "authors": [
        {
          "_id": "68e49051e4e093a7044e4d9d",
          "name": "Zhenyu Pan",
          "hidden": false
        },
        {
          "_id": "68e49051e4e093a7044e4d9e",
          "name": "Yiting Zhang",
          "hidden": false
        },
        {
          "_id": "68e49051e4e093a7044e4d9f",
          "name": "Zhuo Liu",
          "hidden": false
        },
        {
          "_id": "68e49051e4e093a7044e4da0",
          "name": "Yolo Yunlong Tang",
          "hidden": false
        },
        {
          "_id": "68e49051e4e093a7044e4da1",
          "name": "Zeliang Zhang",
          "hidden": false
        },
        {
          "_id": "68e49051e4e093a7044e4da2",
          "name": "Haozheng Luo",
          "hidden": false
        },
        {
          "_id": "68e49051e4e093a7044e4da3",
          "name": "Yuwei Han",
          "hidden": false
        },
        {
          "_id": "68e49051e4e093a7044e4da4",
          "name": "Jianshu Zhang",
          "hidden": false
        },
        {
          "_id": "68e49051e4e093a7044e4da5",
          "name": "Dennis Wu",
          "hidden": false
        },
        {
          "_id": "68e49051e4e093a7044e4da6",
          "name": "Hong-Yu Chen",
          "hidden": false
        },
        {
          "_id": "68e49051e4e093a7044e4da7",
          "name": "Haoran Lu",
          "hidden": false
        },
        {
          "_id": "68e49051e4e093a7044e4da8",
          "name": "Haoyang Fang",
          "hidden": false
        },
        {
          "_id": "68e49051e4e093a7044e4da9",
          "name": "Manling Li",
          "hidden": false
        },
        {
          "_id": "68e49051e4e093a7044e4daa",
          "name": "Chenliang Xu",
          "hidden": false
        },
        {
          "_id": "68e49051e4e093a7044e4dab",
          "name": "Philip S. Yu",
          "hidden": false
        },
        {
          "_id": "68e49051e4e093a7044e4dac",
          "name": "Han Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-02T02:06:30.000Z",
      "submittedOnDailyAt": "2025-10-07T02:34:02.960Z",
      "title": "AdvEvo-MARL: Shaping Internalized Safety through Adversarial\n  Co-Evolution in Multi-Agent Reinforcement Learning",
      "submittedOnDailyBy": {
        "_id": "669794c5813d96b4eb0b3fd6",
        "avatarUrl": "/avatars/b4e2bb7b07cc281932d783dcbb64f211.svg",
        "isPro": false,
        "fullname": "Zhenyu Pan",
        "user": "zhenyupan",
        "type": "user"
      },
      "summary": "LLM-based multi-agent systems excel at planning, tool use, and role\ncoordination, but their openness and interaction complexity also expose them to\njailbreak, prompt-injection, and adversarial collaboration. Existing defenses\nfall into two lines: (i) self-verification that asks each agent to pre-filter\nunsafe instructions before execution, and (ii) external guard modules that\npolice behaviors. The former often underperforms because a standalone agent\nlacks sufficient capacity to detect cross-agent unsafe chains and\ndelegation-induced risks; the latter increases system overhead and creates a\nsingle-point-of-failure-once compromised, system-wide safety collapses, and\nadding more guards worsens cost and complexity. To solve these challenges, we\npropose AdvEvo-MARL, a co-evolutionary multi-agent reinforcement learning\nframework that internalizes safety into task agents. Rather than relying on\nexternal guards, AdvEvo-MARL jointly optimizes attackers (which synthesize\nevolving jailbreak prompts) and defenders (task agents trained to both\naccomplish their duties and resist attacks) in adversarial learning\nenvironments. To stabilize learning and foster cooperation, we introduce a\npublic baseline for advantage estimation: agents within the same functional\ngroup share a group-level mean-return baseline, enabling lower-variance updates\nand stronger intra-group coordination. Across representative attack scenarios,\nAdvEvo-MARL consistently keeps attack-success rate (ASR) below 20%, whereas\nbaselines reach up to 38.33%, while preserving-and sometimes improving-task\naccuracy (up to +3.67% on reasoning tasks). These results show that safety and\nutility can be jointly improved without relying on extra guard agents or added\nsystem overhead.",
      "upvotes": 1,
      "discussionId": "68e49051e4e093a7044e4dad",
      "ai_summary": "AdvEvo-MARL, a co-evolutionary multi-agent reinforcement learning framework, enhances safety and utility in LLM-based multi-agent systems by internally optimizing task agents against evolving attacks without additional overhead.",
      "ai_keywords": [
        "multi-agent systems",
        "planning",
        "tool use",
        "role coordination",
        "jailbreak",
        "prompt-injection",
        "adversarial collaboration",
        "self-verification",
        "external guard modules",
        "co-evolutionary",
        "reinforcement learning",
        "attackers",
        "defenders",
        "adversarial learning",
        "advantage estimation",
        "group-level mean-return baseline",
        "attack-success rate",
        "task accuracy",
        "reasoning tasks"
      ]
    },
    "publishedAt": "2025-10-01T22:06:30.000Z",
    "title": "AdvEvo-MARL: Shaping Internalized Safety through Adversarial\n  Co-Evolution in Multi-Agent Reinforcement Learning",
    "summary": "LLM-based multi-agent systems excel at planning, tool use, and role\ncoordination, but their openness and interaction complexity also expose them to\njailbreak, prompt-injection, and adversarial collaboration. Existing defenses\nfall into two lines: (i) self-verification that asks each agent to pre-filter\nunsafe instructions before execution, and (ii) external guard modules that\npolice behaviors. The former often underperforms because a standalone agent\nlacks sufficient capacity to detect cross-agent unsafe chains and\ndelegation-induced risks; the latter increases system overhead and creates a\nsingle-point-of-failure-once compromised, system-wide safety collapses, and\nadding more guards worsens cost and complexity. To solve these challenges, we\npropose AdvEvo-MARL, a co-evolutionary multi-agent reinforcement learning\nframework that internalizes safety into task agents. Rather than relying on\nexternal guards, AdvEvo-MARL jointly optimizes attackers (which synthesize\nevolving jailbreak prompts) and defenders (task agents trained to both\naccomplish their duties and resist attacks) in adversarial learning\nenvironments. To stabilize learning and foster cooperation, we introduce a\npublic baseline for advantage estimation: agents within the same functional\ngroup share a group-level mean-return baseline, enabling lower-variance updates\nand stronger intra-group coordination. Across representative attack scenarios,\nAdvEvo-MARL consistently keeps attack-success rate (ASR) below 20%, whereas\nbaselines reach up to 38.33%, while preserving-and sometimes improving-task\naccuracy (up to +3.67% on reasoning tasks). These results show that safety and\nutility can be jointly improved without relying on extra guard agents or added\nsystem overhead.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.01586.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "669794c5813d96b4eb0b3fd6",
      "avatarUrl": "/avatars/b4e2bb7b07cc281932d783dcbb64f211.svg",
      "fullname": "Zhenyu Pan",
      "name": "zhenyupan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.00507",
      "authors": [
        {
          "_id": "68e48ad7e4e093a7044e4d8a",
          "name": "Yurun Chen",
          "hidden": false
        },
        {
          "_id": "68e48ad7e4e093a7044e4d8b",
          "name": "Xavier Hu",
          "hidden": false
        },
        {
          "_id": "68e48ad7e4e093a7044e4d8c",
          "name": "Yuhan Liu",
          "hidden": false
        },
        {
          "_id": "68e48ad7e4e093a7044e4d8d",
          "name": "Ziqi Wang",
          "hidden": false
        },
        {
          "_id": "68e48ad7e4e093a7044e4d8e",
          "name": "Zeyi Liao",
          "hidden": false
        },
        {
          "_id": "68e48ad7e4e093a7044e4d8f",
          "name": "Lin Chen",
          "hidden": false
        },
        {
          "_id": "68e48ad7e4e093a7044e4d90",
          "name": "Feng Wei",
          "hidden": false
        },
        {
          "_id": "68e48ad7e4e093a7044e4d91",
          "name": "Yuxi Qian",
          "hidden": false
        },
        {
          "_id": "68e48ad7e4e093a7044e4d92",
          "name": "Bo Zheng",
          "hidden": false
        },
        {
          "_id": "68e48ad7e4e093a7044e4d93",
          "name": "Keting Yin",
          "hidden": false
        },
        {
          "_id": "68e48ad7e4e093a7044e4d94",
          "name": "Shengyu Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-01T04:37:54.000Z",
      "submittedOnDailyAt": "2025-10-07T02:17:29.765Z",
      "title": "Graph2Eval: Automatic Multimodal Task Generation for Agents via\n  Knowledge Graphs",
      "submittedOnDailyBy": {
        "_id": "6747285ed34bd6f05080ddda",
        "avatarUrl": "/avatars/ea63de7348aaabc8cff44e76207ba65c.svg",
        "isPro": false,
        "fullname": "Yurun Chen",
        "user": "yurun-chen",
        "type": "user"
      },
      "summary": "As multimodal LLM-driven agents continue to advance in autonomy and\ngeneralization, evaluation based on static datasets can no longer adequately\nassess their true capabilities in dynamic environments and diverse tasks.\nExisting LLM-based synthetic data methods are largely designed for LLM training\nand evaluation, and thus cannot be directly applied to agent tasks that require\ntool use and interactive capabilities. While recent studies have explored\nautomatic agent task generation with LLMs, most efforts remain limited to text\nor image analysis, without systematically modeling multi-step interactions in\nweb environments. To address these challenges, we propose Graph2Eval, a\nknowledge graph-based framework that automatically generates both multimodal\ndocument comprehension tasks and web interaction tasks, enabling comprehensive\nevaluation of agents' reasoning, collaboration, and interactive capabilities.\nIn our approach, knowledge graphs constructed from multi-source external data\nserve as the task space, where we translate semantic relations into structured\nmultimodal tasks using subgraph sampling, task templates, and meta-paths. A\nmulti-stage filtering pipeline based on node reachability, LLM scoring, and\nsimilarity analysis is applied to guarantee the quality and executability of\nthe generated tasks. Furthermore, Graph2Eval supports end-to-end evaluation of\nmultiple agent types (Single-Agent, Multi-Agent, Web Agent) and measures\nreasoning, collaboration, and interaction capabilities. We instantiate the\nframework with Graph2Eval-Bench, a curated dataset of 1,319 tasks spanning\ndocument comprehension and web interaction scenarios. Experiments show that\nGraph2Eval efficiently generates tasks that differentiate agent and model\nperformance, revealing gaps in reasoning, collaboration, and web interaction\nacross different settings and offering a new perspective for agent evaluation.",
      "upvotes": 1,
      "discussionId": "68e48ad7e4e093a7044e4d95",
      "githubRepo": "https://github.com/YurunChen/Graph2Eval",
      "ai_summary": "Graph2Eval, a knowledge graph-based framework, generates multimodal and interactive tasks to comprehensively evaluate agents' reasoning, collaboration, and web interaction capabilities.",
      "ai_keywords": [
        "multimodal LLM-driven agents",
        "knowledge graph",
        "subgraph sampling",
        "task templates",
        "meta-paths",
        "node reachability",
        "LLM scoring",
        "similarity analysis",
        "Graph2Eval-Bench",
        "document comprehension",
        "web interaction tasks",
        "Single-Agent",
        "Multi-Agent",
        "Web Agent"
      ],
      "githubStars": 2
    },
    "publishedAt": "2025-10-01T00:37:54.000Z",
    "title": "Graph2Eval: Automatic Multimodal Task Generation for Agents via\n  Knowledge Graphs",
    "summary": "As multimodal LLM-driven agents continue to advance in autonomy and\ngeneralization, evaluation based on static datasets can no longer adequately\nassess their true capabilities in dynamic environments and diverse tasks.\nExisting LLM-based synthetic data methods are largely designed for LLM training\nand evaluation, and thus cannot be directly applied to agent tasks that require\ntool use and interactive capabilities. While recent studies have explored\nautomatic agent task generation with LLMs, most efforts remain limited to text\nor image analysis, without systematically modeling multi-step interactions in\nweb environments. To address these challenges, we propose Graph2Eval, a\nknowledge graph-based framework that automatically generates both multimodal\ndocument comprehension tasks and web interaction tasks, enabling comprehensive\nevaluation of agents' reasoning, collaboration, and interactive capabilities.\nIn our approach, knowledge graphs constructed from multi-source external data\nserve as the task space, where we translate semantic relations into structured\nmultimodal tasks using subgraph sampling, task templates, and meta-paths. A\nmulti-stage filtering pipeline based on node reachability, LLM scoring, and\nsimilarity analysis is applied to guarantee the quality and executability of\nthe generated tasks. Furthermore, Graph2Eval supports end-to-end evaluation of\nmultiple agent types (Single-Agent, Multi-Agent, Web Agent) and measures\nreasoning, collaboration, and interaction capabilities. We instantiate the\nframework with Graph2Eval-Bench, a curated dataset of 1,319 tasks spanning\ndocument comprehension and web interaction scenarios. Experiments show that\nGraph2Eval efficiently generates tasks that differentiate agent and model\nperformance, revealing gaps in reasoning, collaboration, and web interaction\nacross different settings and offering a new perspective for agent evaluation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.00507.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6747285ed34bd6f05080ddda",
      "avatarUrl": "/avatars/ea63de7348aaabc8cff44e76207ba65c.svg",
      "fullname": "Yurun Chen",
      "name": "yurun-chen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  }
]