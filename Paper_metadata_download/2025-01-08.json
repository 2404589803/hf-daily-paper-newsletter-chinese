[
    "{'paper': {'id': '2501.03262', 'authors': [{'_id': '677e18a67edb3025daa99e09', 'user': {'_id': '63f6c04ac96958470d1e9043', 'avatarUrl': '/avatars/da46cdd9e21498e120ca91b67bfbfb5e.svg', 'isPro': False, 'fullname': 'Jian Hu', 'user': 'chuyi777', 'type': 'user'}, 'name': 'Jian Hu', 'status': 'extracted_pending', 'statusLastChangedAt': '2025-01-08T06:18:15.147Z', 'hidden': False}], 'publishedAt': '2025-01-04T02:08:06.000Z', 'title': 'REINFORCE++: A Simple and Efficient Approach for Aligning Large Language\\n  Models', 'summary': 'Reinforcement Learning from Human Feedback (RLHF) has emerged as a critical\\napproach for aligning large language models with human preferences, witnessing\\nrapid algorithmic evolution through methods such as Proximal Policy\\nOptimization (PPO), Direct Preference Optimization (DPO), REINFORCE Leave\\nOne-Out (RLOO), ReMax, and Group Relative Policy Optimization (GRPO). We\\npresent REINFORCE++, an enhanced variant of the classical REINFORCE algorithm\\nthat incorporates key optimization techniques from PPO while eliminating the\\nneed for a critic network. REINFORCE++ achieves three primary objectives: (1)\\nsimplicity (2) enhanced training stability, and (3) reduced computational\\noverhead. Through extensive empirical evaluation, we demonstrate that\\nREINFORCE++ exhibits superior stability compared to GRPO and achieves greater\\ncomputational efficiency than PPO while maintaining comparable performance. The\\nimplementation is available at https://github.com/OpenRLHF/OpenRLHF.', 'upvotes': 54, 'discussionId': '677e18a77edb3025daa99e4f'}, 'publishedAt': '2025-01-08T01:19:01.169Z', 'title': 'REINFORCE++: A Simple and Efficient Approach for Aligning Large Language Models', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.03262.png', 'numComments': 2, 'submittedBy': {'_id': '63f6c04ac96958470d1e9043', 'avatarUrl': '/avatars/da46cdd9e21498e120ca91b67bfbfb5e.svg', 'fullname': 'Jian Hu', 'name': 'chuyi777', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 2}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2501.03575', 'authors': [{'_id': '677dfaa14bf7f0d4734088a4', 'name': 'NVIDIA', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088a6', 'user': {'_id': '667b2e7f0ae3fef85fe33eb9', 'avatarUrl': '/avatars/033f74277bf934d8d9703e9a8c5a6716.svg', 'isPro': False, 'fullname': 'Niket Agarwal', 'user': 'niketa12', 'type': 'user'}, 'name': 'Niket Agarwal', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-08T13:41:16.772Z', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088a7', 'user': {'_id': '6517ec608a6d58d1d7ec8ec1', 'avatarUrl': '/avatars/4aac7fa6643f7f2d32e95cc991130ee9.svg', 'isPro': False, 'fullname': 'Arslan Ali', 'user': 'arslanali', 'type': 'user'}, 'name': 'Arslan Ali', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-08T13:41:24.513Z', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088a8', 'user': {'_id': '675304737c4876b7a1475695', 'avatarUrl': '/avatars/7ee3c443f6a143b4a79d679fb7f60fe5.svg', 'isPro': False, 'fullname': 'Maciej Bala', 'user': 'mbalaNV', 'type': 'user'}, 'name': 'Maciej Bala', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-08T13:41:34.024Z', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088a9', 'user': {'_id': '66cd4e2564a8631d7328a637', 'avatarUrl': '/avatars/024592abd427a8109f85c49e52e3bb7e.svg', 'isPro': False, 'fullname': 'Yogesh Balaji', 'user': 'yogeshbalaji', 'type': 'user'}, 'name': 'Yogesh Balaji', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-08T13:41:53.307Z', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088aa', 'name': 'Erik Barker', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088ab', 'name': 'Tiffany Cai', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088ac', 'user': {'_id': '628d451386d23ad1560882c4', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/628d451386d23ad1560882c4/UMxez0DEvX5qdP5ddqi-8.png', 'isPro': False, 'fullname': 'Prithvijit Chattopadhyay', 'user': 'prithv1', 'type': 'user'}, 'name': 'Prithvijit Chattopadhyay', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-08T13:43:22.176Z', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088ad', 'user': {'_id': '66f4cf1a03b5ba8a7f1f6522', 'avatarUrl': '/avatars/2768d6e37d3f280194cfb8ed274f6015.svg', 'isPro': False, 'fullname': 'Yongxin Chen', 'user': 'Ema11', 'type': 'user'}, 'name': 'Yongxin Chen', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-08T13:43:31.201Z', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088ae', 'user': {'_id': '66f6510a1c8cb854dec2d05c', 'avatarUrl': '/avatars/c344d7f6747beec0c3bab0c023b7b3d4.svg', 'isPro': False, 'fullname': 'Yin Cui', 'user': 'yinc-nvidia', 'type': 'user'}, 'name': 'Yin Cui', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-08T13:43:42.260Z', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088af', 'name': 'Yifan Ding', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088b0', 'name': 'Daniel Dworakowski', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088b1', 'user': {'_id': '67227b67dfa07920c2985d22', 'avatarUrl': '/avatars/98f09002722950eadffe5c199d22bb4f.svg', 'isPro': False, 'fullname': 'Jiaojiao Fan', 'user': 'jjf233', 'type': 'user'}, 'name': 'Jiaojiao Fan', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-08T13:44:52.955Z', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088b2', 'name': 'Michele Fenzi', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088b3', 'user': {'_id': '6353d3e95eac2d2efa7501f9', 'avatarUrl': '/avatars/4b063f54000bed4bfb1bfcc3cde1a09e.svg', 'isPro': False, 'fullname': 'Francesco Ferroni', 'user': 'fferroni', 'type': 'user'}, 'name': 'Francesco Ferroni', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-08T13:45:05.039Z', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088b4', 'name': 'Sanja Fidler', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088b5', 'name': 'Dieter Fox', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088b6', 'user': {'_id': '67291c1a027e12eb38dc8a0c', 'avatarUrl': '/avatars/00a0ca6da11d2ffd14a83d28e57c01b4.svg', 'isPro': False, 'fullname': 'Songwei Ge', 'user': 'SongweiGe', 'type': 'user'}, 'name': 'Songwei Ge', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-08T13:45:28.342Z', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088b7', 'user': {'_id': '6520e493b80dc49ba0f1e262', 'avatarUrl': '/avatars/af21c1ee154b2c9a0d56e69a07508ccb.svg', 'isPro': False, 'fullname': 'Yunhao Ge', 'user': 'yunhaog', 'type': 'user'}, 'name': 'Yunhao Ge', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-08T13:45:53.639Z', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088b8', 'user': {'_id': '647e8118770c299e56fc2bc8', 'avatarUrl': '/avatars/adf80f3473dda42450148789ae5c208f.svg', 'isPro': False, 'fullname': 'Jinwei Gu', 'user': 'jwgu', 'type': 'user'}, 'name': 'Jinwei Gu', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-08T13:46:02.830Z', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088b9', 'name': 'Siddharth Gururani', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088ba', 'user': {'_id': '62b20f6df4a72794189248fc', 'avatarUrl': '/avatars/87e1125868616d4f7d6ee1e5ec4499b4.svg', 'isPro': False, 'fullname': 'Ethan He', 'user': 'ethanhe', 'type': 'user'}, 'name': 'Ethan He', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-08T13:46:14.910Z', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088bb', 'name': 'Jiahui Huang', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088bc', 'name': 'Jacob Huffman', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088bd', 'name': 'Pooya Jannaty', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088be', 'name': 'Jingyi Jin', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088bf', 'name': 'Seung Wook Kim', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088c0', 'name': 'Gergely Kl√°r', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088c1', 'name': 'Grace Lam', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088c2', 'name': 'Shiyi Lan', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088c3', 'name': 'Laura Leal-Taixe', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088c4', 'name': 'Anqi Li', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088c5', 'name': 'Zhaoshuo Li', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088c6', 'name': 'Chen-Hsuan Lin', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088c7', 'name': 'Tsung-Yi Lin', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088c8', 'name': 'Huan Ling', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088c9', 'name': 'Ming-Yu Liu', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088ca', 'name': 'Xian Liu', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088cb', 'name': 'Alice Luo', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088cc', 'name': 'Qianli Ma', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088cd', 'name': 'Hanzi Mao', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088ce', 'name': 'Kaichun Mo', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088cf', 'name': 'Arsalan Mousavian', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088d0', 'name': 'Seungjun Nah', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088d1', 'name': 'Sriharsha Niverty', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088d2', 'name': 'David Page', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088d3', 'name': 'Despoina Paschalidou', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088d4', 'name': 'Zeeshan Patel', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088d5', 'name': 'Lindsey Pavao', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088d6', 'name': 'Morteza Ramezanali', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088d7', 'name': 'Fitsum Reda', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088d8', 'name': 'Xiaowei Ren', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088d9', 'name': 'Vasanth Rao Naik Sabavat', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088da', 'name': 'Ed Schmerling', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088db', 'name': 'Stella Shi', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088dc', 'name': 'Bartosz Stefaniak', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088dd', 'name': 'Shitao Tang', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088de', 'name': 'Lyne Tchapmi', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088df', 'name': 'Przemek Tredak', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088e0', 'name': 'Wei-Cheng Tseng', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088e1', 'name': 'Jibin Varghese', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088e2', 'name': 'Hao Wang', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088e3', 'name': 'Haoxiang Wang', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088e4', 'name': 'Heng Wang', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088e5', 'name': 'Ting-Chun Wang', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088e6', 'name': 'Fangyin Wei', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088e7', 'name': 'Xinyue Wei', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088e8', 'name': 'Jay Zhangjie Wu', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088e9', 'name': 'Jiashu Xu', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088ea', 'name': 'Wei Yang', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088eb', 'name': 'Lin Yen-Chen', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088ec', 'name': 'Xiaohui Zeng', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088ed', 'name': 'Yu Zeng', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088ee', 'name': 'Jing Zhang', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088ef', 'name': 'Qinsheng Zhang', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088f0', 'name': 'Yuxuan Zhang', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088f1', 'name': 'Qingqing Zhao', 'hidden': False}, {'_id': '677dfaa14bf7f0d4734088f2', 'name': 'Artur Zolkowski', 'hidden': False}], 'publishedAt': '2025-01-07T06:55:50.000Z', 'title': 'Cosmos World Foundation Model Platform for Physical AI', 'summary': 'Physical AI needs to be trained digitally first. It needs a digital twin of\\nitself, the policy model, and a digital twin of the world, the world model. In\\nthis paper, we present the Cosmos World Foundation Model Platform to help\\ndevelopers build customized world models for their Physical AI setups. We\\nposition a world foundation model as a general-purpose world model that can be\\nfine-tuned into customized world models for downstream applications. Our\\nplatform covers a video curation pipeline, pre-trained world foundation models,\\nexamples of post-training of pre-trained world foundation models, and video\\ntokenizers. To help Physical AI builders solve the most critical problems of\\nour society, we make our platform open-source and our models open-weight with\\npermissive licenses available via https://github.com/NVIDIA/Cosmos.', 'upvotes': 35, 'discussionId': '677dfaa84bf7f0d473408be8'}, 'publishedAt': '2025-01-08T00:45:09.139Z', 'title': 'Cosmos World Foundation Model Platform for Physical AI', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.03575.png', 'numComments': 2, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isMod': False, 'followerCount': 5591}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2501.02955', 'authors': [{'_id': '677dfb5d0310e9426191dd3e', 'user': {'_id': '62ecd24cb8764c7738ef2793', 'avatarUrl': '/avatars/c1b80b5c55f9d652c1aaac7919e1fa32.svg', 'isPro': False, 'fullname': 'Wenyi Hong', 'user': 'wenyi', 'type': 'user'}, 'name': 'Wenyi Hong', 'status': 'extracted_pending', 'statusLastChangedAt': '2025-01-08T04:13:19.132Z', 'hidden': False}, {'_id': '677dfb5d0310e9426191dd3f', 'user': {'_id': '65acc5afe2a2c8635614de43', 'avatarUrl': '/avatars/c5fce792792cc0b52ed7475d72460c58.svg', 'isPro': False, 'fullname': 'Yean Cheng', 'user': 'LiquidAmmonia', 'type': 'user'}, 'name': 'Yean Cheng', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-08T09:44:31.589Z', 'hidden': False}, {'_id': '677dfb5d0310e9426191dd40', 'user': {'_id': '6466d1640ed2f7a8cba87503', 'avatarUrl': '/avatars/652746e63dfeb5154ae7d34039d1a485.svg', 'isPro': False, 'fullname': 'Zhuoyi Yang', 'user': 'zyyangzy', 'type': 'user'}, 'name': 'Zhuoyi Yang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-08T13:26:08.664Z', 'hidden': False}, {'_id': '677dfb5d0310e9426191dd41', 'name': 'Weihan Wang', 'hidden': False}, {'_id': '677dfb5d0310e9426191dd42', 'name': 'Lefan Wang', 'hidden': False}, {'_id': '677dfb5d0310e9426191dd43', 'name': 'Xiaotao Gu', 'hidden': False}, {'_id': '677dfb5d0310e9426191dd44', 'user': {'_id': '6406db5cd684369027166986', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6406db5cd684369027166986/Zl-orrGcbY0RbfjfKszn1.jpeg', 'isPro': False, 'fullname': 'Shiyu Huang', 'user': 'ShiyuHuang', 'type': 'user'}, 'name': 'Shiyu Huang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-08T13:27:03.140Z', 'hidden': False}, {'_id': '677dfb5d0310e9426191dd45', 'user': {'_id': '640e73bdfdeaae1390857b62', 'avatarUrl': '/avatars/cd6779e30f716002a7838ed93d5c0754.svg', 'isPro': False, 'fullname': 'Yuxiao Dong', 'user': 'yuxiaod', 'type': 'user'}, 'name': 'Yuxiao Dong', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-08T13:27:12.264Z', 'hidden': False}, {'_id': '677dfb5d0310e9426191dd46', 'user': {'_id': '640dff05474aa6f89556677e', 'avatarUrl': '/avatars/1b4591c7322d649c797b3125148f1915.svg', 'isPro': False, 'fullname': 'Jie Tang', 'user': 'jerytang', 'type': 'user'}, 'name': 'Jie Tang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-08T13:27:19.977Z', 'hidden': False}], 'publishedAt': '2025-01-06T11:57:38.000Z', 'title': 'MotionBench: Benchmarking and Improving Fine-grained Video Motion\\n  Understanding for Vision Language Models', 'summary': \"In recent years, vision language models (VLMs) have made significant\\nadvancements in video understanding. However, a crucial capability -\\nfine-grained motion comprehension - remains under-explored in current\\nbenchmarks. To address this gap, we propose MotionBench, a comprehensive\\nevaluation benchmark designed to assess the fine-grained motion comprehension\\nof video understanding models. MotionBench evaluates models' motion-level\\nperception through six primary categories of motion-oriented question types and\\nincludes data collected from diverse sources, ensuring a broad representation\\nof real-world video content. Experimental results reveal that existing VLMs\\nperform poorly in understanding fine-grained motions. To enhance VLM's ability\\nto perceive fine-grained motion within a limited sequence length of LLM, we\\nconduct extensive experiments reviewing VLM architectures optimized for video\\nfeature compression and propose a novel and efficient Through-Encoder (TE)\\nFusion method. Experiments show that higher frame rate inputs and TE Fusion\\nyield improvements in motion understanding, yet there is still substantial room\\nfor enhancement. Our benchmark aims to guide and motivate the development of\\nmore capable video understanding models, emphasizing the importance of\\nfine-grained motion comprehension. Project page: https://motion-bench.github.io .\", 'upvotes': 32, 'discussionId': '677dfb5f0310e9426191de09'}, 'publishedAt': '2025-01-07T23:16:21.534Z', 'title': 'MotionBench: Benchmarking and Improving Fine-grained Video Motion Understanding for Vision Language Models', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.02955.png', 'numComments': 2, 'submittedBy': {'_id': '65acc5afe2a2c8635614de43', 'avatarUrl': '/avatars/c5fce792792cc0b52ed7475d72460c58.svg', 'fullname': 'Yean Cheng', 'name': 'LiquidAmmonia', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2501.03895', 'authors': [{'_id': '677ded917e773a03180e90c9', 'user': {'_id': '64803e5dc57f629056c601f1', 'avatarUrl': '/avatars/a9e9c97c70714e3a29bef2cf929ee6b3.svg', 'isPro': False, 'fullname': 'Shaolei Zhang', 'user': 'zhangshaolei', 'type': 'user'}, 'name': 'Shaolei Zhang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-08T13:27:33.924Z', 'hidden': False}, {'_id': '677ded917e773a03180e90ca', 'user': {'_id': '65b7573482d384513443875e', 'avatarUrl': '/avatars/0f2175e4adf507f5ccb0636c1cb647de.svg', 'isPro': False, 'fullname': 'Qingkai Fang', 'user': 'poeroz', 'type': 'user'}, 'name': 'Qingkai Fang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-08T13:27:41.696Z', 'hidden': False}, {'_id': '677ded917e773a03180e90cb', 'name': 'Zhe Yang', 'hidden': False}, {'_id': '677ded917e773a03180e90cc', 'user': {'_id': '63b39f33922f26a27e7e93dd', 'avatarUrl': '/avatars/fa1562f8c44270647826b293f49483bb.svg', 'isPro': False, 'fullname': 'Yang Feng', 'user': 'fengyang0317', 'type': 'user'}, 'name': 'Yang Feng', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-08T13:29:03.040Z', 'hidden': False}], 'publishedAt': '2025-01-07T16:03:14.000Z', 'title': 'LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One\\n  Vision Token', 'summary': 'The advent of real-time large multimodal models (LMMs) like GPT-4o has\\nsparked considerable interest in efficient LMMs. LMM frameworks typically\\nencode visual inputs into vision tokens (continuous representations) and\\nintegrate them and textual instructions into the context of large language\\nmodels (LLMs), where large-scale parameters and numerous context tokens\\n(predominantly vision tokens) result in substantial computational overhead.\\nPrevious efforts towards efficient LMMs always focus on replacing the LLM\\nbackbone with smaller models, while neglecting the crucial issue of token\\nquantity. In this paper, we introduce LLaVA-Mini, an efficient LMM with minimal\\nvision tokens. To achieve a high compression ratio of vision tokens while\\npreserving visual information, we first analyze how LMMs understand vision\\ntokens and find that most vision tokens only play a crucial role in the early\\nlayers of LLM backbone, where they mainly fuse visual information into text\\ntokens. Building on this finding, LLaVA-Mini introduces modality pre-fusion to\\nfuse visual information into text tokens in advance, thereby facilitating the\\nextreme compression of vision tokens fed to LLM backbone into one token.\\nLLaVA-Mini is a unified large multimodal model that can support the\\nunderstanding of images, high-resolution images, and videos in an efficient\\nmanner. Experiments across 11 image-based and 7 video-based benchmarks\\ndemonstrate that LLaVA-Mini outperforms LLaVA-v1.5 with just 1 vision token\\ninstead of 576. Efficiency analyses reveal that LLaVA-Mini can reduce FLOPs by\\n77%, deliver low-latency responses within 40 milliseconds, and process over\\n10,000 frames of video on the GPU hardware with 24GB of memory.', 'upvotes': 23, 'discussionId': '677ded937e773a03180e9144'}, 'publishedAt': '2025-01-07T22:17:40.411Z', 'title': 'LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One Vision Token', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.03895.png', 'numComments': 4, 'submittedBy': {'_id': '64803e5dc57f629056c601f1', 'avatarUrl': '/avatars/a9e9c97c70714e3a29bef2cf929ee6b3.svg', 'fullname': 'Shaolei Zhang', 'name': 'zhangshaolei', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2501.04001', 'authors': [{'_id': '677e2052dbff7b495e85ec95', 'user': {'_id': '6391e41f2e73987364e6bcb2', 'avatarUrl': '/avatars/d09a9ee329bb8c3a9e2929d67d24e97d.svg', 'isPro': False, 'fullname': 'Haobo Yuan', 'user': 'HarborYuan', 'type': 'user'}, 'name': 'Haobo Yuan', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-08T09:44:27.046Z', 'hidden': False}, {'_id': '677e2052dbff7b495e85ec96', 'user': {'_id': '63958b4414513eaf9029ebf1', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/U1g5H071pWRswGAG9UTpo.png', 'isPro': False, 'fullname': 'Xiangtai Li', 'user': 'LXT', 'type': 'user'}, 'name': 'Xiangtai Li', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-08T13:39:14.906Z', 'hidden': False}, {'_id': '677e2052dbff7b495e85ec97', 'user': {'_id': '660d28db4215cc70372bc432', 'avatarUrl': '/avatars/a515303c6e29725ef3698bb695ffa743.svg', 'isPro': False, 'fullname': 'Tao Zhang', 'user': 'TaoZhang', 'type': 'user'}, 'name': 'Tao Zhang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-08T13:39:23.333Z', 'hidden': False}, {'_id': '677e2052dbff7b495e85ec98', 'name': 'Zilong Huang', 'hidden': False}, {'_id': '677e2052dbff7b495e85ec99', 'user': {'_id': '638598a138f4aec99c50750e', 'avatarUrl': '/avatars/42a4aad213e04a0ded1ab7f81910e082.svg', 'isPro': False, 'fullname': 'Shilin Xu', 'user': 'shilinxu', 'type': 'user'}, 'name': 'Shilin Xu', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-08T13:40:16.694Z', 'hidden': False}, {'_id': '677e2052dbff7b495e85ec9a', 'name': 'Shunping Ji', 'hidden': False}, {'_id': '677e2052dbff7b495e85ec9b', 'name': 'Yunhai Tong', 'hidden': False}, {'_id': '677e2052dbff7b495e85ec9c', 'name': 'Lu Qi', 'hidden': False}, {'_id': '677e2052dbff7b495e85ec9d', 'user': {'_id': '67298e44017b96a1d0101dc4', 'avatarUrl': '/avatars/1f8ed1a3e911e6a3021087b9371d284c.svg', 'isPro': False, 'fullname': 'Jiashi Feng', 'user': 'jshfeng', 'type': 'user'}, 'name': 'Jiashi Feng', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-08T13:41:01.815Z', 'hidden': False}, {'_id': '677e2052dbff7b495e85ec9e', 'name': 'Ming-Hsuan Yang', 'hidden': False}], 'publishedAt': '2025-01-07T18:58:54.000Z', 'title': 'Sa2VA: Marrying SAM2 with LLaVA for Dense Grounded Understanding of\\n  Images and Videos', 'summary': 'This work presents Sa2VA, the first unified model for dense grounded\\nunderstanding of both images and videos. Unlike existing multi-modal large\\nlanguage models, which are often limited to specific modalities and tasks,\\nSa2VA supports a wide range of image and video tasks, including referring\\nsegmentation and conversation, with minimal one-shot instruction tuning. Sa2VA\\ncombines SAM-2, a foundation video segmentation model, with LLaVA, an advanced\\nvision-language model, and unifies text, image, and video into a shared LLM\\ntoken space. Using the LLM, Sa2VA generates instruction tokens that guide SAM-2\\nin producing precise masks, enabling a grounded, multi-modal understanding of\\nboth static and dynamic visual content. Additionally, we introduce Ref-SAV, an\\nauto-labeled dataset containing over 72k object expressions in complex video\\nscenes, designed to boost model performance. We also manually validate 2k video\\nobjects in the Ref-SAV datasets to benchmark referring video object\\nsegmentation in complex environments. Experiments show that Sa2VA achieves\\nstate-of-the-art across multiple tasks, particularly in referring video object\\nsegmentation, highlighting its potential for complex real-world applications.', 'upvotes': 18, 'discussionId': '677e2056dbff7b495e85ede6'}, 'publishedAt': '2025-01-08T03:25:35.451Z', 'title': 'Sa2VA: Marrying SAM2 with LLaVA for Dense Grounded Understanding of Images and Videos', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/63958b4414513eaf9029ebf1/8O0prEI2JWoOmxLtpFrVD.mp4', 'https://cdn-uploads.huggingface.co/production/uploads/63958b4414513eaf9029ebf1/bbTUwkMNRMxAJB-nvxVuG.mp4'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.04001.png', 'numComments': 2, 'submittedBy': {'_id': '63958b4414513eaf9029ebf1', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/U1g5H071pWRswGAG9UTpo.png', 'fullname': 'Xiangtai Li', 'name': 'LXT', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 8}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2501.03847', 'authors': [{'_id': '677e11b751b4ae6d3c98fdca', 'name': 'Zekai Gu', 'hidden': False}, {'_id': '677e11b751b4ae6d3c98fdcb', 'user': {'_id': '64be3a355b8d826146ed2001', 'avatarUrl': '/avatars/85a87270e7c76345e555803ab31c33d1.svg', 'isPro': False, 'fullname': 'ruiyan', 'user': 'ruiyan', 'type': 'user'}, 'name': 'Rui Yan', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-08T13:47:05.657Z', 'hidden': False}, {'_id': '677e11b751b4ae6d3c98fdcc', 'user': {'_id': '64d3abeb1058ae59738ba8ce', 'avatarUrl': '/avatars/4a9de3db835ade2c20f0d2de678e85c4.svg', 'isPro': False, 'fullname': 'jiahao lu', 'user': 'jiahao97', 'type': 'user'}, 'name': 'Jiahao Lu', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-08T13:46:55.864Z', 'hidden': False}, {'_id': '677e11b751b4ae6d3c98fdcd', 'user': {'_id': '634e6bdd2cd84978c48f3985', 'avatarUrl': '/avatars/2f6ae1f50666ca187a7a3c74b5e173f7.svg', 'isPro': False, 'fullname': 'Peng Li', 'user': 'pengHTYX', 'type': 'user'}, 'name': 'Peng Li', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-08T09:44:29.526Z', 'hidden': False}, {'_id': '677e11b751b4ae6d3c98fdce', 'user': {'_id': '645223fb01d7bd9555ea399a', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/645223fb01d7bd9555ea399a/ZED54WEISs5HxUd52Yge8.png', 'isPro': False, 'fullname': 'Zhiyang Dou', 'user': 'frankzydou', 'type': 'user'}, 'name': 'Zhiyang Dou', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-08T13:47:13.361Z', 'hidden': False}, {'_id': '677e11b751b4ae6d3c98fdcf', 'user': {'_id': '635f8ed47c05eb9f59963d3a', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/635f8ed47c05eb9f59963d3a/uQf4p9N9pSaFy87Wg9v4k.jpeg', 'isPro': False, 'fullname': 'ChenyangSi', 'user': 'ChenyangSi', 'type': 'user'}, 'name': 'Chenyang Si', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-08T13:47:21.815Z', 'hidden': False}, {'_id': '677e11b751b4ae6d3c98fdd0', 'user': {'_id': '643ba2f725681c3afaa8f05e', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/643ba2f725681c3afaa8f05e/2RnOdmbBM8WHYhiTGG-Cd.jpeg', 'isPro': False, 'fullname': 'Zhen Dong', 'user': 'zhendongucb', 'type': 'user'}, 'name': 'Zhen Dong', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-08T13:47:32.762Z', 'hidden': False}, {'_id': '677e11b751b4ae6d3c98fdd1', 'name': 'Qifeng Liu', 'hidden': False}, {'_id': '677e11b751b4ae6d3c98fdd2', 'name': 'Cheng Lin', 'hidden': False}, {'_id': '677e11b751b4ae6d3c98fdd3', 'user': {'_id': '62ab1ac1d48b4d8b048a3473', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1656826685333-62ab1ac1d48b4d8b048a3473.png', 'isPro': False, 'fullname': 'Ziwei Liu', 'user': 'liuziwei7', 'type': 'user'}, 'name': 'Ziwei Liu', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-08T13:48:08.642Z', 'hidden': False}, {'_id': '677e11b751b4ae6d3c98fdd4', 'name': 'Wenping Wang', 'hidden': False}, {'_id': '677e11b751b4ae6d3c98fdd5', 'name': 'Yuan Liu', 'hidden': False}], 'publishedAt': '2025-01-07T15:01:58.000Z', 'title': 'Diffusion as Shader: 3D-aware Video Diffusion for Versatile Video\\n  Generation Control', 'summary': 'Diffusion models have demonstrated impressive performance in generating\\nhigh-quality videos from text prompts or images. However, precise control over\\nthe video generation process, such as camera manipulation or content editing,\\nremains a significant challenge. Existing methods for controlled video\\ngeneration are typically limited to a single control type, lacking the\\nflexibility to handle diverse control demands. In this paper, we introduce\\nDiffusion as Shader (DaS), a novel approach that supports multiple video\\ncontrol tasks within a unified architecture. Our key insight is that achieving\\nversatile video control necessitates leveraging 3D control signals, as videos\\nare fundamentally 2D renderings of dynamic 3D content. Unlike prior methods\\nlimited to 2D control signals, DaS leverages 3D tracking videos as control\\ninputs, making the video diffusion process inherently 3D-aware. This innovation\\nallows DaS to achieve a wide range of video controls by simply manipulating the\\n3D tracking videos. A further advantage of using 3D tracking videos is their\\nability to effectively link frames, significantly enhancing the temporal\\nconsistency of the generated videos. With just 3 days of fine-tuning on 8 H800\\nGPUs using less than 10k videos, DaS demonstrates strong control capabilities\\nacross diverse tasks, including mesh-to-video generation, camera control,\\nmotion transfer, and object manipulation.', 'upvotes': 13, 'discussionId': '677e11b851b4ae6d3c98fe56'}, 'publishedAt': '2025-01-08T00:48:51.295Z', 'title': 'Diffusion as Shader: 3D-aware Video Diffusion for Versatile Video Generation Control', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.03847.png', 'numComments': 2, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isMod': False, 'followerCount': 5591}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2501.04561', 'authors': [{'_id': '677f302c7bd81737ce7a9405', 'name': 'Run Luo', 'hidden': False}, {'_id': '677f302c7bd81737ce7a9406', 'name': 'Ting-En Lin', 'hidden': False}, {'_id': '677f302c7bd81737ce7a9407', 'name': 'Haonan Zhang', 'hidden': False}, {'_id': '677f302c7bd81737ce7a9408', 'name': 'Yuchuan Wu', 'hidden': False}, {'_id': '677f302c7bd81737ce7a9409', 'name': 'Xiong Liu', 'hidden': False}, {'_id': '677f302c7bd81737ce7a940a', 'name': 'Min Yang', 'hidden': False}, {'_id': '677f302c7bd81737ce7a940b', 'name': 'Yongbin Li', 'hidden': False}, {'_id': '677f302c7bd81737ce7a940c', 'name': 'Longze Chen', 'hidden': False}, {'_id': '677f302c7bd81737ce7a940d', 'name': 'Jiaming Li', 'hidden': False}, {'_id': '677f302c7bd81737ce7a940e', 'name': 'Lei Zhang', 'hidden': False}, {'_id': '677f302c7bd81737ce7a940f', 'name': 'Yangyi Chen', 'hidden': False}, {'_id': '677f302c7bd81737ce7a9410', 'name': 'Hamid Alinejad-Rokny', 'hidden': False}, {'_id': '677f302c7bd81737ce7a9411', 'name': 'Fei Huang', 'hidden': False}], 'publishedAt': '2025-01-08T15:18:09.000Z', 'title': 'OpenOmni: Large Language Models Pivot Zero-shot Omnimodal Alignment\\n  across Language with Real-time Self-Aware Emotional Speech Synthesis', 'summary': 'Recent advancements in omnimodal learning have been achieved in understanding\\nand generation across images, text, and speech, though mainly within\\nproprietary models. Limited omnimodal datasets and the inherent challenges\\nassociated with real-time emotional speech generation have hindered open-source\\nprogress. To address these issues, we propose openomni, a two-stage training\\nmethod combining omnimodal alignment and speech generation to develop a\\nstate-of-the-art omnimodal large language model. In the alignment phase, a\\npre-trained speech model is further trained on text-image tasks to generalize\\nfrom vision to speech in a (near) zero-shot manner, outperforming models\\ntrained on tri-modal datasets. In the speech generation phase, a lightweight\\ndecoder facilitates real-time emotional speech through training on speech tasks\\nand preference learning. Experiments demonstrate that openomni consistently\\nimproves across omnimodal, vision-language, and speech-language evaluations,\\nenabling natural, emotion-rich dialogues and real-time emotional speech\\ngeneration.', 'upvotes': 10, 'discussionId': '677f302d7bd81737ce7a944a'}, 'publishedAt': '2025-01-08T21:11:17.997Z', 'title': 'OpenOmni: Large Language Models Pivot Zero-shot Omnimodal Alignment across Language with Real-time Self-Aware Emotional Speech Synthesis', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.04561.png', 'numComments': 2, 'submittedBy': {'_id': '62e0ef42edb0462c8d51818d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/62e0ef42edb0462c8d51818d/3YM7DUynIWiiRFM6_enpg.jpeg', 'fullname': 'Ting-En Lin', 'name': 'tnlin', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 8}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2501.03936', 'authors': [{'_id': '677e43433dfa51c15df22b7d', 'user': {'_id': '64380d17819f3ab20d17595b', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64380d17819f3ab20d17595b/LW_JVO54BCoYnKYZ-868o.png', 'isPro': False, 'fullname': 'Zheng Hao', 'user': 'Forceless', 'type': 'user'}, 'name': 'Hao Zheng', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-08T09:44:22.639Z', 'hidden': False}, {'_id': '677e43433dfa51c15df22b7e', 'name': 'Xinyan Guan', 'hidden': False}, {'_id': '677e43433dfa51c15df22b7f', 'user': {'_id': '6354fdcd525beaee68899e50', 'avatarUrl': '/avatars/1135c7e402510b2ad0fe33f60c8d842c.svg', 'isPro': False, 'fullname': 'Hao Kong', 'user': 'Danphnis', 'type': 'user'}, 'name': 'Hao Kong', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-08T13:50:59.404Z', 'hidden': False}, {'_id': '677e43433dfa51c15df22b80', 'name': 'Jia Zheng', 'hidden': False}, {'_id': '677e43433dfa51c15df22b81', 'user': {'_id': '6711c702f858a456b4b9f3a4', 'avatarUrl': '/avatars/178e9567c3111ab22717c3c0dd003a6a.svg', 'isPro': False, 'fullname': 'Hongyu  Lin', 'user': 'sanmusunrise', 'type': 'user'}, 'name': 'Hongyu Lin', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-08T13:49:20.814Z', 'hidden': False}, {'_id': '677e43433dfa51c15df22b82', 'user': {'_id': '6216496a9b34d2fb49144599', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6216496a9b34d2fb49144599/41CKA_h1Ffj3RzVabSAkm.jpeg', 'isPro': False, 'fullname': 'Yaojie Lu', 'user': 'luyaojie', 'type': 'user'}, 'name': 'Yaojie Lu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-08T13:23:32.842Z', 'hidden': False}, {'_id': '677e43433dfa51c15df22b83', 'name': 'Ben He', 'hidden': False}, {'_id': '677e43433dfa51c15df22b84', 'user': {'_id': '65e99a77e71555ed193609cf', 'avatarUrl': '/avatars/38ceb127883944677665da967d17dd18.svg', 'isPro': False, 'fullname': 'Xianpei Han', 'user': 'xphan', 'type': 'user'}, 'name': 'Xianpei Han', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-08T13:49:38.801Z', 'hidden': False}, {'_id': '677e43433dfa51c15df22b85', 'name': 'Le Sun', 'hidden': False}], 'publishedAt': '2025-01-07T16:53:01.000Z', 'title': 'PPTAgent: Generating and Evaluating Presentations Beyond Text-to-Slides', 'summary': 'Automatically generating presentations from documents is a challenging task\\nthat requires balancing content quality, visual design, and structural\\ncoherence. Existing methods primarily focus on improving and evaluating the\\ncontent quality in isolation, often overlooking visual design and structural\\ncoherence, which limits their practical applicability. To address these\\nlimitations, we propose PPTAgent, which comprehensively improves presentation\\ngeneration through a two-stage, edit-based approach inspired by human\\nworkflows. PPTAgent first analyzes reference presentations to understand their\\nstructural patterns and content schemas, then drafts outlines and generates\\nslides through code actions to ensure consistency and alignment. To\\ncomprehensively evaluate the quality of generated presentations, we further\\nintroduce PPTEval, an evaluation framework that assesses presentations across\\nthree dimensions: Content, Design, and Coherence. Experiments show that\\nPPTAgent significantly outperforms traditional automatic presentation\\ngeneration methods across all three dimensions. The code and data are available\\nat https://github.com/icip-cas/PPTAgent.', 'upvotes': 10, 'discussionId': '677e43443dfa51c15df22bd7'}, 'publishedAt': '2025-01-08T05:11:16.667Z', 'title': 'PPTAgent: Generating and Evaluating Presentations Beyond Text-to-Slides', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.03936.png', 'numComments': 2, 'submittedBy': {'_id': '64380d17819f3ab20d17595b', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64380d17819f3ab20d17595b/LW_JVO54BCoYnKYZ-868o.png', 'fullname': 'Zheng Hao', 'name': 'Forceless', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2501.03916', 'authors': [{'_id': '677e3b1ddbff7b495e8f0424', 'user': {'_id': '64a3d1ddb3239f3e3892b24b', 'avatarUrl': '/avatars/7ce585f5fc1d077fb1d70cc18c4da2c1.svg', 'isPro': False, 'fullname': 'Jiakang Yuan', 'user': 'JiakangYuan', 'type': 'user'}, 'name': 'Jiakang Yuan', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-08T13:52:54.625Z', 'hidden': False}, {'_id': '677e3b1ddbff7b495e8f0425', 'user': {'_id': '65b88b92e0bde92c176a888a', 'avatarUrl': '/avatars/fc1cb54328ca93860e97fc73a3c1eb2f.svg', 'isPro': False, 'fullname': 'Xiangchao Yan', 'user': 'yxc97', 'type': 'user'}, 'name': 'Xiangchao Yan', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-08T13:53:08.361Z', 'hidden': False}, {'_id': '677e3b1ddbff7b495e8f0426', 'user': {'_id': '643df87f7cd64d872cb9fabd', 'avatarUrl': '/avatars/c53bfabcee08de448dde973915e8b31d.svg', 'isPro': False, 'fullname': 'Botian Shi', 'user': 'friskit', 'type': 'user'}, 'name': 'Botian Shi', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-08T13:53:16.461Z', 'hidden': False}, {'_id': '677e3b1ddbff7b495e8f0427', 'name': 'Tao Chen', 'hidden': False}, {'_id': '677e3b1ddbff7b495e8f0428', 'name': 'Wanli Ouyang', 'hidden': False}, {'_id': '677e3b1ddbff7b495e8f0429', 'user': {'_id': '643dfd235aafbdca3a5792c0', 'avatarUrl': '/avatars/ce8553cf5936012c692e08054ee27937.svg', 'isPro': False, 'fullname': 'Bo Zhang', 'user': 'BoZhang', 'type': 'user'}, 'name': 'Bo Zhang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-08T09:44:24.813Z', 'hidden': False}, {'_id': '677e3b1ddbff7b495e8f042a', 'name': 'Lei Bai', 'hidden': False}, {'_id': '677e3b1ddbff7b495e8f042b', 'name': 'Yu Qiao', 'hidden': False}, {'_id': '677e3b1ddbff7b495e8f042c', 'user': {'_id': '669f614b59adf5b56e05bce3', 'avatarUrl': '/avatars/ffd4189efbceb0e63a03db273065a44b.svg', 'isPro': False, 'fullname': 'BowenZhou', 'user': 'bowenZhou', 'type': 'user'}, 'name': 'Bowen Zhou', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-08T13:53:56.271Z', 'hidden': False}], 'publishedAt': '2025-01-07T16:31:10.000Z', 'title': 'Dolphin: Closed-loop Open-ended Auto-research through Thinking,\\n  Practice, and Feedback', 'summary': 'The scientific research paradigm is undergoing a profound transformation\\nowing to the development of Artificial Intelligence (AI). Recent works\\ndemonstrate that various AI-assisted research methods can largely improve\\nresearch efficiency by improving data analysis, accelerating computation, and\\nfostering novel idea generation. To further move towards the ultimate goal\\n(i.e., automatic scientific research), in this paper, we propose Dolphin, the\\nfirst closed-loop open-ended auto-research framework to further build the\\nentire process of human scientific research. Dolphin can generate research\\nideas, perform experiments, and get feedback from experimental results to\\ngenerate higher-quality ideas. More specifically, Dolphin first generates novel\\nideas based on relevant papers which are ranked by the topic and task\\nattributes. Then, the codes are automatically generated and debugged with the\\nexception-traceback-guided local code structure. Finally, Dolphin automatically\\nanalyzes the results of each idea and feeds the results back to the next round\\nof idea generation. Experiments are conducted on the benchmark datasets of\\ndifferent topics and results show that Dolphin can generate novel ideas\\ncontinuously and complete the experiment in a loop. We highlight that Dolphin\\ncan automatically propose methods that are comparable to the state-of-the-art\\nin some tasks such as 2D image classification and 3D point classification.', 'upvotes': 8, 'discussionId': '677e3b1edbff7b495e8f049c'}, 'publishedAt': '2025-01-08T03:45:32.176Z', 'title': 'Dolphin: Closed-loop Open-ended Auto-research through Thinking, Practice, and Feedback', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.03916.png', 'numComments': 3, 'submittedBy': {'_id': '643dfd235aafbdca3a5792c0', 'avatarUrl': '/avatars/ce8553cf5936012c692e08054ee27937.svg', 'fullname': 'Bo Zhang', 'name': 'BoZhang', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2501.02790', 'authors': [{'_id': '677dea23e86d0754dc6e3f09', 'user': {'_id': '605e8dfd5abeb13e714c4c18', 'avatarUrl': '/avatars/bc27a0ed17b2bd4311e89d3028fa327b.svg', 'isPro': False, 'fullname': 'yueqin yin', 'user': 'yyqoni', 'type': 'user'}, 'name': 'Yueqin Yin', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-08T09:44:41.339Z', 'hidden': False}, {'_id': '677dea23e86d0754dc6e3f0a', 'user': {'_id': '677ebaf3fcaae73ddd6c8475', 'avatarUrl': '/avatars/98db20fe3bbbf7caffb5821a7242dc54.svg', 'isPro': False, 'fullname': 'Shentao Yang', 'user': 'shentaoyang', 'type': 'user'}, 'name': 'Shentao Yang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-08T21:08:25.725Z', 'hidden': False}, {'_id': '677dea23e86d0754dc6e3f0b', 'name': 'Yujia Xie', 'hidden': False}, {'_id': '677dea23e86d0754dc6e3f0c', 'name': 'Ziyi Yang', 'hidden': False}, {'_id': '677dea23e86d0754dc6e3f0d', 'name': 'Yuting Sun', 'hidden': False}, {'_id': '677dea23e86d0754dc6e3f0e', 'name': 'Hany Awadalla', 'hidden': False}, {'_id': '677dea23e86d0754dc6e3f0f', 'name': 'Weizhu Chen', 'hidden': False}, {'_id': '677dea23e86d0754dc6e3f10', 'name': 'Mingyuan Zhou', 'hidden': False}], 'publishedAt': '2025-01-06T06:17:56.000Z', 'title': 'Segmenting Text and Learning Their Rewards for Improved RLHF in Language\\n  Model', 'summary': 'Reinforcement learning from human feedback (RLHF) has been widely adopted to\\nalign language models (LMs) with human preference. Prior RLHF works typically\\ntake a bandit formulation, which, though intuitive, ignores the sequential\\nnature of LM generation and can suffer from the sparse reward issue. While\\nrecent works propose dense token-level RLHF, treating each token as an action\\nmay be oversubtle to proper reward assignment. In this paper, we seek to get\\nthe best of both by training and utilizing a segment-level reward model, which\\nassigns a reward to each semantically complete text segment that spans over a\\nshort sequence of tokens. For reward learning, our method allows dynamic text\\nsegmentation and compatibility with standard sequence-preference datasets. For\\neffective RL-based LM training against segment reward, we generalize the\\nclassical scalar bandit reward normalizers into location-aware normalizer\\nfunctions and interpolate the segment reward for further densification. With\\nthese designs, our method performs competitively on three popular RLHF\\nbenchmarks for LM policy: AlpacaEval 2.0, Arena-Hard, and MT-Bench. Ablation\\nstudies are conducted to further demonstrate our method.', 'upvotes': 6, 'discussionId': '677dea24e86d0754dc6e3f49'}, 'publishedAt': '2025-01-08T11:02:00.381Z', 'title': 'Segmenting Text and Learning Their Rewards for Improved RLHF in Language Model', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.02790.png', 'numComments': 2, 'submittedBy': {'_id': '605e8dfd5abeb13e714c4c18', 'avatarUrl': '/avatars/bc27a0ed17b2bd4311e89d3028fa327b.svg', 'fullname': 'yueqin yin', 'name': 'yyqoni', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2501.03714', 'authors': [{'_id': '677e48854bf7f0d4735973d3', 'user': {'_id': '677e4ef9c27c05b2d178d775', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/xQoYxKiRmlnCOslOaxjCl.png', 'isPro': False, 'fullname': 'Sangwoon Kwak', 'user': 'sangwoonkwak', 'type': 'user'}, 'name': 'Sangwoon Kwak', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-08T13:51:17.553Z', 'hidden': False}, {'_id': '677e48854bf7f0d4735973d4', 'user': {'_id': '660ca70399760c2ef3ab4450', 'avatarUrl': '/avatars/d1dd8492b8129554b0650d22d0731fed.svg', 'isPro': False, 'fullname': 'Joonsoo Kim', 'user': 'joons1991', 'type': 'user'}, 'name': 'Joonsoo Kim', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-08T13:51:29.754Z', 'hidden': False}, {'_id': '677e48854bf7f0d4735973d5', 'user': {'_id': '677e68cc8a7269e1779e4def', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/PrMRHMrB4aEQPWV0pxNxX.png', 'isPro': False, 'fullname': 'Jun Young Jeong', 'user': 'shurek20', 'type': 'user'}, 'name': 'Jun Young Jeong', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-08T13:51:42.670Z', 'hidden': False}, {'_id': '677e48854bf7f0d4735973d6', 'name': 'Won-Sik Cheong', 'hidden': False}, {'_id': '677e48854bf7f0d4735973d7', 'name': 'Jihyong Oh', 'hidden': False}, {'_id': '677e48854bf7f0d4735973d8', 'name': 'Munchurl Kim', 'hidden': False}], 'publishedAt': '2025-01-07T11:43:13.000Z', 'title': 'MoDec-GS: Global-to-Local Motion Decomposition and Temporal Interval\\n  Adjustment for Compact Dynamic 3D Gaussian Splatting', 'summary': '3D Gaussian Splatting (3DGS) has made significant strides in scene\\nrepresentation and neural rendering, with intense efforts focused on adapting\\nit for dynamic scenes. Despite delivering remarkable rendering quality and\\nspeed, existing methods struggle with storage demands and representing complex\\nreal-world motions. To tackle these issues, we propose MoDecGS, a\\nmemory-efficient Gaussian splatting framework designed for reconstructing novel\\nviews in challenging scenarios with complex motions. We introduce GlobaltoLocal\\nMotion Decomposition (GLMD) to effectively capture dynamic motions in a\\ncoarsetofine manner. This approach leverages Global Canonical Scaffolds (Global\\nCS) and Local Canonical Scaffolds (Local CS), extending static Scaffold\\nrepresentation to dynamic video reconstruction. For Global CS, we propose\\nGlobal Anchor Deformation (GAD) to efficiently represent global dynamics along\\ncomplex motions, by directly deforming the implicit Scaffold attributes which\\nare anchor position, offset, and local context features. Next, we finely adjust\\nlocal motions via the Local Gaussian Deformation (LGD) of Local CS explicitly.\\nAdditionally, we introduce Temporal Interval Adjustment (TIA) to automatically\\ncontrol the temporal coverage of each Local CS during training, allowing\\nMoDecGS to find optimal interval assignments based on the specified number of\\ntemporal segments. Extensive evaluations demonstrate that MoDecGS achieves an\\naverage 70% reduction in model size over stateoftheart methods for dynamic 3D\\nGaussians from realworld dynamic videos while maintaining or even improving\\nrendering quality.', 'upvotes': 6, 'discussionId': '677e48884bf7f0d473597469'}, 'publishedAt': '2025-01-08T04:56:15.945Z', 'title': 'MoDec-GS: Global-to-Local Motion Decomposition and Temporal Interval Adjustment for Compact Dynamic 3D Gaussian Splatting', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/6576b99d58ce19fa1e33eb1d/NS4zIqathzflivNtM6cuZ.mp4'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.03714.png', 'numComments': 2, 'submittedBy': {'_id': '6576b99d58ce19fa1e33eb1d', 'avatarUrl': '/avatars/b533e776aa3d95d722b46ef0cd381acd.svg', 'fullname': 'Jihyong Oh', 'name': 'ozbro', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2501.03931', 'authors': [{'_id': '677df1e52f09432f00e506ae', 'user': {'_id': '6418554a0956be7233a1023e', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6418554a0956be7233a1023e/9EKN0GoOpcDbvBDmAQEJf.png', 'isPro': False, 'fullname': 'zhang yuechen', 'user': 'julianjuaner', 'type': 'user'}, 'name': 'Yuechen Zhang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-08T09:44:33.746Z', 'hidden': False}, {'_id': '677df1e52f09432f00e506af', 'user': {'_id': '662d968eac05b4f7c280d6f9', 'avatarUrl': '/avatars/59d55142f33f650d9116a04110db839e.svg', 'isPro': False, 'fullname': 'YaoYang Liu', 'user': 'LazySheeep', 'type': 'user'}, 'name': 'Yaoyang Liu', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-08T13:55:13.065Z', 'hidden': False}, {'_id': '677df1e52f09432f00e506b0', 'name': 'Bin Xia', 'hidden': False}, {'_id': '677df1e52f09432f00e506b1', 'user': {'_id': '673a10f911b7efeeedabc252', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/T7ySn7F0pTVCvRdcvMz3d.png', 'isPro': False, 'fullname': 'Bohao Peng', 'user': 'BoHao0326', 'type': 'user'}, 'name': 'Bohao Peng', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-08T13:56:25.937Z', 'hidden': False}, {'_id': '677df1e52f09432f00e506b2', 'name': 'Zexin Yan', 'hidden': False}, {'_id': '677df1e52f09432f00e506b3', 'user': {'_id': '6447485270338c0376087917', 'avatarUrl': '/avatars/359d17963a6d1c5f1fddf027169332a5.svg', 'isPro': False, 'fullname': 'Eric Lo', 'user': 'ericlo', 'type': 'user'}, 'name': 'Eric Lo', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-08T13:56:05.809Z', 'hidden': False}, {'_id': '677df1e52f09432f00e506b4', 'name': 'Jiaya Jia', 'hidden': False}], 'publishedAt': '2025-01-07T16:48:31.000Z', 'title': 'Magic Mirror: ID-Preserved Video Generation in Video Diffusion\\n  Transformers', 'summary': 'We present Magic Mirror, a framework for generating identity-preserved videos\\nwith cinematic-level quality and dynamic motion. While recent advances in video\\ndiffusion models have shown impressive capabilities in text-to-video\\ngeneration, maintaining consistent identity while producing natural motion\\nremains challenging. Previous methods either require person-specific\\nfine-tuning or struggle to balance identity preservation with motion diversity.\\nBuilt upon Video Diffusion Transformers, our method introduces three key\\ncomponents: (1) a dual-branch facial feature extractor that captures both\\nidentity and structural features, (2) a lightweight cross-modal adapter with\\nConditioned Adaptive Normalization for efficient identity integration, and (3)\\na two-stage training strategy combining synthetic identity pairs with video\\ndata. Extensive experiments demonstrate that Magic Mirror effectively balances\\nidentity consistency with natural motion, outperforming existing methods across\\nmultiple metrics while requiring minimal parameters added. The code and model\\nwill be made publicly available at:\\nhttps://github.com/dvlab-research/MagicMirror/', 'upvotes': 6, 'discussionId': '677df1e92f09432f00e507e3'}, 'publishedAt': '2025-01-07T22:35:08.996Z', 'title': 'Magic Mirror: ID-Preserved Video Generation in Video Diffusion Transformers', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/6418554a0956be7233a1023e/WHDQsdFlVcnveOf06Oaz8.qt'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.03931.png', 'numComments': 2, 'submittedBy': {'_id': '6418554a0956be7233a1023e', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6418554a0956be7233a1023e/9EKN0GoOpcDbvBDmAQEJf.png', 'fullname': 'zhang yuechen', 'name': 'julianjuaner', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 4}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2501.02393', 'authors': [{'_id': '677cfd73f7545101cef8b48b', 'name': 'Markus J. Buehler', 'hidden': False}], 'publishedAt': '2025-01-04T22:30:21.000Z', 'title': 'Graph-Aware Isomorphic Attention for Adaptive Dynamics in Transformers', 'summary': \"We present an approach to modifying Transformer architectures by integrating\\ngraph-aware relational reasoning into the attention mechanism, merging concepts\\nfrom graph neural networks and language modeling. Building on the inherent\\nconnection between attention and graph theory, we reformulate the Transformer's\\nattention mechanism as a graph operation and propose Graph-Aware Isomorphic\\nAttention. This method leverages advanced graph modeling strategies, including\\nGraph Isomorphism Networks (GIN) and Principal Neighborhood Aggregation (PNA),\\nto enrich the representation of relational structures. Our approach captures\\ncomplex dependencies and generalizes across tasks, as evidenced by a reduced\\ngeneralization gap and improved learning performance. Additionally, we expand\\nthe concept of graph-aware attention to introduce Sparse GIN-Attention, a\\nfine-tuning approach that employs sparse GINs. By interpreting attention\\nmatrices as sparse adjacency graphs, this technique enhances the adaptability\\nof pre-trained foundational models with minimal computational overhead,\\nendowing them with graph-aware capabilities. Sparse GIN-Attention fine-tuning\\nachieves improved training dynamics and better generalization compared to\\nalternative methods like low-rank adaption (LoRA). We discuss latent graph-like\\nstructures within traditional attention mechanisms, offering a new lens through\\nwhich Transformers can be understood. By evolving Transformers as hierarchical\\nGIN models for relational reasoning. This perspective suggests profound\\nimplications for foundational model development, enabling the design of\\narchitectures that dynamically adapt to both local and global dependencies.\\nApplications in bioinformatics, materials science, language modeling, and\\nbeyond could benefit from this synthesis of relational and sequential data\\nmodeling, setting the stage for interpretable and generalizable modeling\\nstrategies.\", 'upvotes': 3, 'discussionId': '677cfd74f7545101cef8b4f4'}, 'publishedAt': '2025-01-08T11:52:30.909Z', 'title': 'Graph-Aware Isomorphic Attention for Adaptive Dynamics in Transformers', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.02393.png', 'numComments': 2, 'submittedBy': {'_id': '623ce1c6b66fedf374859fe7', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/623ce1c6b66fedf374859fe7/lhbMLg6BxLCb9DD4rgjfx.jpeg', 'fullname': 'Markus Buehler', 'name': 'mjbuehler', 'type': 'user', 'isPro': True, 'isHf': False, 'isMod': False, 'followerCount': 21}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2501.02260', 'authors': [{'_id': '677df32e9b2af6b36b22668e', 'user': {'_id': '6629604bdf81c3e1d90ca5ec', 'avatarUrl': '/avatars/bb35569fe124de1374f149b940a4b56c.svg', 'isPro': False, 'fullname': 'Mengting Wei', 'user': 'mengtingwei', 'type': 'user'}, 'name': 'Mengting Wei', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-08T13:54:19.846Z', 'hidden': False}, {'_id': '677df32e9b2af6b36b22668f', 'name': 'Tuomas Varanka', 'hidden': False}, {'_id': '677df32e9b2af6b36b226690', 'user': {'_id': '667690daf2312ba18e81a160', 'avatarUrl': '/avatars/f502d8ba6cf1de352032e62acab4022d.svg', 'isPro': False, 'fullname': 'Xingxun JIANG', 'user': 'Xingxun', 'type': 'user'}, 'name': 'Xingxun Jiang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-08T13:54:36.033Z', 'hidden': False}, {'_id': '677df32e9b2af6b36b226691', 'name': 'Huai-Qian Khor', 'hidden': False}, {'_id': '677df32e9b2af6b36b226692', 'name': 'Guoying Zhao', 'hidden': False}], 'publishedAt': '2025-01-04T11:28:49.000Z', 'title': 'MagicFace: High-Fidelity Facial Expression Editing with Action-Unit\\n  Control', 'summary': \"We address the problem of facial expression editing by controling the\\nrelative variation of facial action-unit (AU) from the same person. This\\nenables us to edit this specific person's expression in a fine-grained,\\ncontinuous and interpretable manner, while preserving their identity, pose,\\nbackground and detailed facial attributes. Key to our model, which we dub\\nMagicFace, is a diffusion model conditioned on AU variations and an ID encoder\\nto preserve facial details of high consistency. Specifically, to preserve the\\nfacial details with the input identity, we leverage the power of pretrained\\nStable-Diffusion models and design an ID encoder to merge appearance features\\nthrough self-attention. To keep background and pose consistency, we introduce\\nan efficient Attribute Controller by explicitly informing the model of current\\nbackground and pose of the target. By injecting AU variations into a denoising\\nUNet, our model can animate arbitrary identities with various AU combinations,\\nyielding superior results in high-fidelity expression editing compared to other\\nfacial expression editing works. Code is publicly available at\\nhttps://github.com/weimengting/MagicFace.\", 'upvotes': 3, 'discussionId': '677df32f9b2af6b36b2266dd'}, 'publishedAt': '2025-01-07T22:38:56.286Z', 'title': 'MagicFace: High-Fidelity Facial Expression Editing with Action-Unit Control', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.02260.png', 'numComments': 2, 'submittedBy': {'_id': '6321ac39a97afe3c4c64701d', 'avatarUrl': '/avatars/408fc06e818f5b0ffb8bc42d464a2a5c.svg', 'fullname': 'Tuomas', 'name': 'Tvaranka', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2501.02376', 'authors': [{'_id': '677f28c33d2765f5655e948f', 'name': 'Wenhao Wang', 'hidden': False}, {'_id': '677f28c33d2765f5655e9490', 'name': 'Yifan Sun', 'hidden': False}, {'_id': '677f28c33d2765f5655e9491', 'name': 'Zongxin Yang', 'hidden': False}, {'_id': '677f28c33d2765f5655e9492', 'name': 'Zhentao Tan', 'hidden': False}, {'_id': '677f28c33d2765f5655e9493', 'name': 'Zhengdong Hu', 'hidden': False}, {'_id': '677f28c33d2765f5655e9494', 'name': 'Yi Yang', 'hidden': False}], 'publishedAt': '2025-01-04T20:34:53.000Z', 'title': 'Generalizable Origin Identification for Text-Guided Image-to-Image\\n  Diffusion Models', 'summary': 'Text-guided image-to-image diffusion models excel in translating images based\\non textual prompts, allowing for precise and creative visual modifications.\\nHowever, such a powerful technique can be misused for spreading misinformation,\\ninfringing on copyrights, and evading content tracing. This motivates us to\\nintroduce the task of origin IDentification for text-guided Image-to-image\\nDiffusion models (ID^2), aiming to retrieve the original image of a given\\ntranslated query. A straightforward solution to ID^2 involves training a\\nspecialized deep embedding model to extract and compare features from both\\nquery and reference images. However, due to visual discrepancy across\\ngenerations produced by different diffusion models, this similarity-based\\napproach fails when training on images from one model and testing on those from\\nanother, limiting its effectiveness in real-world applications. To solve this\\nchallenge of the proposed ID^2 task, we contribute the first dataset and a\\ntheoretically guaranteed method, both emphasizing generalizability. The curated\\ndataset, OriPID, contains abundant Origins and guided Prompts, which can be\\nused to train and test potential IDentification models across various diffusion\\nmodels. In the method section, we first prove the existence of a linear\\ntransformation that minimizes the distance between the pre-trained Variational\\nAutoencoder (VAE) embeddings of generated samples and their origins.\\nSubsequently, it is demonstrated that such a simple linear transformation can\\nbe generalized across different diffusion models. Experimental results show\\nthat the proposed method achieves satisfying generalization performance,\\nsignificantly surpassing similarity-based methods (+31.6% mAP), even those\\nwith generalization designs.', 'upvotes': 0, 'discussionId': '677f28c63d2765f5655e95ae'}, 'publishedAt': '2025-01-08T20:40:01.926Z', 'title': 'Generalizable Origin Identification for Text-Guided Image-to-Image Diffusion Models', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.02376.png', 'numComments': 2, 'submittedBy': {'_id': '62b32a4429a410b7f6b06710', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/62b32a4429a410b7f6b06710/VzgvmnlYZWuifZTkIkCxy.jpeg', 'fullname': 'Wenhao Wang', 'name': 'WenhaoWang', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 13}, 'isAuthorParticipating': False}"
]