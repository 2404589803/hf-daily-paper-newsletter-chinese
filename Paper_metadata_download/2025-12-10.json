[
  {
    "paper": {
      "id": "2512.08765",
      "authors": [
        {
          "_id": "6938da63dfc35938ba129f3c",
          "name": "Ruihang Chu",
          "hidden": false
        },
        {
          "_id": "6938da63dfc35938ba129f3d",
          "name": "Yefei He",
          "hidden": false
        },
        {
          "_id": "6938da63dfc35938ba129f3e",
          "name": "Zhekai Chen",
          "hidden": false
        },
        {
          "_id": "6938da63dfc35938ba129f3f",
          "name": "Shiwei Zhang",
          "hidden": false
        },
        {
          "_id": "6938da63dfc35938ba129f40",
          "name": "Xiaogang Xu",
          "hidden": false
        },
        {
          "_id": "6938da63dfc35938ba129f41",
          "name": "Bin Xia",
          "hidden": false
        },
        {
          "_id": "6938da63dfc35938ba129f42",
          "name": "Dingdong Wang",
          "hidden": false
        },
        {
          "_id": "6938da63dfc35938ba129f43",
          "name": "Hongwei Yi",
          "hidden": false
        },
        {
          "_id": "6938da63dfc35938ba129f44",
          "name": "Xihui Liu",
          "hidden": false
        },
        {
          "_id": "6938da63dfc35938ba129f45",
          "name": "Hengshuang Zhao",
          "hidden": false
        },
        {
          "_id": "6938da63dfc35938ba129f46",
          "name": "Yu Liu",
          "hidden": false
        },
        {
          "_id": "6938da63dfc35938ba129f47",
          "name": "Yingya Zhang",
          "hidden": false
        },
        {
          "_id": "6938da63dfc35938ba129f48",
          "name": "Yujiu Yang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/GRHR-g0jymQza9KMw0iLn.png",
        "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/K8nyGDyLuL_hxp15wJdMk.png",
        "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/4b8dLB_xvGpTjJlWP8oeh.mp4"
      ],
      "publishedAt": "2025-12-09T16:13:55.000Z",
      "submittedOnDailyAt": "2025-12-10T00:20:18.797Z",
      "title": "Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We present Wan-Move, a simple and scalable framework that brings motion control to video generative models. Existing motion-controllable methods typically suffer from coarse control granularity and limited scalability, leaving their outputs insufficient for practical use. We narrow this gap by achieving precise and high-quality motion control. Our core idea is to directly make the original condition features motion-aware for guiding video synthesis. To this end, we first represent object motions with dense point trajectories, allowing fine-grained control over the scene. We then project these trajectories into latent space and propagate the first frame's features along each trajectory, producing an aligned spatiotemporal feature map that tells how each scene element should move. This feature map serves as the updated latent condition, which is naturally integrated into the off-the-shelf image-to-video model, e.g., Wan-I2V-14B, as motion guidance without any architecture change. It removes the need for auxiliary motion encoders and makes fine-tuning base models easily scalable. Through scaled training, Wan-Move generates 5-second, 480p videos whose motion controllability rivals Kling 1.5 Pro's commercial Motion Brush, as indicated by user studies. To support comprehensive evaluation, we further design MoveBench, a rigorously curated benchmark featuring diverse content categories and hybrid-verified annotations. It is distinguished by larger data volume, longer video durations, and high-quality motion annotations. Extensive experiments on MoveBench and the public dataset consistently show Wan-Move's superior motion quality. Code, models, and benchmark data are made publicly available.",
      "upvotes": 74,
      "discussionId": "6938da64dfc35938ba129f49",
      "githubRepo": "https://github.com/ali-vilab/Wan-Move",
      "ai_summary": "Wan-Move enhances motion control in video generative models by integrating motion-aware features into latent space, enabling high-quality and scalable video synthesis.",
      "ai_keywords": [
        "motion control",
        "video generative models",
        "dense point trajectories",
        "latent space",
        "spatiotemporal feature map",
        "motion guidance",
        "image-to-video model",
        "auxiliary motion encoders",
        "fine-tuning",
        "MoveBench",
        "motion annotations"
      ],
      "githubStars": 42,
      "organization": {
        "_id": "67d15cca6e2cf0e062dbfb54",
        "name": "AlibabaTongyiLab",
        "fullname": "TongyiLab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
      }
    },
    "publishedAt": "2025-12-09T11:13:55.000Z",
    "title": "Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance",
    "summary": "We present Wan-Move, a simple and scalable framework that brings motion control to video generative models. Existing motion-controllable methods typically suffer from coarse control granularity and limited scalability, leaving their outputs insufficient for practical use. We narrow this gap by achieving precise and high-quality motion control. Our core idea is to directly make the original condition features motion-aware for guiding video synthesis. To this end, we first represent object motions with dense point trajectories, allowing fine-grained control over the scene. We then project these trajectories into latent space and propagate the first frame's features along each trajectory, producing an aligned spatiotemporal feature map that tells how each scene element should move. This feature map serves as the updated latent condition, which is naturally integrated into the off-the-shelf image-to-video model, e.g., Wan-I2V-14B, as motion guidance without any architecture change. It removes the need for auxiliary motion encoders and makes fine-tuning base models easily scalable. Through scaled training, Wan-Move generates 5-second, 480p videos whose motion controllability rivals Kling 1.5 Pro's commercial Motion Brush, as indicated by user studies. To support comprehensive evaluation, we further design MoveBench, a rigorously curated benchmark featuring diverse content categories and hybrid-verified annotations. It is distinguished by larger data volume, longer video durations, and high-quality motion annotations. Extensive experiments on MoveBench and the public dataset consistently show Wan-Move's superior motion quality. Code, models, and benchmark data are made publicly available.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/GRHR-g0jymQza9KMw0iLn.png",
      "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/K8nyGDyLuL_hxp15wJdMk.png",
      "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/4b8dLB_xvGpTjJlWP8oeh.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.08765.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 181
    },
    "organization": {
      "_id": "67d15cca6e2cf0e062dbfb54",
      "name": "AlibabaTongyiLab",
      "fullname": "TongyiLab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.07951",
      "authors": [
        {
          "_id": "6938e892dfc35938ba129ff5",
          "name": "Zekai Luo",
          "hidden": false
        },
        {
          "_id": "6938e892dfc35938ba129ff6",
          "name": "Zongze Du",
          "hidden": false
        },
        {
          "_id": "6938e892dfc35938ba129ff7",
          "name": "Zhouhang Zhu",
          "hidden": false
        },
        {
          "_id": "6938e892dfc35938ba129ff8",
          "name": "Hao Zhong",
          "hidden": false
        },
        {
          "_id": "6938e892dfc35938ba129ff9",
          "name": "Muzhi Zhu",
          "hidden": false
        },
        {
          "_id": "6938e892dfc35938ba129ffa",
          "name": "Wen Wang",
          "hidden": false
        },
        {
          "_id": "6938e892dfc35938ba129ffb",
          "name": "Yuling Xi",
          "hidden": false
        },
        {
          "_id": "6938e892dfc35938ba129ffc",
          "name": "Chenchen Jing",
          "hidden": false
        },
        {
          "_id": "6938e892dfc35938ba129ffd",
          "name": "Hao Chen",
          "hidden": false
        },
        {
          "_id": "6938e892dfc35938ba129ffe",
          "name": "Chunhua Shen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-08T19:00:04.000Z",
      "submittedOnDailyAt": "2025-12-10T00:59:39.366Z",
      "title": "Preserving Source Video Realism: High-Fidelity Face Swapping for Cinematic Quality",
      "submittedOnDailyBy": {
        "_id": "632179745fc60c44fd91fc33",
        "avatarUrl": "/avatars/37d4fefbcc19f091dccffefec9706de2.svg",
        "isPro": false,
        "fullname": "zhumuzhi",
        "user": "Z-MU-Z",
        "type": "user"
      },
      "summary": "Video face swapping is crucial in film and entertainment production, where achieving high fidelity and temporal consistency over long and complex video sequences remains a significant challenge. Inspired by recent advances in reference-guided image editing, we explore whether rich visual attributes from source videos can be similarly leveraged to enhance both fidelity and temporal coherence in video face swapping. Building on this insight, this work presents LivingSwap, the first video reference guided face swapping model. Our approach employs keyframes as conditioning signals to inject the target identity, enabling flexible and controllable editing. By combining keyframe conditioning with video reference guidance, the model performs temporal stitching to ensure stable identity preservation and high-fidelity reconstruction across long video sequences. To address the scarcity of data for reference-guided training, we construct a paired face-swapping dataset, Face2Face, and further reverse the data pairs to ensure reliable ground-truth supervision. Extensive experiments demonstrate that our method achieves state-of-the-art results, seamlessly integrating the target identity with the source video's expressions, lighting, and motion, while significantly reducing manual effort in production workflows. Project webpage: https://aim-uofa.github.io/LivingSwap",
      "upvotes": 28,
      "discussionId": "6938e892dfc35938ba129fff",
      "projectPage": "https://aim-uofa.github.io/LivingSwap",
      "ai_summary": "LivingSwap enhances video face swapping by using keyframes and reference guidance to maintain identity and fidelity over long sequences, reducing manual effort and achieving state-of-the-art results.",
      "ai_keywords": [
        "keyframe conditioning",
        "video reference guidance",
        "temporal stitching",
        "identity preservation",
        "high-fidelity reconstruction",
        "Face2Face dataset"
      ],
      "organization": {
        "_id": "61bac2af530e5c78d7b99667",
        "name": "zju",
        "fullname": "Zhejiang University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"
      }
    },
    "publishedAt": "2025-12-08T14:00:04.000Z",
    "title": "Preserving Source Video Realism: High-Fidelity Face Swapping for Cinematic Quality",
    "summary": "Video face swapping is crucial in film and entertainment production, where achieving high fidelity and temporal consistency over long and complex video sequences remains a significant challenge. Inspired by recent advances in reference-guided image editing, we explore whether rich visual attributes from source videos can be similarly leveraged to enhance both fidelity and temporal coherence in video face swapping. Building on this insight, this work presents LivingSwap, the first video reference guided face swapping model. Our approach employs keyframes as conditioning signals to inject the target identity, enabling flexible and controllable editing. By combining keyframe conditioning with video reference guidance, the model performs temporal stitching to ensure stable identity preservation and high-fidelity reconstruction across long video sequences. To address the scarcity of data for reference-guided training, we construct a paired face-swapping dataset, Face2Face, and further reverse the data pairs to ensure reliable ground-truth supervision. Extensive experiments demonstrate that our method achieves state-of-the-art results, seamlessly integrating the target identity with the source video's expressions, lighting, and motion, while significantly reducing manual effort in production workflows. Project webpage: https://aim-uofa.github.io/LivingSwap",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.07951.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "632179745fc60c44fd91fc33",
      "avatarUrl": "/avatars/37d4fefbcc19f091dccffefec9706de2.svg",
      "fullname": "zhumuzhi",
      "name": "Z-MU-Z",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "organization": {
      "_id": "61bac2af530e5c78d7b99667",
      "name": "zju",
      "fullname": "Zhejiang University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.07843",
      "authors": [
        {
          "_id": "6938e51ddfc35938ba129fb2",
          "name": "Long Lian",
          "hidden": false
        },
        {
          "_id": "6938e51ddfc35938ba129fb3",
          "name": "Sida Wang",
          "hidden": false
        },
        {
          "_id": "6938e51ddfc35938ba129fb4",
          "name": "Felix Juefei-Xu",
          "hidden": false
        },
        {
          "_id": "6938e51ddfc35938ba129fb5",
          "name": "Tsu-Jui Fu",
          "hidden": false
        },
        {
          "_id": "6938e51ddfc35938ba129fb6",
          "name": "Xiuyu Li",
          "hidden": false
        },
        {
          "_id": "6938e51ddfc35938ba129fb7",
          "name": "Adam Yala",
          "hidden": false
        },
        {
          "_id": "6938e51ddfc35938ba129fb8",
          "name": "Trevor Darrell",
          "hidden": false
        },
        {
          "_id": "6938e51ddfc35938ba129fb9",
          "name": "Alane Suhr",
          "hidden": false
        },
        {
          "_id": "6938e51ddfc35938ba129fba",
          "name": "Yuandong Tian",
          "hidden": false
        },
        {
          "_id": "6938e51ddfc35938ba129fbb",
          "name": "Xi Victoria Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-24T18:55:59.000Z",
      "submittedOnDailyAt": "2025-12-10T02:14:20.520Z",
      "title": "ThreadWeaver: Adaptive Threading for Efficient Parallel Reasoning in Language Models",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": true,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Scaling inference-time computation has enabled Large Language Models (LLMs) to achieve strong reasoning performance, but inherently sequential decoding leads to substantial latency, especially on complex tasks. Recent work on adaptive parallel reasoning aims to improve inference efficiency by decomposing the problem-solving process into concurrent reasoning threads when beneficial. However, existing methods on realistic tasks are either limited to supervised behavior cloning or exhibit significant accuracy drops compared to widely-used sequential long chain-of-thought (CoT) baselines. Moreover, many require customized inference engines, complicating deployment. We introduce ThreadWeaver, a framework for adaptive parallel reasoning that achieves accuracy on par with popular sequential reasoning models of comparable size while significantly reducing inference latency. ThreadWeaver's performance stems from three key innovations: 1) a two-stage parallel trajectory generator that produces large-scale, high-quality CoT data with parallel annotations for supervised fine-tuning; 2) a trie-based training-inference co-design that enables parallel reasoning on any off-the-shelf autoregressive inference engine without modifying position embeddings or KV caches; and 3) a parallelization-aware reinforcement learning framework that teaches the model to balance accuracy with effective parallelization. Across six challenging mathematical reasoning benchmarks, ThreadWeaver trained atop Qwen3-8B achieves accuracy comparable to cutting-edge sequential reasoning models (71.9% on average and 79.9% on AIME24) while delivering up to 1.53x average speedup in token latency, establishing a new Pareto frontier between accuracy and efficiency.",
      "upvotes": 11,
      "discussionId": "6938e51edfc35938ba129fbc",
      "ai_summary": "ThreadWeaver, a framework for adaptive parallel reasoning, achieves accuracy comparable to sequential models while reducing inference latency through parallel trajectory generation, trie-based training-inference co-design, and parallelization-aware reinforcement learning.",
      "ai_keywords": [
        "Large Language Models",
        "sequential decoding",
        "adaptive parallel reasoning",
        "long chain-of-thought",
        "CoT",
        "ThreadWeaver",
        "parallel trajectory generator",
        "trie-based training-inference co-design",
        "parallelization-aware reinforcement learning",
        "Qwen3-8B",
        "AIME24"
      ]
    },
    "publishedAt": "2025-11-24T13:55:59.000Z",
    "title": "ThreadWeaver: Adaptive Threading for Efficient Parallel Reasoning in Language Models",
    "summary": "Scaling inference-time computation has enabled Large Language Models (LLMs) to achieve strong reasoning performance, but inherently sequential decoding leads to substantial latency, especially on complex tasks. Recent work on adaptive parallel reasoning aims to improve inference efficiency by decomposing the problem-solving process into concurrent reasoning threads when beneficial. However, existing methods on realistic tasks are either limited to supervised behavior cloning or exhibit significant accuracy drops compared to widely-used sequential long chain-of-thought (CoT) baselines. Moreover, many require customized inference engines, complicating deployment. We introduce ThreadWeaver, a framework for adaptive parallel reasoning that achieves accuracy on par with popular sequential reasoning models of comparable size while significantly reducing inference latency. ThreadWeaver's performance stems from three key innovations: 1) a two-stage parallel trajectory generator that produces large-scale, high-quality CoT data with parallel annotations for supervised fine-tuning; 2) a trie-based training-inference co-design that enables parallel reasoning on any off-the-shelf autoregressive inference engine without modifying position embeddings or KV caches; and 3) a parallelization-aware reinforcement learning framework that teaches the model to balance accuracy with effective parallelization. Across six challenging mathematical reasoning benchmarks, ThreadWeaver trained atop Qwen3-8B achieves accuracy comparable to cutting-edge sequential reasoning models (71.9% on average and 79.9% on AIME24) while delivering up to 1.53x average speedup in token latency, establishing a new Pareto frontier between accuracy and efficiency.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.07843.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": true,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8874
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.08153",
      "authors": [
        {
          "_id": "6938de86dfc35938ba129f4b",
          "name": "Zheng Ding",
          "hidden": false
        },
        {
          "_id": "6938de86dfc35938ba129f4c",
          "name": "Weirui Ye",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-09T01:17:34.000Z",
      "submittedOnDailyAt": "2025-12-10T00:14:54.011Z",
      "title": "TreeGRPO: Tree-Advantage GRPO for Online RL Post-Training of Diffusion Models",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Reinforcement learning (RL) post-training is crucial for aligning generative models with human preferences, but its prohibitive computational cost remains a major barrier to widespread adoption. We introduce TreeGRPO, a novel RL framework that dramatically improves training efficiency by recasting the denoising process as a search tree. From shared initial noise samples, TreeGRPO strategically branches to generate multiple candidate trajectories while efficiently reusing their common prefixes. This tree-structured approach delivers three key advantages: (1) High sample efficiency, achieving better performance under same training samples (2) Fine-grained credit assignment via reward backpropagation that computes step-specific advantages, overcoming the uniform credit assignment limitation of trajectory-based methods, and (3) Amortized computation where multi-child branching enables multiple policy updates per forward pass. Extensive experiments on both diffusion and flow-based models demonstrate that TreeGRPO achieves 2.4times faster training while establishing a superior Pareto frontier in the efficiency-reward trade-off space. Our method consistently outperforms GRPO baselines across multiple benchmarks and reward models, providing a scalable and effective pathway for RL-based visual generative model alignment. The project website is available at treegrpo.github.io.",
      "upvotes": 4,
      "discussionId": "6938de87dfc35938ba129f4d",
      "ai_summary": "TreeGRPO, a novel RL framework, enhances training efficiency for generative models by using a tree-structured denoising process, leading to faster training and better performance.",
      "ai_keywords": [
        "Reinforcement learning",
        "TreeGRPO",
        "denoising process",
        "search tree",
        "candidate trajectories",
        "sample efficiency",
        "fine-grained credit assignment",
        "reward backpropagation",
        "amortized computation",
        "diffusion models",
        "flow-based models",
        "training efficiency",
        "Pareto frontier",
        "efficiency-reward trade-off"
      ]
    },
    "publishedAt": "2025-12-08T20:17:34.000Z",
    "title": "TreeGRPO: Tree-Advantage GRPO for Online RL Post-Training of Diffusion Models",
    "summary": "Reinforcement learning (RL) post-training is crucial for aligning generative models with human preferences, but its prohibitive computational cost remains a major barrier to widespread adoption. We introduce TreeGRPO, a novel RL framework that dramatically improves training efficiency by recasting the denoising process as a search tree. From shared initial noise samples, TreeGRPO strategically branches to generate multiple candidate trajectories while efficiently reusing their common prefixes. This tree-structured approach delivers three key advantages: (1) High sample efficiency, achieving better performance under same training samples (2) Fine-grained credit assignment via reward backpropagation that computes step-specific advantages, overcoming the uniform credit assignment limitation of trajectory-based methods, and (3) Amortized computation where multi-child branching enables multiple policy updates per forward pass. Extensive experiments on both diffusion and flow-based models demonstrate that TreeGRPO achieves 2.4times faster training while establishing a superior Pareto frontier in the efficiency-reward trade-off space. Our method consistently outperforms GRPO baselines across multiple benchmarks and reward models, providing a scalable and effective pathway for RL-based visual generative model alignment. The project website is available at treegrpo.github.io.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.08153.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 181
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.08868",
      "authors": [
        {
          "_id": "6938e481dfc35938ba129f9c",
          "name": "Rui Min",
          "hidden": false
        },
        {
          "_id": "6938e481dfc35938ba129f9d",
          "name": "Zile Qiao",
          "hidden": false
        },
        {
          "_id": "6938e481dfc35938ba129f9e",
          "name": "Ze Xu",
          "hidden": false
        },
        {
          "_id": "6938e481dfc35938ba129f9f",
          "name": "Jiawen Zhai",
          "hidden": false
        },
        {
          "_id": "6938e481dfc35938ba129fa0",
          "name": "Wenyu Gao",
          "hidden": false
        },
        {
          "_id": "6938e481dfc35938ba129fa1",
          "name": "Xuanzhong Chen",
          "hidden": false
        },
        {
          "_id": "6938e481dfc35938ba129fa2",
          "name": "Haozhen Sun",
          "hidden": false
        },
        {
          "_id": "6938e481dfc35938ba129fa3",
          "name": "Zhen Zhang",
          "hidden": false
        },
        {
          "_id": "6938e481dfc35938ba129fa4",
          "name": "Xinyu Wang",
          "hidden": false
        },
        {
          "_id": "6938e481dfc35938ba129fa5",
          "name": "Hong Zhou",
          "hidden": false
        },
        {
          "_id": "6938e481dfc35938ba129fa6",
          "name": "Wenbiao Yin",
          "hidden": false
        },
        {
          "_id": "6938e481dfc35938ba129fa7",
          "name": "Xuan Zhou",
          "hidden": false
        },
        {
          "_id": "6938e481dfc35938ba129fa8",
          "name": "Yong Jiang",
          "hidden": false
        },
        {
          "_id": "6938e481dfc35938ba129fa9",
          "name": "Haicheng Liu",
          "hidden": false
        },
        {
          "_id": "6938e481dfc35938ba129faa",
          "name": "Liang Ding",
          "hidden": false
        },
        {
          "_id": "6938e481dfc35938ba129fab",
          "name": "Ling Zou",
          "hidden": false
        },
        {
          "_id": "6938e481dfc35938ba129fac",
          "name": "Yi R.",
          "hidden": false
        },
        {
          "_id": "6938e481dfc35938ba129fad",
          "name": "Fung",
          "hidden": false
        },
        {
          "_id": "6938e481dfc35938ba129fae",
          "name": "Yalong Li",
          "hidden": false
        },
        {
          "_id": "6938e481dfc35938ba129faf",
          "name": "Pengjun Xie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-09T18:00:26.000Z",
      "submittedOnDailyAt": "2025-12-10T00:40:05.606Z",
      "title": "EcomBench: Towards Holistic Evaluation of Foundation Agents in E-commerce",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Foundation agents have rapidly advanced in their ability to reason and interact with real environments, making the evaluation of their core capabilities increasingly important. While many benchmarks have been developed to assess agent performance, most concentrate on academic settings or artificially designed scenarios while overlooking the challenges that arise in real applications. To address this issue, we focus on a highly practical real-world setting, the e-commerce domain, which involves a large volume of diverse user interactions, dynamic market conditions, and tasks directly tied to real decision-making processes. To this end, we introduce EcomBench, a holistic E-commerce Benchmark designed to evaluate agent performance in realistic e-commerce environments. EcomBench is built from genuine user demands embedded in leading global e-commerce ecosystems and is carefully curated and annotated through human experts to ensure clarity, accuracy, and domain relevance. It covers multiple task categories within e-commerce scenarios and defines three difficulty levels that evaluate agents on key capabilities such as deep information retrieval, multi-step reasoning, and cross-source knowledge integration. By grounding evaluation in real e-commerce contexts, EcomBench provides a rigorous and dynamic testbed for measuring the practical capabilities of agents in modern e-commerce.",
      "upvotes": 2,
      "discussionId": "6938e482dfc35938ba129fb0",
      "ai_summary": "EcomBench is a benchmark that evaluates agent performance in real-world e-commerce environments through deep information retrieval, multi-step reasoning, and cross-source knowledge integration.",
      "ai_keywords": [
        "deep information retrieval",
        "multi-step reasoning",
        "cross-source knowledge integration"
      ],
      "organization": {
        "_id": "67d15cca6e2cf0e062dbfb54",
        "name": "AlibabaTongyiLab",
        "fullname": "TongyiLab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
      }
    },
    "publishedAt": "2025-12-09T13:00:26.000Z",
    "title": "EcomBench: Towards Holistic Evaluation of Foundation Agents in E-commerce",
    "summary": "Foundation agents have rapidly advanced in their ability to reason and interact with real environments, making the evaluation of their core capabilities increasingly important. While many benchmarks have been developed to assess agent performance, most concentrate on academic settings or artificially designed scenarios while overlooking the challenges that arise in real applications. To address this issue, we focus on a highly practical real-world setting, the e-commerce domain, which involves a large volume of diverse user interactions, dynamic market conditions, and tasks directly tied to real decision-making processes. To this end, we introduce EcomBench, a holistic E-commerce Benchmark designed to evaluate agent performance in realistic e-commerce environments. EcomBench is built from genuine user demands embedded in leading global e-commerce ecosystems and is carefully curated and annotated through human experts to ensure clarity, accuracy, and domain relevance. It covers multiple task categories within e-commerce scenarios and defines three difficulty levels that evaluate agents on key capabilities such as deep information retrieval, multi-step reasoning, and cross-source knowledge integration. By grounding evaluation in real e-commerce contexts, EcomBench provides a rigorous and dynamic testbed for measuring the practical capabilities of agents in modern e-commerce.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.08868.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 181
    },
    "organization": {
      "_id": "67d15cca6e2cf0e062dbfb54",
      "name": "AlibabaTongyiLab",
      "fullname": "TongyiLab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.08564",
      "authors": [
        {
          "_id": "6938fc77dfc35938ba12a12c",
          "name": "Mahmoud Afifi",
          "hidden": false
        },
        {
          "_id": "6938fc77dfc35938ba12a12d",
          "name": "Zhongling Wang",
          "hidden": false
        },
        {
          "_id": "6938fc77dfc35938ba12a12e",
          "name": "Ran Zhang",
          "hidden": false
        },
        {
          "_id": "6938fc77dfc35938ba12a12f",
          "name": "Michael S. Brown",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-09T13:04:08.000Z",
      "submittedOnDailyAt": "2025-12-10T02:29:57.817Z",
      "title": "Modular Neural Image Signal Processing",
      "submittedOnDailyBy": {
        "_id": "6574973c05e573071548922c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6574973c05e573071548922c/KT-KAo4aepdpbZguXfKPv.jpeg",
        "isPro": false,
        "fullname": "Mahmoud Afifi",
        "user": "mafifi",
        "type": "user"
      },
      "summary": "This paper presents a modular neural image signal processing (ISP) framework that processes raw inputs and renders high-quality display-referred images. Unlike prior neural ISP designs, our method introduces a high degree of modularity, providing full control over multiple intermediate stages of the rendering process.~This modular design not only achieves high rendering accuracy but also improves scalability, debuggability, generalization to unseen cameras, and flexibility to match different user-preference styles. To demonstrate the advantages of this design, we built a user-interactive photo-editing tool that leverages our neural ISP to support diverse editing operations and picture styles. The tool is carefully engineered to take advantage of the high-quality rendering of our neural ISP and to enable unlimited post-editable re-rendering. Our method is a fully learning-based framework with variants of different capacities, all of moderate size (ranging from ~0.5 M to ~3.9 M parameters for the entire pipeline), and consistently delivers competitive qualitative and quantitative results across multiple test sets. Watch the supplemental video at: https://youtu.be/ByhQjQSjxVM",
      "upvotes": 2,
      "discussionId": "6938fc78dfc35938ba12a130",
      "githubRepo": "https://github.com/mahmoudnafifi/modular_neural_isp",
      "ai_summary": "A modular neural ISP framework provides high rendering accuracy, scalability, and flexibility for diverse photo-editing operations with competitive results.",
      "ai_keywords": [
        "neural ISP",
        "modular design",
        "rendering accuracy",
        "scalability",
        "debuggability",
        "generalization",
        "user-preference styles",
        "photo-editing tool",
        "post-editable re-rendering"
      ],
      "githubStars": 0
    },
    "publishedAt": "2025-12-09T08:04:08.000Z",
    "title": "Modular Neural Image Signal Processing",
    "summary": "This paper presents a modular neural image signal processing (ISP) framework that processes raw inputs and renders high-quality display-referred images. Unlike prior neural ISP designs, our method introduces a high degree of modularity, providing full control over multiple intermediate stages of the rendering process.~This modular design not only achieves high rendering accuracy but also improves scalability, debuggability, generalization to unseen cameras, and flexibility to match different user-preference styles. To demonstrate the advantages of this design, we built a user-interactive photo-editing tool that leverages our neural ISP to support diverse editing operations and picture styles. The tool is carefully engineered to take advantage of the high-quality rendering of our neural ISP and to enable unlimited post-editable re-rendering. Our method is a fully learning-based framework with variants of different capacities, all of moderate size (ranging from ~0.5 M to ~3.9 M parameters for the entire pipeline), and consistently delivers competitive qualitative and quantitative results across multiple test sets. Watch the supplemental video at: https://youtu.be/ByhQjQSjxVM",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.08564.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "6574973c05e573071548922c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6574973c05e573071548922c/KT-KAo4aepdpbZguXfKPv.jpeg",
      "fullname": "Mahmoud Afifi",
      "name": "mafifi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.08186",
      "authors": [
        {
          "_id": "6938ecf3dfc35938ba12a01e",
          "name": "Meng Wei",
          "hidden": false
        },
        {
          "_id": "6938ecf3dfc35938ba12a01f",
          "name": "Chenyang Wan",
          "hidden": false
        },
        {
          "_id": "6938ecf3dfc35938ba12a020",
          "name": "Jiaqi Peng",
          "hidden": false
        },
        {
          "_id": "6938ecf3dfc35938ba12a021",
          "name": "Xiqian Yu",
          "hidden": false
        },
        {
          "_id": "6938ecf3dfc35938ba12a022",
          "name": "Yuqiang Yang",
          "hidden": false
        },
        {
          "_id": "6938ecf3dfc35938ba12a023",
          "name": "Delin Feng",
          "hidden": false
        },
        {
          "_id": "6938ecf3dfc35938ba12a024",
          "name": "Wenzhe Cai",
          "hidden": false
        },
        {
          "_id": "6938ecf3dfc35938ba12a025",
          "name": "Chenming Zhu",
          "hidden": false
        },
        {
          "_id": "6938ecf3dfc35938ba12a026",
          "name": "Tai Wang",
          "hidden": false
        },
        {
          "_id": "6938ecf3dfc35938ba12a027",
          "name": "Jiangmiao Pang",
          "hidden": false
        },
        {
          "_id": "6938ecf3dfc35938ba12a028",
          "name": "Xihui Liu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6440fc05603214724eba4766/-paN7Eygd15byTE9X6DIj.mp4"
      ],
      "publishedAt": "2025-12-09T02:29:36.000Z",
      "submittedOnDailyAt": "2025-12-10T02:51:53.938Z",
      "title": "Ground Slow, Move Fast: A Dual-System Foundation Model for Generalizable Vision-and-Language Navigation",
      "submittedOnDailyBy": {
        "_id": "6440fc05603214724eba4766",
        "avatarUrl": "/avatars/1a82a3361c96ba7bfd429dbd3e6f0bad.svg",
        "isPro": false,
        "fullname": "weimeng",
        "user": "mengwei0427",
        "type": "user"
      },
      "summary": "While recent large vision-language models (VLMs) have improved generalization in vision-language navigation (VLN), existing methods typically rely on end-to-end pipelines that map vision-language inputs directly to short-horizon discrete actions. Such designs often produce fragmented motions, incur high latency, and struggle with real-world challenges like dynamic obstacle avoidance. We propose DualVLN, the first dual-system VLN foundation model that synergistically integrates high-level reasoning with low-level action execution. System 2, a VLM-based global planner, \"grounds slowly\" by predicting mid-term waypoint goals via image-grounded reasoning. System 1, a lightweight, multi-modal conditioning Diffusion Transformer policy, \"moves fast\" by leveraging both explicit pixel goals and latent features from System 2 to generate smooth and accurate trajectories. The dual-system design enables robust real-time control and adaptive local decision-making in complex, dynamic environments. By decoupling training, the VLM retains its generalization, while System 1 achieves interpretable and effective local navigation. DualVLN outperforms prior methods across all VLN benchmarks and real-world experiments demonstrate robust long-horizon planning and real-time adaptability in dynamic environments.",
      "upvotes": 2,
      "discussionId": "6938ecf3dfc35938ba12a029",
      "projectPage": "https://internrobotics.github.io/internvla-n1-dualvln.github.io/",
      "githubRepo": "https://github.com/InternRobotics/InternNav",
      "ai_summary": "DualVLN integrates high-level reasoning and low-level action execution to improve vision-language navigation in dynamic environments, achieving robust real-time control and long-horizon planning.",
      "ai_keywords": [
        "VLMs",
        "vision-language navigation",
        "VLN",
        "dual-system",
        "global planner",
        "image-grounded reasoning",
        "Diffusion Transformer",
        "pixel goals",
        "latent features",
        "real-time control",
        "adaptive local decision-making",
        "long-horizon planning",
        "real-time adaptability"
      ],
      "githubStars": 448,
      "organization": {
        "_id": "6881c146ff13df8b65153273",
        "name": "InternRobotics",
        "fullname": "Intern Robotics",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65d9f09bbcd15bc5cb255fed/REfA3nEK1_Y-PTfGn_5H1.jpeg"
      }
    },
    "publishedAt": "2025-12-08T21:29:36.000Z",
    "title": "Ground Slow, Move Fast: A Dual-System Foundation Model for Generalizable Vision-and-Language Navigation",
    "summary": "While recent large vision-language models (VLMs) have improved generalization in vision-language navigation (VLN), existing methods typically rely on end-to-end pipelines that map vision-language inputs directly to short-horizon discrete actions. Such designs often produce fragmented motions, incur high latency, and struggle with real-world challenges like dynamic obstacle avoidance. We propose DualVLN, the first dual-system VLN foundation model that synergistically integrates high-level reasoning with low-level action execution. System 2, a VLM-based global planner, \"grounds slowly\" by predicting mid-term waypoint goals via image-grounded reasoning. System 1, a lightweight, multi-modal conditioning Diffusion Transformer policy, \"moves fast\" by leveraging both explicit pixel goals and latent features from System 2 to generate smooth and accurate trajectories. The dual-system design enables robust real-time control and adaptive local decision-making in complex, dynamic environments. By decoupling training, the VLM retains its generalization, while System 1 achieves interpretable and effective local navigation. DualVLN outperforms prior methods across all VLN benchmarks and real-world experiments demonstrate robust long-horizon planning and real-time adaptability in dynamic environments.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6440fc05603214724eba4766/-paN7Eygd15byTE9X6DIj.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.08186.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6440fc05603214724eba4766",
      "avatarUrl": "/avatars/1a82a3361c96ba7bfd429dbd3e6f0bad.svg",
      "fullname": "weimeng",
      "name": "mengwei0427",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "6881c146ff13df8b65153273",
      "name": "InternRobotics",
      "fullname": "Intern Robotics",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65d9f09bbcd15bc5cb255fed/REfA3nEK1_Y-PTfGn_5H1.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.07921",
      "authors": [
        {
          "_id": "6938e671dfc35938ba129fd5",
          "name": "Zongwei Li",
          "hidden": false
        },
        {
          "_id": "6938e671dfc35938ba129fd6",
          "name": "Zhonghang Li",
          "hidden": false
        },
        {
          "_id": "6938e671dfc35938ba129fd7",
          "name": "Zirui Guo",
          "hidden": false
        },
        {
          "_id": "6938e671dfc35938ba129fd8",
          "name": "Xubin Ren",
          "hidden": false
        },
        {
          "_id": "6938e671dfc35938ba129fd9",
          "name": "Chao Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-08T16:07:13.000Z",
      "submittedOnDailyAt": "2025-12-10T00:48:19.359Z",
      "title": "DeepCode: Open Agentic Coding",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Recent advances in large language models (LLMs) have given rise to powerful coding agents, making it possible for code assistants to evolve into code engineers. However, existing methods still face significant challenges in achieving high-fidelity document-to-codebase synthesis--such as scientific papers to code--primarily due to a fundamental conflict between information overload and the context bottlenecks of LLMs. In this work, we introduce DeepCode, a fully autonomous framework that fundamentally addresses this challenge through principled information-flow management. By treating repository synthesis as a channel optimization problem, DeepCode seamlessly orchestrates four information operations to maximize task-relevant signals under finite context budgets: source compression via blueprint distillation, structured indexing using stateful code memory, conditional knowledge injection via retrieval-augmented generation, and closed-loop error correction. Extensive evaluations on the PaperBench benchmark demonstrate that DeepCode achieves state-of-the-art performance, decisively outperforming leading commercial agents such as Cursor and Claude Code, and crucially, surpassing PhD-level human experts from top institutes on key reproduction metrics. By systematically transforming paper specifications into production-grade implementations comparable to human expert quality, this work establishes new foundations for autonomous scientific reproduction that can accelerate research evaluation and discovery.",
      "upvotes": 2,
      "discussionId": "6938e672dfc35938ba129fda",
      "githubRepo": "https://github.com/HKUDS/DeepCode",
      "ai_summary": "DeepCode, a fully autonomous framework, addresses the challenges of document-to-codebase synthesis by optimizing information flow through source compression, structured indexing, knowledge injection, and error correction, achieving state-of-the-art performance and surpassing human experts.",
      "ai_keywords": [
        "large language models",
        "coding agents",
        "document-to-codebase synthesis",
        "information overload",
        "context bottlenecks",
        "DeepCode",
        "channel optimization",
        "blueprint distillation",
        "stateful code memory",
        "retrieval-augmented generation",
        "closed-loop error correction",
        "PaperBench",
        "autonomous scientific reproduction"
      ],
      "githubStars": 11705
    },
    "publishedAt": "2025-12-08T11:07:13.000Z",
    "title": "DeepCode: Open Agentic Coding",
    "summary": "Recent advances in large language models (LLMs) have given rise to powerful coding agents, making it possible for code assistants to evolve into code engineers. However, existing methods still face significant challenges in achieving high-fidelity document-to-codebase synthesis--such as scientific papers to code--primarily due to a fundamental conflict between information overload and the context bottlenecks of LLMs. In this work, we introduce DeepCode, a fully autonomous framework that fundamentally addresses this challenge through principled information-flow management. By treating repository synthesis as a channel optimization problem, DeepCode seamlessly orchestrates four information operations to maximize task-relevant signals under finite context budgets: source compression via blueprint distillation, structured indexing using stateful code memory, conditional knowledge injection via retrieval-augmented generation, and closed-loop error correction. Extensive evaluations on the PaperBench benchmark demonstrate that DeepCode achieves state-of-the-art performance, decisively outperforming leading commercial agents such as Cursor and Claude Code, and crucially, surpassing PhD-level human experts from top institutes on key reproduction metrics. By systematically transforming paper specifications into production-grade implementations comparable to human expert quality, this work establishes new foundations for autonomous scientific reproduction that can accelerate research evaluation and discovery.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.07921.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 181
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.08358",
      "authors": [
        {
          "_id": "6938e0d4dfc35938ba129f70",
          "name": "Jiahao Lu",
          "hidden": false
        },
        {
          "_id": "6938e0d4dfc35938ba129f71",
          "name": "Weitao Xiong",
          "hidden": false
        },
        {
          "_id": "6938e0d4dfc35938ba129f72",
          "name": "Jiacheng Deng",
          "hidden": false
        },
        {
          "_id": "6938e0d4dfc35938ba129f73",
          "name": "Peng Li",
          "hidden": false
        },
        {
          "_id": "6938e0d4dfc35938ba129f74",
          "name": "Tianyu Huang",
          "hidden": false
        },
        {
          "_id": "6938e0d4dfc35938ba129f75",
          "name": "Zhiyang Dou",
          "hidden": false
        },
        {
          "_id": "6938e0d4dfc35938ba129f76",
          "name": "Cheng Lin",
          "hidden": false
        },
        {
          "_id": "6938e0d4dfc35938ba129f77",
          "name": "Sai-Kit Yeung",
          "hidden": false
        },
        {
          "_id": "6938e0d4dfc35938ba129f78",
          "name": "Yuan Liu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/IVOTOtMI7zOMowz45koOL.mp4"
      ],
      "publishedAt": "2025-12-09T08:35:42.000Z",
      "submittedOnDailyAt": "2025-12-10T00:25:01.396Z",
      "title": "TrackingWorld: World-centric Monocular 3D Tracking of Almost All Pixels",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Monocular 3D tracking aims to capture the long-term motion of pixels in 3D space from a single monocular video and has witnessed rapid progress in recent years. However, we argue that the existing monocular 3D tracking methods still fall short in separating the camera motion from foreground dynamic motion and cannot densely track newly emerging dynamic subjects in the videos. To address these two limitations, we propose TrackingWorld, a novel pipeline for dense 3D tracking of almost all pixels within a world-centric 3D coordinate system. First, we introduce a tracking upsampler that efficiently lifts the arbitrary sparse 2D tracks into dense 2D tracks. Then, to generalize the current tracking methods to newly emerging objects, we apply the upsampler to all frames and reduce the redundancy of 2D tracks by eliminating the tracks in overlapped regions. Finally, we present an efficient optimization-based framework to back-project dense 2D tracks into world-centric 3D trajectories by estimating the camera poses and the 3D coordinates of these 2D tracks. Extensive evaluations on both synthetic and real-world datasets demonstrate that our system achieves accurate and dense 3D tracking in a world-centric coordinate frame.",
      "upvotes": 0,
      "discussionId": "6938e0d4dfc35938ba129f79",
      "projectPage": "https://igl-hkust.github.io/TrackingWorld.github.io/",
      "ai_summary": "TrackingWorld provides dense 3D tracking of pixels in a world-centric coordinate system by upsampling sparse 2D tracks and optimizing camera poses and 3D coordinates.",
      "ai_keywords": [
        "monocular 3D tracking",
        "tracking upsampler",
        "dense 3D tracking",
        "world-centric 3D coordinate system",
        "2D tracks",
        "camera poses",
        "3D coordinates",
        "optimization-based framework",
        "synthetic datasets",
        "real-world datasets"
      ]
    },
    "publishedAt": "2025-12-09T03:35:42.000Z",
    "title": "TrackingWorld: World-centric Monocular 3D Tracking of Almost All Pixels",
    "summary": "Monocular 3D tracking aims to capture the long-term motion of pixels in 3D space from a single monocular video and has witnessed rapid progress in recent years. However, we argue that the existing monocular 3D tracking methods still fall short in separating the camera motion from foreground dynamic motion and cannot densely track newly emerging dynamic subjects in the videos. To address these two limitations, we propose TrackingWorld, a novel pipeline for dense 3D tracking of almost all pixels within a world-centric 3D coordinate system. First, we introduce a tracking upsampler that efficiently lifts the arbitrary sparse 2D tracks into dense 2D tracks. Then, to generalize the current tracking methods to newly emerging objects, we apply the upsampler to all frames and reduce the redundancy of 2D tracks by eliminating the tracks in overlapped regions. Finally, we present an efficient optimization-based framework to back-project dense 2D tracks into world-centric 3D trajectories by estimating the camera poses and the 3D coordinates of these 2D tracks. Extensive evaluations on both synthetic and real-world datasets demonstrate that our system achieves accurate and dense 3D tracking in a world-centric coordinate frame.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/IVOTOtMI7zOMowz45koOL.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.08358.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 181
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.06776",
      "authors": [
        {
          "_id": "693925a0dfc35938ba12a166",
          "name": "Yuchuan Tian",
          "hidden": false
        },
        {
          "_id": "693925a0dfc35938ba12a167",
          "name": "Yuchen Liang",
          "hidden": false
        },
        {
          "_id": "693925a0dfc35938ba12a168",
          "name": "Jiacheng Sun",
          "hidden": false
        },
        {
          "_id": "693925a0dfc35938ba12a169",
          "name": "Shuo Zhang",
          "hidden": false
        },
        {
          "_id": "693925a0dfc35938ba12a16a",
          "name": "Guangwen Yang",
          "hidden": false
        },
        {
          "_id": "693925a0dfc35938ba12a16b",
          "name": "Yingte Shu",
          "hidden": false
        },
        {
          "_id": "693925a0dfc35938ba12a16c",
          "name": "Sibo Fang",
          "hidden": false
        },
        {
          "_id": "693925a0dfc35938ba12a16d",
          "name": "Tianyu Guo",
          "hidden": false
        },
        {
          "_id": "693925a0dfc35938ba12a16e",
          "name": "Kai Han",
          "hidden": false
        },
        {
          "_id": "693925a0dfc35938ba12a16f",
          "name": "Chao Xu",
          "hidden": false
        },
        {
          "_id": "693925a0dfc35938ba12a170",
          "name": "Hanting Chen",
          "hidden": false
        },
        {
          "_id": "693925a0dfc35938ba12a171",
          "name": "Xinghao Chen",
          "hidden": false
        },
        {
          "_id": "693925a0dfc35938ba12a172",
          "name": "Yunhe Wang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64ba7a73a01da43118fce871/7Siv6RPU1ej8LtLmucAF7.gif"
      ],
      "publishedAt": "2025-12-07T10:28:21.000Z",
      "submittedOnDailyAt": "2025-12-10T05:20:49.824Z",
      "title": "From Next-Token to Next-Block: A Principled Adaptation Path for Diffusion LLMs",
      "submittedOnDailyBy": {
        "_id": "64ba7a73a01da43118fce871",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ba7a73a01da43118fce871/x2QDw6N8QRZkwwYOLNLYD.png",
        "isPro": false,
        "fullname": "YuchuanTian",
        "user": "yuchuantian",
        "type": "user"
      },
      "summary": "Large language models (LLMs) excel at generation but dominant autoregressive (AR) decoding is inherently sequential, creating a throughput bottleneck. Diffusion Language Models (DLMs)--especially block-wise variants--enable parallel generation and intra-block bidirectional reasoning, yet training large DLMs from scratch is costly and wastes the knowledge in mature AR checkpoints. Prior \"adaptation\" attempts either modify logits or randomly grow attention masks to full-sequence diffusion, or simply transplant AR weights into a block-diffusion recipe, leaving a fundamental mismatch between AR causality and block-wise bidirectionality unaddressed. We reframe adaptation as a intra-paradigm path from AR to Block-Diffusion by viewing AR as Block-Diffusion with blocksize=1. Concretely, we design the pathway of adaptation as follows: we use a context-causal attention mask (causal in context, bidirectional only within the active block), an efficient parallel adaptation procedure, an auxiliary AR loss to maximize data utilization and retain pretrained knowledge, and gradual increment of the generation block size. The recipe integrates cleanly with masked block-diffusion and maintains train-inference consistency. Built on these components, NBDiff-7B (Base and Instruct) could inherit the long-context modeling and reasoning capabilities, and achieve state-of-the-art performance among the 7B-class DLMs, delivering strong gains on general-knowledge, math, and code benchmarks over strong baselines. These results demonstrate that principled AR-to-block-diffusion adaptation is an effective and compute-efficient alternative to training DLMs from scratch. Codes: https://github.com/YuchuanTian/NBDiff.",
      "upvotes": 0,
      "discussionId": "693925a1dfc35938ba12a173",
      "ai_summary": "Adapting autoregressive models to block-wise diffusion enables parallel generation and retains pretrained knowledge, achieving state-of-the-art performance among 7B-class diffusion language models.",
      "ai_keywords": [
        "Large language models",
        "autoregressive decoding",
        "diffusion language models",
        "block-wise variants",
        "parallel generation",
        "intra-block bidirectional reasoning",
        "context-causal attention mask",
        "parallel adaptation procedure",
        "auxiliary AR loss",
        "masked block-diffusion",
        "train-inference consistency",
        "NBDiff-7B"
      ],
      "organization": {
        "_id": "61dcd8e344f59573371b5cb6",
        "name": "PekingUniversity",
        "fullname": "Peking University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
      }
    },
    "publishedAt": "2025-12-07T05:28:21.000Z",
    "title": "From Next-Token to Next-Block: A Principled Adaptation Path for Diffusion LLMs",
    "summary": "Large language models (LLMs) excel at generation but dominant autoregressive (AR) decoding is inherently sequential, creating a throughput bottleneck. Diffusion Language Models (DLMs)--especially block-wise variants--enable parallel generation and intra-block bidirectional reasoning, yet training large DLMs from scratch is costly and wastes the knowledge in mature AR checkpoints. Prior \"adaptation\" attempts either modify logits or randomly grow attention masks to full-sequence diffusion, or simply transplant AR weights into a block-diffusion recipe, leaving a fundamental mismatch between AR causality and block-wise bidirectionality unaddressed. We reframe adaptation as a intra-paradigm path from AR to Block-Diffusion by viewing AR as Block-Diffusion with blocksize=1. Concretely, we design the pathway of adaptation as follows: we use a context-causal attention mask (causal in context, bidirectional only within the active block), an efficient parallel adaptation procedure, an auxiliary AR loss to maximize data utilization and retain pretrained knowledge, and gradual increment of the generation block size. The recipe integrates cleanly with masked block-diffusion and maintains train-inference consistency. Built on these components, NBDiff-7B (Base and Instruct) could inherit the long-context modeling and reasoning capabilities, and achieve state-of-the-art performance among the 7B-class DLMs, delivering strong gains on general-knowledge, math, and code benchmarks over strong baselines. These results demonstrate that principled AR-to-block-diffusion adaptation is an effective and compute-efficient alternative to training DLMs from scratch. Codes: https://github.com/YuchuanTian/NBDiff.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64ba7a73a01da43118fce871/7Siv6RPU1ej8LtLmucAF7.gif"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.06776.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ba7a73a01da43118fce871",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ba7a73a01da43118fce871/x2QDw6N8QRZkwwYOLNLYD.png",
      "fullname": "YuchuanTian",
      "name": "yuchuantian",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "organization": {
      "_id": "61dcd8e344f59573371b5cb6",
      "name": "PekingUniversity",
      "fullname": "Peking University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
    },
    "isAuthorParticipating": false
  }
]