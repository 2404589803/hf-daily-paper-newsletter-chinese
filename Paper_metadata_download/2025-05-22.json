[
  {
    "paper": {
      "id": "2505.15277",
      "authors": [
        {
          "_id": "682e854551706f69070aca6b",
          "user": {
            "_id": "64c8f4cec547ed5243ebd0a8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c8f4cec547ed5243ebd0a8/MiOH5YbMg8Gh9KYlQsLmX.jpeg",
            "isPro": false,
            "fullname": "Hyungjoo Chae",
            "user": "hyungjoochae",
            "type": "user"
          },
          "name": "Hyungjoo Chae",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:16:37.301Z",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca6c",
          "user": {
            "_id": "646a0897c37ca1e12308b026",
            "avatarUrl": "/avatars/6d720a9e366db9bec15c8c10878c0c75.svg",
            "isPro": false,
            "fullname": "Sunghwan Kim",
            "user": "KimSHine",
            "type": "user"
          },
          "name": "Sunghwan Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:16:32.322Z",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca6d",
          "name": "Junhee Cho",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca6e",
          "name": "Seungone Kim",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca6f",
          "name": "Seungjun Moon",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca70",
          "name": "Gyeom Hwangbo",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca71",
          "user": {
            "_id": "6683b8680b72be136701de35",
            "avatarUrl": "/avatars/0c135e570b16b81ee2fb81ad65b01ba8.svg",
            "isPro": false,
            "fullname": "Dongha Lim",
            "user": "donghalim",
            "type": "user"
          },
          "name": "Dongha Lim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:16:34.741Z",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca72",
          "name": "Minjin Kim",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca73",
          "name": "Yeonjun Hwang",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca74",
          "name": "Minju Gwak",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca75",
          "name": "Dongwook Choi",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca76",
          "name": "Minseok Kang",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca77",
          "name": "Gwanhoon Im",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca78",
          "name": "ByeongUng Cho",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca79",
          "name": "Hyojun Kim",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca7a",
          "name": "Jun Hee Han",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca7b",
          "name": "Taeyoon Kwon",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca7c",
          "name": "Minju Kim",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca7d",
          "name": "Beong-woo Kwak",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca7e",
          "name": "Dongjin Kang",
          "hidden": false
        },
        {
          "_id": "682e854551706f69070aca7f",
          "name": "Jinyoung Yeo",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64c8f4cec547ed5243ebd0a8/hXqaaoJTvW35xMW1lPVv0.png"
      ],
      "publishedAt": "2025-05-21T08:56:55.000Z",
      "submittedOnDailyAt": "2025-05-22T00:31:53.858Z",
      "title": "Web-Shepherd: Advancing PRMs for Reinforcing Web Agents",
      "submittedOnDailyBy": {
        "_id": "64c8f4cec547ed5243ebd0a8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c8f4cec547ed5243ebd0a8/MiOH5YbMg8Gh9KYlQsLmX.jpeg",
        "isPro": false,
        "fullname": "Hyungjoo Chae",
        "user": "hyungjoochae",
        "type": "user"
      },
      "summary": "Web navigation is a unique domain that can automate many repetitive real-life\ntasks and is challenging as it requires long-horizon sequential decision making\nbeyond typical multimodal large language model (MLLM) tasks. Yet, specialized\nreward models for web navigation that can be utilized during both training and\ntest-time have been absent until now. Despite the importance of speed and\ncost-effectiveness, prior works have utilized MLLMs as reward models, which\nposes significant constraints for real-world deployment. To address this, in\nthis work, we propose the first process reward model (PRM) called Web-Shepherd\nwhich could assess web navigation trajectories in a step-level. To achieve\nthis, we first construct the WebPRM Collection, a large-scale dataset with 40K\nstep-level preference pairs and annotated checklists spanning diverse domains\nand difficulty levels. Next, we also introduce the WebRewardBench, the first\nmeta-evaluation benchmark for evaluating PRMs. In our experiments, we observe\nthat our Web-Shepherd achieves about 30 points better accuracy compared to\nusing GPT-4o on WebRewardBench. Furthermore, when testing on WebArena-lite by\nusing GPT-4o-mini as the policy and Web-Shepherd as the verifier, we achieve\n10.9 points better performance, in 10 less cost compared to using GPT-4o-mini\nas the verifier. Our model, dataset, and code are publicly available at LINK.",
      "upvotes": 66,
      "discussionId": "682e854951706f69070acbf0",
      "githubRepo": "https://github.com/kyle8581/Web-Shepherd",
      "ai_summary": "The paper introduces Web-Shepherd, a process reward model for web navigation, which improves accuracy and cost-effectiveness in step-level trajectory assessment compared to existing multimodal large language models.",
      "ai_keywords": [
        "multimodal large language model",
        "process reward model",
        "web navigation",
        "webPRM collection",
        "webrewardbench",
        "long-horizon sequential decision making",
        "preference pairs",
        "annotated checklists",
        "step-level assessment",
        "webarena-lite",
        "policy",
        "verifier"
      ]
    },
    "publishedAt": "2025-05-21T04:56:55.000Z",
    "title": "Web-Shepherd: Advancing PRMs for Reinforcing Web Agents",
    "summary": "Web navigation is a unique domain that can automate many repetitive real-life\ntasks and is challenging as it requires long-horizon sequential decision making\nbeyond typical multimodal large language model (MLLM) tasks. Yet, specialized\nreward models for web navigation that can be utilized during both training and\ntest-time have been absent until now. Despite the importance of speed and\ncost-effectiveness, prior works have utilized MLLMs as reward models, which\nposes significant constraints for real-world deployment. To address this, in\nthis work, we propose the first process reward model (PRM) called Web-Shepherd\nwhich could assess web navigation trajectories in a step-level. To achieve\nthis, we first construct the WebPRM Collection, a large-scale dataset with 40K\nstep-level preference pairs and annotated checklists spanning diverse domains\nand difficulty levels. Next, we also introduce the WebRewardBench, the first\nmeta-evaluation benchmark for evaluating PRMs. In our experiments, we observe\nthat our Web-Shepherd achieves about 30 points better accuracy compared to\nusing GPT-4o on WebRewardBench. Furthermore, when testing on WebArena-lite by\nusing GPT-4o-mini as the policy and Web-Shepherd as the verifier, we achieve\n10.9 points better performance, in 10 less cost compared to using GPT-4o-mini\nas the verifier. Our model, dataset, and code are publicly available at LINK.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64c8f4cec547ed5243ebd0a8/hXqaaoJTvW35xMW1lPVv0.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15277.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64c8f4cec547ed5243ebd0a8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c8f4cec547ed5243ebd0a8/MiOH5YbMg8Gh9KYlQsLmX.jpeg",
      "fullname": "Hyungjoo Chae",
      "name": "hyungjoochae",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.14302",
      "authors": [
        {
          "_id": "682e887e866a44f6a81409b1",
          "user": {
            "_id": "64aea082704210bf815e7551",
            "avatarUrl": "/avatars/5c8dc0df57596c526b2bccea21835f53.svg",
            "isPro": false,
            "fullname": "Mengzhao Chen",
            "user": "ChenMnZ",
            "type": "user"
          },
          "name": "Mengzhao Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:16:29.873Z",
          "hidden": false
        },
        {
          "_id": "682e887e866a44f6a81409b2",
          "name": "Chaoyi Zhang",
          "hidden": false
        },
        {
          "_id": "682e887e866a44f6a81409b3",
          "name": "Jing Liu",
          "hidden": false
        },
        {
          "_id": "682e887e866a44f6a81409b4",
          "name": "Yutao Zeng",
          "hidden": false
        },
        {
          "_id": "682e887e866a44f6a81409b5",
          "name": "Zeyue Xue",
          "hidden": false
        },
        {
          "_id": "682e887e866a44f6a81409b6",
          "name": "Zhiheng Liu",
          "hidden": false
        },
        {
          "_id": "682e887e866a44f6a81409b7",
          "name": "Yunshui Li",
          "hidden": false
        },
        {
          "_id": "682e887e866a44f6a81409b8",
          "name": "Jin Ma",
          "hidden": false
        },
        {
          "_id": "682e887e866a44f6a81409b9",
          "name": "Jie Huang",
          "hidden": false
        },
        {
          "_id": "682e887e866a44f6a81409ba",
          "name": "Xun Zhou",
          "hidden": false
        },
        {
          "_id": "682e887e866a44f6a81409bb",
          "name": "Ping Luo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T12:54:43.000Z",
      "submittedOnDailyAt": "2025-05-22T00:44:46.277Z",
      "title": "Scaling Law for Quantization-Aware Training",
      "submittedOnDailyBy": {
        "_id": "64aea082704210bf815e7551",
        "avatarUrl": "/avatars/5c8dc0df57596c526b2bccea21835f53.svg",
        "isPro": false,
        "fullname": "Mengzhao Chen",
        "user": "ChenMnZ",
        "type": "user"
      },
      "summary": "Large language models (LLMs) demand substantial computational and memory\nresources, creating deployment challenges. Quantization-aware training (QAT)\naddresses these challenges by reducing model precision while maintaining\nperformance. However, the scaling behavior of QAT, especially at 4-bit\nprecision (W4A4), is not well understood. Existing QAT scaling laws often\nignore key factors such as the number of training tokens and quantization\ngranularity, which limits their applicability. This paper proposes a unified\nscaling law for QAT that models quantization error as a function of model size,\ntraining data volume, and quantization group size. Through 268 QAT experiments,\nwe show that quantization error decreases as model size increases, but rises\nwith more training tokens and coarser quantization granularity. To identify the\nsources of W4A4 quantization error, we decompose it into weight and activation\ncomponents. Both components follow the overall trend of W4A4 quantization\nerror, but with different sensitivities. Specifically, weight quantization\nerror increases more rapidly with more training tokens. Further analysis shows\nthat the activation quantization error in the FC2 layer, caused by outliers, is\nthe primary bottleneck of W4A4 QAT quantization error. By applying\nmixed-precision quantization to address this bottleneck, we demonstrate that\nweight and activation quantization errors can converge to similar levels.\nAdditionally, with more training data, weight quantization error eventually\nexceeds activation quantization error, suggesting that reducing weight\nquantization error is also important in such scenarios. These findings offer\nkey insights for improving QAT research and development.",
      "upvotes": 45,
      "discussionId": "682e887e866a44f6a81409f0",
      "ai_summary": "A unified scaling law for quantization-aware training (QAT) identifies key factors affecting quantization error, leading to improvements through mixed-precision quantization.",
      "ai_keywords": [
        "quantization-aware training",
        "QAT",
        "quantization error",
        "model size",
        "training tokens",
        "quantization granularity",
        "weight quantization",
        "activation quantization",
        "mixed-precision quantization",
        "FC2 layer"
      ]
    },
    "publishedAt": "2025-05-20T08:54:43.000Z",
    "title": "Scaling Law for Quantization-Aware Training",
    "summary": "Large language models (LLMs) demand substantial computational and memory\nresources, creating deployment challenges. Quantization-aware training (QAT)\naddresses these challenges by reducing model precision while maintaining\nperformance. However, the scaling behavior of QAT, especially at 4-bit\nprecision (W4A4), is not well understood. Existing QAT scaling laws often\nignore key factors such as the number of training tokens and quantization\ngranularity, which limits their applicability. This paper proposes a unified\nscaling law for QAT that models quantization error as a function of model size,\ntraining data volume, and quantization group size. Through 268 QAT experiments,\nwe show that quantization error decreases as model size increases, but rises\nwith more training tokens and coarser quantization granularity. To identify the\nsources of W4A4 quantization error, we decompose it into weight and activation\ncomponents. Both components follow the overall trend of W4A4 quantization\nerror, but with different sensitivities. Specifically, weight quantization\nerror increases more rapidly with more training tokens. Further analysis shows\nthat the activation quantization error in the FC2 layer, caused by outliers, is\nthe primary bottleneck of W4A4 QAT quantization error. By applying\nmixed-precision quantization to address this bottleneck, we demonstrate that\nweight and activation quantization errors can converge to similar levels.\nAdditionally, with more training data, weight quantization error eventually\nexceeds activation quantization error, suggesting that reducing weight\nquantization error is also important in such scenarios. These findings offer\nkey insights for improving QAT research and development.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14302.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64aea082704210bf815e7551",
      "avatarUrl": "/avatars/5c8dc0df57596c526b2bccea21835f53.svg",
      "fullname": "Mengzhao Chen",
      "name": "ChenMnZ",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 22
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.15809",
      "authors": [
        {
          "_id": "682e7e061d7637a25846bf52",
          "name": "Ling Yang",
          "hidden": false
        },
        {
          "_id": "682e7e061d7637a25846bf53",
          "user": {
            "_id": "64e357dd825f4133e7427bf8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e357dd825f4133e7427bf8/HwaWhINrzkbXG6SHG2oyf.jpeg",
            "isPro": false,
            "fullname": "tyfeld",
            "user": "tyfeld",
            "type": "user"
          },
          "name": "Ye Tian",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:17:11.786Z",
          "hidden": false
        },
        {
          "_id": "682e7e061d7637a25846bf54",
          "name": "Bowen Li",
          "hidden": false
        },
        {
          "_id": "682e7e061d7637a25846bf55",
          "user": {
            "_id": "653e5d31ffd60206c8b64bb5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653e5d31ffd60206c8b64bb5/JrfOSCunFSW39vdW7E59y.png",
            "isPro": false,
            "fullname": "Xinchen Zhang",
            "user": "comin",
            "type": "user"
          },
          "name": "Xinchen Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:17:14.114Z",
          "hidden": false
        },
        {
          "_id": "682e7e061d7637a25846bf56",
          "name": "Ke Shen",
          "hidden": false
        },
        {
          "_id": "682e7e061d7637a25846bf57",
          "name": "Yunhai Tong",
          "hidden": false
        },
        {
          "_id": "682e7e061d7637a25846bf58",
          "name": "Mengdi Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T17:59:05.000Z",
      "submittedOnDailyAt": "2025-05-22T00:24:15.122Z",
      "title": "MMaDA: Multimodal Large Diffusion Language Models",
      "submittedOnDailyBy": {
        "_id": "64fde4e252e82dd432b74ce9",
        "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
        "isPro": false,
        "fullname": "Ling Yang",
        "user": "Lingaaaaaaa",
        "type": "user"
      },
      "summary": "We introduce MMaDA, a novel class of multimodal diffusion foundation models\ndesigned to achieve superior performance across diverse domains such as textual\nreasoning, multimodal understanding, and text-to-image generation. The approach\nis distinguished by three key innovations: (i) MMaDA adopts a unified diffusion\narchitecture with a shared probabilistic formulation and a modality-agnostic\ndesign, eliminating the need for modality-specific components. This\narchitecture ensures seamless integration and processing across different data\ntypes. (ii) We implement a mixed long chain-of-thought (CoT) fine-tuning\nstrategy that curates a unified CoT format across modalities. By aligning\nreasoning processes between textual and visual domains, this strategy\nfacilitates cold-start training for the final reinforcement learning (RL)\nstage, thereby enhancing the model's ability to handle complex tasks from the\noutset. (iii) We propose UniGRPO, a unified policy-gradient-based RL algorithm\nspecifically tailored for diffusion foundation models. Utilizing diversified\nreward modeling, UniGRPO unifies post-training across both reasoning and\ngeneration tasks, ensuring consistent performance improvements. Experimental\nresults demonstrate that MMaDA-8B exhibits strong generalization capabilities\nas a unified multimodal foundation model. It surpasses powerful models like\nLLaMA-3-7B and Qwen2-7B in textual reasoning, outperforms Show-o and SEED-X in\nmultimodal understanding, and excels over SDXL and Janus in text-to-image\ngeneration. These achievements highlight MMaDA's effectiveness in bridging the\ngap between pretraining and post-training within unified diffusion\narchitectures, providing a comprehensive framework for future research and\ndevelopment. We open-source our code and trained models at:\nhttps://github.com/Gen-Verse/MMaDA",
      "upvotes": 31,
      "discussionId": "682e7e0a1d7637a25846c03b",
      "projectPage": "https://huggingface.co/spaces/Gen-Verse/MMaDA",
      "githubRepo": "https://github.com/Gen-Verse/MMaDA",
      "ai_summary": "MMaDA, a multimodal diffusion foundation model, achieves superior performance through a unified architecture, mixed long chain-of-thought fine-tuning, and a unified policy-gradient-based RL algorithm.",
      "ai_keywords": [
        "multimodal diffusion foundation models",
        "unified diffusion architecture",
        "modality-agnostic design",
        "mixed long chain-of-thought fine-tuning",
        "cold-start training",
        "reinforcement learning",
        "UniGRPO",
        "policy-gradient-based RL algorithm",
        "diversified reward modeling",
        "generalization capabilities",
        "textual reasoning",
        "multimodal understanding",
        "text-to-image generation"
      ]
    },
    "publishedAt": "2025-05-21T13:59:05.000Z",
    "title": "MMaDA: Multimodal Large Diffusion Language Models",
    "summary": "We introduce MMaDA, a novel class of multimodal diffusion foundation models\ndesigned to achieve superior performance across diverse domains such as textual\nreasoning, multimodal understanding, and text-to-image generation. The approach\nis distinguished by three key innovations: (i) MMaDA adopts a unified diffusion\narchitecture with a shared probabilistic formulation and a modality-agnostic\ndesign, eliminating the need for modality-specific components. This\narchitecture ensures seamless integration and processing across different data\ntypes. (ii) We implement a mixed long chain-of-thought (CoT) fine-tuning\nstrategy that curates a unified CoT format across modalities. By aligning\nreasoning processes between textual and visual domains, this strategy\nfacilitates cold-start training for the final reinforcement learning (RL)\nstage, thereby enhancing the model's ability to handle complex tasks from the\noutset. (iii) We propose UniGRPO, a unified policy-gradient-based RL algorithm\nspecifically tailored for diffusion foundation models. Utilizing diversified\nreward modeling, UniGRPO unifies post-training across both reasoning and\ngeneration tasks, ensuring consistent performance improvements. Experimental\nresults demonstrate that MMaDA-8B exhibits strong generalization capabilities\nas a unified multimodal foundation model. It surpasses powerful models like\nLLaMA-3-7B and Qwen2-7B in textual reasoning, outperforms Show-o and SEED-X in\nmultimodal understanding, and excels over SDXL and Janus in text-to-image\ngeneration. These achievements highlight MMaDA's effectiveness in bridging the\ngap between pretraining and post-training within unified diffusion\narchitectures, providing a comprehensive framework for future research and\ndevelopment. We open-source our code and trained models at:\nhttps://github.com/Gen-Verse/MMaDA",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15809.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64fde4e252e82dd432b74ce9",
      "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
      "fullname": "Ling Yang",
      "name": "Lingaaaaaaa",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.13909",
      "authors": [
        {
          "_id": "682d525994ed89a9b2aec35d",
          "user": {
            "_id": "661b9ac57cfb7bcb3057a578",
            "avatarUrl": "/avatars/f8afaa8eaad3a1e5963a4feebec3f7ab.svg",
            "isPro": false,
            "fullname": "Yanheng He",
            "user": "henryhe0123",
            "type": "user"
          },
          "name": "Yanheng He",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:17:46.371Z",
          "hidden": false
        },
        {
          "_id": "682d525994ed89a9b2aec35e",
          "user": {
            "_id": "663f5e959e6f865ec6d4fb62",
            "avatarUrl": "/avatars/eb0a908562b2c57335ae8bb949220430.svg",
            "isPro": false,
            "fullname": "Jiahe Jin",
            "user": "zizi-0123",
            "type": "user"
          },
          "name": "Jiahe Jin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:17:43.438Z",
          "hidden": false
        },
        {
          "_id": "682d525994ed89a9b2aec35f",
          "name": "Pengfei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T04:20:18.000Z",
      "submittedOnDailyAt": "2025-05-22T00:48:35.836Z",
      "title": "Efficient Agent Training for Computer Use",
      "submittedOnDailyBy": {
        "_id": "663f5e959e6f865ec6d4fb62",
        "avatarUrl": "/avatars/eb0a908562b2c57335ae8bb949220430.svg",
        "isPro": false,
        "fullname": "Jiahe Jin",
        "user": "zizi-0123",
        "type": "user"
      },
      "summary": "Scaling up high-quality trajectory data has long been a critical bottleneck\nfor developing human-like computer use agents. We introduce PC Agent-E, an\nefficient agent training framework that significantly reduces reliance on\nlarge-scale human demonstrations. Starting with just 312 human-annotated\ncomputer use trajectories, we further improved data quality by synthesizing\ndiverse action decisions with Claude 3.7 Sonnet. Trained on these enriched\ntrajectories, our PC Agent-E model achieved a remarkable 141% relative\nimprovement, surpassing the strong Claude 3.7 Sonnet with extended thinking on\nWindowsAgentArena-V2, an improved benchmark we also released. Furthermore, PC\nAgent-E demonstrates strong generalizability to different operating systems on\nOSWorld. Our findings suggest that strong computer use capabilities can be\nstimulated from a small amount of high-quality trajectory data.",
      "upvotes": 24,
      "discussionId": "682d525a94ed89a9b2aec397",
      "githubRepo": "https://github.com/GAIR-NLP/PC-Agent-E",
      "ai_summary": "PC Agent-E framework improves data efficiency and achieves superior performance on human-like computer use tasks through enhanced trajectory synthesis and training.",
      "ai_keywords": [
        "agent training framework",
        "human-annotated trajectories",
        "action decisions",
        "Claude 3.7 Sonnet",
        "WindowsAgentArena-V2",
        "OSWorld",
        "generalizability"
      ]
    },
    "publishedAt": "2025-05-20T00:20:18.000Z",
    "title": "Efficient Agent Training for Computer Use",
    "summary": "Scaling up high-quality trajectory data has long been a critical bottleneck\nfor developing human-like computer use agents. We introduce PC Agent-E, an\nefficient agent training framework that significantly reduces reliance on\nlarge-scale human demonstrations. Starting with just 312 human-annotated\ncomputer use trajectories, we further improved data quality by synthesizing\ndiverse action decisions with Claude 3.7 Sonnet. Trained on these enriched\ntrajectories, our PC Agent-E model achieved a remarkable 141% relative\nimprovement, surpassing the strong Claude 3.7 Sonnet with extended thinking on\nWindowsAgentArena-V2, an improved benchmark we also released. Furthermore, PC\nAgent-E demonstrates strong generalizability to different operating systems on\nOSWorld. Our findings suggest that strong computer use capabilities can be\nstimulated from a small amount of high-quality trajectory data.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13909.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "663f5e959e6f865ec6d4fb62",
      "avatarUrl": "/avatars/eb0a908562b2c57335ae8bb949220430.svg",
      "fullname": "Jiahe Jin",
      "name": "zizi-0123",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.15045",
      "authors": [
        {
          "_id": "682e9672d28d9650c90db133",
          "user": {
            "_id": "638f1803c67af472d317a922",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638f1803c67af472d317a922/9BMVXqHa-AsdZPmBprcbd.jpeg",
            "isPro": false,
            "fullname": "siyue zhang",
            "user": "siyue",
            "type": "user"
          },
          "name": "Siyue Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:16:04.722Z",
          "hidden": false
        },
        {
          "_id": "682e9672d28d9650c90db134",
          "user": {
            "_id": "62f662bcc58915315c4eccea",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
            "isPro": true,
            "fullname": "Yilun",
            "user": "yilunzhao",
            "type": "user"
          },
          "name": "Yilun Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:16:02.079Z",
          "hidden": false
        },
        {
          "_id": "682e9672d28d9650c90db135",
          "user": {
            "_id": "65457e29bd25cef7d118122c",
            "avatarUrl": "/avatars/67777b3f4584b8ba92f12e95dbf93482.svg",
            "isPro": false,
            "fullname": "Liyuan Geng",
            "user": "LYGeng",
            "type": "user"
          },
          "name": "Liyuan Geng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:15:59.203Z",
          "hidden": false
        },
        {
          "_id": "682e9672d28d9650c90db136",
          "name": "Arman Cohan",
          "hidden": false
        },
        {
          "_id": "682e9672d28d9650c90db137",
          "name": "Anh Tuan Luu",
          "hidden": false
        },
        {
          "_id": "682e9672d28d9650c90db138",
          "name": "Chen Zhao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/638f1803c67af472d317a922/2-30it6-2JiegJOHJEfOy.png"
      ],
      "publishedAt": "2025-05-21T02:59:14.000Z",
      "submittedOnDailyAt": "2025-05-22T01:48:05.988Z",
      "title": "Diffusion vs. Autoregressive Language Models: A Text Embedding\n  Perspective",
      "submittedOnDailyBy": {
        "_id": "638f1803c67af472d317a922",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638f1803c67af472d317a922/9BMVXqHa-AsdZPmBprcbd.jpeg",
        "isPro": false,
        "fullname": "siyue zhang",
        "user": "siyue",
        "type": "user"
      },
      "summary": "Large language model (LLM)-based embedding models, benefiting from large\nscale pre-training and post-training, have begun to surpass BERT and T5-based\nmodels on general-purpose text embedding tasks such as document retrieval.\nHowever, a fundamental limitation of LLM embeddings lies in the unidirectional\nattention used during autoregressive pre-training, which misaligns with the\nbidirectional nature of text embedding tasks. To this end, We propose adopting\ndiffusion language models for text embeddings, motivated by their inherent\nbidirectional architecture and recent success in matching or surpassing LLMs\nespecially on reasoning tasks. We present the first systematic study of the\ndiffusion language embedding model, which outperforms the LLM-based embedding\nmodel by 20% on long-document retrieval, 8% on reasoning-intensive retrieval,\n2% on instruction-following retrieval, and achieve competitive performance on\ntraditional text embedding benchmarks. Our analysis verifies that bidirectional\nattention is crucial for encoding global context in long and complex text.",
      "upvotes": 23,
      "discussionId": "682e9673d28d9650c90db154",
      "ai_summary": "Diffusion language models outperform large language model embeddings in text retrieval tasks due to their bidirectional architecture.",
      "ai_keywords": [
        "large language model (LLM)",
        "diffusion language models",
        "unidirectional attention",
        "bidirectional attention",
        "document retrieval",
        "reasoning-intensive retrieval",
        "instruction-following retrieval",
        "text embedding benchmarks"
      ]
    },
    "publishedAt": "2025-05-20T22:59:14.000Z",
    "title": "Diffusion vs. Autoregressive Language Models: A Text Embedding\n  Perspective",
    "summary": "Large language model (LLM)-based embedding models, benefiting from large\nscale pre-training and post-training, have begun to surpass BERT and T5-based\nmodels on general-purpose text embedding tasks such as document retrieval.\nHowever, a fundamental limitation of LLM embeddings lies in the unidirectional\nattention used during autoregressive pre-training, which misaligns with the\nbidirectional nature of text embedding tasks. To this end, We propose adopting\ndiffusion language models for text embeddings, motivated by their inherent\nbidirectional architecture and recent success in matching or surpassing LLMs\nespecially on reasoning tasks. We present the first systematic study of the\ndiffusion language embedding model, which outperforms the LLM-based embedding\nmodel by 20% on long-document retrieval, 8% on reasoning-intensive retrieval,\n2% on instruction-following retrieval, and achieve competitive performance on\ntraditional text embedding benchmarks. Our analysis verifies that bidirectional\nattention is crucial for encoding global context in long and complex text.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/638f1803c67af472d317a922/2-30it6-2JiegJOHJEfOy.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15045.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "638f1803c67af472d317a922",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638f1803c67af472d317a922/9BMVXqHa-AsdZPmBprcbd.jpeg",
      "fullname": "siyue zhang",
      "name": "siyue",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.15612",
      "authors": [
        {
          "_id": "682e97bf9c1b77a503087f4f",
          "user": {
            "_id": "6458af46f4d212d780bd7c68",
            "avatarUrl": "/avatars/832fd34bcc041b0b7b551873a459fc3c.svg",
            "isPro": false,
            "fullname": "Wei Liu",
            "user": "PeterV09",
            "type": "user"
          },
          "name": "Wei Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:15:57.023Z",
          "hidden": false
        },
        {
          "_id": "682e97bf9c1b77a503087f50",
          "name": "Ruochen Zhou",
          "hidden": false
        },
        {
          "_id": "682e97bf9c1b77a503087f51",
          "name": "Yiyun Deng",
          "hidden": false
        },
        {
          "_id": "682e97bf9c1b77a503087f52",
          "name": "Yuzhen Huang",
          "hidden": false
        },
        {
          "_id": "682e97bf9c1b77a503087f53",
          "name": "Junteng Liu",
          "hidden": false
        },
        {
          "_id": "682e97bf9c1b77a503087f54",
          "user": {
            "_id": "63081e15a670ed10f9d44229",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63081e15a670ed10f9d44229/w1b9uq-9774bMMgJbSPsS.jpeg",
            "isPro": true,
            "fullname": "Yuntian Deng",
            "user": "yuntian-deng",
            "type": "user"
          },
          "name": "Yuntian Deng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:15:54.170Z",
          "hidden": false
        },
        {
          "_id": "682e97bf9c1b77a503087f55",
          "name": "Yizhe Zhang",
          "hidden": false
        },
        {
          "_id": "682e97bf9c1b77a503087f56",
          "name": "Junxian He",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T15:03:26.000Z",
      "submittedOnDailyAt": "2025-05-22T01:49:54.677Z",
      "title": "Learn to Reason Efficiently with Adaptive Length-based Reward Shaping",
      "submittedOnDailyBy": {
        "_id": "6458af46f4d212d780bd7c68",
        "avatarUrl": "/avatars/832fd34bcc041b0b7b551873a459fc3c.svg",
        "isPro": false,
        "fullname": "Wei Liu",
        "user": "PeterV09",
        "type": "user"
      },
      "summary": "Large Reasoning Models (LRMs) have shown remarkable capabilities in solving\ncomplex problems through reinforcement learning (RL), particularly by\ngenerating long reasoning traces. However, these extended outputs often exhibit\nsubstantial redundancy, which limits the efficiency of LRMs. In this paper, we\ninvestigate RL-based approaches to promote reasoning efficiency. Specifically,\nwe first present a unified framework that formulates various efficient\nreasoning methods through the lens of length-based reward shaping. Building on\nthis perspective, we propose a novel Length-bAsed StEp Reward shaping method\n(LASER), which employs a step function as the reward, controlled by a target\nlength. LASER surpasses previous methods, achieving a superior Pareto-optimal\nbalance between performance and efficiency. Next, we further extend LASER based\non two key intuitions: (1) The reasoning behavior of the model evolves during\ntraining, necessitating reward specifications that are also adaptive and\ndynamic; (2) Rather than uniformly encouraging shorter or longer chains of\nthought (CoT), we posit that length-based reward shaping should be\ndifficulty-aware i.e., it should penalize lengthy CoTs more for easy queries.\nThis approach is expected to facilitate a combination of fast and slow\nthinking, leading to a better overall tradeoff. The resulting method is termed\nLASER-D (Dynamic and Difficulty-aware). Experiments on\nDeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, and\nDeepSeek-R1-Distill-Qwen-32B show that our approach significantly enhances both\nreasoning performance and response length efficiency. For instance, LASER-D and\nits variant achieve a +6.1 improvement on AIME2024 while reducing token usage\nby 63%. Further analysis reveals our RL-based compression produces more concise\nreasoning patterns with less redundant \"self-reflections\". Resources are at\nhttps://github.com/hkust-nlp/Laser.",
      "upvotes": 18,
      "discussionId": "682e97bf9c1b77a503087f81",
      "githubRepo": "https://github.com/hkust-nlp/Laser",
      "ai_summary": "RL-based reward shaping methods, particularly LASER-D, enhance reasoning efficiency and performance in large reasoning models by dynamically adapting to difficulty and reducing redundancy.",
      "ai_keywords": [
        "reinforcement learning",
        "long reasoning traces",
        "length-based reward shaping",
        "LASER",
        "LASER-D",
        "reasoning behavior",
        "target length",
        "step function",
        "adaptive",
        "dynamic",
        "difficulty-aware",
        "chains of thought",
        "self-reflections"
      ]
    },
    "publishedAt": "2025-05-21T11:03:26.000Z",
    "title": "Learn to Reason Efficiently with Adaptive Length-based Reward Shaping",
    "summary": "Large Reasoning Models (LRMs) have shown remarkable capabilities in solving\ncomplex problems through reinforcement learning (RL), particularly by\ngenerating long reasoning traces. However, these extended outputs often exhibit\nsubstantial redundancy, which limits the efficiency of LRMs. In this paper, we\ninvestigate RL-based approaches to promote reasoning efficiency. Specifically,\nwe first present a unified framework that formulates various efficient\nreasoning methods through the lens of length-based reward shaping. Building on\nthis perspective, we propose a novel Length-bAsed StEp Reward shaping method\n(LASER), which employs a step function as the reward, controlled by a target\nlength. LASER surpasses previous methods, achieving a superior Pareto-optimal\nbalance between performance and efficiency. Next, we further extend LASER based\non two key intuitions: (1) The reasoning behavior of the model evolves during\ntraining, necessitating reward specifications that are also adaptive and\ndynamic; (2) Rather than uniformly encouraging shorter or longer chains of\nthought (CoT), we posit that length-based reward shaping should be\ndifficulty-aware i.e., it should penalize lengthy CoTs more for easy queries.\nThis approach is expected to facilitate a combination of fast and slow\nthinking, leading to a better overall tradeoff. The resulting method is termed\nLASER-D (Dynamic and Difficulty-aware). Experiments on\nDeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, and\nDeepSeek-R1-Distill-Qwen-32B show that our approach significantly enhances both\nreasoning performance and response length efficiency. For instance, LASER-D and\nits variant achieve a +6.1 improvement on AIME2024 while reducing token usage\nby 63%. Further analysis reveals our RL-based compression produces more concise\nreasoning patterns with less redundant \"self-reflections\". Resources are at\nhttps://github.com/hkust-nlp/Laser.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15612.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6458af46f4d212d780bd7c68",
      "avatarUrl": "/avatars/832fd34bcc041b0b7b551873a459fc3c.svg",
      "fullname": "Wei Liu",
      "name": "PeterV09",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.15400",
      "authors": [
        {
          "_id": "682ec5f6b16c79c271c8559e",
          "user": {
            "_id": "67f33b43ccb05db5f0190cf6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/c-UeSyHfO7QchwNXW9mgu.png",
            "isPro": false,
            "fullname": "ZhangXiaoyun",
            "user": "DadaCloud01",
            "type": "user"
          },
          "name": "Xiaoyun Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:15:16.038Z",
          "hidden": false
        },
        {
          "_id": "682ec5f6b16c79c271c8559f",
          "user": {
            "_id": "64872abb3fb9bc8f4994e014",
            "avatarUrl": "/avatars/9743b5a2b6f9b9990aa1cb06bd6eb5c6.svg",
            "isPro": false,
            "fullname": "Jingqing Ruan",
            "user": "Amanda2023",
            "type": "user"
          },
          "name": "Jingqing Ruan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:15:09.457Z",
          "hidden": false
        },
        {
          "_id": "682ec5f6b16c79c271c855a0",
          "user": {
            "_id": "663bc46ae14047f71026c2b3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/663bc46ae14047f71026c2b3/lo4zFH0aIkFUQSNuzJW4I.jpeg",
            "isPro": false,
            "fullname": "Maxing",
            "user": "Machine981",
            "type": "user"
          },
          "name": "Xing Ma",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:15:12.313Z",
          "hidden": false
        },
        {
          "_id": "682ec5f6b16c79c271c855a1",
          "name": "Yawen Zhu",
          "hidden": false
        },
        {
          "_id": "682ec5f6b16c79c271c855a2",
          "name": "Haodong Zhao",
          "hidden": false
        },
        {
          "_id": "682ec5f6b16c79c271c855a3",
          "name": "Hao Li",
          "hidden": false
        },
        {
          "_id": "682ec5f6b16c79c271c855a4",
          "name": "Jiansong Chen",
          "hidden": false
        },
        {
          "_id": "682ec5f6b16c79c271c855a5",
          "name": "Ke Zeng",
          "hidden": false
        },
        {
          "_id": "682ec5f6b16c79c271c855a6",
          "name": "Xunliang Cai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T11:41:39.000Z",
      "submittedOnDailyAt": "2025-05-22T05:06:49.326Z",
      "title": "When to Continue Thinking: Adaptive Thinking Mode Switching for\n  Efficient Reasoning",
      "submittedOnDailyBy": {
        "_id": "64872abb3fb9bc8f4994e014",
        "avatarUrl": "/avatars/9743b5a2b6f9b9990aa1cb06bd6eb5c6.svg",
        "isPro": false,
        "fullname": "Jingqing Ruan",
        "user": "Amanda2023",
        "type": "user"
      },
      "summary": "Large reasoning models (LRMs) achieve remarkable performance via long\nreasoning chains, but often incur excessive computational overhead due to\nredundant reasoning, especially on simple tasks. In this work, we\nsystematically quantify the upper bounds of LRMs under both Long-Thinking and\nNo-Thinking modes, and uncover the phenomenon of \"Internal Self-Recovery\nMechanism\" where models implicitly supplement reasoning during answer\ngeneration. Building on this insight, we propose Adaptive Self-Recovery\nReasoning (ASRR), a framework that suppresses unnecessary reasoning and enables\nimplicit recovery. By introducing accuracy-aware length reward regulation, ASRR\nadaptively allocates reasoning effort according to problem difficulty,\nachieving high efficiency with negligible performance sacrifice. Experiments\nacross multiple benchmarks and models show that, compared with GRPO, ASRR\nreduces reasoning budget by up to 32.5% (1.5B) and 25.7% (7B) with minimal\naccuracy loss (1.2% and 0.6% pass@1), and significantly boosts harmless rates\non safety benchmarks (up to +21.7%). Our results highlight the potential of\nASRR for enabling efficient, adaptive, and safer reasoning in LRMs.",
      "upvotes": 14,
      "discussionId": "682ec5f7b16c79c271c855df",
      "ai_summary": "ASRR framework optimizes reasoning efficiency in large models by suppressing redundant information processing without significantly impacting performance or safety.",
      "ai_keywords": [
        "Large reasoning models",
        "LRMs",
        "Long-Thinking modes",
        "No-Thinking modes",
        "Internal Self-Recovery Mechanism",
        "Adaptive Self-Recovery Reasoning",
        "accuracy-aware length reward regulation",
        "reasoning budget",
        "pass@1",
        "harmless rates",
        "safety benchmarks",
        "efficiency",
        "adaptive reasoning",
        "safety"
      ]
    },
    "publishedAt": "2025-05-21T07:41:39.000Z",
    "title": "When to Continue Thinking: Adaptive Thinking Mode Switching for\n  Efficient Reasoning",
    "summary": "Large reasoning models (LRMs) achieve remarkable performance via long\nreasoning chains, but often incur excessive computational overhead due to\nredundant reasoning, especially on simple tasks. In this work, we\nsystematically quantify the upper bounds of LRMs under both Long-Thinking and\nNo-Thinking modes, and uncover the phenomenon of \"Internal Self-Recovery\nMechanism\" where models implicitly supplement reasoning during answer\ngeneration. Building on this insight, we propose Adaptive Self-Recovery\nReasoning (ASRR), a framework that suppresses unnecessary reasoning and enables\nimplicit recovery. By introducing accuracy-aware length reward regulation, ASRR\nadaptively allocates reasoning effort according to problem difficulty,\nachieving high efficiency with negligible performance sacrifice. Experiments\nacross multiple benchmarks and models show that, compared with GRPO, ASRR\nreduces reasoning budget by up to 32.5% (1.5B) and 25.7% (7B) with minimal\naccuracy loss (1.2% and 0.6% pass@1), and significantly boosts harmless rates\non safety benchmarks (up to +21.7%). Our results highlight the potential of\nASRR for enabling efficient, adaptive, and safer reasoning in LRMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15400.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64872abb3fb9bc8f4994e014",
      "avatarUrl": "/avatars/9743b5a2b6f9b9990aa1cb06bd6eb5c6.svg",
      "fullname": "Jingqing Ruan",
      "name": "Amanda2023",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.15779",
      "authors": [
        {
          "_id": "682e9d84a0db46260ccc15e1",
          "name": "Chuanhao Li",
          "hidden": false
        },
        {
          "_id": "682e9d84a0db46260ccc15e2",
          "name": "Jianwen Sun",
          "hidden": false
        },
        {
          "_id": "682e9d84a0db46260ccc15e3",
          "name": "Yukang Feng",
          "hidden": false
        },
        {
          "_id": "682e9d84a0db46260ccc15e4",
          "name": "Mingliang Zhai",
          "hidden": false
        },
        {
          "_id": "682e9d84a0db46260ccc15e5",
          "name": "Yifan Chang",
          "hidden": false
        },
        {
          "_id": "682e9d84a0db46260ccc15e6",
          "name": "Kaipeng Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T17:31:49.000Z",
      "submittedOnDailyAt": "2025-05-22T02:37:20.764Z",
      "title": "IA-T2I: Internet-Augmented Text-to-Image Generation",
      "submittedOnDailyBy": {
        "_id": "6794cd79b72b1721ea69f4f2",
        "avatarUrl": "/avatars/4e4fb9e9e127a0c031131ace705687cd.svg",
        "isPro": false,
        "fullname": "Ming Li",
        "user": "afdsafas",
        "type": "user"
      },
      "summary": "Current text-to-image (T2I) generation models achieve promising results, but\nthey fail on the scenarios where the knowledge implied in the text prompt is\nuncertain. For example, a T2I model released in February would struggle to\ngenerate a suitable poster for a movie premiering in April, because the\ncharacter designs and styles are uncertain to the model. To solve this problem,\nwe propose an Internet-Augmented text-to-image generation (IA-T2I) framework to\ncompel T2I models clear about such uncertain knowledge by providing them with\nreference images. Specifically, an active retrieval module is designed to\ndetermine whether a reference image is needed based on the given text prompt; a\nhierarchical image selection module is introduced to find the most suitable\nimage returned by an image search engine to enhance the T2I model; a\nself-reflection mechanism is presented to continuously evaluate and refine the\ngenerated image to ensure faithful alignment with the text prompt. To evaluate\nthe proposed framework's performance, we collect a dataset named Img-Ref-T2I,\nwhere text prompts include three types of uncertain knowledge: (1) known but\nrare. (2) unknown. (3) ambiguous. Moreover, we carefully craft a complex prompt\nto guide GPT-4o in making preference evaluation, which has been shown to have\nan evaluation accuracy similar to that of human preference evaluation.\nExperimental results demonstrate the effectiveness of our framework,\noutperforming GPT-4o by about 30% in human evaluation.",
      "upvotes": 12,
      "discussionId": "682e9d85a0db46260ccc161f",
      "ai_summary": "An Internet-Augmented text-to-image generation framework improves uncertain text prompt handling by integrating reference images, enhancing image quality and fidelity.",
      "ai_keywords": [
        "text-to-image generation",
        "IA-T2I framework",
        "reference images",
        "active retrieval module",
        "hierarchical image selection module",
        "self-reflection mechanism",
        "Img-Ref-T2I dataset",
        "GPT-4o"
      ]
    },
    "publishedAt": "2025-05-21T13:31:49.000Z",
    "title": "IA-T2I: Internet-Augmented Text-to-Image Generation",
    "summary": "Current text-to-image (T2I) generation models achieve promising results, but\nthey fail on the scenarios where the knowledge implied in the text prompt is\nuncertain. For example, a T2I model released in February would struggle to\ngenerate a suitable poster for a movie premiering in April, because the\ncharacter designs and styles are uncertain to the model. To solve this problem,\nwe propose an Internet-Augmented text-to-image generation (IA-T2I) framework to\ncompel T2I models clear about such uncertain knowledge by providing them with\nreference images. Specifically, an active retrieval module is designed to\ndetermine whether a reference image is needed based on the given text prompt; a\nhierarchical image selection module is introduced to find the most suitable\nimage returned by an image search engine to enhance the T2I model; a\nself-reflection mechanism is presented to continuously evaluate and refine the\ngenerated image to ensure faithful alignment with the text prompt. To evaluate\nthe proposed framework's performance, we collect a dataset named Img-Ref-T2I,\nwhere text prompts include three types of uncertain knowledge: (1) known but\nrare. (2) unknown. (3) ambiguous. Moreover, we carefully craft a complex prompt\nto guide GPT-4o in making preference evaluation, which has been shown to have\nan evaluation accuracy similar to that of human preference evaluation.\nExperimental results demonstrate the effectiveness of our framework,\noutperforming GPT-4o by about 30% in human evaluation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15779.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6794cd79b72b1721ea69f4f2",
      "avatarUrl": "/avatars/4e4fb9e9e127a0c031131ace705687cd.svg",
      "fullname": "Ming Li",
      "name": "afdsafas",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15210",
      "authors": [
        {
          "_id": "682e84d83291b134b3370184",
          "name": "Jie Ma",
          "hidden": false
        },
        {
          "_id": "682e84d83291b134b3370185",
          "user": {
            "_id": "653d0136d6f7982a7a52054d",
            "avatarUrl": "/avatars/5d47d022dd5954a4c3d243a7f0292d32.svg",
            "isPro": false,
            "fullname": "QUNING",
            "user": "stillqu",
            "type": "user"
          },
          "name": "Ning Qu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:16:55.205Z",
          "hidden": false
        },
        {
          "_id": "682e84d83291b134b3370186",
          "name": "Zhitao Gao",
          "hidden": false
        },
        {
          "_id": "682e84d83291b134b3370187",
          "name": "Rui Xing",
          "hidden": false
        },
        {
          "_id": "682e84d83291b134b3370188",
          "name": "Jun Liu",
          "hidden": false
        },
        {
          "_id": "682e84d83291b134b3370189",
          "name": "Hongbin Pei",
          "hidden": false
        },
        {
          "_id": "682e84d83291b134b337018a",
          "name": "Jiang Xie",
          "hidden": false
        },
        {
          "_id": "682e84d83291b134b337018b",
          "name": "Linyun Song",
          "hidden": false
        },
        {
          "_id": "682e84d83291b134b337018c",
          "name": "Pinghui Wang",
          "hidden": false
        },
        {
          "_id": "682e84d83291b134b337018d",
          "name": "Jing Tao",
          "hidden": false
        },
        {
          "_id": "682e84d83291b134b337018e",
          "name": "Zhou Su",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T07:38:45.000Z",
      "submittedOnDailyAt": "2025-05-22T00:31:54.508Z",
      "title": "Deliberation on Priors: Trustworthy Reasoning of Large Language Models\n  on Knowledge Graphs",
      "submittedOnDailyBy": {
        "_id": "67a7099286a55d5569acb213",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/rYISkvTtyraUdbgSsNfpC.png",
        "isPro": false,
        "fullname": "JieMa",
        "user": "JamesMile",
        "type": "user"
      },
      "summary": "Knowledge graph-based retrieval-augmented generation seeks to mitigate\nhallucinations in Large Language Models (LLMs) caused by insufficient or\noutdated knowledge. However, existing methods often fail to fully exploit the\nprior knowledge embedded in knowledge graphs (KGs), particularly their\nstructural information and explicit or implicit constraints. The former can\nenhance the faithfulness of LLMs' reasoning, while the latter can improve the\nreliability of response generation. Motivated by these, we propose a\ntrustworthy reasoning framework, termed Deliberation over Priors (DP), which\nsufficiently utilizes the priors contained in KGs. Specifically, DP adopts a\nprogressive knowledge distillation strategy that integrates structural priors\ninto LLMs through a combination of supervised fine-tuning and Kahneman-Tversky\noptimization, thereby improving the faithfulness of relation path generation.\nFurthermore, our framework employs a reasoning-introspection strategy, which\nguides LLMs to perform refined reasoning verification based on extracted\nconstraint priors, ensuring the reliability of response generation. Extensive\nexperiments on three benchmark datasets demonstrate that DP achieves new\nstate-of-the-art performance, especially a Hit@1 improvement of 13% on the\nComplexWebQuestions dataset, and generates highly trustworthy responses. We\nalso conduct various analyses to verify its flexibility and practicality. The\ncode is available at https://github.com/reml-group/Deliberation-on-Priors.",
      "upvotes": 12,
      "discussionId": "682e84d93291b134b33701cb",
      "githubRepo": "https://github.com/reml-group/Deliberation-on-Priors",
      "ai_summary": "The Deliberation over Priors framework enhances the trustworthiness of LLMs by integrating structural and constraint priors from knowledge graphs through knowledge distillation and reasoning introspection.",
      "ai_keywords": [
        "knowledge graph-based retrieval-augmented generation",
        "Large Language Models (LLMs)",
        "knowledge graphs (KGs)",
        "structural information",
        "explicit constraints",
        "implicit constraints",
        "trustworthworthy reasoning framework",
        "Deliberation over Priors (DP)",
        "progressive knowledge distillation",
        "supervised fine-tuning",
        "Kahneman-Tversky optimization",
        "relation path generation",
        "reasoning-introspection strategy",
        "ComplexWebQuestions dataset",
        "Hit@1 improvement"
      ]
    },
    "publishedAt": "2025-05-21T03:38:45.000Z",
    "title": "Deliberation on Priors: Trustworthy Reasoning of Large Language Models\n  on Knowledge Graphs",
    "summary": "Knowledge graph-based retrieval-augmented generation seeks to mitigate\nhallucinations in Large Language Models (LLMs) caused by insufficient or\noutdated knowledge. However, existing methods often fail to fully exploit the\nprior knowledge embedded in knowledge graphs (KGs), particularly their\nstructural information and explicit or implicit constraints. The former can\nenhance the faithfulness of LLMs' reasoning, while the latter can improve the\nreliability of response generation. Motivated by these, we propose a\ntrustworthy reasoning framework, termed Deliberation over Priors (DP), which\nsufficiently utilizes the priors contained in KGs. Specifically, DP adopts a\nprogressive knowledge distillation strategy that integrates structural priors\ninto LLMs through a combination of supervised fine-tuning and Kahneman-Tversky\noptimization, thereby improving the faithfulness of relation path generation.\nFurthermore, our framework employs a reasoning-introspection strategy, which\nguides LLMs to perform refined reasoning verification based on extracted\nconstraint priors, ensuring the reliability of response generation. Extensive\nexperiments on three benchmark datasets demonstrate that DP achieves new\nstate-of-the-art performance, especially a Hit@1 improvement of 13% on the\nComplexWebQuestions dataset, and generates highly trustworthy responses. We\nalso conduct various analyses to verify its flexibility and practicality. The\ncode is available at https://github.com/reml-group/Deliberation-on-Priors.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15210.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67a7099286a55d5569acb213",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/rYISkvTtyraUdbgSsNfpC.png",
      "fullname": "JieMa",
      "name": "JamesMile",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15146",
      "authors": [
        {
          "_id": "682e980fb16c79c271babcbd",
          "user": {
            "_id": "6301d6455e305a35cb0846a7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6301d6455e305a35cb0846a7/aT2AtzRMSY_T3y02MIUap.jpeg",
            "isPro": true,
            "fullname": "Lanxiang Hu",
            "user": "Snyhlxde",
            "type": "user"
          },
          "name": "Lanxiang Hu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:15:51.281Z",
          "hidden": false
        },
        {
          "_id": "682e980fb16c79c271babcbe",
          "name": "Mingjia Huo",
          "hidden": false
        },
        {
          "_id": "682e980fb16c79c271babcbf",
          "user": {
            "_id": "653200c5d0f5a9e537e76695",
            "avatarUrl": "/avatars/d1beb984d91908c03e0daeb77589a475.svg",
            "isPro": false,
            "fullname": "Yuxuan Zhang",
            "user": "Yuxuan13",
            "type": "user"
          },
          "name": "Yuxuan Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:15:48.879Z",
          "hidden": false
        },
        {
          "_id": "682e980fb16c79c271babcc0",
          "name": "Haoyang Yu",
          "hidden": false
        },
        {
          "_id": "682e980fb16c79c271babcc1",
          "name": "Eric P. Xing",
          "hidden": false
        },
        {
          "_id": "682e980fb16c79c271babcc2",
          "name": "Ion Stoica",
          "hidden": false
        },
        {
          "_id": "682e980fb16c79c271babcc3",
          "name": "Tajana Rosing",
          "hidden": false
        },
        {
          "_id": "682e980fb16c79c271babcc4",
          "name": "Haojian Jin",
          "hidden": false
        },
        {
          "_id": "682e980fb16c79c271babcc5",
          "name": "Hao Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T06:02:55.000Z",
      "submittedOnDailyAt": "2025-05-22T01:52:33.947Z",
      "title": "lmgame-Bench: How Good are LLMs at Playing Games?",
      "submittedOnDailyBy": {
        "_id": "6301d6455e305a35cb0846a7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6301d6455e305a35cb0846a7/aT2AtzRMSY_T3y02MIUap.jpeg",
        "isPro": true,
        "fullname": "Lanxiang Hu",
        "user": "Snyhlxde",
        "type": "user"
      },
      "summary": "Playing video games requires perception, memory, and planning, exactly the\nfaculties modern large language model (LLM) agents are expected to master. We\nstudy the major challenges in using popular video games to evaluate modern LLMs\nand find that directly dropping LLMs into games cannot make an effective\nevaluation, for three reasons -- brittle vision perception, prompt sensitivity,\nand potential data contamination. We introduce lmgame-Bench to turn games into\nreliable evaluations. lmgame-Bench features a suite of platformer, puzzle, and\nnarrative games delivered through a unified Gym-style API and paired with\nlightweight perception and memory scaffolds, and is designed to stabilize\nprompt variance and remove contamination. Across 13 leading models, we show\nlmgame-Bench is challenging while still separating models well. Correlation\nanalysis shows that every game probes a unique blend of capabilities often\ntested in isolation elsewhere. More interestingly, performing reinforcement\nlearning on a single game from lmgame-Bench transfers both to unseen games and\nto external planning tasks. Our evaluation code is available at\nhttps://github.com/lmgame-org/GamingAgent/lmgame-bench.",
      "upvotes": 11,
      "discussionId": "682e9810b16c79c271babd61",
      "projectPage": "https://lmgame.org/",
      "ai_summary": "lmgame-Bench evaluates large language models using games with diverse challenges, demonstrating unique capability blends and transfer learning potential.",
      "ai_keywords": [
        "LLMs",
        "large language model agents",
        "brittle vision perception",
        "prompt sensitivity",
        "data contamination",
        "Gym-style API",
        "perception scaffolds",
        "memory scaffolds",
        "reinforcement learning",
        "transfer learning"
      ]
    },
    "publishedAt": "2025-05-21T02:02:55.000Z",
    "title": "lmgame-Bench: How Good are LLMs at Playing Games?",
    "summary": "Playing video games requires perception, memory, and planning, exactly the\nfaculties modern large language model (LLM) agents are expected to master. We\nstudy the major challenges in using popular video games to evaluate modern LLMs\nand find that directly dropping LLMs into games cannot make an effective\nevaluation, for three reasons -- brittle vision perception, prompt sensitivity,\nand potential data contamination. We introduce lmgame-Bench to turn games into\nreliable evaluations. lmgame-Bench features a suite of platformer, puzzle, and\nnarrative games delivered through a unified Gym-style API and paired with\nlightweight perception and memory scaffolds, and is designed to stabilize\nprompt variance and remove contamination. Across 13 leading models, we show\nlmgame-Bench is challenging while still separating models well. Correlation\nanalysis shows that every game probes a unique blend of capabilities often\ntested in isolation elsewhere. More interestingly, performing reinforcement\nlearning on a single game from lmgame-Bench transfers both to unseen games and\nto external planning tasks. Our evaluation code is available at\nhttps://github.com/lmgame-org/GamingAgent/lmgame-bench.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15146.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6301d6455e305a35cb0846a7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6301d6455e305a35cb0846a7/aT2AtzRMSY_T3y02MIUap.jpeg",
      "fullname": "Lanxiang Hu",
      "name": "Snyhlxde",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.14357",
      "authors": [
        {
          "_id": "682d2b9ba006b93dbe79a333",
          "user": {
            "_id": "66b06fe4ce2224a4d066cc0a",
            "avatarUrl": "/avatars/3f89007c5c17e57e623a10d82637b5fc.svg",
            "isPro": false,
            "fullname": "knightnemo",
            "user": "knightnemo",
            "type": "user"
          },
          "name": "Siqiao Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-21T18:04:10.245Z",
          "hidden": false
        },
        {
          "_id": "682d2b9ba006b93dbe79a334",
          "user": {
            "_id": "643b866bff50448bcfc7d1d1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/q12v-BatVKimoi8q-coi-.jpeg",
            "isPro": false,
            "fullname": "Jialong Wu",
            "user": "manchery",
            "type": "user"
          },
          "name": "Jialong Wu",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-22T05:44:25.954Z",
          "hidden": false
        },
        {
          "_id": "682d2b9ba006b93dbe79a335",
          "name": "Qixing Zhou",
          "hidden": false
        },
        {
          "_id": "682d2b9ba006b93dbe79a336",
          "name": "Shangchen Miao",
          "hidden": false
        },
        {
          "_id": "682d2b9ba006b93dbe79a337",
          "name": "Mingsheng Long",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T13:41:45.000Z",
      "submittedOnDailyAt": "2025-05-22T00:17:33.386Z",
      "title": "Vid2World: Crafting Video Diffusion Models to Interactive World Models",
      "submittedOnDailyBy": {
        "_id": "66b06fe4ce2224a4d066cc0a",
        "avatarUrl": "/avatars/3f89007c5c17e57e623a10d82637b5fc.svg",
        "isPro": false,
        "fullname": "knightnemo",
        "user": "knightnemo",
        "type": "user"
      },
      "summary": "World models, which predict transitions based on history observation and\naction sequences, have shown great promise in improving data efficiency for\nsequential decision making. However, existing world models often require\nextensive domain-specific training and still produce low-fidelity, coarse\npredictions, limiting their applicability in complex environments. In contrast,\nvideo diffusion models trained on large, internet-scale datasets have\ndemonstrated impressive capabilities in generating high-quality videos that\ncapture diverse real-world dynamics. In this work, we present Vid2World, a\ngeneral approach for leveraging and transferring pre-trained video diffusion\nmodels into interactive world models. To bridge the gap, Vid2World performs\ncasualization of a pre-trained video diffusion model by crafting its\narchitecture and training objective to enable autoregressive generation.\nFurthermore, it introduces a causal action guidance mechanism to enhance action\ncontrollability in the resulting interactive world model. Extensive experiments\nin robot manipulation and game simulation domains show that our method offers a\nscalable and effective approach for repurposing highly capable video diffusion\nmodels to interactive world models.",
      "upvotes": 10,
      "discussionId": "682d2b9da006b93dbe79a3af",
      "projectPage": "https://knightnemo.github.io/vid2world/",
      "ai_summary": "Vid2World repurposes pre-trained video diffusion models into interactive world models via causalization and action guidance, enhancing action controllability and scalability in complex environments.",
      "ai_keywords": [
        "world models",
        "transitions",
        "history observation",
        "action sequences",
        "data efficiency",
        "sequential decision making",
        "low-fidelity",
        "coarse predictions",
        "video diffusion models",
        "internet-scale datasets",
        "high-quality videos",
        "real-world dynamics",
        "autoregressive generation",
        "causal action guidance",
        "robot manipulation",
        "game simulation"
      ]
    },
    "publishedAt": "2025-05-20T09:41:45.000Z",
    "title": "Vid2World: Crafting Video Diffusion Models to Interactive World Models",
    "summary": "World models, which predict transitions based on history observation and\naction sequences, have shown great promise in improving data efficiency for\nsequential decision making. However, existing world models often require\nextensive domain-specific training and still produce low-fidelity, coarse\npredictions, limiting their applicability in complex environments. In contrast,\nvideo diffusion models trained on large, internet-scale datasets have\ndemonstrated impressive capabilities in generating high-quality videos that\ncapture diverse real-world dynamics. In this work, we present Vid2World, a\ngeneral approach for leveraging and transferring pre-trained video diffusion\nmodels into interactive world models. To bridge the gap, Vid2World performs\ncasualization of a pre-trained video diffusion model by crafting its\narchitecture and training objective to enable autoregressive generation.\nFurthermore, it introduces a causal action guidance mechanism to enhance action\ncontrollability in the resulting interactive world model. Extensive experiments\nin robot manipulation and game simulation domains show that our method offers a\nscalable and effective approach for repurposing highly capable video diffusion\nmodels to interactive world models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14357.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66b06fe4ce2224a4d066cc0a",
      "avatarUrl": "/avatars/3f89007c5c17e57e623a10d82637b5fc.svg",
      "fullname": "knightnemo",
      "name": "knightnemo",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.15765",
      "authors": [
        {
          "_id": "682e85aa7b41e70cf11758b6",
          "name": "Kaizhi Zheng",
          "hidden": false
        },
        {
          "_id": "682e85aa7b41e70cf11758b7",
          "name": "Ruijian Zhang",
          "hidden": false
        },
        {
          "_id": "682e85aa7b41e70cf11758b8",
          "name": "Jing Gu",
          "hidden": false
        },
        {
          "_id": "682e85aa7b41e70cf11758b9",
          "name": "Jie Yang",
          "hidden": false
        },
        {
          "_id": "682e85aa7b41e70cf11758ba",
          "name": "Xin Eric Wang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64679a226192d39142245e5e/LlIYiX21uaVLjqWu6lAQI.mp4"
      ],
      "publishedAt": "2025-05-21T17:10:47.000Z",
      "submittedOnDailyAt": "2025-05-22T00:36:41.981Z",
      "title": "Constructing a 3D Town from a Single Image",
      "submittedOnDailyBy": {
        "_id": "64679a226192d39142245e5e",
        "avatarUrl": "/avatars/05abee0b6317f100923936ca2099e9eb.svg",
        "isPro": false,
        "fullname": "Xin Eric Wang",
        "user": "xw-eric",
        "type": "user"
      },
      "summary": "Acquiring detailed 3D scenes typically demands costly equipment, multi-view\ndata, or labor-intensive modeling. Therefore, a lightweight alternative,\ngenerating complex 3D scenes from a single top-down image, plays an essential\nrole in real-world applications. While recent 3D generative models have\nachieved remarkable results at the object level, their extension to full-scene\ngeneration often leads to inconsistent geometry, layout hallucinations, and\nlow-quality meshes. In this work, we introduce 3DTown, a training-free\nframework designed to synthesize realistic and coherent 3D scenes from a single\ntop-down view. Our method is grounded in two principles: region-based\ngeneration to improve image-to-3D alignment and resolution, and spatial-aware\n3D inpainting to ensure global scene coherence and high-quality geometry\ngeneration. Specifically, we decompose the input image into overlapping regions\nand generate each using a pretrained 3D object generator, followed by a masked\nrectified flow inpainting process that fills in missing geometry while\nmaintaining structural continuity. This modular design allows us to overcome\nresolution bottlenecks and preserve spatial structure without requiring 3D\nsupervision or fine-tuning. Extensive experiments across diverse scenes show\nthat 3DTown outperforms state-of-the-art baselines, including Trellis,\nHunyuan3D-2, and TripoSG, in terms of geometry quality, spatial coherence, and\ntexture fidelity. Our results demonstrate that high-quality 3D town generation\nis achievable from a single image using a principled, training-free approach.",
      "upvotes": 8,
      "discussionId": "682e85b17b41e70cf1175aa0",
      "ai_summary": "A training-free framework named 3DTown generates realistic 3D scenes from a single top-down image using region-based generation and spatial-aware 3D inpainting techniques.",
      "ai_keywords": [
        "3D generative models",
        "3DTown",
        "region-based generation",
        "spatial-aware 3D inpainting",
        "masked rectified flow",
        "Trellis",
        "Hunyuan3D-2",
        "TripoSG"
      ]
    },
    "publishedAt": "2025-05-21T13:10:47.000Z",
    "title": "Constructing a 3D Town from a Single Image",
    "summary": "Acquiring detailed 3D scenes typically demands costly equipment, multi-view\ndata, or labor-intensive modeling. Therefore, a lightweight alternative,\ngenerating complex 3D scenes from a single top-down image, plays an essential\nrole in real-world applications. While recent 3D generative models have\nachieved remarkable results at the object level, their extension to full-scene\ngeneration often leads to inconsistent geometry, layout hallucinations, and\nlow-quality meshes. In this work, we introduce 3DTown, a training-free\nframework designed to synthesize realistic and coherent 3D scenes from a single\ntop-down view. Our method is grounded in two principles: region-based\ngeneration to improve image-to-3D alignment and resolution, and spatial-aware\n3D inpainting to ensure global scene coherence and high-quality geometry\ngeneration. Specifically, we decompose the input image into overlapping regions\nand generate each using a pretrained 3D object generator, followed by a masked\nrectified flow inpainting process that fills in missing geometry while\nmaintaining structural continuity. This modular design allows us to overcome\nresolution bottlenecks and preserve spatial structure without requiring 3D\nsupervision or fine-tuning. Extensive experiments across diverse scenes show\nthat 3DTown outperforms state-of-the-art baselines, including Trellis,\nHunyuan3D-2, and TripoSG, in terms of geometry quality, spatial coherence, and\ntexture fidelity. Our results demonstrate that high-quality 3D town generation\nis achievable from a single image using a principled, training-free approach.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64679a226192d39142245e5e/LlIYiX21uaVLjqWu6lAQI.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15765.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64679a226192d39142245e5e",
      "avatarUrl": "/avatars/05abee0b6317f100923936ca2099e9eb.svg",
      "fullname": "Xin Eric Wang",
      "name": "xw-eric",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15781",
      "authors": [
        {
          "_id": "682ea1129c1b77a5030b31f2",
          "name": "Xinyin Ma",
          "hidden": false
        },
        {
          "_id": "682ea1129c1b77a5030b31f3",
          "name": "Runpeng Yu",
          "hidden": false
        },
        {
          "_id": "682ea1129c1b77a5030b31f4",
          "name": "Gongfan Fang",
          "hidden": false
        },
        {
          "_id": "682ea1129c1b77a5030b31f5",
          "name": "Xinchao Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T17:32:10.000Z",
      "submittedOnDailyAt": "2025-05-22T02:29:25.873Z",
      "title": "dKV-Cache: The Cache for Diffusion Language Models",
      "submittedOnDailyBy": {
        "_id": "64396ebc21221ac7411852b3",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64396ebc21221ac7411852b3/SR0dC8N0bdj9tZFxYPpSf.jpeg",
        "isPro": false,
        "fullname": "Xinyin Ma",
        "user": "horseee",
        "type": "user"
      },
      "summary": "Diffusion Language Models (DLMs) have been seen as a promising competitor for\nautoregressive language models. However, diffusion language models have long\nbeen constrained by slow inference. A core challenge is that their\nnon-autoregressive architecture and bidirectional attention preclude the\nkey-value cache that accelerates decoding. We address this bottleneck by\nproposing a KV-cache-like mechanism, delayed KV-Cache, for the denoising\nprocess of DLMs. Our approach is motivated by the observation that different\ntokens have distinct representation dynamics throughout the diffusion process.\nAccordingly, we propose a delayed and conditioned caching strategy for key and\nvalue states. We design two complementary variants to cache key and value\nstep-by-step: (1) dKV-Cache-Decode, which provides almost lossless\nacceleration, and even improves performance on long sequences, suggesting that\nexisting DLMs may under-utilise contextual information during inference. (2)\ndKV-Cache-Greedy, which has aggressive caching with reduced lifespan, achieving\nhigher speed-ups with quadratic time complexity at the cost of some performance\ndegradation. dKV-Cache, in final, achieves from 2-10x speedup in inference,\nlargely narrowing the gap between ARs and DLMs. We evaluate our dKV-Cache on\nseveral benchmarks, delivering acceleration across general language\nunderstanding, mathematical, and code-generation benchmarks. Experiments\ndemonstrate that cache can also be used in DLMs, even in a training-free manner\nfrom current DLMs.",
      "upvotes": 7,
      "discussionId": "682ea1139c1b77a5030b322a",
      "ai_summary": "A KV-cache-like mechanism, delayed KV-Cache, accelerates diffusion language models' inference without significantly degrading performance.",
      "ai_keywords": [
        "Diffusion Language Models",
        "KV-cache",
        "non-autoregressive architecture",
        "bidirectional attention",
        "key-value states",
        "token representation dynamics",
        "dKV-Cache-Decode",
        "dKV-Cache-Greedy",
        "speedup",
        "autoregressive models"
      ]
    },
    "publishedAt": "2025-05-21T13:32:10.000Z",
    "title": "dKV-Cache: The Cache for Diffusion Language Models",
    "summary": "Diffusion Language Models (DLMs) have been seen as a promising competitor for\nautoregressive language models. However, diffusion language models have long\nbeen constrained by slow inference. A core challenge is that their\nnon-autoregressive architecture and bidirectional attention preclude the\nkey-value cache that accelerates decoding. We address this bottleneck by\nproposing a KV-cache-like mechanism, delayed KV-Cache, for the denoising\nprocess of DLMs. Our approach is motivated by the observation that different\ntokens have distinct representation dynamics throughout the diffusion process.\nAccordingly, we propose a delayed and conditioned caching strategy for key and\nvalue states. We design two complementary variants to cache key and value\nstep-by-step: (1) dKV-Cache-Decode, which provides almost lossless\nacceleration, and even improves performance on long sequences, suggesting that\nexisting DLMs may under-utilise contextual information during inference. (2)\ndKV-Cache-Greedy, which has aggressive caching with reduced lifespan, achieving\nhigher speed-ups with quadratic time complexity at the cost of some performance\ndegradation. dKV-Cache, in final, achieves from 2-10x speedup in inference,\nlargely narrowing the gap between ARs and DLMs. We evaluate our dKV-Cache on\nseveral benchmarks, delivering acceleration across general language\nunderstanding, mathematical, and code-generation benchmarks. Experiments\ndemonstrate that cache can also be used in DLMs, even in a training-free manner\nfrom current DLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15781.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64396ebc21221ac7411852b3",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64396ebc21221ac7411852b3/SR0dC8N0bdj9tZFxYPpSf.jpeg",
      "fullname": "Xinyin Ma",
      "name": "horseee",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15656",
      "authors": [
        {
          "_id": "682e8add58a17fe0e9ec02e6",
          "name": "Zhexin Zhang",
          "hidden": false
        },
        {
          "_id": "682e8add58a17fe0e9ec02e7",
          "name": "Yuhao Sun",
          "hidden": false
        },
        {
          "_id": "682e8add58a17fe0e9ec02e8",
          "user": {
            "_id": "65d859a3661492b25c46a117",
            "avatarUrl": "/avatars/06a547a00b472f512a25eef4ddd047bf.svg",
            "isPro": false,
            "fullname": "junxiao yang",
            "user": "yangjunxiao2021",
            "type": "user"
          },
          "name": "Junxiao Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:16:27.527Z",
          "hidden": false
        },
        {
          "_id": "682e8add58a17fe0e9ec02e9",
          "name": "Shiyao Cui",
          "hidden": false
        },
        {
          "_id": "682e8add58a17fe0e9ec02ea",
          "name": "Hongning Wang",
          "hidden": false
        },
        {
          "_id": "682e8add58a17fe0e9ec02eb",
          "name": "Minlie Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T15:32:14.000Z",
      "submittedOnDailyAt": "2025-05-22T00:55:08.348Z",
      "title": "Be Careful When Fine-tuning On Open-Source LLMs: Your Fine-tuning Data\n  Could Be Secretly Stolen!",
      "submittedOnDailyBy": {
        "_id": "61b58aa0d65058ce70beb98c",
        "avatarUrl": "/avatars/aefd9271b891abc6dd2ded1a30eebca4.svg",
        "isPro": false,
        "fullname": "Zhexin Zhang",
        "user": "nonstopfor",
        "type": "user"
      },
      "summary": "Fine-tuning on open-source Large Language Models (LLMs) with proprietary data\nis now a standard practice for downstream developers to obtain task-specific\nLLMs. Surprisingly, we reveal a new and concerning risk along with the\npractice: the creator of the open-source LLMs can later extract the private\ndownstream fine-tuning data through simple backdoor training, only requiring\nblack-box access to the fine-tuned downstream model. Our comprehensive\nexperiments, across 4 popularly used open-source models with 3B to 32B\nparameters and 2 downstream datasets, suggest that the extraction performance\ncan be strikingly high: in practical settings, as much as 76.3% downstream\nfine-tuning data (queries) out of a total 5,000 samples can be perfectly\nextracted, and the success rate can increase to 94.9% in more ideal settings.\nWe also explore a detection-based defense strategy but find it can be bypassed\nwith improved attack. Overall, we highlight the emergency of this newly\nidentified data breaching risk in fine-tuning, and we hope that more follow-up\nresearch could push the progress of addressing this concerning risk. The code\nand data used in our experiments are released at\nhttps://github.com/thu-coai/Backdoor-Data-Extraction.",
      "upvotes": 7,
      "discussionId": "682e8ade58a17fe0e9ec0348",
      "ai_summary": "There is a newly identified risk that creators of open-source LLMs can extract fine-tuning data from downstream models through backdoor training, even with black-box access.",
      "ai_keywords": [
        "Large Language Models",
        "fine-tuning",
        "open-source",
        "black-box access",
        "backdoor training",
        "data extraction",
        "data breach"
      ]
    },
    "publishedAt": "2025-05-21T11:32:14.000Z",
    "title": "Be Careful When Fine-tuning On Open-Source LLMs: Your Fine-tuning Data\n  Could Be Secretly Stolen!",
    "summary": "Fine-tuning on open-source Large Language Models (LLMs) with proprietary data\nis now a standard practice for downstream developers to obtain task-specific\nLLMs. Surprisingly, we reveal a new and concerning risk along with the\npractice: the creator of the open-source LLMs can later extract the private\ndownstream fine-tuning data through simple backdoor training, only requiring\nblack-box access to the fine-tuned downstream model. Our comprehensive\nexperiments, across 4 popularly used open-source models with 3B to 32B\nparameters and 2 downstream datasets, suggest that the extraction performance\ncan be strikingly high: in practical settings, as much as 76.3% downstream\nfine-tuning data (queries) out of a total 5,000 samples can be perfectly\nextracted, and the success rate can increase to 94.9% in more ideal settings.\nWe also explore a detection-based defense strategy but find it can be bypassed\nwith improved attack. Overall, we highlight the emergency of this newly\nidentified data breaching risk in fine-tuning, and we hope that more follow-up\nresearch could push the progress of addressing this concerning risk. The code\nand data used in our experiments are released at\nhttps://github.com/thu-coai/Backdoor-Data-Extraction.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15656.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61b58aa0d65058ce70beb98c",
      "avatarUrl": "/avatars/aefd9271b891abc6dd2ded1a30eebca4.svg",
      "fullname": "Zhexin Zhang",
      "name": "nonstopfor",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15404",
      "authors": [
        {
          "_id": "682e8b3de3d1137730d0517b",
          "name": "Zhexin Zhang",
          "hidden": false
        },
        {
          "_id": "682e8b3de3d1137730d0517c",
          "name": "Xian Qi Loye",
          "hidden": false
        },
        {
          "_id": "682e8b3de3d1137730d0517d",
          "name": "Victor Shea-Jay Huang",
          "hidden": false
        },
        {
          "_id": "682e8b3de3d1137730d0517e",
          "user": {
            "_id": "65d859a3661492b25c46a117",
            "avatarUrl": "/avatars/06a547a00b472f512a25eef4ddd047bf.svg",
            "isPro": false,
            "fullname": "junxiao yang",
            "user": "yangjunxiao2021",
            "type": "user"
          },
          "name": "Junxiao Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:16:24.874Z",
          "hidden": false
        },
        {
          "_id": "682e8b3de3d1137730d0517f",
          "name": "Qi Zhu",
          "hidden": false
        },
        {
          "_id": "682e8b3de3d1137730d05180",
          "name": "Shiyao Cui",
          "hidden": false
        },
        {
          "_id": "682e8b3de3d1137730d05181",
          "name": "Fei Mi",
          "hidden": false
        },
        {
          "_id": "682e8b3de3d1137730d05182",
          "name": "Lifeng Shang",
          "hidden": false
        },
        {
          "_id": "682e8b3de3d1137730d05183",
          "name": "Yingkang Wang",
          "hidden": false
        },
        {
          "_id": "682e8b3de3d1137730d05184",
          "name": "Hongning Wang",
          "hidden": false
        },
        {
          "_id": "682e8b3de3d1137730d05185",
          "name": "Minlie Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T11:45:29.000Z",
      "submittedOnDailyAt": "2025-05-22T00:57:01.326Z",
      "title": "How Should We Enhance the Safety of Large Reasoning Models: An Empirical\n  Study",
      "submittedOnDailyBy": {
        "_id": "61b58aa0d65058ce70beb98c",
        "avatarUrl": "/avatars/aefd9271b891abc6dd2ded1a30eebca4.svg",
        "isPro": false,
        "fullname": "Zhexin Zhang",
        "user": "nonstopfor",
        "type": "user"
      },
      "summary": "Large Reasoning Models (LRMs) have achieved remarkable success on\nreasoning-intensive tasks such as mathematics and programming. However, their\nenhanced reasoning capabilities do not necessarily translate to improved safety\nperformance-and in some cases, may even degrade it. This raises an important\nresearch question: how can we enhance the safety of LRMs? In this paper, we\npresent a comprehensive empirical study on how to enhance the safety of LRMs\nthrough Supervised Fine-Tuning (SFT). Our investigation begins with an\nunexpected observation: directly distilling safe responses from DeepSeek-R1\nfails to significantly enhance safety. We analyze this phenomenon and identify\nthree key failure patterns that contribute to it. We then demonstrate that\nexplicitly addressing these issues during the data distillation process can\nlead to substantial safety improvements. Next, we explore whether a long and\ncomplex reasoning process is necessary for achieving safety. Interestingly, we\nfind that simply using short or template-based reasoning process can attain\ncomparable safety performance-and are significantly easier for models to learn\nthan more intricate reasoning chains. These findings prompt a deeper reflection\non the role of reasoning in ensuring safety. Finally, we find that mixing math\nreasoning data during safety fine-tuning is helpful to balance safety and\nover-refusal. Overall, we hope our empirical study could provide a more\nholistic picture on enhancing the safety of LRMs. The code and data used in our\nexperiments are released in https://github.com/thu-coai/LRM-Safety-Study.",
      "upvotes": 7,
      "discussionId": "682e8b3fe3d1137730d051f5",
      "ai_summary": "The study investigates methods to enhance the safety of Large Reasoning Models (LRMs) through Supervised Fine-Tuning (SFT), finding that explicit addressing of failure patterns and use of simpler reasoning processes can improve safety without requiring complex reasoning chains or excessive data.",
      "ai_keywords": [
        "Large Reasoning Models",
        "LRMs",
        "Supervised Fine-Tuning",
        "SFT",
        "DeepSeek-R1",
        "data distillation",
        "reasoning process",
        "safety improvements",
        "over-refusal",
        "math reasoning"
      ]
    },
    "publishedAt": "2025-05-21T07:45:29.000Z",
    "title": "How Should We Enhance the Safety of Large Reasoning Models: An Empirical\n  Study",
    "summary": "Large Reasoning Models (LRMs) have achieved remarkable success on\nreasoning-intensive tasks such as mathematics and programming. However, their\nenhanced reasoning capabilities do not necessarily translate to improved safety\nperformance-and in some cases, may even degrade it. This raises an important\nresearch question: how can we enhance the safety of LRMs? In this paper, we\npresent a comprehensive empirical study on how to enhance the safety of LRMs\nthrough Supervised Fine-Tuning (SFT). Our investigation begins with an\nunexpected observation: directly distilling safe responses from DeepSeek-R1\nfails to significantly enhance safety. We analyze this phenomenon and identify\nthree key failure patterns that contribute to it. We then demonstrate that\nexplicitly addressing these issues during the data distillation process can\nlead to substantial safety improvements. Next, we explore whether a long and\ncomplex reasoning process is necessary for achieving safety. Interestingly, we\nfind that simply using short or template-based reasoning process can attain\ncomparable safety performance-and are significantly easier for models to learn\nthan more intricate reasoning chains. These findings prompt a deeper reflection\non the role of reasoning in ensuring safety. Finally, we find that mixing math\nreasoning data during safety fine-tuning is helpful to balance safety and\nover-refusal. Overall, we hope our empirical study could provide a more\nholistic picture on enhancing the safety of LRMs. The code and data used in our\nexperiments are released in https://github.com/thu-coai/LRM-Safety-Study.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15404.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61b58aa0d65058ce70beb98c",
      "avatarUrl": "/avatars/aefd9271b891abc6dd2ded1a30eebca4.svg",
      "fullname": "Zhexin Zhang",
      "name": "nonstopfor",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.13934",
      "authors": [
        {
          "_id": "682e8369b16c79c271b4db81",
          "user": {
            "_id": "643b866bff50448bcfc7d1d1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/q12v-BatVKimoi8q-coi-.jpeg",
            "isPro": false,
            "fullname": "Jialong Wu",
            "user": "manchery",
            "type": "user"
          },
          "name": "Jialong Wu",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-22T05:44:24.633Z",
          "hidden": false
        },
        {
          "_id": "682e8369b16c79c271b4db82",
          "name": "Shaofeng Yin",
          "hidden": false
        },
        {
          "_id": "682e8369b16c79c271b4db83",
          "name": "Ningya Feng",
          "hidden": false
        },
        {
          "_id": "682e8369b16c79c271b4db84",
          "name": "Mingsheng Long",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/643b866bff50448bcfc7d1d1/OXSeIgpwt_mjJJqBffXyA.png"
      ],
      "publishedAt": "2025-05-20T05:02:53.000Z",
      "submittedOnDailyAt": "2025-05-22T00:24:21.597Z",
      "title": "RLVR-World: Training World Models with Reinforcement Learning",
      "submittedOnDailyBy": {
        "_id": "643b866bff50448bcfc7d1d1",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/q12v-BatVKimoi8q-coi-.jpeg",
        "isPro": false,
        "fullname": "Jialong Wu",
        "user": "manchery",
        "type": "user"
      },
      "summary": "World models predict state transitions in response to actions and are\nincreasingly developed across diverse modalities. However, standard training\nobjectives such as maximum likelihood estimation (MLE) often misalign with\ntask-specific goals of world models, i.e., transition prediction metrics like\naccuracy or perceptual quality. In this paper, we present RLVR-World, a unified\nframework that leverages reinforcement learning with verifiable rewards (RLVR)\nto directly optimize world models for such metrics. Despite formulating world\nmodeling as autoregressive prediction of tokenized sequences, RLVR-World\nevaluates metrics of decoded predictions as verifiable rewards. We demonstrate\nsubstantial performance gains on both language- and video-based world models\nacross domains, including text games, web navigation, and robot manipulation.\nOur work indicates that, beyond recent advances in reasoning language models,\nRLVR offers a promising post-training paradigm for enhancing the utility of\ngenerative models more broadly.",
      "upvotes": 7,
      "discussionId": "682e836bb16c79c271b4dc78",
      "projectPage": "https://thuml.github.io/RLVR-World/",
      "githubRepo": "https://github.com/thuml/RLVR-World",
      "ai_summary": "RLVR-World uses reinforcement learning with verifiable rewards to optimize world models for task-specific metrics, achieving improved performance across language and video domains.",
      "ai_keywords": [
        "world models",
        "maximum likelihood estimation",
        "transition prediction",
        "reinforcement learning",
        "verifiable rewards",
        "autoregressive prediction",
        "tokenized sequences",
        "text games",
        "web navigation",
        "robot manipulation"
      ]
    },
    "publishedAt": "2025-05-20T01:02:53.000Z",
    "title": "RLVR-World: Training World Models with Reinforcement Learning",
    "summary": "World models predict state transitions in response to actions and are\nincreasingly developed across diverse modalities. However, standard training\nobjectives such as maximum likelihood estimation (MLE) often misalign with\ntask-specific goals of world models, i.e., transition prediction metrics like\naccuracy or perceptual quality. In this paper, we present RLVR-World, a unified\nframework that leverages reinforcement learning with verifiable rewards (RLVR)\nto directly optimize world models for such metrics. Despite formulating world\nmodeling as autoregressive prediction of tokenized sequences, RLVR-World\nevaluates metrics of decoded predictions as verifiable rewards. We demonstrate\nsubstantial performance gains on both language- and video-based world models\nacross domains, including text games, web navigation, and robot manipulation.\nOur work indicates that, beyond recent advances in reasoning language models,\nRLVR offers a promising post-training paradigm for enhancing the utility of\ngenerative models more broadly.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/643b866bff50448bcfc7d1d1/OXSeIgpwt_mjJJqBffXyA.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13934.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643b866bff50448bcfc7d1d1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/q12v-BatVKimoi8q-coi-.jpeg",
      "fullname": "Jialong Wu",
      "name": "manchery",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.13529",
      "authors": [
        {
          "_id": "682e8c441ffd3af3cb2f27d3",
          "user": {
            "_id": "65d859a3661492b25c46a117",
            "avatarUrl": "/avatars/06a547a00b472f512a25eef4ddd047bf.svg",
            "isPro": false,
            "fullname": "junxiao yang",
            "user": "yangjunxiao2021",
            "type": "user"
          },
          "name": "Junxiao Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:16:19.492Z",
          "hidden": false
        },
        {
          "_id": "682e8c441ffd3af3cb2f27d4",
          "name": "Jinzhe Tu",
          "hidden": false
        },
        {
          "_id": "682e8c441ffd3af3cb2f27d5",
          "name": "Haoran Liu",
          "hidden": false
        },
        {
          "_id": "682e8c441ffd3af3cb2f27d6",
          "name": "Xiaoce Wang",
          "hidden": false
        },
        {
          "_id": "682e8c441ffd3af3cb2f27d7",
          "name": "Chujie Zheng",
          "hidden": false
        },
        {
          "_id": "682e8c441ffd3af3cb2f27d8",
          "name": "Zhexin Zhang",
          "hidden": false
        },
        {
          "_id": "682e8c441ffd3af3cb2f27d9",
          "name": "Shiyao Cui",
          "hidden": false
        },
        {
          "_id": "682e8c441ffd3af3cb2f27da",
          "name": "Caishun Chen",
          "hidden": false
        },
        {
          "_id": "682e8c441ffd3af3cb2f27db",
          "name": "Tiantian He",
          "hidden": false
        },
        {
          "_id": "682e8c441ffd3af3cb2f27dc",
          "name": "Hongning Wang",
          "hidden": false
        },
        {
          "_id": "682e8c441ffd3af3cb2f27dd",
          "name": "Yew-Soon Ong",
          "hidden": false
        },
        {
          "_id": "682e8c441ffd3af3cb2f27de",
          "name": "Minlie Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-18T07:27:34.000Z",
      "submittedOnDailyAt": "2025-05-22T01:02:19.522Z",
      "title": "BARREL: Boundary-Aware Reasoning for Factual and Reliable LRMs",
      "submittedOnDailyBy": {
        "_id": "65d859a3661492b25c46a117",
        "avatarUrl": "/avatars/06a547a00b472f512a25eef4ddd047bf.svg",
        "isPro": false,
        "fullname": "junxiao yang",
        "user": "yangjunxiao2021",
        "type": "user"
      },
      "summary": "Recent advances in Large Reasoning Models (LRMs) have shown impressive\ncapabilities in mathematical and logical reasoning. However, current LRMs\nrarely admit ignorance or respond with \"I don't know\". Instead, they often\nproduce incorrect answers while showing undue confidence, raising concerns\nabout their factual reliability. In this work, we identify two pathological\nreasoning patterns characterized by overthinking that contribute to the\noverconfident and incorrect answers: last-minute guessing and second-thought\nspiraling. To address these issues, we propose BARREL-a novel framework that\npromotes concise and boundary-aware factual reasoning. Our experiments show\nthat BARREL-training increases the reliability of DeepSeek-R1-Distill-Llama-8B\nfrom 39.33% to 61.48%, while still achieving accuracy comparable to models\nfinetuned on reasoning data generated by R1. These results demonstrate that our\npilot study is inspiring to build more reliable and factual System 2 LRMs.",
      "upvotes": 7,
      "discussionId": "682e8c451ffd3af3cb2f281f",
      "ai_summary": "A novel framework, BARREL, addresses overconfidence in Large Reasoning Models by promoting concise and factual reasoning, significantly improving their reliability.",
      "ai_keywords": [
        "Large Reasoning Models",
        "LRMs",
        "overthinking",
        "last-minute guessing",
        "second-thought spiraling",
        "boundary-aware",
        "DeepSeek",
        "R1-Distill-Llama-8B",
        "reliable System 2 LRMs"
      ]
    },
    "publishedAt": "2025-05-18T03:27:34.000Z",
    "title": "BARREL: Boundary-Aware Reasoning for Factual and Reliable LRMs",
    "summary": "Recent advances in Large Reasoning Models (LRMs) have shown impressive\ncapabilities in mathematical and logical reasoning. However, current LRMs\nrarely admit ignorance or respond with \"I don't know\". Instead, they often\nproduce incorrect answers while showing undue confidence, raising concerns\nabout their factual reliability. In this work, we identify two pathological\nreasoning patterns characterized by overthinking that contribute to the\noverconfident and incorrect answers: last-minute guessing and second-thought\nspiraling. To address these issues, we propose BARREL-a novel framework that\npromotes concise and boundary-aware factual reasoning. Our experiments show\nthat BARREL-training increases the reliability of DeepSeek-R1-Distill-Llama-8B\nfrom 39.33% to 61.48%, while still achieving accuracy comparable to models\nfinetuned on reasoning data generated by R1. These results demonstrate that our\npilot study is inspiring to build more reliable and factual System 2 LRMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13529.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65d859a3661492b25c46a117",
      "avatarUrl": "/avatars/06a547a00b472f512a25eef4ddd047bf.svg",
      "fullname": "junxiao yang",
      "name": "yangjunxiao2021",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.15778",
      "authors": [
        {
          "_id": "682e8b59a5b1e59c6978645a",
          "name": "Zhen Zhang",
          "hidden": false
        },
        {
          "_id": "682e8b59a5b1e59c6978645b",
          "name": "Xuehai He",
          "hidden": false
        },
        {
          "_id": "682e8b59a5b1e59c6978645c",
          "name": "Weixiang Yan",
          "hidden": false
        },
        {
          "_id": "682e8b59a5b1e59c6978645d",
          "name": "Ao Shen",
          "hidden": false
        },
        {
          "_id": "682e8b59a5b1e59c6978645e",
          "name": "Chenyang Zhao",
          "hidden": false
        },
        {
          "_id": "682e8b59a5b1e59c6978645f",
          "name": "Shuohang Wang",
          "hidden": false
        },
        {
          "_id": "682e8b59a5b1e59c69786460",
          "name": "Yelong Shen",
          "hidden": false
        },
        {
          "_id": "682e8b59a5b1e59c69786461",
          "name": "Xin Eric Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T17:29:15.000Z",
      "submittedOnDailyAt": "2025-05-22T00:56:40.217Z",
      "title": "Soft Thinking: Unlocking the Reasoning Potential of LLMs in Continuous\n  Concept Space",
      "submittedOnDailyBy": {
        "_id": "64679a226192d39142245e5e",
        "avatarUrl": "/avatars/05abee0b6317f100923936ca2099e9eb.svg",
        "isPro": false,
        "fullname": "Xin Eric Wang",
        "user": "xw-eric",
        "type": "user"
      },
      "summary": "Human cognition typically involves thinking through abstract, fluid concepts\nrather than strictly using discrete linguistic tokens. Current reasoning\nmodels, however, are constrained to reasoning within the boundaries of human\nlanguage, processing discrete token embeddings that represent fixed points in\nthe semantic space. This discrete constraint restricts the expressive power and\nupper potential of such reasoning models, often causing incomplete exploration\nof reasoning paths, as standard Chain-of-Thought (CoT) methods rely on sampling\none token per step. In this work, we introduce Soft Thinking, a training-free\nmethod that emulates human-like \"soft\" reasoning by generating soft, abstract\nconcept tokens in a continuous concept space. These concept tokens are created\nby the probability-weighted mixture of token embeddings, which form the\ncontinuous concept space, enabling smooth transitions and richer\nrepresentations that transcend traditional discrete boundaries. In essence,\neach generated concept token encapsulates multiple meanings from related\ndiscrete tokens, implicitly exploring various reasoning paths to converge\neffectively toward the correct answer. Empirical evaluations on diverse\nmathematical and coding benchmarks consistently demonstrate the effectiveness\nand efficiency of Soft Thinking, improving pass@1 accuracy by up to 2.48 points\nwhile simultaneously reducing token usage by up to 22.4% compared to standard\nCoT. Qualitative analysis further reveals that Soft Thinking outputs remain\nhighly interpretable and readable, highlighting the potential of Soft Thinking\nto break the inherent bottleneck of discrete language-based reasoning. Code is\navailable at https://github.com/eric-ai-lab/Soft-Thinking.",
      "upvotes": 6,
      "discussionId": "682e8b5aa5b1e59c697864ce",
      "projectPage": "https://soft-thinking.github.io",
      "githubRepo": "https://github.com/eric-ai-lab/Soft-Thinking",
      "ai_summary": "Soft Thinking, a training-free method, enhances reasoning by generating soft, abstract concept tokens in a continuous space, improving accuracy and efficiency in mathematical and coding benchmarks.",
      "ai_keywords": [
        "Soft Thinking",
        "continuous concept space",
        "token embeddings",
        "Chain-of-Thought",
        "pass@1 accuracy"
      ]
    },
    "publishedAt": "2025-05-21T13:29:15.000Z",
    "title": "Soft Thinking: Unlocking the Reasoning Potential of LLMs in Continuous\n  Concept Space",
    "summary": "Human cognition typically involves thinking through abstract, fluid concepts\nrather than strictly using discrete linguistic tokens. Current reasoning\nmodels, however, are constrained to reasoning within the boundaries of human\nlanguage, processing discrete token embeddings that represent fixed points in\nthe semantic space. This discrete constraint restricts the expressive power and\nupper potential of such reasoning models, often causing incomplete exploration\nof reasoning paths, as standard Chain-of-Thought (CoT) methods rely on sampling\none token per step. In this work, we introduce Soft Thinking, a training-free\nmethod that emulates human-like \"soft\" reasoning by generating soft, abstract\nconcept tokens in a continuous concept space. These concept tokens are created\nby the probability-weighted mixture of token embeddings, which form the\ncontinuous concept space, enabling smooth transitions and richer\nrepresentations that transcend traditional discrete boundaries. In essence,\neach generated concept token encapsulates multiple meanings from related\ndiscrete tokens, implicitly exploring various reasoning paths to converge\neffectively toward the correct answer. Empirical evaluations on diverse\nmathematical and coding benchmarks consistently demonstrate the effectiveness\nand efficiency of Soft Thinking, improving pass@1 accuracy by up to 2.48 points\nwhile simultaneously reducing token usage by up to 22.4% compared to standard\nCoT. Qualitative analysis further reveals that Soft Thinking outputs remain\nhighly interpretable and readable, highlighting the potential of Soft Thinking\nto break the inherent bottleneck of discrete language-based reasoning. Code is\navailable at https://github.com/eric-ai-lab/Soft-Thinking.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15778.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64679a226192d39142245e5e",
      "avatarUrl": "/avatars/05abee0b6317f100923936ca2099e9eb.svg",
      "fullname": "Xin Eric Wang",
      "name": "xw-eric",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15776",
      "authors": [
        {
          "_id": "682eb95671d01f3fc74f0f8c",
          "user": {
            "_id": "644a41fcd9a3ae8341055179",
            "avatarUrl": "/avatars/21b35abdc60a34589443b5879901eb46.svg",
            "isPro": false,
            "fullname": "Changtai Zhu",
            "user": "BeastyZ",
            "type": "user"
          },
          "name": "Changtai Zhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:15:31.485Z",
          "hidden": false
        },
        {
          "_id": "682eb95671d01f3fc74f0f8d",
          "name": "Siyin Wang",
          "hidden": false
        },
        {
          "_id": "682eb95671d01f3fc74f0f8e",
          "name": "Ruijun Feng",
          "hidden": false
        },
        {
          "_id": "682eb95671d01f3fc74f0f8f",
          "name": "Kai Song",
          "hidden": false
        },
        {
          "_id": "682eb95671d01f3fc74f0f90",
          "name": "Xipeng Qiu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T17:27:42.000Z",
      "submittedOnDailyAt": "2025-05-22T04:51:57.147Z",
      "title": "ConvSearch-R1: Enhancing Query Reformulation for Conversational Search\n  with Reasoning via Reinforcement Learning",
      "submittedOnDailyBy": {
        "_id": "64c3c631e77ea9f28111172a",
        "avatarUrl": "/avatars/495dbb73b69c399bae780da3118e332f.svg",
        "isPro": false,
        "fullname": "Siyin Wang",
        "user": "sinwang",
        "type": "user"
      },
      "summary": "Conversational search systems require effective handling of context-dependent\nqueries that often contain ambiguity, omission, and coreference. Conversational\nQuery Reformulation (CQR) addresses this challenge by transforming these\nqueries into self-contained forms suitable for off-the-shelf retrievers.\nHowever, existing CQR approaches suffer from two critical constraints: high\ndependency on costly external supervision from human annotations or large\nlanguage models, and insufficient alignment between the rewriting model and\ndownstream retrievers. We present ConvSearch-R1, the first self-driven\nframework that completely eliminates dependency on external rewrite supervision\nby leveraging reinforcement learning to optimize reformulation directly through\nretrieval signals. Our novel two-stage approach combines Self-Driven Policy\nWarm-Up to address the cold-start problem through retrieval-guided\nself-distillation, followed by Retrieval-Guided Reinforcement Learning with a\nspecially designed rank-incentive reward shaping mechanism that addresses the\nsparsity issue in conventional retrieval metrics. Extensive experiments on\nTopiOCQA and QReCC datasets demonstrate that ConvSearch-R1 significantly\noutperforms previous state-of-the-art methods, achieving over 10% improvement\non the challenging TopiOCQA dataset while using smaller 3B parameter models\nwithout any external supervision.",
      "upvotes": 5,
      "discussionId": "682eb95771d01f3fc74f0fdd",
      "githubRepo": "https://github.com/BeastyZ/ConvSearch-R1",
      "ai_summary": "ConvSearch-R1 uses reinforcement learning and self-distillation to improve conversational query reformulation without relying on external supervision, outperforming state-of-the-art methods.",
      "ai_keywords": [
        "Conversational Query Reformulation",
        "CQR",
        "reinforcement learning",
        "self-distillation",
        "cold-start problem",
        "rank-incentive reward shaping",
        "retrieval signals",
        "TopiOCQA",
        "QReCC"
      ]
    },
    "publishedAt": "2025-05-21T13:27:42.000Z",
    "title": "ConvSearch-R1: Enhancing Query Reformulation for Conversational Search\n  with Reasoning via Reinforcement Learning",
    "summary": "Conversational search systems require effective handling of context-dependent\nqueries that often contain ambiguity, omission, and coreference. Conversational\nQuery Reformulation (CQR) addresses this challenge by transforming these\nqueries into self-contained forms suitable for off-the-shelf retrievers.\nHowever, existing CQR approaches suffer from two critical constraints: high\ndependency on costly external supervision from human annotations or large\nlanguage models, and insufficient alignment between the rewriting model and\ndownstream retrievers. We present ConvSearch-R1, the first self-driven\nframework that completely eliminates dependency on external rewrite supervision\nby leveraging reinforcement learning to optimize reformulation directly through\nretrieval signals. Our novel two-stage approach combines Self-Driven Policy\nWarm-Up to address the cold-start problem through retrieval-guided\nself-distillation, followed by Retrieval-Guided Reinforcement Learning with a\nspecially designed rank-incentive reward shaping mechanism that addresses the\nsparsity issue in conventional retrieval metrics. Extensive experiments on\nTopiOCQA and QReCC datasets demonstrate that ConvSearch-R1 significantly\noutperforms previous state-of-the-art methods, achieving over 10% improvement\non the challenging TopiOCQA dataset while using smaller 3B parameter models\nwithout any external supervision.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15776.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c3c631e77ea9f28111172a",
      "avatarUrl": "/avatars/495dbb73b69c399bae780da3118e332f.svg",
      "fullname": "Siyin Wang",
      "name": "sinwang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.14231",
      "authors": [
        {
          "_id": "682db25d265177367e35d5b1",
          "name": "Sule Bai",
          "hidden": false
        },
        {
          "_id": "682db25d265177367e35d5b2",
          "name": "Mingxing Li",
          "hidden": false
        },
        {
          "_id": "682db25d265177367e35d5b3",
          "name": "Yong Liu",
          "hidden": false
        },
        {
          "_id": "682db25d265177367e35d5b4",
          "name": "Jing Tang",
          "hidden": false
        },
        {
          "_id": "682db25d265177367e35d5b5",
          "name": "Haoji Zhang",
          "hidden": false
        },
        {
          "_id": "682db25d265177367e35d5b6",
          "name": "Lei Sun",
          "hidden": false
        },
        {
          "_id": "682db25d265177367e35d5b7",
          "user": {
            "_id": "66d255e3947594430c723ff6",
            "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg",
            "isPro": false,
            "fullname": "xiaochonglinghu",
            "user": "xiaochonglinghu",
            "type": "user"
          },
          "name": "Xiangxiang Chu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T12:27:07.959Z",
          "hidden": false
        },
        {
          "_id": "682db25d265177367e35d5b8",
          "name": "Yansong Tang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T11:40:43.000Z",
      "submittedOnDailyAt": "2025-05-22T00:13:45.780Z",
      "title": "UniVG-R1: Reasoning Guided Universal Visual Grounding with Reinforcement\n  Learning",
      "submittedOnDailyBy": {
        "_id": "6513a0f14f1682e4407758a9",
        "avatarUrl": "/avatars/b2a6886114492944cfa235363817565f.svg",
        "isPro": false,
        "fullname": "Mingxing Li",
        "user": "MingxingLi",
        "type": "user"
      },
      "summary": "Traditional visual grounding methods primarily focus on single-image\nscenarios with simple textual references. However, extending these methods to\nreal-world scenarios that involve implicit and complex instructions,\nparticularly in conjunction with multiple images, poses significant challenges,\nwhich is mainly due to the lack of advanced reasoning ability across diverse\nmulti-modal contexts. In this work, we aim to address the more practical\nuniversal grounding task, and propose UniVG-R1, a reasoning guided multimodal\nlarge language model (MLLM) for universal visual grounding, which enhances\nreasoning capabilities through reinforcement learning (RL) combined with\ncold-start data. Specifically, we first construct a high-quality\nChain-of-Thought (CoT) grounding dataset, annotated with detailed reasoning\nchains, to guide the model towards correct reasoning paths via supervised\nfine-tuning. Subsequently, we perform rule-based reinforcement learning to\nencourage the model to identify correct reasoning chains, thereby incentivizing\nits reasoning capabilities. In addition, we identify a difficulty bias arising\nfrom the prevalence of easy samples as RL training progresses, and we propose a\ndifficulty-aware weight adjustment strategy to further strengthen the\nperformance. Experimental results demonstrate the effectiveness of UniVG-R1,\nwhich achieves state-of-the-art performance on MIG-Bench with a 9.1%\nimprovement over the previous method. Furthermore, our model exhibits strong\ngeneralizability, achieving an average improvement of 23.4% in zero-shot\nperformance across four image and video reasoning grounding benchmarks. The\nproject page can be accessed at https://amap-ml.github.io/UniVG-R1-page/.",
      "upvotes": 5,
      "discussionId": "682db25e265177367e35d638",
      "projectPage": "https://amap-ml.github.io/UniVG-R1-page/",
      "ai_summary": "UniVG-R1, a reasoning-guided multimodal large language model, enhances visual grounding by leveraging reinforcement learning and a difficulty-aware strategy, achieving state-of-the-art results and strong generalizability.",
      "ai_keywords": [
        "multimodal large language model",
        "reasoning guided",
        "reinforcement learning",
        "cold-start data",
        "Chain-of-Thought dataset",
        "supervised fine-tuning",
        "rule-based reinforcement learning",
        "difficulty bias",
        "difficulty-aware weight adjustment"
      ]
    },
    "publishedAt": "2025-05-20T07:40:43.000Z",
    "title": "UniVG-R1: Reasoning Guided Universal Visual Grounding with Reinforcement\n  Learning",
    "summary": "Traditional visual grounding methods primarily focus on single-image\nscenarios with simple textual references. However, extending these methods to\nreal-world scenarios that involve implicit and complex instructions,\nparticularly in conjunction with multiple images, poses significant challenges,\nwhich is mainly due to the lack of advanced reasoning ability across diverse\nmulti-modal contexts. In this work, we aim to address the more practical\nuniversal grounding task, and propose UniVG-R1, a reasoning guided multimodal\nlarge language model (MLLM) for universal visual grounding, which enhances\nreasoning capabilities through reinforcement learning (RL) combined with\ncold-start data. Specifically, we first construct a high-quality\nChain-of-Thought (CoT) grounding dataset, annotated with detailed reasoning\nchains, to guide the model towards correct reasoning paths via supervised\nfine-tuning. Subsequently, we perform rule-based reinforcement learning to\nencourage the model to identify correct reasoning chains, thereby incentivizing\nits reasoning capabilities. In addition, we identify a difficulty bias arising\nfrom the prevalence of easy samples as RL training progresses, and we propose a\ndifficulty-aware weight adjustment strategy to further strengthen the\nperformance. Experimental results demonstrate the effectiveness of UniVG-R1,\nwhich achieves state-of-the-art performance on MIG-Bench with a 9.1%\nimprovement over the previous method. Furthermore, our model exhibits strong\ngeneralizability, achieving an average improvement of 23.4% in zero-shot\nperformance across four image and video reasoning grounding benchmarks. The\nproject page can be accessed at https://amap-ml.github.io/UniVG-R1-page/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14231.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6513a0f14f1682e4407758a9",
      "avatarUrl": "/avatars/b2a6886114492944cfa235363817565f.svg",
      "fullname": "Mingxing Li",
      "name": "MingxingLi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15817",
      "authors": [
        {
          "_id": "682ec132fbbbc6e3a91b106e",
          "user": {
            "_id": "6623ea65b642e29cdf90a1b4",
            "avatarUrl": "/avatars/e32e90574c1162b2be87ed78604e3e4d.svg",
            "isPro": true,
            "fullname": "TongZheng",
            "user": "TongZheng1999",
            "type": "user"
          },
          "name": "Tong Zheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:15:27.903Z",
          "hidden": false
        },
        {
          "_id": "682ec132fbbbc6e3a91b106f",
          "name": "Lichang Chen",
          "hidden": false
        },
        {
          "_id": "682ec132fbbbc6e3a91b1070",
          "name": "Simeng Han",
          "hidden": false
        },
        {
          "_id": "682ec132fbbbc6e3a91b1071",
          "name": "R. Thomas McCoy",
          "hidden": false
        },
        {
          "_id": "682ec132fbbbc6e3a91b1072",
          "name": "Heng Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T17:59:54.000Z",
      "submittedOnDailyAt": "2025-05-22T06:15:33.221Z",
      "title": "Learning to Reason via Mixture-of-Thought for Logical Reasoning",
      "submittedOnDailyBy": {
        "_id": "6623ea65b642e29cdf90a1b4",
        "avatarUrl": "/avatars/e32e90574c1162b2be87ed78604e3e4d.svg",
        "isPro": true,
        "fullname": "TongZheng",
        "user": "TongZheng1999",
        "type": "user"
      },
      "summary": "Human beings naturally utilize multiple reasoning modalities to learn and\nsolve logical problems, i.e., different representational formats such as\nnatural language, code, and symbolic logic. In contrast, most existing\nLLM-based approaches operate with a single reasoning modality during training,\ntypically natural language. Although some methods explored modality selection\nor augmentation at inference time, the training process remains modality-blind,\nlimiting synergy among modalities. To fill in this gap, we propose\nMixture-of-Thought (MoT), a framework that enables LLMs to reason across three\ncomplementary modalities: natural language, code, and a newly introduced\nsymbolic modality, truth-table, which systematically enumerates logical cases\nand partially mitigates key failure modes in natural language reasoning. MoT\nadopts a two-phase design: (1) self-evolving MoT training, which jointly learns\nfrom filtered, self-generated rationales across modalities; and (2) MoT\ninference, which fully leverages the synergy of three modalities to produce\nbetter predictions. Experiments on logical reasoning benchmarks including FOLIO\nand ProofWriter demonstrate that our MoT framework consistently and\nsignificantly outperforms strong LLM baselines with single-modality\nchain-of-thought approaches, achieving up to +11.7pp average accuracy gain.\nFurther analyses show that our MoT framework benefits both training and\ninference stages; that it is particularly effective on harder logical reasoning\nproblems; and that different modalities contribute complementary strengths,\nwith truth-table reasoning helping to overcome key bottlenecks in natural\nlanguage inference.",
      "upvotes": 4,
      "discussionId": "682ec133fbbbc6e3a91b10c1",
      "githubRepo": "https://github.com/zhengkid/Truth_Table_Logical_Reasoning",
      "ai_summary": "A Mixture-of-Thought framework enables LLMs to reason across natural language, code, and symbolic logic, improving accuracy on logical reasoning tasks compared to single-modality approaches.",
      "ai_keywords": [
        "LMM-based approaches",
        "reasoning modality",
        "natural language",
        "code",
        "symbolic logic",
        "modality-blind",
        "self-evolving MoT training",
        "MoT inference",
        "FOLIO",
        "ProofWriter",
        "truth-table",
        "logical reasoning"
      ]
    },
    "publishedAt": "2025-05-21T13:59:54.000Z",
    "title": "Learning to Reason via Mixture-of-Thought for Logical Reasoning",
    "summary": "Human beings naturally utilize multiple reasoning modalities to learn and\nsolve logical problems, i.e., different representational formats such as\nnatural language, code, and symbolic logic. In contrast, most existing\nLLM-based approaches operate with a single reasoning modality during training,\ntypically natural language. Although some methods explored modality selection\nor augmentation at inference time, the training process remains modality-blind,\nlimiting synergy among modalities. To fill in this gap, we propose\nMixture-of-Thought (MoT), a framework that enables LLMs to reason across three\ncomplementary modalities: natural language, code, and a newly introduced\nsymbolic modality, truth-table, which systematically enumerates logical cases\nand partially mitigates key failure modes in natural language reasoning. MoT\nadopts a two-phase design: (1) self-evolving MoT training, which jointly learns\nfrom filtered, self-generated rationales across modalities; and (2) MoT\ninference, which fully leverages the synergy of three modalities to produce\nbetter predictions. Experiments on logical reasoning benchmarks including FOLIO\nand ProofWriter demonstrate that our MoT framework consistently and\nsignificantly outperforms strong LLM baselines with single-modality\nchain-of-thought approaches, achieving up to +11.7pp average accuracy gain.\nFurther analyses show that our MoT framework benefits both training and\ninference stages; that it is particularly effective on harder logical reasoning\nproblems; and that different modalities contribute complementary strengths,\nwith truth-table reasoning helping to overcome key bottlenecks in natural\nlanguage inference.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15817.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6623ea65b642e29cdf90a1b4",
      "avatarUrl": "/avatars/e32e90574c1162b2be87ed78604e3e4d.svg",
      "fullname": "TongZheng",
      "name": "TongZheng1999",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.12650",
      "authors": [
        {
          "_id": "682c057b36dd2dbca3931150",
          "user": {
            "_id": "64ca1b79ec9a33183aa3bd29",
            "avatarUrl": "/avatars/cafeafeccced927aac986575338a804c.svg",
            "isPro": false,
            "fullname": "yang",
            "user": "yaotianvector",
            "type": "user"
          },
          "name": "Yaotian Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:18:15.239Z",
          "hidden": false
        },
        {
          "_id": "682c057b36dd2dbca3931151",
          "user": {
            "_id": "6552f1ad5d55ccb20e9142a0",
            "avatarUrl": "/avatars/0e3e80cba64b5ae0bc5638694ac33dbf.svg",
            "isPro": false,
            "fullname": "Ivan Tang",
            "user": "IvanTang",
            "type": "user"
          },
          "name": "Yiwen Tang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:18:12.714Z",
          "hidden": false
        },
        {
          "_id": "682c057b36dd2dbca3931152",
          "name": "Yizhe Chen",
          "hidden": false
        },
        {
          "_id": "682c057b36dd2dbca3931153",
          "name": "Xiao Chen",
          "hidden": false
        },
        {
          "_id": "682c057b36dd2dbca3931154",
          "name": "Jiangjie Qiu",
          "hidden": false
        },
        {
          "_id": "682c057b36dd2dbca3931155",
          "name": "Hao Xiong",
          "hidden": false
        },
        {
          "_id": "682c057b36dd2dbca3931156",
          "name": "Haoyu Yin",
          "hidden": false
        },
        {
          "_id": "682c057b36dd2dbca3931157",
          "name": "Zhiyao Luo",
          "hidden": false
        },
        {
          "_id": "682c057b36dd2dbca3931158",
          "name": "Yifei Zhang",
          "hidden": false
        },
        {
          "_id": "682c057b36dd2dbca3931159",
          "name": "Sijia Tao",
          "hidden": false
        },
        {
          "_id": "682c057b36dd2dbca393115a",
          "name": "Wentao Li",
          "hidden": false
        },
        {
          "_id": "682c057b36dd2dbca393115b",
          "name": "Qinghua Zhang",
          "hidden": false
        },
        {
          "_id": "682c057b36dd2dbca393115c",
          "name": "Yuqiang Li",
          "hidden": false
        },
        {
          "_id": "682c057b36dd2dbca393115d",
          "name": "Wanli Ouyang",
          "hidden": false
        },
        {
          "_id": "682c057b36dd2dbca393115e",
          "name": "Bin Zhao",
          "hidden": false
        },
        {
          "_id": "682c057b36dd2dbca393115f",
          "name": "Xiaonan Wang",
          "hidden": false
        },
        {
          "_id": "682c057b36dd2dbca3931160",
          "name": "Fei Wei",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-19T03:04:50.000Z",
      "submittedOnDailyAt": "2025-05-22T00:29:51.182Z",
      "title": "AutoMat: Enabling Automated Crystal Structure Reconstruction from\n  Microscopy via Agentic Tool Use",
      "submittedOnDailyBy": {
        "_id": "6552f1ad5d55ccb20e9142a0",
        "avatarUrl": "/avatars/0e3e80cba64b5ae0bc5638694ac33dbf.svg",
        "isPro": false,
        "fullname": "Ivan Tang",
        "user": "IvanTang",
        "type": "user"
      },
      "summary": "Machine learning-based interatomic potentials and force fields depend\ncritically on accurate atomic structures, yet such data are scarce due to the\nlimited availability of experimentally resolved crystals. Although\natomic-resolution electron microscopy offers a potential source of structural\ndata, converting these images into simulation-ready formats remains\nlabor-intensive and error-prone, creating a bottleneck for model training and\nvalidation. We introduce AutoMat, an end-to-end, agent-assisted pipeline that\nautomatically transforms scanning transmission electron microscopy (STEM)\nimages into atomic crystal structures and predicts their physical properties.\nAutoMat combines pattern-adaptive denoising, physics-guided template retrieval,\nsymmetry-aware atomic reconstruction, fast relaxation and property prediction\nvia MatterSim, and coordinated orchestration across all stages. We propose the\nfirst dedicated STEM2Mat-Bench for this task and evaluate performance using\nlattice RMSD, formation energy MAE, and structure-matching success rate. By\norchestrating external tool calls, AutoMat enables a text-only LLM to\noutperform vision-language models in this domain, achieving closed-loop\nreasoning throughout the pipeline. In large-scale experiments over 450\nstructure samples, AutoMat substantially outperforms existing multimodal large\nlanguage models and tools. These results validate both AutoMat and\nSTEM2Mat-Bench, marking a key step toward bridging microscopy and atomistic\nsimulation in materials science.The code and dataset are publicly available at\nhttps://github.com/yyt-2378/AutoMat and\nhttps://huggingface.co/datasets/yaotianvector/STEM2Mat.",
      "upvotes": 4,
      "discussionId": "682c057d36dd2dbca3931206",
      "ai_summary": "AutoMat, an agent-assisted pipeline, transforms atomic-resolution STEM images into simulation-ready atomic crystal structures and predicts their properties, overcoming the bottleneck in data availability and processing.",
      "ai_keywords": [
        "end-to-end pipeline",
        "agent-assisted",
        "scanning transmission electron microscopy (STEM)",
        "pattern-adaptive denoising",
        "physics-guided template retrieval",
        "symmetry-aware atomic reconstruction",
        "fast relaxation",
        "property prediction",
        "MatterSim",
        "STEM2Mat-Bench",
        "lattice RMSD",
        "formation energy MAE",
        "structure-matching success rate",
        "multimodal large language models"
      ]
    },
    "publishedAt": "2025-05-18T23:04:50.000Z",
    "title": "AutoMat: Enabling Automated Crystal Structure Reconstruction from\n  Microscopy via Agentic Tool Use",
    "summary": "Machine learning-based interatomic potentials and force fields depend\ncritically on accurate atomic structures, yet such data are scarce due to the\nlimited availability of experimentally resolved crystals. Although\natomic-resolution electron microscopy offers a potential source of structural\ndata, converting these images into simulation-ready formats remains\nlabor-intensive and error-prone, creating a bottleneck for model training and\nvalidation. We introduce AutoMat, an end-to-end, agent-assisted pipeline that\nautomatically transforms scanning transmission electron microscopy (STEM)\nimages into atomic crystal structures and predicts their physical properties.\nAutoMat combines pattern-adaptive denoising, physics-guided template retrieval,\nsymmetry-aware atomic reconstruction, fast relaxation and property prediction\nvia MatterSim, and coordinated orchestration across all stages. We propose the\nfirst dedicated STEM2Mat-Bench for this task and evaluate performance using\nlattice RMSD, formation energy MAE, and structure-matching success rate. By\norchestrating external tool calls, AutoMat enables a text-only LLM to\noutperform vision-language models in this domain, achieving closed-loop\nreasoning throughout the pipeline. In large-scale experiments over 450\nstructure samples, AutoMat substantially outperforms existing multimodal large\nlanguage models and tools. These results validate both AutoMat and\nSTEM2Mat-Bench, marking a key step toward bridging microscopy and atomistic\nsimulation in materials science.The code and dataset are publicly available at\nhttps://github.com/yyt-2378/AutoMat and\nhttps://huggingface.co/datasets/yaotianvector/STEM2Mat.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.12650.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6552f1ad5d55ccb20e9142a0",
      "avatarUrl": "/avatars/0e3e80cba64b5ae0bc5638694ac33dbf.svg",
      "fullname": "Ivan Tang",
      "name": "IvanTang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.14827",
      "authors": [
        {
          "_id": "682ea25b2c417303938ae8fc",
          "name": "Yufan Zhuang",
          "hidden": false
        },
        {
          "_id": "682ea25b2c417303938ae8fd",
          "name": "Liyuan Liu",
          "hidden": false
        },
        {
          "_id": "682ea25b2c417303938ae8fe",
          "name": "Chandan Singh",
          "hidden": false
        },
        {
          "_id": "682ea25b2c417303938ae8ff",
          "name": "Jingbo Shang",
          "hidden": false
        },
        {
          "_id": "682ea25b2c417303938ae900",
          "name": "Jianfeng Gao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6438ccbb3b46237de3d052e8/6rJB7g-l02Jn_iSD0C7ZJ.png"
      ],
      "publishedAt": "2025-05-20T18:41:46.000Z",
      "submittedOnDailyAt": "2025-05-22T02:35:44.464Z",
      "title": "Text Generation Beyond Discrete Token Sampling",
      "submittedOnDailyBy": {
        "_id": "6438ccbb3b46237de3d052e8",
        "avatarUrl": "/avatars/baa624d417b0b905e82127dc66346478.svg",
        "isPro": true,
        "fullname": "Yufan Zhuang",
        "user": "yzhuang",
        "type": "user"
      },
      "summary": "In standard autoregressive generation, an LLM predicts the next-token\ndistribution, samples a discrete token, and then discards the distribution,\npassing only the sampled token as new input. To preserve this distribution's\nrich information, we propose Mixture of Inputs (MoI), a training-free method\nfor autoregressive generation. After generating a token following the standard\nparadigm, we construct a new input that blends the generated discrete token\nwith the previously discarded token distribution. Specifically, we employ a\nBayesian estimation method that treats the token distribution as the prior, the\nsampled token as the observation, and replaces the conventional one-hot vector\nwith the continuous posterior expectation as the new model input. MoI allows\nthe model to maintain a richer internal representation throughout the\ngeneration process, resulting in improved text quality and reasoning\ncapabilities. On mathematical reasoning, code generation, and PhD-level QA\ntasks, MoI consistently improves performance across multiple models including\nQwQ-32B, Nemotron-Super-49B, Gemma-3-27B, and DAPO-Qwen-32B, with no additional\ntraining and negligible computational overhead.",
      "upvotes": 3,
      "discussionId": "682ea25c2c417303938ae926",
      "projectPage": "https://github.com/EvanZhuang/mixinputs",
      "githubRepo": "https://github.com/EvanZhuang/mixinputs",
      "ai_summary": "Mixture of Inputs (MoI), a training-free method, enhances autoregressive generation by maintaining a richer internal representation, improving text quality and reasoning capabilities in mathematical reasoning, code generation, and PhD-level QA tasks.",
      "ai_keywords": [
        "Mixture of Inputs (MoI)",
        "autoregressive generation",
        "token distribution",
        "Bayesian estimation",
        "posterior expectation"
      ]
    },
    "publishedAt": "2025-05-20T14:41:46.000Z",
    "title": "Text Generation Beyond Discrete Token Sampling",
    "summary": "In standard autoregressive generation, an LLM predicts the next-token\ndistribution, samples a discrete token, and then discards the distribution,\npassing only the sampled token as new input. To preserve this distribution's\nrich information, we propose Mixture of Inputs (MoI), a training-free method\nfor autoregressive generation. After generating a token following the standard\nparadigm, we construct a new input that blends the generated discrete token\nwith the previously discarded token distribution. Specifically, we employ a\nBayesian estimation method that treats the token distribution as the prior, the\nsampled token as the observation, and replaces the conventional one-hot vector\nwith the continuous posterior expectation as the new model input. MoI allows\nthe model to maintain a richer internal representation throughout the\ngeneration process, resulting in improved text quality and reasoning\ncapabilities. On mathematical reasoning, code generation, and PhD-level QA\ntasks, MoI consistently improves performance across multiple models including\nQwQ-32B, Nemotron-Super-49B, Gemma-3-27B, and DAPO-Qwen-32B, with no additional\ntraining and negligible computational overhead.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6438ccbb3b46237de3d052e8/6rJB7g-l02Jn_iSD0C7ZJ.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14827.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6438ccbb3b46237de3d052e8",
      "avatarUrl": "/avatars/baa624d417b0b905e82127dc66346478.svg",
      "fullname": "Yufan Zhuang",
      "name": "yzhuang",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15816",
      "authors": [
        {
          "_id": "682e8304e9980508c9a2fc73",
          "name": "Penghao Wu",
          "hidden": false
        },
        {
          "_id": "682e8304e9980508c9a2fc74",
          "name": "Lewei Lu",
          "hidden": false
        },
        {
          "_id": "682e8304e9980508c9a2fc75",
          "name": "Ziwei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T17:59:52.000Z",
      "submittedOnDailyAt": "2025-05-22T00:23:40.991Z",
      "title": "Streamline Without Sacrifice - Squeeze out Computation Redundancy in LMM",
      "submittedOnDailyBy": {
        "_id": "64101f81b27543634e377fc1",
        "avatarUrl": "/avatars/557dd9d4707e3b38e0805dfb87c08004.svg",
        "isPro": false,
        "fullname": "Penghao Wu",
        "user": "craigwu",
        "type": "user"
      },
      "summary": "Large multimodal models excel in multimodal tasks but face significant\ncomputational challenges due to excessive computation on visual tokens. Unlike\ntoken reduction methods that focus on token-level redundancy, we identify and\nstudy the computation-level redundancy on vision tokens to ensure no\ninformation loss. Our key insight is that vision tokens from the pretrained\nvision encoder do not necessarily require all the heavy operations (e.g.,\nself-attention, FFNs) in decoder-only LMMs and could be processed more lightly\nwith proper designs. We designed a series of experiments to discover and\nprogressively squeeze out the vision-related computation redundancy. Based on\nour findings, we propose ProxyV, a novel approach that utilizes proxy vision\ntokens to alleviate the computational burden on original vision tokens. ProxyV\nenhances efficiency without compromising performance and can even yield notable\nperformance gains in scenarios with more moderate efficiency improvements.\nFurthermore, the flexibility of ProxyV is demonstrated through its combination\nwith token reduction methods to boost efficiency further. The code will be made\npublic at this https://github.com/penghao-wu/ProxyV URL.",
      "upvotes": 2,
      "discussionId": "682e8305e9980508c9a2fca1",
      "projectPage": "https://penghao-wu.github.io/ProxyV/",
      "githubRepo": "https://github.com/penghao-wu/ProxyV",
      "ai_summary": "ProxyV alleviates computational burdens in large multimodal models by using proxy vision tokens, enhancing efficiency without sacrificing performance.",
      "ai_keywords": [
        "multimodal models",
        "computation-level redundancy",
        "vision tokens",
        "pretrained vision encoder",
        "decoder-only LMMs",
        "self-attention",
        "FFNs",
        "proxy vision tokens",
        "ProxyV"
      ]
    },
    "publishedAt": "2025-05-21T13:59:52.000Z",
    "title": "Streamline Without Sacrifice - Squeeze out Computation Redundancy in LMM",
    "summary": "Large multimodal models excel in multimodal tasks but face significant\ncomputational challenges due to excessive computation on visual tokens. Unlike\ntoken reduction methods that focus on token-level redundancy, we identify and\nstudy the computation-level redundancy on vision tokens to ensure no\ninformation loss. Our key insight is that vision tokens from the pretrained\nvision encoder do not necessarily require all the heavy operations (e.g.,\nself-attention, FFNs) in decoder-only LMMs and could be processed more lightly\nwith proper designs. We designed a series of experiments to discover and\nprogressively squeeze out the vision-related computation redundancy. Based on\nour findings, we propose ProxyV, a novel approach that utilizes proxy vision\ntokens to alleviate the computational burden on original vision tokens. ProxyV\nenhances efficiency without compromising performance and can even yield notable\nperformance gains in scenarios with more moderate efficiency improvements.\nFurthermore, the flexibility of ProxyV is demonstrated through its combination\nwith token reduction methods to boost efficiency further. The code will be made\npublic at this https://github.com/penghao-wu/ProxyV URL.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15816.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64101f81b27543634e377fc1",
      "avatarUrl": "/avatars/557dd9d4707e3b38e0805dfb87c08004.svg",
      "fullname": "Penghao Wu",
      "name": "craigwu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15791",
      "authors": [
        {
          "_id": "682e8e9f1d7637a2584b6b8c",
          "name": "Fengyuan Dai",
          "hidden": false
        },
        {
          "_id": "682e8e9f1d7637a2584b6b8d",
          "name": "Zifeng Zhuang",
          "hidden": false
        },
        {
          "_id": "682e8e9f1d7637a2584b6b8e",
          "name": "Yufei Huang",
          "hidden": false
        },
        {
          "_id": "682e8e9f1d7637a2584b6b8f",
          "name": "Siteng Huang",
          "hidden": false
        },
        {
          "_id": "682e8e9f1d7637a2584b6b90",
          "name": "Bangyan Liao",
          "hidden": false
        },
        {
          "_id": "682e8e9f1d7637a2584b6b91",
          "name": "Donglin Wang",
          "hidden": false
        },
        {
          "_id": "682e8e9f1d7637a2584b6b92",
          "name": "Fajie Yuan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T17:44:37.000Z",
      "submittedOnDailyAt": "2025-05-22T01:10:45.389Z",
      "title": "VARD: Efficient and Dense Fine-Tuning for Diffusion Models with\n  Value-based RL",
      "submittedOnDailyBy": {
        "_id": "65fd82762bf2cd20ddaa193f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yBYbWp_mT7UusYdkqtAvw.png",
        "isPro": false,
        "fullname": "Siteng Huang",
        "user": "huangsiteng",
        "type": "user"
      },
      "summary": "Diffusion models have emerged as powerful generative tools across various\ndomains, yet tailoring pre-trained models to exhibit specific desirable\nproperties remains challenging. While reinforcement learning (RL) offers a\npromising solution,current methods struggle to simultaneously achieve stable,\nefficient fine-tuning and support non-differentiable rewards. Furthermore,\ntheir reliance on sparse rewards provides inadequate supervision during\nintermediate steps, often resulting in suboptimal generation quality. To\naddress these limitations, dense and differentiable signals are required\nthroughout the diffusion process. Hence, we propose VAlue-based Reinforced\nDiffusion (VARD): a novel approach that first learns a value function\npredicting expection of rewards from intermediate states, and subsequently uses\nthis value function with KL regularization to provide dense supervision\nthroughout the generation process. Our method maintains proximity to the\npretrained model while enabling effective and stable training via\nbackpropagation. Experimental results demonstrate that our approach facilitates\nbetter trajectory guidance, improves training efficiency and extends the\napplicability of RL to diffusion models optimized for complex,\nnon-differentiable reward functions.",
      "upvotes": 2,
      "discussionId": "682e8ea11d7637a2584b6c46",
      "ai_summary": "VARD introduces a value function based reinforcement learning approach to enhance diffusion models with dense and differentiable supervision, improving training efficiency and handling non-differentiable rewards.",
      "ai_keywords": [
        "diffusion models",
        "reinforcement learning",
        "stable fine-tuning",
        "non-differentiable rewards",
        "sparse rewards",
        "value function",
        "dense supervision",
        "KL regularization",
        "backpropagation",
        "trajectory guidance",
        "training efficiency"
      ]
    },
    "publishedAt": "2025-05-21T13:44:37.000Z",
    "title": "VARD: Efficient and Dense Fine-Tuning for Diffusion Models with\n  Value-based RL",
    "summary": "Diffusion models have emerged as powerful generative tools across various\ndomains, yet tailoring pre-trained models to exhibit specific desirable\nproperties remains challenging. While reinforcement learning (RL) offers a\npromising solution,current methods struggle to simultaneously achieve stable,\nefficient fine-tuning and support non-differentiable rewards. Furthermore,\ntheir reliance on sparse rewards provides inadequate supervision during\nintermediate steps, often resulting in suboptimal generation quality. To\naddress these limitations, dense and differentiable signals are required\nthroughout the diffusion process. Hence, we propose VAlue-based Reinforced\nDiffusion (VARD): a novel approach that first learns a value function\npredicting expection of rewards from intermediate states, and subsequently uses\nthis value function with KL regularization to provide dense supervision\nthroughout the generation process. Our method maintains proximity to the\npretrained model while enabling effective and stable training via\nbackpropagation. Experimental results demonstrate that our approach facilitates\nbetter trajectory guidance, improves training efficiency and extends the\napplicability of RL to diffusion models optimized for complex,\nnon-differentiable reward functions.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15791.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65fd82762bf2cd20ddaa193f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yBYbWp_mT7UusYdkqtAvw.png",
      "fullname": "Siteng Huang",
      "name": "huangsiteng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15047",
      "authors": [
        {
          "_id": "682e8a7651706f69070c40d3",
          "name": "Yingming Pu",
          "hidden": false
        },
        {
          "_id": "682e8a7651706f69070c40d4",
          "name": "Tao Lin",
          "hidden": false
        },
        {
          "_id": "682e8a7651706f69070c40d5",
          "name": "Hongyu Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T03:09:39.000Z",
      "submittedOnDailyAt": "2025-05-22T00:58:05.428Z",
      "title": "PiFlow: Principle-aware Scientific Discovery with Multi-Agent\n  Collaboration",
      "submittedOnDailyBy": {
        "_id": "63d0c7d16b985b0f25d00a22",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63d0c7d16b985b0f25d00a22/6bNSM4WhVsEndIVx4Mb8U.png",
        "isPro": false,
        "fullname": "Mellen Y. Pu",
        "user": "Mellen",
        "type": "user"
      },
      "summary": "Large Language Model (LLM)-based multi-agent systems (MAS) demonstrate\nremarkable potential for scientific discovery. Existing approaches, however,\noften automate scientific discovery using predefined workflows that lack\nrationality constraints. This often leads to aimless hypothesizing and a\nfailure to consistently link hypotheses with evidence, thereby hindering\nsystematic uncertainty reduction. Overcoming these limitations fundamentally\nrequires systematic uncertainty reduction. We introduce PiFlow, an\ninformation-theoretical framework, treating automated scientific discovery as a\nstructured uncertainty reduction problem guided by principles (e.g., scientific\nlaws). In evaluations across three distinct scientific domains -- discovering\nnanomaterial structures, bio-molecules, and superconductor candidates with\ntargeted properties -- our method significantly improves discovery efficiency,\nreflected by a 73.55\\% increase in the Area Under the Curve (AUC) of property\nvalues versus exploration steps, and enhances solution quality by 94.06\\%\ncompared to a vanilla agent system. Overall, PiFlow serves as a\nPlug-and-Play method, establishing a novel paradigm shift in highly efficient\nautomated scientific discovery, paving the way for more robust and accelerated\nAI-driven research. Code is publicly available at our\nhttps://github.com/amair-lab/PiFlow{GitHub}.",
      "upvotes": 2,
      "discussionId": "682e8a7751706f69070c4121",
      "githubRepo": "https://github.com/amair-lab/PiFlow",
      "ai_summary": "PiFlow, an information-theoretical framework, improves automated scientific discovery by systematically reducing uncertainty and enhancing solution quality across various scientific domains.",
      "ai_keywords": [
        "Large Language Model",
        "multi-agent systems",
        "automated scientific discovery",
        "predefined workflows",
        "scientific discovery",
        "information-theoretical framework",
        "uncertainty reduction",
        "hypothesis",
        "evidence",
        "Area Under the Curve",
        "exploration steps",
        "solution quality",
        "Plug-and-Play method"
      ]
    },
    "publishedAt": "2025-05-20T23:09:39.000Z",
    "title": "PiFlow: Principle-aware Scientific Discovery with Multi-Agent\n  Collaboration",
    "summary": "Large Language Model (LLM)-based multi-agent systems (MAS) demonstrate\nremarkable potential for scientific discovery. Existing approaches, however,\noften automate scientific discovery using predefined workflows that lack\nrationality constraints. This often leads to aimless hypothesizing and a\nfailure to consistently link hypotheses with evidence, thereby hindering\nsystematic uncertainty reduction. Overcoming these limitations fundamentally\nrequires systematic uncertainty reduction. We introduce PiFlow, an\ninformation-theoretical framework, treating automated scientific discovery as a\nstructured uncertainty reduction problem guided by principles (e.g., scientific\nlaws). In evaluations across three distinct scientific domains -- discovering\nnanomaterial structures, bio-molecules, and superconductor candidates with\ntargeted properties -- our method significantly improves discovery efficiency,\nreflected by a 73.55\\% increase in the Area Under the Curve (AUC) of property\nvalues versus exploration steps, and enhances solution quality by 94.06\\%\ncompared to a vanilla agent system. Overall, PiFlow serves as a\nPlug-and-Play method, establishing a novel paradigm shift in highly efficient\nautomated scientific discovery, paving the way for more robust and accelerated\nAI-driven research. Code is publicly available at our\nhttps://github.com/amair-lab/PiFlow{GitHub}.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15047.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63d0c7d16b985b0f25d00a22",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63d0c7d16b985b0f25d00a22/6bNSM4WhVsEndIVx4Mb8U.png",
      "fullname": "Mellen Y. Pu",
      "name": "Mellen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.14990",
      "authors": [
        {
          "_id": "682eb25eb92a286a35d681fb",
          "name": "Ishika Agarwal",
          "hidden": false
        },
        {
          "_id": "682eb25eb92a286a35d681fc",
          "name": "Nimet Beyza Bozdag",
          "hidden": false
        },
        {
          "_id": "682eb25eb92a286a35d681fd",
          "name": "Dilek Hakkani-Tr",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T00:31:13.000Z",
      "submittedOnDailyAt": "2025-05-22T03:43:27.386Z",
      "title": "Language Specific Knowledge: Do Models Know Better in X than in English?",
      "submittedOnDailyBy": {
        "_id": "6391e4e984afa726d66180b9",
        "avatarUrl": "/avatars/e437e2820745b522a868b8da27d9a11f.svg",
        "isPro": false,
        "fullname": "Ishika Agarwal",
        "user": "ishikaa",
        "type": "user"
      },
      "summary": "Code-switching is a common phenomenon of alternating between different\nlanguages in the same utterance, thought, or conversation. We posit that humans\ncode-switch because they feel more comfortable talking about certain topics and\ndomains in one language than another. With the rise of knowledge-intensive\nlanguage models, we ask ourselves the next, natural question: Could models hold\nmore knowledge on some topics in some language X? More importantly, could we\nimprove reasoning by changing the language that reasoning is performed in? We\ncoin the term Language Specific Knowledge (LSK) to represent this phenomenon.\nAs ethnic cultures tend to develop alongside different languages, we employ\nculture-specific datasets (that contain knowledge about cultural and social\nbehavioral norms). We find that language models can perform better when using\nchain-of-thought reasoning in some languages other than English, sometimes even\nbetter in low-resource languages. Paired with previous works showing that\nsemantic similarity does not equate to representational similarity, we\nhypothesize that culturally specific texts occur more abundantly in\ncorresponding languages, enabling specific knowledge to occur only in specific\n\"expert\" languages. Motivated by our initial results, we design a simple\nmethodology called LSKExtractor to benchmark the language-specific knowledge\npresent in a language model and, then, exploit it during inference. We show our\nresults on various models and datasets, showing an average relative improvement\nof 10% in accuracy. Our research contributes to the open-source development of\nlanguage models that are inclusive and more aligned with the cultural and\nlinguistic contexts in which they are deployed.",
      "upvotes": 0,
      "discussionId": "682eb25fb92a286a35d6822d",
      "githubRepo": "https://github.com/agarwalishika/LSKExtractor",
      "ai_summary": "Models perform better in reasoning and accuracy when using language-specific knowledge and chain-of-thought reasoning in certain languages, including low-resource ones, compared to others.",
      "ai_keywords": [
        "code-switching",
        "Language Specific Knowledge (LSK)",
        "culture-specific datasets",
        "chain-of-thought reasoning",
        "culturally specific texts",
        "LSKExtractor"
      ]
    },
    "publishedAt": "2025-05-20T20:31:13.000Z",
    "title": "Language Specific Knowledge: Do Models Know Better in X than in English?",
    "summary": "Code-switching is a common phenomenon of alternating between different\nlanguages in the same utterance, thought, or conversation. We posit that humans\ncode-switch because they feel more comfortable talking about certain topics and\ndomains in one language than another. With the rise of knowledge-intensive\nlanguage models, we ask ourselves the next, natural question: Could models hold\nmore knowledge on some topics in some language X? More importantly, could we\nimprove reasoning by changing the language that reasoning is performed in? We\ncoin the term Language Specific Knowledge (LSK) to represent this phenomenon.\nAs ethnic cultures tend to develop alongside different languages, we employ\nculture-specific datasets (that contain knowledge about cultural and social\nbehavioral norms). We find that language models can perform better when using\nchain-of-thought reasoning in some languages other than English, sometimes even\nbetter in low-resource languages. Paired with previous works showing that\nsemantic similarity does not equate to representational similarity, we\nhypothesize that culturally specific texts occur more abundantly in\ncorresponding languages, enabling specific knowledge to occur only in specific\n\"expert\" languages. Motivated by our initial results, we design a simple\nmethodology called LSKExtractor to benchmark the language-specific knowledge\npresent in a language model and, then, exploit it during inference. We show our\nresults on various models and datasets, showing an average relative improvement\nof 10% in accuracy. Our research contributes to the open-source development of\nlanguage models that are inclusive and more aligned with the cultural and\nlinguistic contexts in which they are deployed.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14990.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6391e4e984afa726d66180b9",
      "avatarUrl": "/avatars/e437e2820745b522a868b8da27d9a11f.svg",
      "fullname": "Ishika Agarwal",
      "name": "ishikaa",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 0
    },
    "isAuthorParticipating": false
  }
]