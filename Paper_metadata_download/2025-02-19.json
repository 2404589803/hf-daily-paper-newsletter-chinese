[
  {
    "paper": {
      "id": "2502.12900",
      "authors": [
        {
          "_id": "67b54851b986e35c41e063da",
          "user": {
            "_id": "66975b9f8031bf92b428e138",
            "avatarUrl": "/avatars/3254281a7bac1c8ddde1d6bc7e518b2f.svg",
            "isPro": false,
            "fullname": "Yuhao Zhang",
            "user": "Yoohao",
            "type": "user"
          },
          "name": "Yuhao Zhang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-19T02:56:18.848Z",
          "hidden": false
        },
        {
          "_id": "67b54851b986e35c41e063db",
          "name": "Zhiheng Liu",
          "hidden": false
        },
        {
          "_id": "67b54851b986e35c41e063dc",
          "name": "Fan Bu",
          "hidden": false
        },
        {
          "_id": "67b54851b986e35c41e063dd",
          "name": "Ruiyu Zhang",
          "hidden": false
        },
        {
          "_id": "67b54851b986e35c41e063de",
          "name": "Benyou Wang",
          "hidden": false
        },
        {
          "_id": "67b54851b986e35c41e063df",
          "name": "Haizhou Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T14:36:39.000Z",
      "title": "Soundwave: Less is More for Speech-Text Alignment in LLMs",
      "summary": "Existing end-to-end speech large language models (LLMs) usually rely on\nlarge-scale annotated data for training, while data-efficient training has not\nbeen discussed in depth. We focus on two fundamental problems between speech\nand text: the representation space gap and sequence length inconsistency. We\npropose Soundwave, which utilizes an efficient training strategy and a novel\narchitecture to address these issues. Results show that Soundwave outperforms\nthe advanced Qwen2-Audio in speech translation and AIR-Bench speech tasks,\nusing only one-fiftieth of the training data. Further analysis shows that\nSoundwave still retains its intelligence during conversation. The project is\navailable at https://github.com/FreedomIntelligence/Soundwave.",
      "upvotes": 41,
      "discussionId": "67b54852b986e35c41e06426"
    },
    "publishedAt": "2025-02-19T00:22:36.628Z",
    "title": "Soundwave: Less is More for Speech-Text Alignment in LLMs",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12900.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66975b9f8031bf92b428e138",
      "avatarUrl": "/avatars/3254281a7bac1c8ddde1d6bc7e518b2f.svg",
      "fullname": "Yuhao Zhang",
      "name": "Yoohao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.11564",
      "authors": [
        {
          "_id": "67b40f93aba9e111862052ab",
          "user": {
            "_id": "65e5bd4568234ef5d6decadc",
            "avatarUrl": "/avatars/c41095a946c0176b949c0b3566136c05.svg",
            "isPro": false,
            "fullname": "Jaehyeong Jo",
            "user": "harryjo97",
            "type": "user"
          },
          "name": "Jaehyeong Jo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-18T09:31:27.544Z",
          "hidden": false
        },
        {
          "_id": "67b40f93aba9e111862052ac",
          "name": "Sung Ju Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-17T08:54:29.000Z",
      "title": "Continuous Diffusion Model for Language Modeling",
      "summary": "Diffusion models have emerged as a promising alternative to autoregressive\nmodels in modeling discrete categorical data. Yet diffusion models that\ndirectly work on discrete data space do not fully exploit the power of\niterative refinement, as the signals are lost during the transition between\ndiscrete states. Existing continuous diffusion models for discrete data have\nlimited performance compared to discrete approaches, and the unclear link\nbetween them restricts the development of diffusion models for discrete data.\nIn this work, we propose a continuous diffusion model for language modeling\nthat incorporates the geometry of the underlying categorical distribution. We\nestablish a connection between the discrete diffusion and continuous flow on\nthe statistical manifold, and building on the analogy, we introduce a simple\ndesign for the diffusion process that generalizes previous discrete diffusion\nmodels. We further propose a simulation-free training framework based on radial\nsymmetry and a simple technique to address the high dimensionality of the\nmanifold. Comprehensive experiments on language modeling benchmarks and other\nmodalities show that our method outperforms existing discrete diffusion models\nand approaches the performance of autoregressive models. Codes available at\nhttps://github.com/harryjo97/RDLM{https://github.com/harryjo97/RDLM}.",
      "upvotes": 27,
      "discussionId": "67b40f94aba9e111862052d5"
    },
    "publishedAt": "2025-02-18T22:43:02.567Z",
    "title": "Continuous Diffusion Model for Language Modeling",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11564.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "65e5bd4568234ef5d6decadc",
      "avatarUrl": "/avatars/c41095a946c0176b949c0b3566136c05.svg",
      "fullname": "Jaehyeong Jo",
      "name": "harryjo97",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.11079",
      "authors": [
        {
          "_id": "67b40141ad717fe02e188c1a",
          "user": {
            "_id": "63a950ac3453852ef5394178",
            "avatarUrl": "/avatars/48a5e537b10e2247a17e63502e3201a6.svg",
            "isPro": false,
            "fullname": "Lijie Liu",
            "user": "liulj13",
            "type": "user"
          },
          "name": "Lijie Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-18T09:31:42.570Z",
          "hidden": false
        },
        {
          "_id": "67b40141ad717fe02e188c1b",
          "name": "Tianxiang Ma",
          "hidden": false
        },
        {
          "_id": "67b40141ad717fe02e188c1c",
          "name": "Bingchuan Li",
          "hidden": false
        },
        {
          "_id": "67b40141ad717fe02e188c1d",
          "name": "Zhuowei Chen",
          "hidden": false
        },
        {
          "_id": "67b40141ad717fe02e188c1e",
          "name": "Jiawei Liu",
          "hidden": false
        },
        {
          "_id": "67b40141ad717fe02e188c1f",
          "name": "Qian He",
          "hidden": false
        },
        {
          "_id": "67b40141ad717fe02e188c20",
          "name": "Xinglong Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-16T11:02:50.000Z",
      "title": "Phantom: Subject-consistent video generation via cross-modal alignment",
      "summary": "The continuous development of foundational models for video generation is\nevolving into various applications, with subject-consistent video generation\nstill in the exploratory stage. We refer to this as Subject-to-Video, which\nextracts subject elements from reference images and generates\nsubject-consistent video through textual instructions. We believe that the\nessence of subject-to-video lies in balancing the dual-modal prompts of text\nand image, thereby deeply and simultaneously aligning both text and visual\ncontent. To this end, we propose Phantom, a unified video generation framework\nfor both single and multi-subject references. Building on existing\ntext-to-video and image-to-video architectures, we redesign the joint\ntext-image injection model and drive it to learn cross-modal alignment via\ntext-image-video triplet data. In particular, we emphasize subject consistency\nin human generation, covering existing ID-preserving video generation while\noffering enhanced advantages. The project homepage is here\nhttps://phantom-video.github.io/Phantom/.",
      "upvotes": 25,
      "discussionId": "67b40144ad717fe02e188cb2"
    },
    "publishedAt": "2025-02-18T21:56:39.407Z",
    "title": "Phantom: Subject-consistent video generation via cross-modal alignment",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63a950ac3453852ef5394178/HuVZ5d9xTlI4R1onRv_F5.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11079.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a950ac3453852ef5394178",
      "avatarUrl": "/avatars/48a5e537b10e2247a17e63502e3201a6.svg",
      "fullname": "Lijie Liu",
      "name": "liulj13",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.12464",
      "authors": [
        {
          "_id": "67b55b2cc92c4aa82c13562d",
          "name": "Seanie Lee",
          "hidden": false
        },
        {
          "_id": "67b55b2cc92c4aa82c13562e",
          "name": "Dong Bok Lee",
          "hidden": false
        },
        {
          "_id": "67b55b2cc92c4aa82c13562f",
          "name": "Dominik Wagner",
          "hidden": false
        },
        {
          "_id": "67b55b2cc92c4aa82c135630",
          "name": "Minki Kang",
          "hidden": false
        },
        {
          "_id": "67b55b2cc92c4aa82c135631",
          "name": "Haebin Seong",
          "hidden": false
        },
        {
          "_id": "67b55b2cc92c4aa82c135632",
          "name": "Tobias Bocklet",
          "hidden": false
        },
        {
          "_id": "67b55b2cc92c4aa82c135633",
          "name": "Juho Lee",
          "hidden": false
        },
        {
          "_id": "67b55b2cc92c4aa82c135634",
          "name": "Sung Ju Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T02:51:17.000Z",
      "title": "SafeRoute: Adaptive Model Selection for Efficient and Accurate Safety\n  Guardrails in Large Language Models",
      "summary": "Deploying large language models (LLMs) in real-world applications requires\nrobust safety guard models to detect and block harmful user prompts. While\nlarge safety guard models achieve strong performance, their computational cost\nis substantial. To mitigate this, smaller distilled models are used, but they\noften underperform on \"hard\" examples where the larger model provides accurate\npredictions. We observe that many inputs can be reliably handled by the smaller\nmodel, while only a small fraction require the larger model's capacity.\nMotivated by this, we propose SafeRoute, a binary router that distinguishes\nhard examples from easy ones. Our method selectively applies the larger safety\nguard model to the data that the router considers hard, improving efficiency\nwhile maintaining accuracy compared to solely using the larger safety guard\nmodel. Experimental results on multiple benchmark datasets demonstrate that our\nadaptive model selection significantly enhances the trade-off between\ncomputational cost and safety performance, outperforming relevant baselines.",
      "upvotes": 23,
      "discussionId": "67b55b2dc92c4aa82c13568b"
    },
    "publishedAt": "2025-02-18T23:23:34.214Z",
    "title": "SafeRoute: Adaptive Model Selection for Efficient and Accurate Safety Guardrails in Large Language Models",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64ad5f59b7e4b2c1ce47eb43/ZEq_vSLjsXuPX3O-TWIpE.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12464.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ad5f59b7e4b2c1ce47eb43",
      "avatarUrl": "/avatars/1f13ebe21a90d8c99920aa2c8cd9ac45.svg",
      "fullname": "Seanie Lee",
      "name": "Seanie-lee",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.13131",
      "authors": [
        {
          "_id": "67b5461d29cc269e5a4eb823",
          "name": "Feng Luo",
          "hidden": false
        },
        {
          "_id": "67b5461d29cc269e5a4eb824",
          "name": "Rui Yang",
          "hidden": false
        },
        {
          "_id": "67b5461d29cc269e5a4eb825",
          "name": "Hao Sun",
          "hidden": false
        },
        {
          "_id": "67b5461d29cc269e5a4eb826",
          "name": "Chunyuan Deng",
          "hidden": false
        },
        {
          "_id": "67b5461d29cc269e5a4eb827",
          "name": "Jiarui Yao",
          "hidden": false
        },
        {
          "_id": "67b5461d29cc269e5a4eb828",
          "name": "Jingyan Shen",
          "hidden": false
        },
        {
          "_id": "67b5461d29cc269e5a4eb829",
          "name": "Huan Zhang",
          "hidden": false
        },
        {
          "_id": "67b5461d29cc269e5a4eb82a",
          "name": "Hanjie Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T18:55:26.000Z",
      "title": "Rethinking Diverse Human Preference Learning through Principal Component\n  Analysis",
      "summary": "Understanding human preferences is crucial for improving foundation models\nand building personalized AI systems. However, preferences are inherently\ndiverse and complex, making it difficult for traditional reward models to\ncapture their full range. While fine-grained preference data can help,\ncollecting it is expensive and hard to scale. In this paper, we introduce\nDecomposed Reward Models (DRMs), a novel approach that extracts diverse human\npreferences from binary comparisons without requiring fine-grained annotations.\nOur key insight is to represent human preferences as vectors and analyze them\nusing Principal Component Analysis (PCA). By constructing a dataset of\nembedding differences between preferred and rejected responses, DRMs identify\northogonal basis vectors that capture distinct aspects of preference. These\ndecomposed rewards can be flexibly combined to align with different user needs,\noffering an interpretable and scalable alternative to traditional reward\nmodels. We demonstrate that DRMs effectively extract meaningful preference\ndimensions (e.g., helpfulness, safety, humor) and adapt to new users without\nadditional training. Our results highlight DRMs as a powerful framework for\npersonalized and interpretable LLM alignment.",
      "upvotes": 23,
      "discussionId": "67b5461f29cc269e5a4eb8bc"
    },
    "publishedAt": "2025-02-18T21:59:45.466Z",
    "title": "Rethinking Diverse Human Preference Learning through Principal Component Analysis",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13131.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64d45451c34a346181b130dd",
      "avatarUrl": "/avatars/9bb8205b889337df5d321539c9b5d69d.svg",
      "fullname": "Rui Yang",
      "name": "Ray2333",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.13143",
      "authors": [
        {
          "_id": "67b546c0d8a1eac02c605f6a",
          "name": "Zekun Qi",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f6b",
          "name": "Wenyao Zhang",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f6c",
          "name": "Yufei Ding",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f6d",
          "name": "Runpei Dong",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f6e",
          "name": "Xinqiang Yu",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f6f",
          "name": "Jingwen Li",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f70",
          "name": "Lingyun Xu",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f71",
          "name": "Baoyu Li",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f72",
          "name": "Xialin He",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f73",
          "name": "Guofan Fan",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f74",
          "name": "Jiazhao Zhang",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f75",
          "name": "Jiawei He",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f76",
          "name": "Jiayuan Gu",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f77",
          "name": "Xin Jin",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f78",
          "name": "Kaisheng Ma",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f79",
          "name": "Zhizheng Zhang",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f7a",
          "name": "He Wang",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f7b",
          "name": "Li Yi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T18:59:02.000Z",
      "title": "SoFar: Language-Grounded Orientation Bridges Spatial Reasoning and\n  Object Manipulation",
      "summary": "Spatial intelligence is a critical component of embodied AI, promoting robots\nto understand and interact with their environments. While recent advances have\nenhanced the ability of VLMs to perceive object locations and positional\nrelationships, they still lack the capability to precisely understand object\norientations-a key requirement for tasks involving fine-grained manipulations.\nAddressing this limitation not only requires geometric reasoning but also an\nexpressive and intuitive way to represent orientation. In this context, we\npropose that natural language offers a more flexible representation space than\ncanonical frames, making it particularly suitable for instruction-following\nrobotic systems. In this paper, we introduce the concept of semantic\norientation, which defines object orientations using natural language in a\nreference-frame-free manner (e.g., the ''plug-in'' direction of a USB or the\n''handle'' direction of a knife). To support this, we construct OrienText300K,\na large-scale dataset of 3D models annotated with semantic orientations that\nlink geometric understanding to functional semantics. By integrating semantic\norientation into a VLM system, we enable robots to generate manipulation\nactions with both positional and orientational constraints. Extensive\nexperiments in simulation and real world demonstrate that our approach\nsignificantly enhances robotic manipulation capabilities, e.g., 48.7% accuracy\non Open6DOR and 74.9% accuracy on SIMPLER.",
      "upvotes": 21,
      "discussionId": "67b546c5d8a1eac02c606090"
    },
    "publishedAt": "2025-02-18T21:51:33.957Z",
    "title": "SoFar: Language-Grounded Orientation Bridges Spatial Reasoning and Object Manipulation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13143.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63c3e8abc7d7f4c63a515a02",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c3e8abc7d7f4c63a515a02/npMHnVP2hHLbvoUGe7C4O.jpeg",
      "fullname": "Zekun Qi",
      "name": "qizekun",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.13145",
      "authors": [
        {
          "_id": "67b54b04bd51b4e46e39d287",
          "name": "Bencheng Liao",
          "hidden": false
        },
        {
          "_id": "67b54b04bd51b4e46e39d288",
          "name": "Hongyuan Tao",
          "hidden": false
        },
        {
          "_id": "67b54b04bd51b4e46e39d289",
          "name": "Qian Zhang",
          "hidden": false
        },
        {
          "_id": "67b54b04bd51b4e46e39d28a",
          "name": "Tianheng Cheng",
          "hidden": false
        },
        {
          "_id": "67b54b04bd51b4e46e39d28b",
          "name": "Yingyue Li",
          "hidden": false
        },
        {
          "_id": "67b54b04bd51b4e46e39d28c",
          "name": "Haoran Yin",
          "hidden": false
        },
        {
          "_id": "67b54b04bd51b4e46e39d28d",
          "name": "Wenyu Liu",
          "hidden": false
        },
        {
          "_id": "67b54b04bd51b4e46e39d28e",
          "name": "Xinggang Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T18:59:57.000Z",
      "title": "Multimodal Mamba: Decoder-only Multimodal State Space Model via\n  Quadratic to Linear Distillation",
      "summary": "Recent Multimodal Large Language Models (MLLMs) have achieved remarkable\nperformance but face deployment challenges due to their quadratic computational\ncomplexity, growing Key-Value cache requirements, and reliance on separate\nvision encoders. We propose mmMamba, a framework for developing\nlinear-complexity native multimodal state space models through progressive\ndistillation from existing MLLMs using moderate academic computational\nresources. Our approach enables the direct conversion of trained decoder-only\nMLLMs to linear-complexity architectures without requiring pre-trained\nRNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba\nfrom trained Transformer and a three-stage distillation recipe, which can\neffectively transfer the knowledge from Transformer to Mamba while preserving\nmultimodal capabilities. Our method also supports flexible hybrid architectures\nthat combine Transformer and Mamba layers for customizable\nefficiency-performance trade-offs. Distilled from the Transformer-based\ndecoder-only HoVLE, mmMamba-linear achieves competitive performance against\nexisting linear and quadratic-complexity VLMs, while mmMamba-hybrid further\nimproves performance significantly, approaching HoVLE's capabilities. At 103K\ntokens, mmMamba-linear demonstrates 20.6times speedup and 75.8% GPU memory\nreduction compared to HoVLE, while mmMamba-hybrid achieves 13.5times speedup\nand 60.2% memory savings. Code and models are released at\nhttps://github.com/hustvl/mmMamba",
      "upvotes": 17,
      "discussionId": "67b54b05bd51b4e46e39d2bb"
    },
    "publishedAt": "2025-02-18T22:08:27.750Z",
    "title": "Multimodal Mamba: Decoder-only Multimodal State Space Model via Quadratic to Linear Distillation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13145.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6577073fc2bf55b1f6bafb49",
      "avatarUrl": "/avatars/58803398b1a918b7570db17893e65122.svg",
      "fullname": "liao",
      "name": "LegendBC",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.11433",
      "authors": [
        {
          "_id": "67b54a644508bd0617598c21",
          "name": "Guojun Xiong",
          "hidden": false
        },
        {
          "_id": "67b54a644508bd0617598c22",
          "name": "Zhiyang Deng",
          "hidden": false
        },
        {
          "_id": "67b54a644508bd0617598c23",
          "name": "Keyi Wang",
          "hidden": false
        },
        {
          "_id": "67b54a644508bd0617598c24",
          "name": "Yupeng Cao",
          "hidden": false
        },
        {
          "_id": "67b54a644508bd0617598c25",
          "name": "Haohang Li",
          "hidden": false
        },
        {
          "_id": "67b54a644508bd0617598c26",
          "name": "Yangyang Yu",
          "hidden": false
        },
        {
          "_id": "67b54a644508bd0617598c27",
          "name": "Xueqing Peng",
          "hidden": false
        },
        {
          "_id": "67b54a644508bd0617598c28",
          "name": "Mingquan Lin",
          "hidden": false
        },
        {
          "_id": "67b54a644508bd0617598c29",
          "name": "Kaleb E Smith",
          "hidden": false
        },
        {
          "_id": "67b54a644508bd0617598c2a",
          "name": "Xiao-Yang Liu",
          "hidden": false
        },
        {
          "_id": "67b54a644508bd0617598c2b",
          "name": "Jimin Huang",
          "hidden": false
        },
        {
          "_id": "67b54a644508bd0617598c2c",
          "name": "Sophia Ananiadou",
          "hidden": false
        },
        {
          "_id": "67b54a644508bd0617598c2d",
          "name": "Qianqian Xie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-17T04:45:53.000Z",
      "title": "FLAG-Trader: Fusion LLM-Agent with Gradient-based Reinforcement Learning\n  for Financial Trading",
      "summary": "Large language models (LLMs) fine-tuned on multimodal financial data have\ndemonstrated impressive reasoning capabilities in various financial tasks.\nHowever, they often struggle with multi-step, goal-oriented scenarios in\ninteractive financial markets, such as trading, where complex agentic\napproaches are required to improve decision-making. To address this, we propose\nFLAG-Trader, a unified architecture integrating linguistic processing\n(via LLMs) with gradient-driven reinforcement learning (RL) policy\noptimization, in which a partially fine-tuned LLM acts as the policy network,\nleveraging pre-trained knowledge while adapting to the financial domain through\nparameter-efficient fine-tuning. Through policy gradient optimization driven by\ntrading rewards, our framework not only enhances LLM performance in trading but\nalso improves results on other financial-domain tasks. We present extensive\nempirical evidence to validate these enhancements.",
      "upvotes": 16,
      "discussionId": "67b54a654508bd0617598c7e"
    },
    "publishedAt": "2025-02-18T22:06:19.200Z",
    "title": "FLAG-Trader: Fusion LLM-Agent with Gradient-based Reinforcement Learning for Financial Trading",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63b58ed5889aa6707f0bb0f4/2C9mhT-1Qz14hik7sxjf2.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11433.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63b58ed5889aa6707f0bb0f4",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b58ed5889aa6707f0bb0f4/9-6SJBOLdqUoc2LrKsI6y.jpeg",
      "fullname": "Jimin Huang",
      "name": "jiminHuang",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.12513",
      "authors": [
        {
          "_id": "67b545fd88527668fa8bcc14",
          "name": "Tiancheng Gu",
          "hidden": false
        },
        {
          "_id": "67b545fd88527668fa8bcc15",
          "name": "Kaicheng Yang",
          "hidden": false
        },
        {
          "_id": "67b545fd88527668fa8bcc16",
          "name": "Chaoyi Zhang",
          "hidden": false
        },
        {
          "_id": "67b545fd88527668fa8bcc17",
          "name": "Yin Xie",
          "hidden": false
        },
        {
          "_id": "67b545fd88527668fa8bcc18",
          "name": "Xiang An",
          "hidden": false
        },
        {
          "_id": "67b545fd88527668fa8bcc19",
          "name": "Ziyong Feng",
          "hidden": false
        },
        {
          "_id": "67b545fd88527668fa8bcc1a",
          "name": "Dongnan Liu",
          "hidden": false
        },
        {
          "_id": "67b545fd88527668fa8bcc1b",
          "name": "Weidong Cai",
          "hidden": false
        },
        {
          "_id": "67b545fd88527668fa8bcc1c",
          "name": "Jiankang Deng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T03:58:38.000Z",
      "title": "RealSyn: An Effective and Scalable Multimodal Interleaved Document\n  Transformation Paradigm",
      "summary": "After pre-training on extensive image-text pairs, Contrastive Language-Image\nPre-training (CLIP) demonstrates promising performance on a wide variety of\nbenchmarks. However, a substantial volume of non-paired data, such as\nmultimodal interleaved documents, remains underutilized for vision-language\nrepresentation learning. To fully leverage these unpaired documents, we\ninitially establish a Real-World Data Extraction pipeline to extract\nhigh-quality images and texts. Then we design a hierarchical retrieval method\nto efficiently associate each image with multiple semantically relevant\nrealistic texts. To further enhance fine-grained visual information, we propose\nan image semantic augmented generation module for synthetic text production.\nFurthermore, we employ a semantic balance sampling strategy to improve dataset\ndiversity, enabling better learning of long-tail concepts. Based on these\ninnovations, we construct RealSyn, a dataset combining realistic and synthetic\ntexts, available in three scales: 15M, 30M, and 100M. Extensive experiments\ndemonstrate that RealSyn effectively advances vision-language representation\nlearning and exhibits strong scalability. Models pre-trained on RealSyn achieve\nstate-of-the-art performance on multiple downstream tasks. To facilitate future\nresearch, the RealSyn dataset and pre-trained model weights are released at\nhttps://github.com/deepglint/RealSyn.",
      "upvotes": 10,
      "discussionId": "67b545fe88527668fa8bcc65"
    },
    "publishedAt": "2025-02-18T21:52:22.326Z",
    "title": "RealSyn: An Effective and Scalable Multimodal Interleaved Document Transformation Paradigm",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12513.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63e202f352b7578dba448ab5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e202f352b7578dba448ab5/8itVBLcv14m7OVsoF8h1o.jpeg",
      "fullname": "Yang",
      "name": "Kaichengalex",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.12859",
      "authors": [
        {
          "_id": "67b576aa489d68b981e086ad",
          "name": "Chenxing Wei",
          "hidden": false
        },
        {
          "_id": "67b576aa489d68b981e086ae",
          "name": "Yao Shu",
          "hidden": false
        },
        {
          "_id": "67b576aa489d68b981e086af",
          "name": "Mingwen Ou",
          "hidden": false
        },
        {
          "_id": "67b576aa489d68b981e086b0",
          "name": "Ying Tiffany He",
          "hidden": false
        },
        {
          "_id": "67b576aa489d68b981e086b1",
          "name": "Fei Richard Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T13:46:47.000Z",
      "title": "PAFT: Prompt-Agnostic Fine-Tuning",
      "summary": "While Large Language Models (LLMs) adapt well to downstream tasks after\nfine-tuning, this adaptability often compromises prompt robustness, as even\nminor prompt variations can significantly degrade performance. To address this,\nwe propose Prompt-Agnostic Fine-Tuning(PAFT), a simple yet effective approach\nthat dynamically adjusts prompts during fine-tuning. This encourages the model\nto learn underlying task principles rather than overfitting to specific prompt\nformulations. PAFT operates in two stages: First, a diverse set of meaningful,\nsynthetic candidate prompts is constructed. Second, during fine-tuning, prompts\nare randomly sampled from this set to create dynamic training inputs. Extensive\nexperiments across diverse datasets and LLMs demonstrate that models trained\nwith PAFT exhibit strong robustness and generalization across a wide range of\nprompts, including unseen ones. This enhanced robustness improves both model\nperformance and inference speed while maintaining training efficiency. Ablation\nstudies further confirm the effectiveness of PAFT.",
      "upvotes": 8,
      "discussionId": "67b576aa489d68b981e08708"
    },
    "publishedAt": "2025-02-19T01:21:54.836Z",
    "title": "PAFT: Prompt-Agnostic Fine-Tuning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12859.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65ed3051492a7f35db21fea2",
      "avatarUrl": "/avatars/4fc0ccc21aa88e4e8ff74a6f850570b8.svg",
      "fullname": "Chenxing Wei",
      "name": "kittttttt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.13130",
      "authors": [
        {
          "_id": "67b5625fb27eb6046b2ceec5",
          "name": "Jianwei Yang",
          "hidden": false
        },
        {
          "_id": "67b5625fb27eb6046b2ceec6",
          "name": "Reuben Tan",
          "hidden": false
        },
        {
          "_id": "67b5625fb27eb6046b2ceec7",
          "name": "Qianhui Wu",
          "hidden": false
        },
        {
          "_id": "67b5625fb27eb6046b2ceec8",
          "name": "Ruijie Zheng",
          "hidden": false
        },
        {
          "_id": "67b5625fb27eb6046b2ceec9",
          "name": "Baolin Peng",
          "hidden": false
        },
        {
          "_id": "67b5625fb27eb6046b2ceeca",
          "name": "Yongyuan Liang",
          "hidden": false
        },
        {
          "_id": "67b5625fb27eb6046b2ceecb",
          "name": "Yu Gu",
          "hidden": false
        },
        {
          "_id": "67b5625fb27eb6046b2ceecc",
          "name": "Mu Cai",
          "hidden": false
        },
        {
          "_id": "67b5625fb27eb6046b2ceecd",
          "name": "Seonghyeon Ye",
          "hidden": false
        },
        {
          "_id": "67b5625fb27eb6046b2ceece",
          "name": "Joel Jang",
          "hidden": false
        },
        {
          "_id": "67b5625fb27eb6046b2ceecf",
          "name": "Yuquan Deng",
          "hidden": false
        },
        {
          "_id": "67b5625fb27eb6046b2ceed0",
          "name": "Lars Liden",
          "hidden": false
        },
        {
          "_id": "67b5625fb27eb6046b2ceed1",
          "name": "Jianfeng Gao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T18:55:21.000Z",
      "title": "Magma: A Foundation Model for Multimodal AI Agents",
      "summary": "We present Magma, a foundation model that serves multimodal AI agentic tasks\nin both the digital and physical worlds. Magma is a significant extension of\nvision-language (VL) models in that it not only retains the VL understanding\nability (verbal intelligence) of the latter, but is also equipped with the\nability to plan and act in the visual-spatial world (spatial-temporal\nintelligence) and complete agentic tasks ranging from UI navigation to robot\nmanipulation. To endow the agentic capabilities, Magma is pretrained on large\namounts of heterogeneous datasets spanning from images, videos to robotics\ndata, where the actionable visual objects (e.g., clickable buttons in GUI) in\nimages are labeled by Set-of-Mark (SoM) for action grounding, and the object\nmovements (e.g., the trace of human hands or robotic arms) in videos are\nlabeled by Trace-of-Mark (ToM) for action planning. Extensive experiments show\nthat SoM and ToM reach great synergy and facilitate the acquisition of\nspatial-temporal intelligence for our Magma model, which is fundamental to a\nwide range of tasks as shown in Fig.1. In particular, Magma creates new\nstate-of-the-art results on UI navigation and robotic manipulation tasks,\noutperforming previous models that are specifically tailored to these tasks. On\nimage and video-related multimodal tasks, Magma also compares favorably to\npopular large multimodal models that are trained on much larger datasets. We\nmake our model and code public for reproducibility at\nhttps://microsoft.github.io/Magma.",
      "upvotes": 8,
      "discussionId": "67b56265b27eb6046b2cf08f"
    },
    "publishedAt": "2025-02-18T23:51:36.910Z",
    "title": "Magma: A Foundation Model for Multimodal AI Agents",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13130.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6141
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.12170",
      "authors": [
        {
          "_id": "67b5434f2b2ec6908fffe75e",
          "name": "Da Xiao",
          "hidden": false
        },
        {
          "_id": "67b5434f2b2ec6908fffe75f",
          "name": "Qingye Meng",
          "hidden": false
        },
        {
          "_id": "67b5434f2b2ec6908fffe760",
          "name": "Shengping Li",
          "hidden": false
        },
        {
          "_id": "67b5434f2b2ec6908fffe761",
          "name": "Xingyuan Yuan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-13T10:26:27.000Z",
      "title": "MUDDFormer: Breaking Residual Bottlenecks in Transformers via Multiway\n  Dynamic Dense Connections",
      "summary": "We propose MUltiway Dynamic Dense (MUDD) connections, a simple yet effective\nmethod to address the limitations of residual connections and enhance\ncross-layer information flow in Transformers. Unlike existing dense connection\napproaches with static and shared connection weights, MUDD generates connection\nweights dynamically depending on hidden states at each sequence position and\nfor each decoupled input stream (the query, key, value or residual) of a\nTransformer block. MUDD connections can be seamlessly integrated into any\nTransformer architecture to create MUDDFormer. Extensive experiments show that\nMUDDFormer significantly outperforms Transformers across various model\narchitectures and scales in language modeling, achieving the performance of\nTransformers trained with 1.8X-2.4X compute. Notably, MUDDPythia-2.8B matches\nPythia-6.9B in pretraining ppl and downstream tasks and even rivals Pythia-12B\nin five-shot settings, while adding only 0.23% parameters and 0.4% computation.\nCode in JAX and PyTorch and pre-trained models are available at\nhttps://github.com/Caiyun-AI/MUDDFormer .",
      "upvotes": 6,
      "discussionId": "67b543502b2ec6908fffe788"
    },
    "publishedAt": "2025-02-18T22:59:16.530Z",
    "title": "MUDDFormer: Breaking Residual Bottlenecks in Transformers via Multiway Dynamic Dense Connections",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12170.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62d77440bad37ef354028365",
      "avatarUrl": "/avatars/df0dea879e06fa814867e9aad03d1e68.svg",
      "fullname": "Da Xiao",
      "name": "xiaoda99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.12215",
      "authors": [
        {
          "_id": "67b56007fa141a55e51d9d78",
          "name": "Zhiyuan Zeng",
          "hidden": false
        },
        {
          "_id": "67b56007fa141a55e51d9d79",
          "name": "Qinyuan Cheng",
          "hidden": false
        },
        {
          "_id": "67b56007fa141a55e51d9d7a",
          "name": "Zhangyue Yin",
          "hidden": false
        },
        {
          "_id": "67b56007fa141a55e51d9d7b",
          "name": "Yunhua Zhou",
          "hidden": false
        },
        {
          "_id": "67b56007fa141a55e51d9d7c",
          "name": "Xipeng Qiu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-17T07:21:11.000Z",
      "title": "Revisiting the Test-Time Scaling of o1-like Models: Do they Truly\n  Possess Test-Time Scaling Capabilities?",
      "summary": "The advent of test-time scaling in large language models (LLMs), exemplified\nby OpenAI's o1 series, has advanced reasoning capabilities by scaling\ncomputational resource allocation during inference. While successors like QwQ,\nDeepseek-R1 (R1) and LIMO replicate these advancements, whether these models\ntruly possess test-time scaling capabilities remains underexplored. This study\nfound that longer CoTs of these o1-like models do not consistently enhance\naccuracy; in fact, correct solutions are often shorter than incorrect ones for\nthe same questions. Further investigation shows this phenomenon is closely\nrelated to models' self-revision capabilities - longer CoTs contain more\nself-revisions, which often lead to performance degradation. We then compare\nsequential and parallel scaling strategies on QwQ, R1 and LIMO, finding that\nparallel scaling achieves better coverage and scalability. Based on these\ninsights, we propose Shortest Majority Vote, a method that combines parallel\nscaling strategies with CoT length characteristics, significantly improving\nmodels' test-time scalability compared to conventional majority voting\napproaches.",
      "upvotes": 5,
      "discussionId": "67b56007fa141a55e51d9da7"
    },
    "publishedAt": "2025-02-18T23:37:46.756Z",
    "title": "Revisiting the Test-Time Scaling of o1-like Models: Do they Truly Possess Test-Time Scaling Capabilities?",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12215.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6141
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.12501",
      "authors": [
        {
          "_id": "67b547ffc9071a3e97139532",
          "name": "Qiyuan Zhang",
          "hidden": false
        },
        {
          "_id": "67b547ffc9071a3e97139533",
          "name": "Yufei Wang",
          "hidden": false
        },
        {
          "_id": "67b547ffc9071a3e97139534",
          "name": "Yuxin Jiang",
          "hidden": false
        },
        {
          "_id": "67b547ffc9071a3e97139535",
          "name": "Liangyou Li",
          "hidden": false
        },
        {
          "_id": "67b547ffc9071a3e97139536",
          "name": "Chuhan Wu",
          "hidden": false
        },
        {
          "_id": "67b547ffc9071a3e97139537",
          "name": "Yasheng Wang",
          "hidden": false
        },
        {
          "_id": "67b547ffc9071a3e97139538",
          "name": "Xin Jiang",
          "hidden": false
        },
        {
          "_id": "67b547ffc9071a3e97139539",
          "name": "Lifeng Shang",
          "hidden": false
        },
        {
          "_id": "67b547ffc9071a3e9713953a",
          "name": "Ruiming Tang",
          "hidden": false
        },
        {
          "_id": "67b547ffc9071a3e9713953b",
          "name": "Fuyuan Lyu",
          "hidden": false
        },
        {
          "_id": "67b547ffc9071a3e9713953c",
          "name": "Chen Ma",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T03:31:06.000Z",
      "title": "Crowd Comparative Reasoning: Unlocking Comprehensive Evaluations for\n  LLM-as-a-Judge",
      "summary": "LLM-as-a-Judge, which generates chain-of-thought (CoT) judgments, has become\na widely adopted auto-evaluation method. However, its reliability is\ncompromised by the CoT reasoning's inability to capture comprehensive and\ndeeper details, often leading to incomplete outcomes. Existing methods mainly\nrely on majority voting or criteria expansion, which is insufficient to address\nthe limitation in CoT. We propose Crowd-based Comparative Evaluation, which\nintroduces additional crowd responses to compare with the candidate responses,\nthereby exposing deeper and more comprehensive details within the candidate\nresponses. This process effectively guides LLM-as-a-Judge to provide a more\ndetailed CoT judgment. Extensive experiments demonstrate that our approach\nenhances evaluation reliability, achieving an average accuracy gain of 6.7%\nacross five benchmarks. Moreover, our method produces higher-quality CoTs that\nfacilitate judge distillation and exhibit superior performance in rejection\nsampling for supervised fine-tuning (SFT), referred to as crowd rejection\nsampling, thereby enabling more efficient SFT. Our analysis confirms that CoTs\ngenerated by ours are more comprehensive and of higher quality, and evaluation\naccuracy improves as inference scales.",
      "upvotes": 5,
      "discussionId": "67b54800c9071a3e9713956c"
    },
    "publishedAt": "2025-02-18T21:55:26.822Z",
    "title": "Crowd Comparative Reasoning: Unlocking Comprehensive Evaluations for LLM-as-a-Judge",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12501.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62a42f22c683d02f5b63320c",
      "avatarUrl": "/avatars/bc611abe9c4ef8d378123cb8ac9fdbf2.svg",
      "fullname": "Qiyuan Zhang",
      "name": "DonJoey",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.09838",
      "authors": [
        {
          "_id": "67b55078a64445f58c771d84",
          "name": "Tianwei Lin",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d85",
          "name": "Wenqiao Zhang",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d86",
          "name": "Sijing Li",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d87",
          "name": "Yuqian Yuan",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d88",
          "name": "Binhe Yu",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d89",
          "name": "Haoyuan Li",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d8a",
          "name": "Wanggui He",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d8b",
          "name": "Hao Jiang",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d8c",
          "name": "Mengze Li",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d8d",
          "name": "Xiaohui Song",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d8e",
          "name": "Siliang Tang",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d8f",
          "name": "Jun Xiao",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d90",
          "name": "Hui Lin",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d91",
          "name": "Yueting Zhuang",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d92",
          "name": "Beng Chin Ooi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-14T00:42:36.000Z",
      "title": "HealthGPT: A Medical Large Vision-Language Model for Unifying\n  Comprehension and Generation via Heterogeneous Knowledge Adaptation",
      "summary": "We present HealthGPT, a powerful Medical Large Vision-Language Model\n(Med-LVLM) that integrates medical visual comprehension and generation\ncapabilities within a unified autoregressive paradigm. Our bootstrapping\nphilosophy is to progressively adapt heterogeneous comprehension and generation\nknowledge to pre-trained large language models (LLMs). This is achieved through\na novel heterogeneous low-rank adaptation (H-LoRA) technique, which is\ncomplemented by a tailored hierarchical visual perception approach and a\nthree-stage learning strategy. To effectively learn the HealthGPT, we devise a\ncomprehensive medical domain-specific comprehension and generation dataset\ncalled VL-Health. Experimental results demonstrate exceptional performance and\nscalability of HealthGPT in medical visual unified tasks. Our project can be\naccessed at https://github.com/DCDmllm/HealthGPT.",
      "upvotes": 4,
      "discussionId": "67b5507aa64445f58c771df9"
    },
    "publishedAt": "2025-02-18T22:35:23.066Z",
    "title": "HealthGPT: A Medical Large Vision-Language Model for Unifying Comprehension and Generation via Heterogeneous Knowledge Adaptation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09838.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65fc18edfb66882aba4d548e",
      "avatarUrl": "/avatars/f70d47fe4aba98b5a5cd64f7e002dfd2.svg",
      "fullname": "wenqiao",
      "name": "wannature",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.11271",
      "authors": [
        {
          "_id": "67b4322c217ec18a40587bec",
          "name": "Pan Lu",
          "hidden": false
        },
        {
          "_id": "67b4322c217ec18a40587bed",
          "name": "Bowen Chen",
          "hidden": false
        },
        {
          "_id": "67b4322c217ec18a40587bee",
          "name": "Sheng Liu",
          "hidden": false
        },
        {
          "_id": "67b4322c217ec18a40587bef",
          "name": "Rahul Thapa",
          "hidden": false
        },
        {
          "_id": "67b4322c217ec18a40587bf0",
          "name": "Joseph Boen",
          "hidden": false
        },
        {
          "_id": "67b4322c217ec18a40587bf1",
          "name": "James Zou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-16T21:18:47.000Z",
      "title": "OctoTools: An Agentic Framework with Extensible Tools for Complex\n  Reasoning",
      "summary": "Solving complex reasoning tasks may involve visual understanding, domain\nknowledge retrieval, numerical calculation, and multi-step reasoning. Existing\nmethods augment large language models (LLMs) with external tools but are\nrestricted to specialized domains, limited tool types, or require additional\ntraining data. In this paper, we introduce OctoTools, a training-free,\nuser-friendly, and easily extensible open-source agentic framework designed to\ntackle complex reasoning across diverse domains. OctoTools introduces\nstandardized tool cards to encapsulate tool functionality, a planner for both\nhigh-level and low-level planning, and an executor to carry out tool usage. We\nvalidate OctoTools' generality across 16 diverse tasks (including MathVista,\nMMLU-Pro, MedQA, and GAIA-Text), achieving substantial average accuracy gains\nof 9.3% over GPT-4o. Furthermore, OctoTools outperforms AutoGen, GPT-Functions\nand LangChain by up to 10.6% when given the same set of tools. Through\ncomprehensive analysis and ablations, OctoTools demonstrates advantages in task\nplanning, effective tool usage, and multi-step problem solving.",
      "upvotes": 3,
      "discussionId": "67b4322d217ec18a40587c27"
    },
    "publishedAt": "2025-02-19T02:27:36.940Z",
    "title": "OctoTools: An Agentic Framework with Extensible Tools for Complex Reasoning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11271.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f5f68fa7fd83d025749234",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60f5f68fa7fd83d025749234/gCeJAZfzaANAcEvI6v5-P.jpeg",
      "fullname": "Pan Lu",
      "name": "lupantech",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.12574",
      "authors": [
        {
          "_id": "67b547f555d0424a31b9c384",
          "name": "Cheng Luo",
          "hidden": false
        },
        {
          "_id": "67b547f555d0424a31b9c385",
          "name": "Zefan Cai",
          "hidden": false
        },
        {
          "_id": "67b547f555d0424a31b9c386",
          "name": "Hanshi Sun",
          "hidden": false
        },
        {
          "_id": "67b547f555d0424a31b9c387",
          "name": "Jinqi Xiao",
          "hidden": false
        },
        {
          "_id": "67b547f555d0424a31b9c388",
          "name": "Bo Yuan",
          "hidden": false
        },
        {
          "_id": "67b547f555d0424a31b9c389",
          "name": "Wen Xiao",
          "hidden": false
        },
        {
          "_id": "67b547f555d0424a31b9c38a",
          "name": "Junjie Hu",
          "hidden": false
        },
        {
          "_id": "67b547f555d0424a31b9c38b",
          "name": "Jiawei Zhao",
          "hidden": false
        },
        {
          "_id": "67b547f555d0424a31b9c38c",
          "name": "Beidi Chen",
          "hidden": false
        },
        {
          "_id": "67b547f555d0424a31b9c38d",
          "name": "Anima Anandkumar",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T06:26:05.000Z",
      "title": "HeadInfer: Memory-Efficient LLM Inference by Head-wise Offloading",
      "summary": "Transformer-based large language models (LLMs) demonstrate impressive\nperformance in long context generation. Extending the context length has\ndisproportionately shifted the memory footprint of LLMs during inference to the\nkey-value cache (KV cache). In this paper, we propose HEADINFER, which offloads\nthe KV cache to CPU RAM while avoiding the need to fully store the KV cache for\nany transformer layer on the GPU. HEADINFER employs a fine-grained, head-wise\noffloading strategy, maintaining only selective attention heads KV cache on the\nGPU while computing attention output dynamically. Through roofline analysis, we\ndemonstrate that HEADINFER maintains computational efficiency while\nsignificantly reducing memory footprint. We evaluate HEADINFER on the\nLlama-3-8B model with a 1-million-token sequence, reducing the GPU memory\nfootprint of the KV cache from 128 GB to 1 GB and the total GPU memory usage\nfrom 207 GB to 17 GB, achieving a 92% reduction compared to BF16 baseline\ninference. Notably, HEADINFER enables 4-million-token inference with an 8B\nmodel on a single consumer GPU with 24GB memory (e.g., NVIDIA RTX 4090) without\napproximation methods.",
      "upvotes": 2,
      "discussionId": "67b547f755d0424a31b9c3e5"
    },
    "publishedAt": "2025-02-18T21:57:00.289Z",
    "title": "HeadInfer: Memory-Efficient LLM Inference by Head-wise Offloading",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12574.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64cb48f7667f4f808535107e",
      "avatarUrl": "/avatars/8f77f378ad665b246e1ea3aaba2153ae.svg",
      "fullname": "chengluo",
      "name": "wdlctc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.12669",
      "authors": [
        {
          "_id": "67b58c806e53744c2a373351",
          "name": "Xiang Liu",
          "hidden": false
        },
        {
          "_id": "67b58c806e53744c2a373352",
          "name": "Penglei Sun",
          "hidden": false
        },
        {
          "_id": "67b58c806e53744c2a373353",
          "name": "Shuyan Chen",
          "hidden": false
        },
        {
          "_id": "67b58c806e53744c2a373354",
          "name": "Longhan Zhang",
          "hidden": false
        },
        {
          "_id": "67b58c806e53744c2a373355",
          "name": "Peijie Dong",
          "hidden": false
        },
        {
          "_id": "67b58c806e53744c2a373356",
          "name": "Huajie You",
          "hidden": false
        },
        {
          "_id": "67b58c806e53744c2a373357",
          "name": "Yongqi Zhang",
          "hidden": false
        },
        {
          "_id": "67b58c806e53744c2a373358",
          "name": "Chang Yan",
          "hidden": false
        },
        {
          "_id": "67b58c806e53744c2a373359",
          "name": "Xiaowen Chu",
          "hidden": false
        },
        {
          "_id": "67b58c806e53744c2a37335a",
          "name": "Tong-yi Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T09:19:24.000Z",
      "title": "Perovskite-LLM: Knowledge-Enhanced Large Language Models for Perovskite\n  Solar Cell Research",
      "summary": "The rapid advancement of perovskite solar cells (PSCs) has led to an\nexponential growth in research publications, creating an urgent need for\nefficient knowledge management and reasoning systems in this domain. We present\na comprehensive knowledge-enhanced system for PSCs that integrates three key\ncomponents. First, we develop Perovskite-KG, a domain-specific knowledge graph\nconstructed from 1,517 research papers, containing 23,789 entities and 22,272\nrelationships. Second, we create two complementary datasets: Perovskite-Chat,\ncomprising 55,101 high-quality question-answer pairs generated through a novel\nmulti-agent framework, and Perovskite-Reasoning, containing 2,217 carefully\ncurated materials science problems. Third, we introduce two specialized large\nlanguage models: Perovskite-Chat-LLM for domain-specific knowledge assistance\nand Perovskite-Reasoning-LLM for scientific reasoning tasks. Experimental\nresults demonstrate that our system significantly outperforms existing models\nin both domain-specific knowledge retrieval and scientific reasoning tasks,\nproviding researchers with effective tools for literature review, experimental\ndesign, and complex problem-solving in PSC research.",
      "upvotes": 1,
      "discussionId": "67b58c826e53744c2a3733c2"
    },
    "publishedAt": "2025-02-19T02:47:33.654Z",
    "title": "Perovskite-LLM: Knowledge-Enhanced Large Language Models for Perovskite Solar Cell Research",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12669.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63024676056ec3a2a8714b24",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661093436322-noauth.jpeg",
      "fullname": "Xiang Liu",
      "name": "Dominic789654",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.13142",
      "authors": [
        {
          "_id": "67b5790132be608036ee94e5",
          "name": "Dantong Niu",
          "hidden": false
        },
        {
          "_id": "67b5790132be608036ee94e6",
          "name": "Yuvan Sharma",
          "hidden": false
        },
        {
          "_id": "67b5790132be608036ee94e7",
          "name": "Haoru Xue",
          "hidden": false
        },
        {
          "_id": "67b5790132be608036ee94e8",
          "name": "Giscard Biamby",
          "hidden": false
        },
        {
          "_id": "67b5790132be608036ee94e9",
          "name": "Junyi Zhang",
          "hidden": false
        },
        {
          "_id": "67b5790132be608036ee94ea",
          "name": "Ziteng Ji",
          "hidden": false
        },
        {
          "_id": "67b5790132be608036ee94eb",
          "name": "Trevor Darrell",
          "hidden": false
        },
        {
          "_id": "67b5790132be608036ee94ec",
          "name": "Roei Herzig",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T18:59:01.000Z",
      "title": "Pre-training Auto-regressive Robotic Models with 4D Representations",
      "summary": "Foundation models pre-trained on massive unlabeled datasets have\nrevolutionized natural language and computer vision, exhibiting remarkable\ngeneralization capabilities, thus highlighting the importance of pre-training.\nYet, efforts in robotics have struggled to achieve similar success, limited by\neither the need for costly robotic annotations or the lack of representations\nthat effectively model the physical world. In this paper, we introduce ARM4R,\nan Auto-regressive Robotic Model that leverages low-level 4D Representations\nlearned from human video data to yield a better pre-trained robotic model.\nSpecifically, we focus on utilizing 3D point tracking representations from\nvideos derived by lifting 2D representations into 3D space via monocular depth\nestimation across time. These 4D representations maintain a shared geometric\nstructure between the points and robot state representations up to a linear\ntransformation, enabling efficient transfer learning from human video data to\nlow-level robotic control. Our experiments show that ARM4R can transfer\nefficiently from human video data to robotics and consistently improves\nperformance on tasks across various robot environments and configurations.",
      "upvotes": 0,
      "discussionId": "67b5790832be608036ee9638"
    },
    "publishedAt": "2025-02-19T01:24:26.365Z",
    "title": "Pre-training Auto-regressive Robotic Models with 4D Representations",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13142.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "667c5764186b27ef806636d3",
      "avatarUrl": "/avatars/5c08f0109bc0e350624112c0aff544f6.svg",
      "fullname": "Roei Herzig",
      "name": "roeiherz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.10852",
      "authors": [
        {
          "_id": "67b55321f703732d151de666",
          "name": "Zeli Su",
          "hidden": false
        },
        {
          "_id": "67b55321f703732d151de667",
          "name": "Ziyin Zhang",
          "hidden": false
        },
        {
          "_id": "67b55321f703732d151de668",
          "name": "Guixian Xu",
          "hidden": false
        },
        {
          "_id": "67b55321f703732d151de669",
          "name": "Jianing Liu",
          "hidden": false
        },
        {
          "_id": "67b55321f703732d151de66a",
          "name": "XU Han",
          "hidden": false
        },
        {
          "_id": "67b55321f703732d151de66b",
          "name": "Ting Zhang",
          "hidden": false
        },
        {
          "_id": "67b55321f703732d151de66c",
          "name": "Yushuang Dong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-15T16:53:10.000Z",
      "title": "Multilingual Encoder Knows more than You Realize: Shared Weights\n  Pretraining for Extremely Low-Resource Languages",
      "summary": "While multilingual language models like XLM-R have advanced multilingualism\nin NLP, they still perform poorly in extremely low-resource languages. This\nsituation is exacerbated by the fact that modern LLMs such as LLaMA and Qwen\nsupport far fewer languages than XLM-R, making text generation models\nnon-existent for many languages in the world. To tackle this challenge, we\npropose a novel framework for adapting multilingual encoders to text generation\nin extremely low-resource languages. By reusing the weights between the encoder\nand the decoder, our framework allows the model to leverage the learned\nsemantic space of the encoder, enabling efficient learning and effective\ngeneralization in low-resource languages. Applying this framework to four\nChinese minority languages, we present XLM-SWCM, and demonstrate its superior\nperformance on various downstream tasks even when compared with much larger\nmodels.",
      "upvotes": 0,
      "discussionId": "67b55322f703732d151de69d"
    },
    "publishedAt": "2025-02-18T22:46:16.586Z",
    "title": "Multilingual Encoder Knows more than You Realize: Shared Weights Pretraining for Extremely Low-Resource Languages",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.10852.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6430bdd8cd31d174a9f900fb",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Y9SPnRfpKSbYc7MhNdP-H.jpeg",
      "fullname": "Ziyin Zhang",
      "name": "Geralt-Targaryen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  }
]