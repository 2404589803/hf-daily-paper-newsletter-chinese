[
  {
    "paper": {
      "id": "2508.08401",
      "authors": [
        {
          "_id": "689d51fdb083e610d741e9ef",
          "name": "Jiatong Li",
          "hidden": false
        },
        {
          "_id": "689d51fdb083e610d741e9f0",
          "name": "Weida Wang",
          "hidden": false
        },
        {
          "_id": "689d51fdb083e610d741e9f1",
          "name": "Qinggang Zhang",
          "hidden": false
        },
        {
          "_id": "689d51fdb083e610d741e9f2",
          "name": "Junxian Li",
          "hidden": false
        },
        {
          "_id": "689d51fdb083e610d741e9f3",
          "name": "Di Zhang",
          "hidden": false
        },
        {
          "_id": "689d51fdb083e610d741e9f4",
          "name": "Changmeng Zheng",
          "hidden": false
        },
        {
          "_id": "689d51fdb083e610d741e9f5",
          "name": "Shufei Zhang",
          "hidden": false
        },
        {
          "_id": "689d51fdb083e610d741e9f6",
          "name": "Xiaoyong Wei",
          "hidden": false
        },
        {
          "_id": "689d51fdb083e610d741e9f7",
          "name": "Qing Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-11T18:50:05.000Z",
      "submittedOnDailyAt": "2025-08-14T01:40:24.947Z",
      "title": "Mol-R1: Towards Explicit Long-CoT Reasoning in Molecule Discovery",
      "submittedOnDailyBy": {
        "_id": "661b9d96c153e4a0a25adc3e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/661b9d96c153e4a0a25adc3e/VRt7kCQ0KdJp-lhPLOajO.jpeg",
        "isPro": false,
        "fullname": "Weida Wang",
        "user": "weidawang",
        "type": "user"
      },
      "summary": "Large language models (LLMs), especially Explicit Long Chain-of-Thought (CoT)\nreasoning models like DeepSeek-R1 and QWQ, have demonstrated powerful reasoning\ncapabilities, achieving impressive performance in commonsense reasoning and\nmathematical inference. Despite their effectiveness, Long-CoT reasoning models\nare often criticized for their limited ability and low efficiency in\nknowledge-intensive domains such as molecule discovery. Success in this field\nrequires a precise understanding of domain knowledge, including molecular\nstructures and chemical principles, which is challenging due to the inherent\ncomplexity of molecular data and the scarcity of high-quality expert\nannotations. To bridge this gap, we introduce Mol-R1, a novel framework\ndesigned to improve explainability and reasoning performance of R1-like\nExplicit Long-CoT reasoning LLMs in text-based molecule generation. Our\napproach begins with a high-quality reasoning dataset curated through Prior\nRegulation via In-context Distillation (PRID), a dedicated distillation\nstrategy to effectively generate paired reasoning traces guided by prior\nregulations. Building upon this, we introduce MoIA, Molecular Iterative\nAdaptation, a sophisticated training strategy that iteratively combines\nSupervised Fine-tuning (SFT) with Reinforced Policy Optimization (RPO),\ntailored to boost the reasoning performance of R1-like reasoning models for\nmolecule discovery. Finally, we examine the performance of Mol-R1 in the\ntext-based molecule reasoning generation task, showing superior performance\nagainst existing baselines.",
      "upvotes": 16,
      "discussionId": "689d51feb083e610d741e9f8",
      "ai_summary": "Mol-R1 framework enhances molecule discovery by improving reasoning performance and explainability through PRID and MoIA strategies.",
      "ai_keywords": [
        "Large language models",
        "Explicit Long Chain-of-Thought",
        "DeepSeek-R1",
        "QWQ",
        "commonsense reasoning",
        "mathematical inference",
        "knowledge-intensive domains",
        "molecule discovery",
        "molecular structures",
        "chemical principles",
        "high-quality reasoning dataset",
        "Prior Regulation via In-context Distillation",
        "PRID",
        "MoIA",
        "Molecular Iterative Adaptation",
        "Supervised Fine-tuning",
        "SFT",
        "Reinforced Policy Optimization",
        "RPO",
        "text-based molecule generation"
      ]
    },
    "publishedAt": "2025-08-11T14:50:05.000Z",
    "title": "Mol-R1: Towards Explicit Long-CoT Reasoning in Molecule Discovery",
    "summary": "Large language models (LLMs), especially Explicit Long Chain-of-Thought (CoT)\nreasoning models like DeepSeek-R1 and QWQ, have demonstrated powerful reasoning\ncapabilities, achieving impressive performance in commonsense reasoning and\nmathematical inference. Despite their effectiveness, Long-CoT reasoning models\nare often criticized for their limited ability and low efficiency in\nknowledge-intensive domains such as molecule discovery. Success in this field\nrequires a precise understanding of domain knowledge, including molecular\nstructures and chemical principles, which is challenging due to the inherent\ncomplexity of molecular data and the scarcity of high-quality expert\nannotations. To bridge this gap, we introduce Mol-R1, a novel framework\ndesigned to improve explainability and reasoning performance of R1-like\nExplicit Long-CoT reasoning LLMs in text-based molecule generation. Our\napproach begins with a high-quality reasoning dataset curated through Prior\nRegulation via In-context Distillation (PRID), a dedicated distillation\nstrategy to effectively generate paired reasoning traces guided by prior\nregulations. Building upon this, we introduce MoIA, Molecular Iterative\nAdaptation, a sophisticated training strategy that iteratively combines\nSupervised Fine-tuning (SFT) with Reinforced Policy Optimization (RPO),\ntailored to boost the reasoning performance of R1-like reasoning models for\nmolecule discovery. Finally, we examine the performance of Mol-R1 in the\ntext-based molecule reasoning generation task, showing superior performance\nagainst existing baselines.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.08401.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "661b9d96c153e4a0a25adc3e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/661b9d96c153e4a0a25adc3e/VRt7kCQ0KdJp-lhPLOajO.jpeg",
      "fullname": "Weida Wang",
      "name": "weidawang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 0
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.09889",
      "authors": [
        {
          "_id": "689d4a16b083e610d741e9c1",
          "name": "Zhitian Xie",
          "hidden": false
        },
        {
          "_id": "689d4a16b083e610d741e9c2",
          "name": "Qintong Wu",
          "hidden": false
        },
        {
          "_id": "689d4a16b083e610d741e9c3",
          "name": "Chengyue Yu",
          "hidden": false
        },
        {
          "_id": "689d4a16b083e610d741e9c4",
          "name": "Chenyi Zhuang",
          "hidden": false
        },
        {
          "_id": "689d4a16b083e610d741e9c5",
          "name": "Jinjie Gu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-13T15:46:25.000Z",
      "submittedOnDailyAt": "2025-08-14T01:13:03.061Z",
      "title": "AWorld: Dynamic Multi-Agent System with Stable Maneuvering for Robust\n  GAIA Problem Solving",
      "submittedOnDailyBy": {
        "_id": "64e847ab5ddcace745b8f5b1",
        "avatarUrl": "/avatars/89525e66ff2900f86f66a11043a298f9.svg",
        "isPro": true,
        "fullname": "chenyi zhuang",
        "user": "chengle",
        "type": "user"
      },
      "summary": "The rapid advancement of large language models (LLMs) has empowered\nintelligent agents to leverage diverse external tools for solving complex\nreal-world problems. However, as agents increasingly depend on multiple tools,\nthey encounter new challenges: extended contexts from disparate sources and\nnoisy or irrelevant tool outputs can undermine system reliability and accuracy.\nThese challenges underscore the necessity for enhanced stability in agent-based\nsystems. To address this, we introduce dynamic supervision and maneuvering\nmechanisms, constructing a robust and dynamic Multi-Agent System (MAS)\narchitecture within the AWorld framework. In our approach, the Execution Agent\ninvokes the Guard Agent at critical steps to verify and correct the reasoning\nprocess, effectively reducing errors arising from noise and bolstering\nproblem-solving robustness. Extensive experiments on the GAIA test dataset\nreveal that our dynamic maneuvering mechanism significantly improves both the\neffectiveness and stability of solutions, outperforming single-agent system\n(SAS) and standard tool-augmented systems. As a result, our dynamic MAS system\nachieved first place among open-source projects on the prestigious GAIA\nleaderboard. These findings highlight the practical value of collaborative\nagent roles in developing more reliable and trustworthy intelligent systems.",
      "upvotes": 15,
      "discussionId": "689d4a17b083e610d741e9c6",
      "githubRepo": "https://github.com/inclusionAI/AWorld",
      "ai_summary": "A dynamic Multi-Agent System (MAS) with Execution and Guard Agents improves the reliability and effectiveness of intelligent agents using external tools, outperforming single-agent systems on the GAIA leaderboard.",
      "ai_keywords": [
        "large language models",
        "Multi-Agent System",
        "Execution Agent",
        "Guard Agent",
        "dynamic supervision",
        "maneuvering mechanisms",
        "GAIA test dataset",
        "single-agent system",
        "tool-augmented systems",
        "GAIA leaderboard"
      ],
      "githubStars": 553
    },
    "publishedAt": "2025-08-13T11:46:25.000Z",
    "title": "AWorld: Dynamic Multi-Agent System with Stable Maneuvering for Robust\n  GAIA Problem Solving",
    "summary": "The rapid advancement of large language models (LLMs) has empowered\nintelligent agents to leverage diverse external tools for solving complex\nreal-world problems. However, as agents increasingly depend on multiple tools,\nthey encounter new challenges: extended contexts from disparate sources and\nnoisy or irrelevant tool outputs can undermine system reliability and accuracy.\nThese challenges underscore the necessity for enhanced stability in agent-based\nsystems. To address this, we introduce dynamic supervision and maneuvering\nmechanisms, constructing a robust and dynamic Multi-Agent System (MAS)\narchitecture within the AWorld framework. In our approach, the Execution Agent\ninvokes the Guard Agent at critical steps to verify and correct the reasoning\nprocess, effectively reducing errors arising from noise and bolstering\nproblem-solving robustness. Extensive experiments on the GAIA test dataset\nreveal that our dynamic maneuvering mechanism significantly improves both the\neffectiveness and stability of solutions, outperforming single-agent system\n(SAS) and standard tool-augmented systems. As a result, our dynamic MAS system\nachieved first place among open-source projects on the prestigious GAIA\nleaderboard. These findings highlight the practical value of collaborative\nagent roles in developing more reliable and trustworthy intelligent systems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.09889.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64e847ab5ddcace745b8f5b1",
      "avatarUrl": "/avatars/89525e66ff2900f86f66a11043a298f9.svg",
      "fullname": "chenyi zhuang",
      "name": "chengle",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.09987",
      "authors": [
        {
          "_id": "689d6033b083e610d741ea4c",
          "name": "Junyan Ye",
          "hidden": false
        },
        {
          "_id": "689d6033b083e610d741ea4d",
          "name": "Dongzhi Jiang",
          "hidden": false
        },
        {
          "_id": "689d6033b083e610d741ea4e",
          "name": "Zihao Wang",
          "hidden": false
        },
        {
          "_id": "689d6033b083e610d741ea4f",
          "name": "Leqi Zhu",
          "hidden": false
        },
        {
          "_id": "689d6033b083e610d741ea50",
          "name": "Zhenghao Hu",
          "hidden": false
        },
        {
          "_id": "689d6033b083e610d741ea51",
          "name": "Zilong Huang",
          "hidden": false
        },
        {
          "_id": "689d6033b083e610d741ea52",
          "name": "Jun He",
          "hidden": false
        },
        {
          "_id": "689d6033b083e610d741ea53",
          "name": "Zhiyuan Yan",
          "hidden": false
        },
        {
          "_id": "689d6033b083e610d741ea54",
          "name": "Jinghua Yu",
          "hidden": false
        },
        {
          "_id": "689d6033b083e610d741ea55",
          "name": "Hongsheng Li",
          "hidden": false
        },
        {
          "_id": "689d6033b083e610d741ea56",
          "name": "Conghui He",
          "hidden": false
        },
        {
          "_id": "689d6033b083e610d741ea57",
          "name": "Weijia Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-13T17:59:28.000Z",
      "submittedOnDailyAt": "2025-08-14T02:38:09.339Z",
      "title": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved\n  Image Generation",
      "submittedOnDailyBy": {
        "_id": "6349214f8146350b3a4c5cdf",
        "avatarUrl": "/avatars/cfd24caac9a87efb528d0f4c375932bc.svg",
        "isPro": false,
        "fullname": "Dongzhi Jiang",
        "user": "CaraJ",
        "type": "user"
      },
      "summary": "Recently, GPT-4o has garnered significant attention for its strong\nperformance in image generation, yet open-source models still lag behind.\nSeveral studies have explored distilling image data from GPT-4o to enhance\nopen-source models, achieving notable progress. However, a key question\nremains: given that real-world image datasets already constitute a natural\nsource of high-quality data, why should we use GPT-4o-generated synthetic data?\nIn this work, we identify two key advantages of synthetic images. First, they\ncan complement rare scenarios in real-world datasets, such as surreal fantasy\nor multi-reference image generation, which frequently occur in user queries.\nSecond, they provide clean and controllable supervision. Real-world data often\ncontains complex background noise and inherent misalignment between text\ndescriptions and image content, whereas synthetic images offer pure backgrounds\nand long-tailed supervision signals, facilitating more accurate text-to-image\nalignment. Building on these insights, we introduce Echo-4o-Image, a 180K-scale\nsynthetic dataset generated by GPT-4o, harnessing the power of synthetic image\ndata to address blind spots in real-world coverage. Using this dataset, we\nfine-tune the unified multimodal generation baseline Bagel to obtain Echo-4o.\nIn addition, we propose two new evaluation benchmarks for a more accurate and\nchallenging assessment of image generation capabilities: GenEval++, which\nincreases instruction complexity to mitigate score saturation, and\nImagine-Bench, which focuses on evaluating both the understanding and\ngeneration of imaginative content. Echo-4o demonstrates strong performance\nacross standard benchmarks. Moreover, applying Echo-4o-Image to other\nfoundation models (e.g., OmniGen2, BLIP3-o) yields consistent performance gains\nacross multiple metrics, highlighting the datasets strong transferability.",
      "upvotes": 12,
      "discussionId": "689d6033b083e610d741ea58",
      "projectPage": "https://yejy53.github.io/Echo-4o/",
      "githubRepo": "https://github.com/yejy53/Echo-4o",
      "ai_summary": "Echo-4o-Image, a synthetic dataset generated by GPT-4o, enhances image generation models by addressing rare scenarios and providing clean supervision, leading to improved performance and transferability.",
      "ai_keywords": [
        "GPT-4o",
        "synthetic dataset",
        "rare scenarios",
        "surreal fantasy",
        "multi-reference image generation",
        "clean supervision",
        "complex background noise",
        "text-to-image alignment",
        "unified multimodal generation",
        "Bagel",
        "Echo-4o",
        "GenEval++",
        "Imagine-Bench",
        "OmniGen2",
        "BLIP3-o"
      ],
      "githubStars": 5
    },
    "publishedAt": "2025-08-13T13:59:28.000Z",
    "title": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved\n  Image Generation",
    "summary": "Recently, GPT-4o has garnered significant attention for its strong\nperformance in image generation, yet open-source models still lag behind.\nSeveral studies have explored distilling image data from GPT-4o to enhance\nopen-source models, achieving notable progress. However, a key question\nremains: given that real-world image datasets already constitute a natural\nsource of high-quality data, why should we use GPT-4o-generated synthetic data?\nIn this work, we identify two key advantages of synthetic images. First, they\ncan complement rare scenarios in real-world datasets, such as surreal fantasy\nor multi-reference image generation, which frequently occur in user queries.\nSecond, they provide clean and controllable supervision. Real-world data often\ncontains complex background noise and inherent misalignment between text\ndescriptions and image content, whereas synthetic images offer pure backgrounds\nand long-tailed supervision signals, facilitating more accurate text-to-image\nalignment. Building on these insights, we introduce Echo-4o-Image, a 180K-scale\nsynthetic dataset generated by GPT-4o, harnessing the power of synthetic image\ndata to address blind spots in real-world coverage. Using this dataset, we\nfine-tune the unified multimodal generation baseline Bagel to obtain Echo-4o.\nIn addition, we propose two new evaluation benchmarks for a more accurate and\nchallenging assessment of image generation capabilities: GenEval++, which\nincreases instruction complexity to mitigate score saturation, and\nImagine-Bench, which focuses on evaluating both the understanding and\ngeneration of imaginative content. Echo-4o demonstrates strong performance\nacross standard benchmarks. Moreover, applying Echo-4o-Image to other\nfoundation models (e.g., OmniGen2, BLIP3-o) yields consistent performance gains\nacross multiple metrics, highlighting the datasets strong transferability.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.09987.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6349214f8146350b3a4c5cdf",
      "avatarUrl": "/avatars/cfd24caac9a87efb528d0f4c375932bc.svg",
      "fullname": "Dongzhi Jiang",
      "name": "CaraJ",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.07901",
      "authors": [
        {
          "_id": "689aaeabfab6fdd2e52ac47c",
          "name": "Bowen Xue",
          "hidden": false
        },
        {
          "_id": "689aaeabfab6fdd2e52ac47d",
          "name": "Qixin Yan",
          "hidden": false
        },
        {
          "_id": "689aaeabfab6fdd2e52ac47e",
          "name": "Wenjing Wang",
          "hidden": false
        },
        {
          "_id": "689aaeabfab6fdd2e52ac47f",
          "name": "Hao Liu",
          "hidden": false
        },
        {
          "_id": "689aaeabfab6fdd2e52ac480",
          "name": "Chen Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-11T12:17:38.000Z",
      "submittedOnDailyAt": "2025-08-14T06:10:24.567Z",
      "title": "Stand-In: A Lightweight and Plug-and-Play Identity Control for Video\n  Generation",
      "submittedOnDailyBy": {
        "_id": "6683a05e74fb1736a4b7c934",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6683a05e74fb1736a4b7c934/eiz6qlqIUjAWGy5zfg8Cs.jpeg",
        "isPro": false,
        "fullname": "QRQ",
        "user": "RichardQRQ",
        "type": "user"
      },
      "summary": "Generating high-fidelity human videos that match user-specified identities is\nimportant yet challenging in the field of generative AI. Existing methods often\nrely on an excessive number of training parameters and lack compatibility with\nother AIGC tools. In this paper, we propose Stand-In, a lightweight and\nplug-and-play framework for identity preservation in video generation.\nSpecifically, we introduce a conditional image branch into the pre-trained\nvideo generation model. Identity control is achieved through restricted\nself-attentions with conditional position mapping, and can be learned quickly\nwith only 2000 pairs. Despite incorporating and training just sim1\\%\nadditional parameters, our framework achieves excellent results in video\nquality and identity preservation, outperforming other full-parameter training\nmethods. Moreover, our framework can be seamlessly integrated for other tasks,\nsuch as subject-driven video generation, pose-referenced video generation,\nstylization, and face swapping.",
      "upvotes": 11,
      "discussionId": "689aaeabfab6fdd2e52ac481",
      "ai_summary": "A lightweight framework for identity preservation in video generation using conditional image branches and restricted self-attentions outperforms full-parameter methods with minimal additional parameters.",
      "ai_keywords": [
        "conditional image branch",
        "pre-trained video generation model",
        "restricted self-attentions",
        "conditional position mapping",
        "subject-driven video generation",
        "pose-referenced video generation",
        "stylization",
        "face swapping"
      ]
    },
    "publishedAt": "2025-08-11T08:17:38.000Z",
    "title": "Stand-In: A Lightweight and Plug-and-Play Identity Control for Video\n  Generation",
    "summary": "Generating high-fidelity human videos that match user-specified identities is\nimportant yet challenging in the field of generative AI. Existing methods often\nrely on an excessive number of training parameters and lack compatibility with\nother AIGC tools. In this paper, we propose Stand-In, a lightweight and\nplug-and-play framework for identity preservation in video generation.\nSpecifically, we introduce a conditional image branch into the pre-trained\nvideo generation model. Identity control is achieved through restricted\nself-attentions with conditional position mapping, and can be learned quickly\nwith only 2000 pairs. Despite incorporating and training just sim1\\%\nadditional parameters, our framework achieves excellent results in video\nquality and identity preservation, outperforming other full-parameter training\nmethods. Moreover, our framework can be seamlessly integrated for other tasks,\nsuch as subject-driven video generation, pose-referenced video generation,\nstylization, and face swapping.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.07901.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6683a05e74fb1736a4b7c934",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6683a05e74fb1736a4b7c934/eiz6qlqIUjAWGy5zfg8Cs.jpeg",
      "fullname": "QRQ",
      "name": "RichardQRQ",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.09983",
      "authors": [
        {
          "_id": "689d7a76b083e610d741ea88",
          "name": "David Dinkevich",
          "hidden": false
        },
        {
          "_id": "689d7a76b083e610d741ea89",
          "name": "Matan Levy",
          "hidden": false
        },
        {
          "_id": "689d7a76b083e610d741ea8a",
          "name": "Omri Avrahami",
          "hidden": false
        },
        {
          "_id": "689d7a76b083e610d741ea8b",
          "name": "Dvir Samuel",
          "hidden": false
        },
        {
          "_id": "689d7a76b083e610d741ea8c",
          "name": "Dani Lischinski",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62f6ab4fc3372328414c8689/PmghfKC-KB0_hrcZIsBwo.webp"
      ],
      "publishedAt": "2025-08-13T17:56:26.000Z",
      "submittedOnDailyAt": "2025-08-14T04:28:00.139Z",
      "title": "Story2Board: A Training-Free Approach for Expressive Storyboard\n  Generation",
      "submittedOnDailyBy": {
        "_id": "62f6ab4fc3372328414c8689",
        "avatarUrl": "/avatars/4f728a5b70c9fe4a64e80e2b643ca620.svg",
        "isPro": false,
        "fullname": "Omri Avrahami",
        "user": "omriav",
        "type": "user"
      },
      "summary": "We present Story2Board, a training-free framework for expressive storyboard\ngeneration from natural language. Existing methods narrowly focus on subject\nidentity, overlooking key aspects of visual storytelling such as spatial\ncomposition, background evolution, and narrative pacing. To address this, we\nintroduce a lightweight consistency framework composed of two components:\nLatent Panel Anchoring, which preserves a shared character reference across\npanels, and Reciprocal Attention Value Mixing, which softly blends visual\nfeatures between token pairs with strong reciprocal attention. Together, these\nmechanisms enhance coherence without architectural changes or fine-tuning,\nenabling state-of-the-art diffusion models to generate visually diverse yet\nconsistent storyboards. To structure generation, we use an off-the-shelf\nlanguage model to convert free-form stories into grounded panel-level prompts.\nTo evaluate, we propose the Rich Storyboard Benchmark, a suite of open-domain\nnarratives designed to assess layout diversity and background-grounded\nstorytelling, in addition to consistency. We also introduce a new Scene\nDiversity metric that quantifies spatial and pose variation across storyboards.\nOur qualitative and quantitative results, as well as a user study, show that\nStory2Board produces more dynamic, coherent, and narratively engaging\nstoryboards than existing baselines.",
      "upvotes": 7,
      "discussionId": "689d7a76b083e610d741ea8d",
      "projectPage": "https://daviddinkevich.github.io/Story2Board/",
      "githubRepo": "https://github.com/daviddinkevich/Story2Board",
      "ai_summary": "Story2Board generates expressive storyboards from natural language using a consistency framework that enhances coherence and diversity without fine-tuning.",
      "ai_keywords": [
        "Latent Panel Anchoring",
        "Reciprocal Attention Value Mixing",
        "diffusion models",
        "Rich Storyboard Benchmark",
        "Scene Diversity metric"
      ],
      "githubStars": 1
    },
    "publishedAt": "2025-08-13T13:56:26.000Z",
    "title": "Story2Board: A Training-Free Approach for Expressive Storyboard\n  Generation",
    "summary": "We present Story2Board, a training-free framework for expressive storyboard\ngeneration from natural language. Existing methods narrowly focus on subject\nidentity, overlooking key aspects of visual storytelling such as spatial\ncomposition, background evolution, and narrative pacing. To address this, we\nintroduce a lightweight consistency framework composed of two components:\nLatent Panel Anchoring, which preserves a shared character reference across\npanels, and Reciprocal Attention Value Mixing, which softly blends visual\nfeatures between token pairs with strong reciprocal attention. Together, these\nmechanisms enhance coherence without architectural changes or fine-tuning,\nenabling state-of-the-art diffusion models to generate visually diverse yet\nconsistent storyboards. To structure generation, we use an off-the-shelf\nlanguage model to convert free-form stories into grounded panel-level prompts.\nTo evaluate, we propose the Rich Storyboard Benchmark, a suite of open-domain\nnarratives designed to assess layout diversity and background-grounded\nstorytelling, in addition to consistency. We also introduce a new Scene\nDiversity metric that quantifies spatial and pose variation across storyboards.\nOur qualitative and quantitative results, as well as a user study, show that\nStory2Board produces more dynamic, coherent, and narratively engaging\nstoryboards than existing baselines.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62f6ab4fc3372328414c8689/PmghfKC-KB0_hrcZIsBwo.webp"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.09983.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62f6ab4fc3372328414c8689",
      "avatarUrl": "/avatars/4f728a5b70c9fe4a64e80e2b643ca620.svg",
      "fullname": "Omri Avrahami",
      "name": "omriav",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.09192",
      "authors": [
        {
          "_id": "689d3293b083e610d741e992",
          "name": "Xu Wang",
          "hidden": false
        },
        {
          "_id": "689d3293b083e610d741e993",
          "name": "Chenkai Xu",
          "hidden": false
        },
        {
          "_id": "689d3293b083e610d741e994",
          "name": "Yijie Jin",
          "hidden": false
        },
        {
          "_id": "689d3293b083e610d741e995",
          "name": "Jiachun Jin",
          "hidden": false
        },
        {
          "_id": "689d3293b083e610d741e996",
          "name": "Hao Zhang",
          "hidden": false
        },
        {
          "_id": "689d3293b083e610d741e997",
          "name": "Zhijie Deng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-08T04:51:37.000Z",
      "submittedOnDailyAt": "2025-08-14T02:37:01.874Z",
      "title": "Diffusion LLMs Can Do Faster-Than-AR Inference via Discrete Diffusion\n  Forcing",
      "submittedOnDailyBy": {
        "_id": "65708920806dee337da0eef5",
        "avatarUrl": "/avatars/945e328dedc8e1e3111f48c344ad5b03.svg",
        "isPro": false,
        "fullname": "xuchenkai",
        "user": "UnhurriedDawn",
        "type": "user"
      },
      "summary": "Diffusion Large Language Models (dLLMs) have emerged as a promising\nalternative to autoregressive (AR) LLMs for text generation, with the potential\nto decode multiple tokens in a single iteration. However, none of the existing\nopen-source dLLMs have achieved superior inference speed over AR LLMs of\nsimilar size. This paper breaks this barrier based on a simple and effective\nstrategy named discrete diffusion forcing (D2F). D2F equips dLLMs with two key\ncapabilities: (1) block-wise autoregressive generation to enable KV cache\nutilization; (2) prediction of following tokens without requiring completion of\nprior blocks for inter-block parallel decoding. In this way, the vanilla dLLMs\nare refurbished into an AR-diffusion hybrid paradigm for efficient inference.\nD2F can be implemented with an asymmetric distillation process based on\npre-trained dLLMs. We further propose a pipelined parallel decoding algorithm,\nwhich enables a trade-off between efficiency and efficacy. Empirically, D2F\ndLLMs achieve more than 2.5times inference speed than LLaMA3 and\nQwen2.5 on GSM8K. Compared to vanilla dLLMs like LLaDA and Dream, the\nacceleration can be more than 50times while maintaining comparable\noutput quality. The code is available at\nhttps://github.com/zhijie-group/Discrete-Diffusion-Forcing.",
      "upvotes": 7,
      "discussionId": "689d3294b083e610d741e998",
      "projectPage": "https://zhijie-group.github.io/Discrete-Diffusion-Forcing/",
      "githubRepo": "https://github.com/zhijie-group/Discrete-Diffusion-Forcing",
      "ai_summary": "Discrete diffusion forcing enhances diffusion large language models with block-wise autoregressive generation and inter-block parallel decoding, significantly improving inference speed while maintaining quality.",
      "ai_keywords": [
        "diffusion large language models",
        "autoregressive LLMs",
        "discrete diffusion forcing",
        "block-wise autoregressive generation",
        "KV cache",
        "inter-block parallel decoding",
        "asymmetric distillation",
        "pipelined parallel decoding",
        "GSM8K",
        "LLaMA3",
        "Qwen2.5",
        "LLaDA",
        "Dream"
      ],
      "githubStars": 9
    },
    "publishedAt": "2025-08-08T00:51:37.000Z",
    "title": "Diffusion LLMs Can Do Faster-Than-AR Inference via Discrete Diffusion\n  Forcing",
    "summary": "Diffusion Large Language Models (dLLMs) have emerged as a promising\nalternative to autoregressive (AR) LLMs for text generation, with the potential\nto decode multiple tokens in a single iteration. However, none of the existing\nopen-source dLLMs have achieved superior inference speed over AR LLMs of\nsimilar size. This paper breaks this barrier based on a simple and effective\nstrategy named discrete diffusion forcing (D2F). D2F equips dLLMs with two key\ncapabilities: (1) block-wise autoregressive generation to enable KV cache\nutilization; (2) prediction of following tokens without requiring completion of\nprior blocks for inter-block parallel decoding. In this way, the vanilla dLLMs\nare refurbished into an AR-diffusion hybrid paradigm for efficient inference.\nD2F can be implemented with an asymmetric distillation process based on\npre-trained dLLMs. We further propose a pipelined parallel decoding algorithm,\nwhich enables a trade-off between efficiency and efficacy. Empirically, D2F\ndLLMs achieve more than 2.5times inference speed than LLaMA3 and\nQwen2.5 on GSM8K. Compared to vanilla dLLMs like LLaDA and Dream, the\nacceleration can be more than 50times while maintaining comparable\noutput quality. The code is available at\nhttps://github.com/zhijie-group/Discrete-Diffusion-Forcing.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.09192.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "65708920806dee337da0eef5",
      "avatarUrl": "/avatars/945e328dedc8e1e3111f48c344ad5b03.svg",
      "fullname": "xuchenkai",
      "name": "UnhurriedDawn",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.06009",
      "authors": [
        {
          "_id": "68999432f022d141f5d4365d",
          "user": {
            "_id": "6894ed02b1bed8e651573585",
            "avatarUrl": "/avatars/4532c56a77a87cf45f1c5040857aa6c3.svg",
            "isPro": false,
            "fullname": "Jun Feng",
            "user": "junfeng0288",
            "type": "user"
          },
          "name": "Jun Feng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-13T07:34:25.197Z",
          "hidden": false
        },
        {
          "_id": "68999432f022d141f5d4365e",
          "name": "Zixin Wang",
          "hidden": false
        },
        {
          "_id": "68999432f022d141f5d4365f",
          "name": "Zhentao Zhang",
          "hidden": false
        },
        {
          "_id": "68999432f022d141f5d43660",
          "name": "Yue Guo",
          "hidden": false
        },
        {
          "_id": "68999432f022d141f5d43661",
          "name": "Zhihan Zhou",
          "hidden": false
        },
        {
          "_id": "68999432f022d141f5d43662",
          "name": "Xiuyi Chen",
          "hidden": false
        },
        {
          "_id": "68999432f022d141f5d43663",
          "name": "Zhenyang Li",
          "hidden": false
        },
        {
          "_id": "68999432f022d141f5d43664",
          "name": "Dawei Yin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-08T04:39:16.000Z",
      "submittedOnDailyAt": "2025-08-14T01:33:48.978Z",
      "title": "MathReal: We Keep It Real! A Real Scene Benchmark for Evaluating Math\n  Reasoning in Multimodal Large Language Models",
      "submittedOnDailyBy": {
        "_id": "6894ed02b1bed8e651573585",
        "avatarUrl": "/avatars/4532c56a77a87cf45f1c5040857aa6c3.svg",
        "isPro": false,
        "fullname": "Jun Feng",
        "user": "junfeng0288",
        "type": "user"
      },
      "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities in visual mathematical reasoning across various existing\nbenchmarks. However, these benchmarks are predominantly based on clean or\nprocessed multimodal inputs, without incorporating the images provided by\nreal-world Kindergarten through 12th grade (K-12) educational users. To address\nthis gap, we introduce MathReal, a meticulously curated dataset comprising\n2,000 mathematical questions with images captured by handheld mobile devices in\nauthentic scenarios. Each question is an image, containing the question text\nand visual element. We systematically classify the real images into three\nprimary categories: image quality degradation, perspective variation, and\nirrelevant content interference, which are further delineated into 14\nsubcategories. Additionally, MathReal spans five core knowledge and ability\ncategories, which encompass three question types and are divided into three\ndifficulty levels. To comprehensively evaluate the multimodal mathematical\nreasoning abilities of state-of-the-art MLLMs in real-world scenarios, we\ndesign six experimental settings that enable a systematic analysis of their\nperformance. Through extensive experimentation, we find that the\nproblem-solving abilities of existing MLLMs are significantly challenged in\nrealistic educational contexts. Based on this, we conduct a thorough analysis\nof their performance and error patterns, providing insights into their\nrecognition, comprehension, and reasoning capabilities, and outlining\ndirections for future improvements. Data and code:\nhttps://github.com/junfeng0288/MathReal.",
      "upvotes": 7,
      "discussionId": "68999433f022d141f5d43665",
      "githubRepo": "https://github.com/junfeng0288/MathReal",
      "ai_summary": "MathReal, a dataset of real-world mathematical questions with images, evaluates the performance of multimodal large language models in authentic educational settings, highlighting challenges and providing insights for improvement.",
      "ai_keywords": [
        "Multimodal Large Language Models",
        "MLLMs",
        "visual mathematical reasoning",
        "MathReal",
        "image quality degradation",
        "perspective variation",
        "irrelevant content interference",
        "multimodal mathematical reasoning",
        "experimental settings",
        "problem-solving abilities",
        "recognition",
        "comprehension",
        "reasoning capabilities"
      ],
      "githubStars": 4
    },
    "publishedAt": "2025-08-08T00:39:16.000Z",
    "title": "MathReal: We Keep It Real! A Real Scene Benchmark for Evaluating Math\n  Reasoning in Multimodal Large Language Models",
    "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities in visual mathematical reasoning across various existing\nbenchmarks. However, these benchmarks are predominantly based on clean or\nprocessed multimodal inputs, without incorporating the images provided by\nreal-world Kindergarten through 12th grade (K-12) educational users. To address\nthis gap, we introduce MathReal, a meticulously curated dataset comprising\n2,000 mathematical questions with images captured by handheld mobile devices in\nauthentic scenarios. Each question is an image, containing the question text\nand visual element. We systematically classify the real images into three\nprimary categories: image quality degradation, perspective variation, and\nirrelevant content interference, which are further delineated into 14\nsubcategories. Additionally, MathReal spans five core knowledge and ability\ncategories, which encompass three question types and are divided into three\ndifficulty levels. To comprehensively evaluate the multimodal mathematical\nreasoning abilities of state-of-the-art MLLMs in real-world scenarios, we\ndesign six experimental settings that enable a systematic analysis of their\nperformance. Through extensive experimentation, we find that the\nproblem-solving abilities of existing MLLMs are significantly challenged in\nrealistic educational contexts. Based on this, we conduct a thorough analysis\nof their performance and error patterns, providing insights into their\nrecognition, comprehension, and reasoning capabilities, and outlining\ndirections for future improvements. Data and code:\nhttps://github.com/junfeng0288/MathReal.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.06009.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6894ed02b1bed8e651573585",
      "avatarUrl": "/avatars/4532c56a77a87cf45f1c5040857aa6c3.svg",
      "fullname": "Jun Feng",
      "name": "junfeng0288",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2508.09736",
      "authors": [
        {
          "_id": "689d8af7b083e610d741eaaf",
          "name": "Lin Long",
          "hidden": false
        },
        {
          "_id": "689d8af7b083e610d741eab0",
          "name": "Yichen He",
          "hidden": false
        },
        {
          "_id": "689d8af7b083e610d741eab1",
          "name": "Wentao Ye",
          "hidden": false
        },
        {
          "_id": "689d8af7b083e610d741eab2",
          "name": "Yiyuan Pan",
          "hidden": false
        },
        {
          "_id": "689d8af7b083e610d741eab3",
          "name": "Yuan Lin",
          "hidden": false
        },
        {
          "_id": "689d8af7b083e610d741eab4",
          "name": "Hang Li",
          "hidden": false
        },
        {
          "_id": "689d8af7b083e610d741eab5",
          "name": "Junbo Zhao",
          "hidden": false
        },
        {
          "_id": "689d8af7b083e610d741eab6",
          "name": "Wei Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-13T12:03:03.000Z",
      "submittedOnDailyAt": "2025-08-14T05:38:40.513Z",
      "title": "Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with\n  Long-Term Memory",
      "submittedOnDailyBy": {
        "_id": "60ea81771cc8dc259c58e905",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60ea81771cc8dc259c58e905/kmGlaNvdS4EEHc_5qongT.jpeg",
        "isPro": false,
        "fullname": "yichen he",
        "user": "hyc2026",
        "type": "user"
      },
      "summary": "We introduce M3-Agent, a novel multimodal agent framework equipped with\nlong-term memory. Like humans, M3-Agent can process real-time visual and\nauditory inputs to build and update its long-term memory. Beyond episodic\nmemory, it also develops semantic memory, enabling it to accumulate world\nknowledge over time. Its memory is organized in an entity-centric, multimodal\nformat, allowing deeper and more consistent understanding of the environment.\nGiven an instruction, M3-Agent autonomously performs multi-turn, iterative\nreasoning and retrieves relevant information from memory to accomplish the\ntask. To evaluate memory effectiveness and memory-based reasoning in multimodal\nagents, we develop M3-Bench, a new long-video question answering benchmark.\nM3-Bench comprises 100 newly recorded real-world videos captured from a robot's\nperspective (M3-Bench-robot) and 929 web-sourced videos across diverse\nscenarios (M3-Bench-web). We annotate question-answer pairs designed to test\nkey capabilities essential for agent applications, such as human understanding,\ngeneral knowledge extraction, and cross-modal reasoning. Experimental results\nshow that M3-Agent, trained via reinforcement learning, outperforms the\nstrongest baseline, a prompting agent using Gemini-1.5-pro and GPT-4o,\nachieving 6.7%, 7.7%, and 5.3% higher accuracy on M3-Bench-robot, M3-Bench-web\nand VideoMME-long, respectively. Our work advances the multimodal agents toward\nmore human-like long-term memory and provides insights into their practical\ndesign. Model, code and data are available at\nhttps://github.com/bytedance-seed/m3-agent",
      "upvotes": 4,
      "discussionId": "689d8af7b083e610d741eab7",
      "projectPage": "https://m3-agent.github.io/",
      "githubRepo": "https://github.com/ByteDance-Seed/m3-agent",
      "ai_summary": "M3-Agent, a multimodal agent with long-term memory, performs multi-turn reasoning and outperforms baselines on a new long-video question answering benchmark.",
      "ai_keywords": [
        "multimodal agent",
        "long-term memory",
        "episodic memory",
        "semantic memory",
        "real-time visual inputs",
        "real-time auditory inputs",
        "multi-turn reasoning",
        "iterative reasoning",
        "M3-Bench",
        "long-video question answering benchmark",
        "reinforcement learning",
        "Gemini-1.5-pro",
        "GPT-4o",
        "human understanding",
        "general knowledge extraction",
        "cross-modal reasoning"
      ],
      "githubStars": 24
    },
    "publishedAt": "2025-08-13T08:03:03.000Z",
    "title": "Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with\n  Long-Term Memory",
    "summary": "We introduce M3-Agent, a novel multimodal agent framework equipped with\nlong-term memory. Like humans, M3-Agent can process real-time visual and\nauditory inputs to build and update its long-term memory. Beyond episodic\nmemory, it also develops semantic memory, enabling it to accumulate world\nknowledge over time. Its memory is organized in an entity-centric, multimodal\nformat, allowing deeper and more consistent understanding of the environment.\nGiven an instruction, M3-Agent autonomously performs multi-turn, iterative\nreasoning and retrieves relevant information from memory to accomplish the\ntask. To evaluate memory effectiveness and memory-based reasoning in multimodal\nagents, we develop M3-Bench, a new long-video question answering benchmark.\nM3-Bench comprises 100 newly recorded real-world videos captured from a robot's\nperspective (M3-Bench-robot) and 929 web-sourced videos across diverse\nscenarios (M3-Bench-web). We annotate question-answer pairs designed to test\nkey capabilities essential for agent applications, such as human understanding,\ngeneral knowledge extraction, and cross-modal reasoning. Experimental results\nshow that M3-Agent, trained via reinforcement learning, outperforms the\nstrongest baseline, a prompting agent using Gemini-1.5-pro and GPT-4o,\nachieving 6.7%, 7.7%, and 5.3% higher accuracy on M3-Bench-robot, M3-Bench-web\nand VideoMME-long, respectively. Our work advances the multimodal agents toward\nmore human-like long-term memory and provides insights into their practical\ndesign. Model, code and data are available at\nhttps://github.com/bytedance-seed/m3-agent",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.09736.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "60ea81771cc8dc259c58e905",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60ea81771cc8dc259c58e905/kmGlaNvdS4EEHc_5qongT.jpeg",
      "fullname": "yichen he",
      "name": "hyc2026",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.09456",
      "authors": [
        {
          "_id": "689d555cb083e610d741e9fa",
          "name": "Junxian Li",
          "hidden": false
        },
        {
          "_id": "689d555cb083e610d741e9fb",
          "name": "Beining Xu",
          "hidden": false
        },
        {
          "_id": "689d555cb083e610d741e9fc",
          "name": "Di Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-13T03:22:19.000Z",
      "submittedOnDailyAt": "2025-08-14T01:51:36.770Z",
      "title": "IAG: Input-aware Backdoor Attack on VLMs for Visual Grounding",
      "submittedOnDailyBy": {
        "_id": "656ae4088fb1ddf0d5ec9ac5",
        "avatarUrl": "/avatars/e38468d2c0274f3c0f5732f30a2e3436.svg",
        "isPro": false,
        "fullname": "Junxian Li",
        "user": "Duke-de-Artois",
        "type": "user"
      },
      "summary": "Vision-language models (VLMs) have shown significant advancements in tasks\nsuch as visual grounding, where they localize specific objects in images based\non natural language queries and images. However, security issues in visual\ngrounding tasks for VLMs remain underexplored, especially in the context of\nbackdoor attacks. In this paper, we introduce a novel input-aware backdoor\nattack method, IAG, designed to manipulate the grounding behavior of VLMs. This\nattack forces the model to ground a specific target object in the input image,\nregardless of the user's query. We propose an adaptive trigger generator that\nembeds the semantic information of the attack target's description into the\noriginal image using a text-conditional U-Net, thereby overcoming the\nopen-vocabulary attack challenge. To ensure the attack's stealthiness, we\nutilize a reconstruction loss to minimize visual discrepancies between poisoned\nand clean images. Additionally, we introduce a unified method for generating\nattack data. IAG is evaluated theoretically and empirically, demonstrating its\nfeasibility and effectiveness. Notably, our ASR@0.5 on InternVL-2.5-8B reaches\nover 65\\% on various testing sets. IAG also shows promising potential on\nmanipulating Ferret-7B and LlaVA-1.5-7B with very little accuracy decrease on\nclean samples. Extensive specific experiments, such as ablation study and\npotential defense, also indicate the robustness and transferability of our\nattack.",
      "upvotes": 4,
      "discussionId": "689d555db083e610d741e9fd",
      "ai_summary": "A novel input-aware backdoor attack method, IAG, manipulates vision-language models to ground specific objects in images regardless of user queries, using a text-conditional U-Net and reconstruction loss to ensure stealthiness.",
      "ai_keywords": [
        "vision-language models",
        "visual grounding",
        "backdoor attacks",
        "input-aware backdoor attack",
        "adaptive trigger generator",
        "text-conditional U-Net",
        "reconstruction loss",
        "ASR@0.5",
        "InternVL-2.5-8B",
        "Ferret-7B",
        "LlaVA-1.5-7B",
        "ablation study",
        "potential defense"
      ]
    },
    "publishedAt": "2025-08-12T23:22:19.000Z",
    "title": "IAG: Input-aware Backdoor Attack on VLMs for Visual Grounding",
    "summary": "Vision-language models (VLMs) have shown significant advancements in tasks\nsuch as visual grounding, where they localize specific objects in images based\non natural language queries and images. However, security issues in visual\ngrounding tasks for VLMs remain underexplored, especially in the context of\nbackdoor attacks. In this paper, we introduce a novel input-aware backdoor\nattack method, IAG, designed to manipulate the grounding behavior of VLMs. This\nattack forces the model to ground a specific target object in the input image,\nregardless of the user's query. We propose an adaptive trigger generator that\nembeds the semantic information of the attack target's description into the\noriginal image using a text-conditional U-Net, thereby overcoming the\nopen-vocabulary attack challenge. To ensure the attack's stealthiness, we\nutilize a reconstruction loss to minimize visual discrepancies between poisoned\nand clean images. Additionally, we introduce a unified method for generating\nattack data. IAG is evaluated theoretically and empirically, demonstrating its\nfeasibility and effectiveness. Notably, our ASR@0.5 on InternVL-2.5-8B reaches\nover 65\\% on various testing sets. IAG also shows promising potential on\nmanipulating Ferret-7B and LlaVA-1.5-7B with very little accuracy decrease on\nclean samples. Extensive specific experiments, such as ablation study and\npotential defense, also indicate the robustness and transferability of our\nattack.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.09456.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "656ae4088fb1ddf0d5ec9ac5",
      "avatarUrl": "/avatars/e38468d2c0274f3c0f5732f30a2e3436.svg",
      "fullname": "Junxian Li",
      "name": "Duke-de-Artois",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.05613",
      "authors": [
        {
          "_id": "6895646848b0ae5ca2710d70",
          "name": "Haitao Hong",
          "hidden": false
        },
        {
          "_id": "6895646848b0ae5ca2710d71",
          "user": {
            "_id": "64098738342c26884c792c93",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64098738342c26884c792c93/SxBUd-wLrl-PjQsrVYJte.jpeg",
            "isPro": false,
            "fullname": "Yuchen Yan",
            "user": "yanyc",
            "type": "user"
          },
          "name": "Yuchen Yan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-11T06:50:10.151Z",
          "hidden": false
        },
        {
          "_id": "6895646848b0ae5ca2710d72",
          "name": "Xingyu Wu",
          "hidden": false
        },
        {
          "_id": "6895646848b0ae5ca2710d73",
          "name": "Guiyang Hou",
          "hidden": false
        },
        {
          "_id": "6895646848b0ae5ca2710d74",
          "name": "Wenqi Zhang",
          "hidden": false
        },
        {
          "_id": "6895646848b0ae5ca2710d75",
          "name": "Weiming Lu",
          "hidden": false
        },
        {
          "_id": "6895646848b0ae5ca2710d76",
          "user": {
            "_id": "5e1058e9fcf41d740b69966d",
            "avatarUrl": "/avatars/ce74839ba871f2b54313a670a233ba82.svg",
            "isPro": false,
            "fullname": "Yongliang Shen",
            "user": "tricktreat",
            "type": "user"
          },
          "name": "Yongliang Shen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-11T06:50:12.645Z",
          "hidden": false
        },
        {
          "_id": "6895646848b0ae5ca2710d77",
          "name": "Jun Xiao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-07T17:53:56.000Z",
      "submittedOnDailyAt": "2025-08-14T01:18:12.379Z",
      "title": "Cooper: Co-Optimizing Policy and Reward Models in Reinforcement Learning\n  for Large Language Models",
      "submittedOnDailyBy": {
        "_id": "64098738342c26884c792c93",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64098738342c26884c792c93/SxBUd-wLrl-PjQsrVYJte.jpeg",
        "isPro": false,
        "fullname": "Yuchen Yan",
        "user": "yanyc",
        "type": "user"
      },
      "summary": "Large language models (LLMs) have demonstrated remarkable performance in\nreasoning tasks, where reinforcement learning (RL) serves as a key algorithm\nfor enhancing their reasoning capabilities. Currently, there are two mainstream\nreward paradigms: model-based rewards and rule-based rewards. However, both\napproaches suffer from limitations: rule-based rewards lack robustness, while\nmodel-based rewards are vulnerable to reward hacking. To address these issues,\nwe propose Cooper(Co-optimizing Policy Model and Reward Model), a RL framework\nthat jointly optimizes both the policy model and the reward model. Cooper\nleverages the high precision of rule-based rewards when identifying correct\nresponses, and dynamically constructs and selects positive-negative sample\npairs for continued training the reward model. This design enhances robustness\nand mitigates the risk of reward hacking. To further support Cooper, we\nintroduce a hybrid annotation strategy that efficiently and accurately\ngenerates training data for the reward model. We also propose a reference-based\nreward modeling paradigm, where the reward model takes a reference answer as\ninput. Based on this design, we train a reward model named VerifyRM, which\nachieves higher accuracy on VerifyBench compared to other models of the same\nsize. We conduct reinforcement learning using both VerifyRM and Cooper. Our\nexperiments show that Cooper not only alleviates reward hacking but also\nimproves end-to-end RL performance, for instance, achieving a 0.54% gain in\naverage accuracy on Qwen2.5-1.5B-Instruct. Our findings demonstrate that\ndynamically updating reward model is an effective way to combat reward hacking,\nproviding a reference for better integrating reward models into RL.",
      "upvotes": 4,
      "discussionId": "6895646948b0ae5ca2710d78",
      "projectPage": "https://zju-real.github.io/cooper/",
      "githubRepo": "https://github.com/ZJU-REAL/cooper",
      "ai_summary": "A reinforcement learning framework jointly optimizes policy and reward models to enhance robustness and mitigate reward hacking in large language models.",
      "ai_keywords": [
        "reinforcement learning",
        "policy model",
        "reward model",
        "model-based rewards",
        "rule-based rewards",
        "reward hacking",
        "hybrid annotation strategy",
        "reference-based reward modeling",
        "VerifyRM",
        "VerifyBench",
        "Qwen2.5-1.5B-Instruct"
      ],
      "githubStars": 9
    },
    "publishedAt": "2025-08-07T13:53:56.000Z",
    "title": "Cooper: Co-Optimizing Policy and Reward Models in Reinforcement Learning\n  for Large Language Models",
    "summary": "Large language models (LLMs) have demonstrated remarkable performance in\nreasoning tasks, where reinforcement learning (RL) serves as a key algorithm\nfor enhancing their reasoning capabilities. Currently, there are two mainstream\nreward paradigms: model-based rewards and rule-based rewards. However, both\napproaches suffer from limitations: rule-based rewards lack robustness, while\nmodel-based rewards are vulnerable to reward hacking. To address these issues,\nwe propose Cooper(Co-optimizing Policy Model and Reward Model), a RL framework\nthat jointly optimizes both the policy model and the reward model. Cooper\nleverages the high precision of rule-based rewards when identifying correct\nresponses, and dynamically constructs and selects positive-negative sample\npairs for continued training the reward model. This design enhances robustness\nand mitigates the risk of reward hacking. To further support Cooper, we\nintroduce a hybrid annotation strategy that efficiently and accurately\ngenerates training data for the reward model. We also propose a reference-based\nreward modeling paradigm, where the reward model takes a reference answer as\ninput. Based on this design, we train a reward model named VerifyRM, which\nachieves higher accuracy on VerifyBench compared to other models of the same\nsize. We conduct reinforcement learning using both VerifyRM and Cooper. Our\nexperiments show that Cooper not only alleviates reward hacking but also\nimproves end-to-end RL performance, for instance, achieving a 0.54% gain in\naverage accuracy on Qwen2.5-1.5B-Instruct. Our findings demonstrate that\ndynamically updating reward model is an effective way to combat reward hacking,\nproviding a reference for better integrating reward models into RL.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.05613.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64098738342c26884c792c93",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64098738342c26884c792c93/SxBUd-wLrl-PjQsrVYJte.jpeg",
      "fullname": "Yuchen Yan",
      "name": "yanyc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2508.09968",
      "authors": [
        {
          "_id": "689d5ebbb083e610d741ea45",
          "name": "Luca Eyring",
          "hidden": false
        },
        {
          "_id": "689d5ebbb083e610d741ea46",
          "name": "Shyamgopal Karthik",
          "hidden": false
        },
        {
          "_id": "689d5ebbb083e610d741ea47",
          "name": "Alexey Dosovitskiy",
          "hidden": false
        },
        {
          "_id": "689d5ebbb083e610d741ea48",
          "name": "Nataniel Ruiz",
          "hidden": false
        },
        {
          "_id": "689d5ebbb083e610d741ea49",
          "name": "Zeynep Akata",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-13T17:33:37.000Z",
      "submittedOnDailyAt": "2025-08-14T02:29:12.254Z",
      "title": "Noise Hypernetworks: Amortizing Test-Time Compute in Diffusion Models",
      "submittedOnDailyBy": {
        "_id": "6254599b6e36fe62e141c8f9",
        "avatarUrl": "/avatars/08d6b68b92c7bfd0a8be022ba9f2f289.svg",
        "isPro": false,
        "fullname": "Shyamgopal Karthik",
        "user": "shyamgopal",
        "type": "user"
      },
      "summary": "The new paradigm of test-time scaling has yielded remarkable breakthroughs in\nLarge Language Models (LLMs) (e.g. reasoning models) and in generative vision\nmodels, allowing models to allocate additional computation during inference to\neffectively tackle increasingly complex problems. Despite the improvements of\nthis approach, an important limitation emerges: the substantial increase in\ncomputation time makes the process slow and impractical for many applications.\nGiven the success of this paradigm and its growing usage, we seek to preserve\nits benefits while eschewing the inference overhead. In this work we propose\none solution to the critical problem of integrating test-time scaling knowledge\ninto a model during post-training. Specifically, we replace reward guided\ntest-time noise optimization in diffusion models with a Noise Hypernetwork that\nmodulates initial input noise. We propose a theoretically grounded framework\nfor learning this reward-tilted distribution for distilled generators, through\na tractable noise-space objective that maintains fidelity to the base model\nwhile optimizing for desired characteristics. We show that our approach\nrecovers a substantial portion of the quality gains from explicit test-time\noptimization at a fraction of the computational cost. Code is available at\nhttps://github.com/ExplainableML/HyperNoise",
      "upvotes": 2,
      "discussionId": "689d5ebcb083e610d741ea4a",
      "ai_summary": "A Noise Hypernetwork is introduced to integrate test-time scaling knowledge into diffusion models, reducing computational cost while maintaining quality.",
      "ai_keywords": [
        "test-time scaling",
        "Large Language Models",
        "generative vision models",
        "diffusion models",
        "reward guided test-time noise optimization",
        "Noise Hypernetwork",
        "noise-space objective",
        "distilled generators"
      ]
    },
    "publishedAt": "2025-08-13T13:33:37.000Z",
    "title": "Noise Hypernetworks: Amortizing Test-Time Compute in Diffusion Models",
    "summary": "The new paradigm of test-time scaling has yielded remarkable breakthroughs in\nLarge Language Models (LLMs) (e.g. reasoning models) and in generative vision\nmodels, allowing models to allocate additional computation during inference to\neffectively tackle increasingly complex problems. Despite the improvements of\nthis approach, an important limitation emerges: the substantial increase in\ncomputation time makes the process slow and impractical for many applications.\nGiven the success of this paradigm and its growing usage, we seek to preserve\nits benefits while eschewing the inference overhead. In this work we propose\none solution to the critical problem of integrating test-time scaling knowledge\ninto a model during post-training. Specifically, we replace reward guided\ntest-time noise optimization in diffusion models with a Noise Hypernetwork that\nmodulates initial input noise. We propose a theoretically grounded framework\nfor learning this reward-tilted distribution for distilled generators, through\na tractable noise-space objective that maintains fidelity to the base model\nwhile optimizing for desired characteristics. We show that our approach\nrecovers a substantial portion of the quality gains from explicit test-time\noptimization at a fraction of the computational cost. Code is available at\nhttps://github.com/ExplainableML/HyperNoise",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.09968.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6254599b6e36fe62e141c8f9",
      "avatarUrl": "/avatars/08d6b68b92c7bfd0a8be022ba9f2f289.svg",
      "fullname": "Shyamgopal Karthik",
      "name": "shyamgopal",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.09776",
      "authors": [
        {
          "_id": "689d93dab083e610d741ead0",
          "name": "Mahdi Dhaini",
          "hidden": false
        },
        {
          "_id": "689d93dab083e610d741ead1",
          "name": "Juraj Vladika",
          "hidden": false
        },
        {
          "_id": "689d93dab083e610d741ead2",
          "name": "Ege Erdogan",
          "hidden": false
        },
        {
          "_id": "689d93dab083e610d741ead3",
          "name": "Zineb Attaoui",
          "hidden": false
        },
        {
          "_id": "689d93dab083e610d741ead4",
          "name": "Gjergji Kasneci",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-13T12:59:08.000Z",
      "submittedOnDailyAt": "2025-08-14T06:15:45.702Z",
      "title": "Can LLM-Generated Textual Explanations Enhance Model Classification\n  Performance? An Empirical Study",
      "submittedOnDailyBy": {
        "_id": "64ccf54986d8dc0caa7d1f64",
        "avatarUrl": "/avatars/ba47e70e4bc07eb2ba6b648192a15243.svg",
        "isPro": false,
        "fullname": "Mahdi Dhaini",
        "user": "mdhaini",
        "type": "user"
      },
      "summary": "In the rapidly evolving field of Explainable Natural Language Processing\n(NLP), textual explanations, i.e., human-like rationales, are pivotal for\nexplaining model predictions and enriching datasets with interpretable labels.\nTraditional approaches rely on human annotation, which is costly,\nlabor-intensive, and impedes scalability. In this work, we present an automated\nframework that leverages multiple state-of-the-art large language models (LLMs)\nto generate high-quality textual explanations. We rigorously assess the quality\nof these LLM-generated explanations using a comprehensive suite of Natural\nLanguage Generation (NLG) metrics. Furthermore, we investigate the downstream\nimpact of these explanations on the performance of pre-trained language models\n(PLMs) and LLMs across natural language inference tasks on two diverse\nbenchmark datasets. Our experiments demonstrate that automated explanations\nexhibit highly competitive effectiveness compared to human-annotated\nexplanations in improving model performance. Our findings underscore a\npromising avenue for scalable, automated LLM-based textual explanation\ngeneration for extending NLP datasets and enhancing model performance.",
      "upvotes": 0,
      "discussionId": "689d93dab083e610d741ead5",
      "ai_summary": "Automated generation of textual explanations using large language models improves model performance in natural language inference tasks, offering a scalable alternative to human annotation.",
      "ai_keywords": [
        "Explainable Natural Language Processing",
        "textual explanations",
        "large language models",
        "Natural Language Generation",
        "pre-trained language models",
        "natural language inference",
        "benchmark datasets"
      ]
    },
    "publishedAt": "2025-08-13T08:59:08.000Z",
    "title": "Can LLM-Generated Textual Explanations Enhance Model Classification\n  Performance? An Empirical Study",
    "summary": "In the rapidly evolving field of Explainable Natural Language Processing\n(NLP), textual explanations, i.e., human-like rationales, are pivotal for\nexplaining model predictions and enriching datasets with interpretable labels.\nTraditional approaches rely on human annotation, which is costly,\nlabor-intensive, and impedes scalability. In this work, we present an automated\nframework that leverages multiple state-of-the-art large language models (LLMs)\nto generate high-quality textual explanations. We rigorously assess the quality\nof these LLM-generated explanations using a comprehensive suite of Natural\nLanguage Generation (NLG) metrics. Furthermore, we investigate the downstream\nimpact of these explanations on the performance of pre-trained language models\n(PLMs) and LLMs across natural language inference tasks on two diverse\nbenchmark datasets. Our experiments demonstrate that automated explanations\nexhibit highly competitive effectiveness compared to human-annotated\nexplanations in improving model performance. Our findings underscore a\npromising avenue for scalable, automated LLM-based textual explanation\ngeneration for extending NLP datasets and enhancing model performance.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.09776.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ccf54986d8dc0caa7d1f64",
      "avatarUrl": "/avatars/ba47e70e4bc07eb2ba6b648192a15243.svg",
      "fullname": "Mahdi Dhaini",
      "name": "mdhaini",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  }
]