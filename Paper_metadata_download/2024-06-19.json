[
    {
        "paper": {
            "id": "2406.09760",
            "authors": [
                {
                    "_id": "66727d57a8a27a82f40dc47b",
                    "user": {
                        "avatarUrl": "/avatars/c2f815e5d06cc6e3cd3f9cae32a3e846.svg",
                        "isPro": false,
                        "fullname": "chenchangyu",
                        "user": "chenchangyu",
                        "type": "user"
                    },
                    "name": "Changyu Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-19T08:31:45.686Z",
                    "hidden": false
                },
                {
                    "_id": "66727d57a8a27a82f40dc47c",
                    "name": "Zichen Liu",
                    "hidden": false
                },
                {
                    "_id": "66727d57a8a27a82f40dc47d",
                    "name": "Chao Du",
                    "hidden": false
                },
                {
                    "_id": "66727d57a8a27a82f40dc47e",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675921369867-63d91b6d255ef6add20e1b38.jpeg",
                        "isPro": false,
                        "fullname": "Tianyu Pang",
                        "user": "P2333",
                        "type": "user"
                    },
                    "name": "Tianyu Pang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-19T08:36:46.369Z",
                    "hidden": false
                },
                {
                    "_id": "66727d57a8a27a82f40dc47f",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/612ee6a7b960e78c6d2319d4/2Hu9BaAyXbyh1vt0v1Qui.jpeg",
                        "isPro": false,
                        "fullname": "Qian Liu",
                        "user": "SivilTaram",
                        "type": "user"
                    },
                    "name": "Qian Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-19T07:37:22.994Z",
                    "hidden": false
                },
                {
                    "_id": "66727d57a8a27a82f40dc480",
                    "name": "Arunesh Sinha",
                    "hidden": false
                },
                {
                    "_id": "66727d57a8a27a82f40dc481",
                    "user": {
                        "avatarUrl": "/avatars/404d682fea0dafa9267c00d067b5b26c.svg",
                        "isPro": false,
                        "fullname": "Pradeep varakantham",
                        "user": "Pradeepv",
                        "type": "user"
                    },
                    "name": "Pradeep Varakantham",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-19T08:36:55.687Z",
                    "hidden": false
                },
                {
                    "_id": "66727d57a8a27a82f40dc482",
                    "name": "Min Lin",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-14T06:57:18.000Z",
            "title": "Bootstrapping Language Models with DPO Implicit Rewards",
            "summary": "Human alignment in large language models (LLMs) is an active area of\nresearch. A recent groundbreaking work, direct preference optimization (DPO),\nhas greatly simplified the process from past work in reinforcement learning\nfrom human feedback (RLHF) by bypassing the reward learning stage in RLHF. DPO,\nafter training, provides an implicit reward model. In this work, we make a\nnovel observation that this implicit reward model can by itself be used in a\nbootstrapping fashion to further align the LLM. Our approach is to use the\nrewards from a current LLM model to construct a preference dataset, which is\nthen used in subsequent DPO rounds. We incorporate refinements that debias the\nlength of the responses and improve the quality of the preference dataset to\nfurther improve our approach. Our approach, named self-alignment with DPO\nImpliCit rEwards (DICE), shows great improvements in alignment and achieves\nsuperior performance than Gemini Pro on AlpacaEval 2, reaching 27.55%\nlength-controlled win rate against GPT-4 Turbo, but with only 8B parameters and\nno external feedback. Our code is available at https://github.com/sail-sg/dice.",
            "upvotes": 26
        },
        "publishedAt": "2024-06-19T05:11:41.787Z",
        "title": "Bootstrapping Language Models with DPO Implicit Rewards",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/612ee6a7b960e78c6d2319d4/mdGqksI70oHY01Rz2EidK.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.09760.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/612ee6a7b960e78c6d2319d4/2Hu9BaAyXbyh1vt0v1Qui.jpeg",
            "fullname": "Qian Liu",
            "name": "SivilTaram",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.11931",
            "authors": [
                {
                    "_id": "66724bb98b5e22ce4f66df3e",
                    "name": "DeepSeek-AI",
                    "hidden": false
                },
                {
                    "_id": "66724bb98b5e22ce4f66df3f",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63cd76b4374057a338e8e703/i4Qk5-0aYx3oRhC8b50aJ.jpeg",
                        "isPro": false,
                        "fullname": "zhuqihao",
                        "user": "zqh11",
                        "type": "user"
                    },
                    "name": "Qihao Zhu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-19T08:26:08.420Z",
                    "hidden": false
                },
                {
                    "_id": "66724bb98b5e22ce4f66df40",
                    "user": {
                        "avatarUrl": "/avatars/12b27ce2c59f53b7e464039deab36a5d.svg",
                        "isPro": false,
                        "fullname": "Daya Guo",
                        "user": "guoday",
                        "type": "user"
                    },
                    "name": "Daya Guo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-19T08:26:15.136Z",
                    "hidden": false
                },
                {
                    "_id": "66724bb98b5e22ce4f66df41",
                    "user": {
                        "avatarUrl": "/avatars/753e9f980eb6786c6b53b2f1becbf745.svg",
                        "isPro": false,
                        "fullname": "Zhihong Shao",
                        "user": "ZhihongShao",
                        "type": "user"
                    },
                    "name": "Zhihong Shao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-19T08:26:29.442Z",
                    "hidden": false
                },
                {
                    "_id": "66724bb98b5e22ce4f66df42",
                    "user": {
                        "avatarUrl": "/avatars/99c99ced2461978df572c27c1b3a4904.svg",
                        "isPro": false,
                        "fullname": "DejianYang",
                        "user": "DejianYang",
                        "type": "user"
                    },
                    "name": "Dejian Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-19T08:26:41.738Z",
                    "hidden": false
                },
                {
                    "_id": "66724bb98b5e22ce4f66df43",
                    "user": {
                        "avatarUrl": "/avatars/7a085da2e2a91d7f41988501a573ebf9.svg",
                        "isPro": false,
                        "fullname": "PEIYI, WANG",
                        "user": "peiyiwang89",
                        "type": "user"
                    },
                    "name": "Peiyi Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-19T08:26:48.355Z",
                    "hidden": false
                },
                {
                    "_id": "66724bb98b5e22ce4f66df44",
                    "name": "Runxin Xu",
                    "hidden": false
                },
                {
                    "_id": "66724bb98b5e22ce4f66df45",
                    "name": "Y. Wu",
                    "hidden": false
                },
                {
                    "_id": "66724bb98b5e22ce4f66df46",
                    "user": {
                        "avatarUrl": "/avatars/b2524700c4fe02c8fbcabdd0b4a0f587.svg",
                        "isPro": false,
                        "fullname": "yukun li",
                        "user": "imli",
                        "type": "user"
                    },
                    "name": "Yukun Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-19T08:27:08.329Z",
                    "hidden": false
                },
                {
                    "_id": "66724bb98b5e22ce4f66df47",
                    "user": {
                        "avatarUrl": "/avatars/0fa1eb6ac6c1aeff3e65bc86a6617f64.svg",
                        "isPro": false,
                        "fullname": "Huazuo Gao",
                        "user": "gaohuazuo",
                        "type": "user"
                    },
                    "name": "Huazuo Gao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-19T08:27:14.990Z",
                    "hidden": false
                },
                {
                    "_id": "66724bb98b5e22ce4f66df48",
                    "user": {
                        "avatarUrl": "/avatars/b26faf19ba1493b91102ac7978ab3230.svg",
                        "isPro": false,
                        "fullname": "Shirong Ma",
                        "user": "msr2000",
                        "type": "user"
                    },
                    "name": "Shirong Ma",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-19T08:27:32.842Z",
                    "hidden": false
                },
                {
                    "_id": "66724bb98b5e22ce4f66df49",
                    "user": {
                        "avatarUrl": "/avatars/fdbff8012fe80d5f2e861cbfa2675125.svg",
                        "isPro": false,
                        "fullname": "wangding zeng",
                        "user": "zwd973-deepseek",
                        "type": "user"
                    },
                    "name": "Wangding Zeng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-19T08:27:39.188Z",
                    "hidden": false
                },
                {
                    "_id": "66724bb98b5e22ce4f66df4a",
                    "name": "Xiao Bi",
                    "hidden": false
                },
                {
                    "_id": "66724bb98b5e22ce4f66df4b",
                    "name": "Zihui Gu",
                    "hidden": false
                },
                {
                    "_id": "66724bb98b5e22ce4f66df4c",
                    "name": "Hanwei Xu",
                    "hidden": false
                },
                {
                    "_id": "66724bb98b5e22ce4f66df4d",
                    "user": {
                        "avatarUrl": "/avatars/896ed9f4cdbd317493b303d070b7e12a.svg",
                        "isPro": false,
                        "fullname": "Damai Dai",
                        "user": "DeepSeekDDM",
                        "type": "user"
                    },
                    "name": "Damai Dai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-19T08:28:06.857Z",
                    "hidden": false
                },
                {
                    "_id": "66724bb98b5e22ce4f66df4e",
                    "name": "Kai Dong",
                    "hidden": false
                },
                {
                    "_id": "66724bb98b5e22ce4f66df4f",
                    "name": "Liyue Zhang",
                    "hidden": false
                },
                {
                    "_id": "66724bb98b5e22ce4f66df50",
                    "name": "Yishi Piao",
                    "hidden": false
                },
                {
                    "_id": "66724bb98b5e22ce4f66df51",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62dcf5d4169bd1d2ef2ca724/oRFFmJDJTLYtPRVPCweQ_.jpeg",
                        "isPro": false,
                        "fullname": "Zhibin Gou",
                        "user": "zubingou",
                        "type": "user"
                    },
                    "name": "Zhibin Gou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-19T07:38:03.852Z",
                    "hidden": false
                },
                {
                    "_id": "66724bb98b5e22ce4f66df52",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6539f6ea26df26ecd1393c37/9VJusLLAiLhUxfBAFggpF.jpeg",
                        "isPro": false,
                        "fullname": "Zhenda Xie",
                        "user": "zdaxie",
                        "type": "user"
                    },
                    "name": "Zhenda Xie",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-19T08:28:37.958Z",
                    "hidden": false
                },
                {
                    "_id": "66724bb98b5e22ce4f66df53",
                    "name": "Zhewen Hao",
                    "hidden": false
                },
                {
                    "_id": "66724bb98b5e22ce4f66df54",
                    "user": {
                        "avatarUrl": "/avatars/07fcf56b5b8a0b64c31bdfe8fbf41cc6.svg",
                        "isPro": false,
                        "fullname": "Bingxuan Wang",
                        "user": "YellowDoge",
                        "type": "user"
                    },
                    "name": "Bingxuan Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-19T08:28:51.550Z",
                    "hidden": false
                },
                {
                    "_id": "66724bb98b5e22ce4f66df55",
                    "user": {
                        "avatarUrl": "/avatars/f5c5441ba74791b64c9740911f952bac.svg",
                        "isPro": false,
                        "fullname": "Junxiao Song",
                        "user": "haha-point",
                        "type": "user"
                    },
                    "name": "Junxiao Song",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-19T08:28:58.526Z",
                    "hidden": false
                },
                {
                    "_id": "66724bb98b5e22ce4f66df56",
                    "user": {
                        "avatarUrl": "/avatars/e131cd22b55e4b62cc48779e512c3da1.svg",
                        "isPro": false,
                        "fullname": "deli chen",
                        "user": "deli96",
                        "type": "user"
                    },
                    "name": "Deli Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-19T08:29:05.063Z",
                    "hidden": false
                },
                {
                    "_id": "66724bb98b5e22ce4f66df57",
                    "name": "Xin Xie",
                    "hidden": false
                },
                {
                    "_id": "66724bb98b5e22ce4f66df58",
                    "name": "Kang Guan",
                    "hidden": false
                },
                {
                    "_id": "66724bb98b5e22ce4f66df59",
                    "name": "Yuxiang You",
                    "hidden": false
                },
                {
                    "_id": "66724bb98b5e22ce4f66df5a",
                    "name": "Aixin Liu",
                    "hidden": false
                },
                {
                    "_id": "66724bb98b5e22ce4f66df5b",
                    "name": "Qiushi Du",
                    "hidden": false
                },
                {
                    "_id": "66724bb98b5e22ce4f66df5c",
                    "name": "Wenjun Gao",
                    "hidden": false
                },
                {
                    "_id": "66724bb98b5e22ce4f66df5d",
                    "name": "Xuan Lu",
                    "hidden": false
                },
                {
                    "_id": "66724bb98b5e22ce4f66df5e",
                    "name": "Qinyu Chen",
                    "hidden": false
                },
                {
                    "_id": "66724bb98b5e22ce4f66df5f",
                    "name": "Yaohui Wang",
                    "hidden": false
                },
                {
                    "_id": "66724bb98b5e22ce4f66df60",
                    "name": "Chengqi Deng",
                    "hidden": false
                },
                {
                    "_id": "66724bb98b5e22ce4f66df61",
                    "user": {
                        "avatarUrl": "/avatars/ae01ac0296d6ce1277dacb6894f570b8.svg",
                        "isPro": false,
                        "fullname": "Jiashi Li",
                        "user": "Beginlner",
                        "type": "user"
                    },
                    "name": "Jiashi Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-19T08:29:13.424Z",
                    "hidden": false
                },
                {
                    "_id": "66724bb98b5e22ce4f66df62",
                    "name": "Chenggang Zhao",
                    "hidden": false
                },
                {
                    "_id": "66724bb98b5e22ce4f66df63",
                    "name": "Chong Ruan",
                    "hidden": false
                },
                {
                    "_id": "66724bb98b5e22ce4f66df64",
                    "name": "Fuli Luo",
                    "hidden": false
                },
                {
                    "_id": "66724bb98b5e22ce4f66df65",
                    "name": "Wenfeng Liang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-17T13:51:35.000Z",
            "title": "DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code\n  Intelligence",
            "summary": "We present DeepSeek-Coder-V2, an open-source Mixture-of-Experts (MoE) code\nlanguage model that achieves performance comparable to GPT4-Turbo in\ncode-specific tasks. Specifically, DeepSeek-Coder-V2 is further pre-trained\nfrom an intermediate checkpoint of DeepSeek-V2 with additional 6 trillion\ntokens. Through this continued pre-training, DeepSeek-Coder-V2 substantially\nenhances the coding and mathematical reasoning capabilities of DeepSeek-V2,\nwhile maintaining comparable performance in general language tasks. Compared to\nDeepSeek-Coder-33B, DeepSeek-Coder-V2 demonstrates significant advancements in\nvarious aspects of code-related tasks, as well as reasoning and general\ncapabilities. Additionally, DeepSeek-Coder-V2 expands its support for\nprogramming languages from 86 to 338, while extending the context length from\n16K to 128K. In standard benchmark evaluations, DeepSeek-Coder-V2 achieves\nsuperior performance compared to closed-source models such as GPT4-Turbo,\nClaude 3 Opus, and Gemini 1.5 Pro in coding and math benchmarks.",
            "upvotes": 25
        },
        "publishedAt": "2024-06-19T01:39:01.494Z",
        "title": "DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.11931.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.12246",
            "authors": [
                {
                    "_id": "6672470286f6b3d1267ddce3",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657152eb12f162153b50ec9d/qnldHP35PclV0pDz_05q8.jpeg",
                        "isPro": false,
                        "fullname": "Byung-Kwan Lee",
                        "user": "BK-Lee",
                        "type": "user"
                    },
                    "name": "Byung-Kwan Lee",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-19T07:38:10.524Z",
                    "hidden": false
                },
                {
                    "_id": "6672470286f6b3d1267ddce4",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65a4bf8e90b5e87bcdff41c7/I6OSoZigV7Fl6OKRl_Yqy.jpeg",
                        "isPro": false,
                        "fullname": "Sangyun Chung",
                        "user": "topyun",
                        "type": "user"
                    },
                    "name": "Sangyun Chung",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-19T08:09:46.674Z",
                    "hidden": false
                },
                {
                    "_id": "6672470286f6b3d1267ddce5",
                    "name": "Chae Won Kim",
                    "hidden": false
                },
                {
                    "_id": "6672470286f6b3d1267ddce6",
                    "user": {
                        "avatarUrl": "/avatars/a930c1f20fa1eef372ca92139cb32887.svg",
                        "isPro": false,
                        "fullname": "Beomchan Park",
                        "user": "bpbpbp0810",
                        "type": "user"
                    },
                    "name": "Beomchan Park",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-19T07:38:08.480Z",
                    "hidden": false
                },
                {
                    "_id": "6672470286f6b3d1267ddce7",
                    "user": {
                        "avatarUrl": "/avatars/0f54479afcfc19df00b25d5aedb4cf67.svg",
                        "isPro": false,
                        "fullname": "Yong Man Ro",
                        "user": "dwightro",
                        "type": "user"
                    },
                    "name": "Yong Man Ro",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-19T08:10:04.741Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-18T03:42:00.000Z",
            "title": "TroL: Traversal of Layers for Large Language and Vision Models",
            "summary": "Large language and vision models (LLVMs) have been driven by the\ngeneralization power of large language models (LLMs) and the advent of visual\ninstruction tuning. Along with scaling them up directly, these models enable\nLLVMs to showcase powerful vision language (VL) performances by covering\ndiverse tasks via natural language instructions. However, existing open-source\nLLVMs that perform comparably to closed-source LLVMs such as GPT-4V are often\nconsidered too large (e.g., 26B, 34B, and 110B parameters), having a larger\nnumber of layers. These large models demand costly, high-end resources for both\ntraining and inference. To address this issue, we present a new efficient LLVM\nfamily with 1.8B, 3.8B, and 7B LLM model sizes, Traversal of Layers (TroL),\nwhich enables the reuse of layers in a token-wise manner. This layer traversing\ntechnique simulates the effect of looking back and retracing the answering\nstream while increasing the number of forward propagation layers without\nphysically adding more layers. We demonstrate that TroL employs a simple layer\ntraversing approach yet efficiently outperforms the open-source LLVMs with\nlarger model sizes and rivals the performances of the closed-source LLVMs with\nsubstantial sizes.",
            "upvotes": 21
        },
        "publishedAt": "2024-06-19T01:21:45.054Z",
        "title": "TroL: Traversal of Layers for Large Language and Vision Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.12246.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657152eb12f162153b50ec9d/qnldHP35PclV0pDz_05q8.jpeg",
            "fullname": "Byung-Kwan Lee",
            "name": "BK-Lee",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.12849",
            "authors": [
                {
                    "_id": "6672e9972376de4b7ab47afd",
                    "user": {
                        "avatarUrl": "/avatars/4df13f878599bc7310ddecdb29a381b0.svg",
                        "isPro": false,
                        "fullname": "Albert Wang",
                        "user": "Albert-NHWang",
                        "type": "user"
                    },
                    "name": "Ning-Hsu Wang",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2024-06-19T14:36:18.321Z",
                    "hidden": false
                },
                {
                    "_id": "6672e9972376de4b7ab47afe",
                    "name": "Yu-Lun Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-18T17:59:31.000Z",
            "title": "Depth Anywhere: Enhancing 360 Monocular Depth Estimation via Perspective\n  Distillation and Unlabeled Data Augmentation",
            "summary": "Accurately estimating depth in 360-degree imagery is crucial for virtual\nreality, autonomous navigation, and immersive media applications. Existing\ndepth estimation methods designed for perspective-view imagery fail when\napplied to 360-degree images due to different camera projections and\ndistortions, whereas 360-degree methods perform inferior due to the lack of\nlabeled data pairs. We propose a new depth estimation framework that utilizes\nunlabeled 360-degree data effectively. Our approach uses state-of-the-art\nperspective depth estimation models as teacher models to generate pseudo labels\nthrough a six-face cube projection technique, enabling efficient labeling of\ndepth in 360-degree images. This method leverages the increasing availability\nof large datasets. Our approach includes two main stages: offline mask\ngeneration for invalid regions and an online semi-supervised joint training\nregime. We tested our approach on benchmark datasets such as Matterport3D and\nStanford2D3D, showing significant improvements in depth estimation accuracy,\nparticularly in zero-shot scenarios. Our proposed training pipeline can enhance\nany 360 monocular depth estimator and demonstrates effective knowledge transfer\nacross different camera projections and data types. See our project page for\nresults: https://albert100121.github.io/Depth-Anywhere/",
            "upvotes": 19
        },
        "publishedAt": "2024-06-19T12:53:28.582Z",
        "title": "Depth Anywhere: Enhancing 360 Monocular Depth Estimation via Perspective Distillation and Unlabeled Data Augmentation",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/aHW1kuO1-XkbFyio7W4pt.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.12849.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
            "fullname": "Yu-Lun Liu",
            "name": "yulunliu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.12275",
            "authors": [
                {
                    "_id": "66724f16cec66c5343fb5915",
                    "user": {
                        "avatarUrl": "/avatars/275302f703e0ccdac558f68a3d4e8a1f.svg",
                        "isPro": false,
                        "fullname": "Xubing Ye",
                        "user": "YxxxB",
                        "type": "user"
                    },
                    "name": "Xubing Ye",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-19T07:38:01.036Z",
                    "hidden": false
                },
                {
                    "_id": "66724f16cec66c5343fb5916",
                    "user": {
                        "avatarUrl": "/avatars/e46fba100dede1f0afbf615154b4e63d.svg",
                        "isPro": false,
                        "fullname": "YukangGan",
                        "user": "ganyk",
                        "type": "user"
                    },
                    "name": "Yukang Gan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-19T08:37:45.720Z",
                    "hidden": false
                },
                {
                    "_id": "66724f16cec66c5343fb5917",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63318b2349a9563915469f3b/zlbeB2997i8YkoyOTb9FL.jpeg",
                        "isPro": false,
                        "fullname": "Xiaoke Huang",
                        "user": "xk-huang",
                        "type": "user"
                    },
                    "name": "Xiaoke Huang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-19T07:37:49.199Z",
                    "hidden": false
                },
                {
                    "_id": "66724f16cec66c5343fb5918",
                    "user": {
                        "avatarUrl": "/avatars/81da37d628163fe3e094b247c7c3a3b5.svg",
                        "isPro": false,
                        "fullname": "Yixiao Ge",
                        "user": "yxgeee",
                        "type": "user"
                    },
                    "name": "Yixiao Ge",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-19T08:37:51.818Z",
                    "hidden": false
                },
                {
                    "_id": "66724f16cec66c5343fb5919",
                    "name": "Ying Shan",
                    "hidden": false
                },
                {
                    "_id": "66724f16cec66c5343fb591a",
                    "name": "Yansong Tang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-18T05:05:12.000Z",
            "title": "VoCo-LLaMA: Towards Vision Compression with Large Language Models",
            "summary": "Vision-Language Models (VLMs) have achieved remarkable success in various\nmulti-modal tasks, but they are often bottlenecked by the limited context\nwindow and high computational cost of processing high-resolution image inputs\nand videos. Vision compression can alleviate this problem by reducing the\nvision token count. Previous approaches compress vision tokens with external\nmodules and force LLMs to understand the compressed ones, leading to visual\ninformation loss. However, the LLMs' understanding paradigm of vision tokens is\nnot fully utilised in the compression learning process. We propose VoCo-LLaMA,\nthe first approach to compress vision tokens using LLMs. By introducing Vision\nCompression tokens during the vision instruction tuning phase and leveraging\nattention distillation, our method distill how LLMs comprehend vision tokens\ninto their processing of VoCo tokens. VoCo-LLaMA facilitates effective vision\ncompression and improves the computational efficiency during the inference\nstage. Specifically, our method achieves minimal performance loss with a\ncompression ratio of 576times, resulting in up to 94.8% fewer FLOPs and\n69.6% acceleration in inference time. Furthermore, through continuous\ntraining using time-series compressed token sequences of video frames,\nVoCo-LLaMA demonstrates the ability to understand temporal correlations,\noutperforming previous methods on popular video question-answering benchmarks.\nOur approach presents a promising way to unlock the full potential of VLMs'\ncontextual window, enabling more scalable multi-modal applications. The project\npage, along with the associated code, can be accessed via\nhttps://yxxxb.github.io/VoCo-LLaMA-page/{this https URL}.",
            "upvotes": 16
        },
        "publishedAt": "2024-06-19T01:53:19.787Z",
        "title": "VoCo-LLaMA: Towards Vision Compression with Large Language Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.12275.png",
        "numComments": 4,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.12793",
            "authors": [
                {
                    "_id": "66724df48171db46e7e4ab53",
                    "name": "Team GLM",
                    "hidden": false
                },
                {
                    "_id": "66724df48171db46e7e4ab55",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1659772051636-62dc173789b4cf157d36ebee.jpeg",
                        "isPro": false,
                        "fullname": "Zeng Aohan",
                        "user": "Sengxian",
                        "type": "user"
                    },
                    "name": "Aohan Zeng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-19T08:41:44.879Z",
                    "hidden": false
                },
                {
                    "_id": "66724df48171db46e7e4ab56",
                    "name": "Bin Xu",
                    "hidden": false
                },
                {
                    "_id": "66724df48171db46e7e4ab57",
                    "name": "Bowen Wang",
                    "hidden": false
                },
                {
                    "_id": "66724df48171db46e7e4ab58",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/CVDqDeJ_fLTULhCTTSogb.png",
                        "isPro": true,
                        "fullname": "Chenhui Zhang",
                        "user": "danielz01",
                        "type": "user"
                    },
                    "name": "Chenhui Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-19T08:42:10.316Z",
                    "hidden": false
                },
                {
                    "_id": "66724df48171db46e7e4ab59",
                    "user": {
                        "avatarUrl": "/avatars/c52d7150b4de6a2eb2d83b345d35cbc2.svg",
                        "isPro": false,
                        "fullname": "Da Yin",
                        "user": "DaYin",
                        "type": "user"
                    },
                    "name": "Da Yin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-19T08:42:24.927Z",
                    "hidden": false
                },
                {
                    "_id": "66724df48171db46e7e4ab5a",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/4TBIdLkQTO05YqOYqvi_L.jpeg",
                        "isPro": false,
                        "fullname": "Diego Rojas",
                        "user": "d4ndres",
                        "type": "user"
                    },
                    "name": "Diego Rojas",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-19T08:42:31.177Z",
                    "hidden": false
                },
                {
                    "_id": "66724df48171db46e7e4ab5b",
                    "user": {
                        "avatarUrl": "/avatars/3db57c8b643fba8302ed39d8cf0f4ddb.svg",
                        "isPro": false,
                        "fullname": "Guanyu Feng",
                        "user": "jiguanglizipao",
                        "type": "user"
                    },
                    "name": "Guanyu Feng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-19T08:42:37.172Z",
                    "hidden": false
                },
                {
                    "_id": "66724df48171db46e7e4ab5c",
                    "user": {
                        "avatarUrl": "/avatars/0735b857db37e59018fb5aa1dd231000.svg",
                        "isPro": false,
                        "fullname": "zhaohanlin",
                        "user": "ultrazhl",
                        "type": "user"
                    },
                    "name": "Hanlin Zhao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-19T08:42:49.585Z",
                    "hidden": false
                },
                {
                    "_id": "66724df48171db46e7e4ab5d",
                    "user": {
                        "avatarUrl": "/avatars/89b215dafa503b51ab212a9b63c82aca.svg",
                        "isPro": false,
                        "fullname": "Hanyu Lai",
                        "user": "hanyullai",
                        "type": "user"
                    },
                    "name": "Hanyu Lai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-19T08:43:16.931Z",
                    "hidden": false
                },
                {
                    "_id": "66724df48171db46e7e4ab5e",
                    "name": "Hao Yu",
                    "hidden": false
                },
                {
                    "_id": "66724df48171db46e7e4ab5f",
                    "name": "Hongning Wang",
                    "hidden": false
                },
                {
                    "_id": "66724df48171db46e7e4ab60",
                    "user": {
                        "avatarUrl": "/avatars/942e6008e8e78583c2750d678fcaadb1.svg",
                        "isPro": false,
                        "fullname": "jiadaisun",
                        "user": "jiadaisun",
                        "type": "user"
                    },
                    "name": "Jiadai Sun",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-19T08:43:35.519Z",
                    "hidden": false
                },
                {
                    "_id": "66724df48171db46e7e4ab61",
                    "name": "Jiajie Zhang",
                    "hidden": false
                },
                {
                    "_id": "66724df48171db46e7e4ab62",
                    "user": {
                        "avatarUrl": "/avatars/c41ae68b406e90fc66c879c1bb50c11d.svg",
                        "isPro": false,
                        "fullname": "Jiale Cheng",
                        "user": "Jialejl",
                        "type": "user"
                    },
                    "name": "Jiale Cheng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-19T08:43:42.588Z",
                    "hidden": false
                },
                {
                    "_id": "66724df48171db46e7e4ab63",
                    "user": {
                        "avatarUrl": "/avatars/bfd87287536ef53ec555c0cc7a25ac72.svg",
                        "isPro": false,
                        "fullname": "Jiayi Gui",
                        "user": "Hypatia",
                        "type": "user"
                    },
                    "name": "Jiayi Gui",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-19T08:43:48.199Z",
                    "hidden": false
                },
                {
                    "_id": "66724df48171db46e7e4ab64",
                    "user": {
                        "avatarUrl": "/avatars/1b4591c7322d649c797b3125148f1915.svg",
                        "isPro": false,
                        "fullname": "Jie Tang",
                        "user": "jerytang",
                        "type": "user"
                    },
                    "name": "Jie Tang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-19T08:44:21.395Z",
                    "hidden": false
                },
                {
                    "_id": "66724df48171db46e7e4ab65",
                    "name": "Jing Zhang",
                    "hidden": false
                },
                {
                    "_id": "66724df48171db46e7e4ab66",
                    "user": {
                        "avatarUrl": "/avatars/63e46f15bb76bd9d4508fd0f54f39829.svg",
                        "isPro": false,
                        "fullname": "Juanzi Li",
                        "user": "juanli",
                        "type": "user"
                    },
                    "name": "Juanzi Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-19T08:45:16.849Z",
                    "hidden": false
                },
                {
                    "_id": "66724df48171db46e7e4ab67",
                    "name": "Lei Zhao",
                    "hidden": false
                },
                {
                    "_id": "66724df48171db46e7e4ab68",
                    "name": "Lindong Wu",
                    "hidden": false
                },
                {
                    "_id": "66724df48171db46e7e4ab69",
                    "user": {
                        "avatarUrl": "/avatars/99068a7a9bf44aca7e5b1cf0d184681e.svg",
                        "isPro": false,
                        "fullname": "zhong",
                        "user": "lucen",
                        "type": "user"
                    },
                    "name": "Lucen Zhong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-19T08:45:05.276Z",
                    "hidden": false
                },
                {
                    "_id": "66724df48171db46e7e4ab6a",
                    "name": "Mingdao Liu",
                    "hidden": false
                },
                {
                    "_id": "66724df48171db46e7e4ab6b",
                    "name": "Minlie Huang",
                    "hidden": false
                },
                {
                    "_id": "66724df48171db46e7e4ab6c",
                    "name": "Peng Zhang",
                    "hidden": false
                },
                {
                    "_id": "66724df48171db46e7e4ab6d",
                    "name": "Qinkai Zheng",
                    "hidden": false
                },
                {
                    "_id": "66724df48171db46e7e4ab6e",
                    "name": "Rui Lu",
                    "hidden": false
                },
                {
                    "_id": "66724df48171db46e7e4ab6f",
                    "name": "Shuaiqi Duan",
                    "hidden": false
                },
                {
                    "_id": "66724df48171db46e7e4ab70",
                    "name": "Shudan Zhang",
                    "hidden": false
                },
                {
                    "_id": "66724df48171db46e7e4ab71",
                    "name": "Shulin Cao",
                    "hidden": false
                },
                {
                    "_id": "66724df48171db46e7e4ab72",
                    "name": "Shuxun Yang",
                    "hidden": false
                },
                {
                    "_id": "66724df48171db46e7e4ab73",
                    "name": "Weng Lam Tam",
                    "hidden": false
                },
                {
                    "_id": "66724df48171db46e7e4ab74",
                    "name": "Wenyi Zhao",
                    "hidden": false
                },
                {
                    "_id": "66724df48171db46e7e4ab75",
                    "name": "Xiao Liu",
                    "hidden": false
                },
                {
                    "_id": "66724df48171db46e7e4ab76",
                    "name": "Xiao Xia",
                    "hidden": false
                },
                {
                    "_id": "66724df48171db46e7e4ab77",
                    "name": "Xiaohan Zhang",
                    "hidden": false
                },
                {
                    "_id": "66724df48171db46e7e4ab78",
                    "name": "Xiaotao Gu",
                    "hidden": false
                },
                {
                    "_id": "66724df48171db46e7e4ab79",
                    "name": "Xin Lv",
                    "hidden": false
                },
                {
                    "_id": "66724df48171db46e7e4ab7a",
                    "name": "Xinghan Liu",
                    "hidden": false
                },
                {
                    "_id": "66724df48171db46e7e4ab7b",
                    "name": "Xinyi Liu",
                    "hidden": false
                },
                {
                    "_id": "66724df48171db46e7e4ab7c",
                    "name": "Xinyue Yang",
                    "hidden": false
                },
                {
                    "_id": "66724df48171db46e7e4ab7d",
                    "name": "Xixuan Song",
                    "hidden": false
                },
                {
                    "_id": "66724df48171db46e7e4ab7e",
                    "name": "Xunkai Zhang",
                    "hidden": false
                },
                {
                    "_id": "66724df48171db46e7e4ab7f",
                    "name": "Yifan An",
                    "hidden": false
                },
                {
                    "_id": "66724df48171db46e7e4ab80",
                    "name": "Yifan Xu",
                    "hidden": false
                },
                {
                    "_id": "66724df48171db46e7e4ab81",
                    "name": "Yilin Niu",
                    "hidden": false
                },
                {
                    "_id": "66724df48171db46e7e4ab82",
                    "name": "Yuantao Yang",
                    "hidden": false
                },
                {
                    "_id": "66724df48171db46e7e4ab83",
                    "name": "Yueyan Li",
                    "hidden": false
                },
                {
                    "_id": "66724df48171db46e7e4ab84",
                    "name": "Yushi Bai",
                    "hidden": false
                },
                {
                    "_id": "66724df48171db46e7e4ab85",
                    "name": "Yuxiao Dong",
                    "hidden": false
                },
                {
                    "_id": "66724df48171db46e7e4ab86",
                    "name": "Zehan Qi",
                    "hidden": false
                },
                {
                    "_id": "66724df48171db46e7e4ab87",
                    "name": "Zhaoyu Wang",
                    "hidden": false
                },
                {
                    "_id": "66724df48171db46e7e4ab88",
                    "name": "Zhen Yang",
                    "hidden": false
                },
                {
                    "_id": "66724df48171db46e7e4ab89",
                    "name": "Zhengxiao Du",
                    "hidden": false
                },
                {
                    "_id": "66724df48171db46e7e4ab8a",
                    "name": "Zhenyu Hou",
                    "hidden": false
                },
                {
                    "_id": "66724df48171db46e7e4ab8b",
                    "name": "Zihan Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-18T16:58:21.000Z",
            "title": "ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All\n  Tools",
            "summary": "We introduce ChatGLM, an evolving family of large language models that we\nhave been developing over time. This report primarily focuses on the GLM-4\nlanguage series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent\nour most capable models that are trained with all the insights and lessons\ngained from the preceding three generations of ChatGLM. To date, the GLM-4\nmodels are pre-trained on ten trillions of tokens mostly in Chinese and\nEnglish, along with a small set of corpus from 24 languages, and aligned\nprimarily for Chinese and English usage. The high-quality alignment is achieved\nvia a multi-stage post-training process, which involves supervised fine-tuning\nand learning from human feedback. Evaluations show that GLM-4 1) closely rivals\nor outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH,\nBBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following\nas measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long\ncontext tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by\nAlignBench. The GLM-4 All Tools model is further aligned to understand user\nintent and autonomously decide when and which tool(s) touse -- including web\nbrowser, Python interpreter, text-to-image model, and user-defined functions --\nto effectively complete complex tasks. In practical applications, it matches\nand even surpasses GPT-4 All Tools in tasks like accessing online information\nvia web browsing and solving math problems using Python interpreter. Over the\ncourse, we have open-sourced a series of models, including ChatGLM-6B (three\ngenerations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting\nover 10 million downloads on Hugging face in the year 2023 alone. The open\nmodels can be accessed through https://github.com/THUDM and\nhttps://huggingface.co/THUDM.",
            "upvotes": 12
        },
        "publishedAt": "2024-06-19T01:48:47.239Z",
        "title": "ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.12793.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.11912",
            "authors": [
                {
                    "_id": "6672de83065dcb2a889d4eab",
                    "name": "Minh Huynh Nguyen",
                    "hidden": false
                },
                {
                    "_id": "6672de83065dcb2a889d4eac",
                    "name": "Thang Phan Chau",
                    "hidden": false
                },
                {
                    "_id": "6672de83065dcb2a889d4ead",
                    "name": "Phong X. Nguyen",
                    "hidden": false
                },
                {
                    "_id": "6672de83065dcb2a889d4eae",
                    "name": "Nghi D. Q. Bui",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-16T17:57:48.000Z",
            "title": "AgileCoder: Dynamic Collaborative Agents for Software Development based\n  on Agile Methodology",
            "summary": "Software agents have emerged as promising tools for addressing complex\nsoftware engineering tasks. However, existing works oversimplify software\ndevelopment workflows by following the waterfall model. Thus, we propose\nAgileCoder, a multi-agent system that integrates Agile Methodology (AM) into\nthe framework. This system assigns specific AM roles such as Product Manager,\nDeveloper, and Tester to different agents, who then collaboratively develop\nsoftware based on user inputs. AgileCoder enhances development efficiency by\norganizing work into sprints, focusing on incrementally developing software\nthrough sprints. Additionally, we introduce Dynamic Code Graph Generator, a\nmodule that creates a Code Dependency Graph dynamically as updates are made to\nthe codebase. This allows agents to better comprehend the codebase, leading to\nmore precise code generation and modifications throughout the software\ndevelopment process. AgileCoder surpasses existing benchmarks, like ChatDev and\nMetaGPT, establishing a new standard and showcasing the capabilities of\nmulti-agent systems in advanced software engineering environments. Our source\ncode can be found at https://github.com/FSoft-AI4Code/AgileCoder.",
            "upvotes": 10
        },
        "publishedAt": "2024-06-19T12:06:07.917Z",
        "title": "AgileCoder: Dynamic Collaborative Agents for Software Development based on Agile Methodology",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63a44141de134926a2ba0458/KK-Lqgl3We9pCxc2Sq_Z3.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.11912.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/441dfc0f7ae5d79ecccab5ee62326919.svg",
            "fullname": "Bui",
            "name": "bdqnghi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.11811",
            "authors": [
                {
                    "_id": "66723ae8cbf550c42aee3a88",
                    "name": "Joao Monteiro",
                    "hidden": false
                },
                {
                    "_id": "66723ae8cbf550c42aee3a89",
                    "name": "Pierre-Andre Noel",
                    "hidden": false
                },
                {
                    "_id": "66723ae8cbf550c42aee3a8a",
                    "name": "Etienne Marcotte",
                    "hidden": false
                },
                {
                    "_id": "66723ae8cbf550c42aee3a8b",
                    "name": "Sai Rajeswar",
                    "hidden": false
                },
                {
                    "_id": "66723ae8cbf550c42aee3a8c",
                    "name": "Valentina Zantedeschi",
                    "hidden": false
                },
                {
                    "_id": "66723ae8cbf550c42aee3a8d",
                    "name": "David Vazquez",
                    "hidden": false
                },
                {
                    "_id": "66723ae8cbf550c42aee3a8e",
                    "name": "Nicolas Chapados",
                    "hidden": false
                },
                {
                    "_id": "66723ae8cbf550c42aee3a8f",
                    "name": "Christopher Pal",
                    "hidden": false
                },
                {
                    "_id": "66723ae8cbf550c42aee3a90",
                    "name": "Perouz Taslakian",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-17T17:52:54.000Z",
            "title": "RepLiQA: A Question-Answering Dataset for Benchmarking LLMs on Unseen\n  Reference Content",
            "summary": "Large Language Models (LLMs) are trained on vast amounts of data, most of\nwhich is automatically scraped from the internet. This data includes\nencyclopedic documents that harbor a vast amount of general knowledge (e.g.,\nWikipedia) but also potentially overlap with benchmark datasets used for\nevaluating LLMs. Consequently, evaluating models on test splits that might have\nleaked into the training set is prone to misleading conclusions. To foster\nsound evaluation of language models, we introduce a new test dataset named\nRepLiQA, suited for question-answering and topic retrieval tasks. RepLiQA is a\ncollection of five splits of test sets, four of which have not been released to\nthe internet or exposed to LLM APIs prior to this publication. Each sample in\nRepLiQA comprises (1) a reference document crafted by a human annotator and\ndepicting an imaginary scenario (e.g., a news article) absent from the\ninternet; (2) a question about the document's topic; (3) a ground-truth answer\nderived directly from the information in the document; and (4) the paragraph\nextracted from the reference document containing the answer. As such, accurate\nanswers can only be generated if a model can find relevant content within the\nprovided document. We run a large-scale benchmark comprising several\nstate-of-the-art LLMs to uncover differences in performance across models of\nvarious types and sizes in a context-conditional language modeling setting.\nReleased splits of RepLiQA can be found here:\nhttps://huggingface.co/datasets/ServiceNow/repliqa.",
            "upvotes": 10
        },
        "publishedAt": "2024-06-19T00:29:44.897Z",
        "title": "RepLiQA: A Question-Answering Dataset for Benchmarking LLMs on Unseen Reference Content",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.11811.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/121fff6aefd09565fd2b01d7fcda6757.svg",
            "fullname": "Sean Hughes",
            "name": "hughesthe1st",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.12459",
            "authors": [
                {
                    "_id": "66729ae8cbf550c42a1401bb",
                    "name": "Panwang Pan",
                    "hidden": false
                },
                {
                    "_id": "66729ae8cbf550c42a1401bc",
                    "name": "Zhuo Su",
                    "hidden": false
                },
                {
                    "_id": "66729ae8cbf550c42a1401bd",
                    "name": "Chenguo Lin",
                    "hidden": false
                },
                {
                    "_id": "66729ae8cbf550c42a1401be",
                    "name": "Zhen Fan",
                    "hidden": false
                },
                {
                    "_id": "66729ae8cbf550c42a1401bf",
                    "name": "Yongjie Zhang",
                    "hidden": false
                },
                {
                    "_id": "66729ae8cbf550c42a1401c0",
                    "name": "Zeming Li",
                    "hidden": false
                },
                {
                    "_id": "66729ae8cbf550c42a1401c1",
                    "name": "Tingting Shen",
                    "hidden": false
                },
                {
                    "_id": "66729ae8cbf550c42a1401c2",
                    "name": "Yadong Mu",
                    "hidden": false
                },
                {
                    "_id": "66729ae8cbf550c42a1401c3",
                    "name": "Yebin Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-18T10:05:33.000Z",
            "title": "HumanSplat: Generalizable Single-Image Human Gaussian Splatting with\n  Structure Priors",
            "summary": "Despite recent advancements in high-fidelity human reconstruction techniques,\nthe requirements for densely captured images or time-consuming per-instance\noptimization significantly hinder their applications in broader scenarios. To\ntackle these issues, we present HumanSplat which predicts the 3D Gaussian\nSplatting properties of any human from a single input image in a generalizable\nmanner. In particular, HumanSplat comprises a 2D multi-view diffusion model and\na latent reconstruction transformer with human structure priors that adeptly\nintegrate geometric priors and semantic features within a unified framework. A\nhierarchical loss that incorporates human semantic information is further\ndesigned to achieve high-fidelity texture modeling and better constrain the\nestimated multiple views. Comprehensive experiments on standard benchmarks and\nin-the-wild images demonstrate that HumanSplat surpasses existing\nstate-of-the-art methods in achieving photorealistic novel-view synthesis.",
            "upvotes": 8
        },
        "publishedAt": "2024-06-19T07:26:42.818Z",
        "title": "HumanSplat: Generalizable Single-Image Human Gaussian Splatting with Structure Priors",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/654866e8cd0a5621395f8287/C-jgIuLZWoQBQjNR2wrLj.jpeg",
            "https://cdn-uploads.huggingface.co/production/uploads/654866e8cd0a5621395f8287/8UYdwnqfJ2Do_stBykBQA.mp4",
            "https://cdn-uploads.huggingface.co/production/uploads/654866e8cd0a5621395f8287/zzxxNvdqnGT-RCNDgv2J9.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.12459.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654866e8cd0a5621395f8287/4Bccwd1ehn-Ee4T1rId5S.jpeg",
            "fullname": "Panwang Pan",
            "name": "paulpanwang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.12742",
            "authors": [
                {
                    "_id": "66727eeef022d14aa15540ea",
                    "user": {
                        "avatarUrl": "/avatars/f37ce036b76180ed0fa004f9c8c09363.svg",
                        "isPro": false,
                        "fullname": "Bingchen Zhao",
                        "user": "tennant",
                        "type": "user"
                    },
                    "name": "Bingchen Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-19T07:37:20.887Z",
                    "hidden": false
                },
                {
                    "_id": "66727eeef022d14aa15540eb",
                    "user": {
                        "avatarUrl": "/avatars/1124128ce69ae6868fd1a2c06420d72f.svg",
                        "isPro": false,
                        "fullname": "Yongshuo Zong",
                        "user": "ys-zong",
                        "type": "user"
                    },
                    "name": "Yongshuo Zong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-19T08:58:49.432Z",
                    "hidden": false
                },
                {
                    "_id": "66727eeef022d14aa15540ec",
                    "user": {
                        "avatarUrl": "/avatars/449723010f64a5c9e274434ebdc03150.svg",
                        "isPro": false,
                        "fullname": "Letian Zhang",
                        "user": "WonderThyme",
                        "type": "user"
                    },
                    "name": "Letian Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-19T08:58:56.282Z",
                    "hidden": false
                },
                {
                    "_id": "66727eeef022d14aa15540ed",
                    "name": "Timothy Hospedales",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-18T16:02:18.000Z",
            "title": "Benchmarking Multi-Image Understanding in Vision and Language Models:\n  Perception, Knowledge, Reasoning, and Multi-Hop Reasoning",
            "summary": "The advancement of large language models (LLMs) has significantly broadened\nthe scope of applications in natural language processing, with multi-modal LLMs\nextending these capabilities to integrate and interpret visual data. However,\nexisting benchmarks for visual language models (VLMs) predominantly focus on\nsingle-image inputs, neglecting the crucial aspect of multi-image\nunderstanding. In this paper, we introduce a Multi-Image Relational Benchmark\nMIRB, designed to evaluate VLMs' ability to compare, analyze, and reason across\nmultiple images. Our benchmark encompasses four categories: perception, visual\nworld knowledge, reasoning, and multi-hop reasoning. Through a comprehensive\nevaluation of a wide range of open-source and closed-source models, we\ndemonstrate that while open-source VLMs were shown to approach the performance\nof GPT-4V in single-image tasks, a significant performance gap remains in\nmulti-image reasoning tasks. Our findings also reveal that even the\nstate-of-the-art GPT-4V model struggles with our benchmark, underscoring the\nneed for further research and development in this area. We believe our\ncontribution of MIRB could serve as a testbed for developing the\nnext-generation multi-modal models.",
            "upvotes": 7
        },
        "publishedAt": "2024-06-19T05:17:49.150Z",
        "title": "Benchmarking Multi-Image Understanding in Vision and Language Models: Perception, Knowledge, Reasoning, and Multi-Hop Reasoning",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.12742.png",
        "numComments": 4,
        "submittedBy": {
            "avatarUrl": "/avatars/f37ce036b76180ed0fa004f9c8c09363.svg",
            "fullname": "Bingchen Zhao",
            "name": "tennant",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.12824",
            "authors": [
                {
                    "_id": "6672553486f6b3d126834783",
                    "user": {
                        "avatarUrl": "/avatars/60cf2037ad7e1e2adb135660bd28204f.svg",
                        "isPro": false,
                        "fullname": "Hitesh Wadhwa",
                        "user": "wadhwahitesh",
                        "type": "user"
                    },
                    "name": "Hitesh Wadhwa",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-19T07:37:38.560Z",
                    "hidden": false
                },
                {
                    "_id": "6672553486f6b3d126834784",
                    "user": {
                        "avatarUrl": "/avatars/4ac23b7ef93848232b46896add4f19a2.svg",
                        "isPro": false,
                        "fullname": "Rahul Seetharaman",
                        "user": "rahulseetharaman",
                        "type": "user"
                    },
                    "name": "Rahul Seetharaman",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-19T08:40:07.803Z",
                    "hidden": false
                },
                {
                    "_id": "6672553486f6b3d126834785",
                    "user": {
                        "avatarUrl": "/avatars/e370887a53c8740944b17c65a4f5eb07.svg",
                        "isPro": false,
                        "fullname": "Somyaa Aggarwal",
                        "user": "Somyaa27",
                        "type": "user"
                    },
                    "name": "Somyaa Aggarwal",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-19T08:40:13.790Z",
                    "hidden": false
                },
                {
                    "_id": "6672553486f6b3d126834786",
                    "user": {
                        "avatarUrl": "/avatars/5fa606f63f841879af1a7617366d18b4.svg",
                        "isPro": false,
                        "fullname": "Reshmi Ghosh",
                        "user": "reshmighosh",
                        "type": "user"
                    },
                    "name": "Reshmi Ghosh",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2024-06-19T12:49:23.836Z",
                    "hidden": false
                },
                {
                    "_id": "6672553486f6b3d126834787",
                    "user": {
                        "avatarUrl": "/avatars/45eb04bf6e0ed1a4daa14070411b693f.svg",
                        "isPro": false,
                        "fullname": "Samyadeep",
                        "user": "samyadeepbasu",
                        "type": "user"
                    },
                    "name": "Samyadeep Basu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-19T08:40:19.705Z",
                    "hidden": false
                },
                {
                    "_id": "6672553486f6b3d126834788",
                    "name": "Soundararajan Srinivasan",
                    "hidden": false
                },
                {
                    "_id": "6672553486f6b3d126834789",
                    "user": {
                        "avatarUrl": "/avatars/1b361d83120f18f7cb950ba039991f6a.svg",
                        "isPro": false,
                        "fullname": "WENLONG ZHAO",
                        "user": "Warren7771818",
                        "type": "user"
                    },
                    "name": "Wenlong Zhao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-19T08:40:29.254Z",
                    "hidden": false
                },
                {
                    "_id": "6672553486f6b3d12683478a",
                    "name": "Shreyas Chaudhari",
                    "hidden": false
                },
                {
                    "_id": "6672553486f6b3d12683478b",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1615820858280-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Ehsan Aghazadeh",
                        "user": "EhsanAghazadeh",
                        "type": "user"
                    },
                    "name": "Ehsan Aghazadeh",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-06-19T08:40:38.293Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-18T17:46:08.000Z",
            "title": "From RAGs to rich parameters: Probing how language models utilize\n  external knowledge over parametric information for factual queries",
            "summary": "Retrieval Augmented Generation (RAG) enriches the ability of language models\nto reason using external context to augment responses for a given user prompt.\nThis approach has risen in popularity due to practical applications in various\napplications of language models in search, question/answering, and chat-bots.\nHowever, the exact nature of how this approach works isn't clearly understood.\nIn this paper, we mechanistically examine the RAG pipeline to highlight that\nlanguage models take shortcut and have a strong bias towards utilizing only the\ncontext information to answer the question, while relying minimally on their\nparametric memory. We probe this mechanistic behavior in language models with:\n(i) Causal Mediation Analysis to show that the parametric memory is minimally\nutilized when answering a question and (ii) Attention Contributions and\nKnockouts to show that the last token residual stream do not get enriched from\nthe subject token in the question, but gets enriched from other informative\ntokens in the context. We find this pronounced shortcut behaviour true across\nboth LLaMa and Phi family of models.",
            "upvotes": 7
        },
        "publishedAt": "2024-06-19T02:21:31.750Z",
        "title": "From RAGs to rich parameters: Probing how language models utilize external knowledge over parametric information for factual queries",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.12824.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/45eb04bf6e0ed1a4daa14070411b693f.svg",
            "fullname": "Samyadeep",
            "name": "samyadeepbasu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.12168",
            "authors": [
                {
                    "_id": "667266a20f6ec76f334bf263",
                    "name": "Wenda Xu",
                    "hidden": false
                },
                {
                    "_id": "667266a20f6ec76f334bf264",
                    "user": {
                        "avatarUrl": "/avatars/acf43ea155105a51c8612dacc4725091.svg",
                        "isPro": false,
                        "fullname": "Jiachen Li",
                        "user": "jiachenli-ucsb",
                        "type": "user"
                    },
                    "name": "Jiachen Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-19T07:37:31.704Z",
                    "hidden": false
                },
                {
                    "_id": "667266a20f6ec76f334bf265",
                    "name": "William Yang Wang",
                    "hidden": false
                },
                {
                    "_id": "667266a20f6ec76f334bf266",
                    "name": "Lei Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-18T00:41:40.000Z",
            "title": "BPO: Supercharging Online Preference Learning by Adhering to the\n  Proximity of Behavior LLM",
            "summary": "Direct alignment from preferences (DAP) has emerged as a promising paradigm\nfor aligning large language models (LLMs) to human desiderata from\npre-collected, offline preference datasets. While recent studies indicate that\nexisting offline DAP methods can directly benefit from online training samples,\nwe highlight the need to develop specific online DAP algorithms to fully\nharness the power of online training. Specifically, we identify that the\nlearned LLM should adhere to the proximity of the behavior LLM, which collects\nthe training samples. To this end, we propose online Preference Optimization in\nproximity to the Behavior LLM (BPO), emphasizing the importance of constructing\na proper trust region for LLM alignment.\n  We conduct extensive experiments to validate the effectiveness and\napplicability of our approach by integrating it with various DAP methods,\nresulting in significant performance improvements across a wide range of tasks\nwhen training with the same amount of preference data. Even when only\nintroducing one additional data collection phase, our online BPO improves its\noffline DAP baseline from 72.0% to 80.2% on TL;DR and from 82.2% to 89.1% on\nAnthropic Helpfulness in terms of win rate against human reference text.",
            "upvotes": 6
        },
        "publishedAt": "2024-06-19T03:34:14.742Z",
        "title": "BPO: Supercharging Online Preference Learning by Adhering to the Proximity of Behavior LLM",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.12168.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/acf43ea155105a51c8612dacc4725091.svg",
            "fullname": "Jiachen Li",
            "name": "jiachenli-ucsb",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.11687",
            "authors": [
                {
                    "_id": "66728498c12108af6ce54bf4",
                    "name": "Yekun Chai",
                    "hidden": false
                },
                {
                    "_id": "66728498c12108af6ce54bf5",
                    "name": "Yewei Fang",
                    "hidden": false
                },
                {
                    "_id": "66728498c12108af6ce54bf6",
                    "name": "Qiwei Peng",
                    "hidden": false
                },
                {
                    "_id": "66728498c12108af6ce54bf7",
                    "name": "Xuhong Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-17T16:05:32.000Z",
            "title": "Tokenization Falling Short: The Curse of Tokenization",
            "summary": "Language models typically tokenize raw text into sequences of subword\nidentifiers from a predefined vocabulary, a process inherently sensitive to\ntypographical errors, length variations, and largely oblivious to the internal\nstructure of tokens-issues we term the curse of tokenization. In this study, we\ndelve into these drawbacks and demonstrate that large language models (LLMs)\nremain susceptible to these problems. This study systematically investigates\nthese challenges and their impact on LLMs through three critical research\nquestions: (1) complex problem solving, (2) token structure probing, and (3)\nresilience to typographical variation. Our findings reveal that scaling model\nparameters can mitigate the issue of tokenization; however, LLMs still suffer\nfrom biases induced by typos and other text format variations. Our experiments\nshow that subword regularization such as BPE-dropout can mitigate this issue.\nWe will release our code and data to facilitate further research.",
            "upvotes": 5
        },
        "publishedAt": "2024-06-19T05:43:27.894Z",
        "title": "Tokenization Falling Short: The Curse of Tokenization",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.11687.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1651142275247-626a6ce8adf559c88de9ace0.jpeg",
            "fullname": "cyk",
            "name": "cyk1337",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.12753",
            "authors": [
                {
                    "_id": "66724205c12108af6ccd1d3b",
                    "name": "Zhen Huang",
                    "hidden": false
                },
                {
                    "_id": "66724205c12108af6ccd1d3c",
                    "name": "Zengzhi Wang",
                    "hidden": false
                },
                {
                    "_id": "66724205c12108af6ccd1d3d",
                    "name": "Shijie Xia",
                    "hidden": false
                },
                {
                    "_id": "66724205c12108af6ccd1d3e",
                    "name": "Xuefeng Li",
                    "hidden": false
                },
                {
                    "_id": "66724205c12108af6ccd1d3f",
                    "name": "Haoyang Zou",
                    "hidden": false
                },
                {
                    "_id": "66724205c12108af6ccd1d40",
                    "name": "Ruijie Xu",
                    "hidden": false
                },
                {
                    "_id": "66724205c12108af6ccd1d41",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/616bfc2b40e2f69baa1c7add/Os7_qgMei-2lRVelrOG7B.jpeg",
                        "isPro": false,
                        "fullname": "Run-Ze Fan",
                        "user": "Vfrz",
                        "type": "user"
                    },
                    "name": "Run-Ze Fan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-19T07:38:50.815Z",
                    "hidden": false
                },
                {
                    "_id": "66724205c12108af6ccd1d42",
                    "name": "Lyumanshan Ye",
                    "hidden": false
                },
                {
                    "_id": "66724205c12108af6ccd1d43",
                    "name": "Ethan Chern",
                    "hidden": false
                },
                {
                    "_id": "66724205c12108af6ccd1d44",
                    "name": "Yixin Ye",
                    "hidden": false
                },
                {
                    "_id": "66724205c12108af6ccd1d45",
                    "name": "Yikai Zhang",
                    "hidden": false
                },
                {
                    "_id": "66724205c12108af6ccd1d46",
                    "name": "Yuqing Yang",
                    "hidden": false
                },
                {
                    "_id": "66724205c12108af6ccd1d47",
                    "name": "Ting Wu",
                    "hidden": false
                },
                {
                    "_id": "66724205c12108af6ccd1d48",
                    "name": "Binjie Wang",
                    "hidden": false
                },
                {
                    "_id": "66724205c12108af6ccd1d49",
                    "name": "Shichao Sun",
                    "hidden": false
                },
                {
                    "_id": "66724205c12108af6ccd1d4a",
                    "name": "Yang Xiao",
                    "hidden": false
                },
                {
                    "_id": "66724205c12108af6ccd1d4b",
                    "name": "Yiyuan Li",
                    "hidden": false
                },
                {
                    "_id": "66724205c12108af6ccd1d4c",
                    "name": "Fan Zhou",
                    "hidden": false
                },
                {
                    "_id": "66724205c12108af6ccd1d4d",
                    "name": "Steffi Chern",
                    "hidden": false
                },
                {
                    "_id": "66724205c12108af6ccd1d4e",
                    "name": "Yiwei Qin",
                    "hidden": false
                },
                {
                    "_id": "66724205c12108af6ccd1d4f",
                    "name": "Yan Ma",
                    "hidden": false
                },
                {
                    "_id": "66724205c12108af6ccd1d50",
                    "name": "Jiadi Su",
                    "hidden": false
                },
                {
                    "_id": "66724205c12108af6ccd1d51",
                    "name": "Yixiu Liu",
                    "hidden": false
                },
                {
                    "_id": "66724205c12108af6ccd1d52",
                    "name": "Yuxiang Zheng",
                    "hidden": false
                },
                {
                    "_id": "66724205c12108af6ccd1d53",
                    "name": "Shaoting Zhang",
                    "hidden": false
                },
                {
                    "_id": "66724205c12108af6ccd1d54",
                    "name": "Dahua Lin",
                    "hidden": false
                },
                {
                    "_id": "66724205c12108af6ccd1d55",
                    "name": "Yu Qiao",
                    "hidden": false
                },
                {
                    "_id": "66724205c12108af6ccd1d56",
                    "name": "Pengfei Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-18T16:20:53.000Z",
            "title": "OlympicArena: Benchmarking Multi-discipline Cognitive Reasoning for\n  Superintelligent AI",
            "summary": "The evolution of Artificial Intelligence (AI) has been significantly\naccelerated by advancements in Large Language Models (LLMs) and Large\nMultimodal Models (LMMs), gradually showcasing potential cognitive reasoning\nabilities in problem-solving and scientific discovery (i.e., AI4Science) once\nexclusive to human intellect. To comprehensively evaluate current models'\nperformance in cognitive reasoning abilities, we introduce OlympicArena, which\nincludes 11,163 bilingual problems across both text-only and interleaved\ntext-image modalities. These challenges encompass a wide range of disciplines\nspanning seven fields and 62 international Olympic competitions, rigorously\nexamined for data leakage. We argue that the challenges in Olympic competition\nproblems are ideal for evaluating AI's cognitive reasoning due to their\ncomplexity and interdisciplinary nature, which are essential for tackling\ncomplex scientific challenges and facilitating discoveries. Beyond evaluating\nperformance across various disciplines using answer-only criteria, we conduct\ndetailed experiments and analyses from multiple perspectives. We delve into the\nmodels' cognitive reasoning abilities, their performance across different\nmodalities, and their outcomes in process-level evaluations, which are vital\nfor tasks requiring complex reasoning with lengthy solutions. Our extensive\nevaluations reveal that even advanced models like GPT-4o only achieve a 39.97%\noverall accuracy, illustrating current AI limitations in complex reasoning and\nmultimodal integration. Through the OlympicArena, we aim to advance AI towards\nsuperintelligence, equipping it to address more complex challenges in science\nand beyond. We also provide a comprehensive set of resources to support AI\nresearch, including a benchmark dataset, an open-source annotation platform, a\ndetailed evaluation tool, and a leaderboard with automatic submission features.",
            "upvotes": 5
        },
        "publishedAt": "2024-06-19T01:43:21.930Z",
        "title": "OlympicArena: Benchmarking Multi-discipline Cognitive Reasoning for Superintelligent AI",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.12753.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.12311",
            "authors": [
                {
                    "_id": "6672682a6d27e245f657825a",
                    "user": {
                        "avatarUrl": "/avatars/7ef1aaadd5b378d00e17dc548e42cb7e.svg",
                        "isPro": false,
                        "fullname": "Dongwon Jo",
                        "user": "dongwonjo",
                        "type": "user"
                    },
                    "name": "Dongwon Jo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-19T07:37:26.386Z",
                    "hidden": false
                },
                {
                    "_id": "6672682a6d27e245f657825b",
                    "name": "Taesu Kim",
                    "hidden": false
                },
                {
                    "_id": "6672682a6d27e245f657825c",
                    "name": "Yulhwa Kim",
                    "hidden": false
                },
                {
                    "_id": "6672682a6d27e245f657825d",
                    "name": "Jae-Joon Kim",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-18T06:32:23.000Z",
            "title": "Mixture of Scales: Memory-Efficient Token-Adaptive Binarization for\n  Large Language Models",
            "summary": "Binarization, which converts weight parameters to binary values, has emerged\nas an effective strategy to reduce the size of large language models (LLMs).\nHowever, typical binarization techniques significantly diminish linguistic\neffectiveness of LLMs. To address this issue, we introduce a novel binarization\ntechnique called Mixture of Scales (BinaryMoS). Unlike conventional methods,\nBinaryMoS employs multiple scaling experts for binary weights, dynamically\nmerging these experts for each token to adaptively generate scaling factors.\nThis token-adaptive approach boosts the representational power of binarized\nLLMs by enabling contextual adjustments to the values of binary weights.\nMoreover, because this adaptive process only involves the scaling factors\nrather than the entire weight matrix, BinaryMoS maintains compression\nefficiency similar to traditional static binarization methods. Our experimental\nresults reveal that BinaryMoS surpasses conventional binarization techniques in\nvarious natural language processing tasks and even outperforms 2-bit\nquantization methods, all while maintaining similar model size to static\nbinarization techniques.",
            "upvotes": 4
        },
        "publishedAt": "2024-06-19T06:35:35.657Z",
        "title": "Mixture of Scales: Memory-Efficient Token-Adaptive Binarization for Large Language Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.12311.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/7ef1aaadd5b378d00e17dc548e42cb7e.svg",
            "fullname": "Dongwon Jo",
            "name": "dongwonjo",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.12066",
            "authors": [
                {
                    "_id": "66722b0886f6b3d126722a0e",
                    "name": "Jack Gallifant",
                    "hidden": false
                },
                {
                    "_id": "66722b0886f6b3d126722a0f",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63600b93d9e4214b9a67807c/BovBE1oiZPdU8Td9I2YIn.jpeg",
                        "isPro": false,
                        "fullname": "Shan Chen",
                        "user": "shanchen",
                        "type": "user"
                    },
                    "name": "Shan Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-19T07:38:57.688Z",
                    "hidden": false
                },
                {
                    "_id": "66722b0886f6b3d126722a10",
                    "name": "Pedro Moreira",
                    "hidden": false
                },
                {
                    "_id": "66722b0886f6b3d126722a11",
                    "name": "Nikolaj Munch",
                    "hidden": false
                },
                {
                    "_id": "66722b0886f6b3d126722a12",
                    "name": "Mingye Gao",
                    "hidden": false
                },
                {
                    "_id": "66722b0886f6b3d126722a13",
                    "name": "Jackson Pond",
                    "hidden": false
                },
                {
                    "_id": "66722b0886f6b3d126722a14",
                    "name": "Leo Anthony Celi",
                    "hidden": false
                },
                {
                    "_id": "66722b0886f6b3d126722a15",
                    "name": "Hugo Aerts",
                    "hidden": false
                },
                {
                    "_id": "66722b0886f6b3d126722a16",
                    "name": "Thomas Hartvigsen",
                    "hidden": false
                },
                {
                    "_id": "66722b0886f6b3d126722a17",
                    "name": "Danielle Bitterman",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-17T20:09:24.000Z",
            "title": "Language Models are Surprisingly Fragile to Drug Names in Biomedical\n  Benchmarks",
            "summary": "Medical knowledge is context-dependent and requires consistent reasoning\nacross various natural language expressions of semantically equivalent phrases.\nThis is particularly crucial for drug names, where patients often use brand\nnames like Advil or Tylenol instead of their generic equivalents. To study\nthis, we create a new robustness dataset, RABBITS, to evaluate performance\ndifferences on medical benchmarks after swapping brand and generic drug names\nusing physician expert annotations.\n  We assess both open-source and API-based LLMs on MedQA and MedMCQA, revealing\na consistent performance drop ranging from 1-10\\%. Furthermore, we identify a\npotential source of this fragility as the contamination of test data in widely\nused pre-training datasets. All code is accessible at\nhttps://github.com/BittermanLab/RABBITS, and a HuggingFace leaderboard is\navailable at https://huggingface.co/spaces/AIM-Harvard/rabbits-leaderboard.",
            "upvotes": 3
        },
        "publishedAt": "2024-06-19T00:46:58.243Z",
        "title": "Language Models are Surprisingly Fragile to Drug Names in Biomedical Benchmarks",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.12066.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63600b93d9e4214b9a67807c/BovBE1oiZPdU8Td9I2YIn.jpeg",
            "fullname": "Shan Chen",
            "name": "shanchen",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.12031",
            "authors": [
                {
                    "_id": "66722b866d27e245f6419224",
                    "name": "Josh Gardner",
                    "hidden": false
                },
                {
                    "_id": "66722b866d27e245f6419225",
                    "name": "Juan C. Perdomo",
                    "hidden": false
                },
                {
                    "_id": "66722b866d27e245f6419226",
                    "name": "Ludwig Schmidt",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-17T18:58:20.000Z",
            "title": "Large Scale Transfer Learning for Tabular Data via Language Modeling",
            "summary": "Tabular data -- structured, heterogeneous, spreadsheet-style data with rows\nand columns -- is widely used in practice across many domains. However, while\nrecent foundation models have reduced the need for developing task-specific\ndatasets and predictors in domains such as language modeling and computer\nvision, this transfer learning paradigm has not had similar impact in the\ntabular domain. In this work, we seek to narrow this gap and present TabuLa-8B,\na language model for tabular prediction. We define a process for extracting a\nlarge, high-quality training dataset from the TabLib corpus, proposing methods\nfor tabular data filtering and quality control. Using the resulting dataset,\nwhich comprises over 1.6B rows from 3.1M unique tables, we fine-tune a Llama\n3-8B large language model (LLM) for tabular data prediction (classification and\nbinned regression) using a novel packing and attention scheme for tabular\nprediction. Through evaluation across a test suite of 329 datasets, we find\nthat TabuLa-8B has zero-shot accuracy on unseen tables that is over 15\npercentage points (pp) higher than random guessing, a feat that is not possible\nwith existing state-of-the-art tabular prediction models (e.g. XGBoost,\nTabPFN). In the few-shot setting (1-32 shots), without any fine-tuning on the\ntarget datasets, TabuLa-8B is 5-15 pp more accurate than XGBoost and TabPFN\nmodels that are explicitly trained on equal, or even up to 16x more data. We\nrelease our model, code, and data along with the publication of this paper.",
            "upvotes": 2
        },
        "publishedAt": "2024-06-19T08:20:38.663Z",
        "title": "Large Scale Transfer Learning for Tabular Data via Language Modeling",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.12031.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1627505688463-60107b385ac3e86b3ea4fc34.jpeg",
            "fullname": "Daniel van Strien",
            "name": "davanstrien",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.12644",
            "authors": [
                {
                    "_id": "66725e5e6f64b0d7be8785e1",
                    "name": "Devichand Budagam",
                    "hidden": false
                },
                {
                    "_id": "66725e5e6f64b0d7be8785e2",
                    "name": "Sankalp KJ",
                    "hidden": false
                },
                {
                    "_id": "66725e5e6f64b0d7be8785e3",
                    "name": "Ashutosh Kumar",
                    "hidden": false
                },
                {
                    "_id": "66725e5e6f64b0d7be8785e4",
                    "name": "Vinija Jain",
                    "hidden": false
                },
                {
                    "_id": "66725e5e6f64b0d7be8785e5",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
                        "isPro": false,
                        "fullname": "Aman Chadha",
                        "user": "amanchadha",
                        "type": "user"
                    },
                    "name": "Aman Chadha",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-06-19T07:37:36.231Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-18T14:12:27.000Z",
            "title": "Hierarchical Prompting Taxonomy: A Universal Evaluation Framework for\n  Large Language Models",
            "summary": "Assessing the effectiveness of large language models (LLMs) in addressing\ndiverse tasks is essential for comprehending their strengths and weaknesses.\nConventional evaluation techniques typically apply a single prompting strategy\nuniformly across datasets, not considering the varying degrees of task\ncomplexity. We introduce the Hierarchical Prompting Taxonomy (HPT), a taxonomy\nthat employs a Hierarchical Prompt Framework (HPF) composed of five unique\nprompting strategies, arranged from the simplest to the most complex, to assess\nLLMs more precisely and to offer a clearer perspective. This taxonomy assigns a\nscore, called the Hierarchical Prompting Score (HP-Score), to datasets as well\nas LLMs based on the rules of the taxonomy, providing a nuanced understanding\nof their ability to solve diverse tasks and offering a universal measure of\ntask complexity. Additionally, we introduce the Adaptive Hierarchical Prompt\nframework, which automates the selection of appropriate prompting strategies\nfor each task. This study compares manual and adaptive hierarchical prompt\nframeworks using four instruction-tuned LLMs, namely Llama 3 8B, Phi 3 3.8B,\nMistral 7B, and Gemma 7B, across four datasets: BoolQ, CommonSenseQA (CSQA),\nIWSLT-2017 en-fr (IWSLT), and SamSum. Experiments demonstrate the effectiveness\nof HPT, providing a reliable way to compare different tasks and LLM\ncapabilities. This paper leads to the development of a universal evaluation\nmetric that can be used to evaluate both the complexity of the datasets and the\ncapabilities of LLMs. The implementation of both manual HPF and adaptive HPF is\npublicly available.",
            "upvotes": 2
        },
        "publishedAt": "2024-06-19T03:11:26.268Z",
        "title": "Hierarchical Prompting Taxonomy: A Universal Evaluation Framework for Large Language Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.12644.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
            "fullname": "Aman Chadha",
            "name": "amanchadha",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.12292",
            "authors": [
                {
                    "_id": "6672597e7e7ff5d8bd0b18cf",
                    "name": "Boyu Chen",
                    "hidden": false
                },
                {
                    "_id": "6672597e7e7ff5d8bd0b18d0",
                    "name": "Peike Li",
                    "hidden": false
                },
                {
                    "_id": "6672597e7e7ff5d8bd0b18d1",
                    "name": "Yao Yao",
                    "hidden": false
                },
                {
                    "_id": "6672597e7e7ff5d8bd0b18d2",
                    "name": "Alex Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-18T05:54:11.000Z",
            "title": "JEN-1 DreamStyler: Customized Musical Concept Learning via Pivotal\n  Parameters Tuning",
            "summary": "Large models for text-to-music generation have achieved significant progress,\nfacilitating the creation of high-quality and varied musical compositions from\nprovided text prompts. However, input text prompts may not precisely capture\nuser requirements, particularly when the objective is to generate music that\nembodies a specific concept derived from a designated reference collection. In\nthis paper, we propose a novel method for customized text-to-music generation,\nwhich can capture the concept from a two-minute reference music and generate a\nnew piece of music conforming to the concept. We achieve this by fine-tuning a\npretrained text-to-music model using the reference music. However, directly\nfine-tuning all parameters leads to overfitting issues. To address this\nproblem, we propose a Pivotal Parameters Tuning method that enables the model\nto assimilate the new concept while preserving its original generative\ncapabilities. Additionally, we identify a potential concept conflict when\nintroducing multiple concepts into the pretrained model. We present a concept\nenhancement strategy to distinguish multiple concepts, enabling the fine-tuned\nmodel to generate music incorporating either individual or multiple concepts\nsimultaneously. Since we are the first to work on the customized music\ngeneration task, we also introduce a new dataset and evaluation protocol for\nthe new task. Our proposed Jen1-DreamStyler outperforms several baselines in\nboth qualitative and quantitative evaluations. Demos will be available at\nhttps://www.jenmusic.ai/research#DreamStyler.",
            "upvotes": 2
        },
        "publishedAt": "2024-06-19T02:38:39.013Z",
        "title": "JEN-1 DreamStyler: Customized Musical Concept Learning via Pivotal Parameters Tuning",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.12292.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.11939",
            "authors": [
                {
                    "_id": "667304d105a87a94e686c084",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ab7f188a1a9187c219e597/cieU4bdmPIn9dTNIVf3n7.jpeg",
                        "isPro": false,
                        "fullname": "Tianle Li",
                        "user": "Timmli",
                        "type": "user"
                    },
                    "name": "Tianle Li",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2024-06-19T16:28:32.308Z",
                    "hidden": false
                },
                {
                    "_id": "667304d105a87a94e686c085",
                    "user": {
                        "avatarUrl": "/avatars/ec68d20dcedf4553bb31d9f6e0ded813.svg",
                        "isPro": true,
                        "fullname": "Wei-Lin Chiang",
                        "user": "weichiang",
                        "type": "user"
                    },
                    "name": "Wei-Lin Chiang",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2024-06-19T16:21:23.012Z",
                    "hidden": false
                },
                {
                    "_id": "667304d105a87a94e686c086",
                    "name": "Evan Frick",
                    "hidden": false
                },
                {
                    "_id": "667304d105a87a94e686c087",
                    "name": "Lisa Dunlap",
                    "hidden": false
                },
                {
                    "_id": "667304d105a87a94e686c088",
                    "name": "Tianhao Wu",
                    "hidden": false
                },
                {
                    "_id": "667304d105a87a94e686c089",
                    "name": "Banghua Zhu",
                    "hidden": false
                },
                {
                    "_id": "667304d105a87a94e686c08a",
                    "name": "Joseph E. Gonzalez",
                    "hidden": false
                },
                {
                    "_id": "667304d105a87a94e686c08b",
                    "name": "Ion Stoica",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-17T17:26:10.000Z",
            "title": "From Crowdsourced Data to High-Quality Benchmarks: Arena-Hard and\n  BenchBuilder Pipeline",
            "summary": "The rapid evolution of language models has necessitated the development of\nmore challenging benchmarks. Current static benchmarks often struggle to\nconsistently distinguish between the capabilities of different models and fail\nto align with real-world user preferences. On the other hand, live\ncrowd-sourced platforms like the Chatbot Arena collect a wide range of natural\nprompts and user feedback. However, these prompts vary in sophistication and\nthe feedback cannot be applied offline to new models. In order to ensure that\nbenchmarks keep up with the pace of LLM development, we address how one can\nevaluate benchmarks on their ability to confidently separate models and their\nalignment with human preference. Under these principles, we developed\nBenchBuilder, a living benchmark that filters high-quality prompts from live\ndata sources to enable offline evaluation on fresh, challenging prompts.\nBenchBuilder identifies seven indicators of a high-quality prompt, such as the\nrequirement for domain knowledge, and utilizes an LLM annotator to select a\nhigh-quality subset of prompts from various topic clusters. The LLM evaluation\nprocess employs an LLM judge to ensure a fully automated, high-quality, and\nconstantly updating benchmark. We apply BenchBuilder on prompts from the\nChatbot Arena to create Arena-Hard-Auto v0.1: 500 challenging user prompts from\na wide range of tasks. Arena-Hard-Auto v0.1 offers 3x tighter confidence\nintervals than MT-Bench and achieves a state-of-the-art 89.1% agreement with\nhuman preference rankings, all at a cost of only $25 and without human\nlabelers. The BenchBuilder pipeline enhances evaluation benchmarks and provides\na valuable tool for developers, enabling them to extract high-quality\nbenchmarks from extensive data with minimal effort.",
            "upvotes": 0
        },
        "publishedAt": "2024-06-19T15:00:23.748Z",
        "title": "From Crowdsourced Data to High-Quality Benchmarks: Arena-Hard and BenchBuilder Pipeline",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.11939.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ab7f188a1a9187c219e597/cieU4bdmPIn9dTNIVf3n7.jpeg",
            "fullname": "Tianle Li",
            "name": "Timmli",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    }
]