[
    {
        "paper": {
            "id": "2410.18057",
            "authors": [
                {
                    "_id": "671a4315a7873ff9b21ce9fb",
                    "user": {
                        "_id": "60cd95ee15ecba5f2200304a",
                        "avatarUrl": "/avatars/42fcc5a4b14550b7174b8417d3915709.svg",
                        "isPro": false,
                        "fullname": "Alexey Dontsov",
                        "user": "therem",
                        "type": "user"
                    },
                    "name": "Alexey Dontsov",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2024-10-30T06:58:08.467Z",
                    "hidden": false
                },
                {
                    "_id": "671a4315a7873ff9b21ce9fc",
                    "name": "Dmitrii Korzh",
                    "hidden": false
                },
                {
                    "_id": "671a4315a7873ff9b21ce9fd",
                    "name": "Alexey Zhavoronkin",
                    "hidden": false
                },
                {
                    "_id": "671a4315a7873ff9b21ce9fe",
                    "name": "Boris Mikheev",
                    "hidden": false
                },
                {
                    "_id": "671a4315a7873ff9b21ce9ff",
                    "name": "Denis Bobkov",
                    "hidden": false
                },
                {
                    "_id": "671a4315a7873ff9b21cea00",
                    "name": "Aibek Alanov",
                    "hidden": false
                },
                {
                    "_id": "671a4315a7873ff9b21cea01",
                    "name": "Oleg Y. Rogov",
                    "hidden": false
                },
                {
                    "_id": "671a4315a7873ff9b21cea02",
                    "name": "Ivan Oseledets",
                    "hidden": false
                },
                {
                    "_id": "671a4315a7873ff9b21cea03",
                    "name": "Elena Tutubalina",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-23T17:30:50.000Z",
            "title": "CLEAR: Character Unlearning in Textual and Visual Modalities",
            "summary": "Machine Unlearning (MU) is critical for enhancing privacy and security in\ndeep learning models, particularly in large multimodal language models (MLLMs),\nby removing specific private or hazardous information. While MU has made\nsignificant progress in textual and visual modalities, multimodal unlearning\n(MMU) remains significantly underexplored, partially due to the absence of a\nsuitable open-source benchmark. To address this, we introduce CLEAR, a new\nbenchmark designed to evaluate MMU methods. CLEAR contains 200 fictitious\nindividuals and 3,700 images linked with corresponding question-answer pairs,\nenabling a thorough evaluation across modalities. We assess 10 MU methods,\nadapting them for MMU, and highlight new challenges specific to multimodal\nforgetting. We also demonstrate that simple ell_1 regularization on LoRA\nweights significantly mitigates catastrophic forgetting, preserving model\nperformance on retained data. The dataset is available at\nhttps://huggingface.co/datasets/therem/CLEAR",
            "upvotes": 166,
            "discussionId": "671a4318a7873ff9b21ceb14"
        },
        "publishedAt": "2024-10-30T05:21:40.034Z",
        "title": "CLEAR: Character Unlearning in Textual and Visual Modalities",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.18057.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/03707f5ea4e2aa8dc825a9782b00ed85.svg",
            "fullname": "Aibek Alanov",
            "name": "ai-alanov",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 2
        }
    },
    {
        "paper": {
            "id": "2410.20424",
            "authors": [
                {
                    "_id": "6721abd9c8acb97a181c3674",
                    "name": "Ziming Li",
                    "hidden": false
                },
                {
                    "_id": "6721abd9c8acb97a181c3675",
                    "name": "Qianbo Zang",
                    "hidden": false
                },
                {
                    "_id": "6721abd9c8acb97a181c3676",
                    "name": "David Ma",
                    "hidden": false
                },
                {
                    "_id": "6721abd9c8acb97a181c3677",
                    "name": "Jiawei Guo",
                    "hidden": false
                },
                {
                    "_id": "6721abd9c8acb97a181c3678",
                    "user": {
                        "_id": "64ab99dcb76bfd863eba64c1",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ab99dcb76bfd863eba64c1/UBXwDPx17X-gl-SzBPvrc.jpeg",
                        "isPro": false,
                        "fullname": "TY.Zheng",
                        "user": "aaabiao",
                        "type": "user"
                    },
                    "name": "Tuney Zheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-30T09:52:11.057Z",
                    "hidden": false
                },
                {
                    "_id": "6721abd9c8acb97a181c3679",
                    "user": {
                        "_id": "6417d9ea8f689506e7148417",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6417d9ea8f689506e7148417/bAYcruWNw4WvmuQcGgcwC.jpeg",
                        "isPro": false,
                        "fullname": "minghao",
                        "user": "Liam-Liu",
                        "type": "user"
                    },
                    "name": "Minghao Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-30T09:52:13.528Z",
                    "hidden": false
                },
                {
                    "_id": "6721abd9c8acb97a181c367a",
                    "name": "Xinyao Niu",
                    "hidden": false
                },
                {
                    "_id": "6721abd9c8acb97a181c367b",
                    "name": "Yue Wang",
                    "hidden": false
                },
                {
                    "_id": "6721abd9c8acb97a181c367c",
                    "name": "Jian Yang",
                    "hidden": false
                },
                {
                    "_id": "6721abd9c8acb97a181c367d",
                    "name": "Jiaheng Liu",
                    "hidden": false
                },
                {
                    "_id": "6721abd9c8acb97a181c367e",
                    "name": "Wanjun Zhong",
                    "hidden": false
                },
                {
                    "_id": "6721abd9c8acb97a181c367f",
                    "name": "Wangchunshu Zhou",
                    "hidden": false
                },
                {
                    "_id": "6721abd9c8acb97a181c3680",
                    "name": "Wenhao Huang",
                    "hidden": false
                },
                {
                    "_id": "6721abd9c8acb97a181c3681",
                    "user": {
                        "_id": "638efcf4c67af472d316d424",
                        "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
                        "isPro": false,
                        "fullname": "Ge Zhang",
                        "user": "zhangysk",
                        "type": "user"
                    },
                    "name": "Ge Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-30T09:52:09.081Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-27T12:44:25.000Z",
            "title": "AutoKaggle: A Multi-Agent Framework for Autonomous Data Science\n  Competitions",
            "summary": "Data science tasks involving tabular data present complex challenges that\nrequire sophisticated problem-solving approaches. We propose AutoKaggle, a\npowerful and user-centric framework that assists data scientists in completing\ndaily data pipelines through a collaborative multi-agent system. AutoKaggle\nimplements an iterative development process that combines code execution,\ndebugging, and comprehensive unit testing to ensure code correctness and logic\nconsistency. The framework offers highly customizable workflows, allowing users\nto intervene at each phase, thus integrating automated intelligence with human\nexpertise. Our universal data science toolkit, comprising validated functions\nfor data cleaning, feature engineering, and modeling, forms the foundation of\nthis solution, enhancing productivity by streamlining common tasks. We selected\n8 Kaggle competitions to simulate data processing workflows in real-world\napplication scenarios. Evaluation results demonstrate that AutoKaggle achieves\na validation submission rate of 0.85 and a comprehensive score of 0.82 in\ntypical data science pipelines, fully proving its effectiveness and\npracticality in handling complex data science tasks.",
            "upvotes": 17,
            "discussionId": "6721abd9c8acb97a181c36b2"
        },
        "publishedAt": "2024-10-30T02:16:36.358Z",
        "title": "AutoKaggle: A Multi-Agent Framework for Autonomous Data Science Competitions",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.20424.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
            "fullname": "Ge Zhang",
            "name": "zhangysk",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 30
        }
    },
    {
        "paper": {
            "id": "2410.19609",
            "authors": [
                {
                    "_id": "6720ac237fb8a490619de385",
                    "name": "Hongliang He",
                    "hidden": false
                },
                {
                    "_id": "6720ac237fb8a490619de386",
                    "name": "Wenlin Yao",
                    "hidden": false
                },
                {
                    "_id": "6720ac237fb8a490619de387",
                    "name": "Kaixin Ma",
                    "hidden": false
                },
                {
                    "_id": "6720ac237fb8a490619de388",
                    "name": "Wenhao Yu",
                    "hidden": false
                },
                {
                    "_id": "6720ac237fb8a490619de389",
                    "name": "Hongming Zhang",
                    "hidden": false
                },
                {
                    "_id": "6720ac237fb8a490619de38a",
                    "name": "Tianqing Fang",
                    "hidden": false
                },
                {
                    "_id": "6720ac237fb8a490619de38b",
                    "name": "Zhenzhong Lan",
                    "hidden": false
                },
                {
                    "_id": "6720ac237fb8a490619de38c",
                    "name": "Dong Yu",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-25T15:01:27.000Z",
            "title": "OpenWebVoyager: Building Multimodal Web Agents via Iterative Real-World\n  Exploration, Feedback and Optimization",
            "summary": "The rapid development of large language and multimodal models has sparked\nsignificant interest in using proprietary models, such as GPT-4o, to develop\nautonomous agents capable of handling real-world scenarios like web navigation.\nAlthough recent open-source efforts have tried to equip agents with the ability\nto explore environments and continuously improve over time, they are building\ntext-only agents in synthetic environments where the reward signals are clearly\ndefined. Such agents struggle to generalize to realistic settings that require\nmultimodal perception abilities and lack ground-truth signals. In this paper,\nwe introduce an open-source framework designed to facilitate the development of\nmultimodal web agent that can autonomously conduct real-world exploration and\nimprove itself. We first train the base model with imitation learning to gain\nthe basic abilities. We then let the agent explore the open web and collect\nfeedback on its trajectories. After that, it further improves its policy by\nlearning from well-performing trajectories judged by another general-purpose\nmodel. This exploration-feedback-optimization cycle can continue for several\niterations. Experimental results show that our web agent successfully improves\nitself after each iteration, demonstrating strong performance across multiple\ntest sets.",
            "upvotes": 11,
            "discussionId": "6720ac247fb8a490619de3c4"
        },
        "publishedAt": "2024-10-30T03:04:13.107Z",
        "title": "OpenWebVoyager: Building Multimodal Web Agents via Iterative Real-World Exploration, Feedback and Optimization",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.19609.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/40549a59fc5ba04a4baa5a1d5dba0847.svg",
            "fullname": "Wenlin Yao",
            "name": "wenlinyao",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        }
    },
    {
        "paper": {
            "id": "2410.21411",
            "authors": [
                {
                    "_id": "6721931db02ab0e490b45411",
                    "user": {
                        "_id": "658bb7e47459b6e471b9d2e6",
                        "avatarUrl": "/avatars/efd8051b468b4dbcb5d149479de67c58.svg",
                        "isPro": false,
                        "fullname": "Wanhua Li",
                        "user": "EthanTaylor",
                        "type": "user"
                    },
                    "name": "Wanhua Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-30T09:52:46.803Z",
                    "hidden": false
                },
                {
                    "_id": "6721931db02ab0e490b45412",
                    "user": {
                        "_id": "64d3a3a29d84115700581720",
                        "avatarUrl": "/avatars/db23c6585975e08c6629393538ef8980.svg",
                        "isPro": false,
                        "fullname": "Zibin Meng",
                        "user": "zmeng0116",
                        "type": "user"
                    },
                    "name": "Zibin Meng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-30T09:52:48.983Z",
                    "hidden": false
                },
                {
                    "_id": "6721931db02ab0e490b45413",
                    "name": "Jiawei Zhou",
                    "hidden": false
                },
                {
                    "_id": "6721931db02ab0e490b45414",
                    "name": "Donglai Wei",
                    "hidden": false
                },
                {
                    "_id": "6721931db02ab0e490b45415",
                    "name": "Chuang Gan",
                    "hidden": false
                },
                {
                    "_id": "6721931db02ab0e490b45416",
                    "name": "Hanspeter Pfister",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-28T18:10:26.000Z",
            "title": "SocialGPT: Prompting LLMs for Social Relation Reasoning via Greedy\n  Segment Optimization",
            "summary": "Social relation reasoning aims to identify relation categories such as\nfriends, spouses, and colleagues from images. While current methods adopt the\nparadigm of training a dedicated network end-to-end using labeled image data,\nthey are limited in terms of generalizability and interpretability. To address\nthese issues, we first present a simple yet well-crafted framework named\n{\\name}, which combines the perception capability of Vision Foundation Models\n(VFMs) and the reasoning capability of Large Language Models (LLMs) within a\nmodular framework, providing a strong baseline for social relation recognition.\nSpecifically, we instruct VFMs to translate image content into a textual social\nstory, and then utilize LLMs for text-based reasoning. {\\name} introduces\nsystematic design principles to adapt VFMs and LLMs separately and bridge their\ngaps. Without additional model training, it achieves competitive zero-shot\nresults on two databases while offering interpretable answers, as LLMs can\ngenerate language-based explanations for the decisions. The manual prompt\ndesign process for LLMs at the reasoning phase is tedious and an automated\nprompt optimization method is desired. As we essentially convert a visual\nclassification task into a generative task of LLMs, automatic prompt\noptimization encounters a unique long prompt optimization issue. To address\nthis issue, we further propose the Greedy Segment Prompt Optimization (GSPO),\nwhich performs a greedy search by utilizing gradient information at the segment\nlevel. Experimental results show that GSPO significantly improves performance,\nand our method also generalizes to different image styles. The code is\navailable at https://github.com/Mengzibin/SocialGPT.",
            "upvotes": 11,
            "discussionId": "6721931eb02ab0e490b454d0"
        },
        "publishedAt": "2024-10-30T01:47:45.994Z",
        "title": "SocialGPT: Prompting LLMs for Social Relation Reasoning via Greedy Segment Optimization",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.21411.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "/avatars/efd8051b468b4dbcb5d149479de67c58.svg",
            "fullname": "Wanhua Li",
            "name": "EthanTaylor",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.22304",
            "authors": [
                {
                    "_id": "672196c94ed4ff86a1a2d479",
                    "user": {
                        "_id": "642f4c789b2484d7d8551a93",
                        "avatarUrl": "/avatars/aa49b92a4e650f0837fd7436d14e6426.svg",
                        "isPro": true,
                        "fullname": "Yihe Deng",
                        "user": "ydeng9",
                        "type": "user"
                    },
                    "name": "Yihe Deng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-30T09:52:44.867Z",
                    "hidden": false
                },
                {
                    "_id": "672196c94ed4ff86a1a2d47a",
                    "name": "Paul Mineiro",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-29T17:50:31.000Z",
            "title": "Flow-DPO: Improving LLM Mathematical Reasoning through Online\n  Multi-Agent Learning",
            "summary": "Mathematical reasoning is a crucial capability for Large Language Models\n(LLMs), yet generating detailed and accurate reasoning traces remains a\nsignificant challenge. This paper introduces a novel approach to produce\nhigh-quality reasoning traces for LLM fine-tuning using online learning\nFlows. Our method employs an incremental output production Flow, where\ncomponent LLMs collaboratively construct solutions through iterative\ncommunication. We train the Flow using online Direct Preference Optimization\n(DPO) learning with rollouts, generating DPO pairs for each training example\nand updating models in real-time. We directly compare the quality of reasoning\ntraces generated by our method with those produced through direct model\ninference, demonstrating the effectiveness of our approach in improving LLM\nperformance in mathematical reasoning tasks.",
            "upvotes": 8,
            "discussionId": "672196ca4ed4ff86a1a2d4a0"
        },
        "publishedAt": "2024-10-30T00:46:03.166Z",
        "title": "Flow-DPO: Improving LLM Mathematical Reasoning through Online Multi-Agent Learning",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.22304.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/aa49b92a4e650f0837fd7436d14e6426.svg",
            "fullname": "Yihe Deng",
            "name": "ydeng9",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isMod": false,
            "followerCount": 8
        }
    },
    {
        "paper": {
            "id": "2410.22325",
            "authors": [
                {
                    "_id": "67219f7ee14a54a801766802",
                    "name": "Guangqi Jiang",
                    "hidden": false
                },
                {
                    "_id": "67219f7ee14a54a801766803",
                    "name": "Yifei Sun",
                    "hidden": false
                },
                {
                    "_id": "67219f7ee14a54a801766804",
                    "name": "Tao Huang",
                    "hidden": false
                },
                {
                    "_id": "67219f7ee14a54a801766805",
                    "name": "Huanyu Li",
                    "hidden": false
                },
                {
                    "_id": "67219f7ee14a54a801766806",
                    "name": "Yongyuan Liang",
                    "hidden": false
                },
                {
                    "_id": "67219f7ee14a54a801766807",
                    "name": "Huazhe Xu",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-29T17:58:13.000Z",
            "title": "Robots Pre-train Robots: Manipulation-Centric Robotic Representation\n  from Large-Scale Robot Dataset",
            "summary": "The pre-training of visual representations has enhanced the efficiency of\nrobot learning. Due to the lack of large-scale in-domain robotic datasets,\nprior works utilize in-the-wild human videos to pre-train robotic visual\nrepresentation. Despite their promising results, representations from human\nvideos are inevitably subject to distribution shifts and lack the dynamics\ninformation crucial for task completion. We first evaluate various pre-trained\nrepresentations in terms of their correlation to the downstream robotic\nmanipulation tasks (i.e., manipulation centricity). Interestingly, we find that\nthe \"manipulation centricity\" is a strong indicator of success rates when\napplied to downstream tasks. Drawing from these findings, we propose\nManipulation Centric Representation (MCR), a foundation representation learning\nframework capturing both visual features and the dynamics information such as\nactions and proprioceptions of manipulation tasks to improve manipulation\ncentricity. Specifically, we pre-train a visual encoder on the DROID robotic\ndataset and leverage motion-relevant data such as robot proprioceptive states\nand actions. We introduce a novel contrastive loss that aligns visual\nobservations with the robot's proprioceptive state-action dynamics, combined\nwith a behavior cloning (BC)-like actor loss to predict actions during\npre-training, along with a time contrastive loss. Empirical results across 4\nsimulation domains with 20 tasks verify that MCR outperforms the strongest\nbaseline method by 14.8%. Moreover, MCR boosts the performance of\ndata-efficient learning with a UR5e arm on 3 real-world tasks by 76.9%. Project\nwebsite: https://robots-pretrain-robots.github.io/.",
            "upvotes": 5,
            "discussionId": "67219f82e14a54a80176696d"
        },
        "publishedAt": "2024-10-30T01:23:17.885Z",
        "title": "Robots Pre-train Robots: Manipulation-Centric Robotic Representation from Large-Scale Robot Dataset",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.22325.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/JFH3ZTPvlaVSg4RJJBb6L.jpeg",
            "fullname": "Yongyuan Liang",
            "name": "cheryyunl",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        }
    },
    {
        "paper": {
            "id": "2410.21465",
            "authors": [
                {
                    "_id": "67219228541e69bc0e6d3b6b",
                    "user": {
                        "_id": "651b23a0a6e00a1678c9d8af",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/651b23a0a6e00a1678c9d8af/_Ip-ysvcjGpFCI9HtcTgx.jpeg",
                        "isPro": false,
                        "fullname": "Hanshi",
                        "user": "preminstrel",
                        "type": "user"
                    },
                    "name": "Hanshi Sun",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-30T09:52:51.173Z",
                    "hidden": false
                },
                {
                    "_id": "67219228541e69bc0e6d3b6c",
                    "name": "Li-Wen Chang",
                    "hidden": false
                },
                {
                    "_id": "67219228541e69bc0e6d3b6d",
                    "name": "Wenlei Bao",
                    "hidden": false
                },
                {
                    "_id": "67219228541e69bc0e6d3b6e",
                    "name": "Size Zheng",
                    "hidden": false
                },
                {
                    "_id": "67219228541e69bc0e6d3b6f",
                    "name": "Ningxin Zheng",
                    "hidden": false
                },
                {
                    "_id": "67219228541e69bc0e6d3b70",
                    "name": "Xin Liu",
                    "hidden": false
                },
                {
                    "_id": "67219228541e69bc0e6d3b71",
                    "name": "Harry Dong",
                    "hidden": false
                },
                {
                    "_id": "67219228541e69bc0e6d3b72",
                    "name": "Yuejie Chi",
                    "hidden": false
                },
                {
                    "_id": "67219228541e69bc0e6d3b73",
                    "name": "Beidi Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-28T19:08:12.000Z",
            "title": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM\n  Inference",
            "summary": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6times larger batch\nsizes and boost throughput by up to 3.04times on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV.",
            "upvotes": 4,
            "discussionId": "67219229541e69bc0e6d3ba9"
        },
        "publishedAt": "2024-10-30T00:26:43.152Z",
        "title": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.21465.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/651b23a0a6e00a1678c9d8af/_Ip-ysvcjGpFCI9HtcTgx.jpeg",
            "fullname": "Hanshi",
            "name": "preminstrel",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        }
    },
    {
        "paper": {
            "id": "2410.21845",
            "authors": [
                {
                    "_id": "672190cb52c4c6ffde46d129",
                    "user": {
                        "_id": "64f8cb8ed04a890f5380d9a4",
                        "avatarUrl": "/avatars/d6fdfdbb0c10141aa3b4c832d928121b.svg",
                        "isPro": false,
                        "fullname": "Jianlan Luo",
                        "user": "jianlanluo",
                        "type": "user"
                    },
                    "name": "Jianlan Luo",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2024-10-30T02:00:09.505Z",
                    "hidden": false
                },
                {
                    "_id": "672190cb52c4c6ffde46d12a",
                    "user": {
                        "_id": "658fdaacac02633c0d78e3e8",
                        "avatarUrl": "/avatars/c63481494f86368185a9e33587de77d3.svg",
                        "isPro": false,
                        "fullname": "Charles Xu",
                        "user": "charlesxu0124",
                        "type": "user"
                    },
                    "name": "Charles Xu",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2024-10-30T02:18:37.616Z",
                    "hidden": false
                },
                {
                    "_id": "672190cb52c4c6ffde46d12b",
                    "name": "Jeffrey Wu",
                    "hidden": false
                },
                {
                    "_id": "672190cb52c4c6ffde46d12c",
                    "name": "Sergey Levine",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-29T08:12:20.000Z",
            "title": "Precise and Dexterous Robotic Manipulation via Human-in-the-Loop\n  Reinforcement Learning",
            "summary": "Reinforcement learning (RL) holds great promise for enabling autonomous\nacquisition of complex robotic manipulation skills, but realizing this\npotential in real-world settings has been challenging. We present a\nhuman-in-the-loop vision-based RL system that demonstrates impressive\nperformance on a diverse set of dexterous manipulation tasks, including dynamic\nmanipulation, precision assembly, and dual-arm coordination. Our approach\nintegrates demonstrations and human corrections, efficient RL algorithms, and\nother system-level design choices to learn policies that achieve near-perfect\nsuccess rates and fast cycle times within just 1 to 2.5 hours of training. We\nshow that our method significantly outperforms imitation learning baselines and\nprior RL approaches, with an average 2x improvement in success rate and 1.8x\nfaster execution. Through extensive experiments and analysis, we provide\ninsights into the effectiveness of our approach, demonstrating how it learns\nrobust, adaptive policies for both reactive and predictive control strategies.\nOur results suggest that RL can indeed learn a wide range of complex\nvision-based manipulation policies directly in the real world within practical\ntraining times. We hope this work will inspire a new generation of learned\nrobotic manipulation techniques, benefiting both industrial applications and\nresearch advancements. Videos and code are available at our project website\nhttps://hil-serl.github.io/.",
            "upvotes": 4,
            "discussionId": "672190cf52c4c6ffde46d2b5"
        },
        "publishedAt": "2024-10-30T00:22:10.932Z",
        "title": "Precise and Dexterous Robotic Manipulation via Human-in-the-Loop Reinforcement Learning",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64f8cb8ed04a890f5380d9a4/_nshPyx99NApx6A_AnjF1.mp4",
            "https://cdn-uploads.huggingface.co/production/uploads/64f8cb8ed04a890f5380d9a4/0l2hbONAIIvroV-TrFH8y.mp4",
            "https://cdn-uploads.huggingface.co/production/uploads/64f8cb8ed04a890f5380d9a4/NXP1HXB9g4AFSuUf0EUol.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.21845.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/d6fdfdbb0c10141aa3b4c832d928121b.svg",
            "fullname": "Jianlan Luo",
            "name": "jianlanluo",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 3
        }
    },
    {
        "paper": {
            "id": "2410.20088",
            "authors": [
                {
                    "_id": "67216eb8a907ce439de3ea07",
                    "user": {
                        "_id": "64eff79182c6eea604bd8ad4",
                        "avatarUrl": "/avatars/df1dd421fd3849d5d4d88c5e95a0fa14.svg",
                        "isPro": false,
                        "fullname": "Atula Tejaswi",
                        "user": "atutej",
                        "type": "user"
                    },
                    "name": "Atula Tejaswi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-30T09:52:53.289Z",
                    "hidden": false
                },
                {
                    "_id": "67216eb8a907ce439de3ea08",
                    "name": "Yoonsang Lee",
                    "hidden": false
                },
                {
                    "_id": "67216eb8a907ce439de3ea09",
                    "name": "Sujay Sanghavi",
                    "hidden": false
                },
                {
                    "_id": "67216eb8a907ce439de3ea0a",
                    "name": "Eunsol Choi",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-26T05:46:20.000Z",
            "title": "RARe: Retrieval Augmented Retrieval with In-Context Examples",
            "summary": "We investigate whether in-context examples, widely used in decoder-only\nlanguage models (LLMs), can improve embedding model performance in retrieval\ntasks. Unlike in LLMs, naively prepending in-context examples (query-document\npairs) to the target query at inference time does not work out of the box. We\nintroduce a simple approach to enable retrievers to use in-context examples.\nOur approach, RARe, finetunes a pre-trained model with in-context examples\nwhose query is semantically similar to the target query. This can be applied to\nadapt various base architectures (i.e., decoder-only language models, retriever\nmodels) and consistently achieves performance gains of up to +2.72% nDCG across\nvarious open-domain retrieval datasets (BeIR, RAR-b). In particular, we find\nRARe exhibits stronger out-of-domain generalization compared to models using\nqueries without in-context examples, similar to what is seen for in-context\nlearning in LLMs. We further provide analysis on the design choices of\nin-context example augmentation and lay the foundation for future work in this\nspace.",
            "upvotes": 1,
            "discussionId": "67216eb9a907ce439de3ea4b"
        },
        "publishedAt": "2024-10-30T14:31:13.848Z",
        "title": "RARe: Retrieval Augmented Retrieval with In-Context Examples",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64eff79182c6eea604bd8ad4/MrXGcD_1qG-TgGFUmrKR7.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.20088.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/df1dd421fd3849d5d4d88c5e95a0fa14.svg",
            "fullname": "Atula Tejaswi",
            "name": "atutej",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.21333",
            "authors": [
                {
                    "_id": "672221bd3c54079d2cdeb460",
                    "name": "Ryan Liu",
                    "hidden": false
                },
                {
                    "_id": "672221bd3c54079d2cdeb461",
                    "name": "Jiayi Geng",
                    "hidden": false
                },
                {
                    "_id": "672221bd3c54079d2cdeb462",
                    "name": "Addison J. Wu",
                    "hidden": false
                },
                {
                    "_id": "672221bd3c54079d2cdeb463",
                    "name": "Ilia Sucholutsky",
                    "hidden": false
                },
                {
                    "_id": "672221bd3c54079d2cdeb464",
                    "name": "Tania Lombrozo",
                    "hidden": false
                },
                {
                    "_id": "672221bd3c54079d2cdeb465",
                    "name": "Thomas L. Griffiths",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-27T18:30:41.000Z",
            "title": "Mind Your Step (by Step): Chain-of-Thought can Reduce Performance on\n  Tasks where Thinking Makes Humans Worse",
            "summary": "Chain-of-thought (CoT) prompting has become a widely used strategy for\nworking with large language and multimodal models. While CoT has been shown to\nimprove performance across many tasks, determining the settings in which it is\neffective remains an ongoing effort. In particular, it is still an open\nquestion in what settings CoT systematically reduces model performance. In this\npaper, we seek to identify the characteristics of tasks where CoT reduces\nperformance by drawing inspiration from cognitive psychology, looking at cases\nwhere (i) verbal thinking or deliberation hurts performance in humans, and (ii)\nthe constraints governing human performance generalize to language models.\nThree such cases are implicit statistical learning, visual recognition, and\nclassifying with patterns containing exceptions. In extensive experiments\nacross all three settings, we find that a diverse collection of\nstate-of-the-art models exhibit significant drop-offs in performance (e.g., up\nto 36.3% absolute accuracy for OpenAI o1-preview compared to GPT-4o) when using\ninference-time reasoning compared to zero-shot counterparts. We also identify\nthree tasks that satisfy condition (i) but not (ii), and find that while verbal\nthinking reduces human performance in these tasks, CoT retains or increases\nmodel performance. Overall, our results show that while there is not an exact\nparallel between the cognitive processes of models and those of humans,\nconsidering cases where thinking has negative consequences for human\nperformance can help us identify settings where it negatively impacts models.\nBy connecting the literature on human deliberation with evaluations of CoT, we\noffer a new tool that can be used in understanding the impact of prompt choices\nand inference-time reasoning.",
            "upvotes": 1,
            "discussionId": "672221be3c54079d2cdeb4c6"
        },
        "publishedAt": "2024-10-30T12:21:23.665Z",
        "title": "Mind Your Step (by Step): Chain-of-Thought can Reduce Performance on Tasks where Thinking Makes Humans Worse",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/647a12028d17fcf64761a3fc/FCh-8lgoBgHrFYrRPrbHY.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.21333.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/3GEiY7YCU_FzJing5wP-C.jpeg",
            "fullname": "Ryan Liu",
            "name": "theryanliu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    }
]