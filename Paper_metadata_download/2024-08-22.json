[
    {
        "paper": {
            "id": "2408.11318",
            "authors": [
                {
                    "_id": "66c6a6df191c0295d6cfc8c7",
                    "name": "Hyeongmin Lee",
                    "hidden": false
                },
                {
                    "_id": "66c6a6df191c0295d6cfc8c8",
                    "name": "Jin-Young Kim",
                    "hidden": false
                },
                {
                    "_id": "66c6a6df191c0295d6cfc8c9",
                    "name": "Kyungjune Baek",
                    "hidden": false
                },
                {
                    "_id": "66c6a6df191c0295d6cfc8ca",
                    "name": "Jihwan Kim",
                    "hidden": false
                },
                {
                    "_id": "66c6a6df191c0295d6cfc8cb",
                    "name": "Hyojun Go",
                    "hidden": false
                },
                {
                    "_id": "66c6a6df191c0295d6cfc8cc",
                    "name": "Seongsu Ha",
                    "hidden": false
                },
                {
                    "_id": "66c6a6df191c0295d6cfc8cd",
                    "name": "Seokjin Han",
                    "hidden": false
                },
                {
                    "_id": "66c6a6df191c0295d6cfc8ce",
                    "name": "Jiho Jang",
                    "hidden": false
                },
                {
                    "_id": "66c6a6df191c0295d6cfc8cf",
                    "name": "Raehyuk Jung",
                    "hidden": false
                },
                {
                    "_id": "66c6a6df191c0295d6cfc8d0",
                    "name": "Daewoo Kim",
                    "hidden": false
                },
                {
                    "_id": "66c6a6df191c0295d6cfc8d1",
                    "name": "GeunOh Kim",
                    "hidden": false
                },
                {
                    "_id": "66c6a6df191c0295d6cfc8d2",
                    "name": "JongMok Kim",
                    "hidden": false
                },
                {
                    "_id": "66c6a6df191c0295d6cfc8d3",
                    "name": "Jongseok Kim",
                    "hidden": false
                },
                {
                    "_id": "66c6a6df191c0295d6cfc8d4",
                    "name": "Junwan Kim",
                    "hidden": false
                },
                {
                    "_id": "66c6a6df191c0295d6cfc8d5",
                    "name": "Soonwoo Kwon",
                    "hidden": false
                },
                {
                    "_id": "66c6a6df191c0295d6cfc8d6",
                    "name": "Jangwon Lee",
                    "hidden": false
                },
                {
                    "_id": "66c6a6df191c0295d6cfc8d7",
                    "name": "Seungjoon Park",
                    "hidden": false
                },
                {
                    "_id": "66c6a6df191c0295d6cfc8d8",
                    "name": "Minjoon Seo",
                    "hidden": false
                },
                {
                    "_id": "66c6a6df191c0295d6cfc8d9",
                    "name": "Jay Suh",
                    "hidden": false
                },
                {
                    "_id": "66c6a6df191c0295d6cfc8da",
                    "name": "Jaehyuk Yi",
                    "hidden": false
                },
                {
                    "_id": "66c6a6df191c0295d6cfc8db",
                    "name": "Aiden Lee",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-08-21T03:56:27.000Z",
            "title": "TWLV-I: Analysis and Insights from Holistic Evaluation on Video\n  Foundation Models",
            "summary": "In this work, we discuss evaluating video foundation models in a fair and\nrobust manner. Unlike language or image foundation models, many video\nfoundation models are evaluated with differing parameters (such as sampling\nrate, number of frames, pretraining steps, etc.), making fair and robust\ncomparisons challenging. Therefore, we present a carefully designed evaluation\nframework for measuring two core capabilities of video comprehension:\nappearance and motion understanding. Our findings reveal that existing video\nfoundation models, whether text-supervised like UMT or InternVideo2, or\nself-supervised like V-JEPA, exhibit limitations in at least one of these\ncapabilities. As an alternative, we introduce TWLV-I, a new video foundation\nmodel that constructs robust visual representations for both motion- and\nappearance-based videos. Based on the average top-1 accuracy of linear probing\non five action recognition benchmarks, pretrained only on publicly accessible\ndatasets, our model shows a 4.6%p improvement compared to V-JEPA (ViT-L) and a\n7.7%p improvement compared to UMT (ViT-L). Even when compared to much larger\nmodels, our model demonstrates a 7.2%p improvement compared to DFN (ViT-H), a\n2.7%p improvement compared to V-JEPA~(ViT-H) and a 2.8%p improvement compared\nto InternVideo2 (ViT-g). We provide embedding vectors obtained by TWLV-I from\nvideos of several commonly used video benchmarks, along with evaluation source\ncode that can directly utilize these embeddings. The code is available on\n\"https://github.com/twelvelabs-io/video-embeddings-evaluation-framework\".",
            "upvotes": 37,
            "discussionId": "66c6a6e1191c0295d6cfc97c"
        },
        "publishedAt": "2024-08-22T01:18:08.423Z",
        "title": "TWLV-I: Analysis and Insights from Holistic Evaluation on Video Foundation Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.11318.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2408.11796",
            "authors": [
                {
                    "_id": "66c6c723e8e0c0e0a2cafd1b",
                    "name": "Sharath Turuvekere Sreenivas",
                    "hidden": false
                },
                {
                    "_id": "66c6c723e8e0c0e0a2cafd1c",
                    "name": "Saurav Muralidharan",
                    "hidden": false
                },
                {
                    "_id": "66c6c723e8e0c0e0a2cafd1d",
                    "name": "Raviraj Joshi",
                    "hidden": false
                },
                {
                    "_id": "66c6c723e8e0c0e0a2cafd1e",
                    "name": "Marcin Chochowski",
                    "hidden": false
                },
                {
                    "_id": "66c6c723e8e0c0e0a2cafd1f",
                    "name": "Mostofa Patwary",
                    "hidden": false
                },
                {
                    "_id": "66c6c723e8e0c0e0a2cafd20",
                    "name": "Mohammad Shoeybi",
                    "hidden": false
                },
                {
                    "_id": "66c6c723e8e0c0e0a2cafd21",
                    "name": "Bryan Catanzaro",
                    "hidden": false
                },
                {
                    "_id": "66c6c723e8e0c0e0a2cafd22",
                    "name": "Jan Kautz",
                    "hidden": false
                },
                {
                    "_id": "66c6c723e8e0c0e0a2cafd23",
                    "name": "Pavlo Molchanov",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-08-21T17:38:48.000Z",
            "title": "LLM Pruning and Distillation in Practice: The Minitron Approach",
            "summary": "We present a comprehensive report on compressing the Llama 3.1 8B and Mistral\nNeMo 12B models to 4B and 8B parameters, respectively, using pruning and\ndistillation. We explore two distinct pruning strategies: (1) depth pruning and\n(2) joint hidden/attention/MLP (width) pruning, and evaluate the results on\ncommon benchmarks from the LM Evaluation Harness. The models are then aligned\nwith NeMo Aligner and tested in instruct-tuned versions. This approach produces\na compelling 4B model from Llama 3.1 8B and a state-of-the-art\nMistral-NeMo-Minitron-8B (MN-Minitron-8B for brevity) model from Mistral NeMo\n12B. We found that with no access to the original data, it is beneficial to\nslightly fine-tune teacher models on the distillation dataset. We open-source\nour base model weights on Hugging Face with a permissive license.",
            "upvotes": 25,
            "discussionId": "66c6c724e8e0c0e0a2cafd7a"
        },
        "publishedAt": "2024-08-22T03:39:53.791Z",
        "title": "LLM Pruning and Distillation in Practice: The Minitron Approach",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.11796.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/450d6378ed877e817137e3eda6e179ce.svg",
            "fullname": "Pavlo",
            "name": "pmolchanov",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2408.11475",
            "authors": [
                {
                    "_id": "66c6a4afc33122b03a2c1c27",
                    "name": "Haitao Zhou",
                    "hidden": false
                },
                {
                    "_id": "66c6a4afc33122b03a2c1c28",
                    "name": "Chuang Wang",
                    "hidden": false
                },
                {
                    "_id": "66c6a4afc33122b03a2c1c29",
                    "name": "Rui Nie",
                    "hidden": false
                },
                {
                    "_id": "66c6a4afc33122b03a2c1c2a",
                    "name": "Jinxiao Lin",
                    "hidden": false
                },
                {
                    "_id": "66c6a4afc33122b03a2c1c2b",
                    "name": "Dongdong Yu",
                    "hidden": false
                },
                {
                    "_id": "66c6a4afc33122b03a2c1c2c",
                    "name": "Qian Yu",
                    "hidden": false
                },
                {
                    "_id": "66c6a4afc33122b03a2c1c2d",
                    "name": "Changhu Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-08-21T09:42:04.000Z",
            "title": "TrackGo: A Flexible and Efficient Method for Controllable Video\n  Generation",
            "summary": "Recent years have seen substantial progress in diffusion-based controllable\nvideo generation. However, achieving precise control in complex scenarios,\nincluding fine-grained object parts, sophisticated motion trajectories, and\ncoherent background movement, remains a challenge. In this paper, we introduce\nTrackGo, a novel approach that leverages free-form masks and arrows for\nconditional video generation. This method offers users with a flexible and\nprecise mechanism for manipulating video content. We also propose the\nTrackAdapter for control implementation, an efficient and lightweight adapter\ndesigned to be seamlessly integrated into the temporal self-attention layers of\na pretrained video generation model. This design leverages our observation that\nthe attention map of these layers can accurately activate regions corresponding\nto motion in videos. Our experimental results demonstrate that our new\napproach, enhanced by the TrackAdapter, achieves state-of-the-art performance\non key metrics such as FVD, FID, and ObjMC scores. The project page of TrackGo\ncan be found at: https://zhtjtcz.github.io/TrackGo-Page/",
            "upvotes": 9,
            "discussionId": "66c6a4b0c33122b03a2c1cb2"
        },
        "publishedAt": "2024-08-22T01:09:25.371Z",
        "title": "TrackGo: A Flexible and Efficient Method for Controllable Video Generation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.11475.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2408.11745",
            "authors": [
                {
                    "_id": "66c6a8c20f390b546ae64da2",
                    "name": "Zhenyu Li",
                    "hidden": false
                },
                {
                    "_id": "66c6a8c20f390b546ae64da3",
                    "name": "Yike Zhang",
                    "hidden": false
                },
                {
                    "_id": "66c6a8c20f390b546ae64da4",
                    "name": "Tengyu Pan",
                    "hidden": false
                },
                {
                    "_id": "66c6a8c20f390b546ae64da5",
                    "name": "Yutao Sun",
                    "hidden": false
                },
                {
                    "_id": "66c6a8c20f390b546ae64da6",
                    "name": "Zhichao Duan",
                    "hidden": false
                },
                {
                    "_id": "66c6a8c20f390b546ae64da7",
                    "name": "Junjie Fang",
                    "hidden": false
                },
                {
                    "_id": "66c6a8c20f390b546ae64da8",
                    "name": "Rong Han",
                    "hidden": false
                },
                {
                    "_id": "66c6a8c20f390b546ae64da9",
                    "name": "Zixuan Wang",
                    "hidden": false
                },
                {
                    "_id": "66c6a8c20f390b546ae64daa",
                    "name": "Jianyong Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-08-21T16:11:59.000Z",
            "title": "FocusLLM: Scaling LLM's Context by Parallel Decoding",
            "summary": "Empowering LLMs with the ability to utilize useful information from a long\ncontext is crucial for many downstream applications. However, achieving long\ncontext lengths with the conventional transformer architecture requires\nsubstantial training and inference resources. In this paper, we present\nFocusLLM, a framework designed to extend the context length of any decoder-only\nLLM, enabling the model to focus on relevant information from very long\nsequences. FocusLLM processes long text inputs by dividing them into chunks\nbased on the model's original context length to alleviate the issue of\nattention distraction. Then, it appends the local context to each chunk as a\nprompt to extract essential information from each chunk based on a novel\nparallel decoding mechanism, and ultimately integrates the extracted\ninformation into the local context. FocusLLM stands out for great training\nefficiency and versatility: trained with an 8K input length with much less\ntraining cost than previous methods, FocusLLM exhibits superior performance\nacross downstream long-context tasks and maintains strong language modeling\nability when handling extensive long texts, even up to 400K tokens. Our code is\navailable at https://github.com/leezythu/FocusLLM.",
            "upvotes": 8,
            "discussionId": "66c6a8c30f390b546ae64df1"
        },
        "publishedAt": "2024-08-22T01:26:29.770Z",
        "title": "FocusLLM: Scaling LLM's Context by Parallel Decoding",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.11745.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2408.11721",
            "authors": [
                {
                    "_id": "66c767219e496a77e83e1954",
                    "name": "Oz Zafar",
                    "hidden": false
                },
                {
                    "_id": "66c767219e496a77e83e1955",
                    "name": "Lior Wolf",
                    "hidden": false
                },
                {
                    "_id": "66c767219e496a77e83e1956",
                    "name": "Idan Schwartz",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-08-21T15:51:46.000Z",
            "title": "Iterative Object Count Optimization for Text-to-image Diffusion Models",
            "summary": "We address a persistent challenge in text-to-image models: accurately\ngenerating a specified number of objects. Current models, which learn from\nimage-text pairs, inherently struggle with counting, as training data cannot\ndepict every possible number of objects for any given object. To solve this, we\npropose optimizing the generated image based on a counting loss derived from a\ncounting model that aggregates an object\\'s potential. Employing an\nout-of-the-box counting model is challenging for two reasons: first, the model\nrequires a scaling hyperparameter for the potential aggregation that varies\ndepending on the viewpoint of the objects, and second, classifier guidance\ntechniques require modified models that operate on noisy intermediate diffusion\nsteps. To address these challenges, we propose an iterated online training mode\nthat improves the accuracy of inferred images while altering the text\nconditioning embedding and dynamically adjusting hyperparameters. Our method\noffers three key advantages: (i) it can consider non-derivable counting\ntechniques based on detection models, (ii) it is a zero-shot plug-and-play\nsolution facilitating rapid changes to the counting techniques and image\ngeneration methods, and (iii) the optimized counting token can be reused to\ngenerate accurate images without additional optimization. We evaluate the\ngeneration of various objects and show significant improvements in accuracy.\nThe project page is available at https://ozzafar.github.io/count_token.",
            "upvotes": 2,
            "discussionId": "66c767259e496a77e83e1aae"
        },
        "publishedAt": "2024-08-22T15:02:33.052Z",
        "title": "Iterative Object Count Optimization for Text-to-image Diffusion Models",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/617fb19badaa26f5e57e97cd/cnRt7kzOBJWJUKFpJqXIt.qt"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.11721.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1645002335184-617fb19badaa26f5e57e97cd.png",
            "fullname": "Schwartz",
            "name": "Idan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2408.11706",
            "authors": [
                {
                    "_id": "66c73909a575572fcb8d3cf5",
                    "name": "Liyao Jiang",
                    "hidden": false
                },
                {
                    "_id": "66c73909a575572fcb8d3cf6",
                    "name": "Negar Hassanpour",
                    "hidden": false
                },
                {
                    "_id": "66c73909a575572fcb8d3cf7",
                    "name": "Mohammad Salameh",
                    "hidden": false
                },
                {
                    "_id": "66c73909a575572fcb8d3cf8",
                    "name": "Mohan Sai Singamsetti",
                    "hidden": false
                },
                {
                    "_id": "66c73909a575572fcb8d3cf9",
                    "name": "Fengyu Sun",
                    "hidden": false
                },
                {
                    "_id": "66c73909a575572fcb8d3cfa",
                    "name": "Wei Lu",
                    "hidden": false
                },
                {
                    "_id": "66c73909a575572fcb8d3cfb",
                    "name": "Di Niu",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-08-21T15:30:35.000Z",
            "title": "FRAP: Faithful and Realistic Text-to-Image Generation with Adaptive\n  Prompt Weighting",
            "summary": "Text-to-image (T2I) diffusion models have demonstrated impressive\ncapabilities in generating high-quality images given a text prompt. However,\nensuring the prompt-image alignment remains a considerable challenge, i.e.,\ngenerating images that faithfully align with the prompt's semantics. Recent\nworks attempt to improve the faithfulness by optimizing the latent code, which\npotentially could cause the latent code to go out-of-distribution and thus\nproduce unrealistic images. In this paper, we propose FRAP, a simple, yet\neffective approach based on adaptively adjusting the per-token prompt weights\nto improve prompt-image alignment and authenticity of the generated images. We\ndesign an online algorithm to adaptively update each token's weight\ncoefficient, which is achieved by minimizing a unified objective function that\nencourages object presence and the binding of object-modifier pairs. Through\nextensive evaluations, we show FRAP generates images with significantly higher\nprompt-image alignment to prompts from complex datasets, while having a lower\naverage latency compared to recent latent code optimization methods, e.g., 4\nseconds faster than D&B on the COCO-Subject dataset. Furthermore, through\nvisual comparisons and evaluation on the CLIP-IQA-Real metric, we show that\nFRAP not only improves prompt-image alignment but also generates more authentic\nimages with realistic appearances. We also explore combining FRAP with prompt\nrewriting LLM to recover their degraded prompt-image alignment, where we\nobserve improvements in both prompt-image alignment and image quality.",
            "upvotes": 1,
            "discussionId": "66c7390da575572fcb8d3e96"
        },
        "publishedAt": "2024-08-22T11:44:24.507Z",
        "title": "FRAP: Faithful and Realistic Text-to-Image Generation with Adaptive Prompt Weighting",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.11706.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/e9f5482cffdd1d5917523a496a3805f0.svg",
            "fullname": "Liyao Jiang",
            "name": "LiyaoJiang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2408.11817",
            "authors": [
                {
                    "_id": "66c6eb81e41514386f743ce6",
                    "name": "Jonathan Roberts",
                    "hidden": false
                },
                {
                    "_id": "66c6eb81e41514386f743ce7",
                    "name": "Kai Han",
                    "hidden": false
                },
                {
                    "_id": "66c6eb81e41514386f743ce8",
                    "name": "Samuel Albanie",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-08-21T17:59:32.000Z",
            "title": "GRAB: A Challenging GRaph Analysis Benchmark for Large Multimodal Models",
            "summary": "Large multimodal models (LMMs) have exhibited proficiencies across many\nvisual tasks. Although numerous well-known benchmarks exist to evaluate model\nperformance, they increasingly have insufficient headroom. As such, there is a\npressing need for a new generation of benchmarks challenging enough for the\nnext generation of LMMs. One area that LMMs show potential is graph analysis,\nspecifically, the tasks an analyst might typically perform when interpreting\nfigures such as estimating the mean, intercepts or correlations of functions\nand data series. In this work, we introduce GRAB, a graph analysis benchmark,\nfit for current and future frontier LMMs. Our benchmark is entirely synthetic,\nensuring high-quality, noise-free questions. GRAB is comprised of 2170\nquestions, covering four tasks and 23 graph properties. We evaluate 20 LMMs on\nGRAB, finding it to be a challenging benchmark, with the highest performing\nmodel attaining a score of just 21.7%. Finally, we conduct various ablations to\ninvestigate where the models succeed and struggle. We release GRAB to encourage\nprogress in this important, growing domain.",
            "upvotes": 1,
            "discussionId": "66c6eb82e41514386f743d25"
        },
        "publishedAt": "2024-08-22T11:27:55.782Z",
        "title": "GRAB: A Challenging GRaph Analysis Benchmark for Large Multimodal Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.11817.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1671535517748-632456d21ed511c0c5231afd.jpeg",
            "fullname": "Jonathan Roberts",
            "name": "jonathan-roberts1",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2408.11812",
            "authors": [
                {
                    "_id": "66c766d98ad5f9c46f8c9bdb",
                    "name": "Ria Doshi",
                    "hidden": false
                },
                {
                    "_id": "66c766d98ad5f9c46f8c9bdc",
                    "name": "Homer Walke",
                    "hidden": false
                },
                {
                    "_id": "66c766d98ad5f9c46f8c9bdd",
                    "name": "Oier Mees",
                    "hidden": false
                },
                {
                    "_id": "66c766d98ad5f9c46f8c9bde",
                    "name": "Sudeep Dasari",
                    "hidden": false
                },
                {
                    "_id": "66c766d98ad5f9c46f8c9bdf",
                    "name": "Sergey Levine",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-08-21T17:57:51.000Z",
            "title": "Scaling Cross-Embodied Learning: One Policy for Manipulation,\n  Navigation, Locomotion and Aviation",
            "summary": "Modern machine learning systems rely on large datasets to attain broad\ngeneralization, and this often poses a challenge in robot learning, where each\nrobotic platform and task might have only a small dataset. By training a single\npolicy across many different kinds of robots, a robot learning method can\nleverage much broader and more diverse datasets, which in turn can lead to\nbetter generalization and robustness. However, training a single policy on\nmulti-robot data is challenging because robots can have widely varying sensors,\nactuators, and control frequencies. We propose CrossFormer, a scalable and\nflexible transformer-based policy that can consume data from any embodiment. We\ntrain CrossFormer on the largest and most diverse dataset to date, 900K\ntrajectories across 20 different robot embodiments. We demonstrate that the\nsame network weights can control vastly different robots, including single and\ndual arm manipulation systems, wheeled robots, quadcopters, and quadrupeds.\nUnlike prior work, our model does not require manual alignment of the\nobservation or action spaces. Extensive experiments in the real world show that\nour method matches the performance of specialist policies tailored for each\nembodiment, while also significantly outperforming the prior state of the art\nin cross-embodiment learning.",
            "upvotes": 0,
            "discussionId": "66c766dd8ad5f9c46f8c9cfa"
        },
        "publishedAt": "2024-08-22T15:00:57.802Z",
        "title": "Scaling Cross-Embodied Learning: One Policy for Manipulation, Navigation, Locomotion and Aviation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.11812.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2408.08793",
            "authors": [
                {
                    "_id": "66c73f8e3fa97c9db6245365",
                    "name": "Simone Ricci",
                    "hidden": false
                },
                {
                    "_id": "66c73f8e3fa97c9db6245366",
                    "name": "Niccolò Biondi",
                    "hidden": false
                },
                {
                    "_id": "66c73f8e3fa97c9db6245367",
                    "name": "Federico Pernici",
                    "hidden": false
                },
                {
                    "_id": "66c73f8e3fa97c9db6245368",
                    "name": "Alberto Del Bimbo",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-08-16T15:05:28.000Z",
            "title": "Backward-Compatible Aligned Representations via an Orthogonal\n  Transformation Layer",
            "summary": "Visual retrieval systems face significant challenges when updating models\nwith improved representations due to misalignment between the old and new\nrepresentations. The costly and resource-intensive backfilling process involves\nrecalculating feature vectors for images in the gallery set whenever a new\nmodel is introduced. To address this, prior research has explored\nbackward-compatible training methods that enable direct comparisons between new\nand old representations without backfilling. Despite these advancements,\nachieving a balance between backward compatibility and the performance of\nindependently trained models remains an open problem. In this paper, we address\nit by expanding the representation space with additional dimensions and\nlearning an orthogonal transformation to achieve compatibility with old models\nand, at the same time, integrate new information. This transformation preserves\nthe original feature space's geometry, ensuring that our model aligns with\nprevious versions while also learning new data. Our Orthogonal Compatible\nAligned (OCA) approach eliminates the need for re-indexing during model updates\nand ensures that features can be compared directly across different model\nupdates without additional mapping functions. Experimental results on CIFAR-100\nand ImageNet-1k demonstrate that our method not only maintains compatibility\nwith previous models but also achieves state-of-the-art accuracy, outperforming\nseveral existing methods.",
            "upvotes": 0,
            "discussionId": "66c73f8e3fa97c9db6245384"
        },
        "publishedAt": "2024-08-22T12:09:57.508Z",
        "title": "Backward-Compatible Aligned Representations via an Orthogonal Transformation Layer",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.08793.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/fabc006d74d567110dfd0681b7ebf425.svg",
            "fullname": "Niccolò Biondi",
            "name": "NiccoBiondi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2408.11457",
            "authors": [
                {
                    "_id": "66c7004f60ef51a02f30164f",
                    "name": "Felermino D. M. Antonio Ali",
                    "hidden": false
                },
                {
                    "_id": "66c7004f60ef51a02f301650",
                    "name": "Henrique Lopes Cardoso",
                    "hidden": false
                },
                {
                    "_id": "66c7004f60ef51a02f301651",
                    "name": "Rui Sousa-Silva",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-08-21T09:23:20.000Z",
            "title": "Expanding FLORES+ Benchmark for more Low-Resource Settings:\n  Portuguese-Emakhuwa Machine Translation Evaluation",
            "summary": "As part of the Open Language Data Initiative shared tasks, we have expanded\nthe FLORES+ evaluation set to include Emakhuwa, a low-resource language widely\nspoken in Mozambique. We translated the dev and devtest sets from Portuguese\ninto Emakhuwa, and we detail the translation process and quality assurance\nmeasures used. Our methodology involved various quality checks, including\npost-editing and adequacy assessments. The resulting datasets consist of\nmultiple reference sentences for each source. We present baseline results from\ntraining a Neural Machine Translation system and fine-tuning existing\nmultilingual translation models. Our findings suggest that spelling\ninconsistencies remain a challenge in Emakhuwa. Additionally, the baseline\nmodels underperformed on this evaluation set, underscoring the necessity for\nfurther research to enhance machine translation quality for Emakhuwa. The data\nis publicly available at https://huggingface.co/datasets/LIACC/Emakhuwa-FLORES.",
            "upvotes": 0,
            "discussionId": "66c7005060ef51a02f30169f"
        },
        "publishedAt": "2024-08-22T07:39:37.760Z",
        "title": "Expanding FLORES+ Benchmark for more Low-Resource Settings: Portuguese-Emakhuwa Machine Translation Evaluation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.11457.png",
        "numComments": 0,
        "submittedBy": {
            "avatarUrl": "/avatars/1208629f14f010dbc2cd94f3c30f9baf.svg",
            "fullname": "JB D.",
            "name": "IAMJB",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    }
]