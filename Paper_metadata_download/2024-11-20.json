[
    {
        "paper": {
            "id": "2411.12372",
            "authors": [
                {
                    "_id": "673db8720916efb6cf79a413",
                    "user": {
                        "_id": "6329ee3dab49d487dd1439ec",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6329ee3dab49d487dd1439ec/vxGvdBK0XMZaCpc5dGOIa.jpeg",
                        "isPro": false,
                        "fullname": "Maurice Weber",
                        "user": "mauriceweber",
                        "type": "user"
                    },
                    "name": "Maurice Weber",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-20T13:20:10.244Z",
                    "hidden": false
                },
                {
                    "_id": "673db8720916efb6cf79a414",
                    "name": "Daniel Fu",
                    "hidden": false
                },
                {
                    "_id": "673db8720916efb6cf79a415",
                    "name": "Quentin Anthony",
                    "hidden": false
                },
                {
                    "_id": "673db8720916efb6cf79a416",
                    "name": "Yonatan Oren",
                    "hidden": false
                },
                {
                    "_id": "673db8720916efb6cf79a417",
                    "name": "Shane Adams",
                    "hidden": false
                },
                {
                    "_id": "673db8720916efb6cf79a418",
                    "name": "Anton Alexandrov",
                    "hidden": false
                },
                {
                    "_id": "673db8720916efb6cf79a419",
                    "user": {
                        "_id": "6512adc7d4556f9e6508e89f",
                        "avatarUrl": "/avatars/fbae9a9092d6c02ef49e60bc38c6640c.svg",
                        "isPro": false,
                        "fullname": "xiaozhong lyu",
                        "user": "xiaozhong-idata",
                        "type": "user"
                    },
                    "name": "Xiaozhong Lyu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-20T13:30:20.737Z",
                    "hidden": false
                },
                {
                    "_id": "673db8720916efb6cf79a41a",
                    "name": "Huu Nguyen",
                    "hidden": false
                },
                {
                    "_id": "673db8720916efb6cf79a41b",
                    "user": {
                        "_id": "61919e59d221f4281b3833d5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669164541734-61919e59d221f4281b3833d5.jpeg",
                        "isPro": false,
                        "fullname": "Xiaozhe Yao",
                        "user": "xzyao",
                        "type": "user"
                    },
                    "name": "Xiaozhe Yao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-20T13:28:26.931Z",
                    "hidden": false
                },
                {
                    "_id": "673db8720916efb6cf79a41c",
                    "user": {
                        "_id": "664d56756083f276c4197f46",
                        "avatarUrl": "/avatars/64032612a471a0d53f5d04d89617f393.svg",
                        "isPro": false,
                        "fullname": "Adams",
                        "user": "VirginiaAdams",
                        "type": "user"
                    },
                    "name": "Virginia Adams",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-20T13:28:43.998Z",
                    "hidden": false
                },
                {
                    "_id": "673db8720916efb6cf79a41d",
                    "user": {
                        "_id": "6395124b3d608ea4d9cd2339",
                        "avatarUrl": "/avatars/13f3dc014495dfdef2c460e9ce9c9869.svg",
                        "isPro": false,
                        "fullname": "Ben Athiwaratkun",
                        "user": "benathi",
                        "type": "user"
                    },
                    "name": "Ben Athiwaratkun",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-20T13:29:45.831Z",
                    "hidden": false
                },
                {
                    "_id": "673db8720916efb6cf79a41e",
                    "user": {
                        "_id": "648abf5f0c2a2d8592ca07e7",
                        "avatarUrl": "/avatars/333a96e0b2e9420b2abacf538c776c18.svg",
                        "isPro": false,
                        "fullname": "Rahul Chalamala",
                        "user": "rchalamala",
                        "type": "user"
                    },
                    "name": "Rahul Chalamala",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-20T13:30:03.317Z",
                    "hidden": false
                },
                {
                    "_id": "673db8720916efb6cf79a41f",
                    "user": {
                        "_id": "65caa69b8870f8c093a8abc1",
                        "avatarUrl": "/avatars/1b76dcc6a31b827a5b597024a0f29164.svg",
                        "isPro": false,
                        "fullname": "Kezhen Chen",
                        "user": "kezhentogether",
                        "type": "user"
                    },
                    "name": "Kezhen Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-20T13:30:10.106Z",
                    "hidden": false
                },
                {
                    "_id": "673db8720916efb6cf79a420",
                    "user": {
                        "_id": "607d59fb921db717010c7ccc",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1625736058289-607d59fb921db717010c7ccc.png",
                        "isPro": false,
                        "fullname": "Max Ryabinin",
                        "user": "mryab",
                        "type": "user"
                    },
                    "name": "Max Ryabinin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-20T13:27:41.096Z",
                    "hidden": false
                },
                {
                    "_id": "673db8720916efb6cf79a421",
                    "user": {
                        "_id": "64b8a6b5cf14c2fabe98159b",
                        "avatarUrl": "/avatars/dbc009451865435bf290791beadc4723.svg",
                        "isPro": false,
                        "fullname": "Tri Dao",
                        "user": "tridao",
                        "type": "user"
                    },
                    "name": "Tri Dao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-20T13:27:47.803Z",
                    "hidden": false
                },
                {
                    "_id": "673db8720916efb6cf79a422",
                    "user": {
                        "_id": "6409651b9e9f790c905b2335",
                        "avatarUrl": "/avatars/1fb8c80b60f21f65a0a027319101f236.svg",
                        "isPro": false,
                        "fullname": "Percy Liang",
                        "user": "percyliang",
                        "type": "user"
                    },
                    "name": "Percy Liang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-20T13:27:53.332Z",
                    "hidden": false
                },
                {
                    "_id": "673db8720916efb6cf79a423",
                    "name": "Christopher RÃ©",
                    "hidden": false
                },
                {
                    "_id": "673db8720916efb6cf79a424",
                    "user": {
                        "_id": "6227f764236b7b2eefa7f173",
                        "avatarUrl": "/avatars/0204e3da8e9ef12d88c98c6e955f8340.svg",
                        "isPro": false,
                        "fullname": "Irina Rish",
                        "user": "irinarish",
                        "type": "user"
                    },
                    "name": "Irina Rish",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-20T13:23:17.710Z",
                    "hidden": false
                },
                {
                    "_id": "673db8720916efb6cf79a425",
                    "name": "Ce Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-19T09:35:28.000Z",
            "title": "RedPajama: an Open Dataset for Training Large Language Models",
            "summary": "Large language models are increasingly becoming a cornerstone technology in\nartificial intelligence, the sciences, and society as a whole, yet the optimal\nstrategies for dataset composition and filtering remain largely elusive. Many\nof the top-performing models lack transparency in their dataset curation and\nmodel development processes, posing an obstacle to the development of fully\nopen language models. In this paper, we identify three core data-related\nchallenges that must be addressed to advance open-source language models. These\ninclude (1) transparency in model development, including the data curation\nprocess, (2) access to large quantities of high-quality data, and (3)\navailability of artifacts and metadata for dataset curation and analysis. To\naddress these challenges, we release RedPajama-V1, an open reproduction of the\nLLaMA training dataset. In addition, we release RedPajama-V2, a massive\nweb-only dataset consisting of raw, unfiltered text data together with quality\nsignals and metadata. Together, the RedPajama datasets comprise over 100\ntrillion tokens spanning multiple domains and with their quality signals\nfacilitate the filtering of data, aiming to inspire the development of numerous\nnew datasets. To date, these datasets have already been used in the training of\nstrong language models used in production, such as Snowflake Arctic,\nSalesforce's XGen and AI2's OLMo. To provide insight into the quality of\nRedPajama, we present a series of analyses and ablation studies with\ndecoder-only language models with up to 1.6B parameters. Our findings\ndemonstrate how quality signals for web data can be effectively leveraged to\ncurate high-quality subsets of the dataset, underscoring the potential of\nRedPajama to advance the development of transparent and high-performing\nlanguage models at scale.",
            "upvotes": 13,
            "discussionId": "673db8740916efb6cf79a47e"
        },
        "publishedAt": "2024-11-20T08:53:08.732Z",
        "title": "RedPajama: an Open Dataset for Training Large Language Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.12372.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1627505688463-60107b385ac3e86b3ea4fc34.jpeg",
            "fullname": "Daniel van Strien",
            "name": "davanstrien",
            "type": "user",
            "isPro": true,
            "isHf": true,
            "isMod": false,
            "followerCount": 405
        }
    },
    {
        "paper": {
            "id": "2411.11925",
            "authors": [
                {
                    "_id": "673d4d484bec167076968b8c",
                    "user": {
                        "_id": "6410998db27543634e3aacc3",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678809388600-noauth.png",
                        "isPro": false,
                        "fullname": "Zili Wang",
                        "user": "MarkWang",
                        "type": "user"
                    },
                    "name": "Zili Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-11-20T10:08:39.427Z",
                    "hidden": false
                },
                {
                    "_id": "673d4d484bec167076968b8d",
                    "name": "Robert Zhang",
                    "hidden": false
                },
                {
                    "_id": "673d4d484bec167076968b8e",
                    "name": "Kun Ding",
                    "hidden": false
                },
                {
                    "_id": "673d4d484bec167076968b8f",
                    "name": "Qi Yang",
                    "hidden": false
                },
                {
                    "_id": "673d4d484bec167076968b90",
                    "name": "Fei Li",
                    "hidden": false
                },
                {
                    "_id": "673d4d484bec167076968b91",
                    "name": "Shiming Xiang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-18T09:19:15.000Z",
            "title": "Continuous Speculative Decoding for Autoregressive Image Generation",
            "summary": "Continuous-valued Autoregressive (AR) image generation models have\ndemonstrated notable superiority over their discrete-token counterparts,\nshowcasing considerable reconstruction quality and higher generation fidelity.\nHowever, the computational demands of the autoregressive framework result in\nsignificant inference overhead. While speculative decoding has proven effective\nin accelerating Large Language Models (LLMs), their adaptation to\ncontinuous-valued visual autoregressive models remains unexplored. This work\ngeneralizes the speculative decoding algorithm from discrete tokens to\ncontinuous space. By analyzing the intrinsic properties of output distribution,\nwe establish a tailored acceptance criterion for the diffusion distributions\nprevalent in such models. To overcome the inconsistency that occurred in\nspeculative decoding output distributions, we introduce denoising trajectory\nalignment and token pre-filling methods. Additionally, we identify the\nhard-to-sample distribution in the rejection phase. To mitigate this issue, we\npropose a meticulous acceptance-rejection sampling method with a proper upper\nbound, thereby circumventing complex integration. Experimental results show\nthat our continuous speculative decoding achieves a remarkable 2.33times\nspeed-up on off-the-shelf models while maintaining the output distribution.\nCodes will be available at https://github.com/MarkXCloud/CSpD",
            "upvotes": 10,
            "discussionId": "673d4d4e4bec167076968cfc"
        },
        "publishedAt": "2024-11-20T01:19:04.868Z",
        "title": "Continuous Speculative Decoding for Autoregressive Image Generation",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6410998db27543634e3aacc3/IjWKC99EPlGeXqIz2ZojF.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.11925.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678809388600-noauth.png",
            "fullname": "Zili Wang",
            "name": "MarkWang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 3
        }
    },
    {
        "paper": {
            "id": "2411.12044",
            "authors": [
                {
                    "_id": "673d97e8f48649bd99d56715",
                    "user": {
                        "_id": "63923c2a5ece53ea0b364c8a",
                        "avatarUrl": "/avatars/d856933ecd8966ef72f1e67c6ec166b6.svg",
                        "isPro": false,
                        "fullname": "Mustafa Arda AydÄ±n",
                        "user": "aydnarda",
                        "type": "user"
                    },
                    "name": "M. Arda AydÄ±n",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-11-20T10:08:20.077Z",
                    "hidden": false
                },
                {
                    "_id": "673d97e8f48649bd99d56716",
                    "user": {
                        "_id": "65f411590718933804abbf56",
                        "avatarUrl": "/avatars/eb562172c0587c89288cfc5f32205a41.svg",
                        "isPro": false,
                        "fullname": "Efe Mert ÃÄ±rpar",
                        "user": "efemcirpar",
                        "type": "user"
                    },
                    "name": "Efe Mert ÃÄ±rpar",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-20T13:33:17.654Z",
                    "hidden": false
                },
                {
                    "_id": "673d97e8f48649bd99d56717",
                    "name": "Elvin Abdinli",
                    "hidden": false
                },
                {
                    "_id": "673d97e8f48649bd99d56718",
                    "user": {
                        "_id": "644b88b00fbe4830f192583f",
                        "avatarUrl": "/avatars/0e1cbab70d9b3ad4340985ee3087a8a7.svg",
                        "isPro": false,
                        "fullname": "Gozde Unal",
                        "user": "gozy",
                        "type": "user"
                    },
                    "name": "Gozde Unal",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-20T13:33:26.419Z",
                    "hidden": false
                },
                {
                    "_id": "673d97e8f48649bd99d56719",
                    "user": {
                        "_id": "6454104b9dcddd329ae73fc6",
                        "avatarUrl": "/avatars/996ee479de8efa7c1deafe8de634e94b.svg",
                        "isPro": false,
                        "fullname": "Yusuf H Sahin",
                        "user": "sahinyu",
                        "type": "user"
                    },
                    "name": "Yusuf H. Sahin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-20T13:33:31.767Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-18T20:31:38.000Z",
            "title": "ITACLIP: Boosting Training-Free Semantic Segmentation with Image, Text,\n  and Architectural Enhancements",
            "summary": "Recent advances in foundational Vision Language Models (VLMs) have reshaped\nthe evaluation paradigm in computer vision tasks. These foundational models,\nespecially CLIP, have accelerated research in open-vocabulary computer vision\ntasks, including Open-Vocabulary Semantic Segmentation (OVSS). Although the\ninitial results are promising, the dense prediction capabilities of VLMs still\nrequire further improvement. In this study, we enhance the semantic\nsegmentation performance of CLIP by introducing new modules and modifications:\n1) architectural changes in the last layer of ViT and the incorporation of\nattention maps from the middle layers with the last layer, 2) Image\nEngineering: applying data augmentations to enrich input image representations,\nand 3) using Large Language Models (LLMs) to generate definitions and synonyms\nfor each class name to leverage CLIP's open-vocabulary capabilities. Our\ntraining-free method, ITACLIP, outperforms current state-of-the-art approaches\non segmentation benchmarks such as COCO-Stuff, COCO-Object, Pascal Context, and\nPascal VOC. Our code is available at https://github.com/m-arda-aydn/ITACLIP.",
            "upvotes": 7,
            "discussionId": "673d97eaf48649bd99d567b0"
        },
        "publishedAt": "2024-11-20T11:03:39.352Z",
        "title": "ITACLIP: Boosting Training-Free Semantic Segmentation with Image, Text, and Architectural Enhancements",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.12044.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/d856933ecd8966ef72f1e67c6ec166b6.svg",
            "fullname": "Mustafa Arda AydÄ±n",
            "name": "aydnarda",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2411.12734",
            "authors": [
                {
                    "_id": "673d69231b0fe168ad4ba290",
                    "name": "Yunchao Yao",
                    "hidden": false
                },
                {
                    "_id": "673d69231b0fe168ad4ba291",
                    "user": {
                        "_id": "666ae4e53315aa1fa833bbcd",
                        "avatarUrl": "/avatars/a8c541c7408a4438fe4409b746e4f2d6.svg",
                        "isPro": false,
                        "fullname": "Uksang Yoo",
                        "user": "uyoo344",
                        "type": "user"
                    },
                    "name": "Uksang Yoo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-11-20T13:27:27.098Z",
                    "hidden": false
                },
                {
                    "_id": "673d69231b0fe168ad4ba292",
                    "user": {
                        "_id": "638779412ee2261893db9872",
                        "avatarUrl": "/avatars/98e1d86afbb8a105db946703fa44d430.svg",
                        "isPro": false,
                        "fullname": "Jean Oh",
                        "user": "jeanoh",
                        "type": "user"
                    },
                    "name": "Jean Oh",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-20T12:34:24.106Z",
                    "hidden": false
                },
                {
                    "_id": "673d69231b0fe168ad4ba293",
                    "user": {
                        "_id": "667207df662051878805403f",
                        "avatarUrl": "/avatars/53130f7d5eba540ef87085ebfeeb8d7a.svg",
                        "isPro": false,
                        "fullname": "Christopher Atkeson",
                        "user": "cgacgacga",
                        "type": "user"
                    },
                    "name": "Christopher G. Atkeson",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-20T12:34:33.830Z",
                    "hidden": false
                },
                {
                    "_id": "673d69231b0fe168ad4ba294",
                    "name": "Jeffrey Ichnowski",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-19T18:57:41.000Z",
            "title": "Soft Robotic Dynamic In-Hand Pen Spinning",
            "summary": "Dynamic in-hand manipulation remains a challenging task for soft robotic\nsystems that have demonstrated advantages in safe compliant interactions but\nstruggle with high-speed dynamic tasks. In this work, we present SWIFT, a\nsystem for learning dynamic tasks using a soft and compliant robotic hand.\nUnlike previous works that rely on simulation, quasi-static actions and precise\nobject models, the proposed system learns to spin a pen through trial-and-error\nusing only real-world data without requiring explicit prior knowledge of the\npen's physical attributes. With self-labeled trials sampled from the real\nworld, the system discovers the set of pen grasping and spinning primitive\nparameters that enables a soft hand to spin a pen robustly and reliably. After\n130 sampled actions per object, SWIFT achieves 100% success rate across three\npens with different weights and weight distributions, demonstrating the\nsystem's generalizability and robustness to changes in object properties. The\nresults highlight the potential for soft robotic end-effectors to perform\ndynamic tasks including rapid in-hand manipulation. We also demonstrate that\nSWIFT generalizes to spinning items with different shapes and weights such as a\nbrush and a screwdriver which we spin with 10/10 and 5/10 success rates\nrespectively. Videos, data, and code are available at\nhttps://soft-spin.github.io.",
            "upvotes": 7,
            "discussionId": "673d69251b0fe168ad4ba370"
        },
        "publishedAt": "2024-11-20T03:14:30.556Z",
        "title": "Soft Robotic Dynamic In-Hand Pen Spinning",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.12734.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 5181
        }
    },
    {
        "paper": {
            "id": "2411.10161",
            "authors": [
                {
                    "_id": "673d449ca2f8aff91ef2615a",
                    "user": {
                        "_id": "63e78d4edb40d9e67fedad16",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e78d4edb40d9e67fedad16/7QjxNI7p7iZD_6s0TZvTD.jpeg",
                        "isPro": false,
                        "fullname": "Zevin",
                        "user": "Zevin2023",
                        "type": "user"
                    },
                    "name": "Zewen Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-11-20T10:08:51.252Z",
                    "hidden": false
                },
                {
                    "_id": "673d449ca2f8aff91ef2615b",
                    "name": "Juan Wang",
                    "hidden": false
                },
                {
                    "_id": "673d449ca2f8aff91ef2615c",
                    "name": "Wen Wang",
                    "hidden": false
                },
                {
                    "_id": "673d449ca2f8aff91ef2615d",
                    "user": {
                        "_id": "661d280abca1a6038b2bfe5e",
                        "avatarUrl": "/avatars/62b4818824c6c0ef9afad0d727618681.svg",
                        "isPro": false,
                        "fullname": "Sunhan Xu",
                        "user": "Shuanghan",
                        "type": "user"
                    },
                    "name": "Sunhan Xu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-20T13:42:43.952Z",
                    "hidden": false
                },
                {
                    "_id": "673d449ca2f8aff91ef2615e",
                    "name": "Hang Xiong",
                    "hidden": false
                },
                {
                    "_id": "673d449ca2f8aff91ef2615f",
                    "user": {
                        "_id": "66a0bc420b187dd45f7bf08f",
                        "avatarUrl": "/avatars/31bc135113862d932958607fdccd9c4b.svg",
                        "isPro": false,
                        "fullname": "Yun Zeng",
                        "user": "Yun0418",
                        "type": "user"
                    },
                    "name": "Yun Zeng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-20T13:54:47.572Z",
                    "hidden": false
                },
                {
                    "_id": "673d449ca2f8aff91ef26160",
                    "name": "Jian Guo",
                    "hidden": false
                },
                {
                    "_id": "673d449ca2f8aff91ef26161",
                    "user": {
                        "_id": "6459f8cac70a926dc46b10d1",
                        "avatarUrl": "/avatars/d68619fb59edd7a93a9709695b12c94d.svg",
                        "isPro": false,
                        "fullname": "wangshuxun",
                        "user": "wangshuxun",
                        "type": "user"
                    },
                    "name": "Shuxun Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-20T13:55:23.017Z",
                    "hidden": false
                },
                {
                    "_id": "673d449ca2f8aff91ef26162",
                    "name": "Chunfeng Yuan",
                    "hidden": false
                },
                {
                    "_id": "673d449ca2f8aff91ef26163",
                    "name": "Bing Li",
                    "hidden": false
                },
                {
                    "_id": "673d449ca2f8aff91ef26164",
                    "user": {
                        "_id": "6641c7c05aa8e7031cc6afc0",
                        "avatarUrl": "/avatars/dcf2b101a7afab828bc308f922648e3f.svg",
                        "isPro": false,
                        "fullname": "Weiming Hu",
                        "user": "huweim",
                        "type": "user"
                    },
                    "name": "Weiming Hu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-20T13:56:57.438Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-15T13:07:22.000Z",
            "title": "SEAGULL: No-reference Image Quality Assessment for Regions of Interest\n  via Vision-Language Instruction Tuning",
            "summary": "Existing Image Quality Assessment (IQA) methods achieve remarkable success in\nanalyzing quality for overall image, but few works explore quality analysis for\nRegions of Interest (ROIs). The quality analysis of ROIs can provide\nfine-grained guidance for image quality improvement and is crucial for\nscenarios focusing on region-level quality. This paper proposes a novel\nnetwork, SEAGULL, which can SEe and Assess ROIs quality with GUidance from a\nLarge vision-Language model. SEAGULL incorporates a vision-language model\n(VLM), masks generated by Segment Anything Model (SAM) to specify ROIs, and a\nmeticulously designed Mask-based Feature Extractor (MFE) to extract global and\nlocal tokens for specified ROIs, enabling accurate fine-grained IQA for ROIs.\nMoreover, this paper constructs two ROI-based IQA datasets, SEAGULL-100w and\nSEAGULL-3k, for training and evaluating ROI-based IQA. SEAGULL-100w comprises\nabout 100w synthetic distortion images with 33 million ROIs for pre-training to\nimprove the model's ability of regional quality perception, and SEAGULL-3k\ncontains about 3k authentic distortion ROIs to enhance the model's ability to\nperceive real world distortions. After pre-training on SEAGULL-100w and\nfine-tuning on SEAGULL-3k, SEAGULL shows remarkable performance on fine-grained\nROI quality assessment. Code and datasets are publicly available at the\nhttps://github.com/chencn2020/Seagull.",
            "upvotes": 4,
            "discussionId": "673d449fa2f8aff91ef26300"
        },
        "publishedAt": "2024-11-20T11:04:09.860Z",
        "title": "SEAGULL: No-reference Image Quality Assessment for Regions of Interest via Vision-Language Instruction Tuning",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.10161.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e78d4edb40d9e67fedad16/7QjxNI7p7iZD_6s0TZvTD.jpeg",
            "fullname": "Zevin",
            "name": "Zevin2023",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2411.12275",
            "authors": [
                {
                    "_id": "673d461ca2f8aff91ef34192",
                    "user": {
                        "_id": "66c6a7667d41b231148b3135",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c6a7667d41b231148b3135/k0-hHlIdN6pLBUP35eQoC.jpeg",
                        "isPro": false,
                        "fullname": "Huzaifa Sidhpurwala",
                        "user": "huzaifas-sidhpurwala",
                        "type": "user"
                    },
                    "name": "Huzaifa Sidhpurwala",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-11-20T10:08:42.328Z",
                    "hidden": false
                },
                {
                    "_id": "673d461ca2f8aff91ef34193",
                    "name": "Garth Mollett",
                    "hidden": false
                },
                {
                    "_id": "673d461ca2f8aff91ef34194",
                    "user": {
                        "_id": "673df7c463db0f5cbecfe5a7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/CyGJ5bhIqqSsVp42wz6sZ.png",
                        "isPro": false,
                        "fullname": "Emily Fox",
                        "user": "TheMoxieFox",
                        "type": "user"
                    },
                    "name": "Emily Fox",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-11-20T15:31:19.525Z",
                    "hidden": false
                },
                {
                    "_id": "673d461ca2f8aff91ef34195",
                    "user": {
                        "_id": "664f940834ec32b5a0b047e4",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/35RN8uUX90GntnpLXvDFg.jpeg",
                        "isPro": false,
                        "fullname": "Mark Bestavros",
                        "user": "mbestavr-rh",
                        "type": "user"
                    },
                    "name": "Mark Bestavros",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-20T13:35:44.843Z",
                    "hidden": false
                },
                {
                    "_id": "673d461ca2f8aff91ef34196",
                    "user": {
                        "_id": "64b8311fa62c52b252b0e704",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b8311fa62c52b252b0e704/EkHr_iUwZw3GFqo4p1uwj.jpeg",
                        "isPro": false,
                        "fullname": "Huamin Chen",
                        "user": "HuaminChen",
                        "type": "user"
                    },
                    "name": "Huamin Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-20T13:35:38.578Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-19T06:55:57.000Z",
            "title": "Building Trust: Foundations of Security, Safety and Transparency in AI",
            "summary": "This paper explores the rapidly evolving ecosystem of publicly available AI\nmodels, and their potential implications on the security and safety landscape.\nAs AI models become increasingly prevalent, understanding their potential risks\nand vulnerabilities is crucial. We review the current security and safety\nscenarios while highlighting challenges such as tracking issues, remediation,\nand the apparent absence of AI model lifecycle and ownership processes.\nComprehensive strategies to enhance security and safety for both model\ndevelopers and end-users are proposed. This paper aims to provide some of the\nfoundational pieces for more standardized security, safety, and transparency in\nthe development and operation of AI models and the larger open ecosystems and\ncommunities forming around them.",
            "upvotes": 4,
            "discussionId": "673d461ca2f8aff91ef341db"
        },
        "publishedAt": "2024-11-20T08:49:12.165Z",
        "title": "Building Trust: Foundations of Security, Safety and Transparency in AI",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.12275.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c6a7667d41b231148b3135/k0-hHlIdN6pLBUP35eQoC.jpeg",
            "fullname": "Huzaifa Sidhpurwala",
            "name": "huzaifas-sidhpurwala",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        }
    },
    {
        "paper": {
            "id": "2411.10818",
            "authors": [
                {
                    "_id": "673df3308f9f9408fe5b8d41",
                    "user": {
                        "_id": "638c81fa61eb51017518fa31",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/f0eCrzBrxz7Y9n25WkZ2v.png",
                        "isPro": false,
                        "fullname": "Hmrishav Bandyopadhyay",
                        "user": "Hmrishav",
                        "type": "user"
                    },
                    "name": "Hmrishav Bandyopadhyay",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-11-20T15:31:23.868Z",
                    "hidden": false
                },
                {
                    "_id": "673df3308f9f9408fe5b8d42",
                    "name": "Yi-Zhe Song",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-16T14:53:03.000Z",
            "title": "FlipSketch: Flipping Static Drawings to Text-Guided Sketch Animations",
            "summary": "Sketch animations offer a powerful medium for visual storytelling, from\nsimple flip-book doodles to professional studio productions. While traditional\nanimation requires teams of skilled artists to draw key frames and in-between\nframes, existing automation attempts still demand significant artistic effort\nthrough precise motion paths or keyframe specification. We present FlipSketch,\na system that brings back the magic of flip-book animation -- just draw your\nidea and describe how you want it to move! Our approach harnesses motion priors\nfrom text-to-video diffusion models, adapting them to generate sketch\nanimations through three key innovations: (i) fine-tuning for sketch-style\nframe generation, (ii) a reference frame mechanism that preserves visual\nintegrity of input sketch through noise refinement, and (iii) a dual-attention\ncomposition that enables fluid motion without losing visual consistency. Unlike\nconstrained vector animations, our raster frames support dynamic sketch\ntransformations, capturing the expressive freedom of traditional animation. The\nresult is an intuitive system that makes sketch animation as simple as doodling\nand describing, while maintaining the artistic essence of hand-drawn animation.",
            "upvotes": 3,
            "discussionId": "673df3348f9f9408fe5b8e0e"
        },
        "publishedAt": "2024-11-20T14:25:43.122Z",
        "title": "FlipSketch: Flipping Static Drawings to Text-Guided Sketch Animations",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/638c81fa61eb51017518fa31/6woQz2d3WZNbfHyPd5RdP.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.10818.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/f0eCrzBrxz7Y9n25WkZ2v.png",
            "fullname": "Hmrishav Bandyopadhyay",
            "name": "Hmrishav",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 2
        }
    },
    {
        "paper": {
            "id": "2411.12240",
            "authors": [
                {
                    "_id": "673d599340b6ff437a9ae9d8",
                    "user": {
                        "_id": "65d3518c40e07f7dbf7ca772",
                        "avatarUrl": "/avatars/018e780ebeacc8440487d5f550574906.svg",
                        "isPro": false,
                        "fullname": "SAGAR TAMANG",
                        "user": "tamang0000",
                        "type": "user"
                    },
                    "name": "S. Tamang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-11-20T10:08:33.948Z",
                    "hidden": false
                },
                {
                    "_id": "673d599340b6ff437a9ae9d9",
                    "name": "D. J. Bora",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-19T05:37:17.000Z",
            "title": "Evaluating Tokenizer Performance of Large Language Models Across\n  Official Indian Languages",
            "summary": "Large Language Models (LLMs) based on transformer architectures have\nrevolutionized a variety of domains, with tokenization playing a pivotal role\nin their pre-processing and fine-tuning stages. In multilingual models,\nparticularly those tailored for Indic languages, effective tokenization is\ncrucial for optimizing performance. This paper presents a comprehensive\nevaluation of tokenizers used by 12 LLMs across all 22 official languages of\nIndia, with a focus on comparing the efficiency of their tokenization\nprocesses. We employed the Normalized Sequence Length (NSL) as a key metric in\nour analysis. Our findings reveal that the SUTRA tokenizer outperforms all\nother models, including several Indic-specific models, excelling in 14\nlanguages. Notable insights include the SUTRA tokenizer's superior handling of\nIndic languages, GPT-4o's advancement over its predecessor GPT-4 in processing\nIndian languages, and the limited performance of Project Indus in certain\nlanguages. This study underscores the critical importance of developing\ntargeted tokenization strategies for multilingual and Indic-centric models,\nlaying the groundwork for future improvements in tokenizer design to enhance\nlinguistic coverage and model efficiency.",
            "upvotes": 2,
            "discussionId": "673d599440b6ff437a9aea33"
        },
        "publishedAt": "2024-11-20T02:11:54.071Z",
        "title": "Evaluating Tokenizer Performance of Large Language Models Across Official Indian Languages",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.12240.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/018e780ebeacc8440487d5f550574906.svg",
            "fullname": "SAGAR TAMANG",
            "name": "tamang0000",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    }
]