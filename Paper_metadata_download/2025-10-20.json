[
  {
    "paper": {
      "id": "2510.15870",
      "authors": [
        {
          "_id": "68f592478589920bf4d32084",
          "name": "Hanrong Ye",
          "hidden": false
        },
        {
          "_id": "68f592478589920bf4d32085",
          "name": "Chao-Han Huck Yang",
          "hidden": false
        },
        {
          "_id": "68f592478589920bf4d32086",
          "name": "Arushi Goel",
          "hidden": false
        },
        {
          "_id": "68f592478589920bf4d32087",
          "name": "Wei Huang",
          "hidden": false
        },
        {
          "_id": "68f592478589920bf4d32088",
          "name": "Ligeng Zhu",
          "hidden": false
        },
        {
          "_id": "68f592478589920bf4d32089",
          "name": "Yuanhang Su",
          "hidden": false
        },
        {
          "_id": "68f592478589920bf4d3208a",
          "name": "Sean Lin",
          "hidden": false
        },
        {
          "_id": "68f592478589920bf4d3208b",
          "name": "An-Chieh Cheng",
          "hidden": false
        },
        {
          "_id": "68f592478589920bf4d3208c",
          "name": "Zhen Wan",
          "hidden": false
        },
        {
          "_id": "68f592478589920bf4d3208d",
          "name": "Jinchuan Tian",
          "hidden": false
        },
        {
          "_id": "68f592478589920bf4d3208e",
          "name": "Yuming Lou",
          "hidden": false
        },
        {
          "_id": "68f592478589920bf4d3208f",
          "name": "Dong Yang",
          "hidden": false
        },
        {
          "_id": "68f592478589920bf4d32090",
          "name": "Zhijian Liu",
          "hidden": false
        },
        {
          "_id": "68f592478589920bf4d32091",
          "name": "Yukang Chen",
          "hidden": false
        },
        {
          "_id": "68f592478589920bf4d32092",
          "name": "Ambrish Dantrey",
          "hidden": false
        },
        {
          "_id": "68f592478589920bf4d32093",
          "name": "Ehsan Jahangiri",
          "hidden": false
        },
        {
          "_id": "68f592478589920bf4d32094",
          "name": "Sreyan Ghosh",
          "hidden": false
        },
        {
          "_id": "68f592478589920bf4d32095",
          "name": "Daguang Xu",
          "hidden": false
        },
        {
          "_id": "68f592478589920bf4d32096",
          "name": "Ehsan Hosseini-Asl",
          "hidden": false
        },
        {
          "_id": "68f592478589920bf4d32097",
          "name": "Danial Mohseni Taheri",
          "hidden": false
        },
        {
          "_id": "68f592478589920bf4d32098",
          "name": "Vidya Murali",
          "hidden": false
        },
        {
          "_id": "68f592478589920bf4d32099",
          "name": "Sifei Liu",
          "hidden": false
        },
        {
          "_id": "68f592478589920bf4d3209a",
          "name": "Jason Lu",
          "hidden": false
        },
        {
          "_id": "68f592478589920bf4d3209b",
          "name": "Oluwatobi Olabiyi",
          "hidden": false
        },
        {
          "_id": "68f592478589920bf4d3209c",
          "name": "Frank Wang",
          "hidden": false
        },
        {
          "_id": "68f592478589920bf4d3209d",
          "name": "Rafael Valle",
          "hidden": false
        },
        {
          "_id": "68f592478589920bf4d3209e",
          "name": "Bryan Catanzaro",
          "hidden": false
        },
        {
          "_id": "68f592478589920bf4d3209f",
          "name": "Andrew Tao",
          "hidden": false
        },
        {
          "_id": "68f592478589920bf4d320a0",
          "name": "Song Han",
          "hidden": false
        },
        {
          "_id": "68f592478589920bf4d320a1",
          "name": "Jan Kautz",
          "hidden": false
        },
        {
          "_id": "68f592478589920bf4d320a2",
          "name": "Hongxu Yin",
          "hidden": false
        },
        {
          "_id": "68f592478589920bf4d320a3",
          "name": "Pavlo Molchanov",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-17T17:59:59.000Z",
      "submittedOnDailyAt": "2025-10-20T00:07:23.233Z",
      "title": "OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding\n  LLM",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Advancing machine intelligence requires developing the ability to perceive\nacross multiple modalities, much as humans sense the world. We introduce\nOmniVinci, an initiative to build a strong, open-source, omni-modal LLM. We\ncarefully study the design choices across model architecture and data curation.\nFor model architecture, we present three key innovations: (i) OmniAlignNet for\nstrengthening alignment between vision and audio embeddings in a shared\nomni-modal latent space; (ii) Temporal Embedding Grouping for capturing\nrelative temporal alignment between vision and audio signals; and (iii)\nConstrained Rotary Time Embedding for encoding absolute temporal information in\nomni-modal embeddings. We introduce a curation and synthesis pipeline that\ngenerates 24M single-modal and omni-modal conversations. We find that\nmodalities reinforce one another in both perception and reasoning. Our model,\nOmniVinci, outperforms Qwen2.5-Omni with +19.05 on DailyOmni (cross-modal\nunderstanding), +1.7 on MMAR (audio), and +3.9 on Video-MME (vision), while\nusing just 0.2T training tokens - a 6 times reduction compared to\nQwen2.5-Omni's 1.2T. We finally demonstrate omni-modal advantages in downstream\napplications spanning robotics, medical AI, and smart factory.",
      "upvotes": 32,
      "discussionId": "68f592478589920bf4d320a4",
      "projectPage": "https://nvlabs.github.io/OmniVinci/",
      "githubRepo": "https://github.com/NVlabs/OmniVinci",
      "ai_summary": "OmniVinci, an open-source omni-modal LLM, enhances cross-modal understanding and performance across audio, vision, and robotics applications with innovative architecture and efficient data curation.",
      "ai_keywords": [
        "OmniAlignNet",
        "Temporal Embedding Grouping",
        "Constrained Rotary Time Embedding",
        "omni-modal latent space",
        "DailyOmni",
        "MMAR",
        "Video-MME",
        "omni-modal conversations",
        "omni-modal advantages"
      ],
      "githubStars": 17,
      "organization": {
        "_id": "60262b67268c201cdc8b7d43",
        "name": "nvidia",
        "fullname": "NVIDIA",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
      }
    },
    "publishedAt": "2025-10-17T13:59:59.000Z",
    "title": "OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding\n  LLM",
    "summary": "Advancing machine intelligence requires developing the ability to perceive\nacross multiple modalities, much as humans sense the world. We introduce\nOmniVinci, an initiative to build a strong, open-source, omni-modal LLM. We\ncarefully study the design choices across model architecture and data curation.\nFor model architecture, we present three key innovations: (i) OmniAlignNet for\nstrengthening alignment between vision and audio embeddings in a shared\nomni-modal latent space; (ii) Temporal Embedding Grouping for capturing\nrelative temporal alignment between vision and audio signals; and (iii)\nConstrained Rotary Time Embedding for encoding absolute temporal information in\nomni-modal embeddings. We introduce a curation and synthesis pipeline that\ngenerates 24M single-modal and omni-modal conversations. We find that\nmodalities reinforce one another in both perception and reasoning. Our model,\nOmniVinci, outperforms Qwen2.5-Omni with +19.05 on DailyOmni (cross-modal\nunderstanding), +1.7 on MMAR (audio), and +3.9 on Video-MME (vision), while\nusing just 0.2T training tokens - a 6 times reduction compared to\nQwen2.5-Omni's 1.2T. We finally demonstrate omni-modal advantages in downstream\napplications spanning robotics, medical AI, and smart factory.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.15870.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 131
    },
    "organization": {
      "_id": "60262b67268c201cdc8b7d43",
      "name": "nvidia",
      "fullname": "NVIDIA",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.15019",
      "authors": [
        {
          "_id": "68f5c6ae8589920bf4d321bc",
          "name": "Junliang Ye",
          "hidden": false
        },
        {
          "_id": "68f5c6ae8589920bf4d321bd",
          "name": "Shenghao Xie",
          "hidden": false
        },
        {
          "_id": "68f5c6ae8589920bf4d321be",
          "name": "Ruowen Zhao",
          "hidden": false
        },
        {
          "_id": "68f5c6ae8589920bf4d321bf",
          "name": "Zhengyi Wang",
          "hidden": false
        },
        {
          "_id": "68f5c6ae8589920bf4d321c0",
          "name": "Hongyu Yan",
          "hidden": false
        },
        {
          "_id": "68f5c6ae8589920bf4d321c1",
          "name": "Wenqiang Zu",
          "hidden": false
        },
        {
          "_id": "68f5c6ae8589920bf4d321c2",
          "name": "Lei Ma",
          "hidden": false
        },
        {
          "_id": "68f5c6ae8589920bf4d321c3",
          "name": "Jun Zhu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/65a420cd90e65dc39a6abe9e/ldmAjBXk17bLdgoKhA9W9.mp4"
      ],
      "publishedAt": "2025-10-16T17:51:50.000Z",
      "submittedOnDailyAt": "2025-10-20T04:11:33.811Z",
      "title": "NANO3D: A Training-Free Approach for Efficient 3D Editing Without Masks",
      "submittedOnDailyBy": {
        "_id": "65a420cd90e65dc39a6abe9e",
        "avatarUrl": "/avatars/81ac5b749043e899f5017782409f9e28.svg",
        "isPro": false,
        "fullname": "yejunliang",
        "user": "yejunliang23",
        "type": "user"
      },
      "summary": "3D object editing is essential for interactive content creation in gaming,\nanimation, and robotics, yet current approaches remain inefficient,\ninconsistent, and often fail to preserve unedited regions. Most methods rely on\nediting multi-view renderings followed by reconstruction, which introduces\nartifacts and limits practicality. To address these challenges, we propose\nNano3D, a training-free framework for precise and coherent 3D object editing\nwithout masks. Nano3D integrates FlowEdit into TRELLIS to perform localized\nedits guided by front-view renderings, and further introduces region-aware\nmerging strategies, Voxel/Slat-Merge, which adaptively preserve structural\nfidelity by ensuring consistency between edited and unedited areas. Experiments\ndemonstrate that Nano3D achieves superior 3D consistency and visual quality\ncompared with existing methods. Based on this framework, we construct the first\nlarge-scale 3D editing datasets Nano3D-Edit-100k, which contains over 100,000\nhigh-quality 3D editing pairs. This work addresses long-standing challenges in\nboth algorithm design and data availability, significantly improving the\ngenerality and reliability of 3D editing, and laying the groundwork for the\ndevelopment of feed-forward 3D editing models. Project\nPage:https://jamesyjl.github.io/Nano3D",
      "upvotes": 26,
      "discussionId": "68f5c6ae8589920bf4d321c4",
      "projectPage": "https://jamesyjl.github.io/Nano3D/",
      "githubRepo": "https://github.com/JAMESYJL/Nano3D/tree/main",
      "ai_summary": "Nano3D is a training-free framework that integrates FlowEdit and TRELLIS for precise 3D object editing, using front-view renderings and region-aware merging strategies to maintain structural fidelity and visual quality.",
      "ai_keywords": [
        "FlowEdit",
        "TRELLIS",
        "front-view renderings",
        "region-aware merging",
        "Voxel/Slat-Merge",
        "3D consistency",
        "3D editing datasets",
        "feed-forward 3D editing models"
      ],
      "githubStars": 10
    },
    "publishedAt": "2025-10-16T13:51:50.000Z",
    "title": "NANO3D: A Training-Free Approach for Efficient 3D Editing Without Masks",
    "summary": "3D object editing is essential for interactive content creation in gaming,\nanimation, and robotics, yet current approaches remain inefficient,\ninconsistent, and often fail to preserve unedited regions. Most methods rely on\nediting multi-view renderings followed by reconstruction, which introduces\nartifacts and limits practicality. To address these challenges, we propose\nNano3D, a training-free framework for precise and coherent 3D object editing\nwithout masks. Nano3D integrates FlowEdit into TRELLIS to perform localized\nedits guided by front-view renderings, and further introduces region-aware\nmerging strategies, Voxel/Slat-Merge, which adaptively preserve structural\nfidelity by ensuring consistency between edited and unedited areas. Experiments\ndemonstrate that Nano3D achieves superior 3D consistency and visual quality\ncompared with existing methods. Based on this framework, we construct the first\nlarge-scale 3D editing datasets Nano3D-Edit-100k, which contains over 100,000\nhigh-quality 3D editing pairs. This work addresses long-standing challenges in\nboth algorithm design and data availability, significantly improving the\ngenerality and reliability of 3D editing, and laying the groundwork for the\ndevelopment of feed-forward 3D editing models. Project\nPage:https://jamesyjl.github.io/Nano3D",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/65a420cd90e65dc39a6abe9e/ldmAjBXk17bLdgoKhA9W9.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.15019.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65a420cd90e65dc39a6abe9e",
      "avatarUrl": "/avatars/81ac5b749043e899f5017782409f9e28.svg",
      "fullname": "yejunliang",
      "name": "yejunliang23",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.15869",
      "authors": [
        {
          "_id": "68f593468589920bf4d320d3",
          "name": "Jie-Ying Lee",
          "hidden": false
        },
        {
          "_id": "68f593468589920bf4d320d4",
          "name": "Yi-Ruei Liu",
          "hidden": false
        },
        {
          "_id": "68f593468589920bf4d320d5",
          "name": "Shr-Ruei Tsai",
          "hidden": false
        },
        {
          "_id": "68f593468589920bf4d320d6",
          "name": "Wei-Cheng Chang",
          "hidden": false
        },
        {
          "_id": "68f593468589920bf4d320d7",
          "name": "Chung-Ho Wu",
          "hidden": false
        },
        {
          "_id": "68f593468589920bf4d320d8",
          "name": "Jiewen Chan",
          "hidden": false
        },
        {
          "_id": "68f593468589920bf4d320d9",
          "name": "Zhenjun Zhao",
          "hidden": false
        },
        {
          "_id": "68f593468589920bf4d320da",
          "name": "Chieh Hubert Lin",
          "hidden": false
        },
        {
          "_id": "68f593468589920bf4d320db",
          "name": "Yu-Lun Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-17T17:59:51.000Z",
      "submittedOnDailyAt": "2025-10-20T00:11:33.356Z",
      "title": "Skyfall-GS: Synthesizing Immersive 3D Urban Scenes from Satellite\n  Imagery",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Synthesizing large-scale, explorable, and geometrically accurate 3D urban\nscenes is a challenging yet valuable task in providing immersive and embodied\napplications. The challenges lie in the lack of large-scale and high-quality\nreal-world 3D scans for training generalizable generative models. In this\npaper, we take an alternative route to create large-scale 3D scenes by\nsynergizing the readily available satellite imagery that supplies realistic\ncoarse geometry and the open-domain diffusion model for creating high-quality\nclose-up appearances. We propose Skyfall-GS, the first city-block\nscale 3D scene creation framework without costly 3D annotations, also featuring\nreal-time, immersive 3D exploration. We tailor a curriculum-driven iterative\nrefinement strategy to progressively enhance geometric completeness and\nphotorealistic textures. Extensive experiments demonstrate that Skyfall-GS\nprovides improved cross-view consistent geometry and more realistic textures\ncompared to state-of-the-art approaches. Project page:\nhttps://skyfall-gs.jayinnn.dev/",
      "upvotes": 23,
      "discussionId": "68f593468589920bf4d320dc",
      "projectPage": "https://skyfall-gs.jayinnn.dev/",
      "githubRepo": "https://github.com/jayin92/skyfall-gs",
      "ai_summary": "Skyfall-GS creates large-scale, high-quality 3D urban scenes using satellite imagery and diffusion models, offering real-time exploration and improved geometry and texture consistency.",
      "ai_keywords": [
        "diffusion model",
        "Skyfall-GS",
        "curriculum-driven iterative refinement",
        "geometric completeness",
        "photorealistic textures",
        "cross-view consistent geometry"
      ],
      "githubStars": 11
    },
    "publishedAt": "2025-10-17T13:59:51.000Z",
    "title": "Skyfall-GS: Synthesizing Immersive 3D Urban Scenes from Satellite\n  Imagery",
    "summary": "Synthesizing large-scale, explorable, and geometrically accurate 3D urban\nscenes is a challenging yet valuable task in providing immersive and embodied\napplications. The challenges lie in the lack of large-scale and high-quality\nreal-world 3D scans for training generalizable generative models. In this\npaper, we take an alternative route to create large-scale 3D scenes by\nsynergizing the readily available satellite imagery that supplies realistic\ncoarse geometry and the open-domain diffusion model for creating high-quality\nclose-up appearances. We propose Skyfall-GS, the first city-block\nscale 3D scene creation framework without costly 3D annotations, also featuring\nreal-time, immersive 3D exploration. We tailor a curriculum-driven iterative\nrefinement strategy to progressively enhance geometric completeness and\nphotorealistic textures. Extensive experiments demonstrate that Skyfall-GS\nprovides improved cross-view consistent geometry and more realistic textures\ncompared to state-of-the-art approaches. Project page:\nhttps://skyfall-gs.jayinnn.dev/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.15869.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 131
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.15742",
      "authors": [
        {
          "_id": "68f5a20f8589920bf4d3212e",
          "name": "Qingyan Bai",
          "hidden": false
        },
        {
          "_id": "68f5a20f8589920bf4d3212f",
          "name": "Qiuyu Wang",
          "hidden": false
        },
        {
          "_id": "68f5a20f8589920bf4d32130",
          "name": "Hao Ouyang",
          "hidden": false
        },
        {
          "_id": "68f5a20f8589920bf4d32131",
          "name": "Yue Yu",
          "hidden": false
        },
        {
          "_id": "68f5a20f8589920bf4d32132",
          "name": "Hanlin Wang",
          "hidden": false
        },
        {
          "_id": "68f5a20f8589920bf4d32133",
          "name": "Wen Wang",
          "hidden": false
        },
        {
          "_id": "68f5a20f8589920bf4d32134",
          "name": "Ka Leong Cheng",
          "hidden": false
        },
        {
          "_id": "68f5a20f8589920bf4d32135",
          "name": "Shuailei Ma",
          "hidden": false
        },
        {
          "_id": "68f5a20f8589920bf4d32136",
          "name": "Yanhong Zeng",
          "hidden": false
        },
        {
          "_id": "68f5a20f8589920bf4d32137",
          "name": "Zichen Liu",
          "hidden": false
        },
        {
          "_id": "68f5a20f8589920bf4d32138",
          "name": "Yinghao Xu",
          "hidden": false
        },
        {
          "_id": "68f5a20f8589920bf4d32139",
          "name": "Yujun Shen",
          "hidden": false
        },
        {
          "_id": "68f5a20f8589920bf4d3213a",
          "name": "Qifeng Chen",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63f0baf66309c84d5f4a2226/Ep2-xcIdP80khURfhTDcz.mp4"
      ],
      "publishedAt": "2025-10-17T15:31:40.000Z",
      "submittedOnDailyAt": "2025-10-20T01:18:11.842Z",
      "title": "Scaling Instruction-Based Video Editing with a High-Quality Synthetic\n  Dataset",
      "submittedOnDailyBy": {
        "_id": "63f0baf66309c84d5f4a2226",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f0baf66309c84d5f4a2226/ihOgtwseRkfP1t-60IgyT.jpeg",
        "isPro": true,
        "fullname": "Qingyan",
        "user": "QingyanBai",
        "type": "user"
      },
      "summary": "Instruction-based video editing promises to democratize content creation, yet\nits progress is severely hampered by the scarcity of large-scale, high-quality\ntraining data. We introduce Ditto, a holistic framework designed to tackle this\nfundamental challenge. At its heart, Ditto features a novel data generation\npipeline that fuses the creative diversity of a leading image editor with an\nin-context video generator, overcoming the limited scope of existing models. To\nmake this process viable, our framework resolves the prohibitive cost-quality\ntrade-off by employing an efficient, distilled model architecture augmented by\na temporal enhancer, which simultaneously reduces computational overhead and\nimproves temporal coherence. Finally, to achieve full scalability, this entire\npipeline is driven by an intelligent agent that crafts diverse instructions and\nrigorously filters the output, ensuring quality control at scale. Using this\nframework, we invested over 12,000 GPU-days to build Ditto-1M, a new dataset of\none million high-fidelity video editing examples. We trained our model, Editto,\non Ditto-1M with a curriculum learning strategy. The results demonstrate\nsuperior instruction-following ability and establish a new state-of-the-art in\ninstruction-based video editing.",
      "upvotes": 22,
      "discussionId": "68f5a20f8589920bf4d3213b",
      "projectPage": "https://ezioby.github.io/Ditto_page",
      "githubRepo": "https://github.com/EzioBy/Ditto",
      "ai_summary": "Ditto framework addresses data scarcity in instruction-based video editing by generating a large dataset and using a curriculum learning strategy to train Editto, achieving superior instruction-following ability.",
      "ai_keywords": [
        "data generation pipeline",
        "image editor",
        "in-context video generator",
        "distilled model architecture",
        "temporal enhancer",
        "temporal coherence",
        "intelligent agent",
        "curriculum learning strategy",
        "instruction-following ability"
      ],
      "githubStars": 12
    },
    "publishedAt": "2025-10-17T11:31:40.000Z",
    "title": "Scaling Instruction-Based Video Editing with a High-Quality Synthetic\n  Dataset",
    "summary": "Instruction-based video editing promises to democratize content creation, yet\nits progress is severely hampered by the scarcity of large-scale, high-quality\ntraining data. We introduce Ditto, a holistic framework designed to tackle this\nfundamental challenge. At its heart, Ditto features a novel data generation\npipeline that fuses the creative diversity of a leading image editor with an\nin-context video generator, overcoming the limited scope of existing models. To\nmake this process viable, our framework resolves the prohibitive cost-quality\ntrade-off by employing an efficient, distilled model architecture augmented by\na temporal enhancer, which simultaneously reduces computational overhead and\nimproves temporal coherence. Finally, to achieve full scalability, this entire\npipeline is driven by an intelligent agent that crafts diverse instructions and\nrigorously filters the output, ensuring quality control at scale. Using this\nframework, we invested over 12,000 GPU-days to build Ditto-1M, a new dataset of\none million high-fidelity video editing examples. We trained our model, Editto,\non Ditto-1M with a curriculum learning strategy. The results demonstrate\nsuperior instruction-following ability and establish a new state-of-the-art in\ninstruction-based video editing.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63f0baf66309c84d5f4a2226/Ep2-xcIdP80khURfhTDcz.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.15742.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63f0baf66309c84d5f4a2226",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f0baf66309c84d5f4a2226/ihOgtwseRkfP1t-60IgyT.jpeg",
      "fullname": "Qingyan",
      "name": "QingyanBai",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.15301",
      "authors": [
        {
          "_id": "68f59fc78589920bf4d32123",
          "name": "Minglei Shi",
          "hidden": false
        },
        {
          "_id": "68f59fc78589920bf4d32124",
          "name": "Haolin Wang",
          "hidden": false
        },
        {
          "_id": "68f59fc78589920bf4d32125",
          "name": "Wenzhao Zheng",
          "hidden": false
        },
        {
          "_id": "68f59fc78589920bf4d32126",
          "name": "Ziyang Yuan",
          "hidden": false
        },
        {
          "_id": "68f59fc78589920bf4d32127",
          "name": "Xiaoshi Wu",
          "hidden": false
        },
        {
          "_id": "68f59fc78589920bf4d32128",
          "name": "Xintao Wang",
          "hidden": false
        },
        {
          "_id": "68f59fc78589920bf4d32129",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "68f59fc78589920bf4d3212a",
          "name": "Jie Zhou",
          "hidden": false
        },
        {
          "_id": "68f59fc78589920bf4d3212b",
          "name": "Jiwen Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-17T04:17:44.000Z",
      "submittedOnDailyAt": "2025-10-20T01:05:37.276Z",
      "title": "Latent Diffusion Model without Variational Autoencoder",
      "submittedOnDailyBy": {
        "_id": "662887715d246621f33d2ce6",
        "avatarUrl": "/avatars/3e0b1017b1e1bf284758ce840c174290.svg",
        "isPro": false,
        "fullname": "Shi Minglei",
        "user": "MingleiShi",
        "type": "user"
      },
      "summary": "Recent progress in diffusion-based visual generation has largely relied on\nlatent diffusion models with variational autoencoders (VAEs). While effective\nfor high-fidelity synthesis, this VAE+diffusion paradigm suffers from limited\ntraining efficiency, slow inference, and poor transferability to broader vision\ntasks. These issues stem from a key limitation of VAE latent spaces: the lack\nof clear semantic separation and strong discriminative structure. Our analysis\nconfirms that these properties are crucial not only for perception and\nunderstanding tasks, but also for the stable and efficient training of latent\ndiffusion models. Motivated by this insight, we introduce SVG, a novel latent\ndiffusion model without variational autoencoders, which leverages\nself-supervised representations for visual generation. SVG constructs a feature\nspace with clear semantic discriminability by leveraging frozen DINO features,\nwhile a lightweight residual branch captures fine-grained details for\nhigh-fidelity reconstruction. Diffusion models are trained directly on this\nsemantically structured latent space to facilitate more efficient learning. As\na result, SVG enables accelerated diffusion training, supports few-step\nsampling, and improves generative quality. Experimental results further show\nthat SVG preserves the semantic and discriminative capabilities of the\nunderlying self-supervised representations, providing a principled pathway\ntoward task-general, high-quality visual representations.",
      "upvotes": 21,
      "discussionId": "68f59fc88589920bf4d3212c",
      "projectPage": "https://howlin-wang.github.io/svg/",
      "githubRepo": "https://github.com/shiml20/SVG",
      "ai_summary": "SVG, a novel latent diffusion model without VAEs, uses self-supervised representations to enable efficient training, few-step sampling, and high-quality visual generation with semantic and discriminative capabilities.",
      "ai_keywords": [
        "latent diffusion models",
        "variational autoencoders",
        "diffusion models",
        "self-supervised representations",
        "DINO features",
        "residual branch",
        "high-fidelity reconstruction",
        "semantic discriminability",
        "task-general",
        "visual representations"
      ],
      "githubStars": 12,
      "organization": {
        "_id": "662c559b322afcbae51b3c8b",
        "name": "KwaiVGI",
        "fullname": "Kuaishou Visual Generation and Interaction Center",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"
      }
    },
    "publishedAt": "2025-10-17T00:17:44.000Z",
    "title": "Latent Diffusion Model without Variational Autoencoder",
    "summary": "Recent progress in diffusion-based visual generation has largely relied on\nlatent diffusion models with variational autoencoders (VAEs). While effective\nfor high-fidelity synthesis, this VAE+diffusion paradigm suffers from limited\ntraining efficiency, slow inference, and poor transferability to broader vision\ntasks. These issues stem from a key limitation of VAE latent spaces: the lack\nof clear semantic separation and strong discriminative structure. Our analysis\nconfirms that these properties are crucial not only for perception and\nunderstanding tasks, but also for the stable and efficient training of latent\ndiffusion models. Motivated by this insight, we introduce SVG, a novel latent\ndiffusion model without variational autoencoders, which leverages\nself-supervised representations for visual generation. SVG constructs a feature\nspace with clear semantic discriminability by leveraging frozen DINO features,\nwhile a lightweight residual branch captures fine-grained details for\nhigh-fidelity reconstruction. Diffusion models are trained directly on this\nsemantically structured latent space to facilitate more efficient learning. As\na result, SVG enables accelerated diffusion training, supports few-step\nsampling, and improves generative quality. Experimental results further show\nthat SVG preserves the semantic and discriminative capabilities of the\nunderlying self-supervised representations, providing a principled pathway\ntoward task-general, high-quality visual representations.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.15301.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "662887715d246621f33d2ce6",
      "avatarUrl": "/avatars/3e0b1017b1e1bf284758ce840c174290.svg",
      "fullname": "Shi Minglei",
      "name": "MingleiShi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "organization": {
      "_id": "662c559b322afcbae51b3c8b",
      "name": "KwaiVGI",
      "fullname": "Kuaishou Visual Generation and Interaction Center",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.15868",
      "authors": [
        {
          "_id": "68f592b28589920bf4d320a6",
          "name": "Shr-Ruei Tsai",
          "hidden": false
        },
        {
          "_id": "68f592b28589920bf4d320a7",
          "name": "Wei-Cheng Chang",
          "hidden": false
        },
        {
          "_id": "68f592b28589920bf4d320a8",
          "name": "Jie-Ying Lee",
          "hidden": false
        },
        {
          "_id": "68f592b28589920bf4d320a9",
          "name": "Chih-Hai Su",
          "hidden": false
        },
        {
          "_id": "68f592b28589920bf4d320aa",
          "name": "Yu-Lun Liu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/AA0iCVZKQKkdsonKfSirM.mp4"
      ],
      "publishedAt": "2025-10-17T17:59:50.000Z",
      "submittedOnDailyAt": "2025-10-20T00:10:34.903Z",
      "title": "LightsOut: Diffusion-based Outpainting for Enhanced Lens Flare Removal",
      "submittedOnDailyBy": {
        "_id": "6459d5da3b6fafd9664807ab",
        "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
        "isPro": false,
        "fullname": "Yu-Lun Liu",
        "user": "yulunliu",
        "type": "user"
      },
      "summary": "Lens flare significantly degrades image quality, impacting critical computer\nvision tasks like object detection and autonomous driving. Recent Single Image\nFlare Removal (SIFR) methods perform poorly when off-frame light sources are\nincomplete or absent. We propose LightsOut, a diffusion-based outpainting\nframework tailored to enhance SIFR by reconstructing off-frame light sources.\nOur method leverages a multitask regression module and LoRA fine-tuned\ndiffusion model to ensure realistic and physically consistent outpainting\nresults. Comprehensive experiments demonstrate LightsOut consistently boosts\nthe performance of existing SIFR methods across challenging scenarios without\nadditional retraining, serving as a universally applicable plug-and-play\npreprocessing solution. Project page: https://ray-1026.github.io/lightsout/",
      "upvotes": 17,
      "discussionId": "68f592b38589920bf4d320ab",
      "projectPage": "https://ray-1026.github.io/lightsout/",
      "githubRepo": "https://github.com/Ray-1026/LightsOut-official",
      "ai_summary": "LightsOut enhances Single Image Flare Removal by reconstructing off-frame light sources using a diffusion-based outpainting framework, improving performance across challenging scenarios.",
      "ai_keywords": [
        "diffusion-based outpainting",
        "multitask regression module",
        "LoRA fine-tuned diffusion model",
        "Single Image Flare Removal",
        "off-frame light sources"
      ],
      "githubStars": 7,
      "organization": {
        "_id": "63e39e6499a032b1c950403d",
        "name": "NYCU",
        "fullname": "National Yang Ming Chiao Tung University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63e39df6c65f975b436bb6b8/WLWf1bSpvrXBYYKEdXbgU.png"
      }
    },
    "publishedAt": "2025-10-17T13:59:50.000Z",
    "title": "LightsOut: Diffusion-based Outpainting for Enhanced Lens Flare Removal",
    "summary": "Lens flare significantly degrades image quality, impacting critical computer\nvision tasks like object detection and autonomous driving. Recent Single Image\nFlare Removal (SIFR) methods perform poorly when off-frame light sources are\nincomplete or absent. We propose LightsOut, a diffusion-based outpainting\nframework tailored to enhance SIFR by reconstructing off-frame light sources.\nOur method leverages a multitask regression module and LoRA fine-tuned\ndiffusion model to ensure realistic and physically consistent outpainting\nresults. Comprehensive experiments demonstrate LightsOut consistently boosts\nthe performance of existing SIFR methods across challenging scenarios without\nadditional retraining, serving as a universally applicable plug-and-play\npreprocessing solution. Project page: https://ray-1026.github.io/lightsout/",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/AA0iCVZKQKkdsonKfSirM.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.15868.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6459d5da3b6fafd9664807ab",
      "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
      "fullname": "Yu-Lun Liu",
      "name": "yulunliu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "organization": {
      "_id": "63e39e6499a032b1c950403d",
      "name": "NYCU",
      "fullname": "National Yang Ming Chiao Tung University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63e39df6c65f975b436bb6b8/WLWf1bSpvrXBYYKEdXbgU.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.12838",
      "authors": [
        {
          "_id": "68f5c0698589920bf4d321ab",
          "name": "Qianben Chen",
          "hidden": false
        },
        {
          "_id": "68f5c0698589920bf4d321ac",
          "name": "Jingyi Cao",
          "hidden": false
        },
        {
          "_id": "68f5c0698589920bf4d321ad",
          "name": "Jiayu Zhang",
          "hidden": false
        },
        {
          "_id": "68f5c0698589920bf4d321ae",
          "name": "Tianrui Qin",
          "hidden": false
        },
        {
          "_id": "68f5c0698589920bf4d321af",
          "name": "Xiaowan Li",
          "hidden": false
        },
        {
          "_id": "68f5c0698589920bf4d321b0",
          "name": "King Zhu",
          "hidden": false
        },
        {
          "_id": "68f5c0698589920bf4d321b1",
          "name": "Dingfeng Shi",
          "hidden": false
        },
        {
          "_id": "68f5c0698589920bf4d321b2",
          "name": "He Zhu",
          "hidden": false
        },
        {
          "_id": "68f5c0698589920bf4d321b3",
          "name": "Minghao Liu",
          "hidden": false
        },
        {
          "_id": "68f5c0698589920bf4d321b4",
          "name": "Xiaobo Liang",
          "hidden": false
        },
        {
          "_id": "68f5c0698589920bf4d321b5",
          "name": "Xin Gui",
          "hidden": false
        },
        {
          "_id": "68f5c0698589920bf4d321b6",
          "name": "Ge Zhang",
          "hidden": false
        },
        {
          "_id": "68f5c0698589920bf4d321b7",
          "name": "Jian Yang",
          "hidden": false
        },
        {
          "_id": "68f5c0698589920bf4d321b8",
          "name": "Yuchen Eleanor Jiang",
          "hidden": false
        },
        {
          "_id": "68f5c0698589920bf4d321b9",
          "name": "Wangchunshu Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-13T17:08:25.000Z",
      "submittedOnDailyAt": "2025-10-20T03:28:18.913Z",
      "title": "A^2FM: An Adaptive Agent Foundation Model for Tool-Aware Hybrid\n  Reasoning",
      "submittedOnDailyBy": {
        "_id": "64be3a8b86e7fb5b8a7ec8a9",
        "avatarUrl": "/avatars/58280c82f643a5a8073623eff33fefb2.svg",
        "isPro": false,
        "fullname": "Chen",
        "user": "Qianben",
        "type": "user"
      },
      "summary": "Large language models split into two families: reasoning-centric LLMs, which\nstrengthen internal chain-of-thought reasoning but cannot invoke external\ntools, and agentic LLMs, which learn to interact with environments and leverage\ntools but often lag in deep reasoning. This divide arises from fundamentally\ndifferent training objectives, leading to mismatched strengths and inefficiency\non simple queries, where both families tend to overthink or over-call tools. In\nthis work, we present Adaptive Agent Foundation Model (A^2FM), a unified\nframework that follows a route-then-align principle: the model first learns\ntask-aware routing and then aligns mode-specific trajectories under a shared\nbackbone. To address the inefficiency gap, we introduce a third\nmode-instant-that handles simple queries directly, preventing unnecessary\nreasoning or tool calls while complementing the agentic and reasoning modes. To\njointly enhance accuracy and efficiency, we propose Adaptive Policy\nOptimization (APO), which enforces adaptive sampling across modes and applies a\ncost-regularized reward. On the 32B scale, A^2FM achieves 13.4% on\nBrowseComp, 70.4% on AIME25, and 16.7% on HLE, setting new SOTA among\ncomparable models and performing competitively with frontier LLMs across\nagentic, reasoning, and general benchmarks. Notably, the adaptive execution\nachieves a cost of pass of only $0.00487 per correct answer-cutting cost by\n45.2% relative to reasoning and 33.5% relative to agentic, thus delivering\nsubstantially higher cost efficiency while maintaining comparable accuracy.",
      "upvotes": 15,
      "discussionId": "68f5c06a8589920bf4d321ba",
      "githubRepo": "https://github.com/OPPO-PersonalAI/Adaptive_Agent_Foundation_Models",
      "ai_summary": "A unified framework, A$^2$FM, combines reasoning and agentic capabilities in large language models, improving efficiency and accuracy across benchmarks by adaptively routing queries and optimizing policy.",
      "ai_keywords": [
        "reasoning-centric LLMs",
        "agentic LLMs",
        "task-aware routing",
        "mode-specific trajectories",
        "shared backbone",
        "Adaptive Policy Optimization",
        "APO",
        "cost-regularized reward",
        "BrowseComp",
        "AIME25",
        "HLE",
        "SOTA",
        "adaptive execution"
      ],
      "githubStars": 0,
      "organization": {
        "_id": "67177eecd0fad5b4ccc09461",
        "name": "OPPOer",
        "fullname": "OPPO",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66e24afddce93c7249b418c0/gQ-XFJehEyAH12zhbeR8Z.png"
      }
    },
    "publishedAt": "2025-10-13T13:08:25.000Z",
    "title": "A^2FM: An Adaptive Agent Foundation Model for Tool-Aware Hybrid\n  Reasoning",
    "summary": "Large language models split into two families: reasoning-centric LLMs, which\nstrengthen internal chain-of-thought reasoning but cannot invoke external\ntools, and agentic LLMs, which learn to interact with environments and leverage\ntools but often lag in deep reasoning. This divide arises from fundamentally\ndifferent training objectives, leading to mismatched strengths and inefficiency\non simple queries, where both families tend to overthink or over-call tools. In\nthis work, we present Adaptive Agent Foundation Model (A^2FM), a unified\nframework that follows a route-then-align principle: the model first learns\ntask-aware routing and then aligns mode-specific trajectories under a shared\nbackbone. To address the inefficiency gap, we introduce a third\nmode-instant-that handles simple queries directly, preventing unnecessary\nreasoning or tool calls while complementing the agentic and reasoning modes. To\njointly enhance accuracy and efficiency, we propose Adaptive Policy\nOptimization (APO), which enforces adaptive sampling across modes and applies a\ncost-regularized reward. On the 32B scale, A^2FM achieves 13.4% on\nBrowseComp, 70.4% on AIME25, and 16.7% on HLE, setting new SOTA among\ncomparable models and performing competitively with frontier LLMs across\nagentic, reasoning, and general benchmarks. Notably, the adaptive execution\nachieves a cost of pass of only $0.00487 per correct answer-cutting cost by\n45.2% relative to reasoning and 33.5% relative to agentic, thus delivering\nsubstantially higher cost efficiency while maintaining comparable accuracy.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.12838.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64be3a8b86e7fb5b8a7ec8a9",
      "avatarUrl": "/avatars/58280c82f643a5a8073623eff33fefb2.svg",
      "fullname": "Chen",
      "name": "Qianben",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "organization": {
      "_id": "67177eecd0fad5b4ccc09461",
      "name": "OPPOer",
      "fullname": "OPPO",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66e24afddce93c7249b418c0/gQ-XFJehEyAH12zhbeR8Z.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.14265",
      "authors": [
        {
          "_id": "68f592bd8589920bf4d320be",
          "name": "Xukai Wang",
          "hidden": false
        },
        {
          "_id": "68f592bd8589920bf4d320bf",
          "name": "Xuanbo Liu",
          "hidden": false
        },
        {
          "_id": "68f592bd8589920bf4d320c0",
          "name": "Mingrui Chen",
          "hidden": false
        },
        {
          "_id": "68f592bd8589920bf4d320c1",
          "name": "Haitian Zhong",
          "hidden": false
        },
        {
          "_id": "68f592bd8589920bf4d320c2",
          "name": "Xuanlin Yang",
          "hidden": false
        },
        {
          "_id": "68f592bd8589920bf4d320c3",
          "name": "Bohan Zeng",
          "hidden": false
        },
        {
          "_id": "68f592bd8589920bf4d320c4",
          "name": "Jinbo Hu",
          "hidden": false
        },
        {
          "_id": "68f592bd8589920bf4d320c5",
          "name": "Hao Liang",
          "hidden": false
        },
        {
          "_id": "68f592bd8589920bf4d320c6",
          "name": "Junbo Niu",
          "hidden": false
        },
        {
          "_id": "68f592bd8589920bf4d320c7",
          "name": "Xuchen Li",
          "hidden": false
        },
        {
          "_id": "68f592bd8589920bf4d320c8",
          "name": "Ruitao Wu",
          "hidden": false
        },
        {
          "_id": "68f592bd8589920bf4d320c9",
          "name": "Ruichuan An",
          "hidden": false
        },
        {
          "_id": "68f592bd8589920bf4d320ca",
          "name": "Yang Shi",
          "hidden": false
        },
        {
          "_id": "68f592bd8589920bf4d320cb",
          "name": "Liu Liu",
          "hidden": false
        },
        {
          "_id": "68f592bd8589920bf4d320cc",
          "name": "Xu-Yao Zhang",
          "hidden": false
        },
        {
          "_id": "68f592bd8589920bf4d320cd",
          "name": "Qiang Liu",
          "hidden": false
        },
        {
          "_id": "68f592bd8589920bf4d320ce",
          "name": "Zhouchen Lin",
          "hidden": false
        },
        {
          "_id": "68f592bd8589920bf4d320cf",
          "name": "Wentao Zhang",
          "hidden": false
        },
        {
          "_id": "68f592bd8589920bf4d320d0",
          "name": "Bin Dong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-16T03:30:56.000Z",
      "submittedOnDailyAt": "2025-10-20T00:11:27.651Z",
      "title": "MorphoBench: A Benchmark with Difficulty Adaptive to Model Reasoning",
      "submittedOnDailyBy": {
        "_id": "6671214c92412fd4640714eb",
        "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg",
        "isPro": false,
        "fullname": "bohan zeng",
        "user": "zbhpku",
        "type": "user"
      },
      "summary": "With the advancement of powerful large-scale reasoning models, effectively\nevaluating the reasoning capabilities of these models has become increasingly\nimportant. However, existing benchmarks designed to assess the reasoning\nabilities of large models tend to be limited in scope and lack the flexibility\nto adapt their difficulty according to the evolving reasoning capacities of the\nmodels. To address this, we propose MorphoBench, a benchmark that incorporates\nmultidisciplinary questions to evaluate the reasoning capabilities of large\nmodels and can adjust and update question difficulty based on the reasoning\nabilities of advanced models. Specifically, we curate the benchmark by\nselecting and collecting complex reasoning questions from existing benchmarks\nand sources such as Olympiad-level competitions. Additionally, MorphoBench\nadaptively modifies the analytical challenge of questions by leveraging key\nstatements generated during the model's reasoning process. Furthermore, it\nincludes questions generated using simulation software, enabling dynamic\nadjustment of benchmark difficulty with minimal resource consumption. We have\ngathered over 1,300 test questions and iteratively adjusted the difficulty of\nMorphoBench based on the reasoning capabilities of models such as o3 and GPT-5.\nMorphoBench enhances the comprehensiveness and validity of model reasoning\nevaluation, providing reliable guidance for improving both the reasoning\nabilities and scientific robustness of large models. The code has been released\nin https://github.com/OpenDCAI/MorphoBench.",
      "upvotes": 13,
      "discussionId": "68f592bd8589920bf4d320d1",
      "githubRepo": "https://github.com/OpenDCAI/MorphoBench",
      "ai_summary": "MorphoBench is a benchmark that evaluates large models' reasoning capabilities using multidisciplinary questions, adaptive difficulty, and simulation-generated questions.",
      "ai_keywords": [
        "MorphoBench",
        "reasoning capabilities",
        "multidisciplinary questions",
        "adaptive difficulty",
        "simulation-generated questions"
      ],
      "githubStars": 4,
      "organization": {
        "_id": "68896d3a716ee5bfb1428441",
        "name": "ZGCA",
        "fullname": "Zhongguancun Academy",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6854c3ab09a3ba7d16243875/aZ3tp3lZk1yQoXDwSklye.png"
      }
    },
    "publishedAt": "2025-10-15T23:30:56.000Z",
    "title": "MorphoBench: A Benchmark with Difficulty Adaptive to Model Reasoning",
    "summary": "With the advancement of powerful large-scale reasoning models, effectively\nevaluating the reasoning capabilities of these models has become increasingly\nimportant. However, existing benchmarks designed to assess the reasoning\nabilities of large models tend to be limited in scope and lack the flexibility\nto adapt their difficulty according to the evolving reasoning capacities of the\nmodels. To address this, we propose MorphoBench, a benchmark that incorporates\nmultidisciplinary questions to evaluate the reasoning capabilities of large\nmodels and can adjust and update question difficulty based on the reasoning\nabilities of advanced models. Specifically, we curate the benchmark by\nselecting and collecting complex reasoning questions from existing benchmarks\nand sources such as Olympiad-level competitions. Additionally, MorphoBench\nadaptively modifies the analytical challenge of questions by leveraging key\nstatements generated during the model's reasoning process. Furthermore, it\nincludes questions generated using simulation software, enabling dynamic\nadjustment of benchmark difficulty with minimal resource consumption. We have\ngathered over 1,300 test questions and iteratively adjusted the difficulty of\nMorphoBench based on the reasoning capabilities of models such as o3 and GPT-5.\nMorphoBench enhances the comprehensiveness and validity of model reasoning\nevaluation, providing reliable guidance for improving both the reasoning\nabilities and scientific robustness of large models. The code has been released\nin https://github.com/OpenDCAI/MorphoBench.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14265.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6671214c92412fd4640714eb",
      "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg",
      "fullname": "bohan zeng",
      "name": "zbhpku",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "organization": {
      "_id": "68896d3a716ee5bfb1428441",
      "name": "ZGCA",
      "fullname": "Zhongguancun Academy",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6854c3ab09a3ba7d16243875/aZ3tp3lZk1yQoXDwSklye.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.14438",
      "authors": [
        {
          "_id": "68f1acf66e0bef323a68fda5",
          "name": "Rui Wang",
          "hidden": false
        },
        {
          "_id": "68f1acf66e0bef323a68fda6",
          "name": "Ce Zhang",
          "hidden": false
        },
        {
          "_id": "68f1acf66e0bef323a68fda7",
          "name": "Jun-Yu Ma",
          "hidden": false
        },
        {
          "_id": "68f1acf66e0bef323a68fda8",
          "name": "Jianshu Zhang",
          "hidden": false
        },
        {
          "_id": "68f1acf66e0bef323a68fda9",
          "name": "Hongru Wang",
          "hidden": false
        },
        {
          "_id": "68f1acf66e0bef323a68fdaa",
          "name": "Yi Chen",
          "hidden": false
        },
        {
          "_id": "68f1acf66e0bef323a68fdab",
          "name": "Boyang Xue",
          "hidden": false
        },
        {
          "_id": "68f1acf66e0bef323a68fdac",
          "user": {
            "_id": "641129818573c51c0458b793",
            "avatarUrl": "/avatars/d4bc67c160a07146cf41c614678aa36b.svg",
            "isPro": false,
            "fullname": "Tianqing Fang",
            "user": "tqfang229",
            "type": "user"
          },
          "name": "Tianqing Fang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-17T05:56:46.664Z",
          "hidden": false
        },
        {
          "_id": "68f1acf66e0bef323a68fdad",
          "name": "Zhisong Zhang",
          "hidden": false
        },
        {
          "_id": "68f1acf66e0bef323a68fdae",
          "name": "Hongming Zhang",
          "hidden": false
        },
        {
          "_id": "68f1acf66e0bef323a68fdaf",
          "name": "Haitao Mi",
          "hidden": false
        },
        {
          "_id": "68f1acf66e0bef323a68fdb0",
          "name": "Dong Yu",
          "hidden": false
        },
        {
          "_id": "68f1acf66e0bef323a68fdb1",
          "name": "Kam-Fai Wong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-16T08:37:42.000Z",
      "submittedOnDailyAt": "2025-10-20T00:22:05.632Z",
      "title": "Explore to Evolve: Scaling Evolved Aggregation Logic via Proactive\n  Online Exploration for Deep Research Agents",
      "submittedOnDailyBy": {
        "_id": "67298b338c66e235932ca088",
        "avatarUrl": "/avatars/912990b3b8b2e67663fc395c73287593.svg",
        "isPro": false,
        "fullname": "WANG Rui",
        "user": "Ray121381",
        "type": "user"
      },
      "summary": "Deep research web agents not only retrieve information from diverse sources\nsuch as web environments, files, and multimodal inputs, but more importantly,\nthey need to rigorously analyze and aggregate knowledge for insightful\nresearch. However, existing open-source deep research agents predominantly\nfocus on enhancing information-seeking capabilities of web agents to locate\nspecific information, while overlooking the essential need for information\naggregation, which would limit their ability to support in-depth research. We\npropose an Explore to Evolve paradigm to scalably construct verifiable training\ndata for web agents. Begins with proactive online exploration, an agent sources\ngrounded information by exploring the real web. Using the collected evidence,\nthe agent then self-evolves an aggregation program by selecting, composing, and\nrefining operations from 12 high-level logical types to synthesize a verifiable\nQA pair. This evolution from high-level guidance to concrete operations allowed\nus to scalably produce WebAggregatorQA, a dataset of 10K samples across 50K\nwebsites and 11 domains. Based on an open-source agent framework, SmolAgents,\nwe collect supervised fine-tuning trajectories to develop a series of\nfoundation models, WebAggregator. WebAggregator-8B matches the performance of\nGPT-4.1, while the 32B variant surpasses GPT-4.1 by more than 10% on GAIA-text\nand closely approaches Claude-3.7-sonnet. Moreover, given the limited\navailability of benchmarks that evaluate web agents' information aggregation\nabilities, we construct a human-annotated evaluation split of WebAggregatorQA\nas a challenging test set. On this benchmark, Claude-3.7-sonnet only achieves\n28%, and GPT-4.1 scores 25.8%. Even when agents manage to retrieve all\nreferences, they still struggle on WebAggregatorQA, highlighting the need to\nstrengthen the information aggregation capabilities of web agent foundations.",
      "upvotes": 8,
      "discussionId": "68f1acf66e0bef323a68fdb2",
      "githubRepo": "https://github.com/Tencent/WebAggregator",
      "ai_summary": "A new paradigm, Explore to Evolve, is proposed to enhance web agents' information aggregation by constructing a large dataset and developing foundation models that outperform existing models on a challenging benchmark.",
      "ai_keywords": [
        "Explore to Evolve",
        "proactive online exploration",
        "aggregation program",
        "high-level logical types",
        "verifiable QA pair",
        "WebAggregatorQA",
        "SmolAgents",
        "WebAggregator",
        "GPT-4.1",
        "GAIA-text",
        "Claude-3.7-sonnet",
        "human-annotated evaluation split"
      ],
      "githubStars": 26
    },
    "publishedAt": "2025-10-16T04:37:42.000Z",
    "title": "Explore to Evolve: Scaling Evolved Aggregation Logic via Proactive\n  Online Exploration for Deep Research Agents",
    "summary": "Deep research web agents not only retrieve information from diverse sources\nsuch as web environments, files, and multimodal inputs, but more importantly,\nthey need to rigorously analyze and aggregate knowledge for insightful\nresearch. However, existing open-source deep research agents predominantly\nfocus on enhancing information-seeking capabilities of web agents to locate\nspecific information, while overlooking the essential need for information\naggregation, which would limit their ability to support in-depth research. We\npropose an Explore to Evolve paradigm to scalably construct verifiable training\ndata for web agents. Begins with proactive online exploration, an agent sources\ngrounded information by exploring the real web. Using the collected evidence,\nthe agent then self-evolves an aggregation program by selecting, composing, and\nrefining operations from 12 high-level logical types to synthesize a verifiable\nQA pair. This evolution from high-level guidance to concrete operations allowed\nus to scalably produce WebAggregatorQA, a dataset of 10K samples across 50K\nwebsites and 11 domains. Based on an open-source agent framework, SmolAgents,\nwe collect supervised fine-tuning trajectories to develop a series of\nfoundation models, WebAggregator. WebAggregator-8B matches the performance of\nGPT-4.1, while the 32B variant surpasses GPT-4.1 by more than 10% on GAIA-text\nand closely approaches Claude-3.7-sonnet. Moreover, given the limited\navailability of benchmarks that evaluate web agents' information aggregation\nabilities, we construct a human-annotated evaluation split of WebAggregatorQA\nas a challenging test set. On this benchmark, Claude-3.7-sonnet only achieves\n28%, and GPT-4.1 scores 25.8%. Even when agents manage to retrieve all\nreferences, they still struggle on WebAggregatorQA, highlighting the need to\nstrengthen the information aggregation capabilities of web agent foundations.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.14438.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67298b338c66e235932ca088",
      "avatarUrl": "/avatars/912990b3b8b2e67663fc395c73287593.svg",
      "fullname": "WANG Rui",
      "name": "Ray121381",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.15280",
      "authors": [
        {
          "_id": "68f59b3a8589920bf4d3210e",
          "name": "Fan Liu",
          "hidden": false
        },
        {
          "_id": "68f59b3a8589920bf4d3210f",
          "name": "Jindong Han",
          "hidden": false
        },
        {
          "_id": "68f59b3a8589920bf4d32110",
          "name": "Tengfei Lyu",
          "hidden": false
        },
        {
          "_id": "68f59b3a8589920bf4d32111",
          "name": "Weijia Zhang",
          "hidden": false
        },
        {
          "_id": "68f59b3a8589920bf4d32112",
          "name": "Zhe-Rui Yang",
          "hidden": false
        },
        {
          "_id": "68f59b3a8589920bf4d32113",
          "name": "Lu Dai",
          "hidden": false
        },
        {
          "_id": "68f59b3a8589920bf4d32114",
          "name": "Cancheng Liu",
          "hidden": false
        },
        {
          "_id": "68f59b3a8589920bf4d32115",
          "name": "Hao Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-17T03:40:26.000Z",
      "submittedOnDailyAt": "2025-10-20T03:31:20.249Z",
      "title": "Foundation Models for Scientific Discovery: From Paradigm Enhancement to\n  Paradigm Transition",
      "submittedOnDailyBy": {
        "_id": "65d6a6f7654f85ff0baf161f",
        "avatarUrl": "/avatars/4a46f8b0522fa572c122249a9d6526c4.svg",
        "isPro": false,
        "fullname": "Yansong NING",
        "user": "yasNing",
        "type": "user"
      },
      "summary": "Foundation models (FMs), such as GPT-4 and AlphaFold, are reshaping the\nlandscape of scientific research. Beyond accelerating tasks such as hypothesis\ngeneration, experimental design, and result interpretation, they prompt a more\nfundamental question: Are FMs merely enhancing existing scientific\nmethodologies, or are they redefining the way science is conducted? In this\npaper, we argue that FMs are catalyzing a transition toward a new scientific\nparadigm. We introduce a three-stage framework to describe this evolution: (1)\nMeta-Scientific Integration, where FMs enhance workflows within traditional\nparadigms; (2) Hybrid Human-AI Co-Creation, where FMs become active\ncollaborators in problem formulation, reasoning, and discovery; and (3)\nAutonomous Scientific Discovery, where FMs operate as independent agents\ncapable of generating new scientific knowledge with minimal human intervention.\nThrough this lens, we review current applications and emerging capabilities of\nFMs across existing scientific paradigms. We further identify risks and future\ndirections for FM-enabled scientific discovery. This position paper aims to\nsupport the scientific community in understanding the transformative role of\nFMs and to foster reflection on the future of scientific discovery. Our project\nis available at\nhttps://github.com/usail-hkust/Awesome-Foundation-Models-for-Scientific-Discovery.",
      "upvotes": 7,
      "discussionId": "68f59b3b8589920bf4d32116",
      "ai_summary": "Foundation models are evolving scientific methodologies from enhancement to autonomous discovery, prompting a new paradigm in research.",
      "ai_keywords": [
        ""
      ],
      "organization": {
        "_id": "65c5cfad1f4d1a70a9556a0c",
        "name": "usail-hkust",
        "fullname": "usail-hkust",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c5ceede286dbda4ea1a18e/yXjCSTmbtnk9Q-e1-njfZ.png"
      }
    },
    "publishedAt": "2025-10-16T23:40:26.000Z",
    "title": "Foundation Models for Scientific Discovery: From Paradigm Enhancement to\n  Paradigm Transition",
    "summary": "Foundation models (FMs), such as GPT-4 and AlphaFold, are reshaping the\nlandscape of scientific research. Beyond accelerating tasks such as hypothesis\ngeneration, experimental design, and result interpretation, they prompt a more\nfundamental question: Are FMs merely enhancing existing scientific\nmethodologies, or are they redefining the way science is conducted? In this\npaper, we argue that FMs are catalyzing a transition toward a new scientific\nparadigm. We introduce a three-stage framework to describe this evolution: (1)\nMeta-Scientific Integration, where FMs enhance workflows within traditional\nparadigms; (2) Hybrid Human-AI Co-Creation, where FMs become active\ncollaborators in problem formulation, reasoning, and discovery; and (3)\nAutonomous Scientific Discovery, where FMs operate as independent agents\ncapable of generating new scientific knowledge with minimal human intervention.\nThrough this lens, we review current applications and emerging capabilities of\nFMs across existing scientific paradigms. We further identify risks and future\ndirections for FM-enabled scientific discovery. This position paper aims to\nsupport the scientific community in understanding the transformative role of\nFMs and to foster reflection on the future of scientific discovery. Our project\nis available at\nhttps://github.com/usail-hkust/Awesome-Foundation-Models-for-Scientific-Discovery.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.15280.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "65d6a6f7654f85ff0baf161f",
      "avatarUrl": "/avatars/4a46f8b0522fa572c122249a9d6526c4.svg",
      "fullname": "Yansong NING",
      "name": "yasNing",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "organization": {
      "_id": "65c5cfad1f4d1a70a9556a0c",
      "name": "usail-hkust",
      "fullname": "usail-hkust",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65c5ceede286dbda4ea1a18e/yXjCSTmbtnk9Q-e1-njfZ.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.15857",
      "authors": [
        {
          "_id": "68f592b68589920bf4d320ad",
          "name": "Jiuhai Chen",
          "hidden": false
        },
        {
          "_id": "68f592b68589920bf4d320ae",
          "name": "Le Xue",
          "hidden": false
        },
        {
          "_id": "68f592b68589920bf4d320af",
          "name": "Zhiyang Xu",
          "hidden": false
        },
        {
          "_id": "68f592b68589920bf4d320b0",
          "name": "Xichen Pan",
          "hidden": false
        },
        {
          "_id": "68f592b68589920bf4d320b1",
          "name": "Shusheng Yang",
          "hidden": false
        },
        {
          "_id": "68f592b68589920bf4d320b2",
          "name": "Can Qin",
          "hidden": false
        },
        {
          "_id": "68f592b68589920bf4d320b3",
          "name": "An Yan",
          "hidden": false
        },
        {
          "_id": "68f592b68589920bf4d320b4",
          "name": "Honglu Zhou",
          "hidden": false
        },
        {
          "_id": "68f592b68589920bf4d320b5",
          "name": "Zeyuan Chen",
          "hidden": false
        },
        {
          "_id": "68f592b68589920bf4d320b6",
          "name": "Lifu Huang",
          "hidden": false
        },
        {
          "_id": "68f592b68589920bf4d320b7",
          "name": "Tianyi Zhou",
          "hidden": false
        },
        {
          "_id": "68f592b68589920bf4d320b8",
          "name": "Junnan Li",
          "hidden": false
        },
        {
          "_id": "68f592b68589920bf4d320b9",
          "name": "Silvio Savarese",
          "hidden": false
        },
        {
          "_id": "68f592b68589920bf4d320ba",
          "name": "Caiming Xiong",
          "hidden": false
        },
        {
          "_id": "68f592b68589920bf4d320bb",
          "name": "Ran Xu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-17T17:50:58.000Z",
      "submittedOnDailyAt": "2025-10-20T00:09:25.433Z",
      "title": "BLIP3o-NEXT: Next Frontier of Native Image Generation",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We present BLIP3o-NEXT, a fully open-source foundation model in the BLIP3\nseries that advances the next frontier of native image generation. BLIP3o-NEXT\nunifies text-to-image generation and image editing within a single\narchitecture, demonstrating strong image generation and image editing\ncapabilities. In developing the state-of-the-art native image generation model,\nwe identify four key insights: (1) Most architectural choices yield comparable\nperformance; an architecture can be deemed effective provided it scales\nefficiently and supports fast inference; (2) The successful application of\nreinforcement learning can further push the frontier of native image\ngeneration; (3) Image editing still remains a challenging task, yet instruction\nfollowing and the consistency between generated and reference images can be\nsignificantly enhanced through post-training and data engine; (4) Data quality\nand scale continue to be decisive factors that determine the upper bound of\nmodel performance. Building upon these insights, BLIP3o-NEXT leverages an\nAutoregressive + Diffusion architecture in which an autoregressive model first\ngenerates discrete image tokens conditioned on multimodal inputs, whose hidden\nstates are then used as conditioning signals for a diffusion model to generate\nhigh-fidelity images. This architecture integrates the reasoning strength and\ninstruction following of autoregressive models with the fine-detail rendering\nability of diffusion models, achieving a new level of coherence and realism.\nExtensive evaluations of various text-to-image and image-editing benchmarks\nshow that BLIP3o-NEXT achieves superior performance over existing models.",
      "upvotes": 6,
      "discussionId": "68f592b68589920bf4d320bc",
      "projectPage": "https://jiuhaichen.github.io/BLIP3o-NEXT.github.io/",
      "ai_summary": "BLIP3o-NEXT, a unified text-to-image generation and image editing model, uses an Autoregressive + Diffusion architecture to achieve superior performance and realism.",
      "ai_keywords": [
        "Autoregressive",
        "Diffusion",
        "text-to-image generation",
        "image editing",
        "discrete image tokens",
        "multimodal inputs",
        "hidden states",
        "high-fidelity images",
        "coherence",
        "realism"
      ],
      "organization": {
        "_id": "64e2dbe32fbff6ed9cf0a678",
        "name": "Saleforce",
        "fullname": "Salesforce",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64e2dbce8367d2da8ba618f2/u1ulbo4ifz1X4DV_ZwyPg.png"
      }
    },
    "publishedAt": "2025-10-17T13:50:58.000Z",
    "title": "BLIP3o-NEXT: Next Frontier of Native Image Generation",
    "summary": "We present BLIP3o-NEXT, a fully open-source foundation model in the BLIP3\nseries that advances the next frontier of native image generation. BLIP3o-NEXT\nunifies text-to-image generation and image editing within a single\narchitecture, demonstrating strong image generation and image editing\ncapabilities. In developing the state-of-the-art native image generation model,\nwe identify four key insights: (1) Most architectural choices yield comparable\nperformance; an architecture can be deemed effective provided it scales\nefficiently and supports fast inference; (2) The successful application of\nreinforcement learning can further push the frontier of native image\ngeneration; (3) Image editing still remains a challenging task, yet instruction\nfollowing and the consistency between generated and reference images can be\nsignificantly enhanced through post-training and data engine; (4) Data quality\nand scale continue to be decisive factors that determine the upper bound of\nmodel performance. Building upon these insights, BLIP3o-NEXT leverages an\nAutoregressive + Diffusion architecture in which an autoregressive model first\ngenerates discrete image tokens conditioned on multimodal inputs, whose hidden\nstates are then used as conditioning signals for a diffusion model to generate\nhigh-fidelity images. This architecture integrates the reasoning strength and\ninstruction following of autoregressive models with the fine-detail rendering\nability of diffusion models, achieving a new level of coherence and realism.\nExtensive evaluations of various text-to-image and image-editing benchmarks\nshow that BLIP3o-NEXT achieves superior performance over existing models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.15857.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 131
    },
    "organization": {
      "_id": "64e2dbe32fbff6ed9cf0a678",
      "name": "Saleforce",
      "fullname": "Salesforce",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64e2dbce8367d2da8ba618f2/u1ulbo4ifz1X4DV_ZwyPg.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.15110",
      "authors": [
        {
          "_id": "68f5a8b68589920bf4d32145",
          "name": "Shih-Yang Liu",
          "hidden": false
        },
        {
          "_id": "68f5a8b68589920bf4d32146",
          "name": "Xin Dong",
          "hidden": false
        },
        {
          "_id": "68f5a8b68589920bf4d32147",
          "name": "Ximing Lu",
          "hidden": false
        },
        {
          "_id": "68f5a8b68589920bf4d32148",
          "name": "Shizhe Diao",
          "hidden": false
        },
        {
          "_id": "68f5a8b68589920bf4d32149",
          "name": "Mingjie Liu",
          "hidden": false
        },
        {
          "_id": "68f5a8b68589920bf4d3214a",
          "name": "Min-Hung Chen",
          "hidden": false
        },
        {
          "_id": "68f5a8b68589920bf4d3214b",
          "name": "Hongxu Yin",
          "hidden": false
        },
        {
          "_id": "68f5a8b68589920bf4d3214c",
          "name": "Yu-Chiang Frank Wang",
          "hidden": false
        },
        {
          "_id": "68f5a8b68589920bf4d3214d",
          "name": "Kwang-Ting Cheng",
          "hidden": false
        },
        {
          "_id": "68f5a8b68589920bf4d3214e",
          "name": "Yejin Choi",
          "hidden": false
        },
        {
          "_id": "68f5a8b68589920bf4d3214f",
          "name": "Jan Kautz",
          "hidden": false
        },
        {
          "_id": "68f5a8b68589920bf4d32150",
          "name": "Pavlo Molchanov",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-16T20:05:57.000Z",
      "submittedOnDailyAt": "2025-10-20T01:44:52.713Z",
      "title": "DLER: Doing Length pEnalty Right - Incentivizing More Intelligence per\n  Token via Reinforcement Learning",
      "submittedOnDailyBy": {
        "_id": "64ae22dd1aee69ece065cdcd",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png",
        "isPro": false,
        "fullname": "Min-Hung Chen",
        "user": "cmhungsteve",
        "type": "user"
      },
      "summary": "Reasoning language models such as OpenAI-o1, DeepSeek-R1, and Qwen achieve\nstrong performance via extended chains of thought but often generate\nunnecessarily long outputs. Maximizing intelligence per token--accuracy\nrelative to response length--remains an open problem. We revisit reinforcement\nlearning (RL) with the simplest length penalty--truncation--and show that\naccuracy degradation arises not from the lack of sophisticated penalties but\nfrom inadequate RL optimization. We identify three key challenges: (i) large\nbias in advantage estimation, (ii) entropy collapse, and (iii) sparse reward\nsignal. We address them with Doing Length pEnalty Right (DLER), a training\nrecipe combining batch-wise reward normalization, higher clipping, dynamic\nsampling, and a simple truncation length penalty. DLER achieves\nstate-of-the-art accuracy--efficiency trade-offs, cutting output length by over\n70 percent while surpassing all previous baseline accuracy. It also improves\ntest-time scaling: compared to DeepSeek-R1-7B, DLER-7B generates multiple\nconcise responses in parallel with 28 percent higher accuracy and lower\nlatency. We further introduce Difficulty-Aware DLER, which adaptively tightens\ntruncation on easier questions for additional efficiency gains. We also propose\nan update-selective merging method that preserves baseline accuracy while\nretaining the concise reasoning ability of the DLER model, which is useful for\nscenarios where RL training data is scarce.",
      "upvotes": 6,
      "discussionId": "68f5a8b68589920bf4d32151",
      "ai_summary": "DLER, a reinforcement learning training recipe, improves the accuracy-efficiency trade-off in reasoning language models by addressing challenges in advantage estimation, entropy collapse, and sparse reward signals, leading to shorter outputs and better test-time scaling.",
      "ai_keywords": [
        "reinforcement learning",
        "advantage estimation",
        "entropy collapse",
        "sparse reward signal",
        "Doing Length pEnalty Right",
        "batch-wise reward normalization",
        "dynamic sampling",
        "Difficulty-Aware DLER",
        "update-selective merging"
      ],
      "organization": {
        "_id": "60262b67268c201cdc8b7d43",
        "name": "nvidia",
        "fullname": "NVIDIA",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
      }
    },
    "publishedAt": "2025-10-16T16:05:57.000Z",
    "title": "DLER: Doing Length pEnalty Right - Incentivizing More Intelligence per\n  Token via Reinforcement Learning",
    "summary": "Reasoning language models such as OpenAI-o1, DeepSeek-R1, and Qwen achieve\nstrong performance via extended chains of thought but often generate\nunnecessarily long outputs. Maximizing intelligence per token--accuracy\nrelative to response length--remains an open problem. We revisit reinforcement\nlearning (RL) with the simplest length penalty--truncation--and show that\naccuracy degradation arises not from the lack of sophisticated penalties but\nfrom inadequate RL optimization. We identify three key challenges: (i) large\nbias in advantage estimation, (ii) entropy collapse, and (iii) sparse reward\nsignal. We address them with Doing Length pEnalty Right (DLER), a training\nrecipe combining batch-wise reward normalization, higher clipping, dynamic\nsampling, and a simple truncation length penalty. DLER achieves\nstate-of-the-art accuracy--efficiency trade-offs, cutting output length by over\n70 percent while surpassing all previous baseline accuracy. It also improves\ntest-time scaling: compared to DeepSeek-R1-7B, DLER-7B generates multiple\nconcise responses in parallel with 28 percent higher accuracy and lower\nlatency. We further introduce Difficulty-Aware DLER, which adaptively tightens\ntruncation on easier questions for additional efficiency gains. We also propose\nan update-selective merging method that preserves baseline accuracy while\nretaining the concise reasoning ability of the DLER model, which is useful for\nscenarios where RL training data is scarce.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.15110.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ae22dd1aee69ece065cdcd",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png",
      "fullname": "Min-Hung Chen",
      "name": "cmhungsteve",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "organization": {
      "_id": "60262b67268c201cdc8b7d43",
      "name": "nvidia",
      "fullname": "NVIDIA",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.15564",
      "authors": [
        {
          "_id": "68f593d08589920bf4d320de",
          "name": "Xiaoming Zhu",
          "hidden": false
        },
        {
          "_id": "68f593d08589920bf4d320df",
          "name": "Xu Huang",
          "hidden": false
        },
        {
          "_id": "68f593d08589920bf4d320e0",
          "name": "Qinghongbing Xie",
          "hidden": false
        },
        {
          "_id": "68f593d08589920bf4d320e1",
          "name": "Zhi Deng",
          "hidden": false
        },
        {
          "_id": "68f593d08589920bf4d320e2",
          "name": "Junsheng Yu",
          "hidden": false
        },
        {
          "_id": "68f593d08589920bf4d320e3",
          "name": "Yirui Guan",
          "hidden": false
        },
        {
          "_id": "68f593d08589920bf4d320e4",
          "name": "Zhongyuan Liu",
          "hidden": false
        },
        {
          "_id": "68f593d08589920bf4d320e5",
          "name": "Lin Zhu",
          "hidden": false
        },
        {
          "_id": "68f593d08589920bf4d320e6",
          "name": "Qijun Zhao",
          "hidden": false
        },
        {
          "_id": "68f593d08589920bf4d320e7",
          "name": "Ligang Liu",
          "hidden": false
        },
        {
          "_id": "68f593d08589920bf4d320e8",
          "name": "Long Zeng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-17T11:48:08.000Z",
      "submittedOnDailyAt": "2025-10-20T00:13:55.960Z",
      "title": "Imaginarium: Vision-guided High-Quality 3D Scene Layout Generation",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Generating artistic and coherent 3D scene layouts is crucial in digital\ncontent creation. Traditional optimization-based methods are often constrained\nby cumbersome manual rules, while deep generative models face challenges in\nproducing content with richness and diversity. Furthermore, approaches that\nutilize large language models frequently lack robustness and fail to accurately\ncapture complex spatial relationships. To address these challenges, this paper\npresents a novel vision-guided 3D layout generation system. We first construct\na high-quality asset library containing 2,037 scene assets and 147 3D scene\nlayouts. Subsequently, we employ an image generation model to expand prompt\nrepresentations into images, fine-tuning it to align with our asset library. We\nthen develop a robust image parsing module to recover the 3D layout of scenes\nbased on visual semantics and geometric information. Finally, we optimize the\nscene layout using scene graphs and overall visual semantics to ensure logical\ncoherence and alignment with the images. Extensive user testing demonstrates\nthat our algorithm significantly outperforms existing methods in terms of\nlayout richness and quality. The code and dataset will be available at\nhttps://github.com/HiHiAllen/Imaginarium.",
      "upvotes": 5,
      "discussionId": "68f593d08589920bf4d320e9",
      "ai_summary": "A vision-guided 3D layout generation system uses an image generation model and scene graphs to produce rich and coherent 3D scenes from prompts.",
      "ai_keywords": [
        "image generation model",
        "image parsing module",
        "scene graphs",
        "3D layout generation",
        "visual semantics",
        "geometric information",
        "scene graphs",
        "logical coherence"
      ]
    },
    "publishedAt": "2025-10-17T07:48:08.000Z",
    "title": "Imaginarium: Vision-guided High-Quality 3D Scene Layout Generation",
    "summary": "Generating artistic and coherent 3D scene layouts is crucial in digital\ncontent creation. Traditional optimization-based methods are often constrained\nby cumbersome manual rules, while deep generative models face challenges in\nproducing content with richness and diversity. Furthermore, approaches that\nutilize large language models frequently lack robustness and fail to accurately\ncapture complex spatial relationships. To address these challenges, this paper\npresents a novel vision-guided 3D layout generation system. We first construct\na high-quality asset library containing 2,037 scene assets and 147 3D scene\nlayouts. Subsequently, we employ an image generation model to expand prompt\nrepresentations into images, fine-tuning it to align with our asset library. We\nthen develop a robust image parsing module to recover the 3D layout of scenes\nbased on visual semantics and geometric information. Finally, we optimize the\nscene layout using scene graphs and overall visual semantics to ensure logical\ncoherence and alignment with the images. Extensive user testing demonstrates\nthat our algorithm significantly outperforms existing methods in terms of\nlayout richness and quality. The code and dataset will be available at\nhttps://github.com/HiHiAllen/Imaginarium.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.15564.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 131
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.15831",
      "authors": [
        {
          "_id": "68f5a4b78589920bf4d3213d",
          "name": "Do Xuan Long",
          "hidden": false
        },
        {
          "_id": "68f5a4b78589920bf4d3213e",
          "name": "Xingchen Wan",
          "hidden": false
        },
        {
          "_id": "68f5a4b78589920bf4d3213f",
          "name": "Hootan Nakhost",
          "hidden": false
        },
        {
          "_id": "68f5a4b78589920bf4d32140",
          "name": "Chen-Yu Lee",
          "hidden": false
        },
        {
          "_id": "68f5a4b78589920bf4d32141",
          "name": "Tomas Pfister",
          "hidden": false
        },
        {
          "_id": "68f5a4b78589920bf4d32142",
          "name": "Sercan . Ark",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-17T17:12:08.000Z",
      "submittedOnDailyAt": "2025-10-20T01:26:57.483Z",
      "title": "VISTA: A Test-Time Self-Improving Video Generation Agent",
      "submittedOnDailyBy": {
        "_id": "63a9a0d13453852ef53c0b37",
        "avatarUrl": "/avatars/411c4ffc2a3e89047a23c6e7442f6ed5.svg",
        "isPro": false,
        "fullname": "Do Xuan Long",
        "user": "dxlong2000",
        "type": "user"
      },
      "summary": "Despite rapid advances in text-to-video synthesis, generated video quality\nremains critically dependent on precise user prompts. Existing test-time\noptimization methods, successful in other domains, struggle with the\nmulti-faceted nature of video. In this work, we introduce VISTA (Video\nIterative Self-improvemenT Agent), a novel multi-agent system that autonomously\nimproves video generation through refining prompts in an iterative loop. VISTA\nfirst decomposes a user idea into a structured temporal plan. After generation,\nthe best video is identified through a robust pairwise tournament. This winning\nvideo is then critiqued by a trio of specialized agents focusing on visual,\naudio, and contextual fidelity. Finally, a reasoning agent synthesizes this\nfeedback to introspectively rewrite and enhance the prompt for the next\ngeneration cycle. Experiments on single- and multi-scene video generation\nscenarios show that while prior methods yield inconsistent gains, VISTA\nconsistently improves video quality and alignment with user intent, achieving\nup to 60% pairwise win rate against state-of-the-art baselines. Human\nevaluators concur, preferring VISTA outputs in 66.4% of comparisons.",
      "upvotes": 4,
      "discussionId": "68f5a4b88589920bf4d32143",
      "ai_summary": "VISTA, a multi-agent system, iteratively refines user prompts to enhance video quality and alignment with user intent, outperforming existing methods.",
      "ai_keywords": [
        "text-to-video synthesis",
        "multi-agent system",
        "iterative loop",
        "structured temporal plan",
        "pairwise tournament",
        "visual fidelity",
        "audio fidelity",
        "contextual fidelity",
        "reasoning agent",
        "single-scene video generation",
        "multi-scene video generation"
      ]
    },
    "publishedAt": "2025-10-17T13:12:08.000Z",
    "title": "VISTA: A Test-Time Self-Improving Video Generation Agent",
    "summary": "Despite rapid advances in text-to-video synthesis, generated video quality\nremains critically dependent on precise user prompts. Existing test-time\noptimization methods, successful in other domains, struggle with the\nmulti-faceted nature of video. In this work, we introduce VISTA (Video\nIterative Self-improvemenT Agent), a novel multi-agent system that autonomously\nimproves video generation through refining prompts in an iterative loop. VISTA\nfirst decomposes a user idea into a structured temporal plan. After generation,\nthe best video is identified through a robust pairwise tournament. This winning\nvideo is then critiqued by a trio of specialized agents focusing on visual,\naudio, and contextual fidelity. Finally, a reasoning agent synthesizes this\nfeedback to introspectively rewrite and enhance the prompt for the next\ngeneration cycle. Experiments on single- and multi-scene video generation\nscenarios show that while prior methods yield inconsistent gains, VISTA\nconsistently improves video quality and alignment with user intent, achieving\nup to 60% pairwise win rate against state-of-the-art baselines. Human\nevaluators concur, preferring VISTA outputs in 66.4% of comparisons.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.15831.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a9a0d13453852ef53c0b37",
      "avatarUrl": "/avatars/411c4ffc2a3e89047a23c6e7442f6ed5.svg",
      "fullname": "Do Xuan Long",
      "name": "dxlong2000",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.15624",
      "authors": [
        {
          "_id": "68f5ab458589920bf4d32162",
          "name": "Ed Li",
          "hidden": false
        },
        {
          "_id": "68f5ab458589920bf4d32163",
          "name": "Junyu Ren",
          "hidden": false
        },
        {
          "_id": "68f5ab458589920bf4d32164",
          "name": "Xintian Pan",
          "hidden": false
        },
        {
          "_id": "68f5ab458589920bf4d32165",
          "name": "Cat Yan",
          "hidden": false
        },
        {
          "_id": "68f5ab458589920bf4d32166",
          "name": "Chuanhao Li",
          "hidden": false
        },
        {
          "_id": "68f5ab458589920bf4d32167",
          "name": "Dirk Bergemann",
          "hidden": false
        },
        {
          "_id": "68f5ab458589920bf4d32168",
          "name": "Zhuoran Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-17T13:13:32.000Z",
      "submittedOnDailyAt": "2025-10-20T04:35:41.526Z",
      "title": "Build Your Personalized Research Group: A Multiagent Framework for\n  Continual and Interactive Science Automation",
      "submittedOnDailyBy": {
        "_id": "646d769cda8e99940b71928e",
        "avatarUrl": "/avatars/acee495a23362aa39b3d3e75c9afd967.svg",
        "isPro": false,
        "fullname": "Zhuoran Yang",
        "user": "zhuoranyang",
        "type": "user"
      },
      "summary": "The automation of scientific discovery represents a critical milestone in\nArtificial Intelligence (AI) research. However, existing agentic systems for\nscience suffer from two fundamental limitations: rigid, pre-programmed\nworkflows that cannot adapt to intermediate findings, and inadequate context\nmanagement that hinders long-horizon research. We present\nfreephdlabor, an open-source multiagent framework featuring\nfully dynamic workflows determined by real-time agent reasoning and a\n\\textit{modular architecture} enabling seamless customization --\nusers can modify, add, or remove agents to address domain-specific\nrequirements. The framework provides comprehensive infrastructure including\nautomatic context compaction, workspace-based communication\nto prevent information degradation, memory persistence across\nsessions, and non-blocking human intervention mechanisms. These\nfeatures collectively transform automated research from isolated, single-run\nattempts into continual research programs that build systematically on\nprior explorations and incorporate human feedback. By providing both the\narchitectural principles and practical implementation for building customizable\nco-scientist systems, this work aims to facilitate broader adoption of\nautomated research across scientific domains, enabling practitioners to deploy\ninteractive multiagent systems that autonomously conduct end-to-end research --\nfrom ideation through experimentation to publication-ready manuscripts.",
      "upvotes": 4,
      "discussionId": "68f5ab458589920bf4d32169",
      "projectPage": "https://freephdlabor.github.io/",
      "githubRepo": "https://github.com/ltjed/freephdlabor",
      "ai_summary": "Freephdlabor is an open-source multiagent framework that supports dynamic workflows, modular architecture, and context management to enable continual and interactive automated scientific research.",
      "ai_keywords": [
        "fully dynamic workflows",
        "modular architecture",
        "automatic context compaction",
        "workspace-based communication",
        "memory persistence",
        "non-blocking human intervention",
        "continual research programs",
        "co-scientist systems"
      ],
      "githubStars": 39
    },
    "publishedAt": "2025-10-17T09:13:32.000Z",
    "title": "Build Your Personalized Research Group: A Multiagent Framework for\n  Continual and Interactive Science Automation",
    "summary": "The automation of scientific discovery represents a critical milestone in\nArtificial Intelligence (AI) research. However, existing agentic systems for\nscience suffer from two fundamental limitations: rigid, pre-programmed\nworkflows that cannot adapt to intermediate findings, and inadequate context\nmanagement that hinders long-horizon research. We present\nfreephdlabor, an open-source multiagent framework featuring\nfully dynamic workflows determined by real-time agent reasoning and a\n\\textit{modular architecture} enabling seamless customization --\nusers can modify, add, or remove agents to address domain-specific\nrequirements. The framework provides comprehensive infrastructure including\nautomatic context compaction, workspace-based communication\nto prevent information degradation, memory persistence across\nsessions, and non-blocking human intervention mechanisms. These\nfeatures collectively transform automated research from isolated, single-run\nattempts into continual research programs that build systematically on\nprior explorations and incorporate human feedback. By providing both the\narchitectural principles and practical implementation for building customizable\nco-scientist systems, this work aims to facilitate broader adoption of\nautomated research across scientific domains, enabling practitioners to deploy\ninteractive multiagent systems that autonomously conduct end-to-end research --\nfrom ideation through experimentation to publication-ready manuscripts.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.15624.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "646d769cda8e99940b71928e",
      "avatarUrl": "/avatars/acee495a23362aa39b3d3e75c9afd967.svg",
      "fullname": "Zhuoran Yang",
      "name": "zhuoranyang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.15232",
      "authors": [
        {
          "_id": "68f59eb88589920bf4d3211b",
          "name": "Tiansheng Hu",
          "hidden": false
        },
        {
          "_id": "68f59eb88589920bf4d3211c",
          "name": "Tongyan Hu",
          "hidden": false
        },
        {
          "_id": "68f59eb88589920bf4d3211d",
          "name": "Liuyang Bai",
          "hidden": false
        },
        {
          "_id": "68f59eb88589920bf4d3211e",
          "name": "Yilun Zhao",
          "hidden": false
        },
        {
          "_id": "68f59eb88589920bf4d3211f",
          "name": "Arman Cohan",
          "hidden": false
        },
        {
          "_id": "68f59eb88589920bf4d32120",
          "name": "Chen Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-17T01:45:49.000Z",
      "submittedOnDailyAt": "2025-10-20T01:02:05.029Z",
      "title": "FinTrust: A Comprehensive Benchmark of Trustworthiness Evaluation in\n  Finance Domain",
      "submittedOnDailyBy": {
        "_id": "67492b9e347c3876f22b3684",
        "avatarUrl": "/avatars/0d80d23f7b10ce8bac689f6e8317a014.svg",
        "isPro": false,
        "fullname": "Tiansheng Hu",
        "user": "HughieHu",
        "type": "user"
      },
      "summary": "Recent LLMs have demonstrated promising ability in solving finance related\nproblems. However, applying LLMs in real-world finance application remains\nchallenging due to its high risk and high stakes property. This paper\nintroduces FinTrust, a comprehensive benchmark specifically designed for\nevaluating the trustworthiness of LLMs in finance applications. Our benchmark\nfocuses on a wide range of alignment issues based on practical context and\nfeatures fine-grained tasks for each dimension of trustworthiness evaluation.\nWe assess eleven LLMs on FinTrust and find that proprietary models like o4-mini\noutperforms in most tasks such as safety while open-source models like\nDeepSeek-V3 have advantage in specific areas like industry-level fairness. For\nchallenging task like fiduciary alignment and disclosure, all LLMs fall short,\nshowing a significant gap in legal awareness. We believe that FinTrust can be a\nvaluable benchmark for LLMs' trustworthiness evaluation in finance domain.",
      "upvotes": 3,
      "discussionId": "68f59eb98589920bf4d32121",
      "githubRepo": "https://github.com/HughieHu/FinTrust/",
      "ai_summary": "FinTrust is a benchmark designed to evaluate the trustworthiness of LLMs in finance applications, focusing on alignment issues and revealing gaps in legal awareness.",
      "ai_keywords": [
        "LLMs",
        "FinTrust",
        "trustworthiness",
        "alignment issues",
        "safety",
        "industry-level fairness",
        "fiduciary alignment",
        "disclosure",
        "legal awareness"
      ],
      "githubStars": 1
    },
    "publishedAt": "2025-10-16T21:45:49.000Z",
    "title": "FinTrust: A Comprehensive Benchmark of Trustworthiness Evaluation in\n  Finance Domain",
    "summary": "Recent LLMs have demonstrated promising ability in solving finance related\nproblems. However, applying LLMs in real-world finance application remains\nchallenging due to its high risk and high stakes property. This paper\nintroduces FinTrust, a comprehensive benchmark specifically designed for\nevaluating the trustworthiness of LLMs in finance applications. Our benchmark\nfocuses on a wide range of alignment issues based on practical context and\nfeatures fine-grained tasks for each dimension of trustworthiness evaluation.\nWe assess eleven LLMs on FinTrust and find that proprietary models like o4-mini\noutperforms in most tasks such as safety while open-source models like\nDeepSeek-V3 have advantage in specific areas like industry-level fairness. For\nchallenging task like fiduciary alignment and disclosure, all LLMs fall short,\nshowing a significant gap in legal awareness. We believe that FinTrust can be a\nvaluable benchmark for LLMs' trustworthiness evaluation in finance domain.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.15232.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67492b9e347c3876f22b3684",
      "avatarUrl": "/avatars/0d80d23f7b10ce8bac689f6e8317a014.svg",
      "fullname": "Tiansheng Hu",
      "name": "HughieHu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.15859",
      "authors": [
        {
          "_id": "68f5bdca8589920bf4d3219b",
          "name": "Pengkai Wang",
          "hidden": false
        },
        {
          "_id": "68f5bdca8589920bf4d3219c",
          "name": "Qi Zuo",
          "hidden": false
        },
        {
          "_id": "68f5bdca8589920bf4d3219d",
          "name": "Pengwei Liu",
          "hidden": false
        },
        {
          "_id": "68f5bdca8589920bf4d3219e",
          "name": "Zhijie Sang",
          "hidden": false
        },
        {
          "_id": "68f5bdca8589920bf4d3219f",
          "name": "Congkai Xie",
          "hidden": false
        },
        {
          "_id": "68f5bdca8589920bf4d321a0",
          "name": "Hongxia Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-17T17:51:28.000Z",
      "submittedOnDailyAt": "2025-10-20T03:14:12.579Z",
      "title": "InfiMed-ORBIT: Aligning LLMs on Open-Ended Complex Tasks via\n  Rubric-Based Incremental Training",
      "submittedOnDailyBy": {
        "_id": "64245f2c089d5fae56b4549a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64245f2c089d5fae56b4549a/qUHFsL9Svwyj5BKpfMtaY.jpeg",
        "isPro": false,
        "fullname": "Pengxiang Li",
        "user": "pengxiang",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) have shown substantial advances through\nreinforcement learning (RL), particularly in domains where rewards can be\nprogrammatically verified, such as mathematics and code. In these areas, models\nbenefit from a well-defined operational base guided by explicit rule-based\nobjectives. However, this progress reveals a significant limitation: in\nopen-ended domains where rewards are ambiguous, subjective, or\ncontext-dependent, such as creative writing, scientific reasoning, and notably\nmedical consultation, robust reward functions are lacking, making these areas\nchallenging for current RL strategies. To bridge this gap, we introduce ORBIT,\nan open-ended rubric-based incremental training framework specifically designed\nfor high-stakes medical dialogue. ORBIT integrates syn- thetic dialogue\ngeneration with the dynamic creation of rubrics, employing these rubrics to\ndirect an incremental RL process. In particular, this approach does not depend\non external medical knowledge or manual rules, instead utilizing rubric-guided\nfeedback to shape learning. When implemented on the Qwen3-4B-Instruct model,\nour method can greatly enhance its performance on the HealthBench-Hard\nbenchmark from 7.0 to 27.2 using only 2k samples, thus achieving\nstate-of-the-art results for models of this scale. Our analysis confirms that\nrubric-driven RL fos-ters consistent performance gains across diverse\nconsultation scenarios, going beyond simple numerical improvements. These\nfindings underscore rubric-based feedback as a scalable strategy for advancing\nLLMs in intricate, open-ended tasks.",
      "upvotes": 2,
      "discussionId": "68f5bdcb8589920bf4d321a1",
      "ai_summary": "ORBIT, a rubric-based incremental training framework, enhances LLM performance in medical dialogue by using dynamic rubrics for RL, achieving state-of-the-art results on HealthBench-Hard.",
      "ai_keywords": [
        "reinforcement learning",
        "rubric-based incremental training",
        "synthetic dialogue generation",
        "rubric-guided feedback",
        "HealthBench-Hard",
        "Qwen3-4B-Instruct",
        "medical consultation",
        "open-ended tasks"
      ]
    },
    "publishedAt": "2025-10-17T13:51:28.000Z",
    "title": "InfiMed-ORBIT: Aligning LLMs on Open-Ended Complex Tasks via\n  Rubric-Based Incremental Training",
    "summary": "Large Language Models (LLMs) have shown substantial advances through\nreinforcement learning (RL), particularly in domains where rewards can be\nprogrammatically verified, such as mathematics and code. In these areas, models\nbenefit from a well-defined operational base guided by explicit rule-based\nobjectives. However, this progress reveals a significant limitation: in\nopen-ended domains where rewards are ambiguous, subjective, or\ncontext-dependent, such as creative writing, scientific reasoning, and notably\nmedical consultation, robust reward functions are lacking, making these areas\nchallenging for current RL strategies. To bridge this gap, we introduce ORBIT,\nan open-ended rubric-based incremental training framework specifically designed\nfor high-stakes medical dialogue. ORBIT integrates syn- thetic dialogue\ngeneration with the dynamic creation of rubrics, employing these rubrics to\ndirect an incremental RL process. In particular, this approach does not depend\non external medical knowledge or manual rules, instead utilizing rubric-guided\nfeedback to shape learning. When implemented on the Qwen3-4B-Instruct model,\nour method can greatly enhance its performance on the HealthBench-Hard\nbenchmark from 7.0 to 27.2 using only 2k samples, thus achieving\nstate-of-the-art results for models of this scale. Our analysis confirms that\nrubric-driven RL fos-ters consistent performance gains across diverse\nconsultation scenarios, going beyond simple numerical improvements. These\nfindings underscore rubric-based feedback as a scalable strategy for advancing\nLLMs in intricate, open-ended tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.15859.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64245f2c089d5fae56b4549a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64245f2c089d5fae56b4549a/qUHFsL9Svwyj5BKpfMtaY.jpeg",
      "fullname": "Pengxiang Li",
      "name": "pengxiang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.15262",
      "authors": [
        {
          "_id": "68f5aac18589920bf4d32153",
          "name": "Zhiyuan Fan",
          "hidden": false
        },
        {
          "_id": "68f5aac18589920bf4d32154",
          "name": "Yifeng Liu",
          "hidden": false
        },
        {
          "_id": "68f5aac18589920bf4d32155",
          "name": "Qingyue Zhao",
          "hidden": false
        },
        {
          "_id": "68f5aac18589920bf4d32156",
          "name": "Angela Yuan",
          "hidden": false
        },
        {
          "_id": "68f5aac18589920bf4d32157",
          "name": "Quanquan Gu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-17T02:58:35.000Z",
      "submittedOnDailyAt": "2025-10-20T02:04:06.992Z",
      "title": "Robust Layerwise Scaling Rules by Proper Weight Decay Tuning",
      "submittedOnDailyBy": {
        "_id": "653d276681f52ceb4d12bd85",
        "avatarUrl": "/avatars/56601a25e5f883a8f6dc15f6fd9dcc57.svg",
        "isPro": false,
        "fullname": "Yifeng Liu",
        "user": "Lewis-Lau",
        "type": "user"
      },
      "summary": "Empirical scaling laws prescribe how to allocate parameters, data, and\ncompute, while maximal-update parameterization (muP) enables learning-rate\ntransfer across widths by equalizing early-time update magnitudes. However, in\nmodern scale-invariant architectures, training quickly enters an\noptimizer-governed steady state where normalization layers create backward\nscale sensitivity and the effective learning rate becomes width dependent,\ndegrading muP transfer. We address this by introducing a weight-decay\nscaling rule for AdamW that preserves sublayer gain across widths. Empirically,\nthe singular-value spectrum of each matrix parameter scales in norm as\neta/lambda with an approximately invariant shape; under width\nscaling d, we observe that the top singular value scales approximately as\neta/lambdacdot d^{0.75}. Combining this observation with the muP\nlearning-rate rule eta_2propto d^{-1} for matrix-like parameters implies an\nempirical weight-decay scaling rule lambda_2propto d that\napproximately keeps sublayer gains width invariant. Together with vector-like\nparameters trained at eta_1=Theta_d(1) and lambda_1=0, this yields\nzero-shot transfer of both learning rate and weight decay from proxy to\ntarget widths, removing per-width sweeps. We validate the rule on LLaMA-style\nTransformers and in a minimal synthetic setting, and we provide a simple\ndiagnostic, matching top singular values, to check sublayer-gain invariance.\nOur results extend muP beyond the near-init regime by explicitly controlling\nsteady-state scales set by the optimizer, offering a practical recipe for\nwidth-robust hyperparameter transfer under AdamW.",
      "upvotes": 2,
      "discussionId": "68f5aac28589920bf4d32158",
      "ai_summary": "A new weight-decay scaling rule for AdamW is introduced to preserve sublayer gain across widths in modern scale-invariant architectures, enabling zero-shot transfer of learning rate and weight decay.",
      "ai_keywords": [
        "maximal-update parameterization",
        "$\\mu$P",
        "optimizer-governed steady state",
        "normalization layers",
        "weight-decay scaling rule",
        "AdamW",
        "singular-value spectrum",
        "sublayer gain",
        "LLaMA-style Transformers"
      ],
      "organization": {
        "_id": "63728bde14d543d507ae970d",
        "name": "MIT",
        "fullname": "Massachusetts Institute of Technology",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"
      }
    },
    "publishedAt": "2025-10-16T22:58:35.000Z",
    "title": "Robust Layerwise Scaling Rules by Proper Weight Decay Tuning",
    "summary": "Empirical scaling laws prescribe how to allocate parameters, data, and\ncompute, while maximal-update parameterization (muP) enables learning-rate\ntransfer across widths by equalizing early-time update magnitudes. However, in\nmodern scale-invariant architectures, training quickly enters an\noptimizer-governed steady state where normalization layers create backward\nscale sensitivity and the effective learning rate becomes width dependent,\ndegrading muP transfer. We address this by introducing a weight-decay\nscaling rule for AdamW that preserves sublayer gain across widths. Empirically,\nthe singular-value spectrum of each matrix parameter scales in norm as\neta/lambda with an approximately invariant shape; under width\nscaling d, we observe that the top singular value scales approximately as\neta/lambdacdot d^{0.75}. Combining this observation with the muP\nlearning-rate rule eta_2propto d^{-1} for matrix-like parameters implies an\nempirical weight-decay scaling rule lambda_2propto d that\napproximately keeps sublayer gains width invariant. Together with vector-like\nparameters trained at eta_1=Theta_d(1) and lambda_1=0, this yields\nzero-shot transfer of both learning rate and weight decay from proxy to\ntarget widths, removing per-width sweeps. We validate the rule on LLaMA-style\nTransformers and in a minimal synthetic setting, and we provide a simple\ndiagnostic, matching top singular values, to check sublayer-gain invariance.\nOur results extend muP beyond the near-init regime by explicitly controlling\nsteady-state scales set by the optimizer, offering a practical recipe for\nwidth-robust hyperparameter transfer under AdamW.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.15262.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "653d276681f52ceb4d12bd85",
      "avatarUrl": "/avatars/56601a25e5f883a8f6dc15f6fd9dcc57.svg",
      "fullname": "Yifeng Liu",
      "name": "Lewis-Lau",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "organization": {
      "_id": "63728bde14d543d507ae970d",
      "name": "MIT",
      "fullname": "Massachusetts Institute of Technology",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.15842",
      "authors": [
        {
          "_id": "68f595bf8589920bf4d320fa",
          "name": "Yuhang Chen",
          "hidden": false
        },
        {
          "_id": "68f595bf8589920bf4d320fb",
          "name": "Tianpeng Lv",
          "hidden": false
        },
        {
          "_id": "68f595bf8589920bf4d320fc",
          "name": "Siyi Zhang",
          "hidden": false
        },
        {
          "_id": "68f595bf8589920bf4d320fd",
          "name": "Yixiang Yin",
          "hidden": false
        },
        {
          "_id": "68f595bf8589920bf4d320fe",
          "name": "Yao Wan",
          "hidden": false
        },
        {
          "_id": "68f595bf8589920bf4d320ff",
          "name": "Philip S. Yu",
          "hidden": false
        },
        {
          "_id": "68f595bf8589920bf4d32100",
          "name": "Dongping Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-17T17:35:58.000Z",
      "submittedOnDailyAt": "2025-10-20T00:22:22.687Z",
      "title": "Paper2Web: Let's Make Your Paper Alive!",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Academic project websites can more effectively disseminate research when they\nclearly present core content and enable intuitive navigation and interaction.\nHowever, current approaches such as direct Large Language Model (LLM)\ngeneration, templates, or direct HTML conversion struggle to produce\nlayout-aware, interactive sites, and a comprehensive evaluation suite for this\ntask has been lacking. In this paper, we introduce Paper2Web, a benchmark\ndataset and multi-dimensional evaluation framework for assessing academic\nwebpage generation. It incorporates rule-based metrics like Connectivity,\nCompleteness and human-verified LLM-as-a-Judge (covering interactivity,\naesthetics, and informativeness), and PaperQuiz, which measures paper-level\nknowledge retention. We further present PWAgent, an autonomous pipeline that\nconverts scientific papers into interactive and multimedia-rich academic\nhomepages. The agent iteratively refines both content and layout through MCP\ntools that enhance emphasis, balance, and presentation quality. Our experiments\nshow that PWAgent consistently outperforms end-to-end baselines like\ntemplate-based webpages and arXiv/alphaXiv versions by a large margin while\nmaintaining low cost, achieving the Pareto-front in academic webpage\ngeneration.",
      "upvotes": 1,
      "discussionId": "68f595bf8589920bf4d32101",
      "projectPage": "https://francischen3.github.io/P2W_Website/",
      "ai_summary": "Paper2Web is a benchmark and evaluation framework for academic webpage generation, featuring PWAgent, an autonomous pipeline that enhances content and layout through MCP tools, outperforming end-to-end baselines.",
      "ai_keywords": [
        "Large Language Model (LLM)",
        "Paper2Web",
        "benchmark dataset",
        "evaluation framework",
        "Connectivity",
        "Completeness",
        "LLM-as-a-Judge",
        "PaperQuiz",
        "PWAgent",
        "MCP tools",
        "academic webpage generation",
        "template-based webpages",
        "arXiv/alphaXiv versions",
        "Pareto-front"
      ]
    },
    "publishedAt": "2025-10-17T13:35:58.000Z",
    "title": "Paper2Web: Let's Make Your Paper Alive!",
    "summary": "Academic project websites can more effectively disseminate research when they\nclearly present core content and enable intuitive navigation and interaction.\nHowever, current approaches such as direct Large Language Model (LLM)\ngeneration, templates, or direct HTML conversion struggle to produce\nlayout-aware, interactive sites, and a comprehensive evaluation suite for this\ntask has been lacking. In this paper, we introduce Paper2Web, a benchmark\ndataset and multi-dimensional evaluation framework for assessing academic\nwebpage generation. It incorporates rule-based metrics like Connectivity,\nCompleteness and human-verified LLM-as-a-Judge (covering interactivity,\naesthetics, and informativeness), and PaperQuiz, which measures paper-level\nknowledge retention. We further present PWAgent, an autonomous pipeline that\nconverts scientific papers into interactive and multimedia-rich academic\nhomepages. The agent iteratively refines both content and layout through MCP\ntools that enhance emphasis, balance, and presentation quality. Our experiments\nshow that PWAgent consistently outperforms end-to-end baselines like\ntemplate-based webpages and arXiv/alphaXiv versions by a large margin while\nmaintaining low cost, achieving the Pareto-front in academic webpage\ngeneration.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.15842.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 131
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.15162",
      "authors": [
        {
          "_id": "68f5b0a68589920bf4d3217d",
          "name": "Weizhi Wang",
          "hidden": false
        },
        {
          "_id": "68f5b0a68589920bf4d3217e",
          "name": "Rongmei Lin",
          "hidden": false
        },
        {
          "_id": "68f5b0a68589920bf4d3217f",
          "name": "Shiyang Li",
          "hidden": false
        },
        {
          "_id": "68f5b0a68589920bf4d32180",
          "name": "Colin Lockard",
          "hidden": false
        },
        {
          "_id": "68f5b0a68589920bf4d32181",
          "name": "Ritesh Sarkhel",
          "hidden": false
        },
        {
          "_id": "68f5b0a68589920bf4d32182",
          "name": "Sanket Lokegaonkar",
          "hidden": false
        },
        {
          "_id": "68f5b0a68589920bf4d32183",
          "name": "Jingbo Shang",
          "hidden": false
        },
        {
          "_id": "68f5b0a68589920bf4d32184",
          "name": "Xifeng Yan",
          "hidden": false
        },
        {
          "_id": "68f5b0a68589920bf4d32185",
          "name": "Nasser Zalmout",
          "hidden": false
        },
        {
          "_id": "68f5b0a68589920bf4d32186",
          "name": "Xian Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-16T21:53:28.000Z",
      "submittedOnDailyAt": "2025-10-20T02:17:24.118Z",
      "title": "Train a Unified Multimodal Data Quality Classifier with Synthetic Data",
      "submittedOnDailyBy": {
        "_id": "63d34004b734eaa4d4faeccf",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63d34004b734eaa4d4faeccf/zf6d1p0GN8gsagi8N6y4V.jpeg",
        "isPro": false,
        "fullname": "Weizhi Wang",
        "user": "weizhiwang",
        "type": "user"
      },
      "summary": "The Multimodal Large Language Models (MLLMs) are continually pre-trained on a\nmixture of image-text caption data and interleaved document data, while the\nhigh-quality data filtering towards image-text interleaved document data is\nunder-explored. We propose to train an efficient MLLM as a Unified Mulitmodal\nData Quality Classifier to Filter both high-quality image-text caption and\ninterleaved data (UniFilter). To address the challenge of collecting diverse\nlabeled multimodal data, we introduce a semi-synthetic approach that leverages\nreadily available raw images and generates corresponding text across four\nquality levels. This method enables efficient creation of sample-score pairs\nfor both caption and interleaved document data to train UniFilter. We apply\nUniFilter to curate high-quality caption data from DataComp caption dataset and\ninterleaved data from the OBELICS image-text interleaved dataset. MLLMs\npre-trained on the filtered data demonstrate significantly enhanced\ncapabilities compared to those trained on baseline-filtered data, achieving\nstronger zero-shot reasoning and in-context learning capabilities. After visual\nsupervised fine-tuning, these UniFilter-induced MLLMs achieve stronger\nperformance on various benchmarks, highlighting the downstream benefits of\nhigh-quality multimodal pre-training. We release the synthetic training data\nused for training UniFilter, the UniFilter model checkpoints, and the\nhigh-quality interleaved document subset OBELICS-HQ, curated by UniFilter, to\nthe community for reproduction and further development.",
      "upvotes": 1,
      "discussionId": "68f5b0a78589920bf4d32187",
      "ai_summary": "UniFilter, a Unified Multimodal Data Quality Classifier, enhances Multimodal Large Language Models (MLLMs) by filtering high-quality image-text and interleaved data, leading to improved zero-shot reasoning and in-context learning.",
      "ai_keywords": [
        "Multimodal Large Language Models",
        "MLLMs",
        "Unified Multimodal Data Quality Classifier",
        "UniFilter",
        "semi-synthetic approach",
        "zero-shot reasoning",
        "in-context learning",
        "visual supervised fine-tuning",
        "DataComp caption dataset",
        "OBELICS image-text interleaved dataset",
        "OBELICS-HQ"
      ]
    },
    "publishedAt": "2025-10-16T17:53:28.000Z",
    "title": "Train a Unified Multimodal Data Quality Classifier with Synthetic Data",
    "summary": "The Multimodal Large Language Models (MLLMs) are continually pre-trained on a\nmixture of image-text caption data and interleaved document data, while the\nhigh-quality data filtering towards image-text interleaved document data is\nunder-explored. We propose to train an efficient MLLM as a Unified Mulitmodal\nData Quality Classifier to Filter both high-quality image-text caption and\ninterleaved data (UniFilter). To address the challenge of collecting diverse\nlabeled multimodal data, we introduce a semi-synthetic approach that leverages\nreadily available raw images and generates corresponding text across four\nquality levels. This method enables efficient creation of sample-score pairs\nfor both caption and interleaved document data to train UniFilter. We apply\nUniFilter to curate high-quality caption data from DataComp caption dataset and\ninterleaved data from the OBELICS image-text interleaved dataset. MLLMs\npre-trained on the filtered data demonstrate significantly enhanced\ncapabilities compared to those trained on baseline-filtered data, achieving\nstronger zero-shot reasoning and in-context learning capabilities. After visual\nsupervised fine-tuning, these UniFilter-induced MLLMs achieve stronger\nperformance on various benchmarks, highlighting the downstream benefits of\nhigh-quality multimodal pre-training. We release the synthetic training data\nused for training UniFilter, the UniFilter model checkpoints, and the\nhigh-quality interleaved document subset OBELICS-HQ, curated by UniFilter, to\nthe community for reproduction and further development.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.15162.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63d34004b734eaa4d4faeccf",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63d34004b734eaa4d4faeccf/zf6d1p0GN8gsagi8N6y4V.jpeg",
      "fullname": "Weizhi Wang",
      "name": "weizhiwang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 19
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.12766",
      "authors": [
        {
          "_id": "68f5d9e88589920bf4d3220b",
          "name": "ukasz Borchmann",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-14T17:45:31.000Z",
      "submittedOnDailyAt": "2025-10-20T05:16:48.610Z",
      "title": "Language Models Model Language",
      "submittedOnDailyBy": {
        "_id": "600b381d3cc3b87db94bc0ce",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/600b381d3cc3b87db94bc0ce/I3xpr4gzcG1uXawXBpWpD.jpeg",
        "isPro": false,
        "fullname": "ukasz Borchmann",
        "user": "Borchmann",
        "type": "user"
      },
      "summary": "Linguistic commentary on LLMs, heavily influenced by the theoretical\nframeworks of de Saussure and Chomsky, is often speculative and unproductive.\nCritics challenge whether LLMs can legitimately model language, citing the need\nfor \"deep structure\" or \"grounding\" to achieve an idealized linguistic\n\"competence.\" We argue for a radical shift in perspective towards the\nempiricist principles of Witold Ma\\'nczak, a prominent general and historical\nlinguist. He defines language not as a \"system of signs\" or a \"computational\nsystem of the brain\" but as the totality of all that is said and written. Above\nall, he identifies frequency of use of particular language elements as\nlanguage's primary governing principle. Using his framework, we challenge prior\ncritiques of LLMs and provide a constructive guide for designing, evaluating,\nand interpreting language models.",
      "upvotes": 1,
      "discussionId": "68f5d9e88589920bf4d3220c",
      "ai_summary": "The paper advocates for an empiricist approach to evaluating language models, emphasizing frequency of use over traditional theoretical frameworks.",
      "ai_keywords": [
        ""
      ],
      "organization": {
        "_id": "62cece4aa3a23014aca72499",
        "name": "Snowflake",
        "fullname": "Snowflake",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64dc52cf858f8a41c12fc819/O9-MWzRjWzbNP_DQlMb-7.png"
      }
    },
    "publishedAt": "2025-10-14T13:45:31.000Z",
    "title": "Language Models Model Language",
    "summary": "Linguistic commentary on LLMs, heavily influenced by the theoretical\nframeworks of de Saussure and Chomsky, is often speculative and unproductive.\nCritics challenge whether LLMs can legitimately model language, citing the need\nfor \"deep structure\" or \"grounding\" to achieve an idealized linguistic\n\"competence.\" We argue for a radical shift in perspective towards the\nempiricist principles of Witold Ma\\'nczak, a prominent general and historical\nlinguist. He defines language not as a \"system of signs\" or a \"computational\nsystem of the brain\" but as the totality of all that is said and written. Above\nall, he identifies frequency of use of particular language elements as\nlanguage's primary governing principle. Using his framework, we challenge prior\ncritiques of LLMs and provide a constructive guide for designing, evaluating,\nand interpreting language models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.12766.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "600b381d3cc3b87db94bc0ce",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/600b381d3cc3b87db94bc0ce/I3xpr4gzcG1uXawXBpWpD.jpeg",
      "fullname": "ukasz Borchmann",
      "name": "Borchmann",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "organization": {
      "_id": "62cece4aa3a23014aca72499",
      "name": "Snowflake",
      "fullname": "Snowflake",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64dc52cf858f8a41c12fc819/O9-MWzRjWzbNP_DQlMb-7.png"
    },
    "isAuthorParticipating": false
  }
]