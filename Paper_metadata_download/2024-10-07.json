[
    {
        "paper": {
            "id": "2410.00907",
            "authors": [
                {
                    "_id": "66fc9efa6d3e4a039730c68c",
                    "user": {
                        "_id": "62bc495016897fa76cdcda18",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656506661693-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Hongyin Luo",
                        "user": "luohy",
                        "type": "user"
                    },
                    "name": "Hongyin Luo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-03T09:38:09.909Z",
                    "hidden": false
                },
                {
                    "_id": "66fc9efa6d3e4a039730c68d",
                    "name": "Wei Sun",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-01T17:53:28.000Z",
            "title": "Addition is All You Need for Energy-efficient Language Models",
            "summary": "Large neural networks spend most computation on floating point tensor\nmultiplications. In this work, we find that a floating point multiplier can be\napproximated by one integer adder with high precision. We propose the\nlinear-complexity multiplication L-Mul algorithm that approximates floating\npoint number multiplication with integer addition operations. The new algorithm\ncosts significantly less computation resource than 8-bit floating point\nmultiplication but achieves higher precision. Compared to 8-bit floating point\nmultiplications, the proposed method achieves higher precision but consumes\nsignificantly less bit-level computation. Since multiplying floating point\nnumbers requires substantially higher energy compared to integer addition\noperations, applying the L-Mul operation in tensor processing hardware can\npotentially reduce 95% energy cost by element-wise floating point tensor\nmultiplications and 80% energy cost of dot products. We calculated the\ntheoretical error expectation of L-Mul, and evaluated the algorithm on a wide\nrange of textual, visual, and symbolic tasks, including natural language\nunderstanding, structural reasoning, mathematics, and commonsense question\nanswering. Our numerical analysis experiments agree with the theoretical error\nestimation, which indicates that L-Mul with 4-bit mantissa achieves comparable\nprecision as float8_e4m3 multiplications, and L-Mul with 3-bit mantissa\noutperforms float8_e5m2. Evaluation results on popular benchmarks show that\ndirectly applying L-Mul to the attention mechanism is almost lossless. We\nfurther show that replacing all floating point multiplications with 3-bit\nmantissa L-Mul in a transformer model achieves equivalent precision as using\nfloat8_e4m3 as accumulation precision in both fine-tuning and inference.",
            "upvotes": 76,
            "discussionId": "66fc9efb6d3e4a039730c6c9"
        },
        "publishedAt": "2024-10-07T03:23:44.434Z",
        "title": "Addition is All You Need for Energy-efficient Language Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.00907.png",
        "numComments": 9,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656506661693-noauth.jpeg",
            "fullname": "Hongyin Luo",
            "name": "luohy",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.02613",
            "authors": [
                {
                    "_id": "67024ddf8572eb2d80f01e3a",
                    "user": {
                        "_id": "623f4bd2e801e8c1e59d948e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1648315730065-623f4bd2e801e8c1e59d948e.jpeg",
                        "isPro": false,
                        "fullname": "Mor Ventura",
                        "user": "MorVentura",
                        "type": "user"
                    },
                    "name": "Mor Ventura",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-07T07:26:50.210Z",
                    "hidden": false
                },
                {
                    "_id": "67024ddf8572eb2d80f01e3b",
                    "user": {
                        "_id": "61c865e3d3702a3bbf50bc04",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61c865e3d3702a3bbf50bc04/eGgcyeUnOHz0hyap5vhvr.jpeg",
                        "isPro": false,
                        "fullname": "Michael Toker",
                        "user": "tokeron",
                        "type": "user"
                    },
                    "name": "Michael Toker",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-07T07:26:48.325Z",
                    "hidden": false
                },
                {
                    "_id": "67024ddf8572eb2d80f01e3c",
                    "user": {
                        "_id": "62d6a0c18faee0ac953c51fa",
                        "avatarUrl": "/avatars/ca818cebdb089a8d853c5bc4d5e0987b.svg",
                        "isPro": false,
                        "fullname": "Nitay Calderon",
                        "user": "nitay",
                        "type": "user"
                    },
                    "name": "Nitay Calderon",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-07T08:44:00.870Z",
                    "hidden": false
                },
                {
                    "_id": "67024ddf8572eb2d80f01e3d",
                    "name": "Zorik Gekhman",
                    "hidden": false
                },
                {
                    "_id": "67024ddf8572eb2d80f01e3e",
                    "user": {
                        "_id": "632e0771ae0a7b1fc95630bf",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1663961181981-632e0771ae0a7b1fc95630bf.jpeg",
                        "isPro": false,
                        "fullname": "Yonatan",
                        "user": "yonatanbitton",
                        "type": "user"
                    },
                    "name": "Yonatan Bitton",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-07T08:42:35.195Z",
                    "hidden": false
                },
                {
                    "_id": "67024ddf8572eb2d80f01e3f",
                    "name": "Roi Reichart",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-03T15:51:36.000Z",
            "title": "NL-Eye: Abductive NLI for Images",
            "summary": "Will a Visual Language Model (VLM)-based bot warn us about slipping if it\ndetects a wet floor? Recent VLMs have demonstrated impressive capabilities, yet\ntheir ability to infer outcomes and causes remains underexplored. To address\nthis, we introduce NL-Eye, a benchmark designed to assess VLMs' visual\nabductive reasoning skills. NL-Eye adapts the abductive Natural Language\nInference (NLI) task to the visual domain, requiring models to evaluate the\nplausibility of hypothesis images based on a premise image and explain their\ndecisions. NL-Eye consists of 350 carefully curated triplet examples (1,050\nimages) spanning diverse reasoning categories: physical, functional, logical,\nemotional, cultural, and social. The data curation process involved two steps -\nwriting textual descriptions and generating images using text-to-image models,\nboth requiring substantial human involvement to ensure high-quality and\nchallenging scenes. Our experiments show that VLMs struggle significantly on\nNL-Eye, often performing at random baseline levels, while humans excel in both\nplausibility prediction and explanation quality. This demonstrates a deficiency\nin the abductive reasoning capabilities of modern VLMs. NL-Eye represents a\ncrucial step toward developing VLMs capable of robust multimodal reasoning for\nreal-world applications, including accident-prevention bots and generated video\nverification.",
            "upvotes": 13,
            "discussionId": "67024de58572eb2d80f01fff"
        },
        "publishedAt": "2024-10-07T06:59:53.460Z",
        "title": "NL-Eye: Abductive NLI for Images",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.02613.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1648315730065-623f4bd2e801e8c1e59d948e.jpeg",
            "fullname": "Mor Ventura",
            "name": "MorVentura",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.02703",
            "authors": [
                {
                    "_id": "6700c824c67c24aeada0b186",
                    "user": {
                        "_id": "6394a2244db1a9bedd1f2229",
                        "avatarUrl": "/avatars/f04496811eb6873781e2ba606a2f97f0.svg",
                        "isPro": false,
                        "fullname": "Yaniv Leviathan",
                        "user": "yanivle",
                        "type": "user"
                    },
                    "name": "Yaniv Leviathan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-07T08:39:16.581Z",
                    "hidden": false
                },
                {
                    "_id": "6700c824c67c24aeada0b187",
                    "user": {
                        "_id": "630414f64ec2dfa82a588fb8",
                        "avatarUrl": "/avatars/a2c952fceacb2eab4484cd7f8015e931.svg",
                        "isPro": false,
                        "fullname": "Matan Kalman",
                        "user": "matank",
                        "type": "user"
                    },
                    "name": "Matan Kalman",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-07T15:10:22.649Z",
                    "hidden": false
                },
                {
                    "_id": "6700c824c67c24aeada0b188",
                    "name": "Yossi Matias",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-03T17:27:30.000Z",
            "title": "Selective Attention Improves Transformer",
            "summary": "Unneeded elements in the attention's context degrade performance. We\nintroduce Selective Attention, a simple parameter-free change to the standard\nattention mechanism which reduces attention to unneeded elements. Selective\nattention improves language modeling performance in a variety of model sizes\nand context lengths. For example, a range of transformers trained with the\nlanguage modeling objective on C4 with selective attention perform equivalently\nto standard transformers with ~2X more heads and parameters in their attention\nmodules. Selective attention also allows decreasing the size of the attention's\ncontext buffer, leading to meaningful reductions in the memory and compute\nrequirements during inference. For example, transformers with 100M parameters\ntrained on C4 with context sizes of 512, 1,024, and 2,048 need 16X, 25X, and\n47X less memory for their attention module, respectively, when equipped with\nselective attention, as those without selective attention, with the same\nvalidation perplexity.",
            "upvotes": 12,
            "discussionId": "6700c827c67c24aeada0b1f8"
        },
        "publishedAt": "2024-10-07T02:26:31.762Z",
        "title": "Selective Attention Improves Transformer",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.02703.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d98ef7a4839890b25eb78b/215-CSVLl81z6CAq0ECWU.jpeg",
            "fullname": "Fangyuan Yu",
            "name": "Ksgk-fy",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.01699",
            "authors": [
                {
                    "_id": "67020787400ad7197b4b4a5f",
                    "user": {
                        "_id": "6427e08288215cee63b1c44d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6427e08288215cee63b1c44d/rzaG978FF-ywzicWNl_xl.jpeg",
                        "isPro": false,
                        "fullname": "yao teng",
                        "user": "tytyt",
                        "type": "user"
                    },
                    "name": "Yao Teng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-07T07:26:55.869Z",
                    "hidden": false
                },
                {
                    "_id": "67020787400ad7197b4b4a60",
                    "name": "Han Shi",
                    "hidden": false
                },
                {
                    "_id": "67020787400ad7197b4b4a61",
                    "name": "Xian Liu",
                    "hidden": false
                },
                {
                    "_id": "67020787400ad7197b4b4a62",
                    "name": "Xuefei Ning",
                    "hidden": false
                },
                {
                    "_id": "67020787400ad7197b4b4a63",
                    "name": "Guohao Dai",
                    "hidden": false
                },
                {
                    "_id": "67020787400ad7197b4b4a64",
                    "name": "Yu Wang",
                    "hidden": false
                },
                {
                    "_id": "67020787400ad7197b4b4a65",
                    "name": "Zhenguo Li",
                    "hidden": false
                },
                {
                    "_id": "67020787400ad7197b4b4a66",
                    "name": "Xihui Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-02T16:05:27.000Z",
            "title": "Accelerating Auto-regressive Text-to-Image Generation with Training-free\n  Speculative Jacobi Decoding",
            "summary": "The current large auto-regressive models can generate high-quality,\nhigh-resolution images, but these models require hundreds or even thousands of\nsteps of next-token prediction during inference, resulting in substantial time\nconsumption. In existing studies, Jacobi decoding, an iterative parallel\ndecoding algorithm, has been used to accelerate the auto-regressive generation\nand can be executed without training. However, the Jacobi decoding relies on a\ndeterministic criterion to determine the convergence of iterations. Thus, it\nworks for greedy decoding but is incompatible with sampling-based decoding\nwhich is crucial for visual quality and diversity in the current\nauto-regressive text-to-image generation. In this paper, we propose a\ntraining-free probabilistic parallel decoding algorithm, Speculative Jacobi\nDecoding (SJD), to accelerate auto-regressive text-to-image generation. By\nintroducing a probabilistic convergence criterion, our SJD accelerates the\ninference of auto-regressive text-to-image generation while maintaining the\nrandomness in sampling-based token decoding and allowing the model to generate\ndiverse images. Specifically, SJD facilitates the model to predict multiple\ntokens at each step and accepts tokens based on the probabilistic criterion,\nenabling the model to generate images with fewer steps than the conventional\nnext-token-prediction paradigm. We also investigate the token initialization\nstrategies that leverage the spatial locality of visual data to further improve\nthe acceleration ratio under specific scenarios. We conduct experiments for our\nproposed SJD on multiple auto-regressive text-to-image generation models,\nshowing the effectiveness of model acceleration without sacrificing the visual\nquality.",
            "upvotes": 10,
            "discussionId": "67020788400ad7197b4b4af5"
        },
        "publishedAt": "2024-10-07T13:20:54.329Z",
        "title": "Accelerating Auto-regressive Text-to-Image Generation with Training-free Speculative Jacobi Decoding",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.01699.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6427e08288215cee63b1c44d/rzaG978FF-ywzicWNl_xl.jpeg",
            "fullname": "yao teng",
            "name": "tytyt",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.03017",
            "authors": [
                {
                    "_id": "6703648177f7e874a2a560be",
                    "user": {
                        "_id": "651e391852cd39f995249099",
                        "avatarUrl": "/avatars/59d295863c13f90cb345d5cac303c266.svg",
                        "isPro": false,
                        "fullname": "Rose E Wang",
                        "user": "rose-e-wang",
                        "type": "user"
                    },
                    "name": "Rose E. Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-07T07:26:30.010Z",
                    "hidden": false
                },
                {
                    "_id": "6703648177f7e874a2a560bf",
                    "user": {
                        "_id": "639234b5c1899a08e56a7d2a",
                        "avatarUrl": "/avatars/34829332a4b10cafab77e396e3c57752.svg",
                        "isPro": false,
                        "fullname": "Ana Ribeiro",
                        "user": "Analu",
                        "type": "user"
                    },
                    "name": "Ana T. Ribeiro",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-07T08:56:43.863Z",
                    "hidden": false
                },
                {
                    "_id": "6703648177f7e874a2a560c0",
                    "user": {
                        "_id": "660c6f8b0c47aba6600c247e",
                        "avatarUrl": "/avatars/7f79e19f6c8186cdb40785140cd42578.svg",
                        "isPro": false,
                        "fullname": "Carly Robinson",
                        "user": "carlycodes",
                        "type": "user"
                    },
                    "name": "Carly D. Robinson",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-07T08:56:52.843Z",
                    "hidden": false
                },
                {
                    "_id": "6703648177f7e874a2a560c1",
                    "name": "Susanna Loeb",
                    "hidden": false
                },
                {
                    "_id": "6703648177f7e874a2a560c2",
                    "user": {
                        "_id": "606e3a4c7f3150d181025dd1",
                        "avatarUrl": "/avatars/224ee859c1df3f6c4ab59910f2a4732b.svg",
                        "isPro": false,
                        "fullname": "Dora Demszky",
                        "user": "ddemszky",
                        "type": "user"
                    },
                    "name": "Dora Demszky",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-07T08:57:02.827Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-03T21:58:39.000Z",
            "title": "Tutor CoPilot: A Human-AI Approach for Scaling Real-Time Expertise",
            "summary": "Generative AI, particularly Language Models (LMs), has the potential to\ntransform real-world domains with societal impact, particularly where access to\nexperts is limited. For example, in education, training novice educators with\nexpert guidance is important for effectiveness but expensive, creating\nsignificant barriers to improving education quality at scale. This challenge\ndisproportionately harms students from under-served communities, who stand to\ngain the most from high-quality education. We introduce Tutor CoPilot, a novel\nHuman-AI approach that leverages a model of expert thinking to provide\nexpert-like guidance to tutors as they tutor. This study is the first\nrandomized controlled trial of a Human-AI system in live tutoring, involving\n900 tutors and 1,800 K-12 students from historically under-served communities.\nFollowing a preregistered analysis plan, we find that students working with\ntutors that have access to Tutor CoPilot are 4 percentage points (p.p.) more\nlikely to master topics (p<0.01). Notably, students of lower-rated tutors\nexperienced the greatest benefit, improving mastery by 9 p.p. We find that\nTutor CoPilot costs only $20 per-tutor annually. We analyze 550,000+ messages\nusing classifiers to identify pedagogical strategies, and find that tutors with\naccess to Tutor CoPilot are more likely to use high-quality strategies to\nfoster student understanding (e.g., asking guiding questions) and less likely\nto give away the answer to the student. Tutor interviews highlight how Tutor\nCoPilot's guidance helps tutors to respond to student needs, though they flag\nissues in Tutor CoPilot, such as generating suggestions that are not\ngrade-level appropriate. Altogether, our study of Tutor CoPilot demonstrates\nhow Human-AI systems can scale expertise in real-world domains, bridge gaps in\nskills and create a future where high-quality education is accessible to all\nstudents.",
            "upvotes": 9,
            "discussionId": "6703648277f7e874a2a56165"
        },
        "publishedAt": "2024-10-07T03:03:15.596Z",
        "title": "Tutor CoPilot: A Human-AI Approach for Scaling Real-Time Expertise",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.03017.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.02362",
            "authors": [
                {
                    "_id": "67019c2b223c62ec88ce0b10",
                    "user": {
                        "_id": "6595160e5b7553ca5c40d756",
                        "avatarUrl": "/avatars/443540be7bc7a2ff82e36ceea7e18388.svg",
                        "isPro": false,
                        "fullname": "Shubhi Bansal",
                        "user": "shubhii0712",
                        "type": "user"
                    },
                    "name": "Shubhi Bansal",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-07T08:49:46.045Z",
                    "hidden": false
                },
                {
                    "_id": "67019c2b223c62ec88ce0b11",
                    "user": {
                        "_id": "65f0d7fe6dc445ce852c7df2",
                        "avatarUrl": "/avatars/4b909819fb6fff94b81ebeef869afbeb.svg",
                        "isPro": false,
                        "fullname": "SREE HARISH A",
                        "user": "HARI45SH",
                        "type": "user"
                    },
                    "name": "Sreeharish A",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-07T07:27:02.279Z",
                    "hidden": false
                },
                {
                    "_id": "67019c2b223c62ec88ce0b12",
                    "user": {
                        "_id": "649c0ce8b4ee3b2664590a40",
                        "avatarUrl": "/avatars/090c1f57c29af918e100db0844f60892.svg",
                        "isPro": false,
                        "fullname": "Madhava prasath",
                        "user": "Madddy",
                        "type": "user"
                    },
                    "name": "Madhava Prasath J",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-07T07:27:03.893Z",
                    "hidden": false
                },
                {
                    "_id": "67019c2b223c62ec88ce0b13",
                    "user": {
                        "_id": "6637b2085eedec13fcb1024e",
                        "avatarUrl": "/avatars/9583c5ba5c8b9a920f524f749e3659ed.svg",
                        "isPro": false,
                        "fullname": "Manikandan S",
                        "user": "Man1kandan",
                        "type": "user"
                    },
                    "name": "Manikandan S",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-07T07:27:00.208Z",
                    "hidden": false
                },
                {
                    "_id": "67019c2b223c62ec88ce0b14",
                    "name": "Sreekanth Madisetty",
                    "hidden": false
                },
                {
                    "_id": "67019c2b223c62ec88ce0b15",
                    "name": "Mohammad Zia Ur Rehman",
                    "hidden": false
                },
                {
                    "_id": "67019c2b223c62ec88ce0b16",
                    "name": "Chandravardhan Singh Raghaw",
                    "hidden": false
                },
                {
                    "_id": "67019c2b223c62ec88ce0b17",
                    "user": {
                        "_id": "63d17f2ef7f31a66a2d284d0",
                        "avatarUrl": "/avatars/708148a57aacbb5a7d228d4da650a9e6.svg",
                        "isPro": false,
                        "fullname": "Gaurav Duggal",
                        "user": "gduggal",
                        "type": "user"
                    },
                    "name": "Gaurav Duggal",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-07T08:50:02.439Z",
                    "hidden": false
                },
                {
                    "_id": "67019c2b223c62ec88ce0b18",
                    "user": {
                        "_id": "6703a39390e1fe6f74fac141",
                        "avatarUrl": "/avatars/cec66dea312d27fdf43e3ebff0b58b83.svg",
                        "isPro": false,
                        "fullname": "Nagendra Kumar",
                        "user": "nagendra-cse",
                        "type": "user"
                    },
                    "name": "Nagendra Kumar",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-07T09:16:59.463Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-03T10:23:03.000Z",
            "title": "A Comprehensive Survey of Mamba Architectures for Medical Image\n  Analysis: Classification, Segmentation, Restoration and Beyond",
            "summary": "Mamba, a special case of the State Space Model, is gaining popularity as an\nalternative to template-based deep learning approaches in medical image\nanalysis. While transformers are powerful architectures, they have drawbacks,\nincluding quadratic computational complexity and an inability to address\nlong-range dependencies efficiently. This limitation affects the analysis of\nlarge and complex datasets in medical imaging, where there are many spatial and\ntemporal relationships. In contrast, Mamba offers benefits that make it\nwell-suited for medical image analysis. It has linear time complexity, which is\na significant improvement over transformers. Mamba processes longer sequences\nwithout attention mechanisms, enabling faster inference and requiring less\nmemory. Mamba also demonstrates strong performance in merging multimodal data,\nimproving diagnosis accuracy and patient outcomes. The organization of this\npaper allows readers to appreciate the capabilities of Mamba in medical imaging\nstep by step. We begin by defining core concepts of SSMs and models, including\nS4, S5, and S6, followed by an exploration of Mamba architectures such as pure\nMamba, U-Net variants, and hybrid models with convolutional neural networks,\ntransformers, and Graph Neural Networks. We also cover Mamba optimizations,\ntechniques and adaptations, scanning, datasets, applications, experimental\nresults, and conclude with its challenges and future directions in medical\nimaging. This review aims to demonstrate the transformative potential of Mamba\nin overcoming existing barriers within medical imaging while paving the way for\ninnovative advancements in the field. A comprehensive list of Mamba\narchitectures applied in the medical field, reviewed in this work, is available\nat Github.",
            "upvotes": 7,
            "discussionId": "67019c2c223c62ec88ce0b83"
        },
        "publishedAt": "2024-10-07T06:23:18.713Z",
        "title": "A Comprehensive Survey of Mamba Architectures for Medical Image Analysis: Classification, Segmentation, Restoration and Beyond",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.02362.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "/avatars/090c1f57c29af918e100db0844f60892.svg",
            "fullname": "Madhava prasath",
            "name": "Madddy",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2409.19989",
            "authors": [
                {
                    "_id": "67022b78ae1235c08ba4f564",
                    "user": {
                        "_id": "634c1f9bb6628cbe2861dcc2",
                        "avatarUrl": "/avatars/dd48dff0b639123c605b5c0ee10577d7.svg",
                        "isPro": false,
                        "fullname": "Jangyeong.Kim",
                        "user": "longshiine",
                        "type": "user"
                    },
                    "name": "Jangyeong Kim",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-07T07:26:52.030Z",
                    "hidden": true
                },
                {
                    "_id": "67022b78ae1235c08ba4f565",
                    "user": {
                        "_id": "62e772d0f97481d0a13f5766",
                        "avatarUrl": "/avatars/69415574d0cbf0887a151519ee579669.svg",
                        "isPro": false,
                        "fullname": "Donggoo Kang",
                        "user": "DK9",
                        "type": "user"
                    },
                    "name": "Donggoo Kang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-07T09:16:52.614Z",
                    "hidden": false
                },
                {
                    "_id": "67022b78ae1235c08ba4f566",
                    "name": "Junyoung Choi",
                    "hidden": false
                },
                {
                    "_id": "67022b78ae1235c08ba4f567",
                    "name": "Jeonga Wi",
                    "hidden": false
                },
                {
                    "_id": "67022b78ae1235c08ba4f568",
                    "name": "Junho Gwon",
                    "hidden": false
                },
                {
                    "_id": "67022b78ae1235c08ba4f569",
                    "user": {
                        "_id": "6304292d5c70c21d0eadf328",
                        "avatarUrl": "/avatars/a5e312922ba5026f3f223de7c65f0c3c.svg",
                        "isPro": false,
                        "fullname": "Jiun Bae",
                        "user": "Jiun",
                        "type": "user"
                    },
                    "name": "Jiun Bae",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-07T09:17:12.639Z",
                    "hidden": false
                },
                {
                    "_id": "67022b78ae1235c08ba4f56a",
                    "name": "Dumim Yoon",
                    "hidden": false
                },
                {
                    "_id": "67022b78ae1235c08ba4f56b",
                    "name": "Junghyun Han",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-09-30T06:29:50.000Z",
            "title": "RoCoTex: A Robust Method for Consistent Texture Synthesis with Diffusion\n  Models",
            "summary": "Text-to-texture generation has recently attracted increasing attention, but\nexisting methods often suffer from the problems of view inconsistencies,\napparent seams, and misalignment between textures and the underlying mesh. In\nthis paper, we propose a robust text-to-texture method for generating\nconsistent and seamless textures that are well aligned with the mesh. Our\nmethod leverages state-of-the-art 2D diffusion models, including SDXL and\nmultiple ControlNets, to capture structural features and intricate details in\nthe generated textures. The method also employs a symmetrical view synthesis\nstrategy combined with regional prompts for enhancing view consistency.\nAdditionally, it introduces novel texture blending and soft-inpainting\ntechniques, which significantly reduce the seam regions. Extensive experiments\ndemonstrate that our method outperforms existing state-of-the-art methods.",
            "upvotes": 7,
            "discussionId": "67022b7bae1235c08ba4f635"
        },
        "publishedAt": "2024-10-07T06:12:06.073Z",
        "title": "RoCoTex: A Robust Method for Consistent Texture Synthesis with Diffusion Models",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/634c1f9bb6628cbe2861dcc2/L8bWY8mtgfyzJbCFx1bi_.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.19989.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "/avatars/dd48dff0b639123c605b5c0ee10577d7.svg",
            "fullname": "Jangyeong.Kim",
            "name": "longshiine",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.02760",
            "authors": [
                {
                    "_id": "6701828c797050b1acefdd4c",
                    "user": {
                        "_id": "636daf1b56c0762cfda074b5",
                        "avatarUrl": "/avatars/f44be5eb110acfa2efbd09de6b416239.svg",
                        "isPro": false,
                        "fullname": "Rohit Gandikota",
                        "user": "RohitGandikota",
                        "type": "user"
                    },
                    "name": "Rohit Gandikota",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-07T08:47:27.903Z",
                    "hidden": false
                },
                {
                    "_id": "6701828c797050b1acefdd4d",
                    "user": {
                        "_id": "653bf2002c81c4adc8f99d70",
                        "avatarUrl": "/avatars/0bd74739cef2df5d4889f108de17a120.svg",
                        "isPro": false,
                        "fullname": "Sheridan Feucht",
                        "user": "sfeucht",
                        "type": "user"
                    },
                    "name": "Sheridan Feucht",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-07T08:47:34.143Z",
                    "hidden": false
                },
                {
                    "_id": "6701828c797050b1acefdd4e",
                    "name": "Samuel Marks",
                    "hidden": false
                },
                {
                    "_id": "6701828c797050b1acefdd4f",
                    "name": "David Bau",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-03T17:59:30.000Z",
            "title": "Erasing Conceptual Knowledge from Language Models",
            "summary": "Concept erasure in language models has traditionally lacked a comprehensive\nevaluation framework, leading to incomplete assessments of effectiveness of\nerasure methods. We propose an evaluation paradigm centered on three critical\ncriteria: innocence (complete knowledge removal), seamlessness (maintaining\nconditional fluent generation), and specificity (preserving unrelated task\nperformance). Our evaluation metrics naturally motivate the development of\nErasure of Language Memory (ELM), a new method designed to address all three\ndimensions. ELM employs targeted low-rank updates to alter output distributions\nfor erased concepts while preserving overall model capabilities including\nfluency when prompted for an erased concept. We demonstrate ELM's efficacy on\nbiosecurity, cybersecurity, and literary domain erasure tasks. Comparative\nanalysis shows that ELM achieves superior performance across our proposed\nmetrics, including near-random scores on erased topic assessments, generation\nfluency, maintained accuracy on unrelated benchmarks, and robustness under\nadversarial attacks. Our code, data, and trained models are available at\nhttps://elm.baulab.info",
            "upvotes": 7,
            "discussionId": "6701828d797050b1acefde14"
        },
        "publishedAt": "2024-10-07T02:44:40.251Z",
        "title": "Erasing Conceptual Knowledge from Language Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.02760.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "/avatars/f44be5eb110acfa2efbd09de6b416239.svg",
            "fullname": "Rohit Gandikota",
            "name": "RohitGandikota",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.02241",
            "authors": [
                {
                    "_id": "67037d0c5ce58dd0c356ebce",
                    "user": {
                        "_id": "646f3443c261dc413383b8a4",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646f3443c261dc413383b8a4/KTd5QS8HjUewP8P02inQj.png",
                        "isPro": false,
                        "fullname": "Zhaojian Yu",
                        "user": "zjy2001",
                        "type": "user"
                    },
                    "name": "Zhaojian Yu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-07T07:26:26.903Z",
                    "hidden": false
                },
                {
                    "_id": "67037d0c5ce58dd0c356ebcf",
                    "user": {
                        "_id": "650ed552dc509ae7d7bb1ccc",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/650ed552dc509ae7d7bb1ccc/CPR6gaIjPfnBGfr-D_rpO.jpeg",
                        "isPro": false,
                        "fullname": "Yinghao Wu",
                        "user": "yh1567",
                        "type": "user"
                    },
                    "name": "Yinghao Wu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-07T08:53:52.440Z",
                    "hidden": false
                },
                {
                    "_id": "67037d0c5ce58dd0c356ebd0",
                    "name": "Genesis Wang",
                    "hidden": false
                },
                {
                    "_id": "67037d0c5ce58dd0c356ebd1",
                    "name": "Heming Weng",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-03T06:26:49.000Z",
            "title": "MIGA: Mixture-of-Experts with Group Aggregation for Stock Market\n  Prediction",
            "summary": "Stock market prediction has remained an extremely challenging problem for\nmany decades owing to its inherent high volatility and low information noisy\nratio. Existing solutions based on machine learning or deep learning\ndemonstrate superior performance by employing a single model trained on the\nentire stock dataset to generate predictions across all types of stocks.\nHowever, due to the significant variations in stock styles and market trends, a\nsingle end-to-end model struggles to fully capture the differences in these\nstylized stock features, leading to relatively inaccurate predictions for all\ntypes of stocks. In this paper, we present MIGA, a novel Mixture of Expert with\nGroup Aggregation framework designed to generate specialized predictions for\nstocks with different styles by dynamically switching between distinct style\nexperts. To promote collaboration among different experts in MIGA, we propose a\nnovel inner group attention architecture, enabling experts within the same\ngroup to share information and thereby enhancing the overall performance of all\nexperts. As a result, MIGA significantly outperforms other end-to-end models on\nthree Chinese Stock Index benchmarks including CSI300, CSI500, and CSI1000.\nNotably, MIGA-Conv reaches 24 % excess annual return on CSI300 benchmark,\nsurpassing the previous state-of-the-art model by 8% absolute. Furthermore, we\nconduct a comprehensive analysis of mixture of experts for stock market\nprediction, providing valuable insights for future research.",
            "upvotes": 5,
            "discussionId": "67037d0d5ce58dd0c356ec1c"
        },
        "publishedAt": "2024-10-07T04:50:12.745Z",
        "title": "MIGA: Mixture-of-Experts with Group Aggregation for Stock Market Prediction",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.02241.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646f3443c261dc413383b8a4/KTd5QS8HjUewP8P02inQj.png",
            "fullname": "Zhaojian Yu",
            "name": "zjy2001",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.01273",
            "authors": [
                {
                    "_id": "670391e647f71037735d4ab8",
                    "user": {
                        "_id": "632ced22ea6e62428aba1625",
                        "avatarUrl": "/avatars/0aae0ad956bdb2345732ca47fb28e3b6.svg",
                        "isPro": false,
                        "fullname": "Suhwan Choi",
                        "user": "MilkClouds",
                        "type": "user"
                    },
                    "name": "Suhwan Choi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-07T09:19:23.886Z",
                    "hidden": false
                },
                {
                    "_id": "670391e647f71037735d4ab9",
                    "user": {
                        "_id": "65e972db5686ed1f5eff2327",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/er_WDGEq0HY8rpIg69v0G.png",
                        "isPro": false,
                        "fullname": "Cho Yongjun",
                        "user": "PurpleSand",
                        "type": "user"
                    },
                    "name": "Yongjun Cho",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-07T09:16:57.884Z",
                    "hidden": false
                },
                {
                    "_id": "670391e647f71037735d4aba",
                    "name": "Minchan Kim",
                    "hidden": false
                },
                {
                    "_id": "670391e647f71037735d4abb",
                    "user": {
                        "_id": "646484cfb90150b2706df03b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646484cfb90150b2706df03b/8ocSbXBSbrruhlhcxwzEt.png",
                        "isPro": false,
                        "fullname": "Jaeyoon Jung",
                        "user": "lastdefiance20",
                        "type": "user"
                    },
                    "name": "Jaeyoon Jung",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-07T08:17:20.080Z",
                    "hidden": false
                },
                {
                    "_id": "670391e647f71037735d4abc",
                    "name": "Myunchul Joe",
                    "hidden": false
                },
                {
                    "_id": "670391e647f71037735d4abd",
                    "user": {
                        "_id": "646463124bf91229222a3217",
                        "avatarUrl": "/avatars/ca88658951b3aafa276e1cfa6403fc7c.svg",
                        "isPro": false,
                        "fullname": "Yu Been Park",
                        "user": "Unmanned-YuBeen",
                        "type": "user"
                    },
                    "name": "Yubeen Park",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-07T09:20:02.994Z",
                    "hidden": false
                },
                {
                    "_id": "670391e647f71037735d4abe",
                    "name": "Minseo Kim",
                    "hidden": false
                },
                {
                    "_id": "670391e647f71037735d4abf",
                    "name": "Sungwoong Kim",
                    "hidden": false
                },
                {
                    "_id": "670391e647f71037735d4ac0",
                    "name": "Sungjae Lee",
                    "hidden": false
                },
                {
                    "_id": "670391e647f71037735d4ac1",
                    "user": {
                        "_id": "660f5b86f020db4b89aac9ef",
                        "avatarUrl": "/avatars/ffbb5711622420913686747d36901c50.svg",
                        "isPro": false,
                        "fullname": "Hwiseong Park",
                        "user": "wpiioos",
                        "type": "user"
                    },
                    "name": "Hwiseong Park",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-07T09:21:22.336Z",
                    "hidden": false
                },
                {
                    "_id": "670391e647f71037735d4ac2",
                    "user": {
                        "_id": "60d74d1affe0328e0167dc5f",
                        "avatarUrl": "/avatars/9b1a2df9402e9c26e1eb7c818af9bae0.svg",
                        "isPro": false,
                        "fullname": "Jiwan Chung",
                        "user": "jiwan-chung",
                        "type": "user"
                    },
                    "name": "Jiwan Chung",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-07T09:21:28.641Z",
                    "hidden": false
                },
                {
                    "_id": "670391e647f71037735d4ac3",
                    "user": {
                        "_id": "6504777fb1da3747a05160c4",
                        "avatarUrl": "/avatars/b777d98a5ff971ddb4c3e1060bb3e070.svg",
                        "isPro": false,
                        "fullname": "Youngjae Yu",
                        "user": "yjyu",
                        "type": "user"
                    },
                    "name": "Youngjae Yu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-07T09:22:19.766Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-02T06:34:45.000Z",
            "title": "CANVAS: Commonsense-Aware Navigation System for Intuitive Human-Robot\n  Interaction",
            "summary": "Real-life robot navigation involves more than just reaching a destination; it\nrequires optimizing movements while addressing scenario-specific goals. An\nintuitive way for humans to express these goals is through abstract cues like\nverbal commands or rough sketches. Such human guidance may lack details or be\nnoisy. Nonetheless, we expect robots to navigate as intended. For robots to\ninterpret and execute these abstract instructions in line with human\nexpectations, they must share a common understanding of basic navigation\nconcepts with humans. To this end, we introduce CANVAS, a novel framework that\ncombines visual and linguistic instructions for commonsense-aware navigation.\nIts success is driven by imitation learning, enabling the robot to learn from\nhuman navigation behavior. We present COMMAND, a comprehensive dataset with\nhuman-annotated navigation results, spanning over 48 hours and 219 km, designed\nto train commonsense-aware navigation systems in simulated environments. Our\nexperiments show that CANVAS outperforms the strong rule-based system ROS\nNavStack across all environments, demonstrating superior performance with noisy\ninstructions. Notably, in the orchard environment, where ROS NavStack records a\n0% total success rate, CANVAS achieves a total success rate of 67%. CANVAS also\nclosely aligns with human demonstrations and commonsense constraints, even in\nunseen environments. Furthermore, real-world deployment of CANVAS showcases\nimpressive Sim2Real transfer with a total success rate of 69%, highlighting the\npotential of learning from human demonstrations in simulated environments for\nreal-world applications.",
            "upvotes": 4,
            "discussionId": "670391e747f71037735d4b17"
        },
        "publishedAt": "2024-10-07T06:56:55.035Z",
        "title": "CANVAS: Commonsense-Aware Navigation System for Intuitive Human-Robot Interaction",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.01273.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646484cfb90150b2706df03b/8ocSbXBSbrruhlhcxwzEt.png",
            "fullname": "Jaeyoon Jung",
            "name": "lastdefiance20",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.03535",
            "authors": [
                {
                    "_id": "67039db4dae57ebb932171ec",
                    "user": {
                        "_id": "67039d3a8790e37e2e31c58d",
                        "avatarUrl": "/avatars/809e57f523b78ca5498ff8001f135918.svg",
                        "isPro": false,
                        "fullname": "João Bravo",
                        "user": "joaobravo",
                        "type": "user"
                    },
                    "name": "João Bravo",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2024-10-07T09:30:23.258Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-04T15:54:02.000Z",
            "title": "NRGBoost: Energy-Based Generative Boosted Trees",
            "summary": "Despite the rise to dominance of deep learning in unstructured data domains,\ntree-based methods such as Random Forests (RF) and Gradient Boosted Decision\nTrees (GBDT) are still the workhorses for handling discriminative tasks on\ntabular data. We explore generative extensions of these popular algorithms with\na focus on explicitly modeling the data density (up to a normalization\nconstant), thus enabling other applications besides sampling. As our main\ncontribution we propose an energy-based generative boosting algorithm that is\nanalogous to the second order boosting implemented in popular packages like\nXGBoost. We show that, despite producing a generative model capable of handling\ninference tasks over any input variable, our proposed algorithm can achieve\nsimilar discriminative performance to GBDT on a number of real world tabular\ndatasets, outperforming alternative generative approaches. At the same time, we\nshow that it is also competitive with neural network based models for sampling.",
            "upvotes": 3,
            "discussionId": "67039db5dae57ebb93217243"
        },
        "publishedAt": "2024-10-07T07:17:36.836Z",
        "title": "NRGBoost: Energy-Based Generative Boosted Trees",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.03535.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/809e57f523b78ca5498ff8001f135918.svg",
            "fullname": "João Bravo",
            "name": "joaobravo",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.03103",
            "authors": [
                {
                    "_id": "6703dbae126f9ab39faf2c51",
                    "user": {
                        "_id": "64ff904013f154652687bd80",
                        "avatarUrl": "/avatars/a04deda1d5c9db0c15a9311806a4b2dc.svg",
                        "isPro": false,
                        "fullname": "Yifeng Ding",
                        "user": "YifengDing",
                        "type": "user"
                    },
                    "name": "Yifeng Ding",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2024-10-07T13:02:08.380Z",
                    "hidden": false
                },
                {
                    "_id": "6703dbae126f9ab39faf2c52",
                    "name": "Hantian Ding",
                    "hidden": false
                },
                {
                    "_id": "6703dbae126f9ab39faf2c53",
                    "name": "Shiqi Wang",
                    "hidden": false
                },
                {
                    "_id": "6703dbae126f9ab39faf2c54",
                    "name": "Qing Sun",
                    "hidden": false
                },
                {
                    "_id": "6703dbae126f9ab39faf2c55",
                    "name": "Varun Kumar",
                    "hidden": false
                },
                {
                    "_id": "6703dbae126f9ab39faf2c56",
                    "name": "Zijian Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-04T02:53:52.000Z",
            "title": "Horizon-Length Prediction: Advancing Fill-in-the-Middle Capabilities for\n  Code Generation with Lookahead Planning",
            "summary": "Fill-in-the-Middle (FIM) has become integral to code language models,\nenabling generation of missing code given both left and right contexts.\nHowever, the current FIM training paradigm, which reorders original training\nsequences and then performs regular next-token prediction (NTP), often leads to\nmodels struggling to generate content that aligns smoothly with the surrounding\ncontext. Crucially, while existing works rely on rule-based post-processing to\ncircumvent this weakness, such methods are not practically usable in\nopen-domain code completion tasks as they depend on restrictive,\ndataset-specific assumptions (e.g., generating the same number of lines as in\nthe ground truth). Moreover, model performance on FIM tasks deteriorates\nsignificantly without these unrealistic assumptions.\n  We hypothesize that NTP alone is insufficient for models to learn effective\nplanning conditioned on the distant right context, a critical factor for\nsuccessful code infilling. To overcome this, we propose Horizon-Length\nPrediction (HLP), a novel training objective that teaches models to predict the\nnumber of remaining middle tokens (i.e., horizon length) at each step. HLP\nadvances FIM with lookahead planning, enabling models to inherently learn\ninfilling boundaries for arbitrary left and right contexts without relying on\ndataset-specific post-processing. Our evaluation across different models and\nsizes shows that HLP significantly improves FIM performance by up to 24%\nrelatively on diverse benchmarks, across file-level and repository-level, and\nwithout resorting to unrealistic post-processing methods. Furthermore, the\nenhanced planning capability gained through HLP boosts model performance on\ncode reasoning. Importantly, HLP only incurs negligible training overhead and\nno additional inference cost, ensuring its practicality for real-world\nscenarios.",
            "upvotes": 1,
            "discussionId": "6703dbaf126f9ab39faf2c87"
        },
        "publishedAt": "2024-10-07T14:11:08.458Z",
        "title": "Horizon-Length Prediction: Advancing Fill-in-the-Middle Capabilities for Code Generation with Lookahead Planning",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.03103.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/a04deda1d5c9db0c15a9311806a4b2dc.svg",
            "fullname": "Yifeng Ding",
            "name": "YifengDing",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.03645",
            "authors": [
                {
                    "_id": "6703efb3fa3ab087c58e6b87",
                    "user": {
                        "_id": "6456445e1ca9debab051b5d0",
                        "avatarUrl": "/avatars/769496beded27fa85957e62fa0b46313.svg",
                        "isPro": false,
                        "fullname": "Pu Hua",
                        "user": "piao-0429",
                        "type": "user"
                    },
                    "name": "Pu Hua",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-07T15:10:16.536Z",
                    "hidden": false
                },
                {
                    "_id": "6703efb3fa3ab087c58e6b88",
                    "name": "Minghuan Liu",
                    "hidden": false
                },
                {
                    "_id": "6703efb3fa3ab087c58e6b89",
                    "name": "Annabella Macaluso",
                    "hidden": false
                },
                {
                    "_id": "6703efb3fa3ab087c58e6b8a",
                    "name": "Yunfeng Lin",
                    "hidden": false
                },
                {
                    "_id": "6703efb3fa3ab087c58e6b8b",
                    "name": "Weinan Zhang",
                    "hidden": false
                },
                {
                    "_id": "6703efb3fa3ab087c58e6b8c",
                    "name": "Huazhe Xu",
                    "hidden": false
                },
                {
                    "_id": "6703efb3fa3ab087c58e6b8d",
                    "name": "Lirui Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-04T17:51:33.000Z",
            "title": "GenSim2: Scaling Robot Data Generation with Multi-modal and Reasoning\n  LLMs",
            "summary": "Robotic simulation today remains challenging to scale up due to the human\nefforts required to create diverse simulation tasks and scenes.\nSimulation-trained policies also face scalability issues as many sim-to-real\nmethods focus on a single task. To address these challenges, this work proposes\nGenSim2, a scalable framework that leverages coding LLMs with multi-modal and\nreasoning capabilities for complex and realistic simulation task creation,\nincluding long-horizon tasks with articulated objects. To automatically\ngenerate demonstration data for these tasks at scale, we propose planning and\nRL solvers that generalize within object categories. The pipeline can generate\ndata for up to 100 articulated tasks with 200 objects and reduce the required\nhuman efforts. To utilize such data, we propose an effective multi-task\nlanguage-conditioned policy architecture, dubbed proprioceptive point-cloud\ntransformer (PPT), that learns from the generated demonstrations and exhibits\nstrong sim-to-real zero-shot transfer. Combining the proposed pipeline and the\npolicy architecture, we show a promising usage of GenSim2 that the generated\ndata can be used for zero-shot transfer or co-train with real-world collected\ndata, which enhances the policy performance by 20% compared with training\nexclusively on limited real data.",
            "upvotes": 1,
            "discussionId": "6703efb5fa3ab087c58e6c6c"
        },
        "publishedAt": "2024-10-07T12:57:34.900Z",
        "title": "GenSim2: Scaling Robot Data Generation with Multi-modal and Reasoning LLMs",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.03645.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/0088eb929866face5f95218943e3f478.svg",
            "fullname": "Lirui Wang",
            "name": "liruiw",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    }
]