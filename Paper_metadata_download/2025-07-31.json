[
  {
    "paper": {
      "id": "2507.22827",
      "authors": [
        {
          "_id": "688abfb98b724c8c7187dd3c",
          "name": "Yilei Jiang",
          "hidden": false
        },
        {
          "_id": "688abfb98b724c8c7187dd3d",
          "name": "Yaozhi Zheng",
          "hidden": false
        },
        {
          "_id": "688abfb98b724c8c7187dd3e",
          "name": "Yuxuan Wan",
          "hidden": false
        },
        {
          "_id": "688abfb98b724c8c7187dd3f",
          "name": "Jiaming Han",
          "hidden": false
        },
        {
          "_id": "688abfb98b724c8c7187dd40",
          "name": "Qunzhong Wang",
          "hidden": false
        },
        {
          "_id": "688abfb98b724c8c7187dd41",
          "name": "Michael R. Lyu",
          "hidden": false
        },
        {
          "_id": "688abfb98b724c8c7187dd42",
          "name": "Xiangyu Yue",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-30T16:41:21.000Z",
      "submittedOnDailyAt": "2025-07-31T01:37:19.365Z",
      "title": "ScreenCoder: Advancing Visual-to-Code Generation for Front-End\n  Automation via Modular Multimodal Agents",
      "submittedOnDailyBy": {
        "_id": "62318c0386753f5f41d0e261",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62318c0386753f5f41d0e261/xO_5PvOf7lXhQPnQLcmnq.jpeg",
        "isPro": false,
        "fullname": "Jiaming Han",
        "user": "csuhan",
        "type": "user"
      },
      "summary": "Automating the transformation of user interface (UI) designs into front-end\ncode holds significant promise for accelerating software development and\ndemocratizing design workflows. While recent large language models (LLMs) have\ndemonstrated progress in text-to-code generation, many existing approaches rely\nsolely on natural language prompts, limiting their effectiveness in capturing\nspatial layout and visual design intent. In contrast, UI development in\npractice is inherently multimodal, often starting from visual sketches or\nmockups. To address this gap, we introduce a modular multi-agent framework that\nperforms UI-to-code generation in three interpretable stages: grounding,\nplanning, and generation. The grounding agent uses a vision-language model to\ndetect and label UI components, the planning agent constructs a hierarchical\nlayout using front-end engineering priors, and the generation agent produces\nHTML/CSS code via adaptive prompt-based synthesis. This design improves\nrobustness, interpretability, and fidelity over end-to-end black-box methods.\nFurthermore, we extend the framework into a scalable data engine that\nautomatically produces large-scale image-code pairs. Using these synthetic\nexamples, we fine-tune and reinforce an open-source VLM, yielding notable gains\nin UI understanding and code quality. Extensive experiments demonstrate that\nour approach achieves state-of-the-art performance in layout accuracy,\nstructural coherence, and code correctness. Our code is made publicly available\nat https://github.com/leigest519/ScreenCoder.",
      "upvotes": 41,
      "discussionId": "688abfb98b724c8c7187dd43",
      "projectPage": "https://huggingface.co/spaces/Jimmyzheng-10/ScreenCoder",
      "githubRepo": "https://github.com/leigest519/ScreenCoder",
      "ai_summary": "A modular multi-agent framework improves UI-to-code generation by integrating vision-language models, hierarchical layout planning, and adaptive prompt-based synthesis, achieving state-of-the-art performance.",
      "ai_keywords": [
        "vision-language model",
        "hierarchical layout",
        "adaptive prompt-based synthesis",
        "UI-to-code generation",
        "multimodal",
        "grounding agent",
        "planning agent",
        "generation agent",
        "data engine",
        "fine-tuning",
        "reinforcement"
      ],
      "githubStars": 39
    },
    "publishedAt": "2025-07-30T12:41:21.000Z",
    "title": "ScreenCoder: Advancing Visual-to-Code Generation for Front-End\n  Automation via Modular Multimodal Agents",
    "summary": "Automating the transformation of user interface (UI) designs into front-end\ncode holds significant promise for accelerating software development and\ndemocratizing design workflows. While recent large language models (LLMs) have\ndemonstrated progress in text-to-code generation, many existing approaches rely\nsolely on natural language prompts, limiting their effectiveness in capturing\nspatial layout and visual design intent. In contrast, UI development in\npractice is inherently multimodal, often starting from visual sketches or\nmockups. To address this gap, we introduce a modular multi-agent framework that\nperforms UI-to-code generation in three interpretable stages: grounding,\nplanning, and generation. The grounding agent uses a vision-language model to\ndetect and label UI components, the planning agent constructs a hierarchical\nlayout using front-end engineering priors, and the generation agent produces\nHTML/CSS code via adaptive prompt-based synthesis. This design improves\nrobustness, interpretability, and fidelity over end-to-end black-box methods.\nFurthermore, we extend the framework into a scalable data engine that\nautomatically produces large-scale image-code pairs. Using these synthetic\nexamples, we fine-tune and reinforce an open-source VLM, yielding notable gains\nin UI understanding and code quality. Extensive experiments demonstrate that\nour approach achieves state-of-the-art performance in layout accuracy,\nstructural coherence, and code correctness. Our code is made publicly available\nat https://github.com/leigest519/ScreenCoder.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.22827.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62318c0386753f5f41d0e261",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62318c0386753f5f41d0e261/xO_5PvOf7lXhQPnQLcmnq.jpeg",
      "fullname": "Jiaming Han",
      "name": "csuhan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 21
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.21493",
      "authors": [
        {
          "_id": "688ad3c18b724c8c7187dd67",
          "name": "Longwen Zhang",
          "hidden": false
        },
        {
          "_id": "688ad3c18b724c8c7187dd68",
          "name": "Qixuan Zhang",
          "hidden": false
        },
        {
          "_id": "688ad3c18b724c8c7187dd69",
          "name": "Haoran Jiang",
          "hidden": false
        },
        {
          "_id": "688ad3c18b724c8c7187dd6a",
          "name": "Yinuo Bai",
          "hidden": false
        },
        {
          "_id": "688ad3c18b724c8c7187dd6b",
          "name": "Wei Yang",
          "hidden": false
        },
        {
          "_id": "688ad3c18b724c8c7187dd6c",
          "name": "Lan Xu",
          "hidden": false
        },
        {
          "_id": "688ad3c18b724c8c7187dd6d",
          "name": "Jingyi Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-29T04:21:21.000Z",
      "submittedOnDailyAt": "2025-07-31T00:56:30.360Z",
      "title": "BANG: Dividing 3D Assets via Generative Exploded Dynamics",
      "submittedOnDailyBy": {
        "_id": "636d12455aaed143cd665607",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1679399015950-636d12455aaed143cd665607.png",
        "isPro": false,
        "fullname": "ZLW",
        "user": "ZarkLngeW",
        "type": "user"
      },
      "summary": "3D creation has always been a unique human strength, driven by our ability to\ndeconstruct and reassemble objects using our eyes, mind and hand. However,\ncurrent 3D design tools struggle to replicate this natural process, requiring\nconsiderable artistic expertise and manual labor. This paper introduces BANG, a\nnovel generative approach that bridges 3D generation and reasoning, allowing\nfor intuitive and flexible part-level decomposition of 3D objects. At the heart\nof BANG is \"Generative Exploded Dynamics\", which creates a smooth sequence of\nexploded states for an input geometry, progressively separating parts while\npreserving their geometric and semantic coherence.\n  BANG utilizes a pre-trained large-scale latent diffusion model, fine-tuned\nfor exploded dynamics with a lightweight exploded view adapter, allowing\nprecise control over the decomposition process. It also incorporates a temporal\nattention module to ensure smooth transitions and consistency across time. BANG\nenhances control with spatial prompts, such as bounding boxes and surface\nregions, enabling users to specify which parts to decompose and how. This\ninteraction can be extended with multimodal models like GPT-4, enabling\n2D-to-3D manipulations for more intuitive and creative workflows.\n  The capabilities of BANG extend to generating detailed part-level geometry,\nassociating parts with functional descriptions, and facilitating\ncomponent-aware 3D creation and manufacturing workflows. Additionally, BANG\noffers applications in 3D printing, where separable parts are generated for\neasy printing and reassembly. In essence, BANG enables seamless transformation\nfrom imaginative concepts to detailed 3D assets, offering a new perspective on\ncreation that resonates with human intuition.",
      "upvotes": 26,
      "discussionId": "688ad3c18b724c8c7187dd6e",
      "projectPage": "https://sites.google.com/view/bang7355608",
      "ai_summary": "BANG is a generative approach that uses latent diffusion models and temporal attention to enable intuitive part-level decomposition and manipulation of 3D objects, enhancing 3D creation workflows.",
      "ai_keywords": [
        "Generative Exploded Dynamics",
        "latent diffusion model",
        "exploded view adapter",
        "temporal attention module",
        "spatial prompts",
        "multimodal models",
        "GPT-4",
        "2D-to-3D manipulations",
        "component-aware 3D creation",
        "3D printing"
      ]
    },
    "publishedAt": "2025-07-29T00:21:21.000Z",
    "title": "BANG: Dividing 3D Assets via Generative Exploded Dynamics",
    "summary": "3D creation has always been a unique human strength, driven by our ability to\ndeconstruct and reassemble objects using our eyes, mind and hand. However,\ncurrent 3D design tools struggle to replicate this natural process, requiring\nconsiderable artistic expertise and manual labor. This paper introduces BANG, a\nnovel generative approach that bridges 3D generation and reasoning, allowing\nfor intuitive and flexible part-level decomposition of 3D objects. At the heart\nof BANG is \"Generative Exploded Dynamics\", which creates a smooth sequence of\nexploded states for an input geometry, progressively separating parts while\npreserving their geometric and semantic coherence.\n  BANG utilizes a pre-trained large-scale latent diffusion model, fine-tuned\nfor exploded dynamics with a lightweight exploded view adapter, allowing\nprecise control over the decomposition process. It also incorporates a temporal\nattention module to ensure smooth transitions and consistency across time. BANG\nenhances control with spatial prompts, such as bounding boxes and surface\nregions, enabling users to specify which parts to decompose and how. This\ninteraction can be extended with multimodal models like GPT-4, enabling\n2D-to-3D manipulations for more intuitive and creative workflows.\n  The capabilities of BANG extend to generating detailed part-level geometry,\nassociating parts with functional descriptions, and facilitating\ncomponent-aware 3D creation and manufacturing workflows. Additionally, BANG\noffers applications in 3D printing, where separable parts are generated for\neasy printing and reassembly. In essence, BANG enables seamless transformation\nfrom imaginative concepts to detailed 3D assets, offering a new perspective on\ncreation that resonates with human intuition.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.21493.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "636d12455aaed143cd665607",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1679399015950-636d12455aaed143cd665607.png",
      "fullname": "ZLW",
      "name": "ZarkLngeW",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.22448",
      "authors": [
        {
          "_id": "688adbb28b724c8c7187dd70",
          "name": "Jingwei Zuo",
          "hidden": false
        },
        {
          "_id": "688adbb28b724c8c7187dd71",
          "name": "Maksim Velikanov",
          "hidden": false
        },
        {
          "_id": "688adbb28b724c8c7187dd72",
          "name": "Ilyas Chahed",
          "hidden": false
        },
        {
          "_id": "688adbb28b724c8c7187dd73",
          "name": "Younes Belkada",
          "hidden": false
        },
        {
          "_id": "688adbb28b724c8c7187dd74",
          "name": "Dhia Eddine Rhayem",
          "hidden": false
        },
        {
          "_id": "688adbb28b724c8c7187dd75",
          "name": "Guillaume Kunsch",
          "hidden": false
        },
        {
          "_id": "688adbb28b724c8c7187dd76",
          "name": "Hakim Hacid",
          "hidden": false
        },
        {
          "_id": "688adbb28b724c8c7187dd77",
          "name": "Hamza Yous",
          "hidden": false
        },
        {
          "_id": "688adbb28b724c8c7187dd78",
          "name": "Brahim Farhat",
          "hidden": false
        },
        {
          "_id": "688adbb28b724c8c7187dd79",
          "name": "Ibrahim Khadraoui",
          "hidden": false
        },
        {
          "_id": "688adbb28b724c8c7187dd7a",
          "name": "Mugariya Farooq",
          "hidden": false
        },
        {
          "_id": "688adbb28b724c8c7187dd7b",
          "name": "Giulia Campesan",
          "hidden": false
        },
        {
          "_id": "688adbb28b724c8c7187dd7c",
          "name": "Ruxandra Cojocaru",
          "hidden": false
        },
        {
          "_id": "688adbb28b724c8c7187dd7d",
          "name": "Yasser Djilali",
          "hidden": false
        },
        {
          "_id": "688adbb28b724c8c7187dd7e",
          "name": "Shi Hu",
          "hidden": false
        },
        {
          "_id": "688adbb28b724c8c7187dd7f",
          "name": "Iheb Chaabane",
          "hidden": false
        },
        {
          "_id": "688adbb28b724c8c7187dd80",
          "name": "Puneesh Khanna",
          "hidden": false
        },
        {
          "_id": "688adbb28b724c8c7187dd81",
          "name": "Mohamed El Amine Seddik",
          "hidden": false
        },
        {
          "_id": "688adbb28b724c8c7187dd82",
          "name": "Ngoc Dung Huynh",
          "hidden": false
        },
        {
          "_id": "688adbb28b724c8c7187dd83",
          "name": "Phuc Le Khac",
          "hidden": false
        },
        {
          "_id": "688adbb28b724c8c7187dd84",
          "name": "Leen AlQadi",
          "hidden": false
        },
        {
          "_id": "688adbb28b724c8c7187dd85",
          "name": "Billel Mokeddem",
          "hidden": false
        },
        {
          "_id": "688adbb28b724c8c7187dd86",
          "name": "Mohamed Chami",
          "hidden": false
        },
        {
          "_id": "688adbb28b724c8c7187dd87",
          "name": "Abdalgader Abubaker",
          "hidden": false
        },
        {
          "_id": "688adbb28b724c8c7187dd88",
          "name": "Mikhail Lubinets",
          "hidden": false
        },
        {
          "_id": "688adbb28b724c8c7187dd89",
          "name": "Kacper Piskorski",
          "hidden": false
        },
        {
          "_id": "688adbb28b724c8c7187dd8a",
          "name": "Slim Frikha",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6460c3811db65f878513bcaf/qdWzn2RRJ-qgCk6E3H4-u.png"
      ],
      "publishedAt": "2025-07-30T07:55:33.000Z",
      "submittedOnDailyAt": "2025-07-31T01:30:54.022Z",
      "title": "Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency\n  and Performance",
      "submittedOnDailyBy": {
        "_id": "6460c3811db65f878513bcaf",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6460c3811db65f878513bcaf/CRdJ8lXixDku3k8Rm5Stn.jpeg",
        "isPro": false,
        "fullname": "Jingwei Zuo",
        "user": "JingweiZuo",
        "type": "user"
      },
      "summary": "In this report, we introduce Falcon-H1, a new series of large language models\n(LLMs) featuring hybrid architecture designs optimized for both high\nperformance and efficiency across diverse use cases. Unlike earlier Falcon\nmodels built solely on Transformer or Mamba architectures, Falcon-H1 adopts a\nparallel hybrid approach that combines Transformer-based attention with State\nSpace Models (SSMs), known for superior long-context memory and computational\nefficiency. We systematically revisited model design, data strategy, and\ntraining dynamics, challenging conventional practices in the field. Falcon-H1\nis released in multiple configurations, including base and instruction-tuned\nvariants at 0.5B, 1.5B, 1.5B-deep, 3B, 7B, and 34B parameters. Quantized\ninstruction-tuned models are also available, totaling over 30 checkpoints on\nHugging Face Hub. Falcon-H1 models demonstrate state-of-the-art performance and\nexceptional parameter and training efficiency. The flagship Falcon-H1-34B\nmatches or outperforms models up to 70B scale, such as Qwen3-32B, Qwen2.5-72B,\nand Llama3.3-70B, while using fewer parameters and less data. Smaller models\nshow similar trends: the Falcon-H1-1.5B-Deep rivals current leading 7B-10B\nmodels, and Falcon-H1-0.5B performs comparably to typical 7B models from 2024.\nThese models excel across reasoning, mathematics, multilingual tasks,\ninstruction following, and scientific knowledge. With support for up to 256K\ncontext tokens and 18 languages, Falcon-H1 is suitable for a wide range of\napplications. All models are released under a permissive open-source license,\nunderscoring our commitment to accessible and impactful AI research.",
      "upvotes": 21,
      "discussionId": "688adbb28b724c8c7187dd8b",
      "projectPage": "https://tiiuae.github.io/Falcon-H1/",
      "githubRepo": "https://github.com/tiiuae/Falcon-H1/",
      "ai_summary": "Falcon-H1, a new series of large language models with a hybrid architecture combining Transformer-based attention and State Space Models, achieves state-of-the-art performance and efficiency across various tasks and sizes.",
      "ai_keywords": [
        "large language models",
        "hybrid architecture",
        "Transformer-based attention",
        "State Space Models",
        "long-context memory",
        "computational efficiency",
        "model design",
        "data strategy",
        "training dynamics",
        "instruction-tuned",
        "quantized models",
        "parameter efficiency",
        "training efficiency",
        "context tokens",
        "multilingual tasks",
        "instruction following",
        "scientific knowledge",
        "open-source license"
      ],
      "githubStars": 43
    },
    "publishedAt": "2025-07-30T03:55:33.000Z",
    "title": "Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency\n  and Performance",
    "summary": "In this report, we introduce Falcon-H1, a new series of large language models\n(LLMs) featuring hybrid architecture designs optimized for both high\nperformance and efficiency across diverse use cases. Unlike earlier Falcon\nmodels built solely on Transformer or Mamba architectures, Falcon-H1 adopts a\nparallel hybrid approach that combines Transformer-based attention with State\nSpace Models (SSMs), known for superior long-context memory and computational\nefficiency. We systematically revisited model design, data strategy, and\ntraining dynamics, challenging conventional practices in the field. Falcon-H1\nis released in multiple configurations, including base and instruction-tuned\nvariants at 0.5B, 1.5B, 1.5B-deep, 3B, 7B, and 34B parameters. Quantized\ninstruction-tuned models are also available, totaling over 30 checkpoints on\nHugging Face Hub. Falcon-H1 models demonstrate state-of-the-art performance and\nexceptional parameter and training efficiency. The flagship Falcon-H1-34B\nmatches or outperforms models up to 70B scale, such as Qwen3-32B, Qwen2.5-72B,\nand Llama3.3-70B, while using fewer parameters and less data. Smaller models\nshow similar trends: the Falcon-H1-1.5B-Deep rivals current leading 7B-10B\nmodels, and Falcon-H1-0.5B performs comparably to typical 7B models from 2024.\nThese models excel across reasoning, mathematics, multilingual tasks,\ninstruction following, and scientific knowledge. With support for up to 256K\ncontext tokens and 18 languages, Falcon-H1 is suitable for a wide range of\napplications. All models are released under a permissive open-source license,\nunderscoring our commitment to accessible and impactful AI research.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6460c3811db65f878513bcaf/qdWzn2RRJ-qgCk6E3H4-u.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.22448.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6460c3811db65f878513bcaf",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6460c3811db65f878513bcaf/CRdJ8lXixDku3k8Rm5Stn.jpeg",
      "fullname": "Jingwei Zuo",
      "name": "JingweiZuo",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 31
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.20976",
      "authors": [
        {
          "_id": "6888dca266e37b2a6b291f63",
          "user": {
            "_id": "6697d30fbfec78c1bf6a4962",
            "avatarUrl": "/avatars/17cfd7d7eb2b89b5051bd912e41e5a62.svg",
            "isPro": false,
            "fullname": "Xiao Fang",
            "user": "xiaofanghf",
            "type": "user"
          },
          "name": "Xiao Fang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-30T09:05:02.766Z",
          "hidden": false
        },
        {
          "_id": "6888dca266e37b2a6b291f64",
          "name": "Minhyek Jeon",
          "hidden": false
        },
        {
          "_id": "6888dca266e37b2a6b291f65",
          "name": "Zheyang Qin",
          "hidden": false
        },
        {
          "_id": "6888dca266e37b2a6b291f66",
          "name": "Stanislav Panev",
          "hidden": false
        },
        {
          "_id": "6888dca266e37b2a6b291f67",
          "name": "Celso de Melo",
          "hidden": false
        },
        {
          "_id": "6888dca266e37b2a6b291f68",
          "name": "Shuowen Hu",
          "hidden": false
        },
        {
          "_id": "6888dca266e37b2a6b291f69",
          "name": "Shayok Chakraborty",
          "hidden": false
        },
        {
          "_id": "6888dca266e37b2a6b291f6a",
          "name": "Fernando De la Torre",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-28T16:38:06.000Z",
      "submittedOnDailyAt": "2025-07-31T02:46:45.983Z",
      "title": "Adapting Vehicle Detectors for Aerial Imagery to Unseen Domains with\n  Weak Supervision",
      "submittedOnDailyBy": {
        "_id": "6697d30fbfec78c1bf6a4962",
        "avatarUrl": "/avatars/17cfd7d7eb2b89b5051bd912e41e5a62.svg",
        "isPro": false,
        "fullname": "Xiao Fang",
        "user": "xiaofanghf",
        "type": "user"
      },
      "summary": "Detecting vehicles in aerial imagery is a critical task with applications in\ntraffic monitoring, urban planning, and defense intelligence. Deep learning\nmethods have provided state-of-the-art (SOTA) results for this application.\nHowever, a significant challenge arises when models trained on data from one\ngeographic region fail to generalize effectively to other areas. Variability in\nfactors such as environmental conditions, urban layouts, road networks, vehicle\ntypes, and image acquisition parameters (e.g., resolution, lighting, and angle)\nleads to domain shifts that degrade model performance. This paper proposes a\nnovel method that uses generative AI to synthesize high-quality aerial images\nand their labels, improving detector training through data augmentation. Our\nkey contribution is the development of a multi-stage, multi-modal knowledge\ntransfer framework utilizing fine-tuned latent diffusion models (LDMs) to\nmitigate the distribution gap between the source and target environments.\nExtensive experiments across diverse aerial imagery domains show consistent\nperformance improvements in AP50 over supervised learning on source domain\ndata, weakly supervised adaptation methods, unsupervised domain adaptation\nmethods, and open-set object detectors by 4-23%, 6-10%, 7-40%, and more than\n50%, respectively. Furthermore, we introduce two newly annotated aerial\ndatasets from New Zealand and Utah to support further research in this field.\nProject page is available at: https://humansensinglab.github.io/AGenDA",
      "upvotes": 6,
      "discussionId": "6888dca366e37b2a6b291f6b",
      "projectPage": "https://humansensinglab.github.io/AGenDA",
      "githubRepo": "https://github.com/humansensinglab/AGenDA",
      "ai_summary": "A multi-stage, multi-modal knowledge transfer framework using fine-tuned latent diffusion models improves vehicle detection in aerial imagery across different domains.",
      "ai_keywords": [
        "latent diffusion models",
        "data augmentation",
        "domain adaptation",
        "aerial imagery",
        "vehicle detection",
        "distribution gap",
        "knowledge transfer",
        "AP50",
        "open-set object detectors"
      ],
      "githubStars": 2
    },
    "publishedAt": "2025-07-28T12:38:06.000Z",
    "title": "Adapting Vehicle Detectors for Aerial Imagery to Unseen Domains with\n  Weak Supervision",
    "summary": "Detecting vehicles in aerial imagery is a critical task with applications in\ntraffic monitoring, urban planning, and defense intelligence. Deep learning\nmethods have provided state-of-the-art (SOTA) results for this application.\nHowever, a significant challenge arises when models trained on data from one\ngeographic region fail to generalize effectively to other areas. Variability in\nfactors such as environmental conditions, urban layouts, road networks, vehicle\ntypes, and image acquisition parameters (e.g., resolution, lighting, and angle)\nleads to domain shifts that degrade model performance. This paper proposes a\nnovel method that uses generative AI to synthesize high-quality aerial images\nand their labels, improving detector training through data augmentation. Our\nkey contribution is the development of a multi-stage, multi-modal knowledge\ntransfer framework utilizing fine-tuned latent diffusion models (LDMs) to\nmitigate the distribution gap between the source and target environments.\nExtensive experiments across diverse aerial imagery domains show consistent\nperformance improvements in AP50 over supervised learning on source domain\ndata, weakly supervised adaptation methods, unsupervised domain adaptation\nmethods, and open-set object detectors by 4-23%, 6-10%, 7-40%, and more than\n50%, respectively. Furthermore, we introduce two newly annotated aerial\ndatasets from New Zealand and Utah to support further research in this field.\nProject page is available at: https://humansensinglab.github.io/AGenDA",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.20976.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6697d30fbfec78c1bf6a4962",
      "avatarUrl": "/avatars/17cfd7d7eb2b89b5051bd912e41e5a62.svg",
      "fullname": "Xiao Fang",
      "name": "xiaofanghf",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.22886",
      "authors": [
        {
          "_id": "688ac9788b724c8c7187dd54",
          "name": "Kaining Ying",
          "hidden": false
        },
        {
          "_id": "688ac9788b724c8c7187dd55",
          "name": "Henghui Ding",
          "hidden": false
        },
        {
          "_id": "688ac9788b724c8c7187dd56",
          "name": "Guanquan Jie",
          "hidden": false
        },
        {
          "_id": "688ac9788b724c8c7187dd57",
          "name": "Yu-Gang Jiang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-30T17:59:31.000Z",
      "submittedOnDailyAt": "2025-07-31T00:11:21.540Z",
      "title": "Towards Omnimodal Expressions and Reasoning in Referring Audio-Visual\n  Segmentation",
      "submittedOnDailyBy": {
        "_id": "67ff29ecbf6889a333c69c7a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/fYIJFtF0uciB-1wb0lmTY.png",
        "isPro": false,
        "fullname": "Henghui Ding",
        "user": "HenghuiDing",
        "type": "user"
      },
      "summary": "Referring audio-visual segmentation (RAVS) has recently seen significant\nadvancements, yet challenges remain in integrating multimodal information and\ndeeply understanding and reasoning about audiovisual content. To extend the\nboundaries of RAVS and facilitate future research in this field, we propose\nOmnimodal Referring Audio-Visual Segmentation (OmniAVS), a new dataset\ncontaining 2,098 videos and 59,458 multimodal referring expressions. OmniAVS\nstands out with three key innovations: (1) 8 types of multimodal expressions\nthat flexibly combine text, speech, sound, and visual cues; (2) an emphasis on\nunderstanding audio content beyond just detecting their presence; and (3) the\ninclusion of complex reasoning and world knowledge in expressions. Furthermore,\nwe introduce Omnimodal Instructed Segmentation Assistant (OISA), to address the\nchallenges of multimodal reasoning and fine-grained understanding of\naudiovisual content in OmniAVS. OISA uses MLLM to comprehend complex cues and\nperform reasoning-based segmentation. Extensive experiments show that OISA\noutperforms existing methods on OmniAVS and achieves competitive results on\nother related tasks.",
      "upvotes": 5,
      "discussionId": "688ac9798b724c8c7187dd58",
      "ai_summary": "Omnimodal Referring Audio-Visual Segmentation (OmniAVS) and Omnimodal Instructed Segmentation Assistant (OISA) advance audio-visual segmentation by integrating complex multimodal expressions and leveraging MLLM for reasoning.",
      "ai_keywords": [
        "multimodal referring expressions",
        "audio-visual segmentation",
        "multimodal reasoning",
        "fine-grained understanding",
        "MLLM"
      ]
    },
    "publishedAt": "2025-07-30T13:59:31.000Z",
    "title": "Towards Omnimodal Expressions and Reasoning in Referring Audio-Visual\n  Segmentation",
    "summary": "Referring audio-visual segmentation (RAVS) has recently seen significant\nadvancements, yet challenges remain in integrating multimodal information and\ndeeply understanding and reasoning about audiovisual content. To extend the\nboundaries of RAVS and facilitate future research in this field, we propose\nOmnimodal Referring Audio-Visual Segmentation (OmniAVS), a new dataset\ncontaining 2,098 videos and 59,458 multimodal referring expressions. OmniAVS\nstands out with three key innovations: (1) 8 types of multimodal expressions\nthat flexibly combine text, speech, sound, and visual cues; (2) an emphasis on\nunderstanding audio content beyond just detecting their presence; and (3) the\ninclusion of complex reasoning and world knowledge in expressions. Furthermore,\nwe introduce Omnimodal Instructed Segmentation Assistant (OISA), to address the\nchallenges of multimodal reasoning and fine-grained understanding of\naudiovisual content in OmniAVS. OISA uses MLLM to comprehend complex cues and\nperform reasoning-based segmentation. Extensive experiments show that OISA\noutperforms existing methods on OmniAVS and achieves competitive results on\nother related tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.22886.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67ff29ecbf6889a333c69c7a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/fYIJFtF0uciB-1wb0lmTY.png",
      "fullname": "Henghui Ding",
      "name": "HenghuiDing",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.22607",
      "authors": [
        {
          "_id": "688b17b78b724c8c7187de81",
          "name": "Ruifeng Yuan",
          "hidden": false
        },
        {
          "_id": "688b17b78b724c8c7187de82",
          "name": "Chenghao Xiao",
          "hidden": false
        },
        {
          "_id": "688b17b78b724c8c7187de83",
          "name": "Sicong Leng",
          "hidden": false
        },
        {
          "_id": "688b17b78b724c8c7187de84",
          "name": "Jianyu Wang",
          "hidden": false
        },
        {
          "_id": "688b17b78b724c8c7187de85",
          "name": "Long Li",
          "hidden": false
        },
        {
          "_id": "688b17b78b724c8c7187de86",
          "name": "Weiwen Xu",
          "hidden": false
        },
        {
          "_id": "688b17b78b724c8c7187de87",
          "name": "Hou Pong Chan",
          "hidden": false
        },
        {
          "_id": "688b17b78b724c8c7187de88",
          "name": "Deli Zhao",
          "hidden": false
        },
        {
          "_id": "688b17b78b724c8c7187de89",
          "name": "Tingyang Xu",
          "hidden": false
        },
        {
          "_id": "688b17b78b724c8c7187de8a",
          "name": "Zhongyu Wei",
          "hidden": false
        },
        {
          "_id": "688b17b78b724c8c7187de8b",
          "name": "Hao Zhang",
          "hidden": false
        },
        {
          "_id": "688b17b78b724c8c7187de8c",
          "name": "Yu Rong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-30T12:23:21.000Z",
      "submittedOnDailyAt": "2025-07-31T05:46:56.595Z",
      "title": "VL-Cogito: Progressive Curriculum Reinforcement Learning for Advanced\n  Multimodal Reasoning",
      "submittedOnDailyBy": {
        "_id": "604f67ef0fe8ff3ec13d71ef",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/604f67ef0fe8ff3ec13d71ef/KhUwWvZ3OJ9nEee3B-SXO.png",
        "isPro": false,
        "fullname": "Hou Pong (Ken) Chan",
        "user": "kenchan0226",
        "type": "user"
      },
      "summary": "Reinforcement learning has proven its effectiveness in enhancing the\nreasoning capabilities of large language models. Recent research efforts have\nprogressively extended this paradigm to multimodal reasoning tasks. Due to the\ninherent complexity and diversity of multimodal tasks, especially in semantic\ncontent and problem formulations, existing models often exhibit unstable\nperformance across various domains and difficulty levels. To address these\nlimitations, we propose VL-Cogito, an advanced multimodal reasoning model\ntrained via a novel multi-stage Progressive Curriculum Reinforcement Learning\n(PCuRL) framework. PCuRL systematically guides the model through tasks of\ngradually increasing difficulty, substantially improving its reasoning\nabilities across diverse multimodal contexts. The framework introduces two key\ninnovations: (1) an online difficulty soft weighting mechanism, dynamically\nadjusting training difficulty across successive RL training stages; and (2) a\ndynamic length reward mechanism, which encourages the model to adaptively\nregulate its reasoning path length according to task complexity, thus balancing\nreasoning efficiency with correctness. Experimental evaluations demonstrate\nthat VL-Cogito consistently matches or surpasses existing reasoning-oriented\nmodels across mainstream multimodal benchmarks spanning mathematics, science,\nlogic, and general understanding, validating the effectiveness of our approach.",
      "upvotes": 5,
      "discussionId": "688b17b88b724c8c7187de8d",
      "ai_summary": "VL-Cogito, a multimodal reasoning model, uses a Progressive Curriculum Reinforcement Learning framework to improve performance across diverse tasks by dynamically adjusting difficulty and reasoning path length.",
      "ai_keywords": [
        "reinforcement learning",
        "multimodal reasoning",
        "Progressive Curriculum Reinforcement Learning",
        "PCuRL",
        "online difficulty soft weighting",
        "dynamic length reward mechanism"
      ]
    },
    "publishedAt": "2025-07-30T08:23:21.000Z",
    "title": "VL-Cogito: Progressive Curriculum Reinforcement Learning for Advanced\n  Multimodal Reasoning",
    "summary": "Reinforcement learning has proven its effectiveness in enhancing the\nreasoning capabilities of large language models. Recent research efforts have\nprogressively extended this paradigm to multimodal reasoning tasks. Due to the\ninherent complexity and diversity of multimodal tasks, especially in semantic\ncontent and problem formulations, existing models often exhibit unstable\nperformance across various domains and difficulty levels. To address these\nlimitations, we propose VL-Cogito, an advanced multimodal reasoning model\ntrained via a novel multi-stage Progressive Curriculum Reinforcement Learning\n(PCuRL) framework. PCuRL systematically guides the model through tasks of\ngradually increasing difficulty, substantially improving its reasoning\nabilities across diverse multimodal contexts. The framework introduces two key\ninnovations: (1) an online difficulty soft weighting mechanism, dynamically\nadjusting training difficulty across successive RL training stages; and (2) a\ndynamic length reward mechanism, which encourages the model to adaptively\nregulate its reasoning path length according to task complexity, thus balancing\nreasoning efficiency with correctness. Experimental evaluations demonstrate\nthat VL-Cogito consistently matches or surpasses existing reasoning-oriented\nmodels across mainstream multimodal benchmarks spanning mathematics, science,\nlogic, and general understanding, validating the effectiveness of our approach.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.22607.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "604f67ef0fe8ff3ec13d71ef",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/604f67ef0fe8ff3ec13d71ef/KhUwWvZ3OJ9nEee3B-SXO.png",
      "fullname": "Hou Pong (Ken) Chan",
      "name": "kenchan0226",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.22853",
      "authors": [
        {
          "_id": "688ae0f78b724c8c7187dd8d",
          "name": "Haichuan Hu",
          "hidden": false
        },
        {
          "_id": "688ae0f78b724c8c7187dd8e",
          "name": "Xiaochen Xie",
          "hidden": false
        },
        {
          "_id": "688ae0f78b724c8c7187dd8f",
          "name": "Quanjun Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-30T17:24:05.000Z",
      "submittedOnDailyAt": "2025-07-31T01:51:54.126Z",
      "title": "Repair-R1: Better Test Before Repair",
      "submittedOnDailyBy": {
        "_id": "630341a4ef6f432d84e97db9",
        "avatarUrl": "/avatars/dc9910b5b9053123bad5ff848f677e12.svg",
        "isPro": false,
        "fullname": "tomsawyer",
        "user": "tomhu",
        "type": "user"
      },
      "summary": "APR (Automated Program Repair) aims to automatically locate program defects,\ngenerate patches and validate the repairs. Existing techniques for APR are\noften combined with LLMs (Large Language Models), which leverages the\ncode-related knowledge of LLMs to improve repair effectiveness. Current\nLLM-based APR methods typically utilize test cases only during the inference\nstage, adopting an iterative approach that performs repair first and validates\nit through test execution afterward. This conventional paradigm neglects two\nimportant aspects: the potential contribution of test cases in the training\nphase, and the possibility of leveraging testing prior to repair. To address\nthis, we propose Repair-R1, which introduces test cases into the model's\ntraining phase and shifts test generation to precede repair. The model is\nrequired to first generate discriminative test cases that can distinguish\ndefective behaviors, and then perform repair based on these tests. This enables\nthe model to better locate defects and understand the underlying causes of\ndefects, thereby improving repair effectiveness. We implement Repair-R1 with\nthree different backbone models, using RL (reinforcement learning) to\nco-optimize test generation and bug repair. Experimental results on four widely\nadopted benchmarks demonstrate the superiority of Repair-R1. Specially,\ncompared to vanilla models, Repair-R1 improves repair success rate by 2.68\\% to\n48.29\\%, test generation success rate by 16.38\\% to 53.28\\%, and test coverage\nby 0.78\\% to 53.96\\%. We publish the code and weights at\nhttps://github.com/Tomsawyerhu/APR-RL and\nhttps://huggingface.co/tomhu/Qwen3-4B-RL-5000-step.",
      "upvotes": 3,
      "discussionId": "688ae0f88b724c8c7187dd90",
      "githubRepo": "https://github.com/Tomsawyerhu/APR-RL",
      "ai_summary": "Repair-R1 enhances automated program repair by integrating test cases into the training phase and prioritizing test generation before repair, improving repair success, test generation success, and test coverage.",
      "ai_keywords": [
        "Automated Program Repair",
        "Large Language Models",
        "reinforcement learning",
        "test cases",
        "discriminative test cases",
        "bug repair",
        "repair success rate",
        "test generation success rate",
        "test coverage"
      ],
      "githubStars": 3
    },
    "publishedAt": "2025-07-30T13:24:05.000Z",
    "title": "Repair-R1: Better Test Before Repair",
    "summary": "APR (Automated Program Repair) aims to automatically locate program defects,\ngenerate patches and validate the repairs. Existing techniques for APR are\noften combined with LLMs (Large Language Models), which leverages the\ncode-related knowledge of LLMs to improve repair effectiveness. Current\nLLM-based APR methods typically utilize test cases only during the inference\nstage, adopting an iterative approach that performs repair first and validates\nit through test execution afterward. This conventional paradigm neglects two\nimportant aspects: the potential contribution of test cases in the training\nphase, and the possibility of leveraging testing prior to repair. To address\nthis, we propose Repair-R1, which introduces test cases into the model's\ntraining phase and shifts test generation to precede repair. The model is\nrequired to first generate discriminative test cases that can distinguish\ndefective behaviors, and then perform repair based on these tests. This enables\nthe model to better locate defects and understand the underlying causes of\ndefects, thereby improving repair effectiveness. We implement Repair-R1 with\nthree different backbone models, using RL (reinforcement learning) to\nco-optimize test generation and bug repair. Experimental results on four widely\nadopted benchmarks demonstrate the superiority of Repair-R1. Specially,\ncompared to vanilla models, Repair-R1 improves repair success rate by 2.68\\% to\n48.29\\%, test generation success rate by 16.38\\% to 53.28\\%, and test coverage\nby 0.78\\% to 53.96\\%. We publish the code and weights at\nhttps://github.com/Tomsawyerhu/APR-RL and\nhttps://huggingface.co/tomhu/Qwen3-4B-RL-5000-step.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.22853.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "630341a4ef6f432d84e97db9",
      "avatarUrl": "/avatars/dc9910b5b9053123bad5ff848f677e12.svg",
      "fullname": "tomsawyer",
      "name": "tomhu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.22565",
      "authors": [
        {
          "_id": "688b0f7c8b724c8c7187de5f",
          "name": "Afshin Khadangi",
          "hidden": false
        },
        {
          "_id": "688b0f7c8b724c8c7187de60",
          "name": "Amir Sartipi",
          "hidden": false
        },
        {
          "_id": "688b0f7c8b724c8c7187de61",
          "name": "Igor Tchappi",
          "hidden": false
        },
        {
          "_id": "688b0f7c8b724c8c7187de62",
          "name": "Ramin Bahmani",
          "hidden": false
        },
        {
          "_id": "688b0f7c8b724c8c7187de63",
          "name": "Gilbert Fridgen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-30T10:46:53.000Z",
      "submittedOnDailyAt": "2025-07-31T05:12:37.542Z",
      "title": "Efficient Differentially Private Fine-Tuning of LLMs via Reinforcement\n  Learning",
      "submittedOnDailyBy": {
        "_id": "632292053007fcbf2932da22",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/632292053007fcbf2932da22/Cf4w-OunyQzHAJhyZsHTf.jpeg",
        "isPro": true,
        "fullname": "Afshin Khadangi",
        "user": "akhadangi",
        "type": "user"
      },
      "summary": "The tension between data privacy and model utility has become the defining\nbottleneck for the practical deployment of large language models (LLMs) trained\non sensitive corpora including healthcare. Differentially private stochastic\ngradient descent (DP-SGD) guarantees formal privacy, yet it does so at a\npronounced cost: gradients are forcibly clipped and perturbed with noise,\ndegrading sample efficiency and final accuracy. Numerous variants have been\nproposed to soften this trade-off, but they all share a handicap: their control\nknobs are hard-coded, global, and oblivious to the evolving optimization\nlandscape. Consequently, practitioners are forced either to over-spend privacy\nbudget in pursuit of utility, or to accept mediocre models in order to stay\nwithin privacy constraints. We present RLDP, the first framework to cast DP\noptimization itself as a closed-loop control problem amenable to modern deep\nreinforcement learning (RL). RLDP continuously senses rich statistics of the\nlearning dynamics and acts by selecting fine-grained per parameter\ngradient-clipping thresholds as well as the magnitude of injected Gaussian\nnoise. A soft actor-critic (SAC) hyper-policy is trained online during language\nmodel fine-tuning; it learns, from scratch, how to allocate the privacy budget\nwhere it matters and when it matters. Across more than 1,600 ablation\nexperiments on GPT2-small, Llama-1B, Llama-3B, and Mistral-7B, RLDP delivers\nperplexity reductions of 1.3-30.5% (mean 5.4%) and an average 5.6% downstream\nutility gain. RLDP reaches each baseline's final utility after only 13-43% of\nthe gradient-update budget (mean speed-up 71%), all while honoring the same\n(epsilon, delta)-DP contract and exhibiting equal or lower susceptibility\nto membership-inference and canary-extraction attacks.",
      "upvotes": 0,
      "discussionId": "688b0f7d8b724c8c7187de64",
      "projectPage": "https://wandb.ai/afshin-khadangi-university-of-luxembourg/RLDP/reports/Efficient-Differentially-Private-Fine-Tuning-of-LLMs-via-Reinforcement-Learning--VmlldzoxMzc4NTEwMA?accessToken=qhs4n7sh3o93yql2wprb2vylpmer07r2bjvzft7gty5mhhplt3numljxppfd8z66",
      "githubRepo": "https://github.com/akhadangi/RLDP",
      "ai_summary": "RLDP, a deep reinforcement learning framework, optimizes differentially private training by dynamically adjusting gradient clipping and noise, enhancing model utility and speed while maintaining privacy.",
      "ai_keywords": [
        "differentially private stochastic gradient descent",
        "DP-SGD",
        "deep reinforcement learning",
        "RL",
        "soft actor-critic",
        "SAC",
        "gradient-clipping thresholds",
        "Gaussian noise",
        "($\\epsilon$",
        "$\\delta$)-DP",
        "membership-inference",
        "canary-extraction attacks"
      ],
      "githubStars": 0
    },
    "publishedAt": "2025-07-30T06:46:53.000Z",
    "title": "Efficient Differentially Private Fine-Tuning of LLMs via Reinforcement\n  Learning",
    "summary": "The tension between data privacy and model utility has become the defining\nbottleneck for the practical deployment of large language models (LLMs) trained\non sensitive corpora including healthcare. Differentially private stochastic\ngradient descent (DP-SGD) guarantees formal privacy, yet it does so at a\npronounced cost: gradients are forcibly clipped and perturbed with noise,\ndegrading sample efficiency and final accuracy. Numerous variants have been\nproposed to soften this trade-off, but they all share a handicap: their control\nknobs are hard-coded, global, and oblivious to the evolving optimization\nlandscape. Consequently, practitioners are forced either to over-spend privacy\nbudget in pursuit of utility, or to accept mediocre models in order to stay\nwithin privacy constraints. We present RLDP, the first framework to cast DP\noptimization itself as a closed-loop control problem amenable to modern deep\nreinforcement learning (RL). RLDP continuously senses rich statistics of the\nlearning dynamics and acts by selecting fine-grained per parameter\ngradient-clipping thresholds as well as the magnitude of injected Gaussian\nnoise. A soft actor-critic (SAC) hyper-policy is trained online during language\nmodel fine-tuning; it learns, from scratch, how to allocate the privacy budget\nwhere it matters and when it matters. Across more than 1,600 ablation\nexperiments on GPT2-small, Llama-1B, Llama-3B, and Mistral-7B, RLDP delivers\nperplexity reductions of 1.3-30.5% (mean 5.4%) and an average 5.6% downstream\nutility gain. RLDP reaches each baseline's final utility after only 13-43% of\nthe gradient-update budget (mean speed-up 71%), all while honoring the same\n(epsilon, delta)-DP contract and exhibiting equal or lower susceptibility\nto membership-inference and canary-extraction attacks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.22565.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "632292053007fcbf2932da22",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/632292053007fcbf2932da22/Cf4w-OunyQzHAJhyZsHTf.jpeg",
      "fullname": "Afshin Khadangi",
      "name": "akhadangi",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]