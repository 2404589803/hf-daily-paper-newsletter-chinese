[
    "{'paper': {'id': '2412.15119', 'authors': [{'_id': '6764fd9b10330426aecdded7', 'user': {'_id': '63ea23b9dedfeebe54d02bdf', 'avatarUrl': '/avatars/4d9f9a546aa8c63e277161ea700075c4.svg', 'isPro': False, 'fullname': 'Yuqing Wang', 'user': 'Epiphqny', 'type': 'user'}, 'name': 'Yuqing Wang', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-23T11:11:21.169Z', 'hidden': False}, {'_id': '6764fd9b10330426aecdded8', 'user': {'_id': '60d2e681b8448e1785bbda06', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1624434302056-noauth.jpeg', 'isPro': False, 'fullname': 'Shuhuai Ren', 'user': 'ShuhuaiRen', 'type': 'user'}, 'name': 'Shuhuai Ren', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-23T11:11:23.352Z', 'hidden': False}, {'_id': '6764fd9b10330426aecdded9', 'user': {'_id': '64415957bd0c9726529802f6', 'avatarUrl': '/avatars/1132d1ee68fb58ec635d57c8175caacd.svg', 'isPro': False, 'fullname': 'Zhijie Lin', 'user': 'Ikuinen', 'type': 'user'}, 'name': 'Zhijie Lin', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-23T11:30:34.250Z', 'hidden': False}, {'_id': '6764fd9b10330426aecddeda', 'name': 'Yujin Han', 'hidden': False}, {'_id': '6764fd9b10330426aecddedb', 'name': 'Haoyuan Guo', 'hidden': False}, {'_id': '6764fd9b10330426aecddedc', 'user': {'_id': '6421183b69a2c2933882d652', 'avatarUrl': '/avatars/66813a8fa22915087cccd4dbfb945ca7.svg', 'isPro': False, 'fullname': 'Zhenheng Yang', 'user': 'zhenheny', 'type': 'user'}, 'name': 'Zhenheng Yang', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-23T11:24:14.038Z', 'hidden': False}, {'_id': '6764fd9b10330426aecddedd', 'name': 'Difan Zou', 'hidden': False}, {'_id': '6764fd9b10330426aecddede', 'user': {'_id': '67298e44017b96a1d0101dc4', 'avatarUrl': '/avatars/1f8ed1a3e911e6a3021087b9371d284c.svg', 'isPro': False, 'fullname': 'Jiashi Feng', 'user': 'jshfeng', 'type': 'user'}, 'name': 'Jiashi Feng', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-23T11:23:58.910Z', 'hidden': False}, {'_id': '6764fd9b10330426aecddedf', 'user': {'_id': '65d5ec74cd05bc1eaa125040', 'avatarUrl': '/avatars/2de1b1539a86452c2c89570eeb02f5ab.svg', 'isPro': False, 'fullname': 'Xihui Liu', 'user': 'XihuiLiu', 'type': 'user'}, 'name': 'Xihui Liu', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-23T11:11:19.211Z', 'hidden': False}], 'publishedAt': '2024-12-19T17:59:54.000Z', 'title': 'Parallelized Autoregressive Visual Generation', 'summary': 'Autoregressive models have emerged as a powerful approach for visual\\ngeneration but suffer from slow inference speed due to their sequential\\ntoken-by-token prediction process. In this paper, we propose a simple yet\\neffective approach for parallelized autoregressive visual generation that\\nimproves generation efficiency while preserving the advantages of\\nautoregressive modeling. Our key insight is that parallel generation depends on\\nvisual token dependencies-tokens with weak dependencies can be generated in\\nparallel, while strongly dependent adjacent tokens are difficult to generate\\ntogether, as their independent sampling may lead to inconsistencies. Based on\\nthis observation, we develop a parallel generation strategy that generates\\ndistant tokens with weak dependencies in parallel while maintaining sequential\\ngeneration for strongly dependent local tokens. Our approach can be seamlessly\\nintegrated into standard autoregressive models without modifying the\\narchitecture or tokenizer. Experiments on ImageNet and UCF-101 demonstrate that\\nour method achieves a 3.6x speedup with comparable quality and up to 9.5x\\nspeedup with minimal quality degradation across both image and video generation\\ntasks. We hope this work will inspire future research in efficient visual\\ngeneration and unified autoregressive modeling. Project page:\\nhttps://epiphqny.github.io/PAR-project.', 'upvotes': 30, 'discussionId': '6764fda210330426aecde36f'}, 'publishedAt': '2024-12-22T22:11:03.096Z', 'title': 'Parallelized Autoregressive Visual Generation', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/63ea23b9dedfeebe54d02bdf/VcujHexGXpqTL8TZjz9kv.mp4'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.15119.png', 'numComments': 1, 'submittedBy': {'_id': '63ea23b9dedfeebe54d02bdf', 'avatarUrl': '/avatars/4d9f9a546aa8c63e277161ea700075c4.svg', 'fullname': 'Yuqing Wang', 'name': 'Epiphqny', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2412.16145', 'authors': [{'_id': '6768f050bf7c0f8d9a17c4f2', 'user': {'_id': '6310664063b70252b4779150', 'avatarUrl': '/avatars/d514270c57b6ac494e0a419d792a72e5.svg', 'isPro': False, 'fullname': 'Huaijie Wang', 'user': 'jwhj', 'type': 'user'}, 'name': 'Huaijie Wang', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-23T11:31:56.207Z', 'hidden': False}, {'_id': '6768f050bf7c0f8d9a17c4f3', 'user': {'_id': '660ee5df35d092e3fc2a3685', 'avatarUrl': '/avatars/a7e0472fb7ea49973f74e3eea13dc964.svg', 'isPro': False, 'fullname': 'Shibo Hao', 'user': 'Shibo-UCSD', 'type': 'user'}, 'name': 'Shibo Hao', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-23T11:32:02.176Z', 'hidden': False}, {'_id': '6768f050bf7c0f8d9a17c4f4', 'user': {'_id': '63a3ff69f91ad3ea5703841d', 'avatarUrl': '/avatars/69227c4bce01d33747c1377b6f9672db.svg', 'isPro': False, 'fullname': 'Hanze Dong', 'user': 'hendrydong', 'type': 'user'}, 'name': 'Hanze Dong', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-23T11:32:07.779Z', 'hidden': False}, {'_id': '6768f050bf7c0f8d9a17c4f5', 'user': {'_id': '661213f894e0b3bff3e80c69', 'avatarUrl': '/avatars/d8febbb081825bf91e487aa8bad3a391.svg', 'isPro': False, 'fullname': 'Shenao Zhang', 'user': 'ZhangShenao', 'type': 'user'}, 'name': 'Shenao Zhang', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-23T11:09:50.829Z', 'hidden': False}, {'_id': '6768f050bf7c0f8d9a17c4f6', 'name': 'Yilin Bao', 'hidden': False}, {'_id': '6768f050bf7c0f8d9a17c4f7', 'name': 'Ziran Yang', 'hidden': False}, {'_id': '6768f050bf7c0f8d9a17c4f8', 'user': {'_id': '62c88b04ab9c23f5c459ed90', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/62c88b04ab9c23f5c459ed90/tEaeuKpXdXwqK-zq1H-8a.png', 'isPro': False, 'fullname': 'Yi Wu', 'user': 'yiwu', 'type': 'user'}, 'name': 'Yi Wu', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-23T11:32:27.882Z', 'hidden': False}], 'publishedAt': '2024-12-20T18:49:45.000Z', 'title': 'Offline Reinforcement Learning for LLM Multi-Step Reasoning', 'summary': 'Improving the multi-step reasoning ability of large language models (LLMs)\\nwith offline reinforcement learning (RL) is essential for quickly adapting them\\nto complex tasks. While Direct Preference Optimization (DPO) has shown promise\\nin aligning LLMs with human preferences, it is less suitable for multi-step\\nreasoning tasks because (1) DPO relies on paired preference data, which is not\\nreadily available for multi-step reasoning tasks, and (2) it treats all tokens\\nuniformly, making it ineffective for credit assignment in multi-step reasoning\\ntasks, which often come with sparse reward. In this work, we propose OREO\\n(Offline Reasoning Optimization), an offline RL method for enhancing LLM\\nmulti-step reasoning. Building on insights from previous works of maximum\\nentropy reinforcement learning, it jointly learns a policy model and value\\nfunction by optimizing the soft Bellman Equation. We show in principle that it\\nreduces the need to collect pairwise data and enables better credit assignment.\\nEmpirically, OREO surpasses existing offline learning methods on multi-step\\nreasoning benchmarks, including mathematical reasoning tasks (GSM8K, MATH) and\\nembodied agent control (ALFWorld). The approach can be extended to a\\nmulti-iteration framework when additional resources are available. Furthermore,\\nthe learned value function can be leveraged to guide the tree search for free,\\nwhich can further boost performance during test time.', 'upvotes': 16, 'discussionId': '6768f051bf7c0f8d9a17c53a'}, 'publishedAt': '2024-12-23T00:09:09.595Z', 'title': 'Offline Reinforcement Learning for LLM Multi-Step Reasoning', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.16145.png', 'numComments': 1, 'submittedBy': {'_id': '660ee5df35d092e3fc2a3685', 'avatarUrl': '/avatars/a7e0472fb7ea49973f74e3eea13dc964.svg', 'fullname': 'Shibo Hao', 'name': 'Shibo-UCSD', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 2}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2412.13649', 'authors': [{'_id': '6768cd61aa9027defefa2ad4', 'user': {'_id': '644a4fbc2166258fccc664bc', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg', 'isPro': False, 'fullname': 'Jialong Wu', 'user': 'callanwu', 'type': 'user'}, 'name': 'Jialong Wu', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-23T11:10:00.992Z', 'hidden': False}, {'_id': '6768cd61aa9027defefa2ad5', 'name': 'Zhenglin Wang', 'hidden': False}, {'_id': '6768cd61aa9027defefa2ad6', 'user': {'_id': '66596d64ce1b2838888f4401', 'avatarUrl': '/avatars/d8d0d116a3198571c7e86f09871c2d76.svg', 'isPro': False, 'fullname': 'Linhai Zhang', 'user': 'lzhang472', 'type': 'user'}, 'name': 'Linhai Zhang', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-23T11:30:58.318Z', 'hidden': False}, {'_id': '6768cd61aa9027defefa2ad7', 'name': 'Yilong Lai', 'hidden': False}, {'_id': '6768cd61aa9027defefa2ad8', 'name': 'Yulan He', 'hidden': False}, {'_id': '6768cd61aa9027defefa2ad9', 'user': {'_id': '64e821f2bddc5b1072b15c2e', 'avatarUrl': '/avatars/618b5a48f2fa62daff4e1922a9aa9e8b.svg', 'isPro': False, 'fullname': 'zhoudeyu', 'user': 'zhoudeyu', 'type': 'user'}, 'name': 'Deyu Zhou', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-23T11:31:24.902Z', 'hidden': False}], 'publishedAt': '2024-12-18T09:27:33.000Z', 'title': 'SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation', 'summary': 'Key-Value (KV) cache has become a bottleneck of LLMs for long-context\\ngeneration. Despite the numerous efforts in this area, the optimization for the\\ndecoding phase is generally ignored. However, we believe such optimization is\\ncrucial, especially for long-output generation tasks based on the following two\\nobservations: (i) Excessive compression during the prefill phase, which\\nrequires specific full context impairs the comprehension of the reasoning task;\\n(ii) Deviation of heavy hitters occurs in the reasoning tasks with long\\noutputs. Therefore, SCOPE, a simple yet efficient framework that separately\\nperforms KV cache optimization during the prefill and decoding phases, is\\nintroduced. Specifically, the KV cache during the prefill phase is preserved to\\nmaintain the essential information, while a novel strategy based on sliding is\\nproposed to select essential heavy hitters for the decoding phase. Memory usage\\nand memory transfer are further optimized using adaptive and discontinuous\\nstrategies. Extensive experiments on LongGenBench show the effectiveness and\\ngeneralization of SCOPE and its compatibility as a plug-in to other\\nprefill-only KV compression methods.', 'upvotes': 15, 'discussionId': '6768cd62aa9027defefa2b1b'}, 'publishedAt': '2024-12-22T21:39:56.930Z', 'title': 'SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.13649.png', 'numComments': 2, 'submittedBy': {'_id': '644a4fbc2166258fccc664bc', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg', 'fullname': 'Jialong Wu', 'name': 'callanwu', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 2}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2412.16112', 'authors': [{'_id': '6768d7f797a8f966b3362aa6', 'user': {'_id': '642ad435a096201096ecc580', 'avatarUrl': '/avatars/35d2c8d6615d8ed8ad6d695e5d748ab2.svg', 'isPro': False, 'fullname': 'Songhua Liu', 'user': 'flyingman', 'type': 'user'}, 'name': 'Songhua Liu', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-23T11:32:39.114Z', 'hidden': False}, {'_id': '6768d7f797a8f966b3362aa7', 'user': {'_id': '674e743be91289226ef9e857', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/FnLwJTTeItGbAUg4IPr7l.jpeg', 'isPro': False, 'fullname': '唐振雄', 'user': 'ZhenxiongTang', 'type': 'user'}, 'name': 'Zhenxiong Tan', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-23T11:32:44.224Z', 'hidden': False}, {'_id': '6768d7f797a8f966b3362aa8', 'user': {'_id': '63fc03a50aab060792ffef39', 'avatarUrl': '/avatars/9d5b1bb2a41928e08176b703935133ab.svg', 'isPro': False, 'fullname': 'Wangxinchao', 'user': 'wxcTest', 'type': 'user'}, 'name': 'Xinchao Wang', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-23T11:33:21.470Z', 'hidden': False}], 'publishedAt': '2024-12-20T17:57:09.000Z', 'title': 'CLEAR: Conv-Like Linearization Revs Pre-Trained Diffusion Transformers\\n  Up', 'summary': 'Diffusion Transformers (DiT) have become a leading architecture in image\\ngeneration. However, the quadratic complexity of attention mechanisms, which\\nare responsible for modeling token-wise relationships, results in significant\\nlatency when generating high-resolution images. To address this issue, we aim\\nat a linear attention mechanism in this paper that reduces the complexity of\\npre-trained DiTs to linear. We begin our exploration with a comprehensive\\nsummary of existing efficient attention mechanisms and identify four key\\nfactors crucial for successful linearization of pre-trained DiTs: locality,\\nformulation consistency, high-rank attention maps, and feature integrity. Based\\non these insights, we introduce a convolution-like local attention strategy\\ntermed CLEAR, which limits feature interactions to a local window around each\\nquery token, and thus achieves linear complexity. Our experiments indicate\\nthat, by fine-tuning the attention layer on merely 10K self-generated samples\\nfor 10K iterations, we can effectively transfer knowledge from a pre-trained\\nDiT to a student model with linear complexity, yielding results comparable to\\nthe teacher model. Simultaneously, it reduces attention computations by 99.5%\\nand accelerates generation by 6.3 times for generating 8K-resolution images.\\nFurthermore, we investigate favorable properties in the distilled attention\\nlayers, such as zero-shot generalization cross various models and plugins, and\\nimproved support for multi-GPU parallel inference. Models and codes are\\navailable here: https://github.com/Huage001/CLEAR.', 'upvotes': 11, 'discussionId': '6768d7fa97a8f966b3362bcf'}, 'publishedAt': '2024-12-22T22:25:16.875Z', 'title': 'CLEAR: Conv-Like Linearization Revs Pre-Trained Diffusion Transformers Up', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.16112.png', 'numComments': 1, 'submittedBy': {'_id': '64b929308b53fb5dbd059ce3', 'avatarUrl': '/avatars/564c44cf45db794747a96f79f30ecd91.svg', 'fullname': 'Liu Songhua', 'name': 'Huage001', 'type': 'user', 'isPro': True, 'isHf': False, 'isMod': False, 'followerCount': 6}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2412.15322', 'authors': [{'_id': '6768d18826eb881162077014', 'user': {'_id': '63041b541dd5d3c62486c294', 'avatarUrl': '/avatars/a5286d562f7b9082730f760e66c3bf29.svg', 'isPro': True, 'fullname': 'Ho Kei Cheng', 'user': 'hkchengrex', 'type': 'user'}, 'name': 'Ho Kei Cheng', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-23T11:09:58.995Z', 'hidden': False}, {'_id': '6768d18826eb881162077015', 'user': {'_id': '674545617e0ea169b0c471d1', 'avatarUrl': '/avatars/a422e3efa5fb1c3f2c6c0997c412b088.svg', 'isPro': False, 'fullname': 'Masato Ishii', 'user': 'mi141', 'type': 'user'}, 'name': 'Masato Ishii', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-23T11:33:45.656Z', 'hidden': False}, {'_id': '6768d18826eb881162077016', 'name': 'Akio Hayakawa', 'hidden': False}, {'_id': '6768d18826eb881162077017', 'user': {'_id': '6650773ca6acfdd2aba7d486', 'avatarUrl': '/avatars/d297886ea60dbff98a043caf825820ed.svg', 'isPro': False, 'fullname': 'Takashi Shibuya', 'user': 'TakashiShibuyaSony', 'type': 'user'}, 'name': 'Takashi Shibuya', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-23T11:09:56.433Z', 'hidden': False}, {'_id': '6768d18826eb881162077018', 'name': 'Alexander Schwing', 'hidden': False}, {'_id': '6768d18826eb881162077019', 'user': {'_id': '665e32384ecc8a7181634f6d', 'avatarUrl': '/avatars/8752f952010540d14f45eac849e91371.svg', 'isPro': False, 'fullname': 'Yuki Mitsufuji', 'user': 'mittu1204', 'type': 'user'}, 'name': 'Yuki Mitsufuji', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-23T11:34:10.186Z', 'hidden': False}], 'publishedAt': '2024-12-19T18:59:55.000Z', 'title': 'Taming Multimodal Joint Training for High-Quality Video-to-Audio\\n  Synthesis', 'summary': 'We propose to synthesize high-quality and synchronized audio, given video and\\noptional text conditions, using a novel multimodal joint training framework\\nMMAudio. In contrast to single-modality training conditioned on (limited) video\\ndata only, MMAudio is jointly trained with larger-scale, readily available\\ntext-audio data to learn to generate semantically aligned high-quality audio\\nsamples. Additionally, we improve audio-visual synchrony with a conditional\\nsynchronization module that aligns video conditions with audio latents at the\\nframe level. Trained with a flow matching objective, MMAudio achieves new\\nvideo-to-audio state-of-the-art among public models in terms of audio quality,\\nsemantic alignment, and audio-visual synchronization, while having a low\\ninference time (1.23s to generate an 8s clip) and just 157M parameters. MMAudio\\nalso achieves surprisingly competitive performance in text-to-audio generation,\\nshowing that joint training does not hinder single-modality performance. Code\\nand demo are available at: https://hkchengrex.github.io/MMAudio', 'upvotes': 8, 'discussionId': '6768d18926eb881162077079'}, 'publishedAt': '2024-12-22T21:59:13.541Z', 'title': 'Taming Multimodal Joint Training for High-Quality Video-to-Audio Synthesis', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/63041b541dd5d3c62486c294/Isw6SS_03NoIJoDxGgUmd.mp4'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.15322.png', 'numComments': 1, 'submittedBy': {'_id': '63041b541dd5d3c62486c294', 'avatarUrl': '/avatars/a5286d562f7b9082730f760e66c3bf29.svg', 'fullname': 'Ho Kei Cheng', 'name': 'hkchengrex', 'type': 'user', 'isPro': True, 'isHf': False, 'isMod': False, 'followerCount': 11}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2412.11525', 'authors': [{'_id': '676687cb5b17ac358c9ff22b', 'user': {'_id': '6742e770459000b812f3a276', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/nsEgzIm_-mPQXgYXXh05k.png', 'isPro': False, 'fullname': 'Lani Ko', 'user': 'lanikoisgod', 'type': 'user'}, 'name': 'Hyun-kyu Ko', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-21T15:19:33.358Z', 'hidden': False}, {'_id': '676687cb5b17ac358c9ff22c', 'user': {'_id': '645c7f5a7848314a460e0b69', 'avatarUrl': '/avatars/41eb5620dcfdbacf48b1345a987f8d8f.svg', 'isPro': False, 'fullname': 'Dongheok Park', 'user': 'HEOK', 'type': 'user'}, 'name': 'Dongheok Park', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-21T15:19:35.720Z', 'hidden': False}, {'_id': '676687cb5b17ac358c9ff22d', 'user': {'_id': '64816a98cb64a2b85a460aa3', 'avatarUrl': '/avatars/7a36f54647c9424e37579b56dc6b35d1.svg', 'isPro': False, 'fullname': 'Youngin Park', 'user': 'yi0109-park', 'type': 'user'}, 'name': 'Youngin Park', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-23T11:34:45.160Z', 'hidden': False}, {'_id': '676687cb5b17ac358c9ff22e', 'user': {'_id': '614c218e7e1c37ffc55c7510', 'avatarUrl': '/avatars/f86636fceb97480a73b7ca7b5e20d8f5.svg', 'isPro': False, 'fullname': 'Byeonghyeon Lee', 'user': 'blee', 'type': 'user'}, 'name': 'Byeonghyeon Lee', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-23T11:34:51.729Z', 'hidden': False}, {'_id': '676687cb5b17ac358c9ff22f', 'user': {'_id': '66036d0530394deb4ae1defd', 'avatarUrl': '/avatars/0037ccccb1dccb03d634e3651952fe3e.svg', 'isPro': False, 'fullname': 'Juhee Han', 'user': 'juxhee', 'type': 'user'}, 'name': 'Juhee Han', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-23T11:34:57.023Z', 'hidden': False}, {'_id': '676687cb5b17ac358c9ff230', 'user': {'_id': '655e0141d36a195f663ee4b0', 'avatarUrl': '/avatars/64609d6fd1581e6cf3e057ee569d69a1.svg', 'isPro': False, 'fullname': 'Eunbyung Park', 'user': 'epark', 'type': 'user'}, 'name': 'Eunbyung Park', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-23T11:35:02.518Z', 'hidden': False}], 'publishedAt': '2024-12-16T08:00:50.000Z', 'title': 'Sequence Matters: Harnessing Video Models in 3D Super-Resolution', 'summary': \"3D super-resolution aims to reconstruct high-fidelity 3D models from\\nlow-resolution (LR) multi-view images. Early studies primarily focused on\\nsingle-image super-resolution (SISR) models to upsample LR images into\\nhigh-resolution images. However, these methods often lack view consistency\\nbecause they operate independently on each image. Although various\\npost-processing techniques have been extensively explored to mitigate these\\ninconsistencies, they have yet to fully resolve the issues. In this paper, we\\nperform a comprehensive study of 3D super-resolution by leveraging video\\nsuper-resolution (VSR) models. By utilizing VSR models, we ensure a higher\\ndegree of spatial consistency and can reference surrounding spatial\\ninformation, leading to more accurate and detailed reconstructions. Our\\nfindings reveal that VSR models can perform remarkably well even on sequences\\nthat lack precise spatial alignment. Given this observation, we propose a\\nsimple yet practical approach to align LR images without involving fine-tuning\\nor generating 'smooth' trajectory from the trained 3D models over LR images.\\nThe experimental results show that the surprisingly simple algorithms can\\nachieve the state-of-the-art results of 3D super-resolution tasks on standard\\nbenchmark datasets, such as the NeRF-synthetic and MipNeRF-360 datasets.\\nProject page: https://ko-lani.github.io/Sequence-Matters\", 'upvotes': 4, 'discussionId': '676687cc5b17ac358c9ff2b9'}, 'publishedAt': '2024-12-23T04:21:04.641Z', 'title': 'Sequence Matters: Harnessing Video Models in 3D Super-Resolution', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.11525.png', 'numComments': 1, 'submittedBy': {'_id': '6742e770459000b812f3a276', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/nsEgzIm_-mPQXgYXXh05k.png', 'fullname': 'Lani Ko', 'name': 'lanikoisgod', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2412.14590', 'authors': [{'_id': '676827d8bc5af30a79857669', 'user': {'_id': '65373b2c89dd48faca859d02', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/65373b2c89dd48faca859d02/42HukqnvMykvaoTxnQJjk.jpeg', 'isPro': False, 'fullname': 'Zhen Zheng', 'user': 'JamesTheZ', 'type': 'user'}, 'name': 'Zhen Zheng', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-23T11:10:47.069Z', 'hidden': False}, {'_id': '676827d8bc5af30a7985766a', 'user': {'_id': '66839c29d88415938469b483', 'avatarUrl': '/avatars/38af0856922ea53f607a7158e640923c.svg', 'isPro': False, 'fullname': 'Xiaonan Song', 'user': 'xiaonans', 'type': 'user'}, 'name': 'Xiaonan Song', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-23T11:34:23.206Z', 'hidden': False}, {'_id': '676827d8bc5af30a7985766b', 'user': {'_id': '6528b76d0b44378dcc54ec60', 'avatarUrl': '/avatars/0943b4c7def02c1751e163aa758ccec5.svg', 'isPro': False, 'fullname': 'Chuanjie Liu', 'user': 'chuanjieliu', 'type': 'user'}, 'name': 'Chuanjie Liu', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-23T11:34:28.717Z', 'hidden': False}], 'publishedAt': '2024-12-19T07:15:15.000Z', 'title': 'MixLLM: LLM Quantization with Global Mixed-precision between\\n  Output-features and Highly-efficient System Design', 'summary': 'Quantization has become one of the most effective methodologies to compress\\nLLMs into smaller size. However, the existing quantization solutions still show\\nlimitations of either non-negligible accuracy drop or system inefficiency. In\\nthis paper, we make a comprehensive analysis of the general quantization\\nprinciples on their effect to the triangle of accuracy, memory consumption and\\nsystem efficiency. We propose MixLLM that explores the new optimization space\\nof mixed-precision quantization between output features based on the insight\\nthat different output features matter differently in the model. MixLLM\\nidentifies the output features with high salience in the global view rather\\nthan within each single layer, effectively assigning the larger bit-width to\\noutput features that need it most to achieve good accuracy with low memory\\nconsumption. We present the sweet spot of quantization configuration of\\nalgorithm-system co-design that leads to high accuracy and system efficiency.\\nTo address the system challenge, we design the two-step dequantization to make\\nuse of the int8 Tensor Core easily and fast data type conversion to reduce\\ndequantization overhead significantly, and present the software pipeline to\\noverlap the memory access, dequantization and the MatMul to the best. Extensive\\nexperiments show that with only 10% more bits, the PPL increasement can be\\nreduced from about 0.5 in SOTA to within 0.2 for Llama 3.1 70B, while on\\naverage MMLU-Pro improves by 0.93 over the SOTA of three popular models. In\\naddition to its superior accuracy, MixLLM also achieves state-of-the-art system\\nefficiency.', 'upvotes': 4, 'discussionId': '676827d9bc5af30a798576c2'}, 'publishedAt': '2024-12-22T23:39:08.449Z', 'title': 'MixLLM: LLM Quantization with Global Mixed-precision between Output-features and Highly-efficient System Design', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.14590.png', 'numComments': 1, 'submittedBy': {'_id': '65373b2c89dd48faca859d02', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/65373b2c89dd48faca859d02/42HukqnvMykvaoTxnQJjk.jpeg', 'fullname': 'Zhen Zheng', 'name': 'JamesTheZ', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2412.15035', 'authors': [{'_id': '676931b8bc5af30a79d40a76', 'user': {'_id': '62e7dd4036a8e8a82700041c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/62e7dd4036a8e8a82700041c/Dgk9mXYLVd4LpiNLWjn-q.jpeg', 'isPro': False, 'fullname': 'Felix Friedrich', 'user': 'felfri', 'type': 'user'}, 'name': 'Felix Friedrich', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-23T11:35:28.332Z', 'hidden': False}, {'_id': '676931b8bc5af30a79d40a77', 'user': {'_id': '61b85aa99ba538c73a7dc78b', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/61b85aa99ba538c73a7dc78b/gWxtQAvOYn7cXgE_nAy0p.jpeg', 'isPro': False, 'fullname': 'Simone Tedeschi', 'user': 'sted97', 'type': 'user'}, 'name': 'Simone Tedeschi', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-23T11:09:30.612Z', 'hidden': False}, {'_id': '676931b8bc5af30a79d40a78', 'user': {'_id': '62d021a3dd7bdfc5e5c61c5c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/62d021a3dd7bdfc5e5c61c5c/bnQW2SqirfGaQmI84HW_c.jpeg', 'isPro': False, 'fullname': 'Patrick Schramowski', 'user': 'PSaiml', 'type': 'user'}, 'name': 'Patrick Schramowski', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-23T11:35:22.391Z', 'hidden': False}, {'_id': '676931b8bc5af30a79d40a79', 'user': {'_id': '62fa1d95e8c9c532aa75331c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/62fa1d95e8c9c532aa75331c/WFfk_n8gOj845pSkfdazA.jpeg', 'isPro': False, 'fullname': 'Manuel Brack', 'user': 'mbrack', 'type': 'user'}, 'name': 'Manuel Brack', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-23T11:35:34.064Z', 'hidden': False}, {'_id': '676931b8bc5af30a79d40a7a', 'user': {'_id': '63845c7c99292a80134bc784', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/63845c7c99292a80134bc784/eKOPJMjm8F-JUKT_mi7Dv.png', 'isPro': False, 'fullname': 'Roberto Navigli', 'user': 'navigli', 'type': 'user'}, 'name': 'Roberto Navigli', 'status': 'admin_assigned', 'statusLastChangedAt': '2024-12-23T11:35:40.306Z', 'hidden': False}, {'_id': '676931b8bc5af30a79d40a7b', 'name': 'Huu Nguyen', 'hidden': False}, {'_id': '676931b8bc5af30a79d40a7c', 'name': 'Bo Li', 'hidden': False}, {'_id': '676931b8bc5af30a79d40a7d', 'name': 'Kristian Kersting', 'hidden': False}], 'publishedAt': '2024-12-19T16:46:54.000Z', 'title': 'LLMs Lost in Translation: M-ALERT uncovers Cross-Linguistic Safety Gaps', 'summary': 'Building safe Large Language Models (LLMs) across multiple languages is\\nessential in ensuring both safe access and linguistic diversity. To this end,\\nwe introduce M-ALERT, a multilingual benchmark that evaluates the safety of\\nLLMs in five languages: English, French, German, Italian, and Spanish. M-ALERT\\nincludes 15k high-quality prompts per language, totaling 75k, following the\\ndetailed ALERT taxonomy. Our extensive experiments on 10 state-of-the-art LLMs\\nhighlight the importance of language-specific safety analysis, revealing that\\nmodels often exhibit significant inconsistencies in safety across languages and\\ncategories. For instance, Llama3.2 shows high unsafety in the category\\ncrime_tax for Italian but remains safe in other languages. Similar differences\\ncan be observed across all models. In contrast, certain categories, such as\\nsubstance_cannabis and crime_propaganda, consistently trigger unsafe responses\\nacross models and languages. These findings underscore the need for robust\\nmultilingual safety practices in LLMs to ensure safe and responsible usage\\nacross diverse user communities.', 'upvotes': 2, 'discussionId': '676931b9bc5af30a79d40ad2'}, 'publishedAt': '2024-12-23T04:48:38.485Z', 'title': 'LLMs Lost in Translation: M-ALERT uncovers Cross-Linguistic Safety Gaps', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.15035.png', 'numComments': 1, 'submittedBy': {'_id': '61b85aa99ba538c73a7dc78b', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/61b85aa99ba538c73a7dc78b/gWxtQAvOYn7cXgE_nAy0p.jpeg', 'fullname': 'Simone Tedeschi', 'name': 'sted97', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 31}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2412.15450', 'authors': [{'_id': '676911953727573d69c04ad6', 'user': {'_id': '5e1e17b6fcf41d740b6996a8', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1594192845975-5e1e17b6fcf41d740b6996a8.jpeg', 'isPro': True, 'fullname': 'Bram Vanroy', 'user': 'BramVanroy', 'type': 'user'}, 'name': 'Bram Vanroy', 'status': 'extracted_confirmed', 'statusLastChangedAt': '2024-12-23T09:14:08.298Z', 'hidden': False}], 'publishedAt': '2024-12-19T23:06:01.000Z', 'title': 'Fietje: An open, efficient LLM for Dutch', 'summary': 'This paper introduces Fietje, a family of small language models (SLMs)\\nspecifically designed for the Dutch language. The model is based on Phi 2, an\\nEnglish-centric model of 2.7 billion parameters. Fietje demonstrated\\ncompetitive results with larger language models upon its release. A core\\nemphasis of this work is transparency and reproducibility: Fietje is fully\\nopen-source, with model weights, datasets, training, and evaluation code all\\npublicly accessible.\\n  The paper discusses the performance of Fietje and many other models on an\\nextensive evaluation suite of benchmarks on reasoning, sentiment analysis,\\nworld knowledge, linguistic acceptability and word sense disambiguation.\\nEvaluation results illustrate the rapid progress in the field of LLMs, where\\nrecent small models outperform older, larger models that were fine-tuned for\\nDutch. This trend signals an exciting future for Dutch language processing,\\nsuggesting that even compact LLMs are becoming increasingly capable.\\nFurthermore, ongoing and future efforts to adapt LLMs to Dutch are poised to\\nenhance these models even further, broadening their applicability and\\naccessibility. Fietje is only an intermediate step in improving accessibility\\nto language technology for users of the Dutch language.', 'upvotes': 2, 'discussionId': '676911963727573d69c04b01'}, 'publishedAt': '2024-12-23T02:30:57.045Z', 'title': 'Fietje: An open, efficient LLM for Dutch', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.15450.png', 'numComments': 1, 'submittedBy': {'_id': '5e6a3d4ea9afd5125d9ec064', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1584020801691-noauth.jpeg', 'fullname': 'Stefan Schweter', 'name': 'stefan-it', 'type': 'user', 'isPro': True, 'isHf': False, 'isMod': False, 'followerCount': 1953}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2412.14963', 'authors': [{'_id': '67690222fbd79d33cf57d51d', 'user': {'_id': '65f83ad63545cc30502642ff', 'avatarUrl': '/avatars/07838b6b8a5e5f04a623c61548418252.svg', 'isPro': False, 'fullname': 'zzz', 'user': 'yiyuzzz', 'type': 'user'}, 'name': 'Yiyu Zhuang', 'status': 'claimed_verified', 'statusLastChangedAt': '2024-12-23T11:09:48.780Z', 'hidden': False}, {'_id': '67690222fbd79d33cf57d51e', 'name': 'Jiaxi Lv', 'hidden': False}, {'_id': '67690222fbd79d33cf57d51f', 'name': 'Hao Wen', 'hidden': False}, {'_id': '67690222fbd79d33cf57d520', 'name': 'Qing Shuai', 'hidden': False}, {'_id': '67690222fbd79d33cf57d521', 'name': 'Ailing Zeng', 'hidden': False}, {'_id': '67690222fbd79d33cf57d522', 'name': 'Hao Zhu', 'hidden': False}, {'_id': '67690222fbd79d33cf57d523', 'name': 'Shifeng Chen', 'hidden': False}, {'_id': '67690222fbd79d33cf57d524', 'name': 'Yujiu Yang', 'hidden': False}, {'_id': '67690222fbd79d33cf57d525', 'name': 'Xun Cao', 'hidden': False}, {'_id': '67690222fbd79d33cf57d526', 'name': 'Wei Liu', 'hidden': False}], 'publishedAt': '2024-12-19T15:43:05.000Z', 'title': 'IDOL: Instant Photorealistic 3D Human Creation from a Single Image', 'summary': 'Creating a high-fidelity, animatable 3D full-body avatar from a single image\\nis a challenging task due to the diverse appearance and poses of humans and the\\nlimited availability of high-quality training data. To achieve fast and\\nhigh-quality human reconstruction, this work rethinks the task from the\\nperspectives of dataset, model, and representation. First, we introduce a\\nlarge-scale HUman-centric GEnerated dataset, HuGe100K, consisting of 100K\\ndiverse, photorealistic sets of human images. Each set contains 24-view frames\\nin specific human poses, generated using a pose-controllable\\nimage-to-multi-view model. Next, leveraging the diversity in views, poses, and\\nappearances within HuGe100K, we develop a scalable feed-forward transformer\\nmodel to predict a 3D human Gaussian representation in a uniform space from a\\ngiven human image. This model is trained to disentangle human pose, body shape,\\nclothing geometry, and texture. The estimated Gaussians can be animated without\\npost-processing. We conduct comprehensive experiments to validate the\\neffectiveness of the proposed dataset and method. Our model demonstrates the\\nability to efficiently reconstruct photorealistic humans at 1K resolution from\\na single input image using a single GPU instantly. Additionally, it seamlessly\\nsupports various applications, as well as shape and texture editing tasks.', 'upvotes': 1, 'discussionId': '67690226fbd79d33cf57d65a'}, 'publishedAt': '2024-12-23T08:39:28.353Z', 'title': 'IDOL: Instant Photorealistic 3D Human Creation from a Single Image', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.14963.png', 'numComments': 1, 'submittedBy': {'_id': '65f83ad63545cc30502642ff', 'avatarUrl': '/avatars/07838b6b8a5e5f04a623c61548418252.svg', 'fullname': 'zzz', 'name': 'yiyuzzz', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2412.15487', 'authors': [{'_id': '676984a1edea1efd81017959', 'name': 'Jiangnan Fang', 'hidden': False}, {'_id': '676984a1edea1efd8101795a', 'name': 'Cheng-Tse Liu', 'hidden': False}, {'_id': '676984a1edea1efd8101795b', 'name': 'Jieun Kim', 'hidden': False}, {'_id': '676984a1edea1efd8101795c', 'name': 'Yash Bhedaru', 'hidden': False}, {'_id': '676984a1edea1efd8101795d', 'name': 'Ethan Liu', 'hidden': False}, {'_id': '676984a1edea1efd8101795e', 'name': 'Nikhil Singh', 'hidden': False}, {'_id': '676984a1edea1efd8101795f', 'name': 'Nedim Lipka', 'hidden': False}, {'_id': '676984a1edea1efd81017960', 'name': 'Puneet Mathur', 'hidden': False}, {'_id': '676984a1edea1efd81017961', 'name': 'Nesreen K. Ahmed', 'hidden': False}, {'_id': '676984a1edea1efd81017962', 'name': 'Franck Dernoncourt', 'hidden': False}, {'_id': '676984a1edea1efd81017963', 'name': 'Ryan A. Rossi', 'hidden': False}, {'_id': '676984a1edea1efd81017964', 'name': 'Hanieh Deilamsalehy', 'hidden': False}], 'publishedAt': '2024-12-20T01:55:26.000Z', 'title': 'Multi-LLM Text Summarization', 'summary': 'In this work, we propose a Multi-LLM summarization framework, and investigate\\ntwo different multi-LLM strategies including centralized and decentralized. Our\\nmulti-LLM summarization framework has two fundamentally important steps at each\\nround of conversation: generation and evaluation. These steps are different\\ndepending on whether our multi-LLM decentralized summarization is used or\\ncentralized. In both our multi-LLM decentralized and centralized strategies, we\\nhave k different LLMs that generate diverse summaries of the text. However,\\nduring evaluation, our multi-LLM centralized summarization approach leverages a\\nsingle LLM to evaluate the summaries and select the best one whereas k LLMs are\\nused for decentralized multi-LLM summarization. Overall, we find that our\\nmulti-LLM summarization approaches significantly outperform the baselines that\\nleverage only a single LLM by up to 3x. These results indicate the\\neffectiveness of multi-LLM approaches for summarization.', 'upvotes': 0, 'discussionId': '676984a2edea1efd810179ae'}, 'publishedAt': '2024-12-23T10:41:23.825Z', 'title': 'Multi-LLM Text Summarization', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.15487.png', 'numComments': 1, 'submittedBy': {'_id': '62c5947524171688a9feb992', 'avatarUrl': '/avatars/5a151713b9eae8dc566f5957acee3475.svg', 'fullname': 'Franck Dernoncourt', 'name': 'Franck-Dernoncourt', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 4}, 'isAuthorParticipating': False}"
]