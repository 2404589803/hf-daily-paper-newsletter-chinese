[
  {
    "paper": {
      "id": "2504.10481",
      "authors": [
        {
          "_id": "67fdc1b41d1bc292f7b9358e",
          "user": {
            "_id": "64e18e9ec20c27fcc8df384e",
            "avatarUrl": "/avatars/64ef866b9fa385efcefb34ea76b76802.svg",
            "isPro": false,
            "fullname": "Ding Chen",
            "user": "Hush-cd",
            "type": "user"
          },
          "name": "Ding Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-15T07:54:22.449Z",
          "hidden": false
        },
        {
          "_id": "67fdc1b41d1bc292f7b9358f",
          "name": "Qingchen Yu",
          "hidden": false
        },
        {
          "_id": "67fdc1b41d1bc292f7b93590",
          "name": "Pengyuan Wang",
          "hidden": false
        },
        {
          "_id": "67fdc1b41d1bc292f7b93591",
          "name": "Wentao Zhang",
          "hidden": false
        },
        {
          "_id": "67fdc1b41d1bc292f7b93592",
          "name": "Bo Tang",
          "hidden": false
        },
        {
          "_id": "67fdc1b41d1bc292f7b93593",
          "name": "Feiyu Xiong",
          "hidden": false
        },
        {
          "_id": "67fdc1b41d1bc292f7b93594",
          "name": "Xinchi Li",
          "hidden": false
        },
        {
          "_id": "67fdc1b41d1bc292f7b93595",
          "name": "Minchuan Yang",
          "hidden": false
        },
        {
          "_id": "67fdc1b41d1bc292f7b93596",
          "name": "Zhiyu Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T17:59:36.000Z",
      "submittedOnDailyAt": "2025-04-16T00:53:50.942Z",
      "title": "xVerify: Efficient Answer Verifier for Reasoning Model Evaluations",
      "submittedOnDailyBy": {
        "_id": "64e18e9ec20c27fcc8df384e",
        "avatarUrl": "/avatars/64ef866b9fa385efcefb34ea76b76802.svg",
        "isPro": false,
        "fullname": "Ding Chen",
        "user": "Hush-cd",
        "type": "user"
      },
      "summary": "With the release of the o1 model by OpenAI, reasoning models adopting slow\nthinking strategies have gradually emerged. As the responses generated by such\nmodels often include complex reasoning, intermediate steps, and\nself-reflection, existing evaluation methods are often inadequate. They\nstruggle to determine whether the LLM output is truly equivalent to the\nreference answer, and also have difficulty identifying and extracting the final\nanswer from long, complex responses. To address this issue, we propose xVerify,\nan efficient answer verifier for reasoning model evaluations. xVerify\ndemonstrates strong capability in equivalence judgment, enabling it to\neffectively determine whether the answers produced by reasoning models are\nequivalent to reference answers across various types of objective questions. To\ntrain and evaluate xVerify, we construct the VAR dataset by collecting\nquestion-answer pairs generated by multiple LLMs across various datasets,\nleveraging multiple reasoning models and challenging evaluation sets designed\nspecifically for reasoning model assessment. A multi-round annotation process\nis employed to ensure label accuracy. Based on the VAR dataset, we train\nmultiple xVerify models of different scales. In evaluation experiments\nconducted on both the test set and generalization set, all xVerify models\nachieve overall F1 scores and accuracy exceeding 95\\%. Notably, the smallest\nvariant, xVerify-0.5B-I, outperforms all evaluation methods except GPT-4o,\nwhile xVerify-3B-Ib surpasses GPT-4o in overall performance. These results\nvalidate the effectiveness and generalizability of xVerify.",
      "upvotes": 21,
      "discussionId": "67fdc1b51d1bc292f7b935e8",
      "githubRepo": "https://github.com/IAAR-Shanghai/xVerify",
      "ai_keywords": [
        "reasoning models",
        "o1 model",
        "slow thinking strategies",
        "complex reasoning",
        "intermediate steps",
        "self-reflection",
        "evaluation methods",
        "LLM output",
        "reference answer",
        "final answer",
        "xVerify",
        "equivalence judgment",
        "VAR dataset",
        "multi-round annotation process",
        "F1 scores",
        "xVerify-0.5B-I",
        "xVerify-3B-Ib",
        "GPT-4o"
      ]
    },
    "publishedAt": "2025-04-14T13:59:36.000Z",
    "title": "xVerify: Efficient Answer Verifier for Reasoning Model Evaluations",
    "summary": "With the release of the o1 model by OpenAI, reasoning models adopting slow\nthinking strategies have gradually emerged. As the responses generated by such\nmodels often include complex reasoning, intermediate steps, and\nself-reflection, existing evaluation methods are often inadequate. They\nstruggle to determine whether the LLM output is truly equivalent to the\nreference answer, and also have difficulty identifying and extracting the final\nanswer from long, complex responses. To address this issue, we propose xVerify,\nan efficient answer verifier for reasoning model evaluations. xVerify\ndemonstrates strong capability in equivalence judgment, enabling it to\neffectively determine whether the answers produced by reasoning models are\nequivalent to reference answers across various types of objective questions. To\ntrain and evaluate xVerify, we construct the VAR dataset by collecting\nquestion-answer pairs generated by multiple LLMs across various datasets,\nleveraging multiple reasoning models and challenging evaluation sets designed\nspecifically for reasoning model assessment. A multi-round annotation process\nis employed to ensure label accuracy. Based on the VAR dataset, we train\nmultiple xVerify models of different scales. In evaluation experiments\nconducted on both the test set and generalization set, all xVerify models\nachieve overall F1 scores and accuracy exceeding 95\\%. Notably, the smallest\nvariant, xVerify-0.5B-I, outperforms all evaluation methods except GPT-4o,\nwhile xVerify-3B-Ib surpasses GPT-4o in overall performance. These results\nvalidate the effectiveness and generalizability of xVerify.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10481.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64e18e9ec20c27fcc8df384e",
      "avatarUrl": "/avatars/64ef866b9fa385efcefb34ea76b76802.svg",
      "fullname": "Ding Chen",
      "name": "Hush-cd",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.10337",
      "authors": [
        {
          "_id": "67fddae99a03686367721718",
          "user": {
            "_id": "6471a24381ded91f253ceb1c",
            "avatarUrl": "/avatars/31d447068fe1fd4200ab5d08ab31eed4.svg",
            "isPro": false,
            "fullname": "Wesley Shi",
            "user": "WesleyShi",
            "type": "user"
          },
          "name": "Wenlei Shi",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-04-15T06:43:40.277Z",
          "hidden": false
        },
        {
          "_id": "67fddae99a03686367721719",
          "name": "Xing Jin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T15:46:33.000Z",
      "submittedOnDailyAt": "2025-04-16T00:52:23.733Z",
      "title": "Heimdall: test-time scaling on the generative verification",
      "submittedOnDailyBy": {
        "_id": "6471a24381ded91f253ceb1c",
        "avatarUrl": "/avatars/31d447068fe1fd4200ab5d08ab31eed4.svg",
        "isPro": false,
        "fullname": "Wesley Shi",
        "user": "WesleyShi",
        "type": "user"
      },
      "summary": "An AI system can create and maintain knowledge only to the extent that it can\nverify that knowledge itself. Recent work on long Chain-of-Thought reasoning\nhas demonstrated great potential of LLMs on solving competitive problems, but\ntheir verification ability remains to be weak and not sufficiently\ninvestigated. In this paper, we propose Heimdall, the long CoT verification LLM\nthat can accurately judge the correctness of solutions. With pure reinforcement\nlearning, we boost the verification accuracy from 62.5% to 94.5% on competitive\nmath problems. By scaling with repeated sampling, the accuracy further\nincreases to 97.5%. Through human evaluation, Heimdall demonstrates impressive\ngeneralization capabilities, successfully detecting most issues in challenging\nmath proofs, the type of which is not included during training. Furthermore, we\npropose Pessimistic Verification to extend the functionality of Heimdall to\nscaling up the problem solving. It calls Heimdall to judge the solutions from a\nsolver model and based on the pessimistic principle, selects the most likely\ncorrect solution with the least uncertainty. Taking\nDeepSeek-R1-Distill-Qwen-32B as the solver model, Pessimistic Verification\nimproves the solution accuracy on AIME2025 from 54.2% to 70.0% with 16x compute\nbudget and to 83.3% with more compute budget. With the stronger solver Gemini\n2.5 Pro, the score reaches 93.0%. Finally, we prototype an automatic knowledge\ndiscovery system, a ternary system where one poses questions, another provides\nsolutions, and the third verifies the solutions. Using the data synthesis work\nNuminaMath for the first two components, Heimdall effectively identifies\nproblematic records within the dataset and reveals that nearly half of the data\nis flawed, which interestingly aligns with the recent ablation studies from\nNuminaMath.",
      "upvotes": 18,
      "discussionId": "67fddaea9a03686367721776",
      "ai_keywords": [
        "Chain-of-Thought reasoning",
        "LLMs (Large Language Models)",
        "Heimdall",
        "long CoT verification",
        "pure reinforcement learning",
        "synthetic math problems",
        "human evaluation",
        "generalization capabilities",
        "Pessimistic Verification",
        "DeepSeek-R1-Distill-Qwen-32B",
        "AIME2025",
        "Gemini 2.5 Pro",
        "solution accuracy",
        "automatic knowledge discovery system",
        "ternary system",
        "NuminaMath",
        "data synthesis",
        "data records",
        "flawed data"
      ]
    },
    "publishedAt": "2025-04-14T11:46:33.000Z",
    "title": "Heimdall: test-time scaling on the generative verification",
    "summary": "An AI system can create and maintain knowledge only to the extent that it can\nverify that knowledge itself. Recent work on long Chain-of-Thought reasoning\nhas demonstrated great potential of LLMs on solving competitive problems, but\ntheir verification ability remains to be weak and not sufficiently\ninvestigated. In this paper, we propose Heimdall, the long CoT verification LLM\nthat can accurately judge the correctness of solutions. With pure reinforcement\nlearning, we boost the verification accuracy from 62.5% to 94.5% on competitive\nmath problems. By scaling with repeated sampling, the accuracy further\nincreases to 97.5%. Through human evaluation, Heimdall demonstrates impressive\ngeneralization capabilities, successfully detecting most issues in challenging\nmath proofs, the type of which is not included during training. Furthermore, we\npropose Pessimistic Verification to extend the functionality of Heimdall to\nscaling up the problem solving. It calls Heimdall to judge the solutions from a\nsolver model and based on the pessimistic principle, selects the most likely\ncorrect solution with the least uncertainty. Taking\nDeepSeek-R1-Distill-Qwen-32B as the solver model, Pessimistic Verification\nimproves the solution accuracy on AIME2025 from 54.2% to 70.0% with 16x compute\nbudget and to 83.3% with more compute budget. With the stronger solver Gemini\n2.5 Pro, the score reaches 93.0%. Finally, we prototype an automatic knowledge\ndiscovery system, a ternary system where one poses questions, another provides\nsolutions, and the third verifies the solutions. Using the data synthesis work\nNuminaMath for the first two components, Heimdall effectively identifies\nproblematic records within the dataset and reveals that nearly half of the data\nis flawed, which interestingly aligns with the recent ablation studies from\nNuminaMath.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10337.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6471a24381ded91f253ceb1c",
      "avatarUrl": "/avatars/31d447068fe1fd4200ab5d08ab31eed4.svg",
      "fullname": "Wesley Shi",
      "name": "WesleyShi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.10465",
      "authors": [
        {
          "_id": "67ff26c3414c03ebc1d42529",
          "name": "Tao Zhang",
          "hidden": false
        },
        {
          "_id": "67ff26c3414c03ebc1d4252a",
          "name": "Xiangtai Li",
          "hidden": false
        },
        {
          "_id": "67ff26c3414c03ebc1d4252b",
          "name": "Zilong Huang",
          "hidden": false
        },
        {
          "_id": "67ff26c3414c03ebc1d4252c",
          "name": "Yanwei Li",
          "hidden": false
        },
        {
          "_id": "67ff26c3414c03ebc1d4252d",
          "name": "Weixian Lei",
          "hidden": false
        },
        {
          "_id": "67ff26c3414c03ebc1d4252e",
          "name": "Xueqing Deng",
          "hidden": false
        },
        {
          "_id": "67ff26c3414c03ebc1d4252f",
          "name": "Shihao Chen",
          "hidden": false
        },
        {
          "_id": "67ff26c3414c03ebc1d42530",
          "name": "Shunping Ji",
          "hidden": false
        },
        {
          "_id": "67ff26c3414c03ebc1d42531",
          "name": "Jiashi Feng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T17:52:22.000Z",
      "submittedOnDailyAt": "2025-04-16T02:11:29.898Z",
      "title": "Pixel-SAIL: Single Transformer For Pixel-Grounded Understanding",
      "submittedOnDailyBy": {
        "_id": "63958b4414513eaf9029ebf1",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/U1g5H071pWRswGAG9UTpo.png",
        "isPro": false,
        "fullname": "Xiangtai Li",
        "user": "LXT",
        "type": "user"
      },
      "summary": "Multimodal Large Language Models (MLLMs) achieve remarkable performance for\nfine-grained pixel-level understanding tasks. However, all the works rely\nheavily on extra components, such as vision encoder (CLIP), segmentation\nexperts, leading to high system complexity and limiting model scaling. In this\nwork, our goal is to explore a highly simplified MLLM without introducing extra\ncomponents. Our work is motivated by the recent works on Single trAnsformer as\na unified vIsion-Language Model (SAIL) design, where these works jointly learn\nvision tokens and text tokens in transformers. We present Pixel-SAIL, a single\ntransformer for pixel-wise MLLM tasks. In particular, we present three\ntechnical improvements on the plain baseline. First, we design a learnable\nupsampling module to refine visual token features. Secondly, we propose a novel\nvisual prompt injection strategy to enable the single transformer to understand\nvisual prompt inputs and benefit from the early fusion of visual prompt\nembeddings and vision tokens. Thirdly, we introduce a vision expert\ndistillation strategy to efficiently enhance the single transformer's\nfine-grained feature extraction capability. In addition, we have collected a\ncomprehensive pixel understanding benchmark (PerBench), using a manual check.\nIt includes three tasks: detailed object description, visual prompt-based\nquestion answering, and visual-text referring segmentation. Extensive\nexperiments on four referring segmentation benchmarks, one visual prompt\nbenchmark, and our PerBench show that our Pixel-SAIL achieves comparable or\neven better results with a much simpler pipeline. Code and model will be\nreleased at https://github.com/magic-research/Sa2VA.",
      "upvotes": 16,
      "discussionId": "67ff26c6414c03ebc1d425de",
      "ai_keywords": [
        "multimodal large language models (MLLMs)",
        "pixel-level understanding",
        "vision encoder (CLIP)",
        "segmentation experts",
        "single transformer as a unified vision-language model (SAIL)",
        "pixel-wise MLLM tasks",
        "learnable upsampling module",
        "visual prompt injection",
        "visual prompt embeddings",
        "vision expert distillation",
        "pixel understanding benchmark (PerBench)",
        "detailed object description",
        "visual prompt-based question answering",
        "visual-text referring segmentation",
        "referring segmentation benchmarks",
        "visual prompt benchmark"
      ]
    },
    "publishedAt": "2025-04-14T13:52:22.000Z",
    "title": "Pixel-SAIL: Single Transformer For Pixel-Grounded Understanding",
    "summary": "Multimodal Large Language Models (MLLMs) achieve remarkable performance for\nfine-grained pixel-level understanding tasks. However, all the works rely\nheavily on extra components, such as vision encoder (CLIP), segmentation\nexperts, leading to high system complexity and limiting model scaling. In this\nwork, our goal is to explore a highly simplified MLLM without introducing extra\ncomponents. Our work is motivated by the recent works on Single trAnsformer as\na unified vIsion-Language Model (SAIL) design, where these works jointly learn\nvision tokens and text tokens in transformers. We present Pixel-SAIL, a single\ntransformer for pixel-wise MLLM tasks. In particular, we present three\ntechnical improvements on the plain baseline. First, we design a learnable\nupsampling module to refine visual token features. Secondly, we propose a novel\nvisual prompt injection strategy to enable the single transformer to understand\nvisual prompt inputs and benefit from the early fusion of visual prompt\nembeddings and vision tokens. Thirdly, we introduce a vision expert\ndistillation strategy to efficiently enhance the single transformer's\nfine-grained feature extraction capability. In addition, we have collected a\ncomprehensive pixel understanding benchmark (PerBench), using a manual check.\nIt includes three tasks: detailed object description, visual prompt-based\nquestion answering, and visual-text referring segmentation. Extensive\nexperiments on four referring segmentation benchmarks, one visual prompt\nbenchmark, and our PerBench show that our Pixel-SAIL achieves comparable or\neven better results with a much simpler pipeline. Code and model will be\nreleased at https://github.com/magic-research/Sa2VA.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10465.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63958b4414513eaf9029ebf1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/U1g5H071pWRswGAG9UTpo.png",
      "fullname": "Xiangtai Li",
      "name": "LXT",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.08672",
      "authors": [
        {
          "_id": "67fcb7294a92187863e805ee",
          "user": {
            "_id": "64e6cf78ecce34cb442dc889",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e6cf78ecce34cb442dc889/qVZFiUEpBpSkmH8SQeinm.jpeg",
            "isPro": false,
            "fullname": "Fangzhi Xu",
            "user": "xufangzhi",
            "type": "user"
          },
          "name": "Fangzhi Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-14T09:46:16.537Z",
          "hidden": false
        },
        {
          "_id": "67fcb7294a92187863e805ef",
          "name": "Hang Yan",
          "hidden": false
        },
        {
          "_id": "67fcb7294a92187863e805f0",
          "name": "Chang Ma",
          "hidden": false
        },
        {
          "_id": "67fcb7294a92187863e805f1",
          "name": "Haiteng Zhao",
          "hidden": false
        },
        {
          "_id": "67fcb7294a92187863e805f2",
          "name": "Qiushi Sun",
          "hidden": false
        },
        {
          "_id": "67fcb7294a92187863e805f3",
          "name": "Kanzhi Cheng",
          "hidden": false
        },
        {
          "_id": "67fcb7294a92187863e805f4",
          "name": "Junxian He",
          "hidden": false
        },
        {
          "_id": "67fcb7294a92187863e805f5",
          "name": "Jun Liu",
          "hidden": false
        },
        {
          "_id": "67fcb7294a92187863e805f6",
          "name": "Zhiyong Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-11T16:26:23.000Z",
      "submittedOnDailyAt": "2025-04-16T05:46:28.754Z",
      "title": "Genius: A Generalizable and Purely Unsupervised Self-Training Framework\n  For Advanced Reasoning",
      "submittedOnDailyBy": {
        "_id": "64e6cf78ecce34cb442dc889",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e6cf78ecce34cb442dc889/qVZFiUEpBpSkmH8SQeinm.jpeg",
        "isPro": false,
        "fullname": "Fangzhi Xu",
        "user": "xufangzhi",
        "type": "user"
      },
      "summary": "Advancing LLM reasoning skills has captivated wide interest. However, current\npost-training techniques rely heavily on supervisory signals, such as outcome\nsupervision or auxiliary reward models, which face the problem of scalability\nand high annotation costs. This motivates us to enhance LLM reasoning without\nthe need for external supervision. We introduce a generalizable and purely\nunsupervised self-training framework, named Genius. Without external auxiliary,\nGenius requires to seek the optimal response sequence in a stepwise manner and\noptimize the LLM. To explore the potential steps and exploit the optimal ones,\nGenius introduces a stepwise foresight re-sampling strategy to sample and\nestimate the step value by simulating future outcomes. Further, we recognize\nthat the unsupervised setting inevitably induces the intrinsic noise and\nuncertainty. To provide a robust optimization, we propose an\nadvantage-calibrated optimization (ACO) loss function to mitigate estimation\ninconsistencies. Combining these techniques together, Genius provides an\nadvanced initial step towards self-improve LLM reasoning with general queries\nand without supervision, revolutionizing reasoning scaling laws given the vast\navailability of general queries. The code will be released at\nhttps://github.com/xufangzhi/Genius.",
      "upvotes": 15,
      "discussionId": "67fcb72a4a92187863e8061b",
      "ai_keywords": [
        "self-training framework",
        "Genius",
        "stepwise foresight re-sampling strategy",
        "advantage-calibrated optimization (ACO) loss function"
      ]
    },
    "publishedAt": "2025-04-11T12:26:23.000Z",
    "title": "Genius: A Generalizable and Purely Unsupervised Self-Training Framework\n  For Advanced Reasoning",
    "summary": "Advancing LLM reasoning skills has captivated wide interest. However, current\npost-training techniques rely heavily on supervisory signals, such as outcome\nsupervision or auxiliary reward models, which face the problem of scalability\nand high annotation costs. This motivates us to enhance LLM reasoning without\nthe need for external supervision. We introduce a generalizable and purely\nunsupervised self-training framework, named Genius. Without external auxiliary,\nGenius requires to seek the optimal response sequence in a stepwise manner and\noptimize the LLM. To explore the potential steps and exploit the optimal ones,\nGenius introduces a stepwise foresight re-sampling strategy to sample and\nestimate the step value by simulating future outcomes. Further, we recognize\nthat the unsupervised setting inevitably induces the intrinsic noise and\nuncertainty. To provide a robust optimization, we propose an\nadvantage-calibrated optimization (ACO) loss function to mitigate estimation\ninconsistencies. Combining these techniques together, Genius provides an\nadvanced initial step towards self-improve LLM reasoning with general queries\nand without supervision, revolutionizing reasoning scaling laws given the vast\navailability of general queries. The code will be released at\nhttps://github.com/xufangzhi/Genius.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08672.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64e6cf78ecce34cb442dc889",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e6cf78ecce34cb442dc889/qVZFiUEpBpSkmH8SQeinm.jpeg",
      "fullname": "Fangzhi Xu",
      "name": "xufangzhi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.10766",
      "authors": [
        {
          "_id": "67ff114a3026f8abc4bf7e43",
          "name": "Ming Li",
          "hidden": false
        },
        {
          "_id": "67ff114a3026f8abc4bf7e44",
          "name": "Yanhong Li",
          "hidden": false
        },
        {
          "_id": "67ff114a3026f8abc4bf7e45",
          "name": "Ziyue Li",
          "hidden": false
        },
        {
          "_id": "67ff114a3026f8abc4bf7e46",
          "name": "Tianyi Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T23:53:47.000Z",
      "submittedOnDailyAt": "2025-04-16T00:40:29.697Z",
      "title": "How Instruction and Reasoning Data shape Post-Training: Data Quality\n  through the Lens of Layer-wise Gradients",
      "submittedOnDailyBy": {
        "_id": "647f5af5b0e96764589f3b2a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
        "isPro": false,
        "fullname": "Tianyi Zhou",
        "user": "zhoutianyi",
        "type": "user"
      },
      "summary": "As the post-training of large language models (LLMs) advances from\ninstruction-following to complex reasoning tasks, understanding how different\ndata affect finetuning dynamics remains largely unexplored. In this paper, we\npresent a spectral analysis of layer-wise gradients induced by low/high-quality\ninstruction and reasoning data for LLM post-training. Our analysis reveals that\nwidely-studied metrics for data evaluation, e.g., IFD, InsTag, Difficulty, and\nReward, can be explained and unified by spectral properties computed from\ngradients' singular value decomposition (SVD). Specifically, higher-quality\ndata are usually associated with lower nuclear norms and higher effective\nranks. Notably, effective rank exhibits better robustness and resolution than\nnuclear norm in capturing subtle quality differences. For example, reasoning\ndata achieves substantially higher effective ranks than instruction data,\nimplying richer gradient structures on more complex tasks. Our experiments also\nhighlight that models within the same family share similar gradient patterns\nregardless of their sizes, whereas different model families diverge\nsignificantly. Providing a unified view on the effects of data quality across\ninstruction and reasoning data, this work illuminates the interplay between\ndata quality and training stability, shedding novel insights into developing\nbetter data exploration strategies for post-training.",
      "upvotes": 14,
      "discussionId": "67ff11503026f8abc4bf7fed",
      "githubRepo": "https://github.com/MingLiiii/Gradient_Unified",
      "ai_keywords": [
        "spectral analysis",
        "layer-wise gradients",
        "low/high-quality instruction",
        "reasoning data",
        "IFD",
        "InsTag",
        "Difficulty",
        "Reward",
        "singular value decomposition (SVD)",
        "nuclear norms",
        "effective ranks",
        "gradient structures",
        "training stability"
      ]
    },
    "publishedAt": "2025-04-14T19:53:47.000Z",
    "title": "How Instruction and Reasoning Data shape Post-Training: Data Quality\n  through the Lens of Layer-wise Gradients",
    "summary": "As the post-training of large language models (LLMs) advances from\ninstruction-following to complex reasoning tasks, understanding how different\ndata affect finetuning dynamics remains largely unexplored. In this paper, we\npresent a spectral analysis of layer-wise gradients induced by low/high-quality\ninstruction and reasoning data for LLM post-training. Our analysis reveals that\nwidely-studied metrics for data evaluation, e.g., IFD, InsTag, Difficulty, and\nReward, can be explained and unified by spectral properties computed from\ngradients' singular value decomposition (SVD). Specifically, higher-quality\ndata are usually associated with lower nuclear norms and higher effective\nranks. Notably, effective rank exhibits better robustness and resolution than\nnuclear norm in capturing subtle quality differences. For example, reasoning\ndata achieves substantially higher effective ranks than instruction data,\nimplying richer gradient structures on more complex tasks. Our experiments also\nhighlight that models within the same family share similar gradient patterns\nregardless of their sizes, whereas different model families diverge\nsignificantly. Providing a unified view on the effects of data quality across\ninstruction and reasoning data, this work illuminates the interplay between\ndata quality and training stability, shedding novel insights into developing\nbetter data exploration strategies for post-training.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10766.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647f5af5b0e96764589f3b2a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
      "fullname": "Tianyi Zhou",
      "name": "zhoutianyi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.11442",
      "authors": [
        {
          "_id": "67ff1387e1bfbb6bdd79ab72",
          "name": "Leon Guertler",
          "hidden": false
        },
        {
          "_id": "67ff1387e1bfbb6bdd79ab73",
          "name": "Bobby Cheng",
          "hidden": false
        },
        {
          "_id": "67ff1387e1bfbb6bdd79ab74",
          "name": "Simon Yu",
          "hidden": false
        },
        {
          "_id": "67ff1387e1bfbb6bdd79ab75",
          "name": "Bo Liu",
          "hidden": false
        },
        {
          "_id": "67ff1387e1bfbb6bdd79ab76",
          "name": "Leshem Choshen",
          "hidden": false
        },
        {
          "_id": "67ff1387e1bfbb6bdd79ab77",
          "name": "Cheston Tan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-15T17:55:20.000Z",
      "submittedOnDailyAt": "2025-04-16T01:06:25.215Z",
      "title": "TextArena",
      "submittedOnDailyBy": {
        "_id": "635e3a76106f984574c36409",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1667120725800-635e3a76106f984574c36409.png",
        "isPro": false,
        "fullname": "Bo Liu",
        "user": "Benjamin-eecs",
        "type": "user"
      },
      "summary": "TextArena is an open-source collection of competitive text-based games for\ntraining and evaluation of agentic behavior in Large Language Models (LLMs). It\nspans 57+ unique environments (including single-player, two-player, and\nmulti-player setups) and allows for easy evaluation of model capabilities via\nan online-play system (against humans and other submitted models) with\nreal-time TrueSkill scores. Traditional benchmarks rarely assess dynamic social\nskills such as negotiation, theory of mind, and deception, creating a gap that\nTextArena addresses. Designed with research, community and extensibility in\nmind, TextArena emphasizes ease of adding new games, adapting the framework,\ntesting models, playing against the models, and training models. Detailed\ndocumentation of environments, games, leaderboard, and examples are available\non https://github.com/LeonGuertler/TextArena and https://www.textarena.ai/.",
      "upvotes": 13,
      "discussionId": "67ff1388e1bfbb6bdd79abbe",
      "projectPage": "https://textarena.ai/",
      "githubRepo": "https://github.com/LeonGuertler/TextArena",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "TrueSkill scores",
        "negotiation",
        "theory of mind",
        "deception",
        "dynamic social skills"
      ]
    },
    "publishedAt": "2025-04-15T13:55:20.000Z",
    "title": "TextArena",
    "summary": "TextArena is an open-source collection of competitive text-based games for\ntraining and evaluation of agentic behavior in Large Language Models (LLMs). It\nspans 57+ unique environments (including single-player, two-player, and\nmulti-player setups) and allows for easy evaluation of model capabilities via\nan online-play system (against humans and other submitted models) with\nreal-time TrueSkill scores. Traditional benchmarks rarely assess dynamic social\nskills such as negotiation, theory of mind, and deception, creating a gap that\nTextArena addresses. Designed with research, community and extensibility in\nmind, TextArena emphasizes ease of adding new games, adapting the framework,\ntesting models, playing against the models, and training models. Detailed\ndocumentation of environments, games, leaderboard, and examples are available\non https://github.com/LeonGuertler/TextArena and https://www.textarena.ai/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11442.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "635e3a76106f984574c36409",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1667120725800-635e3a76106f984574c36409.png",
      "fullname": "Bo Liu",
      "name": "Benjamin-eecs",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.11346",
      "authors": [
        {
          "_id": "67ff18961dc5d56fdd6ca724",
          "name": "Yu Gao",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca725",
          "name": "Lixue Gong",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca726",
          "name": "Qiushan Guo",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca727",
          "name": "Xiaoxia Hou",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca728",
          "name": "Zhichao Lai",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca729",
          "name": "Fanshi Li",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca72a",
          "name": "Liang Li",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca72b",
          "name": "Xiaochen Lian",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca72c",
          "name": "Chao Liao",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca72d",
          "name": "Liyang Liu",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca72e",
          "name": "Wei Liu",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca72f",
          "name": "Yichun Shi",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca730",
          "name": "Shiqi Sun",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca731",
          "name": "Yu Tian",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca732",
          "name": "Zhi Tian",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca733",
          "name": "Peng Wang",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca734",
          "name": "Rui Wang",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca735",
          "name": "Xuanda Wang",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca736",
          "name": "Xun Wang",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca737",
          "name": "Ye Wang",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca738",
          "name": "Guofeng Wu",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca739",
          "name": "Jie Wu",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca73a",
          "name": "Xin Xia",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca73b",
          "name": "Xuefeng Xiao",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca73c",
          "name": "Zhonghua Zhai",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca73d",
          "name": "Xinyu Zhang",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca73e",
          "name": "Qi Zhang",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca73f",
          "name": "Yuwei Zhang",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca740",
          "name": "Shijia Zhao",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca741",
          "name": "Jianchao Yang",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca742",
          "name": "Weilin Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-15T16:19:07.000Z",
      "submittedOnDailyAt": "2025-04-16T01:10:39.295Z",
      "title": "Seedream 3.0 Technical Report",
      "submittedOnDailyBy": {
        "_id": "63468720dd6d90d82ccf3450",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
        "isPro": false,
        "fullname": "YSH",
        "user": "BestWishYsh",
        "type": "user"
      },
      "summary": "We present Seedream 3.0, a high-performance Chinese-English bilingual image\ngeneration foundation model. We develop several technical improvements to\naddress existing challenges in Seedream 2.0, including alignment with\ncomplicated prompts, fine-grained typography generation, suboptimal visual\naesthetics and fidelity, and limited image resolutions. Specifically, the\nadvancements of Seedream 3.0 stem from improvements across the entire pipeline,\nfrom data construction to model deployment. At the data stratum, we double the\ndataset using a defect-aware training paradigm and a dual-axis collaborative\ndata-sampling framework. Furthermore, we adopt several effective techniques\nsuch as mixed-resolution training, cross-modality RoPE, representation\nalignment loss, and resolution-aware timestep sampling in the pre-training\nphase. During the post-training stage, we utilize diversified aesthetic\ncaptions in SFT, and a VLM-based reward model with scaling, thereby achieving\noutputs that well align with human preferences. Furthermore, Seedream 3.0\npioneers a novel acceleration paradigm. By employing consistent noise\nexpectation and importance-aware timestep sampling, we achieve a 4 to 8 times\nspeedup while maintaining image quality. Seedream 3.0 demonstrates significant\nimprovements over Seedream 2.0: it enhances overall capabilities, in particular\nfor text-rendering in complicated Chinese characters which is important to\nprofessional typography generation. In addition, it provides native\nhigh-resolution output (up to 2K), allowing it to generate images with high\nvisual quality.",
      "upvotes": 12,
      "discussionId": "67ff189c1dc5d56fdd6ca8e0",
      "projectPage": "https://team.doubao.com/zh/tech/seedream3_0",
      "ai_keywords": [
        "mixed-resolution training",
        "cross-modality RoPE",
        "representation alignment loss",
        "resolution-aware timestep sampling",
        "SFT (Supervised Fine-Tuning)",
        "VLM (Vision Language Model)",
        "consistent noise expectation",
        "importance-aware timestep sampling"
      ]
    },
    "publishedAt": "2025-04-15T12:19:07.000Z",
    "title": "Seedream 3.0 Technical Report",
    "summary": "We present Seedream 3.0, a high-performance Chinese-English bilingual image\ngeneration foundation model. We develop several technical improvements to\naddress existing challenges in Seedream 2.0, including alignment with\ncomplicated prompts, fine-grained typography generation, suboptimal visual\naesthetics and fidelity, and limited image resolutions. Specifically, the\nadvancements of Seedream 3.0 stem from improvements across the entire pipeline,\nfrom data construction to model deployment. At the data stratum, we double the\ndataset using a defect-aware training paradigm and a dual-axis collaborative\ndata-sampling framework. Furthermore, we adopt several effective techniques\nsuch as mixed-resolution training, cross-modality RoPE, representation\nalignment loss, and resolution-aware timestep sampling in the pre-training\nphase. During the post-training stage, we utilize diversified aesthetic\ncaptions in SFT, and a VLM-based reward model with scaling, thereby achieving\noutputs that well align with human preferences. Furthermore, Seedream 3.0\npioneers a novel acceleration paradigm. By employing consistent noise\nexpectation and importance-aware timestep sampling, we achieve a 4 to 8 times\nspeedup while maintaining image quality. Seedream 3.0 demonstrates significant\nimprovements over Seedream 2.0: it enhances overall capabilities, in particular\nfor text-rendering in complicated Chinese characters which is important to\nprofessional typography generation. In addition, it provides native\nhigh-resolution output (up to 2K), allowing it to generate images with high\nvisual quality.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11346.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63468720dd6d90d82ccf3450",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
      "fullname": "YSH",
      "name": "BestWishYsh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 47
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.10462",
      "authors": [
        {
          "_id": "67ff2aa6a0346c2e622afdb2",
          "name": "Weixian Lei",
          "hidden": false
        },
        {
          "_id": "67ff2aa6a0346c2e622afdb3",
          "name": "Jiacong Wang",
          "hidden": false
        },
        {
          "_id": "67ff2aa6a0346c2e622afdb4",
          "name": "Haochen Wang",
          "hidden": false
        },
        {
          "_id": "67ff2aa6a0346c2e622afdb5",
          "name": "Xiangtai Li",
          "hidden": false
        },
        {
          "_id": "67ff2aa6a0346c2e622afdb6",
          "name": "Jun Hao Liew",
          "hidden": false
        },
        {
          "_id": "67ff2aa6a0346c2e622afdb7",
          "name": "Jiashi Feng",
          "hidden": false
        },
        {
          "_id": "67ff2aa6a0346c2e622afdb8",
          "name": "Zilong Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T17:50:20.000Z",
      "submittedOnDailyAt": "2025-04-16T02:27:49.065Z",
      "title": "The Scalability of Simplicity: Empirical Analysis of Vision-Language\n  Learning with a Single Transformer",
      "submittedOnDailyBy": {
        "_id": "63958b4414513eaf9029ebf1",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/U1g5H071pWRswGAG9UTpo.png",
        "isPro": false,
        "fullname": "Xiangtai Li",
        "user": "LXT",
        "type": "user"
      },
      "summary": "This paper introduces SAIL, a single transformer unified multimodal large\nlanguage model (MLLM) that integrates raw pixel encoding and language decoding\nwithin a singular architecture. Unlike existing modular MLLMs, which rely on a\npre-trained vision transformer (ViT), SAIL eliminates the need for a separate\nvision encoder, presenting a more minimalist architecture design. Instead of\nintroducing novel architectural components, SAIL adapts mix-attention\nmechanisms and multimodal positional encodings to better align with the\ndistinct characteristics of visual and textual modalities. We systematically\ncompare SAIL's properties-including scalability, cross-modal information flow\npatterns, and visual representation capabilities-with those of modular MLLMs.\nBy scaling both training data and model size, SAIL achieves performance\ncomparable to modular MLLMs. Notably, the removal of pretrained ViT components\nenhances SAIL's scalability and results in significantly different cross-modal\ninformation flow patterns. Moreover, SAIL demonstrates strong visual\nrepresentation capabilities, achieving results on par with ViT-22B in vision\ntasks such as semantic segmentation. Code and models are available at\nhttps://github.com/bytedance/SAIL.",
      "upvotes": 10,
      "discussionId": "67ff2aa7a0346c2e622afe08",
      "ai_keywords": [
        "single transformer",
        "unified multimodal large language model (MLLM)",
        "raw pixel encoding",
        "language decoding",
        "vision transformer (ViT)",
        "mix-attention mechanisms",
        "multimodal positional encodings",
        "scalability",
        "cross-modal information flow patterns",
        "visual representation capabilities",
        "semantic segmentation",
        "ViT-22B"
      ]
    },
    "publishedAt": "2025-04-14T13:50:20.000Z",
    "title": "The Scalability of Simplicity: Empirical Analysis of Vision-Language\n  Learning with a Single Transformer",
    "summary": "This paper introduces SAIL, a single transformer unified multimodal large\nlanguage model (MLLM) that integrates raw pixel encoding and language decoding\nwithin a singular architecture. Unlike existing modular MLLMs, which rely on a\npre-trained vision transformer (ViT), SAIL eliminates the need for a separate\nvision encoder, presenting a more minimalist architecture design. Instead of\nintroducing novel architectural components, SAIL adapts mix-attention\nmechanisms and multimodal positional encodings to better align with the\ndistinct characteristics of visual and textual modalities. We systematically\ncompare SAIL's properties-including scalability, cross-modal information flow\npatterns, and visual representation capabilities-with those of modular MLLMs.\nBy scaling both training data and model size, SAIL achieves performance\ncomparable to modular MLLMs. Notably, the removal of pretrained ViT components\nenhances SAIL's scalability and results in significantly different cross-modal\ninformation flow patterns. Moreover, SAIL demonstrates strong visual\nrepresentation capabilities, achieving results on par with ViT-22B in vision\ntasks such as semantic segmentation. Code and models are available at\nhttps://github.com/bytedance/SAIL.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10462.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63958b4414513eaf9029ebf1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/U1g5H071pWRswGAG9UTpo.png",
      "fullname": "Xiangtai Li",
      "name": "LXT",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.10559",
      "authors": [
        {
          "_id": "67ff1df03b42083b37219456",
          "name": "Keyu Duan",
          "hidden": false
        },
        {
          "_id": "67ff1df03b42083b37219457",
          "name": "Zichen Liu",
          "hidden": false
        },
        {
          "_id": "67ff1df03b42083b37219458",
          "name": "Xin Mao",
          "hidden": false
        },
        {
          "_id": "67ff1df03b42083b37219459",
          "name": "Tianyu Pang",
          "hidden": false
        },
        {
          "_id": "67ff1df03b42083b3721945a",
          "name": "Changyu Chen",
          "hidden": false
        },
        {
          "_id": "67ff1df03b42083b3721945b",
          "name": "Qiguang Chen",
          "hidden": false
        },
        {
          "_id": "67ff1df03b42083b3721945c",
          "name": "Michael Qizhe Shieh",
          "hidden": false
        },
        {
          "_id": "67ff1df03b42083b3721945d",
          "name": "Longxu Dou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T14:53:56.000Z",
      "submittedOnDailyAt": "2025-04-16T01:34:03.069Z",
      "title": "Efficient Process Reward Model Training via Active Learning",
      "submittedOnDailyBy": {
        "_id": "6214e4ee1e35c843d42d1f88",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6214e4ee1e35c843d42d1f88/fj-9wuIdPhvogh3BrcXTB.jpeg",
        "isPro": true,
        "fullname": "Longxu Dou",
        "user": "dreamerdeo",
        "type": "user"
      },
      "summary": "Process Reward Models (PRMs) provide step-level supervision to large language\nmodels (LLMs), but scaling up training data annotation remains challenging for\nboth humans and LLMs. To address this limitation, we propose an active learning\napproach, ActPRM, which proactively selects the most uncertain samples for\ntraining, substantially reducing labeling costs. During training, we use the\nPRM to estimate uncertainty after the forward pass, retaining only highly\nuncertain data. A capable yet costly reasoning model then labels this data.\nThen we compute the loss with respect to the labels and update the PRM's\nweights. We compare ActPRM vs. vanilla fine-tuning, on a pool-based active\nlearning setting, demonstrating that ActPRM reduces 50% annotation, but\nachieving the comparable or even better performance. Beyond annotation\nefficiency, we further advance the actively trained PRM by filtering over 1M+\nmath reasoning trajectories with ActPRM, retaining 60% of the data. A\nsubsequent training on this selected dataset yields a new state-of-the-art\n(SOTA) PRM on ProcessBench (75.0%) and PRMBench (65.5%) compared with same\nsized models.",
      "upvotes": 6,
      "discussionId": "67ff1df23b42083b372194a8",
      "githubRepo": "https://github.com/sail-sg/ActivePRM",
      "ai_keywords": [
        "active learning",
        "ActPRM",
        "uncertainty estimation",
        "labeling costs",
        "vanilla fine-tuning",
        "pool-based active learning",
        "annotation efficiency",
        "math reasoning trajectories",
        "state-of-the-art (SOTA)",
        "ProcessBench",
        "PRMBench"
      ]
    },
    "publishedAt": "2025-04-14T10:53:56.000Z",
    "title": "Efficient Process Reward Model Training via Active Learning",
    "summary": "Process Reward Models (PRMs) provide step-level supervision to large language\nmodels (LLMs), but scaling up training data annotation remains challenging for\nboth humans and LLMs. To address this limitation, we propose an active learning\napproach, ActPRM, which proactively selects the most uncertain samples for\ntraining, substantially reducing labeling costs. During training, we use the\nPRM to estimate uncertainty after the forward pass, retaining only highly\nuncertain data. A capable yet costly reasoning model then labels this data.\nThen we compute the loss with respect to the labels and update the PRM's\nweights. We compare ActPRM vs. vanilla fine-tuning, on a pool-based active\nlearning setting, demonstrating that ActPRM reduces 50% annotation, but\nachieving the comparable or even better performance. Beyond annotation\nefficiency, we further advance the actively trained PRM by filtering over 1M+\nmath reasoning trajectories with ActPRM, retaining 60% of the data. A\nsubsequent training on this selected dataset yields a new state-of-the-art\n(SOTA) PRM on ProcessBench (75.0%) and PRMBench (65.5%) compared with same\nsized models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10559.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6214e4ee1e35c843d42d1f88",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6214e4ee1e35c843d42d1f88/fj-9wuIdPhvogh3BrcXTB.jpeg",
      "fullname": "Longxu Dou",
      "name": "dreamerdeo",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 19
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.11427",
      "authors": [
        {
          "_id": "67ff1cc4372d6790b1b7da90",
          "name": "Yanrui Bin",
          "hidden": false
        },
        {
          "_id": "67ff1cc4372d6790b1b7da91",
          "name": "Wenbo Hu",
          "hidden": false
        },
        {
          "_id": "67ff1cc4372d6790b1b7da92",
          "name": "Haoyuan Wang",
          "hidden": false
        },
        {
          "_id": "67ff1cc4372d6790b1b7da93",
          "name": "Xinya Chen",
          "hidden": false
        },
        {
          "_id": "67ff1cc4372d6790b1b7da94",
          "name": "Bing Wang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/657a7458afbb0117ba15c59f/dKoipt1ASgjiyDt33avVt.mp4"
      ],
      "publishedAt": "2025-04-15T17:39:07.000Z",
      "submittedOnDailyAt": "2025-04-16T01:28:48.790Z",
      "title": "NormalCrafter: Learning Temporally Consistent Normals from Video\n  Diffusion Priors",
      "submittedOnDailyBy": {
        "_id": "657a7458afbb0117ba15c59f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657a7458afbb0117ba15c59f/8_iwTS1UG_mKnfylFbLsY.jpeg",
        "isPro": false,
        "fullname": "Wenbo Hu",
        "user": "wbhu-tc",
        "type": "user"
      },
      "summary": "Surface normal estimation serves as a cornerstone for a spectrum of computer\nvision applications. While numerous efforts have been devoted to static image\nscenarios, ensuring temporal coherence in video-based normal estimation remains\na formidable challenge. Instead of merely augmenting existing methods with\ntemporal components, we present NormalCrafter to leverage the inherent temporal\npriors of video diffusion models. To secure high-fidelity normal estimation\nacross sequences, we propose Semantic Feature Regularization (SFR), which\naligns diffusion features with semantic cues, encouraging the model to\nconcentrate on the intrinsic semantics of the scene. Moreover, we introduce a\ntwo-stage training protocol that leverages both latent and pixel space learning\nto preserve spatial accuracy while maintaining long temporal context. Extensive\nevaluations demonstrate the efficacy of our method, showcasing a superior\nperformance in generating temporally consistent normal sequences with intricate\ndetails from diverse videos.",
      "upvotes": 3,
      "discussionId": "67ff1cc5372d6790b1b7daee",
      "projectPage": "https://normalcrafter.github.io/",
      "githubRepo": "https://github.com/Binyr/NormalCrafter",
      "ai_keywords": [
        "video diffusion models",
        "Semantic Feature Regularization (SFR)",
        "latent space",
        "pixel space",
        "temporal coherence",
        "spatial accuracy",
        "long temporal context",
        "temporally consistent",
        "intricate details"
      ]
    },
    "publishedAt": "2025-04-15T13:39:07.000Z",
    "title": "NormalCrafter: Learning Temporally Consistent Normals from Video\n  Diffusion Priors",
    "summary": "Surface normal estimation serves as a cornerstone for a spectrum of computer\nvision applications. While numerous efforts have been devoted to static image\nscenarios, ensuring temporal coherence in video-based normal estimation remains\na formidable challenge. Instead of merely augmenting existing methods with\ntemporal components, we present NormalCrafter to leverage the inherent temporal\npriors of video diffusion models. To secure high-fidelity normal estimation\nacross sequences, we propose Semantic Feature Regularization (SFR), which\naligns diffusion features with semantic cues, encouraging the model to\nconcentrate on the intrinsic semantics of the scene. Moreover, we introduce a\ntwo-stage training protocol that leverages both latent and pixel space learning\nto preserve spatial accuracy while maintaining long temporal context. Extensive\nevaluations demonstrate the efficacy of our method, showcasing a superior\nperformance in generating temporally consistent normal sequences with intricate\ndetails from diverse videos.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/657a7458afbb0117ba15c59f/dKoipt1ASgjiyDt33avVt.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11427.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "657a7458afbb0117ba15c59f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657a7458afbb0117ba15c59f/8_iwTS1UG_mKnfylFbLsY.jpeg",
      "fullname": "Wenbo Hu",
      "name": "wbhu-tc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.11343",
      "authors": [
        {
          "_id": "67ff2d4a86e7ad2b4bea1349",
          "name": "Wei Xiong",
          "hidden": false
        },
        {
          "_id": "67ff2d4a86e7ad2b4bea134a",
          "name": "Jiarui Yao",
          "hidden": false
        },
        {
          "_id": "67ff2d4a86e7ad2b4bea134b",
          "name": "Yuhui Xu",
          "hidden": false
        },
        {
          "_id": "67ff2d4a86e7ad2b4bea134c",
          "name": "Bo Pang",
          "hidden": false
        },
        {
          "_id": "67ff2d4a86e7ad2b4bea134d",
          "name": "Lei Wang",
          "hidden": false
        },
        {
          "_id": "67ff2d4a86e7ad2b4bea134e",
          "name": "Doyen Sahoo",
          "hidden": false
        },
        {
          "_id": "67ff2d4a86e7ad2b4bea134f",
          "name": "Junnan Li",
          "hidden": false
        },
        {
          "_id": "67ff2d4a86e7ad2b4bea1350",
          "name": "Nan Jiang",
          "hidden": false
        },
        {
          "_id": "67ff2d4a86e7ad2b4bea1351",
          "name": "Tong Zhang",
          "hidden": false
        },
        {
          "_id": "67ff2d4a86e7ad2b4bea1352",
          "name": "Caiming Xiong",
          "hidden": false
        },
        {
          "_id": "67ff2d4a86e7ad2b4bea1353",
          "name": "Hanze Dong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-15T16:15:02.000Z",
      "submittedOnDailyAt": "2025-04-16T02:39:25.160Z",
      "title": "A Minimalist Approach to LLM Reasoning: from Rejection Sampling to\n  Reinforce",
      "submittedOnDailyBy": {
        "_id": "643e59806db6ba8c5ee123f3",
        "avatarUrl": "/avatars/4052f2a250107f43b3634c3ee3cc30a1.svg",
        "isPro": false,
        "fullname": "Wei Xiong",
        "user": "weqweasdas",
        "type": "user"
      },
      "summary": "Reinforcement learning (RL) has become a prevailing approach for fine-tuning\nlarge language models (LLMs) on complex reasoning tasks. Among recent methods,\nGRPO stands out for its empirical success in training models such as\nDeepSeek-R1, yet the sources of its effectiveness remain poorly understood. In\nthis work, we revisit GRPO from a reinforce-like algorithm perspective and\nanalyze its core components. Surprisingly, we find that a simple rejection\nsampling baseline, RAFT, which trains only on positively rewarded samples,\nyields competitive performance than GRPO and PPO. Our ablation studies reveal\nthat GRPO's main advantage arises from discarding prompts with entirely\nincorrect responses, rather than from its reward normalization. Motivated by\nthis insight, we propose Reinforce-Rej, a minimal extension of policy gradient\nthat filters both entirely incorrect and entirely correct samples.\nReinforce-Rej improves KL efficiency and stability, serving as a lightweight\nyet effective alternative to more complex RL algorithms. We advocate RAFT as a\nrobust and interpretable baseline, and suggest that future advances should\nfocus on more principled designs for incorporating negative samples, rather\nthan relying on them indiscriminately. Our findings provide guidance for future\nwork in reward-based LLM post-training.",
      "upvotes": 3,
      "discussionId": "67ff2d4b86e7ad2b4bea1381",
      "ai_keywords": [
        "GRPO",
        "DeepSeek-R1",
        "reinforcement learning (RL)",
        "fine-tuning",
        "large language models (LLMs)",
        "complex reasoning tasks",
        "RAFT",
        "positively rewarded samples",
        "policy gradient",
        "KL efficiency"
      ]
    },
    "publishedAt": "2025-04-15T12:15:02.000Z",
    "title": "A Minimalist Approach to LLM Reasoning: from Rejection Sampling to\n  Reinforce",
    "summary": "Reinforcement learning (RL) has become a prevailing approach for fine-tuning\nlarge language models (LLMs) on complex reasoning tasks. Among recent methods,\nGRPO stands out for its empirical success in training models such as\nDeepSeek-R1, yet the sources of its effectiveness remain poorly understood. In\nthis work, we revisit GRPO from a reinforce-like algorithm perspective and\nanalyze its core components. Surprisingly, we find that a simple rejection\nsampling baseline, RAFT, which trains only on positively rewarded samples,\nyields competitive performance than GRPO and PPO. Our ablation studies reveal\nthat GRPO's main advantage arises from discarding prompts with entirely\nincorrect responses, rather than from its reward normalization. Motivated by\nthis insight, we propose Reinforce-Rej, a minimal extension of policy gradient\nthat filters both entirely incorrect and entirely correct samples.\nReinforce-Rej improves KL efficiency and stability, serving as a lightweight\nyet effective alternative to more complex RL algorithms. We advocate RAFT as a\nrobust and interpretable baseline, and suggest that future advances should\nfocus on more principled designs for incorporating negative samples, rather\nthan relying on them indiscriminately. Our findings provide guidance for future\nwork in reward-based LLM post-training.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11343.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643e59806db6ba8c5ee123f3",
      "avatarUrl": "/avatars/4052f2a250107f43b3634c3ee3cc30a1.svg",
      "fullname": "Wei Xiong",
      "name": "weqweasdas",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 18
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.10188",
      "authors": [
        {
          "_id": "67fe602166c0e8f3c2df22a9",
          "user": {
            "_id": "649d59cec6b4fdd84ebe0d47",
            "avatarUrl": "/avatars/a070e15659c0686fdfc69e559f3d6493.svg",
            "isPro": false,
            "fullname": "Deyuan Liu",
            "user": "SempraETY",
            "type": "user"
          },
          "name": "Deyuan Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-15T16:46:56.270Z",
          "hidden": false
        },
        {
          "_id": "67fe602166c0e8f3c2df22aa",
          "name": "Peng Sun",
          "hidden": false
        },
        {
          "_id": "67fe602166c0e8f3c2df22ab",
          "name": "Xufeng Li",
          "hidden": false
        },
        {
          "_id": "67fe602166c0e8f3c2df22ac",
          "name": "Tao Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T12:43:17.000Z",
      "submittedOnDailyAt": "2025-04-16T03:03:14.152Z",
      "title": "Efficient Generative Model Training via Embedded Representation Warmup",
      "submittedOnDailyBy": {
        "_id": "649d59cec6b4fdd84ebe0d47",
        "avatarUrl": "/avatars/a070e15659c0686fdfc69e559f3d6493.svg",
        "isPro": false,
        "fullname": "Deyuan Liu",
        "user": "SempraETY",
        "type": "user"
      },
      "summary": "Diffusion models excel at generating high-dimensional data but fall short in\ntraining efficiency and representation quality compared to self-supervised\nmethods. We identify a key bottleneck: the underutilization of high-quality,\nsemantically rich representations during training notably slows down\nconvergence. Our systematic analysis reveals a critical representation\nprocessing region -- primarily in the early layers -- where semantic and\nstructural pattern learning takes place before generation can occur. To address\nthis, we propose Embedded Representation Warmup (ERW), a plug-and-play\nframework where in the first stage we get the ERW module serves as a warmup\nthat initializes the early layers of the diffusion model with high-quality,\npretrained representations. This warmup minimizes the burden of learning\nrepresentations from scratch, thereby accelerating convergence and boosting\nperformance. Our theoretical analysis demonstrates that ERW's efficacy depends\non its precise integration into specific neural network layers -- termed the\nrepresentation processing region -- where the model primarily processes and\ntransforms feature representations for later generation. We further establish\nthat ERW not only accelerates training convergence but also enhances\nrepresentation quality: empirically, our method achieves a 40times\nacceleration in training speed compared to REPA, the current state-of-the-art\nmethods. Code is available at https://github.com/LINs-lab/ERW.",
      "upvotes": 3,
      "discussionId": "67fe602266c0e8f3c2df2334",
      "projectPage": "https://lins-lab.github.io/ERW/",
      "githubRepo": "https://github.com/LINs-lab/ERW",
      "ai_keywords": [
        "diffusion models",
        "high-dimensional data",
        "self-supervised methods",
        "high-quality representations",
        "semantic representations",
        "structural pattern learning",
        "Embedded Representation Warmup (ERW)",
        "warmup",
        "early layers",
        "representation processing region",
        "pretrained representations",
        "convergence",
        "training convergence",
        "representation quality",
        "REPA"
      ]
    },
    "publishedAt": "2025-04-14T08:43:17.000Z",
    "title": "Efficient Generative Model Training via Embedded Representation Warmup",
    "summary": "Diffusion models excel at generating high-dimensional data but fall short in\ntraining efficiency and representation quality compared to self-supervised\nmethods. We identify a key bottleneck: the underutilization of high-quality,\nsemantically rich representations during training notably slows down\nconvergence. Our systematic analysis reveals a critical representation\nprocessing region -- primarily in the early layers -- where semantic and\nstructural pattern learning takes place before generation can occur. To address\nthis, we propose Embedded Representation Warmup (ERW), a plug-and-play\nframework where in the first stage we get the ERW module serves as a warmup\nthat initializes the early layers of the diffusion model with high-quality,\npretrained representations. This warmup minimizes the burden of learning\nrepresentations from scratch, thereby accelerating convergence and boosting\nperformance. Our theoretical analysis demonstrates that ERW's efficacy depends\non its precise integration into specific neural network layers -- termed the\nrepresentation processing region -- where the model primarily processes and\ntransforms feature representations for later generation. We further establish\nthat ERW not only accelerates training convergence but also enhances\nrepresentation quality: empirically, our method achieves a 40times\nacceleration in training speed compared to REPA, the current state-of-the-art\nmethods. Code is available at https://github.com/LINs-lab/ERW.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10188.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649d59cec6b4fdd84ebe0d47",
      "avatarUrl": "/avatars/a070e15659c0686fdfc69e559f3d6493.svg",
      "fullname": "Deyuan Liu",
      "name": "SempraETY",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.11447",
      "authors": [
        {
          "_id": "67ff1026f8afab940cc23f88",
          "name": "An Zhaol",
          "hidden": false
        },
        {
          "_id": "67ff1026f8afab940cc23f89",
          "name": "Shengyuan Zhang",
          "hidden": false
        },
        {
          "_id": "67ff1026f8afab940cc23f8a",
          "name": "Ling Yang",
          "hidden": false
        },
        {
          "_id": "67ff1026f8afab940cc23f8b",
          "name": "Zejian Li",
          "hidden": false
        },
        {
          "_id": "67ff1026f8afab940cc23f8c",
          "name": "Jiale Wu",
          "hidden": false
        },
        {
          "_id": "67ff1026f8afab940cc23f8d",
          "name": "Haoran Xu",
          "hidden": false
        },
        {
          "_id": "67ff1026f8afab940cc23f8e",
          "name": "AnYang Wei",
          "hidden": false
        },
        {
          "_id": "67ff1026f8afab940cc23f8f",
          "name": "Perry Pengyun GU Lingyun Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-15T17:57:13.000Z",
      "submittedOnDailyAt": "2025-04-16T00:35:02.754Z",
      "title": "Diffusion Distillation With Direct Preference Optimization For Efficient\n  3D LiDAR Scene Completion",
      "submittedOnDailyBy": {
        "_id": "63943c882b9483beb473ec25",
        "avatarUrl": "/avatars/abd2aae43e68c34770159c15a01c8297.svg",
        "isPro": false,
        "fullname": "Shengyuan Zhang",
        "user": "SYZhang0805",
        "type": "user"
      },
      "summary": "The application of diffusion models in 3D LiDAR scene completion is limited\ndue to diffusion's slow sampling speed. Score distillation accelerates\ndiffusion sampling but with performance degradation, while post-training with\ndirect policy optimization (DPO) boosts performance using preference data. This\npaper proposes Distillation-DPO, a novel diffusion distillation framework for\nLiDAR scene completion with preference aligment. First, the student model\ngenerates paired completion scenes with different initial noises. Second, using\nLiDAR scene evaluation metrics as preference, we construct winning and losing\nsample pairs. Such construction is reasonable, since most LiDAR scene metrics\nare informative but non-differentiable to be optimized directly. Third,\nDistillation-DPO optimizes the student model by exploiting the difference in\nscore functions between the teacher and student models on the paired completion\nscenes. Such procedure is repeated until convergence. Extensive experiments\ndemonstrate that, compared to state-of-the-art LiDAR scene completion diffusion\nmodels, Distillation-DPO achieves higher-quality scene completion while\naccelerating the completion speed by more than 5-fold. Our method is the first\nto explore adopting preference learning in distillation to the best of our\nknowledge and provide insights into preference-aligned distillation. Our code\nis public available on https://github.com/happyw1nd/DistillationDPO.",
      "upvotes": 2,
      "discussionId": "67ff1027f8afab940cc23fd4",
      "ai_keywords": [
        "diffusion models",
        "LiDAR scene completion",
        "score distillation",
        "direct policy optimization (DPO)",
        "preference alignment",
        "student model",
        "paired completion scenes",
        "LiDAR scene evaluation metrics",
        "winning and losing sample pairs",
        "score functions",
        "preference learning"
      ]
    },
    "publishedAt": "2025-04-15T13:57:13.000Z",
    "title": "Diffusion Distillation With Direct Preference Optimization For Efficient\n  3D LiDAR Scene Completion",
    "summary": "The application of diffusion models in 3D LiDAR scene completion is limited\ndue to diffusion's slow sampling speed. Score distillation accelerates\ndiffusion sampling but with performance degradation, while post-training with\ndirect policy optimization (DPO) boosts performance using preference data. This\npaper proposes Distillation-DPO, a novel diffusion distillation framework for\nLiDAR scene completion with preference aligment. First, the student model\ngenerates paired completion scenes with different initial noises. Second, using\nLiDAR scene evaluation metrics as preference, we construct winning and losing\nsample pairs. Such construction is reasonable, since most LiDAR scene metrics\nare informative but non-differentiable to be optimized directly. Third,\nDistillation-DPO optimizes the student model by exploiting the difference in\nscore functions between the teacher and student models on the paired completion\nscenes. Such procedure is repeated until convergence. Extensive experiments\ndemonstrate that, compared to state-of-the-art LiDAR scene completion diffusion\nmodels, Distillation-DPO achieves higher-quality scene completion while\naccelerating the completion speed by more than 5-fold. Our method is the first\nto explore adopting preference learning in distillation to the best of our\nknowledge and provide insights into preference-aligned distillation. Our code\nis public available on https://github.com/happyw1nd/DistillationDPO.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11447.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63943c882b9483beb473ec25",
      "avatarUrl": "/avatars/abd2aae43e68c34770159c15a01c8297.svg",
      "fullname": "Shengyuan Zhang",
      "name": "SYZhang0805",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.06949",
      "authors": [
        {
          "_id": "67ff12ea58ed263257af79b5",
          "name": "Zhixuan Lin",
          "hidden": false
        },
        {
          "_id": "67ff12ea58ed263257af79b6",
          "name": "Johan Obando-Ceron",
          "hidden": false
        },
        {
          "_id": "67ff12ea58ed263257af79b7",
          "user": {
            "_id": "66906c4e37eadb9c577984d3",
            "avatarUrl": "/avatars/b81765472942fdf94c0ee885ca62df2d.svg",
            "isPro": false,
            "fullname": "Owen He",
            "user": "littleowen",
            "type": "user"
          },
          "name": "Xu Owen He",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-16T02:16:11.020Z",
          "hidden": false
        },
        {
          "_id": "67ff12ea58ed263257af79b8",
          "name": "Aaron Courville",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-09T14:57:55.000Z",
      "submittedOnDailyAt": "2025-04-16T01:28:49.703Z",
      "title": "Adaptive Computation Pruning for the Forgetting Transformer",
      "submittedOnDailyBy": {
        "_id": "6694cc1009326cb83f2d11bb",
        "avatarUrl": "/avatars/1ddaaed70a16ac475a9404848aef5d48.svg",
        "isPro": false,
        "fullname": "Zhixuan Lin",
        "user": "zhixuan-lin",
        "type": "user"
      },
      "summary": "The recently proposed Forgetting Transformer (FoX) incorporates a forget gate\ninto softmax attention and has shown consistently better or on-par performance\ncompared to the standard RoPE-based Transformer. Notably, many attention heads\nin FoX tend to forget quickly, causing their output at each timestep to rely\nprimarily on the local context. Based on this observation, we propose Adaptive\nComputation Pruning (ACP) for FoX, a method that dynamically prunes\ncomputations involving input-output dependencies that are strongly decayed by\nthe forget gate. This is achieved using a dynamically set pruning threshold\nthat ensures that the pruned attention weights remain negligible. We apply ACP\nto language model pretraining with FoX and show it consistently reduces the\nnumber of FLOPs in softmax attention by around 70% across different model sizes\nand context lengths, resulting in a roughly 10% to 35% improvement in training\nthroughput. Furthermore, longer context lengths yield greater computational\nsavings. All these speed improvements are achieved without any performance\ndegradation. We also perform several analyses to provide deeper insights into\nour method, such as examining the pruning patterns and analyzing the\ndistribution of FLOP savings across different attention heads. Our code is\navailable at https://github.com/zhixuan-lin/arctic-fox.",
      "upvotes": 2,
      "discussionId": "67ff12eb58ed263257af79fc",
      "ai_keywords": [
        "Forgetting Transformer (FoX)",
        "forget gate",
        "softmax attention",
        "RoPE-based Transformer",
        "Adaptive Computation Pruning (ACP)",
        "input-output dependencies",
        "pruning threshold",
        "FLOPs",
        "training throughput",
        "pruning patterns"
      ]
    },
    "publishedAt": "2025-04-09T10:57:55.000Z",
    "title": "Adaptive Computation Pruning for the Forgetting Transformer",
    "summary": "The recently proposed Forgetting Transformer (FoX) incorporates a forget gate\ninto softmax attention and has shown consistently better or on-par performance\ncompared to the standard RoPE-based Transformer. Notably, many attention heads\nin FoX tend to forget quickly, causing their output at each timestep to rely\nprimarily on the local context. Based on this observation, we propose Adaptive\nComputation Pruning (ACP) for FoX, a method that dynamically prunes\ncomputations involving input-output dependencies that are strongly decayed by\nthe forget gate. This is achieved using a dynamically set pruning threshold\nthat ensures that the pruned attention weights remain negligible. We apply ACP\nto language model pretraining with FoX and show it consistently reduces the\nnumber of FLOPs in softmax attention by around 70% across different model sizes\nand context lengths, resulting in a roughly 10% to 35% improvement in training\nthroughput. Furthermore, longer context lengths yield greater computational\nsavings. All these speed improvements are achieved without any performance\ndegradation. We also perform several analyses to provide deeper insights into\nour method, such as examining the pruning patterns and analyzing the\ndistribution of FLOP savings across different attention heads. Our code is\navailable at https://github.com/zhixuan-lin/arctic-fox.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.06949.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6694cc1009326cb83f2d11bb",
      "avatarUrl": "/avatars/1ddaaed70a16ac475a9404848aef5d48.svg",
      "fullname": "Zhixuan Lin",
      "name": "zhixuan-lin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.11326",
      "authors": [
        {
          "_id": "67ff25b765b52d1b69c1f6c1",
          "user": {
            "_id": "67ff29ecbf6889a333c69c7a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/fYIJFtF0uciB-1wb0lmTY.png",
            "isPro": false,
            "fullname": "Henghui Ding",
            "user": "HenghuiDing",
            "type": "user"
          },
          "name": "Henghui Ding",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-16T03:55:17.825Z",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6c2",
          "name": "Chang Liu",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6c3",
          "name": "Nikhila Ravi",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6c4",
          "name": "Shuting He",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6c5",
          "name": "Yunchao Wei",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6c6",
          "name": "Song Bai",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6c7",
          "name": "Philip Torr",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6c8",
          "name": "Kehuan Song",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6c9",
          "name": "Xinglin Xie",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6ca",
          "name": "Kexin Zhang",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6cb",
          "name": "Licheng Jiao",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6cc",
          "name": "Lingling Li",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6cd",
          "name": "Shuyuan Yang",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6ce",
          "name": "Xuqiang Cao",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6cf",
          "name": "Linnan Zhao",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6d0",
          "name": "Jiaxuan Zhao",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6d1",
          "name": "Fang Liu",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6d2",
          "name": "Mengjiao Wang",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6d3",
          "name": "Junpei Zhang",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6d4",
          "name": "Xu Liu",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6d5",
          "name": "Yuting Yang",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6d6",
          "name": "Mengru Ma",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6d7",
          "name": "Hao Fang",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6d8",
          "name": "Runmin Cong",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6d9",
          "name": "Xiankai Lu",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6da",
          "name": "Zhiyang Che",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6db",
          "name": "Wei Zhan",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6dc",
          "name": "Tianming Liang",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6dd",
          "name": "Haichao Jiang",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6de",
          "name": "Wei-Shi Zheng",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6df",
          "name": "Jian-Fang Hu",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6e0",
          "name": "Haobo Yuan",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6e1",
          "name": "Xiangtai Li",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6e2",
          "name": "Tao Zhang",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6e3",
          "name": "Lu Qi",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6e4",
          "name": "Ming-Hsuan Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-15T16:02:47.000Z",
      "submittedOnDailyAt": "2025-04-16T02:26:49.281Z",
      "title": "PVUW 2025 Challenge Report: Advances in Pixel-level Understanding of\n  Complex Videos in the Wild",
      "submittedOnDailyBy": {
        "_id": "67ff29ecbf6889a333c69c7a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/fYIJFtF0uciB-1wb0lmTY.png",
        "isPro": false,
        "fullname": "Henghui Ding",
        "user": "HenghuiDing",
        "type": "user"
      },
      "summary": "This report provides a comprehensive overview of the 4th Pixel-level Video\nUnderstanding in the Wild (PVUW) Challenge, held in conjunction with CVPR 2025.\nIt summarizes the challenge outcomes, participating methodologies, and future\nresearch directions. The challenge features two tracks: MOSE, which focuses on\ncomplex scene video object segmentation, and MeViS, which targets\nmotion-guided, language-based video segmentation. Both tracks introduce new,\nmore challenging datasets designed to better reflect real-world scenarios.\nThrough detailed evaluation and analysis, the challenge offers valuable\ninsights into the current state-of-the-art and emerging trends in complex video\nsegmentation. More information can be found on the workshop website:\nhttps://pvuw.github.io/.",
      "upvotes": 1,
      "discussionId": "67ff25b865b52d1b69c1f736"
    },
    "publishedAt": "2025-04-15T12:02:47.000Z",
    "title": "PVUW 2025 Challenge Report: Advances in Pixel-level Understanding of\n  Complex Videos in the Wild",
    "summary": "This report provides a comprehensive overview of the 4th Pixel-level Video\nUnderstanding in the Wild (PVUW) Challenge, held in conjunction with CVPR 2025.\nIt summarizes the challenge outcomes, participating methodologies, and future\nresearch directions. The challenge features two tracks: MOSE, which focuses on\ncomplex scene video object segmentation, and MeViS, which targets\nmotion-guided, language-based video segmentation. Both tracks introduce new,\nmore challenging datasets designed to better reflect real-world scenarios.\nThrough detailed evaluation and analysis, the challenge offers valuable\ninsights into the current state-of-the-art and emerging trends in complex video\nsegmentation. More information can be found on the workshop website:\nhttps://pvuw.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11326.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67ff29ecbf6889a333c69c7a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/fYIJFtF0uciB-1wb0lmTY.png",
      "fullname": "Henghui Ding",
      "name": "HenghuiDing",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.10049",
      "authors": [
        {
          "_id": "67ff5b335cf0fe153845d1c9",
          "name": "Tho Gigant",
          "hidden": false
        },
        {
          "_id": "67ff5b335cf0fe153845d1ca",
          "name": "Camille Guinaudeau",
          "hidden": false
        },
        {
          "_id": "67ff5b335cf0fe153845d1cb",
          "name": "Frdric Dufaux",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T09:55:01.000Z",
      "submittedOnDailyAt": "2025-04-16T05:55:17.253Z",
      "title": "Summarization of Multimodal Presentations with Vision-Language Models:\n  Study of the Effect of Modalities and Structure",
      "submittedOnDailyBy": {
        "_id": "60d35154d7b174177faabd55",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1645712223620-60d35154d7b174177faabd55.jpeg",
        "isPro": false,
        "fullname": "Tho Gigant",
        "user": "gigant",
        "type": "user"
      },
      "summary": "Vision-Language Models (VLMs) can process visual and textual information in\nmultiple formats: texts, images, interleaved texts and images, or even\nhour-long videos. In this work, we conduct fine-grained quantitative and\nqualitative analyses of automatic summarization of multimodal presentations\nusing VLMs with various representations as input. From these experiments, we\nsuggest cost-effective strategies for generating summaries from text-heavy\nmultimodal documents under different input-length budgets using VLMs. We show\nthat slides extracted from the video stream can be beneficially used as input\nagainst the raw video, and that a structured representation from interleaved\nslides and transcript provides the best performance. Finally, we reflect and\ncomment on the nature of cross-modal interactions in multimodal presentations\nand share suggestions to improve the capabilities of VLMs to understand\ndocuments of this nature.",
      "upvotes": 1,
      "discussionId": "67ff5b355cf0fe153845d215",
      "ai_keywords": [
        "Vision-Language Models (VLMs)",
        "automatic summarization",
        "multimodal presentations",
        "text-heavy multimodal documents",
        "input-length budgets",
        "slides",
        "video stream",
        "raw video",
        "structured representation",
        "interleaved slides",
        "transcript",
        "cross-modal interactions"
      ]
    },
    "publishedAt": "2025-04-14T05:55:01.000Z",
    "title": "Summarization of Multimodal Presentations with Vision-Language Models:\n  Study of the Effect of Modalities and Structure",
    "summary": "Vision-Language Models (VLMs) can process visual and textual information in\nmultiple formats: texts, images, interleaved texts and images, or even\nhour-long videos. In this work, we conduct fine-grained quantitative and\nqualitative analyses of automatic summarization of multimodal presentations\nusing VLMs with various representations as input. From these experiments, we\nsuggest cost-effective strategies for generating summaries from text-heavy\nmultimodal documents under different input-length budgets using VLMs. We show\nthat slides extracted from the video stream can be beneficially used as input\nagainst the raw video, and that a structured representation from interleaved\nslides and transcript provides the best performance. Finally, we reflect and\ncomment on the nature of cross-modal interactions in multimodal presentations\nand share suggestions to improve the capabilities of VLMs to understand\ndocuments of this nature.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10049.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60d35154d7b174177faabd55",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1645712223620-60d35154d7b174177faabd55.jpeg",
      "fullname": "Tho Gigant",
      "name": "gigant",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 32
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.11001",
      "authors": [
        {
          "_id": "67ff4bdb1dc5d56fdd7a1bc4",
          "user": {
            "_id": "62d7b2339b629105a5d6888a",
            "avatarUrl": "/avatars/c3f164fde6b8f9a671890e08ce8a3e75.svg",
            "isPro": false,
            "fullname": "Alan Dao",
            "user": "alandao",
            "type": "user"
          },
          "name": "Alan Dao",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-16T06:19:08.903Z",
          "hidden": false
        },
        {
          "_id": "67ff4bdb1dc5d56fdd7a1bc5",
          "name": "Thinh Le",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62d7b2339b629105a5d6888a/s2uxHvFhyqjBXMY5wkDLy.mp4"
      ],
      "publishedAt": "2025-04-15T09:18:21.000Z",
      "submittedOnDailyAt": "2025-04-16T04:49:41.555Z",
      "title": "ReZero: Enhancing LLM search ability by trying one-more-time",
      "submittedOnDailyBy": {
        "_id": "62d7b2339b629105a5d6888a",
        "avatarUrl": "/avatars/c3f164fde6b8f9a671890e08ce8a3e75.svg",
        "isPro": false,
        "fullname": "Alan Dao",
        "user": "alandao",
        "type": "user"
      },
      "summary": "Retrieval-Augmented Generation (RAG) improves Large Language Model (LLM)\nperformance on knowledge-intensive tasks but depends heavily on initial search\nquery quality. Current methods, often using Reinforcement Learning (RL),\ntypically focus on query formulation or reasoning over results, without\nexplicitly encouraging persistence after a failed search. We introduce ReZero\n(Retry-Zero), a novel RL framework that directly rewards the act of retrying a\nsearch query following an initial unsuccessful attempt. This incentivizes the\nLLM to explore alternative queries rather than prematurely halting. ReZero\ndemonstrates significant improvement, achieving 46.88% accuracy compared to a\n25% baseline. By rewarding persistence, ReZero enhances LLM robustness in\ncomplex information-seeking scenarios where initial queries may prove\ninsufficient.",
      "upvotes": 0,
      "discussionId": "67ff4bdc1dc5d56fdd7a1c36",
      "githubRepo": "https://github.com/menloresearch/ReZero",
      "ai_keywords": [
        "Retrieval-Augmented Generation (RAG)",
        "Large Language Model (LLM)",
        "knowledge-intensive tasks",
        "Reinforcement Learning (RL)",
        "query formulation",
        "reasoning over results",
        "ReZero (Retry-Zero)",
        "persistence",
        "search query",
        "unsuccessful attempt",
        "alternative queries",
        "robustness",
        "information-seeking scenarios"
      ]
    },
    "publishedAt": "2025-04-15T05:18:21.000Z",
    "title": "ReZero: Enhancing LLM search ability by trying one-more-time",
    "summary": "Retrieval-Augmented Generation (RAG) improves Large Language Model (LLM)\nperformance on knowledge-intensive tasks but depends heavily on initial search\nquery quality. Current methods, often using Reinforcement Learning (RL),\ntypically focus on query formulation or reasoning over results, without\nexplicitly encouraging persistence after a failed search. We introduce ReZero\n(Retry-Zero), a novel RL framework that directly rewards the act of retrying a\nsearch query following an initial unsuccessful attempt. This incentivizes the\nLLM to explore alternative queries rather than prematurely halting. ReZero\ndemonstrates significant improvement, achieving 46.88% accuracy compared to a\n25% baseline. By rewarding persistence, ReZero enhances LLM robustness in\ncomplex information-seeking scenarios where initial queries may prove\ninsufficient.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62d7b2339b629105a5d6888a/s2uxHvFhyqjBXMY5wkDLy.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11001.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62d7b2339b629105a5d6888a",
      "avatarUrl": "/avatars/c3f164fde6b8f9a671890e08ce8a3e75.svg",
      "fullname": "Alan Dao",
      "name": "alandao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  }
]