[
  {
    "paper": {
      "id": "2504.05741",
      "authors": [
        {
          "_id": "67f726dc0b5aa5777fd3a431",
          "user": {
            "_id": "66615c855fd9d736e670e0a9",
            "avatarUrl": "/avatars/0ff3127b513552432a7c651e21d7f283.svg",
            "isPro": false,
            "fullname": "wangshuai",
            "user": "wangsssssss",
            "type": "user"
          },
          "name": "Shuai Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-10T06:44:49.192Z",
          "hidden": false
        },
        {
          "_id": "67f726dc0b5aa5777fd3a432",
          "name": "Zhi Tian",
          "hidden": false
        },
        {
          "_id": "67f726dc0b5aa5777fd3a433",
          "name": "Weilin Huang",
          "hidden": false
        },
        {
          "_id": "67f726dc0b5aa5777fd3a434",
          "name": "Limin Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-08T07:17:45.000Z",
      "submittedOnDailyAt": "2025-04-10T00:40:02.945Z",
      "title": "DDT: Decoupled Diffusion Transformer",
      "submittedOnDailyBy": {
        "_id": "66615c855fd9d736e670e0a9",
        "avatarUrl": "/avatars/0ff3127b513552432a7c651e21d7f283.svg",
        "isPro": false,
        "fullname": "wangshuai",
        "user": "wangsssssss",
        "type": "user"
      },
      "summary": "Diffusion transformers have demonstrated remarkable generation quality,\nalbeit requiring longer training iterations and numerous inference steps. In\neach denoising step, diffusion transformers encode the noisy inputs to extract\nthe lower-frequency semantic component and then decode the higher frequency\nwith identical modules. This scheme creates an inherent optimization dilemma:\nencoding low-frequency semantics necessitates reducing high-frequency\ncomponents, creating tension between semantic encoding and high-frequency\ndecoding. To resolve this challenge, we propose a new\n\\color{ddtD}ecoupled \\color{ddtD}iffusion\n\\color{ddtT}ransformer~(\\color{ddtDDT}), with a decoupled\ndesign of a dedicated condition encoder for semantic extraction alongside a\nspecialized velocity decoder. Our experiments reveal that a more substantial\nencoder yields performance improvements as model size increases. For ImageNet\n256times256, Our DDT-XL/2 achieves a new state-of-the-art performance of\n{1.31 FID}~(nearly 4times faster training convergence compared to previous\ndiffusion transformers). For ImageNet 512times512, Our DDT-XL/2 achieves a\nnew state-of-the-art FID of 1.28. Additionally, as a beneficial by-product, our\ndecoupled architecture enhances inference speed by enabling the sharing\nself-condition between adjacent denoising steps. To minimize performance\ndegradation, we propose a novel statistical dynamic programming approach to\nidentify optimal sharing strategies.",
      "upvotes": 27,
      "discussionId": "67f726dd0b5aa5777fd3a463",
      "githubRepo": "https://github.com/MCG-NJU/DDT"
    },
    "publishedAt": "2025-04-08T03:17:45.000Z",
    "title": "DDT: Decoupled Diffusion Transformer",
    "summary": "Diffusion transformers have demonstrated remarkable generation quality,\nalbeit requiring longer training iterations and numerous inference steps. In\neach denoising step, diffusion transformers encode the noisy inputs to extract\nthe lower-frequency semantic component and then decode the higher frequency\nwith identical modules. This scheme creates an inherent optimization dilemma:\nencoding low-frequency semantics necessitates reducing high-frequency\ncomponents, creating tension between semantic encoding and high-frequency\ndecoding. To resolve this challenge, we propose a new\n\\color{ddtD}ecoupled \\color{ddtD}iffusion\n\\color{ddtT}ransformer~(\\color{ddtDDT}), with a decoupled\ndesign of a dedicated condition encoder for semantic extraction alongside a\nspecialized velocity decoder. Our experiments reveal that a more substantial\nencoder yields performance improvements as model size increases. For ImageNet\n256times256, Our DDT-XL/2 achieves a new state-of-the-art performance of\n{1.31 FID}~(nearly 4times faster training convergence compared to previous\ndiffusion transformers). For ImageNet 512times512, Our DDT-XL/2 achieves a\nnew state-of-the-art FID of 1.28. Additionally, as a beneficial by-product, our\ndecoupled architecture enhances inference speed by enabling the sharing\nself-condition between adjacent denoising steps. To minimize performance\ndegradation, we propose a novel statistical dynamic programming approach to\nidentify optimal sharing strategies.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05741.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66615c855fd9d736e670e0a9",
      "avatarUrl": "/avatars/0ff3127b513552432a7c651e21d7f283.svg",
      "fullname": "wangshuai",
      "name": "wangsssssss",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.07083",
      "authors": [
        {
          "_id": "67f72c452eec6ce5c8b9e8e6",
          "user": {
            "_id": "64de20c5808492ba6e65d124",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64de20c5808492ba6e65d124/58IX_TI5vJw73qS1knw56.jpeg",
            "isPro": false,
            "fullname": "Zhang Mengchen",
            "user": "Dubhe-zmc",
            "type": "user"
          },
          "name": "Mengchen Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-10T06:39:42.813Z",
          "hidden": false
        },
        {
          "_id": "67f72c452eec6ce5c8b9e8e7",
          "name": "Tong Wu",
          "hidden": false
        },
        {
          "_id": "67f72c452eec6ce5c8b9e8e8",
          "name": "Jing Tan",
          "hidden": false
        },
        {
          "_id": "67f72c452eec6ce5c8b9e8e9",
          "name": "Ziwei Liu",
          "hidden": false
        },
        {
          "_id": "67f72c452eec6ce5c8b9e8ea",
          "name": "Gordon Wetzstein",
          "hidden": false
        },
        {
          "_id": "67f72c452eec6ce5c8b9e8eb",
          "name": "Dahua Lin",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64de20c5808492ba6e65d124/b1N08r8EbruYc8Yapg4J9.qt"
      ],
      "publishedAt": "2025-04-09T17:56:01.000Z",
      "submittedOnDailyAt": "2025-04-10T01:13:43.884Z",
      "title": "GenDoP: Auto-regressive Camera Trajectory Generation as a Director of\n  Photography",
      "submittedOnDailyBy": {
        "_id": "64de20c5808492ba6e65d124",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64de20c5808492ba6e65d124/58IX_TI5vJw73qS1knw56.jpeg",
        "isPro": false,
        "fullname": "Zhang Mengchen",
        "user": "Dubhe-zmc",
        "type": "user"
      },
      "summary": "Camera trajectory design plays a crucial role in video production, serving as\na fundamental tool for conveying directorial intent and enhancing visual\nstorytelling. In cinematography, Directors of Photography meticulously craft\ncamera movements to achieve expressive and intentional framing. However,\nexisting methods for camera trajectory generation remain limited: Traditional\napproaches rely on geometric optimization or handcrafted procedural systems,\nwhile recent learning-based methods often inherit structural biases or lack\ntextual alignment, constraining creative synthesis. In this work, we introduce\nan auto-regressive model inspired by the expertise of Directors of Photography\nto generate artistic and expressive camera trajectories. We first introduce\nDataDoP, a large-scale multi-modal dataset containing 29K real-world shots with\nfree-moving camera trajectories, depth maps, and detailed captions in specific\nmovements, interaction with the scene, and directorial intent. Thanks to the\ncomprehensive and diverse database, we further train an auto-regressive,\ndecoder-only Transformer for high-quality, context-aware camera movement\ngeneration based on text guidance and RGBD inputs, named GenDoP. Extensive\nexperiments demonstrate that compared to existing methods, GenDoP offers better\ncontrollability, finer-grained trajectory adjustments, and higher motion\nstability. We believe our approach establishes a new standard for\nlearning-based cinematography, paving the way for future advancements in camera\ncontrol and filmmaking. Our project website:\nhttps://kszpxxzmc.github.io/GenDoP/.",
      "upvotes": 16,
      "discussionId": "67f72c472eec6ce5c8b9e97b",
      "projectPage": "https://kszpxxzmc.github.io/GenDoP/",
      "githubRepo": "https://github.com/3DTopia/GenDoP"
    },
    "publishedAt": "2025-04-09T13:56:01.000Z",
    "title": "GenDoP: Auto-regressive Camera Trajectory Generation as a Director of\n  Photography",
    "summary": "Camera trajectory design plays a crucial role in video production, serving as\na fundamental tool for conveying directorial intent and enhancing visual\nstorytelling. In cinematography, Directors of Photography meticulously craft\ncamera movements to achieve expressive and intentional framing. However,\nexisting methods for camera trajectory generation remain limited: Traditional\napproaches rely on geometric optimization or handcrafted procedural systems,\nwhile recent learning-based methods often inherit structural biases or lack\ntextual alignment, constraining creative synthesis. In this work, we introduce\nan auto-regressive model inspired by the expertise of Directors of Photography\nto generate artistic and expressive camera trajectories. We first introduce\nDataDoP, a large-scale multi-modal dataset containing 29K real-world shots with\nfree-moving camera trajectories, depth maps, and detailed captions in specific\nmovements, interaction with the scene, and directorial intent. Thanks to the\ncomprehensive and diverse database, we further train an auto-regressive,\ndecoder-only Transformer for high-quality, context-aware camera movement\ngeneration based on text guidance and RGBD inputs, named GenDoP. Extensive\nexperiments demonstrate that compared to existing methods, GenDoP offers better\ncontrollability, finer-grained trajectory adjustments, and higher motion\nstability. We believe our approach establishes a new standard for\nlearning-based cinematography, paving the way for future advancements in camera\ncontrol and filmmaking. Our project website:\nhttps://kszpxxzmc.github.io/GenDoP/.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64de20c5808492ba6e65d124/b1N08r8EbruYc8Yapg4J9.qt"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07083.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64de20c5808492ba6e65d124",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64de20c5808492ba6e65d124/58IX_TI5vJw73qS1knw56.jpeg",
      "fullname": "Zhang Mengchen",
      "name": "Dubhe-zmc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.07096",
      "authors": [
        {
          "_id": "67f72bb1f9d51b79dca06d0a",
          "user": {
            "_id": "635f46d1928a42bc95cfcf7c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/635f46d1928a42bc95cfcf7c/5KF8aLiDCJdl7B1SdJ-7V.png",
            "isPro": false,
            "fullname": "Jiacheng Liu",
            "user": "liujch1998",
            "type": "user"
          },
          "name": "Jiacheng Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-10T06:39:44.913Z",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d0b",
          "name": "Taylor Blanton",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d0c",
          "name": "Yanai Elazar",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d0d",
          "name": "Sewon Min",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d0e",
          "name": "YenSung Chen",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d0f",
          "name": "Arnavi Chheda-Kothary",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d10",
          "name": "Huy Tran",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d11",
          "name": "Byron Bischoff",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d12",
          "name": "Eric Marsh",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d13",
          "name": "Michael Schmitz",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d14",
          "name": "Cassidy Trier",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d15",
          "name": "Aaron Sarnat",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d16",
          "name": "Jenna James",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d17",
          "name": "Jon Borchardt",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d18",
          "name": "Bailey Kuehl",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d19",
          "name": "Evie Cheng",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d1a",
          "name": "Karen Farley",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d1b",
          "name": "Sruthi Sreeram",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d1c",
          "name": "Taira Anderson",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d1d",
          "name": "David Albright",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d1e",
          "name": "Carissa Schoenick",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d1f",
          "name": "Luca Soldaini",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d20",
          "name": "Dirk Groeneveld",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d21",
          "name": "Rock Yuren Pang",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d22",
          "name": "Pang Wei Koh",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d23",
          "name": "Noah A. Smith",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d24",
          "name": "Sophie Lebrecht",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d25",
          "name": "Yejin Choi",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d26",
          "name": "Hannaneh Hajishirzi",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d27",
          "name": "Ali Farhadi",
          "hidden": false
        },
        {
          "_id": "67f72bb1f9d51b79dca06d28",
          "name": "Jesse Dodge",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-09T17:59:35.000Z",
      "submittedOnDailyAt": "2025-04-10T00:54:36.448Z",
      "title": "OLMoTrace: Tracing Language Model Outputs Back to Trillions of Training\n  Tokens",
      "submittedOnDailyBy": {
        "_id": "635f46d1928a42bc95cfcf7c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/635f46d1928a42bc95cfcf7c/5KF8aLiDCJdl7B1SdJ-7V.png",
        "isPro": false,
        "fullname": "Jiacheng Liu",
        "user": "liujch1998",
        "type": "user"
      },
      "summary": "We present OLMoTrace, the first system that traces the outputs of language\nmodels back to their full, multi-trillion-token training data in real time.\nOLMoTrace finds and shows verbatim matches between segments of language model\noutput and documents in the training text corpora. Powered by an extended\nversion of infini-gram (Liu et al., 2024), our system returns tracing results\nwithin a few seconds. OLMoTrace can help users understand the behavior of\nlanguage models through the lens of their training data. We showcase how it can\nbe used to explore fact checking, hallucination, and the creativity of language\nmodels. OLMoTrace is publicly available and fully open-source.",
      "upvotes": 8,
      "discussionId": "67f72bb3f9d51b79dca06d8c"
    },
    "publishedAt": "2025-04-09T13:59:35.000Z",
    "title": "OLMoTrace: Tracing Language Model Outputs Back to Trillions of Training\n  Tokens",
    "summary": "We present OLMoTrace, the first system that traces the outputs of language\nmodels back to their full, multi-trillion-token training data in real time.\nOLMoTrace finds and shows verbatim matches between segments of language model\noutput and documents in the training text corpora. Powered by an extended\nversion of infini-gram (Liu et al., 2024), our system returns tracing results\nwithin a few seconds. OLMoTrace can help users understand the behavior of\nlanguage models through the lens of their training data. We showcase how it can\nbe used to explore fact checking, hallucination, and the creativity of language\nmodels. OLMoTrace is publicly available and fully open-source.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07096.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "635f46d1928a42bc95cfcf7c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/635f46d1928a42bc95cfcf7c/5KF8aLiDCJdl7B1SdJ-7V.png",
      "fullname": "Jiacheng Liu",
      "name": "liujch1998",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 18
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.06514",
      "authors": [
        {
          "_id": "67f72e933eacf8888816f3b0",
          "name": "Chenrui Fan",
          "hidden": false
        },
        {
          "_id": "67f72e933eacf8888816f3b1",
          "name": "Ming Li",
          "hidden": false
        },
        {
          "_id": "67f72e933eacf8888816f3b2",
          "name": "Lichao Sun",
          "hidden": false
        },
        {
          "_id": "67f72e933eacf8888816f3b3",
          "user": {
            "_id": "647f5af5b0e96764589f3b2a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
            "isPro": false,
            "fullname": "Tianyi Zhou",
            "user": "zhoutianyi",
            "type": "user"
          },
          "name": "Tianyi Zhou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-10T06:39:37.906Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/FrHRMBKuB2v57LZWVJPxi.png"
      ],
      "publishedAt": "2025-04-09T01:25:27.000Z",
      "submittedOnDailyAt": "2025-04-10T01:07:13.718Z",
      "title": "Missing Premise exacerbates Overthinking: Are Reasoning Models losing\n  Critical Thinking Skill?",
      "submittedOnDailyBy": {
        "_id": "647f5af5b0e96764589f3b2a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
        "isPro": false,
        "fullname": "Tianyi Zhou",
        "user": "zhoutianyi",
        "type": "user"
      },
      "summary": "We find that the response length of reasoning LLMs, whether trained by\nreinforcement learning or supervised learning, drastically increases for\nill-posed questions with missing premises (MiP), ending up with redundant and\nineffective thinking. This newly introduced scenario exacerbates the general\noverthinking issue to a large extent, which we name as the MiP-Overthinking.\nSuch failures are against the ``test-time scaling law'' but have been widely\nobserved on multiple datasets we curated with MiP, indicating the harm of cheap\noverthinking and a lack of critical thinking. Surprisingly, LLMs not\nspecifically trained for reasoning exhibit much better performance on the MiP\nscenario, producing much shorter responses that quickly identify ill-posed\nqueries. This implies a critical flaw of the current training recipe for\nreasoning LLMs, which does not encourage efficient thinking adequately, leading\nto the abuse of thinking patterns. To further investigate the reasons behind\nsuch failures, we conduct fine-grained analyses of the reasoning length,\noverthinking patterns, and location of critical thinking on different types of\nLLMs. Moreover, our extended ablation study reveals that the overthinking is\ncontagious through the distillation of reasoning models' responses. These\nresults improve the understanding of overthinking and shed novel insights into\nmitigating the problem.",
      "upvotes": 8,
      "discussionId": "67f72e943eacf8888816f3fa",
      "githubRepo": "https://github.com/tianyi-lab/MiP-Overthinking"
    },
    "publishedAt": "2025-04-08T21:25:27.000Z",
    "title": "Missing Premise exacerbates Overthinking: Are Reasoning Models losing\n  Critical Thinking Skill?",
    "summary": "We find that the response length of reasoning LLMs, whether trained by\nreinforcement learning or supervised learning, drastically increases for\nill-posed questions with missing premises (MiP), ending up with redundant and\nineffective thinking. This newly introduced scenario exacerbates the general\noverthinking issue to a large extent, which we name as the MiP-Overthinking.\nSuch failures are against the ``test-time scaling law'' but have been widely\nobserved on multiple datasets we curated with MiP, indicating the harm of cheap\noverthinking and a lack of critical thinking. Surprisingly, LLMs not\nspecifically trained for reasoning exhibit much better performance on the MiP\nscenario, producing much shorter responses that quickly identify ill-posed\nqueries. This implies a critical flaw of the current training recipe for\nreasoning LLMs, which does not encourage efficient thinking adequately, leading\nto the abuse of thinking patterns. To further investigate the reasons behind\nsuch failures, we conduct fine-grained analyses of the reasoning length,\noverthinking patterns, and location of critical thinking on different types of\nLLMs. Moreover, our extended ablation study reveals that the overthinking is\ncontagious through the distillation of reasoning models' responses. These\nresults improve the understanding of overthinking and shed novel insights into\nmitigating the problem.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/FrHRMBKuB2v57LZWVJPxi.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.06514.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "647f5af5b0e96764589f3b2a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
      "fullname": "Tianyi Zhou",
      "name": "zhoutianyi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.07086",
      "authors": [
        {
          "_id": "67f75609b2d783993db63aba",
          "name": "Andreas Hochlehnert",
          "hidden": false
        },
        {
          "_id": "67f75609b2d783993db63abb",
          "name": "Hardik Bhatnagar",
          "hidden": false
        },
        {
          "_id": "67f75609b2d783993db63abc",
          "name": "Vishaal Udandarao",
          "hidden": false
        },
        {
          "_id": "67f75609b2d783993db63abd",
          "name": "Samuel Albanie",
          "hidden": false
        },
        {
          "_id": "67f75609b2d783993db63abe",
          "user": {
            "_id": "6464a0d41683d3c81f51924a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6464a0d41683d3c81f51924a/s7yYVwfUB4WOhVFJS6A6T.jpeg",
            "isPro": false,
            "fullname": "Ameya Prabhu",
            "user": "AmeyaPrabhu",
            "type": "user"
          },
          "name": "Ameya Prabhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-10T06:39:24.192Z",
          "hidden": false
        },
        {
          "_id": "67f75609b2d783993db63abf",
          "name": "Matthias Bethge",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-09T17:58:17.000Z",
      "submittedOnDailyAt": "2025-04-10T03:54:51.677Z",
      "title": "A Sober Look at Progress in Language Model Reasoning: Pitfalls and Paths\n  to Reproducibility",
      "submittedOnDailyBy": {
        "_id": "6464a0d41683d3c81f51924a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6464a0d41683d3c81f51924a/s7yYVwfUB4WOhVFJS6A6T.jpeg",
        "isPro": false,
        "fullname": "Ameya Prabhu",
        "user": "AmeyaPrabhu",
        "type": "user"
      },
      "summary": "Reasoning has emerged as the next major frontier for language models (LMs),\nwith rapid advances from both academic and industrial labs. However, this\nprogress often outpaces methodological rigor, with many evaluations relying on\nbenchmarking practices that lack transparency, robustness, or statistical\ngrounding. In this work, we conduct a comprehensive empirical study and find\nthat current mathematical reasoning benchmarks are highly sensitive to subtle\nimplementation choices - including decoding parameters, random seeds, prompt\nformatting, and even hardware and software-framework configurations.\nPerformance gains reported in recent studies frequently hinge on unclear\ncomparisons or unreported sources of variance. To address these issues, we\npropose a standardized evaluation framework with clearly defined best practices\nand reporting standards. Using this framework, we reassess recent methods and\nfind that reinforcement learning (RL) approaches yield only modest improvements\n- far below prior claims - and are prone to overfitting, especially on\nsmall-scale benchmarks like AIME24. In contrast, supervised finetuning (SFT)\nmethods show consistently stronger generalization. To foster reproducibility,\nwe release all code, prompts, and model outputs, for reasoning benchmarks,\nestablishing more rigorous foundations for future work.",
      "upvotes": 4,
      "discussionId": "67f7560cb2d783993db63b6b"
    },
    "publishedAt": "2025-04-09T13:58:17.000Z",
    "title": "A Sober Look at Progress in Language Model Reasoning: Pitfalls and Paths\n  to Reproducibility",
    "summary": "Reasoning has emerged as the next major frontier for language models (LMs),\nwith rapid advances from both academic and industrial labs. However, this\nprogress often outpaces methodological rigor, with many evaluations relying on\nbenchmarking practices that lack transparency, robustness, or statistical\ngrounding. In this work, we conduct a comprehensive empirical study and find\nthat current mathematical reasoning benchmarks are highly sensitive to subtle\nimplementation choices - including decoding parameters, random seeds, prompt\nformatting, and even hardware and software-framework configurations.\nPerformance gains reported in recent studies frequently hinge on unclear\ncomparisons or unreported sources of variance. To address these issues, we\npropose a standardized evaluation framework with clearly defined best practices\nand reporting standards. Using this framework, we reassess recent methods and\nfind that reinforcement learning (RL) approaches yield only modest improvements\n- far below prior claims - and are prone to overfitting, especially on\nsmall-scale benchmarks like AIME24. In contrast, supervised finetuning (SFT)\nmethods show consistently stronger generalization. To foster reproducibility,\nwe release all code, prompts, and model outputs, for reasoning benchmarks,\nestablishing more rigorous foundations for future work.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07086.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6464a0d41683d3c81f51924a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6464a0d41683d3c81f51924a/s7yYVwfUB4WOhVFJS6A6T.jpeg",
      "fullname": "Ameya Prabhu",
      "name": "AmeyaPrabhu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.04842",
      "authors": [
        {
          "_id": "67f72ca8353d129fc7bdd504",
          "name": "Mengchao Wang",
          "hidden": false
        },
        {
          "_id": "67f72ca8353d129fc7bdd505",
          "user": {
            "_id": "653b195c5f1703225b2fd571",
            "avatarUrl": "/avatars/b7f376225cef6c13952c9c5540dd43be.svg",
            "isPro": false,
            "fullname": "wangqiang",
            "user": "wangqiang9",
            "type": "user"
          },
          "name": "Qiang Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-10T06:39:40.647Z",
          "hidden": false
        },
        {
          "_id": "67f72ca8353d129fc7bdd506",
          "name": "Fan Jiang",
          "hidden": false
        },
        {
          "_id": "67f72ca8353d129fc7bdd507",
          "name": "Yaqi Fan",
          "hidden": false
        },
        {
          "_id": "67f72ca8353d129fc7bdd508",
          "name": "Yunpeng Zhang",
          "hidden": false
        },
        {
          "_id": "67f72ca8353d129fc7bdd509",
          "name": "Yonggang Qi",
          "hidden": false
        },
        {
          "_id": "67f72ca8353d129fc7bdd50a",
          "name": "Kun Zhao",
          "hidden": false
        },
        {
          "_id": "67f72ca8353d129fc7bdd50b",
          "name": "Mu Xu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-07T08:56:01.000Z",
      "submittedOnDailyAt": "2025-04-10T00:58:44.876Z",
      "title": "FantasyTalking: Realistic Talking Portrait Generation via Coherent\n  Motion Synthesis",
      "submittedOnDailyBy": {
        "_id": "653b195c5f1703225b2fd571",
        "avatarUrl": "/avatars/b7f376225cef6c13952c9c5540dd43be.svg",
        "isPro": false,
        "fullname": "wangqiang",
        "user": "wangqiang9",
        "type": "user"
      },
      "summary": "Creating a realistic animatable avatar from a single static portrait remains\nchallenging. Existing approaches often struggle to capture subtle facial\nexpressions, the associated global body movements, and the dynamic background.\nTo address these limitations, we propose a novel framework that leverages a\npretrained video diffusion transformer model to generate high-fidelity,\ncoherent talking portraits with controllable motion dynamics. At the core of\nour work is a dual-stage audio-visual alignment strategy. In the first stage,\nwe employ a clip-level training scheme to establish coherent global motion by\naligning audio-driven dynamics across the entire scene, including the reference\nportrait, contextual objects, and background. In the second stage, we refine\nlip movements at the frame level using a lip-tracing mask, ensuring precise\nsynchronization with audio signals. To preserve identity without compromising\nmotion flexibility, we replace the commonly used reference network with a\nfacial-focused cross-attention module that effectively maintains facial\nconsistency throughout the video. Furthermore, we integrate a motion intensity\nmodulation module that explicitly controls expression and body motion\nintensity, enabling controllable manipulation of portrait movements beyond mere\nlip motion. Extensive experimental results show that our proposed approach\nachieves higher quality with better realism, coherence, motion intensity, and\nidentity preservation. Ours project page:\nhttps://fantasy-amap.github.io/fantasy-talking/.",
      "upvotes": 4,
      "discussionId": "67f72cac353d129fc7bdd60f",
      "projectPage": "https://fantasy-amap.github.io/fantasy-talking/",
      "githubRepo": "https://github.com/Fantasy-AMAP/fantasy-talking"
    },
    "publishedAt": "2025-04-07T04:56:01.000Z",
    "title": "FantasyTalking: Realistic Talking Portrait Generation via Coherent\n  Motion Synthesis",
    "summary": "Creating a realistic animatable avatar from a single static portrait remains\nchallenging. Existing approaches often struggle to capture subtle facial\nexpressions, the associated global body movements, and the dynamic background.\nTo address these limitations, we propose a novel framework that leverages a\npretrained video diffusion transformer model to generate high-fidelity,\ncoherent talking portraits with controllable motion dynamics. At the core of\nour work is a dual-stage audio-visual alignment strategy. In the first stage,\nwe employ a clip-level training scheme to establish coherent global motion by\naligning audio-driven dynamics across the entire scene, including the reference\nportrait, contextual objects, and background. In the second stage, we refine\nlip movements at the frame level using a lip-tracing mask, ensuring precise\nsynchronization with audio signals. To preserve identity without compromising\nmotion flexibility, we replace the commonly used reference network with a\nfacial-focused cross-attention module that effectively maintains facial\nconsistency throughout the video. Furthermore, we integrate a motion intensity\nmodulation module that explicitly controls expression and body motion\nintensity, enabling controllable manipulation of portrait movements beyond mere\nlip motion. Extensive experimental results show that our proposed approach\nachieves higher quality with better realism, coherence, motion intensity, and\nidentity preservation. Ours project page:\nhttps://fantasy-amap.github.io/fantasy-talking/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.04842.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "653b195c5f1703225b2fd571",
      "avatarUrl": "/avatars/b7f376225cef6c13952c9c5540dd43be.svg",
      "fullname": "wangqiang",
      "name": "wangqiang9",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.04010",
      "authors": [
        {
          "_id": "67f766cb1879ad2f13bee3d1",
          "name": "Maksim Siniukov",
          "hidden": false
        },
        {
          "_id": "67f766cb1879ad2f13bee3d2",
          "name": "Di Chang",
          "hidden": false
        },
        {
          "_id": "67f766cb1879ad2f13bee3d3",
          "name": "Minh Tran",
          "hidden": false
        },
        {
          "_id": "67f766cb1879ad2f13bee3d4",
          "name": "Hongkun Gong",
          "hidden": false
        },
        {
          "_id": "67f766cb1879ad2f13bee3d5",
          "name": "Ashutosh Chaubey",
          "hidden": false
        },
        {
          "_id": "67f766cb1879ad2f13bee3d6",
          "name": "Mohammad Soleymani",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-05T01:19:46.000Z",
      "submittedOnDailyAt": "2025-04-10T05:06:42.197Z",
      "title": "DiTaiListener: Controllable High Fidelity Listener Video Generation with\n  Diffusion",
      "submittedOnDailyBy": {
        "_id": "64a5d8219f3b568c202b3137",
        "avatarUrl": "/avatars/eef6fb7c70d272555a53183c0e50dbaf.svg",
        "isPro": false,
        "fullname": "Di Chang",
        "user": "Boese0601",
        "type": "user"
      },
      "summary": "Generating naturalistic and nuanced listener motions for extended\ninteractions remains an open problem. Existing methods often rely on\nlow-dimensional motion codes for facial behavior generation followed by\nphotorealistic rendering, limiting both visual fidelity and expressive\nrichness. To address these challenges, we introduce DiTaiListener, powered by a\nvideo diffusion model with multimodal conditions. Our approach first generates\nshort segments of listener responses conditioned on the speaker's speech and\nfacial motions with DiTaiListener-Gen. It then refines the transitional frames\nvia DiTaiListener-Edit for a seamless transition. Specifically,\nDiTaiListener-Gen adapts a Diffusion Transformer (DiT) for the task of listener\nhead portrait generation by introducing a Causal Temporal Multimodal Adapter\n(CTM-Adapter) to process speakers' auditory and visual cues. CTM-Adapter\nintegrates speakers' input in a causal manner into the video generation process\nto ensure temporally coherent listener responses. For long-form video\ngeneration, we introduce DiTaiListener-Edit, a transition refinement\nvideo-to-video diffusion model. The model fuses video segments into smooth and\ncontinuous videos, ensuring temporal consistency in facial expressions and\nimage quality when merging short video segments produced by DiTaiListener-Gen.\nQuantitatively, DiTaiListener achieves the state-of-the-art performance on\nbenchmark datasets in both photorealism (+73.8% in FID on RealTalk) and motion\nrepresentation (+6.1% in FD metric on VICO) spaces. User studies confirm the\nsuperior performance of DiTaiListener, with the model being the clear\npreference in terms of feedback, diversity, and smoothness, outperforming\ncompetitors by a significant margin.",
      "upvotes": 2,
      "discussionId": "67f766ce1879ad2f13bee47a",
      "projectPage": "https://cv.maxi.su/DiTaiListener/"
    },
    "publishedAt": "2025-04-04T21:19:46.000Z",
    "title": "DiTaiListener: Controllable High Fidelity Listener Video Generation with\n  Diffusion",
    "summary": "Generating naturalistic and nuanced listener motions for extended\ninteractions remains an open problem. Existing methods often rely on\nlow-dimensional motion codes for facial behavior generation followed by\nphotorealistic rendering, limiting both visual fidelity and expressive\nrichness. To address these challenges, we introduce DiTaiListener, powered by a\nvideo diffusion model with multimodal conditions. Our approach first generates\nshort segments of listener responses conditioned on the speaker's speech and\nfacial motions with DiTaiListener-Gen. It then refines the transitional frames\nvia DiTaiListener-Edit for a seamless transition. Specifically,\nDiTaiListener-Gen adapts a Diffusion Transformer (DiT) for the task of listener\nhead portrait generation by introducing a Causal Temporal Multimodal Adapter\n(CTM-Adapter) to process speakers' auditory and visual cues. CTM-Adapter\nintegrates speakers' input in a causal manner into the video generation process\nto ensure temporally coherent listener responses. For long-form video\ngeneration, we introduce DiTaiListener-Edit, a transition refinement\nvideo-to-video diffusion model. The model fuses video segments into smooth and\ncontinuous videos, ensuring temporal consistency in facial expressions and\nimage quality when merging short video segments produced by DiTaiListener-Gen.\nQuantitatively, DiTaiListener achieves the state-of-the-art performance on\nbenchmark datasets in both photorealism (+73.8% in FID on RealTalk) and motion\nrepresentation (+6.1% in FD metric on VICO) spaces. User studies confirm the\nsuperior performance of DiTaiListener, with the model being the clear\npreference in terms of feedback, diversity, and smoothness, outperforming\ncompetitors by a significant margin.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.04010.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a5d8219f3b568c202b3137",
      "avatarUrl": "/avatars/eef6fb7c70d272555a53183c0e50dbaf.svg",
      "fullname": "Di Chang",
      "name": "Boese0601",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.07089",
      "authors": [
        {
          "_id": "67f7676d0ab78ef7b16a820f",
          "name": "Yiting Lu",
          "hidden": false
        },
        {
          "_id": "67f7676d0ab78ef7b16a8210",
          "name": "Jiakang Yuan",
          "hidden": false
        },
        {
          "_id": "67f7676d0ab78ef7b16a8211",
          "name": "Zhen Li",
          "hidden": false
        },
        {
          "_id": "67f7676d0ab78ef7b16a8212",
          "name": "Shitian Zhao",
          "hidden": false
        },
        {
          "_id": "67f7676d0ab78ef7b16a8213",
          "name": "Qi Qin",
          "hidden": false
        },
        {
          "_id": "67f7676d0ab78ef7b16a8214",
          "name": "Xinyue Li",
          "hidden": false
        },
        {
          "_id": "67f7676d0ab78ef7b16a8215",
          "name": "Le Zhuo",
          "hidden": false
        },
        {
          "_id": "67f7676d0ab78ef7b16a8216",
          "name": "Licheng Wen",
          "hidden": false
        },
        {
          "_id": "67f7676d0ab78ef7b16a8217",
          "name": "Dongyang Liu",
          "hidden": false
        },
        {
          "_id": "67f7676d0ab78ef7b16a8218",
          "name": "Yuewen Cao",
          "hidden": false
        },
        {
          "_id": "67f7676d0ab78ef7b16a8219",
          "name": "Xiangchao Yan",
          "hidden": false
        },
        {
          "_id": "67f7676d0ab78ef7b16a821a",
          "name": "Xin Li",
          "hidden": false
        },
        {
          "_id": "67f7676d0ab78ef7b16a821b",
          "name": "Botian Shi",
          "hidden": false
        },
        {
          "_id": "67f7676d0ab78ef7b16a821c",
          "name": "Tao Chen",
          "hidden": false
        },
        {
          "_id": "67f7676d0ab78ef7b16a821d",
          "name": "Zhibo Chen",
          "hidden": false
        },
        {
          "_id": "67f7676d0ab78ef7b16a821e",
          "name": "Lei Bai",
          "hidden": false
        },
        {
          "_id": "67f7676d0ab78ef7b16a821f",
          "name": "Bo Zhang",
          "hidden": false
        },
        {
          "_id": "67f7676d0ab78ef7b16a8220",
          "name": "Peng Gao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-09T17:58:58.000Z",
      "submittedOnDailyAt": "2025-04-10T05:22:13.319Z",
      "title": "OmniCaptioner: One Captioner to Rule Them All",
      "submittedOnDailyBy": {
        "_id": "6614fb3d5aed02b298a4b469",
        "avatarUrl": "/avatars/d0ddb4f989ad1a3f24128cc843347bde.svg",
        "isPro": false,
        "fullname": "yiting lu",
        "user": "yeeeeeyy",
        "type": "user"
      },
      "summary": "We propose OmniCaptioner, a versatile visual captioning framework for\ngenerating fine-grained textual descriptions across a wide variety of visual\ndomains. Unlike prior methods limited to specific image types (e.g., natural\nimages or geometric visuals), our framework provides a unified solution for\ncaptioning natural images, visual text (e.g., posters, UIs, textbooks), and\nstructured visuals (e.g., documents, tables, charts). By converting low-level\npixel information into semantically rich textual representations, our framework\nbridges the gap between visual and textual modalities. Our results highlight\nthree key advantages: (i) Enhanced Visual Reasoning with LLMs, where\nlong-context captions of visual modalities empower LLMs, particularly the\nDeepSeek-R1 series, to reason effectively in multimodal scenarios; (ii)\nImproved Image Generation, where detailed captions improve tasks like\ntext-to-image generation and image transformation; and (iii) Efficient\nSupervised Fine-Tuning (SFT), which enables faster convergence with less data.\nWe believe the versatility and adaptability of OmniCaptioner can offer a new\nperspective for bridging the gap between language and visual modalities.",
      "upvotes": 0,
      "discussionId": "67f767700ab78ef7b16a82d6"
    },
    "publishedAt": "2025-04-09T13:58:58.000Z",
    "title": "OmniCaptioner: One Captioner to Rule Them All",
    "summary": "We propose OmniCaptioner, a versatile visual captioning framework for\ngenerating fine-grained textual descriptions across a wide variety of visual\ndomains. Unlike prior methods limited to specific image types (e.g., natural\nimages or geometric visuals), our framework provides a unified solution for\ncaptioning natural images, visual text (e.g., posters, UIs, textbooks), and\nstructured visuals (e.g., documents, tables, charts). By converting low-level\npixel information into semantically rich textual representations, our framework\nbridges the gap between visual and textual modalities. Our results highlight\nthree key advantages: (i) Enhanced Visual Reasoning with LLMs, where\nlong-context captions of visual modalities empower LLMs, particularly the\nDeepSeek-R1 series, to reason effectively in multimodal scenarios; (ii)\nImproved Image Generation, where detailed captions improve tasks like\ntext-to-image generation and image transformation; and (iii) Efficient\nSupervised Fine-Tuning (SFT), which enables faster convergence with less data.\nWe believe the versatility and adaptability of OmniCaptioner can offer a new\nperspective for bridging the gap between language and visual modalities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07089.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6614fb3d5aed02b298a4b469",
      "avatarUrl": "/avatars/d0ddb4f989ad1a3f24128cc843347bde.svg",
      "fullname": "yiting lu",
      "name": "yeeeeeyy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  }
]