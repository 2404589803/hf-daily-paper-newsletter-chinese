[
    "{'paper': {'id': '2412.18925', 'authors': [{'_id': '677209448e0ed7713b183674', 'name': 'Junying Chen', 'hidden': False}, {'_id': '677209448e0ed7713b183675', 'name': 'Zhenyang Cai', 'hidden': False}, {'_id': '677209448e0ed7713b183676', 'name': 'Ke Ji', 'hidden': False}, {'_id': '677209448e0ed7713b183677', 'name': 'Xidong Wang', 'hidden': False}, {'_id': '677209448e0ed7713b183678', 'name': 'Wanlong Liu', 'hidden': False}, {'_id': '677209448e0ed7713b183679', 'name': 'Rongsheng Wang', 'hidden': False}, {'_id': '677209448e0ed7713b18367a', 'name': 'Jianye Hou', 'hidden': False}, {'_id': '677209448e0ed7713b18367b', 'name': 'Benyou Wang', 'hidden': False}], 'publishedAt': '2024-12-25T15:12:34.000Z', 'title': 'HuatuoGPT-o1, Towards Medical Complex Reasoning with LLMs', 'summary': 'The breakthrough of OpenAI o1 highlights the potential of enhancing reasoning\\nto improve LLM. Yet, most research in reasoning has focused on mathematical\\ntasks, leaving domains like medicine underexplored. The medical domain, though\\ndistinct from mathematics, also demands robust reasoning to provide reliable\\nanswers, given the high standards of healthcare. However, verifying medical\\nreasoning is challenging, unlike those in mathematics. To address this, we\\npropose verifiable medical problems with a medical verifier to check the\\ncorrectness of model outputs. This verifiable nature enables advancements in\\nmedical reasoning through a two-stage approach: (1) using the verifier to guide\\nthe search for a complex reasoning trajectory for fine-tuning LLMs, (2)\\napplying reinforcement learning (RL) with verifier-based rewards to enhance\\ncomplex reasoning further. Finally, we introduce HuatuoGPT-o1, a medical LLM\\ncapable of complex reasoning, which outperforms general and medical-specific\\nbaselines using only 40K verifiable problems. Experiments show complex\\nreasoning improves medical problem-solving and benefits more from RL. We hope\\nour approach inspires advancements in reasoning across medical and other\\nspecialized domains.', 'upvotes': 45, 'discussionId': '677209448e0ed7713b1836cb'}, 'publishedAt': '2024-12-29T22:31:54.173Z', 'title': 'HuatuoGPT-o1, Towards Medical Complex Reasoning with LLMs', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.18925.png', 'numComments': 1, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isMod': False, 'followerCount': 5493}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2412.18619', 'authors': [{'_id': '67720aa292c63806bde6d2be', 'user': {'_id': '61b0a4ce1b3d95b3d1ed9251', 'avatarUrl': '/avatars/b5f8c68801829b5653ee1d55244dbe16.svg', 'isPro': False, 'fullname': 'Liang Chen', 'user': 'leonardPKU', 'type': 'user'}, 'name': 'Liang Chen', 'status': 'extracted_pending', 'statusLastChangedAt': '2024-12-30T02:51:16.924Z', 'hidden': False}, {'_id': '67720aa292c63806bde6d2bf', 'name': 'Zekun Wang', 'hidden': False}, {'_id': '67720aa292c63806bde6d2c0', 'name': 'Shuhuai Ren', 'hidden': False}, {'_id': '67720aa292c63806bde6d2c1', 'name': 'Lei Li', 'hidden': False}, {'_id': '67720aa292c63806bde6d2c2', 'name': 'Haozhe Zhao', 'hidden': False}, {'_id': '67720aa292c63806bde6d2c3', 'name': 'Yunshui Li', 'hidden': False}, {'_id': '67720aa292c63806bde6d2c4', 'name': 'Zefan Cai', 'hidden': False}, {'_id': '67720aa292c63806bde6d2c5', 'name': 'Hongcheng Guo', 'hidden': False}, {'_id': '67720aa292c63806bde6d2c6', 'name': 'Lei Zhang', 'hidden': False}, {'_id': '67720aa292c63806bde6d2c7', 'name': 'Yizhe Xiong', 'hidden': False}, {'_id': '67720aa292c63806bde6d2c8', 'name': 'Yichi Zhang', 'hidden': False}, {'_id': '67720aa292c63806bde6d2c9', 'name': 'Ruoyu Wu', 'hidden': False}, {'_id': '67720aa292c63806bde6d2ca', 'name': 'Qingxiu Dong', 'hidden': False}, {'_id': '67720aa292c63806bde6d2cb', 'name': 'Ge Zhang', 'hidden': False}, {'_id': '67720aa292c63806bde6d2cc', 'name': 'Jian Yang', 'hidden': False}, {'_id': '67720aa292c63806bde6d2cd', 'name': 'Lingwei Meng', 'hidden': False}, {'_id': '67720aa292c63806bde6d2ce', 'name': 'Shujie Hu', 'hidden': False}, {'_id': '67720aa292c63806bde6d2cf', 'name': 'Yulong Chen', 'hidden': False}, {'_id': '67720aa292c63806bde6d2d0', 'name': 'Junyang Lin', 'hidden': False}, {'_id': '67720aa292c63806bde6d2d1', 'name': 'Shuai Bai', 'hidden': False}, {'_id': '67720aa292c63806bde6d2d2', 'name': 'Andreas Vlachos', 'hidden': False}, {'_id': '67720aa292c63806bde6d2d3', 'name': 'Xu Tan', 'hidden': False}, {'_id': '67720aa292c63806bde6d2d4', 'name': 'Minjia Zhang', 'hidden': False}, {'_id': '67720aa292c63806bde6d2d5', 'name': 'Wen Xiao', 'hidden': False}, {'_id': '67720aa292c63806bde6d2d6', 'name': 'Aaron Yee', 'hidden': False}, {'_id': '67720aa292c63806bde6d2d7', 'name': 'Tianyu Liu', 'hidden': False}, {'_id': '67720aa292c63806bde6d2d8', 'name': 'Baobao Chang', 'hidden': False}], 'publishedAt': '2024-12-16T05:02:25.000Z', 'title': 'Next Token Prediction Towards Multimodal Intelligence: A Comprehensive\\n  Survey', 'summary': 'Building on the foundations of language modeling in natural language\\nprocessing, Next Token Prediction (NTP) has evolved into a versatile training\\nobjective for machine learning tasks across various modalities, achieving\\nconsiderable success. As Large Language Models (LLMs) have advanced to unify\\nunderstanding and generation tasks within the textual modality, recent research\\nhas shown that tasks from different modalities can also be effectively\\nencapsulated within the NTP framework, transforming the multimodal information\\ninto tokens and predict the next one given the context. This survey introduces\\na comprehensive taxonomy that unifies both understanding and generation within\\nmultimodal learning through the lens of NTP. The proposed taxonomy covers five\\nkey aspects: Multimodal tokenization, MMNTP model architectures, unified task\\nrepresentation, datasets \\\\& evaluation, and open challenges. This new taxonomy\\naims to aid researchers in their exploration of multimodal intelligence. An\\nassociated GitHub repository collecting the latest papers and repos is\\navailable at https://github.com/LMM101/Awesome-Multimodal-Next-Token-Prediction', 'upvotes': 10, 'discussionId': '67720aa492c63806bde6d350'}, 'publishedAt': '2024-12-30T08:43:12.896Z', 'title': 'Next Token Prediction Towards Multimodal Intelligence: A Comprehensive Survey', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.18619.png', 'numComments': 1, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isMod': False, 'followerCount': 5493}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2412.19326', 'authors': [{'_id': '67722817633a6043c33212aa', 'name': 'Ziang Yan', 'hidden': False}, {'_id': '67722817633a6043c33212ab', 'name': 'Zhilin Li', 'hidden': False}, {'_id': '67722817633a6043c33212ac', 'name': 'Yinan He', 'hidden': False}, {'_id': '67722817633a6043c33212ad', 'name': 'Chenting Wang', 'hidden': False}, {'_id': '67722817633a6043c33212ae', 'name': 'Kunchang Li', 'hidden': False}, {'_id': '67722817633a6043c33212af', 'name': 'Xinhao Li', 'hidden': False}, {'_id': '67722817633a6043c33212b0', 'name': 'Xiangyu Zeng', 'hidden': False}, {'_id': '67722817633a6043c33212b1', 'name': 'Zilei Wang', 'hidden': False}, {'_id': '67722817633a6043c33212b2', 'name': 'Yali Wang', 'hidden': False}, {'_id': '67722817633a6043c33212b3', 'name': 'Yu Qiao', 'hidden': False}, {'_id': '67722817633a6043c33212b4', 'name': 'Limin Wang', 'hidden': False}, {'_id': '67722817633a6043c33212b5', 'name': 'Yi Wang', 'hidden': False}], 'publishedAt': '2024-12-26T18:56:05.000Z', 'title': 'Task Preference Optimization: Improving Multimodal Large Language Models\\n  with Vision Task Alignment', 'summary': \"Current multimodal large language models (MLLMs) struggle with fine-grained\\nor precise understanding of visuals though they give comprehensive perception\\nand reasoning in a spectrum of vision applications. Recent studies either\\ndevelop tool-using or unify specific visual tasks into the autoregressive\\nframework, often at the expense of overall multimodal performance. To address\\nthis issue and enhance MLLMs with visual tasks in a scalable fashion, we\\npropose Task Preference Optimization (TPO), a novel method that utilizes\\ndifferentiable task preferences derived from typical fine-grained visual tasks.\\nTPO introduces learnable task tokens that establish connections between\\nmultiple task-specific heads and the MLLM. By leveraging rich visual labels\\nduring training, TPO significantly enhances the MLLM's multimodal capabilities\\nand task-specific performance. Through multi-task co-training within TPO, we\\nobserve synergistic benefits that elevate individual task performance beyond\\nwhat is achievable through single-task training methodologies. Our\\ninstantiation of this approach with VideoChat and LLaVA demonstrates an overall\\n14.6% improvement in multimodal performance compared to baseline models.\\nAdditionally, MLLM-TPO demonstrates robust zero-shot capabilities across\\nvarious tasks, performing comparably to state-of-the-art supervised models. The\\ncode will be released at https://github.com/OpenGVLab/TPO\", 'upvotes': 7, 'discussionId': '6772281a633a6043c3321365'}, 'publishedAt': '2024-12-29T23:57:59.186Z', 'title': 'Task Preference Optimization: Improving Multimodal Large Language Models with Vision Task Alignment', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.19326.png', 'numComments': 1, 'submittedBy': {'_id': '62aafa49f29ff279b51f0182', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/62aafa49f29ff279b51f0182/rQx8QFQGOY2qIhqJ8zSRj.jpeg', 'fullname': 'yinanhe', 'name': 'ynhe', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 10}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2412.18605', 'authors': [{'_id': '676bb2c29063304d2d9ec676', 'name': 'Zehan Wang', 'hidden': False}, {'_id': '676bb2c29063304d2d9ec677', 'name': 'Ziang Zhang', 'hidden': False}, {'_id': '676bb2c29063304d2d9ec678', 'name': 'Tianyu Pang', 'hidden': False}, {'_id': '676bb2c29063304d2d9ec679', 'name': 'Chao Du', 'hidden': False}, {'_id': '676bb2c29063304d2d9ec67a', 'name': 'Hengshuang Zhao', 'hidden': False}, {'_id': '676bb2c29063304d2d9ec67b', 'name': 'Zhou Zhao', 'hidden': False}], 'publishedAt': '2024-12-24T18:58:43.000Z', 'title': 'Orient Anything: Learning Robust Object Orientation Estimation from\\n  Rendering 3D Models', 'summary': 'Orientation is a key attribute of objects, crucial for understanding their\\nspatial pose and arrangement in images. However, practical solutions for\\naccurate orientation estimation from a single image remain underexplored. In\\nthis work, we introduce Orient Anything, the first expert and foundational\\nmodel designed to estimate object orientation in a single- and free-view image.\\nDue to the scarcity of labeled data, we propose extracting knowledge from the\\n3D world. By developing a pipeline to annotate the front face of 3D objects and\\nrender images from random views, we collect 2M images with precise orientation\\nannotations. To fully leverage the dataset, we design a robust training\\nobjective that models the 3D orientation as probability distributions of three\\nangles and predicts the object orientation by fitting these distributions.\\nBesides, we employ several strategies to improve synthetic-to-real transfer.\\nOur model achieves state-of-the-art orientation estimation accuracy in both\\nrendered and real images and exhibits impressive zero-shot ability in various\\nscenarios. More importantly, our model enhances many applications, such as\\ncomprehension and generation of complex spatial concepts and 3D object pose\\nadjustment.', 'upvotes': 6, 'discussionId': '676bb2c49063304d2d9ec7d0'}, 'publishedAt': '2024-12-29T21:38:37.393Z', 'title': 'Orient Anything: Learning Robust Object Orientation Estimation from Rendering 3D Models', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/663b4d6aa55b0634634cd302/otb9alc3ESHg68f_TBcFp.png'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.18605.png', 'numComments': 3, 'submittedBy': {'_id': '663b4d6aa55b0634634cd302', 'avatarUrl': '/avatars/1191982568ad67895225f22844b6da99.svg', 'fullname': 'ZehanWang', 'name': 'ZehanWang', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2412.17762', 'authors': [{'_id': '676d0a2e0076ad5ba195b88a', 'name': 'Marta Skreta', 'hidden': False}, {'_id': '676d0a2e0076ad5ba195b88b', 'name': 'Lazar Atanackovic', 'hidden': False}, {'_id': '676d0a2e0076ad5ba195b88c', 'name': 'Avishek Joey Bose', 'hidden': False}, {'_id': '676d0a2e0076ad5ba195b88d', 'name': 'Alexander Tong', 'hidden': False}, {'_id': '676d0a2e0076ad5ba195b88e', 'name': 'Kirill Neklyudov', 'hidden': False}], 'publishedAt': '2024-12-23T18:18:07.000Z', 'title': 'The Superposition of Diffusion Models Using the Itô Density Estimator', 'summary': \"The Cambrian explosion of easily accessible pre-trained diffusion models\\nsuggests a demand for methods that combine multiple different pre-trained\\ndiffusion models without incurring the significant computational burden of\\nre-training a larger combined model. In this paper, we cast the problem of\\ncombining multiple pre-trained diffusion models at the generation stage under a\\nnovel proposed framework termed superposition. Theoretically, we derive\\nsuperposition from rigorous first principles stemming from the celebrated\\ncontinuity equation and design two novel algorithms tailor-made for combining\\ndiffusion models in SuperDiff. SuperDiff leverages a new scalable It\\\\^o density\\nestimator for the log likelihood of the diffusion SDE which incurs no\\nadditional overhead compared to the well-known Hutchinson's estimator needed\\nfor divergence calculations. We demonstrate that SuperDiff is scalable to large\\npre-trained diffusion models as superposition is performed solely through\\ncomposition during inference, and also enjoys painless implementation as it\\ncombines different pre-trained vector fields through an automated re-weighting\\nscheme. Notably, we show that SuperDiff is efficient during inference time, and\\nmimics traditional composition operators such as the logical OR and the logical\\nAND. We empirically demonstrate the utility of using SuperDiff for generating\\nmore diverse images on CIFAR-10, more faithful prompt conditioned image editing\\nusing Stable Diffusion, and improved unconditional de novo structure design of\\nproteins. https://github.com/necludov/super-diffusion\", 'upvotes': 4, 'discussionId': '676d0a330076ad5ba195b97c'}, 'publishedAt': '2024-12-30T03:07:39.186Z', 'title': 'The Superposition of Diffusion Models Using the Itô Density Estimator', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/65a49ef27ec6af0f956a5c61/P2wF-TfD9U5L1rN3ezxCI.gif'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.17762.png', 'numComments': 1, 'submittedBy': {'_id': '65a49ef27ec6af0f956a5c61', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/65a49ef27ec6af0f956a5c61/OGJ1OVezooF7dUn1SiEKG.jpeg', 'fullname': 'Marta Skreta', 'name': 'mskrt', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2412.19712', 'authors': [{'_id': '67721e8cb9d0358385f6628d', 'name': 'Jiawei Lin', 'hidden': False}, {'_id': '67721e8cb9d0358385f6628e', 'name': 'Shizhao Sun', 'hidden': False}, {'_id': '67721e8cb9d0358385f6628f', 'name': 'Danqing Huang', 'hidden': False}, {'_id': '67721e8cb9d0358385f66290', 'name': 'Ting Liu', 'hidden': False}, {'_id': '67721e8cb9d0358385f66291', 'name': 'Ji Li', 'hidden': False}, {'_id': '67721e8cb9d0358385f66292', 'name': 'Jiang Bian', 'hidden': False}], 'publishedAt': '2024-12-27T16:13:08.000Z', 'title': 'From Elements to Design: A Layered Approach for Automatic Graphic Design\\n  Composition', 'summary': 'In this work, we investigate automatic design composition from multimodal\\ngraphic elements. Although recent studies have developed various generative\\nmodels for graphic design, they usually face the following limitations: they\\nonly focus on certain subtasks and are far from achieving the design\\ncomposition task; they do not consider the hierarchical information of graphic\\ndesigns during the generation process. To tackle these issues, we introduce the\\nlayered design principle into Large Multimodal Models (LMMs) and propose a\\nnovel approach, called LaDeCo, to accomplish this challenging task.\\nSpecifically, LaDeCo first performs layer planning for a given element set,\\ndividing the input elements into different semantic layers according to their\\ncontents. Based on the planning results, it subsequently predicts element\\nattributes that control the design composition in a layer-wise manner, and\\nincludes the rendered image of previously generated layers into the context.\\nWith this insightful design, LaDeCo decomposes the difficult task into smaller\\nmanageable steps, making the generation process smoother and clearer. The\\nexperimental results demonstrate the effectiveness of LaDeCo in design\\ncomposition. Furthermore, we show that LaDeCo enables some interesting\\napplications in graphic design, such as resolution adjustment, element filling,\\ndesign variation, etc. In addition, it even outperforms the specialized models\\nin some design subtasks without any task-specific training.', 'upvotes': 3, 'discussionId': '67721e92b9d0358385f66457'}, 'publishedAt': '2024-12-29T23:24:15.208Z', 'title': 'From Elements to Design: A Layered Approach for Automatic Graphic Design Composition', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/6440dda9cea37249a0f9b473/uCdgNCCTyvVF56mH0uPnc.png', 'https://cdn-uploads.huggingface.co/production/uploads/6440dda9cea37249a0f9b473/ks1FHTdlYHN9WEYLBJYG1.png', 'https://cdn-uploads.huggingface.co/production/uploads/6440dda9cea37249a0f9b473/q1JODwSII9FQVA0dee4EA.png'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.19712.png', 'numComments': 1, 'submittedBy': {'_id': '6440dda9cea37249a0f9b473', 'avatarUrl': '/avatars/9747e1ca11ed5725b2b9968f028cac93.svg', 'fullname': 'Jiawei Lin', 'name': 'KyleLin', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2412.19512', 'authors': [{'_id': '67727ca4986fbffa7a2208d4', 'user': {'_id': '641a9cb1f9dd6391a2463477', 'avatarUrl': '/avatars/73caf5389f88c7356f02ee9f73146faa.svg', 'isPro': False, 'fullname': 'Farn Hua', 'user': 'farnhua', 'type': 'user'}, 'name': 'Hua Farn', 'status': 'extracted_pending', 'statusLastChangedAt': '2024-12-30T10:57:42.126Z', 'hidden': False}, {'_id': '67727ca4986fbffa7a2208d5', 'name': 'Hsuan Su', 'hidden': False}, {'_id': '67727ca4986fbffa7a2208d6', 'name': 'Shachi H Kumar', 'hidden': False}, {'_id': '67727ca4986fbffa7a2208d7', 'name': 'Saurav Sahay', 'hidden': False}, {'_id': '67727ca4986fbffa7a2208d8', 'name': 'Shang-Tse Chen', 'hidden': False}, {'_id': '67727ca4986fbffa7a2208d9', 'name': 'Hung-yi Lee', 'hidden': False}], 'publishedAt': '2024-12-27T08:03:22.000Z', 'title': 'Safeguard Fine-Tuned LLMs Through Pre- and Post-Tuning Model Merging', 'summary': 'Fine-tuning large language models (LLMs) for downstream tasks is a widely\\nadopted approach, but it often leads to safety degradation in safety-aligned\\nLLMs. Currently, many solutions address this issue by incorporating additional\\nsafety data, which can be impractical in many cases. In this paper, we address\\nthe question: How can we improve downstream task performance while preserving\\nsafety in LLMs without relying on additional safety data? We propose a simple\\nand effective method that maintains the inherent safety of LLMs while enhancing\\ntheir downstream task performance: merging the weights of pre- and\\npost-fine-tuned safety-aligned models. Experimental results across various\\ndownstream tasks, models, and merging methods demonstrate that this approach\\neffectively mitigates safety degradation while improving downstream task\\nperformance, offering a practical solution for adapting safety-aligned LLMs.', 'upvotes': 2, 'discussionId': '67727ca6986fbffa7a220934'}, 'publishedAt': '2024-12-30T05:58:47.315Z', 'title': 'Safeguard Fine-Tuned LLMs Through Pre- and Post-Tuning Model Merging', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.19512.png', 'numComments': 1, 'submittedBy': {'_id': '608abf1272b50b02c4b02865', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1619708309549-608abf1272b50b02c4b02865.jpeg', 'fullname': 'Hsuan Su', 'name': 'jacksukk', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2412.17606', 'authors': [{'_id': '677204632f016f40c42e2180', 'name': 'Risa Shinoda', 'hidden': False}, {'_id': '677204632f016f40c42e2181', 'name': 'Kuniaki Saito', 'hidden': False}, {'_id': '677204632f016f40c42e2182', 'name': 'Shohei Tanaka', 'hidden': False}, {'_id': '677204632f016f40c42e2183', 'name': 'Tosho Hirasawa', 'hidden': False}, {'_id': '677204632f016f40c42e2184', 'name': 'Yoshitaka Ushiku', 'hidden': False}], 'publishedAt': '2024-12-23T14:25:33.000Z', 'title': 'SBS Figures: Pre-training Figure QA from Stage-by-Stage Synthesized\\n  Images', 'summary': 'Building a large-scale figure QA dataset requires a considerable amount of\\nwork, from gathering and selecting figures to extracting attributes like text,\\nnumbers, and colors, and generating QAs. Although recent developments in LLMs\\nhave led to efforts to synthesize figures, most of these focus primarily on QA\\ngeneration. Additionally, creating figures directly using LLMs often encounters\\nissues such as code errors, similar-looking figures, and repetitive content in\\nfigures. To address this issue, we present SBSFigures (Stage-by-Stage Synthetic\\nFigures), a dataset for pre-training figure QA. Our proposed pipeline enables\\nthe creation of chart figures with complete annotations of the visualized data\\nand dense QA annotations without any manual annotation process. Our\\nstage-by-stage pipeline makes it possible to create diverse topic and\\nappearance figures efficiently while minimizing code errors. Our SBSFigures\\ndemonstrate a strong pre-training effect, making it possible to achieve\\nefficient training with a limited amount of real-world chart data starting from\\nour pre-trained weights.', 'upvotes': 1, 'discussionId': '677204652f016f40c42e2367'}, 'publishedAt': '2024-12-30T01:27:40.574Z', 'title': 'SBS Figures: Pre-training Figure QA from Stage-by-Stage Synthesized Images', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.17606.png', 'numComments': 1, 'submittedBy': {'_id': '651a7684a1a5e5d617e28f84', 'avatarUrl': '/avatars/f484ae7c8d980818cd2ba3ffa682b781.svg', 'fullname': 'Risa Shinoda', 'name': 'risashinoda', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2412.19645', 'authors': [{'_id': '677229bf8103ad52cb7031b1', 'name': 'Tao Wu', 'hidden': False}, {'_id': '677229bf8103ad52cb7031b2', 'name': 'Yong Zhang', 'hidden': False}, {'_id': '677229bf8103ad52cb7031b3', 'name': 'Xiaodong Cun', 'hidden': False}, {'_id': '677229bf8103ad52cb7031b4', 'name': 'Zhongang Qi', 'hidden': False}, {'_id': '677229bf8103ad52cb7031b5', 'name': 'Junfu Pu', 'hidden': False}, {'_id': '677229bf8103ad52cb7031b6', 'name': 'Huanzhang Dou', 'hidden': False}, {'_id': '677229bf8103ad52cb7031b7', 'name': 'Guangcong Zheng', 'hidden': False}, {'_id': '677229bf8103ad52cb7031b8', 'name': 'Ying Shan', 'hidden': False}, {'_id': '677229bf8103ad52cb7031b9', 'name': 'Xi Li', 'hidden': False}], 'publishedAt': '2024-12-27T13:49:25.000Z', 'title': 'VideoMaker: Zero-shot Customized Video Generation with the Inherent\\n  Force of Video Diffusion Models', 'summary': \"Zero-shot customized video generation has gained significant attention due to\\nits substantial application potential. Existing methods rely on additional\\nmodels to extract and inject reference subject features, assuming that the\\nVideo Diffusion Model (VDM) alone is insufficient for zero-shot customized\\nvideo generation. However, these methods often struggle to maintain consistent\\nsubject appearance due to suboptimal feature extraction and injection\\ntechniques. In this paper, we reveal that VDM inherently possesses the force to\\nextract and inject subject features. Departing from previous heuristic\\napproaches, we introduce a novel framework that leverages VDM's inherent force\\nto enable high-quality zero-shot customized video generation. Specifically, for\\nfeature extraction, we directly input reference images into VDM and use its\\nintrinsic feature extraction process, which not only provides fine-grained\\nfeatures but also significantly aligns with VDM's pre-trained knowledge. For\\nfeature injection, we devise an innovative bidirectional interaction between\\nsubject features and generated content through spatial self-attention within\\nVDM, ensuring that VDM has better subject fidelity while maintaining the\\ndiversity of the generated video.Experiments on both customized human and\\nobject video generation validate the effectiveness of our framework.\", 'upvotes': 1, 'discussionId': '677229c48103ad52cb7032d5'}, 'publishedAt': '2024-12-30T00:04:19.275Z', 'title': 'VideoMaker: Zero-shot Customized Video Generation with the Inherent Force of Video Diffusion Models', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.19645.png', 'numComments': 1, 'submittedBy': {'_id': '63468720dd6d90d82ccf3450', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg', 'fullname': 'YSH', 'name': 'BestWishYsh', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 24}, 'isAuthorParticipating': False}"
]