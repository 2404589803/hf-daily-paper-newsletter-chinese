[
  {
    "paper": {
      "id": "2505.07591",
      "authors": [
        {
          "_id": "6822e023b1df51252f95e958",
          "user": {
            "_id": "66384be673c2c55f2ded89fa",
            "avatarUrl": "/avatars/1d8721074f0f51fab405f81474f2035f.svg",
            "isPro": false,
            "fullname": "Junjie Ye",
            "user": "Junjie-Ye",
            "type": "user"
          },
          "name": "Junjie Ye",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-14T07:35:57.239Z",
          "hidden": false
        },
        {
          "_id": "6822e023b1df51252f95e959",
          "name": "Caishuang Huang",
          "hidden": false
        },
        {
          "_id": "6822e023b1df51252f95e95a",
          "name": "Zhuohan Chen",
          "hidden": false
        },
        {
          "_id": "6822e023b1df51252f95e95b",
          "name": "Wenjie Fu",
          "hidden": false
        },
        {
          "_id": "6822e023b1df51252f95e95c",
          "name": "Chenyuan Yang",
          "hidden": false
        },
        {
          "_id": "6822e023b1df51252f95e95d",
          "name": "Leyi Yang",
          "hidden": false
        },
        {
          "_id": "6822e023b1df51252f95e95e",
          "name": "Yilong Wu",
          "hidden": false
        },
        {
          "_id": "6822e023b1df51252f95e95f",
          "name": "Peng Wang",
          "hidden": false
        },
        {
          "_id": "6822e023b1df51252f95e960",
          "name": "Meng Zhou",
          "hidden": false
        },
        {
          "_id": "6822e023b1df51252f95e961",
          "name": "Xiaolong Yang",
          "hidden": false
        },
        {
          "_id": "6822e023b1df51252f95e962",
          "name": "Tao Gui",
          "hidden": false
        },
        {
          "_id": "6822e023b1df51252f95e963",
          "name": "Qi Zhang",
          "hidden": false
        },
        {
          "_id": "6822e023b1df51252f95e964",
          "name": "Zhongchao Shi",
          "hidden": false
        },
        {
          "_id": "6822e023b1df51252f95e965",
          "name": "Jianping Fan",
          "hidden": false
        },
        {
          "_id": "6822e023b1df51252f95e966",
          "name": "Xuanjing Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-12T14:16:55.000Z",
      "submittedOnDailyAt": "2025-05-14T05:33:58.516Z",
      "title": "A Multi-Dimensional Constraint Framework for Evaluating and Improving\n  Instruction Following in Large Language Models",
      "submittedOnDailyBy": {
        "_id": "66384be673c2c55f2ded89fa",
        "avatarUrl": "/avatars/1d8721074f0f51fab405f81474f2035f.svg",
        "isPro": false,
        "fullname": "Junjie Ye",
        "user": "Junjie-Ye",
        "type": "user"
      },
      "summary": "Instruction following evaluates large language models (LLMs) on their ability\nto generate outputs that adhere to user-defined constraints. However, existing\nbenchmarks often rely on templated constraint prompts, which lack the diversity\nof real-world usage and limit fine-grained performance assessment. To fill this\ngap, we propose a multi-dimensional constraint framework encompassing three\nconstraint patterns, four constraint categories, and four difficulty levels.\nBuilding on this framework, we develop an automated instruction generation\npipeline that performs constraint expansion, conflict detection, and\ninstruction rewriting, yielding 1,200 code-verifiable instruction-following\ntest samples. We evaluate 19 LLMs across seven model families and uncover\nsubstantial variation in performance across constraint forms. For instance,\naverage performance drops from 77.67% at Level I to 32.96% at Level IV.\nFurthermore, we demonstrate the utility of our approach by using it to generate\ndata for reinforcement learning, achieving substantial gains in instruction\nfollowing without degrading general performance. In-depth analysis indicates\nthat these gains stem primarily from modifications in the model's attention\nmodules parameters, which enhance constraint recognition and adherence. Code\nand data are available in https://github.com/Junjie-Ye/MulDimIF.",
      "upvotes": 2,
      "discussionId": "6822e024b1df51252f95e9be",
      "ai_keywords": [
        "instruction-following",
        "constraint expansion",
        "conflict detection",
        "instruction rewriting",
        "code-verifiable",
        "attention modules"
      ]
    },
    "publishedAt": "2025-05-12T10:16:55.000Z",
    "title": "A Multi-Dimensional Constraint Framework for Evaluating and Improving\n  Instruction Following in Large Language Models",
    "summary": "Instruction following evaluates large language models (LLMs) on their ability\nto generate outputs that adhere to user-defined constraints. However, existing\nbenchmarks often rely on templated constraint prompts, which lack the diversity\nof real-world usage and limit fine-grained performance assessment. To fill this\ngap, we propose a multi-dimensional constraint framework encompassing three\nconstraint patterns, four constraint categories, and four difficulty levels.\nBuilding on this framework, we develop an automated instruction generation\npipeline that performs constraint expansion, conflict detection, and\ninstruction rewriting, yielding 1,200 code-verifiable instruction-following\ntest samples. We evaluate 19 LLMs across seven model families and uncover\nsubstantial variation in performance across constraint forms. For instance,\naverage performance drops from 77.67% at Level I to 32.96% at Level IV.\nFurthermore, we demonstrate the utility of our approach by using it to generate\ndata for reinforcement learning, achieving substantial gains in instruction\nfollowing without degrading general performance. In-depth analysis indicates\nthat these gains stem primarily from modifications in the model's attention\nmodules parameters, which enhance constraint recognition and adherence. Code\nand data are available in https://github.com/Junjie-Ye/MulDimIF.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.07591.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66384be673c2c55f2ded89fa",
      "avatarUrl": "/avatars/1d8721074f0f51fab405f81474f2035f.svg",
      "fullname": "Junjie Ye",
      "name": "Junjie-Ye",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.07215",
      "authors": [
        {
          "_id": "68236b86102b1d3069ebafab",
          "user": {
            "_id": "64b88247e436bbca16603baf",
            "avatarUrl": "/avatars/7bde6b0f75bccc3195fb72cbe5860a7e.svg",
            "isPro": false,
            "fullname": "Vivek Verma",
            "user": "vivekverma",
            "type": "user"
          },
          "name": "Vivek Verma",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-13T15:55:50.678Z",
          "hidden": false
        },
        {
          "_id": "68236b86102b1d3069ebafac",
          "name": "David Huang",
          "hidden": false
        },
        {
          "_id": "68236b86102b1d3069ebafad",
          "name": "William Chen",
          "hidden": false
        },
        {
          "_id": "68236b86102b1d3069ebafae",
          "name": "Dan Klein",
          "hidden": false
        },
        {
          "_id": "68236b86102b1d3069ebafaf",
          "user": {
            "_id": "6269d074a6a7bba9e46d8d50",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1651101782106-noauth.jpeg",
            "isPro": false,
            "fullname": "Nicholas Tomlin",
            "user": "nickatomlin",
            "type": "user"
          },
          "name": "Nicholas Tomlin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-14T07:35:26.733Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6269d074a6a7bba9e46d8d50/RSzjacMbHw27QCpwl_Nte.png"
      ],
      "publishedAt": "2025-05-12T04:01:03.000Z",
      "submittedOnDailyAt": "2025-05-14T06:11:56.396Z",
      "title": "Measuring General Intelligence with Generated Games",
      "submittedOnDailyBy": {
        "_id": "6269d074a6a7bba9e46d8d50",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1651101782106-noauth.jpeg",
        "isPro": false,
        "fullname": "Nicholas Tomlin",
        "user": "nickatomlin",
        "type": "user"
      },
      "summary": "We present gg-bench, a collection of game environments designed to evaluate\ngeneral reasoning capabilities in language models. Unlike most static\nbenchmarks, gg-bench is a data generating process where new evaluation\ninstances can be generated at will. In particular, gg-bench is synthetically\ngenerated by (1) using a large language model (LLM) to generate natural\nlanguage descriptions of novel games, (2) using the LLM to implement each game\nin code as a Gym environment, and (3) training reinforcement learning (RL)\nagents via self-play on the generated games. We evaluate language models by\ntheir winrate against these RL agents by prompting models with the game\ndescription, current board state, and a list of valid moves, after which models\noutput the moves they wish to take. gg-bench is challenging: state-of-the-art\nLLMs such as GPT-4o and Claude 3.7 Sonnet achieve winrates of 7-9% on gg-bench\nusing in-context learning, while reasoning models such as o1, o3-mini and\nDeepSeek-R1 achieve average winrates of 31-36%. We release the generated games,\ndata generation process, and evaluation code in order to support future\nmodeling work and expansion of our benchmark.",
      "upvotes": 1,
      "discussionId": "68236b86102b1d3069ebb00e",
      "ai_keywords": [
        "large language model (LLM)",
        "Gym environment",
        "reinforcement learning (RL)",
        "self-play",
        "prompt",
        "in-context learning",
        "winrate"
      ]
    },
    "publishedAt": "2025-05-12T00:01:03.000Z",
    "title": "Measuring General Intelligence with Generated Games",
    "summary": "We present gg-bench, a collection of game environments designed to evaluate\ngeneral reasoning capabilities in language models. Unlike most static\nbenchmarks, gg-bench is a data generating process where new evaluation\ninstances can be generated at will. In particular, gg-bench is synthetically\ngenerated by (1) using a large language model (LLM) to generate natural\nlanguage descriptions of novel games, (2) using the LLM to implement each game\nin code as a Gym environment, and (3) training reinforcement learning (RL)\nagents via self-play on the generated games. We evaluate language models by\ntheir winrate against these RL agents by prompting models with the game\ndescription, current board state, and a list of valid moves, after which models\noutput the moves they wish to take. gg-bench is challenging: state-of-the-art\nLLMs such as GPT-4o and Claude 3.7 Sonnet achieve winrates of 7-9% on gg-bench\nusing in-context learning, while reasoning models such as o1, o3-mini and\nDeepSeek-R1 achieve average winrates of 31-36%. We release the generated games,\ndata generation process, and evaluation code in order to support future\nmodeling work and expansion of our benchmark.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6269d074a6a7bba9e46d8d50/RSzjacMbHw27QCpwl_Nte.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.07215.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6269d074a6a7bba9e46d8d50",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1651101782106-noauth.jpeg",
      "fullname": "Nicholas Tomlin",
      "name": "nickatomlin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.08665",
      "authors": [
        {
          "_id": "68243bddd08d8e01109d5680",
          "user": {
            "_id": "622dc11fe27c88667db093fc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1667052350862-622dc11fe27c88667db093fc.jpeg",
            "isPro": false,
            "fullname": "Edoardo Bianchi",
            "user": "EdBianchi",
            "type": "user"
          },
          "name": "Edoardo Bianchi",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-14T06:45:06.471Z",
          "hidden": false
        },
        {
          "_id": "68243bddd08d8e01109d5681",
          "name": "Antonio Liotta",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-13T15:27:24.000Z",
      "submittedOnDailyAt": "2025-05-14T05:16:45.606Z",
      "title": "SkillFormer: Unified Multi-View Video Understanding for Proficiency\n  Estimation",
      "submittedOnDailyBy": {
        "_id": "622dc11fe27c88667db093fc",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1667052350862-622dc11fe27c88667db093fc.jpeg",
        "isPro": false,
        "fullname": "Edoardo Bianchi",
        "user": "EdBianchi",
        "type": "user"
      },
      "summary": "Assessing human skill levels in complex activities is a challenging problem\nwith applications in sports, rehabilitation, and training. In this work, we\npresent SkillFormer, a parameter-efficient architecture for unified multi-view\nproficiency estimation from egocentric and exocentric videos. Building on the\nTimeSformer backbone, SkillFormer introduces a CrossViewFusion module that\nfuses view-specific features using multi-head cross-attention, learnable\ngating, and adaptive self-calibration. We leverage Low-Rank Adaptation to\nfine-tune only a small subset of parameters, significantly reducing training\ncosts. In fact, when evaluated on the EgoExo4D dataset, SkillFormer achieves\nstate-of-the-art accuracy in multi-view settings while demonstrating remarkable\ncomputational efficiency, using 4.5x fewer parameters and requiring 3.75x fewer\ntraining epochs than prior baselines. It excels in multiple structured tasks,\nconfirming the value of multi-view integration for fine-grained skill\nassessment.",
      "upvotes": 0,
      "discussionId": "68243bded08d8e01109d56cc",
      "ai_keywords": [
        "parameter-efficient architecture",
        "TimeSformer backbone",
        "CrossViewFusion module",
        "multi-head cross-attention",
        "learnable gating",
        "adaptive self-calibration",
        "Low-Rank Adaptation",
        "fine-tune",
        "multi-view settings",
        "structured tasks",
        "multi-view integration"
      ]
    },
    "publishedAt": "2025-05-13T11:27:24.000Z",
    "title": "SkillFormer: Unified Multi-View Video Understanding for Proficiency\n  Estimation",
    "summary": "Assessing human skill levels in complex activities is a challenging problem\nwith applications in sports, rehabilitation, and training. In this work, we\npresent SkillFormer, a parameter-efficient architecture for unified multi-view\nproficiency estimation from egocentric and exocentric videos. Building on the\nTimeSformer backbone, SkillFormer introduces a CrossViewFusion module that\nfuses view-specific features using multi-head cross-attention, learnable\ngating, and adaptive self-calibration. We leverage Low-Rank Adaptation to\nfine-tune only a small subset of parameters, significantly reducing training\ncosts. In fact, when evaluated on the EgoExo4D dataset, SkillFormer achieves\nstate-of-the-art accuracy in multi-view settings while demonstrating remarkable\ncomputational efficiency, using 4.5x fewer parameters and requiring 3.75x fewer\ntraining epochs than prior baselines. It excels in multiple structured tasks,\nconfirming the value of multi-view integration for fine-grained skill\nassessment.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.08665.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "622dc11fe27c88667db093fc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1667052350862-622dc11fe27c88667db093fc.jpeg",
      "fullname": "Edoardo Bianchi",
      "name": "EdBianchi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.07416",
      "authors": [
        {
          "_id": "68238a5124c55c2bd5bec8b5",
          "user": {
            "_id": "68238b250a4767fd1572ce33",
            "avatarUrl": "/avatars/7e824a30f9d07ed992633aba8ad11b6c.svg",
            "isPro": false,
            "fullname": "Truc Mai-Thanh Nguyen",
            "user": "trucnguyen28",
            "type": "user"
          },
          "name": "Truc Mai-Thanh Nguyen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-14T07:35:04.131Z",
          "hidden": false
        },
        {
          "_id": "68238a5124c55c2bd5bec8b6",
          "name": "Dat Minh Nguyen",
          "hidden": false
        },
        {
          "_id": "68238a5124c55c2bd5bec8b7",
          "name": "Son T. Luu",
          "hidden": false
        },
        {
          "_id": "68238a5124c55c2bd5bec8b8",
          "name": "Kiet Van Nguyen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-12T10:11:28.000Z",
      "submittedOnDailyAt": "2025-05-14T06:09:57.940Z",
      "title": "ViMRHP: A Vietnamese Benchmark Dataset for Multimodal Review Helpfulness\n  Prediction via Human-AI Collaborative Annotation",
      "submittedOnDailyBy": {
        "_id": "68238b250a4767fd1572ce33",
        "avatarUrl": "/avatars/7e824a30f9d07ed992633aba8ad11b6c.svg",
        "isPro": false,
        "fullname": "Truc Mai-Thanh Nguyen",
        "user": "trucnguyen28",
        "type": "user"
      },
      "summary": "Multimodal Review Helpfulness Prediction (MRHP) is an essential task in\nrecommender systems, particularly in E-commerce platforms. Determining the\nhelpfulness of user-generated reviews enhances user experience and improves\nconsumer decision-making. However, existing datasets focus predominantly on\nEnglish and Indonesian, resulting in a lack of linguistic diversity, especially\nfor low-resource languages such as Vietnamese. In this paper, we introduce\nViMRHP (Vietnamese Multimodal Review Helpfulness Prediction), a large-scale\nbenchmark dataset for MRHP task in Vietnamese. This dataset covers four\ndomains, including 2K products with 46K reviews. Meanwhile, a large-scale\ndataset requires considerable time and cost. To optimize the annotation\nprocess, we leverage AI to assist annotators in constructing the ViMRHP\ndataset. With AI assistance, annotation time is reduced (90 to 120 seconds per\ntask down to 20 to 40 seconds per task) while maintaining data quality and\nlowering overall costs by approximately 65%. However, AI-generated annotations\nstill have limitations in complex annotation tasks, which we further examine\nthrough a detailed performance analysis. In our experiment on ViMRHP, we\nevaluate baseline models on human-verified and AI-generated annotations to\nassess their quality differences. The ViMRHP dataset is publicly available at\nhttps://github.com/trng28/ViMRHP",
      "upvotes": 0,
      "discussionId": "68238a5324c55c2bd5bec921",
      "githubRepo": "https://github.com/trng28/ViMRHP"
    },
    "publishedAt": "2025-05-12T06:11:28.000Z",
    "title": "ViMRHP: A Vietnamese Benchmark Dataset for Multimodal Review Helpfulness\n  Prediction via Human-AI Collaborative Annotation",
    "summary": "Multimodal Review Helpfulness Prediction (MRHP) is an essential task in\nrecommender systems, particularly in E-commerce platforms. Determining the\nhelpfulness of user-generated reviews enhances user experience and improves\nconsumer decision-making. However, existing datasets focus predominantly on\nEnglish and Indonesian, resulting in a lack of linguistic diversity, especially\nfor low-resource languages such as Vietnamese. In this paper, we introduce\nViMRHP (Vietnamese Multimodal Review Helpfulness Prediction), a large-scale\nbenchmark dataset for MRHP task in Vietnamese. This dataset covers four\ndomains, including 2K products with 46K reviews. Meanwhile, a large-scale\ndataset requires considerable time and cost. To optimize the annotation\nprocess, we leverage AI to assist annotators in constructing the ViMRHP\ndataset. With AI assistance, annotation time is reduced (90 to 120 seconds per\ntask down to 20 to 40 seconds per task) while maintaining data quality and\nlowering overall costs by approximately 65%. However, AI-generated annotations\nstill have limitations in complex annotation tasks, which we further examine\nthrough a detailed performance analysis. In our experiment on ViMRHP, we\nevaluate baseline models on human-verified and AI-generated annotations to\nassess their quality differences. The ViMRHP dataset is publicly available at\nhttps://github.com/trng28/ViMRHP",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.07416.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "68238b250a4767fd1572ce33",
      "avatarUrl": "/avatars/7e824a30f9d07ed992633aba8ad11b6c.svg",
      "fullname": "Truc Mai-Thanh Nguyen",
      "name": "trucnguyen28",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]