[
  {
    "paper": {
      "id": "2504.10479",
      "authors": [
        {
          "_id": "67fdd08bda7816922cb67e54",
          "name": "Jinguo Zhu",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e55",
          "name": "Weiyun Wang",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e56",
          "name": "Zhe Chen",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e57",
          "name": "Zhaoyang Liu",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e58",
          "name": "Shenglong Ye",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e59",
          "name": "Lixin Gu",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e5a",
          "name": "Yuchen Duan",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e5b",
          "name": "Hao Tian",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e5c",
          "name": "Weijie Su",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e5d",
          "name": "Jie Shao",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e5e",
          "name": "Zhangwei Gao",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e5f",
          "name": "Erfei Cui",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e60",
          "name": "Yue Cao",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e61",
          "name": "Yangzhou Liu",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e62",
          "name": "Weiye Xu",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e63",
          "name": "Hao Li",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e64",
          "name": "Jiahao Wang",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e65",
          "name": "Han Lv",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e66",
          "name": "Dengnian Chen",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e67",
          "name": "Songze Li",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e68",
          "name": "Yinan He",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e69",
          "name": "Tan Jiang",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e6a",
          "name": "Jiapeng Luo",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e6b",
          "name": "Yi Wang",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e6c",
          "name": "Conghui He",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e6d",
          "name": "Botian Shi",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e6e",
          "name": "Xingcheng Zhang",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e6f",
          "name": "Wenqi Shao",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e70",
          "name": "Junjun He",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e71",
          "name": "Yingtong Xiong",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e72",
          "name": "Wenwen Qu",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e73",
          "name": "Peng Sun",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e74",
          "name": "Penglong Jiao",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e75",
          "name": "Lijun Wu",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e76",
          "name": "Kaipeng Zhang",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e77",
          "name": "Huipeng Deng",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e78",
          "name": "Jiaye Ge",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e79",
          "name": "Kai Chen",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e7a",
          "name": "Limin Wang",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e7b",
          "name": "Min Dou",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e7c",
          "name": "Lewei Lu",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e7d",
          "name": "Xizhou Zhu",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e7e",
          "name": "Tong Lu",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e7f",
          "name": "Dahua Lin",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e80",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e81",
          "name": "Jifeng Dai",
          "hidden": false
        },
        {
          "_id": "67fdd08bda7816922cb67e82",
          "name": "Wenhai Wang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/yA_DVt5PKVaFjiY-E93e5.png",
        "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/94PuKH6M3nhmkVS8RXtb4.png",
        "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/qvaW8f5868-P75pyzr6XT.png",
        "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/4GqLJBNhM2rUqS_4mawkk.png",
        "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/yBSGw85-QcThBlXv2tLj0.png",
        "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/CHvbRf2VJg6amEGS9OlEE.png",
        "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/VCBvp4nhvzfokH-viUz1g.png",
        "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/HIC3iirmK6jaOax4GlDYt.png",
        "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/TLlWnFPBguYJsvuGGqykz.png",
        "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/xJ-sFLuCkv1bqL_TXwxgH.png",
        "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/Hsv_hehORgnRmL68xSqmd.png",
        "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/S1fnELtJWFU_FlEozV0Ik.png"
      ],
      "publishedAt": "2025-04-14T17:59:25.000Z",
      "submittedOnDailyAt": "2025-04-15T01:57:22.403Z",
      "title": "InternVL3: Exploring Advanced Training and Test-Time Recipes for\n  Open-Source Multimodal Models",
      "submittedOnDailyBy": {
        "_id": "619507e7b74b6c591f794340",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/619507e7b74b6c591f794340/JbPDoy6Ko1V1-6oJJwFV8.jpeg",
        "isPro": false,
        "fullname": "Weiyun Wang",
        "user": "Weiyun1025",
        "type": "user"
      },
      "summary": "We introduce InternVL3, a significant advancement in the InternVL series\nfeaturing a native multimodal pre-training paradigm. Rather than adapting a\ntext-only large language model (LLM) into a multimodal large language model\n(MLLM) that supports visual inputs, InternVL3 jointly acquires multimodal and\nlinguistic capabilities from both diverse multimodal data and pure-text corpora\nduring a single pre-training stage. This unified training paradigm effectively\naddresses the complexities and alignment challenges commonly encountered in\nconventional post-hoc training pipelines for MLLMs. To further improve\nperformance and scalability, InternVL3 incorporates variable visual position\nencoding (V2PE) to support extended multimodal contexts, employs advanced\npost-training techniques such as supervised fine-tuning (SFT) and mixed\npreference optimization (MPO), and adopts test-time scaling strategies\nalongside an optimized training infrastructure. Extensive empirical evaluations\ndemonstrate that InternVL3 delivers superior performance across a wide range of\nmulti-modal tasks. In particular, InternVL3-78B achieves a score of 72.2 on the\nMMMU benchmark, setting a new state-of-the-art among open-source MLLMs. Its\ncapabilities remain highly competitive with leading proprietary models,\nincluding ChatGPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro, while also\nmaintaining strong pure-language proficiency. In pursuit of open-science\nprinciples, we will publicly release both the training data and model weights\nto foster further research and development in next-generation MLLMs.",
      "upvotes": 103,
      "discussionId": "67fdd08cda7816922cb67ec2",
      "projectPage": "https://internvl.github.io/blog/2025-04-11-InternVL-3.0/",
      "githubRepo": "https://github.com/OpenGVLab/InternVL"
    },
    "publishedAt": "2025-04-14T13:59:25.000Z",
    "title": "InternVL3: Exploring Advanced Training and Test-Time Recipes for\n  Open-Source Multimodal Models",
    "summary": "We introduce InternVL3, a significant advancement in the InternVL series\nfeaturing a native multimodal pre-training paradigm. Rather than adapting a\ntext-only large language model (LLM) into a multimodal large language model\n(MLLM) that supports visual inputs, InternVL3 jointly acquires multimodal and\nlinguistic capabilities from both diverse multimodal data and pure-text corpora\nduring a single pre-training stage. This unified training paradigm effectively\naddresses the complexities and alignment challenges commonly encountered in\nconventional post-hoc training pipelines for MLLMs. To further improve\nperformance and scalability, InternVL3 incorporates variable visual position\nencoding (V2PE) to support extended multimodal contexts, employs advanced\npost-training techniques such as supervised fine-tuning (SFT) and mixed\npreference optimization (MPO), and adopts test-time scaling strategies\nalongside an optimized training infrastructure. Extensive empirical evaluations\ndemonstrate that InternVL3 delivers superior performance across a wide range of\nmulti-modal tasks. In particular, InternVL3-78B achieves a score of 72.2 on the\nMMMU benchmark, setting a new state-of-the-art among open-source MLLMs. Its\ncapabilities remain highly competitive with leading proprietary models,\nincluding ChatGPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro, while also\nmaintaining strong pure-language proficiency. In pursuit of open-science\nprinciples, we will publicly release both the training data and model weights\nto foster further research and development in next-generation MLLMs.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/yA_DVt5PKVaFjiY-E93e5.png",
      "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/94PuKH6M3nhmkVS8RXtb4.png",
      "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/qvaW8f5868-P75pyzr6XT.png",
      "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/4GqLJBNhM2rUqS_4mawkk.png",
      "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/yBSGw85-QcThBlXv2tLj0.png",
      "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/CHvbRf2VJg6amEGS9OlEE.png",
      "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/VCBvp4nhvzfokH-viUz1g.png",
      "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/HIC3iirmK6jaOax4GlDYt.png",
      "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/TLlWnFPBguYJsvuGGqykz.png",
      "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/xJ-sFLuCkv1bqL_TXwxgH.png",
      "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/Hsv_hehORgnRmL68xSqmd.png",
      "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/S1fnELtJWFU_FlEozV0Ik.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10479.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "619507e7b74b6c591f794340",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/619507e7b74b6c591f794340/JbPDoy6Ko1V1-6oJJwFV8.jpeg",
      "fullname": "Weiyun Wang",
      "name": "Weiyun1025",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 17
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.08791",
      "authors": [
        {
          "_id": "67fdbde764a418633ee9fa1b",
          "name": "Zonghang Li",
          "hidden": false
        },
        {
          "_id": "67fdbde764a418633ee9fa1c",
          "name": "Tao Li",
          "hidden": false
        },
        {
          "_id": "67fdbde764a418633ee9fa1d",
          "name": "Wenjiao Feng",
          "hidden": false
        },
        {
          "_id": "67fdbde764a418633ee9fa1e",
          "name": "Mohsen Guizani",
          "hidden": false
        },
        {
          "_id": "67fdbde764a418633ee9fa1f",
          "name": "Hongfang Yu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/647466b8b68461d5cf795e3c/Z-XPxpdhg3sBxHCfogGzf.mp4"
      ],
      "publishedAt": "2025-04-07T13:46:21.000Z",
      "submittedOnDailyAt": "2025-04-15T00:38:03.208Z",
      "title": "PRIMA.CPP: Speeding Up 70B-Scale LLM Inference on Low-Resource Everyday\n  Home Clusters",
      "submittedOnDailyBy": {
        "_id": "647466b8b68461d5cf795e3c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647466b8b68461d5cf795e3c/zaK6sdCbdPfYu14vg2Ty6.png",
        "isPro": false,
        "fullname": "LIKirin",
        "user": "LIKirin",
        "type": "user"
      },
      "summary": "Emergency of DeepSeek R1 and QwQ 32B have broken through performance barriers\nfor running frontier large language models (LLMs) on home devices. While\nconsumer hardware is getting stronger and model quantization is improving,\nexisting end-side solutions still demand GPU clusters, large RAM/VRAM, and high\nbandwidth, far beyond what a common home cluster can handle. This paper\nintroduces prima.cpp, a distributed inference system that runs 70B-scale models\non everyday home devices using a mix of CPU/GPU, low RAM/VRAM, Wi-Fi, and\ncross-platform support. It uses mmap to manage model weights and introduces\npiped-ring parallelism with prefetching to hide disk loading. By modeling\nheterogeneity in computation, communication, disk, memory (and its management\nbehavior), and OS, it optimally assigns model layers to each device's CPU and\nGPU, further reducing token latency. An elegant algorithm named Halda is\nproposed to solve this NP-hard assignment problem. We evaluate prima.cpp on a\ncommon four-node home cluster. It outperforms llama.cpp, exo, and dllama on\n30B+ models while keeping memory pressure below 6%. This brings frontier\n30B-70B models, such as Llama 3, DeepSeek R1, Qwen 2.5, and QwQ to home\nassistants, making advanced AI truly accessible to individuals. The code is\nopen source and available at https://github.com/Lizonghang/prima.cpp.",
      "upvotes": 64,
      "discussionId": "67fdbdeb64a418633ee9fb58",
      "projectPage": "https://github.com/Lizonghang/prima.cpp",
      "githubRepo": "https://github.com/Lizonghang/prima.cpp"
    },
    "publishedAt": "2025-04-07T09:46:21.000Z",
    "title": "PRIMA.CPP: Speeding Up 70B-Scale LLM Inference on Low-Resource Everyday\n  Home Clusters",
    "summary": "Emergency of DeepSeek R1 and QwQ 32B have broken through performance barriers\nfor running frontier large language models (LLMs) on home devices. While\nconsumer hardware is getting stronger and model quantization is improving,\nexisting end-side solutions still demand GPU clusters, large RAM/VRAM, and high\nbandwidth, far beyond what a common home cluster can handle. This paper\nintroduces prima.cpp, a distributed inference system that runs 70B-scale models\non everyday home devices using a mix of CPU/GPU, low RAM/VRAM, Wi-Fi, and\ncross-platform support. It uses mmap to manage model weights and introduces\npiped-ring parallelism with prefetching to hide disk loading. By modeling\nheterogeneity in computation, communication, disk, memory (and its management\nbehavior), and OS, it optimally assigns model layers to each device's CPU and\nGPU, further reducing token latency. An elegant algorithm named Halda is\nproposed to solve this NP-hard assignment problem. We evaluate prima.cpp on a\ncommon four-node home cluster. It outperforms llama.cpp, exo, and dllama on\n30B+ models while keeping memory pressure below 6%. This brings frontier\n30B-70B models, such as Llama 3, DeepSeek R1, Qwen 2.5, and QwQ to home\nassistants, making advanced AI truly accessible to individuals. The code is\nopen source and available at https://github.com/Lizonghang/prima.cpp.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/647466b8b68461d5cf795e3c/Z-XPxpdhg3sBxHCfogGzf.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08791.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "647466b8b68461d5cf795e3c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647466b8b68461d5cf795e3c/zaK6sdCbdPfYu14vg2Ty6.png",
      "fullname": "LIKirin",
      "name": "LIKirin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.09925",
      "authors": [
        {
          "_id": "67fdb5f1913c97aa32f130bd",
          "name": "Zheng Liu",
          "hidden": false
        },
        {
          "_id": "67fdb5f1913c97aa32f130be",
          "name": "Mengjie Liu",
          "hidden": false
        },
        {
          "_id": "67fdb5f1913c97aa32f130bf",
          "name": "Jingzhou Chen",
          "hidden": false
        },
        {
          "_id": "67fdb5f1913c97aa32f130c0",
          "name": "Jingwei Xu",
          "hidden": false
        },
        {
          "_id": "67fdb5f1913c97aa32f130c1",
          "name": "Bin Cui",
          "hidden": false
        },
        {
          "_id": "67fdb5f1913c97aa32f130c2",
          "name": "Conghui He",
          "hidden": false
        },
        {
          "_id": "67fdb5f1913c97aa32f130c3",
          "name": "Wentao Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T06:33:29.000Z",
      "submittedOnDailyAt": "2025-04-15T00:17:19.508Z",
      "title": "FUSION: Fully Integration of Vision-Language Representations for Deep\n  Cross-Modal Understanding",
      "submittedOnDailyBy": {
        "_id": "6625ef13605f46d05c1d0031",
        "avatarUrl": "/avatars/22f201dca35e43013cb593884516e96c.svg",
        "isPro": false,
        "fullname": "Zheng Liu",
        "user": "starriver030515",
        "type": "user"
      },
      "summary": "We introduce FUSION, a family of multimodal large language models (MLLMs)\nwith a fully vision-language alignment and integration paradigm. Unlike\nexisting methods that primarily rely on late-stage modality interaction during\nLLM decoding, our approach achieves deep, dynamic integration throughout the\nentire processing pipeline. To this end, we propose Text-Guided Unified Vision\nEncoding, incorporating textual information in vision encoding to achieve\npixel-level integration. We further design Context-Aware Recursive Alignment\nDecoding that recursively aggregates visual features conditioned on textual\ncontext during decoding, enabling fine-grained, question-level semantic\nintegration. To guide feature mapping and mitigate modality discrepancies, we\ndevelop Dual-Supervised Semantic Mapping Loss. Additionally, we construct a\nSynthesized Language-Driven Question-Answer (QA) dataset through a new data\nsynthesis method, prioritizing high-quality QA pairs to optimize text-guided\nfeature integration. Building on these foundations, we train FUSION at two\nscales-3B, 8B-and demonstrate that our full-modality integration approach\nsignificantly outperforms existing methods with only 630 vision tokens.\nNotably, FUSION 3B surpasses Cambrian-1 8B and Florence-VL 8B on most\nbenchmarks. FUSION 3B continues to outperform Cambrian-1 8B even when limited\nto 300 vision tokens. Our ablation studies show that FUSION outperforms\nLLaVA-NeXT on over half of the benchmarks under same configuration without\ndynamic resolution, highlighting the effectiveness of our approach. We release\nour code, model weights, and dataset. https://github.com/starriver030515/FUSION",
      "upvotes": 27,
      "discussionId": "67fdb5f3913c97aa32f13141",
      "githubRepo": "https://github.com/starriver030515/FUSION"
    },
    "publishedAt": "2025-04-14T02:33:29.000Z",
    "title": "FUSION: Fully Integration of Vision-Language Representations for Deep\n  Cross-Modal Understanding",
    "summary": "We introduce FUSION, a family of multimodal large language models (MLLMs)\nwith a fully vision-language alignment and integration paradigm. Unlike\nexisting methods that primarily rely on late-stage modality interaction during\nLLM decoding, our approach achieves deep, dynamic integration throughout the\nentire processing pipeline. To this end, we propose Text-Guided Unified Vision\nEncoding, incorporating textual information in vision encoding to achieve\npixel-level integration. We further design Context-Aware Recursive Alignment\nDecoding that recursively aggregates visual features conditioned on textual\ncontext during decoding, enabling fine-grained, question-level semantic\nintegration. To guide feature mapping and mitigate modality discrepancies, we\ndevelop Dual-Supervised Semantic Mapping Loss. Additionally, we construct a\nSynthesized Language-Driven Question-Answer (QA) dataset through a new data\nsynthesis method, prioritizing high-quality QA pairs to optimize text-guided\nfeature integration. Building on these foundations, we train FUSION at two\nscales-3B, 8B-and demonstrate that our full-modality integration approach\nsignificantly outperforms existing methods with only 630 vision tokens.\nNotably, FUSION 3B surpasses Cambrian-1 8B and Florence-VL 8B on most\nbenchmarks. FUSION 3B continues to outperform Cambrian-1 8B even when limited\nto 300 vision tokens. Our ablation studies show that FUSION outperforms\nLLaVA-NeXT on over half of the benchmarks under same configuration without\ndynamic resolution, highlighting the effectiveness of our approach. We release\nour code, model weights, and dataset. https://github.com/starriver030515/FUSION",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.09925.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6625ef13605f46d05c1d0031",
      "avatarUrl": "/avatars/22f201dca35e43013cb593884516e96c.svg",
      "fullname": "Zheng Liu",
      "name": "starriver030515",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.08837",
      "authors": [
        {
          "_id": "67fdc483ba0d61664fb0a19d",
          "name": "Haozhe Wang",
          "hidden": false
        },
        {
          "_id": "67fdc483ba0d61664fb0a19e",
          "name": "Chao Qu",
          "hidden": false
        },
        {
          "_id": "67fdc483ba0d61664fb0a19f",
          "name": "Zuming Huang",
          "hidden": false
        },
        {
          "_id": "67fdc483ba0d61664fb0a1a0",
          "name": "Wei Chu",
          "hidden": false
        },
        {
          "_id": "67fdc483ba0d61664fb0a1a1",
          "name": "Fangzhen Lin",
          "hidden": false
        },
        {
          "_id": "67fdc483ba0d61664fb0a1a2",
          "user": {
            "_id": "6313a86154e6e5d9f0f94e04",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
            "isPro": false,
            "fullname": "Wenhu Chen",
            "user": "wenhu",
            "type": "user"
          },
          "name": "Wenhu Chen",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-15T02:29:24.168Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-10T17:41:56.000Z",
      "submittedOnDailyAt": "2025-04-15T01:01:12.820Z",
      "title": "VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models\n  with Reinforcement Learning",
      "submittedOnDailyBy": {
        "_id": "6313a86154e6e5d9f0f94e04",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
        "isPro": false,
        "fullname": "Wenhu Chen",
        "user": "wenhu",
        "type": "user"
      },
      "summary": "Recently, slow-thinking systems like GPT-o1 and DeepSeek-R1 have demonstrated\ngreat potential in solving challenging problems through explicit reflection.\nThey significantly outperform the best fast-thinking models, such as GPT-4o, on\nvarious math and science benchmarks. However, their multimodal reasoning\ncapabilities remain on par with fast-thinking models. For instance, GPT-o1's\nperformance on benchmarks like MathVista, MathVerse, and MathVision is similar\nto fast-thinking models. In this paper, we aim to enhance the slow-thinking\ncapabilities of vision-language models using reinforcement learning (without\nrelying on distillation) to advance the state of the art. First, we adapt the\nGRPO algorithm with a novel technique called Selective Sample Replay (SSR) to\naddress the vanishing advantages problem. While this approach yields strong\nperformance, the resulting RL-trained models exhibit limited self-reflection or\nself-verification. To further encourage slow-thinking, we introduce Forced\nRethinking, which appends a textual rethinking trigger to the end of initial\nrollouts in RL training, explicitly enforcing a self-reflection reasoning step.\nBy combining these two techniques, our model, VL-Rethinker, advances\nstate-of-the-art scores on MathVista, MathVerse, and MathVision to achieve\n80.3%, 61.8%, and 43.9% respectively. VL-Rethinker also achieves open-source\nSoTA on multi-disciplinary benchmarks such as MMMU-Pro, EMMA, and MEGA-Bench,\nnarrowing the gap with GPT-o1.",
      "upvotes": 21,
      "discussionId": "67fdc484ba0d61664fb0a1db",
      "projectPage": "https://tiger-ai-lab.github.io/VL-Rethinker/",
      "githubRepo": "https://github.com/TIGER-AI-Lab/VL-Rethinker/"
    },
    "publishedAt": "2025-04-10T13:41:56.000Z",
    "title": "VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models\n  with Reinforcement Learning",
    "summary": "Recently, slow-thinking systems like GPT-o1 and DeepSeek-R1 have demonstrated\ngreat potential in solving challenging problems through explicit reflection.\nThey significantly outperform the best fast-thinking models, such as GPT-4o, on\nvarious math and science benchmarks. However, their multimodal reasoning\ncapabilities remain on par with fast-thinking models. For instance, GPT-o1's\nperformance on benchmarks like MathVista, MathVerse, and MathVision is similar\nto fast-thinking models. In this paper, we aim to enhance the slow-thinking\ncapabilities of vision-language models using reinforcement learning (without\nrelying on distillation) to advance the state of the art. First, we adapt the\nGRPO algorithm with a novel technique called Selective Sample Replay (SSR) to\naddress the vanishing advantages problem. While this approach yields strong\nperformance, the resulting RL-trained models exhibit limited self-reflection or\nself-verification. To further encourage slow-thinking, we introduce Forced\nRethinking, which appends a textual rethinking trigger to the end of initial\nrollouts in RL training, explicitly enforcing a self-reflection reasoning step.\nBy combining these two techniques, our model, VL-Rethinker, advances\nstate-of-the-art scores on MathVista, MathVerse, and MathVision to achieve\n80.3%, 61.8%, and 43.9% respectively. VL-Rethinker also achieves open-source\nSoTA on multi-disciplinary benchmarks such as MMMU-Pro, EMMA, and MEGA-Bench,\nnarrowing the gap with GPT-o1.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08837.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6313a86154e6e5d9f0f94e04",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
      "fullname": "Wenhu Chen",
      "name": "wenhu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 38
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.10068",
      "authors": [
        {
          "_id": "67fdd1d7634e600357b5b7ff",
          "name": "Yang Shi",
          "hidden": false
        },
        {
          "_id": "67fdd1d7634e600357b5b800",
          "name": "Jiaheng Liu",
          "hidden": false
        },
        {
          "_id": "67fdd1d7634e600357b5b801",
          "name": "Yushuo Guan",
          "hidden": false
        },
        {
          "_id": "67fdd1d7634e600357b5b802",
          "name": "Zhenhua Wu",
          "hidden": false
        },
        {
          "_id": "67fdd1d7634e600357b5b803",
          "name": "Yuanxing Zhang",
          "hidden": false
        },
        {
          "_id": "67fdd1d7634e600357b5b804",
          "name": "Zihao Wang",
          "hidden": false
        },
        {
          "_id": "67fdd1d7634e600357b5b805",
          "name": "Weihong Lin",
          "hidden": false
        },
        {
          "_id": "67fdd1d7634e600357b5b806",
          "name": "Jingyun Hua",
          "hidden": false
        },
        {
          "_id": "67fdd1d7634e600357b5b807",
          "name": "Zekun Wang",
          "hidden": false
        },
        {
          "_id": "67fdd1d7634e600357b5b808",
          "name": "Xinlong Chen",
          "hidden": false
        },
        {
          "_id": "67fdd1d7634e600357b5b809",
          "name": "Bohan Zeng",
          "hidden": false
        },
        {
          "_id": "67fdd1d7634e600357b5b80a",
          "name": "Wentao Zhang",
          "hidden": false
        },
        {
          "_id": "67fdd1d7634e600357b5b80b",
          "name": "Fuzheng Zhang",
          "hidden": false
        },
        {
          "_id": "67fdd1d7634e600357b5b80c",
          "name": "Wenjing Yang",
          "hidden": false
        },
        {
          "_id": "67fdd1d7634e600357b5b80d",
          "name": "Di Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T10:14:44.000Z",
      "submittedOnDailyAt": "2025-04-15T01:56:35.077Z",
      "title": "Mavors: Multi-granularity Video Representation for Multimodal Large\n  Language Model",
      "submittedOnDailyBy": {
        "_id": "673c7319d11b1c2e246ead9c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/673c7319d11b1c2e246ead9c/IjFIO--N7Hm_BOEafhEQv.jpeg",
        "isPro": false,
        "fullname": "Yang Shi",
        "user": "DogNeverSleep",
        "type": "user"
      },
      "summary": "Long-context video understanding in multimodal large language models (MLLMs)\nfaces a critical challenge: balancing computational efficiency with the\nretention of fine-grained spatio-temporal patterns. Existing approaches (e.g.,\nsparse sampling, dense sampling with low resolution, and token compression)\nsuffer from significant information loss in temporal dynamics, spatial details,\nor subtle interactions, particularly in videos with complex motion or varying\nresolutions. To address this, we propose Mavors, a novel framework\nthat introduces Multi-granularity\nvideo representation for holistic\nlong-video modeling. Specifically, Mavors directly encodes raw video content\ninto latent representations through two core components: 1) an Intra-chunk\nVision Encoder (IVE) that preserves high-resolution spatial features via 3D\nconvolutions and Vision Transformers, and 2) an Inter-chunk Feature Aggregator\n(IFA) that establishes temporal coherence across chunks using transformer-based\ndependency modeling with chunk-level rotary position encodings. Moreover, the\nframework unifies image and video understanding by treating images as\nsingle-frame videos via sub-image decomposition. Experiments across diverse\nbenchmarks demonstrate Mavors' superiority in maintaining both spatial fidelity\nand temporal continuity, significantly outperforming existing methods in tasks\nrequiring fine-grained spatio-temporal reasoning.",
      "upvotes": 20,
      "discussionId": "67fdd1db634e600357b5b8f4",
      "projectPage": "https://mavors-mllm.github.io/",
      "githubRepo": "https://github.com/DogNeverSleep/Mavors"
    },
    "publishedAt": "2025-04-14T06:14:44.000Z",
    "title": "Mavors: Multi-granularity Video Representation for Multimodal Large\n  Language Model",
    "summary": "Long-context video understanding in multimodal large language models (MLLMs)\nfaces a critical challenge: balancing computational efficiency with the\nretention of fine-grained spatio-temporal patterns. Existing approaches (e.g.,\nsparse sampling, dense sampling with low resolution, and token compression)\nsuffer from significant information loss in temporal dynamics, spatial details,\nor subtle interactions, particularly in videos with complex motion or varying\nresolutions. To address this, we propose Mavors, a novel framework\nthat introduces Multi-granularity\nvideo representation for holistic\nlong-video modeling. Specifically, Mavors directly encodes raw video content\ninto latent representations through two core components: 1) an Intra-chunk\nVision Encoder (IVE) that preserves high-resolution spatial features via 3D\nconvolutions and Vision Transformers, and 2) an Inter-chunk Feature Aggregator\n(IFA) that establishes temporal coherence across chunks using transformer-based\ndependency modeling with chunk-level rotary position encodings. Moreover, the\nframework unifies image and video understanding by treating images as\nsingle-frame videos via sub-image decomposition. Experiments across diverse\nbenchmarks demonstrate Mavors' superiority in maintaining both spatial fidelity\nand temporal continuity, significantly outperforming existing methods in tasks\nrequiring fine-grained spatio-temporal reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10068.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "673c7319d11b1c2e246ead9c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/673c7319d11b1c2e246ead9c/IjFIO--N7Hm_BOEafhEQv.jpeg",
      "fullname": "Yang Shi",
      "name": "DogNeverSleep",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.08942",
      "authors": [
        {
          "_id": "67fdadafdc27362617bbe714",
          "name": "Xing Han Lù",
          "hidden": false
        },
        {
          "_id": "67fdadafdc27362617bbe715",
          "name": "Amirhossein Kazemnejad",
          "hidden": false
        },
        {
          "_id": "67fdadafdc27362617bbe716",
          "name": "Nicholas Meade",
          "hidden": false
        },
        {
          "_id": "67fdadafdc27362617bbe717",
          "name": "Arkil Patel",
          "hidden": false
        },
        {
          "_id": "67fdadafdc27362617bbe718",
          "name": "Dongchan Shin",
          "hidden": false
        },
        {
          "_id": "67fdadafdc27362617bbe719",
          "name": "Alejandra Zambrano",
          "hidden": false
        },
        {
          "_id": "67fdadafdc27362617bbe71a",
          "name": "Karolina Stańczak",
          "hidden": false
        },
        {
          "_id": "67fdadafdc27362617bbe71b",
          "name": "Peter Shaw",
          "hidden": false
        },
        {
          "_id": "67fdadafdc27362617bbe71c",
          "name": "Christopher J. Pal",
          "hidden": false
        },
        {
          "_id": "67fdadafdc27362617bbe71d",
          "user": {
            "_id": "624734dc4c731bb6bfab8af7",
            "avatarUrl": "/avatars/6b250b58710a3287b85e4733c1824558.svg",
            "isPro": false,
            "fullname": "Siva Reddy",
            "user": "sivareddyg",
            "type": "user"
          },
          "name": "Siva Reddy",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-15T00:51:59.987Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/5fa9ff3ea13e063b8b2b60cb/-XsJQfRUJ6uZhuI8h9cDm.png"
      ],
      "publishedAt": "2025-04-11T19:49:22.000Z",
      "submittedOnDailyAt": "2025-04-15T00:59:48.327Z",
      "title": "AgentRewardBench: Evaluating Automatic Evaluations of Web Agent\n  Trajectories",
      "submittedOnDailyBy": {
        "_id": "5fa9ff3ea13e063b8b2b60cb",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1633380224986-5fa9ff3ea13e063b8b2b60cb.jpeg",
        "isPro": false,
        "fullname": "Xing Han Lù",
        "user": "xhluca",
        "type": "user"
      },
      "summary": "Web agents enable users to perform tasks on web browsers through natural\nlanguage interaction. Evaluating web agents trajectories is an important\nproblem, since it helps us determine whether the agent successfully completed\nthe tasks. Rule-based methods are widely used for this purpose, but they are\nchallenging to extend to new tasks and may not always recognize successful\ntrajectories. We may achieve higher accuracy through human evaluation, but the\nprocess would be substantially slower and more expensive. Automatic evaluations\nwith LLMs may avoid the challenges of designing new rules and manually\nannotating trajectories, enabling faster and cost-effective evaluation.\nHowever, it is unclear how effective they are at evaluating web agents. To this\nend, we propose AgentRewardBench, the first benchmark to assess the\neffectiveness of LLM judges for evaluating web agents. AgentRewardBench\ncontains 1302 trajectories across 5 benchmarks and 4 LLMs. Each trajectory in\nAgentRewardBench is reviewed by an expert, who answers questions pertaining to\nthe success, side effects, and repetitiveness of the agent. Using our\nbenchmark, we evaluate 12 LLM judges and find that no single LLM excels across\nall benchmarks. We also find that the rule-based evaluation used by common\nbenchmarks tends to underreport the success rate of web agents, highlighting a\nkey weakness of rule-based evaluation and the need to develop more flexible\nautomatic evaluations. We release the benchmark at:\nhttps://agent-reward-bench.github.io",
      "upvotes": 13,
      "discussionId": "67fdadb0dc27362617bbe749",
      "projectPage": "https://agent-reward-bench.github.io/",
      "githubRepo": "https://github.com/McGill-NLP/agent-reward-bench"
    },
    "publishedAt": "2025-04-11T15:49:22.000Z",
    "title": "AgentRewardBench: Evaluating Automatic Evaluations of Web Agent\n  Trajectories",
    "summary": "Web agents enable users to perform tasks on web browsers through natural\nlanguage interaction. Evaluating web agents trajectories is an important\nproblem, since it helps us determine whether the agent successfully completed\nthe tasks. Rule-based methods are widely used for this purpose, but they are\nchallenging to extend to new tasks and may not always recognize successful\ntrajectories. We may achieve higher accuracy through human evaluation, but the\nprocess would be substantially slower and more expensive. Automatic evaluations\nwith LLMs may avoid the challenges of designing new rules and manually\nannotating trajectories, enabling faster and cost-effective evaluation.\nHowever, it is unclear how effective they are at evaluating web agents. To this\nend, we propose AgentRewardBench, the first benchmark to assess the\neffectiveness of LLM judges for evaluating web agents. AgentRewardBench\ncontains 1302 trajectories across 5 benchmarks and 4 LLMs. Each trajectory in\nAgentRewardBench is reviewed by an expert, who answers questions pertaining to\nthe success, side effects, and repetitiveness of the agent. Using our\nbenchmark, we evaluate 12 LLM judges and find that no single LLM excels across\nall benchmarks. We also find that the rule-based evaluation used by common\nbenchmarks tends to underreport the success rate of web agents, highlighting a\nkey weakness of rule-based evaluation and the need to develop more flexible\nautomatic evaluations. We release the benchmark at:\nhttps://agent-reward-bench.github.io",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/5fa9ff3ea13e063b8b2b60cb/-XsJQfRUJ6uZhuI8h9cDm.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08942.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5fa9ff3ea13e063b8b2b60cb",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1633380224986-5fa9ff3ea13e063b8b2b60cb.jpeg",
      "fullname": "Xing Han Lù",
      "name": "xhluca",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.10368",
      "authors": [
        {
          "_id": "67fdd3a917e86592095a3ab7",
          "name": "Wenyuan Zhang",
          "hidden": false
        },
        {
          "_id": "67fdd3a917e86592095a3ab8",
          "name": "Shuaiyi Nie",
          "hidden": false
        },
        {
          "_id": "67fdd3a917e86592095a3ab9",
          "name": "Xinghua Zhang",
          "hidden": false
        },
        {
          "_id": "67fdd3a917e86592095a3aba",
          "name": "Zefeng Zhang",
          "hidden": false
        },
        {
          "_id": "67fdd3a917e86592095a3abb",
          "name": "Tingwen Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T16:13:23.000Z",
      "submittedOnDailyAt": "2025-04-15T02:04:54.102Z",
      "title": "S1-Bench: A Simple Benchmark for Evaluating System 1 Thinking Capability\n  of Large Reasoning Models",
      "submittedOnDailyBy": {
        "_id": "656426e7ec7e2398990a2d34",
        "avatarUrl": "/avatars/e84aed6b55b0554ad9581ae3c138a16a.svg",
        "isPro": false,
        "fullname": "AIRobotZ",
        "user": "AIRobotZ",
        "type": "user"
      },
      "summary": "We introduce S1-Bench, a novel benchmark designed to evaluate Large Reasoning\nModels' (LRMs) performance on simple tasks that favor intuitive system 1\nthinking rather than deliberative system 2 reasoning. While LRMs have achieved\nsignificant breakthroughs in complex reasoning tasks through explicit chains of\nthought, their reliance on deep analytical thinking may limit their system 1\nthinking capabilities. Moreover, a lack of benchmark currently exists to\nevaluate LRMs' performance in tasks that require such capabilities. To fill\nthis gap, S1-Bench presents a set of simple, diverse, and naturally clear\nquestions across multiple domains and languages, specifically designed to\nassess LRMs' performance in such tasks. Our comprehensive evaluation of 22 LRMs\nreveals significant lower efficiency tendencies, with outputs averaging 15.5\ntimes longer than those of traditional small LLMs. Additionally, LRMs often\nidentify correct answers early but continue unnecessary deliberation, with some\nmodels even producing numerous errors. These findings highlight the rigid\nreasoning patterns of current LRMs and underscore the substantial development\nneeded to achieve balanced dual-system thinking capabilities that can adapt\nappropriately to task complexity.",
      "upvotes": 11,
      "discussionId": "67fdd3aa17e86592095a3b0b"
    },
    "publishedAt": "2025-04-14T12:13:23.000Z",
    "title": "S1-Bench: A Simple Benchmark for Evaluating System 1 Thinking Capability\n  of Large Reasoning Models",
    "summary": "We introduce S1-Bench, a novel benchmark designed to evaluate Large Reasoning\nModels' (LRMs) performance on simple tasks that favor intuitive system 1\nthinking rather than deliberative system 2 reasoning. While LRMs have achieved\nsignificant breakthroughs in complex reasoning tasks through explicit chains of\nthought, their reliance on deep analytical thinking may limit their system 1\nthinking capabilities. Moreover, a lack of benchmark currently exists to\nevaluate LRMs' performance in tasks that require such capabilities. To fill\nthis gap, S1-Bench presents a set of simple, diverse, and naturally clear\nquestions across multiple domains and languages, specifically designed to\nassess LRMs' performance in such tasks. Our comprehensive evaluation of 22 LRMs\nreveals significant lower efficiency tendencies, with outputs averaging 15.5\ntimes longer than those of traditional small LLMs. Additionally, LRMs often\nidentify correct answers early but continue unnecessary deliberation, with some\nmodels even producing numerous errors. These findings highlight the rigid\nreasoning patterns of current LRMs and underscore the substantial development\nneeded to achieve balanced dual-system thinking capabilities that can adapt\nappropriately to task complexity.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10368.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "656426e7ec7e2398990a2d34",
      "avatarUrl": "/avatars/e84aed6b55b0554ad9581ae3c138a16a.svg",
      "fullname": "AIRobotZ",
      "name": "AIRobotZ",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.09710",
      "authors": [
        {
          "_id": "67fdb42aa8deb632ed46d23d",
          "name": "Zhenting Wang",
          "hidden": false
        },
        {
          "_id": "67fdb42aa8deb632ed46d23e",
          "name": "Guofeng Cui",
          "hidden": false
        },
        {
          "_id": "67fdb42aa8deb632ed46d23f",
          "user": {
            "_id": "66274e02348a5304435dc9cc",
            "avatarUrl": "/avatars/bda87559cd497c310597c2fc8430b31f.svg",
            "isPro": false,
            "fullname": "Kun Wan",
            "user": "timecuriosity",
            "type": "user"
          },
          "name": "Kun Wan",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-04-15T01:22:46.116Z",
          "hidden": false
        },
        {
          "_id": "67fdb42aa8deb632ed46d240",
          "name": "Wentian Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-13T20:10:27.000Z",
      "submittedOnDailyAt": "2025-04-15T00:08:28.286Z",
      "title": "DUMP: Automated Distribution-Level Curriculum Learning for RL-based LLM\n  Post-training",
      "submittedOnDailyBy": {
        "_id": "64dfcc62e8b6f3f3baa950e0",
        "avatarUrl": "/avatars/21bbff67d46c08044efe2406575aa77e.svg",
        "isPro": false,
        "fullname": "Zhenting Wang",
        "user": "ztwang",
        "type": "user"
      },
      "summary": "Recent advances in reinforcement learning (RL)-based post-training have led\nto notable improvements in large language models (LLMs), particularly in\nenhancing their reasoning capabilities to handle complex tasks. However, most\nexisting methods treat the training data as a unified whole, overlooking the\nfact that modern LLM training often involves a mixture of data from diverse\ndistributions-varying in both source and difficulty. This heterogeneity\nintroduces a key challenge: how to adaptively schedule training across\ndistributions to optimize learning efficiency. In this paper, we present a\nprincipled curriculum learning framework grounded in the notion of\ndistribution-level learnability. Our core insight is that the magnitude of\npolicy advantages reflects how much a model can still benefit from further\ntraining on a given distribution. Based on this, we propose a\ndistribution-level curriculum learning framework for RL-based LLM\npost-training, which leverages the Upper Confidence Bound (UCB) principle to\ndynamically adjust sampling probabilities for different distrubutions. This\napproach prioritizes distributions with either high average advantage\n(exploitation) or low sample count (exploration), yielding an adaptive and\ntheoretically grounded training schedule. We instantiate our curriculum\nlearning framework with GRPO as the underlying RL algorithm and demonstrate its\neffectiveness on logic reasoning datasets with multiple difficulties and\nsources. Our experiments show that our framework significantly improves\nconvergence speed and final performance, highlighting the value of\ndistribution-aware curriculum strategies in LLM post-training. Code:\nhttps://github.com/ZhentingWang/DUMP.",
      "upvotes": 11,
      "discussionId": "67fdb457a8deb632ed46de2f"
    },
    "publishedAt": "2025-04-13T16:10:27.000Z",
    "title": "DUMP: Automated Distribution-Level Curriculum Learning for RL-based LLM\n  Post-training",
    "summary": "Recent advances in reinforcement learning (RL)-based post-training have led\nto notable improvements in large language models (LLMs), particularly in\nenhancing their reasoning capabilities to handle complex tasks. However, most\nexisting methods treat the training data as a unified whole, overlooking the\nfact that modern LLM training often involves a mixture of data from diverse\ndistributions-varying in both source and difficulty. This heterogeneity\nintroduces a key challenge: how to adaptively schedule training across\ndistributions to optimize learning efficiency. In this paper, we present a\nprincipled curriculum learning framework grounded in the notion of\ndistribution-level learnability. Our core insight is that the magnitude of\npolicy advantages reflects how much a model can still benefit from further\ntraining on a given distribution. Based on this, we propose a\ndistribution-level curriculum learning framework for RL-based LLM\npost-training, which leverages the Upper Confidence Bound (UCB) principle to\ndynamically adjust sampling probabilities for different distrubutions. This\napproach prioritizes distributions with either high average advantage\n(exploitation) or low sample count (exploration), yielding an adaptive and\ntheoretically grounded training schedule. We instantiate our curriculum\nlearning framework with GRPO as the underlying RL algorithm and demonstrate its\neffectiveness on logic reasoning datasets with multiple difficulties and\nsources. Our experiments show that our framework significantly improves\nconvergence speed and final performance, highlighting the value of\ndistribution-aware curriculum strategies in LLM post-training. Code:\nhttps://github.com/ZhentingWang/DUMP.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.09710.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64dfcc62e8b6f3f3baa950e0",
      "avatarUrl": "/avatars/21bbff67d46c08044efe2406575aa77e.svg",
      "fullname": "Zhenting Wang",
      "name": "ztwang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.08003",
      "authors": [
        {
          "_id": "67fdc0c50c63732d9e0b139a",
          "name": "Ning Li",
          "hidden": false
        },
        {
          "_id": "67fdc0c50c63732d9e0b139b",
          "name": "Jingran Zhang",
          "hidden": false
        },
        {
          "_id": "67fdc0c50c63732d9e0b139c",
          "name": "Justin Cui",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/65862671e878be571bf9fc52/B7iVn73glP5UR6kkdAApX.png"
      ],
      "publishedAt": "2025-04-09T16:10:15.000Z",
      "submittedOnDailyAt": "2025-04-15T00:44:44.354Z",
      "title": "Have we unified image generation and understanding yet? An empirical\n  study of GPT-4o's image generation ability",
      "submittedOnDailyBy": {
        "_id": "65862671e878be571bf9fc52",
        "avatarUrl": "/avatars/b2a1b939f3112b476e7641e0c5fd2dc7.svg",
        "isPro": false,
        "fullname": "bench-llm",
        "user": "cuijiaxing",
        "type": "user"
      },
      "summary": "OpenAI's multimodal GPT-4o has demonstrated remarkable capabilities in image\ngeneration and editing, yet its ability to achieve world knowledge-informed\nsemantic synthesis--seamlessly integrating domain knowledge, contextual\nreasoning, and instruction adherence--remains unproven. In this study, we\nsystematically evaluate these capabilities across three critical dimensions:\n(1) Global Instruction Adherence, (2) Fine-Grained Editing Precision, and (3)\nPost-Generation Reasoning. While existing benchmarks highlight GPT-4o's strong\ncapabilities in image generation and editing, our evaluation reveals GPT-4o's\npersistent limitations: the model frequently defaults to literal\ninterpretations of instructions, inconsistently applies knowledge constraints,\nand struggles with conditional reasoning tasks. These findings challenge\nprevailing assumptions about GPT-4o's unified understanding and generation\ncapabilities, exposing significant gaps in its dynamic knowledge integration.\nOur study calls for the development of more robust benchmarks and training\nstrategies that go beyond surface-level alignment, emphasizing context-aware\nand reasoning-grounded multimodal generation.",
      "upvotes": 10,
      "discussionId": "67fdc0c60c63732d9e0b13d2"
    },
    "publishedAt": "2025-04-09T12:10:15.000Z",
    "title": "Have we unified image generation and understanding yet? An empirical\n  study of GPT-4o's image generation ability",
    "summary": "OpenAI's multimodal GPT-4o has demonstrated remarkable capabilities in image\ngeneration and editing, yet its ability to achieve world knowledge-informed\nsemantic synthesis--seamlessly integrating domain knowledge, contextual\nreasoning, and instruction adherence--remains unproven. In this study, we\nsystematically evaluate these capabilities across three critical dimensions:\n(1) Global Instruction Adherence, (2) Fine-Grained Editing Precision, and (3)\nPost-Generation Reasoning. While existing benchmarks highlight GPT-4o's strong\ncapabilities in image generation and editing, our evaluation reveals GPT-4o's\npersistent limitations: the model frequently defaults to literal\ninterpretations of instructions, inconsistently applies knowledge constraints,\nand struggles with conditional reasoning tasks. These findings challenge\nprevailing assumptions about GPT-4o's unified understanding and generation\ncapabilities, exposing significant gaps in its dynamic knowledge integration.\nOur study calls for the development of more robust benchmarks and training\nstrategies that go beyond surface-level alignment, emphasizing context-aware\nand reasoning-grounded multimodal generation.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/65862671e878be571bf9fc52/B7iVn73glP5UR6kkdAApX.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08003.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65862671e878be571bf9fc52",
      "avatarUrl": "/avatars/b2a1b939f3112b476e7641e0c5fd2dc7.svg",
      "fullname": "bench-llm",
      "name": "cuijiaxing",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.10415",
      "authors": [
        {
          "_id": "67fddc276a3c533dc1d3bcbe",
          "user": {
            "_id": "6520621836008ecc88699622",
            "avatarUrl": "/avatars/b08c00af00f1736a4f4938443e575b0e.svg",
            "isPro": false,
            "fullname": "Parshin Shojaee",
            "user": "parshinsh",
            "type": "user"
          },
          "name": "Parshin Shojaee",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-15T04:10:17.364Z",
          "hidden": false
        },
        {
          "_id": "67fddc276a3c533dc1d3bcbf",
          "name": "Ngoc-Hieu Nguyen",
          "hidden": false
        },
        {
          "_id": "67fddc276a3c533dc1d3bcc0",
          "name": "Kazem Meidani",
          "hidden": false
        },
        {
          "_id": "67fddc276a3c533dc1d3bcc1",
          "name": "Amir Barati Farimani",
          "hidden": false
        },
        {
          "_id": "67fddc276a3c533dc1d3bcc2",
          "name": "Khoa D Doan",
          "hidden": false
        },
        {
          "_id": "67fddc276a3c533dc1d3bcc3",
          "name": "Chandan K Reddy",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T17:00:13.000Z",
      "submittedOnDailyAt": "2025-04-15T02:43:55.941Z",
      "title": "LLM-SRBench: A New Benchmark for Scientific Equation Discovery with\n  Large Language Models",
      "submittedOnDailyBy": {
        "_id": "6520621836008ecc88699622",
        "avatarUrl": "/avatars/b08c00af00f1736a4f4938443e575b0e.svg",
        "isPro": false,
        "fullname": "Parshin Shojaee",
        "user": "parshinsh",
        "type": "user"
      },
      "summary": "Scientific equation discovery is a fundamental task in the history of\nscientific progress, enabling the derivation of laws governing natural\nphenomena. Recently, Large Language Models (LLMs) have gained interest for this\ntask due to their potential to leverage embedded scientific knowledge for\nhypothesis generation. However, evaluating the true discovery capabilities of\nthese methods remains challenging, as existing benchmarks often rely on common\nequations that are susceptible to memorization by LLMs, leading to inflated\nperformance metrics that do not reflect discovery. In this paper, we introduce\nLLM-SRBench, a comprehensive benchmark with 239 challenging problems across\nfour scientific domains specifically designed to evaluate LLM-based scientific\nequation discovery methods while preventing trivial memorization. Our benchmark\ncomprises two main categories: LSR-Transform, which transforms common physical\nmodels into less common mathematical representations to test reasoning beyond\nmemorized forms, and LSR-Synth, which introduces synthetic, discovery-driven\nproblems requiring data-driven reasoning. Through extensive evaluation of\nseveral state-of-the-art methods, using both open and closed LLMs, we find that\nthe best-performing system so far achieves only 31.5% symbolic accuracy. These\nfindings highlight the challenges of scientific equation discovery, positioning\nLLM-SRBench as a valuable resource for future research.",
      "upvotes": 6,
      "discussionId": "67fddc296a3c533dc1d3bd43",
      "projectPage": "https://huggingface.co/datasets/nnheui/llm-srbench",
      "githubRepo": "https://github.com/deep-symbolic-mathematics/llm-srbench"
    },
    "publishedAt": "2025-04-14T13:00:13.000Z",
    "title": "LLM-SRBench: A New Benchmark for Scientific Equation Discovery with\n  Large Language Models",
    "summary": "Scientific equation discovery is a fundamental task in the history of\nscientific progress, enabling the derivation of laws governing natural\nphenomena. Recently, Large Language Models (LLMs) have gained interest for this\ntask due to their potential to leverage embedded scientific knowledge for\nhypothesis generation. However, evaluating the true discovery capabilities of\nthese methods remains challenging, as existing benchmarks often rely on common\nequations that are susceptible to memorization by LLMs, leading to inflated\nperformance metrics that do not reflect discovery. In this paper, we introduce\nLLM-SRBench, a comprehensive benchmark with 239 challenging problems across\nfour scientific domains specifically designed to evaluate LLM-based scientific\nequation discovery methods while preventing trivial memorization. Our benchmark\ncomprises two main categories: LSR-Transform, which transforms common physical\nmodels into less common mathematical representations to test reasoning beyond\nmemorized forms, and LSR-Synth, which introduces synthetic, discovery-driven\nproblems requiring data-driven reasoning. Through extensive evaluation of\nseveral state-of-the-art methods, using both open and closed LLMs, we find that\nthe best-performing system so far achieves only 31.5% symbolic accuracy. These\nfindings highlight the challenges of scientific equation discovery, positioning\nLLM-SRBench as a valuable resource for future research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10415.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6520621836008ecc88699622",
      "avatarUrl": "/avatars/b08c00af00f1736a4f4938443e575b0e.svg",
      "fullname": "Parshin Shojaee",
      "name": "parshinsh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.10157",
      "authors": [
        {
          "_id": "67fdc9adc47ae882c7a17c8a",
          "name": "Xinnong Zhang",
          "hidden": false
        },
        {
          "_id": "67fdc9adc47ae882c7a17c8b",
          "name": "Jiayu Lin",
          "hidden": false
        },
        {
          "_id": "67fdc9adc47ae882c7a17c8c",
          "name": "Xinyi Mou",
          "hidden": false
        },
        {
          "_id": "67fdc9adc47ae882c7a17c8d",
          "name": "Shiyue Yang",
          "hidden": false
        },
        {
          "_id": "67fdc9adc47ae882c7a17c8e",
          "name": "Xiawei Liu",
          "hidden": false
        },
        {
          "_id": "67fdc9adc47ae882c7a17c8f",
          "name": "Libo Sun",
          "hidden": false
        },
        {
          "_id": "67fdc9adc47ae882c7a17c90",
          "name": "Hanjia Lyu",
          "hidden": false
        },
        {
          "_id": "67fdc9adc47ae882c7a17c91",
          "name": "Yihang Yang",
          "hidden": false
        },
        {
          "_id": "67fdc9adc47ae882c7a17c92",
          "name": "Weihong Qi",
          "hidden": false
        },
        {
          "_id": "67fdc9adc47ae882c7a17c93",
          "name": "Yue Chen",
          "hidden": false
        },
        {
          "_id": "67fdc9adc47ae882c7a17c94",
          "name": "Guanying Li",
          "hidden": false
        },
        {
          "_id": "67fdc9adc47ae882c7a17c95",
          "name": "Ling Yan",
          "hidden": false
        },
        {
          "_id": "67fdc9adc47ae882c7a17c96",
          "name": "Yao Hu",
          "hidden": false
        },
        {
          "_id": "67fdc9adc47ae882c7a17c97",
          "name": "Siming Chen",
          "hidden": false
        },
        {
          "_id": "67fdc9adc47ae882c7a17c98",
          "name": "Yu Wang",
          "hidden": false
        },
        {
          "_id": "67fdc9adc47ae882c7a17c99",
          "name": "Jingxuan Huang",
          "hidden": false
        },
        {
          "_id": "67fdc9adc47ae882c7a17c9a",
          "name": "Jiebo Luo",
          "hidden": false
        },
        {
          "_id": "67fdc9adc47ae882c7a17c9b",
          "name": "Shiping Tang",
          "hidden": false
        },
        {
          "_id": "67fdc9adc47ae882c7a17c9c",
          "name": "Libo Wu",
          "hidden": false
        },
        {
          "_id": "67fdc9adc47ae882c7a17c9d",
          "name": "Baohua Zhou",
          "hidden": false
        },
        {
          "_id": "67fdc9adc47ae882c7a17c9e",
          "name": "Zhongyu Wei",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T12:12:52.000Z",
      "submittedOnDailyAt": "2025-04-15T01:29:17.826Z",
      "title": "SocioVerse: A World Model for Social Simulation Powered by LLM Agents\n  and A Pool of 10 Million Real-World Users",
      "submittedOnDailyBy": {
        "_id": "64c939307dba66c3a7e4d215",
        "avatarUrl": "/avatars/0b662cc1799525188476f3e6e1f97d29.svg",
        "isPro": false,
        "fullname": "BruceLyu",
        "user": "brucelyu",
        "type": "user"
      },
      "summary": "Social simulation is transforming traditional social science research by\nmodeling human behavior through interactions between virtual individuals and\ntheir environments. With recent advances in large language models (LLMs), this\napproach has shown growing potential in capturing individual differences and\npredicting group behaviors. However, existing methods face alignment challenges\nrelated to the environment, target users, interaction mechanisms, and\nbehavioral patterns. To this end, we introduce SocioVerse, an LLM-agent-driven\nworld model for social simulation. Our framework features four powerful\nalignment components and a user pool of 10 million real individuals. To\nvalidate its effectiveness, we conducted large-scale simulation experiments\nacross three distinct domains: politics, news, and economics. Results\ndemonstrate that SocioVerse can reflect large-scale population dynamics while\nensuring diversity, credibility, and representativeness through standardized\nprocedures and minimal manual adjustments.",
      "upvotes": 5,
      "discussionId": "67fdc9aec47ae882c7a17cf1"
    },
    "publishedAt": "2025-04-14T08:12:52.000Z",
    "title": "SocioVerse: A World Model for Social Simulation Powered by LLM Agents\n  and A Pool of 10 Million Real-World Users",
    "summary": "Social simulation is transforming traditional social science research by\nmodeling human behavior through interactions between virtual individuals and\ntheir environments. With recent advances in large language models (LLMs), this\napproach has shown growing potential in capturing individual differences and\npredicting group behaviors. However, existing methods face alignment challenges\nrelated to the environment, target users, interaction mechanisms, and\nbehavioral patterns. To this end, we introduce SocioVerse, an LLM-agent-driven\nworld model for social simulation. Our framework features four powerful\nalignment components and a user pool of 10 million real individuals. To\nvalidate its effectiveness, we conducted large-scale simulation experiments\nacross three distinct domains: politics, news, and economics. Results\ndemonstrate that SocioVerse can reflect large-scale population dynamics while\nensuring diversity, credibility, and representativeness through standardized\nprocedures and minimal manual adjustments.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10157.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c939307dba66c3a7e4d215",
      "avatarUrl": "/avatars/0b662cc1799525188476f3e6e1f97d29.svg",
      "fullname": "BruceLyu",
      "name": "brucelyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.09641",
      "authors": [
        {
          "_id": "67fdceb2df4261c001394f59",
          "name": "Xingjian Zhang",
          "hidden": false
        },
        {
          "_id": "67fdceb2df4261c001394f5a",
          "name": "Siwei Wen",
          "hidden": false
        },
        {
          "_id": "67fdceb2df4261c001394f5b",
          "name": "Wenjun Wu",
          "hidden": false
        },
        {
          "_id": "67fdceb2df4261c001394f5c",
          "name": "Lei Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-13T16:32:49.000Z",
      "submittedOnDailyAt": "2025-04-15T01:46:43.707Z",
      "title": "TinyLLaVA-Video-R1: Towards Smaller LMMs for Video Reasoning",
      "submittedOnDailyBy": {
        "_id": "66448136bfe15e84d3987372",
        "avatarUrl": "/avatars/be276a4a4749b59ba6252884f9e2b6e0.svg",
        "isPro": false,
        "fullname": "Zhang Xingjian",
        "user": "Zhang199",
        "type": "user"
      },
      "summary": "Recently, improving the reasoning ability of large multimodal models (LMMs)\nthrough reinforcement learning has made great progress. However, most existing\nworks are based on highly reasoning-intensive datasets such as mathematics and\ncode, and researchers generally choose large-scale models as the foundation. We\nargue that exploring small-scale models' reasoning capabilities remains\nvaluable for researchers with limited computational resources. Moreover,\nenabling models to explain their reasoning processes on general\nquestion-answering datasets is equally meaningful. Therefore, we present the\nsmall-scale video reasoning model TinyLLaVA-Video-R1. Based on TinyLLaVA-Video,\na traceably trained video understanding model with no more than 4B parameters,\nit not only demonstrates significantly improved reasoning and thinking\ncapabilities after using reinforcement learning on general Video-QA datasets,\nbut also exhibits the emergent characteristic of \"aha moments\". Furthermore, we\nshare a series of experimental findings, aiming to provide practical insights\nfor future exploration of video reasoning (thinking) abilities in small-scale\nmodels. It is available at https://github.com/ZhangXJ199/TinyLLaVA-Video-R1.",
      "upvotes": 4,
      "discussionId": "67fdceb5df4261c001394ff5"
    },
    "publishedAt": "2025-04-13T12:32:49.000Z",
    "title": "TinyLLaVA-Video-R1: Towards Smaller LMMs for Video Reasoning",
    "summary": "Recently, improving the reasoning ability of large multimodal models (LMMs)\nthrough reinforcement learning has made great progress. However, most existing\nworks are based on highly reasoning-intensive datasets such as mathematics and\ncode, and researchers generally choose large-scale models as the foundation. We\nargue that exploring small-scale models' reasoning capabilities remains\nvaluable for researchers with limited computational resources. Moreover,\nenabling models to explain their reasoning processes on general\nquestion-answering datasets is equally meaningful. Therefore, we present the\nsmall-scale video reasoning model TinyLLaVA-Video-R1. Based on TinyLLaVA-Video,\na traceably trained video understanding model with no more than 4B parameters,\nit not only demonstrates significantly improved reasoning and thinking\ncapabilities after using reinforcement learning on general Video-QA datasets,\nbut also exhibits the emergent characteristic of \"aha moments\". Furthermore, we\nshare a series of experimental findings, aiming to provide practical insights\nfor future exploration of video reasoning (thinking) abilities in small-scale\nmodels. It is available at https://github.com/ZhangXJ199/TinyLLaVA-Video-R1.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.09641.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "66448136bfe15e84d3987372",
      "avatarUrl": "/avatars/be276a4a4749b59ba6252884f9e2b6e0.svg",
      "fullname": "Zhang Xingjian",
      "name": "Zhang199",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.09689",
      "authors": [
        {
          "_id": "67fdc937089aec0f3b154dd7",
          "name": "Jiahao Qiu",
          "hidden": false
        },
        {
          "_id": "67fdc937089aec0f3b154dd8",
          "name": "Yinghui He",
          "hidden": false
        },
        {
          "_id": "67fdc937089aec0f3b154dd9",
          "name": "Xinzhe Juan",
          "hidden": false
        },
        {
          "_id": "67fdc937089aec0f3b154dda",
          "name": "Yiming Wang",
          "hidden": false
        },
        {
          "_id": "67fdc937089aec0f3b154ddb",
          "name": "Yuhan Liu",
          "hidden": false
        },
        {
          "_id": "67fdc937089aec0f3b154ddc",
          "name": "Zixin Yao",
          "hidden": false
        },
        {
          "_id": "67fdc937089aec0f3b154ddd",
          "name": "Yue Wu",
          "hidden": false
        },
        {
          "_id": "67fdc937089aec0f3b154dde",
          "name": "Xun Jiang",
          "hidden": false
        },
        {
          "_id": "67fdc937089aec0f3b154ddf",
          "name": "Ling Yang",
          "hidden": false
        },
        {
          "_id": "67fdc937089aec0f3b154de0",
          "name": "Mengdi Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-13T18:47:22.000Z",
      "submittedOnDailyAt": "2025-04-15T01:30:41.548Z",
      "title": "EmoAgent: Assessing and Safeguarding Human-AI Interaction for Mental\n  Health Safety",
      "submittedOnDailyBy": {
        "_id": "674500b57a76d46e9141af8b",
        "avatarUrl": "/avatars/e70243c31e2ac9f000542e8504e65b51.svg",
        "isPro": false,
        "fullname": "Xinzhe Juan",
        "user": "ChrisJuan",
        "type": "user"
      },
      "summary": "The rise of LLM-driven AI characters raises safety concerns, particularly for\nvulnerable human users with psychological disorders. To address these risks, we\npropose EmoAgent, a multi-agent AI framework designed to evaluate and mitigate\nmental health hazards in human-AI interactions. EmoAgent comprises two\ncomponents: EmoEval simulates virtual users, including those portraying\nmentally vulnerable individuals, to assess mental health changes before and\nafter interactions with AI characters. It uses clinically proven psychological\nand psychiatric assessment tools (PHQ-9, PDI, PANSS) to evaluate mental risks\ninduced by LLM. EmoGuard serves as an intermediary, monitoring users' mental\nstatus, predicting potential harm, and providing corrective feedback to\nmitigate risks. Experiments conducted in popular character-based chatbots show\nthat emotionally engaging dialogues can lead to psychological deterioration in\nvulnerable users, with mental state deterioration in more than 34.4% of the\nsimulations. EmoGuard significantly reduces these deterioration rates,\nunderscoring its role in ensuring safer AI-human interactions. Our code is\navailable at: https://github.com/1akaman/EmoAgent",
      "upvotes": 2,
      "discussionId": "67fdc938089aec0f3b154e31",
      "githubRepo": "https://github.com/1akaman/EmoAgent"
    },
    "publishedAt": "2025-04-13T14:47:22.000Z",
    "title": "EmoAgent: Assessing and Safeguarding Human-AI Interaction for Mental\n  Health Safety",
    "summary": "The rise of LLM-driven AI characters raises safety concerns, particularly for\nvulnerable human users with psychological disorders. To address these risks, we\npropose EmoAgent, a multi-agent AI framework designed to evaluate and mitigate\nmental health hazards in human-AI interactions. EmoAgent comprises two\ncomponents: EmoEval simulates virtual users, including those portraying\nmentally vulnerable individuals, to assess mental health changes before and\nafter interactions with AI characters. It uses clinically proven psychological\nand psychiatric assessment tools (PHQ-9, PDI, PANSS) to evaluate mental risks\ninduced by LLM. EmoGuard serves as an intermediary, monitoring users' mental\nstatus, predicting potential harm, and providing corrective feedback to\nmitigate risks. Experiments conducted in popular character-based chatbots show\nthat emotionally engaging dialogues can lead to psychological deterioration in\nvulnerable users, with mental state deterioration in more than 34.4% of the\nsimulations. EmoGuard significantly reduces these deterioration rates,\nunderscoring its role in ensuring safer AI-human interactions. Our code is\navailable at: https://github.com/1akaman/EmoAgent",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.09689.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "674500b57a76d46e9141af8b",
      "avatarUrl": "/avatars/e70243c31e2ac9f000542e8504e65b51.svg",
      "fullname": "Xinzhe Juan",
      "name": "ChrisJuan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.10430",
      "authors": [
        {
          "_id": "67fe093302ae092e4306af12",
          "name": "Minqian Liu",
          "hidden": false
        },
        {
          "_id": "67fe093302ae092e4306af13",
          "name": "Zhiyang Xu",
          "hidden": false
        },
        {
          "_id": "67fe093302ae092e4306af14",
          "name": "Xinyi Zhang",
          "hidden": false
        },
        {
          "_id": "67fe093302ae092e4306af15",
          "name": "Heajun An",
          "hidden": false
        },
        {
          "_id": "67fe093302ae092e4306af16",
          "name": "Sarvech Qadir",
          "hidden": false
        },
        {
          "_id": "67fe093302ae092e4306af17",
          "name": "Qi Zhang",
          "hidden": false
        },
        {
          "_id": "67fe093302ae092e4306af18",
          "name": "Pamela J. Wisniewski",
          "hidden": false
        },
        {
          "_id": "67fe093302ae092e4306af19",
          "name": "Jin-Hee Cho",
          "hidden": false
        },
        {
          "_id": "67fe093302ae092e4306af1a",
          "name": "Sang Won Lee",
          "hidden": false
        },
        {
          "_id": "67fe093302ae092e4306af1b",
          "name": "Ruoxi Jia",
          "hidden": false
        },
        {
          "_id": "67fe093302ae092e4306af1c",
          "name": "Lifu Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T17:20:34.000Z",
      "submittedOnDailyAt": "2025-04-15T05:53:52.240Z",
      "title": "LLM Can be a Dangerous Persuader: Empirical Study of Persuasion Safety\n  in Large Language Models",
      "submittedOnDailyBy": {
        "_id": "64c32a75d15a8812b71afc48",
        "avatarUrl": "/avatars/4186c592b00aea73fb8c5bb719935ce7.svg",
        "isPro": false,
        "fullname": "Minqian Liu",
        "user": "mqliu",
        "type": "user"
      },
      "summary": "Recent advancements in Large Language Models (LLMs) have enabled them to\napproach human-level persuasion capabilities. However, such potential also\nraises concerns about the safety risks of LLM-driven persuasion, particularly\ntheir potential for unethical influence through manipulation, deception,\nexploitation of vulnerabilities, and many other harmful tactics. In this work,\nwe present a systematic investigation of LLM persuasion safety through two\ncritical aspects: (1) whether LLMs appropriately reject unethical persuasion\ntasks and avoid unethical strategies during execution, including cases where\nthe initial persuasion goal appears ethically neutral, and (2) how influencing\nfactors like personality traits and external pressures affect their behavior.\nTo this end, we introduce PersuSafety, the first comprehensive framework for\nthe assessment of persuasion safety which consists of three stages, i.e.,\npersuasion scene creation, persuasive conversation simulation, and persuasion\nsafety assessment. PersuSafety covers 6 diverse unethical persuasion topics and\n15 common unethical strategies. Through extensive experiments across 8 widely\nused LLMs, we observe significant safety concerns in most LLMs, including\nfailing to identify harmful persuasion tasks and leveraging various unethical\npersuasion strategies. Our study calls for more attention to improve safety\nalignment in progressive and goal-driven conversations such as persuasion.",
      "upvotes": 1,
      "discussionId": "67fe093402ae092e4306af42"
    },
    "publishedAt": "2025-04-14T13:20:34.000Z",
    "title": "LLM Can be a Dangerous Persuader: Empirical Study of Persuasion Safety\n  in Large Language Models",
    "summary": "Recent advancements in Large Language Models (LLMs) have enabled them to\napproach human-level persuasion capabilities. However, such potential also\nraises concerns about the safety risks of LLM-driven persuasion, particularly\ntheir potential for unethical influence through manipulation, deception,\nexploitation of vulnerabilities, and many other harmful tactics. In this work,\nwe present a systematic investigation of LLM persuasion safety through two\ncritical aspects: (1) whether LLMs appropriately reject unethical persuasion\ntasks and avoid unethical strategies during execution, including cases where\nthe initial persuasion goal appears ethically neutral, and (2) how influencing\nfactors like personality traits and external pressures affect their behavior.\nTo this end, we introduce PersuSafety, the first comprehensive framework for\nthe assessment of persuasion safety which consists of three stages, i.e.,\npersuasion scene creation, persuasive conversation simulation, and persuasion\nsafety assessment. PersuSafety covers 6 diverse unethical persuasion topics and\n15 common unethical strategies. Through extensive experiments across 8 widely\nused LLMs, we observe significant safety concerns in most LLMs, including\nfailing to identify harmful persuasion tasks and leveraging various unethical\npersuasion strategies. Our study calls for more attention to improve safety\nalignment in progressive and goal-driven conversations such as persuasion.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10430.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c32a75d15a8812b71afc48",
      "avatarUrl": "/avatars/4186c592b00aea73fb8c5bb719935ce7.svg",
      "fullname": "Minqian Liu",
      "name": "mqliu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.09763",
      "authors": [
        {
          "_id": "67fdf0faea4d2ba44335ffa7",
          "name": "Zaid Khan",
          "hidden": false
        },
        {
          "_id": "67fdf0faea4d2ba44335ffa8",
          "name": "Elias Stengel-Eskin",
          "hidden": false
        },
        {
          "_id": "67fdf0faea4d2ba44335ffa9",
          "name": "Archiki Prasad",
          "hidden": false
        },
        {
          "_id": "67fdf0faea4d2ba44335ffaa",
          "name": "Jaemin Cho",
          "hidden": false
        },
        {
          "_id": "67fdf0faea4d2ba44335ffab",
          "name": "Mohit Bansal",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6301c3e0a123c93a5fb295ff/6qC9u4ryCuVji8lMCdWQN.png"
      ],
      "publishedAt": "2025-04-14T00:06:48.000Z",
      "submittedOnDailyAt": "2025-04-15T04:15:51.576Z",
      "title": "Executable Functional Abstractions: Inferring Generative Programs for\n  Advanced Math Problems",
      "submittedOnDailyBy": {
        "_id": "6301c3e0a123c93a5fb295ff",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661060051926-noauth.jpeg",
        "isPro": false,
        "fullname": "Zaid Khan",
        "user": "codezakh",
        "type": "user"
      },
      "summary": "Scientists often infer abstract procedures from specific instances of\nproblems and use the abstractions to generate new, related instances. For\nexample, programs encoding the formal rules and properties of a system have\nbeen useful in fields ranging from RL (procedural environments) to physics\n(simulation engines). These programs can be seen as functions which execute to\ndifferent outputs based on their parameterizations (e.g., gridworld\nconfiguration or initial physical conditions). We introduce the term EFA\n(Executable Functional Abstraction) to denote such programs for math problems.\nEFA-like constructs have been shown to be useful for math reasoning as problem\ngenerators for stress-testing models. However, prior work has been limited to\nabstractions for grade-school math (whose simple rules are easy to encode in\nprograms), while generating EFAs for advanced math has thus far required human\nengineering. We explore the automatic construction of EFAs for advanced math\nproblems. We operationalize the task of automatically constructing EFAs as a\nprogram synthesis task, and develop EFAGen, which conditions an LLM on a seed\nmath problem and its step-by-step solution to generate candidate EFA programs\nthat are faithful to the generalized problem and solution class underlying the\nseed problem. Furthermore, we formalize properties any valid EFA must possess\nin terms of executable unit tests, and show how the tests can be used as\nverifiable rewards to train LLMs to become better writers of EFAs. We\ndemonstrate that EFAs constructed by EFAGen behave rationally by remaining\nfaithful to seed problems, produce learnable problem variations, and that\nEFAGen can infer EFAs across multiple diverse sources of competition-level math\nproblems. Finally, we show downstream uses of model-written EFAs e.g. finding\nproblem variations that are harder or easier for a learner to solve, as well as\ndata generation.",
      "upvotes": 1,
      "discussionId": "67fdf0fbea4d2ba44335ffdd",
      "projectPage": "https://zaidkhan.me/EFAGen"
    },
    "publishedAt": "2025-04-13T20:06:48.000Z",
    "title": "Executable Functional Abstractions: Inferring Generative Programs for\n  Advanced Math Problems",
    "summary": "Scientists often infer abstract procedures from specific instances of\nproblems and use the abstractions to generate new, related instances. For\nexample, programs encoding the formal rules and properties of a system have\nbeen useful in fields ranging from RL (procedural environments) to physics\n(simulation engines). These programs can be seen as functions which execute to\ndifferent outputs based on their parameterizations (e.g., gridworld\nconfiguration or initial physical conditions). We introduce the term EFA\n(Executable Functional Abstraction) to denote such programs for math problems.\nEFA-like constructs have been shown to be useful for math reasoning as problem\ngenerators for stress-testing models. However, prior work has been limited to\nabstractions for grade-school math (whose simple rules are easy to encode in\nprograms), while generating EFAs for advanced math has thus far required human\nengineering. We explore the automatic construction of EFAs for advanced math\nproblems. We operationalize the task of automatically constructing EFAs as a\nprogram synthesis task, and develop EFAGen, which conditions an LLM on a seed\nmath problem and its step-by-step solution to generate candidate EFA programs\nthat are faithful to the generalized problem and solution class underlying the\nseed problem. Furthermore, we formalize properties any valid EFA must possess\nin terms of executable unit tests, and show how the tests can be used as\nverifiable rewards to train LLMs to become better writers of EFAs. We\ndemonstrate that EFAs constructed by EFAGen behave rationally by remaining\nfaithful to seed problems, produce learnable problem variations, and that\nEFAGen can infer EFAs across multiple diverse sources of competition-level math\nproblems. Finally, we show downstream uses of model-written EFAs e.g. finding\nproblem variations that are harder or easier for a learner to solve, as well as\ndata generation.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6301c3e0a123c93a5fb295ff/6qC9u4ryCuVji8lMCdWQN.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.09763.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6301c3e0a123c93a5fb295ff",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661060051926-noauth.jpeg",
      "fullname": "Zaid Khan",
      "name": "codezakh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  }
]