[
  {
    "paper": {
      "id": "2511.11257",
      "authors": [
        {
          "_id": "691aa2536bfd5965c0fd358a",
          "name": "Yuqi Yin",
          "hidden": false
        },
        {
          "_id": "691aa2536bfd5965c0fd358b",
          "name": "Yibo Fu",
          "hidden": false
        },
        {
          "_id": "691aa2536bfd5965c0fd358c",
          "name": "Siyuan Wang",
          "hidden": false
        },
        {
          "_id": "691aa2536bfd5965c0fd358d",
          "name": "Peng Sun",
          "hidden": false
        },
        {
          "_id": "691aa2536bfd5965c0fd358e",
          "name": "Hongyu Wang",
          "hidden": false
        },
        {
          "_id": "691aa2536bfd5965c0fd358f",
          "name": "Xiaohui Wang",
          "hidden": false
        },
        {
          "_id": "691aa2536bfd5965c0fd3590",
          "name": "Lei Zheng",
          "hidden": false
        },
        {
          "_id": "691aa2536bfd5965c0fd3591",
          "name": "Zhiyong Li",
          "hidden": false
        },
        {
          "_id": "691aa2536bfd5965c0fd3592",
          "name": "Zhirong Liu",
          "hidden": false
        },
        {
          "_id": "691aa2536bfd5965c0fd3593",
          "name": "Jianji Wang",
          "hidden": false
        },
        {
          "_id": "691aa2536bfd5965c0fd3594",
          "name": "Zhaoxi Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-14T12:53:57.000Z",
      "submittedOnDailyAt": "2025-11-17T03:06:43.136Z",
      "title": "AIonopedia: an LLM agent orchestrating multimodal learning for ionic liquid discovery",
      "submittedOnDailyBy": {
        "_id": "66a5db1c94d2b190a23c9a46",
        "avatarUrl": "/avatars/e2caafde5b9383d63d9917d9476c9ee3.svg",
        "isPro": false,
        "fullname": "Yuqi Yin",
        "user": "chitanda-eru",
        "type": "user"
      },
      "summary": "The discovery of novel Ionic Liquids (ILs) is hindered by critical challenges in property prediction, including limited data, poor model accuracy, and fragmented workflows. Leveraging the power of Large Language Models (LLMs), we introduce AIonopedia, to the best of our knowledge, the first LLM agent for IL discovery. Powered by an LLM-augmented multimodal domain foundation model for ILs, AIonopedia enables accurate property predictions and incorporates a hierarchical search architecture for molecular screening and design. Trained and evaluated on a newly curated and comprehensive IL dataset, our model delivers superior performance. Complementing these results, evaluations on literature-reported systems indicate that the agent can perform effective IL modification. Moving beyond offline tests, the practical efficacy was further confirmed through real-world wet-lab validation, in which the agent demonstrated exceptional generalization capabilities on challenging out-of-distribution tasks, underscoring its ability to accelerate real-world IL discovery.",
      "upvotes": 15,
      "discussionId": "691aa2536bfd5965c0fd3595",
      "ai_summary": "Axonopedia, an AI agent utilizing LLMs and a multimodal foundation model, enhances property prediction and molecular design for Ionic Liquids through hierarchical search and real-world validation.",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "multimodal domain foundation model",
        "hierarchical search architecture",
        "Ionic Liquids (ILs)",
        "property predictions",
        "molecular screening",
        "design",
        "dataset",
        "generalization capabilities",
        "wet-lab validation"
      ]
    },
    "publishedAt": "2025-11-14T07:53:57.000Z",
    "title": "AIonopedia: an LLM agent orchestrating multimodal learning for ionic liquid discovery",
    "summary": "The discovery of novel Ionic Liquids (ILs) is hindered by critical challenges in property prediction, including limited data, poor model accuracy, and fragmented workflows. Leveraging the power of Large Language Models (LLMs), we introduce AIonopedia, to the best of our knowledge, the first LLM agent for IL discovery. Powered by an LLM-augmented multimodal domain foundation model for ILs, AIonopedia enables accurate property predictions and incorporates a hierarchical search architecture for molecular screening and design. Trained and evaluated on a newly curated and comprehensive IL dataset, our model delivers superior performance. Complementing these results, evaluations on literature-reported systems indicate that the agent can perform effective IL modification. Moving beyond offline tests, the practical efficacy was further confirmed through real-world wet-lab validation, in which the agent demonstrated exceptional generalization capabilities on challenging out-of-distribution tasks, underscoring its ability to accelerate real-world IL discovery.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.11257.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66a5db1c94d2b190a23c9a46",
      "avatarUrl": "/avatars/e2caafde5b9383d63d9917d9476c9ee3.svg",
      "fullname": "Yuqi Yin",
      "name": "chitanda-eru",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.11238",
      "authors": [
        {
          "_id": "691a99126bfd5965c0fd34b6",
          "name": "Seed",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34b7",
          "name": "Baisheng Li",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34b8",
          "name": "Banggu Wu",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34b9",
          "name": "Bole Ma",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34ba",
          "name": "Bowen Xiao",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34bb",
          "name": "Chaoyi Zhang",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34bc",
          "name": "Cheng Li",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34bd",
          "name": "Chengyi Wang",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34be",
          "name": "Chenyin Xu",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34bf",
          "name": "Chi Zhang",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34c0",
          "name": "Chong Hu",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34c1",
          "name": "Daoguang Zan",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34c2",
          "name": "Defa Zhu",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34c3",
          "name": "Dongyu Xu",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34c4",
          "name": "Du Li",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34c5",
          "name": "Faming Wu",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34c6",
          "name": "Fan Xia",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34c7",
          "name": "Ge Zhang",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34c8",
          "name": "Guang Shi",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34c9",
          "name": "Haobin Chen",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34ca",
          "name": "Hongyu Zhu",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34cb",
          "name": "Hongzhi Huang",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34cc",
          "name": "Huan Zhou",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34cd",
          "name": "Huanzhang Dou",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34ce",
          "name": "Jianhui Duan",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34cf",
          "name": "Jianqiao Lu",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34d0",
          "name": "Jianyu Jiang",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34d1",
          "name": "Jiayi Xu",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34d2",
          "name": "Jiecao Chen",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34d3",
          "name": "Jin Chen",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34d4",
          "name": "Jin Ma",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34d5",
          "name": "Jing Su",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34d6",
          "name": "Jingji Chen",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34d7",
          "name": "Jun Wang",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34d8",
          "name": "Jun Yuan",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34d9",
          "name": "Juncai Liu",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34da",
          "name": "Jundong Zhou",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34db",
          "name": "Kai Hua",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34dc",
          "name": "Kai Shen",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34dd",
          "name": "Kai Xiang",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34de",
          "name": "Kaiyuan Chen",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34df",
          "name": "Kang Liu",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34e0",
          "name": "Ke Shen",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34e1",
          "name": "Liang Xiang",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34e2",
          "name": "Lin Yan",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34e3",
          "name": "Lishu Luo",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34e4",
          "name": "Mengyao Zhang",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34e5",
          "name": "Ming Ding",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34e6",
          "name": "Mofan Zhang",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34e7",
          "name": "Nianning Liang",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34e8",
          "name": "Peng Li",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34e9",
          "name": "Penghao Huang",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34ea",
          "name": "Pengpeng Mu",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34eb",
          "name": "Qi Huang",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34ec",
          "name": "Qianli Ma",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34ed",
          "name": "Qiyang Min",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34ee",
          "name": "Qiying Yu",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34ef",
          "name": "Renming Pang",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34f0",
          "name": "Ru Zhang",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34f1",
          "name": "Shen Yan",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34f2",
          "name": "Shen Yan",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34f3",
          "name": "Shixiong Zhao",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34f4",
          "name": "Shuaishuai Cao",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34f5",
          "name": "Shuang Wu",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34f6",
          "name": "Siyan Chen",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34f7",
          "name": "Siyu Li",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34f8",
          "name": "Siyuan Qiao",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34f9",
          "name": "Tao Sun",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34fa",
          "name": "Tian Xin",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34fb",
          "name": "Tiantian Fan",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34fc",
          "name": "Ting Huang",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34fd",
          "name": "Ting-Han Fan",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34fe",
          "name": "Wei Jia",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd34ff",
          "name": "Wenqiang Zhang",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd3500",
          "name": "Wenxuan Liu",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd3501",
          "name": "Xiangzhong Wu",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd3502",
          "name": "Xiaochen Zuo",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd3503",
          "name": "Xiaoying Jia",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd3504",
          "name": "Ximing Yang",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd3505",
          "name": "Xin Liu",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd3506",
          "name": "Xin Yu",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd3507",
          "name": "Xingyan Bin",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd3508",
          "name": "Xintong Hao",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd3509",
          "name": "Xiongcai Luo",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd350a",
          "name": "Xujing Li",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd350b",
          "name": "Xun Zhou",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd350c",
          "name": "Yanghua Peng",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd350d",
          "name": "Yangrui Chen",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd350e",
          "name": "Yi Lin",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd350f",
          "name": "Yichong Leng",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd3510",
          "name": "Yinghao Li",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd3511",
          "name": "Yingshuan Song",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd3512",
          "name": "Yiyuan Ma",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd3513",
          "name": "Yong Shan",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd3514",
          "name": "Yongan Xiang",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd3515",
          "name": "Yonghui Wu",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd3516",
          "name": "Yongtao Zhang",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd3517",
          "name": "Yongzhen Yao",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd3518",
          "name": "Yu Bao",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd3519",
          "name": "Yuehang Yang",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd351a",
          "name": "Yufeng Yuan",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd351b",
          "name": "Yunshui Li",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd351c",
          "name": "Yuqiao Xian",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd351d",
          "name": "Yutao Zeng",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd351e",
          "name": "Yuxuan Wang",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd351f",
          "name": "Zehua Hong",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd3520",
          "name": "Zehua Wang",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd3521",
          "name": "Zengzhi Wang",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd3522",
          "name": "Zeyu Yang",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd3523",
          "name": "Zhengqiang Yin",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd3524",
          "name": "Zhenyi Lu",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd3525",
          "name": "Zhexi Zhang",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd3526",
          "name": "Zhi Chen",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd3527",
          "name": "Zhi Zhang",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd3528",
          "name": "Zhiqi Lin",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd3529",
          "name": "Zihao Huang",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd352a",
          "name": "Zilin Xu",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd352b",
          "name": "Ziyun Wei",
          "hidden": false
        },
        {
          "_id": "691a99126bfd5965c0fd352c",
          "name": "Zuo Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-14T12:41:57.000Z",
      "submittedOnDailyAt": "2025-11-17T01:10:23.490Z",
      "title": "Virtual Width Networks",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We introduce Virtual Width Networks (VWN), a framework that delivers the benefits of wider representations without incurring the quadratic cost of increasing the hidden size. VWN decouples representational width from backbone width, expanding the embedding space while keeping backbone compute nearly constant. In our large-scale experiment, an 8-times expansion accelerates optimization by over 2 times for next-token and 3 times for next-2-token prediction. The advantage amplifies over training as both the loss gap grows and the convergence-speedup ratio increases, showing that VWN is not only token-efficient but also increasingly effective with scale. Moreover, we identify an approximately log-linear scaling relation between virtual width and loss reduction, offering an initial empirical basis and motivation for exploring virtual-width scaling as a new dimension of large-model efficiency.",
      "upvotes": 10,
      "discussionId": "691a99136bfd5965c0fd352d",
      "ai_summary": "Virtual Width Networks (VWN) enhance model efficiency by expanding representational width without increasing computational cost, accelerating optimization and improving loss reduction.",
      "ai_keywords": [
        "Virtual Width Networks",
        "VWN",
        "representational width",
        "backbone width",
        "embedding space",
        "next-token prediction",
        "next-2-token prediction",
        "token-efficient",
        "loss reduction"
      ],
      "organization": {
        "_id": "67d1140985ea0644e2f14b99",
        "name": "ByteDance-Seed",
        "fullname": "ByteDance Seed",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
      }
    },
    "publishedAt": "2025-11-14T07:41:57.000Z",
    "title": "Virtual Width Networks",
    "summary": "We introduce Virtual Width Networks (VWN), a framework that delivers the benefits of wider representations without incurring the quadratic cost of increasing the hidden size. VWN decouples representational width from backbone width, expanding the embedding space while keeping backbone compute nearly constant. In our large-scale experiment, an 8-times expansion accelerates optimization by over 2 times for next-token and 3 times for next-2-token prediction. The advantage amplifies over training as both the loss gap grows and the convergence-speedup ratio increases, showing that VWN is not only token-efficient but also increasingly effective with scale. Moreover, we identify an approximately log-linear scaling relation between virtual width and loss reduction, offering an initial empirical basis and motivation for exploring virtual-width scaling as a new dimension of large-model efficiency.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.11238.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 162
    },
    "organization": {
      "_id": "67d1140985ea0644e2f14b99",
      "name": "ByteDance-Seed",
      "fullname": "ByteDance Seed",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.11134",
      "authors": [
        {
          "_id": "691a96096bfd5965c0fd3476",
          "name": "Jingxuan Wei",
          "hidden": false
        },
        {
          "_id": "691a96096bfd5965c0fd3477",
          "name": "Caijun Jia",
          "hidden": false
        },
        {
          "_id": "691a96096bfd5965c0fd3478",
          "name": "Xi Bai",
          "hidden": false
        },
        {
          "_id": "691a96096bfd5965c0fd3479",
          "name": "Xinglong Xu",
          "hidden": false
        },
        {
          "_id": "691a96096bfd5965c0fd347a",
          "name": "Siyuan Li",
          "hidden": false
        },
        {
          "_id": "691a96096bfd5965c0fd347b",
          "name": "Linzhuang Sun",
          "hidden": false
        },
        {
          "_id": "691a96096bfd5965c0fd347c",
          "name": "Bihui Yu",
          "hidden": false
        },
        {
          "_id": "691a96096bfd5965c0fd347d",
          "name": "Conghui He",
          "hidden": false
        },
        {
          "_id": "691a96096bfd5965c0fd347e",
          "name": "Lijun Wu",
          "hidden": false
        },
        {
          "_id": "691a96096bfd5965c0fd347f",
          "name": "Cheng Tan",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/MWaH1mrhdrfkzs10c7_y1.mp4"
      ],
      "publishedAt": "2025-11-14T10:07:53.000Z",
      "submittedOnDailyAt": "2025-11-17T00:57:58.782Z",
      "title": "GGBench: A Geometric Generative Reasoning Benchmark for Unified Multimodal Models",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "The advent of Unified Multimodal Models (UMMs) signals a paradigm shift in artificial intelligence, moving from passive perception to active, cross-modal generation. Despite their unprecedented ability to synthesize information, a critical gap persists in evaluation: existing benchmarks primarily assess discriminative understanding or unconstrained image generation separately, failing to measure the integrated cognitive process of generative reasoning. To bridge this gap, we propose that geometric construction provides an ideal testbed as it inherently demands a fusion of language comprehension and precise visual generation. We introduce GGBench, a benchmark designed specifically to evaluate geometric generative reasoning. It provides a comprehensive framework for systematically diagnosing a model's ability to not only understand and reason but to actively construct a solution, thereby setting a more rigorous standard for the next generation of intelligent systems. Project website: https://opendatalab-raiser.github.io/GGBench/.",
      "upvotes": 8,
      "discussionId": "691a960a6bfd5965c0fd3480",
      "ai_summary": "GGBench is introduced to evaluate geometric generative reasoning, addressing the gap in assessing integrated cognitive processes in multimodal models.",
      "ai_keywords": [
        "Unified Multimodal Models",
        "generative reasoning",
        "evaluation",
        "discriminative understanding",
        "unconstrained image generation",
        "geometric construction",
        "language comprehension",
        "visual generation",
        "benchmark"
      ]
    },
    "publishedAt": "2025-11-14T05:07:53.000Z",
    "title": "GGBench: A Geometric Generative Reasoning Benchmark for Unified Multimodal Models",
    "summary": "The advent of Unified Multimodal Models (UMMs) signals a paradigm shift in artificial intelligence, moving from passive perception to active, cross-modal generation. Despite their unprecedented ability to synthesize information, a critical gap persists in evaluation: existing benchmarks primarily assess discriminative understanding or unconstrained image generation separately, failing to measure the integrated cognitive process of generative reasoning. To bridge this gap, we propose that geometric construction provides an ideal testbed as it inherently demands a fusion of language comprehension and precise visual generation. We introduce GGBench, a benchmark designed specifically to evaluate geometric generative reasoning. It provides a comprehensive framework for systematically diagnosing a model's ability to not only understand and reason but to actively construct a solution, thereby setting a more rigorous standard for the next generation of intelligent systems. Project website: https://opendatalab-raiser.github.io/GGBench/.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/MWaH1mrhdrfkzs10c7_y1.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.11134.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 162
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.09146",
      "authors": [
        {
          "_id": "6917090db63bfc66e049897a",
          "name": "Jing Xiong",
          "hidden": false
        },
        {
          "_id": "6917090db63bfc66e049897b",
          "name": "Liyang Fan",
          "hidden": false
        },
        {
          "_id": "6917090db63bfc66e049897c",
          "name": "Hui Shen",
          "hidden": false
        },
        {
          "_id": "6917090db63bfc66e049897d",
          "name": "Zunhai Su",
          "hidden": false
        },
        {
          "_id": "6917090db63bfc66e049897e",
          "name": "Min Yang",
          "hidden": false
        },
        {
          "_id": "6917090db63bfc66e049897f",
          "name": "Lingpeng Kong",
          "hidden": false
        },
        {
          "_id": "6917090db63bfc66e0498980",
          "name": "Ngai Wong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-12T09:32:35.000Z",
      "submittedOnDailyAt": "2025-11-17T04:37:56.746Z",
      "title": "DoPE: Denoising Rotary Position Embedding",
      "submittedOnDailyBy": {
        "_id": "60851545a5da133ac6c38686",
        "avatarUrl": "/avatars/d385fcc513acef80a3b711aa92d898e5.svg",
        "isPro": false,
        "fullname": "Jing Xiong",
        "user": "menik1126",
        "type": "user"
      },
      "summary": "Rotary Position Embedding (RoPE) in Transformer models has inherent limits that weaken length extrapolation. We reinterpret the attention map with positional encoding as a noisy feature map, and propose Denoising Positional Encoding (DoPE), a training-free method based on truncated matrix entropy to detect outlier frequency bands in the feature map. Leveraging the noise characteristics of the feature map, we further reparameterize it with a parameter-free Gaussian distribution to achieve robust extrapolation. Our method theoretically reveals the underlying cause of the attention sink phenomenon and its connection to truncated matrix entropy. Experiments on needle-in-a-haystack and many-shot in-context learning tasks demonstrate that DoPE significantly improves retrieval accuracy and reasoning stability across extended contexts (up to 64K tokens). The results show that the denoising strategy for positional embeddings effectively mitigates attention sinks and restores balanced attention patterns, providing a simple yet powerful solution for improving length generalization. Our project page is Project: https://The-physical-picture-of-LLMs.github.io",
      "upvotes": 8,
      "discussionId": "6917090db63bfc66e04989a3",
      "projectPage": "https://The-physical-picture-of-LLMs.github.io",
      "ai_summary": "Denoising Positional Encoding (DoPE) enhances length generalization in Transformer models by detecting and mitigating noisy frequency bands in positional embeddings, improving retrieval accuracy and reasoning stability.",
      "ai_keywords": [
        "Rotary Position Embedding (RoPE)",
        "Transformer models",
        "attention map",
        "positional encoding",
        "Denoising Positional Encoding (DoPE)",
        "truncated matrix entropy",
        "Gaussian distribution",
        "attention sink phenomenon",
        "needle-in-a-haystack",
        "many-shot in-context learning",
        "retrieval accuracy",
        "reasoning stability",
        "length generalization"
      ]
    },
    "publishedAt": "2025-11-12T04:32:35.000Z",
    "title": "DoPE: Denoising Rotary Position Embedding",
    "summary": "Rotary Position Embedding (RoPE) in Transformer models has inherent limits that weaken length extrapolation. We reinterpret the attention map with positional encoding as a noisy feature map, and propose Denoising Positional Encoding (DoPE), a training-free method based on truncated matrix entropy to detect outlier frequency bands in the feature map. Leveraging the noise characteristics of the feature map, we further reparameterize it with a parameter-free Gaussian distribution to achieve robust extrapolation. Our method theoretically reveals the underlying cause of the attention sink phenomenon and its connection to truncated matrix entropy. Experiments on needle-in-a-haystack and many-shot in-context learning tasks demonstrate that DoPE significantly improves retrieval accuracy and reasoning stability across extended contexts (up to 64K tokens). The results show that the denoising strategy for positional embeddings effectively mitigates attention sinks and restores balanced attention patterns, providing a simple yet powerful solution for improving length generalization. Our project page is Project: https://The-physical-picture-of-LLMs.github.io",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.09146.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60851545a5da133ac6c38686",
      "avatarUrl": "/avatars/d385fcc513acef80a3b711aa92d898e5.svg",
      "fullname": "Jing Xiong",
      "name": "menik1126",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.08195",
      "authors": [
        {
          "_id": "69170901b63bfc66e04988d9",
          "name": "Zhen Yang",
          "hidden": false
        },
        {
          "_id": "69170901b63bfc66e04988da",
          "name": "Wenyi Hong",
          "hidden": false
        },
        {
          "_id": "69170901b63bfc66e04988db",
          "name": "Mingde Xu",
          "hidden": false
        },
        {
          "_id": "69170901b63bfc66e04988dc",
          "name": "Xinyue Fan",
          "hidden": false
        },
        {
          "_id": "69170901b63bfc66e04988dd",
          "name": "Weihan Wang",
          "hidden": false
        },
        {
          "_id": "69170901b63bfc66e04988de",
          "name": "Jiele Cheng",
          "hidden": false
        },
        {
          "_id": "69170901b63bfc66e04988df",
          "name": "Xiaotao Gu",
          "hidden": false
        },
        {
          "_id": "69170901b63bfc66e04988e0",
          "name": "Jie Tang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62ecd24cb8764c7738ef2793/dw6-SAx5OVrdfn0_0YutW.jpeg",
        "https://cdn-uploads.huggingface.co/production/uploads/62ecd24cb8764c7738ef2793/fHbYJ62-hSRF_OHc03iIk.jpeg"
      ],
      "publishedAt": "2025-11-11T13:00:09.000Z",
      "submittedOnDailyAt": "2025-11-17T02:18:16.514Z",
      "title": "UI2Code^N: A Visual Language Model for Test-Time Scalable Interactive UI-to-Code Generation",
      "submittedOnDailyBy": {
        "_id": "62ecd24cb8764c7738ef2793",
        "avatarUrl": "/avatars/c1b80b5c55f9d652c1aaac7919e1fa32.svg",
        "isPro": false,
        "fullname": "Wenyi Hong",
        "user": "wenyi",
        "type": "user"
      },
      "summary": "User interface (UI) programming is a core yet highly complex part of modern software development. Recent advances in visual language models (VLMs) highlight the potential of automatic UI coding, but current approaches face two key limitations: multimodal coding capabilities remain underdeveloped, and single-turn paradigms make little use of iterative visual feedback. We address these challenges with an interactive UI-to-code paradigm that better reflects real-world workflows and raises the upper bound of achievable performance. Under this paradigm, we present UI2Code^N, a visual language model trained through staged pretraining, fine-tuning, and reinforcement learning to achieve foundational improvements in multimodal coding. The model unifies three key capabilities: UI-to-code generation, UI editing, and UI polishing. We further explore test-time scaling for interactive generation, enabling systematic use of multi-turn feedback. Experiments on UI-to-code and UI polishing benchmarks show that UI2Code^N establishes a new state of the art among open-source models and achieves performance comparable to leading closed-source models such as Claude-4-Sonnet and GPT-5. Our code and models are available at https://github.com/zai-org/UI2Code_N.",
      "upvotes": 8,
      "discussionId": "69170901b63bfc66e04988e1",
      "projectPage": "https://zheny2751-dotcom.github.io/ui2code-n.github.io/",
      "githubRepo": "https://github.com/zai-org/UI2Code_N",
      "ai_summary": "UI2Code$^\\text{N}$, a visual language model enhanced through staged pretraining, fine-tuning, and reinforcement learning, achieves superior performance in UI-to-code generation, editing, and polishing with iterative feedback.",
      "ai_keywords": [
        "visual language models",
        "VLMs",
        "UI-to-code",
        "staged pretraining",
        "fine-tuning",
        "reinforcement learning",
        "UI-to-code generation",
        "UI editing",
        "UI polishing",
        "multi-turn feedback",
        "UI-to-code benchmarks",
        "UI polishing benchmarks"
      ],
      "githubStars": 11
    },
    "publishedAt": "2025-11-11T08:00:09.000Z",
    "title": "UI2Code^N: A Visual Language Model for Test-Time Scalable Interactive UI-to-Code Generation",
    "summary": "User interface (UI) programming is a core yet highly complex part of modern software development. Recent advances in visual language models (VLMs) highlight the potential of automatic UI coding, but current approaches face two key limitations: multimodal coding capabilities remain underdeveloped, and single-turn paradigms make little use of iterative visual feedback. We address these challenges with an interactive UI-to-code paradigm that better reflects real-world workflows and raises the upper bound of achievable performance. Under this paradigm, we present UI2Code^N, a visual language model trained through staged pretraining, fine-tuning, and reinforcement learning to achieve foundational improvements in multimodal coding. The model unifies three key capabilities: UI-to-code generation, UI editing, and UI polishing. We further explore test-time scaling for interactive generation, enabling systematic use of multi-turn feedback. Experiments on UI-to-code and UI polishing benchmarks show that UI2Code^N establishes a new state of the art among open-source models and achieves performance comparable to leading closed-source models such as Claude-4-Sonnet and GPT-5. Our code and models are available at https://github.com/zai-org/UI2Code_N.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62ecd24cb8764c7738ef2793/dw6-SAx5OVrdfn0_0YutW.jpeg",
      "https://cdn-uploads.huggingface.co/production/uploads/62ecd24cb8764c7738ef2793/fHbYJ62-hSRF_OHc03iIk.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.08195.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62ecd24cb8764c7738ef2793",
      "avatarUrl": "/avatars/c1b80b5c55f9d652c1aaac7919e1fa32.svg",
      "fullname": "Wenyi Hong",
      "name": "wenyi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.11373",
      "authors": [
        {
          "_id": "691a81b76bfd5965c0fd3430",
          "name": "Shulin Liu",
          "hidden": false
        },
        {
          "_id": "691a81b76bfd5965c0fd3431",
          "name": "Dong Du",
          "hidden": false
        },
        {
          "_id": "691a81b76bfd5965c0fd3432",
          "name": "Tao Yang",
          "hidden": false
        },
        {
          "_id": "691a81b76bfd5965c0fd3433",
          "name": "Yang Li",
          "hidden": false
        },
        {
          "_id": "691a81b76bfd5965c0fd3434",
          "name": "Boyu Qiu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-14T14:52:34.000Z",
      "submittedOnDailyAt": "2025-11-17T01:31:35.105Z",
      "title": "MarsRL: Advancing Multi-Agent Reasoning System via Reinforcement Learning with Agentic Pipeline Parallelism",
      "submittedOnDailyBy": {
        "_id": "67cffe3d275d90f8097ae10a",
        "avatarUrl": "/avatars/eb4d06abb06c677e4d1be77685fc23f6.svg",
        "isPro": false,
        "fullname": "liu",
        "user": "forestliutc",
        "type": "user"
      },
      "summary": "Recent progress in large language models (LLMs) has been propelled by reinforcement learning with verifiable rewards (RLVR) and test-time scaling. However, the limited output length of LLMs constrains the depth of reasoning attainable in a single inference process. Multi-agent reasoning systems offer a promising alternative by employing multiple agents including Solver, Verifier, and Corrector, to iteratively refine solutions. While effective in closed-source models like Gemini 2.5 Pro, they struggle to generalize to open-source models due to insufficient critic and correction capabilities. To address this, we propose MarsRL, a novel reinforcement learning framework with agentic pipeline parallelism, designed to jointly optimize all agents in the system. MarsRL introduces agent-specific reward mechanisms to mitigate reward noise and employs pipeline-inspired training to enhance efficiency in handling long trajectories. Applied to Qwen3-30B-A3B-Thinking-2507, MarsRL improves AIME2025 accuracy from 86.5% to 93.3% and BeyondAIME from 64.9% to 73.8%, even surpassing Qwen3-235B-A22B-Thinking-2507. These findings highlight the potential of MarsRL to advance multi-agent reasoning systems and broaden their applicability across diverse reasoning tasks.",
      "upvotes": 4,
      "discussionId": "691a81b76bfd5965c0fd3435",
      "githubRepo": "https://github.com/liushulinle/MarsRL",
      "ai_summary": "MarsRL enhances multi-agent reasoning systems by optimizing all agents jointly, improving accuracy in complex reasoning tasks.",
      "ai_keywords": [
        "reinforcement learning with verifiable rewards",
        "test-time scaling",
        "multi-agent reasoning systems",
        "Solver",
        "Verifier",
        "Corrector",
        "agentic pipeline parallelism",
        "agent-specific reward mechanisms",
        "pipeline-inspired training",
        "AIME2025",
        "BeyondAIME"
      ],
      "githubStars": 0,
      "organization": {
        "_id": "66543b6e420092799d2f625c",
        "name": "tencent",
        "fullname": "Tencent",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
      }
    },
    "publishedAt": "2025-11-14T09:52:34.000Z",
    "title": "MarsRL: Advancing Multi-Agent Reasoning System via Reinforcement Learning with Agentic Pipeline Parallelism",
    "summary": "Recent progress in large language models (LLMs) has been propelled by reinforcement learning with verifiable rewards (RLVR) and test-time scaling. However, the limited output length of LLMs constrains the depth of reasoning attainable in a single inference process. Multi-agent reasoning systems offer a promising alternative by employing multiple agents including Solver, Verifier, and Corrector, to iteratively refine solutions. While effective in closed-source models like Gemini 2.5 Pro, they struggle to generalize to open-source models due to insufficient critic and correction capabilities. To address this, we propose MarsRL, a novel reinforcement learning framework with agentic pipeline parallelism, designed to jointly optimize all agents in the system. MarsRL introduces agent-specific reward mechanisms to mitigate reward noise and employs pipeline-inspired training to enhance efficiency in handling long trajectories. Applied to Qwen3-30B-A3B-Thinking-2507, MarsRL improves AIME2025 accuracy from 86.5% to 93.3% and BeyondAIME from 64.9% to 73.8%, even surpassing Qwen3-235B-A22B-Thinking-2507. These findings highlight the potential of MarsRL to advance multi-agent reasoning systems and broaden their applicability across diverse reasoning tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.11373.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67cffe3d275d90f8097ae10a",
      "avatarUrl": "/avatars/eb4d06abb06c677e4d1be77685fc23f6.svg",
      "fullname": "liu",
      "name": "forestliutc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "organization": {
      "_id": "66543b6e420092799d2f625c",
      "name": "tencent",
      "fullname": "Tencent",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.10984",
      "authors": [
        {
          "_id": "691a98606bfd5965c0fd349e",
          "name": "Xiying Zhao",
          "hidden": false
        },
        {
          "_id": "691a98606bfd5965c0fd349f",
          "name": "Zhoufutu Wen",
          "hidden": false
        },
        {
          "_id": "691a98606bfd5965c0fd34a0",
          "name": "Zhixuan Chen",
          "hidden": false
        },
        {
          "_id": "691a98606bfd5965c0fd34a1",
          "name": "Jingzhe Ding",
          "hidden": false
        },
        {
          "_id": "691a98606bfd5965c0fd34a2",
          "name": "Jianpeng Jiao",
          "hidden": false
        },
        {
          "_id": "691a98606bfd5965c0fd34a3",
          "name": "Shuai Li",
          "hidden": false
        },
        {
          "_id": "691a98606bfd5965c0fd34a4",
          "name": "Xi Li",
          "hidden": false
        },
        {
          "_id": "691a98606bfd5965c0fd34a5",
          "name": "Danni Liang",
          "hidden": false
        },
        {
          "_id": "691a98606bfd5965c0fd34a6",
          "name": "Shengda Long",
          "hidden": false
        },
        {
          "_id": "691a98606bfd5965c0fd34a7",
          "name": "Qianqian Liu",
          "hidden": false
        },
        {
          "_id": "691a98606bfd5965c0fd34a8",
          "name": "Xianbo Wu",
          "hidden": false
        },
        {
          "_id": "691a98606bfd5965c0fd34a9",
          "name": "Hongwan Gao",
          "hidden": false
        },
        {
          "_id": "691a98606bfd5965c0fd34aa",
          "name": "Xiang Gao",
          "hidden": false
        },
        {
          "_id": "691a98606bfd5965c0fd34ab",
          "name": "Liang Hu",
          "hidden": false
        },
        {
          "_id": "691a98606bfd5965c0fd34ac",
          "name": "Jiashuo Liu",
          "hidden": false
        },
        {
          "_id": "691a98606bfd5965c0fd34ad",
          "name": "Mengyun Liu",
          "hidden": false
        },
        {
          "_id": "691a98606bfd5965c0fd34ae",
          "name": "Weiran Shi",
          "hidden": false
        },
        {
          "_id": "691a98606bfd5965c0fd34af",
          "name": "Chenghao Yang",
          "hidden": false
        },
        {
          "_id": "691a98606bfd5965c0fd34b0",
          "name": "Qianyu Yang",
          "hidden": false
        },
        {
          "_id": "691a98606bfd5965c0fd34b1",
          "name": "Xuanliang Zhang",
          "hidden": false
        },
        {
          "_id": "691a98606bfd5965c0fd34b2",
          "name": "Ge Zhang",
          "hidden": false
        },
        {
          "_id": "691a98606bfd5965c0fd34b3",
          "name": "Wenhao Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-14T06:09:37.000Z",
      "submittedOnDailyAt": "2025-11-17T01:07:30.798Z",
      "title": "DiscoX: Benchmarking Discourse-Level Translation task in Expert Domains",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "The evaluation of discourse-level translation in expert domains remains inadequate, despite its centrality to knowledge dissemination and cross-lingual scholarly communication. While these translations demand discourse-level coherence and strict terminological precision, current evaluation methods predominantly focus on segment-level accuracy and fluency. To address this limitation, we introduce DiscoX, a new benchmark for discourse-level and expert-level Chinese-English translation. It comprises 200 professionally-curated texts from 7 domains, with an average length exceeding 1700 tokens. To evaluate performance on DiscoX, we also develop Metric-S, a reference-free system that provides fine-grained automatic assessments across accuracy, fluency, and appropriateness. Metric-S demonstrates strong consistency with human judgments, significantly outperforming existing metrics. Our experiments reveal a remarkable performance gap: even the most advanced LLMs still trail human experts on these tasks. This finding validates the difficulty of DiscoX and underscores the challenges that remain in achieving professional-grade machine translation. The proposed benchmark and evaluation system provide a robust framework for more rigorous evaluation, facilitating future advancements in LLM-based translation.",
      "upvotes": 2,
      "discussionId": "691a98606bfd5965c0fd34b4",
      "ai_summary": "A new benchmark DiscoX and evaluation system Metric-S are introduced to assess discourse-level and expert-level Chinese-English translation, highlighting the challenges in achieving professional-grade machine translation.",
      "ai_keywords": [
        "discourse-level translation",
        "DiscoX",
        "Metric-S",
        "reference-free system",
        "accuracy",
        "fluency",
        "appropriateness",
        "human judgments",
        "LLMs",
        "professional-grade machine translation"
      ],
      "organization": {
        "_id": "67d1140985ea0644e2f14b99",
        "name": "ByteDance-Seed",
        "fullname": "ByteDance Seed",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
      }
    },
    "publishedAt": "2025-11-14T01:09:37.000Z",
    "title": "DiscoX: Benchmarking Discourse-Level Translation task in Expert Domains",
    "summary": "The evaluation of discourse-level translation in expert domains remains inadequate, despite its centrality to knowledge dissemination and cross-lingual scholarly communication. While these translations demand discourse-level coherence and strict terminological precision, current evaluation methods predominantly focus on segment-level accuracy and fluency. To address this limitation, we introduce DiscoX, a new benchmark for discourse-level and expert-level Chinese-English translation. It comprises 200 professionally-curated texts from 7 domains, with an average length exceeding 1700 tokens. To evaluate performance on DiscoX, we also develop Metric-S, a reference-free system that provides fine-grained automatic assessments across accuracy, fluency, and appropriateness. Metric-S demonstrates strong consistency with human judgments, significantly outperforming existing metrics. Our experiments reveal a remarkable performance gap: even the most advanced LLMs still trail human experts on these tasks. This finding validates the difficulty of DiscoX and underscores the challenges that remain in achieving professional-grade machine translation. The proposed benchmark and evaluation system provide a robust framework for more rigorous evaluation, facilitating future advancements in LLM-based translation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.10984.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 162
    },
    "organization": {
      "_id": "67d1140985ea0644e2f14b99",
      "name": "ByteDance-Seed",
      "fullname": "ByteDance Seed",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.11002",
      "authors": [
        {
          "_id": "691a8d6d6bfd5965c0fd3460",
          "name": "Zongyang Qiu",
          "hidden": false
        },
        {
          "_id": "691a8d6d6bfd5965c0fd3461",
          "name": "Bingyuan Wang",
          "hidden": false
        },
        {
          "_id": "691a8d6d6bfd5965c0fd3462",
          "name": "Xingbei Chen",
          "hidden": false
        },
        {
          "_id": "691a8d6d6bfd5965c0fd3463",
          "name": "Yingqing He",
          "hidden": false
        },
        {
          "_id": "691a8d6d6bfd5965c0fd3464",
          "name": "Zeyu Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-14T06:44:21.000Z",
      "submittedOnDailyAt": "2025-11-17T00:20:29.652Z",
      "title": "EmoVid: A Multimodal Emotion Video Dataset for Emotion-Centric Video Understanding and Generation",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Emotion plays a pivotal role in video-based expression, but existing video generation systems predominantly focus on low-level visual metrics while neglecting affective dimensions. Although emotion analysis has made progress in the visual domain, the video community lacks dedicated resources to bridge emotion understanding with generative tasks, particularly for stylized and non-realistic contexts. To address this gap, we introduce EmoVid, the first multimodal, emotion-annotated video dataset specifically designed for creative media, which includes cartoon animations, movie clips, and animated stickers. Each video is annotated with emotion labels, visual attributes (brightness, colorfulness, hue), and text captions. Through systematic analysis, we uncover spatial and temporal patterns linking visual features to emotional perceptions across diverse video forms. Building on these insights, we develop an emotion-conditioned video generation technique by fine-tuning the Wan2.1 model. The results show a significant improvement in both quantitative metrics and the visual quality of generated videos for text-to-video and image-to-video tasks. EmoVid establishes a new benchmark for affective video computing. Our work not only offers valuable insights into visual emotion analysis in artistically styled videos, but also provides practical methods for enhancing emotional expression in video generation.",
      "upvotes": 1,
      "discussionId": "691a8d6e6bfd5965c0fd3465",
      "ai_summary": "EmoVid, a multimodal emotion-annotated video dataset, bridges emotion understanding with video generation, leading to improved emotional expression in generated videos.",
      "ai_keywords": [
        "emotion analysis",
        "multimodal",
        "emotion-annotated",
        "text-to-video",
        "image-to-video",
        "Wan2.1 model",
        "affective video computing"
      ]
    },
    "publishedAt": "2025-11-14T01:44:21.000Z",
    "title": "EmoVid: A Multimodal Emotion Video Dataset for Emotion-Centric Video Understanding and Generation",
    "summary": "Emotion plays a pivotal role in video-based expression, but existing video generation systems predominantly focus on low-level visual metrics while neglecting affective dimensions. Although emotion analysis has made progress in the visual domain, the video community lacks dedicated resources to bridge emotion understanding with generative tasks, particularly for stylized and non-realistic contexts. To address this gap, we introduce EmoVid, the first multimodal, emotion-annotated video dataset specifically designed for creative media, which includes cartoon animations, movie clips, and animated stickers. Each video is annotated with emotion labels, visual attributes (brightness, colorfulness, hue), and text captions. Through systematic analysis, we uncover spatial and temporal patterns linking visual features to emotional perceptions across diverse video forms. Building on these insights, we develop an emotion-conditioned video generation technique by fine-tuning the Wan2.1 model. The results show a significant improvement in both quantitative metrics and the visual quality of generated videos for text-to-video and image-to-video tasks. EmoVid establishes a new benchmark for affective video computing. Our work not only offers valuable insights into visual emotion analysis in artistically styled videos, but also provides practical methods for enhancing emotional expression in video generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.11002.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 162
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.07403",
      "authors": [
        {
          "_id": "69140683ac231a5726572002",
          "user": {
            "_id": "62f5c24eea5bd6b1abc8e151",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1660273191881-noauth.jpeg",
            "isPro": false,
            "fullname": "Hunar Batra",
            "user": "hunarbatra",
            "type": "user"
          },
          "name": "Hunar Batra",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-11-12T12:18:58.669Z",
          "hidden": false
        },
        {
          "_id": "69140683ac231a5726572003",
          "name": "Haoqin Tu",
          "hidden": false
        },
        {
          "_id": "69140683ac231a5726572004",
          "name": "Hardy Chen",
          "hidden": false
        },
        {
          "_id": "69140683ac231a5726572005",
          "name": "Yuanze Lin",
          "hidden": false
        },
        {
          "_id": "69140683ac231a5726572006",
          "name": "Cihang Xie",
          "hidden": false
        },
        {
          "_id": "69140683ac231a5726572007",
          "name": "Ronald Clark",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-10T18:52:47.000Z",
      "submittedOnDailyAt": "2025-11-17T04:00:22.549Z",
      "title": "SpatialThinker: Reinforcing 3D Reasoning in Multimodal LLMs via Spatial Rewards",
      "submittedOnDailyBy": {
        "_id": "62f5c24eea5bd6b1abc8e151",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1660273191881-noauth.jpeg",
        "isPro": false,
        "fullname": "Hunar Batra",
        "user": "hunarbatra",
        "type": "user"
      },
      "summary": "Multimodal large language models (MLLMs) have achieved remarkable progress in vision-language tasks, but they continue to struggle with spatial understanding. Existing spatial MLLMs often rely on explicit 3D inputs or architecture-specific modifications, and remain constrained by large-scale datasets or sparse supervision. To address these limitations, we introduce SpatialThinker, a 3D-aware MLLM trained with RL to integrate structured spatial grounding with multi-step reasoning. The model simulates human-like spatial perception by constructing a scene graph of task-relevant objects and spatial relations, and reasoning towards an answer via dense spatial rewards. SpatialThinker consists of two key contributions: (1) a data synthesis pipeline that generates STVQA-7K, a high-quality spatial VQA dataset, and (2) online RL with a multi-objective dense spatial reward enforcing spatial grounding. SpatialThinker-7B outperforms supervised fine-tuning and the sparse RL baseline on spatial understanding and real-world VQA benchmarks, nearly doubling the base-model gain compared to sparse RL, and surpassing GPT-4o. These results showcase the effectiveness of combining spatial supervision with reward-aligned reasoning in enabling robust 3D spatial understanding with limited data and advancing MLLMs towards human-level visual reasoning.",
      "upvotes": 1,
      "discussionId": "69140684ac231a5726572008",
      "ai_summary": "SpatialThinker, a 3D-aware MLLM trained with RL, enhances spatial understanding by integrating structured spatial grounding and multi-step reasoning, outperforming existing models on spatial VQA and real-world benchmarks.",
      "ai_keywords": [
        "MLLMs",
        "spatial understanding",
        "3D inputs",
        "architecture-specific modifications",
        "large-scale datasets",
        "sparse supervision",
        "SpatialThinker",
        "scene graph",
        "spatial rewards",
        "data synthesis pipeline",
        "STVQA-7K",
        "online RL",
        "multi-objective dense spatial reward",
        "spatial grounding",
        "reward-aligned reasoning",
        "visual reasoning"
      ],
      "organization": {
        "_id": "690ddf6f05d3fca3614552a7",
        "name": "OX-PIXL",
        "fullname": "Perceptual Intelligence and Extended Reality Lab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6305ee63d70693fdf1c7dbb8/6pCxseWIvO9rzU5h5nFPr.png"
      }
    },
    "publishedAt": "2025-11-10T13:52:47.000Z",
    "title": "SpatialThinker: Reinforcing 3D Reasoning in Multimodal LLMs via Spatial Rewards",
    "summary": "Multimodal large language models (MLLMs) have achieved remarkable progress in vision-language tasks, but they continue to struggle with spatial understanding. Existing spatial MLLMs often rely on explicit 3D inputs or architecture-specific modifications, and remain constrained by large-scale datasets or sparse supervision. To address these limitations, we introduce SpatialThinker, a 3D-aware MLLM trained with RL to integrate structured spatial grounding with multi-step reasoning. The model simulates human-like spatial perception by constructing a scene graph of task-relevant objects and spatial relations, and reasoning towards an answer via dense spatial rewards. SpatialThinker consists of two key contributions: (1) a data synthesis pipeline that generates STVQA-7K, a high-quality spatial VQA dataset, and (2) online RL with a multi-objective dense spatial reward enforcing spatial grounding. SpatialThinker-7B outperforms supervised fine-tuning and the sparse RL baseline on spatial understanding and real-world VQA benchmarks, nearly doubling the base-model gain compared to sparse RL, and surpassing GPT-4o. These results showcase the effectiveness of combining spatial supervision with reward-aligned reasoning in enabling robust 3D spatial understanding with limited data and advancing MLLMs towards human-level visual reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.07403.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62f5c24eea5bd6b1abc8e151",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1660273191881-noauth.jpeg",
      "fullname": "Hunar Batra",
      "name": "hunarbatra",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "organization": {
      "_id": "690ddf6f05d3fca3614552a7",
      "name": "OX-PIXL",
      "fullname": "Perceptual Intelligence and Extended Reality Lab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6305ee63d70693fdf1c7dbb8/6pCxseWIvO9rzU5h5nFPr.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2511.11168",
      "authors": [
        {
          "_id": "691a97986bfd5965c0fd3489",
          "name": "Hangyu Li",
          "hidden": false
        },
        {
          "_id": "691a97986bfd5965c0fd348a",
          "name": "Bofeng Cao",
          "hidden": false
        },
        {
          "_id": "691a97986bfd5965c0fd348b",
          "name": "Zhaohui Liang",
          "hidden": false
        },
        {
          "_id": "691a97986bfd5965c0fd348c",
          "name": "Wuzhen Li",
          "hidden": false
        },
        {
          "_id": "691a97986bfd5965c0fd348d",
          "name": "Juyoung Oh",
          "hidden": false
        },
        {
          "_id": "691a97986bfd5965c0fd348e",
          "name": "Yuxuan Chen",
          "hidden": false
        },
        {
          "_id": "691a97986bfd5965c0fd348f",
          "name": "Shixiao Liang",
          "hidden": false
        },
        {
          "_id": "691a97986bfd5965c0fd3490",
          "name": "Hang Zhou",
          "hidden": false
        },
        {
          "_id": "691a97986bfd5965c0fd3491",
          "name": "Chengyuan Ma",
          "hidden": false
        },
        {
          "_id": "691a97986bfd5965c0fd3492",
          "name": "Jiaxi Liu",
          "hidden": false
        },
        {
          "_id": "691a97986bfd5965c0fd3493",
          "name": "Zheng Li",
          "hidden": false
        },
        {
          "_id": "691a97986bfd5965c0fd3494",
          "name": "Peng Zhang",
          "hidden": false
        },
        {
          "_id": "691a97986bfd5965c0fd3495",
          "name": "KeKe Long",
          "hidden": false
        },
        {
          "_id": "691a97986bfd5965c0fd3496",
          "name": "Maolin Liu",
          "hidden": false
        },
        {
          "_id": "691a97986bfd5965c0fd3497",
          "name": "Jackson Jiang",
          "hidden": false
        },
        {
          "_id": "691a97986bfd5965c0fd3498",
          "name": "Chunlei Yu",
          "hidden": false
        },
        {
          "_id": "691a97986bfd5965c0fd3499",
          "name": "Shengxiang Liu",
          "hidden": false
        },
        {
          "_id": "691a97986bfd5965c0fd349a",
          "name": "Hongkai Yu",
          "hidden": false
        },
        {
          "_id": "691a97986bfd5965c0fd349b",
          "name": "Xiaopeng Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-14T11:07:04.000Z",
      "submittedOnDailyAt": "2025-11-17T01:04:01.853Z",
      "title": "CATS-V2V: A Real-World Vehicle-to-Vehicle Cooperative Perception Dataset with Complex Adverse Traffic Scenarios",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Vehicle-to-Vehicle (V2V) cooperative perception has great potential to enhance autonomous driving performance by overcoming perception limitations in complex adverse traffic scenarios (CATS). Meanwhile, data serves as the fundamental infrastructure for modern autonomous driving AI. However, due to stringent data collection requirements, existing datasets focus primarily on ordinary traffic scenarios, constraining the benefits of cooperative perception. To address this challenge, we introduce CATS-V2V, the first-of-its-kind real-world dataset for V2V cooperative perception under complex adverse traffic scenarios. The dataset was collected by two hardware time-synchronized vehicles, covering 10 weather and lighting conditions across 10 diverse locations. The 100-clip dataset includes 60K frames of 10 Hz LiDAR point clouds and 1.26M multi-view 30 Hz camera images, along with 750K anonymized yet high-precision RTK-fixed GNSS and IMU records. Correspondingly, we provide time-consistent 3D bounding box annotations for objects, as well as static scenes to construct a 4D BEV representation. On this basis, we propose a target-based temporal alignment method, ensuring that all objects are precisely aligned across all sensor modalities. We hope that CATS-V2V, the largest-scale, most supportive, and highest-quality dataset of its kind to date, will benefit the autonomous driving community in related tasks.",
      "upvotes": 0,
      "discussionId": "691a97986bfd5965c0fd349c",
      "ai_summary": "CATS-V2V is a new real-world dataset for V2V cooperative perception in complex adverse traffic scenarios, providing comprehensive sensor data and precise temporal alignment.",
      "ai_keywords": [
        "LiDAR point clouds",
        "multi-view camera images",
        "RTK-fixed GNSS",
        "IMU records",
        "3D bounding box annotations",
        "4D BEV representation",
        "target-based temporal alignment"
      ]
    },
    "publishedAt": "2025-11-14T06:07:04.000Z",
    "title": "CATS-V2V: A Real-World Vehicle-to-Vehicle Cooperative Perception Dataset with Complex Adverse Traffic Scenarios",
    "summary": "Vehicle-to-Vehicle (V2V) cooperative perception has great potential to enhance autonomous driving performance by overcoming perception limitations in complex adverse traffic scenarios (CATS). Meanwhile, data serves as the fundamental infrastructure for modern autonomous driving AI. However, due to stringent data collection requirements, existing datasets focus primarily on ordinary traffic scenarios, constraining the benefits of cooperative perception. To address this challenge, we introduce CATS-V2V, the first-of-its-kind real-world dataset for V2V cooperative perception under complex adverse traffic scenarios. The dataset was collected by two hardware time-synchronized vehicles, covering 10 weather and lighting conditions across 10 diverse locations. The 100-clip dataset includes 60K frames of 10 Hz LiDAR point clouds and 1.26M multi-view 30 Hz camera images, along with 750K anonymized yet high-precision RTK-fixed GNSS and IMU records. Correspondingly, we provide time-consistent 3D bounding box annotations for objects, as well as static scenes to construct a 4D BEV representation. On this basis, we propose a target-based temporal alignment method, ensuring that all objects are precisely aligned across all sensor modalities. We hope that CATS-V2V, the largest-scale, most supportive, and highest-quality dataset of its kind to date, will benefit the autonomous driving community in related tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.11168.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 162
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.07448",
      "authors": [
        {
          "_id": "691ac9346bfd5965c0fd35dc",
          "name": "Fatemeh Shahhosseini",
          "hidden": false
        },
        {
          "_id": "691ac9346bfd5965c0fd35dd",
          "name": "Arash Marioriyad",
          "hidden": false
        },
        {
          "_id": "691ac9346bfd5965c0fd35de",
          "name": "Ali Momen",
          "hidden": false
        },
        {
          "_id": "691ac9346bfd5965c0fd35df",
          "name": "Mahdieh Soleymani Baghshah",
          "hidden": false
        },
        {
          "_id": "691ac9346bfd5965c0fd35e0",
          "name": "Mohammad Hossein Rohban",
          "hidden": false
        },
        {
          "_id": "691ac9346bfd5965c0fd35e1",
          "name": "Shaghayegh Haghjooy Javanmard",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-05T07:50:43.000Z",
      "submittedOnDailyAt": "2025-11-17T04:36:37.800Z",
      "title": "Large Language Models for Scientific Idea Generation: A Creativity-Centered Survey",
      "submittedOnDailyBy": {
        "_id": "6404a0bf1a3babee78de3a49",
        "avatarUrl": "/avatars/700f0c71946cbe8ef2728526c1ef1683.svg",
        "isPro": false,
        "fullname": "Arash Mari Oriyad",
        "user": "arashmarioriyad",
        "type": "user"
      },
      "summary": "Scientific idea generation lies at the heart of scientific discovery and has driven human progress-whether by solving unsolved problems or proposing novel hypotheses to explain unknown phenomena. Unlike standard scientific reasoning or general creative generation, idea generation in science is a multi-objective and open-ended task, where the novelty of a contribution is as essential as its empirical soundness. Large language models (LLMs) have recently emerged as promising generators of scientific ideas, capable of producing coherent and factual outputs with surprising intuition and acceptable reasoning, yet their creative capacity remains inconsistent and poorly understood. This survey provides a structured synthesis of methods for LLM-driven scientific ideation, examining how different approaches balance creativity with scientific soundness. We categorize existing methods into five complementary families: External knowledge augmentation, Prompt-based distributional steering, Inference-time scaling, Multi-agent collaboration, and Parameter-level adaptation. To interpret their contributions, we employ two complementary frameworks: Boden's taxonomy of Combinatorial, Exploratory and Transformational creativity to characterize the level of ideas each family expected to generate, and Rhodes' 4Ps framework-Person, Process, Press, and Product-to locate the aspect or source of creativity that each method emphasizes. By aligning methodological advances with creativity frameworks, this survey clarifies the state of the field and outlines key directions toward reliable, systematic, and transformative applications of LLMs in scientific discovery.",
      "upvotes": 0,
      "discussionId": "691ac9356bfd5965c0fd35e2",
      "ai_summary": "This survey examines methods for using large language models to generate scientific ideas, categorizing them into five families and aligning them with creativity frameworks to improve scientific soundness and novelty.",
      "ai_keywords": [
        "large language models",
        "scientific ideation",
        "external knowledge augmentation",
        "prompt-based distributional steering",
        "inference-time scaling",
        "multi-agent collaboration",
        "parameter-level adaptation",
        "Boden's taxonomy",
        "Combinatorial creativity",
        "Exploratory creativity",
        "Transformational creativity",
        "Rhodes' 4Ps framework"
      ]
    },
    "publishedAt": "2025-11-05T02:50:43.000Z",
    "title": "Large Language Models for Scientific Idea Generation: A Creativity-Centered Survey",
    "summary": "Scientific idea generation lies at the heart of scientific discovery and has driven human progress-whether by solving unsolved problems or proposing novel hypotheses to explain unknown phenomena. Unlike standard scientific reasoning or general creative generation, idea generation in science is a multi-objective and open-ended task, where the novelty of a contribution is as essential as its empirical soundness. Large language models (LLMs) have recently emerged as promising generators of scientific ideas, capable of producing coherent and factual outputs with surprising intuition and acceptable reasoning, yet their creative capacity remains inconsistent and poorly understood. This survey provides a structured synthesis of methods for LLM-driven scientific ideation, examining how different approaches balance creativity with scientific soundness. We categorize existing methods into five complementary families: External knowledge augmentation, Prompt-based distributional steering, Inference-time scaling, Multi-agent collaboration, and Parameter-level adaptation. To interpret their contributions, we employ two complementary frameworks: Boden's taxonomy of Combinatorial, Exploratory and Transformational creativity to characterize the level of ideas each family expected to generate, and Rhodes' 4Ps framework-Person, Process, Press, and Product-to locate the aspect or source of creativity that each method emphasizes. By aligning methodological advances with creativity frameworks, this survey clarifies the state of the field and outlines key directions toward reliable, systematic, and transformative applications of LLMs in scientific discovery.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.07448.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6404a0bf1a3babee78de3a49",
      "avatarUrl": "/avatars/700f0c71946cbe8ef2728526c1ef1683.svg",
      "fullname": "Arash Mari Oriyad",
      "name": "arashmarioriyad",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]