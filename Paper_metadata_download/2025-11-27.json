[
  {
    "paper": {
      "id": "2511.20639",
      "authors": [
        {
          "_id": "6927c504243b2216fb75cd62",
          "name": "Jiaru Zou",
          "hidden": false
        },
        {
          "_id": "6927c504243b2216fb75cd63",
          "name": "Xiyuan Yang",
          "hidden": false
        },
        {
          "_id": "6927c504243b2216fb75cd64",
          "name": "Ruizhong Qiu",
          "hidden": false
        },
        {
          "_id": "6927c504243b2216fb75cd65",
          "name": "Gaotang Li",
          "hidden": false
        },
        {
          "_id": "6927c504243b2216fb75cd66",
          "name": "Katherine Tieu",
          "hidden": false
        },
        {
          "_id": "6927c504243b2216fb75cd67",
          "name": "Pan Lu",
          "hidden": false
        },
        {
          "_id": "6927c504243b2216fb75cd68",
          "name": "Ke Shen",
          "hidden": false
        },
        {
          "_id": "6927c504243b2216fb75cd69",
          "name": "Hanghang Tong",
          "hidden": false
        },
        {
          "_id": "6927c504243b2216fb75cd6a",
          "name": "Yejin Choi",
          "hidden": false
        },
        {
          "_id": "6927c504243b2216fb75cd6b",
          "name": "Jingrui He",
          "hidden": false
        },
        {
          "_id": "6927c504243b2216fb75cd6c",
          "name": "James Zou",
          "hidden": false
        },
        {
          "_id": "6927c504243b2216fb75cd6d",
          "name": "Mengdi Wang",
          "hidden": false
        },
        {
          "_id": "6927c504243b2216fb75cd6e",
          "name": "Ling Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-25T18:56:57.000Z",
      "submittedOnDailyAt": "2025-11-27T01:00:26.981Z",
      "title": "Latent Collaboration in Multi-Agent Systems",
      "submittedOnDailyBy": {
        "_id": "65c288280aa2d53135734a42",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65c288280aa2d53135734a42/5WHmau52EaRS01TOMI3Qg.jpeg",
        "isPro": false,
        "fullname": "Jiaru Zou",
        "user": "jiaruz2",
        "type": "user"
      },
      "summary": "Multi-agent systems (MAS) extend large language models (LLMs) from independent single-model reasoning to coordinative system-level intelligence. While existing LLM agents depend on text-based mediation for reasoning and communication, we take a step forward by enabling models to collaborate directly within the continuous latent space. We introduce LatentMAS, an end-to-end training-free framework that enables pure latent collaboration among LLM agents. In LatentMAS, each agent first performs auto-regressive latent thoughts generation through last-layer hidden embeddings. A shared latent working memory then preserves and transfers each agent's internal representations, ensuring lossless information exchange. We provide theoretical analyses establishing that LatentMAS attains higher expressiveness and lossless information preservation with substantially lower complexity than vanilla text-based MAS. In addition, empirical evaluations across 9 comprehensive benchmarks spanning math and science reasoning, commonsense understanding, and code generation show that LatentMAS consistently outperforms strong single-model and text-based MAS baselines, achieving up to 14.6% higher accuracy, reducing output token usage by 70.8%-83.7%, and providing 4x-4.3x faster end-to-end inference. These results demonstrate that our new latent collaboration framework enhances system-level reasoning quality while offering substantial efficiency gains without any additional training. Code and data are fully open-sourced at https://github.com/Gen-Verse/LatentMAS.",
      "upvotes": 29,
      "discussionId": "6927c504243b2216fb75cd6f",
      "githubRepo": "https://github.com/Gen-Verse/LatentMAS",
      "ai_summary": "LatentMAS enables efficient and effective collaboration among LLM agents using latent space representations, enhancing reasoning quality and reducing computational costs.",
      "ai_keywords": [
        "multi-agent systems",
        "large language models",
        "latent space",
        "LatentMAS",
        "auto-regressive latent thoughts generation",
        "last-layer hidden embeddings",
        "shared latent working memory",
        "expressiveness",
        "information preservation",
        "complexity",
        "math and science reasoning",
        "commonsense understanding",
        "code generation",
        "accuracy",
        "output token usage",
        "end-to-end inference"
      ],
      "githubStars": 34,
      "organization": {
        "_id": "67a21d7efeeacb7707bf40de",
        "name": "Gen-Verse",
        "fullname": "Princeton-AI",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64fde4e252e82dd432b74ce9/TAEScS71YX5NPRM4TXZc8.png"
      }
    },
    "publishedAt": "2025-11-25T13:56:57.000Z",
    "title": "Latent Collaboration in Multi-Agent Systems",
    "summary": "Multi-agent systems (MAS) extend large language models (LLMs) from independent single-model reasoning to coordinative system-level intelligence. While existing LLM agents depend on text-based mediation for reasoning and communication, we take a step forward by enabling models to collaborate directly within the continuous latent space. We introduce LatentMAS, an end-to-end training-free framework that enables pure latent collaboration among LLM agents. In LatentMAS, each agent first performs auto-regressive latent thoughts generation through last-layer hidden embeddings. A shared latent working memory then preserves and transfers each agent's internal representations, ensuring lossless information exchange. We provide theoretical analyses establishing that LatentMAS attains higher expressiveness and lossless information preservation with substantially lower complexity than vanilla text-based MAS. In addition, empirical evaluations across 9 comprehensive benchmarks spanning math and science reasoning, commonsense understanding, and code generation show that LatentMAS consistently outperforms strong single-model and text-based MAS baselines, achieving up to 14.6% higher accuracy, reducing output token usage by 70.8%-83.7%, and providing 4x-4.3x faster end-to-end inference. These results demonstrate that our new latent collaboration framework enhances system-level reasoning quality while offering substantial efficiency gains without any additional training. Code and data are fully open-sourced at https://github.com/Gen-Verse/LatentMAS.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20639.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65c288280aa2d53135734a42",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65c288280aa2d53135734a42/5WHmau52EaRS01TOMI3Qg.jpeg",
      "fullname": "Jiaru Zou",
      "name": "jiaruz2",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "organization": {
      "_id": "67a21d7efeeacb7707bf40de",
      "name": "Gen-Verse",
      "fullname": "Princeton-AI",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64fde4e252e82dd432b74ce9/TAEScS71YX5NPRM4TXZc8.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.21395",
      "authors": [
        {
          "_id": "6927bed6243b2216fb75cd58",
          "name": "Qixun Wang",
          "hidden": false
        },
        {
          "_id": "6927bed6243b2216fb75cd59",
          "name": "Yang Shi",
          "hidden": false
        },
        {
          "_id": "6927bed6243b2216fb75cd5a",
          "name": "Yifei Wang",
          "hidden": false
        },
        {
          "_id": "6927bed6243b2216fb75cd5b",
          "name": "Yuanxing Zhang",
          "hidden": false
        },
        {
          "_id": "6927bed6243b2216fb75cd5c",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "6927bed6243b2216fb75cd5d",
          "name": "Kun Gai",
          "hidden": false
        },
        {
          "_id": "6927bed6243b2216fb75cd5e",
          "name": "Xianghua Ying",
          "hidden": false
        },
        {
          "_id": "6927bed6243b2216fb75cd5f",
          "name": "Yisen Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-26T13:46:39.000Z",
      "submittedOnDailyAt": "2025-11-27T00:31:32.938Z",
      "title": "Monet: Reasoning in Latent Visual Space Beyond Images and Language",
      "submittedOnDailyBy": {
        "_id": "673c7319d11b1c2e246ead9c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/673c7319d11b1c2e246ead9c/IjFIO--N7Hm_BOEafhEQv.jpeg",
        "isPro": false,
        "fullname": "Yang Shi",
        "user": "DogNeverSleep",
        "type": "user"
      },
      "summary": "\"Thinking with images\" has emerged as an effective paradigm for advancing visual reasoning, extending beyond text-only chains of thought by injecting visual evidence into intermediate reasoning steps. However, existing methods fall short of human-like abstract visual thinking, as their flexibility is fundamentally limited by external tools. In this work, we introduce Monet, a training framework that enables multimodal large language models (MLLMs) to reason directly within the latent visual space by generating continuous embeddings that function as intermediate visual thoughts. We identify two core challenges in training MLLMs for latent visual reasoning: high computational cost in latent-vision alignment and insufficient supervision over latent embeddings, and address them with a three-stage distillation-based supervised fine-tuning (SFT) pipeline. We further reveal a limitation of applying GRPO to latent reasoning: it primarily enhances text-based reasoning rather than latent reasoning. To overcome this, we propose VLPO (Visual-latent Policy Optimization), a reinforcement learning method that explicitly incorporates latent embeddings into policy gradient updates. To support SFT, we construct Monet-SFT-125K, a high-quality text-image interleaved CoT dataset containing 125K real-world, chart, OCR, and geometry CoTs. Our model, Monet-7B, shows consistent gains across real-world perception and reasoning benchmarks and exhibits strong out-of-distribution generalization on challenging abstract visual reasoning tasks. We also empirically analyze the role of each training component and discuss our early unsuccessful attempts, providing insights for future developments in visual latent reasoning. Our model, data, and code are available at https://github.com/NOVAglow646/Monet.",
      "upvotes": 5,
      "discussionId": "6927bed6243b2216fb75cd60",
      "githubRepo": "https://github.com/NOVAglow646/Monet",
      "ai_summary": "Monet, a training framework, enables MLLMs to reason in latent visual space using continuous embeddings, addressing challenges like computational cost and supervision, and outperforms on visual reasoning tasks.",
      "ai_keywords": [
        "multimodal large language models",
        "MLLMs",
        "latent visual space",
        "continuous embeddings",
        "intermediate visual thoughts",
        "three-stage distillation-based supervised fine-tuning",
        "SFT",
        "GRPO",
        "Visual-latent Policy Optimization",
        "VLPO",
        "reinforcement learning",
        "policy gradient updates",
        "CoT dataset",
        "real-world perception",
        "reasoning benchmarks",
        "out-of-distribution generalization"
      ],
      "githubStars": 1
    },
    "publishedAt": "2025-11-26T08:46:39.000Z",
    "title": "Monet: Reasoning in Latent Visual Space Beyond Images and Language",
    "summary": "\"Thinking with images\" has emerged as an effective paradigm for advancing visual reasoning, extending beyond text-only chains of thought by injecting visual evidence into intermediate reasoning steps. However, existing methods fall short of human-like abstract visual thinking, as their flexibility is fundamentally limited by external tools. In this work, we introduce Monet, a training framework that enables multimodal large language models (MLLMs) to reason directly within the latent visual space by generating continuous embeddings that function as intermediate visual thoughts. We identify two core challenges in training MLLMs for latent visual reasoning: high computational cost in latent-vision alignment and insufficient supervision over latent embeddings, and address them with a three-stage distillation-based supervised fine-tuning (SFT) pipeline. We further reveal a limitation of applying GRPO to latent reasoning: it primarily enhances text-based reasoning rather than latent reasoning. To overcome this, we propose VLPO (Visual-latent Policy Optimization), a reinforcement learning method that explicitly incorporates latent embeddings into policy gradient updates. To support SFT, we construct Monet-SFT-125K, a high-quality text-image interleaved CoT dataset containing 125K real-world, chart, OCR, and geometry CoTs. Our model, Monet-7B, shows consistent gains across real-world perception and reasoning benchmarks and exhibits strong out-of-distribution generalization on challenging abstract visual reasoning tasks. We also empirically analyze the role of each training component and discuss our early unsuccessful attempts, providing insights for future developments in visual latent reasoning. Our model, data, and code are available at https://github.com/NOVAglow646/Monet.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.21395.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "673c7319d11b1c2e246ead9c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/673c7319d11b1c2e246ead9c/IjFIO--N7Hm_BOEafhEQv.jpeg",
      "fullname": "Yang Shi",
      "name": "DogNeverSleep",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.21692",
      "authors": [
        {
          "_id": "6927cf95243b2216fb75cd90",
          "name": "Yeganeh Kordi",
          "hidden": false
        },
        {
          "_id": "6927cf95243b2216fb75cd91",
          "name": "Nihal V. Nayak",
          "hidden": false
        },
        {
          "_id": "6927cf95243b2216fb75cd92",
          "name": "Max Zuo",
          "hidden": false
        },
        {
          "_id": "6927cf95243b2216fb75cd93",
          "name": "Ilana Nguyen",
          "hidden": false
        },
        {
          "_id": "6927cf95243b2216fb75cd94",
          "name": "Stephen H. Bach",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-26T18:59:57.000Z",
      "submittedOnDailyAt": "2025-11-27T01:58:38.144Z",
      "title": "Revisiting Generalization Across Difficulty Levels: It's Not So Easy",
      "submittedOnDailyBy": {
        "_id": "64fbb457c7f04f7cee8624e0",
        "avatarUrl": "/avatars/f0512561780625d9be43f00dfd5cd46d.svg",
        "isPro": false,
        "fullname": "Max Zuo",
        "user": "zuom",
        "type": "user"
      },
      "summary": "We investigate how well large language models (LLMs) generalize across different task difficulties, a key question for effective data curation and evaluation. Existing research is mixed regarding whether training on easier or harder data leads to better results, and whether those gains come on easier or harder test data. We address this question by conducting a systematic evaluation of LLMs' generalization across models, datasets, and fine-grained groups of example difficulty. We rank examples in six datasets using the outputs of thousands of different LLMs and Item Response Theory (IRT), a well-established difficulty metric in educational testing. Unlike prior work, our difficulty ratings are therefore determined solely by the abilities of many different LLMs, excluding human opinions of difficulty. With a more objective, larger-scale, and finer-grained analysis, we show that cross-difficulty generalization is often limited; training on either easy or hard data cannot achieve consistent improvements across the full range of difficulties. These results show the importance of having a range of difficulties in both training and evaluation data for LLMs, and that taking shortcuts with respect to difficulty is risky.",
      "upvotes": 4,
      "discussionId": "6927cf95243b2216fb75cd95",
      "githubRepo": "https://github.com/BatsResearch/Cross-Difficulty",
      "ai_summary": "LLMs do not consistently generalize across different task difficulties, indicating the need for a broad range of difficulty levels in both training and evaluation datasets.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "generalization",
        "task difficulties",
        "data curation",
        "evaluation",
        "Item Response Theory",
        "IRT",
        "cross-difficulty generalization"
      ],
      "githubStars": 2,
      "organization": {
        "_id": "650d8bcd5085c0ce1f286c12",
        "name": "BatsResearch",
        "fullname": "Bats Research",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/608849cadf398c3b285ce95b/oFrkKbxGUKIW2W8FFGUje.png"
      }
    },
    "publishedAt": "2025-11-26T13:59:57.000Z",
    "title": "Revisiting Generalization Across Difficulty Levels: It's Not So Easy",
    "summary": "We investigate how well large language models (LLMs) generalize across different task difficulties, a key question for effective data curation and evaluation. Existing research is mixed regarding whether training on easier or harder data leads to better results, and whether those gains come on easier or harder test data. We address this question by conducting a systematic evaluation of LLMs' generalization across models, datasets, and fine-grained groups of example difficulty. We rank examples in six datasets using the outputs of thousands of different LLMs and Item Response Theory (IRT), a well-established difficulty metric in educational testing. Unlike prior work, our difficulty ratings are therefore determined solely by the abilities of many different LLMs, excluding human opinions of difficulty. With a more objective, larger-scale, and finer-grained analysis, we show that cross-difficulty generalization is often limited; training on either easy or hard data cannot achieve consistent improvements across the full range of difficulties. These results show the importance of having a range of difficulties in both training and evaluation data for LLMs, and that taking shortcuts with respect to difficulty is risky.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.21692.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64fbb457c7f04f7cee8624e0",
      "avatarUrl": "/avatars/f0512561780625d9be43f00dfd5cd46d.svg",
      "fullname": "Max Zuo",
      "name": "zuom",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "organization": {
      "_id": "650d8bcd5085c0ce1f286c12",
      "name": "BatsResearch",
      "fullname": "Bats Research",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/608849cadf398c3b285ce95b/oFrkKbxGUKIW2W8FFGUje.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.20478",
      "authors": [
        {
          "_id": "6926d024243b2216fb75cbca",
          "name": "Kateryna Chumachenko",
          "hidden": false
        },
        {
          "_id": "6926d024243b2216fb75cbcb",
          "name": "Amala Sanjay Deshmukh",
          "hidden": false
        },
        {
          "_id": "6926d024243b2216fb75cbcc",
          "name": "Jarno Seppanen",
          "hidden": false
        },
        {
          "_id": "6926d024243b2216fb75cbcd",
          "name": "Ilia Karmanov",
          "hidden": false
        },
        {
          "_id": "6926d024243b2216fb75cbce",
          "name": "Chia-Chih Chen",
          "hidden": false
        },
        {
          "_id": "6926d024243b2216fb75cbcf",
          "name": "Lukas Voegtle",
          "hidden": false
        },
        {
          "_id": "6926d024243b2216fb75cbd0",
          "name": "Philipp Fischer",
          "hidden": false
        },
        {
          "_id": "6926d024243b2216fb75cbd1",
          "name": "Marek Wawrzos",
          "hidden": false
        },
        {
          "_id": "6926d024243b2216fb75cbd2",
          "name": "Saeid Motiian",
          "hidden": false
        },
        {
          "_id": "6926d024243b2216fb75cbd3",
          "name": "Roman Ageev",
          "hidden": false
        },
        {
          "_id": "6926d024243b2216fb75cbd4",
          "name": "Kedi Wu",
          "hidden": false
        },
        {
          "_id": "6926d024243b2216fb75cbd5",
          "name": "Alexandre Milesi",
          "hidden": false
        },
        {
          "_id": "6926d024243b2216fb75cbd6",
          "name": "Maryam Moosaei",
          "hidden": false
        },
        {
          "_id": "6926d024243b2216fb75cbd7",
          "name": "Krzysztof Pawelec",
          "hidden": false
        },
        {
          "_id": "6926d024243b2216fb75cbd8",
          "name": "Padmavathy Subramanian",
          "hidden": false
        },
        {
          "_id": "6926d024243b2216fb75cbd9",
          "name": "Mehrzad Samadi",
          "hidden": false
        },
        {
          "_id": "6926d024243b2216fb75cbda",
          "name": "Xin Yu",
          "hidden": false
        },
        {
          "_id": "6926d024243b2216fb75cbdb",
          "name": "Celina Dear",
          "hidden": false
        },
        {
          "_id": "6926d024243b2216fb75cbdc",
          "name": "Sarah Stoddard",
          "hidden": false
        },
        {
          "_id": "6926d024243b2216fb75cbdd",
          "name": "Jenna Diamond",
          "hidden": false
        },
        {
          "_id": "6926d024243b2216fb75cbde",
          "name": "Jesse Oliver",
          "hidden": false
        },
        {
          "_id": "6926d024243b2216fb75cbdf",
          "name": "Leanna Chraghchian",
          "hidden": false
        },
        {
          "_id": "6926d024243b2216fb75cbe0",
          "name": "Patrick Skelly",
          "hidden": false
        },
        {
          "_id": "6926d024243b2216fb75cbe1",
          "name": "Tom Balough",
          "hidden": false
        },
        {
          "_id": "6926d024243b2216fb75cbe2",
          "name": "Yao Xu",
          "hidden": false
        },
        {
          "_id": "6926d024243b2216fb75cbe3",
          "name": "Jane Polak Scowcroft",
          "hidden": false
        },
        {
          "_id": "6926d024243b2216fb75cbe4",
          "name": "Daniel Korzekwa",
          "hidden": false
        },
        {
          "_id": "6926d024243b2216fb75cbe5",
          "name": "Darragh Hanley",
          "hidden": false
        },
        {
          "_id": "6926d024243b2216fb75cbe6",
          "name": "Sandip Bhaskar",
          "hidden": false
        },
        {
          "_id": "6926d024243b2216fb75cbe7",
          "name": "Timo Roman",
          "hidden": false
        },
        {
          "_id": "6926d024243b2216fb75cbe8",
          "name": "Karan Sapra",
          "hidden": false
        },
        {
          "_id": "6926d024243b2216fb75cbe9",
          "name": "Andrew Tao",
          "hidden": false
        },
        {
          "_id": "6926d024243b2216fb75cbea",
          "name": "Bryan Catanzaro",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-25T16:41:25.000Z",
      "submittedOnDailyAt": "2025-11-27T00:15:47.055Z",
      "title": "NVIDIA Nemotron Parse 1.1",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We introduce Nemotron-Parse-1.1, a lightweight document parsing and OCR model that advances the capabilities of its predecessor, Nemoretriever-Parse-1.0. Nemotron-Parse-1.1 delivers improved capabilities across general OCR, markdown formatting, structured table parsing, and text extraction from pictures, charts, and diagrams. It also supports a longer output sequence length for visually dense documents. As with its predecessor, it extracts bounding boxes of text segments, as well as corresponding semantic classes. Nemotron-Parse-1.1 follows an encoder-decoder architecture with 885M parameters, including a compact 256M-parameter language decoder. It achieves competitive accuracy on public benchmarks making it a strong lightweight OCR solution. We release the model weights publicly on Huggingface, as well as an optimized NIM container, along with a subset of the training data as part of the broader Nemotron-VLM-v2 dataset. Additionally, we release Nemotron-Parse-1.1-TC which operates on a reduced vision token length, offering a 20% speed improvement with minimal quality degradation.",
      "upvotes": 3,
      "discussionId": "6926d024243b2216fb75cbeb",
      "ai_summary": "Nemotron-Parse-1.1 is a lightweight OCR and document parsing model with improved capabilities in general OCR, markdown formatting, structured table parsing, and text extraction from images, using an encoder-decoder architecture.",
      "ai_keywords": [
        "OCR",
        "document parsing",
        "markdown formatting",
        "structured table parsing",
        "text extraction",
        "encoder-decoder architecture",
        "bounding boxes",
        "semantic classes",
        "Huggingface",
        "NIM container",
        "vision tokens"
      ],
      "organization": {
        "_id": "60262b67268c201cdc8b7d43",
        "name": "nvidia",
        "fullname": "NVIDIA",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
      }
    },
    "publishedAt": "2025-11-25T11:41:25.000Z",
    "title": "NVIDIA Nemotron Parse 1.1",
    "summary": "We introduce Nemotron-Parse-1.1, a lightweight document parsing and OCR model that advances the capabilities of its predecessor, Nemoretriever-Parse-1.0. Nemotron-Parse-1.1 delivers improved capabilities across general OCR, markdown formatting, structured table parsing, and text extraction from pictures, charts, and diagrams. It also supports a longer output sequence length for visually dense documents. As with its predecessor, it extracts bounding boxes of text segments, as well as corresponding semantic classes. Nemotron-Parse-1.1 follows an encoder-decoder architecture with 885M parameters, including a compact 256M-parameter language decoder. It achieves competitive accuracy on public benchmarks making it a strong lightweight OCR solution. We release the model weights publicly on Huggingface, as well as an optimized NIM container, along with a subset of the training data as part of the broader Nemotron-VLM-v2 dataset. Additionally, we release Nemotron-Parse-1.1-TC which operates on a reduced vision token length, offering a 20% speed improvement with minimal quality degradation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20478.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 171
    },
    "organization": {
      "_id": "60262b67268c201cdc8b7d43",
      "name": "nvidia",
      "fullname": "NVIDIA",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.20714",
      "authors": [
        {
          "_id": "6927bc37243b2216fb75cd3c",
          "name": "Inferix Team",
          "hidden": false
        },
        {
          "_id": "6927bc37243b2216fb75cd3d",
          "name": "Tianyu Feng",
          "hidden": false
        },
        {
          "_id": "6927bc37243b2216fb75cd3e",
          "name": "Yizeng Han",
          "hidden": false
        },
        {
          "_id": "6927bc37243b2216fb75cd3f",
          "name": "Jiahao He",
          "hidden": false
        },
        {
          "_id": "6927bc37243b2216fb75cd40",
          "name": "Yuanyu He",
          "hidden": false
        },
        {
          "_id": "6927bc37243b2216fb75cd41",
          "name": "Xi Lin",
          "hidden": false
        },
        {
          "_id": "6927bc37243b2216fb75cd42",
          "name": "Teng Liu",
          "hidden": false
        },
        {
          "_id": "6927bc37243b2216fb75cd43",
          "name": "Hanfeng Lu",
          "hidden": false
        },
        {
          "_id": "6927bc37243b2216fb75cd44",
          "name": "Jiasheng Tang",
          "hidden": false
        },
        {
          "_id": "6927bc37243b2216fb75cd45",
          "name": "Wei Wang",
          "hidden": false
        },
        {
          "_id": "6927bc37243b2216fb75cd46",
          "name": "Zhiyuan Wang",
          "hidden": false
        },
        {
          "_id": "6927bc37243b2216fb75cd47",
          "name": "Jichao Wu",
          "hidden": false
        },
        {
          "_id": "6927bc37243b2216fb75cd48",
          "name": "Mingyang Yang",
          "hidden": false
        },
        {
          "_id": "6927bc37243b2216fb75cd49",
          "name": "Yinghao Yu",
          "hidden": false
        },
        {
          "_id": "6927bc37243b2216fb75cd4a",
          "name": "Zeyu Zhang",
          "hidden": false
        },
        {
          "_id": "6927bc37243b2216fb75cd4b",
          "name": "Bohan Zhuang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-25T01:45:04.000Z",
      "submittedOnDailyAt": "2025-11-27T00:19:36.886Z",
      "title": "Inferix: A Block-Diffusion based Next-Generation Inference Engine for World Simulation",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "World models serve as core simulators for fields such as agentic AI, embodied AI, and gaming, capable of generating long, physically realistic, and interactive high-quality videos. Moreover, scaling these models could unlock emergent capabilities in visual perception, understanding, and reasoning, paving the way for a new paradigm that moves beyond current LLM-centric vision foundation models. A key breakthrough empowering them is the semi-autoregressive (block-diffusion) decoding paradigm, which merges the strengths of diffusion and autoregressive methods by generating video tokens in block-applying diffusion within each block while conditioning on previous ones, resulting in more coherent and stable video sequences. Crucially, it overcomes limitations of standard video diffusion by reintroducing LLM-style KV Cache management, enabling efficient, variable-length, and high-quality generation.\n  Therefore, Inferix is specifically designed as a next-generation inference engine to enable immersive world synthesis through optimized semi-autoregressive decoding processes. This dedicated focus on world simulation distinctly sets it apart from systems engineered for high-concurrency scenarios (like vLLM or SGLang) and from classic video diffusion models (such as xDiTs). Inferix further enhances its offering with interactive video streaming and profiling, enabling real-time interaction and realistic simulation to accurately model world dynamics. Additionally, it supports efficient benchmarking through seamless integration of LV-Bench, a new fine-grained evaluation benchmark tailored for minute-long video generation scenarios. We hope the community will work together to advance Inferix and foster world model exploration.",
      "upvotes": 3,
      "discussionId": "6927bc38243b2216fb75cd4c",
      "githubRepo": "https://github.com/alibaba-damo-academy/Inferix",
      "ai_summary": "Inferix is a next-generation inference engine designed for immersive world synthesis using semi-autoregressive decoding, combining diffusion and autoregressive methods for high-quality, real-time video generation and interaction.",
      "ai_keywords": [
        "world models",
        "agentic AI",
        "embodied AI",
        "gaming",
        "visual perception",
        "understanding",
        "reasoning",
        "semi-autoregressive",
        "block-diffusion",
        "diffusion",
        "autoregressive methods",
        "video tokens",
        "KV Cache management",
        "Inferix",
        "vLLM",
        "SGLang",
        "xDiTs",
        "interactive video streaming",
        "profiling",
        "LV-Bench",
        "minute-long video generation"
      ],
      "githubStars": 7
    },
    "publishedAt": "2025-11-24T20:45:04.000Z",
    "title": "Inferix: A Block-Diffusion based Next-Generation Inference Engine for World Simulation",
    "summary": "World models serve as core simulators for fields such as agentic AI, embodied AI, and gaming, capable of generating long, physically realistic, and interactive high-quality videos. Moreover, scaling these models could unlock emergent capabilities in visual perception, understanding, and reasoning, paving the way for a new paradigm that moves beyond current LLM-centric vision foundation models. A key breakthrough empowering them is the semi-autoregressive (block-diffusion) decoding paradigm, which merges the strengths of diffusion and autoregressive methods by generating video tokens in block-applying diffusion within each block while conditioning on previous ones, resulting in more coherent and stable video sequences. Crucially, it overcomes limitations of standard video diffusion by reintroducing LLM-style KV Cache management, enabling efficient, variable-length, and high-quality generation.\n  Therefore, Inferix is specifically designed as a next-generation inference engine to enable immersive world synthesis through optimized semi-autoregressive decoding processes. This dedicated focus on world simulation distinctly sets it apart from systems engineered for high-concurrency scenarios (like vLLM or SGLang) and from classic video diffusion models (such as xDiTs). Inferix further enhances its offering with interactive video streaming and profiling, enabling real-time interaction and realistic simulation to accurately model world dynamics. Additionally, it supports efficient benchmarking through seamless integration of LV-Bench, a new fine-grained evaluation benchmark tailored for minute-long video generation scenarios. We hope the community will work together to advance Inferix and foster world model exploration.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20714.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 171
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.19797",
      "authors": [
        {
          "_id": "6927bd2d243b2216fb75cd52",
          "name": "Linqi Zhou",
          "hidden": false
        },
        {
          "_id": "6927bd2d243b2216fb75cd53",
          "name": "Mathias Parger",
          "hidden": false
        },
        {
          "_id": "6927bd2d243b2216fb75cd54",
          "name": "Ayaan Haque",
          "hidden": false
        },
        {
          "_id": "6927bd2d243b2216fb75cd55",
          "name": "Jiaming Song",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-24T23:55:45.000Z",
      "submittedOnDailyAt": "2025-11-27T00:23:41.631Z",
      "title": "Terminal Velocity Matching",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We propose Terminal Velocity Matching (TVM), a generalization of flow matching that enables high-fidelity one- and few-step generative modeling. TVM models the transition between any two diffusion timesteps and regularizes its behavior at its terminal time rather than at the initial time. We prove that TVM provides an upper bound on the 2-Wasserstein distance between data and model distributions when the model is Lipschitz continuous. However, since Diffusion Transformers lack this property, we introduce minimal architectural changes that achieve stable, single-stage training. To make TVM efficient in practice, we develop a fused attention kernel that supports backward passes on Jacobian-Vector Products, which scale well with transformer architectures. On ImageNet-256x256, TVM achieves 3.29 FID with a single function evaluation (NFE) and 1.99 FID with 4 NFEs. It similarly achieves 4.32 1-NFE FID and 2.94 4-NFE FID on ImageNet-512x512, representing state-of-the-art performance for one/few-step models from scratch.",
      "upvotes": 2,
      "discussionId": "6927bd2d243b2216fb75cd56",
      "ai_summary": "Terminal Velocity Matching (TVM) generalizes flow matching for high-fidelity generative modeling, achieving state-of-the-art performance on ImageNet with minimal computational steps.",
      "ai_keywords": [
        "Terminal Velocity Matching",
        "flow matching",
        "diffusion timesteps",
        "2-Wasserstein distance",
        "Lipschitz continuous",
        "Diffusion Transformers",
        "fused attention kernel",
        "Jacobian-Vector Products",
        "ImageNet-256x256",
        "ImageNet-512x512",
        "FID"
      ]
    },
    "publishedAt": "2025-11-24T18:55:45.000Z",
    "title": "Terminal Velocity Matching",
    "summary": "We propose Terminal Velocity Matching (TVM), a generalization of flow matching that enables high-fidelity one- and few-step generative modeling. TVM models the transition between any two diffusion timesteps and regularizes its behavior at its terminal time rather than at the initial time. We prove that TVM provides an upper bound on the 2-Wasserstein distance between data and model distributions when the model is Lipschitz continuous. However, since Diffusion Transformers lack this property, we introduce minimal architectural changes that achieve stable, single-stage training. To make TVM efficient in practice, we develop a fused attention kernel that supports backward passes on Jacobian-Vector Products, which scale well with transformer architectures. On ImageNet-256x256, TVM achieves 3.29 FID with a single function evaluation (NFE) and 1.99 FID with 4 NFEs. It similarly achieves 4.32 1-NFE FID and 2.94 4-NFE FID on ImageNet-512x512, representing state-of-the-art performance for one/few-step models from scratch.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.19797.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 171
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.17918",
      "authors": [
        {
          "_id": "692680db243b2216fb75cab2",
          "user": {
            "_id": "68e34e1f349df589247872e4",
            "avatarUrl": "/avatars/884f38e7b4370d0661aab7a93dd1cd5d.svg",
            "isPro": false,
            "fullname": "Youngsik Yun",
            "user": "bbangsik13",
            "type": "user"
          },
          "name": "Youngsik Yun",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-11-26T09:27:27.289Z",
          "hidden": false
        },
        {
          "_id": "692680db243b2216fb75cab3",
          "name": "Dongjun Gu",
          "hidden": false
        },
        {
          "_id": "692680db243b2216fb75cab4",
          "name": "Youngjung Uh",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-22T05:04:46.000Z",
      "submittedOnDailyAt": "2025-11-27T02:06:03.890Z",
      "title": "Frequency-Adaptive Sharpness Regularization for Improving 3D Gaussian Splatting Generalization",
      "submittedOnDailyBy": {
        "_id": "68e34e1f349df589247872e4",
        "avatarUrl": "/avatars/884f38e7b4370d0661aab7a93dd1cd5d.svg",
        "isPro": false,
        "fullname": "Youngsik Yun",
        "user": "bbangsik13",
        "type": "user"
      },
      "summary": "Despite 3D Gaussian Splatting (3DGS) excelling in most configurations, it lacks generalization across novel viewpoints in a few-shot scenario because it overfits to the sparse observations. We revisit 3DGS optimization from a machine learning perspective, framing novel view synthesis as a generalization problem to unseen viewpoints-an underexplored direction. We propose Frequency-Adaptive Sharpness Regularization (FASR), which reformulates the 3DGS training objective, thereby guiding 3DGS to converge toward a better generalization solution. Although Sharpness-Aware Minimization (SAM) similarly reduces the sharpness of the loss landscape to improve generalization of classification models, directly employing it to 3DGS is suboptimal due to the discrepancy between the tasks. Specifically, it hinders reconstructing high-frequency details due to excessive regularization, while reducing its strength leads to under-penalizing sharpness. To address this, we reflect the local frequency of images to set the regularization weight and the neighborhood radius when estimating the local sharpness. It prevents floater artifacts in novel viewpoints and reconstructs fine details that SAM tends to oversmooth. Across datasets with various configurations, our method consistently improves a wide range of baselines. Code will be available at https://bbangsik13.github.io/FASR.",
      "upvotes": 0,
      "discussionId": "692680db243b2216fb75cab5",
      "projectPage": "https://bbangsik13.github.io/FASR",
      "githubRepo": "https://github.com/bbangsik13/FASR",
      "ai_summary": "Frequency-Adaptive Sharpness Regularization (FASR) enhances 3D Gaussian Splatting's generalization to novel viewpoints by adaptively adjusting regularization based on local image frequency.",
      "ai_keywords": [
        "3D Gaussian Splatting",
        "3DGS",
        "Frequency-Adaptive Sharpness Regularization",
        "FASR",
        "Sharpness-Aware Minimization",
        "SAM",
        "generalization",
        "novel view synthesis",
        "sharpness regularization",
        "high-frequency details",
        "floater artifacts"
      ],
      "githubStars": 1
    },
    "publishedAt": "2025-11-22T00:04:46.000Z",
    "title": "Frequency-Adaptive Sharpness Regularization for Improving 3D Gaussian Splatting Generalization",
    "summary": "Despite 3D Gaussian Splatting (3DGS) excelling in most configurations, it lacks generalization across novel viewpoints in a few-shot scenario because it overfits to the sparse observations. We revisit 3DGS optimization from a machine learning perspective, framing novel view synthesis as a generalization problem to unseen viewpoints-an underexplored direction. We propose Frequency-Adaptive Sharpness Regularization (FASR), which reformulates the 3DGS training objective, thereby guiding 3DGS to converge toward a better generalization solution. Although Sharpness-Aware Minimization (SAM) similarly reduces the sharpness of the loss landscape to improve generalization of classification models, directly employing it to 3DGS is suboptimal due to the discrepancy between the tasks. Specifically, it hinders reconstructing high-frequency details due to excessive regularization, while reducing its strength leads to under-penalizing sharpness. To address this, we reflect the local frequency of images to set the regularization weight and the neighborhood radius when estimating the local sharpness. It prevents floater artifacts in novel viewpoints and reconstructs fine details that SAM tends to oversmooth. Across datasets with various configurations, our method consistently improves a wide range of baselines. Code will be available at https://bbangsik13.github.io/FASR.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.17918.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "68e34e1f349df589247872e4",
      "avatarUrl": "/avatars/884f38e7b4370d0661aab7a93dd1cd5d.svg",
      "fullname": "Youngsik Yun",
      "name": "bbangsik13",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2511.17889",
      "authors": [
        {
          "_id": "69259d62c97bfa495574df19",
          "name": "Ting Huang",
          "hidden": false
        },
        {
          "_id": "69259d62c97bfa495574df1a",
          "name": "Dongjian Li",
          "hidden": false
        },
        {
          "_id": "69259d62c97bfa495574df1b",
          "name": "Rui Yang",
          "hidden": false
        },
        {
          "_id": "69259d62c97bfa495574df1c",
          "name": "Zeyu Zhang",
          "hidden": false
        },
        {
          "_id": "69259d62c97bfa495574df1d",
          "name": "Zida Yang",
          "hidden": false
        },
        {
          "_id": "69259d62c97bfa495574df1e",
          "name": "Hao Tang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-22T02:34:10.000Z",
      "submittedOnDailyAt": "2025-11-27T05:18:13.911Z",
      "title": "MobileVLA-R1: Reinforcing Vision-Language-Action for Mobile Robots",
      "submittedOnDailyBy": {
        "_id": "675aa0331b310ed01068857f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/PtTMPhuHlfdui_CJOjgHV.png",
        "isPro": false,
        "fullname": "HuangTing",
        "user": "Believe0029",
        "type": "user"
      },
      "summary": "Grounding natural-language instructions into continuous control for quadruped robots remains a fundamental challenge in vision language action. Existing methods struggle to bridge high-level semantic reasoning and low-level actuation, leading to unstable grounding and weak generalization in the real world. To address these issues, we present MobileVLA-R1, a unified vision-language-action framework that enables explicit reasoning and continuous control for quadruped robots. We construct MobileVLA-CoT, a large-scale dataset of multi-granularity chain-of-thought (CoT) for embodied trajectories, providing structured reasoning supervision for alignment. Built upon this foundation, we introduce a two-stage training paradigm that combines supervised CoT alignment with GRPO reinforcement learning to enhance reasoning consistency, control stability, and long-horizon execution. Extensive evaluations on VLN and VLA tasks demonstrate superior performance over strong baselines, with approximately a 5% improvement. Real-world deployment on a quadruped robot validates robust performance in complex environments. Code: https://github.com/AIGeeksGroup/MobileVLA-R1. Website: https://aigeeksgroup.github.io/MobileVLA-R1.",
      "upvotes": 0,
      "discussionId": "69259d62c97bfa495574df1f",
      "ai_summary": "A unified vision-language-action framework, MobileVLA-R1, enhances reasoning and control for quadruped robots through supervised chain-of-thought alignment and GRPO reinforcement learning, achieving superior performance in complex environments.",
      "ai_keywords": [
        "vision-language-action framework",
        "MobileVLA-R1",
        "chain-of-thought (CoT)",
        "GRPO reinforcement learning",
        "VLN tasks",
        "VLA tasks"
      ]
    },
    "publishedAt": "2025-11-21T21:34:10.000Z",
    "title": "MobileVLA-R1: Reinforcing Vision-Language-Action for Mobile Robots",
    "summary": "Grounding natural-language instructions into continuous control for quadruped robots remains a fundamental challenge in vision language action. Existing methods struggle to bridge high-level semantic reasoning and low-level actuation, leading to unstable grounding and weak generalization in the real world. To address these issues, we present MobileVLA-R1, a unified vision-language-action framework that enables explicit reasoning and continuous control for quadruped robots. We construct MobileVLA-CoT, a large-scale dataset of multi-granularity chain-of-thought (CoT) for embodied trajectories, providing structured reasoning supervision for alignment. Built upon this foundation, we introduce a two-stage training paradigm that combines supervised CoT alignment with GRPO reinforcement learning to enhance reasoning consistency, control stability, and long-horizon execution. Extensive evaluations on VLN and VLA tasks demonstrate superior performance over strong baselines, with approximately a 5% improvement. Real-world deployment on a quadruped robot validates robust performance in complex environments. Code: https://github.com/AIGeeksGroup/MobileVLA-R1. Website: https://aigeeksgroup.github.io/MobileVLA-R1.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.17889.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "675aa0331b310ed01068857f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/PtTMPhuHlfdui_CJOjgHV.png",
      "fullname": "HuangTing",
      "name": "Believe0029",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]