[
  {
    "paper": {
      "id": "2507.21809",
      "authors": [
        {
          "_id": "6889a2eb31e1218a08928215",
          "name": "HunyuanWorld Team",
          "hidden": false
        },
        {
          "_id": "6889a2eb31e1218a08928216",
          "name": "Zhenwei Wang",
          "hidden": false
        },
        {
          "_id": "6889a2eb31e1218a08928217",
          "name": "Yuhao Liu",
          "hidden": false
        },
        {
          "_id": "6889a2eb31e1218a08928218",
          "name": "Junta Wu",
          "hidden": false
        },
        {
          "_id": "6889a2eb31e1218a08928219",
          "name": "Zixiao Gu",
          "hidden": false
        },
        {
          "_id": "6889a2eb31e1218a0892821a",
          "name": "Haoyuan Wang",
          "hidden": false
        },
        {
          "_id": "6889a2eb31e1218a0892821b",
          "name": "Xuhui Zuo",
          "hidden": false
        },
        {
          "_id": "6889a2eb31e1218a0892821c",
          "name": "Tianyu Huang",
          "hidden": false
        },
        {
          "_id": "6889a2eb31e1218a0892821d",
          "name": "Wenhuan Li",
          "hidden": false
        },
        {
          "_id": "6889a2eb31e1218a0892821e",
          "name": "Sheng Zhang",
          "hidden": false
        },
        {
          "_id": "6889a2eb31e1218a0892821f",
          "name": "Yihang Lian",
          "hidden": false
        },
        {
          "_id": "6889a2eb31e1218a08928220",
          "name": "Yulin Tsai",
          "hidden": false
        },
        {
          "_id": "6889a2eb31e1218a08928221",
          "name": "Lifu Wang",
          "hidden": false
        },
        {
          "_id": "6889a2eb31e1218a08928222",
          "name": "Sicong Liu",
          "hidden": false
        },
        {
          "_id": "6889a2eb31e1218a08928223",
          "name": "Puhua Jiang",
          "hidden": false
        },
        {
          "_id": "6889a2eb31e1218a08928224",
          "name": "Xianghui Yang",
          "hidden": false
        },
        {
          "_id": "6889a2eb31e1218a08928225",
          "name": "Dongyuan Guo",
          "hidden": false
        },
        {
          "_id": "6889a2eb31e1218a08928226",
          "name": "Yixuan Tang",
          "hidden": false
        },
        {
          "_id": "6889a2eb31e1218a08928227",
          "name": "Xinyue Mao",
          "hidden": false
        },
        {
          "_id": "6889a2eb31e1218a08928228",
          "name": "Jiaao Yu",
          "hidden": false
        },
        {
          "_id": "6889a2eb31e1218a08928229",
          "name": "Junlin Yu",
          "hidden": false
        },
        {
          "_id": "6889a2eb31e1218a0892822a",
          "name": "Jihong Zhang",
          "hidden": false
        },
        {
          "_id": "6889a2eb31e1218a0892822b",
          "name": "Meng Chen",
          "hidden": false
        },
        {
          "_id": "6889a2eb31e1218a0892822c",
          "name": "Liang Dong",
          "hidden": false
        },
        {
          "_id": "6889a2eb31e1218a0892822d",
          "name": "Yiwen Jia",
          "hidden": false
        },
        {
          "_id": "6889a2eb31e1218a0892822e",
          "name": "Chao Zhang",
          "hidden": false
        },
        {
          "_id": "6889a2eb31e1218a0892822f",
          "name": "Yonghao Tan",
          "hidden": false
        },
        {
          "_id": "6889a2eb31e1218a08928230",
          "name": "Hao Zhang",
          "hidden": false
        },
        {
          "_id": "6889a2eb31e1218a08928231",
          "name": "Zheng Ye",
          "hidden": false
        },
        {
          "_id": "6889a2eb31e1218a08928232",
          "name": "Peng He",
          "hidden": false
        },
        {
          "_id": "6889a2eb31e1218a08928233",
          "name": "Runzhou Wu",
          "hidden": false
        },
        {
          "_id": "6889a2eb31e1218a08928234",
          "name": "Minghui Chen",
          "hidden": false
        },
        {
          "_id": "6889a2eb31e1218a08928235",
          "name": "Zhan Li",
          "hidden": false
        },
        {
          "_id": "6889a2eb31e1218a08928236",
          "name": "Wangchen Qin",
          "hidden": false
        },
        {
          "_id": "6889a2eb31e1218a08928237",
          "name": "Lei Wang",
          "hidden": false
        },
        {
          "_id": "6889a2eb31e1218a08928238",
          "name": "Yifu Sun",
          "hidden": false
        },
        {
          "_id": "6889a2eb31e1218a08928239",
          "name": "Lin Niu",
          "hidden": false
        },
        {
          "_id": "6889a2eb31e1218a0892823a",
          "name": "Xiang Yuan",
          "hidden": false
        },
        {
          "_id": "6889a2eb31e1218a0892823b",
          "name": "Xiaofeng Yang",
          "hidden": false
        },
        {
          "_id": "6889a2eb31e1218a0892823c",
          "name": "Yingping He",
          "hidden": false
        },
        {
          "_id": "6889a2eb31e1218a0892823d",
          "name": "Jie Xiao",
          "hidden": false
        },
        {
          "_id": "6889a2eb31e1218a0892823e",
          "name": "Yangyu Tao",
          "hidden": false
        },
        {
          "_id": "6889a2eb31e1218a0892823f",
          "name": "Jianchen Zhu",
          "hidden": false
        },
        {
          "_id": "6889a2eb31e1218a08928240",
          "name": "Jinbao Xue",
          "hidden": false
        },
        {
          "_id": "6889a2eb31e1218a08928241",
          "name": "Kai Liu",
          "hidden": false
        },
        {
          "_id": "6889a2eb31e1218a08928242",
          "name": "Chongqing Zhao",
          "hidden": false
        },
        {
          "_id": "6889a2eb31e1218a08928243",
          "name": "Xinming Wu",
          "hidden": false
        },
        {
          "_id": "6889a2eb31e1218a08928244",
          "name": "Tian Liu",
          "hidden": false
        },
        {
          "_id": "6889a2eb31e1218a08928245",
          "name": "Peng Chen",
          "hidden": false
        },
        {
          "_id": "6889a2eb31e1218a08928246",
          "name": "Di Wang",
          "hidden": false
        },
        {
          "_id": "6889a2eb31e1218a08928247",
          "name": "Yuhong Liu",
          "hidden": false
        },
        {
          "_id": "6889a2eb31e1218a08928248",
          "name": "Linus",
          "hidden": false
        },
        {
          "_id": "6889a2eb31e1218a08928249",
          "name": "Jie Jiang",
          "hidden": false
        },
        {
          "_id": "6889a2eb31e1218a0892824a",
          "name": "Tengfei Wang",
          "hidden": false
        },
        {
          "_id": "6889a2eb31e1218a0892824b",
          "name": "Chunchao Guo",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62e7c26236a8e8a827ff0891/CWQGpX-tbR_8UIlT7YJwQ.mp4"
      ],
      "publishedAt": "2025-07-29T13:43:35.000Z",
      "submittedOnDailyAt": "2025-07-30T03:23:28.585Z",
      "title": "HunyuanWorld 1.0: Generating Immersive, Explorable, and Interactive 3D\n  Worlds from Words or Pixels",
      "submittedOnDailyBy": {
        "_id": "62e7c26236a8e8a827ff0891",
        "avatarUrl": "/avatars/15648407233bf5757262dbc04464d02e.svg",
        "isPro": false,
        "fullname": "Tengfei Wang",
        "user": "tfwang",
        "type": "user"
      },
      "summary": "Creating immersive and playable 3D worlds from texts or images remains a\nfundamental challenge in computer vision and graphics. Existing world\ngeneration approaches typically fall into two categories: video-based methods\nthat offer rich diversity but lack 3D consistency and rendering efficiency, and\n3D-based methods that provide geometric consistency but struggle with limited\ntraining data and memory-inefficient representations. To address these\nlimitations, we present HunyuanWorld 1.0, a novel framework that combines the\nbest of both worlds for generating immersive, explorable, and interactive 3D\nscenes from text and image conditions. Our approach features three key\nadvantages: 1) 360{\\deg} immersive experiences via panoramic world proxies; 2)\nmesh export capabilities for seamless compatibility with existing computer\ngraphics pipelines; 3) disentangled object representations for augmented\ninteractivity. The core of our framework is a semantically layered 3D mesh\nrepresentation that leverages panoramic images as 360{\\deg} world proxies for\nsemantic-aware world decomposition and reconstruction, enabling the generation\nof diverse 3D worlds. Extensive experiments demonstrate that our method\nachieves state-of-the-art performance in generating coherent, explorable, and\ninteractive 3D worlds while enabling versatile applications in virtual reality,\nphysical simulation, game development, and interactive content creation.",
      "upvotes": 32,
      "discussionId": "6889a2ec31e1218a0892824c",
      "projectPage": "https://3d-models.hunyuan.tencent.com/world/",
      "githubRepo": "https://github.com/Tencent-Hunyuan/HunyuanWorld-1.0",
      "ai_summary": "HunyuanWorld 1.0 generates immersive 3D scenes from text and images using a semantically layered 3D mesh representation with panoramic world proxies, offering 360Â° experiences, mesh export, and disentangled object representations.",
      "ai_keywords": [
        "panoramic world proxies",
        "semantically layered 3D mesh representation",
        "disentangled object representations"
      ],
      "githubStars": 1154
    },
    "publishedAt": "2025-07-29T09:43:35.000Z",
    "title": "HunyuanWorld 1.0: Generating Immersive, Explorable, and Interactive 3D\n  Worlds from Words or Pixels",
    "summary": "Creating immersive and playable 3D worlds from texts or images remains a\nfundamental challenge in computer vision and graphics. Existing world\ngeneration approaches typically fall into two categories: video-based methods\nthat offer rich diversity but lack 3D consistency and rendering efficiency, and\n3D-based methods that provide geometric consistency but struggle with limited\ntraining data and memory-inefficient representations. To address these\nlimitations, we present HunyuanWorld 1.0, a novel framework that combines the\nbest of both worlds for generating immersive, explorable, and interactive 3D\nscenes from text and image conditions. Our approach features three key\nadvantages: 1) 360{\\deg} immersive experiences via panoramic world proxies; 2)\nmesh export capabilities for seamless compatibility with existing computer\ngraphics pipelines; 3) disentangled object representations for augmented\ninteractivity. The core of our framework is a semantically layered 3D mesh\nrepresentation that leverages panoramic images as 360{\\deg} world proxies for\nsemantic-aware world decomposition and reconstruction, enabling the generation\nof diverse 3D worlds. Extensive experiments demonstrate that our method\nachieves state-of-the-art performance in generating coherent, explorable, and\ninteractive 3D worlds while enabling versatile applications in virtual reality,\nphysical simulation, game development, and interactive content creation.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62e7c26236a8e8a827ff0891/CWQGpX-tbR_8UIlT7YJwQ.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.21809.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "62e7c26236a8e8a827ff0891",
      "avatarUrl": "/avatars/15648407233bf5757262dbc04464d02e.svg",
      "fullname": "Tengfei Wang",
      "name": "tfwang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.22058",
      "authors": [
        {
          "_id": "6889746631e1218a08928182",
          "name": "Zigang Geng",
          "hidden": false
        },
        {
          "_id": "6889746631e1218a08928183",
          "name": "Yibing Wang",
          "hidden": false
        },
        {
          "_id": "6889746631e1218a08928184",
          "name": "Yeyao Ma",
          "hidden": false
        },
        {
          "_id": "6889746631e1218a08928185",
          "name": "Chen Li",
          "hidden": false
        },
        {
          "_id": "6889746631e1218a08928186",
          "name": "Yongming Rao",
          "hidden": false
        },
        {
          "_id": "6889746631e1218a08928187",
          "name": "Shuyang Gu",
          "hidden": false
        },
        {
          "_id": "6889746631e1218a08928188",
          "name": "Zhao Zhong",
          "hidden": false
        },
        {
          "_id": "6889746631e1218a08928189",
          "name": "Qinglin Lu",
          "hidden": false
        },
        {
          "_id": "6889746631e1218a0892818a",
          "name": "Han Hu",
          "hidden": false
        },
        {
          "_id": "6889746631e1218a0892818b",
          "name": "Xiaosong Zhang",
          "hidden": false
        },
        {
          "_id": "6889746631e1218a0892818c",
          "name": "Linus",
          "hidden": false
        },
        {
          "_id": "6889746631e1218a0892818d",
          "name": "Di Wang",
          "hidden": false
        },
        {
          "_id": "6889746631e1218a0892818e",
          "name": "Jie Jiang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-29T17:59:04.000Z",
      "submittedOnDailyAt": "2025-07-30T01:00:10.830Z",
      "title": "X-Omni: Reinforcement Learning Makes Discrete Autoregressive Image\n  Generative Models Great Again",
      "submittedOnDailyBy": {
        "_id": "64e473f0b78bc92221ab1883",
        "avatarUrl": "/avatars/85dd02c2b4879d7696b552f0f9dc2680.svg",
        "isPro": true,
        "fullname": "Xiaosong Zhang",
        "user": "zhangxiaosong18",
        "type": "user"
      },
      "summary": "Numerous efforts have been made to extend the ``next token prediction''\nparadigm to visual contents, aiming to create a unified approach for both image\ngeneration and understanding. Nevertheless, attempts to generate images through\nautoregressive modeling with discrete tokens have been plagued by issues such\nas low visual fidelity, distorted outputs, and failure to adhere to complex\ninstructions when rendering intricate details. These shortcomings are likely\nattributed to cumulative errors during autoregressive inference or information\nloss incurred during the discretization process. Probably due to this\nchallenge, recent research has increasingly shifted toward jointly training\nimage generation with diffusion objectives and language generation with\nautoregressive objectives, moving away from unified modeling approaches. In\nthis work, we demonstrate that reinforcement learning can effectively mitigate\nartifacts and largely enhance the generation quality of a discrete\nautoregressive modeling method, thereby enabling seamless integration of image\nand language generation. Our framework comprises a semantic image tokenizer, a\nunified autoregressive model for both language and images, and an offline\ndiffusion decoder for image generation, termed X-Omni. X-Omni achieves\nstate-of-the-art performance in image generation tasks using a 7B language\nmodel, producing images with high aesthetic quality while exhibiting strong\ncapabilities in following instructions and rendering long texts.",
      "upvotes": 13,
      "discussionId": "6889746631e1218a0892818f",
      "projectPage": "https://x-omni-team.github.io",
      "githubRepo": "https://github.com/X-Omni-Team/X-Omni",
      "ai_summary": "Reinforcement learning enhances discrete autoregressive modeling for image and language generation, achieving high-quality image generation and instruction-following capabilities.",
      "ai_keywords": [
        "autoregressive modeling",
        "discrete tokens",
        "visual fidelity",
        "distorted outputs",
        "cumulative errors",
        "information loss",
        "diffusion objectives",
        "language generation",
        "reinforcement learning",
        "semantic image tokenizer",
        "unified autoregressive model",
        "offline diffusion decoder",
        "X-Omni",
        "image generation",
        "aesthetic quality",
        "instruction-following"
      ],
      "githubStars": 49
    },
    "publishedAt": "2025-07-29T13:59:04.000Z",
    "title": "X-Omni: Reinforcement Learning Makes Discrete Autoregressive Image\n  Generative Models Great Again",
    "summary": "Numerous efforts have been made to extend the ``next token prediction''\nparadigm to visual contents, aiming to create a unified approach for both image\ngeneration and understanding. Nevertheless, attempts to generate images through\nautoregressive modeling with discrete tokens have been plagued by issues such\nas low visual fidelity, distorted outputs, and failure to adhere to complex\ninstructions when rendering intricate details. These shortcomings are likely\nattributed to cumulative errors during autoregressive inference or information\nloss incurred during the discretization process. Probably due to this\nchallenge, recent research has increasingly shifted toward jointly training\nimage generation with diffusion objectives and language generation with\nautoregressive objectives, moving away from unified modeling approaches. In\nthis work, we demonstrate that reinforcement learning can effectively mitigate\nartifacts and largely enhance the generation quality of a discrete\nautoregressive modeling method, thereby enabling seamless integration of image\nand language generation. Our framework comprises a semantic image tokenizer, a\nunified autoregressive model for both language and images, and an offline\ndiffusion decoder for image generation, termed X-Omni. X-Omni achieves\nstate-of-the-art performance in image generation tasks using a 7B language\nmodel, producing images with high aesthetic quality while exhibiting strong\ncapabilities in following instructions and rendering long texts.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.22058.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64e473f0b78bc92221ab1883",
      "avatarUrl": "/avatars/85dd02c2b4879d7696b552f0f9dc2680.svg",
      "fullname": "Xiaosong Zhang",
      "name": "zhangxiaosong18",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.21183",
      "authors": [
        {
          "_id": "68897f3631e1218a08928196",
          "name": "Guangchen Lan",
          "hidden": false
        },
        {
          "_id": "68897f3631e1218a08928197",
          "name": "Sipeng Zhang",
          "hidden": false
        },
        {
          "_id": "68897f3631e1218a08928198",
          "name": "Tianle Wang",
          "hidden": false
        },
        {
          "_id": "68897f3631e1218a08928199",
          "name": "Yuwei Zhang",
          "hidden": false
        },
        {
          "_id": "68897f3631e1218a0892819a",
          "name": "Daoan Zhang",
          "hidden": false
        },
        {
          "_id": "68897f3631e1218a0892819b",
          "name": "Xinpeng Wei",
          "hidden": false
        },
        {
          "_id": "68897f3631e1218a0892819c",
          "name": "Xiaoman Pan",
          "hidden": false
        },
        {
          "_id": "68897f3631e1218a0892819d",
          "name": "Hongming Zhang",
          "hidden": false
        },
        {
          "_id": "68897f3631e1218a0892819e",
          "name": "Dong-Jun Han",
          "hidden": false
        },
        {
          "_id": "68897f3631e1218a0892819f",
          "name": "Christopher G. Brinton",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-27T05:26:50.000Z",
      "submittedOnDailyAt": "2025-07-30T00:41:59.921Z",
      "title": "MaPPO: Maximum a Posteriori Preference Optimization with Prior Knowledge",
      "submittedOnDailyBy": {
        "_id": "64ff4b1a0e8369f6a8c47c7e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ff4b1a0e8369f6a8c47c7e/X8ocOagj89gexSa253q_8.png",
        "isPro": false,
        "fullname": "Eric Lan",
        "user": "Eric-Lan",
        "type": "user"
      },
      "summary": "As the era of large language models (LLMs) on behalf of users unfolds,\nPreference Optimization (PO) methods have become a central approach to aligning\nLLMs with human preferences and improving performance. We propose Maximum a\nPosteriori Preference Optimization (MaPPO), a framework for learning from\npreferences that explicitly incorporates prior reward knowledge into the\noptimization objective. While existing methods such as Direct Preference\nOptimization (DPO) and its variants treat preference learning as a Maximum\nLikelihood Estimation (MLE) problem, MaPPO extends this paradigm by integrating\nprior reward estimates into a principled Maximum a Posteriori (MaP) objective.\nThis not only generalizes DPO and its variants, but also enhances alignment by\nmitigating the oversimplified binary classification of responses. More\nimportantly, MaPPO introduces no additional hyperparameter, and supports\npreference optimization in both offline and online settings. In addition, MaPPO\ncan be used as a plugin with consistent improvement on DPO variants, including\nwidely used SimPO, IPO, and CPO. Extensive empirical evaluations of different\nmodel sizes and model series on three standard benchmarks, including MT-Bench,\nAlpacaEval 2.0, and Arena-Hard, demonstrate consistent improvements in\nalignment performance without sacrificing computational efficiency.",
      "upvotes": 5,
      "discussionId": "68897f3731e1218a089281a0",
      "ai_summary": "MaPPO, a framework for preference optimization, enhances alignment of large language models with human preferences by integrating prior reward knowledge into a Maximum a Posteriori objective, improving performance across various benchmarks.",
      "ai_keywords": [
        "Maximum a Posteriori Preference Optimization",
        "MaPPO",
        "Preference Optimization",
        "Direct Preference Optimization",
        "DPO",
        "Maximum Likelihood Estimation",
        "MLE",
        "Maximum a Posteriori",
        "MaP",
        "SimPO",
        "IPO",
        "CPO",
        "MT-Bench",
        "AlpacaEval 2.0",
        "Arena-Hard"
      ]
    },
    "publishedAt": "2025-07-27T01:26:50.000Z",
    "title": "MaPPO: Maximum a Posteriori Preference Optimization with Prior Knowledge",
    "summary": "As the era of large language models (LLMs) on behalf of users unfolds,\nPreference Optimization (PO) methods have become a central approach to aligning\nLLMs with human preferences and improving performance. We propose Maximum a\nPosteriori Preference Optimization (MaPPO), a framework for learning from\npreferences that explicitly incorporates prior reward knowledge into the\noptimization objective. While existing methods such as Direct Preference\nOptimization (DPO) and its variants treat preference learning as a Maximum\nLikelihood Estimation (MLE) problem, MaPPO extends this paradigm by integrating\nprior reward estimates into a principled Maximum a Posteriori (MaP) objective.\nThis not only generalizes DPO and its variants, but also enhances alignment by\nmitigating the oversimplified binary classification of responses. More\nimportantly, MaPPO introduces no additional hyperparameter, and supports\npreference optimization in both offline and online settings. In addition, MaPPO\ncan be used as a plugin with consistent improvement on DPO variants, including\nwidely used SimPO, IPO, and CPO. Extensive empirical evaluations of different\nmodel sizes and model series on three standard benchmarks, including MT-Bench,\nAlpacaEval 2.0, and Arena-Hard, demonstrate consistent improvements in\nalignment performance without sacrificing computational efficiency.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.21183.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ff4b1a0e8369f6a8c47c7e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ff4b1a0e8369f6a8c47c7e/X8ocOagj89gexSa253q_8.png",
      "fullname": "Eric Lan",
      "name": "Eric-Lan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.14111",
      "authors": [
        {
          "_id": "687dcba52e8db0930be6f0fc",
          "user": {
            "_id": "6803ddda5c044d396d02f03a",
            "avatarUrl": "/avatars/ec583f6560d3d82fbd3bf95fb83c9356.svg",
            "isPro": false,
            "fullname": "Xiaoya Li",
            "user": "xxiaoyali",
            "type": "user"
          },
          "name": "Xiaoya Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-21T08:29:30.038Z",
          "hidden": false
        },
        {
          "_id": "687dcba52e8db0930be6f0fd",
          "name": "Xiaofei Sun",
          "hidden": false
        },
        {
          "_id": "687dcba52e8db0930be6f0fe",
          "name": "Albert Wang",
          "hidden": false
        },
        {
          "_id": "687dcba52e8db0930be6f0ff",
          "name": "Jiwei Li",
          "hidden": false
        },
        {
          "_id": "687dcba52e8db0930be6f100",
          "name": "Chris Shum",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6803ddda5c044d396d02f03a/hoz_EU0XjHay8KuqruSsw.png"
      ],
      "publishedAt": "2025-07-18T17:43:56.000Z",
      "submittedOnDailyAt": "2025-07-30T06:10:17.267Z",
      "title": "CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement\n  Learning",
      "submittedOnDailyBy": {
        "_id": "6803ddda5c044d396d02f03a",
        "avatarUrl": "/avatars/ec583f6560d3d82fbd3bf95fb83c9356.svg",
        "isPro": false,
        "fullname": "Xiaoya Li",
        "user": "xxiaoyali",
        "type": "user"
      },
      "summary": "The exponential growth in demand for GPU computing resources, driven by the\nrapid advancement of Large Language Models, has created an urgent need for\nautomated CUDA optimization strategies. While recent advances in LLMs show\npromise for code generation, current SOTA models (e.g. R1, o1) achieve low\nsuccess rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an\nautomated reinforcement learning framework for CUDA optimization.\n  CUDA-L1 achieves performance improvements on the CUDA optimization task:\ntrained on NVIDIA A100, it delivers an average speedup of x17.7 across all 250\nCUDA kernels of KernelBench, with peak speedups reaching x449. Furthermore, the\nmodel also demonstrates excellent portability across GPU architectures,\nachieving average speedups of x17.8 on H100, x19.0 on RTX 3090, x16.5 on L40,\nx14.7 on H800, and x13.9 on H20 despite being optimized specifically for A100.\nBeyond these benchmark results, CUDA-L1 demonstrates several remarkable\nproperties: 1) Discovers a variety of CUDA optimization techniques and learns\nto combine them strategically to achieve optimal performance; 2) Uncovers\nfundamental principles of CUDA optimization; 3) Identifies non-obvious\nperformance bottlenecks and rejects seemingly beneficial optimizations that\nharm performance.\n  The capabilities of CUDA-L1 demonstrate that reinforcement learning can\ntransform an initially poor-performing LLM into an effective CUDA optimizer\nthrough speedup-based reward signals alone, without human expertise or domain\nknowledge. More importantly, the trained RL model extend the acquired reasoning\nabilities to new kernels. This paradigm opens possibilities for automated\noptimization of CUDA operations, and holds promise to substantially promote GPU\nefficiency and alleviate the rising pressure on GPU computing resources.",
      "upvotes": 5,
      "discussionId": "687dcba52e8db0930be6f101",
      "ai_summary": "CUDA-L1, an automated reinforcement learning framework, significantly improves CUDA optimization across various GPU architectures, achieving substantial speedups without human expertise.",
      "ai_keywords": [
        "reinforcement learning",
        "CUDA optimization",
        "CUDA-L1",
        "KernelBench",
        "GPU architectures",
        "NVIDIA A100",
        "H100",
        "RTX 3090",
        "L40",
        "H800",
        "H20",
        "speedup",
        "performance bottlenecks"
      ]
    },
    "publishedAt": "2025-07-18T13:43:56.000Z",
    "title": "CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement\n  Learning",
    "summary": "The exponential growth in demand for GPU computing resources, driven by the\nrapid advancement of Large Language Models, has created an urgent need for\nautomated CUDA optimization strategies. While recent advances in LLMs show\npromise for code generation, current SOTA models (e.g. R1, o1) achieve low\nsuccess rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an\nautomated reinforcement learning framework for CUDA optimization.\n  CUDA-L1 achieves performance improvements on the CUDA optimization task:\ntrained on NVIDIA A100, it delivers an average speedup of x17.7 across all 250\nCUDA kernels of KernelBench, with peak speedups reaching x449. Furthermore, the\nmodel also demonstrates excellent portability across GPU architectures,\nachieving average speedups of x17.8 on H100, x19.0 on RTX 3090, x16.5 on L40,\nx14.7 on H800, and x13.9 on H20 despite being optimized specifically for A100.\nBeyond these benchmark results, CUDA-L1 demonstrates several remarkable\nproperties: 1) Discovers a variety of CUDA optimization techniques and learns\nto combine them strategically to achieve optimal performance; 2) Uncovers\nfundamental principles of CUDA optimization; 3) Identifies non-obvious\nperformance bottlenecks and rejects seemingly beneficial optimizations that\nharm performance.\n  The capabilities of CUDA-L1 demonstrate that reinforcement learning can\ntransform an initially poor-performing LLM into an effective CUDA optimizer\nthrough speedup-based reward signals alone, without human expertise or domain\nknowledge. More importantly, the trained RL model extend the acquired reasoning\nabilities to new kernels. This paradigm opens possibilities for automated\noptimization of CUDA operations, and holds promise to substantially promote GPU\nefficiency and alleviate the rising pressure on GPU computing resources.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6803ddda5c044d396d02f03a/hoz_EU0XjHay8KuqruSsw.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.14111.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6803ddda5c044d396d02f03a",
      "avatarUrl": "/avatars/ec583f6560d3d82fbd3bf95fb83c9356.svg",
      "fullname": "Xiaoya Li",
      "name": "xxiaoyali",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.20240",
      "authors": [
        {
          "_id": "6889915e31e1218a089281c3",
          "name": "Risa Shinoda",
          "hidden": false
        },
        {
          "_id": "6889915e31e1218a089281c4",
          "name": "Nakamasa Inoue",
          "hidden": false
        },
        {
          "_id": "6889915e31e1218a089281c5",
          "name": "Iro Laina",
          "hidden": false
        },
        {
          "_id": "6889915e31e1218a089281c6",
          "name": "Christian Rupprecht",
          "hidden": false
        },
        {
          "_id": "6889915e31e1218a089281c7",
          "name": "Hirokatsu Kataoka",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/651a7684a1a5e5d617e28f84/kRNGOtlujvozkvra7doeL.png"
      ],
      "publishedAt": "2025-07-27T11:48:03.000Z",
      "submittedOnDailyAt": "2025-07-30T02:01:13.370Z",
      "title": "AnimalClue: Recognizing Animals by their Traces",
      "submittedOnDailyBy": {
        "_id": "651a7684a1a5e5d617e28f84",
        "avatarUrl": "/avatars/f484ae7c8d980818cd2ba3ffa682b781.svg",
        "isPro": true,
        "fullname": "Risa Shinoda",
        "user": "risashinoda",
        "type": "user"
      },
      "summary": "Wildlife observation plays an important role in biodiversity conservation,\nnecessitating robust methodologies for monitoring wildlife populations and\ninterspecies interactions. Recent advances in computer vision have\nsignificantly contributed to automating fundamental wildlife observation tasks,\nsuch as animal detection and species identification. However, accurately\nidentifying species from indirect evidence like footprints and feces remains\nrelatively underexplored, despite its importance in contributing to wildlife\nmonitoring. To bridge this gap, we introduce AnimalClue, the first large-scale\ndataset for species identification from images of indirect evidence. Our\ndataset consists of 159,605 bounding boxes encompassing five categories of\nindirect clues: footprints, feces, eggs, bones, and feathers. It covers 968\nspecies, 200 families, and 65 orders. Each image is annotated with\nspecies-level labels, bounding boxes or segmentation masks, and fine-grained\ntrait information, including activity patterns and habitat preferences. Unlike\nexisting datasets primarily focused on direct visual features (e.g., animal\nappearances), AnimalClue presents unique challenges for classification,\ndetection, and instance segmentation tasks due to the need for recognizing more\ndetailed and subtle visual features. In our experiments, we extensively\nevaluate representative vision models and identify key challenges in animal\nidentification from their traces. Our dataset and code are available at\nhttps://dahlian00.github.io/AnimalCluePage/",
      "upvotes": 3,
      "discussionId": "6889915f31e1218a089281c8",
      "projectPage": "https://dahlian00.github.io/AnimalCluePage/",
      "githubRepo": "https://github.com/dahlian00/AnimalClue",
      "ai_summary": "AnimalClue is a large-scale dataset for species identification from indirect evidence images, addressing challenges in classification and segmentation tasks.",
      "ai_keywords": [
        "bounding boxes",
        "segmentation masks",
        "classification",
        "detection",
        "instance segmentation"
      ],
      "githubStars": 2
    },
    "publishedAt": "2025-07-27T07:48:03.000Z",
    "title": "AnimalClue: Recognizing Animals by their Traces",
    "summary": "Wildlife observation plays an important role in biodiversity conservation,\nnecessitating robust methodologies for monitoring wildlife populations and\ninterspecies interactions. Recent advances in computer vision have\nsignificantly contributed to automating fundamental wildlife observation tasks,\nsuch as animal detection and species identification. However, accurately\nidentifying species from indirect evidence like footprints and feces remains\nrelatively underexplored, despite its importance in contributing to wildlife\nmonitoring. To bridge this gap, we introduce AnimalClue, the first large-scale\ndataset for species identification from images of indirect evidence. Our\ndataset consists of 159,605 bounding boxes encompassing five categories of\nindirect clues: footprints, feces, eggs, bones, and feathers. It covers 968\nspecies, 200 families, and 65 orders. Each image is annotated with\nspecies-level labels, bounding boxes or segmentation masks, and fine-grained\ntrait information, including activity patterns and habitat preferences. Unlike\nexisting datasets primarily focused on direct visual features (e.g., animal\nappearances), AnimalClue presents unique challenges for classification,\ndetection, and instance segmentation tasks due to the need for recognizing more\ndetailed and subtle visual features. In our experiments, we extensively\nevaluate representative vision models and identify key challenges in animal\nidentification from their traces. Our dataset and code are available at\nhttps://dahlian00.github.io/AnimalCluePage/",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/651a7684a1a5e5d617e28f84/kRNGOtlujvozkvra7doeL.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.20240.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "651a7684a1a5e5d617e28f84",
      "avatarUrl": "/avatars/f484ae7c8d980818cd2ba3ffa682b781.svg",
      "fullname": "Risa Shinoda",
      "name": "risashinoda",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 0
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.22061",
      "authors": [
        {
          "_id": "6889763131e1218a08928191",
          "name": "Kaining Ying",
          "hidden": false
        },
        {
          "_id": "6889763131e1218a08928192",
          "name": "Hengrui Hu",
          "hidden": false
        },
        {
          "_id": "6889763131e1218a08928193",
          "name": "Henghui Ding",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-29T17:59:35.000Z",
      "submittedOnDailyAt": "2025-07-30T00:04:41.715Z",
      "title": "MOVE: Motion-Guided Few-Shot Video Object Segmentation",
      "submittedOnDailyBy": {
        "_id": "67ff29ecbf6889a333c69c7a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/fYIJFtF0uciB-1wb0lmTY.png",
        "isPro": false,
        "fullname": "Henghui Ding",
        "user": "HenghuiDing",
        "type": "user"
      },
      "summary": "This work addresses motion-guided few-shot video object segmentation (FSVOS),\nwhich aims to segment dynamic objects in videos based on a few annotated\nexamples with the same motion patterns. Existing FSVOS datasets and methods\ntypically focus on object categories, which are static attributes that ignore\nthe rich temporal dynamics in videos, limiting their application in scenarios\nrequiring motion understanding. To fill this gap, we introduce MOVE, a\nlarge-scale dataset specifically designed for motion-guided FSVOS. Based on\nMOVE, we comprehensively evaluate 6 state-of-the-art methods from 3 different\nrelated tasks across 2 experimental settings. Our results reveal that current\nmethods struggle to address motion-guided FSVOS, prompting us to analyze the\nassociated challenges and propose a baseline method, Decoupled Motion\nAppearance Network (DMA). Experiments demonstrate that our approach achieves\nsuperior performance in few shot motion understanding, establishing a solid\nfoundation for future research in this direction.",
      "upvotes": 2,
      "discussionId": "6889763131e1218a08928194",
      "ai_summary": "A new dataset and baseline method for motion-guided few-shot video object segmentation are introduced, addressing challenges in motion understanding.",
      "ai_keywords": [
        "few-shot video object segmentation",
        "FSVOS",
        "motion-guided",
        "temporal dynamics",
        "Decoupled Motion Appearance Network",
        "DMA"
      ]
    },
    "publishedAt": "2025-07-29T13:59:35.000Z",
    "title": "MOVE: Motion-Guided Few-Shot Video Object Segmentation",
    "summary": "This work addresses motion-guided few-shot video object segmentation (FSVOS),\nwhich aims to segment dynamic objects in videos based on a few annotated\nexamples with the same motion patterns. Existing FSVOS datasets and methods\ntypically focus on object categories, which are static attributes that ignore\nthe rich temporal dynamics in videos, limiting their application in scenarios\nrequiring motion understanding. To fill this gap, we introduce MOVE, a\nlarge-scale dataset specifically designed for motion-guided FSVOS. Based on\nMOVE, we comprehensively evaluate 6 state-of-the-art methods from 3 different\nrelated tasks across 2 experimental settings. Our results reveal that current\nmethods struggle to address motion-guided FSVOS, prompting us to analyze the\nassociated challenges and propose a baseline method, Decoupled Motion\nAppearance Network (DMA). Experiments demonstrate that our approach achieves\nsuperior performance in few shot motion understanding, establishing a solid\nfoundation for future research in this direction.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.22061.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67ff29ecbf6889a333c69c7a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/fYIJFtF0uciB-1wb0lmTY.png",
      "fullname": "Henghui Ding",
      "name": "HenghuiDing",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.21364",
      "authors": [
        {
          "_id": "6889808631e1218a089281a2",
          "name": "Lukman Jibril Aliyu",
          "hidden": false
        },
        {
          "_id": "6889808631e1218a089281a3",
          "name": "Umar Sani Muhammad",
          "hidden": false
        },
        {
          "_id": "6889808631e1218a089281a4",
          "name": "Bilqisu Ismail",
          "hidden": false
        },
        {
          "_id": "6889808631e1218a089281a5",
          "name": "Nasiru Muhammad",
          "hidden": false
        },
        {
          "_id": "6889808631e1218a089281a6",
          "name": "Almustapha A Wakili",
          "hidden": false
        },
        {
          "_id": "6889808631e1218a089281a7",
          "name": "Seid Muhie Yimam",
          "hidden": false
        },
        {
          "_id": "6889808631e1218a089281a8",
          "name": "Shamsuddeen Hassan Muhammad",
          "hidden": false
        },
        {
          "_id": "6889808631e1218a089281a9",
          "name": "Mustapha Abdullahi",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64e88f2ff2f5545edaff5083/pkJkGJ8E36E41YOorr0DA.png"
      ],
      "publishedAt": "2025-07-28T22:18:13.000Z",
      "submittedOnDailyAt": "2025-07-30T00:55:14.904Z",
      "title": "Evaluating Deep Learning Models for African Wildlife Image\n  Classification: From DenseNet to Vision Transformers",
      "submittedOnDailyBy": {
        "_id": "64e88f2ff2f5545edaff5083",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e88f2ff2f5545edaff5083/ckZTNArwRsH5RfJksL-YJ.jpeg",
        "isPro": false,
        "fullname": "Lukman Jibril Aliyu",
        "user": "lukmanaj",
        "type": "user"
      },
      "summary": "Wildlife populations in Africa face severe threats, with vertebrate numbers\ndeclining by over 65% in the past five decades. In response, image\nclassification using deep learning has emerged as a promising tool for\nbiodiversity monitoring and conservation. This paper presents a comparative\nstudy of deep learning models for automatically classifying African wildlife\nimages, focusing on transfer learning with frozen feature extractors. Using a\npublic dataset of four species: buffalo, elephant, rhinoceros, and zebra; we\nevaluate the performance of DenseNet-201, ResNet-152, EfficientNet-B4, and\nVision Transformer ViT-H/14. DenseNet-201 achieved the best performance among\nconvolutional networks (67% accuracy), while ViT-H/14 achieved the highest\noverall accuracy (99%), but with significantly higher computational cost,\nraising deployment concerns. Our experiments highlight the trade-offs between\naccuracy, resource requirements, and deployability. The best-performing CNN\n(DenseNet-201) was integrated into a Hugging Face Gradio Space for real-time\nfield use, demonstrating the feasibility of deploying lightweight models in\nconservation settings. This work contributes to African-grounded AI research by\noffering practical insights into model selection, dataset preparation, and\nresponsible deployment of deep learning tools for wildlife conservation.",
      "upvotes": 1,
      "discussionId": "6889808631e1218a089281aa",
      "ai_summary": "A comparative study of deep learning models for wildlife image classification highlights trade-offs between accuracy, resource requirements, and deployability, with DenseNet-201 and Vision Transformer ViT-H/14 performing best among evaluated models.",
      "ai_keywords": [
        "transfer learning",
        "frozen feature extractors",
        "DenseNet-201",
        "ResNet-152",
        "EfficientNet-B4",
        "Vision Transformer ViT-H/14",
        "convolutional networks",
        "Hugging Face Gradio Space"
      ]
    },
    "publishedAt": "2025-07-28T18:18:13.000Z",
    "title": "Evaluating Deep Learning Models for African Wildlife Image\n  Classification: From DenseNet to Vision Transformers",
    "summary": "Wildlife populations in Africa face severe threats, with vertebrate numbers\ndeclining by over 65% in the past five decades. In response, image\nclassification using deep learning has emerged as a promising tool for\nbiodiversity monitoring and conservation. This paper presents a comparative\nstudy of deep learning models for automatically classifying African wildlife\nimages, focusing on transfer learning with frozen feature extractors. Using a\npublic dataset of four species: buffalo, elephant, rhinoceros, and zebra; we\nevaluate the performance of DenseNet-201, ResNet-152, EfficientNet-B4, and\nVision Transformer ViT-H/14. DenseNet-201 achieved the best performance among\nconvolutional networks (67% accuracy), while ViT-H/14 achieved the highest\noverall accuracy (99%), but with significantly higher computational cost,\nraising deployment concerns. Our experiments highlight the trade-offs between\naccuracy, resource requirements, and deployability. The best-performing CNN\n(DenseNet-201) was integrated into a Hugging Face Gradio Space for real-time\nfield use, demonstrating the feasibility of deploying lightweight models in\nconservation settings. This work contributes to African-grounded AI research by\noffering practical insights into model selection, dataset preparation, and\nresponsible deployment of deep learning tools for wildlife conservation.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64e88f2ff2f5545edaff5083/pkJkGJ8E36E41YOorr0DA.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.21364.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64e88f2ff2f5545edaff5083",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e88f2ff2f5545edaff5083/ckZTNArwRsH5RfJksL-YJ.jpeg",
      "fullname": "Lukman Jibril Aliyu",
      "name": "lukmanaj",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 37
    },
    "isAuthorParticipating": false
  }
]